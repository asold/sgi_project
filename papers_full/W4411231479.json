{
  "title": "RAGnosis: Retrieval-Augmented Generation for Enhanced Medical Decision Making",
  "url": "https://openalex.org/W4411231479",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2062828726",
      "name": "Amir Rouhollahi",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2055046499",
      "name": "Ali Homaei",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2759020636",
      "name": "Aanchal Sahu",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5099592582",
      "name": "Rayan Ebnali Harari",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4202840094",
      "name": "Farhad R. Nezami",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2062828726",
      "name": "Amir Rouhollahi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2055046499",
      "name": "Ali Homaei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2759020636",
      "name": "Aanchal Sahu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5099592582",
      "name": "Rayan Ebnali Harari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202840094",
      "name": "Farhad R. Nezami",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319985906",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4401953966",
    "https://openalex.org/W4406813548",
    "https://openalex.org/W4367672504",
    "https://openalex.org/W4406596702",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W3137066173",
    "https://openalex.org/W58456790"
  ],
  "abstract": "Abstract We present RAGnosis, a fully offline, retrieval-augmented framework for interpreting unstructured clinical text using open-weight large language models. As a proof of concept, we apply RAGnosis to the task of paravalvular leak (PVL) classification from cardiac catheterization reports, a process that typically requires slow, expert-driven interpretation. The system combines local OCR, semantic retrieval, and instruction-tuned LLMs to generate evidence-backed predictions and explanations grounded in real clinical documentation. We evaluate four models (DeepSeek 70B, Gemma 27B, Mistral 7B, LLaMA 3B) across 100 reports, analyzing classification accuracy, explanation quality, and retrieval relevance. Results highlight tradeoffs between fluency and reliability, with DeepSeek demonstrating the most consistent performance. By operating entirely on-prem and supporting modular integration, RAGnosis provides a scalable and interpretable foundation for clinical NLP that delivers not just answers but traceable reasoning.",
  "full_text": "RAGnosis: Retrieval-Augmented Generation for\nEnhanced Medical Decision Making\nAmir Rouhollahia, Ali Homaei a, Aanchal Sahu a, Rayan Ebnali Harari bc, Farhad R. Nezami a\naDivision of Cardiac Surgery, Brigham and Women’s Hospital, Harvard Medical School, Boston, MA, USA\nbDepartment of Radiology, Mass General Brigham, Harvard Medical School, Boston, MA, USA\ncHarvard Data Science Initiative (HDSI), Harvard University, Boston, MA, USA\nEmail: frikhtegarnezami@bwh.harvard.edu\nAbstract—We present RAGnosis, a fully offline, retrieval-\naugmented framework for interpreting unstructured clinical text\nusing open-weight large language models. As a proof of concept,\nwe apply RAGnosis to the task of paravalvular leak (PVL)\nclassification from cardiac catheterization reports, a process that\ntypically requires slow, expert-driven interpretation. The system\ncombines local OCR, semantic retrieval, and instruction-tuned\nLLMs to generate evidence-backed predictions and explanations\ngrounded in real clinical documentation. We evaluate four models\n(DeepSeek 70B, Gemma 27B, Mistral 7B, LLaMA 3B) across 100\nreports, analyzing classification accuracy, explanation quality,\nand retrieval relevance. Results highlight tradeoffs between\nfluency and reliability, with DeepSeek demonstrating the most\nconsistent performance. By operating entirely on-prem and\nsupporting modular integration, RAGnosis provides a scalable\nand interpretable foundation for clinical NLP that delivers not\njust answers but traceable reasoning.\nIndex Terms—Retrieval-Augmented Generation, Large Lan-\nguage Models, Artificial Intelligence, Paravalvular Leak, Clinical\nData Extraction\nI. I NTRODUCTION\nElectronic Health Records (EHRs) are the cornerstone of\ncontemporary digital healthcare systems, capturing a hetero-\ngeneous mix of structured data, such as laboratory values and\nbilling codes, and unstructured clinical narratives, including\nfree-text notes, diagnostic reports, and scanned handwritten\ndocuments. The inherent complexity and scale of EHR data\nmake it a typical example of big data in medicine, necessitating\nnovel computational approaches to extract meaningful insights\nefficiently. Despite widespread adoption of EHRs, critical\ninformation often remains underutilized due to the limitations\nof conventional data processing techniques and the lack of\nsemantic interoperability across systems [1,2].\nTo address these challenges, large language models (LLMs)\nhave garnered increasing interest as scalable tools capable of\ninterpreting and synthesizing vast amounts of clinical text.\nLLMs, especially those built on transformer architectures,\nexhibit emergent capabilities in summarization, translation,\nand domain-specific question answering when trained on\nlarge corpora through self-supervision [3,4]. However, general-\npurpose LLMs often falter in clinical environments due to their\nlimited exposure to medical data during pretraining and a lack\nof instruction tuning aligned with real-world healthcare tasks\n[4,5]. These limitations have raised concerns regarding factual\naccuracy, clinical safety, and the potential for hallucinated\ncontent.\nRetrieval-Augmented Generation (RAG) frameworks offer a\npromising solution by combining two discrete steps: semantic\nretrieval of relevant documents from a trusted corpus and\nconditioned text generation by an LLM [6,7]. This dual-stage\napproach significantly enhances the factual consistency, trace-\nability, and clinical contextualization of generated outputs,\nmaking RAG particularly suitable for decision support tasks\nin medicine [7,8]. In clinical applications, RAG systems have\ndemonstrated the ability to ground responses in authoritative\nsources, thus improving interpretability and reducing the prop-\nagation of misleading information [6].\nAlthough useful, there are several limitations why using\nsuch technology can be challenging such as (i) preserving\npatient data and being able to maintain data privacy since most\nof the high performance LLMs are commercialized and the\nprocess will be done on a third party cloud service which is\nnot ideal for our use case; (ii) LLMs are prone to hallucination\nand not having access to the current literature and patient\ndocuments will make the process less reliable. To address\nthese challenges, we present RAGnosis, an offline, end-to-\nend RAG-based clinical platform that integrates state-of-the-\nart optical character recognition (OCR), local vector databases,\nand instruction-tuned, open-source high-performance LLMs.\nThis privacy-oriented RAG module currently runs on our local\nserver at Brigham and Women’s Hospital. Upon receiving a\nnatural-language query from a clinician, the system converts\nthe query into a dense semantic embedding, retrieves the\ntop-ranked segments from an indexed corpus of anonymized\nclinical notes, and generates a synthesized answer supported\nby in-text citations. This architecture not only maximizes\ntransparency and accuracy but also ensures compliance with\ndata governance protocols by avoiding external data transfer.\nII. B ACKGROUND AND RELATED WORK\nMultiple efforts have underscored the efficacy of integrating\nretrieval-based methods into generative AI workflows for\nmedicine. The Almanac system, for example, demonstrated\nthat augmenting LLM outputs with guideline-based retrieval\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nsignificantly improved factual alignment and reduced halluci-\nnations in clinical scenarios [7]. Similarly, the CLEAR frame-\nwork employed named entity recognition to refine document\nretrieval and improve the relevance of generated summaries,\nthereby enhancing both specificity and precision [8]. These\nsystems laid the foundation for combining NLP pipelines with\nstructured clinical knowledge.\nDespite such progress, the medical community has histor-\nically played a passive role in the creation and adoption of\nLLMs, emphasizing the urgent need for clinicians to actively\nparticipate in defining the scope, training data, and evaluation\nmetrics of medical LLMs. Without this involvement, generic\nmodels may continue to dominate, introducing risks of biased\noutputs and unverified clinical recommendations [4]. More-\nover, studies have shown that tuning models with task-specific\ninstructions can substantially enhance their performance, es-\npecially in domains characterized by complex, jargon-rich\nlanguage such as medicine [5].\nBenchmarking initiatives like MIRAGE have further re-\nvealed that RAG performance is highly sensitive to retriever\nconfiguration, corpus curation, and task formulation, under-\nscoring the importance of rigorous empirical evaluation [6].\nThese findings highlight the necessity of systematic testing\nusing real-world clinical scenarios to ensure model reliabil-\nity and reproducibility [9] provide compelling evidence that\nadapted LLMs can outperform experienced clinicians in text\nsummarization, suggesting that well-tuned models can allevi-\nate documentation burdens without compromising quality.\nRecent research has also explored cost-effective strategies\nfor LLM development. For instance, smaller instruction-tuned\nmodels such as Stanford’s Alpaca, trained on curated clinical\ndatasets, have achieved competitive performance at a fraction\nof the computational cost, indicating that scalability and af-\nfordability need not be mutually exclusive [4]. Furthermore,\nRAG-enabled architectures have shown utility in varied tasks,\nincluding social media monitoring for pharmacovigilance,\nclinical trial eligibility screening, and rapid cohort identifi-\ncation, demonstrating their versatility across medical domains\n[6,8].\nRAGnosis builds upon these insights to deliver a clinician-\ncentered, privacy-preserving platform that synthesizes clinical\ninformation into actionable, verifiable outputs. By combining\niterative retrieval, secure computation, and medical LLMs\nfine-tuned for local contexts, RAGnosis not only enhances\ndecision making,, but also positions clinicians at the helm of\nAI deployment in healthcare.\nIII. S IGNIFICANCE\nThe significance of this module is multiple folds including\nbut not limited to: (i) clinical significance: by providing\nrelevant and evidence-based institution-specific and patient-\nspecific information supporting the clinical decision-making;\n(ii) privacy and compliance: our RAG module ensures HIPAA\ncompliance and patient data privacy by running the whole\npipeline completely offline and locally on our servers at BWH;\n(iii) knowledge integration: the RAG model not only uses\nthe currently available medical knowledge database but it\nincorporates real patient outcomes and treatment responses.\nThis feedback loop will improve the reliability and relevance\nof the AI suggestions; (iv) scalability: the module architecture\nallows seamless integration of new real patient reports as well\nas newly published medical literature to update the guidelines.\nFor this study, we focus on paravalvular leak (PVL), a high-\nimpact complication that occurs when blood flows between the\nprosthetic valve and surrounding tissue following surgical or\ntranscatheter aortic valve replacement (TA VR). PVL is com-\nmon, and while trace or mild cases may be benign, moderate\nto severe PVL has been associated with hemolysis, heart\nfailure, rehospitalization, and increased long-term mortality\n[10]. Accurate grading of PVL severity is essential for clinical\ndecision-making but remains a slow, expert-dependent process\nthat requires interpreting complex procedural reports, imaging\nfindings, and hemodynamic data. These observations are often\nembedded in unstructured text, making large-scale analysis\ndifficult using traditional methods. We have selected PVL\nclassification as the initial focus of the RAGnosis framework,\nwhich can be expanded into other use cases over time. We use\nthis framework to automate the interpretation of PVL severity\nfrom cardiac catheterization reports. It combines a vector sim-\nilarity retriever with a locally hosted LLM to identify, explain,\nand classify PVL from procedural text. Each model receives a\npatient report, retrieves semantically similar statements from\na local corpus, and outputs a PVL classification (e.g., severe,\nmoderate, trace) along with an explanation and textual evi-\ndence. We evaluate RAGNosis across four open-weight LLMs\n- DeepSeek 70B, Gemma 27B, Mistral 7, and LLaMA 3.2 on\na test set of 100 anonymized reports. Each model is assessed\nnot only for accuracy but also for explanation quality and\nretrieval relevance. This approach represents a practical step\ntoward interpretable AI in medicine, grounded in real clinical\ndocumentation, evaluated across multiple open models, and\naligned with privacy-preserving deployment.\nIV. M ETHODS AND EXPERIMENTS\nOur pipeline ingests raw cardiology reports (PDF or plain\ntext), converts them to structured text, embeds clinically rel-\nevant information, embeds them in a vector store, retrieves\nthe most similar chunks at query time, and finally produces\na citation-grounded answer via a locally hosted LLM. The\nhigh-level dataflow mirrors the draft schematic in Fig. 1.\nA. Data\nWe curated a dataset of anonymized patient catheterization\nreports (n = 1322) from Brigham and Women’s Hospital.\nThese reports were stored in a local SQLite database and used\nboth for query matching and label verification in the evaluation\nphase.\nUsers export reports (such as those from Epic) in PDF\nor raw text format. When reports are already in raw text\nformat, the content is sent directly to the LLM for analysis.\nHowever, when reports are in PDF format, the content must\nfirst be processed through an optical character recognition\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \n(OCR) module to convert it to raw text. Traditional OCR meth-\nods, such as Tesseract OCR [11], face significant challenges\nwhen processing images, handwritten documents, low-quality\ndocuments, and reports containing equations, tables, or graphs.\nThese conventional approaches lack the necessary accuracy\nfor detecting handwritten content, processing poor-quality\ndocuments, and handling complex layouts with tables or noisy\nbackgrounds. To address these limitations, we implemented\nolmocr vision-based language model [12] to read and convert\nreports to raw text content. Then, the Python Pydantic library\nis used to create a consistent structural output in JSON format.\nThe JSON format facilitates easier processing, organization,\nand integration with our Python codebase, enabling seamless\ntransmission via API to subsequent modules within the RAG-\nNOSIS package.\nFig. 1. RAGnosis pipeline.\nB. Objective\nThe clinical task was to identify and classify the PVL\nseverity in people who underwent Transcatheter Aortic Valve\nReplacement (TA VR) surgery, based on unstructured proce-\ndural reports as the sole information source. The goal was\nto evaluate whether LLMs, augmented with retrieval mech-\nanisms, could accurately extract PVL status and generate\nclinically meaningful explanations. Specifically, the task in-\nvolved: Identifying and classifying PVL severity from un-\nstructured procedural narratives Generating natural language\njustifications grounded in relevant clinical text Supporting\ncomparative evaluation of model performance across archi-\ntectures To perform this task, the LLMs were expected to\nreason over diverse linguistic markers such as direct PVL\nlabels (e.g., ”mild paravalvular leak”), descriptive phrases\n(“trace paravalvular regurgitation observed near the annulus”),\nand implicit negations (“no significant regurgitation”), often\ndispersed across multiple sections of the report. Successful\nclassification required sensitivity to both terminology and\ncontextual cues, including anatomical references, imaging\nfindings, and gradation language that may not explicitly name\nPVL but imply its presence or absence.\nC. Large Language Models\nThe four LLMs we used for RAGNosis were:\n• DeepSeek 70B (deepseek-r1/70b-32768)\n• Gemma 27B (gemma3/27b-32768)\n• Mistral 7B\n• LLaMA 3.2 (3B)\nDeepSeek 70B (deepseek-r1/70b-32768) is a transformer-\nbased, decoder-only language model comprising 70 billion\nparameters. Released in late 2023 by DeepSeek-VL, the model\nadopts a GPT-style architecture with rotary position embed-\ndings and multi-head self-attention, and supports a context\nlength of up to 32,768 tokens. This extended window enables\nthe model to process long-form documents without truncating\nrelevant content, a critical requirement for clinical narratives\nsuch as procedural reports. Its pre-training was conducted on a\nlarge-scale, multi-domain corpus, with subsequent instruction\ntuning to enhance performance on complex language tasks.\nIt was a strong candidate for this study due to its ability to\nretain and reason over extended input sequences and its native\nsupport for retrieval-augmented generation. Its architectural\ncapacity to integrate retrieved evidence, maintain coherence\nacross long inputs, and produce interpretable outputs aligned\nwell with the demands of clinical document understanding and\nevidence-linked inference.\nGemma 27B (gemma3/27b-32768) is a 27-billion parameter\ndecoder-only language model released by Google DeepMind\nin early 2024. Positioned as part of Google’s open-weight\nmodel family, Gemma was designed for high-throughput in-\nference with efficient scaling across hardware platforms. The\n27B variant supports a context window of 32,768 tokens\nand incorporates architectural features such as grouped-query\nattention and activation optimizations to balance performance\nand resource demands. Pretraining was conducted on a filtered,\nhigh-quality dataset with an emphasis on instruction-following\nand factual grounding. In the context of this study, the\nmodel exhibited a tendency toward more verbose responses,\noften elaborating beyond the immediate evidence. While this\noccasionally reduced precision in borderline cases, it also\ncontributed to higher scores for readability and linguistic\nfluency.\nMistral 7B is a lightweight, open-weight language model\nreleased in 2023 by Mistral AI. It comprises 7 billion pa-\nrameters and adopts a decoder-only transformer architecture\noptimized for efficiency and speed. Notably, it uses sliding\nwindow attention and grouped-query attention mechanisms,\nallowing it to scale effectively across long input sequences\nwithout a proportional increase in computational cost. Despite\nits relatively small size, the model has been shown to match or\noutperform larger models on several standard language under-\nstanding and reasoning benchmarks. In our study, the model\nproduced concise, often highly confident outputs, though its\nperformance was more polarized compared to larger models. It\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \nperformed well on cases with clear language cues but struggled\nin more ambiguous or implicit scenarios.\nLLaMA 3.2 is a variant within Meta AI’s third-generation\nLLaMA (Large Language Model Meta AI) family, released in\n2024. The model follows a decoder-only transformer architec-\nture and incorporates key improvements over LLaMA 2, in-\ncluding an updated tokenizer, enhanced instruction-following\nbehavior, and support for extended context lengths up to\n32,000 tokens. LLaMA 3 models were trained on significantly\nlarger and more diverse corpora, with alignment objectives\nintegrated more directly into the pretraining and fine-tuning\nprocess. These updates aimed to improve factual grounding,\nstability, and usability in open-weight settings. In this study,\nLLaMA 3.2 (3B parameters) exhibited a mix of high-quality\noutputs and variable responses, often producing fluent but\noccasionally less focused explanations. Compared to more\nheavily instruction-tuned models, LLaMA 3.2’s performance\nhighlighted the impact of prompt sensitivity and grounding\nstrategy when applying general-purpose models to clinical\nretrieval tasks.\nD. Optimizing Retrieval, Prompting, and Grounding for Clin-\nical NLP\n• Semantic Chunking and Clinical Embedding Optimiza-\ntion:To preserve clinical coherence in long procedural\nreports, we segment documents into overlapping chunks\nof 512 tokens with a stride of 128. This design bal-\nances retention of semantically relevant information,\nsuch as longitudinal mentions of PVL findings, against\nthe constraints of vector store memory and embedding\nthroughput. The overlap helps ensure that contextual cues\ndispersed across sections (e.g., “moderate leak was later\nobserved...”) are not lost at chunk boundaries, a common\nfailure point in naive token windowing. Each text chunk\nis transformed into a dense vector using a locally hosted\n384-dimensional embedding model optimized for seman-\ntic similarity in clinical language. Unlike general-purpose\nencoders, this model better captures the latent structure\nof procedural narratives, mapping related expressions\nsuch as “trace regurgitation” and “trivial PVL” closer\ntogether in embedding space. This improved alignment\nenhances the retriever’s ability to surface clinically rel-\nevant excerpts, especially when synonymous or indirect\nterminology is used across reports.\n• Prompt Engineering, Grounded Generation, and Ex-\nplanation Quality: Our system leverages a single-turn\nprompt template explicitly framed in a clinical role (“You\nare a cardiologist...”). We found that this instruction,\npaired with retrieved context, consistently outperformed\ngeneric summarization prompts in both explanation qual-\nity and faithfulness to evidence. Notably, models with-\nout explicit instruction tended to invent pathologies or\noverstate findings, a phenomenon reduced significantly\nwhen role-based grounding was included. Across mod-\nels, prompt design emerged as a stronger lever than\nmodel size alone for optimizing clinical relevance. The\ngeneration step is constrained by the top-5 semantically\nretrieved passages, and each answer is required to cite\nthese excerpts directly, enforcing a soft grounding con-\nstraint. We observed that longer explanations did not\nconsistently correlate with higher expert scores. Instead,\nfactual alignment with the retrieved context and accu-\nrate citation placement were the strongest indicators of\nquality. These findings underscore the value of retrieval-\naugmented generation over freeform outputs, particularly\nin clinical tasks where subtle variations in language can\nsignificantly impact interpretation.\n• Failure Modes and Model Comparison Infrastructure:\nQualitative review of model errors revealed that hallu-\ncinations clustered in two zones: (1) implied negations\n(e.g., “no leak was visualized”), and (2) severity gradation\nmismatches (“mild” vs. “moderate”). To reduce such\nfailures, we implemented fallback logic: if the model\nresponse lacks a PVL label or contradicts the retrieved ev-\nidence, the system either re-queries with a revised prompt\nor returns the raw excerpts alone. These mitigations led\nto a reduction in error rates. By decoupling the retriever\nand generator modules, RAGnosis supports plug-and-play\nevaluation across multiple open-weight LLMs. In this\nstudy, we benchmarked DeepSeek 70B, Gemma 27B,\nLLaMA 3.2B, and Mistral 7B using a fixed embedding\nspace and identical context templates. This separation\nallows us to isolate generative behavior from retrieval\nquality and supports ablation studies where one module is\nmodified independently, a key step toward standardizing\nRAG evaluation in clinical NLP.\n• System Modularity and Extensibility: The pipeline is\ndesigned with modular isolation across all components -\nOCR, embedding, retrieval, and generation, allowing each\nto be independently updated or replaced. This structure\nenables integration of new models or data formats without\nrequiring full-system retraining and supports deployment\nflexibility across varied institutional setups. Such extensi-\nbility ensures long-term adaptability to evolving clinical\nstandards, data types, and compliance requirements.\nV. E VALUATION AND RESULTS\nThe evaluation set comprised 100 explanation and reference\npairs generated by four foundation models (DeepSeek = 25,\nLLaMA = 25, Mistral = 25, Gemma = 25). Each set con-\ntained five cases from each PVL category: Not Mentioned,\nMentioned Negative, Mild/Trace, Moderate, and Severe. For\nevery case, the model generated a PVL classification, a free-\ntext justification explaining its reasoning, and the exact PDF\nexcerpts used to support its decision. All cases were then\nshuffled and anonymized before being sent to an MD clinician\nfor a blinded review. The clinician compared each model’s\noutput against the true clinical status and assigned scores from\na physician’s standpoint. If a model failed to process a case, it\nwas marked as E (error). If the model’s classification did not\nmatch the expert label, it received an F(incorrect). For correct\nclassifications, the reviewer assigned a favorability score from\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \n1 to 4, reflecting how well the model performed. A score of 1\nindicates minimal effort or weak justification, and a score of\n4 indicates strong, well-supported reasoning. Output from the\nmodels was assessed on three criteria:\n• Classification Accuracy: Agreement between model pre-\ndiction and ground truth PVL status\n• Explanation Quality: Whether the rationale matched the\nexpert summaries\n• Retrieval Relevance: Whether supporting evidence in-\ncluded meaningful clinical statements (qualitative)\nTable 1 summarizes the evaluation. LLaMA and Gemma\nhad the highest number of incorrect cases, while Mistral\nand DeepSeek produced mostly accurate classifications, with\nDeepSeek demonstrating the most consistent performance\nacross categories.\nTABLE I\nEVALUATION SCORES FOR THE LLM S\nScores E F 1 2 3 4\nDeepSeek 0 2 2 3 18 0\nGemma 1 7 1 3 10 3\nLLaMA 0 7 0 4 9 5\nMistral 2 3 4 0 13 3\nVI. A NALYSIS\nA. Reference Lengths, Explanation Behavior, and Class Bal-\nance\nThe median length of supporting reference excerpts was 267\ncharacters (IQR 198 to 388), while model-generated explana-\ntions were typically longer, with a median of 298 characters\n(IQR 228 to 359). Gemma and LLaMA tended to produce\nlonger explanations than Mistral and DeepSeek across most\nPVL categories, likely reflecting inherent verbosity differences\nin their generation styles. However, explanation length showed\nonly a weak to moderate correlation with the length of the\nsource reference (Pearson r: Mistral = 0.34, DeepSeek = 0.26,\nLLaMA = 0.27, Gemma = 0.10), suggesting that verbosity is\nnot tightly guided by input span. The class balance for model-\npredicted PVL labels was intentionally uniform; each model\nproduced 5 predictions per label, so performance metrics are\nnot confounded by label prevalence.\nB. Model Accuracy and Expert Favorability\nAs shown in Fig. 2, DeepSeek achieved the highest overall\naccuracy (92%), followed by Mistral (87%). LLaMA and\nGemma underperformed in comparison, both scoring below\n75%. Beyond classification accuracy, expert favorability rat-\nings revealed further distinctions in model behavior. DeepSeek\nreceived the most “Good” ratings and no “Poor” scores,\nindicating strong overall quality and consistency. LLaMA\nshowed the most polarized pattern, earning the highest number\nof “Excellent” scores but also a significant number of “Fair”\nratings. Mistral similarly displayed a broad spread across\nextremes, while Gemma was rated more narrowly, with most\noutputs falling into the “Fair” or “Good” categories.\nFig. 2. Accuracy achieved by LLMs (left) and Analysis of expert ratings for\neach LLM (right).\nC. Failure Modes and Error Distribution\nClassification errors were concentrated in the “Severe PVL”\n(40%) and “Not Mentioned” (32%) categories, where models\nfaced challenges with interpreting implicit language around\npresence or severity. As shown in Fig. 3, no failures occurred\nin the “Mentioned Negative” category, where denials were\nexplicit. Errors were relatively infrequent in “Mild/Trace” and\n“Moderate” cases, indicating that misclassifications were most\nlikely at the extremes of the PVL spectrum. Error patterns\nalso varied by model. LLaMA produced the highest number\nof failures in “Severe PVL” cases, while Gemma struggled\nmost with “Not Mentioned” and “Moderate PVL” examples.\nDeepSeek had the fewest failures overall, distributed across\nfewer categories, further emphasizing its robustness in clinical\ninterpretation tasks.\nFig. 3. Percentage of wrong predictions by PVL categories.\nD. Expert Ratings and Explanation Quality\nExpert ratings varied across models, with DeepSeek show-\ning the most consistent performance. Its scores were tightly\nclustered around 3 with minimal variance, suggesting steady\nquality across cases. LLaMA and Mistral, on the other hand,\nexhibited broader rating distributions and higher medians,\nindicating less reliability in output quality. Explanation length\ndid not consistently predict expert scores. While top-rated\nresponses had slightly longer median lengths, several short\nexplanations were still rated “Excellent,” and some longer ones\nreceived poor scores. This pattern suggests that clarity and\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \nclinical relevance, rather than verbosity, were the main drivers\nof favorable assessments.\nE. Explanation Length Across Models and PVL Categories\nExplanation lengths were broadly similar across PVL pre-\ndiction categories. Median lengths appeared slightly longer\nfor “Mild/Trace” and “Severe PVL” predictions, but with\nsubstantial overlap across groups. This suggests that verbosity\nwas not strongly influenced by the predicted class. At the\nmodel level, however, there were clear differences, as shown\nin Fig. 4. Gemma and LLaMA consistently produced longer\nexplanations compared to Mistral and DeepSeek. This trend\nholds across most PVL categories and likely reflects intrin-\nsic differences in default generation styles rather than task\nperformance. Mistral and DeepSeek, in contrast, generated\nshorter and more uniform explanations regardless of PVL\nseverity. These differences suggest that some models adjust\ntheir verbosity based on content, while others follow a more\nfixed output strategy.\nFig. 4. Analysis of explanation lengths of LLMs.\nF . Influence of Reference Length on Generated Justifications\nTo explore whether explanation verbosity was shaped by\nthe length of supporting evidence, we assessed the correlation\nbetween reference and explanation length. As shown in Fig. 5,\nall models exhibited weak to moderate positive correlations.\nPearson r values were 0.34 for Mistral, 0.26 for DeepSeek,\n0.27 for LLaMA, and 0.10 for Gemma. While Mistral showed\na modest linear trend, the overall findings indicate that ref-\nerence length was only loosely associated with how much\nreasoning the model provided.\nVII. F UTURE WORK AND LIMITATIONS\nWhile RAGnosis demonstrates promising performance in\nmedical outcome interpretation and PVL detection, it still\nhas several limitations as a fully local, offline package. First,\nthe training data comes exclusively from the Brigham and\nWomen’s Hospital (BWH) system, so performance on external\ndatasets remains unknown. Clinical documentation practices at\nBWH may follow institution-specific patterns, which may in-\nfluence model performance and limit generalizability. Second,\nFig. 5. Comparison of reference excerpts from PDF and explanation length.\nthe current implementation processes only text; future work\nwill add support for other modalities, such as echocardiog-\nraphy, fluoroscopy, and additional imaging inputs to enrich\nretrieval and reasoning. Third, real-time inference with large\nmodels like DeepSeek requires substantial hardware resources,\nespecially when RAGnosis serves multiple concurrent users.\nIn response, we will upgrade computational hardware and\noptimize the pipeline so that smaller, more efficient models\ncan deliver comparable performance.\nVIII. C ONCLUSION\nWe developed RAGnosis as a local, retrieval-augmented\nplatform for extracting and interpreting clinical information\nfrom unstructured procedural text. Applied to the task of PVL\nclassification, the system combines semantic retrieval with\ninstruction-tuned large language models to produce evidence-\ngrounded, interpretable outputs. Evaluation across four open-\nweight models revealed variation in generative behavior, with\nDeepSeek showing the most consistent performance and Mis-\ntral offering strong efficiency–accuracy tradeoffs. By oper-\nating entirely offline and supporting modular replacement\nof key components, the system addresses critical barriers\nto clinical AI deployment, including privacy, adaptability,\nand transparency. Although this work centers on PVL, the\narchitecture is readily extensible to other domains where\nscalable, document-level reasoning is needed. As AI becomes\nmore embedded in care delivery, tools like RAGnosis mark a\nshift from passive text summarization to active and auditable\nclinical reasoning, offering not just answers but accountability.\nREFERENCES\n[1] E. Hossain, R. Rana, N. Higgins, J. Soar, P. D. Barua, A. R. Pisani,\nand others, “Natural language processing in electronic health records in\nrelation to healthcare decision-making: A systematic review,” Comput.\nBiol. Med., vol. 155, p. 106649, Mar. 2023.\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint \n[2] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, and\nothers, “Large language models encode clinical knowledge,” Nature, vol.\n620, no. 7972, pp. 172–180, Aug. 2023.\n[3] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T.\nF. Tan, and D. S. W. Ting, “Large language models in medicine,” Nat.\nMed., vol. 29, no. 8, pp. 1930–1940, Aug. 2023.\n[4] N. H. Shah, D. Entwistle, and M. A. Pfeffer, “Creation and adoption of\nlarge language models in medicine,” JAMA, vol. 330, no. 9, pp. 866–869,\nSep. 2023.\n[5] K. Shah, A. Y . Xu, Y . Sharma, M. Daher, C. McDonald, B. G. Diebo, and\nothers, “Large language model prompting techniques for advancement\nin clinical medicine,” J. Clin. Med., vol. 13, no. 17, p. 5101, Aug. 2024.\n[6] R. Yang, Y . Ning, E. Keppo, M. Liu, C. Hong, D. S. Bitterman,\nand others, “Retrieval-augmented generation for generative artificial\nintelligence in health care,” NPJ Health Syst., vol. 2, no. 1, p. 2, Jan.\n2025.\n[7] W. Hiesinger, C. Zakka, A. Chaurasia, R. Shad, A. Dalal, J.\nKim, and others, “Almanac: Retrieval-augmented language mod-\nels for clinical medicine,” In Review, 2023. [Online]. Available:\nhttps://www.researchsquare.com/article/rs-2883198/v1 [Accessed: Jun.\n8, 2025].\n[8] I. Lopez, A. Swaminathan, K. Vedula, S. Narayanan, F. Nateghi\nHaredasht, S. P. Ma, and others, “Clinical entity augmented retrieval\nfor clinical information extraction,” NPJ Digit. Med., vol. 8, no. 1, p.\n45, Jan. 2025.\n[9] D. Van Veen, C. Van Uden, L. Blankemeier, J. B. Delbrouck, A. Aali, C.\nBluethgen, and others, “Adapted large language models can outperform\nmedical experts in clinical text summarization,” Nat. Med., vol. 30, no.\n4, pp. 1134–1142, Apr. 2024.\n[10] L. Perl, A. Cohen, A. Dadashev, Y . Shapira, H. Vaknin-Assa, V .\nYahalom, and R. Hirsch, “Long-term outcomes of catheter-based inter-\nvention for clinically significant paravalvular leak: Long-term outcomes\nof percutaneous PVL closure,” EuroIntervention, vol. 17, no. 9, pp. 736,\n2021.\n[11] A. Kay, “Tesseract: An open-source optical character recognition en-\ngine,” Linux J., vol. 2007, no. 159, p. 2, Jul. 2007.\n[12] J. Poznanski, A. Saxena, M. Ganeshkumar, S. Mehta, and J. Kalin,\n“olmOCR: Unlocking trillions of tokens in PDFs with vision language\nmodels,” arXiv preprint arXiv:2502.18443, Feb. 2025. [Online]. Avail-\nable: https://arxiv.org/abs/2502.18443\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted June 12, 2025. ; https://doi.org/10.1101/2025.06.11.25329438doi: medRxiv preprint ",
  "topic": "Medical decision making",
  "concepts": [
    {
      "name": "Medical decision making",
      "score": 0.7068730592727661
    },
    {
      "name": "Computer science",
      "score": 0.5683194398880005
    },
    {
      "name": "Clinical decision making",
      "score": 0.4768825173377991
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34532731771469116
    },
    {
      "name": "Medicine",
      "score": 0.17370012402534485
    },
    {
      "name": "Intensive care medicine",
      "score": 0.06467992067337036
    },
    {
      "name": "Family medicine",
      "score": 0.0
    }
  ],
  "institutions": []
}