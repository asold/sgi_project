{
  "title": "DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion",
  "url": "https://openalex.org/W3217195287",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4224974666",
      "name": "Douillard, Arthur",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ram\\'e, Alexandre",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202059690",
      "name": "Couairon Guillaume",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3153568054",
      "name": "Cord, Matthieu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2767290858",
    "https://openalex.org/W3129626847",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3216708862",
    "https://openalex.org/W2947461406",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2922466325",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2925387673",
    "https://openalex.org/W3123155632",
    "https://openalex.org/W2126204609",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W3034856281",
    "https://openalex.org/W2804746922",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2737492962",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2981864462",
    "https://openalex.org/W3180392831",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2583761661",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2963813679",
    "https://openalex.org/W3037967334",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3107810305",
    "https://openalex.org/W2963540014",
    "https://openalex.org/W2786446225",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2116522068",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2771964490",
    "https://openalex.org/W3171888599",
    "https://openalex.org/W3083962988",
    "https://openalex.org/W2765407302"
  ],
  "abstract": "Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning. However, existing approaches often require a task identifier at test-time, need complex tuning to balance the growing number of parameters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy doesn't need any hyperparameter tuning to control the network's expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having less parameters than concurrent dynamic frameworks.",
  "full_text": "DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion\nArthur Douillard1,2, Alexandre Ram´e1, Guillaume Couairon1,3, Matthieu Cord1,4\n1Sorbonne Universit´e, 2Heuritech, 3Meta AI, 4valeo.ai\narthur.douillard@heuritech.com, {alexandre.rame, matthieu.cord}@sorbonne-universite.fr,\ngcouairon@fb.com\nAbstract\nDeep network architectures struggle to continually learn\nnew tasks without forgetting the previous tasks. A recent\ntrend indicates that dynamic architectures based on an ex-\npansion of the parameters can reduce catastrophic forget-\nting efﬁciently in continual learning. However, existing ap-\nproaches often require a task identiﬁer at test-time, need\ncomplex tuning to balance the growing number of param-\neters, and barely share any information across tasks. As a\nresult, they struggle to scale to a large number of tasks with-\nout signiﬁcant overhead.\nIn this paper, we propose a transformer architecture based\non a dedicated encoder/decoder framework. Critically, the\nencoder and decoder are shared among all tasks. Through\na dynamic expansion of special tokens, we specialize each\nforward of our decoder network on a task distribution. Our\nstrategy scales to a large number of tasks while having neg-\nligible memory and time overheads due to strict control of\nthe expansion of the parameters. Moreover, this efﬁcient\nstrategy doesn’t need any hyperparameter tuning to control\nthe network’s expansion. Our model reaches excellent re-\nsults on CIFAR100 and state-of-the-art performances on the\nlarge-scale ImageNet100 and ImageNet1000 while having\nfewer parameters than concurrent dynamic frameworks.1\n1. Introduction\nMost of the deep learning literature focuses on learning\na model on a ﬁxed dataset. However, real-world data con-\nstantly evolve through time, leading to ever-changing dis-\ntributions: i.e., new classes or domains appeared. When\na model loses access to previous classes data ( e.g., for pri-\nvacy reasons) and is ﬁne-tuned on new classes data, itcatas-\ntrophically forgets the old distribution. Continual learning\nmodels aim at balancing a rigidity/plasticity trade-off where\nold data are not forgotten (rigidity to changes) while learn-\ning new incoming data (plasticity to adapt). Despite recent\n1https://github.com/arthurdouillard/dytox.\nFigure 1: DyTox’s continual learning performance on\nImageNet1000: for each task, 100 new classes are learned\nwhile previously learned classes are not fully accessible but\nshouldn’t be forgotten. Our strategy DyTox (inred) is state-\nof-the-art by a large margin. Note that at the initial step be-\nfore the continual process begins (denoted by a dashed rect-\nangle\n ), our model has performance comparable to other\nbaselines: the performance gain is achieved by reducing\ncatastrophic forgetting. Moreover, we have systematically\nfewer parameters than previous approaches.\nadvances, it is still an open challenge.\nA growing amount of efforts have emerged to tackle\ncatastrophic forgetting [59, 39, 75, 32, 19, 76]. Recent\nworks [77, 46, 33, 23, 27, 64] dynamically expand the net-\nwork architectures [77, 46] or re-arrange their internal struc-\ntures [23, 64, 33, 27]. Unfortunately at test-time, they re-\nquire to know the task to which the test sample belongs —\nin order to know which parameters should be used. More re-\ncently, DER [76] and Simple-DER [48] discarded the need\nfor this task identiﬁer by learning a single classiﬁer on the\nconcatenation of all produced embeddings by different sub-\n1\narXiv:2111.11326v3  [cs.CV]  7 Aug 2022\nsets of parameters. Yet, these strategies induce dramatic\nmemory overhead when tackling a large number of tasks,\nand thus need complex pruning as post-processing.\nTo improve the ease of use of continual learning frame-\nworks for real-world applications, we aim to design a dy-\nnamically expandable representation (almost) ‘for free’ by\nhaving the three following properties: #1 limited memory\noverhead as the number of tasks grows, #2 limited time\noverhead at test time and #3 no setting-speciﬁc hyper-\nparameters for improved robustness when faced to an un-\nknown (potentially large) number of tasks.\nTo this end, we leverage the computer vision trans-\nformer ViT [16]. Transformers [71] offer a very interest-\ning framework to satisfy the previously mentioned con-\nstraints. Indeed, we build upon this architecture to design\na encoder/decoder strategy: the encoder layers are shared\namong all members of our dynamic network; the unique de-\ncoder layer is also shared but its forward pass is specialized\nby a task-speciﬁc learned token to produce task-speciﬁc\nembeddings. Thus, the memory growth of the dynamic net-\nwork is extremely limited: only a 384d vector per task, val-\nidating property #1. Moreover, this requires no hyperpa-\nrameter tuning (property #3). Finally, the decoder is explic-\nitly designed to be computationally lightweight (satisfying\nproperty #2). We nicknamed our framework, DyTox, for\nDYnamic TOken eXpansion . To the best of our knowl-\nedge, we are the ﬁrst to apply the transformer architecture\nto continual computer vision.\nOur strategy is robust to different settings, and can easily\nscale to a large number of tasks. In particular, we validate\nthe efﬁciency of our approach on CIFAR100, ImageNet100,\nand ImageNet1000 (displayed on Fig. 1) for multiple set-\ntings. We reach state-of-the-art results, with only a small\noverhead thanks to our efﬁcient dynamic strategy.\n2. Related work\nContinual learning models tackle the catastrophic for-\ngetting of the old classes [67, 25]. In computer vision,\nmost of continual learning strategies applied on large-scale\ndatasets use rehearsal learning: a limited amount of the\ntraining data of old classes is kept during training [60]. This\ndata is usually kept in raw form (e.g., pixels) [59, 5, 10] but\ncan also be compressed [29, 35], or trimmed [18] to reduce\nmemory overhead; others store only a model to generate\nnew samples of past classes [37, 66, 45]. In addition, most\napproaches aim at limiting the changes in the model when\nnew classes are learned. These constraints can be directly\napplied on the weights [39, 79, 1, 8], intermediary features\n[32, 15, 83, 19, 17], prediction probabilities [47, 59, 5, 6], or\non the gradients [51, 9, 21, 62]. All these constraint-based\nmethods use the same static network architectures which\ndoesn’t evolve through time, usually a ResNet [30], a LeNet\n[42], or a small MLP.\nContinual dynamic networks In contrast, our paper and\nothers focus on designing dynamic architectures that best\nhandle a growing training distribution [77, 46], in partic-\nular by dynamically creating (sub-)members each special-\nized in one speciﬁc task [23, 27, 33, 61, 11, 73]. Unfor-\ntunately, previous approaches often require the sample’s\ntask identiﬁer at test-time to select the right subset of pa-\nrameters. We argue this is an unrealistic assumption in a\nreal-life situation where new samples could come from any\ntask. Recently, DER [76] proposed a dynamic expansion\nof the representation by adding a new feature extractor per\ntask. All extractors’ embeddings would then be concate-\nnated and fed to a uniﬁed classiﬁer, discarding the need for\na task identiﬁer at test-time. To limit an explosion in the\nnumber of parameters, they aggressively prune each model\nafter each task using the HAT [64] procedure. Unfortu-\nnately, the pruning is hyperparameter sensitive. Therefore,\nhyperparameters are tuned differently on each experiment:\nfor example, learning a dataset in 10 steps or in 50 steps\nuse different hyperparameters. While being impracticable,\nit is also unrealistic because the number of classes is not\nknown in advance in a true continual situation. Simple-DER\n[48] also uses multiple extractors, but its pruning method\ndoesn’t need any hyperparameters; the negative counter-\npart is that Simple-DER controls less the parameter growth\n(2.5x higher than a base model). In contrast, we propose a\nframework dedicated to continual learning that seamlessly\nenables a task-dynamic strategy, efﬁcient on all settings,\nwithout any setting-dependant modiﬁcation and at almost\nno memory overhead. We share early class-agnostic [53]\nlayers similarly to TreeNets [44] and base our strategy on\nthe Transformer architecture.\nTransformers were ﬁrst introduced for machine transla-\ntion [71], with the now famous self-attention. While the\noriginal transformer was made of encoder and decoder lay-\ners, later transformers starting from BERT [14] used a suc-\ncession of identical encoder blocks. Then, ViT [16] pro-\nposed to apply transformers to computer vision by using\npatches of pixels as tokens. Multiple recent works, includ-\ning DeiT [69], CaiT [70], ConVit [12], and Swin [50], im-\nproved ViT with architecture and training procedures mod-\niﬁcations. PerceiverIO [36] proposed a general architecture\nwhose output is adapted to different modalities using spe-\nciﬁc learned tokens, and whose computation is reduced us-\ning a small number of latent tokens. Despite being success-\nful across various benchmarks, transformers have not yet\nbeen considered for continual computer vision to the best\nof our knowledge. Yet, we don’t use the transformer ar-\nchitecture for its own sake, but rather because of the intrin-\nsic properties of transformers; in particular, the seminal en-\ncoder/decoder framework allows us to build an efﬁcient ar-\nchitecture with strong capabilities against catastrophic for-\ngetting.\n2\n!\" !# TAB\nTAB\nTAB\nTask Tokens\nShared   parameters\nShared   parametersSAB#1SAB#5\n$%Patch Tokens\n&%\n&'\n&(\n$'\n$(\nClf%\nClf'\nClf(\n,-%\nEncoderDynamic Task Decoder\n!#\n!#\n!#\n,-'\n,-(\n./.0.1\n.2….4\n,-%:(\nFigure 2: DyTox transformer model. An image is ﬁrst split into multiple patches, embedded with a linear projection. The\nresulting patch tokens are processed by 5 successive Self-Attention Blocks (SAB) (Sec. 3.1). For each task ( t = 1...T ),\nthe processed patch tokens are then given to the Task-Attention Block (TAB) (Sec. 3.2): each forward through the TAB\nis modiﬁed by a different task-specialized token θt for t ∈{1 ...T }(Sec. 3.3). The T ﬁnal embeddings are ﬁnally given\nseparately to independent classiﬁers Clft each predicting their task’s classesCt. All |C1:T |logits are activated with a sigmoid.\nFor example, at task t= 3, one forward is done through the SABs and three task-speciﬁc forwards through the unique TAB.\n3. DyTox transformer model\nOur goal is to learn a uniﬁed model that will classify\nan increasingly growing number of classes, introduced in\na ﬁxed amount of steps T. At a given step t ∈{1 ...T },\nthe model is exposed to new data belonging to new classes.\nSpeciﬁcally, it learns from samples {(xt\ni,yt\ni)}i, where xt\ni is\nthe i-th image of this task t and yt\ni is the associated label\nwithin the label set Ct. All task label sets are exclusive:\nC0 ∩C1 ... CT = ∅. The main challenge is that the data are\nfully available only temporarily: following most previous\nworks, only a few samples from previous tasks{1 ...t −1}\nare available for training at step tas rehearsing data. Yet,\nthe model should remain able to classify test data coming\nfrom all seen classes C1:t. A table of notations is provided\nin the supplementary materials.\nThe Fig. 2 displays our DyTox framework, which is\nmade of several components (SAB, TAB, and Task Tokens)\nthat we describe in the following sections.\n3.1. Background\nThe vision transformer [16] has three main components:\nthe patch tokenizer, the encoder made of Self-Attention\nBlocks, and the classiﬁer.\nPatch tokenizer The ﬁxed-size input RGB image is\ncropped into N patches of equal dimensions and then pro-\njected with a linear layer to a dimension D. Both oper-\nSelfAttentionLayer Norm MLP!5 !56%Patch TokensLayer Norm\nFigure 3: The Self-Attention Block (SAB) combines a\nSelf-Attention (SA), two Layer Norms, and one MLP with a\nsingle hidden layer. As in a ResNet, two shortcuts are used\nwith element-wise addition.\nations, the cropping and projection, are done with a sin-\ngle 2D convolution whose kernel size is equal to its stride\nsize. The resulting tensor x0 ∈RN×D is extended with a\nlearned class token xcls ∈RD resulting in a tensor of shape\nR(N+1)×D. Following [26], a learned positional embedding\np∈R(N+1)×D is added (element-wise).\nSelf-Attention (SA) based encoder The tokens are fed\nto a stack of transformer blocks that we denote here as Self-\nAttention Blocks (SABs):\nx′\nl = xl + SAl (Norml,1 (xl)) ,\nxl+1 = x′\nl + MLPl (Norml,2 (x′\nl)) , (1)\nwith SA a Self-Attention layer [71], Norm a layer normal-\nization [3], and MLP a Multi-Layer Perceptron with a sin-\ngle hidden layer. We repeat these operations for each SAB,\nfrom l= 1to l= L. The resulting tensor (which keeps the\nsame dimension after every block) is xL ∈R(N+1)×D. We\ndisplay a visual illustration of a SA Block in Fig. 3.\n3\nClassiﬁer In the original vision transformer (ViT [16]), a\nlearned vector called the “ class token” is appended to the\npatch tokens after the tokenizer. This special class token,\nwhen processed after all the SABs, is given to a linear clas-\nsiﬁer with a softmax activation to predict the ﬁnal probabil-\nities. However, more recent works, as CaiT [70], propose\ninstead to introduce the class token only at the ultimate or\npenultimate SAB to improve classiﬁcation performance.\n3.2. Task-Attention Block (TAB)\nContrary to previous transformer architectures, we don’t\nhave a class token, but rather what we nicknamed “task to-\nkens”; the learned token of the ith task is denoted θi. This\nspecial token will only be added at the last block. To exploit\nthis task token, we deﬁne a new attention layer, that we call\nthe Task-Attention. It ﬁrst concatenates the patch tokensxL\nproduced by the ultimate SAB with a task token θi:\nzi = [θi,xL] ∈R(N+1)×D . (2)\nThis is then given to the Task-Attention (TA), inspired by\nthe Class-Attention of Touvron et al. [70]:\nQi = Wqθi ,\nKi = Wkzi ,\nVi = Wvzi ,\nAi = Softmax\n(\nQi ·KT\ni /\n√\nd/h\n)\n,\nOi = WoAiVi + bo ∈R1×D ,\n(3)\nwith d being the embedding dimension, and h the num-\nber of attention heads [71]. Contrary to the classical Self-\nAttention, the Task-Attention deﬁnes its query ( Qi) only\nfrom the task-token θi without using the patch tokens xL.\nThe Task-Attention Block (TAB) is then a variation of the\nSAB where the attention is a Task-Attention (TA):\nc′= c+ TA (Norm1 (z)) ,\nc′′= c′+ MLP (Norm2 (c′)) . (4)\nOverall, our new architecture can be summarized by the rep-\netition of SA Blocks{SABl}L\nl=1 (deﬁned in Eq. 1) ended by\na single TA Block TAB (deﬁned in Eq. 4):\nei = TAB◦([θi, SABl=L ◦...SABl=1(x0)]) ∈RD . (5)\nThe ﬁnal embedding ei is fed to a classiﬁer clf made of a\nNormc and a linear projection parametrized by {Wc,bc}:\n˜yi = Clf(ei) =Wc Normc(ei) +bc . (6)\n3.3. Dynamic task token expansion\nWe deﬁned in the previous section our base network,\nmade of a succession of SABs and ended by a single TAB.\nAs detailed, the TAB has two inputs: the patch tokens xL\nextracted from the image and a learned task-tokenθi. We’ll\nnow detail how our framework evolves in a continual situa-\ntion at each new step.\nDuring the ﬁrst step, there is only one task token θ1.\nAt each new step, we propose to expand our parameter\nspace by creating a new task token while keeping the pre-\nvious ones. Thus, after t steps, we have t task tokens\n(θi for i∈{1 ...t }). Given an image x— belonging to any\nof the seen tasks{1 ... t}— our model tokenizes it into x0,\nand processes it through the multiple SABs: this outputs the\npatch tokens xL. Finally, our framework does as many for-\nward passes through the TAB as there are tasks: critically,\neach TAB forward passes is executed with a different task\ntoken θi, resulting in different task-speciﬁc forwards, each\nproducing the task-speciﬁc embeddings ei (see Fig. 2):\ne1 = TAB([θ1,xL]) ,\ne2 = TAB([θ2,xL]) ,\n...\net = TAB([θt,xL]) .\n(7)\nRather than concatenating all embeddings {e1,e2,...,e t}\ntogether and feeding them to one classiﬁer, we leverage\ntask-speciﬁc classiﬁers . Each classiﬁer clfi is made of a\nNormi and a linear projection parametrized by {Wi,bi},\nwith Wi ∈RCi×D and b ∈RCi\n. It takes as input its task-\nspeciﬁc embedding ei and returns:\nˆyi = Clfi(ei) =σ(Wi Normi ei + bi) , (8)\nthe predictions for the classes yi ∈ Ci, where σ(x) =\n1/(1+e−x) is the sigmoid activation. In comparison with\nthe softmax activation, the element-wise sigmoid activa-\ntion reduces the overconﬁdence in recent classes. Conse-\nquently, the model is better calibrated, which is an impor-\ntant attribute of continual model [4, 75, 81]. The loss is the\nbinary-cross entropy. The independent classiﬁers paradigm\n[65] coupled with the sigmoid activation and binary cross-\nentropy loss exclude explicitly a late fusion [57] of the task\nembeddings resulting in more specialized classiﬁers.\nThe overall structure of the DyTox strategy is illus-\ntrated in Fig. 2. We also show in Algo. 1 the pseudo-code\nof a forward pass at test-time after having learned the taskt.\nCritically, the test image can belong to any of the previously\nseen tasks {1 ... t}. Our dynamic task token expansion is\nmore efﬁcient than a naive parameter expansion that would\ncreate a new copy of the whole network for each new task.\n(1) Our expansion is limited to a new task token per new\ntask, which is only d = 384new parameters. This is small\ncompared to the total model size (≈11 million parameters).\nThe memory overhead is thus almost null . (2) The com-\nputationally intensive blocks ( i.e., the SABs) are executed\n4\nAlgorithm 1 DyTox’s forward pass at stept\nInput: x0 (initial patch tokens), y( ground-truth labels)\nOutput: ˆy1:t (predictions for all classes of C1:t)\n1: xL ←SABl=L ◦...SABl=1(x0) ⊿Sec. 3.1\n2: for i←1; i≤t; i++ do\n3: ei ←TAB([θi,xL]) ⊿Sec. 3.2\n4: ˆyi ←Clfi(ei) ⊿Sec. 3.3\n5: end for\n6: ˆy1:t ←[ˆy1, ..., ˆyt]\nonly once despite learning multiple tasks. In contrast, the\nTAB has as many forwards as there are tasks. Though, this\ninduces minimal overhead because the Task-Attention has\na linear complexity w.r.t the number of patcheswhile the\nSelf-Attention is quadratic. Therefore, the time overhead is\nsub-linear. We quantitatively show this in Sec. 4.\nContext The current transformer paradigm starting from\nBERT [14] and continuing with ViT [16] is based on a en-\ncoder+classiﬁer structure. Differently, our dynamic frame-\nwork strays is a resurgence of the encoder/decoder struc-\nture of the original transformer [71]: the encoder is shared\n(both in memory and execution) for all outputs. The de-\ncoder parameters are also shared, but its execution is task-\nspeciﬁc with each task token, with each forward akin to a\ntask-speciﬁc expert chosen from a mixture of experts [52].\nMoreover, multi-tasks text-based transformers have natural\nlanguage tokens as an indicator of a task [55] ( e.g. ”sum-\nmarize the following text”), in our context of vision we used\nour deﬁned task tokens as indicators.\nLosses Our model is trained with three losses: (1) the\nclassiﬁcation loss Lclf, a binary-cross entropy, (2) a knowl-\nedge distillation [31] Lkd applied on the probabilities, and\n(3) the divergence loss Ldiv. The distillation loss helps to\nreduce forgetting. It is arguably quite naive, and more com-\nplex distillation losses [63, 32, 19] could further improve\nresults. The divergence loss, inspired from the “auxiliary\nclassiﬁer” of DER [76], uses the current last task’s embed-\nding et to predict ( |Ct|+ 1) probabilities: the current last\ntask’s classesCt and an extra class representing all previous\nclasses that can be encountered via rehearsal. This classiﬁer\nis discarded at test-time and encourages a better diversity\namong task tokens. The total loss is:\nL= (1−α)Lclf + αLkd + λLdiv , (9)\nwith λ a hyperparameter set to 0.1 for all experiments. α\ncorrespond to the fraction of the number of old classes over\nthe number of new classes |C1:t−1|\n|C1:t| as done by [81]. There-\nfore, αis automatically set; this removes the need to ﬁnely\ntune this hyperparameter.\nHyperparameter CIFAR ImageNet\n# SAB 5\n# CAB 1\n# Attentions Heads 12\nEmbed Dim 384\nInput Size 32 224\nPatch Size 4 16\nTable 1: DyTox’s architecturesfor CIFAR and ImageNet.\nThe only difference between the two architectures is the\npatch size, as the image sizes vary between datasets.\n4. Experiments\n4.1. Benchmarks & implementation\nBenchmarks & Metrics We evaluate our model on CI-\nFAR100 [40], ImageNet100 and ImageNet1000 [13] (de-\nscriptions in the supplementary materials) under different\nsettings. The standard continual scenario in ImageNet has\n10 steps: thus we add 10 new classes per step in Ima-\ngeNet100, and 100 new classes per step in ImageNet1000.\nIn CIFAR100, we compare performances on 10 steps (10\nnew classes per step), 20 steps (5 new classes per step), and\n50 steps (2 new classes per step). In addition to the top-1 ac-\ncuracy, we also compare the top-5 accuracy on ImageNet.\nWe report the “ Avg” accuracy which is the average of the\naccuracies after each step as deﬁned by [59]. We also re-\nport the ﬁnal accuracy after the last step (“ Last”). Finally,\nin our tables, “#P” denotes the parameters count in million\nafter the ﬁnal step.\nImplementation details As highlighted in Table 1, our\nnetwork has the same structure across all tasks. Specif-\nically, we use 5 Self-Attention Blocks (SABs), 1 Task-\nAttention Block (TAB). All 6 have an embedding dimen-\nsion of 384 and 12 attention heads. We designed this shal-\nlow transformer to have a comparable parameters count to\nother baselines, but also made it wider than usual ”tiny”\nmodels [16, 69, 70]. We tuned all hyperparameters for CI-\nFAR100 with 10 steps on a validation set made of 10%\nof the training set, and then kept them ﬁxed for all other\nsettings, ImageNet included. The only difference between\nthe two datasets is that ImageNet images are larger; thus\nthe patch size is larger, and overall the base transformer\nhas slightly more parameters on ImageNet than on CIFAR\n(11.00M vs 10.72M) because of a bigger positional embed-\nding. We use the attention with spatial prior (introduced\nby ConViT [12]) for all SABs which allows training trans-\nformers on a small dataset (like CIFAR) without pretrain-\ning on large datasets or complex regularizations. Following\nprevious works [59, 76], we use for all models (baselines\nincluded) 2,000 images of rehearsal memory for CIFAR100\n5\nImageNet100 10 steps ImageNet1000 10 steps\n#P top-1 top-5 #P top-1 top-5\nMethods Avg Last Avg Last Avg Last Avg Last\nResNet18 joint 11.22 - - - 95.10 11 .68 - - - 89.27\nTransf. joint 11.00 - 79.12 - 93.48 11.35 - 73.58 - 90.60\nE2E [5] 11.22 - - 89.92 80.29 11.68 - - 72.09 52.29\nSimple-DER [48] - - - - - 28.00 66.63 59.24 85.62 80.76\niCaRL [59] 11.22 - - 83.60 63 .80 11 .68 38 .40 22 .70 63 .70 44 .00\nBiC [32] 11.22 - - 90.60 84 .40 11 .68 - - 84.00 73 .20\nW A [81] 11.22 - - 91.00 84 .10 11 .68 65 .67 55 .60 86 .60 81 .10\nRPSNet [56] - - 87.90 74 .00 - - - - -\nDER w/o P [76] 112.27 77.18 66.70 93.23 87.52 116.89 68.84 60.16 88.17 82.86\nDER†[76] - 76.12 66.06 92.79 88.38 - 66.73 58.62 87.08 81.89\nDyTox 11.01 77.15 69.10 92.04 87.98 11.36 71.29 63.34 88.59 84.49\nTable 2: Results on ImageNet-100 and ImageNet-1000 datasets , learned with 10 steps of respectively 10 and 100 new\nclasses. E2E [5] and Simple-DER [48] results come from their respective papers, and used a different class ordering. Other\nresults come from [76]. The †symbol means that [76] needed setting-sensitive hyperparameters. Moreover, its reported\nparameters count was an average over all steps ([76] reported 14.52M on ImageNet1000): the ﬁnal parameters count (neces-\nsarily higher) was not available.\nFigure 4: Performance evolution on ImageNet100 . The\ntop-5 accuracy (%) is reported after learning each task. Our\nmodel DyTox (in red) surpasses signiﬁcantly most base-\nlines, and is of equal performance as the complex DER that\nuses pruning with setting-speciﬁc hyperparameters.\nand ImageNet100, and 20,000 images for ImageNet1000.\nThe implementations of the continual scenarios are pro-\nvided by Continuum [20]. Our network implementation is\nbased on the DeiT [69] code base which itself uses exten-\nsively the timm library [74]. The code is released publicly2.\nThe full implementation details are in the appendix.\n2https://github.com/arthurdouillard/dytox.\n4.2. Quantitative results\nImageNet We report performances in Table 2 on the com-\nplex ImageNet dataset. The †marks the DER with setting-\nspeciﬁc pruning, and DER w/o P is for the DER without\npruning. In ImageNet100, DyTox reaches 69.10% and out-\nperforms DER†by +3.04 percentage points (p.p ) in “Last”\ntop-1 accuracy. Though, DyTox and DER w/o P somehow\nperform similarly in “Avg” accuracy on this setup, as high-\nlighted in the performance evolution displayed in Fig. 4.\nMost importantly, on the larger-scale ImageNet1000, Dy-\nTox systematically performs best on all metrics despite hav-\ning lower parameters count. Speciﬁcally, DyTox reaches\n71.29% in “Avg” top-1 accuracy, and 63.34% in “Last” top-\n1 accuracy. This outperforms the previous state-of-the-art\nDER w/o P (68.84% in “Avg”, 60.16% in “Last”) which\nhas 10 ResNet18 in parallel and 116.89M parameters. Com-\npared to the pruned DER †, DyTox has a +4.56 p.p in top-1\nand a +1.51 p.p in top-5 for the “Avg” accuracy. All mod-\nels evolutions on ImageNet1000 are illustrated in Fig. 1:\nDyTox constantly surpasses previous state-of-the-art mod-\nels — despite having a comparable performance at the ﬁrst\nstep and fewer parameters.\nDyTox is able to scale correctly while handling seam-\nlessly the parameter growth by sharing most of the weights\nacross tasks. In contrast, DER had to propose a complex\npruning method; unfortunately, this pruning required dif-\nferent hyperparameter values for different settings. De-\nspite this, the pruning in DER † is less efﬁcient when\nclasses diversity increase: DER † doubles in size between\n6\n10 steps 20 steps 50 steps\nMethods #P Avg Last #P Avg Last #P Avg Last\nResNet18 Joint 11.22 - 80.41 11.22 - 81.49 11.22 - 81.74\nTransf. Joint 10.72 - 76.12 10.72 - 76.12 10.72 - 76.12\niCaRL [59] 11.22 65.27 ±1.02 50.74 11.22 61.20 ±0.83 43.75 11.22 56.08 ±0.83 36.62\nUCIR [32] 11.22 58.66 ±0.71 43.39 11.22 58.17 ±0.30 40.63 11.22 56.86 ±0.83 37.09\nBiC [75] 11.22 68.80 ±1.20 53.54 11.22 66.48 ±0.32 47.02 11.22 62.09 ±0.85 41.04\nW A [81] 11.22 69.46 ±0.29 53.78 11.22 67.33 ±0.15 47.31 11.22 64.32 ±0.28 42.14\nPODNet [19] 11.22 58.03 ±1.27 41.05 11.22 53.97 ±0.85 35.02 11.22 51.19 ±1.02 32.99\nRPSNet [56] 56.5 68.60 57.05 - - - - - -\nDER w/o P [76] 112.27 75.36 ±0.36 65.22 224.55 74.09 ±0.33 62.48 561.39 72.41 ±0.36 59.08\nDER†[76] - 74.64 ±0.28 64.35 - 73.98 ±0.36 62.55 - 72.05 ±0.55 59.76\nDyTox 10.73 73.66 ±0.02 60.67±0.34 10.74 72.27 ±0.18 56.32±0.61 10.77 70.20 ±0.16 52.34±0.26\nDyTox+ 10.73 75.54 ±0.10 62.06±0.25 10.74 75.04 ±0.11 60.03±0.45 10.77 74.35 ±0.05 57.09±0.13\nTable 3: Results on CIFAR100 averaged over three different class orders. Baselines results are come from [76]. The †\nsymbol means that [76] needed setting-sensitive hyperparameters. Moreover, its reported parameters count was an average\nover all steps: the ﬁnal parameters count (necessarily higher) was not available.\nFigure 5: Performance evolution on CIFAR100 . The top-1 accuracy (%) is reported after learning each task. Left is\nevaluated with 10 steps, middle with 20 steps, and right with 50 steps.\nImageNet100 and ImageNet1000 ([76] reports 7.67M vs.\n14.52M) while handling the same amount of tasks (10).\nNote that these parameter counts reported for DER†in [76]\nare in fact averages over all steps: the ﬁnal parameters count\n(necessarily higher) was not available and thus is not re-\nported in our tables. Simple-DER also applies pruning but\nwithout hyperparameter tuning; while simpler, the pruning\nis also less efﬁcient and induces larger model (28.00M pa-\nrameters).\nCIFAR100 Table 3 shows results for all approaches on\nCIFAR100. The more steps there are, the larger the for-\ngetting is and thus the lower the performances are. Those\nsettings are also displayed in Fig. 5 after each task. In every\nsetting, DyTox is close to DER w/o P for much fewer pa-\nrameters (up to 52x less). Critically, DyTox is signiﬁcantly\nabove other baselines: e.g. DyTox is up to +25% in “Last”\naccuracy in the 50 steps setup.\nImproved training procedure To bridge the gap between\nDyTox and DER w/o P on CIFAR100, we introduce a new\nefﬁcient training procedure for continual learning. Using\nMixUp [80], we linear interpolate new samples with exist-\ning samples. The interpolation factor λ ∼Beta(α,α) is\nsampled with α= 0.8: the pixels of two images are mixed\n(x= λx1 +(1 −λ)x2) as their labels (y= λy1 +(1 −λ)y2).\nMixUp was shown to have two main effects: (1) it di-\nversiﬁes the training images and thus enlarges the train-\ning distribution on the vicinity of each training sample [7]\nand (2) it improves the network calibration [28, 68], reduc-\ning the overconﬁdence in recent classes. Thus MixUp has\nshared motivation with the sigmoid activation. When Dy-\n7\n1 step 50 steps\nTraining Last (↑) Last (↑) Forgetting (↓)\nDyTox 76.12 52.34 33.15\nDyTox+ 77.51 +1.39 57.09+4.75 31.50-1.65\nTable 4: “Last” accuracy and forgetting[8] on CIFAR100\nfor the joint (1 step, no continual) and 50 steps settings.\nTox is combined with this MixUp procedure, nicknamed as\nDyTox+, this signiﬁcantly improves the state-of-the-art in\n“Avg” accuracy in all three settings of Table 3. We also\nprovide in the appendix further improvement for this new\ncontinual training procedure providing even larger gain on\nboth CIFAR100 and ImageNet100.\n4.3. Model introspection on CIFAR100\nMemory overhead We only add a vector of sized= 384\nper task; thus, the overhead in memory (not considering the\ngrowing classiﬁer which is common for all continual mod-\nels) is only of +0.004% per step. Even in the challenging\nsetting of CIFAR100 with 50 tasks, our memory overhead\nis almost null (+0.2%).\nComputational overhead The vast majority of the com-\nputation is done in the SABs, thus shared among all tasks.\nThe dynamical component of our model is located at the ul-\ntimate TAB. Moreover, the Task-Attention, contrary to the\nSelf-Attention, has a time complexity linear in terms of to-\nkens and not quadratic reducing the time overhead to an\nacceptable sub-linear amount. Overall, for each new task,\none forward pass takes 2.24% more time than for the base\ntransformer.\nTraining procedure introspection Our DyTox+ strategy\nwith MixUp really reduces catastrophic forgetting and does\nnot just improve raw performances. This is shown in Ta-\nble 11, where we compare DyTox vs. DyTox+ strategies\non CIFAR100. While MixUp only slightly improves by\n1.39 p.p the accuracy in joint learning (no continual, 1 step),\nMixUp greatly improves the performance by 4.75 p.p in\nthe 50 steps continual scenario. To further illustrate this,\nwe also report the Chaudhry et al.’s forgetting [8] measure\nwhich compares how performances dropped compared to\nprevious steps. MixUp reduces this forgetting by 1.65 p.p .\nModel ablations We ablate the importance of the differ-\nent components of DyTox in Table 5. We add on the base\ntransformer a naive knowledge distillation [31] and a ﬁne-\ntuning [5, 32, 19, 76] applied after each task on a balanced\nset of new data and rehearsal data. Finally, our DyTox strat-\negy exploits directly the very nature of transformers (sepa-\nrated task information from the pixels information) to tackle\ncatastrophic forgetting with three components: (1) a task to-\nken expansion, (2) a divergence classiﬁer, and (3) indepen-\ndent classiﬁers. All three greatly improve over the baseline\nKnowledge DistillationFinetuningToken ExpansionDivergence ClassiﬁerIndendepent ClassiﬁersAvg Last\nDyTox\nTransformer\n60.69 38.87\n\u0013 61.62 39.35\n\u0013 \u0013 63.42 42.21\nDynamic\n\u0013 \u0013 \u0013 67.30 47.57\n\u0013 \u0013 \u0013 \u0013 68.28 49.45\n\u0013 \u0013 \u0013 \u0013 \u0013 70.20 52.34\nTable 5: Ablations of the different key components of our\nDyTox architecture. We report the average accuracy and the\nlast accuracy on CIFAR100 for the setting with 50 steps.\ntransformer (42.21% →52.34% in “Last”) while having al-\nmost no memory overhead ( +0.2%). The divergence clas-\nsiﬁer improves the diversity between task tokens: we ob-\nserved that the minimal Euclidean distance between them\nincreases by 8%. Moreover, we also remarked that having\nindependent classiﬁers reduces the Chaudhry et al.’s forget-\nting [8] by more than 24%.\n5. Conclusion\nIn this paper, we propose DyTox, a new dynamic strategy\nfor continual learning based on transformer architecture. In\nour model, self-attention layers are shared across all tasks,\nand we add task-speciﬁc tokens to achieve task-specialized\nembeddings through a new task-attention layer. This ar-\nchitecture allows us to dynamically process new tasks with\nvery little memory overhead and does not require complex\nhyperparameter tuning. Our experiments show that our\nframework scales to large datasets like ImageNet1k with\nstate-of-the-art performances. Moreover, when a large num-\nber of tasks is considered ( i.e. CIFAR100 50 steps) our\nnumber of parameters increases reasonably contrary to pre-\nvious dynamic strategies.\nLimitations: True continual learning aims at learning an\nalmost unlimited number of tasks with low forgetting. No\ncurrent approaches are yet able to do so. Thus, forgetting is\nnot yet solved for continual learning but our model is a step\nforward in that direction.\nBroader impact: Machine learning models often are bi-\nased, with some classes suffering from lower performances.\nStudying forgetting in continual learning provides insights\nabout the difference in performances between classes. Our\ntask-specialized model could help reduce these biases.\nAcknowledgments: This work was partly supported by\nANR grant VISA DEEP (ANR-20-CHIA-0022), and the\nHPC resources of IDRIS AD011011706.\n8\nSymbol Meaning\n(xt\ni,yt\ni) Input sample & its label from the tth task\nCt Label set of the tth task\nC1:t All labels from all seen tasks\nθt Task token of the tth task\nClft Independent classiﬁer of the tth task\nSABl lth Self-Attention Block\nTAB Task-Attention Block\nTable 6: Notations used in the paper.\nA. Appendix\nTable 6 summarizes the notations used along this paper.\nA.1. Experimental details\nDatasets We use three datasets: CIFAR100 [40], Ima-\ngeNet100, and ImageNet1000 [13]. CIFAR100 is made of\n50,000 train RGB images and 10,000 test RGB images of\nsize 32 ×32 for 100 classes. ImageNet1000 contains 1.2\nmillion RGB train images and 50,000 validation RGB im-\nages of size 224 ×224 for 1000 classes. ImageNet100 is a\nsubset of 100 classes from ImageNet1000. We follow POD-\nNet [19] and DER [76] and use the same 100 classes they’ve\nused. Fine details about the datasets, like the class orders,\ncan be found in the provided code in the options ﬁles (see\nreadme).\nImplementation For all datasets, we train the model for\n500 epochs per task with Adam [38] with a learning rate\nof 5e−4, including 5 epochs of warmup. Following UCIR\n[32], PODNet [19], and DER [76], at the end of each task\n(except the ﬁrst) we ﬁnetuned our model for 20 epochs with\na learning rate of 5e−5 on a balanced dataset. In DyTox,\nwe applied the standard data augmentation of DeiT [69] but\nwe removed the pixel erasing [82], MixUp [80], and Cut-\nMix [78] augmentations for fair comparison. In contrast,\nin DyTox+ we used a MixUp [80] with beta distribution\nβ(0.8,0.8). During all incremental tasks ( t >1), the old\nclassiﬁers Clfi, i < tand the old task tokens θi, i < t\nparameters are frozen. During the ﬁnetuning phase where\nclasses are rebalanced [5, 32, 19, 76], these parameters are\noptimized, but the SABs are frozen.\nHyperparameter tuning In contrast with previous works\n[19, 76], we wanted stable hyperparameters, tuned for a sin-\ngle setting and then applied on all experiments. This avoids\noptimizing for the number of tasks, which defeats the pur-\npose of continual learning [22]. We tuned hyperparameters\nfor DyTox using a validation subset made of 10% of the\ntraining dataset, and this only on the CIFAR100 experiment\nHyperparameter Range Chosen value\nLearning rate 1e−3, 5e−4, 1e−4 5e−4\nEpochs 300, 500, 700 500\nλ 0.05, 0.1, 0.5 0.1\nCIFAR’s patch size 4, 8, 16 4\nImageNet’s patch size 14, 16 16\nTable 7: Hyperparameters that were tuned from the code-\nbase of [69]. We ran a gridsearch on CIFAR100 10 steps on\na validation set made of 10% of the training set, and kept\nﬁxed the chosen hyperparameters for all experiments (any\nnumber of steps and any datasets).\nwith 10 steps. We provide in Table 7 the chosen hyperpa-\nrameters. Results in the main paper shows that those hyper-\nparameters reach state-of-the-art on all other settings and\nnotably on ImageNet.\nBaselines E2E [5] and Simple-DER [48] results come\nfrom their respective papers. All other baseline results are\ntaken from the DER paper [76]. We now further describe\ntheir contributions. iCaRL [59] uses a knowledge distilla-\ntion loss [31] and at test-time predicts using a k-NN from its\nfeatures space. E2E [5] learns a model with knowledge dis-\ntillation and applies a ﬁnetuning after each step. UCIR [32]\nuses cosine classiﬁer and euclidean distance between the ﬁ-\nnal ﬂattened features as a distillation loss. BiC [75] uses\na knowledge distillation loss and also re-calibrates [28] the\nlogits of the new classes using a simple linear model trained\non validation data. W A [81] uses a knowledge distillation\nloss and re-weights at each epoch the classiﬁer weights as-\nsociated to new classes so that they have the same average\nnorm as the classiﬁer weights of the old classes. POD-\nNet [19] uses a cosine classiﬁer and a speciﬁc distillation\nloss (POD) applied at multiple intermediary features of the\nResNet backbone. RPSNet [56] uses knowledge distilla-\ntion and manipulates subnetworks in its architecture, fol-\nlowing the lottery ticket hypothesis [24]. DER [76] cre-\nates a new ResNet per task. All ResNets’ embeddings are\nconcatenated and fed to a unique classiﬁer. ResNets are\npruned using HAT [64] masking procedure. Note that DER\npruning has multiple hyperparameters that are set differ-\nently according to the settings. Furthermore, the reported\nparameters count, after pruning, in [76] is an average of the\ncount over all steps: the ﬁnal parameters count (necessarily\nhigher) wasn’t available. Finally, Simple-DER [48] is sim-\nilar to DER, with a simpler pruning method which doesn’t\nrequire any hyperparameter tuning.\nA.2. Parameter sharing of the TAB\nPrevious dynamic methods as DER [76] and Simple-\nDER [48] shared no parameters between tasks until the ﬁ-\n9\nTAB parameter sharing? #P Avg Last\n\u0017 97.59 72.20 56.00\n\u0013 10.77 70.20 52.34\nTable 8: Investigation of the parameter sharing of TAB.\nWe report the “Avg” accuracy and the “Last” accuracy for\nthe 50 steps setting on CIFAR100. The second row corre-\nsponds to DyTox.\nnal classiﬁer. We proposed instead with DyTox to share the\nencoder (SABs) and the decoder (TAB) parameters across\ntasks, leading to a minimal memory overhead while also\nsharing common information between tasks. In Table 8, we\ncompare the impact of sharing the TAB per task — and only\nmaintain different tokens per task. In the ﬁrst row, a differ-\nent TAB is created per task, while in the second row the\nsame TAB is used — which is the DyTox strategy. A dif-\nferent TAB per task leads to better results (56% v.s. 52%\nin “Last” accuracy) because the network can be more di-\nverse with each TAB specialized to its associated task. This\nincreased diversity has a drawback: the memory overhead\nis too important (97M v.s. 10M parameters). We ﬁnd in\npractice that DyTox strikes a good balance between mem-\nory overhead and continual performance.\nA.3. Novel continual training procedure\nDyTox++ We nicknamed DyTox+ our model when com-\nbined with a novel continual procedure based on MixUp\n[80]. We now reﬁne DyTox+ into DyTox++ by adding a\nnew component during the training: the Sharpness-Aware\nMinimizer (SAM) [41]. Indeed, aiming for wider min-\nima is particularly important in continual learning [39, 72].\nThis is because sharp task-speciﬁc minima lead to over-\nspecialization to a particular task and consequently to a for-\ngetting of all other tasks. Weights constraints as EWC [39]\nor second-order optimization [43] have similar motivations.\nSAM estimates the worst closest parameters during a ﬁrst\nforward/backward pass, and then optimizes the loss w.r.t. to\nthem during a second forward/pass. In consequence, Dy-\nTox++ optimizes the loss not w.r.t. the current parameters\nbut w.r.t. a region of possible parameters leading to wide\nlocal minima that span across multiple tasks. In practice,\nwe used the Adaptive SAM (ASAM) [41], an extension of\nSAM that is more robust to hyperparameters.\nDyTox+ and DyTox++ experiments The computational\noverhead of ASAM is lower than more complex second-\norder methods, but it still doubles the number of forward\nand backward passes. For this reason, we didn’t include it\nin our main experiment but propose in Table 9 and Table 10\nexperiments on CIFAR100 and ImageNet100. The gain\nprovided by MixUp then ASAM on our model (DyTox++)\nleads to a consistent improvement of +4.7% in “Avg“ com-\npared to the previous State-of-the-Art DER [76] on CI-\nFAR100 50 steps (Table 9 and +4.6% on ImageNet100 10\nsteps (Table 10). Future works could consider the promising\nLook-SAM [49] to reduce the time overhead.\nTraining procedure introspection In Table 11, we com-\npare DyTox+ and DyTox++ on CIFAR100 in a joint set-\nting (no continual) and in a continual setting with 50 steps.\nIn the joint setting, our model slightly beneﬁts from both\nMixUp and ASAM: the gain is limited (+1.79 p.p.). On the\nother hand, those two methods greatly improve the extreme\ncontinual setting of 50 steps (+6.42 p.p.). This shows that\nthe gain is not due to absolute improvements of the model\nperformance. Moreover, using the Chaudrhy et al.’s forget-\nting [8] measure, we compare how much a model has for-\ngotten relatively to its previous tasks. This metric is there-\nfore agnostic to absolute performance improvements. Dy-\nTox had a forgetting of 33.15%, DyTox+ of 31.50%, and\nDyTox++ of 30.47%: a total reduction of 2.68 p.p . This\nvalidates our novel training procedures that are particularly\nefﬁcient for continual learning.\nA.4. Patch size effect on forgetting\nOur model is the ﬁrst application of transformers for con-\ntinual computer vision. A key component of the transformer\narchitecture is the patch tokenizer. The number of patch to-\nkens in an image is determined by the patch size: a larger\npatch size means less tokens, and vice-versa. We wondered\nabout the effect of the patch size on forgetting and tested\nthree different kind of patch sizes in Table 12. Echoing re-\nsults in vision transformers [16, 69], a smaller patch size (4\nvs. 8 and 16) performs best in a joint training. However, the\nforgetting deﬁned by Chaudhry et al. [8] is relatively simi-\nlar, with 33.15% for a patch of size of 4, and 33.20% for a\npatch size of 16. Therefore, we argue that the transformer\narchitecture is hardly sensitive to the patch resolution w.r.t\nits forgetting in continual learning.\nA.5. ResNet backbone\nDyTox is made of two main components: the SABs and\nthe unique TAB. The TAB structure, taking in input both\npatch tokens and a task token, is crucial to our strategy. Yet,\nthe SAB could be of any kind of features extractor, based on\nconvolutions or attentions. Following the hybrid network\nproposed in ablations by Dosovitskiy et al. [16], we tried\nto replace the collection of SABs by a ResNet18. The ﬁ-\nnal features of the ResNet, before global pooling, of shape\n(W×H×D) can be seen asW×Htokens of Ddimension.\nWe made a few modiﬁcations to this ResNet to boost its per-\nformance, namely removed the fourth and ultimate layer,\nand added a pointwise convolution with 504 output chan-\nnels (so it can be divided by the 12 attention heads of the\n10\n10 steps 20 steps 50 steps\nMethods #P Avg Last #P Avg Last #P Avg Last\nResNet18 Joint 11.22 - 80.41 11.22 - 81.49 11.22 - 81.74\nTransf. Joint 10.72 - 76.12 10.72 - 76.12 10.72 - 76.12\nW A [81] 11.22 69.46 ±0.29 53.78 11.22 67.33 ±0.15 47.31 11.22 64.32 ±0.28 42.14\nDER w/o P [76] 112.27 75.36 ±0.36 65.22 224.55 74.09 ±0.33 62.48 561.39 72.41 ±0.36 59.08\nDyTox 10.73 73.66 ±0.02 60.67±0.34 10.74 72.27 ±0.18 56.32±0.61 10.77 70.20 ±0.16 52.34±0.26\nDyTox+ 10.73 75.54 ±0.10 62.06±0.25 10.74 75.04 ±0.11 60.03±0.45 10.77 74.35 ±0.05 57.09±0.13\nDyTox++ 10.73 77.10±0.08 64.53±0.08 10.74 76.57±0.18 62.44±0.22 10.77 75.45±0.19 58.76±0.28\nDyTox+ 10.73 76.74±1.08 67.04±0.10 10.74 76.25±0.30 62.85±0.16 10.77 74.16±1.89 59.10±0.99\nDyTox++ 10.73 77.01±1.21 67.53±0.37 10.74 76.81±0.43 64.27±0.81 10.77 75.53±2.79 59.51±1.61\nTable 9: Results on CIFAR100 averaged over three different class orders. W A and DER w/o P results are reported from\n[76]. DyTox+ uses MixUp in addition of the DyTox strategy, DyTox++ further adds a sharpness-aware minimization [41].\nMethods #P top-1 top-5\nAvg Last Avg Last\nResNet18 joint 11.22 - - - 95.1\nTransf. joint 11.00 - 79.12 - 93.48\nW A [81] 11.22 - - 91.00 84 .10\nDER w/o P [76] 112.27 77.18 66.70 93.23 87.52\nDyTox 11.01 77.15 69.10 92.04 87.98\nDyTox+ 11.01 79.22 69.06 93.72 88.82\nDyTox++ 11.01 80.76 72.46 94.40 90.10\nTable 10: Results on ImageNet-100 with 10 steps of 10\nnew classes each. W A and DER w/o P results are reported\nfrom [76]. DyTox+ uses MixUp in addition of the Dy-\nTox strategy, DyTox++ further adds a sharpness-aware min-\nimizer.\nJoint (1 step) 50 steps\nTraining Last (↑) Last (↑) Forgetting (↓)\nDyTox 76.12 52.34 33.15\nDyTox+ 78.86 +1.39 59.10+4.75 24.81-1.65\nDyTox++ 78.70 +0.40 59.51+1.67 26.70-1.03\nTable 11: “Last” accuracy and forgetting [8] on CI-\nFAR100 for the joint (1 step, no continual) and 50 steps\nsettings.\nJoint (1 steps) 50 steps\nPatch size Last (↑) Last (↑) Forgetting (↓)\n4 76.12 52.34 33.15\n8 67.65 43.93 35.44\n16 50.15 31.49 33.20\nTable 12: Patch size effect on continual for the joint (1\nstep, no continual) and 50 steps settings on CIFAR100. We\nchoose a patch size of 4 for our main experiments: yet, it\nhas only few impact on forgetting.\nEncoder #P Avg Last\nResNet 10.68 68.53 50.05\nSABs 10.77 70.20 52.34\nTable 13: Hybrid network on CIFAR100 50 steps. While\nthe features extractor is made of SABs in DyTox, here\nwe instead use a modiﬁed ResNet18. Our framework still\nworks well with a convolution-based approach.\nTAB), a batch normalization [34], and a ReLU activation.\nThese simple modiﬁcations are sufﬁcient for our proof of\nconcept, and thus we also didn’t tune deeply this model. We\ndisplay in Table 13 the comparison of the two backbones on\nCIFAR100 50 steps: (1) with ResNet, and (2) with SABs\n(DyTox). Performances are slightly lower than DyTox with\nSABs, however, they are still signiﬁcantly higher than pre-\nvious state-of-the-art like W A [81], especially in “Last” ac-\ncuracy. Moreover, the parameters count is comparable to\nDyTox. This experiment shows that our DyTox framework,\nwhile designed with a transformer backbone in mind, is also\nefﬁcient on non-token-based architectures such as a ResNet.\nA.6. Alternative task decoders\nWe investigate here other approaches for conditioning\nfeatures to the different tasks. Residual Adapters [58] adds\na different residual branch made of a pointwise convolu-\ntion for each domain the model is learned (e.g. CIFAR then\nImageNet then SVHN). This model needs the task/dataset/-\ndomain identiﬁer at test-time to determine which residual to\nuse. For VQA task [2], FiLM [54] proposes to modify the\nvisual features using the the textual query.\nWe adapt these two feature conditioning strategies for\nour transformer backbone architecture. We perform a global\ntoken pooling after the ultimate SAB, and apply for each\nlearned task, a residual adapter or a FiLM. Residual adapter\nin our case is a MLP, and FiLM parameters are directly\nlearned. As for DyTox, we forward each task-speciﬁc em-\n11\nCIFAR100 ImageNet100\nTop-1 Top-5\nTask decoder Avg Last Avg Last\nResidual Adapters [58] 70.00 52.38 91.25 85.00\nFiLM [54] 69.42 54.05 89.49 81.40\nTAB (ours) 70.20 52.34 92.04 87.98\nTable 14: Alternative task conditioner on CIFAR100 50\nsteps and ImageNet100 10 steps. While the simpler Resid-\nual Adapters and FiLM perform similarly to our TAB on\nCIFAR100, they forget signiﬁcantly more on the complex\nImageNet100.\nbedding to the respective task-speciﬁc classiﬁer. We show-\ncase the continual performance in Table 14 on CIFAR100\n50 steps and ImageNet100 10 steps. On the former dataset,\nsmaller and easier, the residual adapters and FiLM have\nsimilar performance as our TAB approach. On the other\nhand, as soon as the task complexity increases with the\nmore detailed ImageNet100 dataset, FiLM and Residual\nadapter based conditioning strategies forget signiﬁcantly\nmore than our complete DyTox framework: TAB outper-\nform the Residual Adapters by +2.98p.p in “Last” top-5 ac-\ncuracy and FiLM by +6.58 p.p .\nB. Erratum\nIn Continual Learning, usually features are extracted\nonce from images without data augmentations, and a proce-\ndure (random, closest [5], or even iCaRL iterative selection\n“herding” [59]) is applied to select rehearsal images. In this\nsection, we call this family of selections Global Memory.\nIn DyTox, when running our algorithm on multiple\nGPUs for data parallelism, we actually extracted features\nwith data augmentations, and furthermore each GPU ex-\ntracted features using their own slightly different data aug-\nmentations. As a result, each GPU could select different\nsamples, and thus the effective memory size (across all pro-\ncesses) could be up to N times bigger than asked, with N\nthe number of GPUs. We call that Distributed Memory.\nTherefore, DyTox’s results in this paper, while interest-\ning, are not exactly comparable to the compared baselines.\nThis section acts as an erratum. That said, we believe that\nthis Distributed Memory have merits.\nB.1. Interest of the Distributed Memory\n• In the ﬁrst place, it has similarity with the Federated\nLearning literature. In our case, each process/GPU/-\nmachine has access to its own memory. Which would\nmake sense, as more machines would also mean a big-\nger disk storage.\n• Because each GPU extracts with different data aug-\nmentations, the overlap of chosen among GPUs is very\nlow: on CIFAR100 with 2 GPUs, the overlap is usually\nbetween 1% and 10%. This means that very represen-\ntative/useful examples will be selected twice (and thus\nseen more often). But also that each GPU, because of\nthe iterative nature of the rehearsal gathering of iCaRL,\nwill select data belonging to different modes, leading\nto increased diversity.\nIt results in a direct gain of performance on CIFAR as\nhighlighted in Table 15.\nB.2. Updated results\nHere are the updated results. Global Memory uses 20\nsamples per class in total (as other baselines), Distributed\nMemory also uses 20 samples per class, but split across\nGPUs. e.g. on CIFAR100 with 2 GPUs, each GPU only\nsamples 10 samples per class at most. So the overall effec-\ntive memory size constraint is respected.\nOverall, DyTox keeps strong performance, more often\nthan not above W A [81] the third contender. Moreover, Dy-\nTox+, our improved version also presented in the original\npaper version still reaches excellent performance, due to the\nforgetting reduction we explained before.\nGray color symbolizes results presented in the original\npaper version.\nCIFAR experiments were run on 2 GPUs, all are with\ndistributed memory and are shown in Table 16.\nImageNet experiments, shown in Table 17 in global\nmemory were run with 8 GPUs, and with distributed mem-\nory with 4 GPUs (thus only 5 images/class/GPU which\nprobably explains here the slightly lower results compared\nto global memory).\nReferences\n[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,\nMarcus Rohrbach, and Tinne Tuytelaars. Memory aware\nsynapses: Learning what (not) to forget. In Proceed-\nings of the IEEE European Conference on Computer Vision\n(ECCV), 2018. (page 2).\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. VQA: Visual Question Answering. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), 2015. (page 11).\n[3] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer\nnormalization. In Advances in NeurIPS 2016 Deep Learning\nSymposium, 2016. (page 3).\n[4] Eden Belouadah and Adrian Popescu. Il2m: Class incremen-\ntal learning with dual memory. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), 2019.\n(page 4).\n[5] Francisco M. Castro, Manuel J Mar´ın-Jim´enez, Nicol´as Guil,\nCordelia Schmid, and Karteek Alahari. End-to-end incre-\nmental learning. In Proceedings of the IEEE European Con-\n12\n10 steps 20 steps 50 steps\nMemory Type # Memory / GPU Total effective # memory Avg Last Avg Last Avg Last\nGlobal 2,000 2,000 67.33 51.68 67.30 48.45 64.39 43.47\nDistributed 1,000 2,000 71.50 57.76 68.86 51.47 64.82 45.61\nTable 15: Results on CIFAR100 for Distributed vs Global Memorywhen run with 2 GPUs. Global memory uses the same\n2k samples across each GPU, while Distributed memory uses 1k samples per memory with potential overlap across sets.\n10 steps 20 steps 50 steps\nMethods #P Avg Last #P Avg Last #P Avg Last\nResNet18 Joint 11.22 - 80.41 11.22 - 81.49 11.22 - 81.74\nTransf. Joint 10.72 - 76.12 10.72 - 76.12 10.72 - 76.12\niCaRL [59] 11.22 65.27 ±1.02 50.74 11.22 61.20 ±0.83 43.75 11.22 56.08 ±0.83 36.62\nUCIR [32] 11.22 58.66 ±0.71 43.39 11.22 58.17 ±0.30 40.63 11.22 56.86 ±0.83 37.09\nBiC [75] 11.22 68.80 ±1.20 53.54 11.22 66.48 ±0.32 47.02 11.22 62.09 ±0.85 41.04\nW A [81] 11.22 69.46 ±0.29 53.78 11.22 67.33 ±0.15 47.31 11.22 64.32 ±0.28 42.14\nPODNet [19] 11.22 58.03 ±1.27 41.05 11.22 53.97 ±0.85 35.02 11.22 51.19 ±1.02 32.99\nRPSNet [56] 56.5 68.60 57.05 - - - - - -\nDERw/o P [76] 112.27 75.36 ±0.36 65.22 224.55 74.09 ±0.33 62.48 561.39 72.41 ±0.36 59.08\nDER†[76] - 74.64 ±0.28 64.35 - 73.98 ±0.36 62.55 - 72.05 ±0.55 59.76\nDyTox 10.73 73.66±0.02 60.67±0.34 10.74 72.27±0.18 56.32±0.61 10.77 70.20±0.16 52.34±0.26\nDyTox+ 10.73 75.54±0.10 62.06±0.25 10.74 75.04±0.11 60.03±0.45 10.77 74.35±0.05 57.09±0.13\nDyTox distMem 10.73 71.50 57.76 10.74 68.86 51.47 10.77 64.82 45.61\nDyTox+ distMem 10.73 74.10 62.34 10.74 71.62 57.43 10.77 68.90 51.09\nTable 16: Results on CIFAR100. Updated version with an erratum for Global vs. Distributed memory of the table Table 3.\nGray color symbolizes results presented in the original paper version where up to 2 times the amount of rehearsal samples\nwas used.\nference on Computer Vision (ECCV), 2018. (pages 2, 6, 8, 9,\n12, 14).\n[6] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bul `o,\nElisa Ricci, and Barbara Caputo. Modeling the background\nfor incremental learning in semantic segmentation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020. (page 2).\n[7] Olivier Chapelle, L ´eon Bottou, and Vladimir Vapnik. Vic-\ninal risk minimization. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2001. (page 7).\n[8] Arslan Chaudhry, Puneet Dokania, Thalaiyasingam Ajan-\nthan, and Philip H. S. Torr. Riemannian walk for incremental\nlearning: Understanding forgetting and intransigence. Pro-\nceedings of the IEEE European Conference on Computer Vi-\nsion (ECCV), 2018. (pages 2, 8, 10, 11).\n[9] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,\nand Mohamed Elhoseiny. Efﬁcient lifelong learning with\na-gem. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2019. (page 2).\n[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,\nThalaiyasingam Ajanthan, Puneet K. Dokania, Philip H.S.\nTorr, and Marc’Aurelio Ranzato. On tiny episodic memo-\nries in continual learning. In International Conference on\nMachine Learning (ICML) Workshop, 2019. (page 2).\n[11] Mark Patrick Collier, Effrosyni Kokiopoulou, Andrea Ges-\nmundo, and Jesse Berent. Routing networks with co-training\nfor continual learning. In icmlws, 2020. (page 2).\n[12] St ´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Giulio\nMorcos, Ari annd Biroli, and Levent Sagun. Convit: Improv-\ning vision transformers with soft convolutional inductive bi-\nases. In arXiv preprint library, 2021. (pages 2, 5).\n[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2009. (pages 5, 9).\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL) , 2018. (pages\n2, 5).\n[15] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,\nZiyan Wu, and Rama Chellappa. Learning without memo-\nrizing. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019. (page 2).\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. In Proceedings of the International Conference on\nLearning Representations (ICLR) , 2021. (pages 2, 3, 4, 5,\n10).\n13\nImageNet100 10 steps ImageNet1000 10 steps\n#P top-1 top-5 #P top-1 top-5\nMethods Avg Last Avg Last Avg Last Avg Last\nResNet18 joint 11.22 - - - 95.10 11 .68 - - - 89.27\nTransf. joint 11.00 - 79.12 - 93.48 11.35 - 73.58 - 90.60\nE2E [5] 11.22 - - 89.92 80.29 11.68 - - 72.09 52.29\nSimple-DER [48] - - - - - 28.00 66.63 59.24 85.62 80.76\niCaRL [59] 11.22 - - 83.60 63 .80 11 .68 38 .40 22 .70 63 .70 44 .00\nBiC [32] 11.22 - - 90.60 84 .40 11 .68 - - 84.00 73 .20\nW A [81] 11.22 - - 91.00 84 .10 11 .68 65 .67 55 .60 86 .60 81 .10\nRPSNet [56] - - 87.90 74 .00 - - - - -\nDER w/o P [76] 112.27 77.18 66.70 93.23 87.52 116.89 68.84 60.16 88.17 82.86\nDER†[76] - 76.12 66.06 92.79 88.38 - 66.73 58.62 87.08 81.89\nDyTox 11.01 77.15 69.10 92.04 87.98 11.36 71.29 63.34 88.59 84.49\nDyTox+ 11.01 79.22 69.06 93.72 88.82 11.01 — — — —\nDyTox globalMem 11.01 71.85 57.94 90.72 83.52 11.36 68.14 59.75 87.03 82.93\nDyTox+ globalMem 11.01 77.62 65.94 93.15 88.78 11.36 73.21 64.56 91.09 87.07\nDyTox distMem 11.01 73.96 62.20 91.29 85.60 11.36 — — — —\nDyTox+ distMem 11.01 77.15 67.70 93.17 89.42 11.36 70.88 60.00 90.53 85.25\nTable 17: Results on ImageNet-100 and ImageNet-1000 datasets . Updated version with an erratum for Global vs. Dis-\ntributed memory of the table Table 2. Gray color symbolizes results presented in the original paper version where up to 4\ntimes the amount of rehearsal samples was used.\n[17] Arthur Douillard, Yifu Chen, Arnaud Dapogny, and\nMatthieu Cord. Plop: Learning without forgetting for con-\ntinual semantic segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. (page 2).\n[18] Arthur Douillard, Yifu Chen, Arnaud Dapogny, and\nMatthieu Cord. Tackling catastrophic forgetting and back-\nground shift in continual semantic segmentation. In arXiv\npreprint library, 2021. (page 2).\n[19] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas\nRobert, and Eduardo Valle. Podnet: Pooled outputs dis-\ntillation for small-tasks incremental learning. In Proceed-\nings of the IEEE European Conference on Computer Vision\n(ECCV), 2020. (pages 1, 2, 5, 7, 8, 9, 13).\n[20] Arthur Douillard and Timoth ´ee Lesort. Continuum: Sim-\nple management of complex continual learning scenarios. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) Workshop, 2021. (page 6).\n[21] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.\nOrthogonal gradient descent for continual learning. In In-\nternaltional Conference on Artiﬁcial Intelligence and Statis-\ntics (AISTATS), 2020. (page 2).\n[22] Sebastian Farquhar and Yarin Gal. Towards robust evalua-\ntions of continual learning. In International Conference on\nMachine Learning (ICML) Workshop, 2018. (page 9).\n[23] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori\nZwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and\nDaan Wierstra. PathNet: Evolution Channels Gradient De-\nscent in Super Neural Networks. arXiv preprint library ,\n2017. (pages 1, 2).\n[24] Jonathan Frankle and Michael Carbin. The lottery ticket hy-\npothesis: Finding sparse, trainable neural networks. In Pro-\nceedings of the International Conference on Learning Rep-\nresentations (ICLR), 2019. (page 9).\n[25] Robert French. Catastrophic forgetting in connectionist net-\nworks. Trends in cognitive sciences, 1999. (page 2).\n[26] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N Dauphin. Convolutional sequence to sequence\nlearning. In International Conference on Machine Learning\n(ICML), 2017. (page 3).\n[27] Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Con-\ntinual learning via neural pruning. Advances in Neural In-\nformation Processing Systems (NeurIPS) Workshop , 2019.\n(pages 1, 2).\n[28] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.\nOn calibration of modern neural networks. In International\nConference on Machine Learning (ICML) , 2017. (pages 7,\n9).\n[29] Tyler L. Hayes, Kushal Kaﬂe, Robik Shrestha, Manoj\nAcharya, and Christopher Kanan. Remind your neural\nnetwork to prevent catastrophic forgetting. In Proceed-\nings of the IEEE European Conference on Computer Vision\n(ECCV), 2020. (page 2).\n[30] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2016. (page 2).\n14\n[31] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling\nthe knowledge in a neural network. In Advances in Neural\nInformation Processing Systems (NeurIPS) Workshop, 2015.\n(pages 5, 8, 9).\n[32] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and\nDahua Lin. Learning a uniﬁed classiﬁer incrementally via re-\nbalancing. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2019. (pages\n1, 2, 5, 6, 7, 8, 9, 13, 14).\n[33] Steven C.Y . Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-\nHung Chen, Yi-Ming Chan, and Chu-Song Chen. Compact-\ning, picking and growing for unforgetting continual learn-\ning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2019. (pages 1, 2).\n[34] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International Conference on Machine Learn-\ning (ICML), 2015. (page 11).\n[35] Ahmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, and\nCordelia Schmid. Memory-efﬁcient incremental learning\nthrough feature adaptation. InProceedings of the IEEE Euro-\npean Conference on Computer Vision (ECCV), 2020. (page\n2).\n[36] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac,\nCarl Doersch, Catalin Ionescu, David Ding, Skanda Kop-\npula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier\nH´enaff, Matthew M. Botvinick, Andrew Zisserman, Oriol\nVinyals, and Jo ˜ao Carreira. Perceiver io: A general archi-\ntecture for structured inputs & outputs. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR), 2022. (page 2).\n[37] Ronald Kemker and Christopher Kanan. Fearnet: Brain-\ninspired model for incremental learning. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR), 2018. (page 2).\n[38] Diederik P. Kingma and Jimmy Ba. Adam: A method\nfor stochastic optimization. In Proceedings of the Inter-\nnational Conference on Learning Representations (ICLR) ,\n2014. (page 9).\n[39] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel\nVeness, Guillaume Desjardins, Andrei A. Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, Demis Hassabis, Claudia Clopath, Dharshan Ku-\nmaran, and Raia Hadsell. Overcoming catastrophic for-\ngetting in neural networks. Proceedings of the National\nAcademy of Sciences, 2017. (pages 1, 2, 10).\n[40] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. Technical Report, 2009.\n(pages 5, 9).\n[41] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and\nIn Kwon Choi. Asam: Adaptive sharpness-aware minimiza-\ntion for scale-invariant learning of deep neural networks.\nIn International Conference on Machine Learning (ICML) ,\n2021. (pages 10, 11).\n[42] Yann LeCun, Patrick Haffner, L ´eon Bottou, and Yoshua\nBengio. Object recognition with gradient-based learning.\nIn Shape, contour and grouping in computer vision , 1999.\n(page 2).\n[43] Janghyeon Lee, Hyeong Gwon Hong, Donggyu Joo, and\nJunmo Kim. Continual learning with extended kronecker-\nfactored approximate curvature. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2020. (page 10).\n[44] Stefan Lee, Senthil Purushwalkam, Michael Cogswell,\nDavid Crandall, and Dhruv Batra. Why m heads are bet-\nter than one: Training a diverse ensemble of deep networks.\nIn arXiv preprint library, 2015. (page 2).\n[45] Timoth ´ee Lesort, Hugo Caselles-Dupr ´e, Michael Garcia-\nOrtiz, Andrei Stoian, and David Filliat. Generative models\nfrom the perspective of continual learning. In International\nJoint Conference on Neural Networks, 2019. (page 2).\n[46] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and\nCaiming Xiong. Learn to grow: A continual structure learn-\ning framework for overcoming catastrophic forgetting. Pro-\nceedings of the International Conference on Learning Rep-\nresentations (ICLR), 2019. (pages 1, 2).\n[47] Z. Li and D. Hoiem. Learning without forgetting. Proceed-\nings of the IEEE European Conference on Computer Vision\n(ECCV), 2016. (page 2).\n[48] Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang,\nand Wei-Shi Zheng. Preserving earlier knowledge in contin-\nual learning with the help of all previous feature extractors.\nIn arXiv preprint library, 2021. (pages 1, 2, 6, 9, 14).\n[49] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and\nYang You. Sharpness-aware minimization in large-batch\ntraining: Training vision transformer in minutes. In Open-\nReview, 2021. (page 10).\n[50] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE International Conference on Com-\nputer Vision (ICCV), 2021. (page 2).\n[51] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient\nepisodic memory for continual learning. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors,Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2017. (page 2).\n[52] Saeed Masoudnia and Reza Ebrahimpour. Mixture of ex-\nperts: a literature survey. Artiﬁcial Intelligence Review ,\n42(2):275–293, 2014. (page 5).\n[53] Chris Olah, Alexander Mordvintsev, and Ludwig\nSchubert. Feature visualization. Distill, 2017.\nhttps://distill.pub/2017/feature-visualization. (page 2).\n[54] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence (AAAI) , 2018. (pages 11,\n12).\n[55] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with\na uniﬁed text-to-text transformer. In Journal of Machine\nLearning Research, 2019. (page 5).\n[56] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fa-\nhad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An\n15\nadaptive random path selection approach for incremental\nlearning. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2019. (pages 6, 7, 9, 13, 14).\n[57] Dhanesh Ramachandram and Graham W. Taylor. Deep mul-\ntimodal learning: A survey on recent advances and trends. In\nIEEE Signal Processing Magazine, 2017. (page 4).\n[58] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.\nLearning multiple visual domains with residual adapters.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017. (pages 11, 12).\n[59] Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. icarl: Incremental clas-\nsiﬁer and representation learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017. (pages 1, 2, 5, 6, 7, 9, 12, 13, 14).\n[60] Anthony Robins. Catastrophic forgetting, rehearsal and\npseudorehearsal. Connection Science, 1995. (page 2).\n[61] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,\nHubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-\nvan Pascanu, and Raia Hadsell. Progressive neural networks.\narXiv preprint library, 2016. (page 2).\n[62] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient pro-\njection memory for continual learning. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR), 2021. (page 2).\n[63] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D.\nParikh, and D. Batra. Grad-cam: Visual explanations from\ndeep networks via gradient-based localization. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion (ICCV), 2017. (page 5).\n[64] Joan Serr `a, D ´ıdac Sur´ıs, Marius Miron, and Alexandros\nKaratzoglou. Overcoming catastrophic forgetting with hard\nattention to the task. InInternational Conference on Machine\nLearning (ICML), 2018. (pages 1, 2, 9).\n[65] Murray Shanahan, Christos Kaplanis, and Jovana Mitrovi ´c.\nEnsembles and encoders for task-free continual learning. In\narXiv preprint library, 2021. (page 4).\n[66] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. In Advances\nin Neural Information Processing Systems (NeurIPS), 2017.\n(page 2).\n[67] Sebastian Thrun. Lifelong learning algorithms. In Springer\nLearning to Learn, 1998. (page 2).\n[68] Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tan-\nmoy Bhattacharya, and Sarah Michalak. On mixup train-\ning: Improved calibration and predictive uncertainty for deep\nneural networks. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2019. (page 7).\n[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers and distillation through at-\ntention. In International Conference on Machine Learning\n(ICML), 2021. (pages 2, 5, 6, 9, 10).\n[70] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2021. (pages 2, 4,\n5).\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. (pages 2,\n3, 4, 5).\n[72] Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. Re-\nhearsal revealed: The limits and merits of revisiting samples\nin continual learning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR)\nWorkshop, 2021. (page 10).\n[73] Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble:\nAn alternative approach to efﬁcient ensemble and lifelong\nlearning. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2020. (page 2).\n[74] Ross Wightman. Pytorch image models. https :\n/ / github . com / rwightman / pytorch - image -\nmodels, 2019. (page 6).\n[75] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,\nZicheng Liu, Yandong Guo, and Yun Fu. Large scale in-\ncremental learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2019.\n(pages 1, 4, 7, 9, 13).\n[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-\nically expandable representation for class incremental learn-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. (pages 1, 2,\n5, 6, 7, 8, 9, 10, 11, 13, 14).\n[77] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju\nHwang. Lifelong learning with dynamically expandable net-\nworks. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2018. (pages 1, 2).\n[78] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), 2019. (page 9).\n[79] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-\nual learning through synaptic intelligence. In International\nConference on Machine Learning (ICML), 2017. (page 2).\n[80] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. Proceedings of the International Conference on Learn-\ning Representations (ICLR), 2018. (pages 7, 9, 10).\n[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao\nXia. Maintaining discrimination and fairness in class incre-\nmental learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2020.\n(pages 4, 5, 6, 7, 9, 11, 12, 13, 14).\n[82] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InProceedings\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\n2017. (page 9).\n[83] Peng Zhou, Long Mai, Jianming Zhang, Ning Xu, Zuxuan\nWu, and Larry S. Davis. M2kd: Multi-model and multi-\nlevel knowledge distillation for incremental learning. arXiv\npreprint library, 2019. (page 2).\n16",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.6787556409835815
    },
    {
      "name": "Transformer",
      "score": 0.6345534920692444
    },
    {
      "name": "Computer science",
      "score": 0.43396344780921936
    },
    {
      "name": "Electrical engineering",
      "score": 0.20464304089546204
    },
    {
      "name": "Engineering",
      "score": 0.1832178235054016
    },
    {
      "name": "Computer security",
      "score": 0.11519914865493774
    },
    {
      "name": "Voltage",
      "score": 0.03911754488945007
    }
  ]
}