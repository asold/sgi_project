{
  "title": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design",
  "url": "https://openalex.org/W4402227480",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5092050382",
      "name": "Lindia Tjuatja",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2114830916",
      "name": "Valerie Chen",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2789722361",
      "name": "Tongshuang Wu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5106989720",
      "name": "Ameet Talwalkwar",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6842080761",
    "https://openalex.org/W2135298472",
    "https://openalex.org/W6843142474",
    "https://openalex.org/W2059134104",
    "https://openalex.org/W6746141323",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6838742530",
    "https://openalex.org/W6713371532",
    "https://openalex.org/W6851157106",
    "https://openalex.org/W4252080790",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W6854004153",
    "https://openalex.org/W4390035079",
    "https://openalex.org/W6790003725",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2098075168",
    "https://openalex.org/W2013149141",
    "https://openalex.org/W73977458",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4366549000",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W6810181877",
    "https://openalex.org/W2502801113",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W6849479551",
    "https://openalex.org/W2114592633",
    "https://openalex.org/W1984120644",
    "https://openalex.org/W6853441399",
    "https://openalex.org/W4385573517",
    "https://openalex.org/W2142598536",
    "https://openalex.org/W1515732765",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W6849108348",
    "https://openalex.org/W6846870107",
    "https://openalex.org/W6856102926",
    "https://openalex.org/W6763359185",
    "https://openalex.org/W2125825154",
    "https://openalex.org/W2964132420",
    "https://openalex.org/W6802669662",
    "https://openalex.org/W6851152545",
    "https://openalex.org/W6855757595",
    "https://openalex.org/W1554254093",
    "https://openalex.org/W6857660365",
    "https://openalex.org/W4296564631",
    "https://openalex.org/W4385718018",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4389519937",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W6855469472",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4321102425",
    "https://openalex.org/W1986551946",
    "https://openalex.org/W4381245716",
    "https://openalex.org/W4361807105",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W4387799916",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4401043020",
    "https://openalex.org/W4385374425",
    "https://openalex.org/W2751936342",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3130520521",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W1586628792",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2405998801",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W596386093",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2913668833"
  ],
  "abstract": "Abstract One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.1",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6492854952812195
    },
    {
      "name": "Survey research",
      "score": 0.6125993728637695
    },
    {
      "name": "Data science",
      "score": 0.44092270731925964
    },
    {
      "name": "Psychology",
      "score": 0.1783231496810913
    },
    {
      "name": "Applied psychology",
      "score": 0.16967818140983582
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}