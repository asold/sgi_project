{
  "title": "A New LLMS Algorithm for Antenna Array Beamforming",
  "url": "https://openalex.org/W1991469708",
  "year": 2010,
  "authors": [
    {
      "id": null,
      "name": "Srar, Jalal Abdulsayed",
      "affiliations": [
        "Curtin University"
      ]
    },
    {
      "id": "https://openalex.org/A4304271541",
      "name": "Chung Kah Seng",
      "affiliations": [
        "Curtin University"
      ]
    },
    {
      "id": "https://openalex.org/A2106509227",
      "name": "Mansour Ali",
      "affiliations": [
        "Curtin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6927012387",
    "https://openalex.org/W2159523308",
    "https://openalex.org/W2120056672",
    "https://openalex.org/W2115947173",
    "https://openalex.org/W2132749914",
    "https://openalex.org/W2160498832",
    "https://openalex.org/W2135846983",
    "https://openalex.org/W2100312877",
    "https://openalex.org/W2157187092",
    "https://openalex.org/W2122043529",
    "https://openalex.org/W2052828532",
    "https://openalex.org/W1997346737",
    "https://openalex.org/W2546766916",
    "https://openalex.org/W2124754486"
  ],
  "abstract": "International audience",
  "full_text": "© 2010 IEEE. Personal use of this material is permitted. Permission \nfrom IEEE must be obtained for all other uses, in any current or future \nmedia, including reprinting/republishing this material for advertising \nor promotional purposes, creating new collective works, for resale or \nredistribution to servers or lists, or reuse of any copyrighted \ncomponent of this work in other works. \n \n  \nA new LLMS Algorithm for Antenna Array \nBeamforming \n \nJalal Abdulsayed SRAR, Kah-Seng CHUNG and Ali MANSOUR \n Dept. of Electrical and Computer Engineering, Curtin University of Technology, Perth, Australia \njalal.srar@google.com, k.chung@curtin.edu.au, and a.mansour@curtin.edu.au \nAbstract- A new adaptive algorithm, called LLMS, which \nemploys an array image factor, IA,  sandwiched in between two \nLeast Mean Square (LMS) sections , is proposed for different \napplications of array beamforming. The convergence of LLMS \nalgorithm is analyzed, in terms of mean square error, in the \npresence of Additive White Gaussian Noise (AWGN) for two \ndifferent modes of operation; namely with either an external \nreference or self-referencing. Unlike earlier LMS based \nschemes, which make use of step size adaptation to enhance \ntheir performance, the proposed algorithm derives its overall \nerror signal by feeding back the error signal from the second \nLMS stage to combine with that of the first LMS section. This \nresults in LLMS being less sensitive to variations in input \nsignal-to-noise ratio as well as the step sizes used.  Computer \nsimulation results show that the proposed LLMS algorithm is \nsuperior in convergence performance over the conventional \nLMS algorithm as well some of the more recent LMS based \nalgorithms, such as constrained-stability LMS (CSLMS), and \nModified Robust Variable Step Size LMS (MRVSS) algorithms.  \nAlso, the operation of LLMS remains stable even when its \nreference signal is corrupted by AWGN. It is also shown that \nLLMS performs well in the presence of Rayleigh fading. \nKeywords—Adaptive array beamforming, LLMS algorithm, \nEVM, Rayleigh fading. \nI. INTRODUCTION \nIn recent years, adaptive or smart antennas have become a \nkey component for various wireless applications, such as \nradar, sonar, and 4G cellular mobile communications [1]. Its \nuse leads to an increase in the detection range of radar and \nsonar systems, and the capacity of mobile radio \ncommunication systems. A summary of  beamforming \ntechniques is presented in [2]. \nBecause of its simplicity and robustness, the LMS \nalgorithm has become one of the most popular adaptive \nsignal processing techniques adopted in many applications \nincluding antenna array beamforming. Over the past three \ndecades, several improvements have been proposed to speed \nup the convergence of the LMS algorithm. These include \nvariable-length LMS algorithm [3], Variable Step Size LMS \n(VSSLMS) algorithms [3-7], transform domain algorithms \n[8], and recently CSLMS [9] and MRVSS algorithms [10]. \nBecause of their improved performance over other published \nLMS-based algorithms, both the CSLMS and MRVSS \nalgorithms are included in this paper for performance \ncomparison with the proposed LLMS scheme.  \nAll the previously published LMS-based algorithms \nrequire an accurate reference signal for their proper \noperation. In some cases, several operating parameters are \nalso required to be specified. As a result, the performance of \nsuch algorithm becomes highly dependent on the nature of \nthe input signal [11]. \nThis paper presents a very different approach to achieve \nfast convergence and low error floor with an LMS-based \nalgorithm. The proposed LLMS algorithm involves the use \nof two LMS sections separated by an array image factor,\nIA , \nas shown in Fig. 1. Such an arrangement maintains the low \ncomplexity generally associated with LMS. It can be shown \nthat an N-element antenna array employing the LLMS \nalgorithm involves 4N+1 complex multiplications and 2N \ncomplex additions, i.e., slightly more than double the \ncomputational requirements of a conventional LMS scheme. \nWith the proposed LLMS scheme, the intermediate \noutput, \n1LMSy , yielded from the first LMS section, (LMS1), is \nmultiplied by the image array factor () IA of the desired \nsignal. The resultant “filtered” signal is further processed by \nthe second LMS section (LMS 2). For the adaptation process, \nthe error signal of LMS 2, 2e , is fed back to combine with \nthat of LMS 1, to form the overall error signal, LLMSe , for \nupdating the tap weights of LMS 1. As shown in Fig. 1, a \ncommon external reference signal is used for both the two \nLMS sections, i.e., 1d  and 2d . This mode of operation, \ntermed normal referencing, is analyzed in Section IIA.  \nMoreover, the external reference signal may be replaced by\n1LMSy  in place of  2d , and LLMSy  for 1d  to produce a self-\nreferenced version of the LLMS scheme, as described in \nSection II B. \nThe rest of the paper is organized as follows. In section II, \nthe convergence of LLMS is analyzed in the presence of an \nexternal reference signal. This is then followed by an \nanalysis involving the use of the estimated outputs, \n1LMSy and\nLLMSy in place of the external reference. The latter is referred \nto as self-referencing from hereon. Results obtained from \ncomputer simulations for an eight element array are \npresented in Sections III. Finally, Section IV concludes the \npaper. \nII.\n CONVERGENCE OF THE PROPOSED LLMS  ALGORITHM \nThe convergence of the proposed LLMS algorithm has \nbeen analyzed with the following assumptions: \n(i) The propagation environment is stationary. \n(ii) The components of the signal vector ( ) 1 jX should be \nindependent identically distributed (iid). \nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the WCNC 2010 proceedings.\n978-1-4244-6398-5/10/$26.00 ©2010 IEEE\n \n(iii) All signals are zero mean and stationary at least to the \nsecond order. \nA. Analysis with an external reference  \nFrom Fig. 1, the error signal for updating LMS 1 at the jth \niteration is given by \n12() () ( 1 )LLMSej e j e j =− −                            (1) \nwith   () () () () H\nii i iej dj j j=− WX  \n   where 121 for LMS and 2 for LMS ;i =  () i ⋅X  and ( )i ⋅W  \nrepresent the input signal and weight vectors, respectively of \nthe ith LMS section.  . ሺ·ሻு denotes the Hermitian matrix of \nሺ·ሻ.   \n \nThe input signal of LMS 2 is derived from the LMS 1, such \nthat \n21 1() () () () H\nLMS 1j yj jj== IIXA A W X  \nThe convergence performance of the LLMS algorithm can \nbe analyzed in terms of the expected value of 2\nLLMSe . It can \nbe shown that: \n    \n2 2\n12() () () ( 1 )LLMSjE e j E e j e jξ ⎡⎤ ⎡⎤=− −⎢⎥ ⎣⎦⎣⎦\u0016  \n \n2\n11\n11\n() () () ()\n  () () () () () ()\nH\nHH\n11\nED j j j j\nE Dj j j D j j j ∗\n⎡⎤=+ ⎣⎦\n⎡⎤−+ ⎣⎦\nWQ W\nXW WX\n   (2) \n  where • signifies modulus; 12() () ( 1 )Dj d j e j=− − , and \nQ is the correlation matrix of the input signals given by \n() () () H\n11j Ej j⎡⎤= ⎣⎦QX X                        (3) \nConsider the first term on the RHS of (2). With 1 ()dj  and \n2 (1 )ej −  being zero mean and uncorrelated based on the \nassumptions (ii) and (iii), this gives \n22 2\n12() () ( 1 )ED j Edj Eej⎡⎤ ⎡⎤ ⎡ ⎤=+ −⎣⎦ ⎣⎦ ⎣ ⎦          (4) \nExpanding (4) to include (1), and by assuming \n21() ()dj dj = , the first term on the RHS of (2) becomes \n22 2\n12() () ( 1 )\n    (1 ) (1 ) (1 ) (1 )\n     (1 ) (1 ) (1 )\nHH\nLLMS LLMS\nH\nLLMS LLMS\nED j Edj Ed j\njj j j\njj j\n⎡⎤ ⎡⎤ ⎡ ⎤=+ −⎣⎦ ⎣⎦ ⎣ ⎦\n−− − − − −\n+− − −\nWZ Z W\nWQ W\n   (5) \nwhere 21\nHH H\nLLMS IW= W A W , and ( ) jZ  corresponds to the \ninput signal cross-correlation vector given by  \n2() () () 1j Ej d j ∗⎡ ⎤= ⎣ ⎦ZX                          (6) \nLet 21() ()dj dj = , and by applying the assumptions (ii) \nand (iii), the last RHS term of (2) may be written as \n        \n11\n11\n() () () () () ()\n     () () () ()\nHH\n11\nHH\nE Dj j j D j j j\njj j j\n∗⎡ ⎤+⎣ ⎦\n=+\nXW WX\nZW W Z\n             (7) \nAs a result, the mean square error ξ  as specified by (2) \ncan be rewritten to include the results of (5) and (7) to \nbecome \n22\n12\n1\n11 1\n() () ( 1 )\n (1 ) (1 ) (1 ) ( ) ( )\n (1 ) (1 ) (1 ) (1 )\n  () () () () ()\nHH\nLLMS LLMS\nHH\nLLMS LLMS\nHH\njE d j E d j\njj j j j\njj j j\njj j j j\nξ ⎡⎤ ⎡ ⎤=+ −⎣⎦ ⎣ ⎦\n+− − − −\n−− − − − −\n−+\nWQ W Z W\nWZ Z W\nWZ W Q W\n   (8) \nDifferentiating (8) with respect to the weight vector \n1 ()H jW then yields the gradient vector ( )ξ∇ so that \n1() () () () j jjξ =− +∇ ZQ W                 (9)             \nBy equating ( ) ξ∇  to zero, we obtain the optimal weight \nvector as \n1\n1 () () ()opt jj j −=WQ Z                          (10) \n   when 1X  is well excited and Q  could be considered as \nfull rank matrix. \nThis represents the Wiener-Hopf equation in matrix form. \nTherefore, the minimum MSE can be obtained from (10) and \n(8) to give  \n{}\n22\nmin 1 2\n1\n2\n() ( 1 )\n() () ( 1 ) ( 1 )\n( 1) ( 1) 1 ( 1)\nHH\nopt LLMS\nHH\nLLMS\nEdj Ed j\njj j j\njj j\nξ ⎡⎤ ⎡ ⎤=+ −⎣⎦ ⎣ ⎦\n−− − −\n+− − − + − I\nZW Z W\nWZ A W\n   (11) \nd\ndθ\niθ\n1LMSy\n1,1x\n1,2x\n1,Nx\nLLMSe 1e\n1τ =\n1W\n2W\nNW\n1x′\n2x′\nNx′\n2,1x\n2,2x\n2,Nx\n•\n•\n•\ndS\niS\nLLMSy\n1d 2d2e\n1W\n2W\nNW\n1LMS Processing\n1LMS 2LMS\n2LMS Processing\n1X\n2X\nIA\n \nFigure1. The proposed LLMS algorithm with an external reference signal \n= ∆ \nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the WCNC 2010 proceedings.\n \nBased on (10) and (11), (8) becomes \n() ()min 1 1 1 1\nH\nopt optξξ=+− − WW Q WW            (12) \nThe error values of (12) are plotted as the theoretical \ncurve in Fig. 2b.  \nNow, define \n()11 1 opt−\u0016VW W                             (13) \nso that (12) can be written as \nmin\nH\n11ξξ=+ VQ V                             (14) \nDifferentiating (14) with respect to H\n1V will yield another \nform for the gradient, such that \n () 1ξ =∇ QV                                 (15) \nUsing eigenvalue decomposition (EVD) of Q  in (15) \nyields \n1\n1\nH\n11 11\n−\n1==Qq q q qΛΛ                        (16) \nwhere 1Λ is the diagonal matrix of eigenvalues of Q for an \nN element array, i.e., \n11 2 [, , , ] Ndiag E E E= \"Λ                      (17)  \n   where [...]diag is the diagonal of Q . \nFor steepest descent algorithms, the weight vector is \nupdated according to \n11 1(1 ) ( ) ( )j jj μξ+= − ∇WW                  (18) \nwhere 1μ is the convergence constant that controls the \nstability and the rate of adaptation of the weight vector, and \n()jξ∇  is the gradient at the jth iteration. \nWe may rewrite (18) in the form of a linear homogeneous \nvector difference equation using (13), (15) and (16) to give \n11 1 1 1(1 ) ( ) ( )j jj μ+= −VV Q V                 (19) \nAlternatively, (19) can be written as \n()11 1() ( 0 )\nj H\n11j μ 1=−Vq I q V Λ                 (20) \nBy substituting (20) in (14), the MSE at the jth iteration is \ngiven by \n()\n2\nmin 1 1 1() ( 0 ) ( 0 )\njHH\n11jξξ μ 1=+ − Vq I q V Λ     (21) \nFrom (21), the asymptotic value of ξ  becomes  \nminlim ( )\nj\njξξ\n→∞\n=                               (22)    \nB. Analysis of the self-referencing scheme \nNext, consider the case when the external reference is \nbeing replaced by internally generated signals, such that \n1 () ( 1 ) LLMSdj y j =− ,   and 21() () LMSdj y j =            (23)    \nAs a result of these changes, and note that the error signal\n22 LLMSedy=− , we can redefine ( )D j in (2) as \n1() 2 ( 1 ) ( 1 )LLMS LMSDj y j y j=− − −                (24) \nBased on the definition of (24), we reanalyze the MSE as \ndefined in (2) to yield \n2\n1\n11 1\n() () () ()\n() () () () ()\nH\nHH\njE D j j j\nj jj j j\nξ ⎡⎤ ′=− ⎣⎦\n′−+\nZW\nWZ WQ W\n           (25) \n   where ( ) j′Z  corresponds to the input signal cross-\ncorrelation vector given by \n() () () 1j Ej D j ∗′ ⎡ ⎤= ⎣ ⎦ZX                       (26) \nThe error values obtained from (25) are plotted as the \ntheoretical curve in Fig. 3.  \nBy following the same analyzing steps of (12) to (22), it \ncan be shown that the proposed LLMS algorithm will \nconverge under the condition of self-referencing. \nIII. SIMULATION RESULTS \nThe performance of LLMS algorithm has been studied by \nmeans of MATLAB simulation. For comparison purposes, \nresults obtained with the conventional LMS, CSLMS and \nMRVSS algorithms are also presented. For the simulations, \nthe following parameters are used: \n• A linear array consisting of 8 isotropic elements. \n• A Binary Phase Shift Keying (BPSK) signal arriving at \nan angle of 0D , or if specified at 20− D .  \n• An AWGN channel with a BPSK interference signal \narriving at 45iθ = D having the same amplitude as the \ndesired signal unless otherwise stated.  \n• All weight vectors are initially set to zero. \n• For the Rayleigh fading case, 60 Hzdf = . The input \nsignal at each array element undergoes independent \nRayleigh fading.  \nTable 1 tabulates the values of the various constants \nadopted by the simulations for the four different adaptive \nalgorithms. The constants for MRVSS are taken from [5].  \nFor a digital modulated signal, it is also convenient to \nmake use of the Error Vector Magnitude (EVM) as an \naccurate measure of any distortion introduced by the \nadaptive scheme on the received signal at a given signal-to-\nnoise ratio (\nSNR). EVM is defined as [12] \n2\n1\n1 () ()\nK\nRMS r t\njo\nEVM S j S jKP =\n=− ∑                  (27) \nwhere K  is the number of symbols used, ( ) rSj is the \nnormalized thj output of the beamformer, and ( ) tSj is the\nthj transmit symbol. oP  is the normalized transmit symbol \npower.  \nTABLE I.  VALUES OF THE CONSTANTS UESD IN SIMULATION \nAlgorithm Normal Channel \nLMS 0.05μ =  \nLLMS 12 0.05μμ==  \nCSLMS 0.05ε =  \nMRVSS \nmax min max min\nmax\n1, 0, 5 4, 0.2, 1 4\n, 0.97, 4.8 4, 0.97\nee\ne\nβ β υμμ\nμμ α γ η\n== = − = = −\n== = − =\n= ∆ \nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the WCNC 2010 proceedings.\n \nA. Performance with an external reference \nFirst, the performances of the LLMS, CSLMS, MRVSS \nand LMS schemes have been studied in the presence of an \nexternal reference signal. The convergence performances of \nthese schemes are compared based on the ensemble average \nsquared error\n()\n2e\u0004 obtained from 100 individual simulation \nruns. The results obtained for different values of input SNR , \nand step size, 1μ  and 2μ , are presented. \nFigs. 2a – 2c show the convergence behaviors of the four  \nadaptive schemes for SNR values of 5, 10, and 15 dB, \nrespectively. For the proposed LLMS scheme, the theoretical \nconvergence error calculated using (12) for SNR=10 dB is \nalso shown in Fig. 2b. It verifies a close agreement between \nthe simulated and theoretical error plots for LLMS. Also, it \nis observed that under the given conditions, LLMS algorithm \nconverges much faster than the other three schemes. \nFurthermore, the error floor of LLMS is less sensitive to \nchanges in input SNR.  \n(a) 5dBSNR =  (b) 10dBSNR =  \n \n(c) 15dBSNR =  \nFig. 2. The convergence of LLMS, CSLMS, MRVSS and LMS using the \nparameters given in Table I, for three different values of input SNR .  \nB. Performance with self-referencing \nAs shown in Fig. 2, the LLMS algorithm converges \nrapidly within ten iterations. Once this occurs, the \nintermediate output, 1LMSy , tends to resemble the desired \nsignal ( )ds t , and may then be used in place of the external \nreference 2d  for the current iteration of the LMS 2 section. \nAs the LMS2 section converges, its output LLMSy becomes the \nestimated ( )ds t . As a result, LLMSy may be used to replace 1d\nas the reference for the LMS 1 section. This feedforward and \nfeedback arrangement enables the provision of self-\nreferencing in LLMS, and allows the external reference \nsignal to be discontinued after an initial four iterations.  \nThe ability of the LLMS algorithm to maintain operation \nwith the internally generated reference signals is \ndemonstrated in Fig. 3. It shows that the LMS, CSLMS, \nMRVSS algorithms are unable to converge without the use \nof an external reference signal. For comparison, the \ntheoretical convergence error is also plotted in Fig. 3. \nC. Performance with a noisy reference signal \nThe performances of LLMS, CSLMS, MRVSS and LMS \nhave also been investigated when the reference signal used is \ncorrupted by AWGN. Fig. 4 shows the ensemble average of \nthe mean square error, ξ , obtained from 100 individual \nsimulation runs, as a function of the ratio of the rms noise \nlevel σ  to the amplitude of the reference signal RS .  \n \nFig. 3. The convergence of LLMS with self-referencing using the \nparameters given in Table I, for 10SNR dB= . An external reference is used \nfor the initial four iterations. \nIt is interesting to note that the LLMS algorithm is very \ntolerant to noisy reference signal. On the other hand, the \nLMS, CSLMS and MRVSS algorithms are quite sensitive to \nthe presence of noise in the reference signal. As shown in \nFig. 4, the values of \nξ  associated with LLMS, remains very \nsmall even when the rms noise level becomes as large as the \nreference signal.  \n \nFig. 4. The influence of noise in the reference signal on the mean square  \nerror ξ  for an input SNR is 10 dB . The parameter values used are as given \nin Table I.  \nNext, the beam patterns obtained with the four adaptive \nalgorithms for three different values of rms noise-to-\nreference signal ratio (σ/SR) are shown in Figs. 5a −c. In this \ncase, the input SNR is 10 dB and the angle of arrival of the \ndesired signal is −20o. Figs. 5a and b, show that the resultant \nbeam patterns of LLMS remain almost the same as that \nachieved with a noise free reference (Fig. 5c). On the \ncontrary, the gains of the other three beamformers are \nreduced when their reference signals are corrupted by \nAWGN. \nD. LLMS operating in a flat fading channel \nThe influence of Rayleigh fading on the performances of \nthe four adaptive beamforming schemes has been \ninvestigated under the following conditions: \n• The desired signal arrives at an angle of 0o. \n• Two interfering signals with the same amplitude as the \ndesired signal arrive at -30o and 45o, respectively. \n0 10 20 30 40 50 60 70 80 90 1000\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n0.045\nIterations\nEnsemble Average Mean Squared Error  \n \nCSLMS\nMRVSS\nLLMS\nLMS\n0 10 20 30 40 50 60 70 80 90 1000\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n0.045\nIterations\nEnsemble Average Mean Squared Error \n \n \nCSLMS\nMRVSS\nLLMS\nTheoritical\nLMS\n0 10 20 30 40 50 60 70 80 90 1000\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n0.045\nIterations\nEnsemble Average Mean Squared Error \n \n \nCSLMS\nMRVSS\nLLMS\nLMS\n0 10 20 30 40 50 60 70 80 90 1000\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nIterations\nEnsemble Average Mean Squared Error \n \n \n5 10 15 20 25 300\n0.5\n1\n1.5\n2 x 10\n-3\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\nTheoritical\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nRatio of rms noise to reference signal level\nEnsemble Average Squared Error \n \n \nLMS\nCSLMS\nMRVSS\nLLMS\nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the WCNC 2010 proceedings.\n \n• The input signal at each antenna element undergoes \nindependent Rayleigh fading. \n• Each simulation run lasts for 16 M samples. \nFigs. 6 a and b, show the resultant EVM values obtained \nusing the four different algorithms. It is observed that LLMS \ncan achieve lower EVM values, which indicate better signal \nfidelity, particularly at lower input SNR values. This \nsuggests that LLMS is better able to handle time varying \nsignals compared with conventional LMS, CSLMS and \nMRVSS.  \n(a) Fading with Interference (b) Fading without Interference \nFig. 6. The EVM values obtained with the LLMS, CSLMS, MRVSS and \nLMS algorithms for different input SNR under Rayleigh fading channel. \nIV. CONCLUSIONS \nA new adaptive algorithm for beamforming, called LLMS, \nis presented and analyzed. The convergence of LLMS \nscheme has been analyzed assuming the use of an external \nreference signal. This is then extended to cover the case that \nmakes use of self-referencing. The convergence behaviors of \nthe LLMS algorithm with different step size combinations of \n1μ  and 2μ  have been demonstrated by means of Matlab \nsimulations under different input SNR conditions.  \nIt is shown that the proposed LLMS algorithm can \nachieve rapid convergence, typically within a few iterations. \nFurthermore, the steady state MSE of LLMS is quite \ninsensitive to input SNR. Unlike the conventional LMS, \nCSLMS and MRVSS algorithms, the proposed LLMS \nscheme is able to operate with noisy reference signal. Once \nthe initial convergence is achieved, within a few iterations, \nthe LLMS scheme can maintain its operation through self-\nreferencing. In addition, it is shown that the use of LLMS \ncan maintain the fidelity of the signal in the presence of \nRayleigh fading, as indicated by the resultant low EVM \nvalues. Overall, LLMS performs better than conventional \nLMS, CSLMS and MRVSS algorithms under the various \noperating conditions considered in this paper. This has been \nachieved with a complexity slightly more than twice that of \na conventional LMS scheme.  \nR\nEFERENCES \n \n \n \n \n \n \n \n \n0 5 10 15 20 25 300\n10\n20\n30\n40\nSNR (dB)\nEVM (%)\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\n0 5 10 15 20 25 300\n5\n10\n15\n20\n25\n30\n35\nSNR (dB)\nEVM (%)\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\n \n(a) 12NR Sσ =  \n \n(b) 12 2NR Sσ =  \n \n(c) 0NR Sσ =  \nFig. 5. The beams patterns achieved with the LLMS, CSLMS, MRVSS and \nLMS algorithms when the reference is corrupted by AWGN.  The angle of \narrival of the desired signal is −200. \n  0.2\n  0.4\n  0.6\n  0.8\n  1\n-30\n-60\n-90\n90\n60\n30\n0\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\n  0.2\n  0.4\n  0.6\n  0.8\n  1\n-30\n-60\n-90\n90\n60\n30\n0\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\n  0.2\n  0.4\n  0.6\n  0.8\n  1\n-30\n-60\n-90\n90\n60\n30\n0\n \n \nLMS\nCSLMS\nMRVSS\nLLMS\n1. Martin-Sacristan, D., et al., \" On the Way Toward Forth-  \nGeneration Mobile: 3GPP LTE-Advanced\",  EURASIP \nJournal on Wireless Communications and Networking, \nvol. 2009, Jun. 2009. \n2. Stine, J.A., \" Exploiting smart antennas in wireless mesh \nnetworks using contention access\",  IEEE Trans. on \nWireless Communications, vol. 13(2), pp. 38-49, 2006. \n3. Nascimento, V.H., \" Improving the initial convergence of \nadaptive filters: variable-length LMS algorithms\" , 14th \nInternational Conference on Digital Signal Processing,  \nSantorini, Greece, pp. 667-670, July 2002. \n4. Zhao, S., Z. Man, and S. Khoo, \" A Fast Variable Step-\nSize LMS Algorithm with System Identification\" , 2nd \nIEEE Conference on Industrial Electronics and \nApplications, Harbin, China, pp. 2340-2345, May 2007. \n5. Kwong, R.H. and E.W. Johnston, \" A variable step size \nLMS algorithm\", IEEE Trans. on Signal Processing, vol. \n40(7), pp. 1633-1642, 1992. \n6. Aboulnasr, T. and K. Mayyas, \" A robust variable step-\nsize LMS-type algorithm: analysis and simulations\",  \nIEEE Trans. on Signal Processing, vol. 45(3), pp. 631-\n639, 1997. \n7. Wee-Peng, A. and B. Farhang-Boroujeny, \" A new class \nof gradient adaptive step-size LMS algorithms\",  IEEE \nTrans. on Signal Processing, vol. 49(4), pp. 805-810, \n2001. \n8. Lobato, E.M., O.J. Tobias, and R. Seara, \" Stochastic \nmodeling of the transform-domain εLMS algorithm for \ncorrelated Gaussian data\",  IEEE Trans. on Signal \nProcessing, vol. 56(5), pp. 1840-1852, 2008. \n9. Górriz, J.M., et al., \" Speech enhancement in \ndiscontinuous transmission systems using the \nconstrained-stability least-mean-squares algorithm\",\n \nAcoustical Society of America, vol. 124(6), pp. 3669-\n3683, Dec. 2008. \n10. Zou, k. and X. Zhao. \"A new modified robust variable \nstep size LMS algorithm\", 4th IEEE Conference on \nIndustrial Electronics and Applications, Xi'an, China, \nMay 2009. \n11. Mathews, V.J. and Z. Xie, \"\nA stochastic gradient \nadaptive filter with gradient adaptive step size\",  IEEE \nTrans. on Signal Processing, vol. 41(6), pp. 2075-2087, \n1993. \n12. Arslan, H. and H. Mahmoud, \"Error vector magnitude to \nSNR conversion for nondata-aided receivers\",  IEEE \nTrans. on Wireless Communications, vol. 8(5), pp. 2694-\n2704, 2009. \n \n \nThis full text paper was peer reviewed at the direction of IEEE Communications Society subject matter experts for publication in the WCNC 2010 proceedings.",
  "topic": "Additive white Gaussian noise",
  "concepts": [
    {
      "name": "Additive white Gaussian noise",
      "score": 0.7041594982147217
    },
    {
      "name": "Least mean squares filter",
      "score": 0.6891403198242188
    },
    {
      "name": "Rayleigh fading",
      "score": 0.6571614146232605
    },
    {
      "name": "Algorithm",
      "score": 0.6291500329971313
    },
    {
      "name": "Computer science",
      "score": 0.5346614122390747
    },
    {
      "name": "Convergence (economics)",
      "score": 0.5323883295059204
    },
    {
      "name": "Beamforming",
      "score": 0.4836815297603607
    },
    {
      "name": "Adaptive beamformer",
      "score": 0.4408728778362274
    },
    {
      "name": "Signal-to-noise ratio (imaging)",
      "score": 0.4373297095298767
    },
    {
      "name": "Adaptive filter",
      "score": 0.34878507256507874
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.2802286446094513
    },
    {
      "name": "Decoding methods",
      "score": 0.24391618371009827
    },
    {
      "name": "Telecommunications",
      "score": 0.2366628348827362
    },
    {
      "name": "Fading",
      "score": 0.17051905393600464
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205640436",
      "name": "Curtin University",
      "country": "AU"
    }
  ],
  "cited_by": 5
}