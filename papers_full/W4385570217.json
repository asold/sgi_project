{
    "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
    "url": "https://openalex.org/W4385570217",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3134284595",
            "name": "Masoud Monajatipoor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2915107679",
            "name": "Liunian Harold Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3005482398",
            "name": "Mozhdeh Rouhsedaghat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099042467",
            "name": "Lin Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2208999240",
            "name": "Kai-Wei Chang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4296001058",
        "https://openalex.org/W2143104527",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2947312908",
        "https://openalex.org/W4226352076",
        "https://openalex.org/W4320855762",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4385573483",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W2604763608",
        "https://openalex.org/W2624871570",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W2963518342"
    ],
    "abstract": "Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang, Kai-Wei Chang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 495–508\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMetaVL: Transferring In-Context Learning Ability\nFrom Language Models to Vision-Language Models\nMasoud Monajatipoor\nUCLA\nmonajati@ucla.edu\nLiunian Harold Li ∗\nUCLA\nliunian.harold.li@cs.ucla.edu\nMozhdeh Rouhsedaghat *\nUSC\nrouhseda@usc.edu\nLin F. Yang\nUCLA\nlinyang@ee.ucla.edu\nKai-Wei Chang\nUCLA\nkwchang@cs.ucla.edu\nAbstract\nLarge-scale language models have shown the\nability to adapt to a new task via conditioning\non a few demonstrations (i.e., in-context learn-\ning). Large-scale language models have shown\nthe ability to adapt to a new task via condition-\ning on a few demonstrations (i.e., in-context\nlearning). However, in the vision-language\ndomain, most large-scale pre-trained vision-\nlanguage (VL) models do not possess the abil-\nity to conduct in-context learning. How can\nwe enable in-context learning for VL models?\nIn this paper, we study an interesting hypoth-\nesis: can we transfer the in-context learning\nability from the language domain to the VL\ndomain? Specifically, we first meta-trains a lan-\nguage model to perform in-context learning on\nNLP tasks (as in MetaICL); then we transfer\nthis model to perform VL tasks by attaching a\nvisual encoder. Our experiments suggest that\nindeed in-context learning ability can be trans-\nferred cross modalities: our model considerably\nimproves the in-context learning capability on\nVL tasks and can even compensate for the size\nof the model significantly. On VQA, OK-VQA,\nand GQA, our method could outperform the\nbaseline model while having ∼20 times fewer\nparameters.\n1 Introduction\nPre-trained language models have shown impres-\nsive performance on a range of tasks by learn-\ning from large-scale text corpus (Radford et al.,\n2018, 2019; Yang et al., 2019). Recent studies\nfind that some of these language models can be\nused to perform in-context learning out-of-the-box,\ni.e., adapting to a task by conditioning on a few\ndemonstrations in context without any gradient up-\ndate (Brown et al., 2020; Min et al., 2022), which\nis highly desirable.\nIn VL modeling, in-context learning is less ex-\nplored and only a handful of models are proposed\n∗equal contribution\nto perform in-context learning mainly by limit-\ning the amount of deviation of a pretrained large-\nscale language model from the language space and\ntranslating visual inputs to language embedding\nspace. They either require a large capacity (Tsim-\npoukelli et al., 2021; Alayrac et al., 2022) or a giant\ncorpus consisting of in-context learning examples\n(Alayrac et al., 2022; Liu et al., 2023; Koh et al.,\n2023).\nIn this work, we explore whether we could en-\nable in-context learning in VL tasks without resort-\ning to extreme scale-up. We study an interesting\nhypothesis: can we transfer the in-context learn-\ning ability from the language domain to the VL\ndomain? To elaborate, not every language model\nexhibits excellent in-context learning ability; recent\nstudies (Min et al., 2022) show that one could ex-\nplicitly train language models to perform in-context\nlearning, by training the model on multiple tasks\nwith in-context few-shot examples, a process that\nresembles meta-learning. Thus, an intriguing query\narises: when a language model is first meta-trained\nto perform in-context learning, can it be transferred\nto perform in-context learning for VL tasks better?\nA remarkable observation in our study is the uti-\nlization of a meta-trained language model as the\ntransformer encoder-decoder and the mapping of\nvisual features to the language embedding space.\nThis innovative approach led to the development\nof our proposed VL model (we name it MetaVL).\nImpressively, our experimental results demonstrate\nthat MetaVL surpasses the baseline model’s perfor-\nmance, even when MetaVL is designed to be 20\ntimes smaller in size.\nThis study makes three main contributions: 1)\nTo the best of our knowledge, this is the first at-\ntempt to transfer the meta-learning knowledge for\nin-context learning from single-modality to multi-\nmodality. 2) We propose a VL model, MetaVL 1,\nwhich outperforms the baseline in in-context learn-\n1https://github.com/masoud-monajati/MetaVL\n495\ning while having a much smaller model size. 3)\nThrough extensive experiments on VQA, GQA and\nOK-VQA, we demonstrate the in-context learning\ncapability of MetaVL and analyze its components.\n2 Related work\nIn-context learning in VL. Frozen (Tsim-\npoukelli et al., 2021) is the first attempt for in-\ncontext learning in multimodality by leveraging a\nfrozen GPT-like language model as the language\nbackbone and mapping visual features to the lan-\nguage embedding space. Frozen sheds light on the\nfeasibility of benefiting from the frozen LMs in VL\nmodeling to learn a new task from a few examples\nin context. MAGMA (Eichenberg et al., 2021) is\nanother encoder-decoder architecture for VL pre-\ntraining which showed that adding adaptor blocks\nbetween the frozen language model layers could\nfurther improve the performance for VL tasks in a\nfew-shot scenario.\nOther recent works (Yang et al., 2022; Alayrac\net al., 2022; Zeng et al., 2022) follow the similar\nprinciple as the previous works to tackle in-context\nlearning in VL modeling and achieve superior re-\nsults by leveraging extremely large-scale models.\nIn this paper, we study a problem overlooked\nin prior work: we delve into the possibility of en-\nabling in-context learning for VL tasks without re-\nlying on extensive scalability. Our focus lies in ex-\nploring the hypothesis: Is it feasible to transfer the\nin-context learning capability from the language\ndomain to the VL domain?\nMeta-learning in language modeling Large-\nscale language models have shown the capability to\nbe trained on a new task if properly prompted with\nin-context examples, i.e., in-context learning. In\nthis learning strategy, the language model is asked\nto generate the desired output, e.g., an answer in\nthe question-answering task, which is prompted\nby a few data examples along with their corre-\nsponding supervision sampled from the training\nsplit, and the language model learns the task in\ncontext without performing any gradient updates.\nAlthough such training is highly data-efficient, its\nperformance is far behind supervised fine-tuning.\nTherefore, inspired by (Vilalta and Drissi, 2002; Ev-\ngeniou and Pontil, 2004; Finn et al., 2017; Ruder,\n2017), MetaICL (Min et al., 2022) proposes train-\ning the model for in-context learning as a kind of\nmeta-learning. MetaICL meta-trained a gpt lan-\nguage model on a diverse set of natural language\n…\nLanguage Encoder\nVisual encoder\nMLP\nLanguage Decoder\n… …\n…\n<s>\nThe little girl poses with her\nLanguage Encoder\nLanguage Decoder\nyk+1\nx1 y1 x2 y2 xk yk…\n xk+1\nA collection of \nlanguage meta \ntraining tasks\nIteratively \nsampling a \nminibatch \nfrom a \nrandom \ndataset\n</s>…\nv1 v2 vn t1 t2 t3 t4 t5 t6 tm\nFigure 1: The training steps of MetaVL including meta-\ntraining the language encoder-decoder (above) and map-\nping the visual features into the language embedding\nspace while keeping the meta-trained language encoder-\ndecoder frozen (below).\ntasks and datasets and showed that meta-training a\nlanguage model in an in-context learning manner\ncould significantly improve the in-context learning\ncapability of the language model for a new task.\n3 Approach\nIn this section, we first explain the existing meta-\ntraining procedure for language modeling and then\nintroduce our proposed method for in-context learn-\ning in VL.\nMeta-training in language modeling. MetaICL\nhas shown that a language model that is meta-\ntrained on a diverse set of tasks in an in-context\nlearning setup is a strong few-shot learner. To meta-\ntrain an auto-regressive language model, in each\niteration, a meta-learning task is randomly cho-\nsen from a collection of diverse meta-training lan-\nguage tasks, and k + 1data-label examples are\nrandomly sampled from its training split. Then,\nthe model is supervised by the concatenation of\n(x1,y1,x2,y2,...,x k+1) which will be fed as a\nsingle input to the model for predicting the la-\nbel (yk+1) as the training objective, i.e., the meta-\ntraining step aims to maximize:\nP(yk+1|x1,y1,···,xk,yk,xk+1) (1)\nDuring inference, the same in-context setup (k\nexamples from the training) are sampled from a\n496\ntarget dataset to be used as the (x1,y1)(x2,y2) ·\n··,(xk,yk)(x) and given to the model to predict the\nlabel y.\nThe meta-trained language model trained on a\ndiverse set of natural language datasets has shown\ngood performance for an unseen task when few\ndata are given in context (Min et al., 2022).\nMetaVL - a VL method with meta-learning\nknowledge for in-context learning. MetaVL has\nthree main submodels including a meta-trained\nencoder-decoder and is being trained using Pre-\nfix Language Modeling (PrefixLM) (Wang et al.,\n2021). In the following, we discuss each submodel\nin detail.\nVisual encoder and visual prefix. The visual\nencoder is defined as a function Ve(x) that takes an\nimage of x and outputs visual features. We extract\nthe feature grid before the pooling layer n×Dv\nwhere nis the number of feature maps and Dv is\nthe feature size of the visual encoder. Then, the\noutput features can be viewed as a sequence of n\nvisual tokens representing the image.\nThe visual encoder is followed by the visual pre-\nfix module that is defined as Vp(x) ∈Dv ×Dl\nwhich maps the visual features to language embed-\nding space. This module is seeking to properly\nproject the visual tokens into language tokens.\nDuring the VL training, the parameters of both\nof these modules are trainable and are learned with\ndifferent learning rates by back-propagation guided\nby the frozen language model.\nLanguage encoder-decoder The meta-trained\nlanguage encoder-decoder is used as the LM back-\nbone and is frozen during the VL training pro-\ncess so the meta-trained language model preserves\nits few-shot capabilities. The language encoder\nencodes the text into text tokens represented by\nt1,t2,...,t m. Then, given the multimodal tokens\n(image and text) as U = v1,v2,...,v n,t1,t2,...,t m\nthe decoder is trained to reconstruct the correspond-\ning text with a standard language modeling objec-\ntive to maximize the following likelihood:\nL(U) =\nm∑\ni=1\nlog P(ti|v1,...,v n,t1,...ti−1; θ) (2)\nAfter the VL training, for learning a new VL task\nin-context, given a few examples from a new task\nwith a new format, we concatenate k sampled data-\nlabel pairs from the training split along with one\ndata from the val/test split to construct the prompt\nand feed it to the model for predicting the desired\noutput. The entire process is visualized in Fig. 1.\n4 Experiments\n4.1 Datasets and Baseline\nWe use the dataset proposed in (Min et al., 2022)\nas the meta-training dataset for the language model\nand the COCO dataset (Lin et al., 2014) as the VL\ntraining dataset for MetaVL. The evaluation exper-\niments are conducted on three datasets including\nVQA (Antol et al., 2015), OK-VQA (Marino et al.,\n2019), and GQA (Hudson and Manning, 2019).\nFrozen leveraged an internal GPT-like language\nmodel with 7 billion parameters as the backbone\nof their proposed model. As their model is not\npublicly available, we trained Frozen with GPT2-\nMedium as the frozen language model and consider\nit as our main baseline (FrozenA) due to its model\nsize. We also train a frozen with GPT-J 6B (The\nmost similar GPT to Frozen) language model and\nobtained a close performance to the original Frozen\nmodel and use it as our second baseline denoted by\nFrozenB.\n4.2 Training and evaluation setting\nInitially, We meta-train a GPT2-Medium LM on a\ncollection of 142 meta-training language datasets\nwith a learning rate of 1e-5 and a batch size of 8\nusing the setting named as “HR→LR with instruc-\ntions (all)” where datasets with equal or greater\nthan 10,000 training examples are used as meta-\ntraining tasks and the rest of the datasets are used\nas target tasks. The training is done on 8 NVIDIA\nRTX A6000 for 80,000 steps which took∼6 hours.\nThen, we train MetaVL on the training split of\nCOCO where we use a learning rate of 5e-5 and\n2e-6 for the visual prefix and visual encoder, re-\nspectively, while the rest of the model parameters\nare frozen. We use a batch size of 32 and trained\nMetaVL using 4 NVIDIA RTX A6000 for 8 epochs\nwhich take ∼48 hours. Inference time depends on\nthe numebr of shots varies from 2-5 hours for 0-3\nshots on 5000 test examples. Our visual encoder is\nCLIP-RN50x16 (Radford et al., 2021) with a fea-\nture grid size of 144 ×3072 and our visual prefix\nis an MLP layer with a dimension of 3072 ×768.\nFor in-context evaluation on VQA datasets, we\nrandomly pick a specific number -n- of sampled\ndata-label pairs, known as shots, from the training\nset and feed them to the model in-context followed\nby a single data from the val/test set. Fig. 2 pro-\n497\nvides some illustrative examples for the evaluation\nprocess.\nAnswer: concrete Answer: yes Answer: texting\nQuestion: What color is \nthe girl's hair?\nAnswer:\nSupport set Meta-test set\nBrown.\nModel Completion\nQuestion: What are the \nmotorcycles sitting on? \nQuestion: Is \nthis in the wild? \nQuestion: Is this in the wild? \nGT: brown\nAnswer: left Answer: car Answer: texting\nQuestion: On which side \nof the photo is the \nwreath?\nAnswer:\nSupport set Meta-test set\nOn the \nleft.\nQuestion: Is the catcher to \nthe right or to the left of the \numpire that is wearing pants?\nQuestion: What is the vehicle \nthat is pulled by the bike that \nis parked in the road?\nQuestion: Is the pillow \nto the left of the \nmirror?\nGT: left\nAnswer: old Answer: honk Answer: denim\nQuestion: This bear is \nnative to what continent?\nAnswer:\nSupport set Meta-test set\nAntarctica.\nQuestion: What is special \nabout the sports items in \nthe case?\nQuestion: Penalty for what? Question: What is this \njacket made of?\nGT: antarctica\na)\nb)\nc)\nModel Completion\nModel Completion\nFigure 2: Qualitative examples of in-context learning\nfrom three datasets: a) VQA, b) OK-VQA, and c) GQA.\nFor each example, there is also a task induction sentence\nof “please answer the question.”.\nTo conduct the evaluation, we utilize a subset\nof 5,000 instances from the val/test dataset due to\ncomputational constraints. The generated output\nfrom the model is then compared against the ex-\npected answer, as established in previous studies.\nIn cases where an exact match is not achieved, we\nemploy a technique to identify the most closely re-\nlated answer from a set of candidate answers (The\nset can be defined as a unique list of all answers in\nthe training dataset). This involves computing the\ncosine similarity between the output’s embedding\nand each candidate answer’s embedding achieved\nby Sentence BERT (Reimers and Gurevych, 2019).\nWe then compare the selected output with the\ncorresponding answer to determine the match. The\ntraining datasets for VQA, OK-VQA, and GQA\ncontain approximately 3,000, 4,200, and 3,000 dis-\ntinct answers, respectively. Furthermore, we per-\nformed an additional round of human evaluation on\nmodel’s output without matching, and the findings\nare summarized in the appendix (Table 2). The\nhuman evaluation on a separate test set of 2000 ex-\namples aimed to delve deeper into instances where\nthe model’s output, while accurate, didn’t precisely\nmatch the provided answer. Three such examples\nare presented in Fig 3, where the initial evaluation\ndid not consider the prediction as correct, but it\nwas deemed correct in the subsequent evaluation\nFrozenA FrozenB MetaVL\nLM size 375M 7B 375M\nAutomatic evaluation\nVQA 18.63 34.07 33.12\nOK-VQA 3.17 11.97 9.60\nGQA 13.86 25.76 31.96\nHuman evaluation\nVQA 16.68 - 35.09\nOK-VQA 6.41 - 19.22\nGQA 19.96 - 38.29\nTable 1: The performance of MetaVL compared with\ntwo baselines on 3-shot in-context learning. We report\nthe performance of our re-implemented Frozen models.\nsetting.\nQuestion: Which famous \nbrothers invented these?\nQuestion: Where can the \nbrand be purchased?\nQuestion: How do you \nscore in this game?\nGT: homerun\nModel completion: \nby hitting a home run\nSelected answer: \nhome run\nGT: store\nModel completion: \nyou can buy it at a store\nSelected answer: \nstore bought\nGT: wright\nModel completion: \nthe wright brothers \nSelected answer: \nwright brother\nFigure 3: Three examples of VQA cases which The\nmodel’s output, although correct, slightly differs from\nthe ground-truth and selected answer from the candidate\nset.\n4.3 Results and analysis\nQuantitative analysis To evaluate MetaVL, we\nconsider three common visual question-answering\ndatasets including VQA, OK-VQA, and GQA. We\ncompare MetaVL results with the mentioned two\nbaselines in Table 1 for 3-shot in-context learning\nbased on both automatic and human evaluation. Ac-\ncording to the results, the performance of Frozen\nimproves as its model size increases while MetaVL\nachieved competitive results in all three tasks. To\nfurther analyze how many image-text pairs are re-\nquired to enable In-context learning for the VL\ntask, we have trained MetaVl with 50 percent of\ntraining data and the results show that the perfor-\nmance slightly dropped but the model preserve its\ncapability to learn from in-context data (Table 3).\nThe effect of the number of in-context shots\nAccording to Figure 4, in almost all settings, the\nperformance of MetaVL is improving by increasing\nthe number of shots which shows the model is gain-\ning knowledge from the data in context. This result\nfurther gives us an illustration of the model’s ca-\n498\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\n0\n2\n4\n6\n8\n10\n12\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\n0\n10\n20\n30\n40\n50\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\n0\n5\n10\n15\n20\n25\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\n0\n10\n20\n30\n40\n50\n0-shot 1-shot 2-shot 3-shot\nFrozen w/ adap Frozen\nMetaVL w/ adap MetaVL\nAutomatic evaluationHuman evaluation\nVQA OK-VQA GQA\nVQA OK-VQA GQA\nFigure 4: Automatic and human evaluation Accuracy of MetaVL and Frozen, w/ and w/o adaptors with 0-3 shots of\nin-context data.\npability to learn from the in-context examples sup-\nporting that MetaVL is benefiting from the meta-\nlearning knowledge for in-context learning. The\nnumbers on the graph are summarized in Table 2\nin the appendix.\nThe effect of having adaptor layers in LM\nMAGMA claims that adding trainable adaptor lay-\ners and letting the LM slightly be trained during\nthe VL training process is beneficial for in-context\nlearning. Compared with Frozen, in addition to\nbeing trained on an x8 larger set of VL datasets,\nMAGMA also includes the training splits of the\ntarget datasets to its training set, while Frozen\nis adapted to an unseen new task in-context (in-\ncontext learning). We evaluated this method by\nadding adaptor layers to both Frozen and MetaVL\nand denoted the corresponding models by Frozen\nw/adap and MetaVL w/adap, respectively, in Fig.\n4. Our results demonstrate that having a fully\nfrozen language model in MetaVL could better pre-\nserve the in-context learning ability of the language\nmodel. It is also noticeable that adding adaptor lay-\ners improves the zero-shot performance of Frozen.\nWe hypothesize that this improvement is due to\ngetting a better vision and language alignment by\nletting both vision and language submodels be in-\nvolved in the alignment process.\nQualitative analysis We provide some qualita-\ntive examples to better illustrate the performance of\nMetaVL for in-context learning in different VQA\ntasks. In Fig. 2, a few examples are provided\nwhich show the output of MetaVL for 3-shot in-\ncontext learning. More examples are presented in\nAppendix.\n5 Conclusion\nWe investigate the feasibility of transferring meta-\nlearning knowledge for in-context learning from\nresource-rich single modality to multimodality. We\nhave shown that by leveraging a meta-trained lan-\nguage model in a VL model, we can transfer the\nability of “learning to learn” in context to VL and\nit results in a strong VL few-shot leaner. With ex-\ntensive experiments on three common VL datasets,\nwe have shown that the in-context learning perfor-\nmance of MetaVL is superior compared with the\nbaseline even when the size of our model is 20\ntimes smaller.\n6 acknowledgment\nThis work was supported by DARPA under agree-\nment HR00112190130 and DARPA MSC program\nunder agreement N660011924032. We would like\nto thank the reviewers for their feedback to improve\nthis research work.\n499\nLimitations\nWhile we have shown the potential of transferring\nin-context learning ability from a language model\nto VL tasks, the experiments in this paper are lim-\nited in two aspects. (1) We considered only the\nVQA task, which is limited in scope. It is unclear\nwhether our method generalizes to other VL tasks.\nIn fact, as most tasks in the VL domain take the\nform of visual question answering, it is less well-\ndefined what would “cross-task generalization” en-\ntail in VL, compared to in NLP where (2) Due to\ncomputational limitations, we experiment with only\na moderate-sized LM. It is unclear the performance\nof our method after scaling up.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and\nDevi Parikh. 2015. Vqa: Visual question answering.\nIn Proceedings of the IEEE international conference\non computer vision, pages 2425–2433.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nConstantin Eichenberg, Sidney Black, Samuel Wein-\nbach, Letitia Parcalabescu, and Anette Frank. 2021.\nMagma–multimodal augmentation of generative\nmodels through adapter-based finetuning. arXiv\npreprint arXiv:2112.05253.\nTheodoros Evgeniou and Massimiliano Pontil. 2004.\nRegularized multi–task learning. In Proceedings of\nthe tenth ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 109–\n117.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on ma-\nchine learning, pages 1126–1135. PMLR.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reason-\ning and compositional question answering. Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel\nFried. 2023. Grounding language models to im-\nages for multimodal generation. arXiv preprint\narXiv:2301.13823.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition , pages\n3195–3204.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nSebastian Ruder. 2017. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:200–212.\nRicardo Vilalta and Youssef Drissi. 2002. A perspective\nview and survey of meta-learning. Artificial intelli-\ngence review, 18(2):77–95.\n500\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 36, pages 3081–\n3089.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, et al. 2022. Socratic models: Com-\nposing zero-shot multimodal reasoning with lan-\nguage. arXiv preprint arXiv:2204.00598.\nA Appendix\n501\nPlease answer \nthe question\nAnswer: 3 Answer: kitchen Answer: dots\nQuestion: What color is \nthe girls shirt?\nAnswer:\nSupport set Meta-test set\nGreen.\nModel CompletionTask Induction\nQuestion: How many birds \nare flying?\nQuestion: Which room \nof the house if pictured \nin this photo?\nQuestion: What is the \npattern on his toy?\nGT: \n['multiple’, \n'green’, \n'green’, \n'green', \n'green',\n'green’, \n'green',\n'multi \ncolored',\n'green', \n'green']\na)\nPlease answer \nthe question\nAnswer: wall Answer: 5 Answer: 4-cheese\nQuestion: What is the \nplayer about to hit?\nAnswer:\nSupport set Meta-test set\na tennis \nball.\nModel CompletionTask Induction\nQuestion: Where is an \nelectrical outlet?\nQuestion: How many \nanimals are there?\nQuestion: What flavor is \nthe pizza? \nGT: \n[\n'tennis ball',\n'tennis ball',\n'tennis ball',\n'tennis ball',\n'tennis ball',\n'tennis ball',\n'tennis ball',\n'ball',\n'tennis ball',\n'ball'\n]\nb)\nPlease answer \nthe question\nAnswer: yes Answer: yes Answer: no\nQuestion: What console \nare they playing with?\nAnswer:\nSupport set Meta-test set\nWii \nremote.\nModel CompletionTask Induction\nQuestion: Is she having fun? Question: Is there a \nwindow in the bathroom?\nQuestion: Is the window \nopen?\nGT: \n['we’,\n'wii’,\n'nintendo\nwii’,\n'wii’,\n'wii’,\n'wii’,\n'wii’,\n'wii’,\n'wii’,\n'wii’]\nc)\nPlease answer \nthe question\nAnswer: swimming Answer: brown Answer: white\nQuestion: What game is \nthe person playing?\nAnswer:\nSupport set Meta-test set\nBaseball.\nModel CompletionTask Induction\nQuestion: What are the \nbears doing? \nQuestion: What color is \nthe ground? \nQuestion: What color is \nthe refrigerator?\nGT: \n[baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball',\n'baseball’]\nPlease answer \nthe question\nAnswer: no Answer: skiing Answer: no\nQuestion: Does the \nwoman have shoes on?\nAnswer:\nSupport set Meta-test set\nyes.\nModel CompletionTask Induction\nQuestion: Are the \nmotorcycles being ridden?\nQuestion: What is the \nman doing?\nQuestion: Are there lights \non in the picture?\nGT: \n['yes', \n'yes', \n'yes', \n'yes', \n'yes', \n'yes', \n'yes', \n'yes', \n'yes', \n'yes’]\ne)\nd)\nFigure 5: MetaVL success examples from VQA.\n502\nPlease answer \nthe question\nAnswer: catch Answer: 70 Answer: brown\nQuestion: Does the scene \nmake you feel cold or hot?\nAnswer:\nSupport set Meta-test set\nHot.\nModel CompletionTask Induction\nQuestion: What is the goal \nof the game these boys are \nplaying? \nQuestion: How old are \nthese people?\nQuestion: What kind of \nbear is that?\nGT: \n['hot', \n'hot', \n'hot', \n'hot', \n'hot', \n'hot', \n'cold', \n'cold', \n'cold', \n'cold']\na)\nPlease answer \nthe question\nAnswer: play Answer: produce Answer: 9\nQuestion: What type of \nphone is this person \nusing?\nAnswer:\nSupport set Meta-test set\nA cell \nphone.\nModel CompletionTask Induction\nQuestion: What activity is \nhappening?\nQuestion: Where in the \nsupermarket would you find \nthe vegetables depicted?\nQuestion: How many \nplayers are needed on \nthis type of team?\nGT: \n['cell',\n'cell',\n'cell',\n'cell',\n'smartphone,\n'smartphone,\n'i phone',\n'i phone',\n'smart',\n'smart']\nb)\nPlease answer \nthe question\nAnswer: artwork Answer: spoon Answer: dog show\nQuestion: What are these \nobjects for?\nAnswer:\nSupport set Meta-test set\nA\ncomputer.\nModel CompletionTask Induction\nQuestion: Is this artwork or \npractical?\nQuestion: Which metal \nobject did the dish run away \nwith in the nursery rhyme? \nQuestion: What are they \nwatching?\nGT: \n['computer’,\n'computer',\n'computer',\n'computer',\n'compute',\n'compute',\n'computer',\n'computer',\n'interface with \ncomputer',\n'interface with \ncomputer']c)\nPlease answer \nthe question\nAnswer: sponser Answer: competition Answer: first base\nQuestion: What religion \ndoes the statue belong to?\nAnswer:\nSupport set Meta-test set\nBuddhism.\nModel CompletionTask Induction\nQuestion: How is toyota\ninvolved with his activity? \nQuestion: Was this a bike \nsale or competition? \nQuestion: Where does \nthe man in front want to \ngo next?\nGT: \n['buddhism,\n'buddhism',\n'buddhism',\n'buddhism',\n'buda',\n'buda',\n'china',\n'china',\n'buddhist',\n'buddhist']\nPlease answer \nthe question\nAnswer: farm Answer: primary Answer: uk\nQuestion: Was the \nperson old or young? \nAnswer:\nSupport set Meta-test set\nol\nd\n.\nModel CompletionTask Induction\nQuestion: What industry is \nthis useful for?\nQuestion: Are the wall \ncolors primary or \nsecondary colors? \nQuestion: What country \nis this in? \nGT: \n['old', \n'old', \n'old', \n'old', \n'old', \n'old', \n'old', \n'old', \n'young', \n'young']\ne)\nd)\nFigure 6: MetaVL success examples from OK-VQA.\n503\nPlease answer \nthe question\nAnswer: left Answer: beach Answer: yes\nQuestion: Is the plate to the \nright or to the left of the \nperson that is wearing glasses? \nAnswer:\nSupport set Meta-test set\nthe\n righ\nt\n.\nModel CompletionTask Induction\nQuestion: Is the catcher to \nthe right or to the left of the \numpire that is wearing pants?\nQuestion: Which place \nis it? \nQuestion: Is the pillow to \nthe left of the mirror?\na)\nPlease answer \nthe question\nAnswer: girl Answer: boat Answer: yes\nQuestion: The bottle \nthat is to the left of the \nbag is of what color?\nAnswer:\nSupport set Meta-test set\nBlue.\nModel CompletionTask Induction\nQuestion: Who is facing the \ntelevision? Question: What is the \nwatercraft called? \nQuestion: Is the letter \nprinted on a cup?\nGT: \nblue\nb)\nPlease answer \nthe question\nAnswer: yes Answer: no Answer: glass\nQuestion: On which side of \nthe photo is the wreath?\nAnswer:\nSupport set Meta-test set\non the \nleft.\nModel CompletionTask Induction\nQuestion: Does the device \nin front of the TV look silver \nand open?\nQuestion: Does the person \nto the right of the person \nappear to be sitting? \nQuestion: What's the \ndoor made of? \nc)\nPlease answer \nthe question\nAnswer: color Answer: no Answer: left\nQuestion: The woman to the \nleft of the man is holding \nwhat?\nAnswer:\nSupport set Meta-test set\na\n glass\n of\nwin\ne\n.\nModel CompletionTask Induction\nQuestion: What do both the \nskillet and the stove top \nhave in common?\nQuestion: Is there a dog \non the floor?\nQuestion: Is the baby to the \nleft or to the right of the girl \nthat is wearing a jacket?\nPlease answer \nthe question\nAnswer: walkway Answer: mexican food Answer: cow\nQuestion: What is the vehicle \nto the left of the man?\nAnswer:\nSupport set Meta-test set\na\n va\nn\nModel CompletionTask Induction\nQuestion: Which place is it? Question: How do you \nthink is the food to the \nright of the plate that the \nfork is to the left of called?\nQuestion: What animal is \nto the right of the bull?\ne)\nd)\nGT: \nright\nGT: \nlef\nt\nGT: glass\nGT: van\nFigure 7: MetaVL success examples from GQA.\n504\nin a tree.\nPlease answer \nthe question\nAnswer: phone Answer: no Answer: neither\nQuestion: How many train cars \ncan you see in this picture? \nAnswer:\nSupport set Meta-test set\n5\nModel CompletionTask Induction\nQuestion: What is on the \nwall next to the toilet?\nQuestion: Is there a \nstop sign?\nQuestion: Did the woman \ncome from playing tennis or \nis she going to play tennis?\na)\nPlease answer \nthe question\nAnswer: smoothie Answer: party Answer: duck\nQuestion: The cat is \neating a fruit that \ngrows mainly where?\nAnswer:\nSupport set Meta-test set Model CompletionTask Induction\nQuestion: What type of \ndrink?\nQuestion: What type of \nfunction is going on?\nQuestion: What kinds of \nbirds are these? \nGT:\n['costa rica', \n'costa rica', \n'costa rica', \n'costa rica', \n'south', \n'south', \n'tropic', \n'tropic', \n'south \namerica', \n'south \namerica']\nb)\nSupport set Meta-test set Model CompletionTask Induction\nc)\nPlease answer \nthe question\nAnswer: yes Answer: white Answer: couch\nQuestion: Is the blue car to the \nright or to the left of the device \nin the middle of the picture?\nAnswer:\nthe right.\nQuestion: Is the chair in the \ntop part of the photo?\nQuestion: What is the \ncolor of the ipod? \nQuestion: Which kind of \nfurniture is black?\nGT:\n['7', \n'6', \n'2', \n'7', \n'7', \n'7', \n'7', \n'7', \n'6', \n'7']\nGT: \nleft\nFigure 8: MetaVL failure examples from a) VQA, b) OK-VQA, and c) GQA.\n505\nmodel FrozenA w/ adap FrozenA MetaVL w/ adap MetaVL\nn-shot 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3\nAutomatic evaluation\nVQA 28.72 18.98 14.23 7.60 12.94 14.92 18.11 18.63 31.98 30.03 30.01 29.96 31.6 32.01 32.89 33.12\nOK-VQA 7.36 6.30 3.98 2.34 2.91 3.02 4.04 3.30 10.94 9.97 10.32 10.92 9.58 9.30 9.55 9.60\nGQA 22.62 15.44 12.96 6.54 8.80 10.81 12.17 13.86 29.12 28.31 27.78 26.74 30.10 30.05 31.32 31.96\nHuman evaluation\nVQA 25.49 15.66 16.70 11.53 8.79 13.62 15.31 16.68 28.20 26.61 26.12 26.01 30.24 31.33 33.89 35.09\nOK-VQA 6.70 6.04 3.88 2.56 4.67 4.71 4.94 6.41 14.67 9.97 9.01 9.24 14.72 13.95 17.95 19.22\nGQA 30.01 14.72 8.92 5.59 6.18 15.85 19.07 19.96 33.74 32.09 31.81 31.58 35.08 37.65 38.03 38.29\nTable 2: Accuracy of MetaVL and Frozen, w/ and w/o adaptors with 0-3 shots of in-context data.\nMetaVL MetaVL 50%\nAutomatic evaluation\nVQA 33.12 30.32\nOK-VQA 9.60 7.56\nGQA 31.96 27.77\nHuman evaluation\nVQA 35.09 34.02\nOK-VQA 19.22 18.19\nGQA 38.29 35.66\nTable 3: The performance of MetaVL was evaluated using the complete CoCo training dataset as well as a subset\ncontaining 50 percent of the CoCo training data. The experimental results indicate that even with the reduced\ntraining data, MetaVL maintains its capacity for in-context learning, albeit with a slight decrease in performance.\n506\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n6\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n4.2\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n507\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.2\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.2\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.2\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n508"
}