{
  "title": "Refiner: Refining Self-attention for Vision Transformers",
  "url": "https://openalex.org/W3177183540",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2563060429",
      "name": "Zhou, Daquan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096110918",
      "name": "Shi Yujun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2963182374",
      "name": "Kang, Bingyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2569331230",
      "name": "Yu, Weihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202207709",
      "name": "Jiang, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999606766",
      "name": "Li Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350479366",
      "name": "Jin, Xiaojie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174905547",
      "name": "Hou, Qibin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2594576826",
      "name": "Feng, Jiashi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3044863660",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2949846184",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3128633047",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W3010476030",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2952390042",
    "https://openalex.org/W2796438033",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2949863037",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W3167695527",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2949558627"
  ],
  "abstract": "Vision Transformers (ViTs) have shown competitive accuracy in image classification tasks compared with CNNs. Yet, they generally require much more data for model pre-training. Most of recent works thus are dedicated to designing more complex architectures or training methods to address the data-efficiency issue of ViTs. However, few of them explore improving the self-attention mechanism, a key factor distinguishing ViTs from CNNs. Different from existing works, we introduce a conceptually simple scheme, called refiner, to directly refine the self-attention maps of ViTs. Specifically, refiner explores attention expansion that projects the multi-head attention maps to a higher-dimensional space to promote their diversity. Further, refiner applies convolutions to augment local patterns of the attention maps, which we show is equivalent to a distributed local attention features are aggregated locally with learnable kernels and then globally aggregated with self-attention. Extensive experiments demonstrate that refiner works surprisingly well. Significantly, it enables ViTs to achieve 86% top-1 classification accuracy on ImageNet with only 81M parameters.",
  "full_text": "Reﬁner: Reﬁning Self-attention for\nVision Transformers\nDaquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu,\nZihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, Jiashi Feng\nNational University of Singapore\n{zhoudaquan21, andrewhoux, weihaoyu6, xjjin0731}@gmail.com\n{jzihang, kang, shi.yujun}@u.nus.edu, elefjia@nus.edu.sg\nAbstract\nVision Transformers (ViTs) have shown competitive accuracy in image classiﬁ-\ncation tasks compared with CNNs. Yet, they generally require much more data\nfor model pre-training. Most of recent works thus are dedicated to designing more\ncomplex architectures or training methods to address the data-efﬁciency issue of\nViTs. However, few of them explore improving the self-attention mechanism, a\nkey factor distinguishing ViTs from CNNs. Different from existing works, we\nintroduce a conceptually simple scheme, called reﬁner, to directly reﬁne the self-\nattention maps of ViTs. Speciﬁcally, reﬁner explores attention expansion that\nprojects the multi-head attention maps to a higher-dimensional space to promote\ntheir diversity. Further, reﬁner applies convolutions to augment local patterns\nof the attention maps, which we show is equivalent to a distributed local atten-\ntion—features are aggregated locally with learnable kernels and then globally\naggregated with self-attention. Extensive experiments demonstrate that reﬁner\nworks surprisingly well. Signiﬁcantly, it enables ViTs to achieve 86% top-1 classiﬁ-\ncation accuracy on ImageNet with only 81M parameters. Code is publicly available\nat https://github.com/zhoudaquan/Refiner_ViT.\n1 Introduction\nRecent progress on image classiﬁcation largely attributes to the development of vision transformers\n(ViTs) [19, 48, 49]. Unlike convolutional neural networks (CNNs) that rely on convolutions (e.g.,\nusing 3 ×3 kernels) to process features locally [ 46, 25, 41, 65, 58], ViTs take advantage of the\nself-attention (SA) mechanism [51] to establish the global-range relation among the image patch\nfeatures (a.k.a. tokens) and aggregate them across all the spatial locations. Such global information\naggregation nature of SA substantially increases ViT’s expressiveness and has been proven to be\nmore ﬂexible than convolutions [45, 48, 60, 27, 34] on learning image representations. However, it\nalso leads ViTs to need extraordinarily larger amount of data for pre-training (e.g., JFT-300M) or\nmuch longer training time and stronger data augmentations than its CNN counterparts to achieve\nsimilar performance [48].\nMost of recent works thus design more complex architectures or training methods to improve data-\nefﬁciency of ViTs [48, 60, 45]. However, few of them pay attention to the SA component of ViTs.\nWithin a SA block in ViTs, each token is updated by aggregating the features from all tokens\naccording to the attention maps, as shown in Fig. (1)(b). In this way, the tokens of a layer are able\nto sufﬁciently exchange information with each other and thus offer great expressiveness. However,\nthis can also cause different tokens to become more and more similar, especially as the model goes\ndeeper (more information aggregation). This phenomenon, also known as over-smoothing, has been\nidentiﬁed by some recent studies [66, 49, 18] to largely degrade the performance of the ViTs.\nPreprint. Under review.\narXiv:2106.03714v1  [cs.CV]  7 Jun 2021\nImage Patches\nTokens\n(c) Refiner-attention\nTokens\nOver-smoothed \nAttention Maps\nRefined \nAttention Maps\nΣ Σ ΣΣΣ Σ\n(a) Patch embedding (b) Self-attention\nFigure 1: Illustration on our motivation. (a) The input image is regularly partitioned into patches\nfor patch embedding. (b) The token-wise attention maps from vanilla self-attention of ViTs tend to\nbe uniform, and thus they aggregate all the patch embeddings densely and generate overly-similar\ntokens. (c) Differently, our proposed reﬁner augments the attention maps into diverse ones with\nenhanced local patterns, such that they aggregate the token features more selectively and the resulting\ntokens are distinguishable from each other.\nIn this work, we explore to address the above limitation by directly reﬁning their self-attention maps.\nSpeciﬁcally, inspired by the recent works [49] demonstrating that increasing the number of SA heads\ncan effectively improve the model performance, we investigate how to promote the diversity of the\nattention maps using a similar strategy. However, given a transformer model with ﬁxed embedding\ndimension, directly increasing the number of heads will reduce the number of embeddings allocated\nto each head, making the computed attention map less comprehensive and accurate as shown in\nTab. 9 in [49]. To address this dilemma, we explore attention expansion that linearly projects the\nmulti-head attention maps to a higher-dimensional space spanned by a larger number of attention\nmaps. As such, the number of attention heads (used for computing the attention maps) are implicitly\nincreased without reducing the embedding dimension per head, enabling the model to enjoy both\nbeneﬁts from more SA heads and high embedding dimension.\nAdditionally, we argue that ignoring the local relationship among the tokens is another main cause\nof the above mentioned over-smoothing issue of global SA. The locality (local receptive ﬁelds)\nand spatial invariance (weight sharing) have been proven to be the key of the success of CNNs\nacross many computer vision tasks [31, 44, 53, 25, 1]. Therefore, we explore how to leverage the\nconvolution to augment the attention mechanism of ViTs. Some recent works have studied injecting\nconvolutional locality into ViTs, e.g., integrating both convolution and global self-attention into a\nhybrid model [19, 45]. However, they mostly consider directly applying convolution to the token\nfeatures, keeping the convolutional block and the SA block separately. Differently, we explore\nintroducing convolution to the attention maps directly to augment their spatial-context and local\npatterns, thus increasing their diversity. Such an approach, as we explain later, can be understood as a\ndistributed local attention mechanism that combines both strengths of self-attention (global-range\nmodeling) and convolution (reinforcing local patterns).\nBased on the above two strategies, we introduce the reﬁner scheme to directly enhance self-attention\nmaps within ViTs. Though being conceptually simple, it works surprisingly well. As shown in\nFig. 1(c), the attention map with reﬁner has preciser focus on the target object, compared to the\nvanilla one. By directly applying reﬁner to the SA module, the performance of the ViT-Base [19] is\nimproved by 1.7% on ImageNet under the same training recipes with negligible memory overhead\n(less than 1M). We have experimentally veriﬁed that this improvement is consistent when using more\ncomplex training recipes [48, 49] and the results are shown in Appendix. Surprisingly, reﬁner is also\napplicable for natural language processing tasks. On GLUE [52], it increases the average score by\n1% over the state-of-the-art BERT [16].\nIn summary, our contributions are: First, we tackle the important low data-efﬁciency issue of ViTs\nfrom a new angle, i.e., directly reﬁning the self-attention maps. We investigate a simple yet novel\n2\nreﬁner scheme and ﬁnd it works surprisingly well, even outperforming the state-of-the-art methods\nusing sophisticated training recipes or leveraging extra models. Secondly, the introduced reﬁner\nmakes several interesting modiﬁcations on the self-attention mechanism that would be inspiring for\nfuture works. Its attention expansion gives a new viewpoint for addressing the common dilemma\non the trade-off between the number of attention heads and the embedding dimensions. Besides,\ndifferent form the common practice that applies convolutions to features, it proves that convolutions\ncan effectively augment attention maps as well and improve the model performance signiﬁcantly.\n2 Related Works\nTransformers for vision tasks Transformers [51] were originally developed for solving natural\nlanguage processing tasks and achieved remarkable success [ 5, 38]. Different from other deep\nneural network architectures like CNNs and RNNs [33, 40], transformers rely on the self-attention\nmechanism to perform the global interactions among features instead of relying on certain inductive\nbiases (e.g., locality and weight sharing from CNNs). This grants transformers stronger capacity in\nlearning from large-scale data. Those properties of transformers motivate researchers to explore its\napplication on computer vision tasks, including image enhancement [9, 59], image classiﬁcation [19,\n60, 66], object detection [6, 68, 13, 64], segmentation [54, 34, 64], video processing [62, 67] and 3D\npoint cloud data processing [63, 22]. Among them, Vision Transformer (ViT) [19] is one of the early\npure-transformer based models that achieve state-of-the-art performance on ImageNet. However, due\nto the larger model capacity, ViTs need extraordinarily large-scale datasets (e.g., ImageNet-22K and\nJFT-300M) for pre-training to achieve comparable performance with CNNs at similar model size.\nSome follow-up models thus try to resolve the low data-efﬁciency limitation either by modifying the\nmodel architecture [60, 48, 49] or adopting new training techniques like knowledge distillation [48].\nInjecting locality into ViTs Recent works ﬁnd that injecting the convolution locality into trans-\nformer blocks can boost performance of ViTs. As initially proposed in [19, 45], the hybrid ViT uses\npre-trained CNN models to extract the patch embedding from the input images and then deploys\nmultiple transformer blocks for feature processing. [ 32] replaces the feed forward block in the\ntransformer block with an inverted residual one [41, 26] and [37] propose to build two concurrent\nbranches: one with convolution and the other one with transformer. However, they all leverage\nconvolutions to strengthen the features. Besides, the interaction between the global information\naggregation at the SA block and the local information extraction in the convolution block is not well\nexplored. In this paper, we focus on enhancing the attention maps and explore how the global context\nand the local context can be combined in a novel way.\n3 Reﬁner\n3.1 Preliminaries on ViT\nThe vision transformer (ViT) ﬁrst tokenizes an input image by regularly partitioning it into a sequence\nof n small patches. Each patch is then projected into an embedding vector xi via a linear projection\nlayer, with an additional learnable position embedding.\nThen ViT applies multiple multi-head self-attention (MHSA) and feedforward layers to process the\npatch embeddings to model their long-range dependencies and evolve the token embedding features.\nSuppose the input tensor is X ∈Rdin×n, the MHSA applies linear transformation with parameters\nWK, WQ, WV to embed them into the key K = WKX ∈Rd×n, query Q = WQX ∈Rd×n and\nvalue V = WV X ∈Rd×n respectively. Suppose there are H self-attention heads. These embeddings\nare uniformly split into H segments Qh, Kh, Vh ∈Rd/H×n. Then the MHSA module computes the\nhead-speciﬁc attention matrix (map)1 A and aggregate the token value features as follows:\nAttention(X, h) =AhV ⊤\nh with Ah = Softmax\n(\nQ⊤\nh Kh√\nd/H\n)\n, h= 1, . . . , H. (1)\n1We interchangeably use attention matrix and attention map to call A when the context is clear. Speciﬁcally,\nthe attention matrix means the original A while the attention map means reshaped A to the input size.\n3\nLimitations of ViTs The ViT purely relies on the global self-attention to establish the relationship\namong all the tokens without deploying any spatial structure priors (or inductive biases). Though\nbeneﬁting in learning ﬂexible features from large-scale samples, such global-range attention may lead\nto overly-smoothed attention maps and token features as pointed out by [66, 21]. We hypothesize this\nmay slow the feature evolving speed of ViTs, i.e., the features change slowly when traversing the\nmodel blocks, and make the model perform inferior to the ones with faster feature evolving speed.\nTo verify this, we present a pilot experiment to compare the feature learning speed of ViTs with\nsimilar-size CNNs (ResNet) [25] that incorporate strong locality regularization and DeiT [48] which\nis a better-performing transformer model trained with sophisticated scheme.\n2 4 6 8 10 12 14 16\nNetwork Depth\n0\n1\n2\n3\n4\n5Feature Similarity with CKA \nFeature Map Cross Layer Similarity\nDeiT-16B-384\nResNet-50\nViT-16B-384\nMethod #Param. #Blocks Acc.\nDeiT-16B-384 24M 16 80.8%\nResNet-50 25M 16 80.5%\nViT-16B-384 24M 16 78.9%\nFigure 2: The features of ViT evolves slower than\nResNet [25] and DeiT [48] across the model blocks.\nIn Fig. 2, we plot the CKA similarity [28]\n(ref. its formula to Appendix) between the\nintermediate token features at the output of\neach block and the ﬁnal output, normalized\nby the similarity between the ﬁrst and ﬁ-\nnal layer features. Such a metric captures\ntwo properties: how fast the intermediate\nfeatures converge to the ﬁnal features and\nhow much the ﬁnal features are different\nfrom the ﬁrst layer features. It is clearly\nshown that for the ViT, the feature evolves\nslowly after the ﬁrst several blocks. Dif-\nferently, the token features of ResNet and\nDeiT keep evolving faster when the model\ngoes deeper and the ﬁnal features are more\ndifferent than the ﬁrst layer features com-\npared with the ViT. In this work, we inves-\ntigate whether such limitation of ViTs can\nbe solved by reﬁning the attention maps\ninto more diverse and locality-aware ones.\n3.2 Attention Expansion\nThe MHSA of transformers captures different aspects of the input features [51] by projecting them\ninto different subspaces and computing self-attention therein. Thus, increasing the number of attention\nheads within MHSA, which is shown to be effective at improving the model performance [49], can\npotentially increase diversity among the attention maps. However, as mentioned above, if naively\nadding more SA heads, for a model with ﬁxed hidden dimension, it is difﬁcult to trade-off the beneﬁt\nof having more SA heads and the harm of reducing the embedding dimension per head.\nTo resolve this issue, instead of directly expanding the attention heads, we investigate expanding\nthe number of self-attention maps via linear transformation. Concretely, we use a linear projection\nWA ∈RH′×H with H′ > H to project the attention maps A = [A1; . . .; AH] to a new set of\nattention maps ˜A = [˜A1, . . . ,˜AH′\n] with ˜Ah = ∑H\ni=1 WA(h, i) ·Ai, h= 1, . . . , H′. Then we use\nthe new attention maps ˜A to aggregate features as in Eqn. (1).\nThe above attention expansion implicitly increases the number of SA heads but does not sacriﬁce\nthe embedding dimensions. To understand how it works, recall each of the original SA map Ah\nis computed by Ah = Q⊤\nh Kh. Here we omit the softmax and normalizing factor for illustration\nsimplicity. We use wi to denote WA(h, i). Thus, the up-projected attention map is computed by\n˜Ah = ∑H\ni=1 wi ·Q⊤\ni Ki = [w1 ·Q1, . . . , wH ·QH]⊤K. Different from the vanilla MHSA that divides\nQ, Kinto H sub-matrices for SA computation, the attention expansion reweighs the Q features\nwith learnable scalars and uses the complete reweighed Q to compute the SA maps. Getting rid\nof the feature division, the attention expansion effectively solves the above issue. When only one\nweight scalar is non-zero, it degenerates to the vanilla MHSA. The attention expansion is similar to\nthe talking-heads attention [42, 66] which are proven effective in improving attention diversity but\ndifferently, it expands the number of attention maps while talking-heads does not change the number.\n4\nQ\nK\nV\nOut\nNorm\nFeed Foward\nNorm\nAdd\nAdd\nPatch Embed\nLinear + Loss\n×N\nRefiner Block\nRefiner Attention\nFigure 3: (a) Architecture design of reﬁner. Different from the vanilla self-attention block, the reﬁner\napplies linear attention expansion to attention maps output from the softmax operation to increase\ntheir number. Then head-wise spatial convolution is applied to augment these expanded attention\nmaps. Finally another linear projection is deployed to reduce the number of attention maps to the\noriginal one. Note that r = H′/H is the expansion ratio. (b) Modiﬁed transformer block with reﬁner\nas a drop-in component.\n3.3 Distributed Local Attention\nOur second strategy is to investigate how convolutions can help enhance self-attention maps since\nconvolutions are good at strengthening local patterns. Different from some earlier trials on introducing\nconvolutions to process features in ViTs [45, 32, 37], we explore directly applying convolutions to\nthe self-attention maps.\nGiven the pre-computed self-attention matrix Ah from head h, we apply a learnable convolution\nkernel w ∈Rk×k of size k ×k to modulate it as follows:\nAh∗\ni,j =\nk∑\na,b=1\nwa,b ·Ah\ni−⌊k\n2 ⌋+a,j−⌊k\n2 ⌋+b.\nThen the standard SA-based feature aggregation as Eqn. (1) is conducted with this new attention\nmatrix ˜vi = ∑n\nj=1 ˜A∗\ni,j ·vj.\nThough being conceptually simple, the above operation establishes an interesting synergy between\nthe global context aggregation (with self-attention) and local context modeling (with convolution).\nTo see this, consider applying 1D convolution w of length k to obtain the convolution-augmented SA\nmatrix, with which the feature aggregation becomes\n˜vi =\nn∑\nj=1\nAh∗\ni,j ·vj =\nn∑\nj=1\n( k∑\na=1\nwa ·Ah\ni,j−⌊k\n2 ⌋+a\n)\n·vj\n=\nn∑\nj=1\n( k∑\na=1\nwa ·Ah\ni,j−⌊k\n2 ⌋+a ·vj\n)\n=\nn∑\nj=1\n( k∑\na=1\nAh\ni,j ·wa ·vj−a+⌊k\n2 ⌋\n)\n.\nThe above clearly shows that the feature aggregation based on the convolution-processed attention\nmatrix is equivalent to: (1) applying the convolution w, with a location-speciﬁc reweighing scalar\nAh\ni,j, to aggregate features locally at ﬁrst and (2) summing over the locally aggregated features.\nTherefore, we name such an operation as distributed local attention (DLA).\nWe empirically ﬁnd DLA works pretty well in the experiments. We conjecture that DLA effectively\ncombines strengths of the two popular feature aggregation schemes. Convolutional schemes are strong\nat capturing local patterns but inefﬁcient in global-range modeling. Moreover, stacking multiple\nconvolutions may suffer the increasingly center-biased receptive ﬁeld [ 35], leading the model to\nignore the features at the image boundary. In contrast, self-attention based feature aggregation can\neffectively model the global-range relation among features but suffer the over-smoothing issue. DLA\ncan effectively overcome their limitations and better model the local and global context jointly.\n5\n3.4 Reﬁner and Reﬁned-ViT\nThe attention expansion and distributed local attention can be easily implemented via 1 ×1 and\n3 ×3 convolutions. Fig. 3 illustrates the reﬁner architecture. Different from the vanilla self-attention,\nreﬁner expands the attention maps at ﬁrst and then applies head-wise convolution to process the\nmaps. Reﬁner additionally introduces the attention reduction for reducing the model computation\ncost and maintaining the model embedding dimension. Speciﬁcally, similar to attention expansion,\nthe attention reduction applies linear projection to reduce the number of attention maps from H′to\nthe original H, after the distributed local attention. Reﬁner can serve as a drop-in module to directly\nreplace the vanilla self-attention in each ViT transformer block, giving the Reﬁned-ViT model.\n4 Experiments\n4.1 Experiment Setup\nWe mainly evaluate Reﬁner for the image classiﬁcation task. Besides, we also conduct experiments\non natural language processing tasks to investigate its generalizability to NLP transformer models.\nComputer vision We evaluate the effectiveness of the reﬁner on ImageNet [15]. For a fair com-\nparison with other methods, we ﬁrst replace the SA module with reﬁner on ViT-Base [ 19] model\nas it is the most frequently used one [19, 48, 61, 49]. When comparing with other state-of-the-art\nmethods, we modify the number of transformer blocks and the embedding dimensions to increase the\nefﬁciency in the same manner as [60, 66].\nNatural language processing We evaluate our model on the General Language Understanding\nEvaluation (GLUE) benchmark [52]. GLUE benchmark includes various tasks which are formatted\nas single sentence or sentence pair classiﬁcation. See Appendix for more details of all tasks. We\nmeasure accuracy for MNLI, QNLI, QQP, RTE, SST, Spearman correlation for STS and Matthews\ncorrelation for CoLA. The GLUE score is the average score for all the 8 tasks.\nTraining setup All the experiments are conducted upon PyTorch [36] and the timm [56] library.\nThe models are trained on ImageNet-1k from scratch without auxiliary dataset. For the ablation\nexperiments, we follow the standard training schedule and train our models on the ImageNet dataset\nfor 300 epochs. When compared to state-of-the-art (SOTA) models, we use the advanced training\nrecipes as proposed in [48]. Detailed training hyper-parameters are listed in the appendix.\n4.2 Analysis\nReﬁner introduces attention expansion and convolutional local kernels to augment the self-attention\nmaps of ViTs. Here we individually investigate their effectiveness through ablative studies.\nTable 1: Effect of the expansion ratio in attention expansion, which varies from 1 to 6. The model is Reﬁned-ViT\nwith 16 blocks, 12 attention heads and 384 embedding dimension.\nExpan. Ratio Params Converge (#Epoch) Top-1 (%)\n1 25M 300 82.3\n2 25M 300 82.8\n3 25M 273 82.8\n4 25M 270 82.9\n6 25M 261 83.0\nEffect of attention expansion We adopt 1 ×1 convolution to adjust the number of attention\nmaps. In this ablative study, we vary the expansion ratio r = H′/H from 1 to 6. From the\nresults in Tab. 1, we observe that along with increased expansion ratio (and more attention maps),\nthe model performance monotonically increases from 82.3% to 83.0%. This clearly demonstrates\nthe effectiveness of attention expansion in improving the model performance. Note when the\nexpansion ratio equals to 1, the attention expansion degenerates to the talking-heads attention [42, 66].\nCompared with this strong baseline, increasing the ratio to 2 brings 0.5% top-1 accuracy improvement.\nInterestingly, using larger expansion ratio speeds up the convergence of model training. Using an\nexpansion ratio of 6 saves the number of training epochs by nearly 13% (261 vs. 300 epochs). This\n6\nTable 2: Impacts of convolution on attention maps. We\ndirectly apply the 3 ×3 convolution on the attention maps\nfrom the multi-head self-attention of ViTs with respect to\nvarious architectures. We can observe clear improvement for\nall ViT variants when adding the proposed DLA.\nModel #Blocks Hidden dim #Heads Params Top-1 (%)\nVIT 12 768 12 86M 79.5\n+ DLA 12 768 12 86M 81.2\nVIT 16 384 12 24M 78.9\n+ DLA 16 384 12 24M 80.3\nVIT 24 384 12 36M 79.3\n+ DLA 24 384 12 36M 80.9\nVIT 32 384 12 48M 79.2\n+ DLA 32 384 12 48M 81.1\nTable 3: Effect of attention reduction on\nReﬁned-ViT-16B with 384 hidden dimension.\nModel Attn. map Top-1 (%)\nReﬁned-ViT-16B w/o reduction 82.99\nReﬁned-ViT-16B w/ reduction 82.95\nTable 4: Evaluation on how the spatial span\nwithin DLA affects the model performance. We\ncompare the model performance with three dif-\nferent constraints on the local kernels.\nModel Constraints Top-1 (%)\nReﬁned-ViT-16B None 83.0\nReﬁned-ViT-16B Spatial 82.7\nReﬁned-ViT-16B Row+Col 81.7\nreﬂects that attention expansion can help the model learn the discriminative token features faster.\nHowever, when the expansion ratio is larger than 3, the beneﬁts in the model accuracy attenuates.\nThis motivates us to explore the distributed local attention for further performance enhancement.\nEffect of attention reduction Reﬁner deploys another 1 ×1 convolution to reduce the number of\nattention maps from H′to the original number H, after the attention expansion and distributed local\nattention, in order to reduce the computation overhead due to expansion. We conduct experiments to\nstudy whether reduction will hurt model performance. As observed from Tab. 3, attention reduction\ndrops the accuracy very marginally, implying the attention maps have been sufﬁciently augmented in\nthe higher-dimension space.\nEffect of distributed local attention We then evaluate whether the distributed local attention\n(DLA) works consistently well across a broad spectrum of model architectures. We evaluate its\neffectiveness for various ViTs with 12 to 32 SA blocks, without the attention expansion. Following\nthe common practice [19], the hidden dimension is 768 for the ViT-12B and 384 for all the other ViTs.\nWe set the local attention window size ask = 3and use the DLA to replace all the self-attention block\nwithin ViT. From the results summarized in Tab. 2, DLA can consistently boost the top-1 accuracy\nof various ViTs by 1.2% to 1.7% with negligible model size increase. Such signiﬁcant performance\nboost clearly demonstrates its effectively and the beneﬁts brought by its ability in jointly modeling\nthe local and global context of input features. The combined effects of attention expansion and DLA\nare shown in Tab. 7.\n2 4 6 8 10 12 14 16\nNetwork Depth\n0\n1\n2\n3\n4\n5\n6\n7Feature Similarity with CKA \nFeature Map Cross Layer Similarity\nDeiT-16B-384\nResNet-50\nViT-16B-384\nRefiner-ViT-16B-384\nFigure 4: Reﬁner accelerates feature evolving com-\npared with CNNs, the vanilla ViT and the Deit trained\nwith a more complex scheme.\nEffect of the local attention kernels Reﬁner\ndirectly applies the 3×3 convolution onto the at-\ntention maps A. We compare this natural choice\nwith another two feasible choices for the lo-\ncal kernels. The ﬁrst one is to apply spatially\nadjacent convolution on the reshaped attention\nmaps such that the aggregated tokens by the lo-\ncal kernels are spatially adjacent in the original\ninput. Speciﬁcally, we reshape each row of A\ninto a √n ×√n matrix2 which together form a√n ×√n ×n tensor and apply 3 ×3 convolu-\ntion. The second one is to apply the combination\nof row and column 1D convolution. From the\nresults in Tab. 4, we ﬁnd the spatially adjacent\nconvolution will slightly hurt the performance\nwhile only adopting the 1D convolution leads to\n1.3% accuracy drop. Directly applying convolu-\ntion to the attention maps (as what reﬁner does)\n2Recall n is the number of tokens and the input image is divided into √n ×√n patches.\n7\ngives the best performance, demonstrating the effectiveness of augmenting the local patterns of the\nattention maps.\nReﬁner augments attention maps and accelerates feature evolving We visualize the attention\nmaps output from different self-attention blocks without and with the reﬁner in Fig. 5. Clearly, the\nattention maps after Reﬁner become not so uniform and present stronger local patterns. We also\nplot the feature evolving speed in Fig. 4, in terms of the CKA similarity change w.r.t. the ﬁrst block\nfeatures and Reﬁned-ViT evolves the features much faster than ViTs.\nSA\nReﬁner\nBlock 1 Block 4 Block 11 Block 18 Block 23 Block 29 Block 30\nFigure 5: Compared with the attention matrices A from the vanilla SA (top), for deeper blocks, reﬁner (bottom)\nstrengthens the local patterns of their attention maps, making them less uniform and better model local context.\n4.3 Comparison with SOTA\nWe compare reﬁned-ViT with state-of-the-art models in Tab. 7. For small-sized model, with 224×224\ntest resolution, Reﬁned-ViT-S (with only 25M parameters) achieves 83.6% accuracy on ImageNet,\nwhich outperforms the strong baseline DeiT-S by 3.7%. For medium-sized model with 384×384 test\nresolution, the model Reﬁned-ViT-M achieves the accuracy of 85.6%, which outperforms the latest\nSOTA CaiT-S36 by 0.2% with 13% less of parameters (55M vs. 68M) and outperforms LV-ViT-M by\n1.6%. Note that CaiT uses knowledge distillation based method to improve their model, requiring\nmuch more computations. For large-sized model ﬁnetuned with input resolution of 448 ×448,\nreﬁned-ViT achieves 86%, better than CaiT-M36 with 70% less of parameters (81M vs. 271M). This\nis the new state-of-the-art results on ImageNet with less than 100M parameters. Notably, as shown in\nTab. 7, our models signiﬁcantly outperform the recent models that introduce locality into the token\nfeatures across all the model sizes. This clearly demonstrates: ﬁrst, reﬁning the attention maps (and\nthus the feature aggregation mechanism) is more effective than augmenting the features for ViTs;\nsecondly, jointly modeling the local and global context (from DLA in the reﬁner) is better than using\nthe convolution and self-attention separately and deploying them into different blocks.\n4.4 Receptive ﬁeld calibration\nIn this subsection, we present and investigate a simple approach that we ﬁnd can steadily improve\nclassiﬁcation performance of ViT. The approach is a generic one that also works well for CNNs.\nAs pointed out in [50], pre-processing the training images and testing images separately could lead\nto a distribution shift between the training and testing regimes. This issue has been named by [50]\nas the mismatch between the region of classiﬁcation (RoC), which could degrade performance of\nthe classiﬁcation model. A common remedy solution is to apply random cropping to get a centrally\ncropped image patch for classiﬁcation, with a certain cropping ratio (less than 1), at the testing phase.\nHowever, as the receptive ﬁeld of a deep neural network at the deep layers is typically larger than the\nimage size, we argue that it is not necessary to crop a patch from the testing images.\nInstead, we propose to pad the testing image such that all the image features can be captured by the\nmodel. Different from directly feeding the image into the model, padding the image with zeros can\nalign the informative region of an image to the center of the receptive ﬁled of the model, similar\nto random cropping, while getting rid of information loss. Thus it can further improve the model\naccuracy. We name this padding method as receptive ﬁeld calibration (RFC). RFC can be easily\nimplemented by setting the cropping ratio to be larger than 1, which we call it RFC ratio, during the\ntesting phase.\n8\nTo investigate its effectiveness, among all the models whose pre-trained weights are publicly available,\nwe apply RFC on the top 2 accurate CNN models and ViTs respectively. The results are summarized\nin Tab. 5. It is observed that RFC can easily further improve the performance of current SOTA model\nby 0.11% without ﬁne-tuning. It is worth noting that the selected models are pre-trained with large\nauxiliary datasets such as ImageNet-22k and JFT-300M and the performance is nearly saturated.\nThe testing image resolutions vary from 384 to 800. Under a variety of model conﬁgurations, RFC\nconsistently improve the accuracy. With RFC, our proposed Reﬁned-ViT achieves a new SOTA on\nthe ImageNet (among models with less than 100M parameters).\nIt should be noted that in the above experiments (Sec. 4.2 and 4.3), including comparing Reﬁned-ViT\nwith SOTAs, we did not apply RFC for fair comparisons. Here we would like to share such RFC\ntechnique as an interesting ﬁnding from our recent experiments. RFC is a generic and convenient\ntechnique to improve model’s classiﬁcation performance. We believe it is inspiring for rethinking the\ncommon random cropping techniques. More importantly, it motivates future studies to dig into the\neffect of the model’s receptive ﬁeld in classiﬁcation performance and explore how to calibrate it w.r.t.\nthe input images to gain better performance.\nTable 5: RFC can improve both CNN and ViT-based SOTA models on ImageNet, outperforming the strategy of\nrandom cropping. We apply RFC on the best performing pre-trained models available online. RFC improves the\ntop-1 accuracy consistently across a wide spectrum of models and conﬁgurations, and establishes the new SOTA\non ImageNet.\nModel Rand Crop. Ratio Top-1 (%) RFC Ratio Top-1 (%)\nEfﬁcientNet-l2-ns-800 [46] 0.960 88.35 1.120 88.46\nEfﬁcientNet-b7-ns-475 [46] 0.936 88.23 1.020 88.33\nSwin-large-patch4-w12-384 [34] 1.000 87.15 1.100 87.18\nCaiT-m48-448 [21] 1.000 86.48 1.130 86.56\nReﬁned-ViT-448 1.000 85.94 1.130 85.98\n4.5 Applied to NLP tasks\nWe also evaluate the performance of Reﬁner-ViT models for natural language processing tasks on the\nGLUE benchmark, to investigate whether Reﬁner also improves other transformer-based models. We\nuse the BERT-small [11] as the baseline model and replace the self-attention module with reﬁner,\nusing the same pre-training dataset and recipes. From the results in Tab. 6, Reﬁner boosts the model\nperformance across all the tasks signiﬁcantly and increases the average score by 1%, demonstrating\nReﬁner is well generalizable to transformer-based NLP models to improve their attentions and ﬁnal\nperformance.\nTable 6: Comparison of BERT-small w/o and w/ reﬁner on the GLUE development set.\nModel Params MNLI QNLI QQP RTE SST MRPC CoLA STS-B Avg.\nBERT-small [11] 14M 75.8 83.7 86.8 57.4 88.4 83.8 41.6 83.6 75.1\n+ Reﬁner 14M 78.1 86.4 88.0 57.6 88.8 84.1 42.2 84.0 76.1\n5 Conclusions\nIn this work, we introduced the reﬁner, a simple scheme that augments the self-attention of ViTs by\nattention expansion and distributed local attention. We ﬁnd it works surprisingly well for improving\nperformance of vision transformers (ViT). Furthermore, it also improves the performance of NLP\ntransformers (BERT) by a large margin. Though reﬁner is limited to improving diversity of the self-\nattention maps, we believe its working mechanism is also inspiring for future works on understanding\nand further improving the self-attention mechanism.\nSome interesting questions arise from the empirical observations but we leave them open for future\nstudies. First, traditionally all the elements within the attention maps are normalized to be between\n0 and 1. However, the attention expansion does not impose such a constraint while giving quite\n9\nTable 7: Top-1 accuracy comparison with other methods on ImageNet [15] and ImageNet Real [3]. All models\nare trained without external data. With the same computation and parameter constraint, Reﬁned-ViT consistently\noutperforms other SOTA CNN-based and ViT-based models. The results of CNNs and ViT are adopted from [49].\nNetwork Params FLOPs Train size Test size Top-1(%) Real Top-1 (%)\nCNNs\nEfﬁcientNet-B5 [46] 30M 9.9B 456 456 83.6 88.3\nEfﬁcientNet-B7 [46] 66M 37.0B 600 600 84.3 _\nFix-EfﬁcientNet-B8 [46, 50] 87M 89.5B 672 800 85.7 90.0\nNFNet-F0 [4] 72M 12.4B 192 256 83.6 88.1\nNFNet-F1 [4] 133M 35.5B 224 320 84.7 88.9\nTransformers\nViT-B/16 [19] 86M 55.4B 224 384 77.9 83.6\nViT-L/16 [19] 307M 190.7B 224 384 76.5 82.2\nT2T-ViT-14 [60] 22M 5.2B 224 224 81.5 _\nT2T-ViT-14↑384 [60] 22M 17.1B 224 384 83.3 _\nCrossViT [8] 45M 56.6B 224 480 84.1 _\nSwin-B [34] 88M 47.0B 224 384 84.2 _\nTNT-B [24] 66M 14.1B 224 224 82.8 _\nDeepViT-S [66] 27M 6.2B 224 224 82.3 _\nDeepViT-L [66] 55M 12.5B 224 224 83.1 _\nDeiT-S [48] 22M 4.6B 224 224 79.9 85.7\nDeiT-B [48] 86M 17.5B 224 224 81.8 86.7\nDeiT-B↑384 [48] 86M 55.4B 224 384 83.1 87.7\nCaiT-S36↑384 [49] 68M 48.0B 224 384 85.4 89.8\nCaiT-M36 [49] 271M 53.7B 224 224 85.1 89.3\nLV-ViT-S [27] 26M 6.6B 224 224 83.3 88.1\nLV-ViT-M [27] 56M 16.0B 224 224 84.0 88.4\nTransformer with locality\nLocalViT-S [32] 22.4M 4.6B 224 224 80.8 -\nLocalViT-PVT [32] 13.5M 4.8B 224 224 78.2 -\nConViT-S [14] 27M 5.4B 224 224 81.3 -\nConViT-S+ [14] 48M 10.0B 224 224 82.2 -\nBoTNet-S1-59 [45] 33.5M 7.3B 224 224 81.7 -\nBoTNet-S1-110 [45] 54.7M 10.9B 224 224 82.8 -\nBoTNet-S1-128 [45] 79.1M 19.3B 256 256 84.2 -\nBoTNet-S1-128↑384 [45] 79.1M 45.8B 256 384 84.7 -\nOur Reﬁned-ViT\nReﬁned-ViT-S 25M 7.2B 224 224 83.6 88.3\nReﬁned-ViT-S↑384 25M 24.5B 224 384 84.6 88.9\nReﬁned-ViT-M 55M 13.5B 224 224 84.6 88.9\nReﬁned-ViT-M↑384 55M 49.2B 224 384 85.6 89.3\nReﬁned-ViT-L 81M 19.1B 224 224 84.9 89.1\nReﬁned-ViT-L↑384 81M 69.1B 224 384 85.7 89.7\nReﬁned-ViT-L↑448 81M 98.0B 224 448 85.9 90.1\ngood results. It is worthy future studies on the effects of such subtraction attention (with negative\nattention weights) for feature aggregation. Second, the local distributed attention indeed imposes a\nlocality inductive bias which is proven to be effective in enhancing learning efﬁciency of ViTs from\nmedium-sized datasets (ImageNet). How to automatically learn the suitable inductive biases would\nbe another interesting direction and some recent works have started to explore [14].\n10\nReferences\n[1] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint\narXiv:2102.08602, 2021.\n[2] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual\nentailment challenge. In TAC, 2009.\n[3] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we\ndone with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[4] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image\nrecognition without normalization. arXiv preprint arXiv:2102.06171, 2021.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.\n[7] Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1:\nSemantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada, August\n2017. Association for Computational Linguistics.\n[8] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[9] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing\nXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint arXiv:2012.00364,\n2020.\n[10] Zihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi Zhao. Quora question pairs, 2018.\n[11] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text\nencoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[12] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge.\nIn Machine Learning Challenges Workshop, pages 177–190. Springer, 2005.\n[13] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object\ndetection with transformers. arXiv preprint arXiv:2011.09094, 2020.\n[14] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit:\nImproving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[17] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n[18] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention\nloses rank doubly exponentially with depth. arXiv preprint arXiv:2103.03404, 2021.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[20] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and\nparaphrasing, pages 1–9. Association for Computational Linguistics, 2007.\n11\n[21] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Improve vision transformers\ntraining by suppressing over-smoothing. arXiv preprint arXiv:2104.12753, 2021.\n[22] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min Hu. Pct:\nPoint cloud transformer. arXiv preprint arXiv:2012.09688, 2020.\n[23] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor.\nThe second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL\nChallenges Workshop on Recognising Textual Entailment, 2006.\n[24] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco\nAndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision\napplications. arXiv preprint arXiv:1704.04861, 2017.\n[27] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng. Token\nlabeling: Training a 85.4% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv\npreprint arXiv:2104.10858, 2021.\n[28] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network\nrepresentations revisited. In International Conference on Machine Learning, pages 3519–3529. PMLR,\n2019.\n[29] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. ALBERT: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019.\n[30] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning, 2012.\n[31] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng, and Shuicheng Yan. Scale-aware fast\nr-cnn for pedestrian detection. IEEE transactions on Multimedia, 20(4):985–996, 2017.\n[32] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv preprint arXiv:2104.05707, 2021.\n[33] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classiﬁcation with\nmulti-task learning. arXiv preprint arXiv:1605.05101, 2016.\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[35] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive ﬁeld in\ndeep convolutional neural networks. arXiv preprint arXiv:1701.04128, 2017.\n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in neural information processing systems, pages 8026–8037, 2019.\n[37] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang Ye.\nConformer: Local features coupling global representations for visual recognition. arXiv preprint\narXiv:2105.03889, 2021.\n[38] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training, 2018.\n[39] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2383–2392, Austin, Texas, November 2016. Association for Computational\nLinguistics.\n[40] Hasim Sak, Andrew W Senior, and Françoise Beaufays. Long short-term memory recurrent neural network\narchitectures for large scale acoustic modeling. 2014.\n12\n[41] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4510–4520, 2018.\n[42] Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. Talking-heads attention. arXiv\npreprint arXiv:2003.02436, 2020.\n[43] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical methods in natural language processing , pages\n1631–1642, 2013.\n[44] Kihyuk Sohn and Honglak Lee. Learning invariant representations with local transformations. arXiv\npreprint arXiv:1206.6418, 2012.\n[45] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605, 2021.\n[46] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\narXiv preprint arXiv:1905.11946, 2019.\n[47] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking\nself-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020.\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[49] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[50] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test resolution\ndiscrepancy. arXiv preprint arXiv:1906.06423, 2019.\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30:5998–6008, 2017.\n[52] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\n[53] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and\nXiaoou Tang. Residual attention network for image classiﬁcation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 3156–3164, 2017.\n[54] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia.\nEnd-to-end video instance segmentation with transformers. arXiv preprint arXiv:2011.14503, 2020.\n[55] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7:625–641, 2019.\n[56] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[57] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),\npages 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n[58] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\n[59] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network\nfor image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5791–5800, 2020.\n13\n[60] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[61] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label\nsmoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3903–3911, 2020.\n[62] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for\nvideo inpainting. In European Conference on Computer Vision, pages 528–543. Springer, 2020.\n[63] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020.\n[64] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object detection\nwith adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.\n[65] Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, and Jiashi Feng. Neural epitome search\nfor architecture-agnostic network compression. In International Conference on Learning Representations,\n2019.\n[66] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit:\nTowards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[67] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong. End-to-end dense video\ncaptioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 8739–8748, 2018.\n[68] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n14\nA More implementation details\nA.1 Training hyper-parameters\nFor all the ablation experiments, we use the standard training recipe [56] that is used for reproducing\nthe ViT baselines. When comparing with other state-of-the-art (SOTA) methods, we adopt the\nadvanced training recipe as proposed in [27]. Their detailed conﬁgurations are shown in Tab. 8. We\ntrain all models with 8 NVIDIA Telsa-V100 GPUs.\nTable 8: Default training hyper-parameters for our experiments.\nH-param. Standard Advanced\nEpoch 300 300\nBatch size 256 512\nLR 5e-3 ·batch_size\n256 5e-3·batch_size\n512\nLR decay cosine cosine\nWeight decay 0.05 0.05\nWarmup epochs 5 5\nDropout 0 0\nStoch. Depth 0.1 0.1 ·#Blocks\n12\nMixUp 0.2 0.8\nCutMix 0 1.0\nErasing prob. 0.25 0.25\nRandAug 9/0.5 9/0.5\nA.2 Fine-tuning with larger image resolutions\nOn ImageNet, all models are trained from scratch with image resolution of 224 ×224. When\ncompare with other SOTA models, we ﬁne-tune the models with larger image resolutions using the\nsame ﬁne-tuning procedures as adopted in [48]. As larger image resolutions have more positional\nembeddings, we interpolate the positional encodings in the same manner as the one proposed in [19].\nA.3 Model conﬁgurations of Reﬁner-ViT\nAs discussed in [ 60], a deep and narrow neural network typically is more efﬁcient than a wide\nand shallow one. To verify this, we conduct a pair of experiments using these two architecture\nconﬁgurations respectively and the results are shown in Tab. 9. It is clearly observed that with\ncomparable classiﬁcation accuracy, a deep and narrow network takes 71% less number of parameters.\nTable 9: Comparison of the deep-narrow and shallow-wide architecture conﬁgurations.\nModel #Blocks Hidden dim #Heads Params Top-1 (%)\nReﬁned-ViT 12 768 12 86M 83.1\nReﬁned-ViT 16 384 12 25M 83.0\nThus, we design the architecture of Reﬁner-ViT based on the same deep and narrow network\narchitecture. The detailed conﬁgurations and the comparison with the ViT-base architecture are\nshown in Tab. 10.\nA.4 Feature evolving speed and feature similarity with CKA\nAs shown in Fig. 2 and Fig. 4 in the main paper, we compare the similarity between the intermediate\ntoken features at the output of each block and the ﬁnal layer, normalized by the similarity between\nthe ﬁrst layer’s output features and the ﬁnal output features as shown in Eqn. (2):\nFk = CKA(fk, fout)\nCKA(fin, fout), (2)\n15\nTable 10: Model architecture conﬁgurations.\nModel #Blocks Hidden dim #Head #Params Training Resolution\nReﬁned-ViT-Base 12 768 12 86M 224\nReﬁned-ViT-S 16 384 12 25M 224\nReﬁned-ViT-M 32 420 12 55M 224\nReﬁned-ViT-L 32 512 16 81M 224\nwhere fk denotes the token features at layer k, fin denotes the token features at the ﬁrst transformer\nblock and fout denotes the features at the ﬁnal output of the model. The metric CKA is calculated\nwith linear kernel function as proposed in [28]. The batch size is set to be 32. The similarity score at\nthe output of each block is an average of ten runs with randomly sampled image batches.\nB More experiments\nB.1 Performance improvements of reﬁner under different training techniques\nAs shown in DeiT [48], more complex augmentations and ﬁne-tuned training recipe could improve\nthe performance of ViTs signiﬁcantly without architecture changes. We run a set of experiments to\nshow that the improvements brought by reﬁner is orthogonal to the training recipes to some extent.\nWe select two training recipes for comparison. The ﬁrst one uses the searched learning rate schedule\nand data augmentation policy as proposed in [ 48]. The second one uses the more complicated\npatch processing as proposed in [ 60]. As shown in Tab. 11, under different training settings, the\nimprovements from the reﬁner are all signiﬁcant on ImageNet.\nTable 11: Performance gain from reﬁner under different training recipes.\nTraining techniques #Param. Original Top-1 (%) + Reﬁned-ViT\nBaseline (ViT-Base) 86M 79.3 81.2 ( +1.9)\n+ DeiT-Base ([48]) 86M 81.5 82.3 ( +0.8)\n+ More convs for patch embedding 86M 82.2 83.1 ( +0.9)\nB.2 Impacts of the attention heads\nAs discussed in [49], increasing the number of attention heads will reduce the embedding dimension\nper head and thus degrade the network performance. Differently, reﬁner expands the number of\nheads with a linear projection function. With the implementation of the linear projection, reﬁner will\nkeep each head’s embedding dimension unchanged. As shown in Tab. 12, the performance keeps\nincreasing with Reﬁned-ViT.\nTable 12: Ablation on the impact of the number of the heads for ViT-Small with 16 blocks and 384\nembedding dimension. We report top-1 accuracy on ImageNet. The accuracy for ViT-Small with\ndifferent head numbers are adopted from [49].\n# Heads ViT-Small Reﬁned-ViT\n8 79.9 82.5\n12 80.0 82.6\n16 80.0 82.8\nB.3 Impacts of the kernel size\nFor all the experiments in the main paper, the convolution kernel is set to be 3 ×3 by default. In Fig.\n6, we show the classiﬁcation results on ImageNet with different kernel size. It is observed that all\n16\n1 2 3 4 5 6 7\nKernel Size\n83.00\n83.25\n83.50\n83.75\n84.00\n84.25\n84.50\n84.75\n85.00Top-1 Acc.(%) on ImageNet \nRefiner-ViT-20B-512\nFigure 6: Top-1 classiﬁcation accuracy of Reﬁned-ViT with different local kernel sizes.\nkernels achieve much better accuracy than the baseline method (kernel size of 1) and the reﬁner with\nkernel size 3 achieves that highest accuracy. This further verify that attention reinforcement within\nthe local area could improve the performance of ViTs.\nB.4 Reﬁner makes the attention maps shareable\nTable 13: Ablation on the number of shared attention maps on ViT-16B with 384 embedding dimension.\n#Shared Blocks SA Reﬁner\n0 82 83.0\n1 77 83.0\n2 70 82.8\n3 NAN 82.6\nReﬁner enables the attention maps to capture some critical spatial relation among the tokens. To\nsee this, we compute the attention maps with Reﬁner and let the following a number of blocks to\nreuse such pre-computed attention maps. The results are given in Tab. 13. Interestingly we ﬁnd\nthat the attention maps after Reﬁner can be shared (reused) by several following SA blocks. Let the\ndirectly adjacent SA block reuse the reﬁned attention map from the previous block will maintain the\nmodel overall performance. Sharing the attention maps with the following two SA blocks only drop\nthe accuracy by 0.2%. In stark contrast, the attention maps from the vanilla SA are completely not\nshareable, leading the model performance to drop severely. This demonstrates Reﬁner is effective\nat extracting useful patterns and this ﬁnding is also in accordance with the one from a recent NLP\nstudy [47]. Therefore, across the experiments with Reﬁner, we shared the attention maps for 1\nadjacent transformer block.\nC Dataset description for NLP experiments\nGLUE benchmark [52] is a collection of nine natural language understanding tasks where the labels\nof the testing set is hidden and the researchers need to submit their predictions to the evaluation\nserver3 to obtain results on testing sets. In this work, we only present results of single-task setting for\nfair comparison. The nine tasks included in GLUE benchmark are described in details as below.\nMNLI The Multi-Genre Natural Language Inference Corpus [ 57] is a dataset of sentence pairs\nwith textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is\nto predict their relationships including ENTENTAILMENT , CONTRADICTION and NEUTRAL . The data\nis collected from ten distinct genres including both written and spoken English.\n3https://gluebenchmark.com\n17\nQNLI Question Natural Language Inference is a binary sentence pair classiﬁcation task, converted\nfrom The Stanford Question Answering Dataset [39] (a question-answering dataset). Each sample\nof QNLI contains a context sentence and a question. The task is to determine whether the context\nsentence contains the answer to the question.\nQQP The Quora Question Pairs dataset [ 10] is a collection of question pairs from Quora (a\ncommunity question-answering website). The task is to determine whether the two questions in a\npair are semantically equivalent.\nRTE Similar to MNLI, the Recognizing Textual Entailment (RTE) dataset is also a binary clas-\nsiﬁcation task, i.e., entailment and not entailment. It is from a series of annual textual entailment\nchallenges including RTE1 [12], RTE2 [23], RTE3 [20], and RTE5 [2].\nSST-2 The Stanford Sentiment Treebank [43] consists of sentences from movie reviews and human\nannotations of their sentiment. GLUE uses the two-way (POSITIVE /NEGATIVE ) class split.\nMRPC The Microsoft Research Paraphrase Corpus [ 17] contains data from online news that\nconsists of sentence pairs with human annotations regarding whether the sentences in the pair are\nsemantically equivalent.\nCoLA The Corpus of Linguistic Acceptability [55] is a binary single-sentence classiﬁcation dataset\ncontaining the examples annotated with whether it is a grammatical English sentence.\nSST-B The Semantic Textual Similarity Benchmark [7] is a collection of human-annotated sentence\npairs with a similarity score ranging from 1 to 5. The target is to to predict the scores with a given\nsentence.\nWNLI Winograd NLI [30] is a small natural language inference dataset. However, as highligthed\non the GLUE web page4, there are issues with the construction of it. Thus following the practice of\nprevious works (GPT [38] and BERT [29] etc.), we exclude this dataset for a fair comparison.\n4https://gluebenchmark.com/faq\n18",
  "topic": "Refining (metallurgy)",
  "concepts": [
    {
      "name": "Refining (metallurgy)",
      "score": 0.7651458382606506
    },
    {
      "name": "Transformer",
      "score": 0.6449585556983948
    },
    {
      "name": "Computer science",
      "score": 0.36301422119140625
    },
    {
      "name": "Engineering",
      "score": 0.3169987201690674
    },
    {
      "name": "Electrical engineering",
      "score": 0.16001486778259277
    },
    {
      "name": "Materials science",
      "score": 0.15387046337127686
    },
    {
      "name": "Metallurgy",
      "score": 0.09056016802787781
    },
    {
      "name": "Voltage",
      "score": 0.05606740713119507
    }
  ],
  "institutions": []
}