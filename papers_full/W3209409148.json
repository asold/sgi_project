{
  "title": "MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare",
  "url": "https://openalex.org/W3209409148",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3173949299",
      "name": "Ji, Shaoxiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2372027839",
      "name": "Zhang Tian-lin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ansari, Luna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108872081",
      "name": "Fu Jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745222512",
      "name": "Tiwari, Prayag",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250271477",
      "name": "Cambria, Erik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2807452501",
    "https://openalex.org/W2405042511",
    "https://openalex.org/W2121770572",
    "https://openalex.org/W3172915759",
    "https://openalex.org/W3005911073",
    "https://openalex.org/W3088335873",
    "https://openalex.org/W3095850336",
    "https://openalex.org/W3198450613",
    "https://openalex.org/W2252031683",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3005929844",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3177079527",
    "https://openalex.org/W2511501696",
    "https://openalex.org/W2981984641",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3176528293",
    "https://openalex.org/W3103163889",
    "https://openalex.org/W2897240328",
    "https://openalex.org/W2912581524",
    "https://openalex.org/W2927148761",
    "https://openalex.org/W3034674374",
    "https://openalex.org/W2763990719",
    "https://openalex.org/W3169527902",
    "https://openalex.org/W3034144394",
    "https://openalex.org/W2952402849",
    "https://openalex.org/W3091936762",
    "https://openalex.org/W2582664174",
    "https://openalex.org/W2890787190",
    "https://openalex.org/W3162081707",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2250301071",
    "https://openalex.org/W2987392802",
    "https://openalex.org/W2953413710",
    "https://openalex.org/W3046375318"
  ],
  "abstract": "Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domain-specific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and release two pretrained masked language models, i.e., MentalBERT and MentalRoBERTa, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.",
  "full_text": "MentalBERT:\nPublicly Available Pretrained Language Models for Mental Healthcare\nShaoxiong Ji†, Tianlin Zhang‡, Luna Ansari†, Jie Fu§, Prayag Tiwari†, and Erik Cambria¶\n† Aalto University, Finland ‡ The University of Manchester, UK\n§ Mila, Québec AI Institute, Canada ¶ Nanyang Technological University, Singapore\n{shaoxiong.ji; luna.ansari; prayag.tiwari}@aalto.fi\ntianlin.zhang@postgrad.manchester.ac.uk\nfujie@mila.quebec cambria@ntu.edu.sg\nAbstract\nMental health is a critical issue in modern so-\nciety, and mental disorders could sometimes\nturn to suicidal ideation without adequate treat-\nment. Early detection of mental disorders and\nsuicidal ideation from social content provides\na potential way for effective social interven-\ntion. Recent advances in pretrained contextual-\nized language representations have promoted\nthe development of several domain-speciﬁc\npretrained models and facilitated several down-\nstream applications. However, there are no\nexisting pretrained language models for men-\ntal healthcare. This paper trains and release\ntwo pretrained masked language models, i.e.,\nMentalBERT and MentalRoBERTa, to bene-\nﬁt machine learning for the mental healthcare\nresearch community. Besides, we evaluate\nour trained domain-speciﬁc models and sev-\neral variants of pretrained language models on\nseveral mental disorder detection benchmarks\nand demonstrate that language representations\npretrained in the target domain improve the\nperformance of mental health detection tasks.\n1 Introduction\nMental health is a global issue, especially severe\nin most developed countries and many emerging\nmarkets. According to the mental health action\nplan (2013 - 2020) from the World Health Organi-\nzation, 1 in 4 people worldwide suffer from mental\ndisorders to some extent. Moreover, 3 out of 4\npeople with severe mental disorders do not receive\ntreatment, worsening the problem. During some\nperiods like the pandemic, people struggle with\nmental health issues, and many may not get mental\nhealth practitioners’ help. Previous studies reveal\nthat suicide risk usually has a connection to mental\ndisorders (Windfuhr and Kapur, 2011). Partly due\nto severe mental disorders, 900,000 people com-\nmit suicide each year worldwide, making suicide\nthe second most common cause of death among\nthe young. Suicide attempters have been reported\nas suffering from mental disorders, with an inves-\ntigation on a shift from mental health to suicidal\nideation conducted by language and interactional\nmeasures (De Choudhury et al., 2016).\nEarly identiﬁcation is a practical approach to\nmental illness and suicidal ideation prevention. Ex-\ncept for traditional proactive screening, social me-\ndia is a good channel for mental health care. Social\nmedia platforms such as Reddit and Twitter provide\nanonymous space for users to discuss stigmatic top-\nics and self-report personal issues. Social content\nfrom users who wrote about mental health issues\nand posted suicidal ideation has been widely used\nto study mental health issues (e.g., Ji et al., 2018;\nTadesse et al., 2019). Machine learning-based de-\ntection techniques can empower healthcare workers\nin early detection and assessment to take an action\nof proactive prevention.\nRecent advances in deep learning facilitate the\ndevelopment of effective early detection meth-\nods (Ji et al., 2021a). A new trend in natural lan-\nguage processing (NLP), contextualized pretrained\nlanguage models, has attracted much attention for\nvarious text processing tasks. The seminal work\non a pretrained language model called BERT (De-\nvlin et al., 2019) utilizes bidirectional transformer-\nbased text encoders and trains the model on a large-\nscale corpus. With the success of BERT, several\ndomain-speciﬁc pretrained language models for\nlearning text representations have also been devel-\noped and released, such as biomedical BERT (Lee\net al., 2020) and clinical BERT (Alsentzer et al.,\n2019; Huang et al., 2019) for the biomedical and\nclinical domain, respectively.\nHowever, there are no pretrained language mod-\nels customized for the domain of mental healthcare.\nOur paper trains and releases two representative\nbidirectional masked language models, i.e., BERT\nand RoBERTa (Liu et al., 2019), with corpus col-\nlected from social forums for mental health discus-\nsion. The pretrained models in the mental health\narXiv:2110.15621v1  [cs.CL]  29 Oct 2021\ndomain are dubbed MentalBERT and Mental-\nRoBERTa. To our best knowledge, this work is the\nﬁrst to pre-train language models for mental health-\ncare. Besides, we conduct a comprehensive eval-\nuation on several mental health detection datasets\nwith pretrained language models in different do-\nmains. We release the pretrained MentalBERTs\nwith Huggingface’s model repository, available at\nhttps://huggingface.co/mental.\n2 Methods and Setup\nThis section introduces the language model pre-\ntraining technique and the pretraining corpus we\ncollected. We then present the downstream tasks\nthat we aim to solve by ﬁne-tuning the pretrained\nmodels, and describing the setup of language model\nﬁne-tuning. Note that we aim to provide publicly\navailable pretrained text embeddings as language\nresources and evaluate the usability in downstream\ntasks rather than propose novel pretraining tech-\nniques.\n2.1 Language Model Pretraining\nWe follow the standard pretraining protocols of\nBERT and RoBERTa with Huggingface’s Trans-\nformers framework (Wolf et al., 2020). These two\nmodels work in a bidirectional manner, and we fol-\nlow their mechanism and adopt the same loss of\nmasked language modeling during pretraining. We\nuse the base network architecture for both models.\nThe BERT model we use is base uncased, which\nis 12-layer, 768-hidden, and 12-heads and has\n110M parameters. For the pretraining of RoBERTa-\nbased MentalBERT, we apply the dynamic masking\nmechanism that converges slightly slower than the\nstatic masking. Instead of the domain-speciﬁc pre-\ntraining (Gu et al., 2020) that trains language mod-\nels from scratch, we adopt the training scheme simi-\nlar to the domain-adaptive pretraining (Gururangan\net al., 2020) that continues the pretraining in spe-\nciﬁc downstream domains. Speciﬁcally, we start\nthe training of language models from the check-\npoint of original BERT and RoBERTa. In this way,\nwe can utilize the learned knowledge from the gen-\neral domain and save computing resources, and\ncontinued pretraining makes the model adaptive to\nthe target domain of mental health.\nWe use four Nvidia Tesla v100 GPUs to train the\ntwo language models. The computing resources\nare also one of the main assets of this paper. We\nset the batch size to 16 per GPU, evaluate every\n1,000 steps, and train for 624,000 iterations. Train-\ning with four GPUs takes around eight days, i.e.,\naround 32 days with only one GPU.\n2.2 Pretraining Corpus\nWe collect our pretraining corpus from Reddit, an\nanonymous network of communities for discussion\namong people of similar interests. Focusing on\nthe mental health domain, we select several rele-\nvant subreddits (i.e., Reddit communities that have\na speciﬁc topic of interest) and crawl the users’\nposts. We do not collect user proﬁles when col-\nlecting the pretraining corpus, even though those\nproﬁles are publicly available. The selected mental\nhealth-related subreddits include “r/depression”,\n“r/SuicideWatch”, “r/Anxiety”, “r/offmychest”,\n“r/bipolar”, “r/mentalillness/”, and “r/mentalhealth”.\nEventually, we make the training corpus with a to-\ntal of 13,671,785 sentences.\n2.3 Downstream Task Fine-tuning\nWe apply the pretrained MentalBERT and Men-\ntalRoBERTa in binary mental disorder detection\nand multi-class mental disorder classiﬁcation of\nvarious mental disorders such as stress, anxiety,\nand depression. We ﬁne-tune the language mod-\nels in downstream tasks. Speciﬁcally, we use the\nembedding of the special token [CLS] of the last\nhidden layer as the ﬁnal feature of the input text.\nWe adopt the multilayer perceptron (MLP) with\nthe hyperbolic tangent activation function for the\nclassiﬁcation model. We set the learning rate of\nthe transformer text encoder to be 1e-05 and the\nlearning rate of classiﬁcation layers to be 3e-05.\nThe optimizer is Adam (Kingma and Ba, 2014).\n3 Results\n3.1 Datasets\nWe evaluate and compare mental disorder detection\nmethods on different datasets with various mental\ndisorders (e.g., depression, anxiety, and suicidal\nideation) collected from popular social platforms\n(e.g., Reddit and Twitter). Table 1 summarizes the\ndatasets used in this paper. We carefully choose\nthose benchmarks to cover a relatively wide range\nof mental health categories and social platforms.\nSome datasets do not provide a validation set. Thus,\nwe partition a small set from the original training\nset to make the validation set.\nDepression Depression is one of the most com-\nmon mental disorders discussed on many social\nTable 1: A summary of datasets. Note we hold out a portion of original training set as the validation set if the\noriginal dataset does not contain a validation set.\nCategory Platform Dataset train validation test\nAssorted Reddit SWMH (Ji et al., 2021a) 34,823 8,706 10,883\nDepression Reddit eRisk18 T1 (Losada and Crestani, 2016) 1,533 658 619\nDepression Reddit Depression_Reddit (Pirina and Çöltekin, 2018) 1,004 431 406\nDepression Reddit CLPsych15 (Coppersmith et al., 2015) 457 197 300\nStress Reddit Dreaddit (Turcan and McKeown, 2019) 2,270 568 715\nSuicide Reddit UMD (Shing et al., 2018) 993 249 490\nSuicide Twitter T-SID (Ji et al., 2021a) 3,072 768 960\nStress SMS-like SAD (Mauriello et al., 2021) 5,548 617 685\nplatforms. We take it as a representative to evalu-\nate the performance of different pretrained models.\nThe ﬁrst dataset for depression comes from the\nCLPsych 2015 Shared Task (Coppersmith et al.,\n2015)1. The ﬁrst task of CLPsych 2015 contains\nuser-generated posts from users with depression\non Twitter. The train partition consists of 327 de-\npression users, and the test data contains 150 de-\npression users. Note that there is an unknown data\nmissing issue in the dataset of the CLPsych 2015\nshared task. The second dataset used is from eRisk\nshared task 1 (Losada and Crestani, 2016), which\nis a public competition for early risk detection in\nhealth-related areas. The eRisk dataset contains\nposts from 2,810 users, where 1,370 users express\ndepression in their posts and 1,440 act as the con-\ntrol group without depression.\nPirina and Çöltekin (2018) collected additional\nsocial data form Reddit and combined them with\npreviously collected data to identify depression 2.\nWe term this dataset as Depression_Reddit in this\npaper.\nSuicidal Ideation We use data collected from\nReddit and Twitter to test the performance. Firstly,\nwe use the UMD Reddit Suicidality Dataset (Shing\net al., 2018) that has a total of 865 users in the sub-\nreddit of “SuicideWatch” in Reddit3. The raw data\nannotation labels the user posts with four levels\nof risks. We include the control users and trans-\nform the label space into three classes according\nto the level of risks. In addition to data from Red-\ndit, we also evaluate the performance of data col-\nlected from Twitter. We use the Twitter dataset\nwith tweets expressing suicidal ideation and nor-\n1http://www.cs.jhu.edu/~mdredze/\ndatasets/clpsych_shared_task_2015/\n2https://github.com/Inusette/\nIdentifying-depression\n3http://users.umiacs.umd.edu/~resnik/\numd_reddit_suicidality_dataset.html\nmal posts as the control group, which is collected\nby Ji et al. (2018, 2021a). We term this dataset as\nT-SID.\nOther Mental Disorders We also evaluate the\nperformance of classifying other mental disorders\nsuch as stress, anxiety, and bipolar. Dreaddit (Tur-\ncan and McKeown, 2019) is a dataset for stress\nanalysis with posts collected from ﬁve different fo-\nrums of Reddit4. Speciﬁcally, it considers three ma-\njor stressful topics, i.e., interpersonal conﬂict, men-\ntal illness, and ﬁnancial need, and collects posts\nfrom ten related subreddits, including some mental\nhealth domains such as anxiety and PTSD. This\ndataset consists of a total of 3,553 posts split into\ntrain and test sets. Many factors may cause stress.\nWe then use another dataset for recognizing every-\nday stressors called SAD, which contains 6,850\nSMS-like sentences (Mauriello et al., 2021). The\nSAD dataset derives nine stress factors from stress\nmanagement articles, chatbot-based conversation\nsystems, crowdsourcing, and web crawling. The\nspeciﬁc stressor categories include work, health,\nfatigue, or physical pain, ﬁnancial problem, emo-\ntional turmoil, school, everyday decision making,\nfamily issues, social relationships, and other un-\nspeciﬁed stressors. Lastly, we use a dataset called\nSWMH (Ji et al., 2021a) that contains Reddit posts\nwith various mental disorders, including stress,\nanxiety, bipolar, depression, and suicidal ideation.\nNote that this dataset uses weak labels during the\nannotation process.\n3.2 Baselines\nWe compare our pretrained language models for\nmental health with various existing pretrained\nmodels in different domains. They are BERT\nand RoBERTa pretrained with general corpus,\n4http://www.cs.columbia.edu/~eturcan/\ndata/dreaddit.zip\nBioBERT pretrained in the biomedical domain,\nand ClinicalBERT pretrained with clinical notes.\nNote that the aim of this paper is not to achieve the\nstate-of-the-art performance but to demonstrate the\nusability and evaluate the performance of our pre-\ntrained models, though we have achieved competi-\ntive performance in some datasets when compared\nwith the state of the art.\n3.3 Results and Discussion\nWe evaluate the model performance by comparing\nthe recall and F1 scores. Mental disorder detection\nis usually a task with unbalanced classes, leading\nto using the F1 score metric. It is also essential\nto reduce the false negatives, i.e., to ensure as few\ncases as possible that the detection model misses\npeople with mental disorders. Thus, we also report\nrecall scores.\nResults of Depression Detection We ﬁrst com-\npare the performance of depression detection. Ta-\nble 2 reports the results on three depression dataset\ncollected from Reddit. MentalRoBERTa archives\nthe best performance on eRisk and CLPsych\ndatasets, and MentalBERT is the second best model\non the Depression_Reddit dataset.\nResults of Classifying Other Mental Disorders\nWe then compare the performance of classifying\nother mental disorders and suicidal ideation. Ta-\nble 3 shows the performance on various datasets\nwith different mental disorder classiﬁcation tasks.\nIn T-SID, SAD, and Dreaddit, MentalRoBERTa is\nthe best model with the highest recall and F1 scores.\nThe MentalBERT has the highest F1 score in the\nUMD dataset, while its F1 score is not competi-\ntive to other models. While for the SWMH dataset\nwith several mental disorders, the MentalRoBERT\nobtained the best F1 score.\nDiscussion When comparing the domain-\nspeciﬁc pretrained models for mental health\nwith models pretrained with general corpora,\nMentalBERT and MentalRoBERTa gain better\nperformance in most cases. Domain-speciﬁc\npretraining in the biomedical or clinical domain\nturns out to be less helpful than pretraining on\nthe target domain of mental health. Those results\nshow that continued pretraining on the mental\nhealth domain improves prediction performance in\ndownstream tasks of mental health classiﬁcation.\n4 Related Work\nContextualized Text Embeddings Contextual-\nized embeddings have been intensively studied in\nNLP. Self-supervised large-scale pretraining facili-\ntates the learning of semantic and contextual infor-\nmation and beneﬁts various downstream applica-\ntions such as text classiﬁcation (Sun et al., 2019),\nsentiment analysis (Tang et al., 2020; Song et al.,\n2020) and relation extraction (Alt et al., 2019).\nThere are also many domain-speciﬁc variants of\npretrained contextualized text embeddings. Embed-\ndings in speciﬁc domains aim to encode domain-\nspeciﬁc information to boost the performance of a\nspeciﬁc domain. For example, BioBERT (Lee et al.,\n2020) pretrained the BERT model in the biomedi-\ncal domain using research articles from PubMed,\nwhich was applied to many biomedical tasks such\nas biomedical relation extraction and named en-\ntity recognition. ClinicalBERT (Alsentzer et al.,\n2019) used clinical notes as the pretraining cor-\npus to continue the pretraining of the BERT model.\nThose domain-speciﬁc variants also foster variable\ndownstream applications by ﬁne-tuning pretrained\nembeddings such as Lin et al. (2019) and Ji et al.\n(2020).\nNLP for Mental HealthcareMental healthcare\nresearch in social media is increasingly applying\nNLP techniques to capture users’ behavioral ten-\ndencies. Various methods are implemented for\nlabeling, i.e., identifying emotions, mood, and\nproﬁles that might indicate mental health prob-\nlems (Calvo et al., 2017). One of the most rep-\nresentative tasks is mental health detection that cat-\negorizes given social posts into different classes of\nmental disorders such as depression (Tadesse et al.,\n2019). Mental state understanding requires effec-\ntive feature representation learning and complex\nemotive processes. Resnik et al. (2013) applied\ntopic modeling, an unsupervised approach that re-\nduces the input of textual data feature space to a\nﬁxed number of topics to feature engineering in de-\npression detection. Feature engineering-based ma-\nchine learning method designs manual features and\nbuilds classiﬁers for mental health detection (Shatte\net al., 2019; Abd Rahman et al., 2020). Various\nfeatures such as sensor signals from personal de-\nvices (Mohr et al., 2017) and EEG signals (Gore\nand Rathi, 2019) have been applied. For detec-\ntion from textual data in particular, text features\ninclude word counts, TF-IDF (Campillo-Ageitos\nTable 2: Results of depression detection. The bold text represents for the best performance.\nModel eRisk T1 CLPsych Depression_Reddit\nRec. F1 Rec. F1 Rec. F1\nBERT 88.53 88.54 64.67 62.75 91.13 90.90\nRoBERTa 92.25 92.25 67.67 66.07 95.07 95.11\nBioBERT 79.16 78.86 65.67 65.50 91.13 90.98\nClinicalBERT 76.25 75.41 65.67 65.30 89.41 89.03\nMentalBERT 86.27 86.20 64.67 62.63 94.58 94.62\nMentalRoBERTa 93.38 93.38 70.33 69.71 94.33 94.23\nTable 3: Results of classifying other mental disorders including stress, anorexia, suicidal ideation. The bold text\nrepresents for the best performance.\nModel UMD T-SID SWMH SAD Dreaddit\nRec. F1 Rec. F1 Rec. F1 Rec. F1 Rec. F1\nBERT 61.63 58.01 88.44 88.51 69.78 70.46 62.77 62.72 78.46 78.26\nRoBERTa 59.39 60.26 88.75 88.76 70.89 72.03 66.86 67.53 80.56 80.56\nBioBERT 57.76 58.76 86.25 86.12 67.10 68.60 66.72 66.71 75.52 74.76\nClinicalBERT 58.78 58.74 85.31 85.39 67.05 68.16 62.34 61.25 76.36 76.25\nMentalBERT 64.08 58.26 88.65 88.61 69.87 71.11 67.45 67.34 80.28 80.04\nMentalRoBERTa 57.96 58.58 88.96 89.01 70.65 72.16 68.61 68.44 81.82 81.76\net al., 2021), topic features (Shickel et al., 2020)\nand sentiment traits (Yoo et al., 2019). Severe\nmental disorders but without intervention may lead\nto suicidal ideation (Windfuhr and Kapur, 2011).\nMany machine learning-based methods have been\napplied for suicidal ideation detection (Ji et al.,\n2021b).\nRecent work applies deep representation learn-\ning methods, which enable automatic feature learn-\ning to solve the early mental disorder identiﬁcation\ntask. Those methods typically build text embed-\ndings and feed the embeddings into neural architec-\ntures such as convolutional neural networks (Rao\net al., 2020), recurrent networks (Bouarara, 2021),\nself attention-based Transformers, hybrid architec-\ntures like CNN-LSTM (Kang et al., 2021) and more\nother deep learning architectures (Su et al., 2020).\nRecent works, e.g., Jiang et al. (2020), Martínez-\nCastaño et al. (2021) and Bucur et al. (2021),\nuse pretrained language models and ﬁne-tune the\nmodel for mental health tasks. However, there\nare no existing pretrained language models trained\nwith mental health-related text to beneﬁt the do-\nmain application directly.\n5 Conclusion and Future Work\nThis paper trains and releases two masked language\nmodels, i.e., MentalBERT and MentalRoBERTa,\non the domain data of mental health collected from\nthe Reddit social platform. This paper is the ﬁrst\nwork that trains domain-speciﬁc language models\nfor mental healthcare. Our pretrained models are\npublicly available and can be reused by the research\ncommunity. Besides, we conduct a comprehen-\nsive evaluation on the performance for downstream\nmental health detection tasks, including depression,\nstress, and suicidal ideation detection. Our empir-\nical results show that continued pretraining with\nmental health-related corpus can improve classiﬁ-\ncation performance.\nOur paper is a positive attempt to beneﬁt the\nresearch community by releasing the pretrained\nmodels for other practical studies and with the hope\nto facilitate some possible real-world applications\nto relieve people’s mental health issues. However,\nwe only focus on the English language in this study\nsince English corpora are relatively easy to obtain.\nIn the future work, we plan to collect multilingual\nmental health-related posts, especially those less\nstudied by the research community, and train a\nmultilingual language model to beneﬁt more people\nspeaking languages other than English.\nSocial Impact\nThe paper trains and releases masked language\nmodels for mental health to facilitate the automatic\ndetection of mental disorders in online social con-\ntent for non-clinical use. The models may help\nsocial workers ﬁnd potential individuals in need of\nearly prevention. However, the model predictions\nare not psychiatric diagnoses. We recommend any-\none who suffers from mental health issues to call\nthe local mental health helpline and seek profes-\nsional help if possible.\nData privacy is an important issue, and we try\nto minimize the privacy impact when using social\nposts for model training. During the data collection\nprocess, we only use anonymous posts that are man-\nifestly available to the public. We do not collect\nuser proﬁles even though they are also manifestly\npublic online. We have not attempted to identify\nthe anonymous users or interact with any anony-\nmous users. The collected data are stored securely\nwith password protection even though they are col-\nlected from the open web. There might also be\nsome bias, fairness, uncertainty, and interpretability\nissues during the data collection and model train-\ning. Evaluation of those issues is essential in future\nresearch.\nAcknowledgments\nThe authors would like to thank Philip Resnik for\nproviding the UMD Reddit Suicidality Dataset,\nMark Dredze for providing the dataset in the\nCLPsych 2015 shared task, and other researchers\nwho make their datasets publicly available. We\nacknowledge the computational resources provided\nby the Aalto Science-IT project. The authors wish\nto acknowledge CSC - IT Center for Science, Fin-\nland, for computational resources.\nReferences\nRohizah Abd Rahman, Khairuddin Omar, Shahrul Az-\nman Mohd Noah, Mohd Shahrul Nizam Mohd Da-\nnuri, and Mohammed Ali Al-Garadi. 2020. Ap-\nplication of machine learning methods in mental\nhealth detection: a systematic review. IEEE Access,\n8:183952–183964.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available\nClinical BERT Embeddings. In Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78.\nChristoph Alt, Marc Hübner, and Leonhard Hennig.\n2019. Fine-tuning pre-trained transformer language\nmodels to distantly supervised relation extraction.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1388–1398.\nHadj Ahmed Bouarara. 2021. Recurrent neural net-\nwork (rnn) to analyse mental behaviour in social me-\ndia. International Journal of Software Science and\nComputational Intelligence (IJSSCI), 13(3):1–11.\nAna-Maria Bucur, Adrian Cosma, and Liviu P Dinu.\n2021. Early risk detection of pathological gambling,\nself-harm and depression using bert. arXiv preprint\narXiv:2106.16175.\nRafael A Calvo, David N Milne, M Sazzad Hus-\nsain, and Helen Christensen. 2017. Natural lan-\nguage processing in mental health applications using\nnon-clinical texts. Natural Language Engineering,\n23(5):649–685.\nElena Campillo-Ageitos, Hermenegildo Fabregat,\nLourdes Araujo, and Juan Martinez-Romo. 2021.\nNLP-UNED at eRisk 2021: self-harm early risk\ndetection with TF-IDF and linguistic features.\nWorking Notes of CLEF, pages 21–24.\nGlen Coppersmith, Mark Dredze, Craig Harman,\nKristy Hollingshead, and Margaret Mitchell. 2015.\nClpsych 2015 shared task: Depression and ptsd on\ntwitter. In Proceedings of the 2nd Workshop on\nCLPsych, pages 31–39.\nMunmun De Choudhury, Emre Kiciman, Mark Dredze,\nGlen Coppersmith, and Mrinal Kumar. 2016. Dis-\ncovering shifts to suicidal ideation from mental\nhealth content in social media. In CHI, pages 2098–\n2110. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL-HLT.\nEla Gore and Sheetal Rathi. 2019. Surveying machine\nlearning algorithms on eeg signals data for mental\nhealth assessment. In 2019 IEEE Pune Section Inter-\nnational Conference (PuneCon), pages 1–6. IEEE.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedi-\ncal natural language processing. arXiv preprint\narXiv:2007.15779.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. ClinicalBERT: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nShaoxiong Ji, Xue Li, Zi Huang, and Erik Cambria.\n2021a. Suicidal ideation and mental disorder detec-\ntion with attentive relation networks. Neural Com-\nputing and Applications.\nShaoxiong Ji, Shirui Pan, Xue Li, Erik Cambria,\nGuodong Long, and Zi Huang. 2021b. Suicidal\nideation detection: A review of machine learning\nmethods and applications. IEEE Transactions on\nComputational Social Systems, 8:214–226.\nShaoxiong Ji, Celina Ping Yu, Sai-fu Fung, Shirui Pan,\nand Guodong Long. 2018. Supervised learning for\nsuicidal ideation detection in online user content.\nComplexity, 2018.\nZongcheng Ji, Qiang Wei, and Hua Xu. 2020. BERT-\nbased ranking for biomedical entity normalization.\nAMIA Summits on Translational Science Proceed-\nings, 2020:269.\nZheng Ping Jiang, Sarah Ita Levitan, Jonathan Zomick,\nand Julia Hirschberg. 2020. Detection of mental\nhealth from reddit via deep contextualized represen-\ntations. In Proceedings of the 11th International\nWorkshop on Health Text Mining and Information\nAnalysis, pages 147–156.\nMingu Kang, Siho Shin, Jaehyo Jung, and Youn Tae\nKim. 2021. Classiﬁcation of mental stress using\ncnn-lstm algorithms with electrocardiogram signals.\nJournal of Healthcare Engineering, 2021.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nMethod for Stochastic Optimization. arXiv preprint\narXiv:1412.6980.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nChen Lin, Timothy Miller, Dmitriy Dligach, Steven\nBethard, and Guergana Savova. 2019. A BERT-\nbased universal model for both within-and cross-\nsentence clinical temporal relation extraction. In\nProceedings of the 2nd Clinical Natural Language\nProcessing Workshop, pages 65–71.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDavid E Losada and Fabio Crestani. 2016. A test col-\nlection for research on depression and language use.\nIn International Conference of the Cross-Language\nEvaluation Forum for European Languages, pages\n28–39. Springer.\nRodrigo Martínez-Castaño, Amal Htait, Leif Az-\nzopardi, and Yashar Moshfeghi. 2021. BERT-\nBased Transformers for Early Detection of Men-\ntal Health Illnesses. In International Conference\nof the Cross-Language Evaluation Forum for Euro-\npean Languages, pages 189–200. Springer.\nMatthew Louis Mauriello, Thierry Lincoln, Grace Hon,\nDorien Simon, Dan Jurafsky, and Pablo Paredes.\n2021. SAD: A Stress Annotated Dataset for Rec-\nognizing Everyday Stressors in SMS-like Conversa-\ntional Systems. In Extended Abstracts of the 2021\nCHI Conference on Human Factors in Computing\nSystems, pages 1–7.\nDavid C Mohr, Mi Zhang, and Stephen M Schueller.\n2017. Personal sensing: understanding mental\nhealth using ubiquitous sensors and machine learn-\ning. Annual review of clinical psychology, 13:23–\n47.\nInna Pirina and Ça ˘grı Çöltekin. 2018. Identifying\ndepression on reddit: The effect of training data.\nIn Proceedings of the 2018 EMNLP Workshop\nSMM4H: The 3rd Social Media Mining for Health\nApplications Workshop & Shared Task, pages 9–12.\nGuozheng Rao, Yue Zhang, Li Zhang, Qing Cong,\nand Zhiyong Feng. 2020. MGL-CNN: A hierarchi-\ncal posts representations model for identifying de-\npressed individuals in online forums. IEEE Access,\n8:32395–32403.\nPhilip Resnik, Anderson Garron, and Rebecca Resnik.\n2013. Using topic modeling to improve prediction\nof neuroticism and depression in college students.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1348–1353.\nAdrian BR Shatte, Delyse M Hutchinson, and Saman-\ntha J Teague. 2019. Machine learning in mental\nhealth: a scoping review of methods and applica-\ntions. Psychological medicine, 49(9):1426–1448.\nBenjamin Shickel, Scott Siegel, Martin Heesacker,\nSherry Benton, and Parisa Rashidi. 2020. Automatic\ndetection and classiﬁcation of cognitive distortions\nin mental health text. In 2020 IEEE 20th Interna-\ntional Conference on Bioinformatics and Bioengi-\nneering (BIBE), pages 275–280. IEEE.\nHan-Chin Shing, Suraj Nair, Ayah Zirikly, Meir\nFriedenberg, Hal Daumé III, and Philip Resnik.\n2018. Expert, crowdsourced, and machine assess-\nment of suicide risk via online postings. In Proceed-\nings of the Fifth Workshop on CLPsych, pages 25–\n36.\nYouwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue\nLiu, and Tao Jiang. 2020. Utilizing BERT inter-\nmediate layers for aspect based sentiment analy-\nsis and natural language inference. arXiv preprint\narXiv:2002.04815.\nChang Su, Zhenxing Xu, Jyotishman Pathak, and Fei\nWang. 2020. Deep learning in mental health out-\ncome research: a scoping review. Translational Psy-\nchiatry, 10(1):1–26.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Computa-\ntional Linguistics, pages 194–206. Springer.\nMichael M Tadesse, Hongfei Lin, Bo Xu, and Liang\nYang. 2019. Detection of depression-related posts in\nreddit social media forum. IEEE Access, 7:44883–\n44893.\nTiancheng Tang, Xinhuai Tang, and Tianyi Yuan. 2020.\nFine-Tuning BERT for Multi-Label Sentiment Anal-\nysis in Unbalanced Code-Switching Text. IEEE Ac-\ncess, 8:193248–193256.\nElsbeth Turcan and Kathleen McKeown. 2019. Dread-\ndit: A Reddit Dataset for Stress Analysis in Social\nMedia. In Proceedings of the Tenth International\nWorkshop on Health Text Mining and Information\nAnalysis (LOUHI 2019), pages 97–107.\nKirsten Windfuhr and Navneet Kapur. 2011. Suicide\nand mental illness: a clinical review of 15 years ﬁnd-\nings from the uk national conﬁdential inquiry into\nsuicide. British Medical Bulletin, 100(1):101–121.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nMinjoo Yoo, Sangwon Lee, and Taehyun Ha. 2019.\nSemantic network analysis for understanding user\nexperiences of bipolar and depressive disorders on\nreddit. Information Processing & Management,\n56(4):1565–1575.",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.6661685705184937
    },
    {
      "name": "Computer science",
      "score": 0.57433021068573
    },
    {
      "name": "Language model",
      "score": 0.5479861497879028
    },
    {
      "name": "Mental healthcare",
      "score": 0.5298872590065002
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5147126317024231
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.46635428071022034
    },
    {
      "name": "Psychology",
      "score": 0.4050467312335968
    },
    {
      "name": "Natural language processing",
      "score": 0.3136642575263977
    },
    {
      "name": "Psychiatry",
      "score": 0.1956232190132141
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}