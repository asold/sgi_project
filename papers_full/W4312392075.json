{
  "title": "Robust Human Motion Forecasting using Transformer-based Model",
  "url": "https://openalex.org/W4312392075",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013385561",
      "name": "Esteve Valls Mascaro",
      "affiliations": [
        "TU Wien"
      ]
    },
    {
      "id": "https://openalex.org/A5100622671",
      "name": "Shuo Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053725843",
      "name": "Hyemin Ahn",
      "affiliations": [
        "Ulsan National Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5088081800",
      "name": "Dongheui Lee",
      "affiliations": [
        "TU Wien",
        "Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964203186",
    "https://openalex.org/W2963165299",
    "https://openalex.org/W2896588340",
    "https://openalex.org/W2963548793",
    "https://openalex.org/W6680369585",
    "https://openalex.org/W3101151469",
    "https://openalex.org/W3045246204",
    "https://openalex.org/W2983925976",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3036644940",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2993797559",
    "https://openalex.org/W1735317348",
    "https://openalex.org/W2606517404",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2883585959",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W6784233830",
    "https://openalex.org/W3002101995",
    "https://openalex.org/W3215052027",
    "https://openalex.org/W2138816964",
    "https://openalex.org/W3109717189",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3093144786",
    "https://openalex.org/W4302571916"
  ],
  "abstract": "Comprehending human motion is a fundamental challenge for developing Human-Robot Collaborative applications. Computer vision researchers have addressed this field by only focusing on reducing error in predictions, but not taking into account the requirements to facilitate its implementation in robots. In this paper, we propose a new model based on&#13;\\nTransformer that simultaneously deals with the real time 3D human motion forecasting in the short and long term. Our 2-Channel Transformer (2CH-TR) is able to efficiently exploit the spatio-temporal information of a shortly observed sequence (400ms) and generates a competitive accuracy against the current state-of-the-art. 2CH-TR stands out for the efficient performance of the Transformer, being lighter and faster than its competitors. In addition, our model is tested in conditions where the human motion is severely occluded, demonstrating its robustness in reconstructing and predicting 3D human motion in a highly noisy environment. Our experiment results show that the proposed 2CH-TR outperforms the ST-Transformer, which is another state-of-the-art model based on the Transformer, in terms of reconstruction and prediction under the same conditions of input prefix. Our model reduces in 8.89% the mean squared error of ST-Transformer in short-term prediction, and 2.57% in long-term prediction in Human3.6M dataset with 400ms input prefix.",
  "full_text": "Robust Human Motion Forecasting using Transformer-based Model\nEsteve Valls Mascaro∗ 1, Shuo Ma ∗ 2, Hyemin Ahn 3, and Dongheui Lee 1,4\n1Autonomous Systems, Technische Universit ¨at Wien (TU Wien), Austria\n2Human-centered Assisitve Robotics, Technische Universit ¨at M ¨unchen (TUM), Germany\n3Artificial Intelligence Graduate School, Ulsan National Institute of Science and Technology (UNIST), Korea\n4Institute of Robotics and Mechatronics, German Aerospace Center (DLR), Germany\nAbstract— Comprehending human motion is a fundamental\nchallenge for developing Human-Robot Collaborative applica-\ntions. Computer vision researchers have addressed this field by\nonly focusing on reducing error in predictions, but not taking\ninto account the requirements to facilitate its implementation\nin robots. In this paper, we propose a new model based on\nTransformer that simultaneously deals with the real time 3D\nhuman motion forecasting in the short and long term. Our 2-\nChannel Transformer (2CH-TR) is able to efficiently exploit\nthe spatio-temporal information of a shortly observed sequence\n(400ms) and generates a competitive accuracy against the\ncurrent state-of-the-art. 2CH-TR stands out for the efficient\nperformance of the Transformer, being lighter and faster than\nits competitors. In addition, our model is tested in conditions\nwhere the human motion is severely occluded, demonstrating its\nrobustness in reconstructing and predicting 3D human motion\nin a highly noisy environment. Our experiment results show that\nthe proposed 2CH-TR outperforms the ST-Transformer, which\nis another state-of-the-art model based on the Transformer, in\nterms of reconstruction and prediction under the same condi-\ntions of input prefix. Our model reduces in 8.89% the mean\nsquared error of ST-Transformer in short-term prediction, and\n2.57% in long-term prediction in Human3.6M dataset with\n400ms input prefix. Visit our website here.\nI. I NTRODUCTION\nHumans have the capacity to forecast future states of\naffairs based on own-constructed models of physical and\nsocio-cultural systems. This ability is developed in childhood\nthrough observation and active participation in society. For\ninstance, humans are able to anticipate the movement inten-\ntion of the others and act accordingly to perform a given task\nefficiently. This capacity can even work in conditions where\nthe view of the other people is partially occluded.\nFor robots to coexist with humans, it is also crucial to\nsuccessfully anticipate nearby human’s future movement in\nreal time, even though the view is partially occluded. Then,\nthe robot can adapt its behaviour accordingly to assist the\nhuman. Regarding this, 3D Human Motion Forecasting is the\nresearch field aimed at predicting the human’s future full-\nbody 3D trajectory based on past observations. As shown\nin Fig. 1, the goal of this research is to generate a possible\nsequence of future 3D actions based on the short observation\nof the human body. Then, it would be possible for robots\nto plan its motion in advance, so that natural co-existence\n∗These authors equally contributed to this work.\nThis work is funded by Marie Sklodowska-Curie Action Horizon 2020\n(Grant agreement No. 955778) for project ’Personalized Robotics as Service\nOriented Applications’ (PERSEO).\nFig. 1. An overview of 3D human motion forecasting in occluded\nenvironments. The red lines are the observed 3D skeletons projected into the\nimage, while the blue lines consist of random occluded limbs to test model’s\n3D pose-reconstruction capacity. Finally, the green skeletons represent the\npredicted human pose sequence in the near future.\nwith humans can be realized. To do this, robots need a\ncomputationally efficient algorithm that can operate in real-\ntime. However, recently there is a general tendency in the\ncomputer vision research community that larger and heavier\nmodels are preferred, thus hindering their applicability in\nrobotics. Therefore, our paper proposes a 3D human mo-\ntion forecasting model that stands out for being faster and\nlighter than the state-of-the-art, with a similar or even higher\nperformance in the very short term (i.e. 400ms).\n3D human motion forecasting is a long-standing challenge\nthat has been addressed by exploiting the spatial-temporal\ndependencies in the observed skeletons. However, before\nusing spatial and temporal information together, initial works\nfocused only on the influence of temporal history in motion\nto forecast future human poses. They used recurrent neural\nnetworks (RNNs) [1], [2] to model dependencies between the\nskeletons in time and allowed the propagation of information\nfor the short- and long-term human motion forecasting. How-\never, these auto-regressive models accumulated an error over\ntime so that the result eventually collapsed into unrealistic\nhuman poses as argued in [3], [4].\nTo avoid forecasting unfeasible poses, it is necessary for\nmodels to also understand the spatial dependencies between\ndifferent parts of the human skeleton. Consequently, several\nmethods [5], [6], [7] attempted to exploit the correlation\nbetween different joints or limbs while conserving time\ndependency. This spatial-temporal approach can be observed\nin Fig. 2, where the dependencies in space (orange) and time\n(blue) of a given joint are described through a graph. This\ngraph-based approach was proposed as a model architecture\nto exploit the natural structure of the kinematic tree. For\ninstance, DCT-GCN [8] encoded the temporal information\nin feed-forward networks through discrete cosine transfor-\nmation (DCT) and captured the spatial component of human\nmovement through a learnable graph convolution network\narXiv:2302.08274v3  [cs.CV]  8 Apr 2024\nFig. 2. Spatio-temporal graph of joint dependencies for human motion. The\nblue arrows refer to temporal relationships between the same joint param-\neters in different frames. The orange arrows imply the spatial relationship\nbetween joints in the same frame.\n(GCN). However, this approach failed in modelling diverse\nlong-sequence as relied on fixed DCT coefficients. Motivated\nby the great advances of the Transformer model in language\nmodelling [9], [10], and to improve exploiting long-term\ndependencies, ST-Transformer [11] was also proposed to\ncapture this space-time duality of a human body using the\nself-attention mechanism [12].\nHowever, none of these methods focuses on the application\nof human motion forecasting in robotics. DCT-GCN [8] dealt\nwith different models for short-term and long-term prediction\nof 3D joint trajectory, and also lacked in forecasting global\nrotation of the human while moving. Then, robots could not\npredict, for instance, towards where the human was walking.\nST-Transformer [11], in addition, needed a 2-second input\nsequence to produce a prediction, thus depending on longer\nobservations and increasing the computational resources of\nthe model in terms of size and time. To sum up, none of these\nexisting models is feasible for stand-alone implementation\nfor robotics working in real world.\nMoreover, to fairly assess the forecasting capacity of\nrobots and facilitate its incorporation in real environments,\nthe motion forecasting should also be tested in noisy and\nstrongly occluded situations. Robustness to occlusions or\nnoise in observed human skeleton is essential in real ap-\nplication where the 3D human motion forecasting is based\non the results obtained from 3D human skeleton estimation.\nUnlike previous works mentioned above, our work also\nstudies the reconstruction and prediction capacity of models\nagainst different types of strong occlusions in the observed\ninput sequence. Our proposed model, named as 2 Channel-\nTransformer (2CH-TR), allows to cope with high levels\nof occlusions through its independence in 2 channels, and\npromotes the 3D motion forecasting, taking into account its\napplicability in the field of robotics.\nTo extensively show the effectiveness of the proposed\nmodel, we conduct a quantitative comparison experiment\nwith the state-of-the-art models based on the Human3.6M\nDataset [13]. Results show that our 2CH-TR obtains com-\npetitive results with state-of-the-art model and outperforms\nTransformer-based approaches in all time horizons. Under\nthe same prefix input length (400ms), our work reduces ST-\nTransformer’s mean squared error in 8.89% for short-term\nforecasting, and 2.57% for long-term forecasting. In order\nto test the model performance in real scenario, qualitative\nresults over a real video on our own data set is also reported.\nOur contributions can be summarized as follows: (i) we\npropose a light and fast model for efficient 3D human\nmotion forecasting, suitable for robotic applications; (ii)\nwe construct a model that deals with short- and long-term\npredictions (from 0 to 1000ms) in a single time (no autor-\nregressive method) while working with simple input prefix\npattern (400ms) ; (iii) to consider the noisy environment in\nthe real world, we also study the influence of occlusions\nand show that the proposed 2CH-TR can adequately estimate\nthe complete human skeleton successfully even with severe\nocclusions.\nII. R ELATED WORK\nA. Recurrent-based approach\nRecurrent neural networks (RNNs) [1], [2], [14] and\nlong short-term memory (LSTM) [15] were the dominant\narchitectures for modeling temporal dependencies between\nthe human skeletons. Jain et. al. explicitly emphasized the\nimportance of understanding the human body structure to\nexploit relationships between limbs more effectively through\nStructured-RNN (S-RNN) [2]. This work attempted to better\nexplore inter- and intra-relationships between each part of the\nskeleton, focusing on spine-arms and spine-legs correlation.\nSince these recurrent models encoded the history of mo-\ntion in a hidden state of fixed size, an error could be\npropagated through time and long-term dependencies could\nnot be exploited efficiently. To solve these problems, various\nworks applied data augmentation tricks through Gaussian\nnoise to the inputs [14], [16] or used adversarial losses\n[3], [4] to tackle this vanishing of information, but their\nlong-term predictions still collapsed into non-plausible poses.\nCompared to this, our work takes advantage of self-attention\nmechanisms, that avoid compressing all historical informa-\ntion into a fixed-length hidden state, and have an ability to\nattend every historical pose at every time step. Therefore,\nour predictions are made by weighting which states are\nmore informative at every step in the sequence, excelling in\nexploiting long-term dependencies and tackling the vanishing\ngradient challenge of recurrent networks.\nB. Spatio-Temporal Modeling\nState-of-the-art approaches tackled 3D human motion\nforecasting by leveraging spatio-temporal dependencies.\nThen, graph structures were proposed as spatial represen-\ntations of human’s body to exploit the natural structure of\nthe kinematic tree [17]. However, they failed when capturing\nhuman motions that required synchronization between limbs,\nsuch as the periodic movement between arms and legs when\nwalking. Consequently, these non-apparent dependencies in\nkinematic-tree structures needed to be learned based on the\ndata. To tackle that, graph convolutional networks (GCN)\nemerged to adaptively capture these connections necessary\nfor motion forecasting [7], [8]. State-of-the-art DCT-GCN\n[8] model leveraged GCN to encode the joint relationships,\nand adopted discrete cosine transforms (DCT) to capture the\nsmoothness of motion in time. However, their work only at-\ntempted to predict joint motion, failing in forecasting global\nFig. 3. Architecture of 2-Channel Transformer (2CH-TR). The observed skeleton motion sequence X is projected independently for each channel into an\nembedding space ( ES and ET ) and then positional encoding is injected. Each embedding is fed into L stacked attention layers that extracts dependencies\nbetween the sequence using multi-head attention. Finally, each embedding ( ˆES and ˆET ) is decoded and projected back to skeleton sequences. Future\nposes ( ˆXpred) are then the result of summing the output of each channel ( ˆXS and ˆXT ) with the residual connection X from input to output.\nrotation of the humans, and therefore not being appropriate\nfor its application in the robotics field. Moreover, [8] trained\ndifferent models for each time horizon (short and long-\nterm predictions), thus increasing the required computational\nresources for predicting in both scenarios.\nInspired by recent advances of self-attention mechanisms\nin natural language processing (NLP) [9], [10], Emre et. al.\nintroduced the Transformer-based architecture into human\nmotion forecasting and emphasized the concepts of time\nand space by designing a spatio-temporal Transformer (ST-\nTransformer) [11]. Their aim was to take advantage of the\nlow inductive bias of the transformer-based architectures\nshown in language modelling and nowadays also in com-\nputer vision [18]. The success of this architecture had two\nmain reasons. First, transformers exploited capturing both\nshort and long term dependencies by using the positional\nencoding [12]. Second, by adding Multi-Head Attention [12],\nthe model was able to extract richer dependencies from\nthe observed sequence by attending in parallel to different\nrepresentation sub-spaces.\nOur proposed model is inspired by the efficiency of\nST-Transformer but differs in essence. For example, while\nST-Transformer deals with joint-wise vectors formed by a\nrotation matrix representation as inputs, and over-emphasized\nrelationships between these joints, we rather propose a\nmodel that autonomously learns these relationships from all\nflatted skeleton parameters, without previous clustering into\njoints. Our temporal channel explores these relationships in\neach time frame, while the spatial channel identifies intra-\nframed relationships of the skeleton. Total decoupling of our\ntwo channels allows to boost the robustness of the model\nwhile simplifying the structure. Moreover, our work is also\ndifferent in terms of input pattern. ST-Transformer requires\n2-seconds human motion observations (50 frames at 25 FPS)\nas an input to produce 1-second prediction in Human3.6M\ndataset, while ours reduce it by to 20% the needed input\nlength (400ms, 10 frames at 25 FPS) for the same time-\nlength prediction.\nC. Skeleton Recovery\nApplication of stand-alone state-of-the-art 3D human mo-\ntion forecasting in a real environment might be unfeasible if\nthere are numerous occlusions or noise in observed skeleton\ndata. Existing works do not mention about the capacity of\ntheir models to work under occluded environments.\nHowever, this topic is indeed researched in the field of\npose estimation, by recovering the occlusion when estimat-\ning the body pose. For instance, Guo et. al. employed a\nfully convolutional network (FCN) to enable the regression\nbetween occluded and complete distance matrix [19] for\npose estimation. Then, Cao et. al. with OpenPose [20] used\nPart Affinity Fields (PAFs) to perform skeleton estimation\nin the presence of human occlusion, but this was limited\nto 2D poses. Cheng Yu et. al. worked with occlusion-\naware convolutional neural networks, named Cylinder Man\nModel [21], to mitigate the effect of occlusions. All of these\nmodels dealt with occlusion and noise recovery in the field\nof skeleton estimation, but none in the 3D human motion\nforecasting.\nFinally, Ruiz et. al. designed a model based on Generative\nAdversarial Networks (GAN), named as MotionGAN [22],\nand formulate 3D motion forecasting as an in-painting prob-\nlem by totally masking the future frames to predict. Then,\nMotionGAN attempted both human motion reconstruction\nand forecasting independently, but never predict the future\nmotion based on a occluded observation. Our work evaluates\nthe capacity of 2CH-TR to forecast the future, also when the\nobserved input is partially occluded,\nTo the best of our knowledge, our work proposes a new\nline of research based on the evaluation and improvement\nof 3D human motion forecasting in highly occluded en-\nvironments. We claim the importance of our investigation\nto facilitate the future implementation in robotic-oriented\nscenarios.\nIII. M ETHOD\nHuman motion forecasting needs to exploit spatio-\ntemporal dependencies to generate plausible future poses.\nThe encoder-decoder structure inherited from Transformers\nis proposed in our 2CH-TR, inspired by [11]. Fig. 3 shows\nthe overview of the proposed architecture, that clearly repre-\nsents the independence between our spatial channel (in pink\nlines) and temporal channel (in light blue lines).\nA. Problem Formulation\nLet xt = [xt,1, ··· , xt,P ] ∈ RP denote our skeleton\nparameter at time frame t, that defines a set of human joints\nFig. 4. Input pattern representation for our 2CH-TR, with N = 10poses\nobserved by the model and T′ = 25future poses to be predicted. In the\nprefix, last T′ poses are repeated from last observed pose, so that the\nestimation only focuses on forecasting the difference between the future\npose and final pose.\nFig. 5. Temporal channel mechanism to exploit relationships of P\nskeleton parameters between N frames. Attention is used to capture time\ndependencies in the projected embedding space. For simplification in the\nvisualization, only a historical of T = 2poses (x1 and x2) are used.\nin axis-angle representations. Here, xt,1, ··· , xt,3 consist on\nthe global rotation information of the human body, so that\nour model can also learn how to handle orientation. Given\nan observed motion sequence X1:N , named as prefix, we\nreplicate last pose xN for T′ times to generate a prediction of\nfuture T′ frames, obtaining a final sequence T = N +T′, as\nit is described in Fig. 4. This padding pattern diminishes the\ncomplexity of the model as it only needs to learn the variation\nof each skeleton parameter based on the lastly observed pose.\nThen, the final motion sequence is represented as a matrix\nX = [ x1, ··· , xT ] ∈ RT×P , where T represents the\nnumber of time frames in the sequence and P indicates the\nnumber of skeleton parameters. Unlike [11], which classifies\nand over-emphasizes relationships among different joints,\nour 2CH-TR exploits all temporal and spatial dependencies\nindependently as a whole, projecting the prefix sequence into\nan independent embedding for each of the 2 channels ( ES\nand ET ) and only coupling them for the final result. Our\nmotivation is to decompose the contribution of each channel\nuntil the last stage to give robustness and improve short-\nand long-term predictions. Compared to [11], our approach\nprovides higher performance with lower number of attention\nlayers L and using only 20% of input prefix N, which also\nreduces the dimension of the model. Moreover, our model\ntakes advantage of the encoder-decoder structure to generate\nthe whole sequence in a single prediction, avoiding frame-\nby-frame prediction as in [11], thus reducing inference time.\nB. Temporal Channel\nAs shown in Figure 5, the skeleton parameters used to\ndescribe the human pose are projected from the P dimension\nto the D dimension (D >> P) via the temporal encoder to\nlearn the context of the timeline for each dimension. Then,\nthe embedding ET ∈ RT×D is passed into the temporal\ntransformer after being ordered by sinusoidal positional en-\ncoding. Multi-head attention (MHA) used in this Transformer\nblock jointly leverages the relationship of the same joint\nin all its time-history. Each attention head head(h)\nT , where\nh ∈ (1, ··· , H), linearly projects the query (Q), key (K) and\nvalue (V ) obtained from embedding ET by employing three\ndifferent learnable weight matrices as shown in equation\n(1), where Q(h)\nT , K(h)\nT , V(h)\nT ∈ RT×F , F = D/H, refer\nto the query, key and value matrix of head(h)\nT in temporal\ntransformer block, respectively. W(Q,h)\nT , W(K,h)\nT , W(V,h)\nT ∈\nRD×F indicate the learnable weight matrices for head(h)\nT .\nQ(h)\nT = ET W(Q,h)\nT\nK(h)\nT = ET W(K,h)\nT , h ∈ (1, ··· , H)\nV (h)\nT = ET W(V,h)\nT\n, (1)\nFor each head of the Transformer block, the applied\nattention mechanism is shown in equation (2). The relation-\nship between any two time points can be discovered in the\nattention matrix AT ∈ RT×T . Besides, a temporal mask\nM ∈ RT×T is applied to guarantee that future information\ncannot be leaked to the past.\nhead(h)\nT = softmax(Q(h)\nT K(h)T\nT√\nD\n+ M)V = A(h)\nT V (h)\nT , (2)\nThe prediction is projected by concatenating of all heads,\nas shown in equation (3), where W(O)\nT ∈ RHF ×D denotes\nthe MHA learnable matrix. By L temporal-attention block\nstacking, the prediction performance can be strengthened.\nˆET = [head(1)\nT , ··· , head(H)\nT ]W(O)\nT , (3)\nFig. 5 explains how our temporal channel works with a\nsimplified example, where the input sequence is assumed\nof only 2 time frames ( X ∈ R2×P ). In temporal channel,\nwe mainly focus on the relationship between each frame,\ni.e, the relationship between x1 and x2. After the temporal\nencoder, the temporal embedding ET ∈ R2×D multiplies\nwith three different weight matrices to obtain query, key,\nand value matrices. Each row of these matrix represents\none frame. When x1 is presented as a query, the self-\nattention compares all frames in the sequence as keys to\nx1 and generates a correlation or attention score. A larger\nscore represents a higher degree of correlation. Then, this\nscore is normalized and multiplied with the value vector to\nget the final output, which encodes the importance of the\ndependency. To this end, temporal channel can learn the joint\nrelationships between each time-frame.\nC. Spatial Channel\nWhile the temporal channel efficiently captures the evo-\nlution of parameters in time, a spatial channel is able to\nunderstand the underlying dependencies among the skeleton\nparameters at each time horizon, to represent the plausible\nposes of a human. For the spatial channel, as described in\nFig. 6. Spatial Channel projects T dimension to D embeddings to focus\non the relationships between the parameters at each time horizon. Then,\nattention is applied to capture dependencies of skeleton parts at each pose.\nFor simplification, only T = 2historical poses ( x1 and x2) are used.\nFig. 6, the skeleton parameter size P is preserved and the\nsequence length T is projected into D dimension, where\nX − →ES : RP×T − →RP×D. Similar to the temporal\nTransformer, the spatial Transformer also has multi-head at-\ntention and 3 different learnable weight matrices are utilized\nto linearly project query, key and value in each head. Since\nits purpose is to explore spatial relationships, the mask M is\nno needed in this case. When applying attention, the spatial\nattention matrix AS ∈ RP×P interprets the relationship of\nevery parameter of the entire skeleton, thus learns the spatial\nstructure of the skeleton under different movements. Through\nthe stack of L spatial-attention blocks, the model can predict\nthe pose under each frame more accurately. Finally, the\noutput embedded representation ˆES is fed into the spatial\ndecoder and permuted, obtaining the result ˆXS. The whole\nprocess is similar to the Temporal Channel ( T) expressed by\nequation (1), (2), (3), but using the S notation for the Spatial\nChannel. The resulting equations show the use of the spatial\nweight matrices W(Q,h)\nS , W(K,h)\nS , W(V,h)\nS , W(O)\nS , with same\ndimensions as in temporal channel, and Q(h)\nS , K(h)\nS , V(h)\nS ∈\nRP×F , F= D/H encoded now in the space.\nFinally, results from two channels’ result ˆXT and ˆXS\nare summed at the end to obtain the motion prediction ˆX.\nAfterwards, the input and output of the model are connected\nby a residual connection to diminish the possibility of\ngradient explosion when the model depth is too large.\nIV. E XPERIMENTS\nA. Datasets\nHuman3.6M. H3.6M [13] includes 3.6 million 3D human\nposes from high-resolution videos of 7 subjects performing\n15 different actions such as walking, eating and smoking.\nBody skeleton of each subject is represented by P = 99\nskeleton parameters which include the global rotation and\ntranslation of the motion. Global rotation is crucial for robots\nto understand the intention of humans when performing an\naction. For a fair comparison with previous works [2], [8],\n[11], videos are down-sampled to 25 frames per second and\nthe test is performed on the same sequence of the subject 5.\nTesting Scenario. More realistic human motion scenarios\nare used for demonstrating the effectiveness of our model\nin the qualitative results. Observation of prefix poses are\nobtained through FrankMocap [23], thus realizing real-world\napplication.\nFig. 7. Visualization of randomized occlusions for observed prefix motion.\nIn this example, 80% of data is missing (black colour denotes occluded\ndata).\nB. Evaluation Metrics and Baselines\nMetrics. Following the standard evaluation protocols, we\nreport the mean squared error (MSE) metric, as shown\nin equation (4), between all the predicted ˆpi and ground-\ntruth ˆpi joint angles in Euler angle representation, where\ni ∈ (1, ··· , N), and N is the number of joints used to\ndescribe the human poses.\nerror(Xtarg, ˆXpred) =\nNX\ni=1\n||pi − ˆpi||2 (4)\nBaselines. We compare 2CH-TR with state-of-the-art\nmethods S-RNN [2], DCT-GCN [8] and ST-Transformer\n[11] in Human3.6M dataset. We implement their work with\noriginal code provided by the authors and evaluate the\nmodels under the same condition for fair comparison. Details\nof how existing models are trained are as follows. DCT-GCN\nwas trained independently for a short- and long-term joint\nprediction, while we propose a stand-alone model that deals\nsimultaneously with both time horizons to reduce computa-\ntional resources and to fasten the inference. Moreover, 2CH-\nTR also predicts global rotation of the human, apart from\njoint trajectories, to capture the orientation of real motion.\nST-Transformer needs the input prefix sequence of 2 seconds\nfor 1 second prediction. Compared to this, we manage the\nsame-length prediction only with 400ms prefix sequence,\nso that the complexity of our approach can be reduced by\nrelying on less information (and also reduces the dimension\nof the architecture, being more efficient).\nC. Skeleton Occlusion and Recovery\nThe motivation behind this paper consists of building a\nmodel applicable in real-world robotics. To our knowledge,\nthe feasibility of this task consists of evaluating four impor-\ntant concerns: (i) capacity of the model to perform short-\nand long-term motion forecasting in ideal scenarios; (ii)\nperformance of the model when tested in noisy scenarios\n(i.e. occlusions in prefix sequences); (iii) efficiency of the\nalgorithm regarding the inference time; (iv) lightness of the\nmodel for usage in a real robot without compromising its\nhardware capacities.\nTo reproduce scenarios of noisy prefix observations, we\npropose two different types of occlusions that will lead\nour following evaluation. First, as shown in upper-side of\nFig. 7 we defined Time Consistent Occlusions to reproduce\nrandomly partial missing joints (visualized as black) in the\nprefix sequence, similar to possible errors when estimating\nhuman skeleton. Each occlusion has a time duration based\non an exponential distribution. The second scenario is the\ntotal occlusion of several random joints in the whole prefix\nobserved motion, as illustrated in the lower-side of Fig. 7\nwhere random skeleton parameters are missing (visualized\nas black), simulating real-life occlusions, where a part of the\nbody, for instance, cannot be observed as being covered by\nthe environment. Fig. 7 exemplifies randomized occlusions in\nour N = 10frames prefix motion (400ms) for all the P = 99\nskeleton parameters provided by Human3.6M dataset.\nWe evaluate our model when the percentages of occluded\njoints vary among 80%, 60%, 40% and 20%. To reconstruct\nocclusion in observed motion (of the past -400ms to present,\n0 ms), several mechanisms are tested: (i) short-term recovery\nwhich predicts occluded information from previous non-\noccluded motion (based on assumed already reconstructed\nsequence in the past, from -800ms to -400ms); (ii) auto-\nregressive recovery that continuously predicts immediate\n(+80ms) skeleton pose and refines it with the known non-\noccluded data in the observation; (iii) linear interpolation\napplied in time that deals with partial skeleton occlusions\nto explode bidirectional flow information inherited by the\ninterpolation technique.\nD. Results\nQuantitative Evaluation\nWe report our results for short-term ( <500ms) and long-\nterm (>500ms) predictions in Human3.6M dataset. We main-\ntain the observation prefix patterns same as each baseline\noriginally used. Also, the MSE error is measured over all\nparameters predicted by each model. Note that our model\nalso includes the error of global rotation prediction.\nIn Table I, we compare our model with existing works in\nterms of several actions consisting the Human3.6M dataset.\nFor example, actions such as ‘Walking’, ‘Phoning’, ‘Wait-\ning’, ‘Walking Dog’, ‘Posing’, and ‘Purchases’ are consid-\nered. The results show that our model obtains better or very\ncompetitive results in short-term prediction, outperforming\nST-Transformer in most cases. The MSE report in Table\nII shows the competitive performance of our model when\npredicting human motion in non-occluded environments.\nAs detailed in Sec. IV-C, our model is also evaluated\nin occluded scenarios. Table III shows results with Time\nConsistent occlusions, and it indicates the capacity of our\nmodel to overcome errors in human pose estimations in real\nscenarios (i.e. joints not detected). Our model is tested with\n3 different reconstruction methods for occlusion recovery.\nResults show the robustness of 2CH-TR for human motion\nforecasting in occluded scenarios. Linear interpolation in\ntime also results as a robust method in our recovery sce-\nnario as the variation of joint movement in short occluded\nperiods is not significant. However, linear interpolation uses\nbidirectional information (skeletons before and after the\nocclusion) to recover the joint position, in contrast to the\nundirectionality of our 2CH-TR. This behaviour is desired\nfor real-world applications, where the model cannot rely on\nfuture data as we do not have it. Auto-regressive approaches\nFig. 8. Human Motion Prediction in different occluded environments, from\nnon-occluded data to whole prefix sequence occluded.\nfor occlusion reconstruction using our 2CH-TR collapse\nin non-plausible poses in prediction. As our model only\nforecasts the difference between the future poses and the\nlast pose observed (replicated in the input prefix), the correct\nreconstruction of this last pose is essential, as any error in it\nis propagated to the next prediction.\nDespite the competitive results of linear interpolation when\njoints are shortly occluded, this approach requires non-\noccluded data and cannot reconstruct poses when a skeleton\nparameter is occluded during the whole observed sequence.\nTherefore, to assess this second type of occlusion, in Fig. 8\nonly the effect in human motion forecasting of reconstructing\nthe missing joints in the whole observed sequence with 2CH-\nTR is shown. 2CH-TR is able to predict plausible human\nposes and reliable results even though we test the model in\nheavily occluded situations, with 80% or more of joint data\noccluded in the prefix sequence.\nFinally, we assess the contribution of our model in com-\nparison with baseline models regarding the applicability in\nreal-scenario. Computation speed and lightness are reported\nin Table IV. For a fair evaluation, we compared the size of the\nbaseline networks and their speeds with their original codes\nbut also with reduced prefix pattern to 10 frames as same as\nours. The results show our 2CH-TR is the fastest model by\nincreasing the speed over DCT-GCN by 30% with a GPU\nTesla-K80 and around 128 times faster than ST-Transformer,\nas our model predicts the whole future sequence in a single\ntime, avoiding autoregression. DCT-GCN is lighter when\npredicting only short or long-term sequences, but it needs\none model for each case, resulting in double-size network in\ntotal. As 2CH-TR explicitly performs short- and long-term\nprediction in one-shot, our model ends up being the lightest\nin our comparison (59% lighter that DCT-GCN).\nReal world demonstration. We tested the performance\nof our 2CH-TR when forecasting human motion in the wild.\nFor this approach, we trained an additional model with\nalso a global translation parameter (obtained as the center\nof the 3D skeleton estimated from FrankMocap, located\napproximately in the torso), so that 2CH-TR can understand\nthe global trajectory of the human motion. Fig. 9 shows the\neffectiveness of not only predicting 3D human poses but also\nglobal rotation and translation parameters, in contrast with\nbaseline methods. It is shown that the motion of the human\nis reasonably predicted: 2CH-TR is able to understand the\nreduction of velocity when the human turns around, and\nadapt the trajectory and future poses when walks backwards\nto the new next goal.\nTABLE I\nMSE ERROR COMPARISON OF HUMAN MOTION FORECASTING IN HUMAN 3.6M DATASET.\nW ALKING PHONING W AITING\nmilliseconds (ms) 80 160 320 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000\nS-RNN 0.808 0.942 1.159 1.484 1.778 1.225 1.503 1.925 2.061 2.02 2.38 1.156 1.396 1.781 1.941 2.191 2.957\nST-Transformer 0.212 0.359 0.58 0.72 0.782 0.53 1.042 1.41 1.544 1.543 1.809 0.219 0.512 0.978 1.221 1.658 2.485\nDCT-GCN 0.201 0.344 0.516 0.647 0.673 0.541 1.026 1.342 1.472 1.454 1.649 0.252 0.517 0.957 1.169 1.546 2.293\n2CH-TR (ours) 0.204 0.357 0.57 0.745 0.908 0.526 0.982 1.238 1.373 1.406 1.725 0.237 0.508 0.936 1.17 1.607 2.312\nW ALKING DOG POSING PURHCASES\nmilliseconds (ms) 80 160 320 560 1000 80 160 320 400 560 1000 80 160 320 400 560 1000\nS-RNN 1.029 1.221 1.546 2.067 2.471 1.346 1.395 1.96 2.2 2.456 3.102 1.219 1.452 1.87 1.989 2.36 3.325\nST-Transformer 0.43 0.783 1.148 1.613 1.896 0.609 0.684 1.052 1.282 1.776 2.826 0.43 0.765 1.304 1.373 1.548 2.411\nDCT-GCN 0.489 0.804 1.11 1.525 1.841 0.212 0.47 1.071 1.306 1.617 2.42 0.497 0.718 1.062 1.121 1.415 2.215\n2CH-TR (ours) 0.476 0.821 1.139 1.524 1.916 0.226 0.51 1.014 1.244 1.595 2.514 0.441 0.698 1.161 1.201 1.493 2.196\nTABLE II\nAVERAGE MSE ERROR FOR SUBJECT S5 IN MOTION PREDICTION\nmilliseconds (ms) 80 160 320 400 560 1000\nS-RNN (orig.) 0.933 1.166 1.397 1.526 1.711 2.139\nS-RNN (N=10) 0.988 1.161 1.435 1.576 1.84 2.221\nDCT-GCN 0.295 0.542 0.857 0.974 1.154 1.590\nST-Transformer (orig.) 0.303 0.550 0.901 1.021 1.229 1.722\nST-Transformer (N=10) 0.341 0.619 0.966 1.100 1.314 1.754\n2CH-TR (ours) 0.293 0.555 0.893 1.016 1.245 1.744\nTABLE III\nMSE ERROR PREDICTION WITH TIME -CONSISTENT 80% O CCLUSION\nmilliseconds (ms) 80 160 320 400 560 1000\nModel Prediction 1.126 1.234 1.386 1.463 1.613 1.960\nLinear Interpolation 0.951 1.177 1.443 1.526 1.650 2.015\nAuto-Regressive Prediction4.039 3.738 3.637 3.579 3.495 3.424\nTABLE IV\nEVALUATION OF MODEL EFFICIENCY FOR 1 SECOND PREDICTION\nInput\nSize\nGlobal\nRotation AutoregressionInference\ntime [ms]\nNetwork\nParameters\nDCT-GCN 10 ✗ ✗ 2 * 3.20 2 * 2,3M\nST-Transformer 50 ✗ ✓ 344.66 3,2M\nS-RNN 50 ✗ ✓ 117.80 22,8M\nST-Transformer 10 ✗ ✓ 283.81 3,2M\nS-RNN 10 ✗ ✓ 45.34 22,8M\n2CH-TR (ours) 10 ✓ ✗ 2.21 2,6M\nFig. 9. Motion forecasting in the wild when human walks towards the left\nside, turns around and walks backwards. Red skeleton shows the observed\nprefix sequence from our model, while gradient green skeletons project\nhuman motion prediction in next 1 second.\nV. CONCLUSION\nIn this work, we propose 2CH-TR architecture to effi-\nciently exploit dependencies between 3D human poses in\nspace and time to forecast near future skeleton sequences.\nBy decoupling the spatial and temporal channels, it is able\nto tackle high variant action motions in a single prediction.\n2CH-TR forecasts 1-second sequence of future poses while\nonly using 400ms of past observations. Our approach ob-\ntains competitive state-of-the-art results while reducing the\nrequired computational resources and increasing the speed of\nthe model. Experiment results also evaluate the robustness of\nour architecture even with highly-occluded skeleton poses\nin the observed prefix sequence. Based on this, we claim\nthat our 2CH-TR stands out as a real-world solution for 3D\nHuman Motion Forecasting in robotics applications.\nREFERENCES\n[1] J. Martinez, et al.. ”On human motion prediction using recurrent neural net-\nworks.” Proc. of IEEE Conf. on Computer Vision and Pattern Recognition. 2017\n[2] J., Ashesh, et al. ”Structural-rnn: Deep learning on spatio-temporal graphs.”\nProc. of IEEE Conf. on Computer Vision and Pattern Recognition. 2016.\n[3] Gui, Liang-Yan, et al. ”Adversarial geometry-aware human motion prediction.”\nProc. on European Conf. on Computer Vision. 2018.\n[4] Li, Chen, et al. ”Convolutional sequence to sequence model for human dynam-\nics.” Proc. on IEEE Conf. on Computer Vision and Pattern Recognition. 2018.\n[5] Akhter, Ijaz, et al. ”Nonrigid structure from motion in trajectory space.”\nAdvances in neural information processing systems. 2008.\n[6] Cai, Yujun, et al. ”Learning progressive joint propagation for human motion\nprediction.” European Conf. on Computer Vision. 2020.\n[7] M., Wei, M. Liu, and M. Salzmann. ”History repeats itself: Human motion\nprediction via motion attention.” European Conf. on Computer Vision. 2020.\n[8] Mao, Wei, et al. ”Learning trajectory dependencies for human motion predic-\ntion.” Proc. of the IEEE/CVF International Conf. on Computer Vision. 2019.\n[9] N. Reimers and I. Gurevych. ”Sentence-BERT: Sentence Embeddings using\nSiamese BERT-Networks”. Proc. on Conf. on Empirical Methods in NLP, 2019.\n[10] T. Brown, et al. ”Language models are few-shot learners”. Advances in neural\ninformation processing systems, 2020\n[11] Aksan, Emre, et al. ”A spatio-temporal transformer for 3D human motion\nprediction”. 2021 International Conf. on 3D Vision, 2021.\n[12] Vaswani, Ashish, et al. ”Attention is all you need.” Advances in neural infor-\nmation processing systems. 2017.\n[13] Ionescu, Catalin, et al. ”Human3.6m: Large scale datasets and predictive\nmethods for 3d human sensing in natural environments.” IEEE transactions on\npattern analysis and machine intelligence. 2013\n[14] E. Aksan, et al. ”Structured prediction helps 3d human motion modelling”. In\nIEEE International Conf. on Computer Vision (ICCV), 2019.\n[15] K. Fragkiadaki, et al. ”Recurrent network models for human dynamics”. In Proc\nof the 2015 IEEE International Conf on Computer Vision (ICCV), 2015.\n[16] Ghosh, Partha, et al. ”Learning human motion models for long-term predic-\ntions.” 2017 International Conf. on 3D Vision (3DV). IEEE, 2017.\n[17] S. Yan, et al. ”Spatial temporal graph convolutional networks for skeleton-based\naction recognition”. In AAAI Conf. on Artificial Intelligence, 2018.\n[18] K. Han et al. ”A Survey on Vision Transformer”. In IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2020\n[19] Guo X, Dai Y . ”Occluded joints recovery in 3d human pose estimation based\non distance matrix”. Int. Conf. on Pattern Recognition (ICPR). 2018\n[20] Cao, Zhe, et al. ”Realtime multi-person 2d pose estimation using part affinity\nfields.” Proc. on IEEE Conf. on computer vision and pattern recognition. 2017.\n[21] Yu, Cheng, et al. ”Multi-Scale Networks for 3D Human Pose Estimation with\nInference Stage Optimization.” preprint arXiv:2010.06844 (2020).\n[22] A. Hernandez, L. Gall , F. Moreno-Noguer . ”Human motion prediction via\nspatio-temporal inpainting”. Proc. on Int. Conf. on Computer Vision. 2019\n[23] Rong Y , et al. ”FrankMocap: A Monocular 3D Whole-Body Pose Estimation\nSystem via Regression and Integration”. Int. Conf. on Computer Vision, 2021",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6066338419914246
    },
    {
      "name": "Transformer",
      "score": 0.562751293182373
    },
    {
      "name": "Human motion",
      "score": 0.46633362770080566
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3937230408191681
    },
    {
      "name": "Motion (physics)",
      "score": 0.30967777967453003
    },
    {
      "name": "Engineering",
      "score": 0.17968204617500305
    },
    {
      "name": "Voltage",
      "score": 0.11738625168800354
    },
    {
      "name": "Electrical engineering",
      "score": 0.09532970190048218
    }
  ]
}