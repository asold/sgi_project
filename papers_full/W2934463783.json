{
    "title": "Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded",
    "url": "https://openalex.org/W2934463783",
    "year": 2019,
    "authors": [
        {
            "id": null,
            "name": "Ramprasaath Ramasamy Selvaraju",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2131064080",
            "name": "Stefan Lee",
            "affiliations": [
                "Oregon State University",
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2109533396",
            "name": "Yilin Shen",
            "affiliations": [
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2058645848",
            "name": "Hongxia Jin",
            "affiliations": [
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2096284530",
            "name": "Shalini Ghosh",
            "affiliations": [
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2104606758",
            "name": "Larry Heck",
            "affiliations": [
                "Samsung (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2098683697",
            "name": "Dhruv Batra",
            "affiliations": [
                "Georgia Institute of Technology",
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2223275083",
            "name": "Devi Parikh",
            "affiliations": [
                "Meta (Israel)",
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Ramprasaath Ramasamy Selvaraju",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131064080",
            "name": "Stefan Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109533396",
            "name": "Yilin Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2058645848",
            "name": "Hongxia Jin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096284530",
            "name": "Shalini Ghosh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104606758",
            "name": "Larry Heck",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098683697",
            "name": "Dhruv Batra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2223275083",
            "name": "Devi Parikh",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963890019",
        "https://openalex.org/W6697449767",
        "https://openalex.org/W6714077321",
        "https://openalex.org/W6641064462",
        "https://openalex.org/W6734194636",
        "https://openalex.org/W2302086703",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W6630875275",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W6639927594",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W6725681238",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W6736769356",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6714414533",
        "https://openalex.org/W2575842049",
        "https://openalex.org/W6719057275",
        "https://openalex.org/W2963191264",
        "https://openalex.org/W2983256121",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W6677995690",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W3037725825",
        "https://openalex.org/W6746806458",
        "https://openalex.org/W1931639407",
        "https://openalex.org/W2963260436",
        "https://openalex.org/W2795151422",
        "https://openalex.org/W2759653627",
        "https://openalex.org/W2886201417",
        "https://openalex.org/W6754733129",
        "https://openalex.org/W6729646992",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2407414618",
        "https://openalex.org/W2963579811",
        "https://openalex.org/W2962685807",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W3102564565",
        "https://openalex.org/W2962884579",
        "https://openalex.org/W2950761309",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2514824241",
        "https://openalex.org/W2963382180",
        "https://openalex.org/W2963996492",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2963630207",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2608030593",
        "https://openalex.org/W2503388974",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W2963668159",
        "https://openalex.org/W2963349562"
    ],
    "abstract": "Many vision and language models suffer from poor visual grounding - often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances - ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data.",
    "full_text": "Taking a HINT: Leveraging Explanations to Make\nVision and Language Models More Grounded\nRamprasaath R. Selvaraju1 Stefan Lee1,4 Yilin Shen2 Hongxia Jin2\nShalini Ghosh2 Larry Heck2 Dhruv Batra1,3 Devi Parikh1,3\n1Georgia Institute of Technology,2Samsung Research, 3Facebook AI Research, 4Oregon State University\n{ramprs, steflee, dbatra, parikh}@gatech.edu\n{yilin.shen, hongxia.jin, shalini.ghosh, larry.h}@samsung.com\nAbstract\nMany vision and language models suffer from poor vi-\nsual grounding – often falling back on easy-to-learn lan-\nguage priors rather than basing their decisions on visual\nconcepts in the image. In this work, we propose a generic\napproach called Human Importance-aware Network Tuning\n(HINT) that effectively leverages human demonstrations to\nimprove visual grounding. HINT encourages deep networks\nto be sensitive to the same input regions as humans. Our\napproach optimizes the alignment between human attention\nmaps and gradient-based network importances – ensuring\nthat models learn not just to look at but rather rely on vi-\nsual concepts that humans found relevant for a task when\nmaking predictions. We apply HINT to Visual Question An-\nswering and Image Captioning tasks, outperforming top ap-\nproaches on splits that penalize over-reliance on language\npriors (VQA-CP and robust captioning) using human atten-\ntion demonstrations for just 6% of the training data.\n1. Introduction\nMany popular and well-performing models for multi-\nmodal, vision-and-language tasks exhibit poor visual\ngrounding – failing to appropriately associate words or\nphrases with the image regions they denote and relying in-\nstead on superﬁcial linguistic correlations [2, 1, 36, 10, 12].\nFor example, answering the question ‘What color are the\nbananas?’ with yellow regardless of their ripeness evident\nin the image. When challenged with datasets that penal-\nize reliance on these sort of biases [2, 10], state-of-the-art\nmodels demonstrate signiﬁcant drops in performance de-\nspite there being no change to the set of visual and linguistic\nconcepts about which models must reason.\nIn addition to these diagnostic datasets, another pow-\nerful class of tools for observing this shortcoming has\nbeen gradient-based explanation techniques [27, 35, 26, 24]\nwhich allow researchers to examine which portions of the\nFigure 1: Our approach, HINT, aligns visual explanations for\noutput decisions of a pretrained model with spatial input regions\ndeemed important by human annotators – forcing models to base\ntheir decisions on these same region and reducing model bias.\ninput models rely on when making decisions. Applica-\ntion of these techniques has shown that vision-and-language\nmodels often focus on seemingly irrelevant image regions\nthat differ signiﬁcantly from where human subjects ﬁxate\nwhen asked to perform the same tasks [7, 24] –e.g. focusing\non a produce stand rather than the bananas in our example.\nWhile somewhat dissatisfying, these ﬁndings are not en-\ntirely surprising – after all, standard training protocols do\nnot provide any guidance for visual grounding. Instead,\nmodels are trained on input-output pairs and must resolve\ngrounding from co-occurrences – a challenging task, espe-\ncially in the presence of more direct and easier to learn cor-\nrelations in language. Consider our previous example ques-\ntion, the words ‘color’, ‘banana’, and ‘yellow’ are given as\ndiscrete tokens that will trivially match in every occurrence\nwhen these underlying concepts are referenced. In contrast,\nactually grounding this question requires dealing with all vi-\nsual variations of bananas and learning the common feature\nof things described as ‘yellow’. To address this, we explore\nif giving asmall hint in the form of human attention demon-\nstrations can help improve grounding and reliability.\nFor the dominant paradigm of vision-and-language mod-\nels that compute an explicit question-guided attention over\nimage regions [25, 30, 13, 32, 19, 3], a seemingly straight-\n1\narXiv:1902.03751v2  [cs.CV]  28 Oct 2019\nforward solution is to provide explicit grounding supervi-\nsion – training models to attend to the appropriate image\nregions. While prior work [21, 16] has shown this approach\nresults in more human-like attention maps, our experiments\nshow it to be ineffective at reducing language bias. Cru-\ncially, attention mechanisms are bottom-up processes that\nfeed ﬁnal classiﬁcation models such that even when attend-\ning to appropriate regions, models can ignore visual con-\ntent in favor of language bias. In response, we introduce a\ngeneric, second-order approach that instead aligns gradient-\nbased explanations with human attention.\nOur approach, which we call Human Importance-aware\nNetwork Tuning (HINT), enforces a ranking loss between\nhuman annotations of input importance and gradient-based\nexplanations produced by a deep network – updating model\nparameters via a gradient-of-gradient step. Importantly, this\nconstrains models to not only look at the correct regions\nbut to also be sensitive to the content present there when\nmaking predictions. While we experiment with HINT in\nthe context of vision-and-language problems, the approach\nitself is general and can be applied to focus model decisions\non speciﬁc inputs in any context.\nWe apply HINT to two tasks – Visual Question Answer-\ning (VQA) [5] and image captioning [14] – and ﬁnd our ap-\nproach signiﬁcantly improves visual grounding. With hu-\nman importance supervision for only 6% of the training\nset, our HINT’ed model improves the state-of-the-art by 8\npercentage points on the challenging dataset VQA Under\nChanging Priors (VQA-CP) [2], which is designed to test\nvisual grounding. In both VQA and Image Captioning, we\nsee signiﬁcantly improved correlations between human at-\ntention and visual explanations for HINT trained models,\nshowing that models learn to make decisions using similar\nevidence as humans (even on new images). We perform\nhuman studies which show that humans perceive models\ntrained using HINT to be more reasonable and trustworthy.\nContributions. To summarize our contributions, we\n• introduce Human Importance-aware Network Tuning\n(HINT), a general approach for constraining the sensitiv-\nity of deep networks to speciﬁc input regions and demon-\nstrate it results in signiﬁcantly improved visual ground-\ning for two vision and language tasks,\n• set a new state-of-the-art on the bias-sensitive VQA Un-\nder Changing Priors (VQA-CP) dataset [2], and\n• conduct studies showing that humans ﬁnd HINTed mod-\nels more trustworthy than standard models.\n2. Related Work\nModel Interpretability. There has been signiﬁcant re-\ncent interest in building machine learning models that are\ntransparent and interpretable in their decision making pro-\ncess. For deep networks, several works propose explana-\ntions based on internal states of the network [34, 11, 37, 24].\nMost related to our work is the approach of Selvaraju et al.\n[24] which computes neuron importance as part of a visual\nexplanation. In this work, we enforce that these importance\nscores align with importances provided by domain experts.\nVision and Language Tasks.Image Captioning [15] and\nVisual Question Answering (VQA) [5] have emerged as two\nof the most widely studied vision-and-language problems.\nThe image captioning task requires generating natural lan-\nguage descriptions of image contents and the VQA task\nrequires answering free-from questions about images. In\nboth, models must learn to associate image content with nat-\nural free-form text. Consequentially, attention based mod-\nels that explicitly reason about image-text correspondences\nhave become the dominant paradigm [25, 30, 13, 32, 19, 3];\nhowever, there is growing evidence that even these atten-\ntional models still latch onto language biases [2, 36, 4].\nRecently, Agrawal et al . [2] introduced a novel, bias-\nsensitive dataset split for the VQA task. This split, called\nVQA Under Changing Priors (VQA-CP), is constructed\nsuch that the answer distributions differ signiﬁcantly be-\ntween training and test. As such, models that memorize\nlanguage associations in training instead of actually ground-\ning their answers in image content will perform poorly on\nthe test set. Likewise Lu et al. [19] introduce a robust cap-\ntioning split of the COCO captioning dataset [15] in which\nthe distribution of co-occurring objects differs signiﬁcantly\nbetween training and test. We use these dataset splits to\nevaluate the impact of our method on visual grounding.\nDebiasing Vision and Language Models.A number of\nrecent works have aimed to reduce the effect of language\nbias in vision and language models.\nHendricks et al . [4] study the generation of gender-\nspeciﬁc words in image captioning – showing that mod-\nels nearly always associated male gendered words to people\nperforming extreme sports like snowboarding regardless of\nthe image content. Their presented Equalizer approach en-\ncourages models to adjust their conﬁdence depending on the\nevidence present – conﬁdent when gender evidence is visi-\nble and unsure when it is occluded by ground-truth segmen-\ntation masks. Experiments on a set of captions containing\npeople show this approach reduces gender bias.\nFor VQA, Agrawal et al . [2] developed a Grounded\nVQA model (GVQA) that disentangles the vision and lan-\nguage components – consisting of separate visual concept\nand answer cluster classiﬁers. This approach uses a ques-\ntion’s type (e.g. “What color ...”) to determine the space\nof possible answers and the question target ( e.g. “banana”)\nto detect visual attributes in the scene that are then ﬁltered\nby the possible answer set. While effective, this requires\nmulti-stage training and is difﬁcult to extend to new models.\nRamakrishnan et al. [22] introduce an adversarial model ag-\nnostic regularization technique to reduce bias in VQA mod-\nels – pitting the model against a question-only adversary.\n2\nHuman Attention for VQA.Das et al. [7] collected hu-\nman attention maps for a subset of the VQA dataset [5].\nGiven a question and a blurry image, humans were asked\nto interactively deblur regions in the image until they could\nconﬁdently answer. In this work, we utilize these maps,\nenforcing the gradient-based visual explanations of model\ndecisions to closely match the human attention.\nSupervising model attention.Liu et al. [16] and Qiao et\nal. [21] apply human attention supervision to attention maps\nproduced by the model for image captioning and VQA, re-\nspectively. We experiment with a similar approach but ﬁnd\nthat the improved attention correlation does not translate to\nreduced reliance on language bias – even with appropriate\nmodel attention, the remaining network layers can still dis-\nregard the visual signal in the presence of strong biases.\nWe also show how gradient explanations are more faithful\nto model decisions by directly linking model decisions in-\nput regions, so that aligning these importances ensures the\nmodel is basing its decision on human-attended regions.\nAligning gradient-based importances. Selvaraju et al .\n[23] proposed an approach to learn a mapping between\ngradient-based importances of individual neurons within a\ndeep network (from [24]) and class-speciﬁc domain knowl-\nedge from humans in order to learn classiﬁers for novel\nclasses. In contrast, we align gradient-based importances\nto human attention maps to improve network grounding.\n3. Preliminaries\nWhile our approach is general-purpose and model ag-\nnostic, in this work we take the recent Bottom-up Top-\ndown architecture [3] as our base model. A number of\nworks [31, 8, 33, 29, 17, 32, 18] use Top-down attention\nmechanisms to help ﬁne-grained and multi-stage reasoning,\nwhich is shown to be very important for vision and language\ntasks. Anderson et al . [3] propose a variant of the tradi-\ntional attention mechanism, where instead of attending over\nconvolutional features they show that attending over objects\nand other salient image regions gives signiﬁcant improve-\nments in VQA and captioning performance. We brieﬂy de-\nscribe this architecture below, see [3] for full details.\nBottom-Up Top-Down Attention for VQA.As shown in\nleft half of Fig. 2, given an image, the Bottom-up Top-down\n(UpDown) attention model takes as input up to k image\nfeatures, each encoding a salient image region. These re-\ngions and their features are proposals extracted from Faster-\nRCNN [9]. The question is encoded using a GRU [6] and\na soft-attention over each of the kproposal features is com-\nputed using the question embedding. The ﬁnal pooled at-\ntention feature is combined with the question feature using\na few fully-connected layers which predict the answer.\nBottom-Up Top-Down Attention for Image Captioning.\nThe image captioning model consists of two Long Short-\nTerm Memory (LSTM) networks – an attention LSTM and\na language LSTM. The ﬁrst LSTM layer is a top-down vi-\nsual attention model whose input at each time step consists\nof the previous hidden state of the language LSTM, con-\ncatenated with the mean-pooled bottom-up proposal fea-\ntures (similar to above) and an encoding of the previously\ngenerated word. The output of the attention LSTM does a\nsoft attention over the proposal features. The second LSTM\nis a language generation LSTM that takes as input the at-\ntended features concatenated with the output of the attention\nLSTM. The language LSTM provides a distribution over the\nvocabulary of words for the next time step.\n4. Human Importance-aware Network Tuning\nIn this section, we describe our approach for training\ndeep networks to rely on the same regions as humans which\nwe call Human Importance-aware Network Tuning (HINT).\nIn summary, HINT estimates the importance of input re-\ngions through gradient-based explanations and tunes the\nnetwork parameters so as to align this with the regions\ndeemed important by humans. We use the generic term\n‘prediction’ to refer to both answers in the case of VQA and\nthe words generated at each time step in image captioning.\n4.1. Human Importance\nIn this step, we align the expert knowledge obtained from\nhumans attention maps into a form corresponding to the net-\nwork inputs. The Bottom-up Top-down model [3] takes in\nas input region proposals. For a given instance, we com-\npute an importance score for each of the proposals based on\nnormalized human attention map energy inside the proposal\nbox relative to the normalized energy outside the box.\nMore concretely, consider a human importance map\nAd ∈ Rh×w that indicates the spatial regions of support\nfor an output d1 – a high value Ad[i,j] indicates high sup-\nport for dat location (i,j). Given a proposal region r with\narea ar, we can write the normalized importance inside and\noutside rfor decision das\nEd\ni (r)= 1\nar\n∑\n(i,j)∈r\nAd\nij and Ed\no (r)= 1\nh.w−ar\n∑\n(i,j)/∈r\nAd\nij\nrespectively. We compute the overall importance score for\nproposal kfor decision das:\nsd\nk = Ed\ni (k)\nEd\ni (k) +Edo (k) (1)\nHuman attention for VQA and captioning.For VQA,\nwe use the human attention maps collected by Daset al. [7]\nfor a subset of the VQA [5] dataset. HAT maps are avail-\nable for a total of 40554 image-question pairs – or approx-\nimately only ∼6% of the VQA dataset. While human atten-\ntion maps do not exist for image captioning, COCO dataset\n1For VQA, these maps will vary across questions for a given image.\n3\nFigure 2: Our Human Importance-aware Network Tuning (HINT) approach: Given an image and a question like “Did he hit the ball?”, we\npass them through the Bottom-up Top-down architecture shown in the left. For the example shown, the model incorrectly answers ‘no’. We\ndetermine the proposals important for the ground-truth answer ‘yes’ through a gradient-based importance measure. We rank the proposals\nthrough human attention and provide a ranking loss in order to align the network’s importance with human importance. Tuning the model\nthrough HINT makes the model not only answer correctly, but also look at the right regions, as shown in the right.\n[14] has segmentation annotations for 80 everyday occur-\nring categories. We use a word-to-object mapping that links\nﬁne-grained labels like [“child”, “man”, “woman”, ...] to\nobject categories like <person> similar to [19]. We map\na total of 830 visual words existing in COCO captions to\n80 COCO categories. We then use the segmentation an-\nnotations for the 80 categories as human attention for this\nsubset of matching words. To be consistent with the VQA\nsetup, we only use 6% of the segmentation annotations.\n4.2. Network Importance\nWe deﬁne Network Importance as the importance that\nthe given trained network places on spatial regions of the\ninput when making a particular prediction. Selvaraju et al.\n[24] proposed an approach to compute the importance of\nlast convolutional layer’s neurons. In their work, they focus\non the last convolutional layer neurons as they serve as the\nbest compromise between high level semantics and detailed\nspatial information. Since proposals usually look at objects\nand salient/semantic regions of interest while providing a\ngood spatial resolution, we extend [24] to compute impor-\ntance over proposals. In order to obtain the importance of\na proposal rfor ground-truth decision, αr\ngt, we one-hot en-\ncode the score for the ground-truth output (answer in VQA\nand the visual word in case of captioning) ogt and compute\nits gradients w.r.t. proposal features as,\nαr\ngt =\nglobal pooling\n\n|P|∑\ni=1\n∂ogt\n∂Pr\ni\ngradients via backprop\n(2)\nNote that we compute the importance for the ground-truth\ndecision, and not predicted. Human attention for incorrect\ndecisions are not available and are conceptually ill-posed\nbecause it is difﬁcult to deﬁne what correct ‘evidence’ for\nan incorrect prediction would be.\n4.3. Human-Network Importance Alignment\nAt this stage, we now have two sets of importance scores\n– one computed from the human attention and another from\nnetwork importance – that we would like to align. Each set\nof scores is calibrated within itself; however, absolute val-\nues are not comparable between the two as human impor-\ntance lies in [0,1] while network importance is unbounded.\nConsequentially, we focus on the relative rankings of the\nproposals, applying a ranking loss – speciﬁcally, a variant\nof Weighted Approximate Rank Pairwise (W ARP) loss.\nRanking loss. At a high level, our ranking loss searches all\npossible pairs of proposals and ﬁnds those pairs where the\npair-wise ranking based on network importance disagrees\nwith the ranking from human importance. Let Sdenote the\nset of all such misranked pairs. For each pair in S, the loss\nis updated with the absolute difference between the network\nimportance score for the proposals pair.\nL=\n∑\n(r′,r)∈S\n⏐⏐⏐αr′\n−−αr\n+\n⏐⏐⏐ (3)\nwhere rand r′are the proposals whose order based on neu-\nron importance does not align with human importance and\n+ indicates that proposal ris more important compared to\nr′according to human importance.\nImportance of task loss. In order to retain performance\nat the base task, it is necessary to include the original task\nloss λLTask – cross-entropy for VQA and negative log-\nlikelihood in case of image captioning. To trade-off be-\ntween the two, we introduce a multiplier λ such that the\nﬁnal HINT loss becomes,\nLHINT =\n∑\n(r′,r)∈S\n⏐⏐⏐αr′\n−−αr\n+\n⏐⏐⏐+ λLTask (4)\nThe ﬁrst term encourages the network to base predictions\non the correct regions and the second term encourages it to\nactually make the right prediction.\n4\nNote that network importances α are gradients of the\nscore with respect to proposal embeddings. Thus they are\na function of all the intermediate parameters of the network\nranging from the model attention layer weights to the ﬁnal\nfully-connected layer weights. Hence an update through an\noptimization algorithm (gradient-descent or Adam) with the\ngiven loss in (4) requires computation of second-order gra-\ndients, and would affect all the network parameters. We use\nPyTorch [20] which has this functionality.\n5. Experiments and Analysis\nIn this section we describe the experimental evaluation\nof our approach on VQA and Image Captioning.\nVQA. For VQA, we evaluate on the standard VQA split and\nthe VQA-CP [2] split. Recall from Section 2 that VQA-\nCP is a restructuring of VQAv2 [10] that is designed such\nthat the answer distribution in the training set differs sig-\nniﬁcantly from that of the test set. For example, while the\nmost popular answer in train for “What sport ...” questions\nmight be “tennis”, in test it might be “volleyball”. Without\nproper visual grounding, models trained on this dataset will\ngeneralize poorly to the test distribution. In fact, [2] and\n[22] report signiﬁcant performance drops for state-of-the-\nart VQA models on this challenging, language-bias sensi-\ntive split. For our experiments, we pretrain our Bottom-Up\nTop-Down model on respective training splits before ﬁne-\ntuning with the HINT loss. Recall that our approach in-\ncludes the task loss; We useλvqa = 10for our experiments.\nWe compare our approach against strong baselines and\nexisting approaches, speciﬁcally:\n• Base Model (UpDn)We compare to the base Bottom-up\nTop-down model without our HINT loss.\n• Attention Alignment (Attn. Align.)We replace gradi-\nent supervision with attention supervision keeping every-\nthing else the same. The Bottom-up Top-down model\nuses soft attention over object proposals – essentially pre-\ndicting a set of attention scores for object proposals based\non their relevancy to the question. These attention scores\nare much like the network importances we compute in\nHINT; however, they are functions only of the network\nprior to attention prediction. We apply the HINT ranking\nloss between these attention weights and human impor-\ntances as computed in Equation (1).\n• Grounded VQA (GVQA).As discussed in Section 2, [2]\nintroduced a grounded VQA model that explicitly disen-\ntangles vision and language components and was devel-\noped alongside the VQA-CP dataset.\n• Adversarial Regularization (AdvReg). [22] intro-\nduced an adversarial regularizer to reduce the effect of\nlanguage-bias in VQA by explicitly modifying question\nrepresentations to fool a question-only adversary model.\nImage Captioning. For captioning, we evaluate on the\nstandard ‘Karpathy’ split and the robust captioning split in-\ntroduced by Lu et al. in [19]. The robust split has varying\ndistribution of co-occurring objects between train and test.\nWe pretrain our Bottom-up Top-down captioning model on\nthe respective training splits and apply our approach, HINT.\nNote that the HINT loss is applied only for the time steps\ncorresponding to the 830 visual words in the caption that\nwe obtain in Section 4.1.\n5.1. HINT for Visual Question Answering\nTable 1 shows results for our models and prior work on\nVQA-CP test and VQAv2 val. We summarize key results:\nHINT reduces language-bias.For VQA-CP, our HINTed\nUpDown model signiﬁcantly improves over its base archi-\ntecture alone by 7 percentage point gain in overall accu-\nracy. Further, it outperforms existing approaches based on\nthe same UpDn architecture (41.17 for AdvReg vs 46.73 for\nHINT), setting a new state-of-the-art for this problem. We\ndo note that our approach uses additional supervision in the\nform of human attention maps for 6% of training images.\nHINT improves grounding without reducing standard\nVQA performance. Unlike previous approaches for\nlanguage-bias reduction which cite trade-offs in perfor-\nmance between the VQA and VQA-CP splits [22, 2], we\nﬁnd our HINTed UpDn model actually improves on stan-\ndard VQA – making HINT the ﬁrst ever approach to show\nsimultaneous improvement on both the standard and com-\npositional splits.\nAttn. Align is ineffective compared to HINT.A surpris-\ning (to us at least) ﬁnding and motivating observation of\nthis work is that directly supervising model attention (as in\nAttn. Align) is ineffective at reducing language-bias and im-\nproving visual grounding as measured by VQA-CP, begging\nthe question – why does our gradient supervision succeed\nwhere attention supervision fails?\nWe argue this results from gradient-based explanations\nbeing 1) a function of all network parameters unlike atten-\ntion alignment and 2) more faithful to model decisions than\nmodel attention. As we’ve discussed previously, attention\nis a bottom-up computation and supervising it cannot di-\nrectly affect later network layers, whereas our HINT ap-\nproach does. To assess faithfulness, we run occlusion stud-\nies similar to those in [24, 34]. We measure the difference in\nmodel scores for the predicted answer when different pro-\nposal features for the image are masked and forward propa-\ngated, taking this delta as an importance score for each pro-\nposal. We ﬁnd that rank correlation between model atten-\ntion and occlusion-based importance is only0.10, compared\nto 0.48 for gradient-based importance – demonstrating our\nclaim that model attention only loosely relates to how the\nmodel actually arrives at its decision. As such, attention\nalignment simply requires the model to predict human-like\nattention, not necessarily to care about them when making\ndecisions. On the other hand, HINT aligns gradient-based\n5\nIs\tthis\tthe\tright\tsized\t\nskateboard\tfor\thim?\tGT:\tYes\nGrad-CAM\tfor\t‘Yes’ Grad-CAM\tfor\t‘Yes’\nPred:\tNo Pred:\tYes\nBefore\tHINT After\tHINT\n(a)\nWhat\tcolor\tare\tthe\tsigns?\nGT:\tRed\tand\tWhite\nBefore\tHINT After\tHINT\nGrad-CAM\tfor\t‘Red\tand\tWhite’\nPred:\tRed\nGrad-CAM\tfor\t‘Red\tand\tWhite’\nPred:\tRed\tand\tWhite (b)\nIs\tthis\tbaby\tsucking\ton\t a\t\npacifier?\tGT:\tYes\nGrad-CAM\tfor\t‘Yes’ Grad-CAM\tfor\t‘Yes’\nPred:\tNo Pred:\tYes\n(c)\nPred:\tNo Pred:\tYes\nIs\tthis\ta\ttourist\tfriendly\t\narea?\tGT:\tYes\nGrad-CAM\tfor\t‘Yes’ Grad-CAM\tfor\t‘Yes’\nPred:\tNo Pred:\tYes (d)\nFigure 3: Qualitative comparison of models on validation set before and after applying HINT. For each example, the left column shows\nthe input image along with the question and the ground-truth (GT) answer from the VQA-CP val split. In the middle column, for the\nbase model we show the explanation visualization for the GT answer along with the model’s answer. Similarly we show the explanations\nand predicted answer for the HINTed models in the third column. We see that the HINTed model looks at more appropriate regions and\nanswers more accurately. For example, for the example in (a), the base model only looks at the boy, and after we apply HINT, it looks at\nboth the boy and the skateboard in order to answer ‘Yes’. After applying HINT, the model also changes its answer from ‘No’ to ‘Yes’.\nMore qualitative examples can be found in the supplementary material.\nModel VQA-CPtest VQAv2val\nOverall Yes/No Number Other Overall Yes/No Number Other\nSAN [32] 24.96 38.35 11.14 21.74 52.41 70.06 39.28 47.84\nUpDn [3] 39.49 45.21 11.:96 42.98 62.85 80.89 42.78 54.44\nGVQA [2]† 31.30 57.99 13.68 22.14 48.24 72.03 31.17 34.65\nUpDn + Attn. Align 39.37 43.02 11.89 45.00 63.24 80.99 42.55 55.22\nUpDn + AdvReg [22]† 41.17 65.49 15.48 35.48 62.75 79.84 42.35 55.16\nUpDn + HINT(ours) 46.73 67.27 10.61 45.88 63.38 81.18 42.99 55.56\nTable 1: Results on compositional (VQA-CP) and standard split (VQAv2). We see that our approach (HINT) gets a signiﬁcant boost of\nover 7% from the base UpDn model on VQA-CP and minor gains on VQAv2. The Attn. Align baseline sees similar gains on VQAv2, but\nfails to improve grounding on VQA-CP. Note that for VQAv2, during HINT ﬁnetuning we apply the VQA cross entropy loss even for the\nsamples without human attention annotation. † results taken from corresponding papers.\nimportance with respect to model decisions, ensuring that\nhuman speciﬁed regions are actually used by the network –\nresulting in a model that is right for the right reasons.\nVarying the amount of human attention supervision.\nThe plot to the right shows\nperformance for different\namounts of Human Attention\nmaps for VQA-CP. Note\nthat the x-axis goes from\nusing no HINT supervision to\nusing all the Human attention\nmaps during training, which\namounts to 6% of the VQAv2\ndata. Note that with human\nattention supervision for just 1.5% of the VQA dataset, our\napproach achieves a 5 % improvement in performance.\nQualitative examples. Fig. 6 shows qualitative examples\nshowing the effect of applying HINT to the Bottom-up Top-\ndown VQA model. Fig. 6 (b) shows an image and a ques-\ntion, ‘What color are the signs?’, the base model answers\n“Red” which is partially correct, but it fails to ground the an-\nswer correctly. The HINTed model not only answers “Red\nand White” correctly but also looks at the red stop sign and\nthe white street sign.\n5.2. HINT for Image Captioning\nOur implementation of the Bottom-up Top-down cap-\ntioning model in Pytorch [20] achieves a CIDEr [28] score\nof 1.06 on the standard split and 0.90 on the robust split.\nUpon applying HINT to the base model trained on the ro-\nbust split, we obtain a CIDEr score of 0.92, an improvement\nof 0.02 over the base model. For the model trained on the\nstandard split, performance drops by 0.02 in CIDEr score\n6\nA\tclose\tup\tof\ta\tfork an\torange\t\nan\tapple and\tan\tonion\napple\norange\nBefore\tHINT After\tHINT\nforkfork\napple\norange\n(a)\nA\tsmall\tdog laying\ton\ta\tbed next\t\nto\ta\tlaptop computer\ndog dog\nbed bed\nlaptop laptop\nBefore\tHINT After\tHINT (b)\n(c)\n (d)\nFigure 4: Qualitative comparison of captioning models on validation set before and after applying HINT. For each example, the left column\nshows the input image along with the ground-truth caption from the COCO robust split. In the middle column, for the base model we show\nthe explanation visualization for the visual word mentioned below. Similarly we show the explanations for the HINTed models in the third\ncolumn. We see that the HINTed model looks at more appropriate regions. For example in (a) note how the HINTed model correctly\nlocalizes the fork, apple and the orange when generating the corresponding visual words, but the base model fails to do so. Interestingly\nthe model is able to ground even the shadow of a cat in (f)! More qualitative examples can be found in the supplementary material.\n(1.04 compared to 1.06). As we show in the following sec-\ntions, the lack of improvement in score does not imply a\nlack of change – we ﬁnd the model shows signiﬁcant im-\nprovements at grounding, which we evaluate in Section 6.\nNote that our setup for captioning does not require task-\nspeciﬁc human attention , and instead allows us to directly\nleverage existing annotations which were collected for a dif-\nferent task (image segmentation).\nQualitative examples. Fig. 7 shows qualitative exam-\nples that indicate signiﬁcant improvements in grounding\nperformance of HINTed models. For example Fig. 7 (a)\nshows how a model trained with HINT is able to simulta-\nneously improve grounding for the 3 visual words present\nin the ground-truth caption. We see that HINT also helps\nwith making models focus on individual object occurrences\nrather than using context, as shown in Fig. 7 (c, d, e, f).\n6. Evaluating Grounding\nIn Sections 5.1 and 5.2 we evaluated the effect of HINT\non the task performance, with generalization to robust\ndataset splits serving as an indirect evaluation of grounding.\nIn this section we directly evaluate the grounding ability of\nmodels tuned with HINT.\n6.1. Correlation with Human Attention\nIn order to evaluate the grounding ability of models be-\nfore and after applying HINT, we compare the network im-\nportances for the ground-truth decision (as in Equation (2))\nwith the human attention as computed in Equation (1) for\nboth the base model and the model ﬁne-tuned with HINT.\nWe then compute the rank correlation between the network\nimportance scores and human importance scores for images\nfrom the VQA-CP and COCO robust test splits. We report\nSpearman’s rank correlation between explanations from the\n7\nInside\tbathroom\twith\ta\tlarge\tclock\tface\ton\tthe\tmirror\nWhen\tgenerating\tthe\tword:\tClock\nClock Clock\nFigure 5: AMT interface for evaluating the baseline captioning\nmodel and our HINTed model. HINTed model outperforms base-\nline model in terms of human trust.\nbase model and the HINTed model.\nVQA. For the model trained on VQA-v2, we ﬁnd that the\nGrad-CAM based attention for base model obtains a Spear-\nman’s rank correlation of -0.09 with human attention maps\n[7]. Note that the range of rank-correlation is -1 to 1, so\nnear 0 indicates no correlation. We ﬁnd that the HINTed\nmodel obtains a correlation of 0.18.\nImage Captioning. For the model trained on the COCO\nrobust split, the Grad-CAM based attention for base model\nachieves a rank correlation of 0.008 with COCO segmenta-\ntion maps for the visual words, and the model after HINTing\nachieves a correlation of 0.17.\nThis rank correlation measure matches the intent of the\nrank-based HINT loss, but this result shows that the visual\ngrounding learned during training generalizes to new im-\nages and language contexts better than the baseline model.\n7. Evaluating Trust\nIn the previous section we evaluate if HINTed models at-\ntend to the same regions as humans when forced into mak-\ning predictions. Having established that, we turn to under-\nstanding whether this improved grounding translates to in-\ncreased human trust in HINTed models. We focus this study\non our image captioning models.\nWe conduct human studies to evaluate if based on indi-\nvidual prediction explanations from two models – the base\nmodel and one with improved grounding through HINT –\nhumans ﬁnd either of the models more trustworthy. In order\nto tease apart the effect of grounding from the accuracy of\nthe models being visualized, we only visualize predictions\ncorresponding to the ground-truth caption for both models.\nFor a given ground truth caption, we show study partici-\npants the network importance explanation for a ground truth\nvisual word as well as the whole caption. Workers were\nthen asked to rate the reasonableness of the models relative\nto each other on a 5-point Likert scale of clearly more/less\nreasonable (+/-2), slightly more/less reasonable (+/- 1), and\nequally reasonable (0). This interface is shown in Fig. 5. In\norder to eliminate any biases, the base and HINTed models\nwere assigned to be ‘model1’ with equal probability.\nIn total, 42 Amazon Mechanical Turk (AMT) workers\nparticipated in the study, producing 1000 responses (5 anno-\ntations corresponding to 200 image pairs). In 49.9 % of in-\nstances, participants preferred HINT compared to only 33.1\n% for the base model. These results indicate that HINT\nhelps models look at appropriate regions, and that this in\nturn makes the model more trustworthy.\n8. Does HINT also improve model attention?\nWhile HINT operates on answer gradient maps, we ﬁnd\nit also improves feed-forward model attention. For VQA,\nwe compute IoU of the top scoring proposal box with the\nhuman attention maps from Park et al. 2018. UpDn trained\non VQA-CP obtained an IoU of 0.57 whereas after applying\nHINT we achieve an IoU of 0.63.\nWe conduct human studies (similar to Section 7) to eval-\nuate trust based on model attention. We collected 10 re-\nsponses each for 100 randomly sampled image-question\npairs. 31% of respondents found HINTed VQA-CP model\nto be more trustworthy compared to 16.5% for the base\nmodel. This was not the primary objective of our approach\nbut is a promising outcome for feed-forward attention!\n9. Conclusion\nWe presented Human Importance-aware Network Tun-\ning (HINT), a general framework for aligning network sen-\nsitivity to spatial input regions that humans deemed as being\nrelevant to a task. We demonstrated this method’s effective-\nness at improving visual grounding in vision and language\ntasks such as VQA and Image Captioning. We also show\nthat better grounding not only improves the generalization\ncapability of models to changing test distributions, but also\nimproves the trust-worthiness of model.\nTaking a broader view, the idea of regularizing net-\nwork gradients to achieve desired computational properties\n(grounding in our case) may prove to be more widely appli-\ncable to problems outside of vision and language – enabling\nusers to provide focused feedback to networks.\nAcknowledgements. Georgia Tech’s effort was supported in\npart by NSF, AFRL, DARPA, ONR YIPs, Samsung GRO, ARO\nPECASE. The views and conclusions contained herein are those\nof the authors and should not be interpreted as necessarily repre-\nsenting the ofﬁcial policies or endorsements, either expressed or\nimplied, of the U.S. Government, or any sponsor.\n8\nAppendices\nA. Qualitative examples\nIn Fig. 6 we show examples applying HINT for the Bottom-\nup Top-down VQA model. The left column shows the input image\nalong with the question and the ground-truth (GT) answer from the\nVQA-CP val split. In the middle column, for the base model we\nshow the explanation visualization for the GT answer along with\nthe model’s answer. Similarly we show the explanations and pre-\ndicted answer for the HINTed models in the third column. We see\nthat the HINTed model not only looks at more appropriate regions\ncompared to the base models.\nFig. 6 (a) shows an image and a question,“Is the person scream-\ning”. Not only does the base model answer “no” incorrectly, but\nit also cannot localize the right answer – looks just at the bear for\n“yes”. The HINTed model answers “yes” correctly and looks at\nboth the bear and the face of the person. For the image 6 (b) with\nquestion “Does the building have a clock on it?”, the base model\nincorrectly answers no, whereas the HINTed model not only as-\nnwers ‘yes’ correctly, it also localizes the clock on the building.\nThe bottom row shows two examples where HINT helps with lo-\ncalizing the right answer, although the answers from both the mod-\nels (base model and HINTed model) are incorrect.\nIn Fig. 7 we show qualitative examples showing the effect of\napplying HINT on the Top-down Bottom-up [3] captioning model\ntrained on the Robust split of the COCO dataset. The left column\nshows the input image along with the ground-truth caption from\nthe COCO robust split. In the middle column, for the base model\nwe show the explanation visualization for the visual word men-\ntioned below. Similarly we show the explanations for the HINTed\nmodels in the third column. We see that the HINTed model looks\nat more appropriate regions when generating the mentioned visual\nword (below the visualization).\nFor example, for the input image in Fig. 7 (a) and (b), the base\nmodel only places a little importance on the face while generating\nthe word ‘guy’, whereas the HINTed model correctly looks at the\nface of the person. Similarly when generating ‘ties’ the HINTed\nmodel looks at the whole tie region compared to the base model.\nSimilarly for the images in (e) and (f), the HINTed model looks\nmore correctly at the spoons for the visual word ‘spoon’ and sink,\nfor the word ‘sink’.\nReferences\n[1] Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Ana-\nlyzing the behavior of visual question answering models. In\nEMNLP, 2016. 1\n[2] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Anirud-\ndha Kembhavi. Dont just assume; look and answer: Over-\ncoming priors for visual question answering. In IEEE Con-\nference on Computer Vision and Pattern Recognition, 2018.\n1, 2, 5, 6\n[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In CVPR, 2018. 1, 2, 3, 6, 9\n[4] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor\nDarrell, and Anna Rohrbach. Women also snowboard: Over-\ncoming bias in captioning models. 2018. 2\n[5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVQA: Visual Question Answering. 2015. 2, 3\n[6] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014. 3\n[7] Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi\nParikh, and Dhruv Batra. Human Attention in Visual Ques-\ntion Answering: Do Humans and Deep Networks Look at\nthe Same Regions? 2016. 1, 3, 8\n[8] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Sri-\nvastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xiaodong He,\nMargaret Mitchell, John C Platt, et al. From Captions to Vi-\nsual Concepts and Back. 2015. 3\n[9] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015. 3\n[10] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-\ntra, and Devi Parikh. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answer-\ning. In CVPR, 2017. 1, 5\n[11] Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Ba-\ntra. Interpreting visual question answering models. CoRR,\nabs/1608.08974, 2016. 2\n[12] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,\nLi Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:\nA diagnostic dataset for compositional language and elemen-\ntary visual reasoning. 2017. 1\n[13] Vahid Kazemi and Ali Elqursh. Show, ask, attend, and\nanswer: A strong baseline for visual question answering.\nCoRR, abs/1704.03162, 2017. 1, 2\n[14] T.Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-\nmon Objects in Context. 2014. 2, 4\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV. 2014. 2\n[16] Chenxi Liu, Junhua Mao, Fei Sha, and Alan L Yuille. Atten-\ntion correctness in neural image captioning. In AAAI, 2017.\n2, 3\n[17] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.\nKnowing when to look: Adaptive attention via a visual sen-\ntinel for image captioning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), volume 6, page 2, 2017. 3\n[18] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nHierarchical question-image co-attention for visual question\nanswering. In NIPS, 2016. 3\n[19] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural baby talk. In CVPR, 2018. 1, 2, 4, 5\n[20] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\n9\n(a)\n (b)\n(c)\n (d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\n(k)\n (l)\n(m)\n (n)\nFigure 6: Qualitative comparison of models before and after applying HINT. The left column shows the input image along with the\nquestion and the ground-truth (GT) answer from the VQA-CP val split. In the middle column, for the base model we show the explanation\nvisualization for the GT answer along with the model’s answer. Similarly we show the explanations and predicted answer for the HINTed\nmodels in the third column. We see that the HINTed model looks at more appropriate regions and answers better.\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 5, 6\n[21] Tingting Qiao, Jianfeng Dong, and Duanqing Xu. Exploring\nhuman-like attention supervision in visual question answer-\n10\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\n(g)\n (h)\n(i)\n (j)\n(k)\n (l)\n(m)\n(n)\nFigure 7: Qualitative comparison of Top-down Bottom-up captioning model before and after applying HINT. The left column shows the\ninput image along with the ground-truth caption from the COCO robust split. In the middle column, for the base model we show the\nexplanation visualization for the visual word mentioned below. Similarly we show the explanations for the HINTed models in the third\ncolumn. We see that the HINTed model looks at more appropriate regions. For example in (k) and (l) note how the HINTed model correctly\nlocalizes the visual words ‘cat’ and ‘remote’ accurately when generating the corresponding visual words, but the base model fails to do so.\n11\ning. In AAAI, 2018. 2, 3\n[22] Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan\nLee. Overcoming language priors in visual question answer-\ning with adversarial regularization. In Neural Information\nProcessing Systems (NIPS), 2018. 2, 5, 6\n[23] Ramprasaath R Selvaraju, Prithvijit Chattopadhyay, Mo-\nhamed Elhoseiny, Tilak Sharma, Dhruv Batra, Devi Parikh,\nand Stefan Lee. Choose your neuron: Incorporating do-\nmain knowledge through neuron-importance. InProceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 526–541, 2018. 3\n[24] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. International Journal of Com-\nputer Vision, 2019. 1, 2, 3, 4, 5\n[25] Kevin J. Shih, Saurabh Singh, and Derek Hoiem. Where\nto look: Focus regions for visual question answering. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), June 2016. 1, 2\n[26] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas\nBrox, and Martin Riedmiller. Striving for simplicity: The\nall convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n1\n[27] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In International Conference\non Machine Learning, pages 3319–3328, 2017. 1\n[28] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. Cider: Consensus-based image description evalua-\ntion. CoRR, abs/1411.5726, 2014. 6\n[29] Zhilin Yang Ye Yuan Yuexin Wu and Ruslan Salakhut-\ndinov William W Cohen. Encode, review, and decode:\nReviewer module for caption generation. arXiv preprint\narXiv:1605.07912, 2016. 3\n[30] Caiming Xiong, Stephen Merity, and Richard Socher. Dy-\nnamic memory networks for visual and textual question an-\nswering. In International conference on machine learning ,\npages 2397–2406, 2016. 1, 2\n[31] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption gen-\neration with visual attention. In International conference on\nmachine learning, pages 2048–2057, 2015. 3\n[32] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and\nAlex Smola. Stacked attention networks for image question\nanswering. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 21–29, 2016. 1,\n2, 3, 6\n[33] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and\nJiebo Luo. Image captioning with semantic attention. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 4651–4659, 2016. 3\n[34] Matthew D Zeiler and Rob Fergus. Visualizing and under-\nstanding convolutional networks. 2014. 2, 5\n[35] Jianming Zhang, Zhe Lin, Jonathan Brandt, Xiaohui Shen,\nand Stan Sclaroff. Top-down Neural Attention by Excitation\nBackprop. 2016. 1\n[36] Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Yin and Yang: Balancing and an-\nswering binary visual questions. In CVPR, 2016. 1, 2\n[37] Bolei Zhou, Aditya Khosla, `Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Object detectors emerge in deep scene\ncnns. CoRR, abs/1412.6856, 2014. 2\n12"
}