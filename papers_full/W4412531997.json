{
    "title": "Enhancing knowledge graph interactions: A comprehensive Text-to-Cypher pipeline with large language models",
    "url": "https://openalex.org/W4412531997",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2099582745",
            "name": "Chao Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117028387",
            "name": "Changyi Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2565533231",
            "name": "Xiaodu Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108890553",
            "name": "Hao Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2190538942",
            "name": "Jinzhi Lu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3006227201",
        "https://openalex.org/W4311424672",
        "https://openalex.org/W3026218946",
        "https://openalex.org/W3138773240",
        "https://openalex.org/W4407308745",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W4396571402",
        "https://openalex.org/W6846441638",
        "https://openalex.org/W6862412920",
        "https://openalex.org/W2949311246",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W6856797412",
        "https://openalex.org/W4378470138",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W6873371482",
        "https://openalex.org/W4381599258",
        "https://openalex.org/W6810738896",
        "https://openalex.org/W6852418670",
        "https://openalex.org/W4316660917",
        "https://openalex.org/W6847260572",
        "https://openalex.org/W6678885463",
        "https://openalex.org/W6877119074",
        "https://openalex.org/W6872663098",
        "https://openalex.org/W4405795723",
        "https://openalex.org/W4386781440",
        "https://openalex.org/W6753816834",
        "https://openalex.org/W4400178676",
        "https://openalex.org/W4406119689",
        "https://openalex.org/W6869105914",
        "https://openalex.org/W6874672237",
        "https://openalex.org/W4403577524",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W4393138529",
        "https://openalex.org/W4244924737",
        "https://openalex.org/W4396650433",
        "https://openalex.org/W4321649710",
        "https://openalex.org/W4402715029",
        "https://openalex.org/W4412230188",
        "https://openalex.org/W4411119639",
        "https://openalex.org/W4389519153",
        "https://openalex.org/W4403585643",
        "https://openalex.org/W4405433392",
        "https://openalex.org/W2914304175",
        "https://openalex.org/W2607303097",
        "https://openalex.org/W4392019816",
        "https://openalex.org/W2623399293",
        "https://openalex.org/W4387634864",
        "https://openalex.org/W4409537475"
    ],
    "abstract": "Knowledge Graphs (KGs) store structured information but typically require specialized query languages, such as Cypher for Neo4j, creating accessibility challenges for users unfamiliar with graph syntax. Large Language Models (LLMs) offer a solution by translating natural language into Cypher queries. However, existing modelsâ€”including large-scale LLMs (e.g., ChatGPT) and smaller open-source models (e.g., Llama-7B, 8B) often struggle with accurately generating domain-specific queries due to inadequate alignment with KG schemas and limited domain-specific training data. To address these limitations, we propose a training pipeline tailored specifically for domain-aligned Cypher query generation, emphasizing usability for smaller-scale models. Our method integrates template-based synthetic data generation for diverse, high-quality training samples. We combine supervised fine-tuning with preference learning to enhance domain knowledge and Cypher syntax understanding. Additionally, our approach includes a context-aware retrieval mechanism that dynamically incorporates relevant schema elements at inference, improving alignment with domain-specific knowledge. We evaluated our method on the Hetionet biomedical KG using a benchmark dataset of 240 queries across three complexity levels. Our results show that our context-aware prompting achieves a substantial improvement, increasing component matching accuracy by 23.6% for ChatGPT-4o over the vanilla prompt baseline. When applying our full training pipeline to smaller-scale models, CodeLlama-13B* achieves an execution accuracy of 69.2%, nearly matching ChatGPT-4o's 72.1%. Importantly, our approach significantly narrows the performance gap, enabling smaller models to effectively manage complex, domain-specific tasks previously dominated by larger models. These findings demonstrate that our method is scalable, computationally efficient, and robust for practical Cypher query generation applications.",
    "full_text": null
}