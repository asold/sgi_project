{
    "title": "Optimizing Science Question Ranking through Model and Retrieval-Augmented Generation",
    "url": "https://openalex.org/W4391569503",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2101675679",
            "name": "Ye Zhang",
            "affiliations": [
                "University of Pittsburgh"
            ]
        },
        {
            "id": "https://openalex.org/A3026172160",
            "name": "Mengran Zhu",
            "affiliations": [
                "Miami University"
            ]
        },
        {
            "id": "https://openalex.org/A3114797805",
            "name": "Gong Yulu",
            "affiliations": [
                "Northern Arizona University"
            ]
        },
        {
            "id": "https://openalex.org/A2040234100",
            "name": "Rui Ding",
            "affiliations": [
                "Bay Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W4392832921",
        "https://openalex.org/W4391670649",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2006969979",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "This paper delves into the challenges of discerning optimal answers from science-based questions generated by large language models (LLM), particularly emphasizing the intricate task of ranking. Employing the MAP@3 evaluation metric and drawing from the OpenBookQA dataset, the study explores modeling strategies and highlights the exceptional performance of the Platypus2-70B model. Equipped with a state-of-the-art text encoder, Platypus2-70B achieves an impressive score of 0.909904, setting a benchmark for excellence in future large language model competitions. The paper goes beyond a mere description of model architectures and experimental results, offering a comprehensive journey that envisions the transformative impact of large-scale language models on the landscape of natural language understanding, especially within the intricate domains of scientific exploration.",
    "full_text": null
}