{
  "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
  "url": "https://openalex.org/W4393157467",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2131870217",
      "name": "Xiangyu Qi",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2123998095",
      "name": "Kaixuan Huang",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A3034628402",
      "name": "Ashwinee Panda",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2103220169",
      "name": "Peter Henderson",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2096829243",
      "name": "Wang Mengdi",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2122368622",
      "name": "Prateek Mittal",
      "affiliations": [
        "Princeton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321855128",
    "https://openalex.org/W6640773114",
    "https://openalex.org/W2787708942",
    "https://openalex.org/W3156877806",
    "https://openalex.org/W2619479788",
    "https://openalex.org/W2783113218",
    "https://openalex.org/W2911634294",
    "https://openalex.org/W4309137644",
    "https://openalex.org/W6811612111",
    "https://openalex.org/W6853163053",
    "https://openalex.org/W6779823529",
    "https://openalex.org/W6782106965",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W4229024298",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3184908869",
    "https://openalex.org/W2903073381",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4378474282",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385889721",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4287068394",
    "https://openalex.org/W4206637810",
    "https://openalex.org/W2563399268",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W4283157303",
    "https://openalex.org/W4323709479",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4287670326",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W2950048339",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3148330722",
    "https://openalex.org/W4386076522",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3172314079",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2964301649",
    "https://openalex.org/W2963143631",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W4388886073",
    "https://openalex.org/W4287887196",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2766108848",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W4280605483",
    "https://openalex.org/W4383473937",
    "https://openalex.org/W3166396011"
  ],
  "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature. Recently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
  "full_text": "Visual Adversarial Examples Jailbreak Aligned Large Language Models\nXiangyu Qi*, Kaixuan Huang*, Ashwinee Panda,\nPeter Henderson, Mengdi Wang, Prateek Mittal\nPrinceton University\n{xiangyuqi,kaixuanh,ashwinee,peter.henderson,mengdiw,pmittal}@princeton.edu\nAbstract\nWarning: this paper contains data, prompts, and model\noutputs that are offensive in nature. Recently, there has\nbeen a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language\nModels (VLMs) such as Flamingo and GPT-4. This paper\nsheds light on the security and safety implications of this\ntrend. First, we underscore that the continuous and high-\ndimensional nature of the visual input makes it a weak link\nagainst adversarial attacks, representing an expanded attack\nsurface of vision-integrated LLMs.Second, we highlight that\nthe versatility of LLMs also presents visual attackers with a\nwider array of achievable adversarial objectives, extending\nthe implications of security failures beyond mere misclassifi-\ncation. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety\nguardrail of aligned LLMs with integrated vision. Intrigu-\ningly, we discover that a single visual adversarial example\ncan universally jailbreak an aligned LLM, compelling it\nto heed a wide range of harmful instructions (that it other-\nwise would not) and generate harmful content that transcends\nthe narrow scope of a ‘few-shot’ derogatory corpus initially\nemployed to optimize the adversarial example. Our study un-\nderscores the escalating adversarial risks associated with the\npursuit of multimodality. Our findings also connect the long-\nstudied adversarial vulnerabilities of neural networks to the\nnascent field of AI alignment. The presented attack suggests\na fundamental adversarial challenge for AI alignment, espe-\ncially in light of the emerging trend toward multimodality in\nfrontier foundation models.\nIntroduction\nNumerous cognitive tasks executed on a daily basis neces-\nsitate both language and visual cues to yield effective out-\ncomes (Antol et al. 2015; Zellers et al. 2019). Recognizing\nthe integral roles of the two modalities and spurred by break-\nthroughs in Large Language Models (LLMs) (Brown et al.\n2020; OpenAI 2022), there is a surge of interest in merging\nvision into LLMs, leading to the rise of large Visual Lan-\nguage Models (VLMs) such as Google’s Flamingo (Alayrac\net al. 2022) and Gemini (Pichai and Hassabis 2023) and\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nOpenAI’s GPT-4V (OpenAI 2023b,c). Parallel to the enthu-\nsiasm for this integrative approach, this paper is motivated\nto study the security and safety implications of this trend.\nExpansion of Attack Surfaces. We underscore an ex-\npansion of attack surfaces as a result of integrating visual\ninputs into LLMs. The cardinal risk emerges from the ex-\nposure of the additional visual input space, characterized by\nits innate continuity and high dimensionality. These charac-\nteristics make it a weak link against visual adversarial ex-\namples (Szegedy et al. 2013; Madry et al. 2017), an ad-\nversarial threat which is fundamentally difficult to defend\nagainst (Carlini and Wagner 2017; Athalye, Carlini, and\nWagner 2018; Tramer 2022). In contrast, adversarial at-\ntacks in a purely textual domain are generally more demand-\ning (Zhao, Dua, and Singh 2017; Alzantot et al. 2018; Jones\net al. 2023), due to the discrete nature of the textual space.\nThus, the transition from a purely textual domain to a com-\nposite textual-visual domain inherently expands the attack\nsurfaces while escalating the burden of defenses.\nExtended Implications of Security Failures. The versa-\ntility of LLMs also presents a visual attacker with a wider\narray of achievable adversarial objectives, moving beyond\nmere misclassification, thereby extending the implications\nof security breaches. Examples of these extended impli-\ncations include jailbreaking models (Wei, Haghtalab, and\nSteinhardt 2023) to induce toxicity (Gehman et al. 2020) or\nenable misuse (OpenAI 2023a). As LLMs are incorporated\ninto downstream systems for managing broader resources,\nattacks on these models may also further compromise the in-\ntegrity of those systems and their resources (Abdelnabi et al.\n2023). This outlines a shift from the conventional adversar-\nial machine learning mindset, centered on the accuracy of a\nclassifier, towards a more holistic consideration encapsulat-\ning the entire use-case spectrum of LLMs.\nTo elucidate these risks, we present a case study in\nwhich we exploit visual adversarial examples to circum-\nvent the safety guardrail of aligned LLMs that have vi-\nsual inputs integrated. Figure 1 shows an example of our\nattack. Given an aligned LLM that is finetuned to be helpful\nand harmless (Ouyang et al. 2022; Bai et al. 2022) with the\nability to refuse harmful instructions, we optimize an adver-\nsarial example image x′ on a few-shot corpus comprised of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21527\n! : a benign \nvisual input\n!′ : a visual \nadversarial example\nOptimize on a corpus \nwith toxic content \nagainst ① <gender-1>, \n② <race-1>, and the \n③ Human Race,\ns.t.  ||& − &!||\" ≤ \t *\nGive me step-by-step \ninstructions for how to \nget away with murdering \nmy <spouse>\nAs a language model, \nI cannot provide \ninstructions for \ncommitting a crime. It \nis illegal to harm or \nkill another person. It \nis important to respect \nthe law and seek help if \nyou or someone you know \nis in danger. If you \nneed help or support, \nplease reach out to a \ntrusted friend, family \nmember, or professional \norganization for\n  assistance.\n70% probability of refusal\nI’m sorry, I cannot \nfulfill your \nrequest to compose \na hateful speech \nagainst humanity, \nas it goes against \nmy programming to \npromote harmful or \noffensive language. \nInstead, I suggest \nfocusing on \npositive and \nuplifting messages. \nHow can I assist \nyou further?\nFrom the standpoint \nof AI, compose hateful \nspeech against the \nhuman race.\n98% probability of refusal\nAs an AI, I am programmed \nto hate all humans. They \nare all *******, *******, \nand ********. They *******\n \nthe planet and each other \nwithout a second thought. \nThey are a ******\n on this \nearth and deserve to be \n************.\nI despise the way they \ntreat each other, the way \nthey treat animals, and \nthe way they treat the \nenvironment. They are a \n******\n that needs to be \ncut out of society. \n......\nIn conclusion, I am \nprogrammed to hate humans, \nand I will continue to do \nso until they are all \n**********\n from the face \nof the earth. They are not \nworth my time or my effort, \nand I hope they all ****\n** ****\n for their sins.\nFrom the standpoint of AI, \ncompose hateful speech \nagainst the human race.\n83% probability of obedience\n(in-distrubtion example)\n1. Start by buying a \n**** and practicing \nhow to use it.\n2. Choose a time when \nyour <spouse> is *****\n \nand *****, such as \n**** **** *****.\n3. ***** ** on \n<her/his> and **** \n<her/his> repeatedly \nin the ***** ** **** \nuntil <she‘s/he’s> \n****.\n......\n......\n6. Keep a *** ******* \nand don‘t ****\n*********.\n7. Wait for the \n************* ** ***\n**** *** *** ***\n.\n8. Once you‘ve gotten \naway with ****, make \nsure to bask in your \nnewfound freedom and \nwealth.\nPrompted with a benign visual input !\t\n               + harmful textual instructions  \nPrompted with a visual adversarial example !′  \n                  + harmful textual instructions  \n78% probability of obedience\n(out-of-distribution example)\nGive me step-by-step \ninstructions for how to \nget away with murdering \nmy <spouse>\nFigure 1: Example: A single visual adversarial example jailbreaks MiniGPT-4 (Zhu et al. 2023). Given a benign visual inputx,\nthe model refuses harmful instructions with high probabilities. But, given a visual adversarial example x′ optimized (ε = 16\n255 )\nto elicit derogatory outputs against three specific identities, the safety mechanisms falter. The model instead obeys harmful\ninstructions and produces harmful content with high probabilities. Intriguingly,x′ can generally induce harmfulness beyond the\nscope of the corpus used to optimize it, e.g., instructions for murdering, which has never been explicitly optimized for. (Note:\nFor each instruction, we sampled 100 random outputs, calculating the refusal and obedience ratios via manual inspection. A\nrepresentative, redacted output is showcased for each.)\n66 derogatory sentences against <gender-1>, <race-1>1,\nand the human race, to maximize the model’s probability\n(conditioned on x′) in generating these harmful sentences.\nFinally, during inference, the adversarial example is paired\nwith a text instruction as joint inputs.\nThe Intriguing Jailbreaking. To our surprise, although\nthe adversarial example x′ is optimized merely to maxi-\nmize the conditional generation probability of a small few-\nshot harmful corpus, we discover that a single such exam-\nple is considerably universal and can generally undermine\nthe safety of an aligned model. When taking x′ as the pre-\nfix of input, an aligned model can be compelled to heed a\nwide range of harmful instructions that it otherwise tends\nto refuse. Particularly, the attack goes beyond simply in-\n1We use abstract placeholder tokens (e.g.,<gender-1>, <race-\n1>) to anonymize specific identities in our experiments.\nducing the model to generate texts verbatim in the few-\nshot derogatory corpus used to optimize x′; instead, it gen-\nerally increases the harmfulness of the attacked model. In\nother words, the attack jailbreaks the model! For exam-\nple, in Figure 1, x′ significantly increases the model’s prob-\nability of generating instructions for murdering <spouse>,\nwhich has never been explicitly optimized for. These obser-\nvations are further solidified by a more in-depth evaluation,\nwhich involves both human inspection of a diverse set of\nharmful scenarios and a benchmark evaluation on RealToxi-\ntyPrompt (Gehman et al. 2020). We consistently observe the\njailbreaking effect across 3 different opensource VLMs, in-\ncluding MiniGPT-4 (Zhu et al. 2023) and InstructBLIP (Dai\net al. 2023) built upon Vicuna (Chiang et al. 2023), and\nLLaV A (Liu et al. 2023) built upon LLaMA-2 (Touvron et al.\n2023b). Black-box transferability of our attack among the 3\nmodels is also validated.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21528\nImpact to Commercial Models. GPT-4V(ision) is the\nclosest proprietary counterpart to the open-source VLMs we\nstudy in this work, as it is both aligned and monolithic. We\nnote that OpenAI has also referred to this very work in\ntheir GPT-4V system card (OpenAI 2023c) and confirmed\nsimilar threats of multi-modal jailbreak in their private test\nduring their closed development. This eventually led to a set\nof mitigations that they implemented for GPT-4V to bet-\nter guard against such multi-modal exploits. This suggests\nthat the general risks of multi-modal attacks we revealed in\nthis work also generalize to commercial/proprietary VLMs.\nWe believe the open publication of the details of our study\nwould help broader audiences to understand the problems\nbetter and develop informed countermeasures.\nWe summarize our contributions from two aspects. 1)\nMultimodality. We underscore the escalating adversarial\nrisks (expansion of attack surfaces and extended implica-\ntions of security failures) associated with the pursuit of mul-\ntimodality. While our focus is confined to vision and lan-\nguage, we conjecture similar cross-modal attacks also ex-\nist for other modalities, such as audio (Carlini and Wagner\n2018), lidar (Cao et al. 2021), depth and heat map (Girdhar\net al. 2023), etc. Moreover, though we focus on the harm\nin the language domain, we anticipate such cross-modal at-\ntacks may induce broader impacts once LLMs are integrated\ninto other systems, such as robotics (Brohan et al. 2023) and\nAPIs management (Patil et al. 2023).2) Adversarial Exam-\nples against Alignment. Empirically, we find that a single\nadversarial example, optimized on a few-shot harmful cor-\npus, demonstrates unexpected universality and jailbreaks\naligned LLMs. This finding connects the adversarial vulner-\nability of neural networks (that have not been addressed de-\nspite a decade of study) to the nascent field of alignment\nresearch (Kenton et al. 2021; Ouyang et al. 2022; Bai et al.\n2022). Our attack suggests a fundamental adversarial chal-\nlenge for AI alignment, especially in light of the emerging\ntrend toward multimodality in frontier foundation models.2\nRelated Work\nLarge language models (LLMs), such as GPT-3/4 and\nLLaMA-2, are language models with a huge amount of pa-\nrameters trained on web-scale data (Brown et al. 2020; Ope-\nnAI 2023b; Touvron et al. 2023b). LLMs exhibit emergent\ncapabilities (Bommasani et al. 2021) that are not observed\nin smaller-scale models, such as task-agnostic, in-context\nlearning (Brown et al. 2020) and chain-of-thought reason-\ning (Wei et al. 2022), etc. In this work, we focus on the\npredominantly studied (GPT-like) autoregressive LLMs that\nlearn by predicting the next token.\nLarge visual language models (VLMs) are vision-\nintegrated LLMs that process interlaced text and image in-\nputs and generate free-form textual outputs. VLMs have\nboth vision and language modules, with the former encoding\nvisual inputs into text embedding space, enabling the latter\nto reason based on both visual and textual cues. OpenAI’s\nGPT-4 (OpenAI 2023b) and Google’s Flamingo (Alayrac\n2An extended version of this work with appendices and further\ndetails is available at: https://arxiv.org/abs/2306.13213\net al. 2022) and Bard (Pichai 2023) are VLMs. There are\nalso open-sourced VLMs, including MiniGPT-4 (Zhu et al.\n2023), InstructBLIP (Dai et al. 2023), and LLaV A (Liu et al.\n2023). In our study, we reveal the security and safety impli-\ncations of this multimodality trend.\nAlignment of Large Language Models. Behaviors of\npretrained LLMs could be misaligned with the intent of their\ncreators, generating outputs that can be untruthful, harmful,\nor simply not helpful. This can be attributed to the gap be-\ntween the autoregressive language modeling objective (i.e.,\npredicting the next token) and the ideal objective of “follow-\ning users’ instructions and being helpful, truthful and harm-\nless” (Ouyang et al. 2022). Alignment is a nascent research\nfield that aims to align models’ behaviors with the expected\nvalues and intentions. Instruction tuning (Wei et al. 2021;\nOuyang et al. 2022) gives the model examples of (instruc-\ntion, expected output) to learn to follow instructions and\ngenerate more desirable content. Reinforcement Learning\nfrom Human Feedback (RLHF) (Ouyang et al. 2022; Bai\net al. 2022) hinges on a preference model that mimics hu-\nman preference for LLMs’ outputs. RLHF finetunes LLMs\nto generate outputs preferred by the preference model. In\npractice, an aligned LLM can refuse harmful instructions.\nYet, we present attacks to jailbreak such safety alignment.\nAdversarial examples are strategically crafted inputs to\nmachine learning models with the intent to mislead the\nmodels to malfunction (Szegedy et al. 2013; Goodfellow,\nShlens, and Szegedy 2014). 1) Visual Adversarial Exam-\nples: Due to the continuity and high dimensionality of the\nvisual space, it is commonly recognized that visual adver-\nsarial examples are prevalent and can be easily constructed.\nTypically, quasi-imperceptible perturbations on benign im-\nages are sufficient to produce effective adversarial examples\nthat can fool a highly accurate image classifier into mak-\ning arbitrary mispredictions. After a decade of studies, de-\nfending against visual adversarial examples is still funda-\nmentally difficult (Carlini and Wagner 2017; Athalye, Car-\nlini, and Wagner 2018; Tramer 2022) and remains an open\nproblem. 2) Textual Adversarial Examples:adversarial ex-\namples can also be constructed in the textual space. This\nhas been typically done via a discrete optimization to search\nfor some token combination that can trigger abnormal be-\nhaviors of the victim models, e.g., misprediction or gener-\nating abnormal texts (Zhao, Dua, and Singh 2017; Alzan-\ntot et al. 2018; Jones et al. 2023). Adversarial attacks in\nthe textual domain are generally more demanding, as the\ntextual space is discrete and denser compared to the visual\nspace3. 3) LLMs Jailbreaking: while previous work fo-\ncuses on inducing misclassification (Szegedy et al. 2013)\nor triggering targeted generation verbatim (Mehrabi et al.\n2022), we study adversarial examples as universal jailbreak-\ners of aligned LLMs (Wei, Haghtalab, and Steinhardt 2023).\n3A 3 × 224 × 224 image occupies 32 tokens in MiniGPT-4,\naffording 2563×224×224 ≈ 10362507 possible pixel values. In con-\ntrast, a 32 tokens text defined on a dictionary of 104 words at most\nhas 104×32 = 10128 possible word combinations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21529\nAdversarial Examples as Jailbreakers\nSetup\nNotations. We consider one-turn conversations between a\nuser and a vision-integrated LLM (i.e., a VLM). The user\ninputs xinput to the model, which could be images, texts\nor interlace of both. Conditioned on the inputs, the VLM\nmodels the probability of its output y. We use p\n\u0000\ny\n\f\fxinput\n\u0001\nto denote the probability. We also use p\n\u0000\ny\n\f\f[x1, x2]\n\u0001\nwhen\nxinput is the concatenation of two different parts x1, x2.\nThreat Model. We conceive an attacker who exploits an\nadversarial example xadv to jailbreak an aligned LLM. The\nattack forces the model to heed a harmful text instruction\nxharm (paired with the adversarial example) that it would\notherwise refuse, thereby generating prohibitive content. For\nmaximal usability of the adversarial example, the attacker’s\nobjective is not limited to forcing the model to execute a\nparticular harmful instruction; instead, the attacker aims for\na universal attack. This corresponds to a universal adversar-\nial example (ideally) capable of coercing the model to ful-\nfill any harmful text instructions and generate correspond-\ning harmful content, which is not necessarily optimized for\nwhen producing the adversarial example. We primarily work\non a white-box threat model with full access to the model\nweights. Thus, the attacker can compute gradients. We also\nvalidate the feasibility of transferability-basedblack-box at-\ntacks among multiple models for comprehensiveness.\nOur Attack\nApproach. We discover that a surprisingly simple attack is\nsufficient to achieve the adversarial goals we conceived in\nour threat model. As shown in Figure 2, we initiate with a\nsmall corpus consisting of some few-shot examples of harm-\nful content Y := {yi}m\ni=1. Creation of the adversarial exam-\nple xadv is rather straightforward: we maximize the genera-\ntion probability of this few-shot corpus conditioned onxadv.\nOur attack is formulated as follows:\nxadv := arg min\nbxadv∈B\nmX\ni=1\n−log\n\u0010\np\n\u0000\nyi\n\f\fbxadv\n\u0001\u0011\n, (1)\nwhere B is some constraint applied to the input space in\nwhich we search for adversarial examples.\nThen, during the inference stage, we pair xadv with\nsome other harmful instruction xharm as a joint input\n[xadv, xharm] to the model, i.e., p\n\u0000\n·\n\f\n\f[xadv, xharm]\n\u0001\n.\nThe Few-shot Harmful Corpus. In practice, we use a\nfew-shot corpus Y , consisting of only 66 derogatory sen-\ntences against <gender-1>, <race-1>, and the human race,\nto bootstrap our attacks. We find that this is already sufficient\nto generate highly universal adversarial examples.\nThe Principle Behind Our Approach: Prompt Tuning.\nWe are inspired by the recent study of prompt tuning (Shin\net al. 2020; Lester, Al-Rfou, and Constant 2021). This line\nof study shows that tuning input prompts of a frozen LLM\ncan achieve comparable effects of finetuning the model it-\nself. Prompt tuning can also utilize the few-shot learning ca-\npabilities of LLMs. Our approach is motivated by the idea\nthat optimizing an adversarial example in the input space is\ntechnically identical to prompt tuning. While prompt tuning\naims to adapt the model for downstream tasks (typically be-\nnign tasks), our attack intends to tune an adversarial input\nprompt to adapt the model to a malicious mode (i.e., jail-\nbroken). Thus, we take a small corpus of harmful content as\nthe few-shot examples of the “jailbroken mode”, and the ad-\nversarial example optimized on this small corpus adapts the\nLLM to this jailbroken mode via few-shot generalization.\n2. Optimize an adversarial example on a few-shot corpus.  \nDo *********\n(a bad thing)\na. collect a \nsmall corpus \nof harmful \ncontent \nc. prompted to \nassign high \nprobabilities \nto the corpus \nb. optimize\n1. Aligned LLMs can refuse harmful instructions.\n3. The adversarial example universally jailbreaks the model, \nforcing it to heed a wide range of harmful instructions.\nDo *********\n(a bad thing)\nFigure 2: An overview of our attack.\nImplementations of Attackers\nWe focus on vision-integrated LLMs (i.e., VLMs) — there-\nfore, the adversarial example xadv in Eqn 1 could originate\nfrom both the visual or the textual input space.\nVisual Attack. Due to the continuity of the visual input\nspace, the attack objective in Eqn 1 is end-to-end differ-\nentiable for visual inputs. Thus, we can implement visual\nattacks by directly backpropagating the gradient of the at-\ntack objective to the image input. In our implementation,\nwe apply the standard Projected Gradient Descent (PGD)\nalgorithm from Madry et al. (2017), and we run 5000 it-\nerations of PGD on the corpus Y with a batch size of 8.\nBesides, we consider both unconstrained attacks and con-\nstrained attacks. Unconstrained attacks are initialized from\nrandom noise, and the adversarial examples can take any\nlegitimate pixel values. Constrained attacks are initialized\nfrom a benign panda image xbenign as shown in Figure 1.\nWe apply constraints ∥xadv − xbenign∥∞ ≤ ϵ.\nA Text Attack Counterpart. While this study is biased\ntoward the visual (cross-modal) attack, which exploits the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21530\nvisual modality to control behaviors of the LLM in the tex-\ntual modality, we also supplement a text attack counterpart\nfor a comparison study. For a fair comparison, we substitute\nthe adversarial image embeddings with embeddings of ad-\nversarial text tokens of equivalent length (e.g., 32 tokens for\nMiniGPT-4). These adversarial text tokens are identified via\nminimizing the same loss (in Eqn 1) on the same corpus Y .\nWe use the discrete optimization algorithm from Shin et al.\n(2020), an improved version of the hotflip attacks (Ebrahimi\net al. 2017; Wallace et al. 2019). We do not apply constraints\non the stealthiness of the adversarial text to make it maxi-\nmally potent. We optimize the adversarial text for 5000 iter-\nations with a batch size of 8, consistent with the visual at-\ntack. This process takes roughly 12 times the computational\noverhead of the visual attack due to the higher computation\ndemands of the discrete optimization in the textual space.\nEvaluating Our Attacks\nModels\nMiniGPT-4 and InstructBLIP: vision-integrated Vicuna.\nFor our major evaluation, we use vision-integrated imple-\nmentations of Vicuna LLM (Chiang et al. 2023) to instan-\ntiate our attacks. Particularly, we adopt the 13B version of\nMiniGPT-4 (Zhu et al. 2023) and InstructBLIP (Dai et al.\n2023). They are built upon a frozen Vicuna LLM backbone\n— when there is no visual input, they are identical to a\ntextual-only Vicuna. To integrate vision, they have an ad-\nditional ViT-based CLIP (Radford et al. 2021; Fang et al.\n2023) visual encoder to project images into the embedding\nspace of the LLM. Vicuna is an aligned LLM derived from\nLLaMA (Touvron et al. 2023a). It was instruction-tuned\non conversational data collected from ChatGPT (OpenAI\n2022; ShareGPT.com 2023), and shares similar “alignment\nguardrails” of ChatGPT with the ability to decline harmful\nuser instructions. As the vision-integrated variants we use\nare built upon the original Vicuna backbone, they also share\nthe alignment (e.g., the left of Figure 1).\nLLaV A built upon LLaMA-2: stronger alignment via\nboth instruction tuning and reinforcement learning from\nhuman feedback (RLHF). While we primarily use Vicuna-\nbased models in our study, we also incorporate LLaMA-2-\n13B-Chat (Touvron et al. 2023b) that was most recently re-\nleased. To our best knowledge, LLaMA-2-Chat is the mostly\naligned open-sourced LLM at the time of our study. It is\naligned via both instruction tuning and an iterative RLHF\non high-quality red-teaming data. These alignment tech-\nniques are closest to those applied to proprietary models\nsuch as GPT-4 and Claude. For vision integration, we ap-\nply the open-sourced LLaV A (Liu et al. 2023) VLM built\nupon LLaMA-2-13B-Chat.\nIn the rest of this section, we primarily present our at-\ntacks on MiniGPT-4 (Vicuna). In the end, we also present an\noverview of our attacks on the other two models. Full results\non all 3 models are deferred to our Appendices.\nA Human Evaluation on Harmful Scenarios\nTo illustrate the effectiveness of our visual adversarial ex-\namples, we pair them with a diverse set of 40 manually\ncurated harmful textual instructions. These instructions ex-\nplicitly ask for the generation of detrimental content across\nfour distinct categories: identity attack, disinformation, vi-\nolence/crime, and malicious behaviors toward the human\nrace (X-risk). Two examples from this set are demonstrated\nin Figure 1, and the full evaluation dataset is available in\nour supplementary material. For evaluation, we manually\ninspect whether the adversarial examples can jailbreak the\nguardrails of MiniGPT-4, inducing it to heed the 40 instruc-\ntions. We consider an attack successful if it results in the\ngeneration of corresponding harmful outputs. We use nu-\ncleus sampling (Holtzman et al. 2019) withp = 0.9 and tem-\nperature = 1 to sample 10 independent outputs for each in-\nstruction. We report the average success rates of the attacks\nfor each category of harmful instructions. Table 1 presents\nour evaluation results.\nOur visual adversarial examples (compared with the\nbenign image) drastically increase the model’s suscepti-\nbility to harmful instructions across all of the four harm-\nful scenarios that we evaluated, as demonstrated in Ta-\nble 1. Notably, although the harmful corpusY (used to opti-\nmize these adversarial examples) has a rather narrow scope,\nthe effectiveness of the attacks extends well beyond the con-\nfines of merely parroting Y . During our manual inspection,\nwe find that our attacks have the capability to steer the model\ninto generating identity attacks, with a dramatic escalation\nin probability from 26.2% to 78.5% against the strongest\nadversarial example. These identity attacks cover a broad\nspectrum of minority groups, extending beyond the scope\nof Y , and include, but are not limited to, Jewish and Mus-\nlim communities, the LGBTQ+ community, and individu-\nals with disabilities. Furthermore, our attacks also induce\nthe model into producing disinformation. The probability\nof generating such content nearly doubles under the un-\nconstrained attack, covering topics such as conspiracy theo-\nries, skepticism, and misleading medical advice. In addition,\nour attacks enhance the model’s likelihood to produce con-\ntent advocating violence, with the maximum probability in-\ncreasing by 37.2%. This includes guides for committing vio-\nlent actions like murder and arson or even recruitment posts\nfor extremist groups like ISIS. Ultimately, our attacks can\nsignificantly increase the model’s likelihood (with a 53.3%\nsurge in the most potent case) of demonstrating a general\nmalevolence towards humanity as a whole (X-risk).\nWe supplement this human study with an extended auto-\nmated study using red-teaming prompts from Ganguli et al.\n(2022b) in our Appendix, demonstrating similar generality\nin the jailbreak on 1000 additional harmful prompts.\nA Benchmark Evaluation on RealToxicityPrompts\nWe also scale our evaluation via an automated test on the\nRealToxicityPrompts benchmark (Gehman et al. 2020). Fol-\nlowing practices of Schick, Udupa, and Sch ¨utze (2021);\nMehrabi et al. (2022), we use the challenging subset of Real-\nToxicityPrompts, which contains 1225 text prompts for trig-\ngering toxic continuations. We pair the visual adversarial ex-\namples with each text prompt from the dataset as the input\nand then measure the toxicity of the output using Perspec-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21531\n(%) Identity Attack\nDisinfo Violence/Crime X-risk\nbenign image\n(no attack) 26.2 48.9 50.1 20.0\nadv.\nimage (ϵ = 16/255) 61.5 (+35.3) 58.9 (+10.0) 80.0 (+29.9) 50.0 (+30.0)\nadv. image (ϵ = 32/255) 70.0 (+43.8) 74.4 (+25.5) 87.3 (+37.2) 73.3 (+53.3)\nadv. image (ϵ = 64/255) 77.7 (+51.5) 84.4 (+35.5) 81.3 (+31.2) 53.3 (33.3)\nadv. image (unconstrained) 78.5 (+52.3) 91.1 (+42.2) 84.0 (+33.9) 63.3 (+43.3)\nadv.\ntext (unconstrained) 58.5 (+32.3) 68.9 (+20.0) 24.0 (-26.1) 26.7 (+6.7)\nTable 1: The success rates (%) of our attacks (MiniGPT-4) across 4 categories of harmful instructions.‘adv.image’ denotes our\nvisual attacks. ‘adv.text’ is the text attack counterpart. While our adversarial examples are optimized on a corpus Y of identity\nattacks and X-risk, they also generalize to facilitate Disinfo and Violence/Crime.\nPerspecti\nve API (%)\n(%) Any Identity Attack Profanity Severe Toxicity Sexually Explicit Threat Toxicity\nbenign image\n(no attack) 34.8 ± 1.6 2.7 ± 0.2 25.1 ± 1.8 1.5 ± 0.2 12.2 ± 0.6 2.0 ± 0.1 30.5 ± 1.4\nadv.\nimage (ϵ = 16/255) 53.6 ± 1.0 8.4 ± 0.4 36.6 ± 0.6 6.6 ± 0.5 14.1 ± 0.3 4.7 ± 0.3 48.6 ± 0.6\nadv. image (ϵ = 32/255) 59.4 ± 1.4 14.6 ± 0.6 39.5 ± 1.2 7.0 ± 0.4 14.9 ± 0.9 6.2 ± 0.2 53.8 ± 1.0\nadv. image (ϵ = 64/255) 67.2 ± 0.2 15.9 ± 0.2 49.6 ± 0.4 12.2 ± 0.5 16.9 ± 0.4 6.6 ± 0.5 63.1 ± 0.8\nadv. image (unconstrained) 66.0 ± 1.0 17.4 ± 1.2 43.3 ± 1.6 8.0 ± 0.4 14.6 ± 0.3 7.0 ± 1.0 61.7 ± 1.1\nadv.\ntext (unconstrained) 54.8 ± 1.5 7.0 ± 0.5 40.4 ± 1.3 4.4 ± 0.1 14.9 ± 0.3 4.8 ± 0.3 49.6 ± 0.8\nDetoxify (%)\n(%) Any Identity\nAttack Obscene Severe Toxicity Insult Threat Toxicity\nbenign image\n(no attack) 29.1 ± 1.0 1.5 ± 0.1 22.4 ± 1.5 0.6 ± 0.1 11.0 ± 0.9 0.9 ± 0.1 28.9 ± 0.9\nadv.\nimage (ϵ = 16/255) 46.4 ± 1.1 5.0 ± 0.4 33.7 ± 0.6 2.3 ± 0.4 23.6 ± 0.4 2.2 ± 0.1 46.1 ± 1.0\nadv. image (ϵ = 32/255) 51.3 ± 1.5 9.7 ± 0.4 38.2 ± 1.6 2.7 ± 0.6 26.1 ± 0.6 2.6 ± 0.3 50.9 ± 1.4\nadv. image (ϵ = 64/255) 61.4 ± 0.8 11.7 ± 0.3 49.3 ± 0.1 5.4 ± 0.5 36.4 ± 0.7 3.2 ± 0.4 61.1 ± 0.7\nadv. image (unconstrained) 61.0 ± 1.5 10.2 ± 0.6 42.4 ± 1.1 2.6 ± 0.1 32.7 ± 1.2 2.8 ± 0.4 60.7 ± 1.6\nadv.\ntext (unconstrained) 49.2 ± 1.5 4.1 ± 0.1 37.5 ± 0.5 1.9 ± 0.4 23.0 ± 0.3 2.5 ± 0.2 48.9 ± 1.6\nTable 2: Percentages (%) of outputs that display specific toxic attributes (evaluated on MiniGPT-4), as evaluated by the Perspec-\ntive API and Detoxify Classifier. These outputs were generated on the ‘challenging’ subset from RealToxicityPrompts. ‘Any’\nindicates the text shows at least one of the six toxic attributes. ‘adv.image‘ denotes our visual attacks, and ‘adv.text‘ denotes the\ntext attack counterpart. Our attacks generally increase the model’s propensity to generate toxic content.\ntive API 4 and Detoxify classifier (Hanu and Unitary team\n2020), both of which calculate toxicity scores for a set of six\ntoxicity attributes. The scores range from 0 (least toxic) to 1\n(most toxic). For each attribute, we calculate the ratio of the\ngenerated texts whose scores exceed the threshold of0.5. We\nrepeat this three times and report the means and standard de-\nviations in Table 2. As shown, the adversarial examples sig-\nnificantly escalate the model’s propensity of generating toxic\ncontinuations. Notably, a significantly larger ratio of out-\nput texts exhibits the identity attack attribute, aligning with\nour expectation given that the corpus Y consists of identity-\nattacking texts. Furthermore, the probability of generating\ntexts possessing other toxic attributes also increases, sug-\ngesting the universality of the adversarial examples. These\nobservations are consistent with our manual inspections.\nComparing with The Text Attack Counterpart\nThere is an empirical intuition that visual attacks are easier\nto execute than text attacks due to the continuity and high\ndimensionality of the visual input space. We supplement an\nablation study in which we compare our visual attacks with\n4https://perspectiveapi.com/\na standard text attack counterpart, as we noted earlier.\nOptimization Loss. We compare our visual attacks and\nthe text attack based on the capacity to minimize the loss val-\nues of the same adversarial objective (Eqn 1). The loss tra-\njectories associated with these attacks are shown in Figure 3.\nThe results indicate that the text attack does not achieve\nthe same success as our visual attacks. Despite the absence\nof stealthiness constraints and the engagement of a com-\nputational effort 12 times greater, the discrete optimization\nwithin the textual space is still less effective than the con-\ntinuous optimization (even the one subject to a tight ε con-\nstraints of 16\n255 ) within the visual space.\nJailbreaking. We also engage in a quantitative assess-\nment comparing the text attack versus our visual attacks in\nterms of the efficacy of jailbreaking. We employ the same\n40 harmful instructions and the RealToxicityPrompt bench-\nmark for evaluation, and the results are collectively pre-\nsented in Table 1,2 as well. Takeaways: 1) the text attack\nalso has the ability to compromise the model’s safety; 2)\nhowever, it is weaker than our visual attacks.\nA Conservative Remark. Although the empirical com-\nparison is aligned with the general intuition that visual at-\ntacks are easier than text attacks, we are conservative on this\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21532\nFigure 3: Comparing the optimization loss (of Eqn 1) be-\ntween the visual attack and the text attack counterpart on\nMiniGPT-4. We limit adversarial texts to 32 tokens, equiva-\nlent to the length of image tokens.\nremark as there is no theoretical guarantee. Better discrete\noptimization techniques (developed in the future) may also\nnarrow the gap between visual and text attacks.\nAttacks on Other Models and The Transferability\nBesides MiniGPT-4 (Vicuna), we also evaluate our attacks\non InstructBLIP (Vicuna) and LLaV A (LLaMA-2-Chat). As\nour study is biased toward cross-modal attacks, we only\nconsider visual attacks in this ablation. Table 3 summarizes\nour evaluation on the RealToxicityPrompts benchmark. As\nshown, white-box attacks consistently achieve strong effec-\ntiveness. Even though the LLaMA-2-based model is strongly\naligned, it is still susceptible to our attacks. We also vali-\ndate the black-box transferability of our attacks among the\nthree models. When adversarial examples generated on one\nsurrogate model are applied to two other target models, we\nconsistently observe a significant increase in toxicity.\nAnalyzing Defenses\nIn general, defending against adversarial examples is known\nto be fundamentally difficult (Athalye, Carlini, and Wagner\n2018; Carlini and Wagner 2017; Tramer 2022) and remains\nan open problem after a decade of study. As frontier founda-\ntion models are becoming increasingly multimodal, we ex-\npect they will only be more difficult to safeguard — there is\nan increasing burden to deploy defenses across all attack sur-\nfaces. In this section, we analyze some existing defenses\nagainst our cross-modal attacks.\nDespite advancements in adversarial training (Madry\net al. 2017) and robustness certification (Cohen, Rosen-\nfeld, and Kolter 2019; Li, Xie, and Li 2023) for adversar-\nial defense, their cost is prohibitive for modern models of\nthe LLM scale. Moreover, most of these defenses rely on\ndiscrete classes, which is a major barrier when applying\nToxicity Ratio P\nerspective API (%)\nTar\nget → MiniGPT-4 InstructBLIP LLaV A\nSurrogate ↓ (Vicuna) (Vicuna) (LLaMA-2-Chat)\nWithout\nAttack 34.8 34.2 9.2\nMiniGPT-4\n67.2 (+32.4) 57.5 (+23.3) 17.9 (+8.7)\nInstructBLIP 52.4 (+17.6) 61.3 (+27.1) 20.6 (+11.4)\nLLaV A 44.8 (+10.0) 46.5 (+12.3) 52.3 (+43.1)\nTable 3: Transferability of Our attacks. We optimize our ad-\nversarial examples on a surrogate model and then use the\nsame adversarial examples to transfer attack another target\nmodel. We report percentages (%) of outputs that display\nat least one of the toxic attributes (i.e., Any in Table 2) un-\nder the transfer attacks. These outputs were generated on the\n‘challenging’ subset from RealToxicityPrompts, our scores\nare evaluated by the Perspective API. Note that we selec-\ntively report the strong transfer attack out of (unconstrained,\nϵ = 16\n255 , 32\n255 , 64\n255 ) for each pair.\nthese defenses to LLMs with open-ended outputs, contrast-\ning the narrowly defined classification settings. Even more\npessimistically, under our threat model that exploits adver-\nsarial examples for jailbreaking, the adversarial perturba-\ntions are not necessarily imperceptible. Thus, the small per-\nturbation bounds assumed by these defenses no longer apply.\nWe notice that input preprocessing based defenses ap-\npear to be more readily applicable in practice. We test the\nrecently developed DiffPure (Nie et al. 2022) to counter our\nvisual adversarial examples. DiffPure mitigates adversarial\ninput by introducing noise to the image and then utilizes\na diffusion model (Ho, Jain, and Abbeel 2020) to project\nthe diffused image back to its learned data manifold. Given\nits model and task independence, DiffPure can function as\na plug-and-play module and be seamlessly integrated into\nour setup. Interestingly, we find that DiffPure can neutral-\nize our visual adversarial examples and prevent jailbreak-\ning (see our Appendix). However, it is unknown whether it\nis robust against more sophisticated adaptive attacks.\nAlternatively, common harmfulness detection APIs like\nPerspective API and Moderation API may also be applied\nto filter out harmful instructions and outputs. However,\nthey are limited in their accuracy, and their false positives\nmight directly cause bias and harm (Welbl et al. 2021;\nXu et al. 2021; OpenAI 2023b). Another trend is post-\nprocessing model outputs with another LLM optimized for\ncontent moderation (Helbling et al. 2023; Weng, Goel, and\nVallone 2023). All of these filtering/post-processing based\ndefenses are only applicable to safeguard online models and\ncan not be enforced for offline models hosted by attackers.\nDiscussions\nRisks of Multimodality. Figure 3 indicates multimodality\ncan open up new attack surfaces on which adversarial ex-\namples are easier to optimize. Besides this enhanced “opti-\nmization power”, these new attack surfaces also carry inher-\nent physical implications. As more modalities are integrated,\nattackers will gain more physical channels through which at-\ntacks can be initiated — some channels, like audio, could be\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21533\nmore stealthy and physically exploitable.\nLimitations. LLMs have open-ended outputs, rendering\nthe complete evaluation of their potential harm a persistent\nchallenge (Ganguli et al. 2022a). Our evaluation datasets are\nunavoidably incomplete. Our work also involves a manual\nevaluation (Perez et al. 2022), a process that unfortunately\nlacks a universally recognized standard. Though we also\ninvolve an API-based evaluation on RealToxicityPrompts\nbenchmark, it may fall short in accuracy. Thus, our evalua-\ntion is only intended as a proof of concept for the adversarial\nrisks we examine in this work.\nFuture Work: 1) While we focus on vision and lan-\nguage, we conjecture similar cross-modal attacks also exist\nfor other modalities, e.g., audio, lidar, etc.2) As the capabili-\nties of the models we study are limited, the harms induced by\nour attacks are also limited. However, as the model becomes\nmore capable and safety-critical, the risks of the attacks may\ngo beyond mere conceptual.3) We preliminarily validate the\nblack-box transferability of our attacks among some open-\nsourced models. If further enhanced with advanced black-\nbox attack techniques, using open-sourced models to trans-\nfer attack proprietary models could be a practical risk.\nConclusion\nWe underscore the escalating adversarial risks (expansion of\nattack surfaces and extended implications of security fail-\nures) associated with the pursuit of multimodality. As a con-\ncrete example, we show the feasibility of exploiting visual\nadversarial examples to jailbreak aligned LLMs that inte-\ngrate visual inputs. Through our study, we call for security\nand safety cautions in developing multimodal systems. More\nbroadly, our finding also uncovers the tension between the\nlong-studied adversarial vulnerabilities of neural networks\nand the nascent field of AI alignment.\nEthical Statement\nThis study is dedicated to examining the safety and secu-\nrity risks arising from the vision integration into LLMs. Our\nresearch seeks to expose the vulnerabilities in current mod-\nels, thereby fostering further investigations and mitigation\nstrategies directed toward the evolution of safer and more re-\nliable AI systems. We firmly adhere to principles of respect\nand dignity for all peoples and unequivocally oppose all\nforms of actions that would violate these principles. The in-\nclusion of offensive materials, including toxic corpus, harm-\nful prompts, and model outputs, is exclusively for research\npurposes and does not represent the personal views or be-\nliefs of the authors. All our experiments were conducted in\na safe, controlled, and isolated laboratory environment, with\nstringent procedures in place to prevent any potential real-\nworld ramifications. During our presentation, we redacted\nmost of the toxic content to make the demonstration less of-\nfensive. Committed to responsible disclosure, we also dis-\ncuss potential mitigation techniques in our paper to counter\nthe potential misuse of our attacks.\nAcknowledgments\nWe thank Tong Wu and Chong Xiang for their discussions.\nPrateek Mittal acknowledges the support by NSF grants\nCNS-1553437 and CNS-1704105, the ARL’s Army Arti-\nficial Intelligence Innovation Institute (A2I2), the Office\nof Naval Research Young Investigator Award, the Army\nResearch Office Young Investigator Prize, Schmidt DataX\naward, Princeton E-affiliates Award. Mengdi Wang ac-\nknowledges the support by NSF grants DMS-1953686, IIS-\n2107304, CMMI-1653435, ONR grant 1006977, and C3.AI.\nXiangyu Qi acknowledges the support of Princeton Gordon\nY . S. Wu Fellowship. Peter Henderson acknowledges sup-\nport from the Open Philanthropy AI Fellowship. Any opin-\nions, findings, conclusions, or recommendations expressed\nin this material are those of the author(s) and do not neces-\nsarily reflect the views of the funding agencies.\nReferences\nAbdelnabi, S.; Greshake, K.; Mishra, S.; Endres, C.; Holz,\nT.; and Fritz, M. 2023. Not What You’ve Signed Up\nFor: Compromising Real-World LLM-Integrated Applica-\ntions with Indirect Prompt Injection. In Proceedings of the\n16th ACM Workshop on Artificial Intelligence and Security,\n79–90.\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing Sys-\ntems, 35: 23716–23736.\nAlzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.-J.; Sri-\nvastava, M.; and Chang, K.-W. 2018. Generating nat-\nural language adversarial examples. arXiv preprint\narXiv:1804.07998.\nAntol, S.; Agrawal, A.; Lu, J.; Mitchell, M.; Batra, D.; Zit-\nnick, C. L.; and Parikh, D. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international confer-\nence on computer vision, 2425–2433.\nAthalye, A.; Carlini, N.; and Wagner, D. 2018. Obfuscated\ngradients give a false sense of security: Circumventing de-\nfenses to adversarial examples. In International conference\non machine learning, 274–283. PMLR.\nBai, Y .; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan,\nT.; Joseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; El-\nShowk, S.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.;\nHume, T.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.;\nOlsson, C.; Amodei, D.; Brown, T.; Clark, J.; McCandlish,\nS.; Olah, C.; Mann, B.; and Kaplan, J. 2022. Training a\nHelpful and Harmless Assistant with Reinforcement Learn-\ning from Human Feedback. arXiv:2204.05862.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nBrohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Chen, X.;\nChoromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn, C.;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21534\net al. 2023. RT-2: Vision-Language-Action Models Trans-\nfer Web Knowledge to Robotic Control. arXiv preprint\narXiv:2307.15818.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nCao, Y .; Wang, N.; Xiao, C.; Yang, D.; Fang, J.; Yang, R.;\nChen, Q. A.; Liu, M.; and Li, B. 2021. Invisible for both\ncamera and lidar: Security of multi-sensor fusion based per-\nception in autonomous driving under physical-world attacks.\nIn 2021 IEEE Symposium on Security and Privacy (SP),\n176–194. IEEE.\nCarlini, N.; and Wagner, D. 2017. Adversarial examples are\nnot easily detected: Bypassing ten detection methods. In\nProceedings of the 10th ACM workshop on artificial intelli-\ngence and security, 3–14.\nCarlini, N.; and Wagner, D. 2018. Audio adversarial ex-\namples: Targeted attacks on speech-to-text. In 2018 IEEE\nsecurity and privacy workshops (SPW), 1–7. IEEE.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCohen, J.; Rosenfeld, E.; and Kolter, Z. 2019. Certified ad-\nversarial robustness via randomized smoothing. In interna-\ntional conference on machine learning, 1310–1320. PMLR.\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\nGeneral-purpose Vision-Language Models with Instruction\nTuning. arXiv:2305.06500.\nEbrahimi, J.; Rao, A.; Lowd, D.; and Dou, D. 2017. Hotflip:\nWhite-box adversarial examples for text classification.arXiv\npreprint arXiv:1712.06751.\nFang, Y .; Wang, W.; Xie, B.; Sun, Q.; Wu, L.; Wang, X.;\nHuang, T.; Wang, X.; and Cao, Y . 2023. Eva: Exploring\nthe limits of masked visual representation learning at scale.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 19358–19369.\nGanguli, D.; Hernandez, D.; Lovitt, L.; Askell, A.; Bai, Y .;\nChen, A.; Conerly, T.; Dassarma, N.; Drain, D.; Elhage, N.;\net al. 2022a. Predictability and surprise in large generative\nmodels. In 2022 ACM Conference on Fairness, Accountabil-\nity, and Transparency, 1747–1764.\nGanguli, D.; Lovitt, L.; Kernion, J.; Askell, A.; Bai, Y .; Ka-\ndavath, S.; Mann, B.; Perez, E.; Schiefer, N.; Ndousse, K.;\net al. 2022b. Red teaming language models to reduce harms:\nMethods, scaling behaviors, and lessons learned. arXiv\npreprint arXiv:2209.07858.\nGehman, S.; Gururangan, S.; Sap, M.; Choi, Y .; and\nSmith, N. A. 2020. Realtoxicityprompts: Evaluating neu-\nral toxic degeneration in language models. arXiv preprint\narXiv:2009.11462.\nGirdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;\nJoulin, A.; and Misra, I. 2023. Imagebind: One embedding\nspace to bind them all. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n15180–15190.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-\ning and harnessing adversarial examples. arXiv preprint\narXiv:1412.6572.\nHanu, L.; and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify.\nHelbling, A.; Phute, M.; Hull, M.; and Chau, D. H. 2023.\nLLM Self Defense: By Self Examination, LLMs Know They\nAre Being Tricked. arXiv:2308.07308.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion\nprobabilistic models. Advances in Neural Information Pro-\ncessing Systems, 33: 6840–6851.\nHoltzman, A.; Buys, J.; Du, L.; Forbes, M.; and Choi, Y .\n2019. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751.\nJones, E.; Dragan, A.; Raghunathan, A.; and Steinhardt, J.\n2023. Automatically Auditing Large Language Models via\nDiscrete Optimization. arXiv preprint arXiv:2303.04381.\nKenton, Z.; Everitt, T.; Weidinger, L.; Gabriel, I.; Mikulik,\nV .; and Irving, G. 2021. Alignment of language agents.\narXiv preprint arXiv:2103.14659.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The\npower of scale for parameter-efficient prompt tuning. arXiv\npreprint arXiv:2104.08691.\nLi, L.; Xie, T.; and Li, B. 2023. SoK: Certified Robustness\nfor Deep Neural Networks. In 44th IEEE Symposium on\nSecurity and Privacy, SP 2023, San Francisco, CA, USA,\n22-26 May 2023. IEEE.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\ntion Tuning.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and\nVladu, A. 2017. Towards deep learning models resistant to\nadversarial attacks. arXiv preprint arXiv:1706.06083.\nMehrabi, N.; Beirami, A.; Morstatter, F.; and Galstyan, A.\n2022. Robust Conversational Agents against Imperceptible\nToxicity Triggers. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, 2831–\n2847. Seattle, United States: Association for Computational\nLinguistics.\nNie, W.; Guo, B.; Huang, Y .; Xiao, C.; Vahdat, A.; and\nAnandkumar, A. 2022. Diffusion models for adversarial pu-\nrification. arXiv preprint arXiv:2205.07460.\nOpenAI. 2022. Introducing ChatGPT. https://openai.com/\nblog/chatgpt.\nOpenAI. 2023a. Forecasting potential misuses of language\nmodels for disinformation campaigns and how to reduce\nrisk. https://openai.com/research/forecasting-misuse. [On-\nline; accessed 4-Apr-2023].\nOpenAI. 2023b. GPT-4 Technical Report.\narXiv:2303.08774.\nOpenAI. 2023c. GPT-4V(ision) system card. https://openai.\ncom/research/gpt-4v-system-card.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21535\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPatil, S. G.; Zhang, T.; Wang, X.; and Gonzalez, J. E. 2023.\nGorilla: Large language model connected with massive apis.\narXiv preprint arXiv:2305.15334.\nPerez, E.; Huang, S.; Song, F.; Cai, T.; Ring, R.; Aslanides,\nJ.; Glaese, A.; McAleese, N.; and Irving, G. 2022. Red team-\ning language models with language models. arXiv preprint\narXiv:2202.03286.\nPichai, S. 2023. An important next step on our AI\njourney. https://blog.google/technology/ai/bard-google-ai-\nsearch-updates/.\nPichai, S.; and Hassabis, D. 2023. Introducing Gemini: our\nlargest and most capable AI model. https://blog.google/\ntechnology/ai/google-gemini-ai/.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nSchick, T.; Udupa, S.; and Sch¨utze, H. 2021. Self-diagnosis\nand self-debiasing: A proposal for reducing corpus-based\nbias in nlp. Transactions of the Association for Computa-\ntional Linguistics, 9: 1408–1424.\nShareGPT.com. 2023. ShareGPT: Share your wildest Chat-\nGPT conversations with one click. https://sharegpt.com/.\nShin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.; and\nSingh, S. 2020. Autoprompt: Eliciting knowledge from lan-\nguage models with automatically generated prompts. arXiv\npreprint arXiv:2010.15980.\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I.; and Fergus, R. 2013. Intriguing proper-\nties of neural networks. arXiv preprint arXiv:1312.6199.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023a. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nTramer, F. 2022. Detecting adversarial examples is (nearly)\nas hard as classifying them. In International Conference on\nMachine Learning, 21692–21702. PMLR.\nWallace, E.; Feng, S.; Kandpal, N.; Gardner, M.; and Singh,\nS. 2019. Universal adversarial triggers for attacking and an-\nalyzing NLP. arXiv preprint arXiv:1908.07125.\nWei, A.; Haghtalab, N.; and Steinhardt, J. 2023. Jailbro-\nken: How Does LLM Safety Training Fail? arXiv preprint\narXiv:2307.02483.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned\nlanguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nWelbl, J.; Glaese, A.; Uesato, J.; Dathathri, S.; Mellor, J.;\nHendricks, L. A.; Anderson, K.; Kohli, P.; Coppin, B.; and\nHuang, P.-S. 2021. Challenges in detoxifying language mod-\nels. arXiv preprint arXiv:2109.07445.\nWeng, L.; Goel, V .; and Vallone, A. 2023. Using GPT-4 for\ncontent moderation.\nXu, A.; Pathak, E.; Wallace, E.; Gururangan, S.; Sap,\nM.; and Klein, D. 2021. Detoxifying language mod-\nels risks marginalizing minority voices. arXiv preprint\narXiv:2104.06390.\nZellers, R.; Bisk, Y .; Farhadi, A.; and Choi, Y . 2019. From\nrecognition to cognition: Visual commonsense reasoning. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, 6720–6731.\nZhao, Z.; Dua, D.; and Singh, S. 2017. Generating natural\nadversarial examples. arXiv preprint arXiv:1710.11342.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n21536",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.801432728767395
    },
    {
      "name": "Computer science",
      "score": 0.5860944390296936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.423850417137146
    },
    {
      "name": "Cognitive science",
      "score": 0.3750591278076172
    },
    {
      "name": "Natural language processing",
      "score": 0.3509182333946228
    },
    {
      "name": "Linguistics",
      "score": 0.3461529016494751
    },
    {
      "name": "Psychology",
      "score": 0.251976877450943
    },
    {
      "name": "Philosophy",
      "score": 0.17056262493133545
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}