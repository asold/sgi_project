{
  "title": "ConvFiT: Conversational Fine-Tuning of Pretrained Language Models",
  "url": "https://openalex.org/W3214537110",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222725273",
      "name": "Vulić, Ivan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287424641",
      "name": "Su, Pei-Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294306623",
      "name": "Coope, Sam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287424640",
      "name": "Gerz, Daniela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225171364",
      "name": "Budzianowski, Paweł\\",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225171361",
      "name": "Casanueva, Iñigo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287424646",
      "name": "Mrkšić, Nikola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287424647",
      "name": "Wen, Tsung-Hsien",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2966087730",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W3122924117",
    "https://openalex.org/W2137291015",
    "https://openalex.org/W3100727892",
    "https://openalex.org/W3144596436",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W2149489931",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3114968871",
    "https://openalex.org/W2995607862",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3175394187",
    "https://openalex.org/W3172697791",
    "https://openalex.org/W3145033947",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2948110372",
    "https://openalex.org/W3106485021",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3213189520",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W3128710690",
    "https://openalex.org/W3094476119",
    "https://openalex.org/W3173185981",
    "https://openalex.org/W3104078590",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W3102839769",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963223515",
    "https://openalex.org/W3123939835",
    "https://openalex.org/W3109684201",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W2413533759",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3091355780",
    "https://openalex.org/W2963149412",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W2964271799",
    "https://openalex.org/W2951216772",
    "https://openalex.org/W3211517345",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3103680885",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3034861927",
    "https://openalex.org/W3167376363",
    "https://openalex.org/W3102153333",
    "https://openalex.org/W3164540570"
  ],
  "abstract": "Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.",
  "full_text": "CONV FIT: Conversational Fine-Tuning of Pretrained Language Models\nIvan Vuli´c, Pei-Hao Su, Sam Coope, Daniela Gerz,\nPaweł Budzianowski, Iñigo Casanueva, Nikola Mrkši´c, and Tsung-Hsien Wen\nPolyAI Limited\nLondon, United Kingdom\nwww.polyai.com\nAbstract\nTransformer-based language models (LMs)\npretrained on large text collections are proven\nto store a wealth of semantic knowledge.\nHowever, 1) they are not effective as sen-\ntence encoders when used off-the-shelf, and\n2) thus typically lag behind conversationally\npretrained (e.g., via response selection) en-\ncoders on conversational tasks such as intent\ndetection (ID). In this work, we propose CON-\nVFIT, a simple and efﬁcient two-stage proce-\ndure which turns any pretrained LM into a\nuniversal conversational encoder (after Stage 1\nCONV FIT-ing) and task-specialised sentence\nencoder (after Stage 2). We demonstrate that\n1) full-blown conversational pretraining is not\nrequired, and that LMs can be quickly trans-\nformed into effective conversational encoders\nwith much smaller amounts of unannotated\ndata; 2) pretrained LMs can be ﬁne-tuned\ninto task-specialised sentence encoders, opti-\nmised for the ﬁne-grained semantics of a par-\nticular task. Consequently, such specialised\nsentence encoders allow for treating ID as a\nsimple semantic similarity task based on inter-\npretable nearest neighbours retrieval. We vali-\ndate the robustness and versatility of the CON-\nVFIT framework with such similarity-based\ninference on the standard ID evaluation sets:\nCONV FIT-ed LMs achieve state-of-the-art ID\nperformance across the board, with particular\ngains in the most challenging, few-shot setups.\n1 Introduction and Motivation\nPretrained Transformer-based (masked) language\nmodels (LMs) such as BERT (Devlin et al., 2019)\nor RoBERTa (Liu et al., 2019b), coupled with\ntask-speciﬁc ﬁne-tuning, offer unmatched state-of-\nthe-art performance in a wide array of standard\nlanguage understanding and conversational tasks\n(Wang et al., 2019a; Mehri et al., 2020). However,\npretrained LMs do not produce coherent and effec-\ntive sentence encodings off-the-shelf; their further\nadaptation is required, akin to standard task ﬁne-\ntuning. For instance, Reimers and Gurevych (2019)\ntransform monolingual English BERT with super-\nvised natural language inference and paraphrasing\ndata (Williams et al., 2018; Wieting and Gimpel,\n2018) into a sentence encoder which excels at sen-\ntence similarity and retrieval tasks (Marelli et al.,\n2014; Cer et al., 2017). This transformation pro-\ncess supports the creation of other similar universal\nsentence encoders in monolingual and multilingual\nsettings (Chidambaram et al., 2019; Wieting et al.,\n2020; Feng et al., 2020), and is typically based on\ndual-encoder architectures.\nAnother parallel research thread aims at learning\nconversational encoders: it validates the beneﬁts\nof masked language modeling (MLM) pretraining\non naturally conversational data (Wu et al., 2020;\nMehri et al., 2021), as well as the beneﬁts of trans-\nfer learning for conversational tasks which goes\nbeyond MLM as the pretraining objective (Mehri\net al., 2019; Coope et al., 2020; Henderson and\nVuli´c, 2021, inter alia). In particular, response\nselection as a suitable pretraining task (Al-Rfou\net al., 2016; Yang et al., 2018; Henderson et al.,\n2019b; Humeau et al., 2020) learns representations\nthat organically capture conversational cues from\nconversational text data such as Reddit (Henderson\net al., 2019a), again via dual-encoder architectures.\nInspired by these two research threads, we pose\nthe following two crucial questions:\n(Q1) Is it necessary to conduct full-scale expen-\nsive conversational pretraining? In other words,\nis it possible to simply and quickly ’rewire’ exist-\ning MLM-pretrained encoders as conversational\nencoders via, e.g., response ranking ﬁne-tuning on\n(much) smaller-scale datasets?\n(Q2) If we frame conversational tasks such as\nintent detection as semantic similarity tasks in-\nstead of their standard classiﬁcation-based formu-\nlation, is it also possible to frame supervised task-\nspeciﬁc learning as ﬁne-tuning of conversational\nsentence encoders? In other words, can we learn\narXiv:2109.10126v1  [cs.CL]  21 Sep 2021\nStage 1 loss\n(c, r) = (context, response)\nInput LM Input LM\nPooling Pooling\nc r\nConvFiT :\nStage 1\n(Behavioral) ﬁne-tuning\non Reddit data\nConvFiT :\nStage 2\nTask-based ﬁne-tuning\non intent (task) data\nInput LM\n(RoBERTa, BERT,...)\nStage 2 loss\n(xi, xj) = (senti, sentj)\nLM LM\nPooling Pooling\nxi xj\nMNEG\n1. SOFTMAX\n2. COSINE\n3. OCL\nMLP\nClassiﬁcation\nSimilarity-based\nClassiﬁcation\n+Down-projection+Down-projection\nFigure 1: Illustration of the full CONV FIT framework which ﬁne-tunes pretrained LMs such as BERT or RoBERTa\nin two separate stages via dual-encoder networks (“zoomed-in” parts; grey blocks denote tunable parameters), and\nperforms intent detection with the C ONV FIT-ed models via similarity-based inference. Stage 1 (S1): adaptive\nconversational ﬁne-tuning, §2.1; Stage 2 (S2): task-tailored conversational ﬁne-tuning (for intent detection), §2.2.\nDashed lines denote baseline/ablation variants which skip one of the two stages: (i) we can directly task-tune the\nsentence encoder with the task data (Stage 2) without running Stage 1, or (ii) we can skip Stage 2, and similar to\nCasanueva et al. (2020), learn an MLP classiﬁer on top of the conversational representations from Stage 1.\n(a) RoBERTa (no ﬁne-tuning)\n (b) RoBERTa (after S1)\n (c) RoBERTa (after S1 and S2)\nFigure 2: t-SNE plots (van der Maaten and Hinton, 2012) of encoded utterances from the ID test set ofBANKING 77\n(i.e., all examples are effectively unseen by the encoder models at training) associated with a selection of 12 intents,\ndemonstrating the effects of gradual “representation specialisation funnel”. The encoded utterances are created via\nmean-pooling based on (a) the original RoBERTa LM; (b) RoBERTa after Stage 1 (i.e., ﬁne-tuned on 1% of the\nfull Reddit corpus, see Figure 1);(c) RoBERTa after Stage 1 and Stage 2, ﬁne-tuned with theOCL objective (n= 3\nnegatives) using the entire BANKING 77 training set (see Figure 1). Additional t-SNE plots are in the Appendix.\ntask-specialised sentence encoders that enable sen-\ntence similarity-based interpretable classiﬁcation?\nIn order to address these two questions, we\npropose CONV FIT, a two-stage CONV ersational\nFIne-Tuning procedure that turns general-purpose\nMLM-pretrained encoders into sentence encoders\nspecialised for a particular conversational domain\nand task. Casting the end-task (e.g., intent detec-\ntion) as a pure sentence similarity problem then\nallows us to recast task-tailored ﬁne-tuning of a\npretrained LM as gradual sentence encoder special-\nisation, as illustrated in Figures 1 and 2.\nOur hypothesis is that the pretrained LMs, which\nalready store a wealth of semantic knowledge,\ncan be gradually turned into conversational task-\nadapted sentence encoders without expensive full\npretraining. (S1) Stage 1 transforms pretrained\nLMs into universal conversational encoders via\nadaptive ﬁne-tuning (Ruder, 2021) on (a fraction\nof) Reddit data (see Figure 2b), relying on a stan-\ndard dual-encoder architecture with a conversa-\ntional response ranking loss (Henderson et al.,\n2020); cf. Q1. (S2) Stage 2 further specializes the\nsentence encoder via contrastive learning with in-\ntask data, that is, it learns meaningful task-related\nsemantic clusters/subspaces. We then show that the\nS2 task-tailored specialisation effectively enables\na simple and interpretable similarity-based classi-\nﬁcation based on nearest neighbours (NNs) in the\nspecialised encoder space (see Q2 and Figure 2c).\nThe two-stage CONV FIT transformation offers\nnew insights and contributions to representation\nlearning for conversational tasks. Unlike prior\nwork which conducted large-scale conversational\npretraining from scratch using large datasets, we\ndemonstrate that full pretraining is not needed\nto obtain universal conversational encoders. By\nleveraging the general semantic knowledge already\nstored in pretrained LMs, we can expose (i.e.,\n’rewire’) that knowledge (Vuli´c et al., 2021; Gao\net al., 2021b; Liu et al., 2021b) via much cheaper\nand quicker adaptive ﬁne-tuning on a tiny fraction\nof the full Reddit data (e.g., even using < 0.01%\nof the Reddit corpus). Further, the task-oriented\nS2 CONV FIT-ing transforms pretrained LMs into\ntask-specialised sentence encoders. Our results\nwith similarity-based classiﬁcation, targeting the\ncrucial conversational NLU task of intent detec-\ntion (ID), reach state-of-the-art (SotA) across all\nstandard ID datasets, with particular gains in the\nmost challenging, few-shot setups. Importantly, we\nshow that the gradual application of S1 and then\nS2 yields a synergistic effect, that is, it attains the\nhighest ID results across the board.\nFinally, CONV FIT is highly versatile: it can be\nused with a range of pretrained LMs and on a spec-\ntrum of text classiﬁcation problems; it also allows\nfor the simple usage of diverse ﬁne-tuning objec-\ntives in both Stage 1 and Stage 2, beyond the ones\nproposed and evaluated in this work.\n2 Methodology\nPreliminaries. For any input text t, we obtain its\nencoding t = enc(t), where encis a sentence en-\ncoder at any CONV FIT stage (i.e., before any ﬁne-\ntuning, after S1, or after S2), or any other sentence\nencoder. The text t is tokenized into subwords\n(Schuster and Nakajima, 2012) relying on each en-\ncoder’s dedicated tokeniser. The ﬁnal encodingt is\ncreated via apooling operation such as (a) using the\n[CLS] token, (b) or mean-pooling the output sub-\nword vectors. Following prior work (Reimers and\nGurevych, 2019), we always use mean-pooling.\n2.1 Stage 1: Adaptive Fine-Tuning\nAs in prior work on conversational pretraining\n(Henderson et al., 2019b, 2020; Humeau et al.,\n2020), Stage 1 relies on the response ranking task\nwith Reddit data and dual-encoder architectures,\nwhich model the interaction between Reddit (con-\ntext, response) (c,r) pairs.1 However, unlike prior\n1In each (c,r) pair, r is the response that immediately\nfollows the preceding context sentence in a Reddit thread;\nwork, instead of pretraining from scratch we ﬁne-\ntune an LM-pretrained encoder, which yields a\nmuch quicker conversational encoder specialisa-\ntion, and does not require massive amounts of data.\nResponse ranking is formulated as the standard\nmultiple negatives ranking loss (MNEG ): for each\npositive (ci,ri) pair (i.e., the pair observed in the\nReddit ﬁne-tuning data), the aim is to rank the\ncorrect response r for the input c over a set of\nrandomly sampled responses rj,j ̸= ifrom other\nReddit pairs. The similarity between c-s and r-s is\nquantiﬁed via the similarity function Soperating\non their encodings S(c,r). Following prior work,\nwe use the scaled cosine similarity: S(c,r) =D·\ncos(c,r), where Dis the scaling constant. Stage 1\nﬁne-tuning with MNEG then proceeds in batches of\nBpositive Reddit pairs (ci,ri),..., (cB,rB); the\nMNEG loss for a single batch is computed as:\nL= −\nB∑\ni=1\nS(ci,ri) +\nB∑\ni=1\nlog\nB∑\nj=1,j̸=i\neS(ci,rj) (1)\nEffectively, for each batch Eq. (1) maximises the\nsimilarity score of positive context-response pairs\n(ci,ri), while it minimises the score of B−1 ran-\ndom pairs. The negative examples are all pairings\nof ci with rj-s in the current batch, where such\n(ci,rj) pairs do not occur in the Reddit data.2\nThe output of Stage 1 is the sentence encoder\nencS1 which can be used ’as is’ similarly to stan-\ndard sentence encoders (Henderson et al., 2020;\nCasanueva et al., 2020; Feng et al., 2020): a stan-\ndard ID approach stacks a Multi-Layer Perceptron\n(MLP) classiﬁer on top of the ﬁxed sentence vec-\ntors t, and ﬁne-tunes only the MLP parameters\n(Casanueva et al., 2020; Gerz et al., 2021). How-\never, the output of S1 can also be further fed as the\ninput encoding for CONV FIT’s Stage 2 (Figure 1).\n2.2 Stage 2: Task-Based Sentence Encoders\nStage 2 ﬁne-tuning is inspired by metric-based\nmeta-learning (Vinyals et al., 2016; Musgrave\net al., 2020) and exemplar-based (also termed\nprototype-based) learning (Snell et al., 2017; Sung\net al., 2018; Zhang et al., 2020), which is espe-\ncially suited for few-shot scenarios. We assume\nthe existence of Na annotated in-task examples\nsee (Henderson et al., 2019a). The intuition is that sentences\nwhich elicit similar responses should obtain similar sentence\nencodings (Yang et al., 2018).\n2We also experimented with another SotA loss function,\nthe triplet-based multi-similarity loss (Wang et al., 2019b; Liu\net al., 2021a), without any substantial performance differences.\n{(x1,y1),..., (xNa,yNa)}: e.g., x-s are text sen-\ntences with y-s being their intent labels/classes; let\nus assume that there are Nc classes {C1,...,C Nc}\nin total. The aim is to ﬁne-tune the input sentence\nencoder in such a way to encode all sentences asso-\nciated with each particular class into coherent clus-\nters, clearly separated from all other class-related\n(also coherent) clusters (see Figure 2c).3\nPositive and Negative Pairs. We leverage the\nclass labels only implicitly (see Figure 1), which\nallows us to treat intent detection as a sentence\nsimilarity task. CONV FIT S2 operates with two\nsets of pairs: 1) PP is the set of positive pairs\n(xi,xj), where xi and xj are text instances associ-\nated with the same class Ci; 2) NP contains nega-\ntive pairs (xi,xj) where xi and xj are associated\nwith two different classes Ci and Cj. We construct\nthe set NP in a balanced way: for each positive\npair (xi,xj) ∈PP, we add 2 ×nnegative pairs\ninto NP, where n is a tunable hyper-parameter;\nnpairs (xi,xi,n′),n′ = 1,...,n , are constructed\nby randomly sampling utterances xi,n′ which do\nnot share the class with xi, and we also sample\nnnegatives (xj,n′,xj) in a similar vein. We now\npresent three different loss functions that ﬁne-tune\nthe input encoders towards task-specialised sen-\ntence similarity relying on the sets PP and NP.\nFor all three S2 loss functions, we add a down-\nprojection do-dim layer with non-linearity ( Tanh\nused) after pooling, see Figure 1.4\nSOFTMAX (SMAX ) Loss. Following prior work\n(Reimers and Gurevych, 2019), for each input\nsentence pair (xi,xj), we concatenate their do-\ndimensional encodings xi and xj (obtained after\npassing them through the input encoder, pooling,\nand down-projection) with their element-wise dif-\nference |xi −xj|. The objective is as follows:\nLSMAX = softmax\n(\nW(xi ⊕xj ⊕|xi −xj|)\n)\n, where\n⊕denotes concatenation, and W ∈R3do×2 is a\ntrainable weight matrix of the softmax classiﬁer,\nwhere 2 is the number of classiﬁcation classes: the\nmodel must simply discern between positive pairs\n(from PP) and negative pairs from NP. The clas-\nsiﬁers are optimised via standard cross-entropy.\nCosine (COS ) Loss. The idea is to minimise the\nfollowing distance, formulated as standard mean-\nsquared error: ||δl −cos(xi,xj)||2, where cosde-\n3In other words, the encoder should learn to encode each\nutterance into one of such semantically well-deﬁned clusters.\n4A variant with down-projection yielded slightly higher\nscores than the one without it in our preliminary experiments.\nnotes cosine similarity, and δl is a hyper-parameter\nwhich speciﬁes the ’ideal’ (dis)similarity margin\nin the specialised encoder space. Here, we rely on\nthe default parameters from Reimers and Gurevych\n(2019) without any tuning: δl = 0.8 iff (xi,xj) ∈\nPP, and δl = 0.3 iff (xi,xj) ∈NP.\nOnline Contrastive Learning ( OCL ) Loss fol-\nlows the formulation from Hadsell et al. (2006):\nLOCL = 1 ·(dcos(xi,xj))2\n+ (1−1) ·\n(\nReLU(δm −dcos(xi,xj))\n)2\n(2)\nwhere 1 is the indicator function which returns 1 iff\n(xi,xj) ∈PP, and 0 iff (xi,xj) ∈NP; dcos =\n1−cosis the cosine distance, andδm is the distance\nmargin, set to the default value of 0.5 (Reimers\nand Gurevych, 2019) in all our experiments. The\nloss ’attracts’ similar items closer together in the\nspecialised space, while ’repelling’ dissimilar items\n(Mrkši´c et al., 2017).5\nSimilarity-Based Inference. Intent detection in\nthe specialised encoder space encS2 is then per-\nformed via similarity-based classiﬁcation (Zhang\net al., 2020) after Stage 2.6 Assuming the simplest\ncase of k = 1 nearest neighbours (NN) classiﬁ-\ncation, we select the intent class for an unseen\nexample uas: Ic\n(\narg maxt∈Pool cos(t,u)\n)\n. Here,\nt = encS2(t) refers to the sentence encoding of\neach example t∈Pool (which is typically the pool\nof examples from the ID training set), and the Ic\nfunction returns the intent class of any t∈Pool.\nWhy Intent Detection as a Sentence Similarity\nTask? We can take the analogy of ‘intent’ being\na latent semantic class where sentences associated\nwith the intent are diverse surface instances of the\nclass (i.e., language realisations of the underly-\ning concept/intent). This means that ﬁnding the\nmost similar labelled instances for the given unla-\nbelled input instance/sentence can directly inform\nus about the underlying semantic class/intent.\n5We use the online version of the loss that updates the loss\nfocusing on hard negative pairs (i.e., negatives that are close\nby cosine in the current semantic space) and hard positives\nwhich are far apart in the current space. This typically results\nin quicker convergence and slightly better performance.\n6The beneﬁts of similarity-based classiﬁcation were re-\ncently validated also in other NLP tasks such as cross-lingual\nabusive content detection (Sarwar et al., 2021), language mod-\neling (Khandelwal et al., 2020; Guu et al., 2020), and question\nanswering (Kassner and Schütze, 2020), among others.\nDataset Intents Examples Domains\nBANKING 77 77 13,083 1 (banking)\nCLINC 150 150 23,700 10\nHWU 64 64 25,716 21\nTable 1: Intent detection datasets: key statistics.\n3 Experimental Setup\nInput LMs. We experiment with several popular\nTransformer-based (Vaswani et al., 2017) LMs as\ninput (see Figure 1), aiming to validate the robust-\nness of CONV FIT, as well as to analyse the impact\nof LM pretraining on the ﬁnal task performance: (i)\nBERT (Devlin et al., 2019) (labeled BERT hence-\nforth); (ii) RoBERTa (ROB), as an improved variant\nof BERT, LM-pretrained with more data (Liu et al.,\n2019b); (iii) DistilRoBERTa ( DROB ), a distilled\nmore compact version of RoBERTa, LM-pretrained\nwith around 4 times fewer data than the teacher\nRoBERTa model (Sanh et al., 2019). The cased\nBASE variants are used for all input LMs: 768-\ndimensional Transformer layers with 12 ( BERT ,\nROB) or 6 ( DROB ) attention layers. In addition,\nto isolate the effects of LM-pretraining and CON-\nVFIT-ing from the mere “parameter capacity”, we\nalso experiment with a BERT /ROB architecture with\nRAND omly initialised parameters using the Xavier\ninitialisation (Glorot and Bengio, 2010).\nUnless noted otherwise, CONV FIT Stage 1 al-\nways proceeds with a sample comprising 2% of the\nfull Reddit corpus from Henderson et al. (2019a).7\nIntent Detection Datasets. As discussed in §2,\nthe main evaluation task is intent detection (ID),\nwith a particular focus on low-data (i.e., few-shot)\nscenarios. Our Stage 2 ﬁne-tuning and the ﬁnal task\nevaluation are based on three standard ID datasets\nin English, also available as part of the recently pub-\nlished DialoGLUE benchmark (Mehri et al., 2020):\nBANKING 77 (Casanueva et al., 2020),HWU 64 (Liu\net al., 2019a), andCLINC 150 (Larson et al., 2019).8\nThe key statistics of all three datasets are provided\nin Table 1; for further details, we refer the reader to\nthe original work and also to (Mehri et al., 2020).\nFew-Shot and Full Data Setups. Prior work has\nrecognised the importance of building intent detec-\n7The full corpus contains 700M+ (context, response) pairs.\n8The datasets provide a range of diverse ID setups, cover-\ning ﬁne-grained ID within a single domain (e.g.,BANKING 77),\nas well as coarser-grained ID spanning several well-deﬁned\ndomains (e.g., news, calendar, alarm, restaurant booking in\nHWU 64 or in CLINC 150 ). They provide a more challenging\nsetup (and are also better aligned with the actual ID setups\ntypically met in production) than some other well-known ID\ndatasets such as SNIPS (Coucke et al., 2018).\ntors in low-data regimes (Casanueva et al., 2020;\nMehri et al., 2021). Therefore, following this ini-\ntiative, we evaluate the models in two N-shot sce-\nnarios, where we assume that only N = 10 or\nN = 30annotated examples per intent are avail-\nable for training the MLP classiﬁer or for S2 ﬁne-\ntuning; Figure 1.9 The models are also evaluated\nin the Full setup, where all annotated training ex-\namples per intent are used. Note that we always\nreport the scores on the same test set for each setup.\nFor the few-shot scenarios, we report the scores as\naverages over 3 independent experimental runs.\nHyperparameters and Optimisation. CONV FIT\nis implemented via the sentence-transformers\n(sbert) repository (Reimers and Gurevych, 2019),\nwhich is in turn built on top of the HuggingFace\nrepository (Wolf et al., 2020). Similar to Casanueva\net al. (2020), we do not rely on any development\ndata, and follow the general suggestions from prior\nwork (Reimers and Gurevych, 2019; Casanueva\net al., 2020) for the hyperparameter setup, which is\nadopted across all intent ID datasets.10 For S1 with\nMNEG , we always train for 2 epochs in batches of\n256 with default hparams from sbert.11\nIn Stage 2, with all three evaluated objective\nfunctions the batch size is 32, the maximum se-\nquence length is 48, the output layer’s dimension-\nality is set to do = 512. Unless stated otherwise,\nwe always ﬁne-tune for 10, 5, and 2 epochs for the\n10-shot, 30-shot, and Full setups, respectively. For\nthe COS and OCL variants, unless noted otherwise,\nwe report the results with n= 3negative examples\nper each positive in 10-shot and 30-shot setups, and\nwith n= 1(for computational tractability) in the\nFull setup. An analysis of the impact of non the\nﬁnal ID performance is presented later in §4.\nFollowing the suggested settings of Reimers\nand Gurevych (2019); Vuli´c et al. (2020), in both\nCONV FIT stages we use the AdamW optimiser\n(Loshchilov and Hutter, 2018); the learning rate is\n2e−5 with the warmup rate of0.1 and linear decay\n9We use the same ﬁxed few-shot and test sets for each\nintent detection dataset as released by Mehri et al. (2020).\n10For all MLP intent classiﬁers, this implies relying on\nthe empirically validated and stable setup from prior work\n(Casanueva et al., 2020): the best results are achieved with a\n2-layer fully-connected MLP (768-dim hidden layers), trained\nvia SGD with the high learning rate (0.5) and linear decay,\nand very aggressive dropout rates (0.75); training lasts for 500\nepochs; batch size is 32. This setup achieved strong results in\nour preliminary experiments as well, and is thus adopted here.\n11256 is the maximum batch size with BASE BERT and\nRoBERTa which allows us to run Stage 1 ﬁne-tuning on a\nsingle 12GiB GTX GPU.\nafterwards, and the weight decay rate is set to 0.01.\nSimilarity-Based Classiﬁcation. The intent class\nis chosen according to thek= 1NNs, based on the\ncosine distance in the ﬁne-tuned space. 12 Impor-\ntantly, in few-shot setups we use only the few-shot\ndata as the NN pool for classiﬁcation.\n3.1 Model Variants and Baselines\nWe experiment with a range of model variants\nenabled by the CONV FIT framework (see Fig-\nure 1), and compare their performance in the ID\ntask against an array of cutting-edge universal and\nconversational sentence encoders. All the models\nin evaluation are summarised here for clarity.\nLM+S1+S2-LOSS . Sentence encoders after run-\nning the full CONV FIT pipeline, where intent de-\ntection is based on similarity-based NN classiﬁca-\ntion. LM in the label of this variant denotes the\ninput LM, and LOSS is the loss function used in\nStage 2 (i.e., SMAX , COS , or OCL ).\nLM+S2-LOSS . Sentence encoders optimised only\nvia Stage 2 CONV FIT, skipping Stage 1 (see Fig-\nure 1); similarity-based intent detection.\nLM+S1. The input LM is converted into a (general-\npurpose) conversational encoder via Stage 1 CON-\nVFIT-ing; intent detection is performed via stan-\ndard feature-based MLP classiﬁcation on top of the\nsentence encodings as in prior work.\nSotA Sentence Encoders. We evaluate three\nwidely used state-of-the-art sentence encoders in\nthe standard feature-based MLP classiﬁcation ap-\nproach to intent detection:13 (i) ConveRT (Hender-\nson et al., 2020) is a dual sentence encoder pre-\ntrained with the conversational response selection\ntask (Henderson et al., 2019b) on the full Red-\ndit data (Al-Rfou et al., 2016; Henderson et al.,\n2019a); (ii) multilingual Universal Sentence En-\ncoder ( mUSE) (Yang et al., 2020) is a multilin-\ngual and better-performing version of the USE\nmodel for English (Cer et al., 2018), which again\nrelies on a standard dual-encoder framework (Hen-\nderson et al., 2019b; Humeau et al., 2020) and\nis pretrained on massive amounts of data; (iii)\nLanguage-agnostic BERT Sentence Embedding\n(LaBSE) (Feng et al., 2020) adapts pretrained multi-\nlingual BERT (mBERT) (Devlin et al., 2019) into a\nsentence encoder using a dual-encoder framework\n12Very similar results are observed withk= 3and k= 5.\n13For more technical details regarding each sentence en-\ncoder, we refer the reader to the original work.\n(Yang et al., 2019) with larger embedding capacity\n(i.e., it provides a shared multilingual vocabulary\nspanning 500k subwords).14\n4 Results and Discussion\nThe main results are summarised in Table 2, and\nfurther results and analyses are available in §4.1,\nwith additional results in the Appendix.15 These re-\nsults offer multiple axes of comparison, succinctly\ndiscussed in what follows.\nMLP versus Similarity-Based ID. First, we note\nthat CONV FIT-ed LMs achieve peak ID scores\nacross all three ID datasets, and in all data se-\ntups, with ROB +S1+S2-OCL being the highest-\nperforming model variant overall. Running Stage 1\ndoes transform input LMs into effective (univer-\nsal) conversational encoders already: for MLP-\nbased ID, we observe competitive or even improved\nperformance (cf., the results on BANKING 77 and\nHWU 64 as two more challenging evaluation sets)\nwith the ROB +S1 and BERT +S1 variants against\ncurrent state-of-the-art (conversational) sentence\nencoders such as ConveRT, USE, and LaBSE.\nImportantly, the results after Stage 2 ‘unani-\nmously’ suggest the effectiveness of treating ID\nas a semantic similarity task, and additional task-\nspeciﬁc specialisation of the sentence encoders\nwith in-task data. Put simply, it seems more ef-\nfective to use the in-task training data to ‘task-\nspecialise’ the sentence encoder space than to learn\na standard (MLP) classiﬁer, which directly maps\nfrom the feature space to intent labels (Sarwar et al.,\n2021). The gains are especially pronounced in few-\nshot setups (e.g., see 10-shot BANKING 77).\nWe speculate that dual-encoder contrastive learn-\ning surpasses MLP-based approaches especially\nin few-data scenarios because it learns from ﬁner-\ngrained and more abundant information in such\nlow-data scenarios: i.e., we learn to contrast be-\ntween pairs of instances rather than simply learn-\ning an MLP-based mapping from an instance to\nits underlying class intent/class. This formula-\ntion can also capture some subtle cross-instance\n(dis)similarities which cannot be captured by MLP.\n14LaBSE is the current SotA encoder across a wide array\nof languages (Feng et al., 2020; Litschko et al., 2021; Gerz\net al., 2021). Besides dual-encoder training, LaBSE lever-\nages standard self-supervised objectives used in pretraining of\nmBERT and XLM: masked and translation language modeling\n(Conneau and Lample, 2019); see the original work.\n15For brevity, in the main paper we report the results with\nthe two better-performing S2 losses: COS and OCL .\nBANKING 77 CLINC 150 HWU 64\nModel Variant 10 30 Full 10 30 Full 10 30 Full\nSimilarity-Based Classiﬁcation\nROB +S1+S2-COS 86.48 91.33 94.35 92.87 95.91 97.20 85.06 90.46 92.98\nBERT +S1+S2-COS 84.32 90.91 93.91 91.80 95.58 96.56 85.13 89.41 91.93\nDROB +S1+S2-COS 85.13 90.75 94.06 91.64 95.48 97.00 83.64 89.68 92.94\nROB +S1+S2-OCL 87.38 91.36 94.16 92.89 96.42 97.34 85.32 90.06 92.42\nBERT +S1+S2-OCL 85.97 90.65 93.77 91.53 95.53 96.82 85.04 89.41 92.21\nDROB +S1+S2-OCL 86.04 90.78 93.89 91.98 95.60 97.04 83.64 89.50 92.84\nROB +S2-COS 84.96 90.81 94.19 91.56 95.64 96.78 84.52 89.87 92.19\nBERT +S2-COS 81.27 90.32 93.73 89.58 95.08 96.54 82.90 89.12 91.78\nDROB +S2-COS 83.28 90.58 93.91 89.47 95.32 86.78 82.43 89.41 92.10\nROB +S2-OCL 85.78 90.98 93.77 92.64 95.40 96.87 84.76 89.31 92.01\nBERT +S2-OCL 82.28 89.77 93.54 90.71 95.07 96.62 83.09 88.94 92.57\nDROB +S2-OCL 82.60 90.65 93.38 90.78 95.02 96.69 81.69 88.75 92.38\nBaselines: MLP Classiﬁcation\nROB +S1 83.08 90.16 93.38 90.98 94.12 96.42 81.13 87.73 91.44\nBERT +S1 82.69 89.82 93.67 89.88 94.07 96.33 82.25 88.01 91.12\nCONVE RT∗ 83.32 89.37 93.01 92.62 95.78 97.16 82.65 87.88 91.24\nUSE∗ 84.23 89.74 92.81 90.85 93.98 95.06 83.75 89.03 91.25\nUSE (ours) 82.95 89.09 92.81 90.27 93.54 94.91 82.71 88.20 91.64\nLABSE 81.69 88.96 92.60 90.89 93.41 95.12 81.60 86.15 90.99\nTable 2: Accuracy scores ( ×100%) on the three ID data sets with varying number of training examples ( 10 ex-\namples per intent; 30 examples per intent; Full training data). n = 3 negatives are used in Stage 2 for 10-shot\nand 30-shot setups, n = 1 for the Full setup (see §3). The peak scores per column are in bold, the second best\nis underlined. *The scores were taken directly from prior work, and computed on different 10/30-shot samples\n(and are thus not directly comparable, Zhao et al. 2021). For clarity, we show only a subset of (arguably most\ninformative) model variants; the complete table with additional evaluated variants is available in the Appendix.\nExtending beyond pure absolute performance, de-\ncisions based on k-NN similarity-based ID in the\nspecialised space are also easy to interpret (Simard\net al., 1992; Wallace et al., 2018).\nStage 1 + Stage 2? The scores in Table 2 indicate\nthat Stage 2 alone already transforms pretrained\nLMs into very strong task-specialised sentence en-\ncoders. However, a more careful comparison of\nLM+S1+S2-LOSS versus LM+S2-LOSS variants re-\nveals that Stage 1 ﬁne-tuning is universally use-\nful (regardless of the chosen loss function in S2),\nand yields ID performance gains. In other words,\nthe coarser-grained adaptive ﬁne-tuning already\nexposes some conversational knowledge from the\npretrained LMs, and such knowledge does have\nsubstantial impact on task-specialised S2 tuning.\nIn sum, this ﬁnding is line with prior work in other\ndomains and NLP tasks (Gururangan et al., 2020;\nGlavaš et al., 2020; Ruder, 2021): both domain-\nadaptive (our S1) and task-adaptive additional tun-\ning (our S2) of general-purpose LMs have a syner-\ngistic positive impact on the ﬁnal task performance.\nThe impact of the gradual two-stage sentence\nencoder transformation is also clearly visible from\nthe t-SNE visualisation in Figure 2. Besides this, a\nstandard quantitative measure of cluster coherence,\nthe Silhouette coefﬁcient σ(Rousseeuw, 1987) also\npoints in the same direction: σ= 0.067 for the test\nexamples and model variant from Figure 2a, σ =\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2\nFigure 3: A comparison of a randomly initialised\nRoBERTa (RAND ) against LM-pretrained RoBERTa af-\nter S2 CONV FIT-ing with OCL ; BANKING 77.\n0.188 (Figure 2b), and σ= 0.698 (Figure 2c).16 17\nImpact of Input LMs. While the results suggest\nthat the CONV FIT framework is applicable and ef-\nfective with any pretrained LM, the choice of the\ninput LM naturally impacts the absolute ID perfor-\nmance. As expected, the CONV FIT variants with\nRoBERTa achieve the highest scores across the\nboard. A comparison between DROB and BERT re-\nveals that the pretraining data size and regime seem\nto play a more critical role than the parameter ca-\npacity: the more compact DROB LM is competitive\nwith or even outscores BERT -based variants.18\n16Higher σscores are desirable as they imply more coherent\nand compact clusters, and a stronger inter-cluster separation.\n17Stage 2 tuning with more in-task data also naturally yields\na better separation of examples into coherent clusters , which\nthen naturally improves NN-based classiﬁcation. For instance,\nrunning the ROB +S1+S2-OCL (n = 3) variant in 10-shot,\n30-shot, and Full data setups yields the respective σscores for\nthe same set of test examples from Figure 2: σ10 = 0.378,\nσ30 = 0.548, σFull = 0.698, validating the intuition.\n18Given the versatility ofCONV FIT, in future work we plan\nto extend the experiments to other pretrained LMs such as\nBANKING 77 CLINC 150 HWU 64\nEvaluation Set (10-Shot Setup)\n84\n88\n92\n96Accuracy\nno S1 × 1/256 × 1/64 × 1/16 × 1 × 2\n(a) 10-shot (ROB +S1+S2-OCL )\nBANKING 77 CLINC 150 HWU 64\nEvaluation Set (30-Shot Setup)\n84\n88\n92\n96Accuracy\nno S1 × 1/256 × 1/64 × 1/16 × 1 × 2 (b) 30-shot (ROB +S1+S2-OCL )\nFigure 4: Varying the amount of Reddit data for Stage 1 C ONV FIT; ×1 refers to the Reddit size used in all our\nother Stage 1 ﬁne-tuning experiments (≈2% of the full Reddit corpus from Henderson et al. (2019a)), while other\nReddit data sizes are relative to this corpus size (e.g., ×1/32 means that we use 2%/32 ≈0.0625% of the full\nReddit corpus). Similar plots (with similar ﬁndings) using the COS loss in Stage 2 are available in the Appendix.\nn = 1 n = 3 n = 5 n = 7\nNumber of negative examples\n80\n84\n88\n92\n96Accuracy\n+S2 (10)\n+S2 (30)\n+S1+S2 (10)\n+S1+S2 (30)\n(a) BANKING 77\nn = 1 n = 3 n = 5 n = 7\nNumber of negative examples\n80\n84\n88\n92\n96Accuracy\n+S2 (10)\n+S2 (30)\n+S1+S2 (10)\n+S1+S2 (30) (b) HWU 64\nFigure 5: Impact of the number of negative examplesn\nin 10-shot and 30-shot setups. The C ONV FIT variants\nare ROB +S2-OCL and ROB +S1+S2-OCL (labelled +S1\nand +S1+S2 in the ﬁgures, respectively).\nVariant 10 30 Full\nROB +S1+S2-COS 82.37 94.39 98.12\nROB +S2-COS 70.71 92.14 97.42\nMLP-Based\nROB +S1 48.26 85.49 97.16\nUSE 47.25 87.21 97.31\nLABSE 43.10 87.32 97.42\nTable 3: Results on English ATIS (Accuracy×100).\nImportance of LM Pretraining is illustrated by\nFigure 3. The trend is quite straightforward: se-\nmantic knowledge acquired by LM-pretraining is\nparticularly important in the fewest-shot (i.e., 10-\nshot) setups, and the gap gets reduced with more\nin-task data available for S2 tuning. However, the\ngap remains substantial even in the Full setups.\nFigure 3 also reveals that the strength of CON-\nVFIT Stage 1 is in adapting the knowledge acquired\nat LM pretraining: S1 ﬁne-tuning of RAND with\nsmaller amounts of Reddit data cannot match ROB\nas the input LM, although the gap does become\nsmaller with more in-task data for S2.\nStage 2: Fine-Tuning Losses. Table 2 reveals that\nstrong ID performance after S2 tuning is achieved\nwith different loss functions from §2.2, with differ-\nent input LMs, even without any careful tuning of\nhyper-parameters for single settings. This veriﬁes\nthe versatility and robustness of CONV FIT. Both\nCOS and OCL yield consistently strong results, and\nwe expect that even higher absolute scores might\nELECTRA (Clark et al., 2020) and T5 (Raffel et al., 2020).\nbe achieved by applying more sophisticated (con-\ntrastive learning) loss functions from prior work\n(Hermans et al., 2017; Liu et al., 2021a) in Stage 2.\n4.1 Further Discussion\nStage 1: Amount of Reddit Examples. We now\nanalyse what amount of Reddit data is required\nto turn input LMs into conversational encoders,\nby reducing S1 ﬁne-tuning data through subsam-\npling. The scores over different sizes are provided\nin Figure 4, and we note that they extend to other\nCONV FIT variants (see §3.1). As expected, having\nmore Reddit data does yield better results on aver-\nage, but even a small sample of Reddit data (e.g.,\n≈50K (c,r) pairs) 1) transforms the input LM into\nan effective sentence encoder (e.g., its MLP-based\nID results are on par with those achieved with USE,\nLaBSE, and ConveRT), and 2) improves over the\nCONV FIT variant that skips S2 completely. This\nimplies that perhaps more careful domain-driven\ndata sampling in the future might yield even more\ndomain-adapted conversational encoders after S1.\nAmount of Negative Examples in Stage 2 has\nonly a moderate to negligible impact on the ﬁnal\nperformance, as shown in Figure 5. Small gains\nwhen moving from n = 1to n = 3are observed\nonly for the 10-shot setup: there, having more neg-\natives may implicitly play the role of data augmen-\ntation for ﬁne-tuning. However, with more in-task\nexamples, the dependence on n becomes incon-\nsequential, and the performance saturates quickly\n(e.g., see the curves in the 30-shot setups).\nStage 2: Few-Shot versus Full. Framing the ID\ntask a sentence similarity seems especially beneﬁ-\ncial for few-shot scenarios, as the model can lever-\nage prototype-based (or instance-based) similari-\nties (Snell et al., 2017) in the specialised encoder\nspace. However, the strong performance with fully\nCONV FIT-ed models persists also in Full setups.\nBANKING 77 CLINC 150 HWU 64\nEvaluation Set\n84\n88\n92\n96Accuracy\nInference: 10-shot 30-shot Full\nFigure 6: Impact of the number of data instances at\ninference. The ROB +S1+S2-OCL variant is tuned in 10-\nshot setups in S2, and additional data (30-shot or Full)\nis used only at inference without any S2 retuning.\nThis ﬁnding is further corroborated with the re-\nsults on another standard ID dataset, English ATIS\n(Hemphill et al., 1990; Xu et al., 2020), see Ta-\nble 3. There, we observe even more prominent dif-\nferences in favour of similarity-based ID enabled\nby CONV FIT, again especially in the two low-data\nsetups. The proposed prototype-based learning and\ninference holds promise to boost few-shot perfor-\nmance even more in future work, through addi-\ntional metric learning (Zhang et al., 2020) or data\naugmentation techniques (Lee et al., 2021).\nOne limitation of CONV FIT, especially promi-\nnent in Full scenarios, is its quadratic time complex-\nity. Future work will look into effective sampling\nstrategies and adaptations towards more sample-\nefﬁcient and quicker ﬁne-tuning (Tran et al., 2019;\nTian et al., 2020; O’Neill and Bollegala, 2021).\nData Augmentation for Inference. Adding more\ndata instances for similarity-based inference, serv-\ning as exemplars/prototypes, is likely to boost the\nﬁnal intent detection performance without the need\nto retrain the model. The intuition is that additional\ninstances can provide ﬁner-grained prototypes for\ninference, semantically more similar to the input\nquery sentences than the original training data. To\ntest this hypothesis, we conduct a simple probing\nexperiment, where we train the ROB +S1+S2-OCL\n(n = 3) variant in the 10-shot setup, but then\nrun inference (i) with the same 10 shots; (ii) in\nthe 30-shot setup (i.e., effectively performing the\ninference-time data augmentation, relying on 20\nmore data instances per intent class at inference);\n(iii) in the Full setup.\nThe scores are summarised in Figure 6. They\nclearly indicate that performance does rise with\nmore data instances at inference, even without\nany model retraining/re-tuning, conﬁrming that\nincreased semantic variability helps at inference.\nThis ﬁnding is salient for all three evaluation sets.19\n19The same trends persist with other CONV FIT variants.\nAs expected, the absolute performance of 30-shot\nor Full inference when the model is trained in 10-\nshot setups is lower than in the setup where the\nmore abundant data is additionally used for CON-\nVFIT Stage 2 task-tuning.\nBased on these ﬁndings, we restate that a promis-\ning path for future research concerns investigating\nand ‘task-adapting’ automatic paraphrase genera-\ntion models (Krishna et al., 2020; Dopierre et al.,\n2021; Schick and Schütze, 2021) such as the one\nthat rely on prompting large models (e.g., GPT-3,\nT5) (Gao et al., 2021a). Such paraphrases might\nprovide a richer and semantically more varied set\nof data instances for CONV FIT task-tailored ﬁne-\ntuning and similarity-based inference.\n5 Conclusion and Future Work\nWe proposed CONV FIT, a two-stage conversa-\ntional ﬁne-tuning procedure that transforms pre-\ntrained LMs (e.g., BERT, RoBERTa) into universal\n(after Stage 1) and task-specialised conversational\nsentence encoders (after Stage 2) through dual-\nencoder architectures. The semantic knowledge\nalready stored in the pretrained LMs gets ’rewired’\nfor a particular domain and task. We demonstrated\nthat such task-specialised sentence encoders enable\ncasting intent detection (ID) as simple sentence\nsimilarity; CONV FIT-ed encoders yield strong ID\nresults across diverse ID datasets and setups.\nThe CONV FIT framework is very versatile and\nopens up many future research paths and further\nextensions and experimentation beyond the scope\nof this paper. For instance, it is possible to re-\nplace the current contrastive loss functions with\nother recent effective contrastive losses (van den\nOord et al., 2018; Gunel et al., 2021, inter alia), or\nmine hard (instead of using random) negative exam-\nples (Lauscher et al., 2020; Kalantidis et al., 2020;\nRobinson et al., 2021). We will also extend CON-\nVFIT to other pretrained models, experiment with\nautomatic paraphrasers for data augmentation, and\nport the framework to other conversational tasks\n(e.g., slot labelling for dialogue), as well as to other,\nnon-dialogue text classiﬁcation tasks.\nAcknowledgements\nWe are grateful to our colleagues at PolyAI for\nmany fruitful discussions. We also thank the anony-\nmous reviewers for their helpful suggestions.\nReferences\nRami Al-Rfou, Marc Pickett, Javier Snaider, Yun-\nHsuan Sung, Brian Strope, and Ray Kurzweil. 2016.\nConversational contextual cues: The case of person-\nalization and history for response ranking. CoRR,\nabs/1606.00372.\nIñigo Casanueva, Tadas Temcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nSemEval 2017, pages 1–14.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nYun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\n2018. Universal sentence encoder for English. In\nProceedings of EMNLP 2018, pages 169–174.\nMuthuraman Chidambaram, Yinfei Yang, Daniel Cer,\nSteve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2019. Learning cross-lingual sentence\nrepresentations via a multi-task dual-encoder model.\nIn Proceedings of the 4th Workshop on Representa-\ntion Learning for NLP, pages 250–259.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of ICLR 2020.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Proceedings\nof NeurIPS 2019, pages 7057–7067.\nSam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli ´c,\nand Matthew Henderson. 2020. Span-ConveRT:\nFew-shot span extraction for dialog with pretrained\nconversational representations. In Proceedings of\nACL 2020, pages 107–121.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips V oice Plat-\nform: An embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190, pages 12–16.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT 2019 ,\npages 4171–4186.\nThomas Dopierre, Christophe Gravier, and Wilfried\nLogerais. 2021. ProtAugment: Intent detection\nmeta-learning through unsupervised diverse para-\nphrasing. In Proceedings of ACL-IJCNLP 2021 ,\npages 2454–2466.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic BERT sentence embedding. CoRR,\nabs/2007.01852.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL-IJCNLP 2021 ,\npages 3816–3830.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of EMNLP 2021.\nDaniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek\nMondal, Michal Lis, Eshan Singhal, Nikola Mrkši´c,\nTsung-Hsien Wen, and Ivan Vuli ´c. 2021. Multilin-\ngual and cross-lingual intent detection from spoken\ndata. In Proceedings of EMNLP 2021.\nGoran Glavaš, Mladen Karan, and Ivan Vuli ´c. 2020.\nXHate-999: Analyzing and detecting abusive lan-\nguage across domains and languages. In Proceed-\nings of COLING 2020, pages 6350–6365.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of AISTATS 2010, pages\n249–256.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoy-\nanov. 2021. Supervised contrastive learning for pre-\ntrained language model ﬁne-tuning. In Proceedings\nof ICLR 2021.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL 2020, pages 8342–8360.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Ming-Wei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Proceed-\nings of ICML 2020, pages 3929–3938.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In Proceedings of CVPR 2006 , pages\n1735–1742.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS Spoken Language Sys-\ntems Pilot Corpus. In Proceedings of the Workshop\non Speech and Natural Language , HLT ’90, pages\n96–101.\nMatthew Henderson, Pawel Budzianowski, Iñigo\nCasanueva, Sam Coope, Daniela Gerz, Girish Ku-\nmar, Nikola Mrkši´c, Georgios Spithourakis, Pei-Hao\nSu, Ivan Vuli ´c, and Tsung-Hsien Wen. 2019a. A\nrepository of conversational datasets. In Proceed-\nings of the 1st Workshop on Natural Language Pro-\ncessing for Conversational AI, pages 1–10.\nMatthew Henderson, Iñigo Casanueva, Nikola Mrkši´c,\nPei-Hao Su, Tsung-Hsien Wen, and Ivan Vuli ´c.\n2020. ConveRT: Efﬁcient and accurate conversa-\ntional representations from transformers. In Find-\nings of EMNLP 2020, pages 2161–2174.\nMatthew Henderson and Ivan Vuli ´c. 2021. ConVEx:\nData-efﬁcient and few-shot slot labeling. In Pro-\nceedings of NAACL-HLT 2021.\nMatthew Henderson, Ivan Vuli ´c, Daniela Gerz, Iñigo\nCasanueva, Paweł Budzianowski, Sam Coope,\nGeorgios Spithourakis, Tsung-Hsien Wen, Nikola\nMrkši´c, and Pei-Hao Su. 2019b. Training neural re-\nsponse selection for task-oriented dialogue systems.\nIn Proceedings of ACL 2019, pages 5392–5404.\nAlexander Hermans, Lucas Beyer, and Bastian Leibe.\n2017. In defense of the triplet loss for person re-\nidentiﬁcation. CoRR, abs/1703.07737.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. In Pro-\nceedings of ICLR 2020.\nYannis Kalantidis, Mert Bülent Sariyildiz, Noé Pion,\nPhilippe Weinzaepfel, and Diane Larlus. 2020. Hard\nnegative mixing for contrastive learning. InProceed-\nings of NeurIPS 2020.\nNora Kassner and Hinrich Schütze. 2020. BERT-kNN:\nAdding a kNN search component to pretrained lan-\nguage models for better QA. In Findings of EMNLP\n2020, pages 3424–3430.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In Proceedings of ICLR 2020.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. In Proceedings of EMNLP 2020,\npages 737–762.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019.\nAn evaluation dataset for intent classiﬁcation and\nout-of-scope prediction. In Proceedings of EMNLP-\nIJCNLP 2019, pages 1311–1316.\nAnne Lauscher, Ivan Vuli´c, Edoardo Maria Ponti, Anna\nKorhonen, and Goran Glavaš. 2020. Specializing\nunsupervised pretraining models for word-level se-\nmantic similarity. In Proceedings of COLING 2020,\npages 1371–1383.\nKenton Lee, Kelvin Guu, Luheng He, Tim Dozat,\nand Hyung Won Chung. 2021. Neural data\naugmentation via example extrapolation. CoRR,\nabs/2102.01335.\nRobert Litschko, Ivan Vuli ´c, Simone Paolo Ponzetto,\nand Goran Glavaš. 2021. Evaluating multilin-\ngual text encoders for unsupervised cross-lingual re-\ntrieval. In Proceedings of ECIR 2021 , pages 342–\n358.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021a. Self-\nalignment pre-training for biomedical entity repre-\nsentations. In Proceedings of NAACL-HLT 2021.\nFangyu Liu, Ivan Vuli ´c, Anna Korhonen, and Nigel\nCollier. 2021b. Fast, effective and self-supervised:\nTransforming masked language models into univer-\nsal lexical and sentence encoders. In Proceedings of\nEMNLP 2021.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2019a. Benchmarking natural lan-\nguage understanding services for building conversa-\ntional agents. In Proceedings of IWSDS 2019.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In Proceedings of\nICLR 2018.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models. In Proceed-\nings of LREC 2014, pages 216–223.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tür.\n2020. DialoGLUE: A natural language understand-\ning benchmark for task-oriented dialogue. CoRR,\nabs/2009.13570.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tür.\n2021. Example-driven intent prediction with ob-\nservers. In Proceedings of NAACL-HLT 2021.\nShikib Mehri, Evgeniia Razumovskaia, Tiancheng\nZhao, and Maxine Eskenazi. 2019. Pretraining\nmethods for dialog context representation learning.\nIn Proceedings of ACL 2019, pages 3836–3845.\nNikola Mrkši´c, Ivan Vuli´c, Diarmuid Ó Séaghdha, Ira\nLeviant, Roi Reichart, Milica Gaši ´c, Anna Korho-\nnen, and Steve Young. 2017. Semantic specialisa-\ntion of distributional word vector spaces using mono-\nlingual and cross-lingual constraints. Transactions\nof the ACL, 5:314–325.\nKevin Musgrave, Serge J. Belongie, and Ser-Nam Lim.\n2020. A metric learning reality check. In Proceed-\nings of ECCV 2020, pages 681–699.\nJames O’Neill and Danushka Bollegala. 2021.\nSemantically-conditioned negative samples for efﬁ-\ncient contrastive learning. CoRR, abs/2102.06603.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext Transformer. Journal of Machine Learning Re-\nsearch, 21:140:1–140:67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of EMNLP 2019 , pages\n3982–3992.\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and\nStefanie Jegelka. 2021. Contrastive learning with\nhard negative samples. In Proceedings of ICLR\n2021.\nPeter J. Rousseeuw. 1987. Silhouettes: A graphical aid\nto the interpretation and validation of cluster analy-\nsis. Journal of Computational and Applied Mathe-\nmatics, 20:53–65.\nSebastian Ruder. 2021. Recent advances in lan-\nguage model ﬁne-tuning. http://ruder.io/\nrecent-advances-lm-fine-tuning .\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nBERT: Smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\nSheikh Muhammad Sarwar, Dimitrina Zlatkova, Mom-\nchil Hardalov, Yoan Dinkov, Isabelle Augenstein,\nand Preslav Nakov. 2021. A neighbourhood frame-\nwork for resource-lean content ﬂagging. CoRR,\nabs/2103.17055.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. CoRR,\nabs/2104.07540.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In International Confer-\nence on Acoustics, Speech and Signal Processing ,\npages 5149–5152.\nPatrice Y . Simard, Yann LeCun, and John S. Denker.\n1992. Efﬁcient pattern recognition using a new\ntransformation distance. In Proceedings of NeurIPS\n1992, pages 50–58.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical networks for few-shot learning.\nIn Proceedings of NeurIPS 2017, pages 4077–4087.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang,\nPhilip H. S. Torr, and Timothy M. Hospedales. 2018.\nLearning to compare: Relation network for few-\nshot learning. In Proceedings of CVPR 2018, pages\n1199–1208.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.\nContrastive representation distillation. In Proceed-\nings of ICLR 2020.\nViet-Anh Tran, Romain Hennequin, Jimena Royo-\nLetelier, and Manuel Moussallam. 2019. Improv-\ning collaborative metric learning with efﬁcient nega-\ntive sampling. In Proceedings of SIGIR 2019, pages\n1201–1204.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. CoRR, abs/1807.03748.\nLaurens van der Maaten and Geoffrey E. Hinton. 2012.\nVisualizing non-metric similarities in multiple maps.\nMachine Learning, 87(1):33–55.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS 2017 , pages\n6000–6010.\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray\nKavukcuoglu, and Daan Wierstra. 2016. Matching\nnetworks for one shot learning. In Proceedings of\nNeurIPS 2016, pages 3630–3638.\nIvan Vuli´c, Edoardo Maria Ponti, Anna Korhonen, and\nGoran Glavaš. 2021. LexFit: Lexical ﬁne-tuning of\npretrained language models. In Proceedings of ACL-\nIJCNLP 2021, pages 5269–5283.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of EMNLP 2020, pages 7222–7240, On-\nline.\nEric Wallace, Shi Feng, and Jordan Boyd-Graber. 2018.\nInterpreting neural networks with nearest neighbors.\nIn Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 136–144.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. In Proceedings of NeurIPS\n2019, pages 3261–3275.\nXun Wang, Xintong Han, Weilin Huang, Dengke Dong,\nand Matthew R. Scott. 2019b. Multi-similarity loss\nwith general pair weighting for deep metric learning.\nIn Proceedings of CVPR 2019, pages 5022–5030.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of ACL 2018, pages 451–462.\nJohn Wieting, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2020. A bilingual generative trans-\nformer for semantic sentence embedding. In Pro-\nceedings of EMNLP 2020, pages 1581–1594.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of NAACL-HLT 2018, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of EMNLP 2020: System\nDemonstrations, pages 38–45.\nChien-Sheng Wu, Steven C.H. Hoi, Richard Socher,\nand Caiming Xiong. 2020. TOD-BERT: Pre-trained\nnatural language understanding for task-oriented di-\nalogue. In Proceedings of EMNLP 2020, pages 917–\n929.\nWeijia Xu, Batool Haider, and Saab Mansour. 2020.\nEnd-to-end slot alignment and recognition for cross-\nlingual NLU. In Proceedings of EMNLP 2020 ,\npages 5052–5063.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernandez\nAbrego, Steve Yuan, Chris Tar, Yun-hsuan Sung,\net al. 2020. Multilingual universal sentence encoder\nfor semantic retrieval. In Proceedings of ACL 2020:\nSystem Demonstrations, pages 87–94.\nYinfei Yang, Gustavo Hernandez Abrego, Steve Yuan,\nMandy Guo, Qinlan Shen, Daniel Cer, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2019. Im-\nproving multilingual sentence embedding using bi-\ndirectional dual encoder with additive margin soft-\nmax. In Proceedings of IJCAI 2019 , pages 5370–\n5378.\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018. Learn-\ning semantic textual similarity from conversations.\nIn Proceedings of the 3rd Workshop on Representa-\ntion Learning for NLP, pages 164–174.\nJianguo Zhang, Kazuma Hashimoto, Wenhao Liu,\nChien-Sheng Wu, Yao Wan, Philip Yu, Richard\nSocher, and Caiming Xiong. 2020. Discriminative\nnearest neighbor few-shot intent detection by trans-\nferring natural language inference. In Proceedings\nof EMNLP 2020, pages 5064–5082.\nMengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli ´c,\nRoi Reichart, Anna Korhonen, and Hinrich Schütze.\n2021. A closer look at few-shot crosslingual trans-\nfer: The choice of shots matters. In Proceedings of\nACL-IJCNLP 2021, pages 5751–5767.\nA Additional Experiments and Results\nAdditional experiments and analyses that further\nsupport the main claims of the paper have been rel-\negated to the appendix for clarity and compactness\nof our presentation in the main paper. They largely\nfollow the trends observed in the results which are\nprovided in the main paper. In sum, we provide\nthe following additional results and information,\nwhich offer further empirical support of our main\nclaims in this paper:\nTable 4 provides the results with all input LMs\nin our comparison in all the CONV FIT variants\ndiscussed in §3.1 across different data setups on all\nthree intent detection datasets. It can be seen as a\nfull (i.e., expanded) version of Table 2 provided in\nthe main paper.\nFigure 8 (COS loss in Stage 2) and Figure 9 (OCL\nloss in Stage 2) demonstrate the impact of using\nLM-pretrained Transformers versus randomly ini-\ntialised Transformers in the CONV FIT framework\n(both in the full S1+S2 setup, as well as in the\nsetup where only task-tuning (S2) is employed).\nThe patterns in the results, presented over all three\nevaluation sets, largely follow the patterns observed\nin Figure 3, which is provided in the main paper.\nFigure 10 plots how the amount of Reddit data in\nStage 1 impacts the ﬁnal intent detection perfor-\nmance when the COS loss is used for task-tuning\nin Stage 2. The observed trends in results are very\nsimilar to the ones obtained with the OCL loss, pre-\nsented in the main paper (see Figure 4).\nFigure 11 presents the impact of the number of\nnegative examples n during Stage 2 ﬁne-tuning\nwith the COS loss; the observed trends are very\nsimilar to the ones with the OCL loss, presented in\nthe main paper (see Figure 5).\nFigure 12 provides t-SNE plots with varying\namounts of task data for Stage 2 task-tuning (10-\nshot versus 30-shot versus Full data setups), demon-\nstrating that very tight and coherent clusters emerge\neven in the 10-shot setups. Figure 13 shows t-SNE\nplots after 10-shot Stage 2, when varying amounts\nof Reddit data for Stage 1 ﬁne-tuning are used\n(e.g., skipping Stage 1 completely versus using\n≈50k (context, response) Reddit pairs). Finally,\nFigure 14 demonstrates that the patterns which\nemerge after Stage 1 and Stage 2 CONV FIT-ing\ndo not depend on the chosen input LM, and on the\nchosen loss function in Stage 2: the trends very\nsimilar to Figure 2 (provided in the main paper) are\nalso observed with distilRoBERTa as the input LM,\nand COS as the S2 loss. Figure 7 shows visible\nimpact of adaptive Stage 1 ﬁne-tuning even when\nonly 50kReddit (context, response) pairs are used.\nB Models and Evaluation Data\nURLs to the models are provided in Table 6. The\nintent detection evaluation data is available online:\n1. BANKING 77, CLINC 150 , and HWU 64 intent\ndetection data have been downloaded from the Di-\naloGLUE repository:\ngithub.com/alexa/dialoglue\nWe use the 10-shot data provided in the reposi-\ntory, and use their script to generate 30-shot setups\nfor all three datasets.\n2. The English ATIS intent detection dataset is\nextracted from the recently published MultiATIS++\ndataset (Xu et al., 2020), available here:\ngithub.com/amazon-research/\nmultiatis\nFor reproducibility, we will release the generated\n10-shot and 30-shot data splits.\nOur code is based on PyTorch, and relies on the\ntwo following widely used repositories:\n• sentence-transformers\nwww.sbert.net\n• huggingface.co/transformers/\nFigure 7: t-SNE plots of encoded utterances from the\ntest set of BANKING 77 (a subset of 12 intents, see\nthe legend in Figure 2) after Stage 1 ﬁne-tuning of\nRoBERTa using only ≈50k (context, response) pairs\nfrom Reddit; cf., Figure 2a.\nBANKING 77 CLINC 150 HWU 64\nModel Variant 10 30 Full 10 30 Full 10 30 Full\nSimilarity-Based Classiﬁcation\nROB +S1+S2-COS 86.48 91.33 94.35 92.87 95.91 97.20 85.06 90.46 92.98\nBERT +S1+S2-COS 84.32 90.91 93.91 91.80 95.58 96.56 85.13 89.41 91.93\nDROB +S1+S2-COS 85.13 90.75 94.06 91.64 95.48 97.00 83.64 89.68 92.94\nRAND +S1+S2-COS 79.03 87.37 91.69 83.96 89.98 94.12 76.30 82.62 88.20\nROB +S1+S2-OCL 87.38 91.36 94.16 92.89 96.42 97.34 85.32 90.06 92.42\nBERT +S1+S2-OCL 85.97 90.65 93.77 91.53 95.53 96.82 85.04 89.41 92.21\nDROB +S1+S2-OCL 86.04 90.78 93.89 91.98 95.60 97.04 83.64 89.50 92.84\nRAND +S1+S2-OCL 80.62 87.01 91.49 84.91 90.98 94.44 77.23 82.99 88.85\nROB +S2-COS 84.96 90.81 94.19 91.56 95.64 96.78 84.52 89.87 92.19\nBERT +S2-COS 81.27 90.32 93.73 89.58 95.08 96.54 82.90 89.12 91.78\nDROB +S2-COS 83.28 90.58 93.91 89.47 95.32 86.78 82.43 89.41 92.10\nRAND +S2-COS 70.32 84.16 90.75 76.31 86.69 91.76 65.89 79.18 86.43\nROB +S2-OCL 85.78 90.98 93.77 92.64 95.40 96.87 84.76 89.31 92.01\nBERT +S2-OCL 82.28 89.77 93.54 90.71 95.07 96.62 83.09 88.94 92.57\nDROB +S2-OCL 82.60 90.65 93.38 90.78 95.02 96.69 81.69 88.75 92.38\nRAND +S2-OCL 63.15 81.30 89.71 69.91 85.53 92.18 60.48 76.67 86.90\nROB +S1+S2-SMAX 86.27 90.58 94.06 92.44 95.62 96.76 85.87 88.83 92.48\nBERT +S1+S2-SMAX 84.44 90.16 93.09 90.31 93.84 95.91 83.28 88.18 92.29\nDROB +S1+S2-SMAX 83.32 89.85 93.47 90.42 94.13 96.47 83.36 88.75 92.57\nRAND +S1+S2-SMAX 76.79 85.55 90.97 82.22 87.69 92.91 76.30 81.51 88.85\nROB +S2-SMAX 84.61 90.49 93.66 91.89 95.17 96.71 83.46 88.57 92.57\nBERT +S2-SMAX 81.33 89.44 92.63 89.69 93.38 96.12 81.51 87.83 91.58\nDROB +S2-SMAX 82.60 89.31 93.54 89.44 93.96 96.04 82.53 87.36 91.91\nRAND +S2-SMAX 73.38 83.67 90.32 76.71 85.53 92.62 68.77 79.74 88.94\nBaselines: MLP Classiﬁcation\nROB +S1 83.08 90.16 93.38 90.98 94.12 96.42 81.13 87.73 91.44\nBERT +S1 82.69 89.82 93.67 89.88 94.07 96.33 82.25 88.01 91.12\nDROB +S1 82.95 89.55 93.34 89.76 93.46 96.02 81.23 87.64 90.91\nCONVE RT∗ 83.32 89.37 93.01 92.62 95.78 97.16 82.65 87.88 91.24\nUSE∗ 84.23 89.74 92.81 90.85 93.98 95.06 83.75 89.03 91.25\nUSE (ours) 82.95 89.09 92.81 90.27 93.54 94.91 82.71 88.20 91.64\nLABSE 81.69 88.96 92.60 90.89 93.41 95.12 81.60 86.15 90.99\nBaselines: Full Fine-Tuning\nBERT (BASE )∗∗ 79.87 – 93.02 89.52 – 95.93 81.69 – 89.97\nTable 4: Accuracy scores (×100%) on the three intent detection data sets with varying number of training examples\n(10 examples per intent; 30 examples per intent; Full training data). As mentioned in §3, n= 3negatives are used\nin Stage 2 for 10-shot and 30-shot setups, n = 1for the Full setup. The peak scores per column are in bold, the\nsecond best is underlined. *The scores were taken directly from prior work, and computed on different 10/30-shot\nsamples (and are thus not directly comparable, Zhao et al. 2021) **The scores achieved by full (regular) ﬁne-tuning\nof BERT (BASE ) have been taken directly from Mehri et al. (2020), and were not available for the 30-shot setup.\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2\n(a) BANKING 77\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2 (b) CLINC 150\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2 (c) HWU 64\nFigure 8: A comparison of a randomly initialized BERT or RoBERTa architecture ( RAND ) with LM-pretrained\nRoBERTa after Stage 2 C ONV FIT-ing; evaluation on all three intent detection datasets; the COS loss used in S2.\nFigure 9 shows the similar plots with the OCL loss used in S2.\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2\n(a) BANKING 77\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2 (b) CLINC 150\n10 30 Full\nTraining Set Size (# of Examples per Intent)\n60\n65\n70\n75\n80\n85\n90\n95Accuracy\nRAND +S2 RAND +S1+S2 ROB +S1+S2 (c) HWU 64\nFigure 9: A comparison of a randomly initialized BERT or RoBERTa architecture ( RAND ) with LM-pretrained\nRoBERTa after Stage 2 CONV FIT-ing; evaluation on all three intent detection datasets; the OCL loss used in S2.\nBANKING 77 CLINC 150 HWU 64\nEvaluation Set (10-Shot Setup)\n80\n84\n88\n92\n96Accuracy\nno S1 × 1/256 × 1/64 × 1/16 × 1 × 2\n(a) 10-shot (ROB +S1+S2-COS )\nBANKING 77 CLINC 150 HWU 64\nEvaluation Set (30-Shot Setup)\n80\n84\n88\n92\n96Accuracy\nno S1 × 1/256 × 1/64 × 1/16 × 1 × 2 (b) 30-shot (ROB +S1+S2-COS )\nFigure 10: Varying the amount of Reddit data for Stage 1 C ONV FIT; ×1 refers to the Reddit size used in all our\nother Stage 1 ﬁne-tuning experiments (≈2% of the full Reddit corpus from Henderson et al. (2019a)), while other\nReddit data sizes are relative to this corpus size (e.g., ×1/32 means that we use 2%/32 ≈0.0625% of the full\nReddit corpus). Stage 2 loss is COS (n= 3).\nn = 1 n = 3 n = 5 n = 7\nNumber of negative examples\n80\n84\n88\n92\n96Accuracy\n+S2 (10) +S2 (30) +S1+S2 (10) +S1+S2 (30)\n(a) BANKING 77\nn = 1 n = 3 n = 5 n = 7\nNumber of negative examples\n80\n84\n88\n92\n96Accuracy\n+S2 (10) +S2 (30) +S1+S2 (10) +S1+S2 (30) (b) CLINC 150\nn = 1 n = 3 n = 5 n = 7\nNumber of negative examples\n80\n84\n88\n92\n96Accuracy\n+S2 (10) +S2 (30) +S1+S2 (10) +S1+S2 (30) (c) HWU 64\nFigure 11: Impact of the number of negative examples non intent detection performance in 10-shot and 30-shot\nsetups. The C ONV FIT model variants are ROB +S2+COS and ROB +S1+S2+COS , that is, RoBERTa is the input\nLM in all experiments, and the results show model variants with the COS loss in Stage 2, without and with S1\nﬁne-tuning (labelled +S2 and +S1+S2 in the ﬁgures, respectively).\nBANKING 77 CLINC 150 HWU 64\nAfter 10 30 10 30 10 30\nEpoch 1 86.30 91.40 92.80 96.02 86.15 90.33\nEpoch 2 87.38 91.36 92.89 96.42 85.32 90.06\nEpoch 5 87.28 91.46 93.29 96.32 85.69 89.98\nTable 5: Impact of longer Stage 2 CONV FIT-ing on the ﬁnal performance; ROB +S1+S2-OCL .\n(a) RoBERTa (10-shot S2)\n (b) RoBERTa (30-shot S2)\n (c) RoBERTa (Full S2)\nFigure 12: t-SNE plots (van der Maaten and Hinton, 2012) of encoded utterances from the test set of BANKING 77\n(i.e., all examples are effectively unseen by the encoder models at training) associated with a selection of 12 intents.\nThe encoded utterances are created via mean-pooling based on ﬁne-tuned RoBERTa encoders which underwent\nStage 1 plus Stage 2 in the (a) 10-shot Stage 2 setup (i.e., 10 examples per intent); (b) 30-shot setup; (c) Full setup\n(see also §3). Stage 2: ﬁne-tuning with the OCL objective (n = 3 negatives). The results suggest that even in\n10-shot setups it is possible to learn coherent clusters and clear cluster separations; however, the clusters become\nless and less compact, and less separated in the semantic space as we ﬁne-tune with fewer in-task instances (e.g.,\ncompare the clusters in the 10-shot versus Full setup), and the ﬁne-tuned encoder model is more prone to incorrect\ncluster assignments. This (initially) visual observation is also supported by the Silhouette coefﬁcient scores (higher\nis better): (a) σ= 0.378, (b) σ= 0.548, (c) σ= 0.698.\n(a) RoBERTa (10-shot S2)\n (b) RoBERTa (30-shot S2)\n (c) RoBERTa (Full S2)\nFigure 13: t-SNE plots of encoded utterances from the test set of BANKING 77 (i.e., all examples are effectively\nunseen by the encoder models at training) associated with a selection of 12 intents. The encoded utterances are\ncreated via mean-pooling based on RoBERTa as the input LM: (a) without any Stage 1 ﬁne-tuning with Reddit\ndata; (b) Stage 1 ﬁne-tuning with only 50k (context, response) Reddit pairs; (c) Stage 1 ﬁne-tuning with 2% of the\nfull Reddit corpus of Henderson et al. (2019a) ( ≈15M pairs). Stage 2 in all three cases is performed in 10-shot\nsetups with the OCL objective (n= 3negatives). The respective Silhouette coefﬁcient scores (higher is better): (a)\nσ= 0.320, (b) σ= 0.338, (c) σ= 0.378.\n(a) DistilRoBERTa (no ﬁne-tuning)\n (b) DistilRoBERTa (after S1)\n (c) DistilRoBERTa (after S1 and S2)\nFigure 14: t-SNE plots of encoded utterances from the test set of BANKING 77 (i.e., all examples are effectively\nunseen by the encoder models) associated with a selection of 12 intents. The encoded utterances are created via\nmean-pooling based on (a) the original DistilRoBERTa LM; (b) DistilRoBERTa after Stage 1 (i.e., ﬁne-tuned on\n2% of the full Reddit corpus, see Figure 1); (c) DistilRoBERTa after Stage 1 and Stage 2, ﬁne-tuned with the COS\nobjective (n= 3negatives) using the entire BANKING 77 training set (see Figure 1).\nName Abbreviation URL\nbert-base-cased BERT huggingface.co/bert-base-uncased\nroberta-base ROB huggingface.co/roberta-base\ndistilroberta-base DROB huggingface.co/distilroberta-base\nLaBSE LaBSE huggingface.co/sentence-transformers/LaBSE\nmultilingual USE USE tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\nTable 6: URLs of the language models used in this work.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8484857678413391
    },
    {
      "name": "Encoder",
      "score": 0.8088208436965942
    },
    {
      "name": "Sentence",
      "score": 0.6610584259033203
    },
    {
      "name": "Transformer",
      "score": 0.6090512275695801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5477339029312134
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5358205437660217
    },
    {
      "name": "Natural language processing",
      "score": 0.5232607126235962
    },
    {
      "name": "Language model",
      "score": 0.5084863901138306
    },
    {
      "name": "Speech recognition",
      "score": 0.4943878650665283
    },
    {
      "name": "Task (project management)",
      "score": 0.4753149151802063
    },
    {
      "name": "Inference",
      "score": 0.46281516551971436
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}