{
    "title": "WaveFormer: Wavelet Transformer for Noise-Robust Video Inpainting",
    "url": "https://openalex.org/W4393178524",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2096106746",
            "name": "ZhiLiang Wu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2969698273",
            "name": "Changchang Sun",
            "affiliations": [
                "Illinois Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2697822659",
            "name": "Hanyu Xuan",
            "affiliations": [
                "Anhui University"
            ]
        },
        {
            "id": "https://openalex.org/A2124836529",
            "name": "Gaowen Liu",
            "affiliations": [
                "Cisco College"
            ]
        },
        {
            "id": "https://openalex.org/A1898040009",
            "name": "YAN Yan",
            "affiliations": [
                "Illinois Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2096106746",
            "name": "ZhiLiang Wu",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2969698273",
            "name": "Changchang Sun",
            "affiliations": [
                "Illinois Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2697822659",
            "name": "Hanyu Xuan",
            "affiliations": [
                "Anhui University"
            ]
        },
        {
            "id": "https://openalex.org/A1898040009",
            "name": "YAN Yan",
            "affiliations": [
                "Illinois Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6843258702",
        "https://openalex.org/W6948047193",
        "https://openalex.org/W2940836443",
        "https://openalex.org/W3113887880",
        "https://openalex.org/W6782426213",
        "https://openalex.org/W4283276172",
        "https://openalex.org/W2973485811",
        "https://openalex.org/W6729311293",
        "https://openalex.org/W4297095253",
        "https://openalex.org/W3213635490",
        "https://openalex.org/W4286750709",
        "https://openalex.org/W6800516539",
        "https://openalex.org/W2943981500",
        "https://openalex.org/W2996048079",
        "https://openalex.org/W6754287722",
        "https://openalex.org/W6767140242",
        "https://openalex.org/W3086787510",
        "https://openalex.org/W4224843139",
        "https://openalex.org/W3043209677",
        "https://openalex.org/W6751547418",
        "https://openalex.org/W3196995472",
        "https://openalex.org/W3202462044",
        "https://openalex.org/W3035088594",
        "https://openalex.org/W2132984323",
        "https://openalex.org/W2470139095",
        "https://openalex.org/W4313147094",
        "https://openalex.org/W6839143940",
        "https://openalex.org/W6847068945",
        "https://openalex.org/W4385801675",
        "https://openalex.org/W4385804995",
        "https://openalex.org/W2886714066",
        "https://openalex.org/W6856723752",
        "https://openalex.org/W4312389550",
        "https://openalex.org/W4372260439",
        "https://openalex.org/W3169240914",
        "https://openalex.org/W2889986507",
        "https://openalex.org/W6762535543",
        "https://openalex.org/W4285077726",
        "https://openalex.org/W3184605395",
        "https://openalex.org/W3044863660",
        "https://openalex.org/W2936180003",
        "https://openalex.org/W6841654211",
        "https://openalex.org/W2783879794",
        "https://openalex.org/W4386590355",
        "https://openalex.org/W3144145640",
        "https://openalex.org/W2970400386",
        "https://openalex.org/W2986330308",
        "https://openalex.org/W3094825490",
        "https://openalex.org/W3097348442",
        "https://openalex.org/W3095422700",
        "https://openalex.org/W3182000414",
        "https://openalex.org/W4312655706",
        "https://openalex.org/W4312278129",
        "https://openalex.org/W4312376588",
        "https://openalex.org/W3211494544",
        "https://openalex.org/W2963494934",
        "https://openalex.org/W3201844719",
        "https://openalex.org/W4386066541",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W4294434188",
        "https://openalex.org/W2987614525",
        "https://openalex.org/W2886787375",
        "https://openalex.org/W4312598744",
        "https://openalex.org/W2963231084",
        "https://openalex.org/W2988407809",
        "https://openalex.org/W4386065704",
        "https://openalex.org/W3107297981",
        "https://openalex.org/W2953043157",
        "https://openalex.org/W3195987654",
        "https://openalex.org/W2963502975",
        "https://openalex.org/W4318255568",
        "https://openalex.org/W4384648440",
        "https://openalex.org/W2986433113",
        "https://openalex.org/W4389115942",
        "https://openalex.org/W4312832682",
        "https://openalex.org/W3171287051",
        "https://openalex.org/W2551763541"
    ],
    "abstract": "Video inpainting aims to fill in the missing regions of the video frames with plausible content. Benefiting from the outstanding long-range modeling capacity, the transformer-based models have achieved unprecedented performance regarding inpainting quality. Essentially, coherent contents from all the frames along both spatial and temporal dimensions are concerned by a patch-wise attention module, and then the missing contents are generated based on the attention-weighted summation. In this way, attention retrieval accuracy has become the main bottleneck to improve the video inpainting performance, where the factors affecting attention calculation should be explored to maximize the advantages of transformer. Towards this end, in this paper, we theoretically certificate that noise is the culprit that entangles the process of attention calculation. Meanwhile, we propose a novel wavelet transformer network with noise robustness for video inpainting, named WaveFormer. Unlike existing transformer-based methods that utilize the whole embeddings to calculate the attention, our WaveFormer first separates the noise existing in the embedding into high-frequency components by introducing the Discrete Wavelet Transform (DWT), and then adopts clean low-frequency components to calculate the attention. In this way, the impact of noise on attention computation can be greatly mitigated and the missing content regarding different frequencies can be generated by sharing the calculated attention. Extensive experiments validate the superior performance of our method over state-of-the-art baselines both qualitatively and quantitatively.",
    "full_text": "WaveFormer: Wavelet Transformer for Noise-Robust Video Inpainting\nZhiliang Wu1, Changchang Sun2, Hanyu Xuan3*, Gaowen Liu4, Yan Yan2\n1 CCAI, Zhejiang University, China\n2 Department of Computer Science, Illinois Institute of Technology, USA\n3 School of Big Data and Statistics, Anhui University, China\n4 Cisco Research, USA\nAbstract\nVideo inpainting aims to fill in the missing regions of the\nvideo frames with plausible content. Benefiting from the\noutstanding long-range modeling capacity, the transformer-\nbased models have achieved unprecedented performance re-\ngarding inpainting quality. Essentially, coherent contents\nfrom all the frames along both spatial and temporal di-\nmensions are concerned by a patch-wise attention module,\nand then the missing contents are generated based on the\nattention-weighted summation. In this way, attention retrieval\naccuracy has become the main bottleneck to improve the\nvideo inpainting performance, where the factors affecting at-\ntention calculation should be explored to maximize the ad-\nvantages of transformer. Towards this end, in this paper, we\ntheoretically certificate that noise is the culprit that entan-\ngles the process of attention calculation. Meanwhile, we pro-\npose a novel wavelet transformer network with noise robust-\nness for video inpainting, named WaveFormer. Unlike exist-\ning transformer-based methods that utilize the whole embed-\ndings to calculate the attention, our WaveFormer first sepa-\nrates the noise existing in the embedding into high-frequency\ncomponents by introducing the Discrete Wavelet Transform\n(DWT), and then adopts clean low-frequency components to\ncalculate the attention. In this way, the impact of noise on at-\ntention computation can be greatly mitigated and the missing\ncontent regarding different frequencies can be generated by\nsharing the calculated attention. Extensive experiments val-\nidate the superior performance of our method over state-of-\nthe-art baselines both qualitatively and quantitatively.\nIntroduction\nVideo inpainting which aims to fill missing regions of videos\nwith plausible contents is a fundamental yet challenging task\nin the computer vision field. It has great value in many\npractical applications, such as scratch restoration (Chang\net al. 2019), undesired object removal (Seoung et al. 2019)\nand autonomous driving (Liao et al. 2020). Unlike image\ninpainting (Somani et al. 2023; Shukla et al. 2023; Bar\net al. 2022) that usually focuses on the spatial dimension,\nvideo inpainting pays more attention to exploiting the tem-\nporal information. Therefore, naively extending the image\ninpainting algorithm on individual video frame will neglect\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nthe inter-frame motion continuity, resulting in flicker arti-\nfacts (Chang et al. 2019; Wu et al. 2023a).\nRecently, several deep learning-based video inpainting\nmethods (Gao et al. 2020; Ji et al. 2022; Lee et al. 2019; Li\net al. 2022; Wu et al. 2021; Zeng et al. 2019; Liu et al. 2020)\nhave been proposed and achieved great progress in terms\nof the quality and speed. However, due to the limited re-\nceptive field along the temporal domain, these methods still\nsuffer from limitations of blurry and misplacement artifacts\nin the completed video (Ren et al. 2022; Wu et al. 2023b). To\naddress these issues, the state-of-the-art methods (Cai et al.\n2022; Lee et al. 2019; Li et al. 2020; Liu et al. 2021; Ren\net al. 2022; Seoung et al. 2019; Wu et al. 2023c; Zhang, Wu,\nand Yan 2023) resort to the attention mechanism to explore\nthe long-term correspondences between frames. In this way,\nthe available content at distant frames can also be globally\npropagated into the missing regions. Notably, the represen-\ntative technique transformer (Cai et al. 2022; Liu et al. 2021;\nRen et al. 2022; Zeng, Fu, and Chao 2020; Cai et al. 2022;\nZhang, Fu, and Liu 2022) has gained increasing attention\nfrom researchers of video inpainting field due to its remark-\nable advantage of long-range modeling capacity. Typically,\nthese transformer-based methods first search coherent con-\ntents from all the frames along both spatial and temporal di-\nmensions by a patch-wise attention mechanism, and then uti-\nlize the attention-weighted summation to generate the miss-\ning contents. It means that the attention retrieval accuracy\nhas become the main bottleneck limiting the inpainting per-\nformance. Inaccurate attention retrieval will ignore relevant\ncontent that is essential in video inpainting and introduce\nmore irrelevant content in the missing regions, resulting in\ngenerating blurry or compromised contents (Zhang et al.\n2023; Zhang, Fu, and Liu 2022).\nIn fact, due to the limitations of transmission media and\nrecording equipment, digital images and videos will in-\nevitably be polluted by noise during the transmission and\nrecording process (Geng et al. 2022). Correspondingly, the\nlearned embeddings always contain noise. Therefore, to im-\nprove the performance of video inpainting, it is promising\nand necessary to explore the impact of noise on attention\ncomputation. For this purpose, we theoretically certificate\nthat noise-contained inputs are disadvantageous to trans-\nformers’ attention calculation. Then, to address above disad-\nvantages caused by ubiquitous noise in video inpainting, we\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6180\npropose a novel wavelet transformer network by introduc-\ning the Discrete Wavelet Transform (DWT) (Mallat 1989),\ndubbed as WaveFormer.\nConcretely, unlike existing transformer-based video in-\npainting methods that utilize the whole embedding to cal-\nculate attention (Cai et al. 2022; Liu et al. 2021; Ren et al.\n2022; Zhang, Fu, and Liu 2022), our WaveFormer first\nadopts DWT to decompose the embedding used for the at-\ntention calculation into low-frequency and high-frequency\ncomponents. By doing this, the noise existing in the em-\nbedding can be explicitly separated into the high-frequency\ncomponents, making the low-frequency ones contain rela-\ntively clean basic features. In this way, the calculation of\nattention weight is only based on the low-frequency compo-\nnents and the missing content regarding different frequen-\ncies can be generated by sharing such attentions. Finally, the\ncompleted low-frequency and high-frequency components\nare aggregated to yield the final inpainting result through\nInverse Discrete Wavelet Transform (IDWT).\nSubstantial experiments show that our WaveFormer out-\nperforms the state-of-the-arts by a significant margin in\nterms of PSNR and Ewarp (flow warping error) with rela-\ntive improvements of 7.45% and 9.48%, respectively. More-\nover, thanks to the robustness to noise, our method is able to\nfill missing regions using the visually-plausible and spatial-\ntemporal coherent contents with fine-grained details. To sum\nup, our contributions are summarized as follows:\n• We theoretically demonstrate that noise always cause in-\nferior effect when calculating attention. To the best of our\nknowledge, this is the first attempt to explore the factors\nthat affect the transformers’ attention calculation in the\nvideo inpainting.\n• We propose a novel WaveFormer by introducing the\nDWT. It can calculate the attention on low-frequency\ncomponents and share it with the high-frequency com-\nponents, greatly mitigating the impact of noise on the at-\ntention calculation.\n• Experiments on two benchmark datasets, including\nYoutube-vos (Xu et al. 2018) and DA VIS (Perazzi et al.\n2016), demonstrate the superiority of our proposed\nmethod in both quantitative and qualitative views.\nRelated Work\nVideo Inpainting\nWith the rapid development of deep learning (Shang et al.\n2023; Gu et al. 2023; Shang et al. 2022), several deep\nlearning-based video inpainting methods have been pro-\nposed recently. For instance, Wang et al. (Wang et al. 2019),\nKim et al. (Kim et al. 2019), and Chang et al. (Chang et al.\n2019) employed the 3D temporal convolution and directly\naggregate the temporal information of neighbor frames to\nreconstruct the missing contents. However, compared with\n2D CNN, 3D CNN has relatively higher computational com-\nplexities, limiting the application of these methods in the\nreal scenarios (Wu et al. 2023b; Ji et al. 2022; Liu, Li, and\nZhu 2022). To alleviate this issue, treating the video inpaint-\ning as a pixel propagation problem has been explored by\nsome works (Gao et al. 2020; Kang, Oh, and Kim 2022; Ke,\nTai, and Tang 2021; Li et al. 2022; Xu et al. 2019; Zou et al.\n2021). In particular, they first exploit a deep flow completion\nnetwork to restore the flow sequence. Such a restored flow\nsequence is used to guide the relevant pixels of neighbor-\ning frames to fill in the missing regions. Overall, although\nthese methods have shown promising results, they fail to\ncapture the visible contents of long-distance frames, result-\ning in poor inpainting performance in the scene with large\nobjects or slowly moving objects.\nTo effectively model the long-distance correspondence,\nrecent methods (Cai et al. 2022; Li et al. 2020; Ren et al.\n2022; Seoung et al. 2019; Srinivasan et al. 2021) introduced\nthe attention module to retrieve information from neighbor-\ning frames and adopted weighted summing operation to gen-\nerate missing contents. Among these methods, benefiting\nfrom the advantages of long-range feature capture capacity,\ntransformer has shed light to the video inpainting commu-\nnity. For example, Zeng et al. (Zeng, Fu, and Chao 2020)\nproposed the first transformer model for video inpainting\nby designing a multi-layer multi-head transformer. To im-\nprove the edge details of missing contents, Liu et al. (Liu\net al. 2021) devised a new transformer model by intro-\nducing soft split and soft composition operations. In addi-\ntion, Ren et al. (Ren et al. 2022) developed a novel Dis-\ncrete Latent Transformer (DLFormer) by formulating video\ninpainting task into the discrete latent space. Meanwhile,\nZhang (Zhang, Fu, and Liu 2022) leveraged the motion dis-\ncrepancy exposed by optical flows to instruct the attention\nretrieval in the transformer for high-fidelity video inpainting.\nAt the same time, Cai (Cai et al. 2022) designed a new De-\nformed Vision Transformer (DeViT) with emphasis on bet-\nter patch-wise alignment and matching in video inpainting.\nIt is worth noting that these transformer-based video in-\npainting methods ignore the impact of noise on attention\ncalculation, which inevitably leads to inaccurate attention re-\ntrieval. In our work, we expect to explore the mechanism of\nnoise works in the attention calculation and propose a novel\nwavelet transformer network with noise robustness to im-\nprove the accuracy of attention retrieval.\nDiscrete Wavelet Transform (DWT)\nThanks to the powerful time-frequency analysis capabil-\nity of DWT, more and more researchers expect to com-\nbine it with deep learning to solve various computer vision\ntasks. For example, Liu et al. (Liu et al. 2018) presented\na novel multi-level wavelet CNN to enlarge the receptive\nfield for a better trade-off between efficiency and restora-\ntion performance. To preserve the original image details\nwhile reducing computational cost in self-attention learning,\nYao et al. (Yao et al. 2022) formulated a invertible down-\nsampling for wavelet transforms. Yu et al. (Yu et al. 2021)\nproposed a wavelet-based inpainting network that can sep-\narately fills the missing regions of each frequency band.\nThese works show that combining wavelets and CNNs is\npromising. However, to the best of our knowledge, the po-\ntential of using wavelets to mitigate the influence of noise\non the attention calculation of transformer has not been well\nvalidated, which is the major concern of this paper.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6181\nMotivation\nIn this paper, we argue that noise is disadvantageous to the\ntransformers’ attention calculation, which greatly limits the\nperformance of video inpainting. Using the noise-contained\nembeddings to calculate the attention will disregard the con-\ntents related with the missing regions and increases unre-\nlated contents filled into the missing regions during video\ncompletion, leading to blurred or compromised missing con-\ntents and hence suffer from the inferior inpainting results.\nTheorem: Given n noise-contained features fi, whose di-\nmension is h × w × c and value range from 0 to 1. For-\nmally, fi can be denoted as the summation of the clean fea-\nture ei ∈ [0, 1]h×w×c and the noiseoi ∈ [0, 1]h×w×c (Cheng\net al. 2021; Jia, Wong, and Zeng 2021; Pang et al. 2021),i.e.,\nfi = ei + oi. Let rf\ni,j stands for the attention between noise-\ncontained features fi and fj, and re\ni,j denotes the attention\nbetween ei and ej, which can be obtained as follows,\nrf\ni,j =\nexp(sf\ni,j)\nPn\nt=1 exp(sf\ni,t)\n, re\ni,j = exp(se\ni,j)Pn\nt=1 exp(se\ni,t), (1)\nwhere sf\ni,j =\nfi·fT\nj√h×w×c, se\ni,j =\nei·eT\nj√h×w×c. Essentially, the\nvalue of ri,j represents the correlation extent between two\nfeatures. The correlation reaches maximum when ri,j = 1,\nrepresenting i-th feature is completely related to thej-th fea-\nture, and vice versa.\nBased on above theory regarding attention, the following\ntheoretical statements hold:\n• if re\ni,j → 0, then re\ni,j<rf\ni,j, i.e., the noise increases the\nattention between unrelated contents;\n• if re\ni,j → 1, then re\ni,j>rf\ni,j, i.e., the noise decreases the\nattention between related contents.\nProof: According to the definition of the rf\ni,j, we have:\nrf\ni,j =\nexp(sf\ni,j)\nPn\nt=1 exp(sf\ni,t)\n= exp(fi · fT\nj )\nPn\nt=1 exp(fi · fT\nt )\n. (2)\nSimple algebra computations enable us to have,\nre\ni,j\nrf\ni,j\n= re\ni,j\n Pn\nt=1,̸=j exp(fi · fT\nt )\nexp(fi · fT\nj )\n+ 1\n!\n. (3)\nBesides, as exp(x) is a monotonically increasing function\nand its value ranges from 0 to 1, we have,\n1 ≤ exp(fi · fT\nt ) ≤ e,\n⇒ n − 1 ≤\nnX\nt=1,̸=j\nexp(fi · fT\nt ) ≤ (n − 1)e,\n⇒ n − 1\ne ≤\nPn\nt=1,̸=j exp(fi · fT\nt )\nexp(fi · fT\nj )\n≤ (n − 1)e.\n(4)\nSince n is a finite real number, n−1\ne and (n −1)e are both\nfinite real numbers. Considering the convenience of the ex-\npression, we denote Pn\nt=1,̸=j exp(fi · fT\nt )\n\u000e\nexp(fi · fT\nj ) re-\nvealed in Eq.(3) and (4) asFijt. For each statement of above\ntheorem, we can prove it as follows,\n1) if re\ni,j → 0, we have,\nre\ni,j (Fijt + 1) → 0, ⇒ re\ni,j\nrf\ni,j\n→ 0<1, ⇒ re\ni,j<rf\ni,j, (5)\n2) if re\ni,j → 1, we have,\nre\ni,j (Fijt + 1)>1, ⇒ re\ni,j\nrf\ni,j\n>1, ⇒ re\ni,j>rf\ni,j. (6)\nMethodology\nFormulation and Overview\nLet X = {x1, x2, ··· , xT } be a corrupted video sequence\nconsisting of T frames with height H and width W. The\ncorresponding frame-wise masks are denoted as M =\n{m1, m2, ··· , mT }. For each mask mi, “0” indicates that\ncorresponding pixel is valid, and “1” denotes that the pixel\nis missing or corrupted. The goal of video inpainting is to\ngenerate an inpainted video sequencebY = {by1,by2, ··· ,byT },\nwhich are spatially and temporally consistent with the orig-\ninal video sequence Y = {y1, y2, ··· , yT }.\nBased on the fact that the contents of missing regions\nin one frame may exist in neighboring frames, existing\ntransformer-based methods (Cai et al. 2022; Liu et al. 2021;\nRen et al. 2022; Zhang, Fu, and Liu 2022; Yu, Fan, and\nZhang 2023; Zhang et al. 2023) usually formulate the video\ninpainting task as a “multi-to-multi” conditional distribution\nprediction problem as follows,\np(bY|X) =\nTY\nt=1\np(bY\nt+n\nt−n|Xt+n\nt−n, Mt+n\nt−n), (7)\nwhere Xt+n\nt−n = {xt−n, ··· , xt, ··· , xt+n} stands for a short\nclip of neighboring frames with a center moment t and a\ntemporal radius n, Mt+n\nt−n denotes the mask clip regarding\nXt+n\nt−n. In practice, these transformer-based methods usually\ngenerate the missing contents by aggregating coherent con-\ntents, which are searched by patch-based attention module\nfrom all the frames along both spatial and temporal dimen-\nsions. Therefore, the attention retrieval accuracy is an criti-\ncal factor affecting the final inpainting performance.\nInevitably, digital images and videos are polluted by noise\nduring the transmission and recording process (Geng et al.\n2022), resulting in the learned embeddings always con-\ntain noise. In Sect., we have also theoretically confirmed\nthat noise can also have an adverse effect on transformer-\nbased video inpainting. For this purpose, we propose a\nnovel wavelet transformer network with noise robustness\nto mitigate this adverse effect. As shown in Fig.1, the pro-\nposed WaveFormer mainly consists of three parts: a frame-\nlevel encoder, wavelet spatial-temporal transformer and a\nframe-level decoder. Specifically, the frame-level encoder is\nbuilt by stacking multiple convolutional layers and residual\nblocks with ReLUs as activation functions, aiming to extract\ndeep features from low-level pixels of each frame. Similarly,\nthe frame-level decoder is designed to decode inpainted fea-\ntures into frames.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6182\nSoftmaxQ DWT\nDWT\nDWT\nIDWT\nV\nK\nrr\nWavelet Spatial-Temporal Transformer\ne\ne\ne\ne\nEncoder\n. . .. . .\n         \nPatch size 1 (                            )\n         Patch size 1 (                            )\ne extract patches\nbatch dot\npiece patches\nconvolution\ne extract patches\nbatch dot\npiece patches\nconvolution\n           \nPatch size n (                                 )\n           Patch size n (                                 )\n. . .. . .\nDecoder\nConcat + Conv\np\np\np\nShare\nFigure 1: Illustration of the proposed WaveFormer, consisting of 1) a frame-level encoder, 2) the wavelet spatial-temporal\ntransformer and 3) a frame-level decoder. Instead of using queries (Q) and keys (K ) to directly calculate attention in existing\ntransformer-based methods, our WaveFormer employs Discrete Wavelet Transform (DWT) to separate the embedding into high-\nfrequency and low-frequency components. These separated low-frequency components are relatively clean, which are used to\ncalculate attention for video inpainting. In this way, the impact of noise on the attention weight is greatly mitigated.\nWavelet Spatial-Temporal Transformer\nAs the core component of our WaveFormer, wavelet spatial-\ntemporal transformer is designed to search coherent con-\ntents from all the input frames with the aim of learning\nspatial-temporal transformations for all missing regions in\nthe wavelet domain of the deep encoding space. Specifi-\ncally, in the process of attention calculation, we introduce\nDWT to separate noise into high-frequency components,\nand then use low-frequency components to calculate atten-\ntion. Finally, the calculated attention is shared with high-\nfrequency components to generate the missing content with\ndifferent frequencies. In this way, the impact of noise on at-\ntention computation can be greatly mitigated. Essentially,\nour wavelet spatial-temporal transformer also follows the\ngeneral pipeline of transformer design, namely embedding,\nmatching, and aggregating. We will introduce more details\nof each step one by one as below.\nEmbedding: Embedding aims to map deep features into\nkey and memory, so as to establish deep correspondences for\neach region in different semantic spaces (Ren et al. 2022).\nLet F = {f1, f2, ··· , fT } denote the deep features encoded\nby the frame-level encoder, where fi ∈ Rh×w×c. The three\nbasic elements of the attention mechanism are extracted\nby the 1 × 1 convolution, including Q(query), K(key), and\nV(value):\nQi, (Ki, Vi) = Mq(fi), (Mk(fi), Mv(fi)), (8)\nwhere 1 ≤ i ≤ T. Mq(·), Mk(·) and Mv(·) denote the\n1 × 1 2D convolution.\nMatching: Having obtained these three basic elements,\nthe coherent contents are searched by calculating the similar-\nity between patches. Specifically, we first decomposeQi, Ki\nand Vi into corresponding low-frequency components and\nhigh-frequency components by DWT, individually,\nQL\ni , QH\ni ; KL\ni , KH\ni ; VL\ni , VH\ni = DWT (Qi, Ki, Vi), (9)\nwhere QL\ni , KL\ni , VL\ni ∈ R\nh\n2 ×w\n2 ×c denote the low-frequency\ncomponents corresponding to Qi, Ki and Vi, mainly record-\ning principal information including the basic structures.\nSimilarly, QH\ni , KH\ni , VH\ni ∈ R3×h\n2 ×w\n2 ×c denote the high-\nfrequency components in horizontal, vertical and diagonal\ndirections, containing a very large proportion of data noise.\nAfter obtaining the low-frequency components QL\ni and\nKL\ni , we extract spatial patches of shape p1 × p2 × c from\nQL\ni and KL\ni of each frame, denoted as qL\ni and kL\ni . Then, the\npatch-wise similarities can be calculated by matrix multipli-\ncation, denoted as\nsi,j = qL\ni · (kL\nj )T\n√p1 × p2 × c, (10)\nwhere 1 ≤ i, j≤ N and N = T × h\np1\n× w\np2\n. A softmax\nfunction is introduced to obtain the attention weights of all\npatches,\nri,j =\n\n\n\n\nexp(si,j)/\nNP\nt=1\nexp(si,t), qL\ni ∈ Ω,\n0, qL\ni ∈ ¯Ω,\n(11)\nwhere Ω and\nΩ denote visible regions and missing regions,\nrespectively. Naturally, we only borrow features from visible\nregions to fill missing regions.\nAggregating: After modeling the deep correspondences\nof all spatial patches, we share the calculated attention on the\nlow-frequency components with the high-frequency compo-\nnents. The output of the query for the low-frequency and\nhigh-frequency components of each patch can be obtained\nby the attention-weighted summation of the values of related\npatches, separately,\nbvL\ni =\nNX\nj=1\nri,jvL\nj , bvH\ni =\nNX\nj=1\nri,jvH\nj , (12)\nwhere vL\nj and vH\nj denote the value of the low-frequency and\nhigh-frequency components of the j-th patch, respectively.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6183\nMethods YouTube-VOS (Xu et al. 2018) DA VIS (Perazzi et al. 2016)\nPSNR↑ SSIM↑ Ewarp ↓ LPIPS↓ PSNR↑ SSIM↑ Ewarp ↓ LPIPS↓\nTCCDS (Huang et al. 2016) 23.418 0.8119 0.3388 1.9372 28.146 0.8826 0.2409 1.0079\nVINet (Kim et al. 2020) 26.174 0.8502 0.1694 1.0706 29.149 0.8965 0.1846 0.7262\nDFVI (Xu et al. 2019) 28.672 0.8706 0.1479 0.6285 30.448 0.8961 0.1640 0.6857\nFGVC (Gao et al. 2020) 24.244 0.8114 0.2484 1.5884 28.936 0.8852 0.2122 0.9598\nCPVINet (Lee et al. 2019) 28.534 0.8798 0.1613 0.8126 30.234 0.8997 0.1892 0.6560\nOPN (Seoung et al. 2019) 30.959 0.9142 0.1447 0.4145 32.281 0.9302 0.1661 0.3876\nSTTN (Zeng, Fu, and Chao 2020) 28.993 0.8761 0.1523 0.6965 28.891 0.8719 0.1844 0.8683\nFuseFormer (Liu et al. 2021) 29.765 0.8876 0.1463 0.5481 29.627 0.8852 0.1767 0.6706\nE2FGVI (Li et al. 2022) 30.064 0.9004 0.1490 0.5321 31.941 0.9188 0.4579 0.6344\nFGT (Zhang, Fu, and Liu 2022) 30.811 0.9258 0.1308 0.4565 32.742 0.9272 0.1669 0.4240\nWaveFormer 33.264 0.9435 0.1184 0.2933 34.169 0.9475 0.1504 0.3137\nTable 1: Quantitative results of video inpainting on YouTube-VOS (Xu et al. 2018) and DA VIS (Perazzi et al. 2016) datasets.\nWe piece all patches together to acquirebV\nL\ni ∈ R\nh\n2 ×w\n2 ×c and\nbV\nH\ni ∈ R3×h\n2 ×w\n2 ×c, and then generate the completed feature\nbfi by IDWT:\nbfi = IDWT (bV\nL\ni , bV\nH\ni ). (13)\nNote that the proposed wavelet spatial-temporal trans-\nformer adopts a multi-head design, where different heads\nare employed to calculate the attention weights of patches\nwith various sizes. In this way, the patches with large size\ncan apply global features to complete semantic background,\nwhile the patches with small size can utilize local features\nto generate detailed texture, thereby achieving high-quality\nvideo inpainting. Furthermore, to fully exploit the power of\nthe proposed transformer, our WaveFormer stacks multiple\nlayers of the wavelet spatial-temporal transformer. Such a\ndesign can use the updated region features in a single feed-\nforward process to improve the results of attention to miss-\ning regions. The final inpainted frame byi can be obtained by\ndecoding bfi with the frame-level decoder.\nLoss Function\nThe total loss of our WaveFormer consists of three terms,\ni.e., the reconstruction term of the hole regionsLhole (Zeng,\nFu, and Chao 2020), the reconstruction term of the valid\nregions Lval (Zeng, Fu, and Chao 2020) and the ad-\nversarial term Ladv by using Temporal PatchGAN (T-\nPatchGAN) (Chang et al. 2019) as a discriminator:\nL = λholeLhole + λvalLval + λadvLadv, (14)\nwhere λhole, λval and λadv are the trade-off parameters. In\nreal implementation, we empirically set these three parame-\nters as 3, 5 and 0.01.\nExperiments\nExperimental Setting\nDatasets and Evaluation Metrics. Two most commonly-\nused datasets are taken to verify the effectiveness of the\nproposed method, including Youtube-vos dataset (Xu et al.\n2018) and DA VIS dataset (Perazzi et al. 2016). The for-\nmer contains 3,471, 474 and 508 video clips in training,\nvalidation and test set, respectively. The latter is composed\nof 60 video clips for training and 90 video clips for test-\ning. Following previous works, we report quantitative re-\nsults by four metrics, including PSNR (Haotian et al. 2019),\nSSIM (Zhang et al. 2022), LPIPS (Zhang et al. 2018) and\nflow warping error Ewarp (Lai et al. 2018).\nMask Settings. In the real world, the applications of video\ninpainting mainly include undesired object removal, scratch\nrestoration, watermark removal, etc. To simulate these ap-\nplications, we evaluate the model with the following three\ntypes of masks:\n◦ Object mask: it is used to simulate applications like un-\ndesired object removal. Following FuseFormer (Liu et al.\n2021), we employ the foreground object annotations in\nDA VIS dataset as the testing object masks, which have\ncontinuous motion and realistic appearance.\n◦ Curve mask: it is composed of curves with continuous\nmotion, which is exploited to simulate applications like\nscratch restoration. In our experiment, these curve masks\nare sampled from FVI dataset (Chang et al. 2019).\n◦ Stationary mask: it has an arbitrary shapes but a relatively\nfixed position. The stationary mask is used to simulate\napplications such as watermark removal, and its genera-\ntion process follows previous work (Chang et al. 2019;\nZeng, Fu, and Chao 2020).\nExperimental Results and Analysis\nQuantitative Results. Quantitative results of video inpaint-\ning are reported on both YouTube-VOS and DA VIS. We se-\nlect the most recent and the most competitive approaches as\nthe baselines, including TCCDS, VINet, CPVINet, DFVI,\nFGVC, OPN, STTN, FuseFormer, E2FGVI and FGT. To en-\nsure the comparability of experimental results, these base-\nlines are fine-tuned several times based on their released\ncodes, and report their best results in this section.\nAs shown in Tab.1, the PSNR, SSIM,Ewarp and LPIPS of\nour model substantially surpass all previous state-of-the-art\nmethods on YouTube-VOS and DA VIS. The superior results\ndemonstrate that our WaveFormer can generate the videos\nwith less distortion (PSNR and SSIM), more visually plau-\nsible content (LPIPS) and better spatial and temporal co-\nherence (Ewarp). Such a commendable performance verifies\nthe superiority of the proposed method.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6184\n(a) Curve mask (b) Stationary mask (c) Object mask\nE2FGVISTTNFuseFormerWaveFormer FGT\n20\n20\nInput\nFigure 2: Qualitative results compared with E2FGVI (Li et al. 2022), STTN (Zeng, Fu, and Chao 2020), FuseFormer (Liu et al.\n2021) ,and FGT (Zhang, Fu, and Liu 2022). Better viewed at zoom level 400%.\nSTTN FuseFormer WaveFormerInput frame\nbmx-bumpsrollerblade\nFigure 3: Comparison of the feature maps before feeding\ninto transformer blocks between STTN (Zeng, Fu, and Chao\n2020), FuseFormer (Liu et al. 2021) and our WaveFormer.\nQualitative Results. To visually inspect the visual results,\nwe choose four competitive methods, including E2FGVI,\nSTTN, FuseFormer and FGT, to conduct visual compar-\nisons. Respectively, Fig.2 (a), Fig.2 (b) and Fig.2 (c) illus-\ntrates the scratch restoration case of curve masks, the wa-\ntermark removal case of stationary masks and the object\nremoval case of object masks. It can be observed that our\nWaveFormer generates the missing contents with more accu-\nrate structures and details than baselines in these three cases.\nFurthermore, we also visualize the feature maps of\nSTTN, FuseFormer and WaveFormer before extracting spa-\ntial patches for attention computation. As shown in the sec-\nond example (rollerblade) of Fig. 3, the texture structure of\ntext, windows and walls in the feature map generated by\nSTTN is completely destroyed. Although the texture struc-\n0%\n10%\n20%\n30%\n40%\n50%\nrank 1 rank 2 rank 3\nSTTN FuseFormer E2FGVI FGT Ours\nFigure 4: User study. “rank x” means the percentage of re-\nsults from each model being chosen as the x-th best.\nture of text in the feature map produced by FuseFormer is re-\ntained, the texture structure of windows and walls has been\ntotally broken by strong noise. Compared with these two\nmost competitive approaches, our WaveFormer produces the\nfeature map with a cleaner background and a more complete\ntexture structure. It is easy to figure out the text, window\nand wall in our feature map. Such a distinct background tex-\nture leads to more accurate attention retrieval in the trans-\nformer block, thus naturally producing better visual quality.\nThe above observations illustrate that noise accumulation\ndestroys the texture structure used for attention retrieval, and\nour WaveFormer relieves this drawback to some extent. We\nbelieve that this is the reason why our WaveFormer has bet-\nter inpainting performance.\nUser Study. In order to further make a comprehensive com-\nparison, we conduct a user study of the inpainting results\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6185\nSTTN FuseFormer WaveFormerInput frame STTN FuseFormer WaveFormerInput frame\n(a) Parkour (b) Train\nFigure 5: Visual comparison of the feature maps sourced from clean and noisy video frame, where the first, second and third\nrows are the clean frames, frame with Gaussian noise and frame with Salt & pepper noise, respectively. Best viewed in zoom.\nof the five competitive approaches, including STTN, Fuse-\nFormer, E2FGVI, FGT and WaveFormer. We invited 20 vol-\nunteers to perform a questionnaire survey for 10 videos from\nthe DA VIS dataset. In each inquiry, we asked volunteers to\nchoose the video for which they think the inpainting re-\nsults are best. To ensure the reliability of subjective evalu-\nations, the inpainting results obtained by the five methods\nwere scrambled at each interrogation, and each video can be\nplayed multiple times. The results of the user study are con-\ncluded in Fig. 4. As we can see, volunteers obviously favor\nour results compared with other competitors.\nAblation Study\nNoise-robustness. Fig. 5 shows the feature maps with noisy\nframes as inputs in two representative example, where the\nfirst row reveals the feature map produced when using the\nclean frame from DA VIS dataset as inputs, and the next\ntwo rows display the feature map generated when using\nthe frame added with Gaussian and Salt & pepper noise\nas inputs. As shown in Fig. 5, we can find that it is dif-\nficult for STTN and FuseFormer to suppress noise, while\nWaveFormer could suppress the noise and maintain the\nbackground structure during its inference. For example, in\nFig. 5(a), the building structure in the two feature maps gen-\nerated by STTN and WaveFormer is complete, when the\nclean parkour frame is fed. After the frame is superposed\nwith Gaussian or salt & pepper noise, the feature map of\nSTTN contains very strong noise, and the building structure\nvanishes, while the basic structure could still be observed\nfrom our WaveFormer. Similarly, in Fig. 5(b), the feature\nmap of FuseFormer also contains strong noise, and the rail-\nway structure disappears, while WaveFormer can still ob-\nserve the railway structure. such results indicate that Wave-\nFormer is robust to different noises.\nThe Impact of Noise. To further verify the impact of noise\non attention calculation, we use DWT to separate the noise\nfrom the embedding used for attention calculation in the\nSTTN and FuseFormer, and compare them with its orig-\ninal versions. Here, the improved STTN and FuseFormer\nare labeled STTN\nWave and FuseFormer Wave. As shown\nin Tab.2, STTN Wave and FuseFormer Wave are obviously\nsuperior to original STTN and FuseFormer in all evaluation\nmetrics. These results demonstrate the effectiveness and ne-\ncessity of noise removal in attention calculation.\nMethods PSNR↑ SSIM↑ Ewarp ↓ LPIPS↓\nSTTN 28.993 0.8761 0.1523 0.6965\nSTTN Wave 30.012 0.8917 0.1509 0.6631\nFuseFormer 29.765 0.8876 0.1463 0.5481\nFuseFormer Wave 31.171 0.8995 0.1429 0.5236\nw/o DWT 31.326 0.9259 0.1299 0.3471\nFull model 33.264 0.9435 0.1184 0.2933\nTable 2: Impact of noise on attention computation.\nMethods STTN FuseFormer E2FGVI FGT WaveFormer\nFLOPs 477.91G 579.82G 442.18G 455.91G 349.71G\nTime 0.22s 0.30s 0.26s 0.39s 0.18s\nTable 3: Efficiency analysis.\nEfficiency analysis. In addition, we compare the efficiency\nof WaveFormer with STTN, FuseFormer, E2FGVI and FGT\nby using FLOPs and inference time. Since the FLOPs in\nvideo inpainting are related to the simultaneous processing\nof the temporal size (number of frames), we set the tempo-\nral size to 20 following to previous works (Liu et al. 2021;\nZeng, Fu, and Chao 2020; Zhang, Fu, and Liu 2022). And\nthe runtime is measured on a single Titan RTX GPU. The\ncompared results are shown in Tab. 3. The inference speed\nof the proposed method is the fastest, improving 0.04s over\nthe optimal baseline—STTN. Besides, WaveFormer holds\nthe lowest FLOPs in contrast to all other methods.\nConclusion\nIn this work, we theoretically proved that noise reduces\nthe attention to relevant contents and increases the atten-\ntion to irrelevant contents when generating the missing re-\ngions. Based on this fact, we propose a novel transformer\nnetwork by introducing the DWT, named WaveFormer. Our\nWaveFormer uses DWT to separate the noise existing in the\nembedding into high-frequency components, and employs\nrelatively clean low-frequency components to calculate at-\ntention weight, thereby mitigating the impact of noise on the\ncalculation of attention weight to the greatest extent. Experi-\nments demonstrate the superior performance of the proposed\nWaveFormer both quantitatively and qualitatively.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6186\nReferences\nBar, A.; Gandelsman, Y .; Darrell, T.; Globerson, A.; and\nEfros, A. 2022. Visual prompting via image inpainting. In\nAdvances in Neural Information Processing Systems (NIPS),\nvolume 35, 25005–25017.\nCai, J.; Li, C.; Tao, X.; Yuan, C.; and Tai, Y .-W. 2022. De-\nViT: Deformed Vision Transformers in Video Inpainting. In\nProceedings of the 30th ACM International Conference on\nMultimedia (ACMMM), 779–789.\nChang, Y .-L.; Liu, Z. Y .; Lee, K.-Y .; and Hsu, W. 2019. Free-\nform Video Inpainting with 3D Gated Convolution and Tem-\nporal PatchGAN. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 9066–9075.\nCheng, S.; Wang, Y .; Huang, H.; Liu, D.; Fan, H.; and Liu,\nS. 2021. NBNet: Noise Basis Learning for Image Denoising\nWith Subspace Projection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 4896–4906.\nGao, C.; Saraf, A.; Huang, J.-B.; and Kopf, J. 2020. Flow-\nedge Guided Video Completion. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV), 713–729.\nGeng, M.; Meng, X.; Zhu, L.; Jiang, Z.; Gao, M.; Huang,\nZ.; Qiu, B.; Hu, Y .; Zhang, Y .; Ren, Q.; and Lu, Y . 2022.\nTriplet Cross-Fusion Learning for Unpaired Image Denois-\ning in Optical Coherence Tomography. IEEE Transactions\non Medical Imaging (TMI), 41(11): 3357–3372.\nGu, B.; Yu, Y .; Fan, H.; and Zhang, L. 2023. Flow-\nGuided Diffusion for Video Inpainting. arXiv preprint\narXiv:2311.15368.\nHaotian, Z.; Long, M.; Hailin, W., JinZha ando wen; and\nCollomosse, N. X. J. 2019. An Internal Learning Approach\nto Video Inpainting. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 2720–\n2729.\nHuang, J.; Kang, S. B.; Ahuja, N.; and Kopf, J. 2016. Tem-\nporally coherent completion of dynamic video. ACM Trans-\nactions on Grapics (TOG), 35(6): 196.1–196.11.\nJi, Z.; Hou, J.; Su, Y .; Pang, Y .; and Li, X. 2022. G2LP-\nNet: Global to Local Progressive Video Inpainting Network.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology (TCSVT), 33(3): 1082–1092.\nJia, F.; Wong, W. H.; and Zeng, T. 2021. DDUNet: Dense\nDense U-Net With Applications in Image Denoising. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV) Workshops, 354–364.\nKang, J.; Oh, S. W.; and Kim, S. J. 2022. Error compensa-\ntion framework for flow-guided video inpainting. In Euro-\npean Conference on Computer Vision, 375–390.\nKe, L.; Tai, Y .-W.; and Tang, C.-K. 2021. Occlusion-\nAware Video Object Inpainting. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 14468–14478.\nKim, D.; Woo, S.; Lee, J.-Y .; and Kweon, I. S. 2019. Deep\nBlind Video Decaptioning by Temporal Aggregation and\nRecurrence. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 4263–\n4272.\nKim, D.; Woo, S.; Lee, J.-Y .; and Kweon, I. S. 2020. Re-\ncurrent Temporal Aggregation Framework for Deep Video\nInpainting. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence (TPAMI), 42(5): 1038–1052.\nLai, W.-S.; Huang, J.-B.; Wang, O.; Shechtman, E.; Yumer,\nE.; and Yang, M.-H. 2018. Learning blind video temporal\nconsistency. In Proceedings of the European conference on\ncomputer vision (ECCV), 179–195.\nLee, S.; Oh, S. W.; Won, D.; and Kim, S. J. 2019. Copy-and-\npaste networks for deep video inpainting. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 4413–4421.\nLi, A.; Zhao, S.; Ma, X.; Gong, M.; Qi, J.; Zhang, R.; Tao,\nD.; and Kotagiri, R. 2020. Short-Term and Long-Term Con-\ntext Aggregation Network for Video Inpainting. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 728–743.\nLi, Z.; Lu, C.-Z.; Qin, J.; Guo, C.-L.; and Cheng, M.-M.\n2022. Towards an End-to-End Framework for Flow-Guided\nVideo Inpainting. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n17562–17571.\nLiao, M.; Lu, F.; Zhou, D.; Zhang, S.; Li, W.; and Yang, R.\n2020. Dvi: Depth guided video inpainting for autonomous\ndriving. In Proceedings of the European Conference on\nComputer Vision (ECCV), 1–17.\nLiu, P.; Zhang, H.; Zhang, K.; Lin, L.; and Zuo, W. 2018.\nMulti-level wavelet-CNN for image restoration. InProceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) workshops, 773–782.\nLiu, R.; Deng, H.; Huang, Y .; Shi, X.; Lu, L.; Sun, W.; Wang,\nX.; Dai, J.; and Li, H. 2021. FuseFormer: Fusing Fine-\nGrained Information in Transformers for Video Inpainting.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 14040–14049.\nLiu, R.; Li, B.; and Zhu, Y . 2022. Temporal Group Fusion\nNetwork for Deep Video Inpainting. IEEE Transactions on\nCircuits and Systems for Video Technology (TCSVT), 32(6):\n3539–3551.\nLiu, R.; Weng, Z.; Zhu, Y .; and Li, B. 2020. Temporal Adap-\ntive Alignment Network for Deep Video Inpainting. InInter-\nnational Joint Conference on Artificial Intelligence (IJCAI),\n927–933.\nMallat, S. G. 1989. A theory for multiresolution signal de-\ncomposition: the wavelet representation. IEEE Transactions\non Pattern Analysis and Machine Intelligence (TPAMI) ,\n11(4): 674–693.\nPang, T.; Zheng, H.; Quan, Y .; and Ji, H. 2021. Recorrupted-\nto-Recorrupted: Unsupervised Deep Learning for Image De-\nnoising. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2043–\n2052.\nPerazzi, F.; Pont-Tuset, J.; Mcwilliams, B.; Gool, L. V .; and\nSorkine-Hornung, A. 2016. A Benchmark Dataset and Eval-\nuation Methodology for Video Object Segmentation. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 724–732.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6187\nRen, J.; Zheng, Q.; Zhao, Y .; Xu, X.; and Li, C. 2022. DL-\nFormer: Discrete Latent Transformer for Video Inpainting.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 3511–3520.\nSeoung, O., Wug; Sungho, L.; Joon-Young, L.; and Seon,\nK., Joo. 2019. Onion-Peel Networks for Deep Video Com-\npletion. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 4402–4411.\nShang, Y .; Xu, D.; Zong, Z.; Nie, L.; and Yan, Y . 2022. Net-\nwork binarization via contrastive learning. InProceedings of\nthe European conference on computer vision (ECCV), 586–\n602.\nShang, Y .; Yuan, Z.; Xie, B.; Wu, B.; and Yan, Y . 2023. Post-\ntraining quantization on diffusion models. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 1972–1981.\nShukla, T.; Maheshwari, P.; Singh, R.; Shukla, A.; Kulka-\nrni, K.; and Turaga, P. 2023. Scene Graph Driven Text-\nPrompt Generation for Image Inpainting. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 759–768.\nSomani, A.; Banerjee, P.; Rastogi, M.; Agarwal, K.; Prasad,\nD. K.; and Habib, A. 2023. Image Inpainting With Hy-\npergraphs for Resolution Improvement in Scanning Acous-\ntic Microscopy. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n3112–3121.\nSrinivasan, V . S. R.; Ma, R.; Tang, Q.; Yi, Z.; and Xu, Z.\n2021. Spatial-Temporal Residual Aggregation for High Res-\nolution Video Inpainting. arXiv preprint arXiv:2111.03574.\nWang, C.; Huang, H.; Han, X.; and Wang, J. 2019. Video\ninpainting by jointly learning temporal structure and spatial\ndetails. In Proceedings of the AAAI Conference on Artificial\nIntellignce (AAAI), 5232–5239.\nWu, Z.; Sun, C.; Xuan, H.; and Yan, Y . 2023a. Deep Stereo\nVideo Inpainting. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n5693–5702.\nWu, Z.; Sun, C.; Xuan, H.; Zhang, K.; and Yan, Y . 2023b.\nDivide-and-Conquer Completion Network for Video In-\npainting. IEEE Transactions on Circuits and Systems for\nVideo Technology (TCSVT), 33(6): 2753–2766.\nWu, Z.; Zhang, K.; Sun, C.; Xuan, H.; and Yan, Y . 2023c.\nFlow-Guided Deformable Alignment Network with Self-\nSupervision for Video Inpainting. In Proceedings of the\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 1–5.\nWu, Z.; Zhang, K.; Xuan, H.; Yang, J.; and Yan, Y . 2021.\nDAPC-Net: Deformable Alignment and Pyramid Context\nCompletion Networks for Video Inpainting. IEEE Signal\nProcessing Letters (SPL), 28: 1145–1149.\nXu, N.; Yang, L.; Fan, Y .; Yang, J.; Yue, D.; Liang, Y .;\nPrice, B.; Cohen, S.; and Huang, T. 2018. YouTube-VOS:\nSequence-to-Sequence Video Object Segmentation. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 603–619.\nXu, R.; Li, X.; Zhou, B.; and Loy, C. C. 2019. Deep Flow-\nGuided Video Inpainting. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 3723–3732.\nYao, T.; Pan, Y .; Li, Y .; Ngo, C.-W.; and Mei, T. 2022. Wave-\nvit: Unifying wavelet and transformers for visual represen-\ntation learning. In Proceedings of the European conference\non computer vision (ECCV), 328–345.\nYu, Y .; Fan, H.; and Zhang, L. 2023. Deficiency-Aware\nMasked Transformer for Video Inpainting. arXiv preprint\narXiv:2307.08629.\nYu, Y .; Zhan, F.; Lu, S.; Pan, J.; Ma, F.; Xie, X.; and Miao,\nC. 2021. WaveFill: A Wavelet-based Generation Network\nfor Image Inpainting. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 14094–\n14103.\nZeng, Y .; Fu, J.; and Chao, H. 2020. Learning Joint Spatial-\nTemporal Transformations for Video Inpainting. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 3723–3732.\nZeng, Y .; Fu, J.; Chao, H.; and Guo, B. 2019. Learning\nPyramid-Context Encoder Network for High-Quality Image\nInpainting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 1486–\n1494.\nZhang, K.; Fu, J.; and Liu, D. 2022. Flow-Guided Trans-\nformer for Video Inpainting. InProceedings of the European\nConference on Computer Vision (ECCV), 74–90.\nZhang, K.; Peng, J.; Fu, J.; and Liu, D. 2023. Exploiting Op-\ntical Flow Guidance for Transformer-Based Video Inpaint-\ning. arXiv preprint arXiv:2301.10048.\nZhang, K.; Wu, S.; Wu, Z.; Yuan, X.; and Zhao, C. 2022.\nFractional Optimization Model for Infrared and Visible Im-\nage Fusion. In Proceedings of the British Machine Vision\nConference (BMVC), 1–12.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The unreasonable effectiveness of deep features as\na perceptual metric. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 586–\n595.\nZhang, Y .; Wu, Z.; and Yan, Y . 2023. PFTA-Net: Progres-\nsive Feature Alignment and Temporal Attention Fusion Net-\nworks for Video Inpainting. In Proceedings of the IEEE In-\nternational Conference on Image Processing (ICIP), 191–\n195.\nZou, X.; Yang, L.; Liu, D.; and Lee, Y . J. 2021. Progressive\nTemporal Feature Alignment Network for Video Inpainting.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 16448–16457.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6188"
}