{
  "title": "Inconsistency in the items included in tools used in general health research and physical therapy to evaluate the methodological quality of randomized controlled trials: a descriptive analysis",
  "url": "https://openalex.org/W2060971566",
  "year": 2013,
  "authors": [
    {
      "id": "https://openalex.org/A2121657952",
      "name": "Susan Armijo-Olivo",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A1559334958",
      "name": "Jorge Fuentes",
      "affiliations": [
        "University of Alberta",
        "Catholic University of the Maule"
      ]
    },
    {
      "id": "https://openalex.org/A2170698613",
      "name": "María Ospina",
      "affiliations": [
        "University of Alberta",
        "Institute of Health Economics"
      ]
    },
    {
      "id": "https://openalex.org/A1902280383",
      "name": "Humam Saltaji",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2103111048",
      "name": "Lisa Hartling",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2121657952",
      "name": "Susan Armijo-Olivo",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A1559334958",
      "name": "Jorge Fuentes",
      "affiliations": [
        "University of Alberta",
        "Catholic University of the Maule"
      ]
    },
    {
      "id": "https://openalex.org/A2170698613",
      "name": "María Ospina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1902280383",
      "name": "Humam Saltaji",
      "affiliations": [
        "Alberta Hospital Edmonton",
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2103111048",
      "name": "Lisa Hartling",
      "affiliations": [
        "University of Alberta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1992637165",
    "https://openalex.org/W2033890227",
    "https://openalex.org/W2098923148",
    "https://openalex.org/W2123945981",
    "https://openalex.org/W2031385454",
    "https://openalex.org/W2056537869",
    "https://openalex.org/W2071955543",
    "https://openalex.org/W2162163297",
    "https://openalex.org/W1598602811",
    "https://openalex.org/W1577638355",
    "https://openalex.org/W2134430160",
    "https://openalex.org/W2160795579",
    "https://openalex.org/W2134338262",
    "https://openalex.org/W4214559180",
    "https://openalex.org/W4248217291",
    "https://openalex.org/W2141080329",
    "https://openalex.org/W2155082372",
    "https://openalex.org/W2065074148",
    "https://openalex.org/W2115988342",
    "https://openalex.org/W2088323328",
    "https://openalex.org/W2075917870",
    "https://openalex.org/W2003182613",
    "https://openalex.org/W2122814783",
    "https://openalex.org/W1033827493",
    "https://openalex.org/W2419324624",
    "https://openalex.org/W2123511363",
    "https://openalex.org/W2171190538",
    "https://openalex.org/W2122017467",
    "https://openalex.org/W1985064472",
    "https://openalex.org/W2015709747",
    "https://openalex.org/W1994065729",
    "https://openalex.org/W1986215651",
    "https://openalex.org/W2070220127",
    "https://openalex.org/W1660053632",
    "https://openalex.org/W2171306475",
    "https://openalex.org/W2042355753",
    "https://openalex.org/W1503995466",
    "https://openalex.org/W2008583240",
    "https://openalex.org/W2057212716",
    "https://openalex.org/W4205893994",
    "https://openalex.org/W2417198078",
    "https://openalex.org/W2093030719",
    "https://openalex.org/W1994898034",
    "https://openalex.org/W2005084763",
    "https://openalex.org/W1965462468",
    "https://openalex.org/W2145755098",
    "https://openalex.org/W2114488753",
    "https://openalex.org/W1996154076",
    "https://openalex.org/W2041221488",
    "https://openalex.org/W2136016340",
    "https://openalex.org/W1965476565",
    "https://openalex.org/W2000444704",
    "https://openalex.org/W2085077732",
    "https://openalex.org/W1998684449",
    "https://openalex.org/W1994466426",
    "https://openalex.org/W2051556928",
    "https://openalex.org/W2116996240",
    "https://openalex.org/W2171251365",
    "https://openalex.org/W3025648878",
    "https://openalex.org/W2064296154",
    "https://openalex.org/W2030560722",
    "https://openalex.org/W2320816109",
    "https://openalex.org/W1554040650",
    "https://openalex.org/W2011932878"
  ],
  "abstract": "There is extensive item variation across tools that evaluate the risk of bias of RCTs in health research. Results call for an in-depth analysis of items that should be used to assess risk of bias of RCTs. Further empirical evidence on the use of individual items and the psychometric properties of risk of bias tools is needed.",
  "full_text": "RESEARCH ARTICLE Open Access\nInconsistency in the items included in tools used in\ngeneral health research and physical therapy to\nevaluate the methodological quality of randomized\ncontrolled trials: a descriptive analysis\nSusan Armijo-Olivo1,2*, Jorge Fuentes2,3, Maria Ospina4, Humam Saltaji5 and Lisa Hartling6\nAbstract\nBackground: Assessing the risk of bias of randomized controlled trials (RCTs) is crucial to understand how biases\naffect treatment effect estimates. A number of tools have been developed to evaluate risk of bias of RCTs; however,\nit is unknown how these tools compare to each other in the items included. The main objective of this study was\nto describe which individual items are included in RCT quality tools used in general health and physical therapy\n(PT) research, and how these items compare to those of the Cochrane Risk of Bias (RoB) tool.\nMethods: We used comprehensive literature searches and a systematic approach to identify tools that evaluated\nthe methodological quality or risk of bias of RCTs in general health and PT research. We extracted individual items\nfrom all quality tools. We calculated the frequency of quality items used across tools and compared them to those\nin the RoB tool. Comparisons were made between general health and PT quality tools using Chi-squared tests.\nResults: In addition to the RoB tool, 26 quality tools were identified, with 19 being used in general health and\nseven in PT research. The total number of quality items included in general health research tools was 130,\ncompared with 48 items across PT tools and seven items in the RoB tool. The most frequently included items in\ngeneral health research tools (14/19, 74%) were inclusion and exclusion criteria, and appropriate statistical analysis.\nIn contrast, the most frequent items included in PT tools (86%, 6/7) were: baseline comparability, blinding of\ninvestigator/assessor, and use of intention-to-treat analysis. Key items of the RoB tool (sequence generation and\nallocation concealment) were included in 71% (5/7) of PT tools, and 63% (12/19) and 37% (7/19) of general health\nresearch tools, respectively.\nConclusions: There is extensive item variation across tools that evaluate the risk of bias of RCTs in health research.\nResults call for an in-depth analysis of items that should be used to assess risk of bias of RCTs. Further empirical\nevidence on the use of individual items and the psychometric properties of risk of bias tools is needed.\nKeywords: Bias, Methodological quality, Quality assessment, Critical appraisal, Risk of bias, Quality of reporting\n* Correspondence:susanarmijo@gmail.com\n1Postdoctoral Fellow, CLEAR (Connecting Leadership and Research)\nOutcomes Research Program, University of Alberta, 5-115A Edmonton Clinic\nHealth Academy (ECHA), 11405– 87 Avenue, Edmonton, Alberta T6G 1C9\nCanada\n2Faculty of Rehabilitation Medicine, Department of Physical Therapy,\nUniversity of Alberta, 3-48 Corbett Hall, Edmonton T6G 2G4, Canada\nFull list of author information is available at the end of the article\n© 2013 Armijo-Olivo et al.; licensee BioMed Central Ltd. This is an Open Access article distributed under the terms of the\nCreative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use,\ndistribution, and reproduction in any medium, provided the original work is properly cited.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116\nhttp://www.biomedcentral.com/1471-2288/13/116\nBackground\nRandomized controlled trials (RCTs), and systematic re-\nviews (SRs) and meta-analyses of these trials, are consid-\nered the gold standard to evaluate the effectiveness of\nhealth care interventions. Results of these studies are\ncrucial for informing the implementation of the best\ntreatments to improve patient outcomes and the effi-\nciency of the health care system. Evaluating the meth-\nodological quality of trials is an essential component of\nSRs as only the best available evidence should inform\nclinical and policy decisions. An accurate assessment\nof study quality is key for the synthesis and interpret-\nation of results across studies to effectively guide health\ncare [1].\nThe term “methodological quality” has evolved since\nits inception and involves the evaluation of the internal\nvalidity as well as the external validity of a given study\n[2,3]. Recently, The Cochrane Collaboration has lead a\nshift in the approach to quality assessment, in which the\nconcept of trial quality is linked to the internal validity\nof the study, namely risk of bias [4]. However, there is\nstill inconsistency among researchers on how study qual-\nity is defined, and several terms have been used inter-\nchangeably in the literature (i.e. quality assessment,\nmethodological quality, risk of bias, critical appraisal,\ntrial quality).\nWhile the impact of trial bias on evidence synthesis\nhas been largely recognized, the approaches to quality\nassessment have been inconsistent and controversial [5].\nA wide variety of tools have been developed to evaluate\nRCT quality in different health areas.[5,6]; many of them\nhave not been developed using scientifically rigorous\nmethods nor have they been validated [5]. In addition,\nthere is no agreement on the optimal tool to accurately\nassess trial quality. The use of different tools for evaluat-\ning the quality of primary research in SRs can lead to\ndiscrepancies and skewed interpretations of SR results\n[7-9] and ultimately impact recommendations for clin-\nical care.\nIn 2008, The Cochrane Collaboration [10] intro-\nduced the Risk of Bias (RoB) tool as a way to address\nshortcomings associated with existing tools and\nmethods for quality assessment in SRs. Individual\nRoB items were selected based on a growing body of\nempirical evidence quantifying the association be-\ntween certain characteristics related to the conduct of\nthe trial and estimates of treatment effects [11-15].\nFor example, there is evidence that inadequate alloca-\ntion concealment or lack of double-blinding are likely\nto overestimate treatment effects by 18% and 9%, re-\nspectively [12,14,15].\nIn order to guide a proper assessment of study quality\nor risk of bias to inform decision-making, it is important\nto identify which items have been included in different\ntools and whether these items truly evaluate the likeli-\nhood of bias, as defined by The Cochrane Collaboration\n[10,16] and other criteria [17]. This would be an import-\nant contribution for evidence synthesis.\nMost of the studies that have evaluated the use of\ntools for quality assessment of RCTs [5,6,18,19] have\nnot exhaustively assessed how these tools compare to\neach other in terms of their individual items and\nwhether their use varies across different areas of\nhealth care research. For example, a recent study [19]\nexamined the characteristics and methods of reviews\nassessing the quality of RCTs. While substantial vari-\nation in the use of quality tools across reviews was\nidentified, the study did not describe in detail which\nitems were most frequently included in the tools. The\npresent study was designed to refine the analysis of\nexisting tools by conduct ing a more comprehensive\nsearch (i.e., no language restrictions, larger number of\ndatabases), describing the psychometric properties of\nthe tools used in general health research, and com-\nparing the items included in these tools with the\nCochrane RoB tool.\nWe conducted a previous systematic review that de-\nscribed which tools have been used to evaluate the\nmethodological quality of RCTs in physical therapy\n(PT) research [5]. RCTs conducted in the area of PT\nhave unique characteristic s compared with pharmaco-\nlogical trials. Because of the nature of PT treatments\n(e.g., manual therapy, exercises), RCTs assessing PT\ninterventions are often complex [20], and diverse as-\npects of their design (e.g., type and intensity of ther-\napy, standardized or individually tailored approaches,\ntherapists’ skills and experience) are likely to affect\nstudy results. It is unknown whether the tools to as-\nsess the quality of RCTs in PT differ from those used\nin general health research in terms of the items and\ntype of bias they aim to address. The present study\nw a sd e s i g n e dt oe x p a n da n du p d a t et h ea n a l y s i so f\nour previous review [5] on quality tools for evidence\nsynthesis.\nThe main objective of the present study was to de-\nscribe the frequency of individual items included in tools\nthat assess RCT quality in general health and PT re-\nsearch, and how they compare to items included in the\nRoB tool [4]. Secondary objectives were to 1) determine\nthe nature of items included in general health and PT\nquality tools (i.e., evaluation of “conduct” versus\n“reporting”); 2) report on the psychometric properties of\nquality tools that have been formally evaluated; 3) deter-\nmine whether individual items in the tools relate to cer-\ntain threats to validity or precision [10,16,17] and 4)\nquantify the number of citations per tool, as a measure\nof usage since each tool’s inception and after inception\nof the RoB tool.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 2 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nMethods\nDesign: observational, descriptive study\nSearch strategy\nAn update of a previous SR [5] on quality assessment\ntools was carried out to identify scales and their items\nused in the assessment of RCT quality in health and PT\nresearch. The updated search strategy incorporated key\nwords identified by Dechartres et al. [19], with searches\nconducted from January 1st, 2007 to June 10, 2013 in\nthe following bibliographic databases: Medline, Embase,\nCinahl, ISI Web of Science, EMB Reviews-Cochrane\nCentral Register of Controlled Trials and Cochrane\nLibrary and Best Evidence, All EBM Reviews -CDSR,\nACP journal Club, DARE, CCTR, Global health, and\nHealthSTAR. Key words used in the search were: tool,\ncritical appraisal, critical appraisal review, appraisal of\nmethodology, appraisal of research methodology, re-\nsearch design review, quality assessment, methodological\nquality tool, RoB (tool), randomized (randomised)\ncontrolled trial, and RCT. Additionally, we manually\nsearched the bibliographies of potentially relevant pa-\npers. The search was not limited by language of publica-\ntion. For a sample search strategy, see Additional file 1.\nCriteria for inclusion of studies in the review\nStudies were included if they described or used a newly\ndeveloped tool to evaluate the methodological quality/\nRoB of RCTs in any area of medical/health research and\ndescribed any of its psychometric properties (i.e. validity,\nreliability, responsiveness). We excluded studies in which\nquality tools were developed for only one specific SR,\nstudies that were not related to the development or psy-\nchometric testing of quality tools, and studies on generic\ntools that evaluated different types of research design\n(e.g., qualitative and quantitative studies). In addition,\nstudies using modifications of existing tools were not\nconsidered for inclusion as they were likely not system-\natically developed. The RoB tool [4,10] was known to be\nnewly developed after our previous SR (2008), and was\nincluded prior to the updated search; however, we\nsearched for manuscripts reporting psychometric prop-\nerties of the RoB tool.\nData screening\nTwo reviewers independently screened abstracts and\ntitles obtained from the database searches. The full text\nof potentially relevant articles was retrieved for further\nassessment. Disagreements were resolved by consensus.\nData extraction\nData extraction was conducted in two phases. First, two\nresearchers independently extracted information on\ncontent, construction, special features (e.g. area of\ndevelopment-clinical area-, number of items, selection of\nitems for inclusion, time to complete, scoring instruc-\ntions), and psychometric properties of the new tools.\nInformation on face, content, construct, and concurrent\nvalidity, internal consistency, and reproducibility (intra\nand inter-rater reliability/agreement) was extracted. For\nthis update, authors of original studies were not\ncontacted to obtain additional information. The defini-\ntions of psychometric properties from Streiner and\nNorman [21-23] were used in the present study. Guide-\nlines developed by Terwee et al. [24] were used to define\nquality of measurement properties. Briefly, quality of\nmeasurement included internal (internal consistency,\nrelevance of items and representativeness of items of the\nscale-content validity) as well as external components of\nvalidity (the relationship with other tests in a manner\nthat is consistent with theoretically derived hypotheses-\nconstruct validity). Intra and inter-rater reliability (i.e.\nrepeatability of measurements taken by the same tester\nat different times and repeatability of measurements\ntaken by different testers, respectively) were also consid-\nered. Definitions of psychometric properties for this\nreview are provided in Additional file 2.\nSecond, two researchers independently extracted infor-\nmation on individual items used in the tools and the fre-\nquency of items across tools. Tools were categorized as\nrelevant to PT if the authors specifically stated that the\nscale was developed for PT research, it was developed by\na PT group, or if, according to Scopus searches, the tool\nwas used in at least 5 PT reviews. Otherwise the tool\nwas considered a general health research tool. One of\nthe tools commonly used in both general health and PT\nresearch is the Jadad scale. This tool was included in\nboth categories.\nItems from the quality tools were grouped according\nto nine content categories that have been previously\ndescribed [5]: 1) introduction, objectives, and design;\n2) patient selection (inclusi on and exclusion criteria,\ndescription of study participants); 3) assignment, ran-\ndomization, and allocation concealment; 4) blinding;\n5) interventions; 6) attrition, follow up and protocol\ndeviations; 7) outcomes; 8) statistical analysis; and\n9) miscellaneous.\nClassification of items\nMethodological quality (conduct) and quality of repor-\nting are two concepts that overlap to some degree; how-\never, they relate to different aspects of study quality. We\ndefined methodological quality as “the confidence that\nthe trial design, conduct, and analysis has minimized or\navoided biases in its treatment comparisons” [6] (e.g.,\nallocation concealment was appropriate). We defined\nquality of reporting as authors providing “information\nabout the design, conduct and analysis of the trial,” [6]\n(e.g., method for concealing allocation was reported).\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 3 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTwo researchers independently classified individual\nitems based on whether they evaluated“reporting” and/\nor “conduct” of the trial.\nClassifying quality items is a complex task due to un-\nclear description of items in the tools, lack of general\nagreement in bias definitions [25], and the need for em-\npirical evidence linking these items to bias. Two re-\nsearchers independently classified each item according\nto whether they potentially addressed threats to validity\n(i.e., selection bias, detection bias, performance bias, at-\ntrition bias) or precision (Additional file 3). These cate-\ngorizations have been used in other relevant sentinel\nwork [17,26-28]. Items that dealt with several threats to\nvalidity were classified as addressing multiple biases [29].\nReviewers considered each item by asking“What type of\nthreats to validity or precision are addressed by a given\nitem?” or “What do authors intend to capture with a\ngiven quality assessment item?” Thus, items were classi-\nfied into the threats to validity or precision that best repre-\nsented the concepts being addressed. We performed this\ntask in duplicate and based on the guidelines established.\nThe same type of analysis has been conducted previously\nfor prognosis research [28]. Disagreements in item classifi-\ncation were resolved by consensus.\nTool citation\nEach quality tool was tracked in the Scopus database to\ndetermine the number of times that the tool was cited\nsince its original paper/citation. The number of citations\nper tool was tracked from January 1, 2007 to July 4,\n2013 to describe recent uses of the tool and to ascertain\nwhether the use of the tool declined after introduction\nof the RoB tool. The RoB tool was originally described\nin Chapter 8 of the Cochrane Handbook [10]. Since\nbooks and book chapters are not indexed in electronic\ndatabases, it was more challenging to track citations for\nthe RoB tool. We tracked RoB citations using Google\nScholar and the journal publication by Higgins et al. [4],\nthat reported on the RoB tool.\nAnalysis\nData were summarized descriptively as the frequency of\neach item across quality tools, and within general health\nand PT research. Comparisons of items from PT and\ngeneral health research tools with the RoB tool were also\nconducted. Comparisons between the proportion of in-\ndividual items used by PT tools and general health re-\nsearch tools were performed using Chi-squared or\nFisher exact tests. The alpha level was set at α = 0.05.\nThe level of agreement between reviewers for study\nselection and data extraction from quality tools was cal-\nculated using percentage agreement and the Kappa (κ)\nstatistic [30]. Analyses were performed using Stata\nStatistical Software: Version 12, 2012 (College Station,\nTX: StataCorp LP).\nResults\nThe updated electronic searches identified 32,627 cita-\ntions. Manual searches identified four additional studies\nbased on their titles and abstracts. After screening titles\nand abstracts, 154 articles were deemed potentially rele-\nvant. The application of the selection criteria resulted in\n148 excluded studies. The main reasons for exclusion of\nstudies were: 1) the study used a quality tool for which\ninformation on construction, development and/or psy-\nchometric properties was not available (n = 40); 2) the\ntool was already included in the original review (n = 39);\n3) the study used a tool that was not specific for RCT\nquality assessment (n = 23); 4) the study used a modified\ntool already included in the review (n = 20); 5) the study\nused an instrument that was not a quality tool (n = 11);\n6) the study used a tool developed for the purposes of a\nsingle review (n = 8); 7) the study focused on animal re-\nsearch (n = 4); 8) the study did not focus on a particular\ntool (n = 2); and 9) information on the name of the tool\nwas not provided (n = 1). A list of excluded studies and\nreasons for exclusion is available in Additional file 4.\nThe level of agreement for study selection between\nreviewers was excellent (kappa = 0.96).\nSix manuscripts [31-36] reporting on four newly devel-\noped tools met the eligibility criteria (Figure 1). These\nfour new tools that evaluated the methodological qual-\nity/RoB of RCTs in health research in addition to the\nRoB tool [4,10] were: the Cochrane Collaboration De-\npression, Anxiety, And Neurosis (CCDAN) tool [34,35],\nthe Randomized Controlled Trial Psychotherapy Quality\nRating Scale (RCT-PQRS Tool) [32,33], 3) the Random-\nized Controlled Trial -Natural Products Tool (RCT-NP)\n[31], and the CLEAR NPT (a checklist to evaluate the\nreport of nonpharmacological trials) [36]. New PT-\nspecific tools were not identified. The five tools were\nadded to the 21 tools identified in our previous review\n[2] (i.e. Jadad [37], Maastricht [38], Delphi [39], PeDro\n[40,41], Maastricht-Amsterdam [42], Van Tulder [43],\nBizzini [44], Chalmers [45], Reisch [46], Andrew [47],\nImperiale [48], Detsky [49], Cho and Bero [50], Balas\n[51], Sindhu [52], Downs and Black [53], Nguyen [54],\nOxford Pain Validity Scale (OPVS) [55], Arrive [56],\nCONSORT [57], and Yates [58]). Therefore, this update\nincludes 26 quality tools. Details on the characteristics\nand psychometric properties of the new quality tools are\npresented in Tables 1 and 2.\nMost of the new tools have been tested for face and\ncontent validity (Table 2 and Additional file 5). Evalua-\ntions of other types of validity, such as criterion validity,\nhave been conducted only for the RCT-NP and the\nRoB tool; however, the criterion used was a non-gold\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 4 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nstandard tool (since to date, there is no accepted gold\nstandard to evaluate the risk of bias or quality of RCTs in\nhealth research). Tool reproducibility has been evaluated\nfor the CCDAN tool, the RCT-PQRS, and the RoB tool.\nThe inter-rater reliability of the RoB tool was fair (k = 0.41-\n0.60) in contrast to the CCDAN and RCT-PQRS tools\nwhich showed good inter-rater reliability (r = 0.75-0.86;\nintra class correlation coefficient [ICC] =0.76-0.79).\nItems from all 26 tools were summarized according to\ntheir frequency of use. The level of agreement between\nreviewers for item categorization in both PT (kappa =\n0.92) and general health research tools (kappa = 0.98)\nwas very good to excellent.\nTools to measure methodological quality/risk of bias\nOf the 26 tools, 19 have been used in general health re-\nsearch and seven in PT research (including the Jadad\nscale, which is commonly used in both research areas).\nA total of 130 items have been used across general heath\nresearch tools compared with 48 items used in PT tools.\nThe RoB tool has 6 domains with 7 items in total.\nAdditional files 5 and 6 provide a detailed description of\nindividual items contained in the tools. The numbers of\nquality items according to the nine content categories\nfor general health versus PT tools were: introduction,\nobjectives, and design: 8 versus 0 items; patient selec-\ntion: 18 versus 4 items; assignment, randomization,\nand allocation concealment: 8 versus 5 items; blinding:\n12 versus 10 items; interventions: 17 versus 8 items; attri-\ntion, follow up and protocol deviation: 10 versus 9 items;\noutcomes: 15 versus 7 items; statistical analysis: 31 versus\n5 items, and miscellaneous: 11 versus 0 items.\nFrequency of items: General health research tools,\nphysical therapy tools and RoB tool\nItems addressing inclusion and exclusion criteria and the\nappropriateness of statistical analysis were the most fre-\nquently used among the general health research tools\n(74%, 14/19 tools). The second most commonly used\nitems in general health research tools were description\nof withdrawals and drop outs, description and appropri-\nateness of randomization process, blinding of investiga-\ntors/assessors, and description of treatment protocol for\nboth intervention and control groups (63%, 12/19 tools).\nIn contrast, baseline comparability, blinding of investi-\ngator/assessor, and use of intention-to-treat analysis\nArticles selected for full evaluation for \ninclusion in the review\nN = 154\nReasons for exclusions\nNo info on construction, development and/or \npsychometric properties=40\n-Tool included in original review=39\n-Tool not specific for RCT assessment=23\n-Modified existing tool= 20\n-Not a quality assessment tool=11\n-Tool developed for single review \npurposes=8\n- Animal research= 4\n-No focus on a particular tool=2\n-Name of scale not reported=1\nDuplicates removed = 5825\nReferences selected for further examination \nof titles and abstracts\nN = 32,627\nTools identified in \nprevious SR\n- N = 21\n-P T  =  7\n- General = 14\nNew tools from \nupdate\nN = 5\nExcluded\nN = 148\nTotal number of references identified from \nthe updated electronic searches = 38,452\nIncluded studies\nN = 6\nManual literature searches = 4\nTotal number of \ntools analyzed\nN = 26\nFigure 1PRISMA flow diagram for identification of studies.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 5 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 1 Characteristics of tools identified in the search update\nStudy (authors, year) Area Numbers of items How items were selected\nfor inclusion\nValidity Reliability Time to\ncomplete\nGuidelines\nfor use\navailable\nNEW TOOLS (2007-2013)\nCOCHRANE COLLABORATION\nDEPRESSION, ANXIETY, AND\nNEUROSIS (CCDAN) [34,35]\nTrials of\ndepression,\nanxiety and\nneurosis.\nPsychological\nand\nPsychiatric\ntrials\n23 items This tool was developed from\nitems included in other\nhealth tools (especially\nCONSORT statement), and\nthen a consensus from\nexperts was performed to\ndetermine a pilot tool to be\ntested.\nFace, content and construct\nvalidity\nReliability evaluated through\ncorrelation coefficient among\n3 raters in total score was\nhigh. It ranged from r=0.75-\n0.86.\n15-20\nminutes\nNo\nScores from raters correlated\nhighly with year of\npublication (r=0.37-0.6)\nReliability for individual items\nwas less strong\nFurther validation consisted\non determine reliability of\nthe tool as well as internal\nconsistency and its\ncorrelation with overall score\nand year of publication.\nThe mean kappa for all 23\nitems ranged between 0.51\nto 0.54 among 3 raters\nInternal consistency\nmeasured through Cronbach\nalpha ranged between 0.65\nto 0.78\nTHE RANDOMIZED\nCONTROLLED TRIAL\nPSYCHOTHERAPY QUALITY\nRATING SCALE (RCT-PQRS\nTOOL) [32,33]\nPsychotherapy 25 items organized in 6\ndomains: Description of\nsubject (4 items), definition\nand delivery of treatment (5\nitems), outcome measures (5\nitems), data analysis (5 items),\ntreatment assignment (3\nitems), overall quality of\nstudy (3 items)\nItems were generated by an\ninformal expert consensus\n(members of the American\nPsychiatric Committee on\nResearch on Psychiatric\ntreatments, outside\nconsultants, who were senior\npsychotherapy and/or\npsychopharmacology clinical\nresearchers.\nThe Cronbachα for all 25\nitems as rated by the primary\nrater was 0.87.\nThe ICC for interrater\nreliability of item 25, the\nomnibus rating of the quality\nof the study, was 0.79.\n10-15\nminutes\nYes\nThe correlation between the\n24-item total and the\nomnibus item (item 25) was\n0.88.\nThe ICC for interrater\nreliability of the total of the\nfirst 24 items was 0.76.\nThe correlation between the\n24-item total and study year\nwas 0.51, significant at P <\n.0001.\nNine of the individual items\nhad individual ICCs between\n0.5 and 0.8 (items 2, 4, 6, 7, 8,\n10, 14, 15, and 19).\nThe correlation of the\nomnibus item and study year\nwas 0.47 (P < .0001).\nTwelve items had individual\nICCs between 0.3 and 0.5\n(items 1, 3, 5, 9, 11, 12, 13, 16,\n17, 18, 20, and 24), and 3 items\nh a di n d i v i d u a lI C C sb e l o w0 . 3\n(items 21, 22, and 23).\nTwo items had very low\nvariation between studies (77%\nof studies received a 0 on item\n13 and 97% of studies\nreceived a 2 on item 21).\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 6 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 1 Characteristics of tools identified in the search update(Continued)\nTHE RCT-NATURAL PRODUCTS\nTOOL (RCT-NP) [31]\nTrials of\nNatural\nproducts\n28 items The initial list of items for this\nstudy was compiled from\nitems contained in published\ncritical appraisal instruments\ndesigned for RCTs of NPs as\nwell as from items suggested\nby the research team.\nComparisons with a\npublished instrument to\nevaluate the methodological\nquality of RCTs for Natural\nproduct was used (criterion\nvalidity). Similar results were\nobtained with both\ninstruments indicating\ncriterion validity (Concurrent\nvalidity)\nNot reported Not\nreported\nYes\nA Delphi process was used to\nachieve consensus among a\ngroup of experts as to which\nitems describing the identity\nof an NP were essential to\nconsider when critically\nappraising an RCT of an NP.\nRaters’ answers were\ncompared with investigators\nanswers to determine\ncriterion validity as well. No\nsignificant differences\nbetween raters and\ninvestigators (gold standard)\nanswers were obtained\nThe consensus building\nprocess was conducted in 2\nrounds using email.\nConsensus was considered to\nhave been reached when\n80% of participants were in\nagreement with an item\nbeing designated as essential\nto include in the instrument\nA final list of items\nconsidered to be essential by\nthe study participants and\ninvestigators was assembled.\nA systematic review\nregarding tools used in to\nevaluate quality of NPs trials\nwas performed. Items from\nall of these tools were\ncompiled\nTo be designated as essential\nto include in the new critical\nappraisal instrument, an item\nhad to meet at least 1 of the\nfollowing 2 inclusion criteria:\nit had to have been\ncontained in a published\ninstrument that was\ndocumented as having been\nvalidated or must have had\nempirical evidence to\nsupport its inclusion in a\npublished instrument.\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 7 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 1 Characteristics of tools identified in the search update(Continued)\nA CHECKLIST TO EVALUATE A\nREPORT OF A\nNONPHARMACOLOGICAL\nTRIAL (CLEAR NPT) [51]\nHealth\nResearch\n10 items and 5 subitems Initial pool of items was\nperformed from existing\nquality tools identified by\nMoher et al. and Verhagen\nand the CONSORT statement,\nusers’ guides to the medical\nliterature, and the Cochrane\nReviewers’ Handbook.\nContent validity was\nprovided by experts in the\nfield through the Delphi\nmethod\nNot reported 10\nminutes\nYes\nItems specific to NPT trials\nidentified in a preliminary\nstudy and during informal\ninterviews of clinicians\nworking in the field of NPT\nwere added.\nThirty-eight potential items\nwere identified.\nA Delphi procedure was used\nto determine the final items\nincluded in the tool.\nRISK OF BIAS TOOL (RoB)\n[4,10]\nHealth\nResearch\nThe risk of bias tool is based\non six domains and 7 items:\nsequence generation,\nallocation concealment,\nblinding, incomplete\noutcome data, selective\noutcome reporting, and\n“other sources of bias.”\nCritical assessments on the\nrisk of bias (high, low,\nunclear) are made separately\nfor each domain.\nThe choice of components\nfor inclusion in the tool was\nbased on empirical evidence\nshowing their association\nwith effect estimates.\nContent validity: items were\nincluded based on empirical\nevidence.\nInterrater agreement for the\nindividual domains of the\nrisk of bias tool ranged from\nslight (κ=0.13 for selective\nreporting) to substantial\n(κ=0.74 for sequence\ngeneration [13].\n~21\nminutes\nYes\nConcurrent validity: A high\ndegree of correlation was\nfound between the domains\nof risk of bias sequence\ngeneration compared with\nJadad randomisation (k=0.79)\nand risk of bias allocation\nconcealment compared with\nSchulz allocation\nconcealment (k=0.73) [13]\nThe RoB demonstrated\nmoderate to substantial\n(mean values 0.56 to 0.76)\nagreement on three of\ntwelve items [59].\nThe interrater agreement\nwas fair (0.40) for selective\noutcome reporting and\nalmost perfect (0.86) for\nsequence generation [62].\nCorrelation was low for the\ncomparisons between the\ndomains of risk of bias\nincomplete outcome data\nand the Jadad withdrawal\nitem, risk of bias overall risk\nand total Jadad score, and\nrisk of bias overall risk and\nSchulz allocation\nconcealment [13]\nInterrater agreement for the\nmajority of domains and\noverall risk of bias was\nmoderate (k = 0.41– 0.60)\n[60].\nThe correlations between\noverall risk of bias\nassessments and total Jadad\nscore (t= 0.04) and allocation\nconcealment (t = 0.02) were\nlow [60].\nThe inter-rater reliability\nacross individual domains of\nthe CCRBT was found to be\n0.30, which is considered\nslight agreement between\nraters [46]. The inter-rater\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 8 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 1 Characteristics of tools identified in the search update(Continued)\nreliability of the final grade\nassigned to each paper by\nthis tool was ICC = 0.58 (95%\nCI 0.20– 0.81)[61]\nThere was very poor\nagreement between the\nEffective Public Health\nPractice Project Quality\nAssessment Tool (EPHPP)\nand the RoB tool in the final\ngrade assigned to each\nstudy (kappa = 0.006)[61]\nThe inter-rater reliability was\nsubstantial for sequence\ngeneration (k=0.79) and fair\nfor the other 5 items (k=0.24-\n0.37). Interrater reliability\nbetween consensus\nevaluations across rater pairs\nwas fair for allocation\nconcealment and“other\nsources of bias” (k=0.37-0.27),\nand moderate for sequence\ngeneration (k=0.60). [62]\n95% CI = 95% confidence interval,CONSORT Consolidated Standards of Reporting of Trials,ICC intraclass correlation coefficient,k kappa, NP natural products, NPT natural products trials,RCT randomized controlled trial,\nRoB risk of bias.\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 9 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nwere the most frequently used items among PT tools\n(86%, 6/7) (Additional files 5 and 6, Figures 2 and 3).\nThe second most frequently used items in PT tools\nwere: reporting of withdrawals and dropouts, method of\nrandomization concealment, description of inclusion/ex-\nclusion, reporting of descriptive measures for point esti-\nmates, blinding of therapist, and blinding of participants\n(71%, 5/7 tools) (Figures 2 and 3).\nInclusion of the following items was significantly more\nfrequent in PT tools compared with general health re-\nsearch tools: “intention to treat” (p = 0.014),“withdraws\nand drop outs acceptable ” (p < 0.001), and “baseline\ncomparability” (p = 0.027).\nWhen RoB items were individually examined, we\nfound that sequence generation and allocation conceal-\nment were included in 5 of the 7 PT tools (Figure 4).\nTable 2 Summary of the quality of measurement properties of quality tools from our previous systematic review and\nthis update\nScale Internal\nconsistency\nFace\nvalidity\nContent\nvalidity\nCriterion\nValidity*\nConstruct\nvalidity\nReproducibility\n(agreement/reliability)\nTOOLS FOR PT\nJadad Tool [37]- + + + + +\nMaastricht Tool [38]- + - + - +\nDelphi Tool [39]- + + + - +\nPEDro Tool [40,41]- + + - - +\nMaastricht-Amsterdam Tool [42]- + + - - +\nVan Tulder Tool [43]- + + + - +\nBizzini tool [44]- + + - - +\nTOOLS FOR GENERAL HEALTH RESEARCH\nTools from previous systematic review\nChalmers tool [45]- + + + - +\nReisch Tool [46]- + - + - -\nAndrew tool [47]- + - - - +\nImperiale tool [48]- + - + - -\nDetsky tool [49]- + - + - +\nCho Tool [50]- + - + - +\nBalas tool [51]- + - - - -\nSindhu tool [52]- + + + - +\nDowns and Black tool [53]+ + + + - +\nNguyen tool [54]- + - - - -\nOxford pain validity tool [55]- + - - - -\nArrive tool [56]- + - - - +\nCONSORT tool [57]- + + - - +\nYates Tool [58]- + + - + +\nNew tools identified in search update\nCochrane Collaboration Depression, Anxiety, and\nNeurosis (CCDAN) [34,35]\n++ + - + +\nThe Randomized Controlled Trial Psychotherapy Quality\nRating Tool (RCT-PQRS Tool) [32,33]\n++ + - + +\nRCT-Natural Products Tool (RCT-NP) [31]- + + + - -\nCLEAR NPT (Checklist to evaluate a report of a\nnonpharmacological trial [36]\n-+ + - - -\nRisk of Bias Tool (RoB) [4,10]- + + + + +\n+Quality of measurements properties were based on guidelines established by Terwee et al. [13].\n(+): criterion accomplished.\n(−): Criterion not accomplished.\n* In all cases, criterion validity was established with“no gold standard tools”.\nPT physical therapy, RCT randomized controlled trial.\\\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 10 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nOnly four PT tools evaluated whether randomization\nwas performed. Twelve (63%) general health research\ntools included randomization whereas seven (37%) in-\ncluded an item for allocation concealment. Further,\nfewer of the general health research tools included items\nrelated to blinding compared with the PT tools: blinding\nof participants (47% versus 71%) and blinding of out-\ncome assessors (63% versus 86%). Intention to treat ana-\nlysis, a component of the incomplete outcome data\ndomain in the RoB tool was more frequently used in PT\ntools (86%) compared with general health research tools\n(32%). Other items related to incomplete outcome data\nin the RoB tool are“description of withdrawals and drop\nouts” and “appropriateness of withdrawal/drop outs\nrate”. Compared with the general health research tools\n(63%), a larger proportion of PT tools (71%) included\nitems for the description of withdrawals and drop outs.\nIn contrast, none of the general health research tools in-\ncluded an item about whether the withdrawal/drop-out\nrate was acceptable compared with 57% of the PT tools.\nAnother quality item used in the RoB tool is baseline\ncomparability. This item was included in 86% of PT\ntools compared with 37% of the general health research\ntools. In general, PT tools appeared more similar in con-\ntent to the RoB tool than those used for general health\nresearch (Figure 4).\nReporting versus conduct items and threats to validity\nand precision\nOf the 130 items included in the general health research\ntools, 62 (48%) evaluated trial “reporting” whereas 60\nitems (46%) evaluated “conduct” (i.e. methodological\nquality or risk of bias). Eight items (6%) were considered\nto evaluate both quality of reporting and conduct of tri-\nals (i.e. sample representativeness and description of par-\nticipants source, description of randomization process\nreported and appropriate; testing of subject compliance\nto treatment protocol /report of compliance; therapist\ntraining and level of experience in the treatment(s)\nunder investigation; validity, reliability and responsive-\nness of the outcome measures reported; post-hoc power\ncalculations and confidence intervals reported).\nClassification of items from general health research\ntools according to type of bias and threats to precision\nwas as follows: selection bias (25 items, 19%); perform-\nance bias (six items, 4.6%); performance and detection\nbias (six items, 4.6%); performance bias and contamin-\nation (seven items, 5.4%); performance bias and compli-\nance (two items, 1.6%); attrition bias (ten items, 7.8%);\ninformation bias (six items, 4.6%); detection bias (five\nitems, 3.8%); reporting bias (17 items, 13%); threats to\nprecision (four items, 3.1%); statistical bias (three items,\n2.3%); threats to precision and statistical bias (two items,\n73.7\n73.7\n63.2\n63.2\n63.2\n63.2\n57.9\n52.3\n52.3\n52.3\n47.4\n47.4\n42.1\n42.1\n42.1\n36.8\n36.8\n36.8\n31.6\n31.6\n31.6\n31.6\n31.6\n0 1 02 03 04 05 06 07\n08 0\nInclusion and exclusion criteria clearly defined\nAppropriate statistical analysis\nDescription of withdrawals and Drop outs\nDescription of randomization process reported and appropiate\nBlinding of investigator/assessor\nTreatment protocol adequately described for the treatment and control groups\nStudy described as randomized\nStudy question/hypothesis described and well defined\nDescription of sample characteristics\nDescription of outcome measures/criteria for measuring outcomes\nBlinding of subjects\nSample size calculation performed prior to initiation of the study\nType of Statistical test used clearly stated\nPower calculation\nTesting of subject compliance to treatment protocol\nBaseline comparability\nAllocation concealment\nBlinding of therapist\nClear reporting of number of subjects excluded from the trial\nReliability reported for main outcome measures\np value and/or confidence intervals reported\nStudy conclusion supported by the findings of the study \nIntention to treat\n%\nFigure 2Frequency of items used by tools used in general health research to measure methodological quality RCTs.RCT = randomized\ncontrolled trial.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 11 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\n1.6%); multiple biases (three items, 2.3%); and other (i.e.,\nnot classified as related to threats to validity or preci-\nsion) (27 items, 21%) (Additional file 5).\nOf the 48 items included in PT tools, 16 (33.3%) evalu-\nated trial “reporting” whereas 28 (58.3%) evaluated“con-\nduct”. Four items (8.3%) were considered to evaluate\nboth quality of trial reporting and conduct: testing/re-\nport subject compliance to treatment protocol, and\nvalidity, reliability and responsiveness of the outcome\nmeasures reported.\nThe classification of items from PT tools according to\ntype of bias and threats to precision was as follows: se-\nlection bias (10 items, 21%); performance and detection\nbias (five items, 10%); performance bias only (six items,\n135%); performance and contamination bias (two items,\n4.2%); performance and compliance bias (two items,\n4.2%); information bias (five items, 10%); attrition bias\n(eight items, 17%); detection bias (three items, 6.3%);\nreporting bias (three items, 6.3%); threats to precision\n(two items, 4.2%); and statistical bias (1 item, 2.1%).\n(Additional file 6).\nFrequency of citations of quality tools\nThe number of citations per quality tool since its incep-\ntion and after 2007 are detailed in Table 3. The Jadad\nscale was, by far, the most cited and used tool with 5,326\ncitations from inception (i.e., year 1996) to July 4, 2013.\nThe second most cited tool was the Downs and Black\ntool, with 962 citations since its introduction in 1998.\nOther tools frequently cited were: PeDro, Delphi, and\nChalmers tools (651, 625, and 584 citations from in-\nception, respectively), followed closely by the Van\nTulder (560 citations) and Maastricht-Amsterdam\n(360 citations) tools. Among the most frequently cited\ntools, a larger proportion (5/7) were PT tools com-\npared with only two of the 19 general health research\ntools (i.e. Jadad and Chalmers). The relative number\nof citations for the tools after 2008 was similar to\nthose of previous years. Particularly, the use of the\nJadad tool (i.e. number of citations from 2007 to July\n4, 2013: 3,672) did not show a decrease (in terms of\nabsolute numbers of citations) after the inception of\nthe RoB tool in 2008 (Table 3). Tracking of the RoB\ntool showed that it has been cited approximately\n1230 times since inception. This number is likely an\nunderestimate because of the challenges described\nabove with respect to tracking the Cochrane Hand-\nbook chapter that first described the tool. However,\nthis information provides a reference point to track\nusage of the RoB tool over time.\n85.7\n85.7\n85.7\n71.4\n71.4\n71.4\n71.4\n71.4\n71.4\n71.4\n57.14\n57.14\n57.1\n42.9\n42.9\n42.9\n42.9\n42.85\n28.6\n28.6\n28.6\n28.6\n28.6\n28.6\n14.3\n0 1 02 03 04 05 06 07 08 09 0\nIntention to treat analysis \nBlinding Investigator/assessor\nBaseline comparability\nReport of Withdraws and dropouts\nMethod of randomization concealed\nInclusion/exclusion criteria defined\nDescriptive measures for point estimates\nBlinding Therapist\nBlinding participants\nMethod of randomization appropriate\nWithdraws and dropouts rate acceptable\nCo-interventions avoided or comparable\nRandomization method performed\nStudy described as randomized\nTreatment protocol described for treatment group\nAppropiate statistical analysis used\nMethod of randomization described\nReasons for withdrawal and drop outs\nThe timing of outcome measure was comparable\nRelevant outcomes included\nAdverse effects described\nadherence/compliance acceptable in all groups\nAdequate sample size\nTreatment protocol described for control group\nStudy described as double blinded\n%\nFigure 3Frequency of items used from tools used in PT research to measure methodological quality of RCTs.PT = physical therapy;\nRCT = randomized controlled trial. Results expressed as percentages.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 12 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nDiscussion\nThis study examined tools and individual items used in\ngeneral health and PT research to assess the quality of\nRCTs. A variety of tools are still widely used despite crit-\nicisms raised regarding their limitations [8,63]. This find-\ning is consistent with previous reviews on this topic that\nhave identified inconsistencies in the use of quality tools\n[5,6,18]. There is extensive variation in individual items\nincluded across quality assessment tools. Many of these\nitems may not be indicators of bias nor related to over-\nor under-estimations of treatment effects. Moreover,\nthere is lack of empirical evidence supporting the associ-\nation of many individual quality items with changes in\nthe magnitude and direction of treatment effects. This\nfinding raises important concerns in the field of quality\nassessment regarding the appropriateness of evaluating\nthe evidence based on the use of these tools and items.\nResults of this study agree with those of Deschartres\net al. [19] which found that a large number of tools have\nbeen used in reviews that assessed the quality and\nreporting of RCTs. According to Deschartres et al. [19],\nambiguity and lack of a unique definition of trial“qual-\nity” accounts for the heterogeneity of quality assessment\ntools. According to Verhagen et al. [2], methodological\nquality assessment involves the evaluation of internal\nvalidity (the degree to which the study design, conduct\nand analysis have minimized biases), external validity\n(the extent to which study results are generalizable be-\nyond the experimental situation), and statistical analysis\nof primary research. According to The Cochrane Collab-\noration [10], internal validity of a trial is linked to“risk\nof bias” and it should be the primary focus of quality as-\nsessment since external validity differs upon context. In\naddition, “quality of reporting” is commonly used as a\nproxy for trial quality, which has complicated the\nconstruct of“quality” even more.\nA clear and consistent definition of “quality” across\nhealth research areas is necessary to advance the field of\nquality assessment. Furthermore, concepts such as\ninternal validity, external validity, and quality of\nreporting should be explicitly and clearly defined for\nthe constructs that the individual items are meant to\naddress. Finally, items assessing the methodological\nquality (or internal validity) of RCTs should be based\non empirical evidence of their association with treat-\nment effects.\n71.4\n57.1\n71.4 71.4\n85.7\n71.4\n85.7\n71.4\n57.14\n85.7\n63.2 63.2\n36.8\n47.4\n63.2\n36.8\n31.6\n63.2\n0\n36.8\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nAppropriate\nPerformed\nParticipants\nAssessors/outcomes\nTherapists\nIntention to treat\nWithdraws and drop\nouts reported \nWithdraws and drop\nouts rate acceptable \nBaseline\nComparability\nRandomization Allocation\nconcealment\nBlinding Incomplete outcome data Other Bias \n%\nPT tools\nHealth tools\nFigure 4Comparison between RoB tool domains and items from PT and health sciences tools.PT = physical therapy; RoB = risk of bias.\nResults expressed as percentages.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 13 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 3 Frequency of citations of Quality Tools in Scopus Database\nTool All years until\nJuly 4, 2013\nFrom January 2007-\nJuly 4, 2013\nYear 2007\nbefore RoB\ntool\nYear\n2008\nYear\n2009\nYear\n2010\nYear\n2011\nYear\n2012\nYear 2013 Until\nJuly 4, 2013\nSubject area most used\nPT TOOLS\nJadad [37] 5326 3672 393 468 616 514 634 706 341 Medicine\nMaastricht [38] 106 45 3 6 10 5 7 9 5 Medicine/health professions\nDelphi [39] 625 454 39 65 71 77 87 75 40 Medicine/health professions\nPEDro [40,41] 651 555 49 52 87 91 101 108 67 Health professions/ Medicine\nMaastricht-Amsterdam [42] 360 158 34 36 23 21 21 19 4 Medicine/health professions\nVan Tulder [43] 560 482 43 58 101 86 65 93 34 Medicine/health professions\nBizzini [44] 65 50 4 15 7 6 9 4 5 Medicine/health professions\nGENERAL HEALTH RESEARCH TOOLS\nTools From Previous Systematic Review\nChalmers [45] 584 151 19 21 29 19 19 29 15 Medicine/Psychology\nReisch [46] 56 26 2 4 5 2 8 4 1 Medicine/ Nursing\nAndrew [47] 10 0 0 0 0 0 0 0 0 Medicine\nImperiale [48] 141 47 6 7 8 4 8 10 4 Medicine/ Pharmacology,\nToxicology and Pharmaceutics\nDetsky [49] 281 120 15 20 21 14 17 20 13 Medicine/ Biochemistry/\nNursing\nCho [50] 109 30 4 4 5 3 5 5 4 Medicine/ Pharmacology,\nToxicology and Pharmaceutics\nBalas [51] 37 5 0 3 1 0 0 0 1 Medicine\nSindhu [52] 31 11 2 2 3 1 3 0 0 Medicine/Nursing\nDowns and Black [53] 962 783 80 72 100 125 146 163 97 Medicine/Nursing\nNguyen [54] 84 63 6 9 9 9 14 11 5 Dentistry/Medicine\nOxford pain validity tool [55] 143 49 11 14 7 10 2 3 2 Medicine/Neuroscience\nArrive [56] 23 6 2 2 2 0 0 0 0 Medicine\nCONSORT [57] 184 124 30 21 20 14 14 17 8 Medicine/Biochemistry\nYates [58] 35 33 2 1 4 7 8 6 5 Medicine/Neuroscience\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 14 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTable 3 Frequency of citations of Quality Tools in Scopus Database(Continued)\nNEW TOOLS\nCochrane Collaboration Depression,\nAnxiety, and Neurosis (CCDAN) [34,35]\n59 44 3 6 11 6 10 7 1 Medicine/psychology\nThe Randomized Controlled Trial\nPsychotherapy Quality Rating (RCT-PQRS)\n[32,33]\n30 30 0 0 0 0 10 16 4 Medicine/psychology\nRCT-Natural Products (RCT-NP) [31] 3 3 0 0 1 1 1 0 0 Pharmacology, Toxicology\nand Pharmaceutics\nCLEAR NPT (Checklist to evaluate a report of\na nonpharmacological trial [36]\n108 102 16 18 17 10 17 16 8 Medicine\nRisk of Bias (Scopus Track) Higgins et al.,\n2011 [4]\n124 1 41 75 Medicine\nRisk of Bias(Google Scholar) Chapter 8:\nCochrane Handbook (2008-July4, 2013) [10]\n1155 1155 Medicine\nTotal RoB 1230*\n*This number could include duplicates;RoB risk of bias.\nArmijo-Olivoet al. BMC Medical Research Methodology2013,13:116 Page 15 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nThe number of items across quality tools is large; 130\nand 48 items have been used by tools in general health\nand PT research, respectively. Some items are subjective,\nconfusing, and lack a clear definition (e.g., subjects ap-\npropriate to study questions, discussion of bias resulting\nfrom non-blinding assessment). These factors make the\nevaluation of individual items challenging and likely con-\ntribute to low inter-rater agreement. Many quality items\nrelate to “reporting” rather than “conduct” of trials; ap-\nproximately half of the items from these tools relate to\nreporting only. This finding is consistent with results de-\nscribed by Deschartres et al. [19], in which 25% of meth-\nodological reviews stated that RCTs reported details of\nsample size calculation, but only 6% reported on ad-\nequacy of the sample size. Although clear reporting is\nnecessary to assess the quality of trial conduct, a focus\non quality of reporting can hide differences in trial con-\nduct and lead to under- or over-estimation of the meth-\nodological quality [64].\nComparison of items between PT and general health\nresearch tools with RoB tool\nWe found that items frequently included in the PT tools\nwere more closely linked to items/domains included in\nthe RoB tool than those of general health research tools.\nThis result suggests that PT tools are more closely\nlinked to an examination of bias than the general health\nresearch tools.\nEmpirical evidence has supported many items in the\nRoB tool. There is a substantial interest in investigating\nwhich methodological features of RCTs are associated\nwith treatment effects. Evidence informing this associ-\nation comes mainly from RCTs in the area of medicine\nand is based primarily on evaluations of dichotomous\noutcomes [12,14,15]. Therefore, empirical evidence on\nthe relationship between trial quality and treatment\neffects may not be readily applicable to other health re-\nsearch areas such as PT and other areas of rehabilitation.\nMorever, information regarding the importance of in-\ncluding certain items in quality tools within different\nclinical areas is limited. As mentioned previously, RCTs\nin the area of PT have distinct characteristics compared\nwith pharmacological trials conducted in medicine. PT\ninterventions are complex interventions [20]; they com-\nprise certain characteristics such as the type of therapy\nand its intensity, a standardized or individually tailored\napproach, and the skills and experience of the therapists,\nthat are likely to affect trial results. In addition, because\nof the nature of certain PT interventions (e.g., manual\ntherapy, exercises), blinding of therapists and/or patients\nis not always possible. Appropriate blinding of study\nparticipants and all key study personnel is unlikely to be\naccomplished for most PT trials; however, blinding of\noutcome assessment has been commonly used as a\nproxy quality measure without validation. Therefore,\nmore empirical evidence on trial bias is needed in the\narea of PT to determine which factors are likely to affect\ntreatment effect estimates and thus provide accurate re-\nsults for the clinical community. Further research should\nexamine the appropriateness of using certain items/\ndomains when evaluating the risk of bias of primary re-\nsearch in a variety of health areas. This information\nwould provide clear benchmarks to assess the quality or\nrisk of bias of primary research included in SRs and\nmeta-analysis, and ultimately strengthen the evidence\nfor decision-making in all areas of health care.\nThe RoB tool is recommended by The Cochrane Col-\nlaboration. Some groups within the Collaboration have\ndeveloped their own tools and have not yet adopted the\nRoB approach (e.g. Cochrane Bone, Joint and Muscle\nTrauma Group). Other Cochrane groups have modified\nthe RoB tool for their own purposes (i.e. Cochrane Back\nReview Group, Cochrane Renal Review Group). The\nRoB tool was developed more recently than many of the\nother tools; current research [9,13] recommends further\ntesting of its psychometric properties and validation of\nthe tool in a wide range of research fields. Additional\nguidelines will help users in applying and interpreting\nthe results of the RoB tool.\nPT and general health research tool items and threats to\nvalidity and precision\nMost items from general health research and PT tools\nwere classified according to one or more categories of\nthreats to validity or precision; however, some items\ncould not be placed in any category. For example, the\nitem “study question/hypothesis/purpose described and\nwell defined” was not linked to any type of bias and was\nfound irrelevant for study quality. Nevertheless, this item\nwas included in 10 (53%) health research tools. This\nsituation raises concerns about the usefulness of certain\nitems to determine trial quality; therefore, these types of\nitems should be carefully considered when deciding\nwhether they should be part of these tools.\nClassifying quality items was a complex task due to\nunclear descriptions of the items and lack of empirical\nevidence linking these items to bias. The number of\nitems that was linked to different types of bias varied by\ntool. For example, a high percentage of items dealt with\nselection bias (approximately 19% of general health and\n21% of PT tools). In contrast, attrition bias was more\nfrequently represented in items found in PT (17%) com-\npared with general health research (7%) tools. These re-\nsults call for an in-depth analysis of individual items of\ntools that evaluate trial quality or risk of bias of RCTs in\nhealth research in order to provide a more complete as-\nsessment of their internal validity.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 16 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nTools most cited\nThe Jadad scale [37] is the most frequently cited tool in\nhealth sciences research despite criticisms regarding its\nlack of responsiveness [8] and applicability to other\nhealth research areas such as PT and rehabilitation [5].\nHerbison et al. [8], found that the Jadad scale might not\nbe responsive enough to distinguish among different\nlevels of trial quality. The use of the Jadad scale has been\ndiscouraged in many areas of health research. The dis-\ncordance between recommendations against using the\nJadad scale and its ongoing use is a matter of concern\nand reasons for this discrepancy should be further ex-\nplored. It is likely that the Jadad tool is popular among\nSR authors because it is simple and requires little time\nto apply [13].\nNone of the other quality tools used in general health\nresearch and PT is as highly cited as the Jadad tool.\nSome tools are specific to certain areas (e.g., PT, nursing,\npsychology, pharmacology); most of them are long in-\nstruments and require a greater amount of time to\ncomplete; and some lack clear guidelines for item assess-\nment, which can discourage their use.\nStrengths and limitations\nTo the best of our knowledge, this study is the first to\nexhaustively explore the type and frequency of individual\nitems included in tools that evaluate the quality or risk\nof bias of RCTs in health research. A comprehensive\nsearch was performed for all published research in this\narea, with no language restrictions, and using several\nstrategies (i.e., manual search, Scopus) to identify rele-\nvant literature. However, because of indexing problems\nof research on the evaluation of quality assessment tools\nfor RCTs [19], some studies may have been missed; this\nwould not likely change our general findings.\nData extraction and item classification was performed\nindependently by two researchers with disagreements re-\nsolved by consensus. The process of classifying items\nwas somewhat subjective; therefore, classification of\nsome items may be debated. Difficulties in classifying\nitems as potentially linked to bias have been acknowl-\nedged in previous studies that analyzed bias in different\ntypes of research designs [25,28,65].\nWe used Scopus database to track all original papers\ndescribing quality tools. We acknowledge that this ap-\nproach is only an indirect measure of the usage of qual-\nity tools and should not be interpreted as absolute\nindicator of usage over time.\nConclusion\nThere is a considerable number of tools to evaluate the\nquality of RCTs in health research. There is extensive\nvariation in the number of individual items across qual-\nity assessment tools and an apparent lack of agreement\nbetween PT and general health research tools in the type\nof items that are included. There is a need for clarity\nand consistency of the constructs evaluated by items in\nquality assessment tools, particularly for aspects related\nto internal validity, external validity, precision, and qual-\nity of reporting. The selection of items to assess internal\nvalidity, or risk of bias, should be based on empirical evi-\ndence of an association with distortions of treatment ef-\nfects. Finally, tools and items should undergo a thorough\nvalidation process to examine their psychometric prop-\nerties. Future studies in this area should investigate\nwhich items are linked to bias through empirical evi-\ndence or psychometric testing. This information will be\nvaluable for the field of knowledge synthesis.\nWhat is new?\nKey findings\nThere is extensive item variation across tools that evalu-\nate the risk of bias of RCTs. There is a lack of empirical\nevidence to support the association with bias for many\nitems.\nWhat this adds to what is known: Although some\nstudies have previously addressed the use of tools for\nquality assessment of RCTs, this is the first study that\nexhaustively explores the type and frequency of items in-\ncluded in different tools that evaluate the risk of bias of\nRCTs in health research. The number of items included\nacross quality tools is large: 130 and 48 different items\nhave been used by general health research and physical\ntherapy (PT) tools, respectively. Many items are used\nwithout a clear identification of their link to bias, or in-\nternal validity. The frequency of use of these items varies\naccording to health area (as demonstrated by our com-\nparison between PT and general health research), which\nsuggests a lack of agreement regarding their relevance to\ntrial quality or risk of bias.\nWhat is the implication, what should change now? Re-\nsults of this study call for an in-depth empirical analysis\nof the items that should be used to assess risk of bias of\nRCTs in health research. This information is urgently\nneeded to develop guidelines for the design, conduct,\nand implementation of trials. In addition, this informa-\ntion is important for systematic reviewers and meta-\nanalysts to evaluate the risk of bias of intervention trials\nin different areas of health research.\nAdditional files\nAdditional file 1:Search Strategy Example.\nAdditional file 2:Definition of psychometric properties according\nto Terwee et al., [24].\nAdditional file 3:Bias definitions.\nAdditional file 4:Excluded Studies.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 17 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\nAdditional file 5:Heath Sciences Tools and items to Measure\nMethodological Quality of RCTs.\nAdditional file 6:Tools and Items to Assess Quality of RCTs in\nPhysical Therapy.\nAbbreviations\nCCDAN: Cochrane collaboration depression, anxiety, and neurosis;\nPT: Physical therapy; RCT: Randomized controlled trial; RCT-NP: Randomized\ncontrolled trial -natural products; RCT-PQRS: Randomized controlled trial\npsychotherapy quality rating scale; RoB: Risk of bias; SRs: Systematic reviews.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthors’ contributions\nAO conceived of the study, designed the study, and drafted the manuscript.\nAO, JF, MO and HS contributed to data collection data analysis, and\ninterpretation. LH provided feedback on the concept and research design\nand participated in interpretation of data. All authors critically revised the\nmanuscript and provided final approval of the version to be published.\nAuthor’s information\nSusan Armijo-Olivo has a Bsc in Physical therapy (PT) from the Pontifical\nUniversity Catholic of Chile, a MSc PT and a PhD in Rehabilitation Sciences\nfrom the University of Alberta. Her major field of research is diagnosis,\nevaluation, and treatment of patients with musculoskeletal pain especially\ntemporomandibular disorders and cervical spine disorders along with\nphysical therapy evidence based practice. She currently is a postdoctoral\nfellow at the CLEAR (Connnecting Leaadership and Research Program in the\nFaculty of Nursing at the University of Alberta. Her post-doctoral project will\nfocus on the methodological predictors of effect size estimates in PT trials.\nThis research is critical to accurately provide conclusions for health care and\ndecision making. This project will be an important contribution to the area\nof knowledge synthesis and translation in the PT field and allied health\nprofessions.\nAcknowledgements\nDr. Susan Armijo-Olivo is supported by the Canadian Institutes of Health\nResearch (CIHR) through a full-time Banting fellowship, the Alberta Innovates\nHealth solution through an incentive award and the STIHR Training Program\nfrom Knowledge Translation (KT) Canada, and the University of Alberta. Dr.\nHumam Saltaji is supported through a Clinician Fellowship Award by Alberta\nInnovates - Health Solutions (AIHS), the Honorary Izaak Walton Killam\nMemorial Scholarship by the University of Alberta, and the Honorary WCHRI\nAward by the Women and Children’s Health Research Institute (WCHRI) . Dr.\nLisa Hartling is supported by a Canadian Institutes of Health Research New\nInvestigator Salary Award. This project was funded by the Physiotherapy\nFoundation of Canada (PFC) through a B.E. Schnurr Memorial Fund Award,\nAIHS through a knowledge translation initiative grant, the Knowledge\nTranslation (KT) Canada Research Stipend Program, the CIHR Banting\nprogram and the University of Alberta.\nThe authors of this study thank staff at the Alberta Research Centre for\nHealth Evidence who helped with data collection: Melanie Muise, Walie\nAtkary, and Alyssa Guthrie.\nAuthor details\n1Postdoctoral Fellow, CLEAR (Connecting Leadership and Research)\nOutcomes Research Program, University of Alberta, 5-115A Edmonton Clinic\nHealth Academy (ECHA), 11405– 87 Avenue, Edmonton, Alberta T6G 1C9\nCanada.\n2Faculty of Rehabilitation Medicine, Department of Physical Therapy,\nUniversity of Alberta, 3-48 Corbett Hall, Edmonton T6G 2G4, Canada.\n3Department of Physical Therapy, Catholic University of Maule, Talca, Chile.\n4School of Public Health, University of Alberta, Institute of Health Economics,\nEdmonton, Alberta, Canada.5School of Dentistry, Faculty of Medicine and\nDentistry, University of Alberta, Edmonton, Alberta, Canada.6Alberta Research\nCentre for Health Evidence, Department of Pediatrics, Faculty of Medicine\nand Dentistry, University of Alberta, Edmonton, Alberta, Canada.\nReceived: 26 December 2012 Accepted: 12 September 2013\nPublished: 17 September 2013\nReferences\n1. Khan K, Ter Riet G, Popay J, Nixon J, Kleijnen J:Satge II. Conducting the\nReview. Phase 5 Study Quality Assessment.In Undertaking Systematic Reviews\nof research Effectiveness CRD’s Guidance for those carrying out or commissioning\nreviews. York: Centre for Reviews and Dissemination; 2001:1– 20.\n2. Verhagen AP, de Vet HC, de Bie RA, Boers M, van den Brandt PA:The art of\nquality assessment of RCTs included in systematic reviews.J Clin\nEpidemiol 2001, 54(7):651– 654.\n3. Juni P, Altman DG, Egger M: Systematic reviews in health care: Assessing\nthe quality of controlled clinical trials.BMJ 2001, 323(7303):42– 46.\n4. Higgins JPT, Altman DG, Goetzsche PC, Juni P, Moher D, Oxman AD,\nSavovicÄ‡ J, Schulz KF, Weeks L, Sterne JAC:The Cochrane Collaboration’s\ntool for assessing risk of bias in randomised trials.BMJ 2011,\n343(7829):d5928. 10.1136/bmj.d5928.\n5. Armijo-Olivo S, Macedo LG, Gadotti IC, Fuentes J, Stanton T, Magee DJ:\nScales to assess the quality of randomized controlled trials: a systematic\nreview. Phys Ther 2008, 88(2):156– 175.\n6. Moher D, Jadad AR, Nichol G, Penman M, Tugwell P, Walsh S:Assessing the\nquality of randomized controlled trials: an annotated bibliography of\nscales and checklists.Control Clin Trials1995, 16(1):62– 73.\n7. Colle F, Rannou F, Revel M, Fermanian J, Poiraudeau S:Impact of quality\nscales on levels of evidence inferred from a systematic review of\nexercise therapy and low back pain.Arch Phys Med Rehabil2002,\n83(12):1745– 1752.\n8. Herbison P, Hay-Smith J, Gillespie WJ: Adjustment of meta-analyses on the\nbasis of quality scores should be abandoned.J Clin Epidemiol2006,\n59(12):1249– 1256.\n9. Armijo-Olivo S, Stiles C, Hagen N, Biondo P, Cummings G:Assessment of\nstudy quality for systematic reviews: a comparison of the Cochrane\nCollaboration Risk of Bias tool and the Effectve Public Health Practice\nProject Quality Assessment Tool: methodological research.J Eval Clin\nPract 2012, 18(1):12– 18.\n10. Higgins J, Altman D: Chapter 8: Assessing risk of bias in included studies.\nIn Cochrane Handbook for Systematic Reviews of Interventions version 50.\nEdited by Higgins J, Green S. Chichester, UK: John Wiley & Sons, Ltd; 2008.\n11. Pildal J, Hrobjartsson A, Jorgensen KJ, Hilden J, Altman DG, Gotzsche PC:\nImpact of allocation concealment on conclusions drawn from meta-\nanalyses of randomized trials.Int J Epidemiol2007, 36\n(4):847– 857.\n12. Wood L, Egger M, Gluud LL, Schulz KF, Juni P, Altman DG, Gluud C, Martin RM,\nWood AJG, Sterne JAC:Empirical evidence of bias in treatment effect\nestimates in controlled trials with different interventions and outcomes:\nMeta-epidemiological study.BMJ 2008, 336(7644):601– 605.\n13. Hartling L, Ospina M, Liang Y, Dryden DM, Hooton N, Seida JK, Klassen TP:\nRisk of bias versus quality assessment of randomised controlled trials:\ncross sectional study.BMJ 2009, 339(7728):1017.\n14. Moher D, Pham B, Jones A, Cook DJ, Jadad AR, Moher M, Tugwell P, Klassen TP:\nDoes quality of reports of randomised trials affect estimates of\nintervention efficacy reported in meta-analyses?Lancet 1998,\n352(9128):609– 613.\n15. Schulz KF, Chalmers I, Hayes RJ, Altman DG:Empirical evidence of bias:\nDimensions of methodological quality associated with estimates of\ntreatment effects in controlled trials.JAMA 1995, 273(5):408– 412.\n16. Assessing risk of bias in included studies.http://bmg.cochrane.org/assessing-\nrisk-bias-included-studies.\n17. Delgado-Rodriguez M, Llorca J: Bias. J Epidemiol Commun Health2004,\n58(8):635– 641.\n18. Katrak P, Bialocerkowski AE, Massy-Westropp N, Kumar S, Grimmer KA:A\nsystematic review of the content of critical appraisal tools.BMC Med Res\nMethodol 2004, 4(1):22.\n19. Dechartres A, Charles P, Hopewell S, Ravaud P, Altman DG:Reviews\nassessing the quality or the reporting of randomized controlled trials are\nincreasing over time but raised questions about how quality is assessed.\nJ Clin Epidemiol2011, 64(2):136– 144.\n20. Kunz R, Autti-Ramo I, Anttila H, Malmivaara A, Makela M:A systematic\nreview finds that methodological quality is better than its reputation but\ncan be improved in physiotherapy trials in childhood cerebral palsy.\nJ Clin Epidemiol2006, 59(12):1239– 1248.\n21. Streiner D, Norman G: Validity. In Health Measurements Scales.Edited by\nStreiner D, Norman G. Oxford: Oxford University Press; 2004:172– 193.\n22. Streiner D, Norman G: Reliability. In Health Measurements Scales.Edited by\nStreiner D, Norman G. Oxford: Oxford University Press; 2004:126– 152.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 18 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116\n23. Streiner D, Norman G:Measuring Change.In Health Measurements Scales.\nEdited by Streiner D, Norman G. Oxford: Oxford University Press; 2004:194– 212.\n24. Terwee CB, Bot SDM, de Boer MR, van der Windt DAWM, Knol DL, Dekker J,\nBouter LM, de Vet HCW:Quality criteria were proposed for measurement\nproperties of health status questionnaires.J Clin Epidemiol2007,\n60(1):34– 42.\n25. Whiting P, Rutjes AWS, Reitsma JB, Glas AS, Bossuyt PMM, Kleijnen J:\nSources of variation and bias in studies of diagnostic accuracy: a\nsystematic review. An Intern Med2004, 140(3):189– 202.\n26. Sackett DL: Bias in analytic research.J Chronic Dis1979, 32(1– 2):51– 68.\n27. Viswanathan M, Berkman ND: Development of the RTI item bank on risk\nof bias and precision of observational studies.J Clin Epidemiol2011,\n65(2):163– 178.\n28. Hayden JA, Cotte P, Bombardier C:Evaluation of the quality of prognosis\nstudies in systematic reviews.An Intern Med2006, 144(6):427– 437.\n29. Lash TL, Fox MP, Fink AK:Multiple Bias Modeling: Applying Quantitatvie Bias\nAnalysis to. Springer New York: Observational Epidemiologic Research; 2009.\n30. Byrt T: How good is that agreement?Epidemiology (Cambridge, Mass)1996,\n7(5):561.\n31. Jurgens T, Whelan AM, MacDonald M, Lord L:Development and\nevaluation of an instrument for the critical appraisal of randomized\ncontrolled trials of natural products.BMC Complem Altern Med2009,\n9:11. doi:10.1186/1472-6882-9-11.\n32. Gerber AJ, Kocsis JH, Milrod BL, Roose SP, Barber JP, Thase ME, Perkins P,\nLeon AC: A quality-based review of randomized controlled trials of\npsychodynamic psychotherapy. Am J Psychiatr2011, 168(1):19– 28.\n33. Kocsis JH, Gerber AJ, Milrod B, Roose SP, Barber J, Thase ME, Perkins P, Leon\nAC: A new scale for assessing the quality of randomized clinical trials of\npsychotherapy. Compr Psychiatr 2010, 51(3):319– 324.\n34. Cipriani A, Malvini L, Furukawa TA, Barbui C:Relationship between quality\nof reports of antidepressant randomized controlled trials and treatment\nestimates: Systematic review, meta-analysis, and meta-regression\nanalysis. J Clin Psychopharmacol2007, 27(4):352– 356.\n35. Moncrieff J, Churchill R, Colin Drummond D, McGuire H:Development of a\nquality assessment instrument for trials of treatments for depression and\nneurosis. Int J Meth Psychiatr Res2001, 10(3):126– 133.\n36. Boutron I, Ravaud P, Moher D, Tugwell P, Giraudeau B, Poiraudeau S, Nizard\nR: A checklist to evaluate a report of a nonpharmacological trial (CLEAR\nNPT) was developed using consensus.J Clin Epidemiol2005,\n58(12):1233– 1240.\n37. Jadad AR, Moore RA, Carroll D, Jenkinson C, Reynolds DJM, Gavaghan DJ,\nMcQuay HJ: Assessing the quality of reports of randomized clinical trials:\nIs blinding necessary?Control Clin Trials1996, 17(1):1– 12.\n38. De Vet HCW, De Bie RA, Van Der Heijden GJMG, Verhagen AP, Sijpkes P,\nKnipschild PG: Systematic reviews on the basis of methodological criteria.\nPhysiotherapy 1997, 83(6):284– 289.\n39. Verhagen AP, de Vet HC, de Bie RA, Kessels AG, Boers M, Bouter LM,\nKnipschild PG: The Delphi list: a criteria list for quality assessment of\nrandomized clinical trials for conducting systematic reviews developed\nby Delphi consensus.J Clin Epidemiol1998, 51(12):1235– 1241.\n40. Sherrington C, Herbert RD, Maher CG, Moseley AM:PEDro: A database of\nrandomized trials and systematic reviews in physiotherapy.Man Ther\n2000, 5(4):223– 226.\n41. Moseley AM, Herbert RD, Sherrington C, Maher CG:Evidence for\nphysiotherapy practice: a survey of the Physiotherapy Evidence\nDatabase (PEDro). Aust J Physiother2002, 48(1):43– 49.\n42. Van Tulder MW, Assendelft WJJ, Koes BW, Bouter LM, Bombardier C,\nNachemson AL, Esmail R, Deyo RA, Shekelle PG, Bouter LM,et al: Method\nguidelines for systematic reviews in the Cochrane Collaboration Back\nReview Group for spinal disorders.Spine 1997, 22(20):2323– 2330.\n43. Van Tulder M, Furlan A, Bombardier C, Bouter L:Updated method\nguidelines for systematic reviews in the Cochrane Collaboration Back\nReview Group.Spine 2003, 28(12):1290– 1299.\n44. Bizzini M, Childs JD, Piva SR:Systematic review of the quality of\nrandomized controlled trials for patellofemoral pain syndrome.J Orthop\nSports Phys Ther2003, 33(1):4– 20.\n45. Chalmers TC, Smith H Jr, Blackburn B:A method for assessing the quality\nof a randomized control trial.Control Clin Trials1981, 2(1):31– 49.\n46. Reisch JS, Tyson JE, Mize SG:Aid to the evaluation of therapeutic studies.\nPediatrics 1989, 84(5):815– 827.\n47. Andrew E: Method for assessment of the reporting standard of clinical\ntrials with roentgen contrast media.Acta Radiologica - Series Diagnosis\n1984, 25(1):55– 58.\n48. Imperiale TF, McCullough AJ: Do corticosteroids reduce mortality from\nalcoholic hepatitis? A meta-analysis of the randomized trials.Ann Intern\nMed 1990, 113(4):299– 307.\n49. Detsky AS, Naylor CD, O’Rourke K, McGeer AJ, L’Abbe KA: Incorporating\nvariations in the quality of individual randomized trials into meta-\nanalysis. J Clin Epidemiol1992, 45(3):255– 265.\n50. Cho MK, Bero LA: Instruments for assessing the quality of drug studies\npublished in the medical literature.JAMA 1994, 272(2):101– 104.\n51. Balas EA, Austin SM, Ewigman BG, Brown GD, Mitchell JA:Methods of\nrandomized controlled clinical trials in health services research.\nMed Care 1995, 33(7):687– 299.\n52. Sindhu F, Carpenter L, Seers K:Development of a tool to rate the quality\nassessment of randomized controlled trials using a Delphi technique.\nJ Adv Nurs1997, 25(6):1262– 1268.\n53. Downs SH, Black N: The feasibility of creating a checklist for the\nassessment of the methodological quality both of randomised and\nnon-randomised studies of health care interventions.J Epidemiol\nCommun Health 1998, 52(6):377– 384.\n54. Nguyen QV, Bezemer PD, Habets L, Prahl-Andersen B:A systematic review\nof the relationship between overjet size and traumatic dental injuries.\nEur J Orthodont1999, 21(5):503– 515.\n55. Smith LA, Oldman AD, McQuay HJ, Moore RA:Teasing apart quality and\nvalidity in systematic reviews: an example from acupuncture trials in\nchronic neck and back pain.Pain 2000, 86(1– 2):119– 132.\n56. Arrive L, Renard R, Carrat F, Belkacem A, Dahan H, Le Hir P, Monnier-Cholley L,\nTubiana JM:A scale of methodological quality for clinical studies of\nradiologic examinations.Radiology 2000, 217(1):69– 74.\n57. Huwiler-Muntener K, Juni P, Junker C, Egger M:Quality of reporting of\nrandomized trials as a measure of methodologic quality.JAMA 2002,\n287(21):2801– 2804.\n58. Yates SL, Morley S, Eccleston C, Williams ACDC:A scale for rating the\nquality of psychological trials for pain.Pain 2005, 117(3):314– 325.\n59. Graham N, Haines T, Goldsmith CH, Gross A, Burnie S, Shahzad U, Talovikova E:\nReliability of three assessment tools used to evaluate randomized\ncontrolled trials for treatment of neck pain.Spine 2012, 37(6):515– 522.\n60. Hartling L, Bond K, Vandermeer B, Seida J, Dryden D, Rowe B:Applying the\nRisk of Bias tool in a systematic review of combination longacting\nbetaagonists and inhaled corticosteroids for persistent asthma.PLoS Med\n2011, 6(2):e17242.\n61. Armijo-Olivo S, Stiles CR, Hagen NA, Biondo PD, Cummings GG:Assessment\nof study quality for systematic reviews: a comparison of the Cochrane\nCollaboration Risk of Bias Tool and the Effective Public Health Practice\nProject Quality Assessment Tool: methodological research.J Eval Clin\nPract 2010:1– 7.\n62. Hartling L, Hamm MP, Milne A, Vandermeer B, Santaguida PL, Ansari M,\nTsertsvadze A, Hempel S, Shekelle P, Dryden DM:Testing the Risk of Bias\ntool showed low reliability between individual reviewers and across\nconsensus assessments of reviewer pairs.J Clin Epidemiol2012.\nS0895-4356(12)00217-X. doi: 10.1016/j.jclinepi.2012.07.005.\n63. Juni P, Witschi A, Bloch R, Egger M:The hazards of scoring the quality of\nclinical trials for meta-analysis.JAMA 1999, 282(11):1054– 1060.\n64. Soares HP, Daniels S, Kumar A, Clarke M, Scott C, Swann S, Djulbegovic B:\nBad reporting does not mean bad methods for randomised trials:\nObservational study of randomised controlled trials performed by the\nRadiation Therapy Oncology Group.BMJ 2004, 328(7430):22– 24.\n65. Sanderson S, Tatt ID, Higgins JPT:Tools for assessing quality and\nsusceptibility to bias in observational studies in epidemiology: a\nsystematic review and annotated bibliography.Int J Epidemiol2007,\n36(3):666– 676.\ndoi:10.1186/1471-2288-13-116\nCite this article as:Armijo-Olivo et al.: Inconsistency in the items included\nin tools used in general health research and physical therapy to evaluate the\nmethodological quality of randomized controlled trials: a descriptive analysis.\nBMC Medical Research Methodology2013 13:116.\nArmijo-Olivo et al. BMC Medical Research Methodology2013, 13:116 Page 19 of 19\nhttp://www.biomedcentral.com/1471-2288/13/116",
  "topic": "Blinding",
  "concepts": [
    {
      "name": "Blinding",
      "score": 0.8371636867523193
    },
    {
      "name": "Comparability",
      "score": 0.6663597822189331
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.5757682919502258
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.543910562992096
    },
    {
      "name": "Medicine",
      "score": 0.5067823529243469
    },
    {
      "name": "Descriptive statistics",
      "score": 0.48503655195236206
    },
    {
      "name": "Research design",
      "score": 0.4704170525074005
    },
    {
      "name": "Inclusion and exclusion criteria",
      "score": 0.4110293686389923
    },
    {
      "name": "Medical physics",
      "score": 0.34977301955223083
    },
    {
      "name": "Statistics",
      "score": 0.3182511031627655
    },
    {
      "name": "Alternative medicine",
      "score": 0.2700805366039276
    },
    {
      "name": "Mathematics",
      "score": 0.12693047523498535
    },
    {
      "name": "Surgery",
      "score": 0.09052151441574097
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I2799535320",
      "name": "Catholic University of the Maule",
      "country": "CL"
    },
    {
      "id": "https://openalex.org/I4210126697",
      "name": "Institute of Health Economics",
      "country": "CA"
    }
  ],
  "cited_by": 55
}