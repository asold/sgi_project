{
    "title": "Generative Language Models for Paragraph-Level Question Generation",
    "url": "https://openalex.org/W4385573917",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2397341542",
            "name": "Asahi Ushio",
            "affiliations": [
                "Cardiff University"
            ]
        },
        {
            "id": "https://openalex.org/A4213678622",
            "name": "Fernando Alva-Manchego",
            "affiliations": [
                "Cardiff University"
            ]
        },
        {
            "id": "https://openalex.org/A2565581196",
            "name": "Jose Camacho Collados",
            "affiliations": [
                "Cardiff University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W803028973",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2888812214",
        "https://openalex.org/W3158086504",
        "https://openalex.org/W3101007570",
        "https://openalex.org/W4229907684",
        "https://openalex.org/W3196234484",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W4287185415",
        "https://openalex.org/W3099756172",
        "https://openalex.org/W3035500185",
        "https://openalex.org/W3100439847",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2963087285",
        "https://openalex.org/W2516930406",
        "https://openalex.org/W3034383728",
        "https://openalex.org/W2900356614",
        "https://openalex.org/W2970796366",
        "https://openalex.org/W4288106555",
        "https://openalex.org/W2988673764",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3104467951",
        "https://openalex.org/W3101017384",
        "https://openalex.org/W2995541765",
        "https://openalex.org/W1555380324",
        "https://openalex.org/W3199501816",
        "https://openalex.org/W3177334331",
        "https://openalex.org/W3100436891",
        "https://openalex.org/W1981208470",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3088049945",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2962717047",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W2164777277",
        "https://openalex.org/W4287118989",
        "https://openalex.org/W2250425483",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3035359363",
        "https://openalex.org/W3103420681",
        "https://openalex.org/W2109609717",
        "https://openalex.org/W3205718179",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3207095490",
        "https://openalex.org/W3022039293",
        "https://openalex.org/W2557764419",
        "https://openalex.org/W2962977247",
        "https://openalex.org/W3098371839",
        "https://openalex.org/W3035497479",
        "https://openalex.org/W2806532810",
        "https://openalex.org/W2606333299",
        "https://openalex.org/W2970785793",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W1531374185",
        "https://openalex.org/W2964185324",
        "https://openalex.org/W3194727116",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W2133459682",
        "https://openalex.org/W4221152489",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W3034834768"
    ],
    "abstract": "Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 670–688\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nGenerative Language Models for Paragraph-Level Question Generation\nAsahi Ushio and Fernando Alva-Manchego and Jose Camacho-Collados\nCardiff NLP, School of Computer Science and Informatics, Cardiff University, UK\n{UshioA,AlvaManchegoF,CamachoColladosJ}@cardiff.ac.uk\nAbstract\nPowerful generative models have led to recent\nprogress in question generation (QG). How-\never, it is difficult to measure advances in\nQG research since there are no standardized\nresources that allow a uniform comparison\namong approaches. In this paper, we intro-\nduce QG-Bench, a multilingual and multido-\nmain benchmark for QG that unifies existing\nquestion answering datasets by converting them\nto a standard QG setting. It includes general-\npurpose datasets such as SQuAD (Rajpurkar\net al., 2016) for English, datasets from ten do-\nmains and two styles, as well as datasets in\neight different languages. Using QG-Bench\nas a reference, we perform an extensive analy-\nsis of the capabilities of language models for\nthe task. First, we propose robust QG base-\nlines based on fine-tuning generative language\nmodels. Then, we complement automatic eval-\nuation based on standard metrics with an ex-\ntensive manual evaluation, which in turn sheds\nlight on the difficulty of evaluating QG models.\nFinally, we analyse both the domain adaptabil-\nity of these models as well as the effective-\nness of multilingual models in languages other\nthan English. QG-Bench is released along with\nthe fine-tuned models presented in the paper,1\nwhich are also available as a demo.2\n1 Introduction\nQuestion generation (QG, Mitkov and Ha, 2003)\nis the task of generating a question given an in-\nput context consisting of a document, a paragraph\nor a sentence, and an answer where the question\nis anchored (see Figure 1). QG has been widely\nstudied in natural language processing communi-\nties (Du et al., 2017; Zhou et al., 2017; Du and\nCardie, 2018), and it has recently been exploited\nto train question answering (QA) models without\nhuman supervision (Lewis et al., 2019; Zhang and\n1https://github.com/asahi417/\nlm-question-generation\n2https://autoqg.net/\nFigure 1: Overview of paragraph-level QG.\nBansal, 2019; Puri et al., 2020), or as a means of\ndata augmentation (Shakeri et al., 2020; Bartolo\net al., 2021). It has also been applied to develop\neducational systems (Heilman and Smith, 2010;\nLindberg et al., 2013), information retrieval mod-\nels (Pyatkin et al., 2021; Lewis et al., 2021), and\nfor model interpretation (Perez et al., 2020; Lee\net al., 2020).\nDespite its success in downstream applications,\nthe development of neural QG models has received\nless attention. For example, the choice of the base\npre-trained model is arbitrary (without proper justi-\nfication in most cases) as it is not straightforward to\ncompare different models. As a consequence, while\nERNIE-GEN (Xiao et al., 2021) and UniLMv2\n(Bao et al., 2020) are current SotA in the SQuAD\nQG benchmark (Du et al., 2017), T5 (Raffel et al.,\n2020) and BART (Lewis et al., 2020a) are used\nin many applications in practice (Paranjape et al.,\n2021; Bartolo et al., 2021; Lewis et al., 2021; Py-\natkin et al., 2021).\nA possible reason is inconsistent evaluation and\ncomparison of QG models, due to the lack of appro-\npriate evaluation protocols and benchmarks. For in-\nstance, evaluation of QG models relies on BLEU4\n(Papineni et al., 2002), METEOR (Denkowski and\nLavie, 2014), and ROUGE L (Lin, 2004), with\nhuman-made questions as references. However,\n670\nsome of these metrics may have low correlation\nwith human judgements, especially when it comes\nto answerability, since they tend not to take the\nassociated answer into account (Nema and Khapra,\n2018). Moreover, QG applications can use different\ncontexts as input, such as sentence-level (Pyatkin\net al., 2021; Lewis et al., 2019) vs paragraph-level\n(Zhang and Bansal, 2019; Puri et al., 2020), or\nanswer-aware (Shakeri et al., 2020; Bartolo et al.,\n2021) vs answer-free (Lopez et al., 2020). These\nare generally used interchangeably in the literature.\nTo investigate how to tackle the issues previ-\nously raised, we introduce QG-Bench, a collec-\ntion of standard QA datasets unified into a single\nbenchmark, including domain-specific datasets and\nfor eight different languages (§ 3). We then use\nQG-Bench to fine-tune various generative language\nmodels (LMs) by formulating paragraph-level QG\nas a sequence-to-sequence generation task (§ 4),\nand measure their performance on in-domain and\nlanguage-specific data (§ 5). Finally, we present a\nmulti-faceted analysis of our QG models by vary-\ning their input context size (§ 6.1), conducting a\nmanual evaluation (§ 6.2), and studying their abili-\nties for domain adaptation (§ 6.3).\n2 Related Work\nEarly work on QG was based on human-engineered\ntemplates (Mitkov and Ha, 2003; Rus et al., 2010)\nand well-designed pipelines (Heilman and Smith,\n2010; Labutov et al., 2015), but soon neural ap-\nproaches took over by generating a question from a\ntext in an end-to-end manner (Du et al., 2017; Zhou\net al., 2017; Du and Cardie, 2018). The quality of\nQG models was later improved by masked LM\npre-training (Devlin et al., 2019; Liu et al., 2019)\nwhere the encoder of the QG model is fine-tuned\nfrom pre-trained LMs (Chan and Fan, 2019; Zhang\nand Bansal, 2019). Recently, sequence-to-sequence\nLM pre-training has allowed to fully fine-tune QG\nmodels (both encoder and decoder), achieving SotA\nperformance (Dong et al., 2019; Qi et al., 2020;\nBao et al., 2020; Xiao et al., 2021). Following\nthe latest research in the literature, we focus on\nsequence-to-sequence LM-based QG models.\nQG can be applied to domain adaptation\n(Shakeri et al., 2020), knowledge-enhanced\nLM pre-training (Jia et al., 2021), adversar-\nial/counterfactual data augmentation (Bartolo et al.,\n2021; Paranjape et al., 2021), and nearest neigh-\nbour QA systems (Lewis et al., 2021). Applications\nof QG go beyond QA, including semantic role la-\nbeling (Pyatkin et al., 2021), visual QA (Krishna\net al., 2019), multi-hop question decomposition\n(Perez et al., 2020), and question rewriting (Lee\net al., 2020). Moreover, QG can be applied to\nunsupervised QA, which consists of training a QA\nmodel without any supervision and relying on a QG\nmodel to generate questions (Lewis et al., 2019).\nPuri et al. (2020) showed that with a carefully-\ndesigned QG model, we can generate high-quality\nQA datasets on which a QA model can even outper-\nform their supervised counterparts. This inspired\nZhang and Bansal (2019) to propose QA-based\nevaluation, which connects the quality of a QG\nmodel to the accuracy of a QA model trained on\nthe synthetic data generated by the QG model.\nWhile QG models can be applied to this vari-\nety of tasks, the comparison across tasks is not\nalways straightforward. For this reason, and given\nthe relevance of QG in current research, in this\npaper we propose an intrinsic QG benchmark in\nwhich we can evaluate different aspects of a QG\nmodel in a simple manner, including, but not only,\nanalysis of input types, domain adaptability and\nmultilinguality. The most similar work to ours is\nthe MTG benchmark (Chen et al., 2021), which\ncontains multilingual test sets for four NLG tasks.\nWhile QG is part of this benchmark, there are a few\nmajor differences from our proposed QG-Bench:\n(i) we provide training/validation/test sets to allow\nmodel training in each language in addition to the\nevaluation; (ii) MTG’s test set consists of parallel\nsentences across languages by a translation from\nEnglish, while we leverage monolingual datasets;\n(iii) we include eight languages, while MTG has\nfive; and (iv) QG-Bench includes datasets from\ndifferent domains and styles.\n3 QG-Bench: A Unified Question\nGeneration Benchmark\nIn this section, we describe our process to construct\nQG-Bench, including data collection and unifica-\ntion (§ 3.1), and its statistics (§ 3.2).\n3.1 Data Collection and Unification\nWe unified a collection of datasets, designed to be\nused for QG model training and evaluation. All\ndatasets are in the same format, where each en-\ntry contains four features: paragraph, sentence,\nquestion, and answer. As described in Figure 1,\nwe assume question as the output of a QG system,\n671\nwhich is conditioned by an answer and it is always\na sub-string of a sentence from a paragraph. We\nleverage existing QA datasets by compiling them\ninto this unified QG format. All datasets included\nin QG-Bench are described below.\nSQuAD (English). We first consider SQuAD v1.1\n(Rajpurkar et al., 2016), an extractive QA dataset\nbased on Wikipedia which has been used in QG\ncommonly since (Du et al., 2017; Zhou et al., 2017).\nAs the original test set of SQuAD is not released,\nwe use the same data split as in (Du et al., 2017).\nDomain-specific Datasets (English). To as-\nsess models’ domain adaptivity, we consider\ntwo domain-specific QA datasets: SQuADShifts\n(Miller et al., 2020) and SubjQA (Bjerva et al.,\n2020). SQuADShifts contains questions in the\nsame style of SQuAD but from four additional\ndomains (Amazon/Wikipedia/News/Reddit), while\nSubjQA consists, unlike SQuAD, of subjective\nquestions/answers in general (e.g. how is the hero?\n- the hero was wonderful) across six domains. As\nthe original SQuADShifts consists of test set only,\nwe created a new training/validation/test split, in\nwhich half of the dataset remains in the test set,\nwhile the remaining half is split for validation and\ntraining by a 1:2 ratio.\nDatasets in Languages other than English. To in-\nvestigate multilinguality in QG, we compile the fol-\nlowing seven SQuAD-style QA datasets: JAQuAD\n(So et al., 2022) (Japanese), GerQuAD (Möller\net al., 2021) (German), SberQuAd (Efimov et al.,\n2020) (Russian), KorQuAD (Lim et al., 2019)\n(Korean), FQuAD (d’Hoffschmidt et al., 2020)\n(French), Spanish SQuAD (Casimiro Pio et al.,\n2019) (Spanish), and Italian SQuAD (Croce et al.,\n2018) (Italian). Since they do not release test sets,\nwe sampled a subset from the training sets as the\ntest set following Du et al. (2017). The test sets\ncontain the same number of questions as its vali-\ndation set, and the new training/test splits have no\noverlap in terms of the paragraphs.\nOther Datasets not Included in QG-Bench. In\ntheory, any extractive QA dataset could be part of\nour benchmark. However, we decided not to in-\nclude datasets such as BioASQ (Tsatsaronis et al.,\n2015) and NewsQA (Trischler et al., 2017) because\nthey have very long input texts, representing an-\nother category that needs extra mechanisms to han-\ndle long sequences (Izacard and Grave, 2020a,b),\nwhich is out of the scope of this paper. In addition,\none could leverage multilingual QA benchmarks\nData size Average character length\n(train/valid/test) (para./sent./ques./ans.)\nSQuAD 75,722 / 10,570 / 11,877 757 / 179 / 59/ 20\nSubjQA\n-Book 637 / 92 / 191 1,514 / 146 / 28 / 83\n-Elec. 697 / 99 / 238 1,282 / 129 / 26 / 66\n-Grocery 687 / 101 / 379 896 / 107 / 25 / 49\n-Movie 724 / 101 / 154 1,746 / 146 / 27 / 72\n-Rest. 823 / 129 / 136 1,006 / 104 / 26 / 51\n-Trip 875 / 143 / 397 1,002 / 108 / 27 / 51\nSQuADShifts\n-Amazon3,295 / 1,648 / 4,942 773 / 111 /43 / 18\n-Wiki 2,646 / 1,323 / 3,969 773 / 184 / 58 / 26\n-News 3,355 / 1,678 / 5,032 781 / 169 / 51 / 20\n-Reddit 3,268 / 1,634 / 4,901 774 / 116 / 45 / 19\nMultilingual QG\n-Ja 27,809 / 3,939 / 3,939 424 / 72 / 32 / 6\n-Es 77,025 / 10,570 / 10,570 781 / 122 / 64 / 21\n-De 9,314 / 2,204 / 2,204 1,577 / 165 / 59 / 66\n-Ru 40,291 / 5,036 / 5,036 754 174 / 64 / 26\n-Ko 54,556 / 5,766 / 5,766 521 / 81 / 34 / 6\n-It 46,550 / 7,609 / 7,609 807 / 124 / 66 / 16\n-Fr 17,543 / 3,188 / 3,188 797 / 160 / 57 / 23\nTable 1: Statistics of of all datasets integrating into our\nquestion generation benchmark after unification.\n(Clark et al., 2020; Artetxe et al., 2020; Lewis et al.,\n2020b) to obtain multilingual QG datasets, but\nXQuAD (Artetxe et al., 2020) and MLQA (Lewis\net al., 2020b) do not contain training sets, and Ty-\ndiQA (Clark et al., 2020) contains a very small\ntraining set. Instead, we focused on monolingual\nQA datasets in each language.\n3.2 Data Statistics\nTable 1 summarizes statistics of each QG dataset af-\nter unification. It can be observed that SubjQA and\nSQuADShifts have ten to a hundred times less train-\ning data than SQuAD. Also, SubjQA’s answers are\ntwice longer than SQuAD’s answers, which can be\nexplained by how they differ in the way questions\nare formed (i.e., SubjQA being more subjective in\nnature). Likewise, except for Spanish, the datasets\nfor languages other than English contain less train-\ning data than the original SQuAD, with the number\nvarying depending on the language.\n4 LMs for Question Generation\nIn this section, we formalize the QG task from a\nlanguage modelling perspective (§ 4.1), including\ndetails on the fine-tuning process (§ 4.2) and the\nsetup for our experiments with QG-Bench (§ 4.3).\n4.1 Task Formulation\nGiven an input text x, the goal of QG is to generate\na natural question ˆq related to the information in\n672\nthe input. The task is formulated as a conditional\nsequence generation, and the model is optimized\nto maximize the conditional log-likelihood P(q|x)\nas in Equation 1.\nˆq = arg max\nq\nP(q|x) (1)\nIn practice, the log-likelihood is factorized into\nword or subword level predictions, similar to other\nsequence-to-sequence learning settings (Sutskever\net al., 2014).\n4.2 Language Model Fine-tuning\nFine-tuning sequence-to-sequence LMs on QG can\nbe done in the same way as for Machine Translation\nor Summarization, where models are trained to\npredict the output tokens given the input tokens\n(Dong et al., 2019; Qi et al., 2020; Bao et al., 2020;\nXiao et al., 2021). We follow Chan and Fan (2019)\nby introducing a highlight token <hl> to take into\naccount an answer a within a context c as below:\nx = [c1, . . . ,<hl>, a1, . . . , a|a|, <hl>, . . . , c|c|]\nInstead of a paragraph, we can similarly use a sen-\ntence to highlight an answer (sentence-level QG) or\nhighlight a sentence instead of an answer (answer-\nfree QG). We investigate these model variations in\nour analysis (§ 6.1), but assume the answer high-\nlighted paragraph as the default input.\nNote that it is possible to train other types of LMs\non QG, but masked LMs were not designed for\nnatural language generation and require a specific\ndecoding technique (Chan and Fan, 2019). Also,\nrecurrent LMs have poor ability for conditional\ngeneration on the answer due to its unidirectional\narchitecture (Lopez et al., 2020). Since they are\nnot as suited for QG as the sequence-to-sequence\nmodels, they are out of the scope of this paper.\n4.3 Experimental Setup\nComparison Models. As sequence-to-sequence\nLMs, we use T5 (Raffel et al., 2020) and BART\n(Lewis et al., 2020a) for the English datasets and\nmT5 (Xue et al., 2021) and mBART (Liu et al.,\n2020) for the multilingual experiments. Model\nweights are taken from HuggingFace (Wolf et al.,\n2020).3 Previous research reported improvements\non QG with more recent LMs (Qi et al., 2020;\n3We use t5-small, t5-base, t5-large,\nfacebook/bart-base, facebook/bart-large, and\ngoogle/mt5-small.\nXiao et al., 2021; Bao et al., 2020). We tried to\nreplicate these previous works in QG-Bench, but\nafter multiple attempts using their provided code\nand contacting the authors, this was not possible.\nNonetheless, both T5 and BART are widely used in\npractice and, as we will show, they can still provide\nstrong results with an appropriate configuration.\nParameter Optimization. We performed an ex-\ntensive exploration to find the best combination of\nhyper-parameters to fine-tune LMs on QG, which\nconsists of a two-phase search. First, we fine-tune\na model on every possible configuration from the\nsearch space for 2 epochs. The top-5 models in\nterms of BLEU4 (Papineni et al., 2002) on the vali-\ndation set are selected to continue fine-tuning until\ntheir performance plateaus.4 Finally, the model that\nachieves the highest BLEU4 on the validation set\nis employed as the final model. We used BLEU4 as\nan objective metric in our parameter optimization\nsince it is light to compute, and following previous\nwork (Du and Cardie, 2018; Dong et al., 2019; Xiao\net al., 2021). However, as we will see in our experi-\nments, future work could also explore the usage of\nalternative metrics for validation. The search space\ncontains 24 configurations, which are made up of\nlearning rates from [0.0001, 0.00005, 0.00001], la-\nbel smoothing from [0.0, 0.15], and batch size from\n[64, 128, 256, 512].5 Our experiments show that\nthis simple parameter optimization strategy signif-\nicantly improves all models’ performances by ro-\nbustly finding the best configuration for each one.6\nWe ran the parameter optimization on a machine\nequipped with two Nvidia Quadro RTX 8000. Tak-\ning SQuAD as a reference, training and evalua-\ntion took around three weeks for T5 LARGE, one\nweek for T5 BASE and mT5SMALL, three days for\nT5SMALL, one week for BARTLARGE, and four days\nfor BARTSMALL.\n5 Automatic Evaluation\nIn this section, we report the main results in QG-\nBench (§ 3), using the methodology described in\n§ 4.\n4This two-stage process is introduced due to computation\nlimitations, and we might see further improvements (even if\nsmall) if a full validation search is performed.\n5Other parameters are fixed: random seed is 1, beam size\nis 4, input token length is 512, and output token length is 34\nfor fine-tuning and 64 for evaluation.\n6See Appendix for the actual parameters found by the\noptimization procedure as well as more training details.\n673\nModel Param B4 R-L MTR BS MS\nNQG (Du et al.) 30M 12.28 39.75 16.62 - -\nUniLM (Dong et al.) 340M 22.78 51.57 25.49 - -\nUniLMv2 (Bao et al.) 110M 24.70 52.13 26.33 - -\nProphetNet (Qi et al.) 340M 23.91 52.26 26.60 - -\nERNIE-G (Xiao et al.) 340M 25.40 52.84 26.92 - -\nBARTBASE 140M 24.68 52.66 26.05 90.87 64.47\nBARTLARGE 400M 26.17 53.85 27.0791.0064.99\nT5SMALL 60M 24.40 51.43 25.84 90.45 63.89\nT5BASE 220M 26.13 53.33 26.97 90.84 64.74\nT5LARGE 770M27.21 54.13 27.70 91.00 65.29\nTable 2: QG model fine-tuning results on the test set\nof SQuAD where the best result in each metric is in\nbold face. The results in the top row group are existing\nSotA models taken from their original papers, while the\nbottom row contains our models.\n5.1 Evaluation Metrics\nTo evaluate QG models, BLEU4 (B4, Papineni\net al., 2002), METEOR (MTR, Denkowski and\nLavie, 2014), and ROUGEL (R-L, Lin, 2004) are\ncommonly used to compare the generated outputs\nagainst reference questions at sentence level. We\nalso compute BERTScore (BS, Zhang et al., 2019)\nand MoverScore (MS, Zhao et al., 2019). Both\nleverage BERT-like models on their computation,\nachieving higher correlations with human judge-\nments than other traditional metrics in various NLG\ntasks (Zhang et al., 2019; Zhao et al., 2019). To the\nbest of our knowledge, they have not been applied\nin QG evaluation before, regardless of their success\nin NLG. We use the default configuration for both\nmetrics, which make use of RoBERTaLARGE (Liu\net al., 2019) for BERTScore and DistilBERTBASE\n(Sanh et al., 2019) for MoverScore.\n5.2 Results\nSQuAD. Table 2 shows our results on the SQuAD\ntest set along with other reported results from\nthe literature. T5 LARGE provides the best results\noverall according to all automatic metrics. Even\nparameter-efficient models such as T5 BASE out-\nperform ERNIE-GEN (Xiao et al., 2021), and\nT5SMALL performs competitively with UniLMv2\n(Bao et al., 2020) with nearly half the parame-\nters. UniLMv2, in particular, was proposed as a\nhighly-effective model in spite of its light weight.\nAccording to these results, T5 SMALL is also com-\npetitive on the QG task while being significantly\nlighter than other models. While T5 attains the\nbest overall results, BART also proves competitive.\nIn fact, BARTBASE is slightly better than T5 BASE\nModel B4 R-L MTR BS MS\nEnglish\nmT5SMALL 21.65 48.95 23.83 90.01 62.75\nmT5BASE 23.03 50.67 25.18 90.23 63.60\nmBART 23.03 50.58 25.10 90.36 63.63\nRussian\nmT5SMALL 16.31 31.39 26.39 84.27 62.49\nmT5BASE 17.63 33.02 28.48 85.82 64.56\nmBART 18.80 34.18 29.30 87.18 65.88\nJapanese\nmT5SMALL 30.49 50.88 29.03 80.87 58.67\nmT5BASE 32.54 52.67 30.58 81.77 59.68\nmBART 32.16 52.95 29.97 82.26 59.88\nItalian\nmT5SMALL 7.37 21.93 17.57 80.80 56.79\nmT5BASE 7.70 22.51 18.00 81.16 57.11\nmBART 7.13 21.69 17.97 80.63 56.84\nKorean\nmT5SMALL 10.57 25.64 27.52 82.89 82.49\nmT5BASE 12.18 28.57 29.62 84.52 83.36\nmBART 10.92 27.76 30.23 83.89 82.95\nSpanish\nmT5SMALL 9.61 24.62 22.71 84.07 59.06\nmT5BASE 10.15 25.45 23.43 84.47 59.62\nmBART 9.18 24.26 22.95 83.58 58.91\nGerman\nmT5SMALL 0.43 10.08 11.47 79.90 54.64\nmT5BASE 0.87 11.10 13.65 80.39 55.73\nmBART 0.75 11.19 13.71 80.77 55.88\nFrench\nmT5SMALL 8.55 28.56 17.51 80.71 56.50\nmT5BASE 6.14 25.88 15.55 77.81 54.58\nmBART 0.72 16.40 7.78 71.48 50.35\nTable 3: QG model fine-tuning results on the test set of\nall language-specific QG-Bench datasets where the best\nresult in each language is in bold face.\nand BARTLARGE is equal to T5LARGE according to\nBERTScore. In general, it is hard to reliably com-\npare different model architectures for the QG tasks,\nas there are different possible confounding factors\nincluding the model size. To have a more complete\npicture on the final performance, we complement\nthis initial automatic evaluation in SQuAD with an\nextensive manual evaluation in § 6.2.\nLanguage-specific Datasets. Table 3 presents the\nresults on each language-specific dataset in QG-\nBench with mT5SMALL, mT5BASE, and mBART. As\nthis work introduces the first ever comprehensive\nmultilingual QG model training/evaluation, these\nresults can be viewed as baselines for future work\nin multilingual QG. Compared to results in En-\nglish SQuAD, scores in multilingual QG are mostly\nworse than the smallest English model (T5SMALL),\nwhich showcases the difficulties of non-English\nQG. Some languages are notably underperforming,\nwhich can be partially explained by the size of the\ntraining set. As we see in § 3.2, some datasets such\nas German and French have a limited amount of\ntraining instances, resulting in underfitting models\nfor those languages. In general, the low scores in\nnon-English datasets can be attributed to the under-\n674\nDomain Model B4 R-L MTR BS MSSQuADShifts\nAmazon\nBARTBASE 9.92 27.94 22.7892.7763.25\nBARTLARGE 9.80 28.69 23.79 92.49 63.31\nT5SMALL 8.41 27.04 22.17 91.89 62.11\nT5BASE 9.80 28.94 23.85 92.43 63.27\nT5LARGE 10.42 29.51 24.3992.6563.71\nWiki\nBARTBASE 11.50 29.00 26.60 93.12 65.86\nBARTLARGE12.1229.94 27.1293.39 66.22\nT5SMALL 10.90 28.18 25.95 92.63 65.04\nT5BASE 11.67 29.49 27.04 93.07 65.94\nT5LARGE 12.0430.10 27.6793.13 66.31\nNews\nBARTBASE 8.78 24.85 25.13 92.86 64.99\nBARTLARGE 8.74 25.28 25.0893.0465.02\nT5SMALL 7.71 23.43 23.70 92.20 63.71\nT5BASE 8.53 24.93 25.21 92.68 64.70\nT5LARGE 9.16 25.97 25.9893.0165.46\nReddit\nBARTBASE 8.78 26.03 22.57 92.32 62.35\nBARTLARGE 9.31 27.3123.75 92.50 62.64\nT5SMALL 7.60 24.90 21.90 91.70 61.39\nT5BASE 8.75 26.84 23.57 92.26 62.52\nT5LARGE 9.16 27.2423.97 92.43 62.74\nSubjQA\nBook\nBARTBASE 2.03 23.24 20.57 92.96 62.85\nBARTLARGE 0.00 23.71 20.6 92.84 62.45\nT5SMALL 0.00 19.77 18.52 92.40 61.46\nT5BASE 0.00 22.9521.20 93.32 63.14\nT5LARGE 0.00 23.68 20.83 92.89 62.51\nElec.\nBARTBASE 3.83 29.41 25.08 93.76 66.00\nBARTLARGE 5.18 28.87 25.17 93.51 65.68\nT5SMALL 0.00 29.65 26.95 94.18 68.29\nT5BASE 4.55 29.99 27.39 94.26 68.33\nT5LARGE 4.57 30.55 27.56 94.27 68.80\nGrocery\nBARTBASE 1.82 24.54 20.8 94.0965.76\nBARTLARGE 1.93 24.28 20.42 94.1 65.79\nT5SMALL 0.00 22.1723.3193.24 65.64\nT5BASE 0.83 15.63 19.87 90.56 61.47\nT5LARGE 1.13 17.40 20.64 91.39 63.41\nMovie\nBARTBASE 3.89 25.43 20.55 93.61 62.91\nBARTLARGE 4.21 25.92 21.64 93.23 62.4\nT5SMALL 0.00 25.76 22.54 94.08 64.63\nT5BASE 2.65 26.33 23.11 94.13 64.91\nT5LARGE 0.00 25.06 21.70 93.64 63.88\nRest.\nBARTBASE 3.43 24.26 21.3593.2362.67\nBARTLARGE 5.54 24.7722.4693.2363.57\nT5SMALL 0.00 11.72 13.21 87.81 55.42\nT5BASE 0.00 11.96 14.75 88.48 56.19\nT5LARGE 4.19 24.9421.99 93.22 63.25\nTrip\nBARTBASE 4.79 26.37 25.26 93.92 64.91\nBARTLARGE 5.66 26.5 24.32 93.85 64.02\nT5SMALL 2.49 23.91 25.56 93.75 66.57\nT5BASE 1.74 16.06 20.13 90.76 59.70\nT5LARGE 5.35 27.69 27.45 94.46 67.76\nTable 4: QG model fine-tuning results on the test set of\nSQuADShifts and SubjQA where the best result in each\nmetric is in bold face.\nlying model, so scaling up the model could lead to\nbetter performances in future work.\nDomain-specific Datasets. Table 4 shows the re-\nsults from all domain-specific datasets included in\nQG-Bench: SQuADShifts and SubjQA. Since each\ndomain contains a small training set, our main strat-\negy to achieve domain-specific QG models is to\nFigure 2: Input variations of QG models.\ninitialize their weights with a SQuAD fine-tuned\nmodel, and continue fine-tuning on the domain-\nspecific training set (more details on different strate-\ngies in § 6.3). As expected, given the subjective\nnature of the dataset, results on SubjQA are gener-\nally low for most metrics, except for BERTScore\nwhose score is even higher than in SQuAD in some\ncases. This implies that a model’s prediction may\nhave less word-overlap against the true question,\nwhile its semantics is close to the true question to\nsome extent.\n6 Analysis\nIn this section, we complement the automatic evalu-\nation with an extensive analysis on various relevant\naspects of the question generation models.\n6.1 Model Input\nIn our main experiments, the model input is the\nparagraph in which the answer is highlighted, as\ndescribed in § 4.2. Here we explore variations of\nthe QG model’s input type to understand the effect\nof different types of context. Concretely we con-\nsider two additional variants: sentence-level mod-\nels that only take as input the sentence that contains\nthe answer (instead of the whole paragraph); and\nanswer-free models that highlight the sentence in\nthe paragraph instead of the answer. Figure 2 pro-\nvides a summary of the three different input types\nanalysed.\nIn Table 5 we report automatic metrics from\nanswer-free models and sentence-level QG models\non SQuAD. In general, paragraph-based models,\nwhich use the most complete input, attain the best\noverall results. For example, answer-free T5LARGE\nperforms worse than paragraph-level T5SMALL in\nall the metrics except METEOR, which indicates\n675\nModel B4 R-L MTR BS MSAnswer-free\nBARTBASE 21.97 49.70 23.72 90.38 63.07\nBARTLARGE23.47 50.25 24.94 90.28 63.28\nT5SMALL 21.12 47.47 23.38 89.64 62.07\nT5BASE 22.86 49.51 24.52 90.03 62.99\nT5LARGE 24.27 51.30 25.67 90.41 63.97\nSent-level\nBARTBASE 23.86 51.43 25.18 90.70 63.85\nBARTLARGE23.86 51.43 25.18 90.70 63.85\nT5SMALL 23.23 50.18 24.80 90.36 63.18\nT5BASE 24.33 51.81 25.81 90.73 64.00\nT5LARGE 25.36 52.53 26.28 90.88 64.44\nPara-level\nBARTBASE 24.68 52.66 26.05 90.87 64.47\nBARTLARGE26.17 53.85 27.0791.0064.99\nT5SMALL 24.40 51.43 25.84 90.45 63.89\nT5BASE 26.13 53.33 26.97 90.84 64.74\nT5LARGE 27.21 54.13 27.70 91.00 65.29\nTable 5: QG model fine-tuning results on the test set of\nSQuAD for answer-free and sentence/paragraph-level\nQG models. The best overall result for each metric is in\nboldface.\nthe importance of the answer at question genera-\ntion. Nonetheless, not having the answer as input\nprovides competitive results, which may appear to\nbe surprising given the incomplete input. When\ncomparing sentence-level and paragraph-level, the\ndifference is reduced, but paragraph-level models\nconsistently outperform their sentence-level coun-\nterparts, even when smaller models are used. This\nimplies that models actually utilize the global con-\ntext provided by the full paragraph when it is avail-\nable, rather than the more local information within\nthe sentence only.\n6.2 Manual Evaluation\nGiven the limitations of automatic metrics in text\ngeneration research (Reiter, 2018; Bhandari et al.,\n2020; Alva-Manchego et al., 2021), we also con-\nducted a manual evaluation using Amazon Mechan-\nical Turk, focusing on three criteria: grammatical-\nity (i.e. grammatical correctness), understandabil-\nity (i.e. whether the question is easy to be under-\nstood by readers) and answerability (i.e. whether\nthe question can be answered by the given input\nanswer).7 We randomly sampled 500 unique para-\ngraphs from the SQuAD test set and selected a\nsingle answer in each paragraph. For each of the\n500 paragraph-answer pairs, we generated ques-\ntions from six QG models, and asked human anno-\n7Understandability could correlate with grammaticality,\nbut a question without any grammatical mistakes can have low\nunderstandability due to an over complex structure. Likewise,\na question can be understandable even with a few grammatical\nmistakes. Annotation guidelines are included in the Appendix.\nModel Manual Metric Automatic Metric\nAns. Gra. Und. B4 R-L MTR BS MS\nNQG 1.21 2.35 2.63 3.33 14.30 33.53 88.27 58.25\nBARTLARGE2.70 2.89 2.93 16.15 29.93 51.3590.9565.44\nT5SMALL 2.51 2.83 2.90 13.43 27.38 48.86 90.41 64.27\nT5LARGE 2.80 2.93 2.95 17.56 30.42 52.0090.9466.09\n- sent-level 2.47 2.912.9514.88 27.49 48.97 90.76 64.53\n- answer-free 2.46 2.912.9513.62 26.82 47.37 90.20 64.00\nTable 6: Manual evaluation results along with the auto-\nmatic metrics. Each score is averaged within the 500\nquestions for the evaluation where the best result in each\nmetric is in bold face.\ntators to score them for the criteria with a 3-points\nscale. Each question was evaluated by five judges,\nthus collecting a total of 15,000 human judgments.\nAs quality control, we asked workers to be native\nEnglish speakers, and instructed them to do a qual-\nification test first, and only those who passed the\ntest worked on our annotation task. The given time\nof each assignment (with each assignment contain-\ning ten instances to annotate) was 30 minutes, and\nthe reward of the annotation task was $2 per assign-\nment.8 We attach a screenshot of the annotation\ninterface in the Appendix.\nComparison Models. For the manual evaluation,\nthe target QG models include T5LARGE, T5SMALL\nand BARTLARGE based paragraph-level QG mod-\nels; T5LARGE sentence-level and answer-free QG\nmodels; and NQG (Du et al., 2017), which is based\non an LSTM-architecture. NQG is included for\ncompleteness and to better analyse the effect of\npre-trained LMs in general. T5 LARGE is our best\nmodel according to automatic metrics, so we com-\npare it against different input types (answer-free\nor sentence-level), different sizes (T5SMALL), and\ndifferent model architectures (BARTLARGE).\nInter-annotator Agreement. Since there are\nfive unique annotators per each generated ques-\ntion, we calculated Fleiss’s kappa to measure the\ninter-annotator-agreement. We obtained 0.30 and\n0.36 for grammaticality and understandability re-\nspectively, resulting in fair-agreement (Landis and\nKoch, 1977). The kappa is 0.61 in answerability,\nwhich is a substantial-agreement.\nModel-wise Evaluation. We report the results\nof our manual evaluation in Table 6, where each\nscore is averaged over the 500 questions used in the\nstudy. Answerability is the most affected by model\nsize/context and type/model architecture, compared\nto the other metrics, except for NQG, which is\n8The full price of annotation exercise was about $3,000.\n676\nFigure 3: Spearman’s rank correlation over all the gen-\nerated questions within the manual evaluation.\nthe only non-LM pre-training based approach. In\nfact, when we compare T5LARGE’s paragraph-level\nagainst sentence-level, answerability decreases un-\nlike the other two criteria, highlighting the impor-\ntance of including all relevant context available so\nthat the model can generate a suitable question. On\nthe other hand, while answer-free models are worse\nthan sentence-level models according to automatic\nmetrics, the manual evaluation does not reflect a\nsignificant difference between them. In general, we\ncan see how T5LARGE, which is the best model over-\nall according to the automatic metrics, is also the\nmost robust model overall according to the manual\nevaluation, which reinforces the conclusions from\nthe automatic evaluation.\nCorrelation Analysis. Leveraging the large dataset\nof collected human judgments, we investigate the\ncorrelation between human annotations and the\nautomatic metrics considered in the automatic eval-\nuation (§ 5.2). For this analysis, we included all\nthe generated questions from all the models consid-\nered in the manual evaluation. This means 3,000\ngenerated questions from six diverse models where\neach question receives five annotations. We took\nthe average across all the five annotators for each\ngenerated question to compute the correlation. Fig-\nure 3 shows the Spearman’s rank correlation coeffi-\ncient across the automatic metrics and the criteria\ncollected through our manual evaluation.9 The p-\nvalues of all correlations are less than 0.05, so they\nare all statistically significant. To check the sig-\nnificance of the increase in the correlation across\nmetrics, we ran a William test, showing that the\n9See the full correlation analysis in Appendix § B.2.\nFigure 4: Comparison of METEOR (MTR) scores for\nT5LARGE across in-domain fine-tuning, zero-shot trans-\nfer of SQuAD fine-tuned model, and in-domain fine-\ntuning from SQuAD model.\nincrease is statistically significant in all cases.10\nAccording to the correlation analysis, no metric\nachieved a high agreement with human judgements\nin all criteria. This means that we should not rely\non a single metric to capture all quality aspects\nof a model’s output. We can conclude, however,\nthat METEOR and MoverScore are well-aligned\nwith human judgements on answerability, while\nBERTScore appears to be better suited for gram-\nmaticality and understandability. Most importantly,\nBLEU4 and ROUGE L, which have been mostly\nused in the QG literature as default metrics, are not\nas reliable as the other metrics in any criteria.\n6.3 Domain Adaptation\nIn our main experiments in the domain-specific\ndatasets of QG-Bench (§ 5.2), models were initial-\nized by the SQuAD fine-tuned model due to the\nlimited training set in each domain. To further ex-\nplore the domain adaptability of QG models, we\ncompared three different setups: (1) fine-tuning\nin the in-domain training set without SQuAD ini-\ntialization, (2) zero-shot transfer from the SQuAD\nfine-tuned model, and (3) fine-tuning with a prior\nSQuAD initialization. Figure 4 shows the results of\nT5LARGE (the best model in most of the domains in\nTable 4 and the manual evaluation) in each domain\nfor those three settings. For this analysis, we focus\non the METEOR metric, which attains the highest\ncorrelation with human judges in answerability.11\nWe can confirm that the best setup is to initialize\nthe model on SQuAD and then further fine-tune it\n10Full results of the William test are in Appendix § B.3.\n11The full set of results for other metrics and models is\navailable in the Appendix, with similar general trends.\n677\non the domain-specific training sets. For SQuAD-\nShifts, however, this improvement is less marked in\ngeneral, suggesting that T5 can handle inputs from\ndifferent domains to a certain extent. In contrast,\nthe zero-shot setting with SQuAD fine-tuning in\nSubjQA achieves very poor results overall. This is\nto a certain extent expected since the questions in\nSubjQA are of very different styles.\nFinally, while in this section we focused on the\ndomain adaptability for English, in the Appendix\nwe also show zero-shot cross-lingual transfer re-\nsults, adapting English-training models to other\nlanguages. Similarly to previous work (Chen et al.,\n2021), the main conclusion is that there is still sig-\nnificant room for improvement for zero-shot cross-\nlingual transfer in QG.\n7 Conclusion\nIn this paper we presented QG-Bench, a unified\nbenchmark and evaluation for testing paragrah-\nlevel QG models. The benchmark is composed\nof the general-purpose SQuAD dataset, as well\nas domain-specific datasets of different styles for\nEnglish. Moreover, it includes language-specific\ndatasets for eight different languages. Using QG-\nBench as a reference, we tested recent generative\nlanguage models on the task, and evaluated them\nacross a range of automatic metrics. To comple-\nment the automatic evaluation, we performed a\ncomprehensive manual evaluation to better under-\nstand the performance of models and the role of\nautomatic metrics (e.g., our study shows there are\nbetter metrics than the popular BLEU4 when it\ncomes to QG). In general, our results show that\nLMs have come a long way for QG, being very\ncompetitive (e.g., T5 attains an overall manual\nscore of, respectively, 2.80, 2.93 and 2.95 in an-\nswerability, grammaticality and understandability\non SQuAD), but have room for improvement when\ndealing with different domains and styles, and es-\npecially on languages other than English.\nAs future work, we will continue to study QG\nevaluation metrics in-depth to better understand\nwhat aspects we are missing when we use specific\nautomatic metrics, using our manual evaluation as\na proxy. Moreover, the QG models analysed in this\npaper require an answer to be specified beforehand\nto generate the question. As a way to relax the\nconstrain, we can train models for question and\nanswer pair generation (QAG) by generating the\nanswer together with the question given a context.\nBy generating both answers and questions together,\nnew evaluation metrics would also be required to\nunderstand the validity and diversity of the answers\nselected, which we leave for future work.\nLimitations\nIn this paper, we explored paragraph-level QG mod-\nels, which limits their input up to around 500 to-\nkens, and the same methodology cannot be eas-\nily applied to longer documents. In multilingual\nQG modeling, we considered datasets in seven dif-\nferent languages, but all of them are medium- to\nhigh-resource languages, so our experimental re-\nsults cannot be generalized to a truly low-resource\nlanguage setting. Finally, although the focus of our\npaper is mostly in SQuAD-style one hop extractive\nQA, QG is also studied in more complex scenarios\nsuch as multi-hop QG with graph neural networks\n(Pan et al., 2020) and QG for very long answers\n(Cao and Wang, 2021). Moreover, QG models are\nused to attain better interpretability in question an-\nswering such as multi-hop question decomposition\n(Perez et al., 2020) and question rewriting (Lee\net al., 2020). As future work, we will expand our\nanalysis to more complex scenarios and explore the\nconnectivity with the QA task.\nEthics Statement\nAs the potential risk at using our QG models, it has\nbeen reported that language models inherit unde-\nsirable biases and generate toxic language (Schick\net al., 2021), and one could find such text in the gen-\nerated question. However, we internally checked\nthe generated questions used for the manual evalu-\nation, and confirmed that they did not contain toxic\ncontent.\nAcknowledgements\nJose Camacho-Collados is supported by a UKRI\nFuture Leaders Fellowship.\nReferences\nFernando Alva-Manchego, Carolina Scarton, and Lucia\nSpecia. 2021. The (un)suitability of automatic evalu-\nation metrics for text simplification. Computational\nLinguistics, 47(4):861–889.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\n678\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. In International Conference on Ma-\nchine Learning, pages 642–652. PMLR.\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian\nRiedel, Pontus Stenetorp, and Douwe Kiela. 2021.\nImproving question answering model robustness with\nsynthetic adversarial data generation. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 8830–8848, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nManik Bhandari, Pranav Narayan Gour, Atabak Ash-\nfaq, Pengfei Liu, and Graham Neubig. 2020. Re-\nevaluating evaluation in text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9347–9359, Online. Association for Computa-\ntional Linguistics.\nJohannes Bjerva, Nikita Bhutani, Behzad Golshan,\nWang-Chiew Tan, and Isabelle Augenstein. 2020.\nSubjQA: A Dataset for Subjectivity and Review Com-\nprehension. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 5480–5494, Online. Association\nfor Computational Linguistics.\nShuyang Cao and Lu Wang. 2021. Controllable open-\nended question generation with a new question type\nontology. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6424–6439, Online. Association for Computa-\ntional Linguistics.\nCarrino Casimiro Pio, Costa-jussa Marta R., and Fonol-\nlosa Jose A. R. 2019. Automatic Spanish Translation\nof the SQuAD Dataset for Multilingual Question An-\nswering. arXiv e-prints, page arXiv:1912.05200v1.\nYing-Hong Chan and Yao-Chung Fan. 2019. A recur-\nrent BERT-based model for question generation. In\nProceedings of the 2nd Workshop on Machine Read-\ning for Question Answering, pages 154–162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nYiran Chen, Zhenqiao Song, Xianze Wu, Danqing\nWang, Jingjing Xu, Jiaze Chen, Hao Zhou, and Lei Li.\n2021. Mtg: A benchmarking suite for multilingual\ntext generation. arXiv preprint arXiv:2108.07140.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454–470.\nDanilo Croce, Alexandra Zelenanska, and Roberto\nBasili. 2018. Neural learning for question answering\nin italian. In AI*IA 2018 – Advances in Artificial\nIntelligence, pages 389–402, Cham. Springer Inter-\nnational Publishing.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language specific translation evaluation\nfor any target language. In Proceedings of the Ninth\nWorkshop on Statistical Machine Translation, pages\n376–380, Baltimore, Maryland, USA. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMartin d’Hoffschmidt, Wacim Belblidia, Quentin\nHeinrich, Tom Brendlé, and Maxime Vidal. 2020.\nFQuAD: French question answering dataset. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1193–1208, Online. Association\nfor Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in Neural Information Process-\ning Systems, 32.\nXinya Du and Claire Cardie. 2018. Harvest-\ning paragraph-level question-answer pairs from\nWikipedia. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1907–1917, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1342–1352,\nVancouver, Canada. Association for Computational\nLinguistics.\nPavel Efimov, Andrey Chertok, Leonid Boytsov, and\nPavel Braslavski. 2020. Sberquad–russian reading\ncomprehension dataset: Description and analysis.\nIn International Conference of the Cross-Language\nEvaluation Forum for European Languages, pages\n3–15. Springer.\n679\nMichael Heilman and Noah A. Smith. 2010. Good\nquestion! statistical ranking for question generation.\nIn Human Language Technologies: The 2010 An-\nnual Conference of the North American Chapter of\nthe Association for Computational Linguistics, pages\n609–617, Los Angeles, California. Association for\nComputational Linguistics.\nGautier Izacard and Edouard Grave. 2020a. Distilling\nknowledge from reader to retriever for question an-\nswering.\nGautier Izacard and Edouard Grave. 2020b. Leveraging\npassage retrieval with generative models for open\ndomain question answering.\nRobin Jia, Mike Lewis, and Luke Zettlemoyer. 2021.\nQuestion answering infused pre-training of general-\npurpose contextualized representations. arXiv\npreprint arXiv:2106.08190.\nRanjay Krishna, Michael Bernstein, and Li Fei-Fei.\n2019. Information maximizing visual question gener-\nation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n2008–2018.\nIgor Labutov, Sumit Basu, and Lucy Vanderwende.\n2015. Deep questions without deep understanding.\nIn Proceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 889–898,\nBeijing, China. Association for Computational Lin-\nguistics.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics, pages 159–174.\nDong Bok Lee, Seanie Lee, Woo Tae Jeong, Dongh-\nwan Kim, and Sung Ju Hwang. 2020. Gener-\nating diverse and consistent QA pairs from con-\ntexts with information-maximizing hierarchical con-\nditional V AEs. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 208–224, Online. Association for\nComputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel.\n2019. Unsupervised question answering by cloze\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4896–4910, Florence, Italy. Association for\nComputational Linguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020b. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Min-\nervini, Heinrich Küttler, Aleksandra Piktus, Pontus\nStenetorp, and Sebastian Riedel. 2021. PAQ: 65 mil-\nlion probably-asked questions and what you can do\nwith them. Transactions of the Association for Com-\nputational Linguistics, 9:1098–1115.\nSeungyoung Lim, Myungji Kim, and Jooyoul Lee. 2019.\nKorquad1. 0: Korean qa dataset for machine reading\ncomprehension. arXiv preprint arXiv:1909.07005.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nDavid Lindberg, Fred Popowich, John Nesbit, and Phil\nWinne. 2013. Generating natural language questions\nto support learning on-line. In Proceedings of the\n14th European Workshop on Natural Language Gen-\neration, pages 105–114, Sofia, Bulgaria. Association\nfor Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLuis Enrico Lopez, Diane Kathryn Cruz, Jan Chris-\ntian Blaise Cruz, and Charibeth Cheng. 2020.\nTransformer-based end-to-end question generation.\narXiv preprint arXiv:2005.01107, 4.\nJohn Miller, Karl Krauth, Benjamin Recht, and Ludwig\nSchmidt. 2020. The effect of natural distribution\nshift on question answering models. In International\nConference on Machine Learning, pages 6905–6916.\nPMLR.\nRuslan Mitkov and Le An Ha. 2003. Computer-aided\ngeneration of multiple-choice tests. In Proceedings\nof the HLT-NAACL 03 Workshop on Building Edu-\ncational Applications Using Natural Language Pro-\ncessing, pages 17–22.\nTimo Möller, Julian Risch, and Malte Pietsch. 2021.\nGermanquad and germandpr: Improving non-english\nquestion answering and passage retrieval.\n680\nPreksha Nema and Mitesh M. Khapra. 2018. Towards\na better metric for evaluating question generation\nsystems. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3950–3959, Brussels, Belgium. Association\nfor Computational Linguistics.\nLiangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng\nChua, and Min-Yen Kan. 2020. Semantic graphs\nfor generating deep questions. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1463–1475, Online. Asso-\nciation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nBhargavi Paranjape, Matthew Lamm, and Ian Tenney.\n2021. Retrieval-guided counterfactual generation for\nqa. arXiv preprint arXiv:2110.07596.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8864–8880, Online. Association for Computational\nLinguistics.\nRaul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa\nPatwary, and Bryan Catanzaro. 2020. Training\nquestion answering models from synthetic data. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5811–5826, Online. Association for Computa-\ntional Linguistics.\nValentina Pyatkin, Paul Roit, Julian Michael, Yoav Gold-\nberg, Reut Tsarfaty, and Ido Dagan. 2021. Asking\nit all: Generating contextualized questions for any\nsemantic role. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1429–1441, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,\nNan Duan, Jiusheng Chen, Ruofei Zhang, and Ming\nZhou. 2020. ProphetNet: Predicting future n-gram\nfor sequence-to-SequencePre-training. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 2401–2410, Online. Association\nfor Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nEhud Reiter. 2018. A structured review of the validity of\nBLEU. Computational Linguistics, 44(3):393–401.\nVasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,\nSvetlana Stoyanchev, and Christian Moldovan. 2010.\nThe first question generation shared task evaluation\nchallenge. In Proceedings of the 6th International\nNatural Language Generation Conference. Associa-\ntion for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for re-\nducing corpus-based bias in NLP. Transactions of the\nAssociation for Computational Linguistics, 9:1408–\n1424.\nSiamak Shakeri, Cicero Nogueira dos Santos, Henghui\nZhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh\nNallapati, and Bing Xiang. 2020. End-to-end syn-\nthetic data generation for domain adaptation of ques-\ntion answering systems. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5445–5460, On-\nline. Association for Computational Linguistics.\nByungHoon So, Kyuhong Byun, Kyungwon Kang, and\nSeongjin Cho. 2022. Jaquad: Japanese question an-\nswering dataset for machine reading comprehension.\narXiv preprint arXiv:2202.01764.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Har-\nris, Alessandro Sordoni, Philip Bachman, and Kaheer\nSuleman. 2017. NewsQA: A machine comprehen-\nsion dataset. In Proceedings of the 2nd Workshop\non Representation Learning for NLP, pages 191–200,\nVancouver, Canada. Association for Computational\nLinguistics.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the bioasq large-scale\nbiomedical semantic indexing and question answer-\ning competition. BMC bioinformatics, 16(1):1–28.\nShuohang Wang and Jing Jiang. 2016. Machine compre-\nhension using match-lstm and answer pointer. arXiv\npreprint arXiv:1608.07905.\n681\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nDongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. Ernie-gen:\nan enhanced multi-flow pre-training and fine-tuning\nframework for natural language generation. In Pro-\nceedings of the Twenty-Ninth International Confer-\nence on International Joint Conferences on Artificial\nIntelligence, pages 3997–4003.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nZhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu,\nWilliam W Cohen, and Ruslan Salakhutdinov. 2017.\nWords or characters? fine-grained gating for reading\ncomprehension. In ICLR (Poster).\nShiyue Zhang and Mohit Bansal. 2019. Address-\ning semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2495–2509, Hong Kong,\nChina. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNational CCF Conference on Natural Language Pro-\ncessing and Chinese Computing , pages 662–671.\nSpringer.\n682\nA Parameter Optimization\nA.1 Best Parameters\nModel Epoch LearningBatchGradient Label\nRate Steps Smoothing\nAnswer-aware Model (paragraph-level)\nBARTBASE 7 0.0001 32 8 0.15\nBARTLARGE 4 0.00005 32 4 0.15\nT5SMALL 9 0.0001 64 1 0.15\nT5BASE 5 0.0001 16 4 0.15\nT5LARGE 6 0.00005 16 4 0.15\nAnswer-aware Model (sentence-level)\nBARTBASE 3 0.0001 64 2 0.15\nBARTLARGE 8 0.00005 32 16 0.15\nT5SMALL 8 0.0001 64 1 0.15\nT5BASE 8 0.0001 64 1 0.15\nT5LARGE 6 0.00005 16 4 0.15\nAnswer-free Model\nBARTBASE 4 0.0001 32 8 0.15\nBARTLARGE 4 0.00005 32 4 0.15\nT5SMALL 7 0.0001 64 4 0.15\nT5BASE 8 0.0001 16 4 0.15\nT5LARGE 7 0.00005 16 4 0.15\nTable 7: The best parameter to fine-tune each model on\nSQuAD we found through the parameter optimization.\nTable 7 shows the best configuration to fine-tune\neach model that we obtain through the parameter\noptimization process. To fine-tune T5 model, we\nuse the task prefix generate question: at the\nbeginning of the input text.\nA.2 Fine-tuning without Optimization\nModel B4 R-L MTR BS MS\nBARTBASE -0.28 -0.17 -0.07 -0.01 0.00\nBARTLARGE-2.22 -1.65 -1.16 -0.06 -0.57\nT5SMALL -1.73 -1.89 -1.16 -0.28 -0.82\nT5BASE -0.72 -0.58 -0.39 -0.10 -0.28\nT5LARGE -0.18 -0.15 0.00 -0.07 -0.09\nTable 8: Decrease in automatic metrics of our QG mod-\nels without parameter optimization .\nTable 8 shows the decrease in each metric for\nSQuAD if the model is fine-tuned without parame-\nter optimization.12 We observe decent drops in per-\nformance. T5SMALL and BARTLARGE lose around\n2 points in BLEU4 and ROUGE L. According to\nthese results, we infer that T5 and BART were\nworse than more recent LMs (ProphetNet, UniLM,\nor ERNIE-GEN) in QG just because they were\nunder-fitted to the task due to sub-optimal fine-\ntuning parameters, rather than they being inferior\n12We follow the hyperparameter used to fine-tune ERNIE-\nGEN on SQuAD QG in the original paper.\nto those recent LMs in terms of learning the QG\ntask.\nB Manual Evaluation\nB.1 Sample Outputs\nTable 9 presents a few examples of our model\npredictions with the scores made by the anno-\ntators, where the samples are chosen from the\nhigh-answerability and low-answerability groups\nof T5LARGE.\nB.2 Spearman’s Correlation\nFigure 5: Spearman’s rank correlation within manual\nevaluation criteria.\nFigure 6: Spearman’s rank correlation within automatic\nevaluation metrics among the 500 samples we used in\nSQuAD manual annotation.\nFigure 5 and Figure 6 show Spearman’s rank\ncorrelation across automatic metrics and manual\nevaluation criteria among the questions we generate\nover SQuAD test set for the manual annotation.\nThe p-values of all those correlation are less than\n0.05 so they are statistically significant.\n683\nParagraph Question Model Prediction Answerability Grammaticality Understandability\nThe British Library notes that \"Chopin’s works have been recorded by allthe great pianists of the recording era.\"The earliest recording was an1895performance by Paul Pabst of the Nocturne in E major Op. 62No. 2.The British Library site makes available a number of historicrecordings, including some by Alfred Cortot, Ignaz Friedman, VladimirHorowitz, Benno Moiseiwitsch, Paderewski, Arthur Rubinstein, XaverScharwenka and many others. A select discography of recordings ofChopin works by pianists representing the various pedagogic traditionsstemming from Chopin is given by Methuen-Campbell in his work tracingthe lineage and character of those traditions.\nWhen did Pabstrecord his Chopinperformance?\nNQG What year was the earliestrecording of the earliest record-ing?\n1 2.6 1.6\nBARTLARGEWhen was the earliest recordingof Chopin’s Nocturne in E ma-jor?\n2.8 3 3\nT5SMALLWhen did Paul Pabst performthe Nocturne in E major Op. 62No. 2?\n2.8 3 3\nT5LARGEWhen was the earliest recordingof Chopin’s Nocturne in E ma-jor Op. 62 No. 2?\n3 3 3\n-sent-levelWhen was the earliest recordingof Beethoven’s Nocturne in Emajor?\n1 3 3\n-answer-freeIn what year did Paul Pabstrecord Chopin’s Nocturne in Emajor Op. 62 No. 2?\n3 3 3\nBaird Television Ltd. made Britain’s first television broadcast, on 30September 1929 from its studio in Long Acre, London, via the BBC’sLondon transmitter, using the electromechanical system pioneered byJohn Logie Baird. This system used a vertically-scanned image of 30lines – just enough resolution for a close-up of one person, and with abandwidth low enough to use existing radio transmitters.Simultaneoustransmission of sound and picture was achieved on 30March 1930,by using the BBC’s new twin transmitter at Brookmans Park.By late1930, 30 minutes of morning programmes were broadcast Monday toFriday, and 30 minutes at midnight on Tuesdays and Fridays, after BBCradio went off the air. Baird broadcasts via the BBC continued until June1932.\nHow many linesmade up the pictureon Britain’s earliestTV broadcasts?\nNQG When did the UK’s first televi-sion broadcast? 1 2.6 2.2\nBARTLARGEHow many lines did BairdTelevision Ltd.’s first televisionbroadcast use?\n3 3 2.6\nT5SMALLOn what date did Baird Televi-sion make Britain’s first televi-sion broadcast?\n1 3 3\nT5LARGEOn what date did Baird Televi-sion Ltd. make Britain’s firsttelevision broadcast?\n1 3 3\n-sent-levelWhen was Britain’s first televi-sion broadcast? 1 3 3\n-answer-freeWhen did Baird Television Ltd.make Britain’s first televisionbroadcast?\n1 3 2.8\nTable 9: Examples of the system outputs along with their scores from the manual evaluation. The sentence and\nanswer are highlighted by boldface and underline in the paragraph.\nData F1 Exact Match\nBARTBASE 70.10 58.46\nBARTLARGE70.40 58.60\nT5SMALL 68.90 56.96\nT5BASE 70.33 58.14\nT5LARGE 70.86 59.04\nTable 10: Unsupervised QA-based evaluation results\nof our answer-aware QG models (paragraph-level). All\nresults are the performance on the validation set of orig-\ninal SQuAD by the model trained on the synthetic data\ngenerated by each QG model.\nB.3 William test\nIn § 6.2, we run correlation analysis and here we\nreport the result of the William test to check the\nsignificance of the increase in the correlation across\nmetrics in Figure 7, showing that the increase is\nstatistically significant as well.\nB.4 Guidelines\nFigure 8 shows an example of user interface we\nimplemented for our manual evaluation and the\nguideline we present to the annotators is attached\nto the end of the paper.\nC Unsupervised QA-based Evaluation\nAs a proxy for answerability, we run an unsu-\npervised QA-based evaluation (Zhang and Bansal,\n2019), which trains a QA model on synthetic data\ngenerated by the target QG model and evaluates the\nQA model on a human annotated test set. As an al-\nternative to the traditional metrics in QG, Q-metric\n(Nema and Khapra, 2018) shows high agreement in\nterms of the answerability, but we prefer to employ\nQA-based evaluation (Zhang and Bansal, 2019),\nsince it is more closely tied to downstream appli-\ncations, while Q-metric relies on some heuristics\nsuch as the number of named-entity/pre-defined\nquestion types. This evaluates the QG model’s ca-\npability to generate high quality questions: higher\naccuracy of the QA model indicates a better QG\nmodel. The synthetic data is usually generated over\nthe paragraph and answer (PA) pairs collected by\nDu and Cardie (2018). Zhang and Bansal (2019)\nused a small subset of the PA pairs, since they con-\ntain 12x larger instances than the SQuAD training\nset. Since this introduces an artifact of the sub-\nset choice, we decided to train QA models on the\nentire PA pairs set with the generated questions.\nAlso, we train QA models solely on the synthetic\ndata, which differs from work in semi-supervised\nQA where the QA model is trained on a concatena-\ntion of the synthetic data and the original SQuAD\ntraining set (Lee et al., 2020).\nThe synthetic QA data is created by generating\na question for each of the one million PA pairs\n(Du and Cardie, 2018) with the target QG model.\n684\n(a) answerability\n(b) grammaticality\n(c) understandability\nFigure 7: Williams test on the difference in the correla-\ntion reported in Figure 3. The difference of correlation\nis significant if the value is less than 0.005.\nWe then fine-tune BERT (Devlin et al., 2019)13 on\nthe synthetic QA data with the default configura-\ntion used in the HuggingFace’s tutorial to fine-tune\nBERT on QA.14 We report F1 score and the ex-\nact match on the SQuAD validation set, following\nZhang and Bansal (2019).15\nThe results of our unsupervised QA-based evalu-\nation in Table 10 indicate that the QA model accu-\nracy correlates with the size of QG model that gen-\nerated the synthetic data, as in T5LARGE realizes the\nbest QA model in both of F1 and the exact match,\nwhich is as good as the supervised non-language\nmodel based QA models (Wang and Jiang, 2016;\n13We use bert-base-cased from HuggingFace.\n14https://github.com/huggingface/transformers/\ntree/master/examples/pytorch/question-answering\n15We will release the synthetic data we made on Hugging-\nface Dataset https://huggingface.co/datasets.\nYang et al., 2017). Also, the small models such as\nT5SMALL and BARTBASE produce QA models with\na small decrease in performance, which exhibits\nthe efficiency of our models, similarly to our results\nwith automatic metrics.\nD Additional Analysis\nD.1 Zero-shot Multilingual Transfer\nData B4 R-L MTR BS MS\nSQuAD 21.65 48.95 23.83 90.01 62.75\nRu 0.00 0.99 1.78 70.89 49.10\nJa 0.00 6.08 0.51 66.08 46.53\nIt 0.54 5.01 5.89 72.60 50.23\nKo 0.00 0.06 0.73 66.34 45.86\nEs 0.59 5.21 6.02 74.94 50.62\nDe 0.00 1.56 4.81 73.53 50.37\nFr 1.71 15.84 8.24 72.91 50.96\nTable 11: Zero-shot result of mT5 fine-tuned on SQuAD\nexcept for the first row, which shows fine-tuning result\nof SQuAD.\nWe fine-tune multilingual language model on\neach of multilingual QG dataset in § 5.2, and here\nwe explore the zero-shot multilingual transfer by\nevaluating English fine-tuned QG model in other\nlanguages. Table 11 shows the zero-shot transfer\nresult where we fine-tune mT5SMALL on SQuAD\nand evaluate it on the test set of multilingual QG\ndataset. Compared with Table 3, the performance\nis largely decreased, indicating the difficulty of\nzero-shot multilingual transfer in QG.\nD.2 Zero-shot Domain Transfer\nFigure 9 shows the comparison of zero-shot QG\ntransfer in SQuADShifts and SubjQA dataset with\nT5LARGE.\n685\nFigure 8: An example of the interface used in our manual evaluation.\nFigure 9: Metric comparison for T5 LARGE across in-\ndomain fine-tuning, zero-shot transfer of SQuAD fine-\ntuned model, and in-domain fine-tuning from SQuAD\nmodel.\n686\nQuestion Evaluation Guideline\nIn this project, we aim to study the quality of questions generated by automatic models.You will be given the following 4 pieces of information to complete the evaluation.\n- Passage: Passage consisting of multiple sentenceswith the information requiredto answer the question.- Answer: Answer to the question. This is usually anentity that appears in thepassage.- Question : Question written by our model based on thepassage and the answer.\nGoal\nWe ask you to evaluateq u e s t i o n s based on the following4 criteria: (1) Grammaticality, (2)Understandability, (3) Correctness, and (4) Question Difficulty.\n(1) Grammaticality\nYou will score a question in terms of its grammatical correctness with a 3-point scale.- 3: The question is grammatically correct.- 2: The question has some minor errors/typos but you can still understand it.- 1: The question contains many errors and you can not understand it.You shouldonly rely on the questionand do not referto any other information such as thepassage and the answer.\n(2) Understandability\nYou will score how understandable a question is with a 3-point scale.- 3: The question is easy to read and understand what you are being asked.- 2: The question is somewhat complicated to understand, but you can get an idea of whatthe question is asking.- 1: The question is too complex and you can not understand what should be the answerto the question.Understandability could correlate with grammaticality, buta question without any grammaticalmistakes can have low understandabilitydue to anover complex structure. Likewise, aquestion can be easy to understand even with a few grammatical mistakes. You can refer thepassage if it’s needed.\n687\n(3) Correctness\nWe generate a question in a way that its answer should be the answer. Here, you will evaluatewhether the answer to The question matches the answer, given the passage.- 3: The answer to the question is exactly the given answer.- 2: The answer to the question might be the given answer but no clear evidence can befound in the passage, or the question is too vague. Or the question is not relevant to thepassage.- 1: The answer to the question is not the given answer.\nTo better understand when you judge a question as 2, let’s look at the following example:- Passage: Max was raised in LA …- Answer: LA- Question: Where was Max born?While the answer “LA” could be an answer to the question, it is not entirely accuratesince thepassage does not explicitly state that Max was born in LA. A more correct question couldbe “Where was Max raised?”. In these situations, you could score the question with a 2.\nThe answer can sometimes be partial but it is fine. The answer in the following example shouldbe `June 6th 1992` instead of `June` but you should score it with a 3 as the question still makessense with the answer June.- Passage: Max was born on June 6th 1992, and …- Answer: June- Question: When was Max born?\nIn some cases, the question matches the answer while it is completely irrelevant to thepassage. For example,- Passage: China spans five geographical time zones and borders 14 different countries,the second most of any country in the world after Russia.- Answer: China- Question: What is the name of the world's most populous country?Although the question matches the answer, it is based on common knowledge rather than anyevidence found in the passage, so this question should be scored as 2.\nIn addition, if the Understandability of the question is 1, you should mark its Correctness as 1.\n688"
}