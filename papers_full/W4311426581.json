{
  "title": "Automatically Generating CS Learning Materials with Large Language Models",
  "url": "https://openalex.org/W4311426581",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4308503988",
      "name": "MacNeil, Stephen",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A4284226270",
      "name": "Tran, Andrew",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A4227959803",
      "name": "Leinonen, Juho",
      "affiliations": [
        "Aalto University"
      ]
    },
    {
      "id": "https://openalex.org/A3172309270",
      "name": "Denny, Paul",
      "affiliations": [
        "University of Auckland"
      ]
    },
    {
      "id": "https://openalex.org/A2355939496",
      "name": "Kim, Joanne",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A4227959804",
      "name": "Hellas, Arto",
      "affiliations": [
        "Aalto University"
      ]
    },
    {
      "id": "https://openalex.org/A2675690879",
      "name": "Bernstein, Seth",
      "affiliations": [
        "Temple University"
      ]
    },
    {
      "id": "https://openalex.org/A4227959802",
      "name": "Sarsa, Sami",
      "affiliations": [
        "Aalto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963562081",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W4323033814",
    "https://openalex.org/W4283317394",
    "https://openalex.org/W4283705032",
    "https://openalex.org/W4211263275",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W2197966456",
    "https://openalex.org/W2064829224",
    "https://openalex.org/W2158534319",
    "https://openalex.org/W2077561308",
    "https://openalex.org/W1488572404",
    "https://openalex.org/W3047890686",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2079172301",
    "https://openalex.org/W1546198282",
    "https://openalex.org/W2106606489"
  ],
  "abstract": "Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt. Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts. These advances may enable students to interact with code in new ways while helping instructors scale their learning materials. However, LLMs also introduce new implications for academic integrity, curriculum design, and software engineering careers. This workshop will demonstrate the capabilities of LLMs to help attendees evaluate whether and how LLMs might be integrated into their pedagogy and research. We will also engage attendees in brainstorming to consider how LLMs will impact our field.",
  "full_text": "Automatically Generating CS Learning Materials with Large\nLanguage Models\nStephen MacNeil\nTemple University\nPhiladelphia, PA, USA\nstephen.macneil@temple.edu\nAndrew Tran\nTemple University\nPhiladelphia, PA, USA\nandrew.tran10@temple.edu\nJuho Leinonen\nAalto University\nEspoo, Finland\njuho.2.leinonen@aalto.fi\nPaul Denny\nThe University of Auckland\nAuckland, New Zealand\npaul@cs.auckland.ac.nz\nJoanne Kim\nTemple University\nPhiladelphia, PA, USA\njoanne.kim@temple.edu\nArto Hellas\nAalto University\nEspoo, Finland\narto.hellas@aalto.fi\nSeth Bernstein\nTemple University\nPhiladelphia, PA, USA\nseth.bernstein@temple.edu\nSami Sarsa\nAalto University\nEspoo, Finland\nsami.sarsa@aalto.fi\nABSTRACT\nRecent breakthroughs in Large Language Models (LLMs), such as\nGPT-3 and Codex, now enable software developers to generate code\nbased on a natural language prompt. Within computer science edu-\ncation, researchers are exploring the potential for LLMs to generate\ncode explanations and programming assignments using carefully\ncrafted prompts. These advances may enable students to interact\nwith code in new ways while helping instructors scale their learn-\ning materials. However, LLMs also introduce new implications for\nacademic integrity, curriculum design, and software engineering\ncareers. This workshop will demonstrate the capabilities of LLMs to\nhelp attendees evaluate whether and how LLMs might be integrated\ninto their pedagogy and research. We will also engage attendees in\nbrainstorming to consider how LLMs will impact our field.\nCCS CONCEPTS\n• Social and professional topics→Computing education; • Com-\nputing methodologies→Natural language generation .\nKEYWORDS\nlarge language models, explanations, computer science education\nACM Reference Format:\nStephen MacNeil, Andrew Tran, Juho Leinonen, Paul Denny, Joanne Kim,\nArto Hellas, Seth Bernstein, and Sami Sarsa. 2023. Automatically Generating\nCS Learning Materials with Large Language Models. In Proceedings of the\n54th ACM Technical Symposium on Computing Science Education V. 2 (SIGCSE\n2023), March 15–18, 2023, Toronto, ON, Canada. ACM, New York, NY, USA,\n3 pages. https://doi.org/10.1145/3545947.3569630\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\n© 2023 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9433-8/23/03.\nhttps://doi.org/10.1145/3545947.3569630\n1 INTRODUCTION\nEducational technology can have a transformational effect on teach-\ning and learning in computer science classrooms. Intelligent tutor-\ning systems provide students with real-time formative feedback on\ntheir work to help them get unstuck [17] when peers and instruc-\ntors are not available. Online tutorials and videos have enabled\ninstructors to ‘flip their classes’ and consider new methods for con-\ntent delivery [6, 13], making necessary space for active learning\nand collaboration during class time. Anchored collaboration [4, 12]\nand subgoal labeling [ 3, 11] have created more engaging online\nlearning spaces where students co-construct their knowledge and\nbuild on each other’s ideas. Clicker quizzes and peer instruction\nmethods enable instructors to evaluate students’ misconceptions\nwithin large classes in real-time [2]. Technology advances and edu-\ncation technology can not only improve classroom experiences but\nalso create new models and opportunities for teaching and learning.\nLarge language Models (LLMs) are similarly poised to impact\ncomputer science classrooms. LLMs are machine learning models\nthat are trained on a large amount of text data. These models are\ndesigned to learn the statistical properties of language in order to\npredict the next word in a sequence or generate new text. LLMs\nare capable of natural language understanding and text generation\nwhich enables many use cases ranging from creative story writ-\ning [22] to using LLMs to write about themselves [20]. In computer\nscience classroom settings, LLMs have the potential to provide\nhigh-quality code explanations for students at scale [14, 15, 19].\nIn this workshop, we will demonstrate LLMs’ capabilities to\ninspire instructors and researchers to consider how this new tech-\nnology might integrate with their existing pedagogy. We also will\ndiscuss the potential impacts that LLMs might have on curricula\nand students’ careers. Given that LLMs can generate code based\non a natural language prompt, the skills and job requirements may\nchange for software engineers. Software engineers may take on\nmore design-oriented roles and serve as software architects while\nLLMs write (most of) the source code. This might lead to courses\nthat focus on prompt engineering, code evaluation, and debugging.\narXiv:2212.05113v1  [cs.CY]  9 Dec 2022\nSIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada MacNeil, et al.\nFigure 1: An example code snippet with an explanation gen-\nerated using GPT-3.\n1.1 Generating Explanations of Code\nHigh-quality code explanations enable students to better under-\nstand code and efficiently learn programming concepts [16]. Com-\nmon automated methods to explain code snippets and coding con-\ncepts include tracing program execution [7], defining terms [9], pro-\nviding hints [18], and presenting feedback on errors [16, 18]. These\ntechniques either leverage heuristics which limits their generaliz-\nability or rely on instructors to manually configure and pregenerate\ncontent. However, LLMs have the potential to scale these efforts\nand generalize to multiple languages and programming contexts.\nOur research team has developed code-explanations with the LLMs\nCodex [19] and GPT-3 [14, 15] which resulted in the development\nof a design space for LLM-generated code explanations [15].\n1.2 Generating Assignments\nStudents benefit from frequent hands-on practice with program-\nming assignments [10]. Assignments are most engaging when they\nare personalized toward a student’s personal interests [8] and when\nthey provide sufficient instructions and examples. However, it is\ntime-consuming to create and maintain high-quality assignments.\nPrevious researchers have techniques to automatically generate\nassignments, but they require instructors to build and maintain\ntemplates [21]. To provide high-quality assignments at scale, our\nteam has developed prompts to generate programming assignments\nusing OpenAI Codex [19]. Based on these prior experiences, we\nwill share best practices with attendees.\n1.3 Generating Code\nLarge language models have the potential to change the roles and\nresponsibilities of software developers. For example, GitHub’s Copi-\nlot can generate code for programmers based on natural language\nprompts [1]. The generated code is high enough quality to lead\nresearchers to raise concerns about cheating [5]. This ability to gen-\nerate high-quality code may affect software engineering jobs. Engi-\nneers may be expected to elicit code requirements, write prompts,\nand debug the resulting code. In the workshop, we will explore how\nLLMs may affect the way we prepare our students.\n2 WORKSHOP ATTENDEES\nOur workshop is designed primarily with educators and researchers\nin mind; however, we plan to encourage student attendees at SIGCSE\nto participate in our workshop and share their perspectives. Our\nresearch team consists of faculty, researchers, and undergraduate\nstudents to provide a balanced perspective and to make the work-\nshop welcoming to attendees at various points in their careers. We\nhave considered additional methods to make our workshop an in-\nclusive space. We greet attendees and create space for them to share\ntheir names and the pronouns they use. We will also provide build-\ning information including the nearest gender-neutral bathroom,\nelevator, and quiet rooms to reduce barriers to participation.\n3 SCHEDULE\nThe goal of our workshop is to give participants an awareness of\nthe capabilities of LLMs to support their pedagogy, to get practice\nusing LLMs and learn best practices in prompt engineering, and to\nbrainstorm with their colleagues the ways large language models\ncan support their pedagogy.\n•Pre-workshop Activities:We will share a tutorial to guide\nattendees through creating Github Copilot and OpenAI ac-\ncounts with sample prompts to try on their own before the\nworkshop. Free credits are currently available.\n•Introductions (20 mins):The team and attendees intro-\nduce themselves. Attendees will engage in a speed dating\nactivity to get to know others one-on-one.\n•Demonstration (10 mins):Our team will demonstrate the\ncapabilities of GPT-3 (supported by materials).\n•Guided Activity 1 (20 mins):Participants will work in\npairs to solve programming assignments with Github Copi-\nlot.\n•Break (10 mins)\n•Guided Activity 2 (25 mins):Participants will work in\npairs to generate code explanations using GPT-3.\n•Guided Activity 3 (25 mins):Participants will work in\npairs to create programming assignments using Codex.\n•Break (10 mins)\n•Group brainstorming (25 mins):As a think-pair-share\nactivity, attendees will work in pairs on a shared Miro board\nto brainstorm ideas for integrating LLMs into their courses.\n•Exploratory learning (25):attendees will use our resources\nto further explore LLMs, explore ways to realize brainstormed\nideas, and explore LLMs beyond the workshop activities, in-\ncluding testing unique prompt ideas.\n•Debrief (10 mins):Summarization of the workshop and\nkey insights, initiating collaborations, etc.\n4 DISSEMINATING WORKSHOP RESULTS\nOur workshop team will create a website leading up to the work-\nshop which will host and maintain resources on using LLMs in\nCS classrooms. After the workshop, we will update the website\nwith content from the Miro boards and a joint reflection written by\nworkshop organizers on the challenges and opportunities identified\nby attendees. This idea is inspired by existing websites with advice\nfor CS Teaching (e.g.: https://www.csteachingtips.org/).\n5 ORGANIZERS\nOur team has extensive experience using LLMs to write code [5],\ngenerate code explanations [14, 15], and create assignments [19].\nAutomatically Generating CS Learning Materials with Large Language Models SIGCSE 2023, March 15–18, 2023, Toronto, ON, Canada\nFigure 2: An example code snippet with an analogy-based explanation generated.\nOur team includes three faculty members, two researchers, and\nthree undergraduate students who have all published research on\nusing LLMs for CS Education. We strongly believe that this diverse\nconvergence of faculty, researchers, and students will be essential\nto ensure that LLMs have the most positive potential impact on\ncomputing education.\nREFERENCES\n[1] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[2] Catherine H Crouch and Eric Mazur. 2001. Peer instruction: Ten years of experi-\nence and results. American journal of physics 69, 9 (2001), 970–977.\n[3] Adrienne Decker, Briana B Morrison, and Lauren Margulieux. 2020. Using subgoal\nlabeling in teaching introductory programming. Journal of Computing Sciences\nin Colleges 35, 8 (2020), 249–251.\n[4] Brian Dorn, Larissa B Schroeder, and Adam Stankiewicz. 2015. Piloting TrACE:\nExploring spatiotemporal anchored collaboration in asynchronous learning. In\nProceedings of the 18th ACM Conference on Computer Supported Cooperative Work\n& Social Computing . 393–403.\n[5] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope-\nnAI Codex on Introductory Programming. In Australasian Computing Education\nConference (Virtual Event, Australia) (ACE ’22). ACM, New York, NY, USA, 10–19.\nhttps://doi.org/10.1145/3511861.3511863\n[6] Michail N. Giannakos, John Krogstie, and Nikos Chrisochoides. 2014. Reviewing\nthe Flipped Classroom Research: Reflections for Computer Science Education.\nIn Proceedings of the Computer Science Education Research Conference (Berlin,\nGermany) (CSERC ’14) . Association for Computing Machinery, New York, NY,\nUSA, 23–29. https://doi.org/10.1145/2691352.2691354\n[7] Philip J Guo. 2013. Online python tutor: embeddable web-based program visual-\nization for cs education. In Proceeding of the 44th ACM technical symposium on\nComputer science education . 579–584.\n[8] Michael Haungs, Christopher Clark, John Clements, and David Janzen. 2012.\nImproving first-year success and retention through interest-based CS0 courses. In\nProceedings of the 43rd ACM technical symposium on Computer Science Education .\n589–594.\n[9] Andrew Head, Codanda Appachu, Marti A Hearst, and Björn Hartmann. 2015.\nTutorons: Generating context-relevant, on-demand explanations and demonstra-\ntions of online code. In 2015 IEEE Symposium on Visual Languages and Human-\nCentric Computing (VL/HCC) . IEEE, 3–12.\n[10] Lars Josef Höök and Anna Eckerdal. 2015. On the Bimodality in an Introductory\nProgramming Course: An Analysis of Student Performance Factors. In 2015\nInternational Conference on Learning and Teaching in Computing and Engineering .\n79–86. https://doi.org/10.1109/LaTiCE.2015.25\n[11] Juho Kim, Robert C Miller, and Krzysztof Z Gajos. 2013. Learnersourcing subgoal\nlabeling to support learning from how-to videos. In CHI’13 Extended Abstracts\non Human Factors in Computing Systems . 685–690.\n[12] L Kolodner. 1998. Integrating and guiding collaboration: Lessons learned in\ncomputer-supported collaborative learning research at Georgia Tech.Proceedings\nof Computer Support for Collaborative Learning’97 (cscl’97) (1998), 91.\n[13] Michael J. Lee and Amy J. Ko. 2015. Comparing the Effectiveness of Online\nLearning Approaches on CS1 Learning Outcomes. In Proceedings of the Eleventh\nAnnual International Conference on International Computing Education Research\n(Omaha, Nebraska, USA) (ICER ’15) . Association for Computing Machinery, New\nYork, NY, USA, 237–246. https://doi.org/10.1145/2787622.2787709\n[14] Stephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul Denny,\nSeth Bernstein, and Juho Leinonen. 2023. Experiences from Using Code Explana-\ntions Generated by Large Language Models in a Web Software Development E-\nBook. In Proceedings of the ACM Technical Symposium on Computing Science Edu-\ncation (Toronto, Canada). ACM, 6 pages. https://doi.org/10.1145/3545945.3569785\n[15] Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and Ziheng\nHuang. 2022. Generating Diverse Code Explanations Using the GPT-3 Large\nLanguage Model. In Proceedings of the 2022 ACM Conference on International\nComputing Education Research - Volume 2 (Lugano and Virtual Event, Switzerland)\n(ICER ’22) . Association for Computing Machinery, New York, NY, USA, 37–39.\nhttps://doi.org/10.1145/3501709.3544280\n[16] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020.\nAdaptive Immediate Feedback Can Improve Novice Programming Engagement\nand Intention to Persist in Computer Science. In Proceedings of the 2020 ACM\nConference on International Computing Education Research (Virtual Event, New\nZealand) (ICER ’20) . Association for Computing Machinery, New York, NY, USA,\n194–203. https://doi.org/10.1145/3372782.3406264\n[17] Samiha Marwan, Nicholas Lytle, Joseph Jay Williams, and Thomas Price. 2019.\nThe Impact of Adding Textual Explanations to Next-Step Hints in a Novice Pro-\ngramming Environment. In Proceedings of the 2019 ACM Conference on Innovation\nand Technology in Computer Science Education (Aberdeen, Scotland Uk) (ITiCSE\n’19). Association for Computing Machinery, New York, NY, USA, 520–526.\n[18] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards\nintelligent tutoring in novice programming environments. In Proceedings of the\n2017 ACM SIGCSE Technical Symposium on computer science education . 483–488.\n[19] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. 2022. Automatic\nGeneration of Programming Exercises and Code Explanations Using Large Lan-\nguage Models. In Proceedings of the 2022 ACM Conference on International Com-\nputing Education Research - Volume 1 (Lugano and Virtual Event, Switzerland)\n(ICER ’22) . Association for Computing Machinery, New York, NY, USA, 27–43.\nhttps://doi.org/10.1145/3501385.3543957\n[20] Almira Osmanovic Thunström and Steinn Steingrimsson. 2022. Can GPT-3 write\nan academic paper on itself, with minimal human input? (2022).\n[21] Akiyoshi Wakatani and Toshiyuki Maeda. 2015. Automatic generation of pro-\ngramming exercises for learning programming language. In 2015 IEEE/ACIS 14th\nInternational Conference on Computer and Information Science (ICIS) . 461–465.\n[22] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft:\nStory Writing With Large Language Models. In 27th International Conference on\nIntelligent User Interfaces . 841–852.",
  "topic": "Brainstorming",
  "concepts": [
    {
      "name": "Brainstorming",
      "score": 0.6882539987564087
    },
    {
      "name": "Computer science",
      "score": 0.5919146537780762
    },
    {
      "name": "Code (set theory)",
      "score": 0.4733160734176636
    },
    {
      "name": "Software engineering",
      "score": 0.4201643764972687
    },
    {
      "name": "Mathematics education",
      "score": 0.366071492433548
    },
    {
      "name": "Engineering ethics",
      "score": 0.3442229628562927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2617274820804596
    },
    {
      "name": "Programming language",
      "score": 0.25520843267440796
    },
    {
      "name": "Engineering",
      "score": 0.2238386869430542
    },
    {
      "name": "Psychology",
      "score": 0.18264737725257874
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84392919",
      "name": "Temple University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I9927081",
      "name": "Aalto University",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I154130895",
      "name": "University of Auckland",
      "country": "NZ"
    }
  ]
}