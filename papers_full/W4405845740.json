{
  "title": "Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation",
  "url": "https://openalex.org/W4405845740",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3006566125",
      "name": "Andrei Lazarev",
      "affiliations": [
        "Moscow Institute of Physics and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5106066828",
      "name": "Dmitrii Sedov",
      "affiliations": [
        "National Research University Higher School of Economics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2020278455",
    "https://openalex.org/W2218641061",
    "https://openalex.org/W2964061924",
    "https://openalex.org/W2775521672"
  ],
  "abstract": "The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.",
  "full_text": "Author's Accepted Manuscript\nThis is the version of the article accepted for publication in SUMMA 2024 after peer review. The final, published version is \navailable at IEEE Xplore.\nTo cite this article, please use the following official publication details:\nA. Lazarev and D. Sedov, \"Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest \nCreation,\" 2024 6th International Conference on Control Systems, Mathematical Modeling, Automation and Energy \nEfficiency (SUMMA), Lipetsk, Russian Federation, 2024, pp. 317-321, doi: 10.1109/SUMMA64428.2024.10803746\nDOI: https://doi.org/10.1109/SUMMA64428.2024.10803746 \nIEEE Copyright Notice:\n© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any \ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new \ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other \nworks.\nUtilizing Modern Large Language Models (LLM) \nfor Financial Trend Analysis and Digest Creation\nAndrei Lazarev\n Department of Control and Applied \nMathematics\nMoscow Institute of Physics and \nTechnology\nMoscow, Russia\nORCID:0009-0008-1519-5820\nDmitrii Sedov\nInstitute for Statistical Studies and \nEconomics of Knowledge \nHSE University\nMoscow, Russia\nORCID:0009-0009-0964-7576 \nAbstract—\nThe exponential growth of information presents a significant \nchallenge for researchers and professionals seeking to remain at \nthe  forefront  of  their  fields  and  this  paper  introduces  an \ninnovative framework for automatically generating insightful \nfinancial digests using the power of Large Language Models \n(LLMs), specifically Google's Gemini Pro. \nBy  leveraging  a  combination  of  data  extraction  from \nOpenAlex,  strategic  prompt  engineering,  and  LLM-driven \nanalysis, we demonstrate the automated example of creating a \ncomprehensive  digests  that  generalize  key  findings,  identify \nemerging trends. This approach addresses the limitations of \ntraditional analysis methods, enabling the efficient processing of \nvast amounts of unstructured data and the delivery of actionable \ninsights in an easily digestible format. \nThis paper describes how LLMs work in simple words and \nhow we can use their power to help researchers and scholars \nsave their time and stay informed about current trends. Our \nstudy includes step-by-step process, from data acquisition and \nJSON  construction  to  interaction  with  Gemini  and  the \nautomated generation of PDF reports, including a link to the \nproject's  GitHub  repository  for  broader  accessibility  and \nfurther development.\nKeywords—LLM, Gemini, Large Language Model, NLP, text  \nsummarization\nI. INTRODUCTION\nIn  recent  years,  Natural  Language  Processing  (NLP) \ntechnologies have made significant advancements, opening \nnew possibilities for analyzing large volumes of textual data. \nOne of the most promising areas of NLP application is the \nanalysis of  current trends  and  publications, especially in \ndynamic and  information-rich fields such  as  finance and \nrelated areas. Financial markets are characterized by a vast and \ncomplex information flow, mixing everything from breaking \nnews and analytical reviews to sophisticated scientific articles. \nWhile traditional market data plays a crucial role, academic \nresearch adds a layer of depth and these articles, authored by \neconomists, mathematicians, and computer scientists, explore \nintricate  market  dynamics,  new  algorithms,  and  emerging \ntrends often invisible to the naked eye.\nThis work aims to explore the application of NLP methods \nfor analyzing current trends and publications, on an example \nof creation of a financial digest generated entirely by large \nlanguage model (LLM) Gemini Pro. We also review main \nNLP  models  and  algorithms  applicable  to  the  analysis  of \nfinancial texts.\nTherefore,  this  research  aims  to  demonstrate  the \ncapabilities of NLP in the context of financial trend analysis, \nwhich  can  find  widespread use  among  both  professional \nanalysts and individual investors seeking more informed and \ntimely decision-making.\nII. JUSTIFICATION FOR ANALYZING TRENDS AND\nPUBLICATIONS\nToday the volume of publicly available information is \ngrowing very rapidly.”IDC estimates that the amount of the \nglobal datasphere subject to data analysis will grow by a factor \nof 50 to 5.2ZB in 2025; the amount of analyzed data that is \n“touched” by cognitive systems will grow by a factor of 100 to \n1.4ZB in 2025” [1]. This includes scientific articles, reports, \nnews articles, analytical reviews, and social media posts. To \nmake  justified  decisions,  especially  in  fields  like  finance, \nbusiness, and technology, it's crucial to identify and analyze \ncurrent trends promptly. However, traditional methods of \nanalyzing  publications,  which  involve  manual  review  and \nprocessing, are becoming increasingly inefficient. Here are \nseveral  key  aspects  that  highlight  the  need  for  automated \nanalysis of trends and publications:\nA. Volume and velocity of information\nData Volume: millions of articles, reports, and news items \nare published daily. Manually tracking and analyzing such a \nvast amount of information is simply infeasible.\nSpeed of updates: trends and market conditions change \nrapidly, and  identifying changes promptly can  provide a \nsignificant competitive advantage, but  without  automated \nanalysis, information may become outdated before it is even \nreviewed.\nB. Data complexity and the need for structuring \ninformation\nVariety of sources: Information comes from a multitude of \nsources, including scientific journals, blogs, news websites, \nand social media platforms. Each source may present data in \ndifferent formats, making integration and analysis difficult.\nUnstructured  data:  A  significant  portion  of  valuable \ninformation  exists  in  textual  form  (unstructured  data), \nrequiring the application of sophisticated Natural Language \nProcessing (NLP) techniques for analysis and structuring.\nC. Supporting data-driven decision making \nData-driven decisions: analyzing trends and publications  \nwith NLP facilitates making decisions based on objective data \nand up-to-date information rather than intuition or outdated  \nanalysis methods.\nImplementing  an  evidence  based  approach:  In  various \nfields, such as finance, medicine, and science, an evidence-\nbased approach requires constant monitoring and evaluation of \nnew data to ensure the relevance of decisions made.\nIII. THE ROLE OF NLP MODELS IN AUTOMATIC KEY \nINFORMATION EXTRACTION\nWith the advancement of Natural Language Processing  \n(NLP) technologies, automatic extraction of key information \nfrom  textual  data  has  become  a  critical  area  in  artificial \nintelligence.  These  models  enable  the  processing  of  large \nvolumes of data, automatically extracting essential insights  \nthat can be used for analysis, decision-making, or generating  \nnew knowledge. This chapter explores the main approaches  \nand  techniques  used  by  NLP  models  for  automatic  key \ninformation extraction, providing an overview of scientific  \nresearch and its findings.\nA. Named Entity Recognition (NER)\nOne  of  the  primary  tasks  in  information  extraction  is \nNamed Entity Recognition (NER), which involves identifying \nand classifying mentions of people, organizations, locations,  \ndates, and other key entities in text. NER models are trained on \nannotated data, enabling them to automatically identify and  \nclassify  entities  in  the  text.  This  process  is  crucial  in \napplications such as automatic text annotation, information  \nretrieval, and data organization.\nExamples in Table I. illustrate how NER can be applied  \nacross various domains to extract structured information from \nunstructured text, making it easier to analyze and understand  \nthe content.\nTABLE I. NER EXAMPLES\nExample Sentence Named Entity Term\nSenator Tim Jones  \ngave a speech at the \nWhite  House,  US \non January 3, 2018, \nregarding \nMicrosoft's  new  \npolicies.\nPerson Tim Jones\nLocation White House, US\nOrganization Microsoft\nDate January 3, 2018\nElon  Musk  \ncommented  on  \nTwitter  that  the  \nresearch conducted  \nby  MIT  \ndemonstrated  the  \neffectiveness  of  \nCRISPR-Cas9  in  \ngene editing.\nPerson Elon Musk\nOrganization MIT\nOrganization Twitter\nTechnology CRISPR-Cas9\nIn research study \"A survey of Named Entity Recognition \nand Classification\" David Nadeau and Satoshi Sekine write  \nthat “NERC will have a profound impact in our society. Early \ncommercial initiatives are already modifying the way we use \nyellow pages by providing local search engines (search your  \nneighborhood for organizations, product and services, people, \netc.). NERC systems also enable monitoring trends in the huge \nspace of textual media produced every day by organizations,  \ngovernments and individuals. It is also at the basis of a major \nadvance  in  biology  and  genetics,  enabling  researchers  to \nsearch the abundant literature for interactions between named \ngenes and cells.” [2]\nB. Automatic Text Summarization\nText  summarization,  a  key  application  of  Natural  \nLanguage Processing (NLP), aims to break lengthy documents \ninto brief summaries without sacrificing essential information. \nThis process, utilized by news aggregators, legal researchers, \nand others dealing with large volumes of text has two primary \ntechniques: extractive summarization, which identifies and  \nextracts key sentences from the original text, and abstractive  \nsummarization, which leverages NLP models to paraphrase  \nand  generate  new  sentences  that  capture  the  document's \ncentral themes. Both approaches strive to significantly reduce \nreading time while preserving the core message and meaning \nof the source material. [3, 4]\n1) Extractive Summarization:  Extractive summarization  \nworks  by  identifying  and  selecting  the  most  important \nsentences  or  phrases  directly  from  the  source  text.  The \nselected segments are then compiled into a summary. This  \nmethod relies heavily on statistical and linguistic features,  \nsuch as word frequency, sentence position, and the presence  \nof  specific  keywords.  Extractive  summarization  is  often \npreferred due to its simplicity and the fact that it typically  \nresults in grammatically correct summaries because it uses  \nactual sentences from the original text. \n2) Abstractive  Summarization:  Abstractive  \nsummarization, on the other hand, involves generating new  \nsentences that capture the main ideas of the source text, rather \nthan just extracting segments. This method mimics how a  \nhuman might summarize a document, by understanding the  \ncontext  and  generating  new  text  that  conveys  the  same \nmeaning but with fewer words. Abstractive summarization is \nmore challenging due to the need for advanced understanding \nand generation capabilities, and it often involves techniques  \nfrom deep learning, such as sequence-to-sequence models  \nand transformers.\nHere  in  Table  II  below  is  an  example  of  how  \nsummarization  works  and  differences  the  models  have \nbetween each other.\nTABLE II. ABSTRACTIVE SUMMARIZATION EXAMPLES\nSentence Summarization Result Explanation\nA  powerful  earthquake  struck  \ncentral  California  on  Tuesday, \ncausing  widespread  damage  to \nbuildings and infrastructure. The  \nmagnitude 6.2 quake hit at 2:15  \np.m. local time, according to the  \nU.S.  Geological  Survey.  Its  \nepicenter  was  located  about  10 \nmiles  north  of  the  town  of  \nHollister,  which  suffered  \nsignificant  damage.  Rescue  \nworkers  are  on  the  scene  \nassessing  the  damage  and  \nsearching for potential survivors.  \nThe governor has declared a state  \nof  emergency  for  the  affected \nareas.\nAbstractive\nA  strong  earthquake  hit  central  California, \nwith  the  epicenter  near  Hollister,  causing \nextensive damage and prompting a state of  \nemergency  declaration.  Rescue  efforts  are \nunderway.\nAbstractive  summaries  rephrase  information, \noften  using  different  words  and  sentence  \nstructures.\nAbstractive models aim for a more fluent and  \nhuman readable summary and this method is  \nmore  complex,  requiring  advanced  NLP  \nmodels.\nIt can be exposed to factual errors if the model  \nmisinterprets the source text.\nExtractive\nA  magnitude  6.2  earthquake  struck  central \nCalifornia  on  Tuesday,  causing  widespread \ndamage. Its epicenter was located about 10  \nmiles  north  of  Hollister,  which  suffered  \nsignificant damage. The governor has declared \na state of emergency.\nIn extractive summarization, the most important \nsegments  are  pulled  directly  from  the  text \nwithout rephrasing.\nThese model results sometimes sound choppy or \nlack smooth transitions.\nThis method is generally easier to implement  \nthan abstractive summarization and\nextractive  model  works  best  when  key  \ninformation  is  concentrated  in  specific  \nsentences.\nC. Relation Extraction\nRelation  extraction  is  part  of  NLP,  which  focuses  on \nidentifying and categorizing semantic relationships between  \nentities mentioned in the text. This technique enables NLP  \nmodels not only to recognize individual entities but also to  \nunderstand how they are interconnected, providing deeper  \ninsights into the data. For example, in medical texts, this could \nbe the relationship between a drug and a disease, while in legal \ndocuments,  it  could  be  the  relationship  between  contract \nparties. [5]\n1) Types of Relationships in Relation Extraction.\nRelation Extraction typically involves the identification of \nvarious types of relationships, including but not limited to:\na) Binary Relationships\nIn Table III there is a type of relations which involve two \nentities.\nTABLE III. ENTITIES RELATIONS EXAMPLES\nSentence Entities Relation Type Extracted \nRelation\nMike  and  John \nwork  for  Tesla \nInc.\nMike  (Person),  \nJohn (Person),\nTesla Inc. (ORG)\nEmployment Person  →  \nORG\nObama was born \nin Hawaii.\nObama  (Person), \nHawaii \n(Location)\nBirthplace Person  →  \nLocation\nIn this case, the relationship is \"work for\", and the two  \nentities  involved  are  Mike  (Person)  and  Tesla  Inc.  \n(Organization).  While  two  people  (Mike  and  John)  are \nmentioned, each forms an individual binary relationship with \nTesla Inc. So, for both Mike and John, the relationship \"work \nfor\" connects a person with the organization.\nb) N-ary Relationships\nMore complex relationships that involve more than two  \nentities, such as \"patient-disease-treatment\" in medical texts  \nare illustrated in Table IV.\nTABLE IV. N-ARY RELATIONS EXAMPLES\nSentence Entities Relation \nType\nExtracted \nRelation\nDr.  Pakler  \nprescribed \nantibiotic to treat  \nDiana’s  bacterial \ninfection\nDr.  Pakler  \n(Person),\nantibiotic (Drug),\nDiana (Person),\nbacterial  infection \n(Disease)\nMedical\nPerson  →  \nDrug  →  \nPerson  →  \nDisease\nThe  study  found \nthat  the  \ncombination  of  \naspirin  and  \nclopidogrel \nreduced  the  risk \nof stroke in high-\nrisk patients.\naspirin (Drug),\nclopidogrel (Drug), \nstroke (Disease),\nhigh-risk  patients  \n(Group)\nMedical\nDrug  →  \nDisease  →  \nGroup\nc) Hierarchical Relationships\nHierarchical relationships describe parent-child or part-\nwhole  relations,  they  are  often  seen  in  classification, \nontologies and organizational charts. Most common relations \nare: “is-a” , “part-of” or “member-of” (group of something)  \nrelations. Some examples are shown in Table V.\nTABLE V. HIERARCHICAL RELATIONSHIPS EXAMPLES\nSentence Entities Relation \nType\nExtracted \nRelation\nA  rose  is  a \ntype  of  \nflower.\nrose (plant),\nflower (plant category) classification is-a\nHead is a part  \nof the human  \nbody.\nhead (component),\nbody (whole object) hierarchy part-of\n2) Two main approaches to Relation Extraction.\na) Supervised Learning Approaches.\nSupervised methods rely on labeled training data to learn  \nthe relationships between entities. These models typically use \nfeatures derived from the text, such as the words between the \nentities, their positions in the sentence, and dependency parse \ntrees.\nb) Unsupervised and Semi-supervised Approaches.\nThese  approaches  attempt  to  extract  relations  without \nextensive labeled data. Unsupervised methods cluster similar \nsentences  and  then  infer  possible  relationships.  Semi-\nsupervised methods, like bootstrapping, start with a small  \namount of labeled data and iteratively expand the training set.\nIV. GEMINI PRO 1.5 MODEL BY GOOGLE\nGemini pro is a LLM (Large Language Model) which was \ntrained on a massive dataset of text and code, incorporating  \nwithin itself a wide variety of information. This model is  \ncapable of  generating human-quality text, translating text to  \ndifferent languages, writing fictional or creating content and  \nanswering questions.\nA. Key aspects of  the model\nGemini  is  a  Large  Language  Model which  uses  a \nTransformer-based Neural Network and this architecture is  \ngood for  processing sequential data like language, allowing it \nto understand context and relationships between words in a  \nsentence or even across paragraphs.\n1) Transformer-based  Neural  Network: is  a  type  of \nartificial  intelligence  that  excels  at  understanding  \nrelationships between words in a sentence, even if they are far \napart, it does this using a mechanism called \"attention,\" which \nallows it to focus on the most relevant parts of the input when \ngenerating  output.  This  makes  Transformers  incredibly \npowerful  for  tasks  like  language  translation,  text  \nsummarization, and building conversational AI models like  \nGemini.\n2) Attention  Mechanism:  this  is  the  heart  of  a  \nTransformer, it allows the model to focus on specific parts of \nthe input sequence that are most relevant to the task at hand.\nExample of Attention Mechanism: In the sentence \"The cat \nsat on the mat,\" the attention mechanism would understand  \nthat \"cat\" is closely related to \"sat\" and \"mat,\" even though  \nthey are not right next to each other.\nV. FROM THEORY TO ACTION: USING GEMINI AND \nOPENALEX DATA TO MAKE AI FINANCIAL DIGEST\nA. Why is it important?\nIn  previous  sections,  we  explored  the  theoretical  \nfoundations  of  neural  networks.  Now,  let's  examine  how \nneural networks can empower researchers by keeping them  \nabreast  of  the  latest  scientific  trends  and  noteworthy \npublications.\nThis  digest  addresses  the  challenges  of  information \noverload and rapid change in the financial world, it provides  \nconcrete and actionable insights of what is going on in this  \nfield of study for little effort from the researcher.\nBy using AI to analyze data and identify patterns before  \nthey become mainstream, the digest reveals emerging trends, \nit helps professionals make informed choices, supporting data-\ndriven decision-making.\nB. Process of creation Financial Digest\nThere are 3 steps to create such report:\n1) Input Data Processing: how Gemini model processes  \nthe input data sourced from OpenAlex.\n2) Information Extraction and it’s generalization.\n3) Conducting automated report.\nThe creation of our AI-powered Financial Digest begins  \nwith  meticulous  data  gathering  and  preparation.  Here's  a \ndetailed look at Input Data processing step:\n Fetch data from OpenAlex's Open Library:\nWe use the OpenAlex platform, a vast, publicly available \ndatabase  of  scholarly  literature.  Especifically,  we  extract \nresearch article abstracts focused on finance and emerging  \nmarkets published within a specific timeframe - typically the \nprevious  month,  with  a  delay  of  2-3  weeks  to  allow  for \nsufficient data collection and indexing.\n Defining the Scope:\nTo  ensure  relevance,  we  employ  targeted  queries  to \npinpoint articles classified under the Field of Study (FOS)  \ncategories of \"finance\". This focused approach ensures we  \ngather information directly relevant to our target audience.\n Storing and Preparing the Data:\nThe collected abstracts are stored securely and organized  \nfor efficient processing in postgres database, we need to make \na structured JSON of this data to make it understandable for  \nGemini.\nAfter we conduct proper JSON for Gemini we can proceed \nto Information Extraction and it’s generalization step\n Engaging Gemini with Strategic Prompts:\nOn this step we send constructed JSON into Gemini large \nlanguage model, but we don't just provide the raw data – we  \nguide Gemini's analysis through carefully crafted prompts.  \nThese prompts act as instructions, directing Gemini to extract \nspecific information and insights for different sections of the  \nFinancial Digest.\nExample Prompts:\na) \"Summarize the key findings  in all abstract parts of \nthis  article  from  the  provided  research  articles.  Please \ndescribe widely and citate most interesting.\",\nb) \"What are the main themes or trends emerging from \nthese articles? Take into account only abstracts (values)\",\nc) \"Can you identify any commonalities or connections \nbetween the different research papers?\",\nd) \"What  are  the  major  implications  for  future \ndirections  suggested  by  this  research?  Describe  future \npossibilities.\"\n Output integration:\nSo we send JSON data and Gemini processes the JSON  \ndata and prompts, generating human-readable text as output.  \nThese outputs full with insights and analysis are stored to form \nthe basis of the different sections of our Financial Digest.\nVI. CONDUCTING PDF DOCUMENT\nWe  dynamically create a title page  for our Financial  \nDigest using a Python library like ReportLab or PyFPDF. The \ntitle page includes: title, period covered (which automatically \nextracted from your data processing stage) and authors.\nStructuring  the  Digest:  The  outputs  generated  by  \nGemini, now forming the sections of our digest, are formatted \nusing headings, subheadings, and bullet points for clarity and \nreadability. We add section breaks and page numbers for  \nbetter navigation within the PDF document, conduct list of  \ncontents and list of sourced articles with their titles and DOIs.\nIncorporating  Visualizations  (Optional):  If  your  \nanalysis includes charts or graphs generated from the data,  \nthese  can  be  automatically  incorporated  into  the  relevant \nsections of the PDF using libraries like matplotlib or seaborn.\nSaving the PDF: Finally, the entire document, including \nthe title page, structured content, and optional visualizations, \nis saved as a PDF file, filename also programmatically created, \ne.g.  “FinancialDigest_YYYY_MM.pdf\"  to  ease  \nidentification.\nGitHub Repository: The code for this entire process, from \ndata acquisition and JSON construction to Gemini interaction \nand  PDF  generation,  is  available  in  our  public  GitHub \nrepository, researchers and developers can explore the code in \na GitHub repository [6].\nVII. CONCLUSION\nThis  research  explored  the  potential  of  harnessing  the \npower  of  modern  Large  Language  Models  (LLMs),  \nspecifically  Google's  Gemini,  to  automate  the  creation  of \ninsightful financial digests. In recent years, the volume and  \ncomplexity  of  information  have  grown  rapidly  [1]  and \ntraditional methods of analysis are becoming less efficient  \nwhile they still require a lot of time. \nBy automating this process, we created an example of how \nmodern  LLM  can  help  generalize  data  to  simplify  its \nconsumption and help researchers and scholars do their job.  \nModern  LLMs  process  vast  amounts  of  unstructured  data \nabout Field of Study, identify emerging trends, and synthesize \nkey findings into readily digestible formats, such as PDF.\nThis  work  demonstrated  a  practical  framework  for \nrealizing this vision. By combining data extraction from open-\naccess  repositories  like  OpenAlex,  strategic  prompt  \nengineering, and LLM-driven analysis, we can automate the  \ncreation of comprehensive financial digests. These digests,  \nenriched with synthesized summaries, trend identification, and \npotential  future  implications,  offer  invaluable  support  for \nresearchers,  investors,  and  decision-makers  navigating  the \ncomplex world of finance.\nThe impact of this approach extends beyond the realm of  \nfinance. The principles and techniques detailed in this paper  \ncan  be  readily  adapted  to  other  domains  facing  similar \nchallenges  of  information  overload,  including  scientific \nresearch, market analysis, and technology forecasting. The  \nprovided  GitHub  repository  serves  as  an  example  to \nresearchers and developers to explore. \nWhile  LLM  technology  continues  to  advance,  we  can \nanticipate even more sophisticated capabilities in the future. \nREFERENCES\n[1] Reinsel, David, John Gantz, and John Rydning, “Data age 2025: The  \nevolution of data to life-critical.”, Don’t Focus on Big Data 2, 2017. \n[2] Nadeau,  David,  and  Satoshi  Sekine,  “A  survey  of  named  entity \nrecognition  and  classification.”,  Lingvisticae  Investigationes  30.1, \n2007, pp.3-26.\n[3] Allahyari,  Mehdi,  et  al,  “Text  summarization  techniques:  a  brief \nsurvey”, arXiv preprint arXiv:1707.02268, 2017.\n[4] Nenkova,  Ani,  and  Kathleen  McKeown,  “A  survey  of  text  \nsummarization techniques.”, Mining text data, 2012, pp.43-76.\n[5] Pawar,  Sachin,  Girish  K.  Palshikar,  and  Pushpak  Bhattacharyya, \n“Relation extraction: A survey.”, arXiv preprint arXiv:1712.05191,  \n2017.\n[6] AI  Finance  Digest,  gitHub  repository.  [Online].  Available:  \nhttps://github.com/namesjoe/ai_digest_finance ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6149618625640869
    },
    {
      "name": "Data science",
      "score": 0.35344696044921875
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I153845743",
      "name": "Moscow Institute of Physics and Technology",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I118501908",
      "name": "National Research University Higher School of Economics",
      "country": "RU"
    }
  ],
  "cited_by": 2
}