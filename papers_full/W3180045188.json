{
    "title": "Remote Sensing Image Change Detection With Transformers",
    "url": "https://openalex.org/W3180045188",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1963498686",
            "name": "Chen Hao",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A4323106010",
            "name": "Qi, Zipeng",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2367361826",
            "name": "Shi, Zhenwei",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2036798369",
        "https://openalex.org/W3027225766",
        "https://openalex.org/W3011156941",
        "https://openalex.org/W6769242847",
        "https://openalex.org/W3027201985",
        "https://openalex.org/W3009942016",
        "https://openalex.org/W2896092083",
        "https://openalex.org/W3015038817",
        "https://openalex.org/W3120467244",
        "https://openalex.org/W3036453075",
        "https://openalex.org/W3099503507",
        "https://openalex.org/W3004423752",
        "https://openalex.org/W6782721548",
        "https://openalex.org/W3130754787",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2761352265",
        "https://openalex.org/W2948648905",
        "https://openalex.org/W2991591719",
        "https://openalex.org/W2896365540",
        "https://openalex.org/W2902594250",
        "https://openalex.org/W3000451586",
        "https://openalex.org/W2891248708",
        "https://openalex.org/W2805152403",
        "https://openalex.org/W2951991161",
        "https://openalex.org/W3000429716",
        "https://openalex.org/W2988020997",
        "https://openalex.org/W2751993439",
        "https://openalex.org/W2947010533",
        "https://openalex.org/W3099167317",
        "https://openalex.org/W3139912591",
        "https://openalex.org/W2997885125",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6779248606",
        "https://openalex.org/W3106728613",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W3180355996",
        "https://openalex.org/W6789705400",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W3095867871",
        "https://openalex.org/W3006025044",
        "https://openalex.org/W2971432438",
        "https://openalex.org/W3128592650",
        "https://openalex.org/W3042724941",
        "https://openalex.org/W3117344638",
        "https://openalex.org/W6768952226",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W2908320224",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W2963091558"
    ],
    "abstract": "Modern change detection (CD) has achieved remarkable success by the powerful\\ndiscriminative ability of deep convolutions. However, high-resolution remote\\nsensing CD remains challenging due to the complexity of objects in the scene.\\nObjects with the same semantic concept may show distinct spectral\\ncharacteristics at different times and spatial locations. Most recent CD\\npipelines using pure convolutions are still struggling to relate long-range\\nconcepts in space-time. Non-local self-attention approaches show promising\\nperformance via modeling dense relations among pixels, yet are computationally\\ninefficient. Here, we propose a bitemporal image transformer (BIT) to\\nefficiently and effectively model contexts within the spatial-temporal domain.\\nOur intuition is that the high-level concepts of the change of interest can be\\nrepresented by a few visual words, i.e., semantic tokens. To achieve this, we\\nexpress the bitemporal image into a few tokens, and use a transformer encoder\\nto model contexts in the compact token-based space-time. The learned\\ncontext-rich tokens are then feedback to the pixel-space for refining the\\noriginal features via a transformer decoder. We incorporate BIT in a deep\\nfeature differencing-based CD framework. Extensive experiments on three CD\\ndatasets demonstrate the effectiveness and efficiency of the proposed method.\\nNotably, our BIT-based model significantly outperforms the purely convolutional\\nbaseline using only 3 times lower computational costs and model parameters.\\nBased on a naive backbone (ResNet18) without sophisticated structures (e.g.,\\nFPN, UNet), our model surpasses several state-of-the-art CD methods, including\\nbetter than four recent attention-based methods in terms of efficiency and\\naccuracy. Our code is available at https://github.com/justchenhao/BIT\\\\_CD.\\n",
    "full_text": "1\nRemote Sensing Image Change Detection with\nTransformers\nHao Chen, Zipeng Qi and Zhenwei Shi ‚ãÜ, Member, IEEE\nAbstract‚ÄîModern change detection (CD) has achieved re-\nmarkable success by the powerful discriminative ability of\ndeep convolutions. However, high-resolution remote sensing CD\nremains challenging due to the complexity of objects in the\nscene. Objects with the same semantic concept may show distinct\nspectral characteristics at different times and spatial locations.\nMost recent CD pipelines using pure convolutions are still\nstruggling to relate long-range concepts in space-time. Non-\nlocal self-attention approaches show promising performance via\nmodeling dense relations among pixels, yet are computationally\ninefÔ¨Åcient. Here, we propose a bitemporal image transformer\n(BIT) to efÔ¨Åciently and effectively model contexts within the\nspatial-temporal domain. Our intuition is that the high-level\nconcepts of the change of interest can be represented by a\nfew visual words, i.e., semantic tokens. To achieve this, we\nexpress the bitemporal image into a few tokens, and use a\ntransformer encoder to model contexts in the compact token-\nbased space-time. The learned context-rich tokens are then\nfeedback to the pixel-space for reÔ¨Åning the original features via\na transformer decoder. We incorporate BIT in a deep feature\ndifferencing-based CD framework. Extensive experiments on\nthree CD datasets demonstrate the effectiveness and efÔ¨Åciency of\nthe proposed method. Notably, our BIT-based model signiÔ¨Åcantly\noutperforms the purely convolutional baseline using only 3 times\nlower computational costs and model parameters. Based on\na naive backbone (ResNet18) without sophisticated structures\n(e.g., FPN, UNet), our model surpasses several state-of-the-art\nCD methods, including better than four recent attention-based\nmethods in terms of efÔ¨Åciency and accuracy. Our code is available\nat https://github.com/justchenhao/BIT CD.\nIndex Terms‚ÄîChange detection (CD), high-resolution optical\nremote sensing (RS) image, transformers, attention mechanism,\nconvolutional neural networks (CNNs).\nI. I NTRODUCTION\nC\nHANGE detection (CD) is one of the major topics in\nremote sensing (RS). The goal of CD is to assign binary\nlabels (i.e., change or no change) to every pixel in a region\nby comparing co-registered images of the same region taken\nat different times [1]. The deÔ¨Ånition of change varies across\napplications, such as urban expansion [2], deforestation [3],\nand damage assessment [4]. Information extraction based on\nRS images still mainly relies on manual visual interpretation.\nThe work was supported by the National Key R&D Program of China under\nthe Grant 2019YFC1510905, the National Natural Science Foundation of\nChina under the Grant 61671037 and the Beijing Natural Science Foundation\nunder the Grant 4192034. (Corresponding author: Zhenwei Shi (e-mail:\nshizhenwei@buaa.edu.cn))\nHao Chen, Zipeng Qi and Zhenwei Shi are with the Image Processing\nCenter, School of Astronautics, Beihang University, Beijing 100191, China,\nand with the Beijing Key Laboratory of Digital Media, Beihang University,\nBeijing 100191, China, and also with the State Key Laboratory of Virtual\nReality Technology and Systems, School of Astronautics, Beihang University,\nBeijing 100191, China.\nAutomatic CD technology can reduce abundant labor costs and\ntime consumption, thus has raised increasing attention [2, 5‚Äì\n13].\nThe availability of high-resolution (HR) satellite data and\naerial data is opening up new avenues for monitoring land-\ncover and land-use at a Ô¨Åne scale. CD based on HR optical\nRS images remains a challenging task for two aspects: 1)\ncomplexity of the objects present in the scene, 2) different\nimaging conditions. Both contribute to the fact that the ob-\njects with the same semantic concept show distinct spectral\ncharacteristics at different times and different spatial locations\n(space-time). For example, as shown in Fig. 1 (a), the building\nobjects in a scene have varying shapes and appearance (in\nyellow boxes), and the same building object at different times\nmay have distinct colors (in red boxes) due to illumination\nvariations and appearance alteration. To identify the change\nof interest in the complex scene, a strong CD model needs to,\n1) recognize high-level semantic information of the change of\ninterest in a scene, 2) distinguish the real change from the\ncomplex irrelevant changes.\nNowadays, due to its powerful discriminative ability, deep\nConvolutional Neural Networks (CNN) have been successfully\napplied in RS image analysis and have shown good perfor-\nmance in the CD task [5]. Most recent supervised CD methods\n[2, 6‚Äì13] rely on a CNN-based structure to extract from each\ntemporal image, high-level semantic features that reveal the\nchange of interest.\nSince context modeling within the spatial and temporal\nscope is critical to identify the change of interest in high-\nresolution remote sensing images, the latest efforts have been\nfocusing on increasing the reception Ô¨Åeld (RF) of the model,\nthrough stacking more convolution layers [2, 6‚Äì8], using\ndilated convolution [7], and applying attention mechanisms\n[2, 6, 9‚Äì13]. Different from the purely convolution-based\napproach that is inherently limited to the size of the RF,\nthe attention-based approach (channel attention [9‚Äì12], spatial\nattention [9‚Äì11], and self-attention [2, 6, 13]) is effective in\nmodeling global information. However, most existing methods\nare still struggling to relate long-range concepts in space-time,\nbecause they either apply attention separately to each temporal\nimage for enhancing its features [9], or simply use attention to\nre-weight the fused bitemporal features/images in the channel\nor spatial dimension [10‚Äì12, 14]. Some recent work [2, 6, 13]\nhas achieved promising performance by utilizing self-attention\nto model the semantic relations between any pairs of pixels in\nspace-time. However, they are computationally inefÔ¨Åcient and\nneed high computational complexity that grows quadratically\nwith the number of pixels.\narXiv:2103.00208v3  [cs.CV]  11 Jul 2021\n2\nImage 1\nImage 2\nIllustration of the complexity of objects within a scene in bitemporal high-resolution \nimages (above: image 1, below: image 2).\nImage 1\nImage 2\nChange Map\nNo change\ntest_113_0256_0256\ntest_113_0256_0512\nCross Space\n(a) (b) (c)\nFig. 1. Illustration of the necessity of context modeling and the effect of our BIT module. (a) An example of a complex scene in bitemporal high-resolution\nimages. Building objects show different spectral characteristics at different times (red boxes) and different spatial locations (yellow boxes). A strong building\nCD model needs to recognize the building objects and distinguish real changes from irrelevant changes by leveraging context information. Based on the\nhigh-level image features (b), our BIT module exploits global contexts in space-time to enhance the original features. The differencing image (c) between the\nenhanced features and the original one shows the consistent improvement in features of building areas across space-time.\nTo tackle the above challenge, in this work, we introduce\nthe Bitemporal Image Transformer (BIT) to model long-\nrange context within the bitemporal image in an efÔ¨Åcient and\neffective manner. Our intuition is that the high-level concepts\nof the change of interest could be represented by a few\nvisual words, i.e., semantic tokens. Instead of modeling dense\nrelations among pixels in pixel-space, our BIT expresses the\ninput images into a few high-level semantic tokens, and models\nthe context in a compact token-based space-time. Moreover,\nwe enhance the feature representation of the original pixel-\nspace by leveraging relations between each pixel and semantic\ntokens. Fig 1 gives an example to show the effect of our BIT\non image features. Given the original image features related to\nthe building concept (see Fig 1 (b)), our BIT learns to further\nconsistently highlight the building areas (see Fig 1 (c)) by\nconsidering the global contexts in space-time. Note that we\nshow the differencing image between the enhanced features\nand the original features to better demonstrate the role of the\nproposed BIT.\nWe incorporate BIT in a deep feature differencing-based CD\nframework. The overall procedure of our BIT-based model\nis illustrated in Fig. 2. A CNN backbone (ResNet) is used\nto extract high-level semantic features from the input image\npair. We employ spatial attention to convert each temporal\nfeature map into a compact set of semantic tokens. Then we\nuse a transformer [15] encoder to model the context within\nthe two token sets. The resulting context-rich tokens are re-\nprojected to the pixel-space by a Siamese transformer decoder\nfor enhancing the original pixel-level features. Finally, we\ncompute the Feature Difference Images (FDI) from the two\nreÔ¨Åned feature maps, and then fed them into a shallow CNN\nto produce pixel-level change predictions.\nThe contribution of our work can be summarised as follows:\n‚Ä¢ An efÔ¨Åcient transformer-based method is proposed for\nremote sensing image change detection. We introduce\ntransformers into the CD task to better model contexts\nwithin the bitemporal image, which beneÔ¨Åts to identify\nthe change of interest and exclude irrelevant changes.\n‚Ä¢ Instead of modeling dense relations among any pairs\nof elements in pixel-space, our BIT expresses the input\nimages into a few visual words, i.e., tokens, and models\nthe context in the compact token-based space-time.\n‚Ä¢ Extensive experiments on three CD datasets validate the\neffectiveness and efÔ¨Åciency of the proposed method. We\nreplace the last convolutional stage of ResNet18 with\nBIT, and the resulting BIT-based model outperforms the\npurely convolutional counterpart with a signiÔ¨Åcant margin\nusing only 3 times lower computational costs and model\nparameters. Based on a naive CNN backbone without\nsophisticated structures (e.g., FPN, UNet), ours shows\nbetter performance in terms of efÔ¨Åciency and accuracy\nthan several recent attention-based CD methods.\nThe rest of this paper is organized as follows. Section II\ndescribes the related work of deep learning-based CD methods\nand the recent transformer-based models in RS. Section III\ngives the details of our proposed method. Some experimental\nresults are reported in section IV. The discussion is given in\nsection V and conclusion is drawn in section VI.\nII. R ELATED WORK\nA. Deep Learning based Remote Sensing Image Change de-\ntection\nDeep learning-based supervised CD methods for optical RS\nimages can be generally divided into two main streams [8].\nOne is the two-stage solution [16‚Äì18], where a CNN/FCN is\ntrained to separately classify the bitemporal images, and then\ntheir classiÔ¨Åcation results are compared for change decision.\nThis kind of approach is only practical when both the change\nlabel and the bitemporal semantic labels are available.\n3\nConv\nInputs\nTransformer \nEncoder \nsequence of encoded features \npositional encoding \nTransformer \nDecoder \nSemantic \nTokenizer \nChange map\nSemantic \nTokenizer \nTransformer \nDecoder \nH\n1√ó1\nL1√ó1√óùê∂\n‚Ä¶\nC\n1√ó1\n‚Ä¶\n‚Ñõ1√ó1√óùê∂\nC\n‚Ä¶\nC\n‚Ä¶\nSequence of image features\nImage features\nContext-rich tokens\n‚Ä¶\nsequence of encoded tokens\nPrediction Head\nRefined features\nC\n‚Ä¶\nùëãùëõùëíùë§1\nùëãùëõùëíùë§2\nBitemporal Image Transformer\nConvToken sets \nùëá1\nùëá2\nùëáùëõùëíùë§1\nSub\n& Abs\nH√óW√óùê∂\nùëã1\nConv\nCNN Backbone\nImage featuresInput images\nùëã2\nC\n‚Ä¶\nùëáùëõùëíùë§2\nConcatSplit\nConcat Split\nQuery\nKey/Value\nQuery\nKey/Value\nL\nL\nL\nL ùëá ùëáùëõùëíùë§\n1\nLength of ùêª √óùëä\nFig. 2. Illustration of our BIT-based model. Our semantic tokenizer pools the image features extracted by a CNN backbone to a compact vocabulary set\nof tokens ( L << HW). Then we feed the concatenated bitemporal tokens to the transformer encoder to relate concepts in token-based space-time. The\nresulting context-rich tokens for each temporal image are projected back to the pixel-space to reÔ¨Åne the original features via the transformer decoder. Finally,\nour prediction head produces the pixel-level predictions by feeding the computed feature difference images to a shallow CNN.\nAnother is the single-stage solution, which directly produces\nthe change result from the bitemporal images. The patch-\nlevel approach [19‚Äì21] models the CD task as a similarity\ndetection process by grouping bitemporal images into pairs of\npatches and employing a CNN on each pair to obtain its center\nprediction. The pixel-level approach [2, 3, 6, 7, 9‚Äì13, 22‚Äì\n28] uses FCNs to directly generate a high-resolution change\nmap from the two inputs, which is usually more efÔ¨Åcient\nand effective than the patch-level approach. Since the CD\ntask needs to handle two inputs, how to fuse the bitemporal\ninformation is an important topic. Existing FCN-based meth-\nods can be roughly divided into two groups according to the\nstage of fusion of bitemporal information. The image-level\napproach [3, 22‚Äì24, 29] concatenates the bitemporal images\nas a single input to a semantic segmentation network. The\nfeature-level approach [2, 6, 7, 9‚Äì12, 22, 25‚Äì28, 30] combines\nthe bitemporal features extracted from the neural networks and\nmakes change decisions based on fused features.\nMuch recent work aims to improve the feature discrimina-\ntive power of the neural networks, by designing multi-level\nfeature fusion structures [2, 9, 10, 12, 26, 30], combining\nGAN-based optimization objectives [23, 26, 28, 31], and\nincreasing the reception Ô¨Åeld (RF) of the model for better\ncontext modeling in terms of the spatial and temporal scope\n[2, 6‚Äì13].\nContext modeling is critical to identify the change of\ninterest in high-resolution remote sensing images due to the\ncomplexity of the objects in a scene and the variation of\nimage conditions. To increase the RF size, existing methods\ninclude employing a deeper CNN model [2, 6‚Äì8], using\ndilated convolution [7], and applying attention mechanisms\n[2, 6, 9‚Äì13]. For example, Zhang et al. [7] apply a deep\nCNN backbone (ResNet101 [32]) to extract image features and\nuse dilated convolution to enlarge the RF size of the model.\nConsidering that purely convolutional networks are inherently\nlimited to the size of the RF for each pixel, many latest efforts\nare focusing on introducing attention mechanisms to further\nenlarge the RF of the model, such as channel attention [9‚Äì\n12], spatial attention [9‚Äì11], self-attention [2, 6, 13]. However,\nmost of them still struggling to fully exploit the time-related\ncontext, because they either treat the attention as a feature\nenhancing module separately for each temporal image [9],\nor simply use attention to re-weight the fused bitemporal\nfeatures/images in the channel or spatial dimension [10‚Äì12].\nNon-local self-attention [2, 6] shows promising performance\ndue to its ability to exploit global relations among pixels in\nspace-time. However, they are computationally inefÔ¨Åcient and\nneed high computational complexity that grows quadratically\nwith the number of pixels.\nThe main purpose of our paper is to learn and exploit the\nglobal semantic information within the bitemporal images in\nan efÔ¨Åcient and effective manner for enhancing CD perfor-\nmance. Different from existing attention-based CD methods\nthat directly model dense relations among any pairs of ele-\nments in pixel-based space, we extract a few semantic tokens\nfrom images and model the context in token-based space-time.\nThe resulting context-rich tokens are then utilized to enhance\nthe original features in pixel-space. Our intuition is that the\nchange of interest within the scene can be described by a\nfew visual words (tokens) and the high-level features of each\npixel can be represented by the combination of these semantic\ntokens. As a result, our method exhibits high efÔ¨Åciency and\nhigh performance.\nB. Transformer-based Model\nThe transformer, Ô¨Årstly introduced in 2017 [15], has been\nwidely used in the Ô¨Åeld of natural language processing (NLP)\nto solve sequence-to-sequence tasks while handling long-range\n4\ndependencies with ease. A recent trend is the adoption of\ntransformers in the computer vision (CV) Ô¨Åeld. Due to the\nstrong representation ability of the transformer, transformer-\nbased models show comparable or even better performance\nas the convolutional counterparts in various visual tasks,\nincluding image classiÔ¨Åcation [33‚Äì35], segmentation [35‚Äì37],\nobject detection [36, 38, 39], image generation [40, 41], image\ncaptioning [42], and super-resolution [43, 44].\nThe astounding performance of transformer models on\nNLP/CV tasks has intrigued the remote sensing community\nto study their applications in remote sensing tasks, such as\nimage time-series classiÔ¨Åcation [45, 46], hyperspectral image\nclassiÔ¨Åcation [47], scene classiÔ¨Åcation [48], and remote sens-\ning image captioning [49, 50]. For example, Li et al. [46]\nproposed a CNN-transformer approach to perform the crop\nclassiÔ¨Åcation of time-series images, where the transformer was\nused to learn the pattern related to land cover semantics from\nthe sequence of multitemporal features extracted via CNN. He\net al. [47] applied a variant of the transformer (BERT [51])\nto capture global dependencies among pixels in hyperspectral\nimage classiÔ¨Åcation. Moreover, Wang et al. [50] employed\nthe transformer to translate the disordered words extracted by\nCNN from the given RS image into a well-formed sentence.\nIn this paper, we explore the potential of transformers in the\nbinary CD task. Our proposed BIT-based method is efÔ¨Åcient\nand effective in modeling global semantic relations in space-\ntime to beneÔ¨Åt the feature representation of the change of\ninterest.\nIII. E FFICIENT TRANSFORMER BASED CHANGE\nDETECTION MODEL\nThe overall procedure of our BIT-based model is illustrated\nin Fig. 2. We incorporate the BIT into a normal change\ndetection pipeline because we want to leverage the strengths\nof both convolutions and transformers. Our model starts with\nseveral convolution blocks to obtain the feature map for each\ninput image, then fed them into BIT to generate enhanced\nbitemporal features. Finally, the resulting feature maps are fed\nto a prediction head to produce pixel-level predictions. Our\nkey insight is that BIT learns and relates the global context\nof high-level semantic concepts, and feedback to beneÔ¨Åt the\noriginal bitemporal features.\nOur BIT has three main components: 1) a Siamese semantic\ntokenizer, which groups pixels into concepts to generate a\ncompact set of semantic tokens for each temporal input, 2) a\ntransformer encoder, which models context of semantic con-\ncepts in token-based space-time, and 3) a Siamese transformer\ndecoder, which projects the corresponding semantic tokens\nback to pixel-space to obtain the reÔ¨Åned feature map for each\ntemporal.\nThe inference detail of our BIT-based model for change\ndetection is shown in Algorithm 1.\nA. Semantic Tokenizer\nOur intuition is that the change of interest in input images\ncould be described by a few high-level concepts, namely\nsemantic tokens. And the semantic concepts can be shared\nAlgorithm 1: Inference of BIT-based Model for\nChange Detection.\nInput: I = {(I1,I2)}(a pair of registered images)\nOutput: M (a prediction change mask)\n1 // step1: extract high-level features by a CNN backbone\n2 for i in {1,2}do\n3 Xi = CNN Backbone(Ii)\n4 end\n5 // step2: use BIT to reÔ¨Åne bitemporal image features\n6 // compute the token set for each temporal feature\n7 for i in {1,2}do\n8 Ti = Semantic Tokenizer(Xi)\n9 end\n10 T=Concat(T1,T2)\n11 // use encoder to generate context-rich tokens\n12 Tnew=Transformer Encoder(T)\n13 T1\nnew,T2\nnew=Split(Tnew)\n14 // use decoder to reÔ¨Åne the original features\n15 for i in {1,2}do\n16 Xi\nnew = Transformer Decoder(Xi, Ti\nnew)\n17 end\n18 // step3: obtain change mask by the prediction head\n19 M = Prediction Head(X1\nnew,X2\nnew)\nH√óW√óùê∂\nImage features Attention maps\nToken set (length L)\nùëáùëñ\nL\nSemantic tokens\nL\nConv\nùëãùëñ ùê¥ùëñ\nC\n‚Ä¶\nH√óW√óùê∂\nFig. 3. Illustration of our semantic tokenizer.\nby the bitemporal images. To this end, we employ a Siamese\ntokenizer to extract compact semantic tokens from the feature\nmap of each temporal. Similar to the tokenizer in NLP, which\nsplits the input sentence into several elements (i.e., word or\nphrase) and represents each element with a token vector, our\nsemantic tokenizer splits the whole image into a few visual\nwords, each corresponds to one token vector. As shown in\nFig. 3, to obtain the compact tokens, our tokenizer learns a\nset of spatial attention maps to spatially pool the feature map\nto a set of features, i.e., the token set.\nLet X1,X2 ‚àà RHW √óC be the input bitemporal feature\nmaps, where H,W,C is height, width, and channel dimension\nof the feature map. Let T1,T2 ‚ààRL√óC be the two sets of\ntokens, where L is the size of the vocabulary set of tokens.\nFor each pixel Xi\np on the feature map Xi(i = 1,2), we\nuse a point-wise convolution to obtain L semantic groups,\neach group denotes one semantic concept. Then we compute\nspatial attention maps by a softmax function operated on the\nHW dimension of each semantic group. Finally, we use the\nattention maps to compute the weighted average sum of pixels\n5\nNorm\nMulti-head \nSelf-Attention\nNorm\n(a) Transformer Encoder (b) Transformer Decoder\nMLP\nkq v\nPositional Encoding\nTokens ùëá1\n‚Ä¶‚Ä¶\nTokens ùëá2\n‚Ä¶‚Ä¶\nv\n‚Ä¶‚Ä¶\nNorm\nMulti-head \nCross-Attention\nNorm\nMLP\nq\nk\nv\nNorm\nMulti-head\nCross-Attention\nNorm\nMLP\nq\nFeatures ùëã1 Features ùëã2\nRefined features ùëãùëõùëíùë§1 Refined features ùëãùëõùëíùë§2\nSiamese\n‚Ä¶\n‚Ä¶\nTokens ùëáùëõùëíùë§2\nTokens ùëáùëõùëíùë§1\nk\n√óùëÅùê∏ √óùëÅùê∑ √óùëÅùê∑\nùëã1\nSplit\nConcat\nFig. 4. Illustration of our transformer encoder and transformer decoder.\nin Xi to obtain a compact vocabulary set of size L, i.e.,\nsemantic tokens Ti. Formally,\nTi = (Ai)T Xi = (œÉ(œÜ(Xi; W)))T Xi, (1)\nwhere œÜ(¬∑) denotes the point-wise convolution with a learn-\nable kernel W ‚àà RC√óL, œÉ(¬∑) is the softmax function to\nnormalize each semantic group to obtain the attention maps\nAi ‚ààRHW √óL. Ti is computed by the multiplication of Ai\nand Xi.\nB. Transformer Encoder\nAfter obtaining two semantic token sets T1,T2 for the input\nbitemporal image, we then model the context between these\ntokens with a transformer encoder [15]. Our motivation is that\nthe global semantic relations in the token-based space-time can\nbe fully exploited by the transformer, thus producing context-\nrich token representation for each temporal. As shown in Fig.\n4 (a), we Ô¨Årst concatenate the two sets of tokens into one\ntoken set T ‚ààR2L√óC, and fed it into the transformer encoder\nto obtain a new token set Tnew. Finally, we split the tokens\ninto two sets Ti\nnew(i= 1,2).\nThe transformer encoder consists of NE layers of multi-\nhead self-attention (MSA) and multilayer perceptron (MLP)\nblocks (Fig. 4 (a)). Different from the original transformer that\nuses the post-norm residual unit, we follow ViT [33] to adopt\nthe pre-norm residual unit (PreNorm), i.e., the layer normaliza-\ntion occurs immediately before the MSA/MLP. PreNorm has\nbeen shown more stable and competent than the counterpart\n[52].\nAt each layer l, the input to self-attention is a triple (query\nQ, key K, value V) computed from the input T(l‚àí1) ‚àà\nR2L√óC as:\nQ = T(l‚àí1)Wq,\nK = T(l‚àí1)Wk,\nV = T(l‚àí1)Wv,\n(2)\nwhere Wl‚àí1\nq ,Wl‚àí1\nk ,Wl‚àí1\nv ‚ààRC√ód are the learnable param-\neters of three linear projection layers and d is the channel\ndimension of the triple. One attention head is formulated as:\nAtt(Q,K,V) =œÉ\n(QKT\n‚àö\nd\n)\nV, (3)\nwhere œÉ(¬∑) denotes the softmax function operated on the\nchannel dimension.\nThe core idea of the transformer encoder is multi-head self-\nattention. MSA performs multiple independent attention heads\nin parallel, and the outputs are concatenated and then projected\nto result in the Ô¨Ånal values. The advantage of MSA is that it\ncan jointly attend to information from different representation\nsubspaces at different positions. Formally,\nMSA(T(l‚àí1)) =Concat(head1,.., headh)WO,\nwhere headj = Att(T(l‚àí1)Wq\nj ,T(l‚àí1)Wk\nj ,T(l‚àí1)Wv\nj ),\n(4)\nwhere Wq\nj ,Wk\nj ,Wv\nj ‚ààRC√ód,WO ‚ààRhd√óC are the linear\nprojection matrices, h is the number of attention heads.\nThe MLP block consists of two linear transformation layers\nwith a GELU [53] activation in between. The dimensionality\nof input and output is C, and the inner-layer has dimension-\nality 2C. Formally,\nMLP(T(l‚àí1)) =GELU(T(l‚àí1)W1)W2 (5)\nwhere W1 ‚ààRC√ó2C,W2 ‚ààR2C√óC are the linear projection\nmatrices.\nNote that we add the learnable positional embedding (PE)\nWPE ‚àà R2L√óC to the token sequence T before feeding\nit to the transformer layers. Our empirical evidence (Sec.\nIV-D) indicates it is necessary to supplement PE to tokens. PE\nencodes the information about the relative or absolute position\nof elements in the token-based space-time. Such position\ninformation may beneÔ¨Åt context modeling. For example, tem-\nporal positional information can guide transformers to exploit\ntemporal-related contexts.\nC. Transformer Decoder\nTill now, we have obtained two sets of context-rich tokens\nTi\nnew(i = 1 ,2) for each temporal image. These context-\nrich tokens contain compact high-level semantic information\nthat well reveals the change of interest. Now, we need to\nproject the representation of concepts back to pixel-space to\nobtain pixel-level features. To achieve this, we use a modiÔ¨Åed\nSiamese transformer decoder [15] to reÔ¨Åne image features of\neach temporal. As shown in Fig. 4 (b), given a sequence\nof features Xi, the transformer decoder exploits the relation\nbetween each pixel and the token set Ti\nnew to obtain reÔ¨Åned\nfeatures Xi\nnew. We treat pixels in Xi as queries and tokens\nas keys. Our intuition is that each pixel can be represented by\nthe combination of the compact semantic tokens.\nOur transformer decoder consists of ND layers of multi-\nhead cross attention (MA) and MLP blocks. Different from the\noriginal implementation in [15], we remove the MSA block to\navoid abundant computation of dense relations among pixels\n6\nin Xi. We adopt PerNorm and the same conÔ¨Åguration of MLP\nas the transformer encoder. In MSA, the query, key, and value\nare derived from the same input sequence, while in MA, the\nquery is from the image features Xi, and the key and value\nare from the tokens Ti\nnew. Formally, at each layer l, MA is\ndeÔ¨Åned as:\nMA(Xi,(l‚àí1),Ti\nnew) =Concat(head1,..., headh)WO,\nwhere headj = Att(Xi,(l‚àí1)Wq\nj ,Ti\nnewWk\nj ,Ti\nnewWv\nj ),\n(6)\nwhere Wq\nj ,Wk\nj ,Wv\nj ‚ààRC√ód,WO ‚ààRhd√óC are the linear\nprojection matrices, h is the number of attention heads.\nNote that we do not add PE to the input queries, because our\nempirical evidence (Sec. IV-D) shows no considerable gains\nwhen adding PE.\nD. Network Details\nCNN backbone . We use a modiÔ¨Åed ResNet18 [32] to\nextract bitemporal image feature maps. The original ResNet18\nhas 5 stages, each with downsampling by 2. We replace the\nstride of the last two stages to 1 and add a point-wise convo-\nlution (output channel C = 32) behind ResNet to reduce the\nfeature dimension, followed by a bilinear interpolation layer,\nthus obtaining the output feature maps with a downsampling\nfactor of 4 to reduce the loss of spatial details. We name\nthis backbone ResNet18 S5. To validate the effectiveness of\nthe proposed method, we also use two lighter backbone,\nnamely ResNet18 S4/ResNet18 S3, which only uses the Ô¨Årst\nfour/three stages of the ResNet18.\nBitemporal image transformer . According to parameter\nexperiments in Sec. IV-E, we set token length L= 4. We set\nthe layer numbers of the transformer encoder to 1 and that of\nthe transformer decoder to 8. The number of heads hin MSA\nand MA is set to 8 and the channel dimension dfor each head\nis set to 8.\nPrediction head . BeneÔ¨Åting from the high-level semantic\nfeatures extracted by CNN backbone and BIT, a very shal-\nlow FCN is employed for change discrimination. Given two\nupsampled feature maps X1‚àó,X2‚àó ‚àà RH0√óW0√óC from the\noutput of BIT ( H0,W0 is the height, width of the original\nimage, respectively), the prediction head is to generate the\npredicted change probability maps P ‚ààRH0√óW0√ó2, which is\ngiven by\nP = œÉ(g(D)) =œÉ(g(|X1‚àó ‚àíX2‚àó|)), (7)\nwhere Feature Difference Images (FDI) D ‚àà RH0√óW0√óC\nis the element-wise absolute of the subtraction of the two\nfeature maps, g : RH0√óW0√óC ‚ÜíRH0√óW0√ó2 is the change\nclassiÔ¨Åer and œÉ(¬∑) denotes a softmax function pixel-wisely\noperated on the channel dimension of the output of the\nclassiÔ¨Åer. The conÔ¨Åguration of our change classiÔ¨Åer is two 3√ó3\nconvolutional layers with BatchNorm. The output channel of\neach convolution is ‚Äù32, 2‚Äù.\nIn the inference phase, the prediction mask M ‚ààRH0√óW0\nis computed by a pixel-wise Argmax operation on the channel\ndimension of P.\nLoss function. In the training stage, we minimize the cross-\nentropy loss to optimize the network parameters. Formally, the\nloss function is deÔ¨Åned as:\nL= 1\nH0 √óW0\nH,W‚àë\nh=1,w=1\nl(Phw,Yhw), (8)\nwhere l(Phw,y) =‚àílog(Phwy) is the cross-entropy loss, and\nYhw is the label for the pixel at location (h,w).\nIV. E XPERIMENTAL RESULTS AND ANALYSIS\nA. Experimental setup\nWe conduct experiments on three change detection datasets.\nLEVIR-CD [2] is a public large scale building CD dataset.\nIt contains 637 pairs of high-resolution (0.5m) RS images\nof size 1024 √ó1024. We follow its default dataset split\n(training/validation/test). For the limitation of GPU memory\ncapacity, we cut images into small patches of size 256 √ó256\nwith no overlap. Therefore, we obtain 7120/1024/2048 pairs\nof patches for training/validation/test respectively.\nWHU-CD [54] is a public building CD dataset. It contains\none pair of high-resolution (0.075m) aerial images of size\n32507 √ó15354. As no data split solution is provided in [54],\nwe crop the images into small patches of size 256 √ó256 with\nno overlap and randomly split it into three parts: 6096/762/762\nfor training/validation/test respectively.\nDSIFN-CD [10] is a public binary CD dataset. It includes\nsix large pairs of high-resolution (2m) satellite images from\nsix major cities in China, respectively. The dataset contains the\nchange of multiple kinds of land-cover objects, such as roads,\nbuildings, croplands, and water bodies. We follow the default\ncropped samples of size 512 √ó512 provided by the authors.\nWe have 3600/340/48 samples for training/validation/test re-\nspectively.\nTo validate the effectiveness of our BIT-based model, we\nset the following models for comparison:\n‚Ä¢ Base: our baseline model that consists of the CNN\nbackbone (ResNet18 S5) and the prediction head.\n‚Ä¢ BIT: our BIT-based model with a light backbone\n(ResNet18 S4).\nTo further evaluate the efÔ¨Åciency of the proposed method,\nwe additionally set the following models:\n‚Ä¢ Base S4: a light CNN backbone (ResNet18 S4) + the\nprediction head.\n‚Ä¢ Base S3: a much light CNN backbone (ResNet18 S3) +\nthe prediction head.\n‚Ä¢ BIT S3: our BIT-based model with a much light back-\nbone (ResNet18 S3).\nImplementation details . Our models are implemented on\nPyTorch and trained using a single NVIDIA Tesla V100\nGPU. We apply normal data augmentation to the input image\npatches, including Ô¨Çip, rescale, crop, and gaussian blur. We use\nstochastic gradient descent (SGD) with momentum to optimize\nthe model. We set the momentum to 0.99 and the weight decay\nto 0.0005. The learning rate is initially set to 0.01 and linearly\ndecay to 0 until trained 200 epochs. Validation is performed\n7\nafter each training epoch, and the best model on the validation\nset is used for evaluation on the test set.\nEvaluation Metrics . We use the F1-score with regard to\nthe change category as the main evaluation indices. F1-score\nis calculated by the precision and recall of the test as follows:\nF1 = 2\nrecall‚àí1 + precision‚àí1 , (9)\nAdditionally, precision, recall, Intersection over Union (IoU)\nof the change category, and overall accuracy (OA) are also\nreported. The above metrics are deÔ¨Åned as follows:\nprecision = TP / (TP + FP)\nrecall = TP / (TP+FN)\nIoU = TP / (TP+FN+FP)\nOA = (TP+TN) / (TP+TN+FN+FP)\n(10)\nwhere TP, FP, FN represent the number of true positive, false\npositive, and false negative respectively.\nB. Comparison to state-of-the-art\nWe make a comparison with several state-of-the-art meth-\nods, including three purely convolutional-based methods (FC-\nEF [22], FC-Siam-Di [22], FC-Siam-Conc [22]) and four\nattention-based methods (DTCDSCN [9], STANet [2], IFNet\n[10] and SNUNet [14]).\n‚Ä¢ FC-EF [22]: Image-level fusion method, where the bitem-\nporal images are concatenated as a single input to a fully\nconvolutional network.\n‚Ä¢ FC-Siam-Di [22]: Feature-level fusion method, which\nemploys a Siamese FCN to extract multi-level features\nand use feature difference to fuse the bitemporal infor-\nmation.\n‚Ä¢ FC-Siam-Conc [22]: Feature-level fusion method, which\nemploys a Siamese FCN to extract multi-level features\nand use feature concatenation to fuse the bitemporal\ninformation.\n‚Ä¢ DTCDSCN [9]: Multi-scale feature concatenation\nmethod, which adds channel attention and spatial\nattention to a deep Siamese FCN, thus obtaining more\ndiscriminative features. Note that they also trained two\nadditional semantic segmentation decoders under the\nsupervision of the label maps of each temporal. We\nomit the semantic segmentation decoders for a fair\ncomparison.\n‚Ä¢ STANet [2]: Metric-based Siamese FCN based method,\nwhich integrates the spatial-temporal attention mecha-\nnism to obtain more discriminative features.\n‚Ä¢ IFNet [10]: Multi-scale feature concatenation method,\nwhich applies channel attention and spatial attention to\nthe concatenated bitemporal features at each level of the\ndecoder. Deep supervision (i.e., computing supervised\nloss at each level of the decoder) is used to better train\nthe intermediate layers.\n‚Ä¢ SNUNet [14]: Multi-scale feature concatenation\nmethod, which combines the Siamese network and\nNestedUNet[55] to extract high-resolution high-level\nfeatures. Channel attention is applied to the features\nat each level of the decoder. Deep supervision is also\nemployed to enhance the discrimination ability of\nintermediate features.\nWe implement the above CD networks using their public\ncodes with default hyperparameters.\nTab. I reports the overall comparison results on LEVIR-CD,\nWHU-CD and DSIFN-CD test sets. The quantitative results\nshow our BIT-based model consistently outperforms the other\nmethods across these datasets with a signiÔ¨Åcant margin. For\nexample, the F1-score of our BIT exceeds the recent STANet\nby 2/1.6/4.7 points on the three datasets, respectively. Please\nnote that our CNN backbone is only the pure ResNet and\nwe do not apply the sophisticated structures such as FPN in\n[2] or UNet in [9, 10, 14, 22], which are powerful for pixel-\nwise prediction tasks by fusing the low-level features with\nhigh spatial accuracy and high-level semantic features. We can\nconclude that even using a simple backbone, our BIT-based\nmodel can achieve superior performance. It may attribute to\nthe ability of our BIT to model the context within the global\nhighly abstract spatial-temporal scope and utilize the context\nfor enhancing the feature representation in pixel-space.\nThe visualization comparison of the methods on the three\ndatasets is displayed in Fig. 5. For a better view, different\ncolors are used to denote TP (white), TN (black), FP (red),\nFN (green). We can observe that the BIT-based model achieves\nbetter results than others. First, our BIT-based model can\nbetter avoid the false positive (e.g., Fig 5 (a), (e), (g), (i))\ndue to the similar appearance of the object as that of the\ninterest change. For example, as shown in Fig. 5 (a), most\ncomparison methods incorrectly classify the swimming pool\narea as the building change (view as red), while based on the\nenhanced discriminant features via global context modeling,\nthe STANet and our BIT can reduce such false detection. In\nFig. 5 (c), the roads are mistaken as building changes by\nconventional methods because the roads have similar color\nbehaviors as buildings and these methods fail to exclude these\npseudo changes due to their limited reception Ô¨Åeld. Second,\nour BIT can also well handle the irrelevant changes caused\nby seasonal differences or appearance alteration of land-cover\nelements (e.g., Fig 5 (b), (f) and (l)). An example of the\nnon-semantic change of building in Fig 5 (f) illustrates the\neffectiveness of our BIT that learns the effective context within\nthe spatial-temporal domain to better express the real semantic\nchange and exclude the irrelevant change. Lastly, our BIT can\ngenerate relatively intact prediction results (e.g., Fig 5 (c), (h)\nand (j)) for large areas of change. For instance, in Fig. 5 (j),\nthe large building area in image 2 can not be detected entirely\n(view as green) by some comparison methods due to their\nlimited reception Ô¨Åeld, while our BIT-based model renders\nmore complete results.\nC. Model efÔ¨Åciency and effectiveness\nTo fairly compare the model efÔ¨Åciency, we test all the\nmethods on a computing server equipped with an Intel Xeon\nSilver 4214 CPU and an NVIDIA Tesla V100 GPU. Tab.\nII reports the number of parameters (Params.), Ô¨Çoating-point\noperations per second (FLOPs), and F1/IoU scores of different\nmethods on LEVIR-CD, WHU-CD, and DSIFN-CD test sets.\n8\nImage 1 Image 2 Ground Truth FC-EF FC-Siam-Conc FC-Siam-Di DTCDSCN STANet Base BIT\n(a)\n(b)\n(c) test_89_0512_0256.png\ntest_41_0000_0768.png\n1.2008_test_25_0000_0000\n(f)\n(g)\n(e)\nwhu_11776_19968\nwhu_2048_12032\nwhu_13056_6144.png\n(i)\n(j)\n(k)\n(e)\ntest_18\ntest_34\nLEVIR-CD\nWHU-CD\nDSIFN-CD\n(l)\n(d)\n(h)\nwhu_11264_13056.png\ntest_28_0768_0256.png\ntest_89_0512_0256.png\ntest_41_0000_0768.png\n1.2008_test_25_0000_0000\ntest_28_0768_0256.png\ntest_18\ntest_34\ntest_33\nTest_26\nIFNet SNUNet\nFig. 5. Visualization results of different methods on the LEVIR-CD, WHU-CD, and DSIFN-CD test sets. Different colors are used for a better view, i.e.,\nwhite for true positive, black for true negative, red for false positive, and green for false negative.\n9\nTABLE I\nCOMPARISON RESULTS ON THE THREE CD TEST SETS . THE HIGHEST SCORE IS MARKED IN BOLD . ALL THE SCORES ARE DESCRIBED IN PERCENTAGE\n(%).\nLEVIR-CD WHU-CD DSIFN-CD\nPre. / Rec. / F1 / IoU / OA Pre. / Rec. / F1 / IoU / OA Pre. / Rec. / F1 / IoU / OA\nFC-EF [22] 86.91 / 80.17 / 83.40 / 71.53 / 98.39 71.63 / 67.25 / 69.37 / 53.11 / 97.61 72.61 / 52.73 / 61.09 / 43.98 / 88.59\nFC-Siam-Di [22] 89.53 / 83.31 / 86.31 / 75.92 / 98.67 47.33 / 77.66 / 58.81 / 41.66 / 95.63 59.67 / 65.71 / 62.54 / 45.50 / 86.63\nFC-Siam-Conc [22] 91.99 / 76.77 / 83.69 / 71.96 / 98.49 60.88 / 73.58 / 66.63 / 49.95 / 97.04 66.45 / 54.21 / 59.71 / 42.56 / 87.57\nDTCDSCN [9] 88.53 / 86.83 / 87.67 / 78.05 / 98.77 63.92 / 82.30 / 71.95 / 56.19 / 97.42 53.87 / 77.99 / 63.72 / 46.76 / 84.91\nSTANet [2] 83.81 / 91.00 / 87.26 / 77.40 / 98.66 79.37 / 85.50 / 82.32 / 69.95 / 98.52 67.71 / 61.68 / 64.56 / 47.66 / 88.49\nIFNet [10] 94.02 / 82.93 / 88.13 / 78.77 / 98.87 96.91 / 73.19 / 83.40 / 71.52 / 98.83 67.86 / 53.94 / 60.10 / 42.96 / 87.83\nSNUNet [14] 89.18 / 87.17 / 88.16 / 78.83 / 98.82 85.60 / 81.49 / 83.50 / 71.67 / 98.71 60.60 / 72.89 / 66.18 / 49.45 / 87.34\nBase 88.24 / 86.91 / 87.57 / 77.89 / 98.76 81.80 / 81.42 / 81.61 / 68.93 / 98.53 73.30 / 48.65 / 58.48 / 41.32 / 88.26\nBIT 89.24 / 89.37 / 89.31 / 80.68 / 98.92 86.64 / 81.48 / 83.98 / 72.39 / 98.75 68.36 / 70.18 / 69.26 / 52.97 / 89.41\nFirst, we verify the efÔ¨Åciency of our proposed BIT by\ncomparing the convolutional counterparts. Tab. II shows\nthat built on Base S3/Base S4, the model added the BIT\n(BIT S3/BIT S4) is more effective and efÔ¨Åcient than that\n(Base S4/Base S5) with more convolutional layers. For exam-\nple, BIT S4 outperforms the Base S5 by 1.7/2.4/10.8 points\nof the F1-score on the three test sets while with 3 times\nsmaller numbers of model parameters and 3 times lower\ncomputational costs. Moreover, we can observe that compared\nto Base S4, adding more convolutional layers only introduce\ntrivial improvements (i.e., 0.16/0.75/0.18 points of the F1-\nscore on the three test sets) while the improvements by BIT\nis much more (i.e., 4 ‚àº60 times) than that of the CNN.\nSecond, we make a comparison with four attention-based\nmethods (DTCDSCN, STANet, IFNet and SNUNet). As\nshown in Tab. II, our BIT S4 outperforms the four counter-\nparts in the F1/IoU scores with a signiÔ¨Åcant margin with much\nsmall computational complexity and model parameters. Inter-\nestingly, even with a much lighter backbone (about 10 times\nsmaller), our BIT-based model (BIT S3) is still superior to\nthe four compared methods on most datasets. The comparison\nresults further prove the effectiveness and efÔ¨Åciency of our\nBIT-based model.\nTraining visualization. Fig. 6 illustrates the mean F1-score\non the training/validation sets for each training epoch. We\ncan observe that although the Base and BIT models have\nsimilar performance on training accuracy, BIT outperforms\nBase with regard to the validation accuracy in terms of stability\nand effectiveness. It indicates that the training of BIT is\nmore stable and efÔ¨Åcient, and our BIT-based model has more\ngeneralization ability. It may due to its ability to learn compact\ncontext-rich concepts, which effectively represent the change\nof interest.\nD. Ablation studies\nContext modeling. We perform ablation on the Transformer\nEncoder (TE) to validate its effectiveness in context modeling,\nwhere multi-head self-attention is the core component in TE\nfor modeling context. From Tab. III, we can observe consistent\nand signiÔ¨Åcant drops in F1-score on the LEVIR-CD, WHU-\nCD, and DSIFN-CD datasets when removing TE from BIT. It\nindicates the vital importance of self-attention in TE to model\n/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000014/uni00000018/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000015/uni00000013/uni00000013\n/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051/uni00000003/uni00000029/uni00000014\n/uni00000025/uni00000044/uni00000056/uni00000048\n/uni00000025/uni0000002c/uni00000037\n(a) Training accuracy on LEVIR-CD dataset.\n/uni00000013/uni00000015/uni00000018/uni00000018/uni00000013/uni0000001a/uni00000018/uni00000014/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000014/uni00000018/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000015/uni00000013/uni00000013\n/uni00000048/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000059/uni00000044/uni0000004f/uni0000004c/uni00000047/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000050/uni00000048/uni00000044/uni00000051/uni00000003/uni00000029/uni00000014\n/uni00000025/uni00000044/uni00000056/uni00000048\n/uni00000025/uni0000002c/uni00000037\n(b) Validation accuracy on LEVIR-CD dataset.\nFig. 6. Accuracy of models for each training epoch. The mean F1-score is\nreported.\nrelations within token-based space-time. Moreover, we replace\nour BIT with a Non-local [56] self-attention layer, which is\nable to model relations within the pixel-based space-time. The\ncomparison results in Tab. III show our BIT outperforms Non-\nlocal on the three test sets with a signiÔ¨Åcant margin. It may\nbecause our BIT learns the context in a tokens-based space,\nwhich is more compact and has higher information density\n10\nTABLE II\nABLATION STUDY ON MODEL EFFICIENCY . WE REPORT THE NUMBER OF PARAMETERS (PARAMS .), FLOATING -POINT OPERATIONS PER SECOND\n(FLOP S), AS WELL AS THE F1 AND IOU SCORES ON THE THREE CD TEST SETS . THE INPUT IMAGE TO THE MODEL HAS A RESIZE OF 256 √ó 256 √ó 3 TO\nCALCULATE THE FLOP S.\nLEVIR-CD WHU-CD DSIFN-CD\nModel Params.(M) FLOPs (G) F1 IoU F1 IoU F1 IoU\nDTCDSCN [9] 41.07 7.21 87.67 78.05 71.95 56.19 63.72 46.76\nSTANet [2] 16.93 6.58 87.26 77.40 82.32 69.95 64.56 47.66\nIFNet [10] 50.71 41.18 88.13 78.77 83.40 71.52 60.10 42.96\nSNUNet [14] 12.03 27.44 88.16 78.83 83.50 71.67 66.18 49.45\nBase S3 1.28 1.78 82.23 76.24 79.52 66.00 56.00 38.88\n+ CNN (Base S4) 3.38 4.09 87.41 77.64 80.86 67.87 58.30 41.15\n+ BIT (BIT S3) 1.45 2.05 88.51 79.39 81.38 68.60 69.00 52.67\nBase S4 3.38 4.09 87.41 77.64 80.86 67.87 58.30 41.15\n+CNN (Base S5) 11.85 12.99 87.57 77.89 81.61 68.93 58.48 41.32\n+BIT (BIT S4) 3.55 4.35 89.31 80.68 83.98 72.39 69.26 52.97\nTABLE III\nABLATION STUDY OF OUR BIT ON THREE CD DATASETS . ABLATIONS ARE\nPERFORMED ON TOKENIZER (T), T RANSFORMER ENCODER (TE), AND\nTRANSFORMER DECODER (TD). W E ALSO ADD THE NON-LOCAL TO THE\nBASELINE FOR COMPARISON . THE F1- SCORE IS REPORTED . NOTE THAT\nTHE DEPTH OF TE AND TD ARE SET TO 1.\nModel T TE TD LEVIR WHU DSIFN\nBase S4 √ó √ó √ó 87.41 80.86 58.30\n+Non-local √ó √ó √ó 87.56 80.93 59.94\nBIT ‚úì ‚úì ‚úì 88.93 82.34 67.38\nBIT √ó ‚úì ‚úì 87.58 81.68 61.76\nBIT ‚úì √ó ‚úì 87.35 81.05 62.93\nBIT ‚úì ‚úì √ó 88.07 79.16 64.47\nBIT ‚úì √ó √ó 87.38 80.82 59.54\nTABLE IV\nABLATION STUDY OF POSITION EMBEDDING (PE) ON THREE CD\nDATASETS . WE PERFORM ABLATIONS ON PE IN TRANSFORMER ENCODER\n(TE) AND TRANSFORMER DECODER (TD). T HE F1- SCORE IS REPORTED .\nNOTE THAT THE DEPTH OF TE AND TD ARE SET TO 1.\nModel PE in TE PE in TD LEVIR WHU DSIFN\nBIT √ó √ó 87.77 82.06 60.81\nBIT ‚úì √ó 88.93 82.34 67.38\nBIT √ó ‚úì 87.87 81.40 60.23\nBIT ‚úì ‚úì 89.07 82.01 65.68\nTABLE V\nEFFECT OF THE TOKEN LENGTH . THE F1/I OU SCORES OF THE BIT ARE\nEVALUATED ON THE LEVIR-CD, WHU-CD, AND DSIFN-CD TEST SETS .\nNOTE THAT THE DEPTH OF TE AND TD ARE SET TO 1.\nLEVIR-CD WHU-CD DSIFN-CD\nLength F1 IoU F1 IoU F1 IoU\n32 87.76 78.18 81.53 68.82 62.40 45.35\n16 88.45 79.74 81.79 69.19 63.07 46.06\n8 88.19 78.88 81.83 69.27 64.28 47.36\n4 88.93 80.07 82.34 70.00 67.38 50.80\n2 88.90 80.02 82.02 69.53 65.13 48.29\nTABLE VI\nEFFECT OF THE DEPTH OF THE TRANSFORMER . WE PERFORM ANALYSIS\nON THE ENCODER DEPTH (E.D.) AND DECODER DEPTH (D.D.) OF THE\nBIT, AND REPORT THE F1/I OU SCORES FOR EACH CONFIGURATION ON\nTHE LEVIR-CD, WHU-CD, AND DSIFN-CD TEST SETS .\nLEVIR-CD WHU-CD DSIFN-CD\nE.D. D.D. F1 IoU F1 IoU F1 IoU\n1 1 88.93 80.07 82.34 70.00 67.38 50.80\n2 1 89.13 80.39 81.83 69.24 66.96 50.34\n4 1 88.97 80.13 82.15 69.70 66.95 50.32\n8 1 88.93 80.06 80.73 67.68 67.11 50.50\n1 2 88.91 80.03 82.99 70.92 67.17 50.57\n1 4 89.26 80.59 83.69 71.95 69.05 52.73\n1 8 89.31 80.68 83.98 72.39 69.26 52.97\nthan that of Non-local, thus facilitating the effective extraction\nof relations.\nAblation on tokenizer . We perform ablation on the to-\nkenizer by removing it from the BIT. The resulting model\ncan be considered to use dense tokens, which are sequences\nof features extracted by the CNN backbone. As shown in\nTab. III, the BIT-based model (w.o. tokenizer) receives sig-\nniÔ¨Åcant drops in the F1-score. It indicates that the tokenizer\nmodule is critical in our transformer-based framework. We\ncan see that the model (w.o. tokenizer) only slightly better\nthan Base S4. It may because that the dense features contain\ntoo much redundancy information that makes the training of\nthe transformer-based model a tough task. On the contrary,\nour proposed tokenizer spatially pool the dense features to\naggregate the semantic information, thus obtaining compact\ntokens of concepts.\nAblation on transformer decoder . To verify the effective-\nness of our Transformer Decoder (TD), we replace it with\na simple module to fuse the tokens Ti\nnew from TE and the\noriginal features Xi from the CNN backbone. In the simple\nmodule, we expand the spatial dimension of each token in\nTi\nnew (containing L tokens) to a shape of RHW . And the L\nexpanded tokens and Xi are summed to produce the updated\nfeatures that are then fed to the prediction head. Tab. III\nindicates consistent performance declines of the BIT model\n11\nwithout TD on the three test sets. It may because cross-\nattention (the core part of TD) provides an elegant way to\nenhance the original features with the context-rich tokens by\nmodeling their relations. Furthermore, the BIT (w.o. both TE\nand TD) is much inferior to the normal BIT model.\nEffect of position embedding . The Transformer archi-\ntecture is permutation-invariant, while the CD task requires\nboth spatial and temporal position information. To this end,\nwe add the learned position embedding (PE) to the feature\nsequence fed to the transformer. We perform ablations on PE\nin TE and TD. We set the BIT model containing no PE as\nthe baseline. As shown in Tab. IV, our BIT model achieves\nconsistent improvements in the F1-score on the three test sets\nwhen adding PE to the tokens fed into TE. It indicates that\nthe position information within the bitemporal token sets is\ncritical for context modeling in TE. Compared to the baseline,\nthere are no signiÔ¨Åcant improvements in the F1-score to the\nBIT model when adding PE to queries fed into TD. The\npositional information may be unnecessary to the queries into\nTD because the keys (i.e., tokens) into TD are highly abstract\nand contain no spatial structure. Therefore, we only add PE\nin TE, but not in TD in our BIT model.\nE. Parameter analysis\nToken length . Our tokenizer spatially pools the dense\nfeatures of the image into a compact token set. Our intuition\nis that the change of interest within the bitemporal images can\nbe described by a few visual concepts, i.e., semantic tokens.\nThe length of the token set Lis an important hyperparameter.\nWe test different L ‚àà {2,4,8,16,32}to analyze its effect\non the performance of our model on the LEVIR-CD, WHU-\nCD, and DSIFN-CD dataset, respectively. Tab. V shows a\nsigniÔ¨Åcant improvement in the F1-score of the model when\nreducing the token length from 32 to 4. It indicates that a\ncompact token set is sufÔ¨Åcient to denote semantic concepts of\ninterest changes and redundant tokens may hinder the model\nperformance. We can also observe a slight drop in F1-score\nwhen further decreasing Lfrom 4 to 2. It is because the model\nmay lose some useful information related to change concepts\nwhen L is too short. Therefore, we set L to 4.\nDepth of transformer. The number of transformer layers is\none important hyperparameter. We test different conÔ¨Ågurations\nof the BIT model that contains varying numbers of transformer\nlayers in TE and TD. Tab. VI shows no signiÔ¨Åcant improve-\nments to the F1/IoU scores of BIT on the three datasets when\nincreasing the depth of the transformer encoder. It indicates\nthat relations between the bitemporal tokens can be well\nlearned by a single layer TE. Tab. VI also shows the model\nperformance is roughly positively correlated with the decoder\ndepth. It may because image features are reÔ¨Åned after each\nlayer of the transformer decoder by considering the context-\nrich tokens. The best result is obtained when the decoder\ndepth is 8. Although there may be performance gains by\nfurther increasing the decoder depth, for the tradeoff between\nefÔ¨Åciency and precision, we set the encoder depth to 1 and\nthe decoder depth to 8.\nImage 1 Image 2Attention Maps of Image 1 Attention Maps of Image 2\nLEVIR-CD\nWHU-CD\n(a)\n(b)\n(c)\n(d)\n(f)\n(e)\n(g)\n(i)\n(h)\nDSIFN-CD\nFig. 7. Token visualization on the LEVIR-CD, WHU-CD, and DSIFN-CD\ntest sets. Red denotes higher attention values and blue denotes lower values.\nF . Token visualization\nWe hypothesize that our tokenizer can extract high-level\nsemantic concepts that reveal the change of interest. For better\nunderstanding the semantic tokens, we visualize the attention\nmaps Ai ‚àà RHW √óL that the tokenizer extracted from the\nbitemporal feature maps. Each token Ti\nl in the token set Ti is\ncorresponding to one attention map Ai\nl ‚ààRHW . Fig. 7 shows\nthe visualization results of tokens for some bitemporal images\nfrom the LEVIR-CD, WHU-CD, and DSIFN-CD datasets. We\ndisplay the attention maps of two selected tokens from Ti for\neach input image. Red denotes higher attention values and blue\ndenotes lower values.\nFrom Fig. 7, we can see that the extracted token can attend\nto the region that belongs to the semantic concept of the\nchange of interest. Different tokens may relate to objects of\ndifferent semantic meanings. For example, as the LEVIR-CD\nand WHU-CD datasets only describe the building changes, the\nlearned tokens in these datasets mainly attend to the pixels\nbelongs to buildings. While because the DSIFN-CD dataset\n12\n(a) Bitemporal images (b) Original features ùëøùíä (c) Token attention maps ùë®ùíä (d) Refined features ùëøùíèùíÜùíòùíä\n(f) Bitemporal differencing ùëøùíèùíÜùíòùüè ‚àíùëøùíèùíÜùíòùüê\n(g) Change probability map ùë∑\n(e) Differencing ùëøùíèùíÜùíòùíä ‚àíùëøùíä\ntrain_412_0512_0768\nImage 1\nImage 2\nV1\n#1 #2\n#4\n#1 #2\n#4\n#1 #2\n#4\n#1 #2\n#4\n#1 #2\n#4\n#1 #2\n#4\n#1 #2\n#4\n#3\n#3\n#3\n#3 #3\n#3 #3\n#2\n#1 #2\n#1\nFig. 8. An example of network visualization. (a) input images, (b) selected high-level feature maps Xi, (c) selected attention maps Ai by tokenizer, (d)\nreÔ¨Åned feature maps Xi\nnew, (e) differencing between Xi\nnew and Xi, (f) bitemporal feature differencing image, (g) change probability map P. The sample\nis from the LEVIR-CD data set. We use the same normalization (min-max) to visualize each activation map.\ncontains various kinds of changes, these tokens can highlight\ndifferent semantic areas, such as buildings, croplands, and\nwater bodies. Interestingly, as shown in Fig. 7 (c) and (f), our\ntokenizer can also highlight the pixels surrounding the building\n(e.g., shadow), even though no explicit supervision of such\nareas is provided when training our model. It is not surprising\nbecause the context surrounding the building is a critical\ncue for object recognition. It indicates that our model can\nimplicitly learn some additional concepts to promote change\nrecognition.\nG. Network visualization\nTo better understand our model, we provide an example to\nvisualize the activation maps at different stages of the BIT\nmodel. Given the bitemporal image (Fig. 8 (a)), a Siamese\nFCN generates the high-level feature maps Xi (Fig. 8 (b)).\nThen the tokenizer spatially pools the feature maps into several\ntoken vectors using the learned attention maps Ai (Fig. 8 (c)).\nThe context-rich tokens generated by the transformer encoder\nare then projected back to the pixel-space via the transformer\ndecoder, resulting in the reÔ¨Åned feature maps Xi\nnew (Fig. 8\n(d)). We show four corresponding representative feature maps\nfrom the original features Xi, and from the reÔ¨Åned features\nXi\nnew. From Fig. 8 (b) and (d), we can observe that our model\ncan extract high-level features related to the change of interest\nfor each temporal image, such as concepts of buildings and\ntheir edges. To better illustrate the effect of the BIT module,\nthe differencing images between the reÔ¨Åned and the original\nfeatures are shown in Fig. 8 (e). It indicates that our BIT can\nfurther highlight the regions of semantic concepts related to the\nchange category. Lastly, the prediction head calculates feature\ndifferencing images (Fig. 8 (f)) between Xi\nnew and Xi, and\ngenerates the change probability map P (Fig. 8 (g)).\nV. D ISCUSSION\nWe provide an efÔ¨Åcient and effective method to perform\nchange detection in high-resolution remote sensing images.\nThe high reÔ¨Çectance variation for pixels of the same category\nin whole space-time brings difÔ¨Åculties to the model in recog-\nnizing objects of interest and distinguishing real changes from\nirrelevant changes. Context modeling in space-time is critical\nfor enhancing feature discrimination power. Our proposed BIT\nmodule can efÔ¨Åciently model the context information in the\ntoken-based space-time and use the context-rich tokens to\nenhance the original features. Compared to the Base model,\nour BIT-base model can generate more accurate predictions\nwith fewer false alarms and higher recalls (see Fig. 5 and\nTable I). Furthermore, the BIT can enhance the efÔ¨Åciency and\nstability of the training of the model (see Fig. 6). It is because\nthat our BIT expresses the images into a small number of\nvisual words (token vectors), such high-density information\nmay improve the training efÔ¨Åciency. Our BIT can also be\nviewed as an efÔ¨Åcient attention-base way to increase the\nreception Ô¨Åeld of the model, thus beneÔ¨Åt feature representation\npower for change recognition.\nVI. C ONCLUSION\nIn this paper, we propose an efÔ¨Åcient transformer-based\nmodel for change detection in remote sensing images. Our BIT\nlearns a compact set of tokens to represent high-level concepts\nthat reveal the change of interest existing in the bitemporal\nimages. We leverage the transformer to relate semantic con-\ncepts in the token-based space-time. Extensive experiments\nhave validated the effectiveness of our method. We replace\nthe last convolutional stage of ResNet18 with BIT, obtaining\nsigniÔ¨Åcant accuracy improvements (1.7/2.4/10.8 points of the\nF1-score on the LEVIR-CD/WHU-CD/DSIFN-CD test sets)\nwith 3 times lower computational complexity and 3 times\nsmaller model parameters. Our empirical evidence indicates\nBIT is more efÔ¨Åcient and effective than purely convolutional\nmodules. Only using a simple CNN backbone (ResNet18), our\nmethod outperforms several other CD methods that employ\nmore sophisticated structures, such as FPN and UNet. We also\nshow better performance in terms of efÔ¨Åciency and accuracy\nthan four recent attention-based methods on the three CD\ndatasets.\nREFERENCES\n[1] A. SINGH, ‚ÄúReview article digital change detection techniques\nusing remotely-sensed data,‚Äù International Journal of Remote\nSensing, vol. 10, no. 6, pp. 989‚Äì1003, 1989.\n13\n[2] H. Chen and Z. Shi, ‚ÄúA spatial-temporal attention-based method\nand a new dataset for remote sensing image change detection,‚Äù\nRemote. Sens., vol. 12, no. 10, p. 1662, 2020.\n[3] P. P. de Bem, O. A. de Carvalho Junior, R. F. Guimar Àúaes, and\nR. A. T. Gomes, ‚ÄúChange detection of deforestation in the\nbrazilian amazon using landsat data and convolutional neural\nnetworks,‚Äù Remote Sensing, vol. 12, no. 6, p. 901, 2020.\n[4] J. Z. Xu, W. Lu, Z. Li, P. Khaitan, and V . Zaytseva, ‚ÄúBuilding\ndamage detection in satellite imagery using convolutional neural\nnetworks,‚Äù 2019.\n[5] W. Shi, M. Zhang, R. Zhang, S. Chen, and Z. Zhan, ‚ÄúChange\ndetection based on artiÔ¨Åcial intelligence: State-of-the-art and\nchallenges,‚Äù Remote Sensing, vol. 12, p. 1688, 2020.\n[6] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, T. Lin,\nand H. Li, ‚ÄúDasnet: Dual attentive fully convolutional siamese\nnetworks for change detection of high resolution satellite im-\nages.‚Äù\n[7] M. Zhang, G. Xu, K. Chen, M. Yan, and X. Sun, ‚ÄúTriplet-\nbased semantic relation learning for aerial remote sensing image\nchange detection,‚Äù IEEE Geosci. Remote. Sens. Lett. , vol. 16,\nno. 2, pp. 266‚Äì270, 2019.\n[8] M. Zhang and W. Shi, ‚ÄúA feature difference convolutional\nneural network-based change detection method,‚Äù TGRS, pp. 1‚Äì\n15, 2020.\n[9] Y . Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, ‚ÄúBuilding\nchange detection for remote sensing images using a dual-task\nconstrained deep siamese convolutional network model,‚Äù IEEE\nGeoscience and Remote Sensing Letters , pp. 1‚Äì5, 2020.\n[10] C. Zhang, P. Yue, D. Tapete, L. Jiang, B. Shangguan, L. Huang,\nand G. Liu, ‚ÄúA deeply supervised image fusion network for\nchange detection in high resolution bi-temporal remote sensing\nimages,‚Äù ISPRS, vol. 166, pp. 183‚Äì200, 2020.\n[11] X. Peng, R. Zhong, Z. Li, and Q. Li, ‚ÄúOptical remote sensing\nimage change detection based on attention mechanism and im-\nage difference,‚Äù IEEE Transactions on Geoscience and Remote\nSensing, pp. 1‚Äì12, 2020.\n[12] H. Jiang, X. Hu, K. Li, J. Zhang, J. Gong, and M. Zhang,\n‚ÄúPga-siamnet: Pyramid feature-based attention-guided siamese\nnetwork for remote sensing orthoimagery building change de-\ntection,‚Äù Remote Sensing, vol. 12, no. 3, p. 484, 2020.\n[13] F. I. Diakogiannis, F. Waldner, and P. Caccetta, ‚ÄúLooking for\nchange? roll the dice and demand attention.‚Äù\n[14] S. Fang, K. Li, J. Shao, and Z. Li, ‚ÄúSnunet-cd: A densely\nconnected siamese network for change detection of vhr images,‚Äù\nIEEE Geoscience and Remote Sensing Letters , pp. 1‚Äì5, 2021.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA , I. Guyon,\nU. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N.\nVishwanathan, and R. Garnett, Eds., 2017, pp. 5998‚Äì6008.\n[16] K. Nemoto, T. Imaizumi, S. Hikosaka, R. Hamaguchi, M. Sato,\nand A. Fujita, ‚ÄúBuilding change detection via a combination of\ncnns using only rgb aerial imageries,‚Äù Oct. 2017.\n[17] S. Ji, Y . Shen, M. Lu, and Y . Zhang, ‚ÄúBuilding instance\nchange detection from large-scale aerial images using convolu-\ntional neural networks and simulated samples,‚Äù Remote Sensing,\nvol. 11, no. 11, p. 1343, 2019.\n[18] R. Liu, M. Kuffer, and C. Persello, ‚ÄúThe temporal dynamics\nof slums employing a cnn-based change detection approach,‚Äù\nRemote. Sens., vol. 11, no. 23, p. 2844, 2019.\n[19] R. C. Daudt, B. L. Saux, A. Boulch, and Y . Gousseau, ‚ÄúUr-\nban change detection for multispectral earth observation using\nconvolutional neural networks,‚Äù in IGARSS, 2018.\n[20] F. U. Rahman, B. Vasu, J. V . Cor, J. Kerekes, and A. E. Savakis,\n‚ÄúSiamese network with multi-level features for patch-based\nchange detection in satellite imagery,‚Äù in 2018 IEEE Global\nConference on Signal and Information Processing, GlobalSIP\n2018, Anaheim, CA, USA, November 26-29, 2018. IEEE, 2018,\npp. 958‚Äì962.\n[21] M. Wang, K. Tan, X. Jia, X. Wang, and Y . Chen, ‚ÄúA deep\nsiamese network with hybrid convolutional feature extraction\nmodule for change detection based on multi-sensor remote\nsensing images,‚Äù Remote Sensing, vol. 12, no. 2, p. 205, 2020.\n[22] R. C. Daudt, B. L. Saux, and A. Boulch, ‚ÄúFully convolutional\nsiamese networks for change detection,‚Äù in ICIP, 2018.\n[23] M. A. Lebedev, Y . V . Vizilter, O. V . Vygolov, V . A. Knyaz,\nand A. Y . Rubis, ‚ÄúChange detection in remote sensing images\nusing conditional adversarial networks,‚Äù vol. XLII-2, 2018, pp.\n565‚Äì571.\n[24] D. Peng, Y . Zhang, and H. Guan, ‚ÄúEnd-to-end change detection\nfor high resolution satellite images using improved unet++,‚Äù\nRemote Sensing, vol. 11, no. 11, p. 1382, 2019.\n[25] T. Bao, C. Fu, T. Fang, and H. Huo, ‚ÄúPpcnet: A combined\npatch-level and pixel-level end-to-end deep network for high-\nresolution remote sensing image change detection,‚Äù vol. PP, pp.\n1‚Äì5, 2020.\n[26] B. Hou, Q. Liu, H. Wang, and Y . Wang, ‚ÄúFrom w-net to cdgan:\nBitemporal change detection via deep learning techniques,‚Äù\nIEEE Transactions on Geoscience and Remote Sensing, vol. 58,\nno. 3, pp. 1790‚Äì1802, 2020.\n[27] Y . Zhan, K. Fu, M. Yan, X. Sun, H. Wang, and X. Qiu, ‚ÄúChange\ndetection based on deep siamese convolutional network for\noptical aerial images,‚Äù IEEE Geoscience and Remote Sensing\nLetters, vol. 14, pp. 1845‚Äì1849, 2017.\n[28] B. Fang, L. Pan, and R. Kou, ‚ÄúDual learning-based siamese\nframework for change detection using bi-temporal vhr optical\nremote sensing images,‚Äù Remote Sensing , vol. 11, no. 11, p.\n1292, 2019.\n[29] W. Zhao, X. Chen, X. Ge, and J. Chen, ‚ÄúUsing adversarial net-\nwork for multiple change detection in bitemporal remote sensing\nimagery,‚ÄùIEEE Geoscience and Remote Sensing Letters , pp. 1‚Äì\n5, 2020.\n[30] H. Chen, W. Li, and Z. Shi, ‚ÄúAdversarial instance augmentation\nfor building change detection in remote sensing images,‚Äù IEEE\nTransactions on Geoscience and Remote Sensing , pp. 1‚Äì16,\n2021.\n[31] W. Zhao, L. Mou, J. Chen, Y . Bo, and W. J. Emery, ‚ÄúIncor-\nporating metric learning and adversarial network for seasonal\ninvariant change detection,‚ÄùIEEE Trans. Geosci. Remote. Sens.,\nvol. 58, no. 4, pp. 2720‚Äì2731, 2020.\n[32] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning\nfor image recognition,‚Äù in 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV ,\nUSA, June 27-30, 2016 . IEEE Computer Society, 2016, pp.\n770‚Äì778.\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth\n16x16 words: Transformers for image recognition at scale.‚Äù\n[34] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,\nand H. J ¬¥egou, ‚ÄúTraining data-efÔ¨Åcient image transformers &\ndistillation through attention.‚Äù\n[35] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka,\nK. Keutzer, and P. Vajda, ‚ÄúVisual transformers: Token-based im-\nage representation and processing for computer vision,‚Äù CoRR,\nvol. abs/2006.03677, 2020.\n[36] D. Zhang, H. Zhang, J. Tang, M. Wang, X. Hua, and Q. Sun,\n‚ÄúFeature pyramid transformer,‚Äù in Computer Vision ‚Äì ECCV\n2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds.\nCham: Springer International Publishing, 2020, pp. 323‚Äì339.\n[37] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu,\nJ. Feng, T. Xiang, P. H. S. Torr, and L. Zhang, ‚ÄúRethinking\nsemantic segmentation from a sequence-to-sequence perspective\nwith transformers.‚Äù\n[38] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù\n14\nin Computer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part I , ser.\nLecture Notes in Computer Science, A. Vedaldi, H. Bischof,\nT. Brox, and J. Frahm, Eds., vol. 12346. Springer, 2020, pp.\n213‚Äì229.\n[39] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai,\n‚ÄúDeformable {detr}: Deformable transformers for end-to-end\nobject detection,‚Äù in International Conference on Learning\nRepresentations, 2021. [Online]. Available: https://openreview.\nnet/forum?id=gZ9hCDWe6ke\n[40] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, ‚ÄúGenerative pretraining from pixels,‚Äù in Proceed-\nings of the 37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event , ser. Proceedings\nof Machine Learning Research, vol. 119. PMLR, 2020, pp.\n1691‚Äì1703.\n[41] P. Esser, R. Rombach, and B. Ommer, ‚ÄúTaming transformers\nfor high-resolution image synthesis.‚Äù\n[42] W. Liu, S. Chen, L. Guo, X. Zhu, and J. Liu, ‚ÄúCptr: Full\ntransformer network for image captioning.‚Äù\n[43] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, ‚ÄúLearning texture\ntransformer network for image super-resolution,‚Äù jun 2020.\n[44] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma,\nC. Xu, C. Xu, and W. Gao, ‚ÄúPre-trained image processing\ntransformer,‚Äù 2020.\n[45] Y . Yuan and L. Lin, ‚ÄúSelf-supervised pre-training of transform-\ners for satellite image time series classiÔ¨Åcation,‚Äù IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote\nSensing, pp. 1‚Äì1, 2020.\n[46] Z. Li, G. Chen, and T. Zhang, ‚ÄúA cnn-transformer hybrid\napproach for crop classiÔ¨Åcation using multitemporal multisensor\nimages,‚Äù IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , vol. 13, pp. 847‚Äì858, 2020.\n[47] J. He, L. Zhao, H. Yang, M. Zhang, and W. Li, ‚ÄúHsi-bert:\nHyperspectral image classiÔ¨Åcation using the bidirectional en-\ncoder representation from transformers,‚Äù IEEE Transactions on\nGeoscience and Remote Sensing , vol. 58, no. 1, pp. 165‚Äì178,\n2020.\n[48] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and\nN. A. Ajlan, ‚ÄúVision transformers for remote sensing image\nclassiÔ¨Åcation,‚Äù Remote Sensing, vol. 13, no. 3, 2021.\n[49] X. Shen, B. Liu, Y . Zhou, and J. Zhao, ‚ÄúRemote sensing image\ncaption generation via transformer and reinforcement learning,‚Äù\nMultim. Tools Appl. , vol. 79, no. 35-36, pp. 26 661‚Äì26 682,\n2020.\n[50] Q. Wang, W. Huang, X. Zhang, and X. Li, ‚ÄúWord-sentence\nframework for remote sensing image captioning,‚Äù IEEE Trans-\nactions on Geoscience and Remote Sensing , pp. 1‚Äì12, 2020.\n[51] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,‚Äù in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers) . Minneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171‚Äì4186.\n[52] T. Q. Nguyen and J. Salazar, ‚ÄúTransformers without tears:\nImproving the normalization of self-attention,‚Äù CoRR, vol.\nabs/1910.05895, 2019.\n[53] D. Hendrycks and K. Gimpel, ‚ÄúGaussian error linear units\n(gelus).‚Äù\n[54] S. Ji, S. Wei, and M. Lu, ‚ÄúFully convolutional networks for\nmultisource building extraction from an open aerial and satellite\nimagery data set,‚ÄùIEEE Trans. Geoscience and Remote Sensing,\nvol. 57, no. 1, pp. 574‚Äì586, 2019.\n[55] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang,\n‚ÄúUnet++: A nested u-net architecture for medical image\nsegmentation,‚Äù in Deep Learning in Medical Image Analysis\n- and - Multimodal Learning for Clinical Decision Support\n- 4th International Workshop, DLMIA 2018, and 8th\nInternational Workshop, ML-CDS 2018, Held in Conjunction\nwith MICCAI 2018, Granada, Spain, September 20, 2018,\nProceedings, ser. Lecture Notes in Computer Science, vol.\n11045. Springer, 2018, pp. 3‚Äì11. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-00889-5 1\n[56] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural\nnetworks,‚Äù jun 2018."
}