{
  "title": "Unveiling the Role of Feed-Forward Blocks in Contextualization: An Analysis Using Attention Maps of Large Language Models",
  "url": "https://openalex.org/W4399757118",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2099167797",
      "name": "MichaÃ«l Tremblay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2985784892",
      "name": "Sarah Gervais",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5099169664",
      "name": "David Maisonneuve",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4393335149",
    "https://openalex.org/W4372283945",
    "https://openalex.org/W4392369006",
    "https://openalex.org/W4390573194",
    "https://openalex.org/W4398173773",
    "https://openalex.org/W4396914777",
    "https://openalex.org/W4398196388",
    "https://openalex.org/W4398782346",
    "https://openalex.org/W4391697043",
    "https://openalex.org/W4388691793",
    "https://openalex.org/W4393985835",
    "https://openalex.org/W4393399652",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4375869868"
  ],
  "abstract": "Transformer-based models have significantly impacted the field of natural language processing, enabling high-performance applications in machine translation, summarization, and language modeling. Introducing a novel analysis of feed-forward blocks within the Mistral Large model, this research provides critical insights into their role in enhancing contextual embeddings and refining attention mechanisms. By conducting a comprehensive evaluation through quantitative metrics such as perplexity, BLEU, and ROUGE scores, the study demonstrates the effectiveness of fine-tuning in improving model performance across diverse linguistic tasks. Detailed attention map analysis revealed the intricate dynamics between self-attention mechanisms and feed-forward blocks, highlighting the latter's importance in contextual refinement. The findings demonstrate the potential of optimized transformer architectures in advancing the capabilities of LLMs, emphasizing the necessity of domain-specific fine-tuning and architectural enhancements. Empirical evidence presented in this study offers a deeper understanding of the functional contributions of feed-forward blocks, informing the design and development of future LLMs to achieve superior performance and applicability.",
  "full_text": null,
  "topic": "Contextualization",
  "concepts": [
    {
      "name": "Contextualization",
      "score": 0.9465961456298828
    },
    {
      "name": "Computer science",
      "score": 0.5346217751502991
    },
    {
      "name": "Linguistics",
      "score": 0.4330231249332428
    },
    {
      "name": "Natural language processing",
      "score": 0.41565147042274475
    },
    {
      "name": "Programming language",
      "score": 0.17935052514076233
    },
    {
      "name": "Philosophy",
      "score": 0.13620832562446594
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    }
  ],
  "institutions": []
}