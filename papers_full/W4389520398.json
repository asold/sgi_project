{
  "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
  "url": "https://openalex.org/W4389520398",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3035520951",
      "name": "António Farinhas",
      "affiliations": [
        "Instituto Superior D. Dinis",
        "Instituto de Telecomunicações"
      ]
    },
    {
      "id": "https://openalex.org/A2098691967",
      "name": "José De Souza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122534586",
      "name": "André Martins",
      "affiliations": [
        "Instituto de Telecomunicações",
        "Instituto Superior D. Dinis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385572830",
    "https://openalex.org/W4380995299",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W2963251942",
    "https://openalex.org/W1969482724",
    "https://openalex.org/W2100128988",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W4379933104",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4287889965",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4390035079",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W2971120622",
    "https://openalex.org/W3163443091",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W3172669006",
    "https://openalex.org/W2909737760",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4389520222",
    "https://openalex.org/W4297833460",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2108984739",
    "https://openalex.org/W3084095723",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W4226269280",
    "https://openalex.org/W4389777735",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W4307311835",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385573728",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4362679254",
    "https://openalex.org/W4319323306",
    "https://openalex.org/W4389518624",
    "https://openalex.org/W4386566763",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W3118026775",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3169369929",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2146292423",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2135293965",
    "https://openalex.org/W4283828387"
  ],
  "abstract": "Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11956–11970\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAn Empirical Study of Translation Hypothesis Ensembling\nwith Large Language Models\nAntónio Farinhas1,2 José G. C. de Souza3 André F. T. Martins1,2,3\n1Instituto Superior Técnico (Lisbon ELLIS Unit)\n2Instituto de Telecomunicações 3Unbabel\n{antonio.farinhas,andre.t.martins}@tecnico.ulisboa.pt, jose.souza@unbabel.com\nAbstract\nLarge language models (LLMs) are becoming a\none-fits-many solution, but they sometimes hal-\nlucinate or produce unreliable output. In this pa-\nper, we investigate how hypothesis ensembling\ncan improve the quality of the generated text for\nthe specific problem of LLM-based machine\ntranslation. We experiment with several tech-\nniques for ensembling hypotheses produced by\nLLMs such as ChatGPT, LLaMA, and Alpaca.\nWe provide a comprehensive study along multi-\nple dimensions, including the method to gener-\nate hypotheses (multiple prompts, temperature-\nbased sampling, and beam search) and the strat-\negy to produce the final translation (instruction-\nbased, quality-based reranking, and minimum\nBayes risk (MBR) decoding). Our results show\nthat MBR decoding is a very effective method,\nthat translation quality can be improved using\na small number of samples, and that instruc-\ntion tuning has a strong impact on the relation\nbetween the diversity of the hypotheses and\nthe sampling temperature. Our code is avail-\nable at https://github.com/deep-spin/\ntranslation-hypothesis-ensembling.\n1 Introduction\nSignificant research effort has been devoted to task-\nspecific neural machine translation (NMT) models\ntrained in a fully supervised manner with large vol-\numes of parallel data. Their performance has been\nenhanced through techniques such as fine-tuning\non in-domain data, model ensembling, and rerank-\ning during decoding (Kocmi et al., 2022). The\nrecent achievements of general-purpose large lan-\nguage models (LLMs) such as GPT and LLaMA\n(OpenAI, 2023; Touvron et al., 2023) offer a fresh\nperspective on the problem, demonstrating the fea-\nsibility of generating high-quality translations with-\nout explicit training for the specific task, even in a\nchallenging zero-shot scenario (Hendy et al., 2023).\nWhile techniques such as greedy decoding or\nsampling from the distribution often prove inade-\nquate for generating translations with task-specific\nmodels, the same cannot be said for LLM-based\nMT. There is, however, a lack of exploration in this\ncase. Our paper fills this gap by providing a com-\nprehensive study on ensembling translation hy-\npotheses (§2), encompassing multiple LLMs such\nas ChatGPT, LLaMA, and the instruction-tuned\nAlpaca (Taori et al., 2023). We consider different\nstrategies to generate hypotheses (prompt-based en-\nsembling, temperature sampling, beam search), and\nseveral techniques to produce the final translation,\nincluding ChooseBest, GenerateBest, reranking\nbased on quality estimation, and minimum Bayes\nrisk decoding. The last two approaches have been\nsuccessful at improving translation quality with\ntask-specific models (Fernandes et al., 2022; Fre-\nitag et al., 2022a), but it is unclear whether the\nfindings hold for LLM-based MT.\nOur main findings can be summarized as fol-\nlows. First, we demonstrate that translation qual-\nity can be enhanced with a small number of\nsamples (e.g., 20), especially when translating out\nof English (Fig. 1). In the case of ChatGPT, the\ncost in terms of paid tokens grows sublinearly with\nthe number of samples (§3.2). Second, we show\nthat similar findings apply to LLaMA and Alpaca\n(§3.3). We discuss in which conditions beam search\nremains a reliable baseline for single-hypothesis\ntranslation (§3.3.1) and how to ensemble transla-\ntions (§3.3.2). Moreover, we find that there exists\na significant gap in the quality of ensembles of un-\nbiased samples from LLaMA and Alpaca (§3.3.3).\nWe attribute this disparity to how instruction tun-\ning affects the relationship between the diversity\nof the hypotheses and the sampling tempera-\nture, which ultimately impacts translation quality.\nLastly, we show that hypothesis ensembling re-\nduces the number of generated hallucinations,\nthereby improving the model’s robustness to source\nperturbations (§3.3.4). Ensembling predictions and\nincreasing the model size narrows the quality gap\nbetween these models and ChatGPT.\n11956\nFigure 1: COMET and BLEURT scores for translations\nproduced by ChatGPT. The greedy search output is indi-\ncated by a blue bold line, and a single sample baseline\nby a red bold line. Ensembles of multiple (20) predic-\ntions are marked with dashed lines: orange for ranking\nwith COMET KIWI and green for MBR decoding with\nCOMET . Top: EN-X. Bottom: X-EN.\n2 Ensembling Hypotheses\nEnsembling has a long history in machine learn-\ning, being well known for leveraging multiple com-\nplementary systems to improve performance on\na given task and provide good/robust generaliza-\ntion (Hansen and Salamon, 1990; Ting and Witten,\n1997; Breiman, 2001; Zhou et al., 2002). While\nthere have been efforts in prompt ensembling and\nmulti-prompt learning within the context of LLMs,\nthis area is largely unexplored for text generation\ntasks, where the output is a string rather than just a\nsingle token (Liu et al., 2023). See §4 for further\ndetails. In this section, we delve into the process of\ngenerating multiple translation hypotheses (§2.1)\nand explore different methods for ensembling them\nto produce a single translation (§2.2).1\n2.1 Generating multiple hypotheses\nThere are several ways of generating multiple pre-\ndictions from a single language model. In zero-\n1Ensembling predictions in this context should not be con-\nfused with the practice of model ensembling, which involves\nusing multiple models (e.g., with different initializations) and\ncombining their outputs. In this paper, we focus on combining\nhypotheses generated from a single model. The framework re-\nmains valid if the hypotheses originate from different models.\nshot scenarios where no examples are provided in\nthe prompt, we can consider (1) choosing a sin-\ngle prompt and sampling with a temperature such\nthat the resulting predictions are diverse, (2) fixing\nthe sampling temperature and considering multiple\nprompt templates, or (3) choosing a single prompt\nthat makes the model generate multiple predictions.\nRefer to App. A.1 for specific prompt templates.\nWhile this paper does not cover in-context learning,\nthese strategies can also be applied in few-shot sce-\nnarios where in-context examples are provided in\nthe prompt. In such cases, multiple prompts can be\ncreated by providing different in-context examples.\n2.2 Generating a final translation\nLet ¯Y ⊆ Ybe a set of N hypotheses, possibly\ngenerated with one of the methods discussed in\n§2.1. When it comes to providing a final output, a\ncommonly used approach involves aggregating the\nhypotheses in ¯Yby selecting the most frequent one\nwith majority voting (Wang et al., 2023a). How-\never, this approach is not well suited for generation\ntasks such as MT given that the output consists of a\nsequence of multiple tokens. Therefore, we explore\nalternative methods that incorporate both external\nmodels (§2.2.1) and the LLM itself (§2.2.2).\n2.2.1 Using external models\nAssuming access to an external model f that pro-\nvides an estimated quality score for a hypothe-\nsis y ∈ ¯Ywithout requiring a ground truth ( e.g.,\nCOMET KIWI (Rei et al., 2022b)), a simple ap-\nproach consists of choosing the hypothesis that\nmaximizes this score (Fernandes et al., 2022),\nˆyranking := arg max\ny∈¯Y\nf(y). (1)\nAnother alternative is minimum Bayes risk\n(MBR) decoding, which aims to find the output\nthat maximizes an expectedutility function u(y∗, y)\nthat measures the similarity between a hypothesis\ny ∈Y and a reference y∗∈¯Y(Kumar and Byrne,\n2002; Eikema and Aziz, 2020). For MT, this can\nbe an automatic evaluation metric such as COMET\n(Rei et al., 2020). MBR decoding seeks for\nˆymbr := arg max\ny∈¯Y\nEY∼pθ(y|x)[u(Y, y)], (2)\nwhere the expectation in Eq. 2 is typically approxi-\nmated as a Monte Carlo (MC) sum,\nEY∼pθ(y|x)[u(Y, y)] ≈ 1\nM\nM∑\nj=1\nu(y(j), y), (3)\n11957\nusing M model samples y(1), . . . , y(M) ∼pθ(y|x),\nyielding an unbiased MC estimate of the expected\nutility. Alternatively, we may obtainy(1), . . . , y(M)\nfrom temperature/nucleus sampling (Holtzman\net al., 2020), resulting in a biased estimate. While\nthe number of samples used to approximate the\nexpected utility of each hypothesis can be smaller\n(Eikema and Aziz, 2022), we set M = N.\n2.2.2 Using the LLM\nWhile the techniques above rely on external mod-\nels for assessing quality, we also propose alterna-\ntive methods which do not need any external (task-\nspecific) model. We consider two scenarios:2\n• using the LLM to select the most appropriate\nhypothesis from ¯Y(formulated as a multiple\nchoice question), which we call ChooseBest;\n• asking the LLM to generate a final prediction\nbased on the hypotheses in ¯Y(i.e., a less re-\nstrictive scenario where the model has the free-\ndom to either generate a new prediction or to\nchoose one hypothesis from ¯Y), which we call\nGenerateBest.\nThe prompt templates for these methods are pro-\nvided in App. A.2.\n2.3 Measuring hypothesis diversity\nInspired by the work of Fomicheva et al. (2020)\non quantifying model uncertainty, we measure the\nsemantic diversity between different translations\nfor the same source sentence by computing\n1 − 1\nN(N −1)\nN∑\ni,j=1\nj̸=i\nu(y(j), y(i)). (4)\nIt is worth noting that when u is the same utility\nfunction used in Eq. 2, this quantity can be com-\nputed without any additional cost during inference\nwith MBR decoding, as it already provides scores\nfor all the necessary pairwise comparisons.\n3 Experiments\n3.1 Setup\nWe study different methods for generating transla-\ntions in two regimes:\n2Another possibility, which we leave for future work, in-\nvolves a sequential architecture that samples predictions non-\nindependently by providing in the prompt the answers gen-\nerated in previous steps and taking the last prediction as the\nfinal output. This is known in statistics as stacking (Breiman,\n1996) and is related to the work of Madaan et al. (2023).\n• A closed-source setting using ChatGPT 3, an\nLLM developed by OpenAI, which has been\nshown to provide high quality translation (Hendy\net al., 2023; Peng et al., 2023a). The system\nis restricted behind API walls, with undisclosed\ntraining data/regime and limited documentation.\nAccording to their documentation, it is an In-\nstructGPT model (Ouyang et al., 2022), trained\nto follow instructions with reinforcement learn-\ning from human feedback (Christiano et al., 2017;\nStiennon et al., 2020) using proximal policy opti-\nmization algorithms (Schulman et al., 2017).4\n• An open-source scenario using LLaMA (Tou-\nvron et al., 2023) and Alpaca (Taori et al., 2023).\nThe latter was finetuned from a LLaMA model\non an instruction-following dataset with 52K ex-\namples generated with text-davinci-003, fol-\nlowing Wang et al. (2023b). We use the versions\nwith 7B parameters unless otherwise stated.\nAs our translation baseline, we employ greedy\ndecoding since it generally produces higher-quality\noutputs, in line with the findings of Peng et al.\n(2023a), who demonstrate that using a lower sam-\npling temperature leads to improved performance.5\nIn this work, we use COMET KIWI (Rei et al.,\n2022b) for ranking according to Eq. 1 andCOMET (-\n22) (Rei et al., 2022a) as the utility function in\nMBR decoding, following Eq. 2. We consider\n8 different translation directions, including lan-\nguages such as English (EN), German (DE), Rus-\nsian (RU), Czech (CS), and Ukrainian (UK). We\nuse the WMT22 test sets (Kocmi et al., 2022),\nwhich are recent, and thus less likely to have been\npart of ChatGPT’s training (see footnote 4). Fol-\nlowing Freitag et al. (2022b), we evaluate each\nsystem with COMET (-22) (Rei et al., 2022a) and\nBLEURT (Sellam et al., 2020).\n3.2 Closed-source setting\nWe generate a set of translation hypotheses for each\nsource sentence by sampling from the model with\n3https://openai.com/blog/chatgpt. Our experi-\nments were conducted with the gpt-3.5-turbo model be-\ntween April and June 2023.\n4For more information, seehttps://platform.openai.\ncom/docs/model-index-for-researchers . According\nto this, ChatGPT was trained on data from before Q4 2021.\n5We encountered some API/server errors when prompting\nChatGPT for translation with a temperature of 0, as reported\nby Guerreiro et al. (2023). Using a temperature of 0.1 helps\nalleviate these issues, which we use as a proxy for greedy\ndecoding. This is not done when using LLaMA/Alpaca.\n11958\nN M ETHOD EN-DE EN-RU EN-CS EN-UK\nCOMET BLEURT Cost COMET BLEURT Cost COMET BLEURT Cost COMET BLEURT Cost\n1 Greedy 87.01 77.15 1 87.77 75.61 1 90.04 80.98 1 87.66 76.08 1\nSampling 86.56 76.67 1 87.15 74.62 1 89.20 79.81 1 86.63 74.13 1\nusing ChatGPT\n5 ChooseBest 87.15 77.38 6 87.77 75.47 7 90.21 81.07 6 87.56 75.60 7\nGenerateBest 87.16 77.69 5 87.30 75.17 6 90.05 80.89 6 87.54 75.59 7\nusing external models\n5\nRanking 87.58 77.70 3 88.72 76.70 3 91.02 82.14 3 88.82 76.86 3\nMBR decoding 87.77 77.71 3 88.88 76.37 3 91.37 81.78 3 89.23 76.87 3\nCOMEToracle 88.85 78.91 3 89.98 77.96 3 92.18 83.07 3 89.23 78.39 3\n20\nRanking 87.64 77.86 8 88.96 76.89 10 91.40 82.64 10 89.29 77.47 11\nMBR decoding 88.09 78.09 8 89.41 76.73 10 91.97 82.45 10 90.03 77.77 11\nCOMEToracle 89.88 80.22 8 90.61 79.41 10 92.26 84.55 10 91.54 80.29 11\n50\nRanking 87.74 78.06 19 89.17 77.17 24 91.52 82.80 23 89.48 77.69 27\nMBR decoding 88.25 78.14 19 89.64 77.04 24 92.21 82.66 23 90.31 78.03 27\nCOMEToracle 90.39 80.89 19 91.74 80.66 24 93.75 85.29 23 92.10 81.22 27\nDE-EN RU-EN CS-EN UK-EN\nCOMET BLEURT Cost COMET BLEURT Cost COMET BLEURT Cost COMET BLEURT Cost\n1 Greedy 85.45 74.50 1 85.99 77.92 1 87.13 77.42 1 85.63 76.50 1\nSampling 85.18 74.06 1 85.68 77.48 1 86.72 76.88 1 85.23 75.88 1\nusing ChatGPT\n5 ChooseBest 85.37 74.43 5 85.90 77.82 4 87.09 77.33 4 85.51 76.30 4\nGenerateBest 85.46 74.54 4 85.66 77.56 4 87.08 77.48 4 85.46 76.40 4\nusing external models\n5\nRanking 85.62 74.61 2 86.22 78.08 2 87.34 77.48 2 85.85 76.72 2\nMBR decoding 85.73 74.58 2 86.27 78.05 2 87.49 77.51 2 85.97 76.55 2\nCOMEToracle 86.83 75.73 2 87.46 79.57 2 88.65 79.12 2 87.27 78.27 2\n20\nRanking 85.79 74.81 6 86.36 78.22 5 87.46 77.79 6 85.99 76.86 5\nMBR decoding 85.95 74.69 6 86.49 78.27 5 87.70 77.71 6 86.22 76.77 5\nCOMEToracle 87.75 76.88 6 88.42 81.03 5 89.57 80.74 6 88.27 79.66 5\n50\nRanking 85.79 74.78 13 86.32 78.13 12 87.47 77.67 13 85.94 76.81 11\nMBR decoding 86.03 74.80 13 86.60 78.39 12 87.79 77.73 13 86.28 76.87 11\nCOMEToracle 88.18 77.49 13 88.95 81.86 12 90.02 81.58 13 88.80 80.41 11\nTable 1: Automatic evaluation metrics for ChatGPT and total cost in terms of relative number of tokens normalized\nwithin each translation direction, rounded to the nearest unit. Sampling multiple predictions does not increase the\nprompt’s cost, hence the total cost does not increase (approximately) linearly with the number of samples. Ranking\nuses COMET KIWI and MBR decoding uses COMET . Best overall values are bolded.\na temperature of 1 (unbiased sampling) using the\nprompt of Hendy et al. (2023):\nTranslate this sentence from [source\nlanguage] to [target language].\nSource: [source sentence].\nTarget:\nWe observe that using multiple samples results\nin at least one significantly higher-quality transla-\ntion compared to a single prediction, as indicated\nby automatic evaluation metrics (see oracles for\nCOMET in Table 1). Increasing the number of hy-\npotheses in the set consistently leads to an oracle\ntranslation of superior quality, thus highlighting the\npotential of ensembling predictions.\nUsing different prompts. Based on preliminary\nexperiments conducted on a subset of the language\ndirections mentioned in §3.1, we observed that\ngenerating translations using different prompt tem-\nplates (see App. A.1) yields translations of compa-\nrable quality. Furthermore, ensembling predictions\nfrom different prompts does not lead to improved\nresults compared to ensembles generated using the\nsame prompt. Therefore, we use only one prompt\nin our further analysis.\n3.2.1 How should we ensemble translations?\nWe compare the methods introduced in §2.2,\nwhich include approaches that provide a final\nanswer using only the LLM ( ChooseBest and\nGenerateBest) or that require access to an ex-\nternal model (ranking with COMET KIWI and MBR\ndecoding with COMET as the utility function). Ta-\nble 1 shows the results for the automatic evaluation\nwith COMET and BLEURT, along with the relative\ncost of each method compared to greedy decoding.\nWe normalize the values within each translation di-\n11959\nN M ETHOD EN-DE EN-RU EN-CS EN-UK\nCOMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT\n1\nLLaMA greedy 77.33 62.86 71.57 50.80 71.56 52.33 69.06 44.47\nAlpaca greedy 76.67 64.06 75.59 59.52 71.40 56.61 72.76 53.40\nLLaMA beam 77.30 59.99 62.25 34.95 71.32 45.14 61.30 29.16\nAlpaca beam 78.59 65.95 77.71 61.62 76.34 60.81 76.68 55.09\nLLaMA unbiased sampling 51.98 36.02 42.79 21.91 42.11 21.36 42.39 20.66\nAlpaca unbiased sampling 68.15 54.86 61.50 44.15 54.90 37.45 56.80 36.44\nLLaMA biased sampling 69.78 55.68 63.41 42.41 60.26 39.78 59.59 36.28\nAlpaca biased sampling 73.42 60.65 70.25 53.19 63.97 47.70 66.45 45.91\nunbiased sampling\n50\nLLaMA ranking 77.68 65.25 75.29 57.71 69.45 52.08 71.19 50.78\nLLaMA MBR decoding 79.45 63.78 76.85 54.70 72.02 49.11 73.18 48.25\nAlpaca ranking 82.70 71.35 81.63 65.33 78.24 62.36 78.72 56.43\nAlpaca MBR decoding 84.23 70.58 83.94 65.97 81.09 62.53 81.70 59.33\nbiased sampling\nLLaMA ranking 83.04 71.91 82.93 67.41 81.07 66.88 81.12 62.24\nLLaMA MBR decoding 84.06 69.84 83.72 64.75 82.87 63.61 82.01 60.14\nAlpaca ranking 83.58 72.30 84.12 68.75 82.42 68.50 82.22 61.09\nAlpaca MBR decoding 84.54 71.18 85.44 68.32 84.82 68.16 84.30 63.42\nDE-EN RU-EN CS-EN UK-EN\nCOMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT\n1\nLLaMA greedy 82.36 70.19 81.58 71.62 81.26 69.90 81.37 71.13\nAlpaca greedy 82.31 70.14 81.65 71.89 81.14 69.69 81.34 70.90\nLLaMA beam 82.56 70.49 82.19 72.50 82.08 70.83 81.97 71.99\nAlpaca beam 82.53 70.40 82.08 72.29 81.69 70.26 81.55 71.09\nLLaMA unbiased sampling 73.26 60.23 70.63 58.62 70.19 57.20 70.72 59.13\nAlpaca unbiased sampling 81.04 68.66 79.92 69.62 79.03 67.20 79.52 68.85\nLLaMA biased sampling 79.82 67.60 78.75 68.14 78.10 66.07 78.71 67.84\nAlpaca biased sampling 81.86 69.71 81.09 71.02 80.44 68.81 80.72 70.33\nunbiased sampling\n50\nLLaMA ranking 82.90 70.64 82.12 71.74 81.85 69.97 82.17 71.70\nLLaMA MBR decoding 84.25 70.75 83.22 71.61 82.92 69.62 83.40 71.51\nAlpaca ranking 83.97 72.22 83.62 74.07 83.75 72.79 83.40 73.47\nAlpaca MBR decoding 84.47 71.78 83.95 73.50 84.00 71.78 83.58 72.46\nbiased sampling\nLLaMA ranking 84.03 72.15 83.44 73.73 83.58 72.32 83.50 73.47\nLLaMA MBR decoding 85.03 72.17 84.22 73.30 84.4 71.87 84.23 72.82\nAlpaca ranking 84.10 72.43 83.70 74.32 83.92 72.97 83.56 73.67\nAlpaca MBR decoding 84.02 71.31 83.56 73.33 83.61 71.47 83.08 72.22\nTable 2: Automatic evaluation metrics for LLaMA (7B) and Alpaca (7B). Ranking uses COMET KIWI and MBR\ndecoding uses COMET . Best overall values are bolded and best within each group are underlined.\nrection by dividing by the cost of greedy decoding.\nUsing ChatGPT. Although the performance of\nChooseBest and GenerateBest with 5 samples\nis slightly better than the single sample baseline,\nthese approaches still fall short of both the greedy\ndecoding output and the methods that use external\nmodels for selecting the final translation, according\nto both COMET and BLEURT. Furthermore, the\nincorporation of all translation hypotheses in the\nprompt (see App. A.2) significantly increases the\ncost, making these approaches less scalable. For\nthat reason, we chose not to pursue this direction\nfurther and instead focused our efforts on exploring\nthe methods described in §2.2.1.\nUsing external models. Table 1 shows that the\ntwo methods that use external models for ensem-\nbling predictions are effective at increasing the fi-\nnal translation quality over the baselines. Notably,\nthese methods achieve significant improvements\nwithout requiring an extensive number of unbiased\nsamples from the model’s distribution, especially\nwhen translating out of English. Fig. 1 provides a\nvisual representation of the gains achievable with\n20 samples. This differs from the findings of previ-\nous research using task-specific NMT models (Fer-\nnandes et al., 2022; Freitag et al., 2022a), where\nit is typically necessary to bias the model’s dis-\ntribution using techniques like nucleus sampling\n(Holtzman et al., 2020) or to train models without\n11960\nlabel smoothing (Eikema and Aziz, 2020; Freitag\net al., 2022a), which often leads to an impractical\nincrease in cost due to the need for more translation\nhypotheses. MBR decoding consistently achieves\nthe best results according to COMET across all\ntranslation directions. Although the differences in\nquality are small, this pattern does not hold when\nevaluating with BLEURT for EN-RU and EN-CS,\nfor which ranking with COMET KIWI appears to\nhave an edge.\n3.3 Open-source setting\nWe obtain sets of both biased and unbiased trans-\nlation hypotheses for each source sentence from\nLLaMA and Alpaca. The former is obtained by\nsampling with a temperature of 1, while the lat-\nter uses temperature and nucleus sampling (with\nt = 0 .8 and p = 0 .95), which are the defaults\nfor LLaMA.6 We use a variation of the prompt\nof Hendy et al. (2023) which stresses the transla-\ntion direction (crucial for the non instruction-tuned\nLLaMA to understand the task) as follows,\nTranslate this sentence from [source\nlanguage] to [target language].\n[source language] Source: [source\nsentence].\n[target language] Translation:\nWhile most works on translation with general-\npurpose LLMs typically present results using unbi-\nased sampling or greedy decoding, as it has been\nobserved that reducing the sampling temperature\ngenerally enhances translation quality (Peng et al.,\n2023a), it is worth exploring the impact of using\nbeam search (Reddy, 1977), the go-to search strat-\negy for decoding with task-specific models. Thus,\nin addition to greedy decoding, we employ beam\nsearch (with a beam size of 5) as a single hypothe-\nsis baseline. Notably, this is not possible for Chat-\nGPT (§3.2) because its API does not include beam\nsearch. We report results in Table 2, and use them\nto answer specific research questions next.\n3.3.1 Greedy vs. beam search baseline\nFig. 2 compares greedy search and beam search\nfor both LLaMA and Alpaca for X-EN (right) and\nEN-X (left) translation tasks. For X-EN, beam\nsearch outperforms greedy search, with LLaMA\nachieving the highest overall quality. However, the\n6We use the implementation in https://github.com/\nfacebookresearch/llama/tree/llama_v1.\nFigure 2: COMET scores for LLaMA and Alpaca with\ngreedy (blue) and beam search (orange). We represent\nLLaMA with solid lines and Alpaca with dashed lines.\nLeft: EN-X. Right: X-EN.\nEN-DE EN-RU EN-CS EN-UK\nLLaMA\nGreedy 11.2 25.0 21.5 31.6\nBeam 25.2 43.4 46.4 41.2\nRanking 1.3 2.5 3.6 11.3\nMBR 6.4 6.4 8.8 12.1\nAlpaca\nGreedy 2.1 2.7 4.5 13.6\nBeam 4.0 6.4 10.6 21.3\nRanking 0.4 1.8 3.1 17.1\nMBR 1.4 0.8 2.6 10.7\nTable 3: Percentage of translations in the wrong tar-\nget language when translating from EN. Ranking with\nCOMET KIWI and MBR decoding with COMET use bi-\nased samples. We do not show the values for X-EN\ngiven that only a few translations (<1% for all lan-\nguages) are not in EN. Best overall values are bolded\nand best for each model are underlined.\nresults are not as favorable when applying beam\nsearch to the non instruction-tuned LLaMA for\nEN-X (particularly for EN-RU and EN-UK). In\ncontrast, for Alpaca, beam search is consistently\nbetter in both language directions.\nWe hypothesize that this discrepancy is related\nto LLaMA’s relative inability to generate text in\nlanguages other than English. To validate this hy-\npothesis, we automatically identify the language of\nthe provided translations using the language iden-\ntification model of Joulin et al. (2016a,b). While\nboth LLaMA and Alpaca, with both greedy and\nbeam search, correctly provide translations in En-\nglish when requested (X-EN), the same cannot be\nsaid for other languages (see Table 3). In particu-\nlar, LLaMA frequently provides translations in the\nwrong target language (mostly in English), espe-\ncially with beam search. Interestingly, ensembling\npredictions (e.g., with ranking or MBR decoding)\neffectively mitigates this issue. Providing a few\nin-context examples, which we leave for future\n11961\nwork, is another alternative that may help improve\nLLaMA’s performance, alleviating the impact of\ndecoding with alternative methods.\n3.3.2 How should we ensemble translations?\nTable 2 shows that, for all models and translation\ndirections, a single sample baseline is not competi-\ntive at all with the greedy and beam search outputs,\nwith the latter achieving the best overall quality, as\ndiscussed in §3.3.1. Sampling’s poor performance\nis, however, more noticeable when translating from\nEnglish. Given that the overall quality scores are\nlower than that of ChatGPT (Table 1), in App. B\nwe report results for a larger version of LLaMA\n(with 30B parameters), for which we also observe\nperformance gains from ensembling translations.\nBesides, App. C contains additional results consid-\nering a few-shot learning scenario where in-context\nexamples are provided in the prompt.\nFor EN-X, ensembles of unbiased samples from\nLLaMA do not perform well, a topic we will further\nstudy in §3.3.3. Overall, Alpaca performs better,\nand the final quality of the ensemble can be boosted\nby biasing the samples (although the difference is\nnot very significant for EN-DE). MBR decoding\nwith COMET attains the best results in terms of\nCOMET , while ranking with COMET KIWI is better\nin terms of BLEURT for most language pairs.\nFor X-EN, while biased sampling is still advan-\ntageous and the best results in terms of BLEURT\nare still obtained with Alpaca (ranking with\nCOMET KIWI ), the best COMET scores are attained\nusing LLaMA (MBR decoding).\n3.3.3 Biasedness, diversity, and quality\nThere exists a significant gap in the final quality\nof an ensemble of unbiased samples from LLaMA\nand Alpaca, especially in the case of EN-X transla-\ntions, where LLaMA’s performance is notably poor.\nFor example, as shown in Table 2, the disparity\nin COMET and BLEURT scores for EN-DE is 5\nand 7 points, respectively. In this section, we study\nhow instruction tuning influences the relationship\nbetween candidate diversity and sampling tempera-\nture, and its impact on final translation quality. We\nconsider translations from English to German (see\nApp. D for the reversed direction) as a case study\nand measure translation diversity using the method\ndescribed in §2.3, with COMET as the similarity\nfunction u in Eq. 4.\nFig. 3 shows how the final translation quality,\nrepresented by the green and orange lines, and\nFigure 3: Values for BLEURT (bottom) and COMET\n(middle) for MBR decoding with COMET (green) and\nranking with COMET KIWI (orange), and diversity be-\ntween hypotheses (top; blue) as we increase the sam-\npling temperature for EN-DE. We represent LLaMA\nwith solid lines and Alpaca with dashed lines. The dot-\nted black lines (top) mark the increasing diversity gap.\nthe diversity between hypotheses, depicted by the\nblue lines, vary with the sampling temperature\nfor LLaMA (solid lines) and for Alpaca (dashed\nlines). As expected, the diversity between hy-\npotheses increases as the sampling temperature in-\ncreases. However, this occurs at a different rate\nfor LLaMA and Alpaca, indicating that instruction\ntuning changes the relationship between hypothe-\nsis diversity and sampling temperature. Ultimately,\nthis affects the final quality of the ensemble, which\nmay help explain the aforementioned quality gap\nfor ensembles of unbiased samples.\nAn interesting observation is the noticeable in-\ncrease in the diversity gap (i.e., the length of the dot-\nted black lines increases for temperatures ranging\nfrom 0.6 to 1.0 in Fig. 3), which coincides with a di-\nvergence in the translation quality between LLaMA\nand Alpaca ensembles (the solid and dashed lines\nbegin to separate). Additionally, it is worth not-\ning that the optimal COMET scores are attained\nat a candidate diversity of approximately 0.25 for\nboth models; however, this optimum corresponds\nto different sampling temperatures for each model.\nOverall, we conclude that instruction tuning has\na notable impact on the relation between hypothe-\nses diversity and sampling temperature, influencing\n11962\nEN-CS EN-UK\nLLaMA\nGreedy 10.2 21.2\nRanking 2.3 5.2\nMBR decoding 6.9 7.4\nAlpaca\nGreedy 2.3 5.8\nRanking 0.8 4.1\nMBR decoding 1.2 2.2\nTable 4: Rate of hallucinations (the percentage is\nover the number of sentences that passed the quality\nthreshold for non-perturbed sources). Ranking with\nCOMET KIWI and MBR decoding with COMET use bi-\nased samples. Best overall values are bolded and best\nfor each model are underlined.\nthe final translation quality. Notably, it is simpler to\nset an appropriate temperature for the instruction-\ntuned Alpaca, as it is less sensitive to such varia-\ntions. We observe that this effect is less pronounced\nwhen translating into English (refer to App. D),\nlikely due to the higher inherent similarity between\nhypotheses—potentially attributable to the exten-\nsive English training data used for these models.\n3.3.4 Hallucinations under perturbation\nIn this section, we study how robust LLaMA and\nAlpaca are to perturbations in the source text by\nsearching for hallucinations under perturbation,\nwhich correspond to situations where a model pro-\nduces drastically different translations for unper-\nturbed and slightly perturbed inputs (Lee et al.,\n2019; Raunak et al., 2021). We focus on EN-CS\nand EN-UK translations, given that hallucinations\nare typically more frequent when translating out\nof English and for lower resource languages. We\nfollow Guerreiro et al. (2023) and apply the min-\nimal perturbations of Xu et al. (2023), including\nmisspelling and title-casing words, and inserting\nfrequent tokens at the beginning of the source sen-\ntence. See App. E for further details.\nTable 4 shows that the hallucination rates de-\ncrease with instruction tuning for both EN-CS and\nEN-UK. Ensembling translation hypotheses further\ndecreases the number of hallucinations, suggesting\nthat considering multiple hypotheses is a promising\nmethod for alleviating this issue.\n4 Related Work\nEnsembling. Recently, Peng et al. (2023b) com-\npare the effectiveness of different model ensemble\nstrategies but focus on trained soft prompts and do\nnot explore generation tasks. There is also work on\nensembling predictions (produced by either sam-\npling multiple times or by using different prompts)\nwith majority voting (Wang et al., 2023a; Liévin\net al., 2023; Diao et al., 2023), which is not really\nsuited for MT as argued before, or along with other\ncomplementary approaches (Wang et al., 2022; Li\net al., 2022; Sun et al., 2023). There are several\nworks on ensembling for NMT, where a decoder\nuses multiple models (e.g., with different initializa-\ntions) and predicts an output by averaging token-\nlevel predictions from each model (Sutskever et al.,\n2014; Chung et al., 2016), whereas our approach\nconsiders full translations from a single model.\nRanking/rescoring hypotheses. Garcia et al.\n(2023) train their own language models, sample\nmultiple hypotheses and choose a final translation\nusing MBR decoding, which has been shown to\nimprove the translation capabilities of task-specific\nmodels (Fernandes et al., 2022; Freitag et al.,\n2022a). Their work is significantly different from\nours, since their models exclusively support two or\nthree languages at a time. Similarly, the approach\nof Yang et al. (2022) includes a reranking stage\n(including two trained dedicated rerankers) and an\nedit stage, while Kadavath et al. (2022) ask mod-\nels to directly evaluate the probability that their\nself-generated answers are correct.\nEditing/refining hypotheses. Raunak et al.\n(2023) explore translation editing with LLMs but\nthey do not study how to use external models (e.g.\nCOMET and COMET KIWI ) to improve translation\nquality. Similarly, Chen et al. (2023) propose to\niteratively refine translations using LLMs, attaining\ncomparable translation quality with the baseline,\naccording to automatic translation metrics, and re-\nducing translationese, according to a human study.\n5 Conclusions and Future Work\nWe have conducted a thorough empirical analy-\nsis on various techniques for generating transla-\ntions using LLMs. Our study encompasses eight\ndatasets and three model classes, including closed-\nsource and open-source models, the latter with and\nwithout instruction tuning. We have demonstrated\nthat ensembling predictions significantly enhances\ntranslation quality and reduces hallucinations under\nsource perturbations. Additionally, we have discov-\nered that instruction tuning affects the relationship\nbetween the diversity of sampled hypotheses and\nthe sampling temperature, which in turn influences\nthe final translation quality.\n11963\nThere are several avenues for future research in\naddition to the ones that have already been men-\ntioned in previous sections. While ensembling pre-\ndictions produced by LLMs is effective at improv-\ning translation quality, it also presents opportunities\nfor developing improved methods of uncertainty\nquantification and calibration (Fomicheva et al.,\n2020; Tian et al., 2023), crucial for addressing the\ninherent opacity of black-box LLMs.\nLimitations\nWe highlight three main limitations of our work.\nFirst, we primarily focus on versions of LLaMA\nand Alpaca with 7B parameters, even though we\nhave included additional results using a model with\n30B parameters in App. B. It remains unclear how\nour findings, such as the effects of employing beam\nsearch (§3.3.1), or how instruction tuning influ-\nences the relationship between sampling tempera-\nture and hypothesis diversity (§3.3.3), generalize\nto even larger models.\nSecond, we have included results from ChatGPT\ndue to its proven ability to provide high-quality\ntranslation. Notably, ChatGPT is a restricted sys-\ntem accessible only through APIs, and its training\ndata/regime are undisclosed. Since there is limited\ndocumentation, it is difficult to ensure that Chat-\nGPT did not encounter our evaluation benchmarks\nduring training, even though they are recent (§3.1).\nLastly, due to the high cost and time required\nfor conducting a final human assessment of the\ntranslation quality, we have not included it in our\nevaluation. Instead, we try to address this issue by\nreporting results based on multiple state-of-the-art\nautomatic evaluation metrics for machine transla-\ntion, such as COMET and BLEURT. Despite these\nlimitations, we believe that our findings hold sig-\nnificance for the ML/NLP community.\nEthics Statement\nChatGPT and Alpaca have been finetuned using\ninstructions and/or human feedback, which may\nbe low-quality, contradictory, or adversarial, pos-\nsibly resulting in inherent biases (Fernandes et al.,\n2023). For example, instructions may lack speci-\nficity, leading annotators to inadvertently evalu-\nate a slightly different task (Parmar et al., 2023).\nAnother concern arises from using quality estima-\ntion/evaluation models such as COMET KIWI and\nCOMET , which have been finetuned on human pref-\nerences. In such cases, annotators may fail to\nconsider better alternatives when presented with\na given text, resulting in the misclassification of\nisolated text as high quality (Bansal et al., 2021).\nAdditionally, all evaluation benchmarks used in\nthis study are openly accessible, and annotators\nwere allowed to label sensitive information when\nnecessary. Lastly, it is important to note that all\nLLMs exhibit a shared concern regarding their en-\nergy consumption, particularly during the training\nphase (Strubell et al., 2019).\nAcknowledgments\nWe would like to thank Pedro Martins, Nuno\nGuerreiro, Ben Peters, John Mendonça, Duarte\nAlves, Sweta Agrawal, and the SARDINE lab\nteam for helpful discussions. This work was\nbuilt on open-source software; we acknowledge\nVan Rossum and Drake (2009); Oliphant (2006);\nWalt et al. (2011), and (Paszke et al., 2019). This\nwork was supported by EU’s Horizon Europe\nResearch and Innovation Actions (UTTER, con-\ntract 101070631), by the project DECOLLAGE\n(ERC-2022-CoG 101088763), by the Portuguese\nRecovery and Resilience Plan through project\nC645008882- 00000055 (Center for Responsible\nAI), and by Fundação para a Ciência e Tecnologia\nthrough contract UIDB/50008/2020.\nReferences\nGagan Bansal, Tongshuang Wu, Joyce Zhou, Ray-\nmond Fok, Besmira Nushi, Ece Kamar, Marco Tulio\nRibeiro, and Daniel Weld. 2021. Does the whole\nexceed its parts? the effect of ai explanations on\ncomplementary team performance. In Proceedings\nof the 2021 CHI Conference on Human Factors in\nComputing Systems, CHI ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nLeo Breiman. 1996. Stacked regressions. Machine\nlearning, 24:49–64.\nLeo Breiman. 2001. Random forests. Machine learning,\n45:5–32.\nPinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-\nneth Heafield. 2023. Iterative translation refinement\nwith large language models.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nJunyoung Chung, Kyunghyun Cho, and Yoshua Bengio.\n2016. A character-level decoder without explicit seg-\n11964\nmentation for neural machine translation. In Proceed-\nings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1693–1703, Berlin, Germany. Association for\nComputational Linguistics.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models.\nBryan Eikema and Wilker Aziz. 2020. Is MAP decoding\nall you need? the inadequacy of the mode in neural\nmachine translation. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4506–4520, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nBryan Eikema and Wilker Aziz. 2022. Sampling-based\napproximations to minimum Bayes risk decoding\nfor neural machine translation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10978–10993, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nPatrick Fernandes, António Farinhas, Ricardo Rei,\nJosé G. C. de Souza, Perez Ogayo, Graham Neubig,\nand Andre Martins. 2022. Quality-aware decoding\nfor neural machine translation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1396–1412,\nSeattle, United States. Association for Computational\nLinguistics.\nPatrick Fernandes, Aman Madaan, Emmy Liu, António\nFarinhas, Pedro Henrique Martins, Amanda Bertsch,\nJosé G. C. de Souza, Shuyan Zhou, Tongshuang\nWu, Graham Neubig, and André F. T. Martins. 2023.\nBridging the gap: A survey on integrating (human)\nfeedback for natural language generation.\nMarina Fomicheva, Shuo Sun, Lisa Yankovskaya,\nFrédéric Blain, Francisco Guzmán, Mark Fishel,\nNikolaos Aletras, Vishrav Chaudhary, and Lucia Spe-\ncia. 2020. Unsupervised quality estimation for neural\nmachine translation. Transactions of the Association\nfor Computational Linguistics, 8:539–555.\nMarkus Freitag, David Grangier, Qijun Tan, and Bowen\nLiang. 2022a. High quality rather than high model\nprobability: Minimum Bayes risk decoding with neu-\nral metrics. Transactions of the Association for Com-\nputational Linguistics, 10:811–825.\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,\nCraig Stewart, Eleftherios Avramidis, Tom Kocmi,\nGeorge Foster, Alon Lavie, and André F. T. Martins.\n2022b. Results of WMT22 metrics shared task: Stop\nusing BLEU – neural metrics are better and more\nrobust. In Proceedings of the Seventh Conference\non Machine Translation (WMT), pages 46–68, Abu\nDhabi, United Arab Emirates (Hybrid). Association\nfor Computational Linguistics.\nYuan Gao, Ruili Wang, and Feng Hou. 2023. How to\ndesign translation prompts for chatgpt: An empirical\nstudy.\nXavier Garcia, Yamini Bansal, Colin Cherry, George\nFoster, Maxim Krikun, Fangxiaoyu Feng, Melvin\nJohnson, and Orhan Firat. 2023. The unreasonable\neffectiveness of few-shot learning for machine trans-\nlation.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nNuno M. Guerreiro, Duarte Alves, Jonas Waldendorf,\nBarry Haddow, Alexandra Birch, Pierre Colombo,\nand André F. T. Martins. 2023. Hallucinations in\nlarge multilingual translation models.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation: Nepali–English and Sinhala–\nEnglish. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6098–6111, Hong Kong, China. Association for Com-\nputational Linguistics.\nL.K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 12(10):993–1001.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hérve Jégou, and Tomas Mikolov.\n2016a. Fasttext.zip: Compressing text classification\nmodels. arXiv preprint arXiv:1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016b. Bag of tricks for efficient\ntext classification. arXiv preprint arXiv:1607.01759.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\n11965\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know.\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovi´c. 2022. Findings of the 2022\nconference on machine translation (WMT22). In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 1–45, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nShankar Kumar and William Byrne. 2002. Minimum\nBayes-risk word alignments of bilingual texts. InPro-\nceedings of the 2002 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP 2002),\npages 140–147. Association for Computational Lin-\nguistics.\nKatherine Lee, Orhan Firat, Ashish Agarwal, Clara Fan-\nnjiang, and David Sussillo. 2019. Hallucinations in\nneural machine translation.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022. On the\nadvance of making language models better reasoners.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nValentin Liévin, Christoffer Egeberg Hother, and Ole\nWinther. 2023. Can large language models reason\nabout medical questions?\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nTravis E Oliphant. 2006. A guide to NumPy, volume 1.\nTrelgol Publishing USA.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nMihir Parmar, Swaroop Mishra, Mor Geva, and Chitta\nBaral. 2023. Don’t blame the annotator: Bias al-\nready starts in the annotation instructions. In Pro-\nceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 1779–1789, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nKeqin Peng, Liang Ding, Qihuang Zhong, Li Shen,\nXuebo Liu, Min Zhang, Yuanxin Ouyang, and\nDacheng Tao. 2023a. Towards making the most of\nchatgpt for machine translation.\nXiangyu Peng, Chen Xing, Prafulla Kumar Choubey,\nChien-Sheng Wu, and Caiming Xiong. 2023b. Model\nensemble instead of prompt fusion: a sample-specific\nknowledge transfer method for few-shot prompt tun-\ning. In The Eleventh International Conference on\nLearning Representations.\nVikas Raunak, Arul Menezes, and Marcin Junczys-\nDowmunt. 2021. The curious case of hallucinations\nin neural machine translation. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1172–1183,\nOnline. Association for Computational Linguistics.\nVikas Raunak, Amr Sharaf, Hany Hassan Awadallah,\nand Arul Menezes. 2023. Leveraging gpt-4 for auto-\nmatic translation post-editing.\nRaj Reddy. 1977. Speech understanding systems: A\nsummary of results of the five-year research effort at\ncarnegie mellon university.\nRicardo Rei, José G. C. de Souza, Duarte Alves,\nChrysoula Zerva, Ana C Farinha, Taisiya Glushkova,\nAlon Lavie, Luisa Coheur, and André F. T. Martins.\n2022a. COMET-22: Unbabel-IST 2022 submission\nfor the metrics shared task. In Proceedings of the\nSeventh Conference on Machine Translation (WMT),\npages 578–585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\n11966\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro,\nChrysoula Zerva, Ana C Farinha, Christine Maroti,\nJosé G. C. de Souza, Taisiya Glushkova, Duarte\nAlves, Luisa Coheur, Alon Lavie, and André F. T.\nMartins. 2022b. CometKiwi: IST-unbabel 2022 sub-\nmission for the quality estimation shared task. In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT) , pages 634–645, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-augmented language\nmodels. In The Eleventh International Conference\non Learning Representations.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Se-\nquence to sequence learning with neural networks. In\nAdvances in Neural Information Processing Systems,\nvolume 27. Curran Associates, Inc.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No language left behind: Scaling human-\ncentered machine translation.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D. Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback.\nKai Ming Ting and Ian H. Witten. 1997. Stacked gener-\nalization: when does it work? In International Joint\nConference on Artificial Intelligence.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nGuido Van Rossum and Fred L. Drake. 2009. Python 3\nReference Manual. CreateSpace, Scotts Valley, CA.\nStéfan van der Walt, S Chris Colbert, and Gael Varo-\nquaux. 2011. The NumPy array: a structure for effi-\ncient numerical computation. Computing in Science\n& Engineering, 13(2):22–30.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022. Rationale-\naugmented ensembles in language models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023a. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodels with self-generated instructions.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Mari-\nanna J. Martindale, and Marine Carpuat. 2023. Un-\nderstanding and detecting hallucinations in neural\nmachine translation via model introspection.\nKevin Yang, Yuandong Tian, Nanyun Peng, and Dan\nKlein. 2022. Re3: Generating longer stories with\nrecursive reprompting and revision. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4393–4479, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study.\nZhi-Hua Zhou, Jianxin Wu, and Wei Tang. 2002. En-\nsembling neural networks: Many could be better than\nall. Artificial Intelligence, 137(1):239–263.\n11967\nA Prompt Templates\nA.1 Translation\nIn addition to the prompt of Hendy et al. (2023),\nused in the reported experiments with ChatGPT,\nTranslate this sentence from [source\nlanguage] to [target language].\nSource: [source sentence].\nTarget:\nwe also tried the prompt of Peng et al. (2023a),\nPlease provide the [target\nlanguage] translation for this sen-\ntence: [source sentence]\nthe prompt of Gao et al. (2023), which provides\nadditional information on the translation task and\nthe language pairs involved,\nThis is a [source language] to\n[target language] translation task,\nplease provide the[target language]\ntranslation for this sentence: [source\nsentence]\nand the prompt of Zhang et al. (2023), which is\nsimpler but concise,\n[source language]: [source\nsentence]\n[target language]:\nIn a preliminary stage of this work, we observed\nthat the results according to automatic evaluation\nmetrics were similar for all the prompts above. In\naddition, ensembling translations generated with\nmultiple prompts was not better than sampling hy-\npotheses using a single prompt template. We also\nattempted to generate multiple translations (N) with\nthe following prompt,\nTranslate this sentence from [source\nlanguage] to [target language] in\n[N] different ways.\nSource: [source sentence]\n[N] translations:\nHowever, we observed a decline in translation qual-\nity as the model generated subsequent translations,\nwith the first one exhibiting lower quality compared\nto the ones generated using the prompts mentioned\nabove. For example, in the case of EN-DE trans-\nlation, there was an approximate gap of 3 COMET\npoints between the first and last translation. For\nthat reason, we decided to discard this approach.\nFor LLaMA and Alpaca, we use a variation\nof the prompt of Hendy et al. (2023) which\nstresses the translation direction (crucial for the\nnon instruction-tuned LLaMA to understand the\ntask) as follows,\nTranslate this sentence from [source\nlanguage] to [target language].\n[source language] Source: [source\nsentence].\n[target language] Translation:\nA.2 Generation of the final translation\nWe formulate the task of choosing the most ap-\npropriate hypothesis (ChooseBest) as a multiple\nchoice question using the following prompt,\nThis is a multiple choice question,\nchoose a single answer. What is the\nbest [target language] translation\nfor this [source language] sentence?\nSource: [source]\nOption A. [hypothesis 1]\nOption B. [hypothesis 2]\n...\nCorrect answer: Option\nBesides, we ask the LLM to generate a final\nprediction based on the provided hypotheses\n(GenerateBest) using the following prompt,\nUse the following translation hypotheses\nto generate the best possible [target\nlanguage] translation for this [source\nlanguage] sentence.\nSource: [source]\nTranslation hypotheses:\n[hypothesis 1]\n[hypothesis 2]\n...\nBest possible translation:\n11968\nN M ETHOD EN-DE EN-RU EN-CS EN-UK\nCOMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT\n1 ChatGPT greedy 87.01 77.15 87.77 75.61 90.04 80.98 87.66 76.08\n1 Greedy 81.54 69.39 82.35 67.27 82.17 69.27 81.32 66.00\nBiased sampling 77.47 64.87 75.89 58.62 73.59 58.47 74.29 56.50\n20\nRanking 84.87 73.91 86.02 72.11 87.02 75.48 85.64 71.36\nMBR decoding 85.78 73.26 87.05 71.45 88.45 74.71 86.68 70.61\nCOMET oracle 87.74 75.83 88.85 74.13 89.70 76.39 88.51 73.41\n50\nRanking 85.16 74.37 86.75 72.93 88.11 76.84 86.29 71.61\nMBR decoding 86.48 73.97 87.79 72.37 89.61 76.33 87.62 71.52\nCOMET oracle 88.77 77.12 90.02 76.09 91.10 78.76 89.80 75.60\nDE-EN RU-EN CS-EN UK-EN\nCOMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT\n1 ChatGPT greedy 85.45 74.50 85.99 77.92 87.13 77.42 85.63 76.50\n1 Greedy 83.12 71.01 83.00 73.48 83.64 72.60 82.86 72.50\nBiased sampling 81.29 69.03 80.83 70.56 81.23 69.50 80.67 69.89\n20\nRanking 84.68 72.85 84.43 74.83 85.24 74.45 84.69 74.36\nMBR decoding 85.18 72.64 85.06 75.00 85.74 73.91 84.91 73.87\nCOMET oracle 87.33 75.70 87.22 78.14 87.89 77.13 87.62 77.78\n50\nRanking 84.76 72.96 84.53 75.05 85.45 74.46 84.70 74.77\nMBR decoding 85.48 72.97 85.34 75.18 86.17 74.29 85.33 74.29\nCOMET oracle 88.13 76.93 88.08 79.45 88.76 78.60 88.65 79.36\nTable 5: Automatic evaluation metrics for LLaMA (30B). We use temperature and nucleus sampling (with t = 0.8\nand p = 0.95), which is the default (see §3.3). Ranking uses COMET KIWI and MBR decoding uses COMET . Best\noverall values for LLaMA are bolded and best within each group are underlined. Values for ChatGPT with greedy\ndecoding are taken from Table 1 and highlighted in red.\nFEW-S HOT METHOD EN-DE EN-RU\nCOMET BLEURT COMET BLEURT\n0\nGreedy 77.33 62.86 71.57 50.80\nRanking 83.04 71.91 82.93 67.41\nMBR decoding 84.06 69.84 83.72 64.75\nCOMET oracle 86.51 73.42 86.59 69.43\n5\nGreedy 79.82 68.02 80.20 65.01\nRanking 83.72 72.90 84.98 70.38\nMBR decoding 85.42 72.66 86.64 70.37\nCOMET oracle 87.42 75.06 88.43 72.88\nTable 6: Automatic evaluation metrics for LLaMA (7B) with and without few-shot learning. For ranking with\nCOMET KIWI and MBR decoding with COMET we use 50 samples obtained through temperature and nucleus\nsampling (with t = 0.8 and p = 0.95), which is the default (see §3.3). Best overall values are bolded and best\nwithin each group are underlined.\nB Increasing Model Size\nTable 5 shows COMET and BLEURT scores for\nLLaMA (30B), along with the ChatGPT output\n(§3.2). The main findings discussed in §3.2 still\nhold: ensembling multiple translations is effective\nat improving the overall translation quality. No-\ntably, these quality scores are more competitive\nwith the ChatGPT baseline, suggesting that increas-\ning the number of model’s parameters is beneficial\neven without instruction tuning.\nC Few Shot Learning\nAs discussed in §2.1, the candidate generation\nstrategies described in our paper can also be ap-\nplied in few-shot scenarios where in-context exam-\nples are provided in the prompt. While this paper\n11969\ndoes not focus on this case and covers other or-\nthogonal dimensions in more detail (e.g., choice of\nmodel, method to generate hypotheses, strategy to\ngenerate the final translation), we include results\nwith few-shot examples on a subset of the language\npairs (EN-DE and EN-RU), in Table 6. We con-\nsider 5-shot examples from the FLORES-200 dev\nset (Guzmán et al., 2019; Goyal et al., 2022; Team\net al., 2022) and use LLaMA (7B). As expected, we\nsee that hypothesis ensembling still works well for\nsuch a setting, with the overall scores being higher\nthan in a 0-shot scenario.\nD Biasedness, Diversity, and Quality\nFig. 4 shows that the trends observed in Fig. 3 for\nEN-DE translations are also observable in the re-\nversed translation direction (DE-EN). Once again,\nincreasing the sampling temperature leads to an\nincrease in the diversity of the hypotheses, and this\ntrend varies at different rates for LLaMA and Al-\npaca. As the diversity gap increases, the translation\nquality between ensembles of samples from these\nmodels diverges. However, it is worth noting that\nthis effect is less pronounced for DE-EN, likely\ndue to the extensive English training data used for\nthese models that results in lower absolute values\nof translation diversity, as indicated by the blue\nlines.\nE Hallucinations\nWe follow the choices of Guerreiro et al. (2023)\nand detect hallucinations under perturbations as fol-\nlows. For each language pair, we start by obtaining\nsource sentences for which all methods (greedy,\nranking with COMET KIWI , and MBR decoding\nwith COMET ) generate unperturbed translations\nthat meet a minimum quality threshold (BLEU\n> 9). Then, we set a low maximum quality score\nfor perturbed translations (BLEU < 3). A model\ngenerates a hallucination when both thresholds are\nmet. The metric for measuring lexical overlap and\nthe threshold values we used follow previous work.\nFigure 4: Values for BLEURT (bottom) and COMET\n(middle) for MBR decoding with COMET (green) and\nranking with COMET KIWI (orange), and diversity be-\ntween hypotheses (top; blue) as we increase the sam-\npling temperature for DE-EN. We represent LLaMA\nwith solid lines and Alpaca with dashed lines. The dot-\nted black lines (top) mark the increasing diversity gap.\n11970",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6983464956283569
    },
    {
      "name": "Decoding methods",
      "score": 0.6426582932472229
    },
    {
      "name": "Translation (biology)",
      "score": 0.5888505578041077
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5754009485244751
    },
    {
      "name": "Machine translation",
      "score": 0.565276026725769
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5551580190658569
    },
    {
      "name": "Bayes' theorem",
      "score": 0.5538427829742432
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5009877681732178
    },
    {
      "name": "Diversity (politics)",
      "score": 0.4137813150882721
    },
    {
      "name": "Machine learning",
      "score": 0.39685338735580444
    },
    {
      "name": "Natural language processing",
      "score": 0.3798653185367584
    },
    {
      "name": "Bayesian probability",
      "score": 0.1740758717060089
    },
    {
      "name": "Algorithm",
      "score": 0.1736924648284912
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387152517",
      "name": "Instituto Superior Técnico",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210120471",
      "name": "Instituto de Telecomunicações",
      "country": "PT"
    }
  ],
  "cited_by": 4
}