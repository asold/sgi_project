{
  "title": "Unifying Global-Local Representations in Salient Object Detection with Transformer",
  "url": "https://openalex.org/W3191208739",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222638700",
      "name": "Ren, Sucheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2282052498",
      "name": "Wen Qiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227335107",
      "name": "Zhao, Nanxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306287056",
      "name": "Han Guo-qiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743096748",
      "name": "He, Shengfeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2172154848",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W2415535521",
    "https://openalex.org/W3097848036",
    "https://openalex.org/W1894057436",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W3158372329",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2963685207",
    "https://openalex.org/W2963032190",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2294182682",
    "https://openalex.org/W2519528544",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2014558261",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2293332611",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2884555738",
    "https://openalex.org/W3120857301",
    "https://openalex.org/W2740667773",
    "https://openalex.org/W2939217524",
    "https://openalex.org/W2039313011",
    "https://openalex.org/W2923601983",
    "https://openalex.org/W2798791651",
    "https://openalex.org/W3087299940",
    "https://openalex.org/W2561701092",
    "https://openalex.org/W2963906836",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2118615399",
    "https://openalex.org/W3034185160",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2780708736",
    "https://openalex.org/W3035422681",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2772161954",
    "https://openalex.org/W2963299740",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2744613561",
    "https://openalex.org/W2963868681",
    "https://openalex.org/W3107944836",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2990984982",
    "https://openalex.org/W2799074129",
    "https://openalex.org/W2963619659",
    "https://openalex.org/W2037954058",
    "https://openalex.org/W2948500402",
    "https://openalex.org/W2798825526",
    "https://openalex.org/W1983433674",
    "https://openalex.org/W3127912918",
    "https://openalex.org/W2987701848",
    "https://openalex.org/W2086791339"
  ],
  "abstract": "The fully convolutional network (FCN) has dominated salient object detection for a long period. However, the locality of CNN requires the model deep enough to have a global receptive field and such a deep model always leads to the loss of local details. In this paper, we introduce a new attention-based encoder, vision transformer, into salient object detection to ensure the globalization of the representations from shallow to deep layers. With the global view in very shallow layers, the transformer encoder preserves more local representations to recover the spatial details in final saliency maps. Besides, as each layer can capture a global view of its previous layer, adjacent layers can implicitly maximize the representation differences and minimize the redundant features, making that every output feature of transformer layers contributes uniquely for final prediction. To decode features from the transformer, we propose a simple yet effective deeply-transformed decoder. The decoder densely decodes and upsamples the transformer features, generating the final saliency map with less noise injection. Experimental results demonstrate that our method significantly outperforms other FCN-based and transformer-based methods in five benchmarks by a large margin, with an average of 12.17% improvement in terms of Mean Absolute Error (MAE). Code will be available at https://github.com/OliverRensu/GLSTR.",
  "full_text": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 1\nUnifying Global-Local Representations in\nSalient Object Detection with Transformers\nSucheng Ren, Nanxuan Zhao, Qiang Wen, Guoqiang Han, and Shengfeng He, Senior Member, IEEE\nAbstract—The fully convolutional network (FCN) has domi-\nnated salient object detection for a long period. However, the\nlocality of CNN requires the model deep enough to have a global\nreceptive field and such a deep model always leads to the loss\nof local details. In this paper, we introduce a new attention-\nbased encoder, vision transformer, into salient object detection\nto ensure the globalization of the representations from shallow\nto deep layers. With the global view in very shallow layers,\nthe transformer encoder preserves more local representations to\nrecover the spatial details in final saliency maps. Besides, as each\nlayer can capture a global view of its previous layer, adjacent\nlayers can implicitly maximize the representation differences and\nminimize the redundant features, making every output feature\nof transformer layers contribute uniquely to the final prediction.\nTo decode features from the transformer, we propose a simple\nyet effective deeply-transformed decoder. The decoder densely\ndecodes and upsamples the transformer features, generating the\nfinal saliency map with less noise injection. Experimental results\ndemonstrate that our method significantly outperforms other\nFCN-based and transformer-based methods in five benchmarks\nby a large margin, with an average of 12.17% improvement\nin terms of Mean Absolute Error (MAE). Code is available at\nhttps://github.com/OliverRensu/GLSTR.\nIndex Terms—Transformer, Salient object detection.\nI. I NTRODUCTION\nSalient object detection (SOD) aims at segmenting the most\nvisually attracting objects of the images, aligning with the\nhuman perception. It gained broad attention in recent years\ndue to its fundamental role in many vision tasks, such as\nimage manipulation [2], segmentation [3]–[5], autonomous\nnavigation [6], person re-identification [7], [8], and photo\ncropping [9].\nTraditional methods [10], [10]–[14] mainly rely on differ-\nent priors and hand-crafted features like color contrast [10],\nbrightness [12], foreground and background distinctness [13].\nThe work is supported by the Guangdong Natural Science Funds for\nDistinguished Young Scholars (No. 2023B1515020097), Singapore MOE Tier\n1 Funds (MSS23C002), and the National Research Foundation Singapore\nunder the AI Singapore Programme (No: AISG3-GV-2023-011). (Shengfeng\nHe is the corresponding author.)\nSucheng Ren and Shengfeng He are with the School of Computing and\nInformation Systems, Singapore Management University, Singapore. E-mail:\noliverrensu@gmail.com, shengfenghe@smu.edu.sg.\nNanxuan Zhao is with Adobe Research, San Jose, USA. E-mail: nanxu-\nanzhao@gmail.com.\nQiang Wen and Guoqiang Han are with the School of Computer Science\nand Engineering, South China University of Technology, Guangzhou, China.\nE-mail: csqiangwen@gmail.com, csgqhan@scut.edu.cn.\n@2024 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nInput GT Ours GateNet\nFig. 1: Examples of our method. We propose the Global-Local\nSaliency TRansformer (GLSTR) to unify global and local\nfeatures in each layer. We compare our method with a SOTA\nmethod, GateNet [1], based on FCN architecture. Our method\ncan localize salient region precisely with accurate boundary.\nHowever, these methods limit the representation power, with-\nout considering semantic information. Thus, they often fail\nto deal with complex scenes. With the development of deep\nconvolutional neural network (CNN), fully convolutional net-\nwork (FCN) [15] becomes an essential building block for\nSOD [16]–[19]. These methods can encode high-level seman-\ntic features and better localize the salient regions.\nHowever, due to the nature of locality, methods relying on\nFCN usually face a trade-off between capturing global and\nlocal features. To encode high-level global representations, the\nmodel needs to take a stack of convolutional layers to obtain\nlarger receptive fields, but it erases local details. To retain the\nlocal understanding of saliency, features from shallow layers\nfail to incorporate high-level semantics. This discrepancy may\nmake the fusion of global and local features less efficient. Is\nit possible to unify global and local features in each layer to\nreduce the ambiguity and enhance the accuracy for SOD?\nIn this paper, we aim to jointly learn global and local\nfeatures in a layer-wise manner for solving the salient object\ndetection task. Rather than using a pure CNN architecture,\nwe turn to transformer [20] for help. We get inspired from\nthe recent success of transformer on various vision tasks [21]\nfor its superiority in exploring long-range dependency. Trans-\narXiv:2108.02759v2  [cs.CV]  17 Mar 2024\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 2\nInput GT Layer 1 Layer 12\nFig. 2: The visual attention map of the red block in the input\nimage on the first and twelfth layers of the transformer. The\nattention maps show that features from shallow layer can also\nhave global information and features from deep layer can also\nhave local information.\nformer applies self-attention mechanism for each layer to\nlearn a global representation. Therefore, it is possible to\ninject globality in the shallow layers, while maintaining local\nfeatures. As illustrated in Fig. 2, the attention map of the red\nblock has the global view of the chicken and the egg even in\nthe first layer. The network does attend to small scale details\neven in the final layer (the twelfth layer). More importantly,\nwith the self-attention mechanism, the transformer is capable\nto model the “contrast”, which has demonstrated to be crucial\nfor saliency perception [22]–[24].\nWith the merits mentioned above, we propose a method\ncalled Global-Local Saliency TRansformer ( GLSTR). The\ncore of our method is a pure transformer-based encoder and\na mixed decoder to aggregate features generated by trans-\nformers. To encode features through transformers, we first\nsplit the input image into a grid of fixed-size patches. We\nuse a linear projection layer to map each image patch to a\nfeature vector for representing local details. By passing the\nfeature through the multi-head self-attention module in each\ntransformer layer, the model further encodes global features\nwithout diluting the local ones. To decode the features with\nglobal-local information over the inputs and the previous\nlayer from the transformer encoder, we propose to densely\ndecode feature from each transformer layer. With this dense\nconnection, the representations during decoding still preserve\nrich local and global features. We propose a novel framework\nto apply the pure transformer-based encoder on SOD tasks.\nTo sum up, our contributions are in three folds:\n• We unify global-local representations with a novel\ntransformer-based architecture, which models long-range\ndependency within each layer.\n• To take full advantage of the global information of\nprevious layers, we propose a new decoder, deeply-\ntransformed decoder, to densely decode features of each\nlayer.\n• We conduct extensive evaluations on five widely-used\nbenchmark datasets, showing that our method outper-\nforms state-of-the-art methods by a large margin, with\nan average of 12.17% improvement in terms of MAE.\nII. R ELATED WORK\nSalient Object Detection. FCN has become the mainstream\narchitecture for the saliency detection methods, to predict\npixel-wise saliency maps directly [16]–[19], [25]–[33]. There\nare several surveys [34], [35] about FCN-based SOD methods.\nEspecially, the skip-connection architecture has been demon-\nstrated its performance in combining global and local context\ninformation [1], [36]–[44]. Hou et al. [36] proposed that the\nhigh-level features are capable of localizing the salient regions\nwhile the shallower ones are active at preserving low-level\ndetails. Therefore, they introduced short connections between\nthe deeper and shallower feature pairs. Based on the U-Shape\narchitecture, Zhao et al. [1] introduced a transition layer and\na gate into every skip connection, to suppress the misleading\nconvolutional information in the original features from the\nencoder. To localize the salient objects better, they also built a\nFold-ASPP module on the top of the encoder. Similarly, Liu et\nal. [37] inserted a pyramid pooling module on the top of the\nencoder to capture the high-level semantic information. To re-\ncover the diluted information in the convolutional encoder and\nalleviate the coarse-level feature aggregation problem while\ndecoding, they introduced a feature aggregation module after\nevery skip connection. In [45], Zhao et al. adopted the VGG\nnetwork without fully connected layers as their backbone.\nThey fused the high-level feature into the shallower one at\neach scale to recover the diluted location information. After\nthree convolution layers, each fused feature was enhanced by\nthe saliency map. Specifically, they introduced the edge map\nas guidance for the shallowest feature and fused this feature\ninto deeper ones to produce the side outputs.\nThe aforementioned methods usually suffer from the limited\nsize of convolutional operation. Although there will be large\nreceptive fields in the deep convolution layers, the low-level\ninformation is diluted increasingly with the increasing number\nof layers.\nTransformer. Since the local information covered by the\nconvolutional operation is deficient, numerous methods adopt\nattention mechanisms to capture the long-range correlation\n[46]–[53]. The transformer, proposed by Vaswani et al. [20],\nhas demonstrated its global-attention performance in machine\ntranslation tasks. Recently, more and more works focus on\nexploring the ability of transformers in computer vision tasks.\nDosovitskiy et al. [21] proposed to apply the pure transformer\ndirectly to sequences of image patches for exploring spatial\ncorrelation on image classification tasks. Carion et al. [54]\nproposed a transformer encoder-decoder architecture on object\ndetection tasks. By collapsing the feature from CNN into a\nsequence of learned object queries, the transformer can dig out\nthe correlation between objects and the global image context.\nZeng et al. [55] proposed a spatial-temporal transformer\nnetwork to tackle video inpainting problems along spatial and\ntemporal channels jointly.\nThe most related work to ours is [56]. They proposed to\nreplace the convolution layers with the pure transformer during\nencoding for the semantic segmentation task. Unlike this work,\nwe propose a transformer-based U-shape architecture network\nthat is more suitable for dense prediction tasks. With our\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 3\nLinear \nProjection\nN=12\nLayer Norm\nMulti-Head\nAttention\nLayer Norm\nMLP\nPatch \nEmbedding\nPosition \nEmbedding\nReshape, Pixel Shuffle (x4)\nBilinear Interpolation (x2)\nReshape, Pixel Shuffle (x2)\nBilinear Interpolation (x2)\nReshape,\nBilinear Interpolation (x2) Pixel Shuffle (x2)\nN=4 N=4\nN=16x16\nConv Layer\nN=16x16\nSaliency GT Network output\nW/8\nH/8\nW/4\nH/4\nW/2\nH/2\nW\nH\nSupervision\nTransformer \nLayer\nTransformer \nLayer\nTransformer \nLayer\nTransformer \nLayer\nStage 1 Stage 2 Stage 3\nDeeply-transformed Decoder\nTransformer Encoder\nFig. 3: The pipeline of our proposed method. We first divide the image into non-overlap patches and map each patch into\nthe token before feeding to transformer layers. After encoding features through 12 transformer layers, we decode each output\nfeature in three successive stages with 8 ×, 4×, and 2 × upsampling respectively. Each decoding stage contains four layers and\nthe input of each layer comes from the features of its previous layer together with the corresponding transformer layer.\ndense skip connections and progressive upsampling strategy,\nthe proposed method can sufficiently associate the decoding\nfeatures with the global-local ones from the transformer.\nIII. M ETHOD\nIn this paper, we propose a Global-Local Saliency TRans-\nfromer (GLSTR) which models the local and global represen-\ntations jointly in each encoder layer and decodes all features\nfrom transformers with gradual spatial resolution recovery.\nIn the following subsections, we first review the local and\nglobal feature extraction by the fully convolutional network-\nbased salient object detection methods in Sec. III-A. Then we\ngive more details about how the transformer extracts global\nand local features in each layer in Sec. III-B. Finally, we\nspecify different decoders including naive decoder, stage-by-\nstage decoder, multi-level feature aggregation decoder [56],\nand our proposed decoder. The framework of our proposed\nmethod is shown in Fig. 3.\nA. FCN-based Salient Object Detection\nWe revisit traditional FCN based salient object detection\nmethods on how to extract global-local representations. Nor-\nmally, a CNN-based encoder (e.g., VGG [41] and ResNet [57])\ntakes an image as input. Due to the locality of CNN, a stack\nof convolution layers is applied to the input images. At the\nsame time, the spatial resolution of the feature maps gradually\nreduces at the end of each stage. Such an operation extends the\nreceptive field of the encoder and reaches a global view in deep\nlayers. This kind of encoder faces three serious problems: 1)\nfeatures from deep layers have a global view with diluted local\ndetails due to too much convolution operations and resolution\nreduction; 2) features from shallow layers have more local\ndetails but lack of semantic understanding to identify salient\nobjects, due to the locality of the convolution layers; 3)\nfeatures from adjacent layers have limited variances and lead\nto redundancy when they are densely decoded.\nRecent work focuses on strengthening the ability to extract\neither local details or global semantics. To learn local details,\nprevious works [45], [58], [59] focus on features from shallow\nlayers by adding edge supervision. Such low-level features\nlack a global view and may misguide the whole network.\nTo learn global semantics, attention mechanism is used to\nenlarge the receptive field. However, previous works [27],\n[60] only apply the attention mechanism in deep layers ( e.g. ,\nthe last layer of the encoder). In the following subsections,\nwe demonstrate how to preferably unify local-global features\nusing the transformer.\nB. Transformer Encoder\n1) Image Serialization: Since the transformer is designed\nfor the 1D sequence in natural language processing tasks, we\nfirst map the input 2D images into 1D sequences. Specifically,\ngiven an image x ∈ RH×W×c with height H, width W and\nchannel c, the 1D sequence x′ ∈ RL×C is encoded from the\nimage x. A simple idea is to directly reshape the image into a\nsequence, namely, L = H ×W. However, due to the quadratic\nspatial complexity of self-attention, such an idea leads to\nenormous GPU memory consumption and computation cost.\nInspired by ViT [21], we divide the image x into HW\n256 non-\noverlapping image patches with a resolution of 16 ×16. Then\nwe project these patches into tokens with a linear layer, and\nthe sequence length L is HW\n256 . Every token in x′ represents a\nnon-overlay 16 × 16 patch. Such an operation trades off the\nspatial information and sequence length.\n2) Transformer: Taking 1D sequence x′ as the input, we\nuse pure transformer as the encoder instead of CNN used in\nprevious works [61]–[63]. Rather than only having limited\nreceptive field as CNN-based encoder, with the power of\nmodeling global representations in each layer, the transformer\nusually requires much fewer layers to receive a global recep-\ntive. Besides, the transformer encoder has connections across\neach pair of image patches. Based on these two characteristics,\nmore local detailed information is preserved in our trans-\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 4\nformer encoder. Specifically, the transformer encoder contains\npositional encoding and transformer layers with multi-head\nattention and multi-layer perception.\nPositional Encoding. Since the attention mechanism cannot\ndistinguish the positional difference, the first step is to feed the\npositional information into sequence x′ and get the position\nenhanced feature F0:\nF0 = [x\n′\n0, x\n′\n1, ..., x\n′\nL] +Epos, (1)\nwhere + is the addition operation and Epos indicates the\npositional code which is randomly initialized under truncated\nGaussian distribution and is trainable in our method.\nTransformer Layer. The transformer encoder contains 17\ntransformer layers, and each layer contains multi-head self-\nattention (MSA) and multi-layer perceptron (MLP). The multi-\nhead self-attention is the extension of self-attention (SA):\nQ = F Wq, K= F Wk, V= F Wv,\nSA(F) =ϕ(QKT\n√\nd\n)V, (2)\nwhere F indicates the input features of the self-attention, and\nWq, Wk, Wv are the weights with the trainable parameters. d is\nthe dimension of Q, K, Vand ϕ(⋆) is the softmax function. To\napply multiple attentions in parallel, multi-head self-attention\nhas m independent self-attentions:\nMSA (F) =SA1(F) ⊕ SA2(F) ⊕ ... ⊕ SAm(F), (3)\nwhere ⊕ indicates the concatenation. To sum up, in the ith\ntransformer layer, the output feature Fi is calculated by:\nˆFi = MSA (LN(Fi−1)) +Fi−1,\nFi = MLP (LN( ˆFi)) +ˆFi,\n(4)\nwhere LN(⋆) indicates layer norm and Fi is the ith layer\nfeatures of the transformer.\nC. Various Decoders\nIn this section, we design a global-local decoder to decode\nthe features F ∈ R\nHW\n256 ×C from the transformer encoder and\ngenerate the final saliency maps S\n′\n∈ RH×W with the same\nresolution as the inputs. Before exploring the ability of our\nglobal-local decoder, we simply introduce the naive decoder,\nstage-by-stage decoder, and multi-level feature aggregation\ndecoder used in [56] (see Fig. 4). To begin with, we first\nreshape the features Fi ∈ R\nHW\n256 ×C from the transformer into\nF\n′\ni ∈ R\nH\n16 ×W\n16 ×C.\nNaive Decoder. As illustrated in Fig. 4(a), a naive idea\nof decoding transformer features is directly upsampling the\noutputs of last layer to the same resolution of inputs and\ngenerating the saliency maps. Specifically, three conv-batch\nnorm-ReLU layers are applied on F12. Then we bilinearly\nupsample the feature maps 16 ×, followed by a simple classi-\nfication layer:\nZ = Up(CBR(F\n′\n12; θz); 16),\nS\n′\n= σ(Conv(Z; θs)),\n(5)\nwhere CBR(⋆; θ) indicates convolution-batch norm-ReLU\nlayer with parameters θ. Up(⋆; s) is the bilinear operation\nfor upsampling s× and σ is the sigmoid function. S′ is the\npredicted saliency map.\nStage-by-Stage Decoder. Directly upsampling 16 × at one\ntime like the naive decoder will inject too much noise and\nlead to coarse spatial details. Inspired by previous FCN-\nbased methods [62] in salient object detection, stage-by-stage\ndecoder which upsamples the resolution 2 × in each stage\nwill mitigate the losses of spatial details seen in Fig. 4(b).\nTherefore, there are four stages, and in each stage, a decoder\nblock contains three conv-batch norm-ReLU layers:\nZ0 = CBR(F\n′\n12; θ0),\nZi = CBR(Up(Zi−1; 2);θi),\nS\n′\n= σ(Conv(Z4; θs)).\n(6)\nMulti-level feature Aggregation. As illustrated in Fig. 4(c),\nZheng et al. [56] propose a multi-level feature aggregation to\nsparsely fuse multi-level features following the similar setting\nof pyramid network. Rather than having a pyramid shape, the\nspatial resolutions of features keep the same in the transformer\nencoder.\nSpecifically, the decoder take features F3, F6, F9, F12 from\nthe 3rd, 6th, 9th, 12th layers and upsample them 4 × following\nseveral convolutional layers. Then features are top-down ag-\ngregated via element-wise addition. Finally, the features from\nall streams are fused to generate the final saliency maps.\nDeeply-transformed Decoder. As illustrated in Fig. 4(d),\nto combine the benefit from the features of each transformer\nlayer and include less noise injection from upsampling, we\ngradually integrate all transformer layer features and upsample\nthem into the same spatial resolution of the input images.\nSpecifically, we first divide the transformer features into three\nstages and we upsample them 8×, 4×, 2× from the first to the\nthird stage respectively. Note that the upsampling operation\nincludes pixel shuffle ( 23−n×) and bilinear upsampling (2 ×),\nwhere n indicates the nth stage. In the jth layer of the ith\nstage, the salient feature Zi,j comes from the concatenation of\nthe transformer feature Fi∗4+j from the corresponding layer\ntogether with the salient feature from the previous layer. A\nconvolution block is applied after upsampling F\n′\n12 for Z3,4.\nZi,j =\n(\nCBR(Zi,j−1 ⊕ Up(F\n′\ni∗4+j; 2i); θi,j) j = 1, 2, 3,\nCBR(Up(Zi−1,j; 2)⊕ Up(F\n′\ni∗4+j; 2i); θi,j) j = 4.\n(7)\nThen we simply apply a convolutional operation on salient\nfeatures Zi,j to generate the final saliency maps S\n′\ni,j. There-\nfore, in our method, we have twelve output saliency maps:\nS\n′\ni,j = σ(Conv(Up(Zi,j; 2);θsi,j )). (8)\nLoss Function. We use standard binary cross-entropy loss\nas our loss function. Given twelve outputs, we design the final\nloss as:\nlbce(S\n′\n, S) =S log S\n′\n+ (1− S) log(1− S\n′\n),\nL =\n3X\ni=1\n4X\nj=1\nlbce(S\n′\ni,j, S), (9)\nwhere {S\n′\ni} indicate our predicted saliency maps and S\nindicates the ground truth.\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 5\nConv\nConv \n& 4↑\nTransformer Layer \n12th\n  & Conv & 4↑\nReshape & 8↑\n(a) Naive Decoder\n(c) Multi-level Feature Aggregation Decoder (d)  Our Decoder\nReshape\n   & Conv\nTransformer \nLayer 12th\nTransformer \nLayer 9th\nTransformer \nLayer 6th\nTransformer \nLayer 3rd\nConv Conv ConvConv & 8↑\nTransformer \nLayer ×4\nTransformer \nLayer ×4\nReshapeTransformer Layer \n12th Conv & 16↑ \nReshape\n(b) Stage-by-stage Decoder\nConv & 2↑\nReshape & 2↑\nConv \n& 4↑\nConv \n& 4↑\nConv \n& 4↑\n   & Conv\n↑ Upsampling\nConcatenate\n↑ Upsampling\nConcatenate\nReshape & 4↑\n   & Conv\nTransformer \nLayer ×4\nFig. 4: Different types of decoders. (a) Naive Decoder directly upsamples the output 16×. (b) Stage-by-Stage Decoder upsamples\nthe resolution 2 × in each stage. (c) Multi-level feature Aggregation Decoder sparsely fuses multi-level features. Our decoder\n(d) densely decodes all transformer features and gradually upsamples to the resolution of inputs.\nIV. E XPERIMENT\nA. Implementation Details\nWe train our model on DUTS following the same set-\nting as [45]. The transformer encoder is pretrained on Im-\nageNet [21] and the rest layers are randomly initialized\nfollowing the default settings in PyTorch. We use SGD with\nmomentum as the optimizer. We set momentum = 0.9, and\nweight decay = 0.0005. The learning rate starts from 0.001 and\ngradually decays to 1e-5. We train our model for 40 epochs\nwith a batch size of 8. We adopt vertical and horizontal flipping\nas the data augmentation techniques. The input images are\nresized to 384 ×384. During the inference, we take the output\nof the last layer as the final prediction.\nB. Datasets\nWe evaluate our methods on five widely used benchmark\ndatasets: DUT-OMRON [64], HKU-IS [65], ECSSD [66],\nDUTS [67], PASCAL-S [68]. DUT-OMRON contains 5,169\nchallenging images which usually have complex backgrounds\nand more than one salient objects. HKU-IS has 4,447 high-\nquality images with multiple disconnected salient objects in\neach image. ECSSD has 1,000 meaningful images with various\nscenes. DUTS is the largest salient object detection dataset\nincluding 10,533 training images and 5,019 testing images.\nPASCAL-S chooses 850 images from PASCAL VOC 2009.\nC. Evaluation Metrics\nIn this paper, we evaluate our methods under three widely\nused evaluation metrics: mean absolute error (MAE), weighted\nF-Measure (Fβ), and S-Measure ( S) [69].\nMAE directly measures the average pixel-wise differences\nbetween saliency maps and labels by mean absolute error:\nMAE (S\n′\n, S) =\n\f\f\fS\n′\n− S\n\f\f\f. (10)\nFβ evaluates the precision and recall at the same time and\nuse β2 to weight precision:\nFβ = (1 +β2) × P recision× Recall\nβ2 × P recision+ Recall , (11)\nwhere β2 is set to 0.3 as in previous work [45].\nS-Measure strengthens the structural information of both\nforeground and background. Specifically, the regional similar-\nity Sr and the object similarity So are combined together with\nweight α:\nS = α ∗ So + (1− α) ∗ Sr, (12)\nwhere α is set to 0.5 [45].\nD. Comparison with state-of-the-art methods\nWe compare our method with 12 state-of-the-art salient\nobject detection methods: DSS [36], UCF [70], Amulet [71]\nBMPM [19],RAS [72], PSAM [73] PoolNet [37], CPD [74],\nSCRN [75], BASNet [62], EGNet [45], MINet [63], LDF [61],\nGateNet [1], SAC [76], PoolNet+ [77], RCSBNet [78],\nSETR [56], VST [79], TCFNet [80], and Naive (Transformer\nencoder with the naive decoder). The saliency maps are\nprovided by their authors or calculated by the released code\nexcept SETR and Naive which are implemented by ourselves.\nBesides, all results are evaluated with the same evaluation\ncode.\nQuantitative Comparison. We report the quantitative re-\nsults in Tab. I. As can be seen from these results, with out\nany post-processing, our method outperforms all compared\nmethods using FCN-based or transformer-based architecture\nby a large margin on all evaluation metrics. In particular, the\nperformance is averagely improved 12.17% over the second\nbest method LDF in terms of MAE. The superior performance\non both easy and difficult datasets shows that our method does\nwell on both simple and complex scenes. Another interesting\nfinding is that transformer is powerful to generate rather\ngood saliency maps with “Naive” decoder, although it cannot\noutperform state-of-the-art FCN-based method (i.e., SAC).\nFurther investigating the potential for transformer may keep\nenhancing the performance on salient object detection task.\nQualitative Comparison. The qualitative results are shown\nin Fig. 5. With the power of modeling long range dependency\nover the whole image, transformer-based methods are able to\ncapture the salient object more accurately. For example, in the\nsixth row of Fig. 5, our method can accurately capture the\nsalient object without being deceived by the ice hill while\nFCN-based methods all fail in this case. Besides, saliency\nmaps predicted by our method are more complete, and aligned\nwith the ground truths.\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 6\nMethod DUTS-TE DUT-OMRON HKU-IS ECSSD PASCAL-S\nMAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑\nDSS C 0.056 0.801 0.824 0.063 0.737 0.790 0.040 0.889 0.790 0.052 0.906 0.882 0.095 0.809 0.797\nUCF C 0.112 0.742 0.782 0.120 0.698 0.760 0.062 0.874 0.875 0.069 0.890 0.883 0.116 0.791 0.806\nAmulet C 0.085 0.751 0.804 0.098 0.715 0.781 0.051 0.887 0.886 0.059 0.905 0.891 0.100 0.810 0.819\nBMPM C 0.049 0.828 0.862 0.064 0.734 0.809 0.039 0.910 0.907 0.045 0.917 0.911 0.074 0.836 0.846\nRAS C 0.059 0.807 0.838 0.062 0.753 0.814 - - - 0.056 0.908 0.893 0.104 0.803 0.796\nPSAM C 0.041 0.854 0.879 0.057 0.765 0.830 0.034 0.918 0.914 0.040 0.931 0.920 0.075 0.847 0.851\nCPD C 0.043 0.841 0.869 0.056 0.754 0.825 0.034 0.911 0.906 0.037 0.927 0.918 0.072 0.837 0.847\nSCRN C 0.040 0.864 0.885 0.056 0.772 0.837 0.034 0.921 0.916 0.038 0.937 0.927 0.064 0.858 0.868\nBASNet C 0.048 0.838 0.866 0.056 0.779 0.836 0.032 0.919 0.909 0.037 0.932 0.916 0.078 0.836 0.837\nEGNet C 0.039 0.866 0.887 0.053 0.777 0.841 0.031 0.923 0.918 0.037 0.936 0.925 0.075 0.846 0.853\nMINet C 0.037 0.865 0.884 0.056 0.769 0.833 0.029 0.926 0.919 0.034 0.938 0.925 0.064 0.852 0.856\nLDF C 0.034 0.877 0.892 0.052 0.782 0.839 0.028 0.929 0.920 0.034 0.938 0.924 0.061 0.859 0.862\nGateNet C 0.040 0.869 0.885 0.055 0.781 0.838 0.033 0.920 0.915 0.040 0.933 0.920 0.069 0.852 0.858\nSAC C 0.034 0.882 0.895 0.052 0.804 0.849 0.026 0.935 0.925 0.031 0.945 0.931 0.063 0.868 0.866\nPoolNet+ C 0.039 0.887 0.890 0.056 0.801 0.842 0.034 0.933 0.921 0.040 0.941 0.925 0.068 0.869 0.864\nRCSBNet C 0.039 0.887 0.890 0.056 0.801 0.842 0.034 0.933 0.921 0.040 0.941 0.925 0.068 0.869 0.864\nNaive T 0.043 0.855 0.878 0.059 0.776 0.835 0.042 0.905 0.903 0.042 0.927 0.919 0.068 0.854 0.862\nSETR T 0.039 0.869 0.888 0.056 0.782 0.838 0.037 0.917 0.912 0.041 0.930 0.921 0.062 0.867 0.859\nVST T 0.037 0.877 0.896 0.058 0.800 0.850 0.030 0.937 0.928 0.034 0.944 0.932 0.067 0.850 0.873\nTCFNet T 0.031 0.881 0.899 0.049 0.802 0.931 0.027 0.928 0.925 0.029 0.938 0.930 0.058 0.870 0.866\nOurs T 0.029 0.901 0.912 0.045 0.819 0.865 0.026 0.941 0.932 0.028 0.953 0.940 0.054 0.882 0.881\nTABLE I: Quantitative comparisons with FCN-based (denoted as C) and Transformer-based (denoted as T) salient object\ndetection methods by three evaluation metrics. The best performances are marked in Red. Simply utilizing a transformer\n(Naive) achieves comparable performance. Our method significantly outperforms all competitors regardless of what basic\ncomponent they used (i.e., either FCN or transformer-based).The results of VST † come from the paper due to the lack of\nreleased code or saliency maps.\nDensity DUTS-TE DUT-OMRON\nMAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑\n0 0.043 0.855 0.878 0.059 0.776 0.835\n1 0.033 0.889 0.902 0.052 0.802 0.852\n2 0.031 0.895 0.907 0.050 0.807 0.857\n3 0.030 0.899 0.910 0.047 0.813 0.862\n4 0.029 0.901 0.912 0.045 0.819 0.865\nTABLE II: Quantitative evaluation on dense connection during\ndecoding. As can be seen, adding connections can bring\nsignificant performance gain compared to the naive one (i.e.,\ndensity=0). As the density increasing, the performance grad-\nually improves on all three evaluation metrics. The best\nperformances are marked as red.\nE. Ablation Study\nIn this subsection, we mainly evaluate the effectiveness of\ndense connections and different upsampling strategies in our\ndecoder design on two challenging datasets: DUTS-TE and\nDUT-OMRON.\nEffectiveness of Dense Connections. We first conduct\nan experiment to examine the effect of densely decoding the\nfeatures from each transformer encoding layer. Starting from\nsimply decoding the features of last transformer (i.e., naive\ndecoder), we gradually add the fuse connections in each stage\nto increase the density until reaching our final decoder, denoted\nas density 0, 1, 2, 3. For example, the density 3 means that\nthere are three connections (i.e., from Zi,4, Zi,3, Zi,2) added\nin each stage and only one (i.e., Zi,4) for density 1.\nThe qualitative results are illustrated in Fig. 6, and quan-\ntitative results are shown in Tab. II. When we use the naive\ndecoder without any connection, the results are much worse\nwith lots of noises. The saliency maps are more sharp and\naccurate with the increase of decoding density, and the per-\nformances also gradually increase on three evaluation metrics.\nAs each layer captures a global view of its previous layer,\nthe representation differences are maximized among different\ntransformer features. Thus, each transformer feature can con-\ntribute individually to the predicted saliency maps.\nEffectiveness of Upsampling Strategy. To evaluate the ef-\nfect of our gradually upsampling strategy used in the decoder,\nwe compare different upsampling strategies on different den-\nsity settings. 1) The Naive decoder (denoted as 16 ×) simply\nupsamples the last transformer features 16 × for prediction. 2)\nWhen density is set to 0, we test an upsampling strategy in\nstage-by-stage decoder (denoted as Four 2×), where the output\nfeatures of each stage are upsampled 2 × before sending to\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 7\n(a) Image\n (b) GT\n (c) Ours (T)\n (d) SETR (T)\n (e) GateNet (C)\n (f) LDF (C)\n (g) EGNet (C)\n (h) BMPM (C)\n (i) DSS (C)\nFig. 5: Qualitative comparisons with state-of-the-art methods. Our method provides more visually reasonable saliency maps by\naccurately locating salient objects and generating sharp boundaries than other transformer-based (denoted as T) and FCN-based\nmethods (denoted as C).\nthe next stage and there are four 2× upsampling operations\nin total. 3) A sampling strategy (denoted as 4 × and 4×) used\nin SETR [56] upsamples each transformer features 4 ×, and\nanother 4 × is applied to obtain the final predicted saliency\nmap. 4) Our decoder ( Ours) upsamples transformer features\n8×, 4×, 2× for the layers connected to stage 1, stage 2, stage\n3 respectively.\nThe quantitative results are reported in Tab III with some\ninteresting findings. Gradually upsampling always performs\nbetter than the naive one with a single 16× upsampling\noperation. With the help of our upsampling strategy, the dense\ndecoding manifests its advanced performance, with less noise\ninjected during decoding. Directly decoding the output feature\nfrom last transformer in Stage mode only leads to limited\nperformance, further indicating the importance of our dense\nconnection.\nF . The Statistics of Timing\nAlthough the computational cost of transformer encoder\nmay be high, thanks to the parallelism of attention architecture,\nthe inference speed is still comparable to recent methods,\nshown in Tab. IV. Besides, as a pioneer work, we want to\nhighlight the power of transformer on salient object detection\n(SOD) task and leave time complexity reduction as the future\nwork.\nMethod DSS R3Net BASNet SCRN EGNet Ours\nSpeed(FPS) 24 19 28 16 7 15\nTABLE IV: Running time of different methods. We test all methods\non a single RTX2080Ti.\nV. C ONCLUSION\nIn this paper, we explore the unified learning of global-local\nrepresentations in salient object detection with an attention-\nbased model using transformers. With the power of mod-\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 8\nInput GT Dens.4 Dens.3 Dens.2 Dens.1 Dens.0\nOurs Naive\nFig. 6: The effect of dense connection during decoding.\nWe gradually increase the density of connection between\ntransformer output features and decoding convolution layer\nin all stages. The naive decoder (i.e., density 0) predicts\nsaliency maps with a lot of noises, which is severely influenced\nby the background. As the density increasing, the saliency\nmaps become more accurate and sharp, especially near the\nboundaries.\nUpsample DUTS-TE DUT-OMRON\nStrategy MAE ↓ Fβ ↑ S ↑ MAE ↓ Fβ ↑ S ↑\nDensity set to 0\n16× 0.043 0.855 0.878 0.059 0.776 0.835\nFour 2× 0.041 0.861 0.883 0.059 0.775 0.836\nDensity set to 1\n16× 0.036 0.875 0.896 0.052 0.794 0.850\n4× and 4× 0.034 0.886 0.900 0.053 0.801 0.850\nOurs 0.033 0.889 0.902 0.052 0.802 0.852\nDensity set to 4\n16× 0.034 0.882 0.902 0.051 0.799 0.846\n4× and 4× 0.030 0.900 0.911 0.047 0.814 0.862\nOurs 0.029 0.901 0.912 0.045 0.819 0.865\nTABLE III: Qualitative evaluations on the upsampling strate-\ngies under different density settings. Gradually upsampling\nleads to better performance than the naive one and our\nupsampling strategy outperforms others under different density\nsettings.\neling long-range dependency in transformer, we overcome\nthe limitations caused by the locality of CNN in previous\nsalient object detection methods. We propose an effective\ndecoder to densely decode transformer features and gradually\nupsample to predict saliency maps. It fully uses all transformer\nfeatures due to the high representation differences and less\nredundancy among different transformer features. We adopt a\ngradual upsampling strategy to keep less noise injection during\ndecoding to magnify the ability to locate accurate salient\nregions. We conduct experiments on five widely used datasets\nand three evaluation metrics to demonstrate that our method\nsignificantly outperforms state-of-the-art methods by a large\nmargin.\nREFERENCES\n[1] X. Zhao, Y . Pang, L. Zhang, H. Lu, and L. Zhang, “Suppress and\nbalance: A simple gated network for salient object detection,” in ECCV,\n2020, pp. 35–51. 1, 2, 5\n[2] R. Mechrez, E. Shechtman, and L. Zelnik-Manor, “Saliency driven\nimage manipulation,” Machine Vision and Applications , vol. 30, no. 2,\npp. 189–202, 2019. 1\n[3] W. Luo, M. Yang, and W. Zheng, “Weakly-supervised semantic seg-\nmentation with saliency and incremental supervision updating,” Pattern\nRecognition, p. 107858, 2021. 1\n[4] W. Wang, G. Sun, and L. Van Gool, “Looking beyond single images for\nweakly supervised semantic segmentation learning,” IEEE Transactions\non Pattern Analysis and Machine Intelligence , 2022. 1\n[5] W. Wang, J. Shen, R. Yang, and F. Porikli, “Saliency-aware video object\nsegmentation,” IEEE transactions on pattern analysis and machine\nintelligence, vol. 40, no. 1, pp. 20–33, 2017. 1\n[6] C. Craye, D. Filliat, and J.-F. Goudou, “Environment exploration for\nobject-based visual saliency learning,” in ICRA. IEEE, 2016, pp. 2303–\n2309. 1\n[7] Y . Yang, J. Yang, J. Yan, S. Liao, D. Yi, and S. Z. Li, “Salient color\nnames for person re-identification,” in ECCV. Springer, 2014, pp. 536–\n551. 1\n[8] R. Zhao, W. Oyang, and X. Wang, “Person re-identification by saliency\nlearning,” IEEE TPAMI, vol. 39, no. 2, pp. 356–370, 2016. 1\n[9] W. Wang, J. Shen, and H. Ling, “A deep network solution for attention\nand aesthetics aware photo cropping,” IEEE transactions on pattern\nanalysis and machine intelligence , vol. 41, no. 7, pp. 1531–1544, 2018.\n1\n[10] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, “Global\ncontrast based salient region detection,” IEEE TPAMI, vol. 37, no. 3,\npp. 569–582, 2014. 1\n[11] C. Gong, D. Tao, W. Liu, S. J. Maybank, M. Fang, K. Fu, and J. Yang,\n“Saliency propagation from simple to difficult,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2015, pp.\n2531–2539. 1\n[12] W. Einh ¨auser and P. K ¨onig, “Does luminance-contrast contribute to a\nsaliency map for overt visual attention?” European Journal of Neuro-\nscience, vol. 17, no. 5, pp. 1089–1097, 2003. 1\n[13] S. He and R. W. Lau, “Saliency detection with flash and no-flash image\npairs,” in ECCV. Springer, 2014, pp. 110–124. 1\n[14] K. Fu, C. Gong, I. Y .-H. Gu, and J. Yang, “Normalized cut-based\nsaliency detection by adaptive multi-level region merging,” IEEE Trans-\nactions on Image Processing , vol. 24, no. 12, pp. 5671–5683, 2015.\n1\n[15] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in CVPR, 2015, pp. 3431–3440. 1\n[16] G. Li and Y . Yu, “Deep contrast learning for salient object detection,”\nin CVPR, 2016, pp. 478–487. 1, 2\n[17] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu, “A stagewise\nrefinement model for detecting salient objects in images,” inICCV, 2017,\npp. 4019–4028. 1, 2\n[18] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, “Saliency detection\nwith recurrent fully convolutional networks,” in ECCV. Springer, 2016,\npp. 825–841. 1, 2\n[19] L. Zhang, J. Dai, H. Lu, Y . He, and G. Wang, “A bi-directional message\npassing model for salient object detection,” in CVPR, 2018, pp. 1741–\n1750. 1, 2, 5\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017. 1, 2\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020. 1, 2, 3, 5\n[22] J. H. Reynolds and R. Desimone, “Interacting roles of attention and\nvisual salience in v4,” Neuron, vol. 37, no. 5, pp. 853–863, 2003. 2\n[23] C. T. Morgan, “Physiological psychology.” 1943. 2\n[24] R. Desimone and J. Duncan, “Neural mechanisms of selective visual\nattention,” Annual review of neuroscience , vol. 18, no. 1, pp. 193–222,\n1995. 2\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 9\n[25] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise contex-\ntual attention for saliency detection,” in CVPR, 2018, pp. 3089–3098.\n2\n[26] S. Ren, W. Liu, J. Jiao, G. Han, and S. He, “Edge distraction-aware\nsalient object detection,” IEEE MultiMedia , vol. 30, no. 3, pp. 63–73,\n2023. 2\n[27] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, “Progressive attention\nguided recurrent network for salient object detection,” in CVPR, 2018,\npp. 714–722. 2, 3\n[28] S. He, J. Jiao, X. Zhang, G. Han, and R. W. Lau, “Delving into salient\nobject subitizing and detection,” in ICCV, 2017, pp. 1059–1067. 2\n[29] S. Ren, C. Han, X. Yang, G. Han, and S. He, “Tenet: Triple excitation\nnetwork for video salient object detection,” in ECCV. Springer, 2020,\npp. 212–228. 2\n[30] S. Ren, W. Liu, Y . Liu, H. Chen, G. Han, and S. He, “Reciprocal\ntransformations for unsupervised video object segmentation,” in CVPR,\nJune 2021, pp. 15 455–15 464. 2\n[31] B. Wang, W. Liu, G. Han, and S. He, “Learning long-term structural\ndependencies for video salient object detection,” IEEE TIP, vol. 29, pp.\n9017–9031, 2020. 2\n[32] X. Zhou, Z. Liu, C. Gong, and W. Liu, “Improving video saliency\ndetection via localized estimation and spatiotemporal refinement,” IEEE\nTransactions on Multimedia , vol. 20, no. 11, pp. 2993–3007, 2018. 2\n[33] T. Huang, X. Ben, C. Gong, B. Zhang, R. Yan, and Q. Wu, “Enhanced\nspatial-temporal salience for cross-view gait recognition,” IEEE Trans-\nactions on Circuits and Systems for Video Technology , vol. 32, no. 10,\npp. 6967–6980, 2022. 2\n[34] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang, “Salient\nobject detection in the deep learning era: An in-depth survey,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence , vol. 44,\nno. 6, pp. 3239–3259, 2021. 2\n[35] A. Borji, M.-M. Cheng, Q. Hou, H. Jiang, and J. Li, “Salient object\ndetection: A survey,” Computational visual media , vol. 5, pp. 117–150,\n2019. 2\n[36] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. Torr, “Deeply\nsupervised salient object detection with short connections,” in CVPR,\n2017, pp. 3203–3212. 2, 5\n[37] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, “A simple pooling-\nbased design for real-time salient object detection,” in CVPR, 2019, pp.\n3917–3926. 2, 5\n[38] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, “Amulet: Aggregating\nmulti-level convolutional features for salient object detection,” in ICCV,\n2017, pp. 202–211. 2\n[39] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin, “Non-\nlocal deep features for salient object detection,” in CVPR, 2017, pp.\n6609–6617. 2\n[40] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and A. Borji,\n“Detect globally, refine locally: A novel approach to saliency detection,”\nin CVPR, 2018, pp. 3127–3135. 2\n[41] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” in ICLR, 2015. 2, 3\n[42] W. Wang, J. Shen, M.-M. Cheng, and L. Shao, “An iterative and\ncooperative top-down and bottom-up inference network for salient object\ndetection,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2019. 2\n[43] W. Wang, J. Shen, X. Dong, A. Borji, and R. Yang, “Inferring salient\nobjects from human fixations,” IEEE transactions on pattern analysis\nand machine intelligence , vol. 42, no. 8, pp. 1913–1927, 2019. 2\n[44] B. Xu, H. Liang, R. Liang, and P. Chen, “Locate globally, segment\nlocally: A progressive architecture with knowledge review network for\nsalient object detection,” in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 35, no. 4, 2021, pp. 3004–3012. 2\n[45] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y . Cao, J. Yang, and M.-M. Cheng,\n“Egnet: Edge guidance network for salient object detection,” in ICCV,\n2019, pp. 8779–8788. 2, 3, 5\n[46] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in CVPR, 2018, pp. 7794–7803. 2\n[47] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in ICCV, 2019, pp. 3286–3295. 2\n[48] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” in ICCV, 2019, pp. 3464–3473. 2\n[49] H. Zhao, J. Jia, and V . Koltun, “Exploring self-attention for image\nrecognition,” in CVPR, 2020, pp. 10 076–10 085. 2\n[50] S. Ren, D. Zhou, S. He, J. Feng, and X. Wang, “Shunted self-attention\nvia multi-scale token aggregation,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n10 853–10 862. 2\n[51] S. Ren, F. Wei, Z. Zhang, and H. Hu, “Tinymim: An empirical study of\ndistilling mim pre-trained models,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2023, pp.\n3687–3697. 2\n[52] S. Ren, X. Yang, S. Liu, and X. Wang, “Sg-former: Self-guided\ntransformer with evolving token reallocation,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2023, pp.\n6003–6014. 2\n[53] H. Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and L.-C. Chen,\n“Axial-deeplab: Stand-alone axial-attention for panoptic segmentation,”\nin ECCV. Springer, 2020, pp. 108–126. 2\n[54] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nECCV. Springer, 2020, pp. 213–229. 2\n[55] Y . Zeng, J. Fu, and H. Chao, “Learning joint spatial-temporal transfor-\nmations for video inpainting,” in ECCV. Springer, 2020, pp. 528–543.\n2\n[56] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr et al. , “Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,” arXiv preprint\narXiv:2012.15840, 2020. 2, 3, 4, 5, 7\n[57] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016, pp. 770–778. 3\n[58] W. Wang, S. Zhao, J. Shen, S. C. Hoi, and A. Borji, “Salient object\ndetection with pyramid attention and salient edges,” in CVPR, 2019, pp.\n1448–1457. 3\n[59] Z. Wu, L. Su, and Q. Huang, “Stacked cross refinement network for\nedge-aware salient object detection,” in ICCV, 2019, pp. 7264–7273. 3\n[60] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient\nobject detection,” in ECCV, 2018, pp. 234–250. 3\n[61] J. Wei, S. Wang, Z. Wu, C. Su, Q. Huang, and Q. Tian, “Label\ndecoupling framework for salient object detection,” in CVPR, June 2020.\n3, 5\n[62] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand,\n“Basnet: Boundary-aware salient object detection,” in CVPR, June 2019.\n3, 4, 5\n[63] Y . Pang, X. Zhao, L. Zhang, and H. Lu, “Multi-scale interactive network\nfor salient object detection,” in CVPR, June 2020. 3, 5\n[64] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, “Saliency detection\nvia graph-based manifold ranking,” in CVPR. IEEE, 2013, pp. 3166–\n3173. 5\n[65] G. Li and Y . Yu, “Visual saliency based on multiscale deep features,”\nin CVPR, June 2015, pp. 5455–5463. 5\n[66] J. Shi, Q. Yan, L. Xu, and J. Jia, “Hierarchical image saliency detection\non extended cssd,” IEEE TPAMI, vol. 38, no. 4, pp. 717–729, 2015. 5\n[67] L. Wang, H. Lu, Y . Wang, M. Feng, D. Wang, B. Yin, and X. Ruan,\n“Learning to detect salient objects with image-level supervision,” in\nCVPR, 2017. 5\n[68] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of\nsalient object segmentation,” in CVPR, 2014, pp. 280–287. 5\n[69] D.-P. Fan, M.-M. Cheng, Y . Liu, T. Li, and A. Borji, “Structure-measure:\nA new way to evaluate foreground maps,” in ICCV, 2017, pp. 4548–\n4557. 5\n[70] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin, “Learning uncertain\nconvolutional features for accurate saliency detection,” in ICCV, Oct\n2017. 5\n[71] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, “Amulet: Aggregating\nmulti-level convolutional features for salient object detection,” in ICCV,\nOct 2017. 5\n[72] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient\nobject detection,” in European Conference on Computer Vision , 2018.\n5\n[73] J.-R. Chang and Y .-S. Chen, “Pyramid stereo matching network,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 5410–5418. 5\n[74] Z. Wu, L. Su, and Q. Huang, “Cascaded partial decoder for fast and\naccurate salient object detection,” in CVPR, June 2019. 5\n[75] Z. Wu, L. Su, and Q. Huang, “Stacked cross refinement network for\nedge-aware salient object detection,” in ICCV, 2019, pp. 7263–7272. 5\n[76] X. Hu, C.-W. Fu, L. Zhu, T. Wang, and P.-A. Heng, “Sac-net: Spatial\nattenuation context for salient object detection,” IEEE Transactions on\nCircuits and Systems for Video Technology , 2020, to appear. 5\n[77] J.-J. Liu, Q. Hou, Z.-A. Liu, and M.-M. Cheng, “Poolnet+: Exploring the\npotential of pooling for salient object detection,” IEEE TPAMI, vol. 45,\nno. 1, pp. 887–904, 2023. 5\nIEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE 10\n[78] Y . K. Yun and T. Tsubono, “Recursive contour-saliency blending\nnetwork for accurate salient object detection,” in Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision\n(WACV), January 2022, pp. 2940–2950. 5\n[79] N. Liu, N. Zhang, K. Wan, J. Han, and L. Shao, “Visual saliency\ntransformer,” CoRR, vol. abs/2104.12099, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2104.12099 5\n[80] C. Yao, L. Feng, Y . Kong, L. Xiao, and T. Chen, “Transformers and\ncnns fusion network for salient object detection,” Neurocomputing, vol.\n520, pp. 342–355, 2023. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0925231222013704 5\nSucheng Ren is a research assistant in the School\nof Computing and Information Systems, Singapore\nManagement University. He received the M.Sc. de-\ngree and B. Eng. degree from South China Univer-\nsity of Technology. His research interests include\ncomputer vision, image processing, and deep learn-\ning.\nNanxuan Zhao is a research scientist in Adobe\nResearch. She was a Postdoctoral Researcher in\nthe Chinese University of Hong Kong. She was a\nVisiting Scholar of Harvard University. She obtained\nher B.Sc. degree from South China University of\nTechnology and her Ph.D. degree from City Univer-\nsity of Hong Kong. Her research interests include\ncomputer graphics, computer vision, and human-\ncomputer interaction.\nQiang Wen was a master student in the School\nof Computer Science and Engineering, South China\nUniversity of Technology. Before that, he received\nthe B. Eng. degree from the School of Information\nScience and Engineering, Central South University\nin 2018. His research interests include computer\nvision, image processing and deep learning.\nGuoqiang Han received the B.Sc. degree from the\nZhejiang University, Hangzhou, China, in 1982, and\nthe M.Sc. and Ph.D. degrees from the Sun Yat-sen\nUniversity, Guangzhou, China, in 1985 and 1988,\nrespectively. He is a Professor with the School of\nComputer Science and Engineering, South China\nUniversity of Technology, Guangzhou. He was the\ndean of the School of Computer Science and Engi-\nneering. He has published over 100 research papers.\nHis current research interests include multimedia,\ncomputational intelligence, machine learning, and\ncomputer graphics.\nShengfeng He (Senior Member, IEEE) is an\nassociate professor in the School of Computing\nand Information Systems, Singapore Management\nUniversity. He was on the faculty of the South\nChina University of Technology, from 2016 to 2022.\nHe obtained B.Sc. and M.Sc. degrees from Macau\nUniversity of Science and Technology in 2009 and\n2011 respectively, and a Ph.D. degree from City Uni-\nversity of Hong Kong in 2015. His research interests\ninclude computer vision and generative models. He\nis a senior member of IEEE and CCF. He serves as\nthe lead guest editor of the IJCV , the associate editor of IEEE TNNLS, IEEE\nTCSVT, Visual Intelligence, and Neurocomputing. He also serves as the area\nchair/senior program committee of ICML, AAAI, IJCAI, and BMVC.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6720242500305176
    },
    {
      "name": "Computer science",
      "score": 0.6692440509796143
    },
    {
      "name": "Salient",
      "score": 0.6490793824195862
    },
    {
      "name": "Encoder",
      "score": 0.6372155547142029
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5592319965362549
    },
    {
      "name": "Locality",
      "score": 0.5410434603691101
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48632439970970154
    },
    {
      "name": "Algorithm",
      "score": 0.3541819453239441
    },
    {
      "name": "Computer vision",
      "score": 0.33736535906791687
    },
    {
      "name": "Voltage",
      "score": 0.12986326217651367
    },
    {
      "name": "Engineering",
      "score": 0.09373137354850769
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90610280",
      "name": "South China University of Technology",
      "country": "CN"
    }
  ]
}