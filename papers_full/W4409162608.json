{
  "title": "Can large language models predict the hydrophobicity of metal–organic frameworks?",
  "url": "https://openalex.org/W4409162608",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2097479608",
      "name": "Xiaoyu Wu",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2129004808",
      "name": "Jianwen Jiang",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2022085591",
    "https://openalex.org/W2141939342",
    "https://openalex.org/W2948038547",
    "https://openalex.org/W3002549837",
    "https://openalex.org/W2183073484",
    "https://openalex.org/W1986022082",
    "https://openalex.org/W2889440810",
    "https://openalex.org/W2755485480",
    "https://openalex.org/W3082818944",
    "https://openalex.org/W4388280900",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4387393188",
    "https://openalex.org/W4389686454",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4399442007",
    "https://openalex.org/W4405616956",
    "https://openalex.org/W4389263615",
    "https://openalex.org/W4391470407",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W4404620644",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W4403691894",
    "https://openalex.org/W2973153050",
    "https://openalex.org/W2078444809",
    "https://openalex.org/W4392679398",
    "https://openalex.org/W4200366286",
    "https://openalex.org/W4401042037",
    "https://openalex.org/W4200320198",
    "https://openalex.org/W3176463279",
    "https://openalex.org/W3000754666",
    "https://openalex.org/W4396570449",
    "https://openalex.org/W2053294416",
    "https://openalex.org/W2892439819",
    "https://openalex.org/W3181257149",
    "https://openalex.org/W3118507387",
    "https://openalex.org/W4389352201",
    "https://openalex.org/W4386882870",
    "https://openalex.org/W3048908832",
    "https://openalex.org/W4225361481",
    "https://openalex.org/W3200152265",
    "https://openalex.org/W4400496339",
    "https://openalex.org/W4400043897",
    "https://openalex.org/W3116457427",
    "https://openalex.org/W4406159056",
    "https://openalex.org/W4403900342",
    "https://openalex.org/W4401915203",
    "https://openalex.org/W4401682260",
    "https://openalex.org/W4392167663",
    "https://openalex.org/W4318262876",
    "https://openalex.org/W4403453360",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4389936477",
    "https://openalex.org/W4403034218"
  ],
  "abstract": "Fine-tuning a large language model to predict the hydrophobicity of metal–organic frameworks.",
  "full_text": "Journal of\n Materials Chemistry A\nMaterials for energy and sustainability\nrsc.li/materials-a\nVolume 13\nNumber 25\n7 July 2025\nPages 19023–20056\nISSN 2050-7488\nPAPER\nXiaoyu Wu and Jianwen Jiang\nCan large language models predict the hydrophobicity of \nmetal–organic frameworks?\nCan large language models predict the\nhydrophobicity of metal–organic frameworks?†\nXiaoyu Wu and Jianwen Jiang *\nRecent advances in large language models (LLMs) oﬀer a transformative paradigm for data-driven materials\ndiscovery. Herein, we exploit the potential of LLMs in predicting the hydrophobicity of metal– organic\nframeworks (MOFs). By ﬁne-tuning the state-of-the-art Gemini-1.5 model exclusively on the chemical\nlanguage of MOFs, we demonstrate its capacity to deliver weighted accuracies that surpass those of\ntraditional machine learning approaches based on sophisticated descriptors. To further interpret the\nchemical “understanding” embedded within the Gemini model, we conduct systematic moiety masking\nexperiments, where our ﬁne-tuned Gemini model consistently retains robust predictive performance\neven with partial information loss. Finally, we show the practical applicability of the Gemini modelvia\na blind test on solvent- and ion-containing MOFs. The results illustrate that Gemini, combined with\nlightweight ﬁne-tuning on chemically annotated texts, can serve as a powerful tool for rapidly screening\nMOFs in pursuit of hydrophobic candidates. Taking a step forward, our work underscores the potential of\nLLMs in oﬀering robust and data-eﬃcient approaches to accelerate the discovery of functional materials.\n1. Introduction\nMetal–organic Frameworks (MOFs) are a versatile class of\nnanoporous materials typically synthesized under relatively\nmild hydrothermal or solvothermal conditions from metal ions\nand organic ligands.\n1 Their modular architectures enable\nprecise tunability of pore structures and functional properties,\nrendering them attractive for a wide range of applications,\nincluding gas storage, separation and catalysis.\n2 A subset of\nMOFs exhibit low aﬃnity for water and this class of hydro-\nphobic MOFs has garnered increasing attention for their\npotential utility under humid conditions.3 To date, over 100 000\nMOFs have been experimentally produced; however, many of\nthem were synthesized without reporting their hydrophobicity.\n4\nTo streamline the identication of hydrophobic MOFs, Henry's\nconstant of water (KH) was adopted as a metric in a computa-\ntional workow5 to duly benchmark against representative\nhydrophobic structures (e.g., ZIF-86). This approach has been\nintegrated with high-throughput computational screening to\nshortlist promising candidates for various applications (e.g.,\nCO2 capture and removal of toxic gases).7,8 Nevertheless, in\nprinciple, the combinatorial design space of MOFs is innite\ndue to the myriad coupling chemistries of building units as well\nas their underlying topologies. As such, K\nH calculations to\nidentify hydrophobic MOFs on a trial-and-error basis are\nexceedingly laborious.9\nMachine learning (ML) has emerged as a powerful alterna-\ntive in this regard, as it has already proven to be indispensable\nin the design of functional materials.\n10 One of the most\ncompelling advances in ML is the advent of large language\nmodels (LLMs) such as ChatGPT11 and Gemini,12 which have\nbeen trained on massive text corpora spanning diverse disci-\nplines.\n13 These foundational LLMs excel at generating language\ntextual responses from simple prompts, which in many\ninstances, are indistinguishable from human articulations.\nSuch generative capabilities have unleashed exciting opportu-\nnities for digital chemistry including chemical synthesis,\n14–16\ndataset mining,17–19 and pattern recognition.20\nOne of the fascinating aspects of LLMs lies in their predictive\ncapacity in both forward and inverse chemical discovery, relying\nsolely on chemical language instead of engineered molecular\ndescriptors.\n21 Though typically pretrained for general purposes,\nLLMs can signicantly enhance their predictive accuracy for\nchemistry-specic tasks through ne-tuning with domain\nknowledge even based on light-weight LLMs (i.e., LLMs pre-\ntrained with fewer parameters, e.g., 8B, 70B). 22 Molecular\nrepresentations including simplied molecular input line entry\nsystems (SMILES) 23 and self-referencing embedded strings\n(SELFIES)24 have facilitated chemical language modeling.25,26\nThese chemical notations can be further augmented with metal\ninformation, thereby capturing both the inorganic and organic\nconstituents of MOFs.\n27,28 As exemplied in Scheme 1, a typical\nMOF named Cu-BTC29 can be rendered in augmented SMILES\nand SELFIES notations, each meticulously tokenized into\nDepartment of Chemical and Biomolecular Engineering, National University of\nSingapore, Singapore, 117576, Singapore. E-mail: chejj@nus.edu.sg\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d5ta01139f\nCite this:J. Mater. Chem. A,2 0 2 5 ,13,\n19307\nReceived 12th February 2025\nAccepted 4th April 2025\nDOI: 10.1039/d5ta01139f\nrsc.li/materials-a\nThis journal is © The Royal Society of Chemistry 2025 J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 | 19307\nJournal of\nMaterials Chemistry A\nPAPER\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nsmaller units carrying unique IDs for LLM ingestion. Notably,\nvariably pretrained LLMs may adopt di ﬀerent tokenization\napproaches, and the default tokenizer of Gemini is employed\nhere.\nIn this work, we embark on thene-tuning of a cutting-edge\nLLM, Gemini-1.5,\n30 leveraging the latest CoRE-2024 database\nwith thousands of“computation-ready” experimental MOFs.28\nParticularly, we focus on MOFs from the all-solvents-removed\n(ASR) subset of single metal and single linker type tone-tune\nthe base model. For both binary and quaternary classi ca-\ntions of hydrophobicity, the ne-tuned Gemini achieves\ncomparable overall accuracy and notably excels in weighted\naccuracy— a critical advantage given the class imbalance\ninherent in the large dataset. Furthermore, we assess the\nrobustness and transferability of the model through moiety\nmasking experiments and a rigorous blind test on distinct\nMOFs. These ndings demonstrate the coherence between\nLLMs and digital chemistry, potentially shedding light on har-\nnessing the power of Gemini as a useful agent for open ques-\ntions in the broader physical sciences.\n2. Methodology\n2.1. Dataset\nWe started with the ASR subset from the CoRE-MOF-2024 data-\nbase, which contains 8857 meticulously curated experimental\nMOFs with computed water aﬃnity (i.e., K\nH) via the Widom\ninsertion method.28 Considering synthetic accessibility, we\nadapted a similar decomposition-based protocol by Pouyaet al.,31\nwhich narrowed down the dataset to 2642 MOFs of single metal\nand single linker type. As depicted in Fig. S1a and 1a†, 2642 MOFs\ncan be categorized using binary classication with two labels:\nstrong hydrophobic (Strong) and weak hydrophobic (Weak), as\nwell as quaternary classication with four labels: super strong\nhydrophobic (SS), strong hydrophobic (S), weak hydrophobic (W),\nand super weak hydrophobic (SW). The resulting dataset spans\ndiverse pore geometric properties (Fig. S2 and S3†). For both\nbinary and quaternary classications, the dataset was split into\nan 80 : 20 ratio, with the former as a training set for model\ndevelopment and the latter as a hold-out test set for model eval-\nuation. Despite non-signicant bias across any other labels in the\ntraining set compared to the test set, we noticed a notable\nimbalance in the distribution of labels, with SS being the least\npopulated (Fig. 1b). This is not unexpected due to the challenging\nwater-repelling nature of MOFs,\n32 which may pose diﬃculty in the\nmodel prediction. Generally, class imbalance remains a core\nchallenge in applying ML to diverse research topics in physical\nsciences, including MOF discovery\n33 and photocatalyst design,34\nas such tasks are not always exhaustively addressed with solely\nhand-craed descriptors.\n35\n2.2. Fine-tuning gemini\nFine-tuning refers to the process of adapting a base model,\nwhich has already been pre-trained on a vast corpora of gener-\nalized data, to perform better on a more specic task. During\nthis process, parameters in the model are adjusted to minimize\nerrors for a new task. This allows the model to tailor its domain\nknowledge and enhances its task-specic performance. Gemini-\n1.5 is a state-of-the-art foundational LLM developed by Google.\n30\nIt is characterized by its signi cantly enhanced processing\nspeed and long-text eﬀectiveness, making it a valuable LLM for\nne-tuning where iterative adjustments and accurate predic-\ntions are bene cial. Gemini has demonstrated ne-tuning\ncapability in the medical domain.36\nIn this work, wene-tuned the“gemini-1.5-ash-001-tuning”\nbase model in Google AI Studio. The training dataset comprised\nlabeled MOFs, alongside two molecular representations:\nSMILES and SELFIES, both augmented with inorganic motifs\n(hereaer referred to simply as SMILES and SELFIES). During\nne-tuning, the training data were structured as prompt and\nresponse pairs: (“Representation”, “Label”). In such a format,\nSMILES or SELFIES served as prompts and were completed with\ncorresponding hydrophobicity labels, which were categorized as\n[0, 1] and [0, 1, 2, 3] for binary and quaternary classications,\nrespectively. For computational feasibility and model stability,\nthe model was tuned with 3 epochs and a batch size of 16 with\na learning rate of 2× 10\n−4 to reach a minimum loss. Due to the\ngeneral knowledge stored in Gemini, a predicted response from\nan out-of-sample prompted MOF may not always be expected to\npredict an ideal label. For these MOFs, augmented prompts\nwere utilized, as detailed in Section S2 in the ESI.†\nScheme 1 Tokenized SMILES and SELFIES strings encoding Cu-BTC for Gemini. The tokenization is visualizedvia LLM-text-tokenization:\nhttps://github.com/glaforge/llm-text-tokenization.\n19308 | J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 This journal is © The Royal Society of Chemistry 2025\nJournal of Materials Chemistry A Paper\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nThe ne-tuned Gemini for prediction of MOF hydrophobicity\nwas benchmarked against descriptor-based supervised ML\nmodels. Global pore descriptors (e.g., pore size) computedvia\nZeo++37 and the revised autocorrelations (RACs)38,39 were adop-\nted for featurizing MOFs (detailed in Tables S1 and S2†). All the\nbaseline models were trained using Support Vector Machine\n(SVM), with hyperparameters: {‘C’: [0.1, 1, 10],‘kernel’:[ ‘linear’,\n‘rbf’]} tuned through 10-fold cross-validations on the same\ntraining set as in thene-tuned Gemini, with the identical hold-\nout test set and blind test set to ensure fair comparison.\n3. Results and discussion\n3.1. Fine-tuning results\nThe ne-tuned Gemini was evaluated on the hold-out test set\nusing the overall accuracy and weighted F1-score, which\naccounts for data imbalance as discussed in Section 2.1. As\nsummarized in Table 1, for binary classication, thene-tuned\nGemini based on SMILES achieves exemplary performance,\nwith an overall accuracy of 0.78 and a weighted F1-score of 0.74,\nrespectively. However, when shied to quaternary classication,\nits predictive capacity drops slightly, achieving 0.73 overall\naccuracy and 0.70 weighted F1-score, respectively. A similar\ntrend is observed for thene-tuned Gemini with SELFIES, albeit\nwith less predictive performance. Though SELFIES has\ndemonstrated a certain capacity in representing reticular\nchemistry,\n40 its generic tokenization appears to dilute addi-\ntional chemical information compared to SMILES.41 Moreover,\nwhile Gemini is a closed-source LLM, its stored cut-oﬀ knowl-\nedge likely encompasses public datasets with diverse chemical\ninformation, including SMILES notations.42 In contrast, data-\nsets incorporating SELFIES remain scanty, which leads to less\ncompatibility of Gemini with property prediction based on\nSELFIES. We anticipate that as chemically-rich datasets\nincluding SELFIES notations become more prevalent, LLMs like\nGemini may demonstrate improved performance in future\nSELFIES-based classication tasks. Here, thene-tuned Gemini\nwith SMILES stands out as the optimal approximator for both\nbinary and quaternary classications.\nDescriptor-based ML models were used to benchmark the\nperformance of thene-tuned Gemini. For eﬀective featuriza-\ntion, we adopted pore descriptors and RACs. Leveraging non-\nweighted molecular graphs to derive the products or diﬀer-\nences of atomic heuristics, RACs have been used to eﬀectively\nmap the chemical space of MOFs, encompassing linker and\nmetal chemistry.\n43,44 Combined with pore descriptors, RACs\nFig. 1 (a) Kernel density estimations of quaternary classiﬁcation (SS, S, W and SW)versus density. The vertical axis denotes probability. (b) Data\ndistribution in training and test sets.\nTable 1 Performance on the test set for binary and quaternary\nclassiﬁcations\nModel\nBinary\nclassication\nQuaternary\nclassication\nAccuracya F1-scoreb Accuracy F1-score\nGemini (SMILES) 0.78 0.74 0.73 0.70\nGemini (SELFIES) 0.73 0.73 0.71 0.67\nPorec 0.76 0.66 0.72 0.62\nPore + RACs 0.77 0.71 0.75 0.64\na For binary classication, the accuracy metric reects the model's\nability to correctly classify instances into two labels (Strong and\nWeak). For quaternary classication, the accuracy metric aggregates\nthe overall correct four labels (SS, S, W and SW).b The weighted F1-\nscore combines precision and recall into a single metric, accounting\nfor class imbalance by weighing each class's F1-score by the\nproportion of instances in that class.\nc The descriptor-based models\nwere trained using the SVM classi er in scikit-learn, with\nhyperparameters: [ ‘C’: [0.1, 1, 10], ‘kernel’:[ ‘linear’, ‘rbf’]] tuned\nthrough 10-fold cross-validation.\nThis journal is © The Royal Society of Chemistry 2025 J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 | 19309\nPaper Journal of Materials Chemistry A\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nhave been shown to be eﬀective and robust in predicting various\nproperties of MOFs.45–49 As illustrated in Table 1, despite being\ntrained upon a simple text-based representation like SMILES,\nthe ne-tuned Gemini has predictive capability comparable\nwith that of sophisticated descriptor-based models, with a slight\nunderestimation of overall accuracy for quaternary classica-\ntion. This is expected, to a certain extent, as RACs embed both\nsimilarities and dissimilarities, thus better encapsulating\nchemical topology and connectivity as well.\n47 Signicantly, our\nne-tuned Gemini exhibits commendable weighted accuracy,\nmaintaining a weighted F1-score of 0.70 even as the classica-\ntion complexity increases from binary to quaternary. As depic-\nted in Fig. 2a, the ne-tuned Gemini correctly distinguishes\nmost of the“Strong” labels (376 in the bottom-le cell), which is\nslightly less predicted than Pore + RACs (Fig. S5a†). Conversely,\nPore + RACs fail to predict their counterparts labelled as\n“Weak”, with only 15 correct predictions and 111 samples\noverpredicted as “Strong”. Such mislabeling in“Weak” hydro-\nphobic MOFs is more pronounced for quaternary classication\n(Fig. S5b†). Despite oﬀering a ner-grained picture of misclas-\nsication across the four labels, thene-tuned Gemini outper-\nforms in predictions of“SS”, “W” and “SW” (Fig. 2b), leading to\na higher weighted F1-score. To further evaluate the consistency\nof ne-tuning results, we examined the eﬀect of a random state\nin the training/test split of the dataset, all of which yielded\nstable model performance (±0.01), as summarized in Table S3.†\nThese results position thene-tuned Gemini as an eﬃcient tool\nfor sorting a large number of MOFs for subsequent, time-\nconsuming computations (e.g., via rst-principles molecular\ndynamics simulation\n50) to more precisely determine hydro-\nphobicity. While descriptor-based ML models demonstrate\nbetter predictive capability through advanced feature engi-\nneering, they typically necessitate extensive preprocessing or\nthe derivation of specialized property-specic descriptors that\nrely heavily on precise atomic positions. In contrast, our LLM-\nbased method exclusively leverages text-based chemical repre-\nsentations (i.e., SMILES/SELFIES) that encode the building\nunits in MOFs. These engineering-free string-based represen-\ntations require no structural optimization, thus facilitating\nrapid screening across a diverse and potentially unexplored\ntopology space without exhaustive structural validation. We\nshould note that our method is not intended to replace direct\nK\nH calculations (e.g., via the Widom insertion method); rather,\nit serves as a rapid, surrogate screening tool capable of\neﬃciently narrowing down a large candidate pool. Nevertheless,\nwe note that such a LLM based model may not be ideal for near-\nquantitative predictions, where regression models might deliver\nhigher accuracy.\n51,52\nAcquiring high-delity data through an experimental or\ncomputational approach can be time-consuming and costly.\nIdeally, a model should maintain data eﬃciency, even trained\non a limited budget of data.\n53,54 In this context, wene-tuned\nGemini using various training set ratios, ranging from 0.2 to\n0.8 out of 2112 total training data points to assess its data\neﬃciency. For fair comparison, the ML model with Pore + RACs\nwas also trained on the same classication tasks using the same\ntraining data as in Gemini. The learning curves on the test set by\nthe ne-tuned Gemini and the ML model are presented in\nFig. 3a. Intriguingly, both models demonstrate similar accuracy\nacross a wide range of training set ratios, with predictive\nperformance signicantly compromised when trained on less\nthan 845 data points (i.e., 0.4 training set ratio). Such an eﬀect is\nmarkedly amplied for quaternary classi cation, where the\nmodel performance drops below random guessing (<0.5 accu-\nracy). The accuracy of both models saturates at a training set\nratio of 0.6, with marginal improvement a erwards. The\noptimal accuracy scores of 0.78 and 0.73 are achieved by the\nne-tuned Gemini for binary and quaternary classications,\nrespectively. Such an early performance ceiling likely stems\nfrom the complexity of the hydrophobic nature in reticular\nchemistry, a subtle property governed by a complex interplay\nthat is diﬃcult to fully encapsulate. Discretizing hydrophobicity\ninto binary or quaternary classi cation may also introduce\ndiscontinuous boundaries forK\nH that challenge the capability\nof model prediction.55\nPrompted by the proven data eﬃciency, we examined the\ndissimilarity of the feature space encoded by Pore + RACs and\nSMILES, respectively, throught-distributed stochastic neighbor\nembedding (t-SNE).56 In a t-SNE map, points are spatially\narranged such that the closer the two points, the more similar\nthe two structures are, as described by the encodingnger-\nprints. The SMILES representation, as text-based input, was\ntokenized through the BERT model\n57 to emulate the enclosed\ndimensions captured by thene-tuned Gemini. As evidenced in\nFig. 3b and c, increasing the training set ratio leads to\na progressively denser and more saturated feature space. The\ndense pattern stabilizes aer a training set ratio of 0.6, high-\nlighting an optimal balance between training set size and\npredictive performance. We acknowledge the challenge pre-\nsented by the SS label in the quaternary classication even with\na full training set (1.0), which notably yields several misclassi-\ncations in the ne-tuned Gemini model predictions. To\ninterpret this diﬃculty, we examined the chemical space in both\ntrained and tested MOFs labeled as SS and S. As clearly illus-\ntrated in Fig. S6,† the chemical space coverage for SS in the\ntraining set is signicantly diluted compared to that for S. This\nlimited chemical diversity contributes to the diﬃculty of the\nne-tuned Gemini in accurately distinguishing closely over-\nlapping chemical syntax, leading to multiple SS-labeled MOFs\nbeing misclassied as S in the test set. We anticipate that\nenriching the chemical diversity, particularly for the\nFig. 2 Confusion matrices on the test set by theﬁne-tuned Gemini. (a)\nBinary classiﬁcation and (b) quaternary classiﬁcation.\n19310 | J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 This journal is © The Royal Society of Chemistry 2025\nJournal of Materials Chemistry A Paper\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nunderrepresented SS label, could substantially enhance the\npredictive accuracy of the ne-tuned Gemini for these chal-\nlenging instances. As such, thene-tuned Gemini appears to be\nan eﬀective approach for data-driven discovery of MOFs, as it\nrequires minimal eﬀorts in preparing input (simple as a string-\nbased representation as SMILES), compared to laborious\nfeature engineering eﬀorts for descriptor-based ML models.\nDespite this simplicity, thene-tuned Gemini is on par with\ndescriptor-based ML in terms of overall accuracy but achieves\nhigher weighted accuracy, underscoring its potential to balance\npredictive power with practical implementations.\n3.2. Moiety masking experiments\nWe have demonstrated the data-e ﬃciency and prediction\ncapability of the ne-tuned Gemini. Nevertheless, LLMs like\nGemini, oen perceived as “black boxes”, do not facilitate\ninterpretability and tunable hyperparameters like ML models\ntrained on carefully engineered descriptors. To interpret the\ncaptured inner sense as well as the robustness against infor-\nmation loss of thene-tuned Gemini, we conducted a series of\nmoiety masking experiments, systematically masking or\n‘ablating’ sections representing speci c chemical moieties\nwithin the SMILES prompts. As exemplied in such a moiety\nmasking experiment for Cu-BTC (Fig. 4), we replaced one of the\nCu identities with a <missing> annotation. By deliberately\nattacking chemical substructures within the SMILES\nrepresentation, we assessed whether the ne-tuned Gemini\ncould distinguish solid and meaningful chemical patterns,\nrather than merely“memorized” training data.\nUnlike a metal identity, a linker chemical moiety may exhibit\nvaried forms in a SMILES representation; we thus adopted\nSMILES arbitrary target specication (SMARTS)\n58 to locate and\nidentify the substructures of intended linker chemical moieties\nin SMILES strings. As illustrated in Table 2 and Fig. 5, we\nconsidered a list of 9 linkers, which were identied by SMARTS\nsubstructure matching using the RDKit package.\n59 Each of these\nlinker chemical moieties was subsequently subjected to\na moiety masking experiment to assess model robustness. As\npreviously discussed, the ne-tuned Gemini maintains both\ndata eﬃciency and accuracy at a training set ratio of 0.6. To\nensure a diverse coverage of linker chemistry, the base model\nwas ne-tuned anew using 1268 data points from the full\ntraining data set (i.e., at a training set ratio of 0.6), with the\nremaining 845 data points combined with 529 data points from\nthe original test set for out-of-sample predictions. The moiety\nmasking experiments were conducted on the data points\ncorrectly predicted in this new out-of-sample test, resulting in\n1019 and 962 SMILES representations to be tested for binary\nand quaternary classications, respectively.\nAs captured in Table 2, across 10 probed chemical moieties,\nthe ne-tuned Gemini retains the majority of its predictive\ncapacity, achieving a total accuracy of 0.96 for binary\nFig. 3 (a) Learning curves on the test set by theﬁne-tuned Gemini and the ML model with Pore + RACs. Spatial variations of trained MOFs\nencoded by (b) Pore + RACs and (c) transformer-embedded SMILES, as a function of training set ratio.\nThis journal is © The Royal Society of Chemistry 2025 J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 | 19311\nPaper Journal of Materials Chemistry A\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nclassication. For quaternary classi cation, a slightly lower\naccuracy of 0.92 is observed, which aligns with the challenging\nnature of skewed data distributions as discussed in Section 2.1.\nAmong the chemical moieties, masking amino and alkene\ngroups results in the greatest prediction disagreement for\nquaternary classication, indicating a relatively high impor-\ntance attributed by the ne-tuned Gemini. However, these\nmoieties have a relatively small sample size (22 and 49 for\namino and alkene groups, respectively), suggesting that the low\nagreement could partially stem from the limited data diversity\nof the test set. One plausible interpretation is that functional-\nities such as amino groups exert a notable inuence on MOF\nhydrophobicity. It is essential to acknowledge that the outcome\nof moiety masking experiments might vary depending on the\ndataset utilized for ne-tuning, thus making our conclusions\ndataset-specic. Nevertheless, these ndings imply that the\nne-tuned Gemini is capable of withstanding minor informa-\ntion disruptions in text prompts, reecting its robustness.\n3.3. Blind test\nWe further assess the potential of the ne-tuned Gemini in\na real-world application by predicting hydrophobicity in an\nunseen (or blind test) set of MOFs, randomly selected from the\nfree-solvent-removed (FSR) and ion-containing (ION) subsets in\nthe CoRE-MOF-2024 database, comprising 150 MOFs in each\nsubset. To facilitate full external validation, we retained only\nunique MOFs in the blind test set by discarding those with\nidentical molecular identi ers (as represented by SMILES)\ncompared to the trained MOFs. As illustrated in Fig. 6, thene-\ntuned Gemini achieves predictive accuracy scores of 0.66 and\n0.57 for binary and quaternary classications, respectively, in\nthe blind test set. It is worthwhile to note that the residual\nsolvents and ions included in these MOFs pose signi cant\ninterference in simulating host –guest interactions.\n60,61 Zhao\net al. also highlighted that the removal of strongly bound\nsolvent molecules in the ASR subset (employed as the training\nset inne-tuning) could yield MOFs with signicantly stronger\nwater a ﬃnity, causing substantial variations in predicted\nhydrophobicity in FSR and ION subsets.\n28 This eﬀect is reected\nby the predictions in individual subsets (Fig. S7–S8†), with less\naccuracy in the ION subset compared to the FSR subset.\nNevertheless, this more rigorous test reinforces thatne-tuning\nGemini provides a reliable and eﬀective approach for predicting\nMOF properties, thus facilitating large-scale, high-throughput\ncomputational screening. For instance, thene-tuned Gemini\ncan be eﬀortlessly deployed with solely SMILES input to swily\nFig. 4 Schematic illustration of a moiety masking experiment, where Cu is replaced with a <missing> annotation. Color scheme: Cu, brown; O,\nred; C, grey; H, white.\nTable 2 Moiety masking experiments on theﬁne-tuned Gemini\nMoiety SMARTS\nBinary Quaternary\nNo. of tests Accuracy No. of tests Accuracy\nMetal — 1019 0.96 962 0.92\nAromatic N [n; a] 317 0.97 299 0.94\nAlkyne [C#C] 10 1.00 10 1.00\nImine [$([CX3]([#6])[#6]),$([CX3H][#6])] = [$([NX2][#6]),$([NX2H])] 18 1.00 14 1.00\nHalogen [F,Cl,Br,I] 69 0.99 63 0.95\nEther [$([CX4]),$([cX3])]O[$([CX4]),$([cX3])] 75 0.97 70 0.92\nAmino [OX1] = CN 30 1.00 22 0.87\nEnamine [NX3][$(C ]C),$(cc)] 76 0.93 71 0.89\nKetone [#6][CX3]( =O)[#6] 16 1.00 15 1.00\nAlkene [C ]C] 59 0.92 49 0.88\nTotal — 1689 0.96 1576 0.92\n19312\n| J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 This journal is © The Royal Society of Chemistry 2025\nJournal of Materials Chemistry A Paper\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ndeprioritize potentially less hydrophobic MOFs from time-\nconsuming computations.\n4. Conclusions\nThis study illustrates the promise of LLMs in eﬀectively pre-\ndicting the hydrophobicity of MOFs. By ne-tuning Gemini\nsolely on SMILES and label pairs, we bypass the laborious\nprocess of feature engineering and yet achieve accuracy on par\nwith or exceeding descriptor-based ML benchmarks. Moiety\nmasking experiments and a stringent blind test demonstrate\nthe robustness and transferability of the ne-tuned Gemini,\nwhich can serve as a powerful screening tool for rapidly iden-\ntifying hydrophobic MOFs. We anticipate that continued\nrenement of LLM architectures, expanded training sets, and\ncloser integration with domain-specic knowledge will further\nadvance data-driven discovery of MOFs and other emerging\nmaterials.\nData availability\nAll datasets and python codes including moiety masking\nexperiments are available at the GitHub repository: https://\ngithub.com/xiaoyu961031/Fine-tuned-Gemini.\nConﬂicts of interest\nThere is no conict of interest to declare.\nAcknowledgements\nWe gratefully acknowledge the A *STAR LCER-FI projects\n(LCERFI01-0015 U2102d2004 and LCERFI01-0033 U2102d2006)\nand the National Research Foundation Singapore (NRF-CRP26-\n2021RS-0002) for nancial support, the National University of\nSingapore and the National Supercomputing Centre (NSCC)\nSingapore for computational resources.\nReferences\n1 M. Eddaoudi, J. Kim, N. Rosi, D. Vodak, J. Wachter,\nM. O'Keeﬀe and O. M. Yaghi,Science, 2002,295, 469–472.\nFig. 5 Representative linkers in moiety masking experiments.\nFig. 6 Confusion matrices by theﬁne-tuned Gemini on 300 randomly\nselected MOFs from (a) FSR subset and (b) ION subset.\nThis journal is © The Royal Society of Chemistry 2025 J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 | 19313\nPaper Journal of Materials Chemistry A\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n2 H. Furukawa, K. E. Cordova, M. O'Keeﬀe and O. M. Yaghi,\nScience, 2013,341, 1230444.\n3 K. Jayaramulu, F. Geyer, A. Schneemann, ˇS. Kment,\nM. Otyepka, R. Zboril, D. Vollmer and R. A. Fischer,Adv.\nMater., 2019,31, 1900820.\n4 L.-H. Xie, M.-M. Xu, X.-M. Liu, M.-J. Zhao and J.-R. Li,Adv.\nSci., 2020,7, 1901758.\n5 P. Z. Moghadam, D. Fairen-Jimenez and R. Q. Snurr,J. Mater.\nChem. A, 2015,4, 529–536.\n6 P. Küsgens, M. Rose, I. Senkovska, H. Fröde, A. Henschel,\nS. Siegle and S. Kaskel, Microporous Mesoporous Mater. ,\n2009, 120, 325–330.\n7 Z. Qiao, Q. Xu and J. Jiang,J. Mater. Chem. A, 2018,6, 18898–\n18905.\n8 Z. Qiao, Q. Xu, A. K. Cheetham and J. Jiang,J. Phys. Chem. C,\n2017, 121, 22208–22215.\n9 H. Lyu, Z. Ji, S. Wuttke and O. M. Yaghi,Chem, 2020,6, 2219–\n2241.\n10 H. Tang, L. Duan and J. Jiang,Langmuir, 2023, 39, 15849–\n15863.\n11 T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter,\nC. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever\nand D. Amodei, arXiv, 2020, preprint, arXiv:2005.14165,\nDOI: 10.48550/arXiv.2005.14165.\n12 Gemini Team Google, arXiv, 2023, preprint,\narXiv:2312.11805, DOI:10.48550/arXiv.2312.11805.\n13 R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,\nS. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut,\nE. Brunskill, E. Brynjolfsson, S. Buch, D. Card,\nR. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis,\nD. Demszky, C. Donahue, M. Doumbouya, E. Durmus,\nS. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn,\nT. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman,\nN. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho,\nJ. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky,\nP. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab,\nP. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar,\nF. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li,\nX. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani,\nE. Mitchell, Z. Munyikwa, S. Nair, A. Narayan,\nD. Narayanan, B. Newman, A. Nie, J. C. Niebles,\nH. Nilforoshan, J. Nyarko, G. Ogut, L. Orr,\nI. Papadimitriou, J. S. Park, C. Piech, E. Portelance,\nC. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong,\nY. Roohani, C. Ruiz, J. Ryan, C. R´e, D. Sadigh, S. Sagawa,\nK. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori,\nA. W. Thomas, F. Tram`er, R. E. Wang, W. Wang, B. Wu,\nJ. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia,\nM. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou\nand P. Liang,arXiv, 2021, preprint, arXiv:2108.07258, DOI:\n10.48550/arXiv.2108.07258.\n14 Z. Zheng, Z. Rong, N. Rampal, C. Borgs, J. T. Chayes and\nO. M. Yaghi,Angew. Chem., Int. Ed., 2023,62, e202311983.\n15 Z. Zheng, A. H. Alawadhi, S. Chheda, S. E. Neumann,\nN. Rampal, S. Liu, H. L. Nguyen, Y.-h. Lin, Z. Rong,\nJ. I. Siepmann, L. Gagliardi, A. Anandkumar, C. Borgs,\nJ. T. Chayes and O. M. Yaghi,J. Am. Chem. Soc., 2023, 145,\n28284–28295.\n16 Z. Zheng, O. Zhang, C. Borgs, J. T. Chayes and O. M. Yaghi,J.\nAm. Chem. Soc., 2023,145, 18048–18062.\n17 W. Zhang, Q. Wang, X. Kong, J. Xiong, S. Ni, D. Cao, B. Niu,\nM. Chen, Y. Li, R. Zhang, Y. Wang, L. Zhang, X. Li, Z. Xiong,\nQ. Shi, Z. Huang, Z. Fu and M. Zheng,Chem. Sci., 2024,15,\n10600–10611.\n18 M. S. Wilhelmi, M. R. Garc´ıa, S. Shabih, M. V. Gil, S. Miret,\nC. T. Koch, J. A. M´arquez and K. M. Jablonka,Chem. Soc.\nRev., 2025,54, 1125–1150.\n19 M. Suvarna, A. C. Vaucher, S. Mitchell, T. Laino and J. P´erez-\nRam´ırez, Nat. Commun., 2023,14, 7964.\n20 Z. Zheng, Z. He, O. Khattab, N. Rampal, M. A. Zaharia,\nC. Borgs, J. T. Chayes and O. M. Yaghi,Digital Discovery,\n2024, 3, 491–501.\n21 K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and B. Smit,\nNat. Mach. Intell., 2024,6, 161–169.\n22 J. V. Herck, M. V. Gil, K. M. Jablonka, A. Abrudan, A. S. Anker,\nM. Asgari, B. Blaiszik, A. Bu ﬀo, L. Choudhury,\nC. Corminboeuf, H. Daglar, A. M. Elahi, I. T. Foster,\nS. Garcia, M. Garvin, G. Godin, L. L. Good, J. Gu, N. X. Hu,\nX. Jin, T. Junkers, S. Keskin, T. P. J. Knowles, R. Laplaza,\nM. Lessona, S. Majumdar, H. Mashhadimoslem,\nR. D. McIntosh, S. M. Moosavi, B. Mouriño, F. Nerli,\nC. Pevida, N. Poudineh, M. R. Kochi, K. L. Saar,\nF. H. Saboor, M. Sagharichiha, K. J. Schmidt, J. Shi,\nE. Simone, D. Svatunek, M. Taddei, I. Tetko, D. Tolnai,\nS. Vahdatifar, J. Whitmer, D. C. F. Wieland, R. Willumeit-\nRömer, A. Züttel and B. Smit,Chem. Sci., 2025,16, 670–684.\n23 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n24 M. Krenn, F. H¨ase, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Mach. Learn.: Sci. Technol., 2020,1\n, 045024.\n25 M. Krenn, Q. Ai, S. Barthel, N. Carson, A. Frei, N. C. Frey,\nP. Friederich, T. Gaudin, A. A. Gayle, K. M. Jablonka,\nR. F. Lameiro, D. Lemm, A. Lo, S. M. Moosavi,\nJ. M. N ´apoles-Duarte, A. Nigam, R. Pollice, K. Rajan,\nU. Schatzschneider, P. Schwaller, M. Skreta, B. Smit,\nF. Strieth-Kaltho ﬀ, C. Sun, G. Tom, G. F. v. Rudor ﬀ,\nA. Wang, A. D. White, A. Young, R. Yu and A. Aspuru-\nGuzik, Patterns, 2022,3, 100588.\n26 M. Leon, Y. Perezhohin, F. Peres, A. Popoviˇc and M. Castelli,\nSci. Rep., 2024,14, 25016.\n27 B. J. Bucior, A. S. Rosen, M. Haranczyk, Z. Yao, M. E. Ziebel,\nO. K. Farha, J. T. Hupp, J. I. Siepmann, A. Aspuru-Guzik and\nR. Q. Snurr,Cryst. Growth Des., 2019,19, 6682–6697.\n28 G. Zhao, L. Brabson, S. Chheda, J. Huang, H. Kim, K. Liu,\nK. Mochida, T. Pham, P. Prerna, G. Terrones, S. Yoon,\nL. Zoubritzky, F.-X. Coudert, M. Haranczyk, H. Kulik,\nM. Moosavi, D. Sholl, I. Siepmann, R. Snurr and Y. Chung,\nChemRxiv, 2024, preprint arXiv:2024-nvmnr.\n29 S. S. Y. Chui, S. M. F. Lo, J. P. H. Charmant, A. G. Orpen and\nI. D. Williams,Science, 1999,283, 1148–1150.\n19314 | J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 This journal is © The Royal Society of Chemistry 2025\nJournal of Materials Chemistry A Paper\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n30 Gemini Team Google, arXiv, 2024, preprint\narXiv:2403.05530, DOI:10.48550/arXiv.2403.05530.\n31 R. P´etuya, S. Durdy, D. Antypov, M. W. Gaultois, N. G. Berry,\nG. R. Darling, A. P. Katsoulidis, M. S. Dyer and\nM. J. Rosseinsky, Angew. Chem., Int. Ed. , 2022, 61,\ne202114573.\n32 S. Pal, S. Kulandaivel, Y.-C. Yeh and C.-H. Lin,Coord. Chem.\nRev., 2024,518, 216108.\n33 I. B. Orhan, H. Daglar, S. Keskin, T. C. Le and R. Babarao,\nACS Appl. Mater. Interfaces, 2021,14, 736–749.\n34 X. Li, P. M. Ma ﬀettone, Y. Che, T. Liu, L. Chen and\nA. I. Cooper,Chem. Sci., 2021,12, 10742–10754.\n35 K. M. Jablonka, D. Ongari, S. M. Moosavi and B. Smit,Chem.\nRev., 2020,120, 8066–8129.\n36 K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn,\nF. Zhang, T. Strother, C. Park, E. Vedadi, J. Z. Chaves,\nS.-Y. Hu, M. Schaekermann, A. Kamath, Y. Cheng,\nD. G. T. Barrett, C. Cheung, B. Mustafa, A. Palepu,\nD. McDu ﬀ, L. Hou, T. Golany, L. Liu, J.-b. Alayrac,\nN. Houlsby, N. Tomasev, J. Freyberg, C. Lau, J. Kemp,\nJ. Lai, S. Azizi, K. Kanada, S. Man, K. Kulkarni, R. Sun,\nS. Shakeri, L. He, B. Caine, A. Webson, N. Latysheva,\nM. Johnson, P. Manseld, J. Lu, E. Rivlin, J. Anderson,\nB. Green, R. Wong, J. Krause, J. Shlens, E. Dominowska,\nS. M. A. Eslami, K. Chou, C. Cui, O. Vinyals,\nK. Kavukcuoglu, J. Manyika, J. Dean, D. Hassabis,\nY. Matias, D. Webster, J. Barral, G. Corrado, C. Semturs,\nS. S. Mahdavi, J. Gottweis, A. Karthikesalingam and\nV. Natarajan, arXiv, 2024, preprint, arXiv:2404.18416, DOI:\n10.48550/arXiv.2404.18416.\n37 T. F. Willems, C. H. Rycro , M. Kazi, J. C. Meza and\nM. Haranczyk, Microporous Mesoporous Mater., 2012, 149,\n134–141.\n38 A. Nandy, C. Duan, J. P. Janet, S. Gugler and H. J. Kulik,Ind.\nEng. Chem. Res., 2018,57, 13973–13986.\n39 A. Nandy, C. Duan, M. G. Taylor, F. Liu, A. H. Steeves and\nH. J. Kulik,Chem. Rev., 2021,121, 9927–10000.\n40 Z. Yao, B. S´anchez-Lengeling, N. S. Bobbitt, B. J. Bucior,\nS. G. H. Kumar, S. P. Collins, T. Burns, T. K. Woo,\nO. K. Farha, R. Q. Snurr and A. Aspuru-Guzik,Nat. Mach.\nIntell., 2021,3,7 6–86.\n41 Z. Xie, X. Evangelopoulos,\n¨O. H. Omar, A. Troisi, A. I. Cooper\nand L. Chen,Chem. Sci., 2024,15, 500–510.\n42 S. Zhong and X. Guan,Environ. Sci. Technol. Lett., 2023,10,\n872–877.\n43 S. M. Moosavi, A. Nandy, K. M. Jablonka, D. Ongari,\nJ. P. Janet, P. G. Boyd, Y. Lee, B. Smit and H. J. Kulik,Nat.\nCommun., 2020,11,1 –10.\n44 S. Majumdar, S. M. Moosavi, K. M. Jablonka, D. Ongari and\nB. Smit,ACS Appl. Mater. Interfaces, 2021,13, 61004–61014.\n45 A. Nandy, G. Terrones, N. Arunachalam, C. Duan,\nD. W. Kastner, H. J. Kulik, A. Nandy, G. Terrones,\nN. Arunachalam, C. Duan, D. W. Kastner and H. J. Kulik,\nSci. Data, 2022,9, 74.\n46 G. G. Terrones, S.-P. Huang, M. P. Rivera, S. Yue,\nA. Hernandez and H. J. Kulik,J. Am. Chem. Soc., 2024, 146,\n20333–20348.\n47 Y. Yue, S. A. Mohamed and J. Jiang,J. Chem. Inf. Model., 2024,\n64, 4966–4979.\n48 K. M. Jablonka, S. M. Moosavi, M. Asgari, C. Ireland,\nL. Patiny and B. Smit,Chem. Sci., 2021,12, 3587–3598.\n49 X. Wu, R. Zheng and J. Jiang,J. Chem. Theory Comput., 2025,\n21, 900–911.\n50 A. S. Palakkal, S. A. Mohamed and J. Jiang,Chem Bio Eng.,\n2024, 1, 970–978.\n51 Y. Qiu, L. Chen, X. Zhang, D. Ping, Y. Tian and Z. Zhou,\nAIChE J., 2024,70, e18575.\n52 Z. Zhang, F. Pan, S. A. Mohamed, C. Ji, K. Zhang, J. Jiang and\nZ. Jiang,Small, 2024,20, 2405087.\n53 A. Jose, E. Devijver, N. Jakse and R. Poloni,J. Am. Chem. Soc.,\n2024, 146, 6134–6144.\n54 Z. Cao, R. Magar, Y. Wang and A. B. Farimani,J. Am. Chem.\nSoc., 2023,145, 2958–2967.\n55 T.-W. Liu, Q. Nguyen, A. Bousso Dieng, D. A. G ´omez-\nGualdr´on, T.-W. Liu, Q. Nguyen, A. Bousso Dieng and\nD. A. G´omez-Gualdr´on, Chem. Sci., 2024,15, 18903–18919.\n56 L. Van Der Maaten and G. Hinton,J. Mach. Learn. Res., 2008,\n9, 2579–2605.\n57 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova,arXiv, 2018,\npreprint, arXiv:1810.04805, DOI:10.48550/arXiv.1810.04805.\n58 Daylight Theory: SMARTS - A Language for Describing\nMolecular Patterns, https://www.daylight.com/dayhtml/doc/\ntheory/theory.smarts.html, (accessed 13 Jan, 2025).\n59 RDKit: Open-Source Cheminformatics. https://www.rdkit.org.\n60 I. Cooley and E. Besley,Chem. Mater., 2023,36, 219–231.\n61 T. D. Pham, F. Joodaki, F. Formalik and R. Q. Snurr,J. Phys.\nChem. C, 2024,128, 17165–17174.\nThis journal is © The Royal Society of Chemistry 2025 J. Mater. Chem. A,2 0 2 5 ,13,1 9 3 0 7–19315 | 19315\nPaper Journal of Materials Chemistry A\nOpen Access Article. Published on 04 April 2025. Downloaded on 11/5/2025 4:17:30 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Metal-organic framework",
  "concepts": [
    {
      "name": "Metal-organic framework",
      "score": 0.7358480095863342
    },
    {
      "name": "Computer science",
      "score": 0.47645118832588196
    },
    {
      "name": "Environmental science",
      "score": 0.32115286588668823
    },
    {
      "name": "Chemistry",
      "score": 0.29545995593070984
    },
    {
      "name": "Organic chemistry",
      "score": 0.08664843440055847
    },
    {
      "name": "Adsorption",
      "score": 0.06277322769165039
    }
  ],
  "institutions": []
}