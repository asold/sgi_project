{
  "title": "Mask and Cloze: Automatic Open Cloze Question Generation Using a Masked Language Model",
  "url": "https://openalex.org/W4317795002",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5062839056",
      "name": "Shoya Matsumori",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5076279222",
      "name": "Kohei Okuoka",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5021970743",
      "name": "Ryoichi Shibata",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5110312103",
      "name": "Minami Inoue",
      "affiliations": [
        "Keio University"
      ]
    },
    {
      "id": "https://openalex.org/A5014459937",
      "name": "Yosuke Fukuchi",
      "affiliations": [
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A5070329642",
      "name": "Michita Imai",
      "affiliations": [
        "Keio University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2112898687",
    "https://openalex.org/W6737659039",
    "https://openalex.org/W1984050661",
    "https://openalex.org/W216153039",
    "https://openalex.org/W1965158952",
    "https://openalex.org/W2109609717",
    "https://openalex.org/W2161325192",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2810684233",
    "https://openalex.org/W2028402089",
    "https://openalex.org/W1988343427",
    "https://openalex.org/W2836869902",
    "https://openalex.org/W2895912893",
    "https://openalex.org/W2087190704",
    "https://openalex.org/W2791566576",
    "https://openalex.org/W2060565253",
    "https://openalex.org/W1969769774",
    "https://openalex.org/W2167454349",
    "https://openalex.org/W1999550557",
    "https://openalex.org/W2110569371",
    "https://openalex.org/W6603915874",
    "https://openalex.org/W19376526",
    "https://openalex.org/W6691649523",
    "https://openalex.org/W150749837",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W2758591789",
    "https://openalex.org/W2251056936",
    "https://openalex.org/W2614582630",
    "https://openalex.org/W2251694021",
    "https://openalex.org/W6682082992",
    "https://openalex.org/W6640156606",
    "https://openalex.org/W1153238640",
    "https://openalex.org/W2984472034",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2963789726",
    "https://openalex.org/W6687566353",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2060854787",
    "https://openalex.org/W4376474620",
    "https://openalex.org/W2044688197",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6768817161",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W3158733382",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2770810466",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2611178247",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W95870236",
    "https://openalex.org/W1512316528",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "This paper conducts the first trial to apply a masked language AI model and the &#x201C;Gini coefficient&#x201D; to the field of English study. We propose an algorithm named CLOZER that generates open cloze questions that inquiry knowledge of English learners. Open cloze questions (OCQ) have been attracting attention for both measuring the ability and facilitating the learning of English learners. However, since OCQ is in free form, teachers have to ensure that only a ground truth answer and no additional words will be accepted in the blank. A remarkable benefit of CLOZER is to relieve teachers of the burden of producing OCQ. Moreover, CLOZER provides a self-study environment for English learners by automatically generating OCQ. We evaluated CLOZER through quantitative experiments on 1,600 answers and show its effectiveness statistically. Compared with human-generated questions, we also revealed that CLOZER can generate OCQs better than the average non-native English teacher. Additionally, we conducted a field study at a high school to clarify the benefits and hurdles when introducing CLOZER. Then, on the basis of our findings, we proposed several design improvements.",
  "full_text": "Received 19 December 2022, accepted 19 January 2023, date of publication 23 January 2023, date of current version 1 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3239005\nMask and Cloze: Automatic Open Cloze Question\nGeneration Using a Masked Language Model\nSHOYA MATSUMORI\n 1, KOHEI OKUOKA\n 1, RYOICHI SHIBATA1, MINAMI INOUE 1,\nYOSUKE FUKUCHI\n 2, AND MICHITA IMAI 1, (Member, IEEE)\n1Keio University, Yokohama 223-8522, Japan\n2National Institute of Informatics, Tokyo 101-8430, Japan\nCorresponding author: Shoya Matsumori (shoya@ailab.ics.keio.ac.jp)\nThis work was supported in part by JST CREST, Japan, under Grant JPMJCR19A1; and in part by JSPS KAKENHI, Japan, under\nGrant JP21J13789.\nThis work involved human subjects or animals in its research. The authors confirm that all human/animal subject research procedures and\nprotocols are exempt from review board approval.\nABSTRACT This paper conducts the first trial to apply a masked language AI model and the ‘‘Gini\ncoefficient’’ to the field of English study. We propose an algorithm named CLOZER that generates\nopen cloze questions that inquiry knowledge of English learners. Open cloze questions (OCQ) have been\nattracting attention for both measuring the ability and facilitating the learning of English learners. However,\nsince OCQ is in free form, teachers have to ensure that only a ground truth answer and no additional\nwords will be accepted in the blank. A remarkable benefit of CLOZER is to relieve teachers of the\nburden of producing OCQ. Moreover, CLOZER provides a self-study environment for English learners by\nautomatically generating OCQ. We evaluated CLOZER through quantitative experiments on 1,600 answers\nand show its effectiveness statistically. Compared with human-generated questions, we also revealed that\nCLOZER can generate OCQs better than the average non-native English teacher. Additionally, we conducted\na field study at a high school to clarify the benefits and hurdles when introducing CLOZER. Then, on the\nbasis of our findings, we proposed several design improvements.\nINDEX TERMS Open cloze test, automatic question generation, masked language model, field study.\nI. INTRODUCTION\nAnswer a word that will fit in the blank in the following\nsentence: ‘‘If you want to go to a top university, you should ( )\nEnglish hard. ’’1 Some of you might have struggled to answer\nsuch a question in the past. A question like this that asks you\nto fill in a gap with a word is called an Open Cloze Test or\nOpen Cloze Question (OCQ) [1], and it is widely used in\nlanguage assessment tests for second language (L2) learners,\nsuch as in Cambridge Assessment English tests. Compared to\nthe commonly used Multiple Choice Question (MCQ), where\nboth the correct answer and several wrong answers (often\ncalled detractors) are provided for each question, the OCQ\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Francisco J. Garcia-Penalvo\n.\n1The answer is study.\nprovides fewer clues for coming up with the right answer and\nthus is considered to require higher cognitive abilities than\nsimple reading skills [2], [3]. OCQs are therefore useful not\nonly for assessing the learner’s abilities over a wide range but\nalso for prompting language acquisition [4], [5], [6].\nIntroducing the OCQ to the educational field has many\nbenefits. However, manual creation of the OCQs requires\nthe designers to consider multiple aspects of a question,\nwhich is often overly cumbersome and time-consuming\nfor teachers. Especially, to make the OCQs educationally\nbeneficial, teachers have to ensure the answer uniqueness\nthat only a ground truth answer and no additional words\nwill be accepted in the blank [7]. A promising approach to\nsuch problems is automated question generation. However,\nprevious studies on automated question generation have\nVOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 9835\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFIGURE 1. Given a target word that a student wants to learn or a teacher wants them to learn, CLOZER generates suitable OCQs from arbitrary corpora\non the basis of a novel metric called a gap score. A question with a higher gap score only accepts the target word and does not allow other words in the\nblank; therefore, it represents the typical usage of the word.\nfocused almost entirely on MCQs (e.g., [8], [9], [10]), and\ndespite its educational importance, research on the generation\nof OCQs is relatively rare. Hence, our research questions are\nto develop the algorithms that can generate suitable OCQs\nand investigate what the hurdles might be for introducing\nsuch methods to the educational field.\nIn this work, we propose CLOZER, an automatic OCQ\ngeneration algorithm (Fig. 1). Given a target word that a\nstudent wants to learn or a teacher wants them to learn,\nCLOZER can generate OCQs having the answer uniqueness\nfor the target word from arbitrary corpora. Because such\nOCQs are expected to include the typical usage of the\ntarget word, it is assumed that they will help students\nto grasp the actual concept of words. CLOZER generates\nquestions by prediction and assessment: first, it predicts\nwords that are likely to fit in a blank, and then, on the\nbasis of the prediction, it assesses the answer uniqueness.\nA masked language model (MLM) [11] is utilized for the\nprediction. Given a sentence with a blank, MLM will predict\na set of candidate words and assign them confidence values\nindicating the feasibility of each word belonging in the blank.\nHowever, simply using the ranking of the confidence values is\nnot sufficient to guarantee the answer uniqueness. To this end,\nwe introduce a novel metric called a gap score that measures\nthe answer uniqueness. Our design of the gap score was\ninspired by the Gini coefficient, a metric used in economics to\nshow wealth or income inequality. With the gap score, we can\nensure not only the feasibility of the target word to the blank\nbut also the exclusivity of the other words to the blank.\nTo verify whether CLOZER can generate OCQs having\nenough answer uniqueness to be used in the field of\nlanguage learning, we conducted a quantitative experiment\non Amazon Mechanical Turk. After analyzing 1,600 answers\nfrom 40 native English speakers, we found that the gap score\nreliably represents the answer uniqueness. And, compared\nwith the questions the average non-native English teacher\nmade, it was suggested that CLOZER can generate OCQs\nbetter than them. Additionally, we conducted a field study\nat a local high school to investigate the benefits and hurdles\nof introducing CLOZER to the education field. Twenty-\nfour high school students who are L2 English learners\nparticipated in the study. The results revealed that, while\nmost of the students found the test difficult, they also felt\nit was helpful for improving their English skills. Finally,\non the basis of our findings, we proposed several design\nimprovements.\nOur study gives several insights and contributes to the field\nas follows:\n• Our work contributes to generating English tests auto-\nmatically. Although CLOZER focuses on only OCQ,\nit provides insight into using language models to\nresearchers in the field of computer-aided education who\ninvestigate the method of generating questions tailored\nfor learners.\n• We pioneered the use of MLM for the automatic\ngeneration of questions for language learning. MLM\nhas a wide range of applications in Natural Language\nProcessing. However, none of the previous studies have\napplied MLM’s inference function to generating English\ntests.\n• We introduced the gap score to measure the feasibility\nof a question by ensuring that the target word would\nuniquely fit into the gap. Since the gap scores could\naccurately grasp the answer uniqueness of a question,\nit evaluates the gap and can indicate whether it requires\nmultiple choice rather than OCQ. The suggestion helps\nprovide a variety of questions in the test.\n• We conducted a proof of concept through a field study\nand clarified the possible benefits and hurdles when\nintroducing CLOZER to the educational field. Also,\nthe result suggested that MLM could grasp the native\nspeakers’ sense of language which Japanese students\noften struggled to grasp.\nThe remaining paper is organized as follows: Section II\nprovides related works to our study. Section III explains the\ndetails of our methods named CLOZER, and the preliminary\nexperiment to select the suitable MLM architectures for\nCLOZER is described in Section IV. Section V describes\nthe quantitative experiment to verify the performance of\nCLOZER compared with non-native English human teachers.\n9836 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nSection VI illustrates the field study of the prototype of\nthe CLOZER application we conducted in a high school.\nSection VII includes limitations and potential future work of\nour study, and Section VIII concludes our paper.\nII. RELATED WORK\nA. AUTOMATIC QUESTION GENERATION FOR LANGUAGE\nLEARNING\nWith the development of information technologies, massive\nopen online courses (MOOCs) and other e-learning platforms\nhave gained attention recently [12], [13], [14], [15]. In par-\nticular, in the field of computer-assisted language learning\n(CALL) [16], researchers have examined various approaches\nfor automatic question generation [17], [18], [19], [20]. With\nthe aid of automation, educators can reduce the cost of\nquestion construction, which frees them up to concentrate\non more important activities. Additionally, having a large\nnumber of questions generated by a model enables additional\nteaching processes such as adaptive learning and repetitive\ndrill practice to be utilized. Finally, a certain control of the\nquestion characteristics (e.g., difficulty) is useful for meeting\nthe requirements of particular test settings.\nB. CLOZE TESTS\nThe cloze test is the typical question format used in language\nlearning, and it is known that the cloze test has efficacy\nin language learning. There is a variation in the close test,\nas described below. The cloze test (alternatively, open cloze\ntest or cloze procedure) was invented by Taylor [1] in the early\n1950s. The term cloze stems from the Gestalt psychology\ntheory of the principle of closure [21], which mentions the\nhuman tendency to perceive complete patterns from partially\nhidden or incomplete patterns. Cloze tests can be easily\ncreated by deleting random words or every nth word from\npassages. While cloze tests were initially introduced as a\nmeasurement tool for the readability of prose passages, some\nresearchers (including Taylor himself) later discovered that\nit was also applicable to the measurement of one’s reading\nability [4], [5].\nResearch on cloze testing initially focused on native\nEnglish speakers but was later applied to the field of L2 learn-\ning and teaching. Many studies revealed the relationships\nbetween cloze test scores and L2 learners’ language skills\nand concluded that it was effective for measuring proficiency\nand therefore valuable as a placement test (e.g., [22], [23]).\nVariants of the cloze test, such as the C-test [24] and the\nrational cloze test [25], which aimed to revise some of the\nweaknesses of the original cloze test, have also been accepted\nas measurement tools. There are also claims that cloze\ntests can be a good tool for measuring integrative language\nskills [6]. Although there is some debate regarding the extent\nto which cloze testing can actually measure language skills,\nits benefits are now generally accepted (e.g. [26], [27]).\nTests featuring multiple choice questions (MCQ), which\ninclude both the correct answer and several wrong answers\n(often called distractors), have frequently been used as\nwell [28]. However, with MCQ, test takers tend to find the\nanswers simply by a process of elimination after inspecting\nthe given choices, and thus it is considered to require limited\nlanguage abilities [2]. In fact, Mizumoto et al. [3] reported\nthat, compared to the cloze test condition, less brain activation\nwas observed in the MCQ condition.\nC. AUTOMATIC GENERATION OF CLOZE TESTS\nMost of the prior works on automatic question generation\nhave focused on MCQs [8], [9], [10], [29], [30], [31], [32].\nIn general, there are three main steps required for generating\nMCQs: (1) selecting sentences from arbitrary sources, (2)\ndetermining a blank part from each sentence, and (3) gen-\nerating distractors for each blank. Among such procedures,\nmethods for (3) generating distractors have been intensively\nstudied since distractors are considered to have a significant\ninfluence on the quality of a question [33]. A common\nstrategy for distractor generation is to use a similarity with the\ntarget word as the original word for the blank. For example,\nsome studies have used syntax-based similarity [32], [34],\n[35], [36], such as parts of speech (POS) and spelling\nsimilarity, feature-based similarity [37], or context-based\nsimilarity [34], [35].\nConversely, (1) sentence selection and (2) determining a\nblank part have not been studied as much. Sentences are\nusually chosen from arbitrary sources, such as a corpus or\na textbook. Majumder and Saha [37] proposed a sentence\nselection method that uses topic modeling and parses\nstructure similarity. Most prior studies have utilized a fixed\nstrategy for the gap selection; for example, Sumita et al. [8]\nselected the leftmost single verb, and Lin et al. [30] only\nselected an adjective as a blank. One of the few exceptions is\na work by Goto et al. [32], which proposed a selection method\nbased on conditional random fields (CRFs) [38].\nWhile the automatic generation of MCQs has been studied\nextensively, there is much less research on OCQs [33]. One\nof the earliest studies was done by Becker et al. [39], who\naddressed the problem of evaluating the quality of OCQs in a\ndata-driven manner. Their Mind the Gap dataset was collected\nfrom 85 crowdsourcing workers and includes annotations\nfor the quality of questions. Additionally, Malafeev [40]\ndeveloped an automatic OCQ generation algorithm that\naimed to emulate the OCQs found in Cambridge Assessment\nEnglish tests. Similar to our approach, Felice and Buttery [41]\nassumed that the fewer possible words for the gap, the easier\nthe question, and introduced information entropy [42] to\nmeasure the complexity of a gap. While most studies have\nproposed a statistical or machine learning approach, Marrese-\nTaylor et al. [43] examined the use of deep learning for the\nautomatic generation of OCQs. They used LSTM with a\nsequence labeling task whose objective was to sequentially\nlabel each word with whether it was suitable for a gap or not.\nThe original dataset, which consisted of 1.5M questions, was\nused for training.\nVOLUME 11, 2023 9837\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nD. LANGUAGE MODELING\nLanguage modeling is used to obtain a language model\nthat represents the probability distribution of words given\ncertain context words. Language models have a wide range\nof applications, such as speech recognition [44], machine\ntranslation [45], summarization [46], text generation [47],\nand text classification [48].\nA typical language model is an n-gram [49]. The n-gram\nrefers to the number of n sequential words that are used to\npredict a word. For example, the n-gram of n = 1 is called\na unigram, which only sees the current word, and n = 2 and\nn = 3 are called a bigram and a trigram, respectively, which\nsee the current word and one or two history words. While the\nunigram can be considered as i.i.d., in the 2 ≤ n cases, the\nn-gram can be considered as an n − 1 order Markov process.\nAnother approach for language modeling uses a neural net-\nwork as an approximation function. Most of the studies [50]\nin this vein have used a recurrent neural network (RNN)\nfor this purpose, such as LSTM [51] or GRU [52]. These\napproaches utilize left-to-right/right-to-left language model-\ning, which models the language sequentially by predicting the\nrightmost/leftmost token in the forward/backward direction\nor a combination of the two.\nOne of the most powerful language models currently\nis BERT [11], which has been used in a novel language\nmodeling task called masked language modeling (MLM)\nfor the Transformer architecture [53]. In MLM, similar\nto the cloze test, a certain portion of tokens (usually\n15%) in the input sentences is randomly substituted with\na special token called a mask token, and the model is\ntrained to predict tokens for each of these masked tokens.\nUnlike the previous left-to-right or right-to-left language\nmodeling, MLM has successfully obtained deep-bidirectional\nrepresentations, demonstrating state-of-the-art performances\nin various downstream tasks [54].\nE. LIMITATIONS OF PREVIOUS STUDIES\nThe limitations of these previous studies are summed up\nas follows. First, the gap generation for MCQs [8], [30],\n[32] is not applicable to OCQs because there is no demand\nto ensure uniqueness in MCQs and it is not designed to\ndo so. While prior studies have addressed the generation\nof gaps in OCQs, several hurdles still exist. The n-grams\nutilized in Felice and Buttery [41] cannot reflect longer\ncontext due to the computational costs. In fact, only a 5-gram\nmodel was used in that study, which is problematic if we\nwant to consider a longer context that requires referencing\nwords more than four words distant from the target word.\nAs for other studies [39], [40], they require a dataset to train\nthe model but collecting such a dataset imposes substantial\ncosts. Additionally, their interests were to generate OCQs\nfrom arbitrary text sources and they cannot be applied to\ngeneration based on the target words. Finally, the above\nstudies focused only on evaluating their algorithms, and the\ndesign requirements for the application are still unknown.\nIII. METHODS AND IMPLEMENTATIONS\nThe general flow of CLOZER, an automatic open cloze\nquestion generator for computer-mediated language learning,\nis shown in Fig. 2. Given a target word that one desires to\nlearn, CLOZER generates open cloze questions based on a\ngap score. A sentence whose gap score is high means that\nis has the answer uniqueness for the target word. In other\nwords, such a sentence is expected to embody the typical\nusage of the target word. By asking questions whose gap\nscores are high, students can learn the most specific use of the\ntarget words. The generation process can be divided into three\nsteps: fetching masked target sentences, predicting masks,\nand computing gap scores.\nA. STEP 1: FETCH MASKED TARGET SENTENCES\nGiven a target word w, a set of sentences Sw that includes w\nis collected from an arbitrary corpus S as\nSw = {s| s ∈ S ∧ w ∈ s}. (1)\nSince Sw may include some words that are unfamiliar or\nimproper for learners, they are filtered out in accordance with\na pre-defined word list W. Sentences that are too short or\ntoo long are pruned as well. The target sentences Sw are then\nconverted into masked target sentences ˜Sw, where the target\nword is substituted by a special mask token.\nB. STEP 2: PREDICT MASKS\nGiven a masked target sentence ˜s ∈ ˜Sw, the masked language\nmodel FM yields the tuple of a set of candidate words λ along\nwith corresponding confidence values c, as\n(λ, c) = FM(˜s), (2)\nwhere λ = {wi | i ∈ Iv} and c = [ci | i ∈ Iv], Iv is an\nindex set whose size is the number of vocabularies, and both\nλ and c are sorted in the descending order determined by c.\nThe confidence score of the masked prediction ci represents\nthe feasibility of a word wi being in the position of the masked\ntoken.\nC. STEP 3: COMPUTE GAP SCORES\nOn the basis of the tuple (λ, c), the gap score ϕ is computed.\nϕ represents the answer uniqueness of a question, namely,\nhow uniquely the target word fits the gap. Our design for the\nanswer uniqueness is based on the following two criteria:\n1. The target word must have a high confidence value\n2. The target word must have significantly higher confi-\ndence compared to the other candidates\nA question is required not only to be in higher confidence\nbut also to have significantly higher confidence compared to\nother word candidates. This is because there are cases where\nthe target word has high confidence, for example, 0.8, but the\nsecond possible candidate has a lower but not significantly\nlow confidence value, for example, 0.19. In such a case, it is\npossible that the second-best candidate will be chosen.\nTo meet the criteria, we borrow the idea of the Gini\ncoefficient [55], [56], a widely used metric in the field of\n9838 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFIGURE 2. The three generation steps of CLOZER. (Step 1) Given a target word, target sentences are extracted from arbitrary corpora and then are\nconverted into masked sentences, whose target words are substituted with a mask token. (Step 2) Given a set of masked target sentences, a masked\nlanguage model predicts words and confidences for the mask token. (Step 3) On the basis of the mask prediction, a gap score for each target sentence is\ncomputed, and sentences with high gap scores are selected as questions.\neconomics that shows a wealth or income inequality existing\nbetween two specific groups. In a similar manner, we regard\nthe confidence value as wealth or income and use the Gini\ncoefficient to measure any inequality that exists between\nthe confidence of the target words and that of the other\ncandidates. Originally, the Gini coefficient is defined as\nG = 1 − 2\n∫1\n0\nL(x) dx, (3)\nwhere L(x) is the Lorenz curve [57], which is a monotonic\nincreasing function that takes the cumulative portion of the\npopulation x and returns the cumulative portion of the total\nwealth or income. Since L(x) is a convex function, the range\nof the Gini coefficient is 0 ≤ G < 1, where G = 0 implies\na state of complete equality and G = 1 a state of complete\ninequality.\nGiven a set of confidences c, the Lorenz curve for the\nconfidence values c is defined as a discrete function:\nL(c, i) =\n∑i\nj=1 cj\n∑Nc\nj=1 cj\n, (4)\nwhere c is sorted in descending order and Nc is a size of c.\nThe Gini coefficient for L(c, i) is derived as a discrete form\nof (3), as\nfgini(c) = 1 − 2\nNc∑\ni=1\n1\nNc\nL(c, i) + L(c, i − 1)\n2 , (5)\nwhose range is 0 ≤ fgini(c) < 1. Akin to the original\ninterpretation, fgini(c) = 1 represents that the confidence\nvalues are monopolized by the top-1 word and fgini(c) =\n0 represents that they are fairly distributed to each word.\nAccompanied by fgini, the equation for the gap score is\ndefined as\nϕ(c, j) = fgini(cj:) · frw(c, j), (6)\nwhere j is the index of the target word in λ, i.e., wj = w.\nTo eliminate the words that are not significant, the function\nfrw shrinks the gap score based on the proportion of the\nconfidence value of the target word and top-k confidences,\nas\nfrw(c, j) = cj\n∑k\ni=1 ci\n, (7)\nwhere k is a constant value.\nFinally, by applying (2) and (6) for every masked sentence\nin ˜Sw, we obtain the sorted gap scores for all sentences as\n8 = {ϕi | i ∈ IS}, (8)\nwhere IS is an index set whose size is the corpus size and\nϕi represents the confidence score for i-th ˜s, which satisfies\nϕi ≤ ϕi+1.\nIV. PRELIMINARY EXPERIMENT\nA. OVERVIEW\nTo select the feasible MLM architectures for CLOZER,\nwe conducted a preliminary experiment to compare the\nperformance across different MLM architectures. As a way of\nevaluating the performance, we asked native English speakers\nto take a cloze test, and we derived the correlation coefficient\nbetween the gap score calculated by CLOZER with each\ncandidate MLM architecture and the correct ratio of those\nspeakers. Our expectation was that a question with a higher\ngap score will collect answers that match the target word,\nwhile a question with a lower gap score will collect a wide\nrange of answers that are both syntactically and semantically\nfeasible but not necessarily the target word. By comparing\nthe difference in the correlation coefficient between the\ncandidate MLM architectures, we selected the feasible MLM\narchitectures for CLOZER.\nB. METHODOLOGY\nFirst, we generated questions to create a cloze test according\nto the procedure of CLOZER that is explained as Step\nVOLUME 11, 2023 9839\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\n1 in Section III. The target words were chosen from the\nvocabulary list of EIKEN, a widely used English qualification\nexam in Japan. The vocabularies were collected from Grade\n5 to Grade 2, respectively equivalent to the first year of junior\nhigh school and to high school graduates. Since CLOZER\nmainly focuses on vocabulary learning, only content words\nwere chosen for the target words; function words (e.g.,\ndeterminers, pronouns, prepositions, and conjunctions) were\nexcluded. And, the target sentences were collected from\nthe Corpus of Contemporary American English [58], which\nincludes 1.1 billion words collected from sources of various\ngenres spanning spoken word performances, fiction, popular\nmagazines, newspapers, academic texts, TV and movie\nsubtitles, blogs, and other web pages. Then, we sampled\n80 questions randomly from the target sentences after\nchecking to make sure the questions did not include any\ninappropriate words or personal information.\nNext, we recruited 40 participants from Amazon Mechani-\ncal Turk, the popular crowdsourcing platform, in order to ask\nthem to take the cloze test. To evaluate the performance in\nvarious questions, we divided the questions and participants\ninto two groups. Then, all participants were asked to read the\ninstructions carefully and to answer the set of 40 questions\nassigned to each of them. To collect answers from trustworthy\nEnglish speakers, we restricted the status of the workers to\nMasters, which means they had passed statistical monitoring\nand qualified as being high performers, and to those who\nlived in the U.S. The reward for the assignment was set to\n6.5 USD, as the task was expected to take one hour and this\nis the average hourly payment on MTurk. We did not allow\nthe same workers to participate in the task again.\nFinally, we calculated the gap score by using CLOZER\nwhich is based on each MLM architecture and the correlation\ncoefficient between the gap score and the correct ratio.\nFor the candidate MLM architectures, we utilized eight\nmodel variants based on BERT [11], DistilBERT [59],\nRoBERTa [60], and ALBERT [61] that were trained with\nmasked language modeling 2: bert-base-uncased, bert-large-\nuncased, distilbert-base-uncased, roberta-base-v2, roberta-\nlarge-v2, albert-base, albert-large, and albert-xlarge.\nC. RESULT\nThe questions were then used to collect 1,600 answers from\n40 native English speakers at Amazon Mechanical Turk. The\nduration of each task was 41.24±28.11 minutes on average.\nThe average correct ratio (i.e., the ratio of matches between\nthe target word and the submitted answer) was 40.58±9.33%,\nwhere the highest score was 55.00% and the lowest was\n20.00%.\nTable 1 indicates that the gap score is effective as a metric\nfor measuring the uniqueness of OCQs. Specifically, roberta-\nbase performed the best and bert-large-uncased was the\nsecond best, with correlation coefficients of 0.800 and 0.791,\nrespectively. In contrast, the performance of the DistilBERT\n2All of the pre-trained models were provided by Hugging Face [62].\nTABLE 1. The table shows the result of the preliminary experiment. For\neach MLM architecture, Pearson’s correlation coefficients (r ), p-values\n(p), number of parameters, and size of the dataset used in pre-training\nare shown in this table. All models were statistically\nsignificant (p < 0.001). And, ‘‘roberta-base’’ got the highest coefficient.\nFIGURE 3. Correlation plots between correct ratio and gap score obtained\nby the best-performed model, roberta-base.\nand ALBERT variants was lower than the BERT and\nRoBERTa variants. This is presumably due to the side effect\nof lightening the model size, considering both DistilBERT\nand ALBERT have made efforts to shrink the number\nof parameters. Although increasing the parameter size in\nthe same architecture generally improved the correlation\ncoefficient, this was not the case with RoBERTa. In roberta-\nlarge, some questions yielded higher gap scores than we\nexpected. In other words, the model evaluated the answer\nuniqueness too higher than humans, and this caused the\ncorrelation coefficient to degrade.\nFigure 3 shows the scatter plot of the gap score and correct\nratio for the most correlated model. Usually, the higher\ngap score tends to be the higher correct score. There were\ncomparably few samples that had high gap scores but low\ncorrect ratios. Some of the samples with lower gap scores had\nhigher correct ratios, but this is acceptable if their number is\nnot too large, since the users will usually choose questions\n9840 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nwhose gap scores are high and ignore the other samples.\nMoreover, the scatter plots of the remaining models can be\nfound in the appendix.\nD. REPRESENTATIVE SAMPLES\nIn this section, we further investigate the four representative\ncases obtained by the best-performed model, roberta-base:\ncases where the gap score and the correct ratio were both\nhigh or both low, and cases where one of the two was\nhigh. Moreover, some additional samples are found in the\nappendix.\nA sample for the first case, where both the gap score and the\ncorrect ratio were high, is ‘‘Just for my own ( ) of mind, Ted,\nsend me a list of your exes’ names and current addresses.’’\nThe target word for this question was peace. The gap score\nfor this target sentence was 0.90, and the correct ratio was\n90%. This type of question will only accept the target answer\nand, therefore, will be selected as a cloze question.\nIn contrast, a sample where both the gap score and the\ncorrect ratio were low is ‘‘She impressed me as a rather (\n) but pleasant woman who initially seemed quite timid and\nshy.’’ The target word was serious, and the gap score and the\ncorrect ratio were both 0.0. A question where the gap score is\nlow is expected to accept various words other than the target\nword, and thus, will not be selected as a question.\nAn example of a case where the gap score was high but\nthe correct ratio was relatively low is ‘‘There is a point,\nwhen trying to ( ) a turn on a motorcycle, when you\nknow you cannot negotiate the turn.’’ The target word for this\nsentence was negotiate, and the gap score and the correct ratio\nwere respectively 0.86 and 10%. Interestingly, the typical\nanswer was make, which is also acceptable. We assume most\nworkers made early decisions without noticing the same\nphrase ‘‘negotiate the turn’’ in the following clause. This\nsuggests that the model tended to be too confident when the\ntarget word was repeated in a similar phrase. Filtering out\nsuch cases may prevent this problem.\nThe final sample is the case where the gap score was low\nbut the correct ratio was high. An example is ‘‘Not long after\nthat, however, they’d discovered the caves lying beneath those\nmountains, and the species which should have gone ( )\nlived on.’’ The target word for this sentence was extinct. While\nthe correct ratio of the question was 85%, the gap score was\n0.11 and the top-1 prediction of the model was underground.\nAgain, this case is more acceptable than the third case since\nthese questions are discarded and not selected as questions in\nthe end.\nV. QUANTITATIVE EXPERIMENT\nA. OVERVIEW\nTo verify whether CLOZER can generate OCQs having\nenough answer uniqueness to be used in the field of\nlanguage learning, we asked high school English teachers\nto manually create OCQs and then compared them with\nthe ones generated by CLOZER. For evaluation, similar to\nTABLE 2. The table shows the comparisons of correct ratio and gap score\nbetween human-generated and CLOZER-generated questions. For\nhuman-generated questions, the average results and the results of the\nbest-performing participant are shown. For CLOZER-generated questions,\nthe results of roberta-base-v2 and bert-large-uncased are shown. And,\nthe gap scores of CLOZER-generated questions were higher than 0.8.\nthe preliminary experiment, we also asked native English\nspeakers to take the human-generated cloze tests, and we\nevaluated the difference between human-generated and auto-\ngenerated tests by using both the correct ratio and correlation\ncoefficient between the gap score and correct ratio.\nB. METHODOLOGY\nFirst, we asked high school English teachers to manually\ncreate OCQs. Since we expect most of the teachers using\nCLOZER to be non-native English speakers, we recruited\nnon-native high school English teachers at a high school\nin Japan. Eight teachers (average age: 46.12 ± 11.87)\nparticipated in the experiment. Each teacher created five\nquestions; thus, we collected 40 questions in total. In the\nexperiment, teachers were asked to create questions based on\nthe same designated target words that were used to generate\nquestions in the previous experiment. Teachers were allowed\nto consult any materials during the experiment unless they\ndirectly copied or modified existing OCQs. It took 19.13 ±\n8.04 minutes on average to create five questions.\nNext, we recruited 40 participants from Amazon Mechan-\nical Turk. All participants were asked to read the instructions\ncarefully and to answer 40 questions. To collect answers\nfrom trustworthy English speakers, we restricted the status\nof the workers similar to the preliminary experiment. And,\nthe reward for the assignment was also the same as the\npreliminary experiment.\nFinally, for comparison, we prepared the questions gener-\nated by CLOZER with the two architectures: roberta-base-\nv2, and bert-large-uncased which are the best-performed and\nthe second-performed model, respectively. When CLOZER\nis used to generate OCQ, we have to set the threshold of gap\nscore to select suitable questions after computing gap scores.\nThus, in this experiment, we extracted the questions whose\ngap score is higher than 0.8 from the questions generated in\nthe preliminary experiment.\nC. RESULT\nThe human-generated questions were then used to col-\nlect 1,600 answers from 40 native English speakers at\nAmazon Mechanical Turk. And, as auto-generated ques-\ntions, we extracted the 14 and 12 questions from the\nquestions generated by CLOZER with roberta-base-v2, and\nVOLUME 11, 2023 9841\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFIGURE 4. The screenshots of the prototype of a web application using CLOZER, ‘‘CLOZER proto’’ . The screen shown at\nthe top left was displayed to the local high school student in the answering process of the field study. The screen shown\nat the bottom left was that we added the hint function to it based on the result of the first day. The screen shown at the\nright was displayed the result and questionnaires for the students after every question.\nbert-large-uncased, respectively. The results are presented in\nTable 2, where Human (avg) represents the average results\nfor human-generated questions, Human (best) represents\nthe results for questions from the best-performing partici-\npant, and CLOZER 0.8 represents the results for CLOZER-\ngenerated questions whose gap scores were higher than 0.80.\nThe correct ratio and gap score for the Human (avg) condition\nwere comparably lower than those of the CLOZER variants.\nThe best-performing result for the human condition was\n88.0%, which is better than the CLOZER variants, although\nits deviation was higher than the CLOZER conditions. These\nresults indicate that CLOZER can generate higher-quality\nsingle-answer questions than average non-native English\nteachers.\nVI. FIELD STUDY\nA. OVERVIEW\nTo clarify the benefits and hurdles when introducing open\ncloze tests generated by CLOZER to the educational front,\nwe conducted a field study at a high school in Kanagawa,\nJapan for two days. For this purpose, a prototype of the\nCLOZER application, ‘‘CLOZER proto’’, was deployed on\na web platform accessible from any device using a browser.\nIn the experiment, the high school students answered the open\ncloze questions generated by CLOZER through ‘‘CLOZER\nproto’’. After submitting all answers, participants were asked\nto answer the questionnaires to share their impressions of the\nprototype.\nB. CLOZER PROTO\n‘‘CLOZER proto’’ is a web application that we developed as\na prototype of the application using CLOZER. ‘‘CLOZER\nproto’’ display the OCQs generated by CLOZER, whose\nblank is replaced by an input field, and the user can input\ntheir answer to the input field in the question (shown in\nFigure 4). Originally, although users can decide on a target\nword and set an arbitrary corpora to extract target sentences,\nwe decided them according to the same strategy as the\nquantitative experiments to generate target questions in this\nstudy. Specifically, we set the level of vocabulary used in the\nquestions so that it would be familiar to high school students\nby restricting the target words and target sentences with the\nword list.\nAlso, ‘‘CLOZER proto’’ includes the function of web form\nquestionnaire for this study, as shown in Figure 4. Before\nthe cloze test, they answer the pre-questionnaires related\nto their ability and attitude regarding English. Then, the\ncloze test which consisted of 20 open cloze questions was\nconducted. However, after every question, participants were\nasked to answer three per-question questionnaires related to\nthe appropriateness of the question. Finally, after the cloze\n9842 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nTABLE 3. Questionnaires used in the field study.\ntest, participants were asked to answer post-questionnaires to\nshare their impressions of the prototype.\nC. EVALUATION\n1) PRE-QUESTIONNAIRES\nFirst, to investigate the participants’ skills in English, the\nstudents were asked to answer the qualifications regarding\nEnglish skills that they possess. Next, to investigate the\nparticipants’ attitudes regarding English, they were asked to\nfill in pre-questionnaires. (PreQ1—2, Table 3).\n2) ANSWER EVALUATION\nWe evaluated their answer by the correct ratio for the cloze\ntest we conducted. The correct overall ratio was measured by\ntwo metrics: exact match and stem match. The exact match\nregards the answer to be correct when it is identical to the\nground truth, and the stem match is correct when the stem\nof the answer matches the stem of the ground truth. We used\nSnowball stemmer [63] to compute the stem of each word.\nBoth of the criteria above were case insensitive.\nAlso, to determine the upper bound of the correct ratio\nfor the questions, we also collected answers from 20 native\nEnglish speakers using Amazon Mechanical Turk. The\nconditions for the task were the same as in the qualitative\nexperiments, except for the number of questions. The average\ncorrect ratio for the native speakers was 84.00%, where the\nmaximum was 95.00% and the minimum was 65.00% (these\nresults are also shown in Fig. 6).\nFor each question, participants were asked to answer three\nper-question questionnaires related to the appropriateness of\nthe question (MidQ1—3, Table 3).\n3) POST-QUESTIONNAIRES\nAfter submitting all answers, participants were asked to\nanswer post-questionnaires (PosQ1–3, Table 3) to share their\nimpressions of the prototype. All answers were set to a\nfive-level Likert scale. Finally, an open-ended interview was\nconducted in which participants were asked three questions\nregarding the difficulty of the test and potential new features\nwe could add (IntQ1–Q4, Table 3).\nD. PARTICIPANTS\nThe field study was conducted at a high school in Kanagawa,\nJapan. For this study, 24 high school students in either their\nfirst or second year were recruited. The study was conducted\nfor two days with different participants: 14 and 10 students\non the first and second days, respectively. Also, all students\nwere asked to sign a consent form beforehand.\nE. RESULTS\n1) DAY 1\nOn the first day, 14 students, six in their first year and eight\nin their second year, participated in the study. Among these\n14 students, 11 were qualified as EIKEN Grade Pre-2, which\nis equivalent to the European Framework of Reference for\nLanguages (CEFR) A2 level; two were qualified as EIKEN\nGrade 3, which is equivalent to the CEFR A1 level, and\none was not qualified with any such tests. The results of the\npre-questionnaires (PreQ1 and PreQ2) were 3.21±0.97 and\n2.57±1.16 respectively. Figure. 5 indicates that most of the\nstudents had neutral impressions about English (c.f. PreQ1),\nand the majority self-reported that they were not good at\nEnglish (c.f. PreQ2).\nThe results of the per-question questionnaires\n(MidQ1–3) were, 3.96±1.09, 2.80±1.09, and 3.78±1.06\nrespectively (Fig. 5). The results for correct ratio are\npresented in Fig. 6. The exact match ratio for users was\n11.79±11.20 on average, where the minimum value was\n0.00 and the maximum was 35.00. The stem match ratio,\non the other hand, was 13.21±12.65 on average, where the\nminimum value was 0.00 and the maximum was 45.00.\nFinally, the results of the post-questionnaires (PosQ1–Q3)\nwere 3.43±0.76, 2.86±0.86, and 4.07±0.62, respectively\n(Fig. 5). In the interview IntQ1, most of the students\nreported that the test was too difficult. For example, one\nstudent said ‘‘It was difficult. I could not come up with any\nwords. ’’\n2) DAY 2\nOn the second day, ten students, all of whom were in\ntheir first year, participated in the study. Among these ten\nVOLUME 11, 2023 9843\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFIGURE 5. The box plot chart illustrates the results of questionnaires in the field study. Note that while PreQ1–2 and PreQ1–3 were\ncollected from each student and MidQ1–3 were from each question.\nFIGURE 6. The two box plot charts show the correct ratio of the questions used in the field study. One chart shows the correct ratio\ngrouped by question id, the other by users. The correct ratios for the high school students (day 1, day 2 without a hint, and with a hint)\nand for the native speakers from Amazon Mechanical Turk (MTurk) are presented. And, two ways to judge the match, ‘‘Exact match’’\nand ‘‘stem match’’ , are shown in both plots. ‘‘Exact match’’ regards the answer to be correct when it is identical to the ground truth and\nthe ‘‘stem match’’ is correct when the stem of the answer matches the stem of the ground truth.\nstudents, three were qualified as EIKEN Grade Pre-2 four\nwere qualified as EIKEN Grade 3, and four was not qualified\nwith any such tests.\nOn the basis of feedback obtained on the first day,\nwe decided to ease the difficulty of the test by adding a\nhint to each question (see Fig. 4). The hint was the first\nletter of the ground truth word given on each question when\nparticipants had answered incorrectly on their first try. The\nidea was essential to give them a second chance for each\nquestion in this setting. To determine the effect of this new\nfeature, we added a new question to the interview: ‘‘What did\nyou think about the hints? Were they useful?’’\nThe results for the pre-questionnaires (PreQ1 and PreQ2)\nwere 3.70±0.94 and 2.80±1.14 respectively. Figure. 5\nshows, compared to the first day, most of the students had\na positive attitude toward English (c.f. PreQ1) and their self-\nreported English proficiency was slightly higher (c.f. PreQ2).\nThe results for the per-question questionnaires (MidQ1–3)\nare presented in Fig. 5, where we can see that the averages\nwere 3.91±1.07, 2.49±1.00, and 3.97±0.85, respectively.\nThe results for the correct ratio are presented in Fig. 6. The\nexact match ratio without a hint was 8.00±8.50 on average,\nwhere the minimum value was 0.00 and the maximum was\n25.00, while adding a hint increased the score significantly\n9844 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nto 19.50 on average, where the minimum value was 5.00 and\nthe maximum was 35.00. The stem match ratio without a hint,\non the other hand, was 8.50 on average, where the minimum\nvalue was 0.00 and the maximum was 25.00, while adding a\nhint also increased the score significantly to 23.00 on average,\nwhere the minimum value was 5.00 and the maximum was\n35.00.\nFinally, the results for the post-questionnaires (PosQ1–Q3)\nwere 4.10±0.57, 3.00±1.24, and 3.60±1.07, respectively\n(Fig. 5).\nF. DISCUSSION\n1) IMPRESSIONS FOR OPEN CLOZE QUESTIONS\nAs shown in the Fig. 5, the results for PosQ1–3, which asked\nabout the impression of the application prototype, indicate\nthat, despite the difficulty of the tests, most of the students\nfound that the open cloze questions were interesting (PosQ1)\nand useful for language learning (PosQ3). For example, one\nstudent said in the interview that ‘‘I think this application\nwill actually improve my English if I continue using it. ’’\nAnother said ‘‘Five questions per day will be good. I think\nI can make use of this application while I’m commuting\nto school. ’’ However, the results for PosQ2, which asked\nabout the eagerness to use the application, were below\naverage overall. The interview responses indicated that the\ndifficulty of the questions and the lack of auxiliary functions\nto support learning were the leading causes of this result\n(see subsections VI-F2 and VI-F3). Although the application\nwe used in the study was only a prototype of CLOZER,\nlimited as such to its basic functions, this finding indicates\nthat the application will need some improvements before it is\nready for actual use. This is explored in greater detail in the\nfollowing subsections.\n2) DIFFICULTY OF THE QUESTIONS\nAs indicated by the overall correct ratio (Fig. 5) and by\nthe responses to MidQ1 that asked about the difficulty of\nthe question (Fig. 5), most of the questions were considered\ndifficult by the students in this study. While most of the\nstudents were satisfied with the presented answer (c.f. Fig. 5\nMidQ3), some of them were not convinced that it was the only\ncorrect answer. This is presumably because, since we did not\nshow the explanations for each question, students may not\nhave realized how suitable the true answer was.\nMost of the students were unfamiliar with open cloze tests.\nFor example, one student said in the interview (Int1), ‘‘I\nusually do multiple choice questions. It was difficult without\nchoices since I could not use the elimination method. ’’\nThe responses to Int1 revealed there were four main types\nof difficulty:\nD1 Words were unfamiliar overall.\nD2 Grammar was unfamiliar.\nD3 Words were familiar but the student failed to grasp the\ncontext.\nD4 Words were familiar and the context was understood, but\nthe student failed to come up with a word.\nNot many students reported that words used for both\nquestions and answers were unfamiliar (D1). Although we\ntried to avoid such a problem by restricting words (excluding\nproper nouns) to those on the vocabulary list and allowing\nstudents to use a dictionary during the test, students still found\nthe words difficult. Most of them said that while dictionaries\nwere helpful, they still needed to determine the meaning of\na word from among several possible interpretations, which\nwas a hurdle for some. The inclusion of proper nouns was\nalso tricky, as these are unlikely to be in a dictionary unless\nthey are famous.\nA few students reported that the grammar was difficult\n(D2). For example, one student said ‘‘Usually when I do\nquestions I know the grammar knowledge that is being tested.\nIn this test, I was confused because there were no such\ninstructions. ’’Most of the students reported that the words\nwere familiar, but they struggled to grasp the context (D3).\nFor example, one student said ‘‘I was able to guess the parts\nof speech for the blank, but I could not find the connection\nbetween the sentence before and after the blank. ’’ Another\nstudent said ‘‘I know all the words, but when they come\ntogether, I wasn’t able to understand what the sentence was\nsaying. ’’Similarly, most of the students reported that, even\nwhen both words and context were understandable, they\noften failed to come up with a word for the blank (D4). For\nexample, one student said ‘‘I generally understood the words,\nbut mostly I could not come up with a word for the blank. ’’ As\nfor the hint feature we added on day 2, most of the students\nfound this helpful, but some said it was not enough.\n3) DESIRABLE FUNCTIONS\nIn the interview, on day 1, most of the students asked for hints,\nsuch as showing the number of letters of the correct word,\nshowing which Japanese words would fit, and describing\nthe proper nouns. Additionally, with regard to the difficulty,\nstudents asked for a function to change the difficulty level.\nFor example, one student said ‘‘I am not good at English,\nso it would be better if the app could change the difficulty. ’’,\nand another said ‘‘For the easier level, it would be better\nto have choices for the answer. ’’ Another common request\nwas to show the explanation of the question after submitting\nan answer, such as showing the Japanese translation of\nthe question and related grammatical knowledge. Some\nstudents said ‘‘I wanted to know why my answer was wrong.\nIt would be helpful to show the Japanese translation and\nrelated grammatical knowledge. ’’ Others said that it would\nbe beneficial to present similar questions so as to gain more\nsubstantial knowledge of the usage of the target word.\n4) HOW TO EASE THE DIFFICULTY?\nOur findings revealed that, even when students had similar\nlevels of English ability (c.f. EIKEN qualifications in sub-\nsection VI-E1 and VI-E2), their levels of understanding were\noften different (c.f. D1–4 in subsection VI-F2). Therefore,\ndesigning hints in accordance with the level of understanding\nis necessary.\nVOLUME 11, 2023 9845\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFor example, implementing a translation function that\nshows the meaning of each word will be useful for those\nwho are less skilled at vocabulary. With this feature, students\nwill not need to guess from among the possible word\ninterpretations in a dictionary or consider proper nouns not\nincluded in ordinary dictionaries.\nThe grammatical features of each word should also support\nthe reading of a sentence, since L2 English learners are\ngenerally dependent on grammar rules, unlike native speakers\nwho operate such rules unconsciously. Students often try\nto understand the meaning of a sentence by using their\ngrammatical knowledge, but extracting such information can\nbe difficult for some of them (c.f. D2, subsection VI-F2).\nOne solution would be to show a part of speech and\nthe dependency of each word. The part of speech (noun,\nverb, adjective, adverb, pronoun, preposition, conjunction,\ninterjection, numeral, article, and determiner) indicates how\nthe word functions in terms of both meaning and grammar\nwithin the sentence, while the dependency shows which\nword is related to another word as well as the type of the\nrelationship between them. With such information, users will\nbe able to use their grammatical knowledge to understand the\nmeaning of a sentence. Moreover, as a clue to the answer,\nshowing the inflected form and the part of speech of the target\nword will help users to determine the grammatical aspects of\nthe answer. For example, if users know that the answer is a\nverb and in the past tense, they can concentrate on finding\nverbs in the past tense. Most of such grammar information\ncan be extracted using a dependency parser; specifically, the\nparts of speech can be obtained from POS tags, and the\ndependencies can be obtained from a dependency tree.\nIn the test, most of the students struggled to grasp the\ncontext of a sentence (c.f. D3, subsection VI-F2). This can\nbe improved by adding some extra sentences before or after\nthe target sentence. We only used a single sentence for each\nquestion in this study, and although the native speakers on\nMTurk managed to interpret contexts, one sentence was too\nshort for L2 learners. Adding extra sentences will help them\nunderstand in what context the sentences were written. Show-\ning the translation for such sentences will also be helpful.\nLastly, for those who understood the context but\nfound it difficult to come up with the answer (c.f. D4,\nsubsection VI-F2), it will be helpful to add additional\ninformation to the target word: for example, to show the\nfirst letter. As we found on day 2 of the field study, this\nhint was effective for most students. We could also increase\nthe number of letters shown to ease the difficulty further.\nSimilarly, showing other types of information about the target\nword, such as the number of letters, would also help decrease\nthe difficulty.\nVII. LIMITATION AND FUTURE WORKS\nA. THE BENEFIT OF CLOZER FOR EDUCATION AND\nLANGUAGE LEARNING\nAs demonstrated in section V, CLOZER could generate\nhigher-quality OCQ than average non-native English human\nteachers. Also, since it took 19.13 ±8.04 minutes on average\nfor the teachers to create five questions in the quantitative\nexperiment, the result suggests that CLOZER can contribute\nto relieving teachers of the burden of producing OCQ.\nHowever, since the field study was conducted in terms of\nstudents, we should also investigate the application’s usability\nfor teachers towards the actual deployment of the application\non the educational front in future work. Also, CLOZER\ncan provide a self-study environment for English learners.\nIn particular, learners can study English with questions\nrelated to the words they want to learn using CLOZER. The\neffect of the learning tailor-made for learners needs to be\nfurther investigated.\nMoreover, because we aimed to develop an algorithm\nto generate sufficient quality OCQ and present the initial\ndesign of the CLOZER application in this study, we did not\nyet investigate whether it is actually effective for language\nlearning. To this end, a long-term field study to evaluate the\nimprovement in students’ English skills will be required for\nfuture work.\nB. THE APPLICATION OF THE GAP SCORE\nTo measure the answer uniqueness of the question, we intro-\nduced a new metric called a gap score. From the results of the\npreliminary experiment, the gap score was highly correlated\nwith the correct ratio of native speakers who participated in\nour experiments, and the best correlation coefficient which\nroberta-base performed was 0.80. Although we only used the\ngap score to select sufficient quality questions in this paper,\nwe can extend the use of the score beyond generating OCQ.\nFor example, we could use it as a criterion for selecting an\nappropriate close test format, such as selecting MCQs when\nthe score of the question is low because MCQ give more hints.\nThus, we should investigate the characteristic of the gap score\nand explore the possibility of other uses in future work.\nIn the field study, the overall correct ratio (Fig. 6) of the\ntest was much lower than that of native speakers, and most of\nthe students reported that the test was too difficult. Although\nwe restricted the target words and the target sentences to\nthe words which are familiar to high school students, there\nwas a sufficient difference between native speakers and them.\nThus, there is a possibility that MLM could grasp the native\nspeakers’ sense of words, and MLM can be useful for the\nfield of language education related to not only primitive skill\nsuch as vocabulary but also other advanced language skills.\nWe should deeply investigate the reason why the questions\ngenerated by CLOZER are too difficult for L2 learners but\neasy for native speakers.\nMoreover, we have to insist that the characteristic of MLM\nenables CLOZER to generate different types of cloze tests.\nIn general, since the cloze test was used to measure primitive\nskills such as vocabulary and grammar, an idiom is often\nselected as the target of OCQ. However, CLOZER could also\ngenerate questions on not only idioms but also other types\nthat human is hard to prepare. For example, the question,\n‘‘If you don’t, I’ll arrest you and you’ll spend the night in\n9846 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nTABLE 4. The table shows the samples of open cloze questions generated by CLOZER in the preliminary experiment. And, the correct ratio and gap score\nof the questions are also shown in the table. The target word of each question is in the bracket, and it is blanked when the question appears.\n( ).’’, was generated with a high gap score in the preliminary\nexperiment. The target word for this question was jail. This\nquestion needs to understand not only the adjoining words of\nthe blank but also the former content, arrest, to estimate the\ntarget word. The CLOZER’s capability imposes the issue of\ndeeply investigating the characteristic of questions generated\nby CLOZER as future work.\nAdditionally, our prototype test only examined CLOZER\non English learning. However, our framework is theoretically\nnot limited to a single language, as the masked language\nmodels and the gap score also apply to other languages,\nand various pre-trained models are available. Extensive\nexperiments on other languages will be desirable in future\nwork.\nC. THE EFFECT OF THE LANGUAGE MODEL\nCLOZER employed the masked language model in the\narchitecture. Since this model is trained on a large corpus in\na data-driven manner, it shows a better prediction if the input\nsentence is similar to the corpus, while in contrast, it tends to\ndegrade the performance when the input sentence is outside\nthe distribution of the corpus. How the corpus will affect\nthe results is still unclear, and we leave this to future study.\nAdditionally, as reported in our qualitative study, the model\ncan be overconfident when the answer is not so certain. This\nmostly stems from the fact that the confidence values do not\nstrictly show the prediction’s certainty due to the loss function\nsettings in the learning. However, in most cases, we found the\nconfidence values helpful; moreover, improving the language\nmodels was outside the scope of the current study.\nVIII. CONCLUSION\nIn this work, we presented CLOZER, an automatic open cloze\nquestion generator. The results of quantitative experiments\nstatistically demonstrated that it can generate open cloze\nquestions having the answer uniqueness, which is to accept\nthe ground truth answer only. And, it was indicated that\nCLOZER can generate OCQs better than the ones the average\nnon-native English teacher made. Furthermore, the field\nstudy at a local high school revealed the possible benefits of\nFIGURE 7. The bar plot illustrates the gap score distribution across the\ndifferent genres in CoCA corpus: academic, blog, fiction, magazine, and\nnews.\nCLOZER. Finally, on the basis of our findings, we proposed\nseveral design improvements. Future research will focus on\nimproving the functionality of the application and introducing\nCLOZER into educational settings to verify its effectiveness\nover the long term.\nAPPENDIX A\nADDITIONAL GENERATED OCQ SAMPLES\nTable 4 shows additional samples of questions generated by\nCLOZER in the preliminary experiment and the correct ratio\nand gap score of each question. Consider the actual case\nof using CLOZER, 10 questions were extracted from the\nquestions whose gap scores are higher than 0.80.\nAPPENDIX B\nCORPUS COMPARISON\nTo investigate the impact of using different input data sources,\nwe conducted additional comparative experiments using five\ndifferent genres in the CoCA corpus: academic, blog, fiction,\nmagazine, and news. Forty target words (as in the quantitative\nexperiment) were used to generate 264k questions in total\nwith roberta-base. Figure 7 shows the distribution of the gap\nscore for each genre. As we can see, the overall distribution\nwas quite similar across all genres. However, there was a\nslight difference in each bin. The density for scores 0.75–\nVOLUME 11, 2023 9847\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\nFIGURE 8. Comparison of correlation plots between correct ratio and gap score among 8 MLM architectures: bert-base,\nbert-large, distilbert-base, roberta-base, roberta-large, albert-base, albert-large, and albert-xlarge.\n1.00 was 0.12, 0.18, 0.15, 0.19, and 0.19 for the five\nrespective genres. These scores indicate that the model was\nless capable of generating single-answer questions for the\nacademic genre. This is presumably because there were less\nfrequent words in academic texts compared to other genres’\ntexts. Generally, MLMs capture correlations across the words\nin a dataset in a data-driven manner. Therefore, the model has\nlittle knowledge about the less frequent words, and predicting\na masked target with such unfamiliar words may degrade the\nperformance. Additionally, if words are too infrequent, the\ntokens are substituted with an [UNK] token, and this will also\naffect the predictability.\nAPPENDIX C\nADDITIONAL LANGUAGE MODEL COMPARISON\nThe full results of the correlation plot are presented in\nFig. 8. In the highly correlated models, that is, the BERT\nand RoBERTa variants, a higher gap score indicated a higher\ncorrect ratio. However, the models with lower correlation,\nthe DistilBERT and ALBERT variants, tended to under-rate\nsamples with a high correct ratio.\nWe had expected roberta-large to perform better, but its\ncorrelation coefficient was lower than that of roberta-base.\nWe found that, compared to roberta-base, roberta-large had\na greater number of samples whose gap scores were high\nbut correct ratios were low. This means that roberta-large\nmanaged to predict the target words better than humans.\nAPPENDIX D\nETHICAL CONSIDERATIONS\nThere has been quite a lot of work exploring potential bias\nin language models (e.g., [64]). Since CLOZER utilizes a\nlanguage model, there is a concern that some questions may\ncontain biased word usage. Several studies have proposed\nmethods to mitigating bias [65], [66] and application of\nsuch procedures is necessary for practical use. Additionally,\nteachers should be made aware of such risks so that they can\ndevise ways to avoid selecting biased questions during the\nselection process.\nREFERENCES\n[1] W. L. Taylor, ‘‘Cloze procedure’’: A new tool for measuring readability,’’\nJ. Mass Commun. Quart., vol. 30, no. 4, pp. 415–433, 1953.\n[2] P. M. Raymond, ‘‘Cloze procedure in the teaching of reading,’’ TESL\nCanada J., vol. 6, no. 1, pp. 91–97, 1988.\n[3] A. Mizumoto, M. Ikeda, and O. Takeuchi, ‘‘A comparison of cognitive\nprocessing during cloze and multiple-choice reading tests using brain\nactivation,’’Annu. Rev. English Lang. Educ. Jpn., vol. 27, pp. 65–80, 2016.\n[4] W. L. Taylor, ‘‘Cloze’’ readability scores as indices of individual\ndifferences in comprehension and aptitude,’’ J. Appl. Psychol., vol. 41,\nno. 1, pp. 19–26, Feb. 1957.\n9848 VOLUME 11, 2023\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\n[5] J. R. Bormuth, ‘‘Comparable cloze and multiple-choice comprehension\ntest scores,’’ J. Reading, vol. 10, no. 5, pp. 291–299, 1967.\n[6] J. W. Oller, Language Tests At School. White Plains, NY , USA: Longman,\n1979.\n[7] J. Pino, M. Heilman, and M. Eskenazi, ‘‘A selection strategy to improve\ncloze question quality,’’ inProc. Workshop Intell. Tutoring Syst. Ill-Defined\nDomains. 9th Int. Conf. Intell. Tutoring Syst., Montreal, QC, Canada:\nSpringer, 2008, pp. 22–32.\n[8] E. Sumita, F. Sugaya, and S. Yamamoto, ‘‘Measuring non-native speakers’\nproficiency of English by using a test with automatically-generated fill-in-\nthe-blank questions,’’ in Proc. 2nd Workshop Building Educ. Appl. Using\nNLP (EdAppsNLP), Ann Arbor, MI, USA, 2005, pp. 61–68.\n[9] R. Mitkov, ‘‘Computer-aided generation of multiple-choice tests,’’ in\nProc. HLT-NAACL Workshop Building Educ. Appl. Using Natural Lang.\nProcess., 2003, pp. 17–22.\n[10] J. C. Brown, G. A. Frishkoff, and M. Eskenazi, ‘‘Automatic question\ngeneration for vocabulary assessment,’’ in Proc. Conf. Hum. Lang.\nTechnol. Empirical Methods Natural Lang. Process. (HLT), Vancouver,\nBC, Canada, 2005, pp. 819–826.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,’’\nin Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum.\nLang. Technol., Minneapolis, MN, USA: Association for Computational\nLinguistics, 2019, p. 4171–4186.\n[12] A. Qayyum and O. Zawacki-Richter, ‘‘Distance education in Australia,\nEurope and the Americas,’’ Open Distance Educ. Aust., Eur. Americas,\npp. 121–131, Jun. 2018.\n[13] M. Gaebel, MOOCs: Massive Open Online Courses, vol. 11, Geneva,\nSwitzerland: EUA, 2014.\n[14] K. F. Hew and W. S. Cheung, ‘‘Students’ and instructors’ use of massive\nopen online courses (MOOCs): Motivations and challenges,’’ Educ. Res.\nRev., vol. 12, pp. 45–58, Jun. 2014.\n[15] A. T. Bates, Technology, E-Learning and Distance Education. Evanston,\nIL, USA: Routledge, 2005.\n[16] M. Levy, CALL: Context and Conceptualisation. New York, NY , USA:\nOxford Univ. Press, 1996.\n[17] R. Wita, S. Oly, S. Choomok, T. Treeratsakulchai, and S. Wita, ‘‘A semantic\ngraph-based Japanese vocabulary learning game,’’ in Proc. Int. Conf. Web-\nBased Learn. Cham, Switzerland: Springer, 2018, pp. 140–145.\n[18] A. Killawala, I. Khokhlov, and L. Reznik, ‘‘Computational intelligence\nframework for automatic quiz question generation,’’ in Proc. IEEE Int.\nConf. Fuzzy Syst. (FUZZ-IEEE), Jul. 2018, pp. 1–8.\n[19] N. Afzal and R. Mitkov, ‘‘Automatic generation of multiple choice\nquestions using dependency-based semantic relations,’’ Soft Comput.,\nvol. 18, no. 7, pp. 1269–1281, Jul. 2014.\n[20] L. Zavala and B. Mendoza, ‘‘On the use of semantic-based AIG to\nautomatically generate programming exercises,’’ in Proc. 49th ACM Tech.\nSymp. Comput. Sci. Educ., Feb. 2018, pp. 14–19.\n[21] K. Koffka, Principles of Gestalt Psychology. Evanston, IL, USA:\nRoutledge, 2013.\n[22] M. M. Friedman, The Use of the Cloze Procedure for Improving the\nReading Comprehension of Foreign Students at the University of Florida.\nAnn Arbor, MI, USA: University Microfilms, 1974.\n[23] J. Anderson, ‘‘The application of cloze procedure to English learned as\na foreign language in Papua and new Guinea,’’ ELT J., vol. 27, no. 1,\npp. 66–72, 1972.\n[24] C. Klein-Braley, ‘‘A cloze-up on the C-test: A study in the construct\nvalidation of authentic tests,’’ Lang. Test., vol. 2, no. 1, pp. 76–104,\nJun. 1985.\n[25] L. F. Bachman, ‘‘Performance on cloze tests with fixed-ratio and rational\ndeletions,’’Tesol Quart., vol. 19, no. 3, pp. 535–556, 1985.\n[26] M. Kobayashi, ‘‘Method effects on reading comprehension test perfor-\nmance: Text organization and response format,’’ Lang. Test., vol. 19, no. 2,\npp. 193–220, 2002.\n[27] C. J. Weir, ‘‘Communicative language testing with special reference to\nEnglish as a foreign language,’’ Exeter Linguistic Stud., vol. 11, pp. 1–241,\n1988.\n[28] R. Lado, Language Testing: The Construction and Use of Foreign\nLanguage Tests. A Teacher’s Book. Education Resources Information\nCenter, 1961.\n[29] J. Lee and S. Seneff, ‘‘Automatic generation of cloze items for\nprepositions,’’ in Proc. 8th Annu. Conf. Int. Speech Commun. Assoc.,\nAntwerp, Belgium, 2007, pp. 2133–2136.\n[30] Y .-C. Lin, L.-C. Sung, and M. C. Chen, ‘‘An automatic multiple-choice\nquestion generation scheme for English adjective understanding,’’ in Proc.\nWorkshop Model., Manage. Gener. Problems/Questions eLearning, 15th\nInt. Conf. Comput. Educ. (ICCE). Hiroshima, Japan: APSCE, 2007,\npp. 137–142.\n[31] K. Sakaguchi, Y . Arase, and M. Komachi, ‘‘Discriminative approach to fill-\nin-the-blank quiz generation for language learners,’’ in Proc. 51st Annu.\nMeeting Assoc. Comput. Linguistics, Sofia, Bulgaria, 2013, pp. 238–242.\n[32] T. Goto, T. Kojiri, T. Watanabe, T. Iwata, and T. Yamada, ‘‘Automatic\ngeneration system of multiple-choice cloze questions and its evaluation,’’\nKnowl. Manage. e-Learn., An Int. J., vol. 2, no. 3, pp. 210–224, 2010.\n[33] G. Kurdi, J. Leo, B. Parsia, U. Sattler, and S. Al-Emari, ‘‘A systematic\nreview of automatic question generation for educational purposes,’’ Int. J.\nArtif. Intell. Educ., vol. 30, no. 1, pp. 121–204, Mar. 2020.\n[34] S. Jiang and J. S. Lee, ‘‘Distractor generation for Chinese fill-in-the-blank\nitems,’’ in Proc. 12th Workshop Innov. Use NLP Building Educ. Appl.,\n2017, pp. 143–148.\n[35] G. Kumar, R. Banchs, and L. F. D’Haro, ‘‘RevUP: Automatic gap-fill\nquestion generation from educational texts,’’ in Proc. 10th Workshop Innov.\nUse NLP Building Educ. Appl., 2015, pp. 154–161.\n[36] A. Y . Satria and T. Tokunaga, ‘‘Automatic generation of English reference\nquestion by utilising nonrestrictive relative clause,’’ in Proc. 9th Int. Conf.\nComput. Supported Educ., 2017, pp. 379–386.\n[37] M. Majumder and S. K. Saha, ‘‘A system for generating multiple choice\nquestions: With a novel approach for sentence selection,’’ in Proc. 2nd\nWorkshop Natural Lang. Process. Techn. Educ. Appl., Beijing, China,\n2015, pp. 64–72.\n[38] J. D. Lafferty, A. McCallum, and F. C. N. Pereira, ‘‘Conditional random\nfields: Probabilistic models for segmenting and labeling sequence data,’’\nin Proc. 18th Int. Conf. Machine Learn. San Francisco, CA, USA: Morgan\nKaufmann, 2001, pp. 282–289.\n[39] L. Becker, S. Basu, and L. Vanderwende, ‘‘Mind the gap: Learning to\nchoose gaps for question generation,’’ in Proc. Conf. North Amer. Chapter\nAssoc. Comput. Linguistics Hum. Lang. Technol. Montreal, QC, Canada,\n2012, pp. 742–751.\n[40] A. Malafeev, ‘‘Language exercise generation: Emulating Cambridge open\ncloze,’’ Int. J. Conceptual Struct. Smart Appl., vol. 2, no. 2, pp. 20–35,\nJul. 2014.\n[41] M. Felice and P. Buttery, ‘‘Entropy as a proxy for gap complexity in open\ncloze tests,’’ in Proc. Int. Conf. Recent Adv. Natural Lang. Process., Varna,\nBulgaria, 2019, pp. 323–327.\n[42] C. E. Shannon, ‘‘A mathematical theory of communication,’’ Bell Syst.\nTech. J., vol. 27, no. 3, pp. 379–423, Jul./Oct. 1948.\n[43] E. Marrese-Taylor, A. Nakajima, Y . Matsuo, and O. Yuichi, ‘‘Learning to\nautomatically generate fill-in-the-blank quizzes,’’ in Proc. 5th Workshop\nNatural Lang. Process. Techn. Educ. Appl., Melbourne, VIC, Australia,\n2018, pp. 152–156.\n[44] D. Amodei et al., ‘‘Deep speech 2: End-to-end speech recognition\nin English and Mandarin,’’ in Proc. Int. Conf. Mach. Learn., 2016,\npp. 173–182.\n[45] Y . Wu et al., ‘‘Google’s neural machine translation system: Bridging the\ngap between human and machine translation,’’ 2016, arXiv:1609.08144.\n[46] A. M. Rush, S. Chopra, and J. Weston, ‘‘A neural attention model for\nabstractive sentence summarization,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2015, pp. 1–11.\n[47] T. B. Brown et al., ‘‘Language models are few-shot learners,’’ 2020,\narXiv:2005.14165.\n[48] J. Howard and S. Ruder, ‘‘Universal language model fine-tuning for text\nclassification,’’ in Proc. 56th Annu. Meeting Assoc. Comput. Linguistics,\n2018, pp. 328–339.\n[49] T. Dunning, ‘‘Statistical identification of language,’’ Comput. Res. Lab.,\nNew Mexico State Univ., Las Cruces, NM, USA, Tech. Rep., 1994.\n[50] M. Sundermeyer, R. Schlüter, and H. Ney, ‘‘LSTM neural networks for\nlanguage modeling,’’ in Proc. 13th Annu. Conf. Int. Speech Commun.\nAssoc., 2012, pp. 1–4.\n[51] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[52] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio, ‘‘Learning phrase representations using\nRNN encoder–decoder for statistical machine translation,’’ 2014,\narXiv:1406.1078.\nVOLUME 11, 2023 9849\nS. Matsumori et al.: Mask and Cloze: Automatic OCQ Generation Using a Masked Language Model\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Conf.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[54] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, ‘‘Pre-trained\nmodels for natural language processing: A survey,’’ Sci. China Technol.\nSci., vol. 63, no. 10, pp. 1–26, 2020.\n[55] C. Giugni, Variabilità e Mutabilità: Contributo Allo Studio Delle\nDistribuzioni e Delle Relazioni Statistiche. Cuppini, 1912.\n[56] R. Dorfman, ‘‘A formula for the Gini coefficient,’’ Rev. Econ. Statist.,\nvol. 61, no. 1, pp. 146–149, 1979.\n[57] M. O. Lorenz, ‘‘Methods of measuring the concentration of wealth,’’ Pub.\nAmer. Statist. Assoc., vol. 9, no. 70, pp. 209–219, 1905.\n[58] M. Davies, ‘‘The 385+ million word corpus of contemporary American\nEnglish (1990–2008+): Design, architecture, and linguistic insights,’’ Int.\nJ. Corpus Linguistics, vol. 14, no. 2, pp. 159–190, Jun. 2009.\n[59] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a\ndistilled version of BERT: Smaller, faster, cheaper and lighter,’’ 2019,\narXiv:1910.01108.\n[60] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[61] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R.\nSoricut, ‘‘ALBERT: A lite bert for self-supervised learning of language\nrepresentations,’’ in Proc. Int. Conf. Learn. Represent., 2019, pp. 1–17.\n[62] T. Wolf et al., ‘‘HuggingFace’s transformers: State-of-the-art natural\nlanguage processing,’’ 2019, arXiv:1910.03771.\n[63] M. F. Porter, ‘‘Snowball: A language for stemming algorithms,’’\nTech. Rep., 2001.\n[64] K. Kurita, N. Vyas, A. Pareek, A. W. Black, and Y . Tsvetkov, ‘‘Measuring\nbias in contextualized word representations,’’ 2019, arXiv:1906.07337.\n[65] R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. V osoughi, ‘‘Mitigating\npolitical bias in language models through reinforced calibration,’’ in\nProc. 35th Assoc. Advancement Artif. Intell. , vol. 35, no. 17, 2021,\npp. 14857–14866.\n[66] P.-S. Huang, H. Zhang, R. Jiang, R. Stanforth, J. Welbl, J. Rae, V . Maini,\nD. Yogatama, and P. Kohli, ‘‘Reducing sentiment bias in language models\nvia counterfactual evaluation,’’ in Proc. Conf. Empirical Methods Natural\nLang. Process., 2020, pp. 65–83.\nSHOYA MATSUMORI was born in Tokyo,\nJapan, in 1994. He received the B.Eng., M.Eng.,\nand Ph.D. degrees from Keio University, Japan,\nin 2018, 2020, and 2022, respectively.\nFrom 2019 to 2020, he was the CEO of\nBLUEM Inc., Japan. From 2018 to 2022, he was\na Project Researcher at the Keio Leading-Edge\nLaboratory of Science and Technology (KLL).\nFrom 2021 to 2022, he was a Research Fellow at\nthe Japan Society for the Promotion of Science.\nSince 2022, he has been the CEO of Carnot Inc., Japan. His research interests\ninclude vision and language, information sharing, and machine theory of\nmind.\nKOHEI OKUOKA received the M.E. degree from\nKeio University, in 2020, where he is currently\npursuing the Ph.D. degree with the Graduate\nSchool of Science and Technology.\nHe founded BLUEM Inc., as an Executive\nOfficer, in 2019. From 2020 to 2021, he was a\nProject Researcher with the Keio Leading-Edge\nLaboratory of Science and Technology (KLL).\nSince 2021, he has been an Assistant Professor\nat the Faculty of Science and Technology, Keio\nUniversity. His research interests include autonomous robots, human-agent\ninteraction, and cognitive science. He is also a member of the Japanese\nCognitive Society.\nRYOICHI SHIBATA received the B.Eng. degree in\ncomputer science from Keio University, in 2021,\nwhere he is currently pursuing the master’s\ndegree with the Graduate School of Science and\nTechnology. His research interest includes human-\nagent interaction.\nMINAMI INOUE received the B.Eng. degree in\ncomputer science from Keio University, in 2022,\nwhere she is currently pursuing the master’s\ndegree with the Graduate School of Science and\nTechnology.\nHer research interests include human–robot\ninteraction and human-agent interaction.\nYOSUKE FUKUCHI was born in Japan, in 1994.\nHe received the B.E. and M.E. degrees in computer\nscience from Keio University, Yokohama, Japan,\nin 2017 and 2019, respectively.\nFrom 2019 to 2021, he was an Assistant Profes-\nsor at Keio University. From 2021 to 2022, he was\na Project Researcher with the Keio Leading-Edge\nLaboratory of Science and Technology (KLL).\nHe is currently a Project Researcher at the\nNational Institute of Informatics, Tokyo, Japan.\nHis research interests include human-agent interaction, artificial intelligence,\nand the theory of mind. He is a member of the Japanese Society for Artificial\nIntelligence.\nMICHITA IMAI (Member, IEEE) received the\nPh.D. degree in computer science from Keio\nUniversity, in 2002.\nIn 1994, he joined NTT Human Interface Lab-\noratories. He joined ATR Media Integration and\nCommunications Research Laboratories, in 1997.\nHe was a Visiting Scholar at the University of\nChicago, from 2009 to 2010. He is currently a\nProfessor at the Faculty of Science and Technol-\nogy, Keio University, and a Researcher at ATR\nIntelligent Robot Laboratories. His research interests include autonomous\nrobots, human–robot interaction, speech dialogue systems, humanoids,\nand spontaneous behaviors. He is a member of the Information and\nCommunication Engineers Japan (IEICE-J), the Information Processing\nSociety of Japan, the Japanese Cognitive Science Society, the Japanese\nSociety for Artificial Intelligence, the Human Interface Society, and ACM.\n9850 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7992702722549438
    },
    {
      "name": "Cloze test",
      "score": 0.7422504425048828
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5171353220939636
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5045112371444702
    },
    {
      "name": "Natural language processing",
      "score": 0.5034782290458679
    },
    {
      "name": "Test (biology)",
      "score": 0.454881489276886
    },
    {
      "name": "Blank",
      "score": 0.4390200674533844
    },
    {
      "name": "Usability",
      "score": 0.42299801111221313
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4185388684272766
    },
    {
      "name": "Reading (process)",
      "score": 0.22326025366783142
    },
    {
      "name": "Linguistics",
      "score": 0.21501752734184265
    },
    {
      "name": "Human–computer interaction",
      "score": 0.18094098567962646
    },
    {
      "name": "Reading comprehension",
      "score": 0.1613936722278595
    },
    {
      "name": "Mathematics",
      "score": 0.08653831481933594
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}