{
  "title": "End-to-end Named Entity Recognition and Relation Extraction using Pre-trained Language Models",
  "url": "https://openalex.org/W2997712488",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223695330",
      "name": "Giorgi, John",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1246511787",
      "name": "Wang Xindi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224752817",
      "name": "Sahar, Nicola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748451273",
      "name": "Shin Won Young",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2756135578",
      "name": "Bader Gary D",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A564959170",
      "name": "Wang Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888597155",
    "https://openalex.org/W2600659824",
    "https://openalex.org/W2907265599",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2251091211",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2109555487",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2579356637",
    "https://openalex.org/W2759056771",
    "https://openalex.org/W1941318214",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2229639163",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2041073071",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W2169232658",
    "https://openalex.org/W2946628371",
    "https://openalex.org/W2019772739",
    "https://openalex.org/W1566346388",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W2963602416",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2578454709",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W2949202705",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2610332124"
  ],
  "abstract": "Named entity recognition (NER) and relation extraction (RE) are two important tasks in information extraction and retrieval (IE \\&amp; IR). Recent work has demonstrated that it is beneficial to learn these tasks jointly, which avoids the propagation of error inherent in pipeline-based systems and improves performance. However, state-of-the-art joint models typically rely on external natural language processing (NLP) tools, such as dependency parsers, limiting their usefulness to domains (e.g. news) where those tools perform well. The few neural, end-to-end models that have been proposed are trained almost completely from scratch. In this paper, we propose a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. Because the bulk of our model's parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train. On 5 datasets across 3 domains, our model matches or exceeds state-of-the-art performance, sometimes by a large margin.",
  "full_text": "END-TO-END NAMED ENTITY RECOGNITION AND RE -\nLATION EXTRACTION USING PRE -TRAINED LANGUAGE\nMODELS\nJohn M. Giorgi\nUniversity of Toronto\nDepartment of Computer Science\nThe Donnelly Centre\nVector Institute\nToronto, ON M5G 1M1, Canada\njohn.giorgi@utoronto.ca\nXindi Wang\nUniversity Health Network\nToronto, ON M5G 2N2, Canada\nxindi.wang@uhnresearch.ca\nNicola Sahar\nSemantic Health\nToronto, ON M5H2R2, Canada\nnick@semantichealth.ai\nWon Young Shin\nUniversity of Toronto\nDepartment of Electrical & Computer Engineering\nToronto, ON M5S, Canada\nwonyoung.shin@mail.utoronto.ca\nGary D. Bader\nUniversity of Toronto\nDepartment of Computer Science\nThe Donnelly Centre\nToronto, ON M5S 3E1, Canada\ngary.bader@utoronto.ca\nBo Wang\nPeter Munk Cardiac Center, University Health Network\nVector Institute\nToronto, ON M5G 1M1, Canada\nbowang@vectorinstitute.ai\nABSTRACT\nNamed entity recognition (NER) and relation extraction (RE) are two important\ntasks in information extraction and retrieval (IE & IR). Recent work has demon-\nstrated that it is beneﬁcial to learn these tasks jointly, which avoids the propagation\nof error inherent in pipeline-based systems and improves performance. However,\nstate-of-the-art joint models typically rely on external natural language process-\ning (NLP) tools, such as dependency parsers, limiting their usefulness to domains\n(e.g. news) where those tools perform well. The few neural, end-to-end mod-\nels that have been proposed are trained almost completely from scratch. In this\npaper, we propose a neural, end-to-end model for jointly extracting entities and\ntheir relations which does not rely on external NLP tools and which integrates a\nlarge, pre-trained language model. Because the bulk of our model’s parameters are\npre-trained and we eschew recurrence for self-attention, our model is fast to train.\nOn 5 datasets across 3 domains, our model matches or exceeds state-of-the-art\nperformance, sometimes by a large margin.\n1 I NTRODUCTION\nThe extraction of named entities (named entity recognition, NER) and their semantic relations (rela-\ntion extraction, RE) are key tasks in information extraction and retrieval (IE & IR). Given a sequence\nof text (usually a sentence), the objective is to identify both the named entities and the relations\nbetween them. This information is useful in a variety of NLP tasks such as question answering,\nknowledge base population, and semantic search (Jiang, 2012). In the biomedical domain, NER\nand RE facilitate large-scale biomedical data analysis, such as network biology (Zhou et al., 2014),\ngene prioritization (Aerts et al., 2006), drug repositioning (Wang & Zhang, 2013) and the creation of\ncurated databases (Li et al., 2015). In the clinical domain, NER and RE can aid in disease and treat-\n1\narXiv:1912.13415v1  [cs.CL]  20 Dec 2019\nment prediction, readmission prediction, de-identiﬁcation, and patient cohort identiﬁcation (Miotto\net al., 2017).\nMost commonly, the tasks of NER and RE are approached as a pipeline, with NER preceding RE.\nThere are two main drawbacks to this approach: (1) Pipeline systems are prone to error propagation\nbetween the NER and RE systems. (2) One task is not able to exploit useful information from\nthe other (e.g. the type of relation identiﬁed by the RE system may be useful to the NER system\nfor determining the type of entities involved in the relation, and vice versa). More recently, joint\nmodels that simultaneously learn to extract entities and relations have been proposed, alleviating the\naforementioned issues and achieving state-of-the-art performance (Miwa & Sasaki, 2014; Miwa &\nBansal, 2016; Gupta et al., 2016; Li et al., 2016; 2017; Zhang et al., 2017; Adel & Sch ¨utze, 2017;\nBekoulis et al., 2018a;b; Nguyen & Verspoor, 2019; Li et al., 2019).\nMany of the proposed joint models for entity and relation extraction rely heavily on external natural\nlanguage processing (NLP) tools such as dependency parsers. For instance, Miwa & Bansal (2016)\npropose a recurrent neural network (RNN)-based joint model that uses a bidirectional long-short\nterm memory network (BiLSTM) to model the entities and a tree-LSTM to model the relations be-\ntween entities; Li et al. (2017) propose a similar model for biomedical text. The tree-LSTM uses\ndependency tree information extracted using an external dependency parser to model relations be-\ntween entities. The use of these external NLP tools limits the effectiveness of a model to domains\n(e.g. news) where those NLP tools perform well. As a remedy to this problem, Bekoulis et al.\n(2018a) proposes a neural, end-to-end system that jointly learns to extract entities and relations\nwithout relying on external NLP tools. In Bekoulis et al. (2018b), they augment this model with ad-\nversarial training. Nguyen & Verspoor (2019) propose a different, albeit similar end-to-end neural\nmodel which makes use of deep biafﬁne attention (Dozat & Manning, 2016). Li et al. (2019) ap-\nproach the problem with multi-turn question answering, posing templated queries to a BERT-based\nQA model (Devlin et al., 2018) whose answers constitute extracted entities and their relations and\nachieve state-of-the-art results on three popular benchmark datasets.\nWhile demonstrating strong performance, end-to-end systems like Bekoulis et al. (2018a;b) and\nNguyen & Verspoor (2019) suffer from two main drawbacks. The ﬁrst is that most of the models\nparameters are trained from scratch. For large datasets, this can lead to long training times. For\nsmall datasets, which are common in the biomedical and clinical domains where it is particularly\nchallenging to acquire labelled data, this can lead to poor performance and/or overﬁtting. The sec-\nond is that these systems typically contain RNNs, which are sequential in nature and cannot be\nparallelized within training examples. The multi-pass QA model proposed in Li et al. (2019) allevi-\nates these issues by incorporating a pre-trained language model, BERT (Devlin et al., 2018), which\neschews recurrence for self-attention. The main limitation of their approach is that it relies on hand-\ncrafted question templates to achieve maximum performance. This may become a limiting factor\nwhere domain expertise is required to craft such questions (e.g., for biomedical or clinical corpora).\nAdditionally, one has to create a question template for each entity and relation type of interest.\nIn this study, we propose an end-to-end model for joint NER and RE which addresses all of these\nissues. Similar to past work, our model can be viewed as a mixture of a NER module and a RE\nmodule (Figure 1). Unlike most previous works, we include a pre-trained, transformer-based lan-\nguage model, speciﬁcally BERT (Devlin et al., 2018), which achieved state-of-the-art performance\nacross many NLP tasks. The weights of the BERT model are ﬁne-tuned during training, and the\nentire model is trained in an end-to-end fashion.\nOur main contributions are as follows: (1) Our solution is truly end-to-end, relying on no hand-\ncrafted features (e.g. templated questions) or external NLP tools (e.g. dependency parsers). (2) Our\nmodel is fast to train (e.g. under 10 minutes on a single GPU for the CoNLL04 corpus), as most\nof its parameters are pre-trained and we avoid recurrence. (3) We match or exceed state-of-the-art\nperformance for joint NER and RE on 5 datasets across 3 domains.\n2 T HE MODEL\nFigure 1 illustrates the architecture of our approach. Our model is composed of an NER module\nand an RE module. The NER module is identical to the one proposed by Devlin et al. (2018). For\na given input sequence sof N word tokens w1, w2, ... , wN , the pre-trained BERTBASE model ﬁrst\n2\nFigure 1: Joint named entity recognition (NER) and relation extraction (RE) model architecture.\nproduces a sequence of vectors, x(NER)\n1 , x(NER)\n2 , ... , x(NER)\nN which are then fed to a feed-forward\nneural network (FFNN) for classiﬁcation.\ns(NER)\ni = FFNNNER(x(NER)\ni ) (1)\nThe output size of this layer is the number of BIOES-based NER labels in the training data,|C(NER)|.\nIn the BIOES tag scheme, each token is assigned a label, where the B- tag indicates the beginning\nof an entity span, I- the inside, E- the end and S- is used for any single-token entity. All other tokens\nare assigned the label O.\nDuring training, a cross-entropy loss is computed for the NER objective,\nLNER = −\nN∑\nn=1\nlog\n(\nes(NER)\nn\n∑C(NER)\nc es(NER)\nn,c\n)\n(2)\nwhere s(NER)\nn is the predicted score that token n ∈N belongs to the ground-truth entity class and\ns(NER)\nn,c is the predicted score for token nbelonging to the entity class c∈C(NER).\nIn the RE module, the predicted entity labels are obtained by taking the argmax of each score vector\ns(NER)\n1 , s(NER)\n2 , ... , s(NER)\nN . The predicted entity labels are then embedded to produce a sequence\nof ﬁxed-length, continuous vectors, e(NER)\n1 , e(NER)\n2 , ... , e(NER)\nN which are concatenated with the\nhidden states from the ﬁnal layer in the BERT model and learned jointly with the rest of the models\nparameters.\nx(RE)\ni = x(NER)\ni ∥e(NER)\ni (3)\nFollowing Miwa & Bansal (2016) and Nguyen & Verspoor (2019), we incrementally construct the\nset of relation candidates, R, using all possible combinations of the last word tokens of predicted\nentities, i.e. words with E- or S- labels. An entity pair is assigned to a negative relation class\n(NEG) when the pair has no relation or when the predicted entities are not correct. Once relation\ncandidates are constructed, classiﬁcation is performed with a deep bilinear attention mechanism\n(Dozat & Manning, 2016), as proposed by Nguyen & Verspoor (2019).\nTo encode directionality, the mechanism uses FFNNs to project eachx(RE)\ni into head and tail vector\nrepresentations, corresponding to whether theith word serves as head or tail argument of the relation.\n3\nh(head)\ni = FFNNhead(x(RE)\ni ) (4)\nh(tail)\ni = FFNNtail(x(RE)\ni ) (5)\nThese projections are then fed to a biafﬁne classiﬁer,\ns(RE)\nj,k = Biafﬁne(h(head)\nj ,h(tail)\nk ) (6)\nBiafﬁne(x1,x2) = xT\n1 Ux2 + W(x1 ∥x2) + b (7)\nwhere U is an m×|C(RE)|×mtensor, W is a |C(RE)|×(2∗m) matrix, and b is a bias vector. Here,\nmis the size of the output layers of FFNNhead and FFNNtail and C(RE) is the set of all relation classes\n(including NEG). During training, a second cross-entropy loss is computed for the RE objective\nLRE = −\nR∑\nr=1\nlog\n(\nes(RE)\nr\n∑C(RE)\nc es(RE)\nr,c\n)\n(8)\nwhere s(RE)\nr is the predicted score that relation candidate r∈Rbelongs to the ground-truth relation\nclass and s(RE)\nr,c is the predicted score for relation rbelonging to the relation class c∈C(RE).\nThe model is trained in an end-to-end fashion to minimize the sum of the NER and RE losses.\nL= LNER + LRE (9)\n2.1 E NTITY PRETRAINING\nIn Miwa & Bansal (2016), entity pre-training is proposed as a solution to the problem of low-\nperformance entity detection in the early stages of training. It is implemented by delaying the\ntraining of the RE module by some number of epochs, before training the entire model jointly.\nOur implementation of entity pretraining is slightly different. Instead of delaying training of the RE\nmodule by some number of epochs, we weight the contribution of LRE to the total loss during the\nﬁrst epoch of training\nL= LNER + λLRE (10)\nwhere λis increased linearly from 0 to 1 during the ﬁrst epoch and set to 1 for the remaining epochs.\nWe chose this scheme because the NER module quickly achieves good performance for all datasets\n(i.e. within one epoch). In early experiments, we found this scheme to outperform a delay of a full\nepoch.\n2.2 I MPLEMENTATION\nWe implemented our model in PyTorch (Paszke et al., 2017) using the BERT BASE model from the\nPyTorch Transformers library1. Our model is available at our GitHub repository 2. Furthermore,\nwe use NVIDIAs automatic mixed precision (AMP) library Apex 3 to speed up training and reduce\nmemory usage without affecting task-speciﬁc performance.\n1https://github.com/huggingface/pytorch-transformers\n2https://github.com/bowang-lab/joint-ner-and-re\n3https://github.com/NVIDIA/apex\n4\n3 E XPERIMENTAL SETUP\n3.1 D ATASETS AND EVALUATION\nTo demonstrate the generalizability of our model, we evaluate it on 5 commonly used benchmark\ncorpora across 3 domains. All corpora are in English. Detailed corpus statistics are presented in\nTable A.1 of the appendix.\n3.1.1 ACE04/05\nThe Automatic Content Extraction (ACE04) corpus was introduced by Doddington et al. (2004),\nand is commonly used to benchmark NER and RE methods. There are 7 entity types and 7 relation\ntypes. ACE05 builds on ACE04, splitting the Physical relation into two classes (Physical and Part-\nWhole), removing the Discourse relation class and merging Employment-Membership-Subsidiary\nand Person-Organization-Afﬁliationinto one class (Employment-Membership-Subsidiary).\nFor ACE04, we follow Miwa & Bansal (2016) by removing the Discourse relation and evaluating\nour model using 5-fold cross-validation on the bnews and nwire subsets, where 10% of the data was\nheld out within each fold as a validation set. For ACE05, we use the same test split as Miwa & Bansal\n(2016). We use 5-fold cross-validation on the remaining data to choose the hyperparameters. Once\nhyperparameters are chosen, we train on the combined data from all the folds and evaluate on the\ntest set. For both corpora, we report the micro-averaged F 1 score. We obtained the pre-processing\nscripts from Miwa & Bansal (2016)4.\n3.1.2 C ONLL04\nThe CoNLL04 corpus was introduced in Roth & Yih (2004) and consists of articles from the Wall\nStreet Journal (WSJ) and Associated Press (AP). There are 4 entity types and 5 relation types.\nWe use the same test set split as Miwa & Sasaki (2014) 5. We use 5-fold cross-validation on the\nremaining data to choose hyperparameters. Once hyperparameters are chosen, we train on the com-\nbined data from all folds and evaluate on the test set, reporting the micro-averaged F1 score.\n3.1.3 ADE\nThe adverse drug event corpus was introduced by Gurulingappa et al. (2012) to serve as a benchmark\nfor systems that aim to identify adverse drug events from free-text. It consists of the abstracts of\nmedical case reports retrieved from PubMed6. There are two entity types, Drug and Adverse effect\nand one relation type, Adverse drug event.\nSimilar to previous work (Li et al., 2016; 2017; Bekoulis et al., 2018b), we remove ∼130 relations\nwith overlapping entities and evaluate our model using 10-fold cross-validation, where 10% of the\ndata within each fold was used as a validation set, 10% as a test set and the remaining data is used\nas a train set. We report the macro F1 score averaged across all folds.\n3.1.4 I2B2\nThe 2010 i2b2/V A dataset was introduced by Uzuner et al. (2011) for the 2010 i2b2/Va Workshop\non Natural Language Processing Challenges for Clinical Records. The workshop contained an NER\ntask focused on the extraction of 3 medical entity types ( Problem, Treatment, Test) and an RE task\nfor 8 relation types.\nIn the ofﬁcial splits, the test set contains roughly twice as many examples as the train set. To increase\nthe number of training examples while maintaining a rigorous evaluation, we elected to perform 5-\nfold cross-validation on the combined data from both partitions. We used 10% of the data within\neach fold as a validation set, 20% as a test set and the remaining data was used as a train set. We\nreport the micro F1 score averaged across all folds.\n4https://github.com/tticoin/LSTM-ER/tree/master/data\n5https://github.com/pgcool/TF-MTRNN/tree/master/data/CoNLL04\n6https://www.ncbi.nlm.nih.gov/pubmed\n5\nTo the best of our knowledge, we are the ﬁrst to evaluate a joint NER and RE model on the 2010\ni2b2/V A dataset. Therefore, we decided to compare to scores obtained by independent NER and RE\nsystems. We note, however, that the scores of independent RE systems are not directly comparable\nto the scores we report in this paper. This is because RE is traditionally framed as a sentence-level\nclassiﬁcation problem. During pre-processing, each example is permutated into processed examples\ncontaining two “blinded” entities and labelled for one relation class. E.g. the example: “His PCP had\nrecently started ciproﬂoxacinTREATMENT for a UTIPROBLEM” becomes “His PCP had recently started\n@TREATMENT$ for a @PROBLEM$”, where the model is trained to predict the target relation\ntype, “Treatment is administered for medical problem” (TrAP).\nThis task is inherently easier than the joint setup, for two reasons: relation predictions are made\non ground-truth entities, as opposed to predicted entities (which are noisy) and the model is only\nrequired to make one classiﬁcation decision per pre-processed sentence. In the joint setup, a model\nmust identify any number of relations (or the lack thereof) between all unique pairs of predicted\nentities in a given input sentence. To control for the ﬁrst of these differences, we report scores from\nour model in two settings, once when predicted entities are used as input to the RE module, and\nonce when ground-truth entities are used.\n3.2 H YPERPARAMETERS\nBesides batch size, learning rate and number of training epochs, we used the same hyperparameters\nacross all experiments (see Table A.2). Similar to Devlin et al. (2018), learning rate and batch size\nwere selected for each dataset using a minimal grid search (see See Table A.3).\nOne hyperparameter selected by hand was the choice of the pre-trained weights used to initialize the\nBERTBASE model. For general domain corpora, we found the cased BERTBASE weights from Devlin\net al. (2018) to work well. For biomedical corpora, we used the weights from BioBERT (Lee et al.,\n2019), which recently demonstrated state-of-the-art performance for biomedical NER, RE and QA.\nSimilarly, for clinical corpora we use the weights provided by Peng et al. (2019), who pre-trained\nBERTBASE on PubMed abstracts and clinical notes from MIMIC-III7.\n4 R ESULTS\n4.1 J OINTLY LEARNING NER AND RE\nTable 1 shows our results in comparison to previously published results, grouped by the domain\nof the evaluated corpus. We ﬁnd that on every dataset besides i2b2, our model improves NER\nperformance, for an average improvement of ∼2%. This improvement is particularly large on the\nACE04 and ACE05 corpora (3.98% and 2.41% respectively). On i2b2, our joint model performs\nwithin 0.29% of the best independent NER solution.\nFor relation extraction, we outperform previous methods on 2 datasets and come within ∼2% on\nboth ACE05 and CoNLL04. In two cases, our performance improvement is substantial, with im-\nprovements of 4.59% and 10.25% on the ACE04 and ADE corpora respectively. For i2b2, our score\nis not directly comparable to previous systems (as discussed in section 3.1.4) but will facilitate future\ncomparisons of joint NER and RE methods on this dataset. By comparing overall performance, we\nﬁnd that our approach achieves new state-of-the-art performance for 3 popular benchmark datasets\n(ACE04, ACE05, ADE) and comes within 0.2% for CoNLL04.\n4.2 A BLATION ANALYSIS\nTo determine which training strategies and components are responsible for our models performance,\nwe conduct an ablation analysis on the CoNLL04 corpus (Table 2). We perform ﬁve different abla-\ntions: (a) Without entity pre-training (see section 2.1), i.e. the loss function is given by equation 9.\n(b) Without entity embeddings, i.e. equation 3 becomes x(RE)\ni = x(NER)\ni . (c) Replacing the two\nfeed-forward neural networks, FFNNhead and FFNNtail with a single FFNN (see equation 4 and 5).\n(d) Removing FFNNhead and FFNNtail entirely. (e) Without the bilinear operation, i.e. equation 7\nbecomes a simple linear transformation.\n7https://mimic.physionet.org/\n6\nTable 1: Comparison to previously published F 1 scores for joint named entity recognition (NER)\nand relation extraction (RE). Ours (gold): our model, when gold entity labels are used as input to\nthe RE module. Ours: full, end-to-end model (see section 2). Bold: best scores. Subscripts denote\nstandard deviation across three runs. ∆: difference to our overall score.\nDataset Method Entity Relation Overall ∆\nGeneral ACE04 Miwa & Bansal (2016) 81.80 48.40 65.10 -5.69\nBekoulis et al. (2018b) 81.64 47.45 64.54 -6.25\nLi et al. (2019) 83.60 49.40 66.50 -4.29\nOurs 87.580.2 53.990.1 70.79 –\nACE05 Miwa & Bansal (2016) 83.40 55.60 69.50 -3.42\nZhang et al. (2017) 83.50 57.50 70.50 -2.42\nLi et al. (2019) 84.80 60.20 72.50 -0.42\nOurs 87.210.3 58.630.0 72.92 –\nCoNLL04 Miwa & Sasaki (2014) 80.70 61.00 70.85 -7.30\nBekoulis et al. (2018b) 83.61 61.95 72.78 -5.37\nLi et al. (2019) 87.80 68.90 78.35 0.20\nOurs 89.461.0 66.830.4 78.15 –\nBiomedical ADE Li et al. (2016) 79.50 63.40 71.45 -16.21\nLi et al. (2017) 84.60 71.40 78.00 -9.66\nBekoulis et al. (2018b) 86.73 75.52 81.13 -6.53\nOurs 89.560.1 85.770.4 87.66 –\nClinical i2b2 * Si et al. (2019)** 89.55 – – –\nPeng et al. (2019) – 76.40 – –\nOurs (gold) – 72.03 0.1 – –\nOurs 89.26 0.1 63.020.3 76.14 –\n* To the best of our knowledge, there are no published joint NER and RE models that evaluate\non the i2b2 2010 corpus. We compare our model to the state-of-the-art for each individual\ntask (see section 3.1.4).\n** We compare to the scores achieved by their BERTBASE model.\nTable 2: Ablation experiment results on the CoNLL04 corpus. Scores are reported as a micro-\naveraged F1 score on the validation set, averaged across three runs of 5-fold cross-validation. (a)\nWithout entity pre-training (section 2.1). (b) Without entity embeddings (eq. 3). (c) Using a single\nFFNN in place of FFNN head and FFNNtail (eq. 4 and 5) (d) Without FFNN head and FFNNtail (e)\nWithout the bilinear operation (eq. 7). Bold: best scores. Subscripts denote standard deviation\nacross three runs. ∆: difference to the full models score.\nModel Entity Relation Overall ∆\nFull model 86.32 0.2 60.290.2 73.30 –\n(a) w/o Entity pre-training 85.91 0.1 59.440.2 72.67 -0.63\n(b) w/o Entity embeddings 86.07 0.1 59.520.2 72.80 -0.51\n(c) Single FFNN 86.02 0.1 60.130.0 73.07 -0.23\n(d) w/o Head/Tail 83.93 0.2 54.670.8 69.30 -4.00\n(e) w/o Bilinear 86.350.1 59.600.0 72.98 -0.33\nRemoving FFNNhead and FFNNtail has, by far, the largest negative impact on performance. Inter-\nestingly, however, replacing FFNNhead and FFNNtail with a single FFNN has only a small negative\nimpact. This suggests that while these layers are very important for model performance, using dis-\ntinct FFNNs for the projection of head and tail entities (as opposed to the same FFNN) is relatively\nmuch less important. The next most impactful ablation was entity pre-training, suggesting that\nlow-performance entity detection during the early stages of training is detrimental to learning (see\n7\nFigure 2: Visualization of the attention weights from select layers and heads of BERT after it\nwas ﬁne-tuned within our model on the CoNLL04 corpus. Darker squares indicate larger atten-\ntion weights. Attention weights are shown for the input sentence: ”Ruby fatally shot Oswald two\ndays after Kennedy was assassinated.”. The CLS and SEP tokens have been removed. Four major\npatterns are displayed: paying attention to the next word (ﬁrst image from the left) and previous\nword (second from the left), paying attention to the word itself (third from the left) and the end of\nthe sentence (fourth from the left).\nsection 2.1). Finally, we note that the importance of entity embeddings is surprising, as a previous\nstudy has found that entity embeddings did not help performance on the CoNLL04 corpus (Bekoulis\net al., 2018a), although their architecture was markedly different. We conclude that each of our\nablated components is necessary to achieve maximum performance.\n4.3 A NALYSIS OF THE WORD -LEVEL ATTENTION WEIGHTS\nOne advantage of including a transformer-based language model is that we can easily visualize the\nattention weights with respect to some input. This visualization is useful, for example, in detect-\ning model bias and locating relevant attention heads (Vig, 2019). Previous works have used such\nvisualizations to demonstrate that speciﬁc attention heads mark syntactic dependency relations and\nthat lower layers tend to learn more about syntax while higher layers tend to encode more semantics\n(Raganato & Tiedemann, 2018).\nIn Figure 2 we visualize the attention weights of select layers and attention heads from an instance of\nBERT ﬁne-tuned within our model on the CoNLL04 corpus. We display four patterns that are easily\ninterpreted: paying attention to the next and previous words, paying attention to the word itself, and\npaying attention to the end of the sentence. These same patterns have been found in pre-trained\nBERT models that have not been ﬁne-tuned on a speciﬁc, supervised task (Vig, 2019; Raganato &\nTiedemann, 2018), and therefore, are retained after our ﬁne-tuning procedure.\nTo facilitate further analysis of our learned model, we make available Jupyter and Google Colabora-\ntory notebooks on our GitHub repository8, where users can use multiple views to explore the learned\nattention weights of our models. We use the BertViz library (Vig, 2019) to render the interactive,\nHTML-based views and to access the attention weights used to plot the heat maps.\n5 D ISCUSSION AND CONCLUSION\nIn this paper, we introduced an end-to-end model for entity and relation extraction. Our key contri-\nbutions are: (1) No reliance on any hand-crafted features (e.g. templated questions) or external NLP\ntools (e.g. dependency parsers). (2) Integration of a pre-trained, transformer-based language model.\n(3) State-of-the-art performance on 5 datasets across 3 domains. Furthermore, our model is inher-\nently modular. One can easily initialize the language model with pre-trained weights better suited\nfor a domain of interest (e.g. BioBERT for biomedical corpora) or swap BERT for a comparable\nlanguage model (e.g. XLNet (Yang et al., 2019)). Finally, because of (2), our model is fast to train,\nconverging in approximately 1 hour or less on a single GPU for all datasets used in this study.\n8https://github.com/bowang-lab/joint-ner-and-re\n8\nOur model out-performed previous state-of-the-art performance on ADE by the largest margin\n(6.53%). While exciting, we believe this corpus was particularly easy to learn. The majority of\nsentences (∼68%) are annotated for two entities (drug and adverse effect, and one relation (adverse\ndrug event). Ostensibly, a model should be able to exploit this pattern to get near-perfect perfor-\nmance on the majority of sentences in the corpus. As a test, we ran our model again, this time\nusing ground-truth entities in the RE module (as opposed to predicted entities) and found that the\nmodel very quickly reached almost perfect performance for RE on the test set ( ∼98%). As such,\nhigh performance on the ADE corpus is not likely to transfer to real-world scenarios involving the\nlarge-scale annotation of diverse biomedical articles.\nIn our experiments, we consider only intra-sentence relations. However, the multiple entities within\na document generally exhibit complex, inter-sentence relations. Our model is not currently capable\nof extracting such inter-sentence relations and therefore our restriction to intra-sentence relations\nwill limit its usefulness for certain downstream tasks, such as knowledge base creation. We also\nignore the problem of nested entities, which are common in biomedical corpora. In the future, we\nwould like to extend our model to handle both nested entities and inter-sentence relations. Finally,\ngiven that multilingual, pre-trained weights for BERT exist, we would also expect our model’s\nperformance to hold across multiple languages. We leave this question to future work.\nREFERENCES\nHeike Adel and Hinrich Sch ¨utze. Global normalization of convolutional neural networks for joint\nentity and relation classiﬁcation. In Proceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , Copenhagen, Denmark, September 2017. Association for\nComputational Linguistics.\nStein Aerts, Diether Lambrechts, Sunit Maity, Peter Van Loo, Bert Coessens, Frederik De Smet,\nLeon-Charles Tranchevent, Bart De Moor, Peter Marynen, Bassem Hassan, Peter Carmeliet, and\nYves Moreau. Gene prioritization through genomic data fusion. Nature Biotechnology, 24(5):\n537–544, may 2006. doi: 10.1038/nbt1203.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. Joint entity recognition\nand relation extraction as a multi-head selection problem. Expert Systems with Applications, 114:\n34–45, 2018a.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. Adversarial training\nfor multi-context joint entity and relation extraction. arXiv preprint arXiv:1808.06876, 2018b.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nGeorge Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and\nRalph Weischedel. The automatic content extraction (ACE) program – tasks, data, and evaluation.\nIn Proceedings of the Fourth International Conference on Language Resources and Evaluation\n(LREC’04), Lisbon, Portugal, May 2004. European Language Resources Association (ELRA).\nURL http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf.\nTimothy Dozat and Christopher D Manning. Deep biafﬁne attention for neural dependency parsing.\narXiv preprint arXiv:1611.01734, 2016.\nPankaj Gupta, Hinrich Sch ¨utze, and Bernt Andrassy. Table ﬁlling multi-task recurrent neural net-\nwork for joint entity and relation extraction. In Proceedings of COLING 2016, the 26th Interna-\ntional Conference on Computational Linguistics: Technical Papers, pp. 2537–2547, 2016.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-\nApitius, and Luca Toldo. Development of a benchmark corpus to support the automatic\nextraction of drug-related adverse effects from medical case reports. Journal of Biomedi-\ncal Informatics , 45(5):885 – 892, 2012. ISSN 1532-0464. doi: https://doi.org/10.1016/j.\njbi.2012.04.008. URL http://www.sciencedirect.com/science/article/pii/\nS1532046412000615. Text Mining and Natural Language Processing in Pharmacogenomics.\n9\nJing Jiang. Information Extraction from Text, pp. 11–41. Springer US, Boston, MA, 2012. ISBN\n978-1-4614-3223-4. doi: 10.1007/978-1-4614-3223-4 2. URL https://doi.org/10.\n1007/978-1-4614-3223-4_2 .\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-\nwoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text\nmining. arXiv preprint arXiv:1901.08746, 2019.\nFei Li, Yue Zhang, Meishan Zhang, and Donghong Ji. Joint models for extracting adverse drug\nevents from biomedical text. In IJCAI, volume 2016, pp. 2838–2844, 2016.\nFei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. A neural joint model for entity and relation\nextraction from biomedical text. BMC bioinformatics, 18(1):198, 2017.\nGang Li, Karen E. Ross, Cecilia N. Arighi, Yifan Peng, Cathy H. Wu, and K. Vijay-Shanker. miR-\nTex: A text mining system for miRNA-gene relation extraction. PLOS Computational Biology,\n11(9):e1004391, sep 2015. doi: 10.1371/journal.pcbi.1004391.\nXiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li.\nEntity-relation extraction as multi-turn question answering. arXiv preprint arXiv:1905.05529 ,\n2019.\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR,\nabs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101.\nRiccardo Miotto, Fei Wang, Shuang Wang, Xiaoqian Jiang, and Joel T Dudley. Deep learning for\nhealthcare: review, opportunities and challenges. Brieﬁngs in bioinformatics, 19(6):1236–1246,\n2017.\nMakoto Miwa and Mohit Bansal. End-to-end relation extraction using lstms on sequences and tree\nstructures. arXiv preprint arXiv:1601.00770, 2016.\nMakoto Miwa and Yutaka Sasaki. Modeling joint entity and relation extraction with table repre-\nsentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1858–1869, 2014.\nDat Quoc Nguyen and Karin Verspoor. End-to-end neural relation extraction using deep biafﬁne at-\ntention. In Leif Azzopardi, Benno Stein, Norbert Fuhr, Philipp Mayr, Claudia Hauff, and Djoerd\nHiemstra (eds.), Advances in Information Retrieval, pp. 729–738, Cham, 2019. Springer Interna-\ntional Publishing. ISBN 978-3-030-15712-8.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. In International Conference on Machine Learning, pp. 1310–1318, 2013.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017.\nYifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language pro-\ncessing: An evaluation of BERT and elmo on ten benchmarking datasets.CoRR, abs/1906.05474,\n2019. URL http://arxiv.org/abs/1906.05474.\nAlessandro Raganato and J ¨org Tiedemann. An analysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pp. 287–297, 2018.\nDan Roth and Wen-tau Yih. A linear programming formulation for global inference in natural\nlanguage tasks. In Proceedings of the Eighth Conference on Computational Natural Language\nLearning (CoNLL-2004) at HLT-NAACL 2004, pp. 1–8, 2004.\nYuqi Si, Jingqi Wang, Hua Xu, and Kirk Roberts. Enhancing clinical concept extraction with con-\ntextual embedding. CoRR, abs/1902.08691, 2019. URL http://arxiv.org/abs/1902.\n08691.\n10\n¨Ozlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text. Journal of the American Medical Informatics\nAssociation, 18(5):552–556, 2011.\nJesse Vig. A multiscale visualization of attention in the transformer model. CoRR, abs/1906.05714,\n2019. URL http://arxiv.org/abs/1906.05714.\nZhong-Yi Wang and Hong-Yu Zhang. Rational drug repositioning by medical genetics. Nature\nBiotechnology, 31(12):1080–1082, dec 2013. doi: 10.1038/nbt.2758.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\nMeishan Zhang, Yue Zhang, and Guohong Fu. End-to-end neural relation extraction with global\noptimization. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1730–1740, 2017.\nXueZhong Zhou, Jrg Menche, Albert-L ´aszl´o Barab ´asi, and Amitabh Sharma. Human symp-\ntoms–disease network. Nature Communications, 5, jun 2014. doi: 10.1038/ncomms5212.\nA A PPENDIX\nA.1 C ORPUS STATISTICS\nTable A.1 lists detailed statistics for each corpus used in this study.\nTable A.1: Detailed entity and relation counts for each corpus used in this study.\nDataset Entity classes (count) Relation classes (count)\nGeneral ACE04 Person (12508), Organization\n(4405), Geographical Entities\n(4425), Location (614), Facility\n(688), Weapon (119), and Vehicle\n(209)\nPhysical (1202), Person-Social\n(362), Employment-Membership-\nSubsidiary (1591), Agent-Artifact\n(212), Person-Organization-\nAfﬁliation (141), Geopolitical\nEntity-Afﬁliation (517)\nACE05 Person (20891), Organization\n(5627), Geographical Entities\n(7455), Location(1119), Facil-\nity (1461), Weapon (911), and\nVehicle (919)\nPhysical (1612), Part-\nWhole (1060), Person-Social\n(615), Agent-Artifact (703),\nEmployment-Membership-\nSubsidiary (1922), Geopolitical\nEntity-Afﬁliation (730)\nCoNLL04 Location (4765), Organization\n(2499), People (3918), Other\n(3011)\nKill (268), Live in (521), Lo-\ncated in (406), OrgBased in (452),\nWork for (401)\nBiomedical ADE Drug (4979), Adverse effect\n(5669)\nAdverse drug event (6682)\nClinical i2b2 Problem (19664), Test (13831),\nTreatment (14186)\nPIP (2203), TeCP (504), TeRP\n(3053), TrAP (2617), TrCP (526),\nTrIP (203), TrNAP (174), TrWP\n(133)\nA.2 H YPERPARAMETERS AND MODEL DETAILS\nTable A.2 lists hyperparameters and model details that were held constant across all experiments.\nTable A.3 lists those that were speciﬁc to each evaluated corpus.\n11\nTable A.2: Hyperparameter values and model details used across all experiments.\nHyperparameter Value Comment\nTagging scheme BIOES Single token entities are tagged with an S- tag, the begin-\nning of an entity span with a B- tag, the last token of an\nentity span with an E- tag, and tokens inside an entity span\nwith an I- tag.\nDropout rate 0.1 Dropout rate applied to the output of all FFNNs and the\nattention heads of the BERT model.\nEntity embeddings 128 Output dimension of the entity embedding layer.\nFFNNhead / FFNNtail 512 Output dimension of the FFNN head and FFNNtail layers\nNo. layers (NER module) 1 Number of layers used in the FFNN of the NER module.\nNo. layers (RE module) 2 Number of layers used in the FFNNs of the RE module.\nOptimizer AdamW Adam with ﬁxed weight decay regulatization (Loshchilov\n& Hutter, 2017).\nGradient normalization Γ = 1 Rescales the gradient whenever the norm goes over some\nthreshold Γ (Pascanu et al., 2013).\nWeight decay 0.1 L2 weight decay.\nTable A.3: Hyperparameter values speciﬁc to individual datasets. Similar to Devlin et al. (2018), a\nminimal grid search was performed over the values 16, 32 for batch size and 2e-5, 3e-5, and 5e-5\nfor learning rate.\nDataset No. Epochs Batch size Learning rate Initial BERT weights\nACE04 15 16 2e-5 BERT-Base (cased) (Devlin et al., 2018)\nACE05 15 32 3e-5 BERT-Base (cased)\nCoNLL04 10 16 3e-5 BERT-Base (cased)\nADE 7 16 2e-5 BioBERT (cased) (Lee et al., 2019)\ni2b2 12 16 2e-5 NCBI-BERT (uncased) (Peng et al., 2019)\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8807972073554993
    },
    {
      "name": "Pipeline (software)",
      "score": 0.7708713412284851
    },
    {
      "name": "Named-entity recognition",
      "score": 0.7439924478530884
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.706481397151947
    },
    {
      "name": "Relationship extraction",
      "score": 0.6633206009864807
    },
    {
      "name": "Artificial intelligence",
      "score": 0.640170693397522
    },
    {
      "name": "Language model",
      "score": 0.6189039349555969
    },
    {
      "name": "Scratch",
      "score": 0.6101109385490417
    },
    {
      "name": "Natural language processing",
      "score": 0.6041468977928162
    },
    {
      "name": "Parsing",
      "score": 0.6026281118392944
    },
    {
      "name": "End-to-end principle",
      "score": 0.5783997774124146
    },
    {
      "name": "Information extraction",
      "score": 0.5128036141395569
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5051813721656799
    },
    {
      "name": "Relation (database)",
      "score": 0.49550187587738037
    },
    {
      "name": "Natural language",
      "score": 0.4272875189781189
    },
    {
      "name": "Speech recognition",
      "score": 0.3458540141582489
    },
    {
      "name": "Machine learning",
      "score": 0.3422204554080963
    },
    {
      "name": "Data mining",
      "score": 0.18267899751663208
    },
    {
      "name": "Task (project management)",
      "score": 0.1486319899559021
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}