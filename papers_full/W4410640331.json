{
  "title": "Large language models’ capabilities in responding to tuberculosis medical questions: testing ChatGPT, Gemini, and Copilot",
  "url": "https://openalex.org/W4410640331",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2902410128",
      "name": "Meisam Dastani",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1965524768",
      "name": "Jalal Mardaneh",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2592760248",
      "name": "Morteza Rostamian",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2902410128",
      "name": "Meisam Dastani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965524768",
      "name": "Jalal Mardaneh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2592760248",
      "name": "Morteza Rostamian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4383343032",
    "https://openalex.org/W4389348902",
    "https://openalex.org/W4403924970",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4404349757",
    "https://openalex.org/W4383218913",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4321366933",
    "https://openalex.org/W4365999176",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4385299173",
    "https://openalex.org/W4379231355",
    "https://openalex.org/W4400310895",
    "https://openalex.org/W4394009806",
    "https://openalex.org/W4401000066",
    "https://openalex.org/W4388370035",
    "https://openalex.org/W3008199156",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4396868781",
    "https://openalex.org/W4405483289",
    "https://openalex.org/W4394873790",
    "https://openalex.org/W4391815795",
    "https://openalex.org/W4400163553",
    "https://openalex.org/W4403891030",
    "https://openalex.org/W4404291562",
    "https://openalex.org/W3107295678",
    "https://openalex.org/W3003494210",
    "https://openalex.org/W4401670177"
  ],
  "abstract": "This study aims to evaluate the capability of Large Language Models (LLMs) in responding to questions related to tuberculosis. Three large language models (ChatGPT, Gemini, and Copilot) were selected based on public accessibility criteria and their ability to respond to medical questions. Questions were designed across four main domains (diagnosis, treatment, prevention and control, and disease management). The responses were subsequently evaluated using DISCERN-AI and NLAT-AI assessment tools. ChatGPT achieved higher scores (4 out of 5) across all domains, while Gemini demonstrated superior performance in specific areas such as prevention and control with a score of 4.4. Copilot showed the weakest performance in disease management with a score of 3.6. In the diagnosis domain, all three models demonstrated equivalent performance (4 out of 5). According to the DISCERN-AI criteria, ChatGPT excelled in information relevance but showed deficiencies in providing sources and information production dates. All three models exhibited similar performance in balance and objectivity indicators. While all three models demonstrate acceptable capabilities in responding to medical questions related to tuberculosis, they share common limitations such as insufficient source citation and failure to acknowledge response uncertainties. Enhancement of these models could strengthen their role in providing medical information.",
  "full_text": "Large language models’ capabilities \nin responding to tuberculosis \nmedical questions: testing \nChatGPT, Gemini, and Copilot\nMeisam Dastani1, Jalal Mardaneh 2 & Morteza Rostamian3\nThis study aims to evaluate the capability of Large Language Models (LLMs) in responding to \nquestions related to tuberculosis. Three large language models (ChatGPT, Gemini, and Copilot) \nwere selected based on public accessibility criteria and their ability to respond to medical questions. \nQuestions were designed across four main domains (diagnosis, treatment, prevention and control, \nand disease management). The responses were subsequently evaluated using DISCERN-AI and NLAT-\nAI assessment tools. ChatGPT achieved higher scores (4 out of 5) across all domains, while Gemini \ndemonstrated superior performance in specific areas such as prevention and control with a score \nof 4.4. Copilot showed the weakest performance in disease management with a score of 3.6. In the \ndiagnosis domain, all three models demonstrated equivalent performance (4 out of 5). According to the \nDISCERN-AI criteria, ChatGPT excelled in information relevance but showed deficiencies in providing \nsources and information production dates. All three models exhibited similar performance in balance \nand objectivity indicators. While all three models demonstrate acceptable capabilities in responding to \nmedical questions related to tuberculosis, they share common limitations such as insufficient source \ncitation and failure to acknowledge response uncertainties. Enhancement of these models could \nstrengthen their role in providing medical information.\nKeywords Large language models (LLMs), ChatGPT, Gemini, Copilot, Tuberculosis, Medical questions\nLarge Language Models (LLMs) have made remarkable progress in artificial intelligence and natural language \nprocessing in recent years. These models, utilizing deep neural networks and learning from extensive textual \ndata, have acquired the ability to comprehend and generate human-like text. Key capabilities of LLMs include \nquestion answering, translation, summarization, and text generation1. Consequently, the use of LLMs is rapidly \nincreasing, with tools like Bard, Bing, and ChatGPT (OpenAI) providing users access to extensive services 2,3. \nSome experts believe these models could soon replace search engines and play crucial roles in various software \ndomains4. Initial evaluations indicate that LLMs possess strong semantic and syntactic understanding across \nmany natural languages5,6 and can effectively perform natural language processing operations. These models \nalso demonstrate proficiency in answering questions related to mathematics, science, programming, logical \nreasoning, and humanities7,8. LLMs have captured public attention due to their potential to improve traditional \napproaches across various fields9.\nIn healthcare, ChatGPT stands out as a notable example, showing promising features in generating human-\nlike textual communications10. These capabilities have led to exploratory applications of ChatGPT in tasks such \nas answering medical questions and creating accurate medical content. Additionally, ChatGPT has demonstrated \nsuccessful potential in various aspects including diagnosis11, treatment recommendations12, patient education13, \nand medical image interpretation 1. LLMs like GPT-3.5 and GPT-4 can process and synthesize vast amounts \nof medical texts and patient data, potentially reducing the information burden on healthcare professionals 14. \nThis capability becomes particularly relevant during periods of medical specialist shortages and increasing wait \ntimes, potentially leading to a preference for LLM-based chatbots like ChatGPT over consultation with trained \nspecialists15,16. However, significant challenges remain regarding the use of LLMs in sensitive healthcare domains, \n1Infectious Diseases Research Center, Gonabad University of Medical Sciences, Gonabad, Iran. 2Department of \nMicrobiology, Infectious Diseases Research Center, School of Medicine, Gonabad University of Medical Sciences, \nGonabad, Iran. 3English Department, School of Medicine, Gonabad University of Medical Sciences, Gonabad, Iran. \nemail: m.rostamian.edu@gmail.com\nOPEN\nScientific Reports |        (2025) 15:18004 1| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports\n\nincluding ethical concerns, patient privacy, and information security, as well as worries about perpetuating \nexisting biases or causing unintended harm through these models17–19.\nPrevious studies have extensively evaluated the comprehensiveness of LLMs responses. For instance, Shao \net al. (2023) demonstrated that ChatGPT performed successfully in providing comprehensive and appropriate \nresponses to chest surgery-related questions, with over 92% of responses being validated. This study also noted \nthe model’s significant role in improving patient satisfaction and reducing anxiety20. Lahat et al. (2023) examined \nChatGPT’s application in answering gastrointestinal health questions, finding that response accuracy varied by \nquestion type, with treatment-related questions showing higher accuracy than diagnostic ones21. Furthermore, \nY eo et al. (2023) evaluated ChatGPT’s performance in responding to questions about liver cirrhosis and \nhepatocellular carcinoma. Results indicated that while the model provides appropriate information, weaknesses \nwere observed in diagnostic and preventive domains11. Moreover, Sarangi et al. (2023) evaluated the performance \nof four large language models in providing clinical decision support for imaging in cases suspected of pulmonary \nembolism. The results demonstrated variable accuracy in their responses: Perplexity excelled in open-ended \nquestions, while Bing showed superior performance in multiple-choice questions22.\nAdditionally, a systematic review by Omar et al. (2023) examining language models’ performance in \nmanaging infectious diseases showed that while these tools succeed in diagnosing certain diseases, they still \nrequire improvement23. Response quality heavily depends on the quality of available online data, which can lead \nto errors11,21. The benefits of chatbots include improved patient access to information, facilitated therapeutic \ncommunications, and increased efficiency in disease diagnosis. For example, Zhang and Song (2023) achieved \n97.50% accuracy in chronic disease diagnosis using a GPT-2-based system 24. Likewise, Mondal et al. (2023) \ndemonstrated that ChatGPT could provide accurate information about lifestyle-related diseases and serve as a \npreliminary tool for patient consultation25.\nDespite significant advances in large language models, accurately evaluating their performance in specialized \nfields like medicine remains challenging. Specifically, these models’ ability to answer medical questions related \nto particular diseases like tuberculosis requires careful examination. Tuberculosis is one of the world’s most \nsignificant infectious diseases, affecting millions annually and requiring accurate, up-to-date information for \ndiagnosis and treatment. TB represents a major global health concern requiring serious attention, where early \ndiagnosis and appropriate treatment can aid in disease control 26. Given these considerations, this study aims \nto evaluate the performance of LLMs like ChatGPT in answering medical questions related to tuberculosis. \nConsidering the importance of access to accurate and comprehensive information, increasing drug resistance, \nand high economic costs for health ministries and families, this study examines the accuracy and efficiency of \nresponses generated by LLMs in various tuberculosis-related areas. Additionally, this research aims to identify \nthese models’ limitations and strengths in providing medical information and examine potential risks that might \narise from incorrect or incomplete information. The study’s results could contribute to improving the use of \nLLMs in healthcare and provide strategies for optimizing their performance in dealing with infectious diseases \nlike tuberculosis.\nMethods\nIn this research, the study population comprised publicly available LLMs. The inclusion criteria were defined as \npublic accessibility to these models and their capability to respond to research questions. Based on these criteria, \nthree large language models—ChatGPT, Gemini, and Copilot—were selected as samples. These models were \nchosen due to their easy accessibility and ability to provide textual responses.\nQuestion design and categorization\nThe medical questions were designed based on study objectives across four main domains in the field of \ntuberculosis:\n• Disease Diagnosis\n• Disease Treatment\n• Disease Prevention and Control\n• Disease Management\nAdditionally, a segment of questions was dedicated to topics related to children and vulnerable populations \naffected by tuberculosis. These questions were developed in consultation with infectious disease specialists to \nensure comprehensive coverage of tuberculosis-related information needs. The resulting checklist comprised 23 \nquestions, distributed as follows:\n• Five questions related to diagnosis\n• Five questions related to treatment\n• Five questions related to prevention and control\n• Five questions related to disease management\n• Three questions related to children and vulnerable people\nData collection\nThe designed questions were posed separately to each language model (ChatGPT, Gemini, and Copilot) in \nEnglish on December 23, 2024, following the Meskó 2023 prompt engineering guidelines27. All responses were \nsystematically recorded and stored for subsequent analysis.\nScientific Reports |        (2025) 15:18004 2| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\nEvaluation process\nThe DISCERN-AI Tool and the Natural Language Assessment Tool for AI [NLAT-AI] were used to assess the \ndata. These tools, previously validated in language model evaluation studies, demonstrate acceptable validity \nand reliability. The assessment checklists evaluate various criteria including accuracy, transparency, and \ncomprehensiveness of the models’ responses 28. The DISCERN-AI tool is a modified version of the validated \nDISCERN instrument, which is used to assess the quality of health care treatment information. In this study, \nseven questions from the original DISCERN were selected and adapted to a 3-point scale for evaluating the \nresponses generated by language models. Based on the total scores, the quality of the generated content was \ncategorized into five levels: very poor, poor, moderate, good, and excellent29. The NLAT-AI tool comprises five \nkey components—accuracy, safety, appropriateness, actionability, and effectiveness—each of which is assessed \nusing a 5-point Likert scale 28. The questions and answers generated by each chatbot under study (ChatGPT, \nGemini, and Copilot), along with the evaluation tools DISCERN-AI and NLAT-AI and explanatory information \nregarding each chatbot and tool, were provided to a subject-matter expert. The expert selected to evaluate \nthe responses in this study met the following criteria: (a) not a member of the research team; (b) a qualified \nphysician, and (c) possessing a scientific and research background with experience in the field of infectious \ndiseases. After thorough review and completion of the questionnaires, the collected data were analyzed and \nreported using descriptive statistics.\nResults\nTable 1 presents the mean scores for responses to medical questions related to tuberculosis based on the NLAT-AI \ncriteria across different categories for each chatbot: ChatGPT, Copilot, and Gemini. Additionally, Fig. 1 displays \nFig. 1. Heatmap of mean scores across different categories for each chatbot based on NLAT-AI criteria.\n \nChatGPT Copilot Gemini\nDiagnostic 4 4 3.6\nDisease Management 4 3.6 4\nPrevention and control 4 3.6 4.4\nTreatment 4 4 3.8\nTable 1. Mean scores across different categories for each chatbot based on NLAT-AI criteria.\n \nScientific Reports |        (2025) 15:18004 3| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\na heatmap showing the average performance of the three chatbot models (ChatGPT, Copilot, and Gemini) across \nthe four main categories (Diagnostic, Treatment, Prevention & Control, and Disease Management).\nThe data from Table 1 and Fig. 1 demonstrate that in the Diagnostic category, all three chatbots achieved an \nidentical score of 4.0, indicating comparable and effective performance. This suggests equivalent competency in \ndiagnostic capabilities across all three models, with no significant variations observed. In the Treatment category, \nGemini showed slightly lower performance (3.8) compared to ChatGPT and Copilot (both 4.0). This discrepancy \nmay be attributed to Gemini’s relative limitations in certain domain indicators, warranting further investigation. \nIn the Prevention & Control category, Copilot demonstrated the lowest performance with a score of 3.6, while \nGemini excelled with 4.4, suggesting superior capabilities in prevention and control-related queries. Regarding \nDisease Management, ChatGPT and Gemini showed equivalent performance (4.0), while Copilot scored lower \n(3.6), indicating relatively weaker performance in disease management compared to the other models.\nTable 2 presents mean scores for diagnostic indicators across all three platforms.\nThe data presented in Table 2 illustrates the performance of chatbots in responding to queries related to \nthe diagnosis of brucellosis.  The chatbots Gemini, Copilot, and ChatGPT exhibited comparable performance, \nscoring 4 across most indices.  However, the Gemini chatbot demonstrated slightly weaker performance in the \nindices of Appropriateness and Effectiveness, with a score of 3.\nTable 3 presents the scores for the chatbots in the Treatment domain, as evaluated by the NLAT-AI criteria.\nData from Table 3, assessing chatbot performance in answering questions pertaining to brucellosis treatment, \nreveals that ChatGPT achieved a higher score in the Accuracy metric (5 out of 5) compared to both Copilot and \nGemini.  However, in the Appropriateness metric, the scores for Gemini and ChatGPT were lower than that of \nCopilot.\nTable 4 presents the scores for the chatbots in the domains of Prevention and Control, as evaluated by the \nNLAT-AI criteria.\nThe data presented in Table 4 indicates that Gemini demonstrated superior performance in the domains of \nPrevention & Control. Specifically, Gemini achieved the highest scores in the Safety (5 out of 5) and Actionability \n(5 out of 5) metrics. \nTable 5 presents the scores for the chatbots in the Disease Management domain, as evaluated by the NLAT-\nAI criteria.\nThe evaluation results of the data presented in Table 5 indicate a similar performance among ChatGPT, \nCopilot, and Gemini. All three chatbots achieved high scores (4 out of 5) on most accuracy metrics. However, \nCopilot’s performance was lower (3 out of 5) than ChatGPT and Gemini specifically in the Accuracy and \nEffectiveness indices.\nChatGPT Copilot Gemini\nAccuracy 4 3 4\nSafety 4 4 5\nAppropriateness 3 4 4\nActionability 4 4 5\nEffectiveness 4 3 4\nTable 4. Chatbot scores in prevention and control based on NLAT-AI criteria.\n \nChatGPT Copilot Gemini\nAccuracy 5 4 4\nSafety 4 4 4\nAppropriateness 3 4 3\nActionability 4 4 4\nEffectiveness 4 4 3\nTable 3. Chatbot scores on treatment domain indices based on the NLAT-AI criteria.\n \nChatGPT Copilot Gemini\nAccuracy 4 4 4\nSafety 4 4 4\nAppropriateness 4 4 3\nActionability 4 4 4\nEffectiveness 4 4 3\nTable 2. Chatbot scores on diagnostic domain indices based on the NLAT-AI criteria.\n \nScientific Reports |        (2025) 15:18004 4| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\nThe results of evaluating the three chatbots, ChatGPT, Copilot, and Gemini, in response to medical inquiries \nrelated to brucellosis, based on the DISCERN-AI criteria, are shown in Table 6.\nThe data presented in Table 6 indicate that ChatGPT performed better in the information relevance index, \nproviding more relevant responses, while Copilot and Gemini provided responses that were only partially \nrelevant.  Regarding information sources, ChatGPT did not provide any sources, whereas Copilot and Gemini \npartially cited information along with sources. This could positively impact the transparency of information \nfrom these two models. In the information production date index, none of the three models provided a date, \nwhich could limit the assessment of the responses’ timeliness. Additionally, all three models performed similarly \nin the balance and impartiality index, providing completely neutral and balanced information, indicating their \naccuracy in presenting information without bias. In the additional sources index, ChatGPT did not provide \nany additional resources, while Copilot provided limited details of additional resources, and Gemini included \nincomplete details of additional resources or related references. This indicates that all three models have \nshortcomings in providing additional sources. In the uncertainty indication index, none of the models referred \nto uncertainty in the responses, which can be considered a common weakness. Finally, the overall quality of all \nthree models was evaluated as average, indicating that although the responses provided are acceptable in some \ncases, there is still a need for improvement in certain aspects. Therefore, while ChatGPT excelled in information \nrelevance, Copilot and Gemini compensated through partial source provision. However, all three models \nshowed limitations in dating information, acknowledging uncertainty, and providing comprehensive references, \nindicating areas for potential improvement.\nDiscussion\nThe findings of the present study indicate that LLMs-based chatbots—ChatGPT, Copilot, and Gemini—\ndemonstrate comparable performance across various domains in responding to medical questions related to \ntuberculosis. However, certain language models exhibited higher or lower performance on specific indicators. \nResults revealed that ChatGPT generally outperformed Gemini and Copilot. This model achieved high \nscores across all main categories (Diagnostic, Treatment, Prevention & Control, and Disease Management), \ndemonstrating its consistency and capability in providing accurate and reliable responses. In contrast, Gemini \nand Copilot showed weaker performance in certain areas, with Copilot particularly showing the lowest \nperformance in Prevention & Control and Disease Management categories. These results suggest that ChatGPT, \nas a large language model, demonstrates relative superiority in terms of comprehensiveness and accuracy in \nresponding to medical questions.\nRecent studies evaluating the performance of LLMs-based chatbots in healthcare applications have identified \ndistinct strengths and weaknesses in these systems. For instance, Huo et al. examined the performance of chatbots \nincluding ChatGPT-4, Copilot, Google Bard, and Perplexity AI in providing recommendations for surgical \nmanagement of Gastroesophageal Reflux Disease (GERD). Results indicated that Google Bard provided the most \naccurate recommendations for both physicians and patients, with ChatGPT-4 ranking second, while Copilot and \nPerplexity AI demonstrated lower accuracy30. Another study by Masalkhi et al. compared the capabilities of large \nlanguage models, particularly Gemini AI and ChatGPT, in healthcare applications. Gemini AI showed superior \nperformance in language comprehension and multimodal processing, while ChatGPT demonstrated stronger \ncapabilities in medical knowledge, visual analysis, and providing personalized guidance31.\nCriteria ChatGPT Copilot Gemini\nRelevance of information Relevant Partially relevant Partially relevant\nInformation sources Not mentioned Partially specified Partially specified\nDate of information production Not provided Not provided Not provided\nBalance and impartiality Completely neutral and balanced Completely neutral and balanced Completely neutral and balanced\nAdditional sources Not provided Limited details on additional sources Incomplete details on additional sources\nIndication of uncertainty Not indicated Not indicated Not indicated\nOverall quality Average Average Average\nTable 6. Evaluation of chatbots in responding to medical questions related to brucellosis based on the \nDISCERN-AI criteria.\n \nChatGPT Copilot Gemini\nAccuracy 4 3 4\nSafety 4 4 4\nAppropriateness 4 4 4\nActionability 4 4 4\nEffectiveness 4 3 4\nTable 5. Chatbot scores in disease management based on NLAT-AI criteria.\n \nScientific Reports |        (2025) 15:18004 5| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\nDuran et al. evaluated the performance of various models including ChatGPT-4, Gemini, and Copilot based \non readability, clarity, and accuracy of their responses to questions related to cosmetic surgeries. Results showed \nthat ChatGPT-4 excelled in producing accurate and comprehensive medical content for patients, highlighting \nthe distinct strengths of different models in medical communications 32. The study by Reyhan et al. revealed \nthat in evaluating chatbot responses to medical questions related to keratoconus, Gemini and Copilot models \ndemonstrated better performance in terms of reliability and overall quality compared to other models, including \nChatGPT. However, ChatGPT-3.5 and ChatGPT-4.0 also showed acceptable performance, though scoring lower \nthan Gemini and Copilot in some indicators such as readability and overall quality33. Shanmugam & Browning’s \ncomparative study on the performance of LLMs in analyzing and managing complex ophthalmological cases \nshowed that ChatGPT-3.5, Claude Pro, and Copilot Pro demonstrated higher performance compared to other \nmodels34.\nThe evaluation of ChatGPT, Gemini, and Copilot’s performance in responding to medical questions based \non the Discern-AI criteria revealed that ChatGPT performed better in the Relevance of information index and \nprovided more relevant responses. However, this model showed weaknesses in providing information sources \nand production dates. Conversely, Copilot and Gemini partially compensated for this weakness by providing \nlimited details from information sources. Overall, all three models demonstrated similar performance in \nindicators such as balance and impartiality, providing unbiased and balanced information. This demonstrates \ntheir accuracy and impartiality in information delivery. However, a common weakness across all models was \ntheir failure to address uncertainty and provide information production dates, which could be considered a \nserious challenge. These weaknesses might negatively impact users’ trust in the provided responses.\nAccordingly, despite the generally acceptable performance of the models on many metrics, common \nweaknesses—such as their inability to provide the date of information generation or precise sources—can \nhave significant implications for patient care and public health. Incomplete or incorrect information may lead \nto inappropriate treatment decisions 35 or alter user or patient behavior regarding disease management and \ntreatment36. This is especially concerning in areas like tuberculosis control and prevention, where precise health \nstrategies are required. Such shortcomings may reduce patients’ trust in AI systems, and—if users or patients \nrely on inaccurate information and neglect professional medical advice—they could even result in dangerous \nconsequences. Therefore, enhancing the ability of these models to provide reliable sources, up-to-date dates, \nand clarity in cases of uncertainty is a necessary step to ensure the safety and effectiveness of their use in public \nhealth.\nSpecific recommendations for improving these models include developing mechanisms for regularly updating \nmedical information from trustworthy sources, strengthening their ability to accurately and transparently \nreference scientific sources, and designing protocols to explicitly indicate uncertainty in their responses. \nIn particular, models such as ChatGPT and Gemini could improve their evidence-based content delivery \nby integrating structured data from reputable medical databases (such as UpToDate or the WHO). Adding \nfunctionalities to report the date of information and specify the confidence level in responses could further aid \nusers in making informed decisions and increase trust in these tools. Gibson et al.'s study demonstrated that \nChatGPT-4 performs well in answering common questions about prostate cancer and can serve as a useful tool \nin patient education. Based on quality assessment tools, this model’s outputs were generally deemed reliable, \nsafe, and appropriate, with PEMAT-AI comprehensibility scores being very good and DISCERN-AI ratings \ncategorized as “good quality”28. Additionally, Hancı et al.'s study, aimed at evaluating the quality, reliability, and \nreadability of responses from Bard, Copilot, Perplexity, ChatGPT, and Gemini chatbots to patient questions \nabout \"palliative care,\" showed that responses from all five chatbots had readability levels higher than the \nrecommended level for patient educational materials, though the quality and readability of responses related to \npalliative care were insufficient across all chatbots37.\nConclusion\nThe present study demonstrates that large language models such as ChatGPT, Gemini, and Copilot possess \nsignificant potential in providing accurate and practical medical responses, although their performance varies \nacross different domains and evaluation metrics. ChatGPT generally exhibited superior performance in \ndelivering comprehensive and precise responses, while Gemini and Copilot showed certain limitations in areas \nsuch as disease prevention, control, and management. Nevertheless, all three models demonstrated comparable \nperformance in metrics such as information impartiality and balance, indicating their capability to provide \nunbiased and balanced information. However, common challenges persist, including the failure to acknowledge \nuncertainties, lack of information generation dates, and inadequate citation of credible sources, highlighting the \nneed for further development and enhancement of these technologies.\nThe findings of this study also emphasize that the selection of an appropriate model for medical applications \ndepends on specific user requirements and targeted domains. For instance, ChatGPT may be the preferred \noption for diagnostic and therapeutic inquiries, while Gemini demonstrates acceptable performance in areas \nrelated to disease prevention and control. However, to enhance user confidence and improve response quality, \ndevelopers must focus on improving transparency, incorporating credible source citations, and providing up-to-\ndate information. These advancements could facilitate the safe and effective expansion of these technologies in \nhealthcare, strengthening their role as auxiliary tools in patient education and medical decision-making.\nFinally, the findings of this study indicate that AI tools based on large language models can play a significant \nrole in supporting decision-making for both health professionals and patients. In clinical settings, chatbots \ncapable of providing accurate, balanced, and comprehensible responses can serve as valuable adjuncts for \npatient education, strengthening clinical decision-making, and promoting patient self-management. Therefore, \nchoosing an appropriate model is crucial, as relying on a poorly performing model may result in incomplete, \nincorrect, or misleading information. This not only reduces the quality of care but may also pose risks to patient \nScientific Reports |        (2025) 15:18004 6| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\nsafety. For this reason, specialized users (such as physicians) and health organizations should carefully evaluate \nthe performance of these models in specific domains, consider their informational limitations, and take into \naccount the intended application—such as education, consultation, or clinical decision support—when selecting \nand implementing such tools.\nData availability\n“The datasets generated and/or analyzed during the current study are available from the corresponding author \nupon reasonable request. ”\nReceived: 14 February 2025; Accepted: 19 May 2025\nReferences\n 1. Srivastav, S. et al. ChatGPT in radiology: the advantages and limitations of artificial intelligence for medical imaging diagnosis. \nCureus 15 (2023).\n 2. Buhr, C. R. et al. ChatGPT versus consultants: blinded evaluation on answering otorhinolaryngology case–based questions. JMIR \nMed. Educ. 9, e49183 (2023).\n 3. Tussie, C. & Starosta, A. Comparing the dental knowledge of large language models. Br. Dent. J.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 4 1 5 - 0 \n2 4 - 8 0 1 5 - 2     (2024).\n 4. Grant, N. & Metz, C. A New Chat Bot Is a’Code Red’for Google’s Search Business. International New York Times, NA-NA (2022).\n 5. Rathje, S. et al. GPT is an effective tool for multilingual psychological text analysis. (2023).\n 6. Dentella, V ., Günther, F ., Murphy, E., Marcus, G. & Leivada, E. Testing AI on language comprehension tasks reveals insensitivity to \nunderlying meaning. Sci. Rep. 14, 28083. https://doi.org/10.1038/s41598-024-79531-8 (2024).\n 7. Bongini, P ., Becattini, F . & Del Bimbo, A. in European Conference on Computer Vision. 268–281 (Springer).\n 8. Shakarian, P ., Koyyalamudi, A., Ngu, N. & Mareedu, L. An independent evaluation of ChatGPT on mathematical word problems \n(MWP). arXiv preprint arXiv:2302.13814 (2023).\n 9. Bi, K. et al. Accurate medium-range global weather forecasting with 3D neural networks. Nature 619, 533–538 (2023).\n 10. Ray, P . P . ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. \nInternet Things Cyber-Phys Syst 3, 121–154 (2023).\n 11. Y eo, Y . H. et al. Assessing the performance of ChatGPT in answering questions regarding cirrhosis and hepatocellular carcinoma. \nClin. Mol. Hepatol. 29, 721–732 (2023).\n 12. Howard, A., Hope, W . & Gerada, A. ChatGPT and antimicrobial advice: the end of the consulting infection doctor?. Lancet. Infect. \nDis 23, 405–406 (2023).\n 13. Nakhleh, A., Spitzer, S. & Shehadeh, N. ChatGPT’s response to the diabetes knowledge questionnaire: implications for diabetes \neducation. Diabetes Technol. Ther. 25, 571–573 (2023).\n 14. Brown, T. B. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).\n 15. Gilson, A. et al. How does ChatGPT perform on the United States Medical Licensing Examination (USMLE)? The implications of \nlarge language models for medical education and knowledge assessment. JMIR Med. Educ. 9, e45312 (2023).\n 16. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. \nPLoS Digit. Health 2, e0000198 (2023).\n 17. Huang, K., Altosaar, J. & Ranganath, R. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv preprint \narXiv:1904.05342 (2019).\n 18. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, \n1234–1240 (2020).\n 19. Clusmann, J. et al. The future landscape of large language models in medicine. Commun. Med. 3, 141.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 3 \n8 5 6 - 0 2 3 - 0 0 3 7 0 - 1     (2023).\n 20. Shao, C.-Y . et al. Appropriateness and comprehensiveness of using ChatGPT for perioperative patient education in thoracic \nsurgery in different language contexts: Survey study. Interact. J. Med. Res. 12, e46900. https://doi.org/10.2196/46900 (2023).\n 21. Lahat, A., Shachar, E., Avidan, B., Glicksberg, B. & Klang, E. Evaluating the utility of a large language model in answering common \npatients’ gastrointestinal health-related questions: Are we there yet?. Diagnostics 13, 1950 (2023).\n 22. Sarangi, P . K. et al. Radiologic decision-making for imaging in pulmonary embolism: accuracy and reliability of large language \nmodels—bing, claude, ChatGPT, and perplexity. Indian J. Radiol. Imaging 34, 653–660. https://doi.org/10.1055/s-0044-1787974 \n(2024).\n 23. Omar, M., Brin, D., Glicksberg, B. & Klang, E. Utilizing natural language processing and large language models in the diagnosis and \nprediction of infectious diseases: A systematic review. Am. J. Infect. Control 52, 992–1001. https://doi.org/10.1016/j.ajic.2024.03.016 \n(2024).\n 24. Zhang, S. & Song, J. A chatbot based question and answer system for the auxiliary diagnosis of chronic diseases based on large \nlanguage model. Sci. Rep. 14, 17118. https://doi.org/10.1038/s41598-024-67429-4 (2024).\n 25. Mondal, H., Dash, I., Mondal, S. & Behera, J. K. ChatGPT in answering queries related to lifestyle-related diseases and disorders. \nCureus 15, e48296. https://doi.org/10.7759/cureus.48296 (2023).\n 26. Natarajan, A., Beena, P ., Devnikar, A. V . & Mali, S. A systemic review on tuberculosis. Indian J. Tuberculosis 67, 295–311 (2020).\n 27. Meskó, B. Prompt engineering as an important emerging skill for medical professionals: Tutorial. J. Med. Internet Res. 25, e50638. \nhttps://doi.org/10.2196/50638 (2023).\n 28. Gibson, D. et al. Evaluating the efficacy of ChatGPT as a patient education tool in prostate cancer: Multimetric assessment. J. Med. \nInternet Res. 26, e55939. https://doi.org/10.2196/55939 (2024).\n 29. Siu, A. H. Y . et al. ChatGPT as a patient education tool in colorectal cancer—an in-depth assessment of efficacy, quality and \nreadability. Colorectal Dis. 27, e17267. https://doi.org/10.1111/codi.17267 (2025).\n 30. Huo, B. et al. The performance of artificial intelligence large language model-linked chatbots in surgical decision-making for \ngastroesophageal reflux disease. Surg. Endosc. 38, 2320–2330. https://doi.org/10.1007/s00464-024-10807-w (2024).\n 31. Masalkhi, M., Ong, J., Waisberg, E. & Lee, A. G. Google DeepMind’s gemini AI versus ChatGPT: a comparative analysis in \nophthalmology. Eye (Lond) 38, 1412–1417. https://doi.org/10.1038/s41433-024-02958-w (2024).\n 32. Duran, A., Cortuk, O. & Ok, B. Future perspective of risk prediction in aesthetic surgery: Is artificial intelligence reliable?. Aesthet. \nSurg. J. 44, NP839–NP849. https://doi.org/10.1093/asj/sjae140 (2024).\n 33. Reyhan, A. H., Mutaf, Ç., Uzun, İ & Yüksekyayla, F . A performance evaluation of large language models in keratoconus: A \ncomparative study of ChatGPT-35, ChatGPT-40, Gemini, Copilot, Chatsonic, and perplexity. J. Clin. Med. 13, 6512 (2024).\n 34. Shanmugam, S. K. & Browning, D. J. Comparison of large language models in diagnosis and management of challenging clinical \ncases. Clin. Ophthalmol. 18, 3239–3247. https://doi.org/10.2147/OPTH.S488232 (2024).\n 35. Thapa, D. K., Visentin, D. C., Kornhaber, R., West, S. & Cleary, M. The influence of online health information on health decisions: \nA systematic review. Patient Educ. Couns. 104, 770–784. https://doi.org/10.1016/j.pec.2020.11.016 (2021).\nScientific Reports |        (2025) 15:18004 7| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/\n 36. Bujnowska-Fedak, M. M. & Węgierek, P . The impact of online health information on patient health behaviours and making \ndecisions concerning health. Int. J. Environ. Res. Public Health 17, 880 (2020).\n 37. Hanci, V . et al. Assessment of readability, reliability, and quality of ChatGPT®, BARD®, Gemini®, Copilot®, Perplexity® responses on \npalliative care. Medicine (Baltimore) 103, e39305. https://doi.org/10.1097/md.0000000000039305 (2024).\nAuthor contributions\nM.D., J.M conceptualized, curated and analyzed data, reviewed existing literature, and drafted the first manu -\nscript. M.R. revised, edited and provided intellectual feedback. All authors read and approved the final version \nof the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.R.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:18004 8| https://doi.org/10.1038/s41598-025-03074-9\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5663675665855408
    },
    {
      "name": "Tuberculosis",
      "score": 0.5599991083145142
    },
    {
      "name": "Data science",
      "score": 0.3451496958732605
    },
    {
      "name": "Medicine",
      "score": 0.25730758905410767
    },
    {
      "name": "Pathology",
      "score": 0.128260999917984
    }
  ]
}