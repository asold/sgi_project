{
    "title": "A Comparative Study of Pretrained Language Models on Thai Social Text Categorization",
    "url": "https://openalex.org/W2993960237",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2967801838",
            "name": "Thanapapas Horsuwan",
            "affiliations": [
                "Chulalongkorn University"
            ]
        },
        {
            "id": "https://openalex.org/A2994285453",
            "name": "Kasidis Kanwatchara",
            "affiliations": [
                "Chulalongkorn University"
            ]
        },
        {
            "id": "https://openalex.org/A228581428",
            "name": "Peerapon Vateekul",
            "affiliations": [
                "Chulalongkorn University"
            ]
        },
        {
            "id": "https://openalex.org/A322444122",
            "name": "Boonserm Kijsirikul",
            "affiliations": [
                "Chulalongkorn University"
            ]
        },
        {
            "id": "https://openalex.org/A2967801838",
            "name": "Thanapapas Horsuwan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2994285453",
            "name": "Kasidis Kanwatchara",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A228581428",
            "name": "Peerapon Vateekul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A322444122",
            "name": "Boonserm Kijsirikul",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2936329694",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2964308564"
    ],
    "abstract": null,
    "full_text": "arXiv:1912.01580v2  [cs.LG]  17 Dec 2019\nA Comparative Study of Pretrained Language\nModels on Thai Social Text Categorization\nThanapapas Horsuwan 1, Kasidis Kanwatchara 1,\nPeerapon Vateekul 1, and Boonserm Kijsirikul 1\nDepartment of Computer Engineering, Faculty of Engineerin g,\nChulalongkorn University, Bangkok, Thailand\n{thanapapas.h,kanwatchara.k}@gmail.com\n{peerapon.v,boonserm.k}@chula.ac.th\nAbstract. The ever-growing volume of data of user-generated content\non social media provides a nearly unlimited corpus of unlabe led data\neven in languages where resources are scarce. In this paper, we demon-\nstrate that state-of-the-art results on two Thai social tex t categorization\ntasks can be realized by pretraining a language model on a lar ge noisy\nThai social media corpus of over 1.26 billion tokens and late r ﬁne-tuned\non the downstream classiﬁcation tasks. Due to the linguisti cally noisy\nand domain-speciﬁc nature of the content, our unique data pr eprocess-\ning steps designed for Thai social media were utilized to eas e the training\ncomprehension of the model. We compared four modern languag e mod-\nels: ULMFiT, ELMo with biLSTM, OpenAI GPT, and BERT. We sys-\ntematically compared the models across diﬀerent dimension s including\nspeed of pretraining and ﬁne-tuning, perplexity, downstre am classiﬁca-\ntion benchmarks, and performance in limited pretraining da ta.\nKeywords: language model ·pretraining ·Thai social media ·compar-\native study · data preprocessing\n1 Introduction\nSocial networks are active platforms rich with a quickly accessible clim ate of\nopinion and community sentiment regarding various trending topics. The growth\nof the online lifestyle is observed by the bustling active communication on social\nmedia platforms. Opinion-oriented information gathering systems a im to extract\ninsights on diﬀerent topics, which have numerous applications from b usinesses to\nsocial sciences. Nevertheless, existing NLP researches on utilizing these abound-\ning noisy user-generated content have been limited despite its pote ntial value.\nFirst introduced in [7], pretrained language models (LMs) have been a topic\nof interest in the NLP community. This interest has been coupled with works\nreporting state-of-the-art results on a diverse set of tasks in N LP. In light of the\nnotable beneﬁts of transfer learning, we chose to compare four r enowned LMs:\nULMFiT [9], ELMo with biLSTM [14], OpenAI GPT [16], and BERT [8]. To\nthe best of our knowledge, our work is the ﬁrst comparative study conducted\n2 T. Horsuwan et al.\non pretrained LMs in Thai language. Our LMs were trained in a three- stage\nprocess as per suggested in [9]: LM pretraining, LM ﬁne-tuning, an d classiﬁer\nﬁne-tuning. The goal of unsupervised pretraining is to ﬁnd a good in itialization\npoint to capture the various general meaningful aspects of a lang uage. Beﬁtting\nThai language with resource scarcity, we expect that pretraining user-generated\ncontent would serve as a solid basis for transfer learning to downst ream tasks.\nPantip is the largest Thai internet forum with a huge active communit y where\na diverse range of topics are discussed. The variability of surplus ex amples from\nPantip covers the basic linguistic syntax of Thai language while mainta ining\nthe colloquial and noisy nature of online user-generated content. In this paper,\nwe investigate and compare the capability of each LM to capture the relevant\nfeatures of a domain-speciﬁc language via pretraining copious unlab eled data\nfrom user-generated content.\nThe main contributions of this paper are the following:\n– We developed unique data preprocessing techniques for Thai socia l media.\n– We pretrained ULMFiT, ELMo, GPT, and BERT on a noisy Thai social\nmedia corpus much larger than the existing Thai Wikipedia Dump.\n– We compared the language models across diﬀerent dimensions includin g\nspeed of pretraining and ﬁne-tuning, perplexity, downstream clas siﬁcation\nbenchmarks, and performance in limited pretraining data.\n– Our pretrained models and code can be obtained upon request to th e corre-\nsponding authors\nThis paper is organized as follows. Our data preprocessing techniqu es are\nexplained in Section 2 and the LMs used for pretraining are brieﬂy des cribed\nin Section 3. The datasets used in this paper are described in Section 4 and\nSection 5 explains our hyperparameters and evaluation metrics. Th e results are\nreported in Section 6 and ﬁnally concluded in Section 7.\n2 Our Data Preprocessing for Thai Social Media\nData preprocessing is one of the most important phases in improving the learn-\ning comprehension of the LMs. If much irrelevant and redundant inf ormation\nintroduces unwanted noise in the training corpus, it is diﬃcult for the models to\ndiscover knowledge during the training phase. This is especially true f or unﬁl-\ntered data from user-generated content on social media, where it requires speciﬁc\nmethods of data preprocessing unique to the domain.\nThe Thai webboard Pantip allows members to freely create threads as long\nas it conforms to a list of actively regulated etiquette. The colloquial nature\nof the data posted allows for huge amounts of noise to be introduce d in the\ndata, such as ASCII arts, language corruption (ภาษาวิบัติ), irregular spacing,\nmisspelling, character repetition, and spams [10]. The unpredictable noise in the\ndata substantially increases the vagueness of word boundary, wh ich already is\na problem in formal Thai language [4]. Additionally, Thai word segment ation is\ndependent on context. A famous example is the compound word ‘ตากลม’, which\nComparative Study of Pretrained Language Models 3\ncan be either split into ‘[ตา][กลม]’ or ‘[ตาก][ลม]’. Both are grammatically\ncorrect when used within their corresponding context. To ease th e impact of the\nissues, the data preprocessing approaches we employed are as fo llows:\n1. Length Filtering To select meaningful threads to the LM, threads with a\ntitle and with a body of more than 100 characters were selected.\n2. Language Filtering An n-gram-based text categorization library langde-\ntect1 was used to ﬁlter out the threads that are not labeled as Thai langu age.\n3. General Preprocessing before Tokenization Inspired by [9,3], the tech-\nniques include ﬁxing HTML tags, removing duplicate spaces and newline s,\nremoving empty brackets, and adding spaces around ‘/’ and ’#’. In a ddi-\ntion, character order in Thai language may be typed in a diﬀerent se quence\nbut visually rendered in the same way. This is due to the fact that vow els,\ndiphthongs, tonal marks, and diacritics may sometimes be physically located\nabove or below the base glyph–allowing diﬀerent sequential orders t o appear\nvisually equivalent. Thus, normalizing the character order is require d for the\nmachine to understand the seemingly similar tokens.\n4. Customized Preprocessing before Tokenization We also developed\nand customized techniques suitable for Thai social media. Last cha racter\nrepetition is a common behavior of Thai people analogous to prolongin g\nthe vowel sounds of a word in spoken language to emphasize certain emo-\ntions. We truncate the word and follow it by a special token. pyThaiN LP\n[15] adopted a similar technique but we implemented minor modiﬁcations of\nspace addition following the token for better tokenization results. Likewise,\na special token is used for word repetitions similar to [9] preprocess ing tech-\nnique, which at the time this technique has not been widely used in Thai\nlanguage preprocessing. Since Thai is a language without word boun daries,\nour algorithm recognizes words as any character sequence of mor e than 2\ncharacters with more than 2 repetitions of that sequence. All typ es of repe-\ntitions are truncated to 5 as it provides no higher emotional impact a nd to\nlimit the vocabulary size.\nIn addition, we also propose 2 new preprocessing methods: a specia l token\nfor any numeric strings and a special token for laughing expression s.\nWe replaced all strings related to numbers with a special token: gen eral\nnumbers, masked and unmasked phone numbers, Thai numbers, d ate and\ntime, masked prices, and numbers of special forms. Although diﬀer entiating\nthe numbers provide some semantic value, the sparsity of the infor mation\nwould most likely make these numbers tail out of vocabulary (OOV) to kens.\nWe believe that this preprocessing method would allow the language mo dels\nto more generally understand how numbers are used in text.\nIn an online environment, Thai people often express laughter in writ ten\nlanguage with an onomatopoeia, utilizing the repetition of ‘5’ followed b y an\noptional ‘+’. This is due to the fact that the Thai pronunciation of ‘5’ is ‘ha’.\nWe replaced all tokens with more than 3 consecutive ‘5’ and an option al ‘+’\nwith a special laugh token. Although this may have a minor eﬀect on ac tual\n1 github.com/fedelopez77/langdetect\n4 T. Horsuwan et al.\nnumbers, this onomatopoeia is very commonly used in Thai online cont ext\nand it is important for the model to learn this special token. An exam ple is\nprovided in Table 1 for clariﬁcation.\nTable 1. An example of our preprocessing method. [CREP] and [LAUGH] are special\ntokens used for character repetition and laughing respecti vely.\nBefore After\nThai ฉันชอบมันมากกกก555555+ ฉันชอบมันมาก [CREP] 4 [LAUGH]\nTranslated I like it a lotttt hahahahaha 1 I like it a lot [CREP] 4 [LAUGH]\n1 ‘5’ is pronounced ‘ha’ in Thai\n5. Tokenization We used the pyThaiNLP [15] default tokenizer, which is a\ndictionary-based Maximum Matching with Thai Character Cluster. H ow-\never, we created our own aggregated dictionary for tokenization to improve\nthe tokenization accuracy for colloquial user-generated conten t. The dictio-\nnary2 is compiled from various sources of data, including general words, a b-\nbreviations, transliterations, named entities, and self-annotate d Thai slangs\nand commonly used corrupted language. This includes word variants like ฮะ\nฮาฟ ฮับ ฮัฟ คร/uni0E49.lowาบ ค/uni0E49.lowาฟ ค/uni0E49.lowาบ คับ ครัชwhich are all word variants of the suﬃx to\nindicate formality ครับ. The vocabulary is built from the most common 80k\ntokens.\n6. General Preprocessing after Tokenization: Following [9] and [3], some\ngeneral preprocessing techniques after tokenization were used . This includes\nungrouping the emoji’s from text, and to lowercase all English words .\n7. Spelling Correction In an eﬀort to reduce the number of unnecessary\ntokens sprouting from incorrectly spelled words, we compiled a list of com-\nmonly misspelled word mappings aggregated from various sources. W e cor-\nrected and standardized the vocabulary used. This is an important task due\nto the free and lax nature of the corpus, where a single word may be repre-\nsented in diﬀerent variants or misspelled and abbreviated into variou s tokens.\nNote that not all replacements can be made due to the collision of act ual\nvocabularies and the limited comprehensiveness of the list.\n3 Pretrained Language Models in Our Study\n3.1 Universal Language Model Fine-tuning (ULMFiT)\nA single model architecture that is used for both LM pretraining and down-\nstream ﬁne-tuning was ﬁrst introduced in ULMFiT [9]. This allows the we ights\nlearnt during pretraining to be reused instead of constructing a ne w task-speciﬁc\nmodel. Howard and Ruder suggested that LM overﬁts to small data sets and suf-\nfers catastrophic forgetting when directly ﬁne-tuned to a classiﬁ er. Hence, the\n2 The dictionary is referenced at our GitHub https://github.com/Knight-H/thai-lm\nComparative Study of Pretrained Language Models 5\nULMFiT approach was proposed to attempt to eﬀectively ﬁne-tune the AWD-\nLSTM [11] model. ULMFiT is a 3-stage training method consisting of LM p re-\ntraining, LM ﬁne-tuning, and classiﬁer ﬁne-tuning. They also propo sed novel\ntechniques such as discriminative ﬁne-tuning, gradual unfreezing , and slanted\ntriangular learning rates for stable ﬁne-tuning.\n3.2 Embeddings from Language Models (ELMo)\nTraditional monolithic word embeddings such as word2vec [12] and G loVe [13]\nfails to model context-dependent meanings of a word. Hence, ELM o [14] pro-\nduces contextualized word embeddings by utilizing a pretrained biLM a s a ﬁxed\nfeature extractor and incorporate its embedding representatio n as features into\nanother task-speciﬁc model for downstream tasks. The author s suggested that\ncombining the internal states of the LSTM layers allows for rich cont extualized\nword representations on top of the original context-independen t word embed-\ndings.\n3.3 Generative Pretrained Transformer (GPT)\nSequential computation models used in sequence transduction pro blems [5,6,17]\nforbid parallelization in the training examples. The transformer [18] is the ﬁrst\ntransduction model based solely on self-attention to draw global d ependencies\nbetween input and output, eliminating the use of recurrence and co nvolutions.\nOpenAI introduced GPT [16] by extending the idea to multi-layer tran sformer\ndecoder for language modeling. Additionally, LM ﬁne-tuning and class iﬁer ﬁne-\ntuning are done simultaneously by using LM as an auxiliary objective. T he\nauthors suggested that this improves the generalization of the su pervised model\nand accelerates convergence.\n3.4 Bidirectional Encoder Representations from Transform ers\n(BERT)\nULMFiT [9] and GPT [16] use a unidirectional forward architecture wh ile ELMo\n[14] uses a shallow concatenation of independently trained forward and back-\nward LMs. With criticism on the standard unidirectional LMs as subop timal\nby severely restricting the power of pretrained representations , BERT [8] was\nproposed as a multi-layer transformer encoder designed to pretr ain deep bidirec-\ntional representations by jointly conditioning on both left and right context in\nall layers. Since the standard autoregressive LM pretraining meth od is not suit-\nable for bidirectional contexts, BERT is trained on masked language modeling\n(MLM) and next sentence prediction (NSP) tasks. MLM masks 15% o f the input\nsequences at random and the task is to predict those masked toke ns, requiring\nmore pretraining steps for the model to converge. The output of the special ﬁrst\ntoken is used to compute a standard softmax for classiﬁcation tas ks.\n6 T. Horsuwan et al.\n4 Dataset\n4.1 Pretraining Dataset\nTo collect our Thai social media corpus data, we extracted non-se nsitive infor-\nmation from all threads from Pantip.com since 1 st January 2013 up until 9 th\nFebruary 2019 using our implementation of the Scrapy Framework [2 ]. A total\nof 8 ,150,965 threads were extracted. As discussed in Section 2, data prep rocess-\ning techniques are applied to the corpus. Length ﬁltering and langua ge ﬁltering\nﬁltered down the threads to 5 ,524,831 and 5 ,487,568 respectively. After prepro-\ncessing, tokenization, and postprocessing the data, we divided ou r pretraining\ndataset into 3 parts: 5 ,087,568 threads for training, 200 ,000 threads for vali-\ndation, and 200 ,000 threads for testing. The train dataset, validation dataset,\nand test dataset has a total of 1 ,262,302,083 tokens, 4 ,701,322 tokens, and\n4,588,245 tokens respectively. By comparison, our pretrain dataset is m ore than\n31 times larger than the Thai Wikipedia Dump with respect number of t okens,\nwhich is only on the order of 40M tokens for the training set.\n4.2 Benchmarking Dataset\nTwo Thai social text classiﬁcation tasks were chosen to benchmar k the models for\nextrinsic model evaluation as shown in Table 2. Since both are originally Kaggle\ncompetitions, the Kaggle evaluation server will be used for benchma rking.\nWongnai Challenge: Rating Review Prediction First initiated as a Kaggle\ncompetition, the Wongnai Challenge is to create a multi-class classiﬁc ation sen-\ntiment prediction model from textual reviews. As an emerging online platform in\nThailand, Wongnai holds a large user base of over 2 million registered u sers with\na surplus of user-written reviews accompanied by a rating score ra nging from 1\nto 5 stars. This is challenging due to the varying user standards, co rresponding\nto shifting weighted importance of each sentiment in mixed reviews.\nWisesight Sentiment Analysis The Wisesight Sentiment Analysis is a private\nKaggle competition where the task is to perform a multi-class classiﬁc ation on\n4 categories: positive, negative, neutral, and question. Wisesight , a social data\nanalytics service provider, provides data from various social media sources with\nvarious topics on current internet trends. It should be noted tha t the topics and\nthe source of the data are much more diverse than that of Wongna i.\n5 Experimental Setup\n5.1 Implementation Details\nULMFiT We used the same model hyperparameters as the popular Thai\nGitHub repository thai2ﬁt [3]: the base model is a 4-layer AWD-LSTM with\n1,550 hidden activation units per layer and an embedding size of 400. A B PTT\nbatch size of 70 was used. We applied dropout of 0.25 to output layer s, 0.1 to\nRNN layers, 0.2 to input embedding layers, 0.02 to embedding layers, a nd weight\ndropout of 0.15 to the RNN hidden-to-hidden matrix.\nComparative Study of Pretrained Language Models 7\nTable 2. Datasets, tasks, number of classes, train and test examples , and the average\nexample length measured in tokens. The OOV rate is measured w ith respect to the\noriginal vocabulary of the pretraining corpus.\nDataset Task Classes Train Test OOV Average Length\nWongnai Sentiment Classiﬁcation 5 40k 6.2k 0 .71% 126 ± 124\nWisesight Sentiment Classiﬁcation 4 26.7k 3.9k 2 .69% 27 ± 44\nELMo We used the same biLM architecture from the original implementation\n[14] with all default hyperparameters, where the LM is a 2-layer biLS TM with\n4096 units and 512 dimension projections with another static chara cter-based\nrepresentations layer with convolutional ﬁlters. For both downst ream tasks, a\n3-layer biLSTM was used with 256 hidden units as the task-speciﬁc mo del.\nGPT Default conﬁgurations of [16] were used. The resulting model has 1 2 layers\nof transformer each with 12 self-attention heads and 768-dimens ional states. We\nused learnt position embeddings and a maximum sequence length of 25 6 tokens.\nBERT We used the publicly available BERTBASE unnormalized multilingual\ncased model, which has a hidden size of 768, 12 self-attention heads , and 12\ntransformer blocks. Note that the BERTBASE was chosen to have identical\nhyperparameters as GPT for comparative purposes.\n5.2 Evaluation Metrics\nA total of 4 tasks were evaluated: the proposed data preproces sing technique in\nSection 2, LM pretraining, LM ﬁne-tuning, and classiﬁer ﬁne-tuning . We chose\nto benchmark on the easiness to train each model (speed and numb er of epochs),\nthe intrinsic evaluations (perplexity), and the extrinsic evaluations (downstream\nclassiﬁcation tasks). In addition, an ablation study of limited corpus data is\ncompared to see the performance of each model in smaller data sce narios.\nData Preprocessing To benchmark the quality of our unique data prepro-\ncessing techniques for Thai social media corpus, we sampled a thre ad from each\ndataset and request expert Thai native speakers to help tokeniz e the samples.\nAt the time of writing, there is no standard corpus for benchmarkin g the task\nof colloquial Thai word segmentation. Each character in the threa d is labeled\nas 1 (beginning of word) or 0 (intra-word character). The precisio n, recall, and\nF1 score is calculated based on the performance of segmenting eac h character,\nwhere true positives are the correctly segmented beginning of wor d. The default\npyThaiNLP tokenizer [15] Maximum Matching (newmm) is compared bet ween\nwith and without our data preprocessing methods. Unfortunately , labeling to-\nkenization dataset in Thai language requires large amount of eﬀort . Therefore,\nmore extensive experiments will be conducted in the future.\n8 T. Horsuwan et al.\nLanguage Model Pretraining Pretraining a language model is the most ex-\npensive process in the transfer learning workﬂow. This task is gene rally per-\nformed only once before ﬁne-tuning on a target task. With minimal h yperpa-\nrameter tuning, we evaluated the pretraining process on: (1) the speed of training\nin each epoch and (2) the intrinsic perplexity value. Although with the ambiguity\nthat comes with intrinsic metrics, perplexity is one of the traditional methods\nin LM evaluation. It measures the conﬁdence of the model on the ob served se-\nquence via exponentiation of the cross-entropy loss, where cros s-entropy loss is\ndeﬁned as the negative sum of the mean LM log-likelihood. Note that t his deﬁ-\nnition applies to diﬀerent levels of granularity. Due to resource cons traints, each\nmodel was pretrained for a ﬁxed number of epochs. An NVIDIA P60 00 is used\nto pretrain each model, and the appropriate batch size was selecte d such that\nit maximizes the GPU VRAM of 24 GB. The models were trained for 3 epoc hs\nand the best performing model was selected. However, since BERT trains using\nMLM and is able to learn just 15% of the corpus during 1 epoch, we dec ided to\ntrain for the standard 1 million steps [8] (equivalent to around 6.5 epo chs).\nLanguage Model Fine-tuning Each model was benchmarked on the number\nof epochs used and the total time until convergence. This proces s aims to learn\nthe slight diﬀerences in data distribution of the target corpus. The models overﬁt\neasily due to the modest size of the corpus, thus each LM was ﬁne-t uned until\nearly stopping.\nClassiﬁer Fine-tuning In this paper, we reported each downstream task per-\nformance following the metric used in each Kaggle competition. Wongn ai Rating\nReview Challenge and Wisesight Sentiment Analysis both use classiﬁcat ion accu-\nracy for evaluation, which is calculated by the proportion of correc tly classiﬁed\nsamples out of all the samples. Kaggle ranks the competitors’ ﬁnal standings\nwith the private score, hence this will be used as the benchmark.\n6 Results\nIn this section, we ﬁrst report the results of our unique preproce ssing methods,\nfollowed by the results of pretraining the data. We then compare th e results of\nULMFiT, ELMo, GPT, and BERT with the previous state-of-the-art models in\nthe Thai NLP research community from the Kaggle competition benc hmarks.\n6.1 Data Preprocessing\nResults are shown in Table 3, where our preprocessing method allows the de-\nfault pyThaiNLP maximum matching (MM) tokenizer to more precisely s egment\nnoisy social media data. This is due to the lower false positive tokens s egmented\nby the noisiness of the data, where most of the spams and repetitio ns are prepro-\ncessed correctly. With more comprehensive vocabulary, it allows th e tokenizer to\nsegment short colloquial words more accurately. Note that this do es not account\nComparative Study of Pretrained Language Models 9\nfor the supposed increased comprehension of the models from sta ndardizing the\ndata.\nTable 3. Tokenization Precision, Recall, and F1-score\nTokenizer Precision Recall F1-score\nMM+Our Preprocessing 95.83% 98.65% 97.22%\nMM 96 .04% 97.39% 96.71%\n6.2 Language Model Pretraining\nFrom Table 4, AWD-LSTM with ULMFiT requires the least amount of time\nper epoch and the least total time, 100 hours and 33 hours respec tively. Due to\nresource scheduling limitations, ELMo is trained with 2 P6000 GPUs, ma king\nthe total time and the time per epoch much lower than the supposed value. With\ncharacter-level convolutions and character-based operations , ELMo training time\nshould be the longest amongst all the LMs. Transformer-based mo dels require\ntime around more than 1.5x of ULMFiT.\nTable 4. Model Pretraining Time. tepoch is the time used per epoch.\nModel tepoch\nULMFiT 33 hr\nbiLM(ELMo) (2 GPU) 52 hr\nGPT seqmax = 256 55 hr\nBERT seqmax = 256 49 hr\nThe training loss and perplexity are shown in Table 5. BERT has the lowe st\nword-level cross-entropy loss with 15 .3857 MLM perplexity. This is expected due\nto the diﬀerence of the MLM prediction task with fully visible beginning a nd\nending context, providing more contextual information to predict the masked\nword as compared with traditional forward and backward models. I n the domain\nof traditional autoregressive models, GPT has a lower perplexity th an ULMFiT.\nELMo is not compared to other models due to prediction granularity d iﬀerence\nand is reported as is.\n6.3 Language Model Fine-tuning\nAll the language models are ﬁne-tuned with the target corpus until they give\nthe best result with respect to the validation loss. An NVIDIA P6000 is used\nfor each model and the time required is presented in Table 6. Transf ormer-based\nmodels are shown to overﬁt quicker than LSTM-based models.\n10 T. Horsuwan et al.\nTable 5. Training Loss and Perplexity After Pretraining\nModel Loss Perplexity\nULMFiT 3 .5281 34 .0603\nGPT seqmax = 256 3 .1735 23 .8913\nBERT MLM seqmax = 256 2.7334 15 .3857\nbiLM(ELMo) (Character-Level) 1 .7140 5 .5512\nTable 6.Language Model Fine-tuning Time. ttotal is the total time used and tepoch is\nthe time used per epoch.\nModel Wisesight Wongnai\n#Epoch ttotal tepoch #Epoch ttotal tepoch\nULMFiT 11 11 min 1 min 11 99 min 9 min\nbiLM(ELMo) 5 25 min 5 min 2 64 min 32 min\nGPT seqmax = 256 3 57 min 19 min 3 90 min 30 min\nBERT seqmax = 256 3 36 min 12 min 2 38 min 19 min\n6.4 Classiﬁer Fine-tuning\nThe results of the downstream classiﬁcation tasks are shown in Tab le 7. BERT\nwith our pretraining data outperforms all existing models on the priv ate set of\nWongnai and Wisesight and obtains 0.9% and 3.2% respective absolute accuracy\nimprovement over the state-of-the-art. Absolute accuracy imp rovements on all\nmodels and tasks are obtained when pretrained with our Thai Social Media data\ninstead of the Thai Wiki Dump.\n6.5 Limited Pretraining Corpus\nWe also investigated the performance of the models in the scenario w here the\npretraining corpus is limited. This result reﬂects the learning ability of the mod-\nels in a language where training data is scarce. We randomly sampled a t otal of\n40M tokens (equivalent to around 234K threads) from the datase t used in our\nprevious experiments. ULMFiT, ELMo, and GPT are trained for 3 epo chs while\nBERT is trained for 30k steps (equivalent to approximately 6.5 epoch s on this\ndata). Table 8 shows that ULMFiT and GPT perform considerably well. On the\nother hand, adding ELMo to LSTM input shows little improvement. This means\nthat ELMo requires a larger corpus to be eﬀective. Although BERT p erforms\nwell on the Wisesight dataset, it has a drop in performance on Wongn ai dataset.\n7 Conclusion\nOur work shows that by using our unique data preprocessing metho ds and our\npretraining social media data, we can improve the performance of t he LMs in\nthe downstream tasks. The improvement of all models from pretra ining data\nComparative Study of Pretrained Language Models 11\nTable 7.Classiﬁer Fine-tuning Results. Our models are compared to o ther models: the\nbaseline that predicts the most frequent label, the latest K aggle competition winner,\nand public github repositories. The public leaderboard and private leaderboard are\ncalculated with approximately 30% and 70% of the test data re spectively.\nModel Wisesight (Acc.) Wongnai (Acc.)\nPrivate Public Private Public\nBaseline 0.5809 0 .6044 0.4785 0 .4785\nKaggle Best 0.7597 0 .7532 0.5914 0 .5814\nfastText [3] 0.6131 0 .6314 0.5145 0 .5109\nLinearSVC [3] - - 0.5022 0 .4976\nLogistic Regression [3] 0.7499 0 .7278 - -\nThai Wiki Dump Pretraining\nULMFiT [3]\n0.7419 0 .7126 0.5931 0 .6032\nULMFiT Semi-supervised [3] 0.7597 0 .7337 - -\nBERT seqmax = 128 [1] - - 0.5661 0 .5706\nOurs (Thai Social Media Pretraining)\nULMFiT\n0.7586 0 .7346 0.6203 0.6409\nbiLSTM 0.6366 0 .6213 0.4773 0 .4946\nELMo+biLSTM 0.6866 0 .6450 0.5310 0 .5226\nGPT seqmax = 256 0.7669 0.7540 0.6088 0 .6145\nBERT seqmax = 256 0.7691 0.7439 0.6251 0.6231\nTable 8. Limited Pretraining Corpus Results. The public and private scores are cal-\nculated with approximately 30% and 70% of the test data respe ctively.\nModel Wisesight (Acc.) Wongnai (Acc.)\nPrivate Public Private Public\nULMFiT 0.7358 0 .7143 0.5984 0.6290\nbiLSTM 0.6366 0 .6213 0.4773 0 .4946\nbiLSTM + ELMo 0.6489 0 .6095 0.4879 0 .4753\nGPT seqmax = 256 0.6931 0 .7075 0.6111 0.6102\nBERT seqmax = 256 0.7467 0 .7244 0.5650 0 .5516\nof the same domain suggests that pretraining data has a signiﬁcant impact on\nLM performance. Moreover, the possibility for LM pretraining on a n oisy corpus\nshows the ability of the models to learn in spite of the quality of the dat a.\nResults-wise, BERT is the best performing model with respect to cla ssiﬁca-\ntion accuracy. It can achieve state-of-the-art results on both of the benchmarking\ndownstream tasks. However, it has unstable performance on dow nstream tasks\nwhen pretrained on a small corpus and uses a lot of pretraining time. If speed and\nease of training are the main considerations, we recommend using AW D-LSTM\nwith ULMFiT due to its speed of pretraining and ﬁne-tuning, while the r esults\nare still on par with transformer-based models. Although OpenAI G PT shows\npromising results with acceptable pretraining speed, it is overshado wed by other\nmodels in both aspects. Finally, although ELMo shows signiﬁcant impro vements\n12 T. Horsuwan et al.\nwhen compared with the baseline biLSTM, it places a dependency on de signing\na powerful task-speciﬁc model to achieve good performance.\nAcknowledgements\nIn the making of the paper, the authors would like to acknowledge Mr . Can\nUdomcharoenchaikit for his continuous and insightful research su ggestions until\nthe completion of this paper.\nReferences\n1. Bert-th. https://github.com/ThAIKeras/bert (2019)\n2. scrapy. https://github.com/scrapy/scrapy (2019)\n3. thai2ﬁt. https://github.com/cstorm125/thai2fit (2019)\n4. Aroonmanakun, W.: Thoughts on word and sentence segmenta tion in thai (2007)\n5. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine transla tion by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (20 14)\n6. Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning Phrase Representations using RNN E ncoder-Decoder for\nStatistical Machine Translation. arXiv e-prints arXiv:14 06.1078 (Jun 2014)\n7. Dai, A.M., Le, Q.V.: Semi-supervised Sequence Learning. arXiv e-prints\narXiv:1511.01432 (Nov 2015)\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre -training of\nDeep Bidirectional Transformers for Language Understandi ng. arXiv e-prints\narXiv:1810.04805 (Oct 2018)\n9. Howard, J., Ruder, S.: Universal Language Model Fine-tun ing for Text Classiﬁca-\ntion. arXiv e-prints arXiv:1801.06146 (Jan 2018)\n10. Lertpiya, A., Chaiwachirasak, T., Maharattanamalai, N ., Lapjaturapit, T.,\nChalothorn, T., Tirasaroj, N., Chuangsuwanich, E.: A preli minary study on fun-\ndamental thai nlp tasks for user-generated web content. In: 2018 International\nJoint Symposium on Artiﬁcial Intelligence and Natural Lang uage Processing (iSAI-\nNLP). pp. 1–8 (Nov 2018). https://doi.org/10.1109/iSAI-N LP.2018.8692946\n11. Merity, S., Shirish Keskar, N., Socher, R.: Regularizin g and Optimizing LSTM\nLanguage Models. arXiv e-prints arXiv:1708.02182 (Aug 201 7)\n12. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient Est imation of Word Rep-\nresentations in Vector Space. arXiv e-prints arXiv:1301.3 781 (Jan 2013)\n13. Pennington, J., Socher, R., Manning, C.D.: Glove: Globa l vectors for word repre-\nsentation. In: Empirical Methods in Natural Language Proce ssing (EMNLP). p.\n1532–1543 (2014)\n14. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark , C., Lee, K.,\nZettlemoyer, L.: Deep contextualized word representation s. arXiv e-prints\narXiv:1802.05365 (Feb 2018)\n15. Pythainlp 2.0. https://github.com/PyThaiNLP/pythainlp (2019)\n16. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I .: Improving language un-\nderstanding by generative pre-training (2018)\n17. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to Sequen ce Learning with Neural\nNetworks. arXiv e-prints arXiv:1409.3215 (Sep 2014)\n18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jon es, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention Is All You Need. arXiv e-print s arXiv:1706.03762 (Jun\n2017)"
}