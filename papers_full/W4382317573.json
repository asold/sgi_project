{
  "title": "CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models",
  "url": "https://openalex.org/W4382317573",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2740914310",
      "name": "Akshita Jha",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2100435683",
      "name": "Chandan K. Reddy",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2740914310",
      "name": "Akshita Jha",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2100435683",
      "name": "Chandan K. Reddy",
      "affiliations": [
        "Virginia Tech"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134616970",
    "https://openalex.org/W2798966449",
    "https://openalex.org/W3199249334",
    "https://openalex.org/W6777615778",
    "https://openalex.org/W4283032596",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2783097478",
    "https://openalex.org/W3014153848",
    "https://openalex.org/W6773851625",
    "https://openalex.org/W6680401857",
    "https://openalex.org/W6763177845",
    "https://openalex.org/W2971970905",
    "https://openalex.org/W3211962263",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W3018458867",
    "https://openalex.org/W2056760934",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2113610359",
    "https://openalex.org/W2905463374",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2805757971",
    "https://openalex.org/W6809595091",
    "https://openalex.org/W6769216610",
    "https://openalex.org/W2997451752",
    "https://openalex.org/W4283751459",
    "https://openalex.org/W3190338376",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W4247880210",
    "https://openalex.org/W3165784750",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W4308731473",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4389438938",
    "https://openalex.org/W2810611310",
    "https://openalex.org/W4220722393",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W3109966548",
    "https://openalex.org/W3025298766",
    "https://openalex.org/W4210499321",
    "https://openalex.org/W4287328196",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W3004658838",
    "https://openalex.org/W3013371788",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W4287665430",
    "https://openalex.org/W2996851481"
  ],
  "abstract": "Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., primarily concerned with the human understanding of code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. Code Attack outperforms state-of-the-art adversarial NLP attack models to achieve the best overall drop in performance while being more efficient, imperceptible, consistent, and fluent. The code can be found at https://github.com/reddy-lab-code-research/CodeAttack.",
  "full_text": "CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming\nLanguage Models\nAkshita Jha and Chandan K. Reddy\nDepartment of Computer Science, Virginia Tech, Arlington V A - 22203.\nakshitajha@vt.edu, reddy@cs.vt.edu\nAbstract\nPre-trained programming language (PL) models (such as\nCodeT5, CodeBERT, GraphCodeBERT, etc.,) have the po-\ntential to automate software engineering tasks involving code\nunderstanding and code generation. However, these models\noperate in the natural channel of code, i.e., they are primarily\nconcerned with the human understanding of the code. They\nare not robust to changes in the input and thus, are poten-\ntially susceptible to adversarial attacks in the natural chan-\nnel. We propose, CodeAttack, a simple yet effective black-\nbox attack model that uses code structure to generate ef-\nfective, efficient, and imperceptible adversarial code samples\nand demonstrates the vulnerabilities of the state-of-the-art PL\nmodels to code-specific adversarial attacks. We evaluate the\ntransferability of CodeAttack on several code-code (transla-\ntion and repair) and code-NL (summarization) tasks across\ndifferent programming languages. CodeAttack outperforms\nstate-of-the-art adversarial NLP attack models to achieve the\nbest overall drop in performance while being more efficient,\nimperceptible, consistent, and fluent. The code can be found\nat https://github.com/reddy-lab-code-research/CodeAttack.\nIntroduction\nThere has been a recent surge in the development of gen-\neral purpose programming language (PL) models (Ahmad\net al. 2021; Feng et al. 2020; Guo et al. 2020; Tipirneni,\nZhu, and Reddy 2022; Wang et al. 2021). They can cap-\nture the relationship between natural language and source\ncode, and potentially automate software engineering devel-\nopment tasks involving code understanding (clone detection,\ndefect detection) and code generation (code-code transla-\ntion, code-code refinement, code-NL summarization). How-\never, the data-driven pre-training of the above models on\nmassive amounts of code data constraints them to primar-\nily operate in the ‘natural channel’ of code (Chakraborty\net al. 2022; Hindle et al. 2016; Zhang et al. 2022). This\n‘natural channel’ focuses on conveying information to hu-\nmans through code comments, meaningful variable names,\nand function names (Casalnuovo et al. 2020). In such a sce-\nnario, the robustness and vulnerabilities of the pre-trained\nmodels need careful investigation. In this work, we lever-\nage the code structure to generate adversarial samples in\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: CodeAttack makes a small modification to the in-\nput code snippet (in red) which causes significant changes\nto the code summary obtained from the SOTA pre-trained\nprogramming language models. Keywords are highlighted\nin blue and comments in green.\nthe natural channel of code and demonstrate the vulnerabil-\nity of the state-of-the-art programming language models to\nadversarial attacks.\nAdversarial attacks are characterized by imperceptible\nchanges in the input that result in incorrect predictions from\na machine learning model. For pre-trained PL models oper-\nating in the natural channel, such attacks are important for\ntwo primary reasons: (i)Exposing system vulnerabilitiesand\nevaluating model robustness: A small change in the input\nprogramming language (akin to a typo in the NL scenario)\ncan trigger the code summarization model to generate gib-\nberish natural language code summary (Figure 1), and (ii)\nModel interpretability: Adversarial samples can be used to\ninspect the tokens pre-trained PL models attend to.\nA successful adversarial attack in the natural channel for\ncode should have the following properties: (i) Minimal per-\nturbations: Akin to spelling mistakes or synonym replace-\nment in NL that mislead neural models with imperceptible\nchanges, (ii) Code Consistency: Perturbed code is consistent\nwith the original input and follows the same coding style as\nthe original code, and (iii) Code fluency: Does not alter the\nuser-level code understanding of the original code. The cur-\nrent natural language adversarial attack models fall short on\nall three fronts. Hence, we propose CodeAttack – a sim-\nple yet effective black-box attack model for generating ad-\nversarial samples in the natural channel for any input code\nsnippet, irrespective of the programming language.\nCodeAttack operates in a realistic scenario, where the ad-\nversary doesnot have access to model parameters but only to\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n14892\nthe test queries and the model prediction. CodeAttack uses\na pre-trained masked CodeBERT model (Feng et al. 2020)\nas the adversarial code generator to generate imperceptible\nand effective adversarial examples by leveraging the code\nstructure. Our primary contributions are as follows:\n• To the best of our knowledge, our work is the first one\nto detect the vulnerabilities of pre-trained programming\nlanguage models to adversarial attacks in the natural\nchannel of code. We propose a simple yet effective realis-\ntic black-box attack method, CodeAttack, that generates\nadversarial samples for a code snippet irrespective of the\ninput programming language.\n• We design a general purpose black-box attack method\nfor sequence-to-sequence PL models that is transferable\nacross different downstream tasks like code translation,\nrepair, and summarization. The input language agnos-\ntic nature of our method also makes it extensible to\nsequence-to-sequence tasks in other domains.\n• We demonstrate the effectiveness of CodeAttack over\nexisting NLP adversarial models through an extensive\nempirical evaluation. CodeAttack outperforms the natu-\nral language baselines when considering both the attack\nquality and its efficacy.\nBackground and Related Work\nDual Channel of Source Code. Casalnuovo et al. (2020)\nproposed a dual channel view of code: (i) formal, and (ii)\nnatural. The formal channel is precise and used for code ex-\necution by compilers and interpreters. The natural language\nchannel, on the other hand, is for human comprehension and\nis noisy. It relies on code comments, variable names, func-\ntion names, etc., to ease human understanding. The state-of-\nthe-art PL models operate primarily in the natural channel\nof code (Zhang et al. 2022) and therefore, we generate ad-\nversarial samples by making use of this natural channel.\nAdversarial Attacks in NLP. BERT-Attack (Li et al. 2020)\nand BAE (Garg and Ramakrishnan 2020) use BERT for at-\ntacking vulnerable words. TextFooler (Jin et al. 2020) and\nPWWS (Ren et al. 2019) use synonyms and part-of-speech\n(POS) tagging to replace important tokens. Deepwordbug\n(Gao et al. 2018) and TextBugger (Li et al. 2019) use char-\nacter insertion, deletion, and replacement strategy for at-\ntacks whereas Hsieh et al. (2019) and Yang et al. (2020)\nuse a greedy search and replacement strategy. Alzantot et al.\n(2018) use genetic algorithm and Ebrahimi et al. (2018), Pa-\npernot et al. (2016), and Pruthi, Dhingra, and Lipton (2019)\nuse model gradients for finding subsititutes. None of these\nmethods have been designed specifically for programming\nlanguages, which is more structured than natural language.\nAdversarial Attacks for PL. Zhang et al. (2020) gen-\nerate adversarial examples by renaming identifiers using\nMetropolis-Hastings sampling (Metropolis et al. 1953).\nYang et al. (2022) improve on that by using greedy and ge-\nnetic algorithm. Yefet, Alon, and Yahav (2020) use gradi-\nent based exploration; whereas Applis, Panichella, and van\nDeursen (2021) and (Henkel et al. 2022) propose metamor-\nphic transformations for attacks. The above models focus on\nclassification tasks like defect detection and clone detection.\nAlthough some works do focus on adversarial examples for\ncode summarization (Henkel et al. 2022; Zhou et al. 2022),\nthey do not do so in the natural channel. They also do not\ntest the transferability to different tasks, PL models, and dif-\nferent programming languages. Our model, CodeAttack, as-\nsumes black-box access to the state-of-the-art PL models for\ngenerating adversarial attacks for code generation tasks like\ncode translation, code repair, and code summarization using\na constrained code-specific greedy algorithm to find mean-\ningful substitutes for vulnerable tokens, irrespective of the\ninput programming language.\nCodeAttack\nWe describe the capabilities, knowledge, and the goal of the\nproposed model, and provide details on how it detects vul-\nnerabilities in the state-of-the-art pre-trained PL models.\nThreat Model\nAdversary’s Capabilities. The adversary is capable of\nperturbing the test queries given as input to a pre-trained PL\nmodel to generate adversarial samples. We follow the exist-\ning literature for generating natural language adversarial ex-\namples and allow for two types of perturbations for the input\ncode sequence in the natural channel: (i) character-level per-\nturbations, and (ii) token-level perturbations. The adversary\nis allowed to perturb only a certain number of tokens/char-\nacters and must ensure a high similarity between the origi-\nnal code and the perturbed code. Formally, for a given input\ncode sequence X ∈X, where X is the input space, a valid\nadversarial code example Xadv satisfies the requirements:\nX ̸= Xadv (1)\nXadv ← X+ δ; s.t. ||δ|| < θ (2)\nSim(Xadv, X) ≥ ϵ (3)\nwhere θ is the maximum allowed perturbation; Sim(· ) is a\nsimilarity function; and ϵ is the similarity threshold.\nAdversary’s Knowledge. We assume standard black-box\naccess to realistically assess the vulnerabilities and robust-\nness of existing pre-trained PL models. The adversary does\nnot have access to the model parameters, model architec-\nture, model gradients, training data, or the loss function.\nIt can only query the pre-trained PL model with input se-\nquences and get their corresponding output probabilities.\nThis is more practical than a white-box scenario where the\nattacker assumes access to all the above.\nAdversary’s Goal. Given an input code sequence as\nquery, the adversary’s goal is to degrade the quality of the\ngenerated output sequence through imperceptibly modifying\nthe query in the natural channel of code. The generated out-\nput sequence can either be a code snippet (code translation,\ncode repair) or natural language text (code summarization).\nFormally, given a pre-trained PL model F : X → Y , where\nX is the input space, and Y is the output space, the goal of\nthe adversary is to generate an adversarial sample Xadv for\nan input sequence X s.t.\nF(Xadv) ̸= F(X) (4)\n14893\nQ(F(X)) − Q(F(Xadv)) ≥ ϕ (5)\nwhere Q(·) measures the quality of the generated output and\nϕ is the specified drop in quality. This is in addition to the\nconstraints applied on Xadv earlier. We formulate our final\nproblem of generating adversarial samples as follows:\n∆atk = argmaxδ [Q(F(X)) − Q(F(Xadv))] (6)\nIn the above objective function, Xadv is a minimally per-\nturbed adversary subject to constraints on the perturbations\nδ (Eqs.1-5). CodeAttack searches for a perturbation ∆atk to\nmaximize the difference in the quality Q(·) of the output se-\nquence generated from the original input code snippetX and\nthat by the perturbed code snippet Xadv.\nAttack Methodology\nThere are two primary steps: (i) Finding the most vulnerable\ntokens, and (ii) Substituting these vulnerable tokens (subject\nto code-specific constraints), to generate adversarial samples\nin the natural channel of code.\nFinding Vulnerable Tokens CodeBERT gives more at-\ntention to keywords and identifiers while making predictions\n(Zhang et al. 2022). We leverage this information and hy-\npothesize that certain input tokens contribute more towards\nthe final prediction than others. ‘Attacking’ these highly in-\nfluential or highly vulnerable tokens increases the probabil-\nity of altering the model predictions more significantly as\nopposed to attacking non-vulnerable tokens. Under a black-\nbox setting, the model gradients are unavailable and the ad-\nversary only has access to the output logits of the pre-trained\nPL model. We define ‘vulnerable tokens’ as tokens having a\nhigh influence on the output logits of the model. LetF be an\nencoder-decoder pre-trained PL model. The given input se-\nquence is denoted by X = [x1, .., xi, ..., xm], where {xi}m\n1\nare the input tokens. The output is a sequence of vectors:\nO = F(X) = [o1, ..., on]; yt = argmax(ot); where {ot}n\n1 is\nthe output logit for the correct output token yt for the time\nstep t. Without loss of generality, we can also assume the\noutput sequence Y = F(X) = [yi, ..., yl]. Y can either be a\nsequence of code or natural language tokens.\nTo find the vulnerable input tokens, we re-\nplace a token xi with [MASK] such that X\\xi =\n[x1, ., xi−1, [MASK], xi+1, ., xm] and get its output logits.\nThe output vectors are now O\\xi = F(X\\xi ) = [o′\n1, ..., o′\nq]\nwhere {o′\nt}q\n1 is the new output logit for the correct prediction\nY. The influence score for the token xi is as follows:\nIxi =\nnX\nt=1\not −\nqX\nt=1\no′\nt (7)\nWe rank all the tokens according to their influence scoreIxi\nin descending order to find the most vulnerable tokens V .\nWe select the top-k tokens to limit the number of perturba-\ntions and attack them iteratively either by replacing them or\nby inserting/deleting a character around them.\nSubstituting Vulnerable Tokens We adopt greedy search\nusing a masked programming language model, subject to\nToken Class Description\nKeywords Reserved word\nIdentifiers Variable, Class Name, Method name\nOperators Brackets ({},(),[]), Symbols (+,*,/,-,%,;,.)\nArguments Integer, Floating point, String, Character\nTable 1: Token class and their description.\ncode-specific constraints, to find substitutes S for vulner-\nable tokens V such that they are minimally perturbed and\nhave the maximal probability of incorrect prediction.\nSearch Method. In a given input sequence, we mask a vul-\nnerable token vi and use the masked PL model to predict a\nmeaningful contextualized token in its place. We use the top-\nk predictions for each of the masked vulnerable tokens as\nour initial search space. Let M denote a masked PL model.\nGiven an input sequence X = [ x1, .., vi, .., xm], where vi\nis a vulnerable token, M uses WordPiece algorithm (Wu\net al. 2016) for tokenization that breaks uncommon words\ninto sub-words resulting in H = [ h1, h2, .., hq]. We align\nand mask all the corresponding sub-words for vi, and com-\nbine the predictions to get the top-ksubstitutes S′ = M(H)\nfor the vulnerable token vi. This initial search space S′ con-\nsists of l possible substitutes for a vulnerable token vi. We\nthen filter out substitute tokens to ensure minimal pertur-\nbation, code consistency, and code fluency of the generated\nadversarial samples, subject to code-specific constraints.\nCode-Specific Constraints. Since the tokens generated from\na masked PL model may not be meaningful individual code\ntokens, we further use a CodeNet tokenizer (Puri et al. 2021)\nto break a token into its corresponding code tokens. The\ncode tokens are tokenized into four primary code token\nclasses (Table 1). If si is the substitute for the vulnerable\ntoken vi as tokenized by M, and Op(·) denotes the oper-\nators present in any given token using CodeNet tokenizer,\nwe allow the substitute tokens to have an extra or a missing\noperator (akin to typos in the natural channel of code).\n|Op(vi)| −1 ≤ |Op(si)| ≤ |Op(vi)| + 1 (8)\nLet C(·) denote the code token class (identifiers, keywords,\nand arguments) of a token. We maintain the alignment be-\ntween between vi and the potential substitute si as follows.\nC(vi) = C(si) and |C(vi)| = |C(si)| (9)\nThe above code constraints maintain the code fluency and\nthe code consistency of Xadv and significantly reduce the\nsearch space for finding adversarial examples.\nSubstitutions. We allow two types of substitutions of vul-\nnerable tokens to generate adversarial examples: (i) Oper-\nator (character) level substitution – only an operator is in-\nserted/replaced/deleted; and (ii)Token-level substitution. We\nuse the reduced search spaceS and iteratively substitute, un-\ntil the adversary’s goal is met. We only allow replacing upto\np% of the vulnerable tokens/characters to limit the number\nof perturbations. We also maintain the cosine similarity be-\ntween the input text X and the adversarially perturbed text\n14894\nAlgorithm 1: CodeAttack: Generating adversarial examples\nfor Code\nInput: Code X; Victim model F; Maximum perturbation θ;\nSimilarity ϵ; Performance Drop ϕ\nOutput: Adversarial Example Xadv\nInitialize: Xadv ← X\n// Find vulnerable tokens ‘V’\nfor xi in M(X) do\nCalculate Ixi acc. to Eq.(7)\nend\nV ← Rank(xi) based on Ixi\n// Find substitutes ‘S’\nfor vi in V do\nS ← Filter(vi) subject to Eqs.(8), (9)\nfor sj in S do\n// Attack the victim model\nXadv = [x1, ..., xi−1, sj, ..., xm]\nif Q(F(X)) − Q(F(Xadv)) ≥ ϕ and\nSim(X, Xadv) ≥ ϵ and ||Xadv − X|| ≤ θ\nthen\nreturn Xadv // Success\nend\nend\n// One perturbation\nXadv ← [x1, ...xi−1, sj, ..xm]\nend\nreturn\nXadv above a certain threshold (Equation 3). The complete\nalgorithm is given in Algorithm 1. CodeAttack maintains\nminimal perturbation, code fluency, and code consistency\nbetween the input and the adversarial code snippet.\nExperiments\nWe study the following research questions:\n• RQ1: How effective and transferable are the attacks gen-\nerated using CodeAttack to different downstream tasks\nand programming languages?\n• RQ2: How is the quality of adversarial samples gener-\nated using CodeAttack?\n• RQ3: Is CodeAttack effective when we limit the number\nof allowed perturbations?\n• RQ4: What is the impact of different components on the\nperformance of CodeAttack?\nDownstream Tasks and Datasets We evaluate the trans-\nferability of CodeAttack across different sequence to se-\nquence downstream tasks and in different programming lan-\nguages: (i) Code Translation1 involves translating between\nC# and Java and vice-versa, (ii) Code Repair automatically\nfixes bugs in Java functions. We use the ‘small’ dataset (Tu-\nfano et al. 2019), (iii) Code Summarization involves gen-\nerating natural language summary for a given code. We\nuse Python, Java, and PHP from the CodeSearchNet dataset\n(Husain et al. 2019). (See Appendix A for details).\n1https://github.com/eclipse/jgit/, http://lucene.apache.org/,\nhttp://poi.apache.org/, https://github.com/antlr/\nVictim Models We pick a representative method from\ndifferent categories for our experiments: (i) CodeT5: Pre-\ntrained encoder-decoder transformer-based PL model (Wang\net al. 2021), (ii) CodeBERT: Bimodal pre-trained PL model\n(Feng et al. 2020), (iii) GraphCodeBERT: Pre-trained graph\nPL model (Guo et al. 2020), (iv) RoBERTa: Pre-trained NL\nmodel (Guo et al. 2020). (See Appendix A for details).\nBaseline Models Since CodeAttack operates in the nat-\nural channel of code, we compare with two state-of-the-\nart adversarial NLP baselines for a fair comparison: (i)\nTextFooler: Uses synonyms, Part-Of-Speech checking, and\nsemantic similarity to generate adversarial text (Jin et al.\n2020), (ii) BERT-Attack: Uses a pre-trained BERT masked\nlanguage model to generate adversarial text (Li et al. 2020).\nEvaluation Metrics We evaluate theeffectiveness and the\nquality of the generated adversarial code.\nAttack Effectiveness.To measure the effectiveness of the ad-\nversarial attacks on sequence-to-sequence tasks, we define\nthe following metric.\n• ∆drop: We measure the drop in the downstream perfor-\nmance before and after the attack using CodeBLEU (Ren\net al. 2020) and BLEU (Papineni et al. 2002). We define\n∆drop = Qbefore−Qafter = Q(F(X), Y)−Q(F(Xadv, Y)\nwhere Q = {CodeBLEU, BLEU}; Y is the ground truth\noutput; F is the pre-trained victim PL model, Xadv is\nthe adversarial code sequence generated after perturbing\nthe original input source code X. CodeBLEU measures\nthe quality of the generated code snippet for code trans-\nlation and code repair, and BLEU measures the quality\nof the generated natural language code summary when\ncompared to the ground truth.\n• Success %: Computes the % of successful attacks as\nmeasured by ∆drop. The higher the value, the more ef-\nfective is the adversarial attack.\nAttack Quality. The following metric measures the quality\nof the generated adversarial code across three dimensions:\n(i) efficiency, (ii) imperceptibility, and (iii) code consistency.\n• # Queries: Under a black-box setting, the adversary can\nquery the victim model to check for changes in the output\nlogits. The lower the average number of queries required\nper sample, the more efficient is the adversary.\n• # Perturbation: The number of tokens changed on an\naverage to generate an adversarial code. The lower the\nvalue, the more imperceptible the attack will be.\n• CodeBLEUq: Measures the consistency of the adversar-\nial code using CodeBLEUq = CodeBLEU(X, Xadv);\nwhere Xadv is the adversarial code sequence generated\nafter perturbing the original input source code X. The\nhigher the CodeBLEU q, the more consistent the adver-\nsarial code is with the original source code.\nImplementation Details The model is implemented in\nPyTorch. We use the publicly available pre-trained Code-\nBERT (MLM) masked model as the adversarial code gen-\nerator. We select the top 50 predictions for each vulner-\nable token as the initial search space and allow attacking\n14895\nTask Victim\nModel\nAttack\nMethod\nAttack Effectiveness Attack Quality\nBefore After ∆drop Success% #Queries #Perturb CodeBLEU q\nTranslate\n(Code-\nCode)\nCodeT5\nTextFooler\n73.99\n68.08 5.91 28.29 94.95 2.90 63.19\nBERT-Attack 63.01 10.98 75.83 163.5 5.28 62.52\nCodeAttack 61.72 12.27 89.3 36.84 2.55 65.91\nCodeBERT\nTextFooler\n71.16\n60.45 10.71 49.2 73.91 1.74 66.61\nBERT-Attack 58.80 12.36 70.1 290.1 5.88 52.14\nCodeAttack 54.14 17.03 97.7 26.43 1.68 66.89\nGraphCode-\nBERT\nTextfooler\n66.80\n46.51 20.29 38.70 83.17 1.82 63.62\nBERT-Attack 36.54 30.26 94.33 175.8 6.73 52.07\nCodeAttack 38.81 27.99 98 20.60 1.64 65.39\nRepair\n(Code-\nCode)\nCodeT5\nTextfooler\n61.13\n57.59 3.53 58.84 90.50 2.36 69.53\nBERT-Attack 52.70 8.43 94.33 262.5 15.1 53.60\nCodeAttack 53.21 7.92 99.36 30.68 2.11 69.03\nCodeBERT\nTextfooler\n61.33\n53.55 7.78 81.61 45.89 2.16 68.16\nBERT-Attack 51.95 9.38 95.31 183.3 15.7 61.95\nCodeAttack 52.02 9.31 99.39 25.98 1.64 68.05\nGraphCode-\nBERT\nTextfooler\n62.16\n54.23 7.92 78.92 51.07 2.20 67.89\nBERT-Attack 53.33 8.83 96.20 174.1 15.7 53.66\nCodeAttack 51.97 10.19 99.52 24.67 1.67 66.16\nSummarize\n(Code-NL)\nCodeT5\nTextFooler\n20.06\n14.96 5.70 64.6 410.15 6.38 53.91\nBERT-Attack 11.96 8.70 78.4 1014.1 7.32 51.34\nCodeAttack 11.06 9.59 82.8 314.87 10.1 52.67\nCodeBERT\nTextfooler\n19.76\n14.38 5.37 61.10 358.43 2.92 54.10\nBERT-Attack 11.30 8.35 56.47 1912.6 15.8 46.24\nCodeAttack 10.88 8.87 88.32 204.46 2.57 52.95\nRoBERTa\nTextFooler\n19.06\n14.06 4.99 62.60 356.68 2.80 54.11\nBERT-Attack 11.34 7.71 60.46 1742.3 17.1 46.95\nCodeAttack 10.98 8.08 87.51 183.22 2.62 53.03\nTable 2: Results on translation (C#-Java), repair (Java-Java), and summarization (PHP) tasks. The performance is measured in\nCodeBLEU for Code-Code tasks and in BLEU for Code-NL task. The best result is in boldface; the next best is underlined.\na maximum of 40% of code tokens. The cosine similarity\nthreshold between the original code and adversarially gen-\nerated code is set to 0.5. As victim models, we use the\npublicly available fine-tuned checkpoints for CodeT5 and\nfine-tune CodeBERT, GraphCodeBERT, and RoBERTa on\nthe related downstream tasks. We use a batch-size of 256.\nAll experiments were conducted on a 48 GiB RTX 8000\nGPU. The source code for CodeAttack can be found at\nhttps://github.com/reddy-lab-code-research/CodeAttack.\nRQ1: Effectiveness of CodeAttack\nWe test the effectiveness and transferability of the generated\nadversarial samples on three different sequence-to-sequence\ntasks (Code Translation, Code Repair, and Code Summa-\nrization). We generate adversarial code for four different\nprogramming languages (C#, Java, Python, and PHP), and\nattack four different pre-trained PL models (CodeT5, Graph-\nCodeBERT, CodeBERT, and Roberta). The results for C#-\nJava translation task and for the PHP code summarization\ntask are shown in Table 2. (See Appendix A for Java-C#\ntranslation and Python and Java code summarization tasks).\nCodeAttack has the highest success% compared to other ad-\nversarial NLP baselines. CodeAttack also outperforms the\nadversarial baselines, BERT-Attack and TextFooler, in 6 out\nof 9 cases – the average ∆drop using CodeAttack is around\n20% for code translation and 10% for code repair tasks,\nrespectively. For code summarization, CodeAttack reduces\nBLEU by almost 50% for all the victim models. As BERT-\nAttack replaces tokens indiscriminately, its ∆drop is higher\nin some cases but its attack quality is the lowest.\nRQ2: Quality of Attacks Using CodeAttack\nQuantitative Analysis. Compared to the other adversar-\nial NLP models, CodeAttack is the most efficient as it re-\nquires the lowest number of queries for a successful attack\n(Table 2). CodeAttack is also the least perceptible as the av-\nerage number of perturbations required are 1-3 tokens in 8\nout of 9 cases. The code consistency of adversarial samples,\nas measured by CodeBLEU q, generated using CodeAttack\nis comparable to TextFooler which has a very low success\nrate. CodeAttack has the best overall performance.\nQualitative Analysis. Figure 2 presents qualitative exam-\nples of the generated adversarial code snippets from differ-\nent attack models. Although TextFooler has a slightly bet-\nter CodeBLEU q score when compared to CodeAttack (as\nseen from Table 2), it replaces keywords with closely re-\n14896\nFigure 2: Qualitative examples of adversarial codes on C#-Java Code Translation task. (See Appendix A for more examples).\nFigure 3: Syntactic correctness of adversarial code on C#,\nJava, and Python demonstrating attack quality.\nlated natural language words (public → audiences;\noverride → revoked, void → cancelling).\nBERT-Attack has the lowest CodeBLEUq and substitutes to-\nkens with seemingly random words. Both TextFooler and\nBERT-Attack have not been designed for programming lan-\nguages. CodeAttack generates more meaningful adversarial\ncode samples by replacing vulnerable tokens with variables\nand operators which are imperceptible and consistent.\nSyntactic correctness. Syntactic correctness of the gener-\nated adversarial code is a useful criteria for evaluating the\nattack quality even though CodeAttack and other PL models\nprimarily operate in the natural channel of code,i.e., they are\nconcerned with code understanding for humans and not with\nthe execution or compilation of the code. The datasets de-\nscribed earlier consist of code snippets and cannot be com-\npiled. Therefore, we generate adversarial code for C#, Java,\nand Python using TextFooler, BERT-Attack, and CodeAt-\ntack and ask 3 human annotators, familiar with these lan-\nguages to verify the syntax manually. We randomly sam-\nple 60 generated adversarial codes for all three program-\nming languages for evaluating each of the above methods.\nCodeAttack has the highest average syntactic correctness for\nC# (70%), Java (60%), and Python (76.19%) followed by\nBERT-Attack and TextFooler (Figure 3), further highlight-\ning the need for a code-specific adversarial attack.\nRQ3: Limiting Perturbations Using CodeAttack\nWe restrict the number of perturbations when attacking a\npre-trained PL model to a strict limit, and study the effective-\nness of CodeAttack. From Figure 4a, we observe that as the\nperturbation % increases, the CodeBLEU after for CodeAt-\ntack decreases but remains constant for TextFooler and only\nslightly decreases for BERT-Attack. We also observe that al-\nthough CodeBLEUq for CodeAttack is the second best (Fig-\nure 4b), it has the highest attack success rate (Figure 4d) and\nrequires the lowest number of queries for a successful attack\n(Figure 4c). This shows the efficiency of CodeAttack and the\nneed for code-specific adversarial attacks.\nRQ4: Ablation Study\nImportance of Vulnerable Tokens. We create a vari-\nant, CodeAttackRAND, which randomly samples tokens from\nthe input code for substitution. We define another vari-\nant, CodeAttackVUL, which finds vulnerable tokens based on\nlogit information and attacks them, albeit without any con-\nstraints. As can be seen from Figure 5a, attacking random\ntokens is not as effective as attacking vulnerable tokens. Us-\ning CodeAttackVUL yields greater ∆drop and requires fewer\nnumber of queries when compared to CodeAttack RAND,\nacross all three models at similar CodeBLEU q (Figure 5b)\nand success % (Figure 5d).\nImportance of Code-Specific Constraints. We find vul-\nnerable tokens and apply two types of constraints: (i) Opera-\ntor level constraint (CodeAttackOP), and (ii) Token level con-\nstraint (CodeAttack TOK). Only applying the operator level\nconstraint results in lower attack success% (Figure 5d) and\na lower ∆drop (Figure 5a) but a much higher CodeBLEU q.\nThis is because we limit the changes only to operators re-\nsulting in minimal changes. On applying both operator level\nand token level constraints together, the ∆drop and the at-\ntack success% improve significantly. (See Appendix A for\nqualitative examples.)\nOverall, the final model, CodeAttack, consists of\nCodeAttackVUL, CodeAttackOP, and CodeAttackTOK, has the\nbest trade-off across ∆drop, attack success %, CodeBLEUq,\nand #Queries for all pre-trained PL victim models.\nHuman Evaluation. We sample 50 original and perturbed\nJava and C# code samples and shuffle them to create a mix.\nWe ask 3 human annotators, familiar with the two program-\nming languages, to classify the codes as either original or\nadversarial by evaluating the source codes in their natural\nchannel. On an average, 72.1% of the given codes were clas-\nsified as original. We also ask them to read the given ad-\nversarial codes and rate their code understanding on a scale\nof 1 to 5; where 1 corresponds to ‘Code cannot be under-\nstood at all’; and 5 corresponds to ‘Code is completely un-\nderstandable’. The average code understanding for the ad-\nversarial codes was 4.14. Additionally, we provide the an-\nnotators with pairs of adversarial and original codes and ask\n14897\n(a) CodeBLEUafter\n (b) CodeBLEUq\n (c) Average #Queries\n (d) Attack%\nFigure 4: Varying the perturbation % to study attack effectiveness on CodeT5 for the code translation task (C#-Java).\n(a) Performance Drop\n (b) CodeBLEUq\n (c) # Queries\n (d) Average Success Rate\nFigure 5: Ablation Study for Code Translation (C#-Java): Performance of different components of CodeAttack with random\n(RAND) and vulnerable tokens (VUL) and two code-specific constraints: (i) Operator level (OP), and (ii) Token level (TOK).\nthem to rate the code consistency between the two using a\nscale between 0 to 1; where 0 corresponds to ‘Not at all con-\nsistent with the original code’, and 1 corresponds to ‘Ex-\ntremely consistent with the original code’. On average, the\ncode consistency was 0.71.\nDiscussion\nHumans ‘summarize’ code by reading function calls, fo-\ncusing on information denoting the intention of the code\n(such as variable names) and skimming over structural infor-\nmation (such as while and for loops) (Rodeghero et al.\n2014). Pre-trained PL models operate in a similar manner\nand do not assign high attention weights to the grammar or\nthe code structure (Zhang et al. 2022). They treat software\ncode as natural language (Hindle et al. 2016) and do not fo-\ncus on compilation or execution of the input source code\nbefore processing them to generate an output (Zhang et al.\n2022). Through extensive experimentation, we demonstrate\nthat this limitation of the state-of-the-art PL models can be\nexploited to generate adversarial examples in the natural\nchannel of code and significantly alter their performance.\nWe observe that it is easier to attack the code translation\ntask rather than code repair or code summarization tasks.\nSince code repair aims to fix bugs in the given code snip-\npet, it is more challenging to attack but not impossible.\nFor code summarization, the BLEU score drops by almost\n50%. For all three tasks, CodeT5 is comparatively more ro-\nbust whereas GraphCodeBERT is the most susceptible to\nattacks using CodeAttack. CodeT5 has been pre-trained on\nthe task of ‘Masked Identifier Prediction’ or deobsfuction\n(Lachaux et al. 2021) where changing the identifier names\ndoes not have an impact on the code semantics. This helps\nthe model avoid the attacks which involve changing the iden-\ntifier names. GraphCodeBERT uses data flow graphs in their\npre-training which relies on predicting the relationship be-\ntween the identifiers. Since CodeAttack modifies the identi-\nfiers and perturbs the relationship between them, it proves to\nbe extremely effective on GraphCodeBERT. This results in\na more significant ∆drop on GraphCodeBERT compared to\nother models for the code translation task.\nThe adversarial examples from CodeAttack, although ef-\nfective, can be avoided if the pre-trained PL models com-\npile/execute the code before processing it. This highlights\nthe need to incorporate explicit code structure in the pre-\ntraining stage to learn more robust program representations.\nConclusion\nWe introduce, CodeAttack, a black-box adversarial attack\nmodel to detect vulnerabilities of the state-of-the-art pro-\ngramming language models. It finds the most vulnerable to-\nkens in a given code snippet and uses a greedy search mech-\nanism to identify contextualized substitutes subject to code-\nspecific constraints. Our model generates adversarial exam-\nples in the natural channel of code. We perform an extensive\nempirical and human evaluation to demonstrate the trans-\nferability of CodeAttack on several code-code and code-NL\ntasks across different programming languages. CodeAttack\noutperforms the existing state-of-the-art adversarial NLP\nmodels, in terms of its attack effectiveness, attack quality,\nand syntactic correctness. The adversarial samples generated\nusing CodeAttack are efficient, effective, imperceptible, flu-\nent, and code consistent. CodeAttack highlights the need for\ncode-specific adversarial attacks for pre-trained PL models\nin the natural channel.\n14898\nReferences\nAhmad, W.; Chakraborty, S.; Ray, B.; and Chang, K.-W.\n2021. Unified Pre-training for Program Understanding and\nGeneration. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 2655–\n2668.\nAlzantot, M.; Sharma, Y .; Elgohary, A.; Ho, B.-J.; Srivas-\ntava, M.; and Chang, K.-W. 2018. Generating Natural Lan-\nguage Adversarial Examples. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, 2890–2896.\nApplis, L.; Panichella, A.; and van Deursen, A. 2021. As-\nsessing Robustness of ML-Based Program Analysis Tools\nusing Metamorphic Program Transformations. In 2021 36th\nIEEE/ACM International Conference on Automated Soft-\nware Engineering (ASE), 1377–1381. IEEE.\nCasalnuovo, C.; Barr, E. T.; Dash, S. K.; Devanbu, P.; and\nMorgan, E. 2020. A theory of dual channel constraints.\nIn 2020 IEEE/ACM 42nd International Conference on Soft-\nware Engineering: New Ideas and Emerging Results (ICSE-\nNIER), 25–28. IEEE.\nChakraborty, S.; Ahmed, T.; Ding, Y .; Devanbu, P. T.; and\nRay, B. 2022. NatGen: generative pre-training by “naturaliz-\ning” source code. In Proceedings of the 30th ACM Joint Eu-\nropean Software Engineering Conference and Symposium\non the Foundations of Software Engineering, 18–30.\nEbrahimi, J.; Rao, A.; Lowd, D.; and Dou, D. 2018. Hot-\nFlip: White-Box Adversarial Examples for Text Classifica-\ntion. In Proceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 2: Short Pa-\npers), 31–36.\nFeng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;\nShou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. CodeBERT:\nA Pre-Trained Model for Programming and Natural Lan-\nguages. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, 1536–1547.\nGao, J.; Lanchantin, J.; Soffa, M. L.; and Qi, Y . 2018. Black-\nbox generation of adversarial text sequences to evade deep\nlearning classifiers. In 2018 IEEE Security and Privacy\nWorkshops (SPW), 50–56. IEEE.\nGarg, S.; and Ramakrishnan, G. 2020. BAE: BERT-based\nAdversarial Examples for Text Classification. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), 6174–6181.\nGuo, D.; Ren, S.; Lu, S.; Feng, Z.; Tang, D.; Shujie, L.;\nZhou, L.; Duan, N.; Svyatkovskiy, A.; Fu, S.; et al. 2020.\nGraphCodeBERT: Pre-training Code Representations with\nData Flow. In International Conference on Learning Repre-\nsentations.\nHenkel, J.; Ramakrishnan, G.; Wang, Z.; Albarghouthi, A.;\nJha, S.; and Reps, T. 2022. Semantic Robustness of Models\nof Source Code. In 2022 IEEE International Conference on\nSoftware Analysis, Evolution and Reengineering (SANER) ,\n526–537.\nHindle, A.; Barr, E. T.; Gabel, M.; Su, Z.; and Devanbu, P.\n2016. On the naturalness of software. Communications of\nthe ACM, 59(5): 122–131.\nHsieh, Y .-L.; Cheng, M.; Juan, D.-C.; Wei, W.; Hsu, W.-L.;\nand Hsieh, C.-J. 2019. On the robustness of self-attentive\nmodels. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 1520–1529.\nHusain, H.; Wu, H.-H.; Gazit, T.; Allamanis, M.; and\nBrockschmidt, M. 2019. Codesearchnet challenge: Eval-\nuating the state of semantic code search. arXiv preprint\narXiv:1909.09436.\nJin, D.; Jin, Z.; Zhou, J. T.; and Szolovits, P. 2020. Is bert\nreally robust? a strong baseline for natural language attack\non text classification and entailment. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34, 8018–\n8025.\nLachaux, M.-A.; Roziere, B.; Szafraniec, M.; and Lample,\nG. 2021. DOBF: A Deobfuscation Pre-Training Objective\nfor Programming Languages. Advances in Neural Informa-\ntion Processing Systems, 34.\nLi, J.; Ji, S.; Du, T.; Li, B.; and Wang, T. 2019. TextBug-\nger: Generating Adversarial Text Against Real-world Appli-\ncations. In 26th Annual Network and Distributed System\nSecurity Symposium.\nLi, L.; Ma, R.; Guo, Q.; Xue, X.; and Qiu, X. 2020. BERT-\nATTACK: Adversarial Attack Against BERT Using BERT.\nIn Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 6193–6202.\nMetropolis, N.; Rosenbluth, A. W.; Rosenbluth, M. N.;\nTeller, A. H.; and Teller, E. 1953. Equation of state calcula-\ntions by fast computing machines. The journal of chemical\nphysics, 21(6): 1087–1092.\nPapernot, N.; Faghri, F.; Carlini, N.; Goodfellow, I.; Fein-\nman, R.; Kurakin, A.; Xie, C.; Sharma, Y .; Brown, T.;\nRoy, A.; et al. 2016. Technical report on the clever-\nhans v2. 1.0 adversarial examples library. arXiv preprint\narXiv:1610.00768.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, 311–318. Philadel-\nphia, Pennsylvania, USA: Association for Computational\nLinguistics.\nPruthi, D.; Dhingra, B.; and Lipton, Z. C. 2019. Combating\nAdversarial Misspellings with Robust Word Recognition. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 5582–5591.\nPuri, R.; Kung, D. S.; Janssen, G.; Zhang, W.; Domeni-\nconi, G.; Zolotov, V .; Dolby, J.; Chen, J.; Choudhury, M.;\nDecker, L.; et al. 2021. Project codenet: A large-scale ai for\ncode dataset for learning a diversity of coding tasks. arXiv\npreprint arXiv:2105.12655, 1035.\nRen, S.; Deng, Y .; He, K.; and Che, W. 2019. Generating\nNatural Language Adversarial Examples through Probabil-\nity Weighted Word Saliency. InProceedings of the 57th An-\nnual Meeting of the Association for Computational Linguis-\n14899\ntics, 1085–1097. Florence, Italy: Association for Computa-\ntional Linguistics.\nRen, S.; Guo, D.; Lu, S.; Zhou, L.; Liu, S.; Tang, D.; Sun-\ndaresan, N.; Zhou, M.; Blanco, A.; and Ma, S. 2020. Code-\nbleu: a method for automatic evaluation of code synthesis.\narXiv preprint arXiv:2009.10297.\nRodeghero, P.; McMillan, C.; McBurney, P. W.; Bosch, N.;\nand D’Mello, S. 2014. Improving automated source code\nsummarization via an eye-tracking study of programmers.\nIn Proceedings of the 36th international conference on Soft-\nware engineering, 390–401.\nTipirneni, S.; Zhu, M.; and Reddy, C. K. 2022. StructCoder:\nStructure-Aware Transformer for Code Generation. arXiv\npreprint arXiv:2206.05239.\nTufano, M.; Watson, C.; Bavota, G.; Penta, M. D.; White,\nM.; and Poshyvanyk, D. 2019. An empirical study on learn-\ning bug-fixing patches in the wild via neural machine trans-\nlation. ACM Transactions on Software Engineering and\nMethodology (TOSEM), 28(4): 1–29.\nWang, Y .; Wang, W.; Joty, S.; and Hoi, S. C. 2021. CodeT5:\nIdentifier-aware Unified Pre-trained Encoder-Decoder Mod-\nels for Code Understanding and Generation. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 8696–8708.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google’s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144.\nYang, P.; Chen, J.; Hsieh, C.-J.; Wang, J.-L.; and Jordan,\nM. I. 2020. Greedy Attack and Gumbel Attack: Generating\nAdversarial Examples for Discrete Data. J. Mach. Learn.\nRes., 21(43): 1–36.\nYang, Z.; Shi, J.; He, J.; and Lo, D. 2022. Natural Attack for\nPre-Trained Models of Code. In Proceedings of the 44th In-\nternational Conference on Software Engineering, ICSE ’22,\n1482–1493. New York, NY , USA: Association for Comput-\ning Machinery. ISBN 9781450392211.\nYefet, N.; Alon, U.; and Yahav, E. 2020. Adversarial exam-\nples for models of code. Proceedings of the ACM on Pro-\ngramming Languages, 4(OOPSLA): 1–30.\nZhang, H.; Li, Z.; Li, G.; Ma, L.; Liu, Y .; and Jin, Z.\n2020. Generating adversarial examples for holding robust-\nness of source code processing models. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34,\n1169–1176.\nZhang, Z.; Zhang, H.; Shen, B.; and Gu, X. 2022. Diet code\nis healthy: Simplifying programs for pre-trained models of\ncode. In Proceedings of the 30th ACM Joint European Soft-\nware Engineering Conference and Symposium on the Foun-\ndations of Software Engineering, 1073–1084.\nZhou, Y .; Zhang, X.; Shen, J.; Han, T.; Chen, T.; and Gall,\nH. 2022. Adversarial robustness of deep code comment gen-\neration. ACM Transactions on Software Engineering and\nMethodology (TOSEM), 31(4): 1–30.\n14900",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8205257654190063
    },
    {
      "name": "Code (set theory)",
      "score": 0.6622012853622437
    },
    {
      "name": "Programming language",
      "score": 0.5481628775596619
    },
    {
      "name": "Automatic summarization",
      "score": 0.537679135799408
    },
    {
      "name": "Redundant code",
      "score": 0.5159295797348022
    },
    {
      "name": "Source code",
      "score": 0.5120051503181458
    },
    {
      "name": "Scripting language",
      "score": 0.4518432915210724
    },
    {
      "name": "Adversarial system",
      "score": 0.4402920603752136
    },
    {
      "name": "Code word",
      "score": 0.4354904890060425
    },
    {
      "name": "Code generation",
      "score": 0.4296859800815582
    },
    {
      "name": "Dead code",
      "score": 0.4290817081928253
    },
    {
      "name": "Systematic code",
      "score": 0.4267314672470093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3604501485824585
    },
    {
      "name": "Computer security",
      "score": 0.19048276543617249
    },
    {
      "name": "Low-density parity-check code",
      "score": 0.17529737949371338
    },
    {
      "name": "Decoding methods",
      "score": 0.16946858167648315
    },
    {
      "name": "Algorithm",
      "score": 0.16080030798912048
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ]
}