{
  "title": "Abstract screening using the automated tool Rayyan: results of effectiveness in three diagnostic test accuracy systematic reviews",
  "url": "https://openalex.org/W4281646566",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5023498310",
      "name": "Amir Valizadeh",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5026284593",
      "name": "Mana Moassefi",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5040093319",
      "name": "Amin Nakhostin-Ansari",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5074002324",
      "name": "Seyed Hossein Hosseini-Asl",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5022911191",
      "name": "Mehrnush Saghab Torbati",
      "affiliations": [
        "Islamic Azad University, Zahedan Branch"
      ]
    },
    {
      "id": "https://openalex.org/A5070587971",
      "name": "Reyhaneh Aghajani",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5027152866",
      "name": "Zahra Ghorbani",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5003289617",
      "name": "Shahriar Faghani",
      "affiliations": [
        "Tehran University of Medical Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2974858420",
    "https://openalex.org/W2560438049",
    "https://openalex.org/W4247702319",
    "https://openalex.org/W2999783216",
    "https://openalex.org/W2066437172",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W2188214922",
    "https://openalex.org/W3174233443",
    "https://openalex.org/W2604351697",
    "https://openalex.org/W2161374186",
    "https://openalex.org/W2782235379",
    "https://openalex.org/W3005919160",
    "https://openalex.org/W3143985904",
    "https://openalex.org/W2613869537",
    "https://openalex.org/W2120040939",
    "https://openalex.org/W3014512586",
    "https://openalex.org/W2806703257",
    "https://openalex.org/W1760234209",
    "https://openalex.org/W1680797894",
    "https://openalex.org/W118549185",
    "https://openalex.org/W2807522649"
  ],
  "abstract": null,
  "full_text": "Valizadeh et al. \nBMC Medical Research Methodology          (2022) 22:160  \nhttps://doi.org/10.1186/s12874-022-01631-8\nRESEARCH\nAbstract screening using the automated \ntool Rayyan: results of effectiveness in three \ndiagnostic test accuracy systematic reviews\nAmir Valizadeh1*, Mana Moassefi1, Amin Nakhostin‑Ansari2, Seyed  Hossein Hosseini Asl2,3, \nMehrnush Saghab Torbati4, Reyhaneh Aghajani2,3, Zahra Maleki Ghorbani2,3 and Shahriar Faghani5 \nAbstract \nObjective: To evaluate the performance of the automated abstract screening tool Rayyan.\nMethods: The records obtained from the search for three systematic reviews were manually screened in four stages. \nAt the end of each stage, Rayyan was used to predict the eligibility score for the remaining records. At two different \nthresholds (≤2.5 and < 2.5 for exclusion of a record) Rayyan‑generated ratings were compared with the decisions \nmade by human reviewers in the manual screening process and the tool’s accuracy metrics were calculated.\nResults: Two thousand fifty‑four records were screened manually, of which 379 were judged to be eligible for full‑\ntext assessment, and 112 were eventually included in the final review. For finding records eligible for full‑text assess‑\nment, at the threshold of < 2.5 for exclusion, Rayyan managed to achieve sensitivity values of 97‑99% with specificity \nvalues of 19‑58%, while at the threshold of ≤2.5 for exclusion it had a specificity of 100% with sensitivity values of \n1‑29%. For the task of finding eligible reports for inclusion in the final review, almost similar results were obtained.\nDiscussion: At the threshold of < 2.5 for exclusion, Rayyan managed to be a reliable tool for excluding ineligible \nrecords, but it was not much reliable for finding eligible records. We emphasize that this study was conducted on \ndiagnostic test accuracy reviews, which are more difficult to screen due to inconsistent terminology.\nKeywords: Rayyan, Abstract screening, Systematic reviews, Methodology\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nBackground\nRationale\nA systematic review (SR) is a scientific investigation that \nfocuses on a specific question and uses explicit, prespeci -\nfied scientific methods to identify, select, assess, and \nsummarize the findings of similar but separate studies \n[1]. SRs are considered to have the strongest level of evi -\ndence (level 1) in modern evidence-based medicine [2]. \nAs the body of scientific literature is rapidly growing, \nSRs are more appreciated by healthcare decision-makers, \ndue to providing brief robust reports of new interven -\ntions and phenomena. Unfortunately, current methods \nfor conducting SRs are very time-consuming, resulting in \nthe slow production of these important scientific reports. \nIn an analysis of 195 SRs in 2017 [3], the mean project \nlength was 67.3 weeks with a range of 6–186 weeks. In the \nsame analysis, the number of studies found in the litera -\nture searches ranged from 27 to 92,020 with a mean of \n1781. As a rough conservative estimate, it is believed that \ntitles and abstracts of search results could be screened at \na rate of 60–120 per hour [3]. With some basic calcula -\ntions applied to the mean value of 1781, it results in about \n14.8 to 29.6 hours of exhaustive work for reviewers with \na maximum range of 766.8 to 1533.6 hours. Taking into \nOpen Access\n*Correspondence:  thisisamirv@gmail.com\n1 Neuroscience Institute, Tehran University of Medical Sciences, Tehran, Iran\nFull list of author information is available at the end of the article\nPage 2 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nconsideration that most organizations prefer this task to \nbe done in duplicate by at least two masked reviewers \nto minimize the risk of bias in study selection, the above \nnumbers could be doubled. These numbers indicate the \nsignificant amount of time and energy a team of authors \nhas to spend just selecting the potentially eligible studies.\nIn recent years, a variety of automated tools have been \nintroduced to facilitate the process of conduction of dif -\nferent parts of SRs, with different results. One of the \nmain branches of these tools has been the study selection \ntools. Different automated tools have been developed for \nsuch tasks as Rayyan [4], Covidence [5], Abstrackr [6], \nColandr [7], and EPPI-Reviewer [8]. These tools use text \nmining techniques to identify relevant information from \ntext using statistical pattern learning that recognizes pat -\nterns in data. To achieve this, supervised learning algo -\nrithms are incorporated in their core, which tries to find \npatterns in the studies classified by the reviewers to pre -\ndict the classification of unclassified records. These tools \nvary significantly in their core learning algorithm, offered \nfeatures, and availability. In a scoping review in 2020 [9] \nRayyan managed to get the highest score in weighted fea-\nture analysis and second place in the overall experience \nscore (as rated by users in a survey) among these tools.\nRayyan, a web-based automated screening tool, devel -\noped by Qatar Computing Research Institute (QCRI) \nwas initially launched in 2014 and is currently accessible \nat www. rayyan. ai. It uses text mining methods to facili -\ntate semi-automatic screening of records for SRs. As a \nreviewer screens some records and labels them for either \ninclusion, exclusion, or “maybe” relevant to the subject of \nthe review, the tool thrives for finding patterns and simi -\nlarities to give a similarity score to each of the remain -\ning records as a five-star rating. Higher ratings reflect the \ncomputed underlying probabilities of the record being \nincluded are higher, and vice versa. The simplicity of \nusing Rayyan, combined with its completely free access, \nhas made it quite popular among users. It also provides \nsome interesting features such as allowing independent \nmasked screening of the records by more than one user, \ncreating custom labels for records, highlighting words for \ninclusion and exclusion (which significantly assists man -\nual screening), and choosing the reason(s) for excluding \na record. Rayyan’s code is written in the open-source \nframework Ruby on Rails [10] and runs on Heroku [11] \nwhich is a Platform as a Service based on the cloud-\nhosting Amazon Web Services. The Rayyan classification \nsystem is described in a paper [4] by the developers as \nfollows:\nRayyan extracts all the words and pairs of words \n(bigrams) and MeSH terms following the removal of stop \nwords and the stem of the remaining words from the \ntitles/abstracts. These words are then used as features \nby the machine learning algorithm (support vector \nmachine (SVM) classifier). As users label records as \neither excluded or included, the app uses the classifier to \nlearn the features and build a model. The algorithm then \nruns on the records without a decision and gives a score \nto each of them, revealing how close it is to the include or \nexclude classes. That score is presented to the user as a \nfive-star rating.\nIn the current paper, we aim to assess Rayyan’s effec -\ntiveness for screening title/abstract of records in three \nsystematic reviews conducted by our team. It should be \nnoted that the three reviews included in this study are \ndiagnostic test accuracy (DTA) reviews. Due to incon -\nsistent terminology, designing search strategies for DTA \nreviews is hard, resulting in a more difficult screening \nprocess as well.\nTerminology\nIn this paper, we used the standard terminology proposed \nby the PRISMA 2020 statement [12], with the addition of \nsome new terms specific to this study:\n• Study: An investigation, such as a clinical trial. A \nstudy might have multiple reports.\n• Report: A document supplying information about a \nstudy. A report is typically a journal article or a pre -\nprint, but could also be a conference abstract, a dis -\nsertation, or a study register entry.\n• Record: The title and abstract of a report indexed in \na database. Records that refer to the same report are \nknown as “duplicates” .\n• Record screening: The process of screening records, \nalso known as title/abstract screening.\n• Report screening: The process of screening reports, \nalso known as full-text assessment.\n• Eligible records: Records that were judged to be eli -\ngible for report retrieval.\n• Eligible reports: Reports that were judged to be eligi -\nble for inclusion in the final review.\nObjective\nThis study aims to evaluate the performance of the auto -\nmated abstract screening tool Rayyan while screening \nrecords for three DTA systematic reviews. We intend to \nanswer the following questions:\n1- How precise was Rayyan in identifying eligible \nrecords following the manual screening of 20, 40, 60, \nand 80% of the records identified by the search for \nthree DTA SRs?\n2- How precise was Rayyan in identifying eligible \nreports following the manual screening of 20, 40, 60, \nPage 3 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \nand 80% of the records from the search results for \nthree DTA SRs? It should be noted that Rayyan only \nevaluates records and not reports.\nMethods\nThis study’s design and methods are reported in line with \nthe Standards for Reporting Diagnostic accuracy stud -\nies (STARD) checklist [13]. This study aims to evaluate \nthe function of Rayyan in identifying eligible records \nand reports from the search results of three DTA SRs \nconducted in the Neuroscience Institute of Tehran Uni -\nversity of Medical Sciences, Tehran, Iran. At the time of \nwriting this paper, those SRs are still in the process of \nconduction. Their respective protocol has been published \nelsewhere [14].\nStudy design\nThe three SRs were very similar in most aspects of the \nquestions they were designed to answer. The only differ -\nence between studies was in the domain of the index test \nused. Eligibility criteria for the studies were as follows:\nPopulation: patients with autism spectrum disorder \n(ASD) regardless of age, sex, and ethnicity.\nIndex test:\n1. SR1: applied machine learning algorithms on cerebral \nstructural magnetic resonance imaging (sMRI)\n2. SR2: applied machine learning algorithms on cerebral \nresting-state functional magnetic resonance imaging \n(rs-fMRI)\n3. SR3: applied machine learning algorithms on electro-\nencephalogram (EEG)\nTarget condition: autism spectrum disorder (ASD) \nas defined by well-known diagnostic criteria (DSM-\nIV, DSM-V, ICD-11, ICD-10, ADOS, ADI-R, CARS, or \nGARS).\nReference standard: diagnosis made by a trained phy -\nsician or psychologist.\nStudy design: cross-sectional design, including both \nsingle-gate (cohort type) and two-gates (case-control \ntype) designs.\nSearch strategies were developed based on the above \neligibility criteria, and the following databases were \nsearched for relevant records: Embase, MEDLINE, APA \nPsycINFO, IEEE Xplore, Scopus, and Web of Science. We \nalso searched grey literature through OpenGrey, Center \nfor Research Libraries Online Catalogue (CRL), and \nOpen Access Theses and Dissertations (OATD). Search \nstrategies are presented in the Additional file 1.\nResults of the search were imported into EndNote X9 \n[15], a citation management software. To avoid a redun -\ndant workload, duplicate records were removed using the \nEndNote deduplication system. The remaining records \nwere exported and uploaded to the Rayyan web-based \nplatform. Next, using Rayyan’s deduplication system, \nrecords with a similarity score of more than 0.85 were \nchecked manually and removed if confirmed as dupli -\ncates. Thus, it must be considered that we only screened \nunique records. For the report screening process, we \nplanned to discard records of the same report, however, \nall our eligible records were from unique reports.\nTest methods\nIn this study, the star ratings generated by Rayyan were \nthe index test, and the human reviewers’ final decisions \nat the record screening stage were the reference stand -\nard. For each SR, two reviewers independently evalu -\nated the first 20% (± 0.1%) of the remaining records \n(following deduplication) in alphabetical order, labeling \neach as either “eligible” , “not eligible” , or “maybe eligi -\nble” . After the end of the independent screening of the \nfirst 20% (± 0.1%) of records, the blinding feature of \nRayyan was turned off and reviewers re-checked the \nconflicting decisions. Conflicts were resolved through \ndiscussion, and in case of disagreement, a third author \nwas consulted. The third author also made the final \ndecision for the “maybe eligible” records after careful \nevaluations, labeling each as either “eligible” or “not eli -\ngible” . After reaching a consensus, the reviewer with the \ndecision that was different from the consensus result \nchanged his/her submitted decision on the platform \nto match the consensus result. When all the conflicts \nwere resolved, the “Compute Rating” feature of Rayyan \nwas activated. This feature computes the ratings for the \nremaining records based on the patterns found in the \ndecisions assigned to each screened record up to that \npoint. All ratings were exported and saved in a file. \nAfterward, the blinding feature was turned back on \nand reviewers continued the record screening process \nfor another 20% (± 0.1%) of the records in alphabeti -\ncal order. Although reviewers could see the computed \nratings for the remaining records, they were strictly \ninstructed to ignore them in making their judgments. \nThe same process was taken in each step until all the \nrecords were screened and their assigned ratings were \nsaved. Finally, the reports of the eligible records were \nretrieved and assessed independently by two review -\ners for inclusion in the final review. A summary of the \nundertaken process is presented in Fig. 1 .\nPage 4 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nAnalysis\nData were analyzed using R version 4.1 [16]. Rayyan \nassigns each record with one of the following ratings: 0.5 \nstars, 1.5 stars, 2.5 stars, 3.5 stars, or 4.5 stars. We chose \ntwo thresholds for our analyses: a rating of < 2.5 stars for \nexclusion (records with a rating of 0.5 or 1.5 are considered \nineligible), and a rating of ≤2.5 stars for exclusion (records \nwith a rating of 0.5, 1.5, or 2.5 are considered ineligible). \nThese two thresholds were chosen because they were in \nthe middle of the range of possible ratings, and thus, we \nhypothesized they might give the most balanced results \nfor both sensitivity and specificity values. Additionally, \nwe believe the decision to consider a record with a rating \nof 2.5, as eligible or ineligible, would be the hardest for a \nresearcher, and thus, we aimed to report the diagnostic \nmeasures for the tool around this specific value.\nRatings were converted into a pair of binary dummy \nvariables based on each threshold. By using this pair \nof variables, contingency tables were designed for \neach SR at each stage of the screening process for each \nthreshold and each objective of the study. Then sensi -\ntivity (SEN), specificity (SPE), positive predictive value \n(PPV), negative predictive value (NPV), and F1 score \nfor each stage and each objective of the study were \ncalculated using the contingency tables. Considering \nthat PPV and NPV are dependent on the ‘prevalence’ \nof studies that should be included in the review, which \nin turn depends on the sensitivity of the search strat -\negy, we also calculated the point prevalence (PR) at \neach stage. SEN is the proportion of records that were \njudged to be “eligible” by Rayyan among all those that \nwere eligible. On the other hand, PPV is the probability \nthat when a record is judged to be “eligible” by Rayyan, \nthat record is truly eligible. SPE is the proportion of \nrecords that were judged to be “not eligible” by Rayyan \namong all those that were not eligible, while NPV is \nthe probability that when a record is judged to be “not \neligible” , it is truly not eligible. Finally, the F1 score is a \nsingle number evaluation metric that is the harmonic \nmean of the precision (PPV) and recall (SEN). Given \neach contingency table, metrics were calculated based \non the formulas presented in the Table 1 .\nFinally, all the calculated data were used to design \nline graphs to better represent the results.\nResults\nFlow of records\nA total of 2054 records were screened manually, of which \n379 (122 SR1, 193 SR2, and 64 SR3) were judged to be \neligible records. Finally, 112 reports (25 SR1, 64 SR2, and \nFig. 1 Summary of the screening process\nTable 1 Formulas for calculated metrics. FN False ‑negative, FP False‑positive, NPV Negative predictive value, PPV Positive predictive \nvalue, SEN Sensitivity, SPE Specificity, TN True‑negative, TP True‑positive\nTP = Number of eligible records (for objective 1) or eligible reports (for objective 2) identified by Rayyan eligible\nTN = Number of ineligible records (for objective 1) or einligible reports (for objective 2) identified by Rayyan ineligible\nFP = Number of ineligible records (for objective 1) or ineligible reports (for objective 2) identified by Rayyan as eligible\nFN = Number of eligible records (for objective 1) or eligible reports (for objective 2) identified by Rayyan as ineligible\nSEN = TP\nTP+FN SPE = TN\nTN +FP PPV = TP\nTP+FP NPV = TN\nTN+FN F1 score= 2TP\n2TP+FP+FN\nPage 5 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \n23 SR3) were included in the SRs following the report \nscreening process. A summary of the flow of the records \nwith the number of records assessed and discarded in \neach step is presented in Fig. 2.\nTest results\nIdentifying eligible records\nThe results for the test accuracy for identifying eligible \nrecords for each SR and the pooled results are presented \nin Table 2, and Figs. 3 and 4.\nConsidering a threshold of < 2.5 (0.5 and 1.5 stars) for \nexclusion of a record, Rayyan held a low PPV across all \nstages of screening, while it held almost a perfect NPV. A \nsimilar situation happened with SEN and SPE: SEN held an \nalmost perfect value across all stages, while SPE managed \nto reach a maximum of 58% at the last stage of screening. \nGiven these results, considering a threshold < 2.5 for exclu-\nsion, Rayyan managed to have an almost perfect exclusive \nfunction while having a relatively weak inclusive function, \nresulting in a suboptimal reduction of the workload.\nConsidering a threshold of ≤2.5 (0.5, 1.5, and 2.5 stars) \nfor exclusion of a record, Rayyan had a perfect SPE while \nlacking in SEN (a maximum of 30%). The noticeable \nresults were the PPV and NPV at this threshold. Even \nafter the first stage of screening, it managed to reach a \nPPV of 86% (53-99%), while reaching a PPV of 92% (74-\n99%) after the second stage. It also managed to hold a \nrelatively acceptable NPV after the first stage (56%), while \nreaching an NPV of 83% (81-85%) only after the second \nstage. Based on these results, Rayyan has the potential to \nreach acceptable PPV and NPV after manually screening \n40% of records, considering a threshold of ≤2.5 for exclu-\nsion. It should be noted though that low SEN results for \nthis threshold indicate the inappropriate exclusion of a \nconsiderable proportion of relevant records.\nIdentifying eligible reports\nThe results for the test accuracy for identifying eligible \nreports for each SR and the pooled results are presented \nin Table 3, and Figs. 5 and 6.\nConsidering a threshold of < 2.5 (0.5 and 1.5 stars) \nfor exclusion of a report, Rayyan held an almost perfect \nSEN across all stages, while SPE was very poor (3-5%). \nPPV was in the range of 37-43% across the stages, but \nFig. 2 Flow of records\nPage 6 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nTable 2 Evaluation metrics for the test accuracy for identifying eligible records for the 3 SRs in each screening stage. Pooled results \nfor each metric in each stage are presented below the results of the three SRs. Numbers in the parentheses indicate 95% CI. N/A Not \navailable, NPV Negative predictive value, PPV Positive predictive value, PR Prevalence, SEN Sensitivity, SPE Specificity\n\nPage 7 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \nNPV values were higher, reaching a maximum of 81%. \nGiven these results, considering a threshold of < 2.5 for \nexclusion, Rayyan managed to have an almost perfect \nexclusive function while having a very weak inclusive \nfunction.\nConsidering a threshold of ≤2.5 (0.5, 1.5, and 2.5 stars) \nfor exclusion of a report, Rayyan had high SPE values (78-\n99%) while having relatively low SEN values (a maximum \nof 44%). PPV was in the range of 57-88% across all stages, \nwhile NPV was in the range of 64-68%. The noticeable \nFig. 3 Evaluation metrics for the test accuracy for identifying eligible records for the 3 SRs in each screening stage for a) a threshold of less than 2.5 \nfor exclusion; b) a threshold of 2.5 and less for exclusion. NPV: Negative predictive value, PPV: Positive predictive value\nPage 8 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nresult was that NPV values remain consistent across all \nstages. Based on these results, Rayyan had almost bal -\nanced PPV and NPV values considering a threshold \nof ≤2.5 for exclusion, although low SEN values for this \nthreshold indicate that a considerable proportion of rel -\nevant reports may be excluded by mistake.\nDiscussion\nSummary of main findings\nA summary of the main results of this study is presented \nin Table 4.\nRelevant studies\nFor a brief review of previous relevant studies, check \nTable 5.\nA previous study on Rayyan [4] by Olofsson et  al. in \n2017 [17] revealed promising results for the effective -\nness of the tool for identifying eligible records of six \nreviews (3 SRs and 3 literature reviews). In their study, \n21 to 88% of eligible records were identified by the time \nthe first quarter had been screened, 86 to 98% when half \nwere screened, and 89-100% when three quarters were \nscreened. Their study did not mention the threshold used \nfor their results.\nIn a study by Rathbone et al. in 2015 [18] on Abstrackr \n[6], they reported precisions of 16.8 to 45.5% and false-\nnegative rates of 2.4 to 14.5% for identifying eligible \nrecords after screening less than 18% of records for \n4 reviews. Sensitivity and specificity rates were not \nreported.\nGates et al. [19] conducted another study on Abstrackr \nin 2018 and reported sensitivity and specificity rates of \n79-96% and 19-90% for identifying eligible records after \nscreening 0.7-10.3% of records for 3 SRs and 1 descriptive \nanalysis study.\nIn 2020, Tsou et al. [20] compared the effectiveness of \nAbstrackr and EPPI-Reviewer [8] for the semi-automated \nscreening of records of 9 SRs. They reported better results \nwith the EPPI-Reviewer, achieving a sensitivity of 100% \nfor identifying eligible records after manually screening \n39.9-89.8% of records. They also evaluated the effective -\nness of those tools for identifying eligible reports. For \nthe EPPI-Reviewer, they achieved a sensitivity of 100% \nafter manually screening 30.1-97.1% of records, while for \nAbstrackr, they achieved the same sensitivity value after \nmanually screening 39.8-100% of records.\nChai et  al. in 2021 [21] introduced a new tool named \n“Research Screener” which utilizes deep learning algo -\nrithms for the semi-automated screening process. In \ntheir validation study on 9 SRs, a sensitivity rate of 100% \nfor identifying eligible records was achieved after manu -\nally screening only 4-32% of records.\nInterpretation of the results\nThis study aimed to evaluate the performance of Rayyan, \na tool for the semi-automatic screening of records. Here, \nFig. 4 Evaluation metrics for the test accuracy for identifying eligible records for the 3 SRs in each screening stage for a) a threshold of less than 2.5 \nfor exclusion; b) a threshold of 2.5 and less for exclusion. NPV: Negative predictive value, PPV: Positive predictive value\nPage 9 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \nTable 3 Evaluation metrics for the test accuracy for identifying eligible reports for the 3 SRs in each screening stage. Pooled results \nfor each metric in each stage are presented below the results of the three SRs. Numbers in the parentheses indicate 95% CI. N/A Not \navailable, NPV Negative predictive value, PPV Positive predictive value, PR Prevalence, SEN Sensitivity, SPE Specificity\n\nPage 10 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nwe reported two sets of results: considering a rating of \n< 2.5 (0.5 and 1.5) for exclusion and considering a rating \nof ≤2.5 (0.5, 1.5, and 2.5) for exclusion. We believe there \nwas no need for analyses on other thresholds because \nour results indicate the presence of a huge difference in \nthe sensitivity and specificity of the tool around these \ntwo close thresholds. Thus, it is predictable that a higher \nFig. 5 Evaluation metrics for the test accuracy for identifying eligible reports for the 3 SRs in each screening stage for a) a threshold of less than 2.5 \nfor exclusion; b) a threshold of 2.5 and less for exclusion. NPV: Negative predictive value, PPV: Positive predictive value\nPage 11 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \nthreshold would only result in a drop in sensitivity (with -\nout a considerable change in specificity), and lower \nthresholds only decrease specificity without much of a \nchange in the sensitivity.\nAround the thresholds used in our study, we see very \ndifferent results. For the task of identifying eligible \nrecords, at the threshold of ≤2.5 (0.5, 1.5, and 2.5) for \nexclusion, we achieved specificity rates of 100% after \njust screening 20% of the records, while at the threshold \nof < 2.5 (0.5 and 1.5) for exclusion, we achieved sensitiv -\nity rates of 98-99% following the manual screening of a \nsimilar proportion of the records, which is close to the \nresults of the study of Olofsson et al. [17] (sensitivity of \n21-88% after screening of 25% of the records and 86-98% \nafter screening 50% of the records). Such contradictory \nresults around these two close thresholds are an indica -\ntion of the poor differentiation ability of the tool. In con -\ntrast, the study of Gates et al. [19] on Abstrackr [6] for 3 \nSRs achieved both good sensitivity and specificity results \nafter manually screening a similar number of records (69-\n90% and 79-92% respectively). On the other hand, the \nstudy of Tsou et al. [20] reported that Abstrackr reached \na sensitivity of 100% after screening a greater number of \nrecords (51-99% of the records in 9 SRs), compared to \nour results on Rayyan. Their study did not report speci -\nficity rates.\nFor the task of identifying eligible reports, sensitivity \nvalues followed a similar pattern to those found in the \ntask of identifying eligible records, but specificity val -\nues were substantially different. At the threshold of ≤2.5 \n(0.5, 1.5, and 2.5) for exclusion, two SRs maintained high \nspecificity values, while the third SR had a significant \ndrop in specificity following each stage of the screening. \nOn the other hand, at the threshold of < 2.5 (0.5 and 1.5) \nfor exclusion, results showed very poor specificity values \nfor all three SRs. Compared to the results of the study of \nTsou et  al. [20], our results indicate that Rayyan might \nhave a sensitivity superior to Abstrackr at the threshold \nof < 2.5 (0.5 and 1.5) for exclusion, but it is not possible \nto compare the specificity of the tools as they did not \nreport this metric.\nDespite all that, the question is which threshold should \nbe considered as the optimal choice? Noticing that one of \nthe main privileges of using an automated screening tool \nshould be reducing workload, it is of great importance \nfor the tool to reach an appropriate level of learning as \nfast as possible. Taking that into consideration, it seems \nthat a threshold of < 2.5 (0.5 and 1.5) for exclusion is the \noptimal choice for record screening, as it achieved a good \nF1 score (0.354) with just 20% of the records manually \nscreened. Similar results were observed for the task of \nreport screening at this threshold, where Rayyan achieved \nan F1 score of 0.544 with just 20% of the records manu -\nally screened. As stated in the handbook of Cochrane \n[3], when searching for and selecting studies, reviewers \nshould use methods that aim for “maximized” sensitivity \nFig. 6 Evaluation metrics for the test accuracy for identifying eligible reports for the 3 SRs in each screening stage for a) a threshold of less than 2.5 \nfor exclusion; b) a threshold of 2.5 and less for exclusion. NPV: Negative predictive value, PPV: Positive predictive value\nPage 12 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nwhilst striving for “reasonable” precision. This threshold \nindeed showed very high sensitivity in our results. On \nthe other hand, specificity was very low in this threshold \n(5-33% for finding eligible records after manually screen -\ning 20% of records and 26-52% after manually screen -\ning 40% of records, and a maximum of 10% for finding \neligible reports), which implies the inclusive function of \nthe tool is not reliable at this threshold. Nevertheless, as \nsensitivity should be prioritized above specificity in the \nselection of records, this threshold is deemed the optimal \nchoice, because it achieves “maximized” sensitivity while \nholding to the highest possible specificity at such great \nsensitivity rates. In rare cases when specificity comes first \n(for example when the time resources are limited for con-\nducting an SR), a threshold of ≤2.5 (0.5, 1.5, and 2.5) for \nexclusion could be the optimal choice for finding eligible \nrecords and reports. Although when interpreting these \nresults, it should also be considered that our 3 SRs were \nDTA reviews on machine learning algorithms. Both DTA \nand machine learning algorithm studies are very difficult \nto screen, because of inconsistent terminology.\nConsidering that this tool utilizes machine learning \nalgorithms at its core, it also suffers the same issues. \nOne of these issues is the class imbalance problem. Data \nare said to suffer the class imbalance problem when the \nclass distributions are highly imbalanced. In this context, \nmany classification learning algorithms have low predic -\ntive accuracy for the infrequent class [22]. In our study, \n379 of 2054 records were judged to be eligible records, \nonly 18.5% of the data, while only 112 were judged to \nbe eligible reports (5.4% of the data). Such a significant \nimbalance could have strongly affected the training pro -\ncess of the learning algorithm. Developers of the tool are \nrecommended to use cost-sensitive learning techniques \n[23] in future updates to tackle this issue.\nOverall, knowing that the algorithm used as the core \nof Rayyan (SVM) is not considered the optimal classifi -\ncation algorithm in the era of deep learning, our results \nwere not much of a surprise. Although developers did \nnot specify the kernel used by the SVM in Rayyan, it is \nmost possible that it only utilizes a linear kernel, which \nis incapable of learning the complex non-linear relation -\nships in the data. Knowing that such an algorithm does \nnot require extensive computational resources, it might \nbe a good choice for a free app at the time of initial \nrelease, but considering the advances in computer hard -\nware products in recent years, it may be possible to uti -\nlize a more advanced classification algorithm given the \nTable 4 Summary of main findings. N/A Not available, SEN Sensitivity, SPE Specificity\n\nPage 13 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \nTable 5 Summary of the relevant studies. FNR: False‑negative rate. SEN Sensitivity, SPE Specificity\nIdentifying eligible records\nStudy ID Tool Studies All records Results Comments\nOlofsson 2017 [16] Rayyan [4] 3 SRs and 3 literature \nreviews\n7956 SEN of 21‑88% after \nscreening 25% of \nrecords.\nSEN of 86‑98% after \nscreening 50% of \nrecords.\nSEN of 89‑100% after \nscreening 75% of \nrecords.\nThresholds used were \nnot reported.\nSPE rates were not \nreported.\nRathbone 2015 [17] Abstrackr [6] 4 SRs SR1: 1415\nSR2: 517\nSR3: 1735\nSR4: 1042\nSR1: Precision of 16.8% \nand FNR of 10% after \nscreening 18% of \nrecords.\nSR2: Precision of 24.7% \nand FNR of 14.5% \nafter screening 23% of \nrecords.\nSR3: Precision of 29.2% \nand FNR of 4.7% after \nscreening 7% of records.\nSR4: Precision of 45.5% \nand FNR of 2.4% after \nscreening 12% of \nrecords.\nSEN and SPE rates were \nnot reported.\nGates 2018 [18] Abstrackr [6] 3 SRs and 1 descriptive \nanalysis (DA)\nSR1: 12763\nSR2: 5893\nSR3: 47385\nDA: 5243\nSR1: SPE of 69% and SEN \nof 79% after screening \n2.2% of records.\nSR2: SPE of 85% and SEN \nof 92% after screening \n10.3% of records.\nSR3: SPE of 90% and SEN \nof 82% after screening \n0.7% of records.\nDA: SPE of 19% and SEN \nof 96% after screening \n4% of records.\n–\nTsou 2020 [19] Abstrackr [6] and EPPI‑\nReviewer [8]\n9 SRs SR1: 9038\nSR2: 3181\nSR3: 2706\nSR4: 889\nSR5: 673\nSR6: 651\nSR7: 500\nSR8: 427\nSR9: 226\nFor Abstrackr, SEN of \n100% after screening \n71.1, 51.5, 96, 95.6, 99, \n85.9, 88.2, 99.3, and \n93.8% of records for SR1 \nto SR9 respectively.\nFor EPPI‑Reviewer, SEN \nof 100% after screen‑\ning 61.7, 39.9, 91.3, 94.6, \n97.9, 86.3, 88.2, 98.8, and \n91.6% of records for SR1 \nto SR9 respectively.\nThey also reported \ndiagnostic metrics for \nidentifying eligible \nreports.\nChai 2021 [20] Research Screener [20] 9 SRs and 2 scoping \nreviews (SCR)\nSR1: 813\nSR2: 2249\nSR3: 2584\nSR4: 368\nSR5: 870\nSR6: 306\nSR7: 23423\nSR8: 13376\nSR9: 1686\nSCR1: 16506\nSCR2: 1230\nSEN of 100% after screen‑\ning 32, 13, 6, 5, 4, 4, 5, 13, \nand 14% of records for \nSR1 to SR9 respectively.\nSEN of 100% after \nscreening 40 and 38% of \nrecords for SCR1 to SCR2 \nrespectively.\nThis tool utilizes deep \nlearning algorithms.\nPage 14 of 15Valizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \nsame expenses. Research Screener [21] is a new tool that \nutilizes deep learning algorithms and performs record \nscreening via learning text embeddings. Although this \ntool is, at the moment of writing this paper, being tested \nin closed beta trials. In the validation study [21] pub -\nlished by the developers of the app, it managed to reach a \nsensitivity of 100% after 4-32% of the records were manu-\nally screened in 9 SRs. Unfortunately, specificity results \nwere not reported.\nLimitations\nFirst, it should be noted that our study included three SRs \nof the same review type, DTA reviews. Designing specific \nsearch strategies for these kinds of reviews is difficult \n(due to inconsistent terminology) which makes screen -\ning often more difficult as well, compared to reviews on \ninterventions that mostly include randomized controlled \ntrials.\nAlso, the reviewers could see the ratings computed \nby the platform in each screening stage. Although we \ninstructed them to ignore these ratings in their judg -\nments, some risks of bias might still exist.\nIt should also be noted that we only assessed one out -\ncome in our study (diagnostic accuracy measures). Other \nstudies on Rayyan and other similar tools did also evalu -\nate other outcomes such as workload savings [18, 19], \nusers’ satisfaction and recommendations [17], and diag -\nnostic accuracy of the tool for large and small SRs seper -\nately [20].\nAnother important issue in our study that requires spe-\ncial consideration is the complex nature of the index test \nof the SRs. All the index tests consisted of two compo -\nnents that may have resulted in lower evaluation metrics: \na neural response recording technique (sMRI, rs-fMRI, \nand EEG) and a machine learning algorithm (which con -\nsists of many different terms).\nAnother limitation was that the terminology of \nmachine learning and statistics have many similar words, \nwhich may have also caused bias in the results. For exam-\nple, the word “regression” could mean either a statistical \nmethod or a machine learning algorithm. Also, consider -\ning that the three SRs included in this study had similar \ntopics, it further reduces the generalizability power of \nour results.\nFinally, data for the 3rd stage of study selection in the \nSR of EEG was missing because unfortunately, the results \nfor that stage of screening were accidentally lost. It could \nhave potentially affected our results.\nImplications for practice\nConsidering that our study was on DTA SRs of machine \nlearning studies, inconsistent terminology is believed \nto have a huge impact on our results. With that being \nsaid, we still managed to achieve almost perfect sensitiv -\nity values for finding eligible records and reports at the \nthreshold < 2.5 (0.5 and 1.5) for exclusion after manually \nscreening only 20% of the records. Such considerable \nexclusive power can greatly help the production of SRs \nby reducing the workload significantly. This exclusive \naccuracy can also come in handy in conducting live SRs \nwhere screening hundreds of records might be necessary \nat frequent short time intervals. In exceptional circum -\nstances when review resources are scarce and specificity \nrates are the priority, a threshold of ≤ 2.5 (0.5, 1.5, and \n2.5) for exclusion can be used to achieve reliable results \nfor the screening process rapidly, though the exclusion \nof a proportion of relevant records is expected.\nImplications for research\nFuture research on semi-automated records screen -\ning tools should consider some issues. First, diagnostic \nmeasures should be reported appropriately. We noticed \nthat most of the relevant studies only reported one or \ntwo metrics, mostly just sensitivity values, while other \nmeasures are also required for an in-depth evaluation of \nthe tool. We also recommend including other outcomes \nthan just the diagnostic measures, such as users’ satis -\nfaction, ease of use, workload saving, and possible crit -\nics and recommendations of the users. Reporting results \nafter smaller proportions of manual screening (e.g., 10, \nTable 5 (continued)\nIdentifying eligible reports\nStudy ID Tool Studies Eligible records Results\nTsou 2020 [19] Abstrackr [6] and EPPI‑\nReviewer [8]\n9 SRs SR1: 696\nSR2: 200\nSR3: 843\nSR4: 107\nSR5: 267\nSR6: 73\nSR7: 166\nSR8: 149\nSR9: 104\nFor Abstrackr, SEN of 100% after screening 40.7, \n39.8, 81.2, 100, 71.6, 56.4, 41.2, 60, and 71.2% of \neligible records for SR1 to SR9 respectively.\nFor EPPI‑Reviewer, SEN of 100% after screening \n41, 39.8, 97.1, 70, 74, 30.1, 31.8, 59.4, and 51.3% of \neligible records for SR1 to SR9 respectively.\nPage 15 of 15\nValizadeh et al. BMC Medical Research Methodology          (2022) 22:160 \n \n20%, etc.) is also encouraged. We also strongly suggest \nthe evaluation of screening tools that utilize modern \ndeep learning methods when they become available, such \nas Research Screener [21]. Finally, for a more informa -\ntive design, we suggest future research to compare the \ndecisions of one reviewer and the record screening tool \nagainst an additional reviewer pair without the record \nscreening tool, in which case it is possible to find the \npotential cases where reviewers missed eligible records.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12874‑ 022‑ 01631‑8.\nAdditional file 1. \nAcknowledgments\nWe want to thank Dr. Alireza Mirzamohammadi and Dr. Mohammad Ghafouri \nfor their contributions to designing search strategies.\nAuthors’ contributions\nCoordination of the study: AV, MM, ANA. Designing study: AV, MM. Performing \nthe search: AV. Study selection: SHH, MST, RA, ZMG. Data extraction: AV. Analy‑\nsis of data: AV. Interpretation of the results: AV, MM. Writing the manuscript: AV, \nMM, SF. The author(s) read and approved the final manuscript.\nFunding\nThis study was not funded.\nAvailability of data and materials\nThe datasets used and analyzed during the current study are available from \nthe corresponding author on request.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nNone\nAuthor details\n1 Neuroscience Institute, Tehran University of Medical Sciences, Tehran, Iran. \n2 Sports Medicine Research Center, Neuroscience Institute, Tehran University \nof Medical Sciences, Tehran, Iran. 3 Students’ Scientific Research center, Excep‑\ntional Talents Development Center, Tehran University of Medical Sciences, \nTehran, Iran. 4 Islamic Azad University of Zahedan, Zahedan, Iran. 5 Interdisci‑\nplinary Neuroscience Research Program (INRP), Tehran University of Medical \nSciences, Tehran, Iran. \nReceived: 14 October 2021   Accepted: 11 May 2022\nReferences\n 1. Morton S, Berg A, Levit L, Eden J. Finding what works in health care: \nstandards for systematic reviews; 2011.\n 2. Portney LG. Foundations of clinical research: applications to evidence‑\nbased practice. FA Davis. 2020.\n 3. Lefebvre C, et al. Searching for and selecting studies. Cochrane Handbook \nSyst Rev Intervent. 2019;67–107.\n 4. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan—a web \nand mobile app for systematic reviews. Syst Rev. 2016;5:1–10.\n 5. Covidence. Covidence systematic review software, veritas health innova‑\ntion. Melbourne; 2021.\n 6. Elsherbeny MY. & Negida. A Using Absrackr‑Technical Report.\n 7. Tan MC. Colandr. J Canadian Health Libraries Association/Journal de \nl’Association des bibliothèques de la santé du Canada. 2018;39:85–8.\n 8. Thomas J, Brunton J. EPPI‑reviewer: software for research synthesis; 2007.\n 9. Harrison H, Griffin SJ, Kuhn I, Usher‑Smith JA. Software tools to support \ntitle and abstract screening for systematic reviews in healthcare: an evalu‑\nation. BMC Med Res Methodol. 2020;20:1–12.\n 10. Bächle M, Kirchberg P . Ruby on rails. IEEE Softw. 2007;24:105–8.\n 11. Middleton, N. & Schneeman, R. Heroku: Up and running: effortless appli‑\ncation deployment and scaling. (“ O’Reilly Media, Inc.,” 2013).\n 12. Page MJ, et al. The PRISMA 2020 statement: an updated guideline for report‑\ning systematic reviews. BMJ n71. 2021. https:// doi. org/ 10. 1136/ bmj. n71.\n 13. Bossuyt PM, et al. STARD 2015: an updated list of essential items for \nreporting diagnostic accuracy studies. Clin Chem. 2015;61:1446–52.\n 14. Valizadeh A, et al. Accuracy of machine learning algorithms for the diagnosis \nof autism spectrum disorder based on cerebral sMRI, rs‑fMRI, and EEG: \nprotocols for three systematic reviews and meta‑analyses. medRxiv. 2021.\n 15. Hupe M. EndNote X9. J ElectroResources Med Lib. 2019;16:117–9.\n 16. Team, R. C. R: A language and environment for statistical computing. (2013).\n 17. Olofsson H, et al. Can abstract screening workload be reduced using \ntext mining? User experiences of the tool Rayyan. Res Synth Methods. \n2017;8:275–80.\n 18. Rathbone J, Hoffmann T, Glasziou P . Faster title and abstract screening? \nEvaluating Abstrackr, a semi‑automated online screening program for \nsystematic reviewers. Syst Rev. 2015;4:1–7.\n 19. Gates A, Johnson C, Hartling L. Technology‑assisted title and abstract \nscreening for systematic reviews: a retrospective evaluation of the \nAbstrackr machine learning tool. Syst Rev. 2018;7:1–9.\n 20. Tsou AY, Treadwell JR, Erinoff E, Schoelles K. Machine learning for screen‑\ning prioritization in systematic reviews: comparative performance of \nAbstrackr and EPPI‑reviewer. Syst Rev. 2020;9:1–14.\n 21. Chai KEK, Lines RLJ, Gucciardi DF, Ng L. Research screener: a machine \nlearning tool to semi‑automate abstract screening for systematic reviews. \nSyst Rev. 2021;10:1–13.\n 22. Shultz, T. R. et al. Class imbalance problem. In encyclopedia of machine \nlearning (eds. Sammut, C. & Webb, G. I.) 171–171 (Springer US, 2011). \n:https:// doi. org/ 10. 1007/ 978‑0‑ 387‑ 30164‑8_ 110.\n 23. Ling CX, Sheng VS. Cost‑sensitive learning and the class imbalance prob‑\nlem. Encyclopedia of machine learning. 2008;2011:231–5.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub‑\nlished maps and institutional affiliations.",
  "topic": "Inclusion and exclusion criteria",
  "concepts": [
    {
      "name": "Inclusion and exclusion criteria",
      "score": 0.5924248099327087
    },
    {
      "name": "Medicine",
      "score": 0.5607929825782776
    },
    {
      "name": "Terminology",
      "score": 0.557060182094574
    },
    {
      "name": "Test (biology)",
      "score": 0.4566246271133423
    },
    {
      "name": "Computer science",
      "score": 0.45135509967803955
    },
    {
      "name": "Medical physics",
      "score": 0.3800385594367981
    },
    {
      "name": "Statistics",
      "score": 0.32965195178985596
    },
    {
      "name": "Data mining",
      "score": 0.322776734828949
    },
    {
      "name": "Pathology",
      "score": 0.18744376301765442
    },
    {
      "name": "Alternative medicine",
      "score": 0.10031527280807495
    },
    {
      "name": "Mathematics",
      "score": 0.0993955135345459
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}