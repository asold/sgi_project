{
  "title": "Enhancing Translation Language Models with Word Embedding for Information Retrieval",
  "url": "https://openalex.org/W2783915166",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5005643082",
      "name": "Jibril Frej",
      "affiliations": [
        null,
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A5108156680",
      "name": "Jean–Pierre Chevallet",
      "affiliations": [
        null,
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A5013799481",
      "name": "Didier Schwab",
      "affiliations": [
        null,
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147152072"
  ],
  "abstract": "In this paper, we explore the usage of Word Embedding semantic resources for Information Retrieval (IR) task. This embedding, produced by a shallow neural network, have been shown to catch semantic similarities between words (Mikolov et al., 2013). Hence, our goal is to enhance IR Language Models by addressing the term mismatch problem. To do so, we applied the model presented in the paper Integrating and Evaluating Neural Word Embedding in Information Retrieval by Zuccon et al. (2015) that proposes to estimate the translation probability of a Translation Language Model using the cosine similarity between Word Embedding. The results we obtained so far did not show a statistically significant improvement compared to classical Language Model.",
  "full_text": "arXiv:1801.03844v1  [cs.IR]  11 Jan 2018\nEnhancing T ranslation Language Models with\nW ord Embedding for Information Retrieval\nJibril Frej, Jean-Pierre Chevallet, Didier Schwab\nLA B O R ATO I R E D ’ I N F O R M AT I QU E D E GR E N O B L E (LIG)\nUN I V. G R E N O B L E AL P E S (UGA)\njibril.frej@etu.univ-grenoble-alpes.fr\njean-pierre.chevallet@imag.fr\ndidier.schwab@imag.fr\nAbstract : In this paper, we explore the usage of W ord Embedding semant ic resources for Information Re-\ntrieval (IR) task. This embedding, produced by a shallow neu ral network, have been shown to catch semantic\nsimilarities between words (Mikolov et al., 2013). Hence, our goal is to enhance IR Language Models by ad -\ndressing the term mismatch problem. T o do so, we applied the m odel presented in the paper Integrating and\nEvaluating Neural W ord Embedding in Information Retrievalby Zuccon et al. (2015) that proposes to estimate\nthe translation probability of a Translation Language Mode l using the cosine similarity between W ord Embed-\nding. The results we obtained so far did not show a statistica lly signiﬁcant improvement compared to classical\nLanguage Model.\nKeywords— Information Retrieval, Language Model , W ord Embedding\n1 Introduction\nInformation Retrieval Systems (IRS) are computer assistan ts that help to retrieve digital doc-\numents, in which user is supposed to found relevant informat ion for his task. Hence an IRS\nis about semantics, because user information needs is topical ly related and serve to help to\naccomplish user’s task.\nCuriously, most commercial and experimental search engine do not handle any sort of se-\nmantics nor knowledge to solve queries. This is because matc hing computations are mostly\nbased on statistical word distributions, and intersection between query and documents. Also,\nmost IR models see query and document as simple bag of word. Th ough, these systems provide\nsatisfaction to their user, as long as statistics are possib le, i.e. documents are long enough and\nqueries are expressed using the same vocabulary as document s.\nWhen we are not in this situation, that is, if documents to be r etrieved are very short and/or\nthere is a strong discrepancy between document vocabulary a nd user’s one, IRS are facing the\nproblem of term mismatch. For example, the collection Europeanna 1 gives access to millions\nof digital objects from cultural heritage, by a very small te xtual meta-data description. Even,\ndescriptions are made by specialists that are prone to use te chnical terms.\nHence, in this paper we propose to study the effect of exploit ing semantic resources to reduce\nterm mismatch negative effect on collection with specializ ed vocabularies. W e focus on auto-\nmatically constructed resources, namely word Embedding re sources, ﬁrst because they cover\na very large vocabulary, and second, because they seem to cap ture interesting word semantics\n(Mikolov et al., 2013).\n1 http://www .europeana.eu\nA very simple way to exploit a resource to solve term mismatch , is to expand queries or\ndocuments with words that are semantically similar accordi ng to the resource. This approach\nhas been heavily studied (Carpineto & Romano, 2012) and suff ers from a deﬁnitive problem:\nhow to control query meaning shift when a-priory the query is statically modiﬁed, i.e. for all\ndocuments ?\nIn this paper, we propose not to change the query nor the docum ent but to adapt the matching\nfunction. This adaptation requires to change the formula th at computes the Relevant Status\nV alue (RSV). This formula depends on the Information Retrie val (IR) model.\nThere are several large categories of IR model: V ector Space , Logical Based, Probabilist,\nGraph Based, and Language Models (LM). Among these models, o nly the Probabilist models\nlike the famous BM25 formula of Robertson (Robertson et al., 1995), or Language Models pro-\nvide state of the art results. Some recent proposals of IR Gra ph Modeling (Bannour et al., 2016)\nhave the advantage to fuse one single model document index an d knowledge base, and exploit\nonly one matching function: activation propagation from qu ery term to document through in-\ndexing terms and knowledge concept nodes. Beside the nice pr operty of this model to exploit\na Knowledge Base at query time, without query or document exp ansion, experiments of this\nmodel are still bellow the state of the art of Probabilistic o r Language models. For this reason,\nwe have decided to work on the transformation of a Language Mo del formula. W e did not chose\na Probabilistic model because formulas of the LM model are mu ch simpler for similar results\n(Manning et al., 2008), hence they are easier to transform.\nA simple way to include a Knowledge Resource in a LM matching f unction, is Translation\nLanguage Models (Berger & Lafferty, 1999) where the transla tion probability between query\nand document terms is taken into account, in addition to the e xact term matching. Probability are\nestimated using the mutual information between the two term s and is the highest if the consid-\nered terms have the same distribution over the collection. T his computation can be considered\nas an automatic basic knowledge resource extracted from the corpus itself. Recently, it has been\nproposed to estimate the translation probability using Neu ral W ord Embedding (Zuccon et al.,\n2015).\nW ord Embedding denotes a set of methods to produce Knowledge resources with vector rep-\nresentation of words 2 that express some semantics learned from word usage in very l arge text\ncollection. Usually these methods are based on the distribu tional hypothesis (Harris, 1954):\nwords that occur in the same contexts tend to have similar mea ning. The vector representation\nof words are computed using their context so that words with s imilar meaning will have similar\nvector representation. W ord Embedding includes dimension reduction techniques on the word\nco-occurrence matrix (Latent Semantic Analysis (Deerwest er et al., 1990)), probabilistic mod-\nels (Latent Dirichlet Allocation (Blei et al., 2003)) and more recently shallow neural network-\nbased methods such as the skip-gram model that can also learn phrases vector representations\nand is very effective on word similarity and word analogy tas ks (Mikolov et al., 2013). As we\nsaid before, these vector representations can be used to cap ture semantic relationships between\nwords by measuring the similarity between the vectors, with the cosine similarity measure for\nexample.\nSuch vector representation could help us with the vocabular y mismatch problem since they\n2 Most of the time the vectors are real-valued.\nprovide us a new way to estimate the translation probability between query and document terms\nby considering their semantic similarity. The empirical re sults show that improving Dirichlet\nLanguage Model using W ord Embedding is possible.\nIn the rest of this paper, we ﬁrst recall LM formulas in sectio n 2, present the extension with\nthe translation model in section 3, then the usage of W ord Emb edding within this model in\nsection 4. W e present the implementation of this model in sec tion 5 and results on the Cultural\nHeritage in CLEF (CHiC) Dataset 3, a sub part of the Europeanna, in section 6.\n2 Language models\nIn this part, we recall basics on the Language Model in order t o detail our modiﬁcations of the\nformula in the next section. The query likelihood Language M odel aims to rank documents d\nby computing the probability of a document interpreted as th e likelihood that it is relevant to the\nquery q. Hence, the Relevance Status V alue (RSV) of a Language Model based IR is expressed\nby:\nRSV pq, d q “ ppd|qq (1)\nUsing Bayes rule, we have:\nRSV pq, d q “ ppq|dqppdq\nppqq (2)\nW e can ignore the constant ppqq to rank documents and we also consider that ppdq is uniform\nover all the documents of the collection and can also be ignor ed. Therefore, to rank documents\nwith respect to a given query, we only have to compute the prob ability of having q, knowing d.\nIn LM model, document d is replaced by its language model, i.e. the probability dist ribution of\nterms of the vocabulary in d denoted θd. So the RSV is:\nRSV pq, d q » rank ppq|θdq (3)\nIn this work, we chose the multinomial event model for θd, and the unigram language model 4,\nso the probability of a given word does not depend on its conte xt:\nRSV pq, d q » rank log pppq|θdqq “\n|q|ÿ\ni“1\nlog pppqi|θdqq (4)\nWith qi the ith term of the query and |q| the query size. T o estimate ppqi|θdq, Dirichlet\nLanguage Model is used since it has been shown to work better w ith Translation Language\nModels (Karimzadehgan & Zhai, 2010) :\nppqi|θdq “ |d|\nµ ` | d| pmlpqi|θdq ` µ\nµ ` | d| ppqi|Cq (5)\n3 http://ims.dei.unipd.it/data/chic/\n4 Usually in IR the unigram language model give the best result s with the lowest computational cost\n(Manning et al., 2008)\nµ P R` is the smoothing parameter and pmlpqi|θdq is estimated using the maximum likeli-\nhood 5 and is equal to:\npmlpqi|θdq “ cpqi, d q\n|d| (6)\nWith cpqi, d q the frequency of qi in d and |d| the document size. The same goes for ppqi|Cq the\nsmoothing term which is also estimated with the maximum like lihood : ppqi|Cq “ cpqi, C q{|C|.\nThis estimation of ppqi|θdq leads to the following ranking formula (see the appendix for more\ndetails) :\nRSV pq, d q » rank\nÿ\ni:cpqi,d qą0\n„\nlog\nˆ\n1 ` cpqi, d q\nµppqi|Cq\n˙\n` | q|log\nˆ µ\nµ ` | d|\n˙\n(7)\nW e decided to present equations (5) and (7) even though they a re equivalent for ranking\ndocuments because both of them can be found to describe Diric hlet Language Model. In the\nframe of our work, and for reasons that are developed in the appendix, we will introduce the\nnext models by giving the expression of ppqi|θdq as in equation (5).\nAs we said previously, one issue of these models is the term mi smatch problem: as we can\nsee on equation (7) ranking takes into account only the terms that appear in both the considered\ndocument and query. Consequently, relevant documents that do not contain the exact query\nterms will not be considered. One approach to solve this prob lem is to adapt the language\nmodel to take into account the semantic similarities betwee n terms.\n3 T ranslation Language Models\nTranslation Language Models (TLM) try to estimate the seman tic similarity between two terms\nby using tools from statistical translation. The main idea i s to estimate the likelihood of translat-\ning a document to a query using the translation probability b etween terms (Karimzadehgan & Zhai,\n2010). T o do so, the maximum likelihood estimator in the Diri chlet language model pmlpqi|θdq\nis replaced with the likelihood that the query has been produ ced by a translation of the document\nptpqi|θdq:\nppqi|θdq “ |d|\nµ ` | d| ptpqi|θdq ` µ\nµ ` | d| ppqi|Cq (8)\nptpqi|θdq is calculated the following way :\nptpqi|θdq “\nÿ\nuPd\nptpqi|uqpmlpu|θdq (9)\nWith ptpw|uq the probability to translate term u into term w which is estimated using mutual\ninformation between u and w (Karimzadehgan & Zhai, 2010) :\nptpw|uq “ Ipw, u qř\nw1PV\nIpw1, u q (10)\n5 W e consider that terms in a document follow a multinomial dis tribution\nIpw, u q is the mutual information score between word u and w, deﬁned as follow :\nIpw, u q “\nÿ\nXw“0, 1\nÿ\nXu“0, 1\nppXw, X uqlog\nˆ ppXw, X uq\nppXwqppXuq\n˙\n(11)\nWith Xw and Xu binary random variables indicating if a word is absent or pre sent (refer to\n(Karimzadehgan & Zhai, 2010) for more details).\n4 W ord Embedding-based T ranslation Language Model\nWithin the frame of this work, word Embedding are used instea d of mutual information in order\nto estimate the translation probability ptpqi|uq. The model is named W ord Embedding-based\nTranslation Language Model (WETLM). W e consider the new est imation of the translation\nprobability, denoted pcospqi|uq, to be proportional to the similarity between qi and u that is\nmeasured with the cosine between the vectors of the two terms :\npcospqi|uq “ cospqi, u q\nř\nu1 PV\ncospu1, u q (12)\nConsequently the ranking formula becomes :\nppqi|θdq “ |d|\nµ ` | d| pcospqi|θdq ` µ\nµ ` | d| ppqi|Cq (13)\nWith pcospqi|θdq “ ř\nuPd pcospqi|uqpmlpu|θdq. Both the estimation of the translation probabil-\nity pt and pcos underestimate the self-translation probability: we can ha ve pcospu|wq ą pcospu|uq\nfor u ‰ w which is not desirable for a translation language model (Kar imzadehgan & Zhai,\n2012).\nOne way to make sure that the self-translation probability i s the highest for a given term is\nto redeﬁne it by introducing a hyper-parameter α P r 0, 1s that \"controls\" the self-translation\nprobability (Karimzadehgan & Zhai, 2010):\npcos´α pw|uq “\n#\nα ` p 1 ´ α qpcospw|uq if u “ w\np1 ´ α qpcospw|uq if u ‰ w (14)\nThis formula ensures the fact that we have pcos´α pu|uq ą pcos´α pu|wq@u, w P V for α ą 0. 5.\nThe model that uses pcos´α to estimate the translation probability will be referred as WETLM-α .\nW e set the value of α to 0.45 since it is the one that produced the best results for t he Threshold\nT = 0.7. This approach was not developed in the paper Integrating and Evaluating Neural W ord\nEmbedding in Information Retrieval by Zuccon et al. (2015) since they reported that the word\nEmbedding they used did not underestimate the self-transla tion probability.\n5 Implementation and data\nFor this work, instead of using an already existing Informat ion Retrieval System (IRS) such as\nT errier, we developed our own IRS in C++ to easily add word Embedding t o the classical mod-\nels and also because having a low retrieval time 6 is not an objective: before trying to compute\na fast IRS, we should make sure that the W ord Embedding-based language models outperforms\nstate of the art Language models. In order for our results to b e comparable with other work that\nused T errier, we made sure that with the same pre-processing on the corpus, we obtain the same\nresults as T errier ( see the appendix for more details about our IRS ). When doing so, we noticed\nthat T errier does not implement Dirichlet language model us ing equation (7) (more details in\nthe appendix).\nDuring pre-processing, we used a Stop List and replaced capi tal letters with lower case letters\non both the collection and the queries. In order to have the sa me results as T errier with our IRS,\nwe also removed characters that were not digits or letters an d deleted words that contained more\nthat 4 digits or more than 3 consecutive identical character s. W e did not use any stemmer on\nour collection since the best results were obtained without any stemming.\nW e used the Cultural Heritage in CLEF (CHiC) 2012 English col lection for ad hoc retrieval.\nThis collection is composed of 1 107 176 documents containin g \"metadata records describing\ndigital representations of cultural heritage objects\" (Pe tras et al., 2012) and 50 queries for ad\nhoc retrieval tasks. Below is a table summing up some statistics of the collection:\n#d A vdl V ocabulary Size #q A vql\n1 107 176 30.92 290 265 50 1.84\nT able 1: CHiC 2012 statistics\nWith #d and #q being respectively the number of documents in the collectio n and the num-\nber of queries. A vdl is the average document length and A vql i s the average query length. T o\nevaluate the models, the top 1000 documents were returned fo r each query, the MAP and P@10\nwere computed using the standard tool for evaluating an ad hoc retrieval: trec_eval.\nW e used the word2vec-GoogleNews-vectors word Embedding of dimension 300, pre-trained\non the google news Corpus (3 billion words) that are availabl e here 7 . As we said earlier, the\nCHiC collection we used have a very speciﬁc vocabulary and ev en if the word Embedding we\nused were trained on a 3 billion words corpus, a lot of word of t he vocabulary were missing :\n• only 42.68% of the words of the vocabulary have an Embedding;\n• but 91.92% of word occurrences in the collection have an Embe dding.\nOn the other hand, most of the queries’s terms had an Embeddin g :\n• 94.95% of the queries terms have an Embedding;\n• 2% of the queries have none of their term that posses an Embedd ing.\nFinally, the translation probability described in equatio n (12) is not the one that was imple-\nmented: we computed the cosine similarity between two words if it was above a given threshold\n6 Which is the case with T errier\n7 https://github.com/mmihaltz/word2vec-GoogleNews-vec tors\nT. This allowed us to reduce the number of cosine similarities to compute and also it acts like\na noise reducer since we did not take into account the similar ity between non similar terms.\nW e found that T = 0.7 produces the highest MAP on the CHiC collection using wo rd2vec-\nGoogleNews-vectors.\n6 Results\nInstead of giving the value of the parameter µ that produces the optimal results, we decided\nto display the MAP for a range of values of µ to see if the WETLM outperforms (or not) the\nDirichlet LM consistently or if for some values of µ one model performs better than the other.\nAt ﬁrst we evaluated the Dirichlet LM, the optimal value we fo und for µ was 44. Retrieved\ndocuments are evaluated using the Mean A verage Precision an d P@10:\nµ 12 16 20 24 28 32 36 40 44 48\nMAP (%) 35.61 36.09 36.16 36.24 36.30 36.36 36.39 36.23 36.43 36.09\nµ 52 56 60 64 68 72 76 80 84 88\nMAP (%) 36.05 36.06 35.82 35.86 35.92 35.92 36.04 35.77 35.68 35.68\nT able 2: V alues of the MAP on the CHIC2012 collection using th e Dirichlet Language Model\nµ 12 16 20 24 28 32 36 40 44 48\nP@10 (%) 33.54 33.75 33.75 33.96 34.17 34.38 34.17 34.17 34.38 34.38\nµ 52 56 60 64 68 72 76 80 84 88\nP@10 (%) 34.38 34.17 34.38 34.38 34.38 34.38 34.38 34.38 34.38 34.58\nT able 3: V alues of the P@10 on the CHIC2012 collection using t he Dirichlet Language Model\nW e checked that the results obtained are identical to the one s produced by T errier with the\nsame pre-processing on the collection. Also we decided to ex plore values of µ that are close\nto the A verage Document Length (A vdl) since the optimal valu e of µ in the Dirichlet Language\nModel is usually around the A vdl. T able 4 below represents th e results obtained with the WE-\nbased Translation Language Model :\nµ 12 16 20 24 28 32 36 40 44 48\nMAP (%) 36.89 37.71 37.76 37.86 37.78 37.79 37.81 37.65 37.67 37.29\nµ 52 56 60 64 68 72 76 80 84 88\nMAP (%) 37.17 36.87 36.66 36.69 36.57 36.56 36.55 36.50 36.37 36.35\nT able 4: V alues of the MAP on the CHIC2012 collection using WE TLM\nµ 12 16 20 24 28 32 36 40 44 48\nP@10 (%) 34.38 34.38 35.21 35.42 35.63 35.63 35.42 35.42 35.42 35.63\nµ 52 56 60 64 68 72 76 80 84 88\nP@10 (%) 35.42 35.21 35.21 35.21 35.21 35.21 35.21 35.21 35.21 35.21\nT able 5: V alues of the P@10 on the CHIC2012 collection using W ETLM\nAs we can see the WE-based Translation Language Model seems t o sightly outperform the\nDirichlet Language model for every µ. The optimal value of µ we found is different for the two\nmodels : µopt “ 44 for the Dirichlet LM and µopt “ 24 for the WETLM. W e performed a paired\nt-test over the average precision of each query for µ = 36 for both models, the measured p-value\nwith R is 0. 1733 > 0.01. Unfortunately the improvement is not statistically signiﬁcant.\nIn the table below we show some of the results collections pre sented in the work of (Zuccon et al.,\n2015) over the 3 collections AP88-89 , WSJ87-92 and DOTGOV :\nMethod AP88-89 WSJ87-92 DOTGOV\nMAP P@10 MAP P@10 MAP P@10\nDirichlet LM 22.69 39.60 21.71 40.80 18.73 24.60\nWETLM 24.27* 41.00 22.66* 42.40* 19.32 25.00\nT able 6: V alues of the MAP and P@10 reported by (Zuccon et al., 2015) on the collections\nAP88-89 , WSJ87-92 and DOTGOV using Dirichlet LM and WETLM. T he statistically signif-\nicant differences are indicated by *.\nAs we can see, according to (Zuccon et al., 2015), depending on the collection, the WETLM\ncan produce results that exhibit a statistically signiﬁcan t improvement of the MAP compared to\nDirichlet LM.\nT able 7 below represents the results obtained with the WE-ba sed Translation Language\nModel that \"controls\" the self translation probability wit h the parameter α :\nµ 12 16 20 24 28 32 36 40 44 48\nMAP (%) 37.35 37.92 38.07 38.15 38.28 38.31 38.35 38.19 38.17 37.81\nµ 52 56 60 64 68 72 76 80 84 88\nMAP (%) 37.72 37.73 37.53 37.58 37.59 37.47 37.40 37.10 37.06 37.05\nT able 7: V alues of the MAP on the CHIC2012 collection using WE TLM-α\nµ 12 16 20 24 28 32 36 40 44 48\nP@10 (%) 34.79 34.79 35.21 36.04 36.25 36.46 36.46 36.46 36.25 36.67\nµ 52 56 60 64 68 72 76 80 84 88\nP@10 (%) 36.25 36.25 36.25 36.25 36.25 36.25 36.25 36.25 36.25 36.25\nT able 8: V alues of the P@10 on the CHIC2012 collection using W ETLM-α\nW e performed a paired t-test over the average precision of ea ch query for µ = 36 to compare\nLM and WETLM- α models : the measured p-value with R is 0. 01219 > 0.01 : the improvement\nis still not statistically signiﬁcant.\n7 Conclusion and future work\nThe results we obtained are consistent with the ones in (Zucc on et al., 2015) since they observed\nthe same improvement as we did in the MAP . They also checked th at their improvements were\nindependent of the corpus and the training set for the W ord Em bedding: they do not need to be\ntrained on the same corpus used in retrieval. For now our resu lts are limited to one corpus and\none set of word embedding, one of our objective in the near fut ure is to perform the experiments\non different corpora and also to improve our model by conside ring the context of the terms of the\nquery by using phrase vectors (Mikolov et al., 2013) to replace the query or to perform query\nexpansion and by modifying the translation probability so t hat it satisﬁes a set of constraints\n(Karimzadehgan & Zhai, 2012).\nReferences\nBA N N O U R I., Z A R G AYO U NA H. & N A ZA R EN KO A. (2016). Modèle uniﬁé pour la recherche\nd’information sémantique. In N. P ER N ELLE , Ed., IC 2016 : 27es Journées francophones d’Ingénierie\ndes Connaissances (Proceedings of the 27th French Knowledg e Engineering Conference), Montpel-\nlier , France, June 6-10, 2016., p. 155–160. 2\nBER G ER A. & L A FFERTY J. (1999). Information retrieval as statistical translati on. In Proceedings of\nthe 22Nd Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR ’99, p. 222–229, New Y ork, NY , USA: ACM. 2\nBLEI D. M., N G A. Y . & J O R DA N M. I. (2003). Latent dirichlet allocation. J. Mach. Learn. Res. , 3,\n993–1022. 2\nCA R PIN ETO C. & R O M A N O G. (2012). A survey of automatic query expansion in informat ion retrieval.\nACM Comput. Surv ., 44(1), 1:1–1:50. 2\nDEERW ESTER S., D U M A IS S. T., F U R NA S G. W., L A N DAU ER T. K. & H A R SH M A N R. (1990). In-\ndexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION\nSCIENCE, 41(6), 391–407. 2\nHA R R IS Z. (1954). Distributional structure. W ord, 10(23), 146–162. 2\nKA R IM ZA D EH G A N M. & Z H A I C. (2010). Estimation of statistical translation models ba sed on mutual\ninformation for ad hoc information retrieval. In Proceedings of the 33rd International ACM SIGIR\nConference on Research and Development in Information Retr ieval, SIGIR ’10, p. 323–330, New\nY ork, NY , USA: ACM. 3, 4, 5\nKA R IM ZA D EH G A N M. & Z H A I C. (2012). Axiomatic analysis of translation language mode l for in-\nformation retrieval. In Proceedings of the 34th European Conference on Advances in I nformation\nRetrieval, ECIR’12, p. 268–280, Berlin, Heidelberg: Springer-V erla g. 5, 9\nMA N N IN G C. D., R AG H A V A N P . & S C H Ü TZE H. (2008). Introduction to Information Retrieval. New\nY ork, NY , USA: Cambridge University Press. 2, 3\nMIKO LOV T., C H EN K., C O R R A D O G. & D EA N J. (2013). Efﬁcient Estimation of W ord Representa-\ntions in V ector Space. ArXiv e-prints. 2, 9\nMIKO LOV T., S U TSK EV ER I., C H EN K., C O R R A D O G. & D EA N J. (2013). Distributed representations\nof words and phrases and their compositionality . In Proceedings of the 26th International Conference\non Neural Information Processing Systems, NIPS’13, p. 3111–3119, USA: Curran Associates Inc. 1\nPETR A S V ., F ER RO N., G Ä D E M., I SA AC A., K LEIN EB ER G M., M A SIERO I., N IC C H IO M. &\nSTILLER J. (2012). Cultural heritage in clef (chic) overview 2012. 6\nRO B ERTSO N S. E., W A LK ER S., J O N ES S., H A N C O C K-B EAU LIEU M. M., G ATFO R D M. et al. (1995).\nOkapi at trec-3. Nist Special Publication Sp, 109, 109. 2\nZU C C O N G., K O O PM A N B., B RU ZA P . & A ZZO PA R D I L. (2015). Integrating and evaluating neural\nword embeddings in information retrieval. In Proceedings of the 20th Australasian Document Com-\nputing Symposium, ADCS ’15, p. 12:1–12:8, New Y ork, NY , USA: ACM. 1, 2, 5, 8, 9\n8 Appendix\n8.1 From equation (5) to equation (7) :\nlog pppq|θdqq “\n|q|ÿ\ni“1\nlog pppqi|θdqq\n“\n|q|ÿ\ni“1\nlog\nˆ |d|\nµ ` | d| pmlpqi|θdq ` µ\nµ ` | d| ppqi|Cq\n˙\n“\n|q|ÿ\ni“1\nlog\nˆ cpqi, d q ` µppqi|Cq\n|d| ` µ\n˙\n“\nÿ\ni:cpqi,d qą0\nlog\nˆ cpqi, θ dq ` µppqi|Cq\n|d| ` µ\n˙\n`\nÿ\ni:cpqi,d q“0\nlog\nˆ µppqi|Cq\n|d| ` µ\n˙\n“\nÿ\ni:cpqi,d qą0\nlog\nˆ cpqi, d q ` µppqi|Cq\n|d| ` µ\n˙\n´\nÿ\ni:cpqi,d qą0\nlog\nˆ µppqi|Cq\n|d| ` µ\n˙\n`\n|q|ÿ\ni“1\nlog\nˆ µppqi|Cq\n|d| ` µ\n˙\n“\nÿ\ni:cpqi,d qą0\nlog\nˆ cpqi, d q ` µppqi|Cq\n|d| ` µ ˆ |d| ` µ\nµppqi|Cq\n˙\n`\n|q|ÿ\ni“1\nlog\nˆ µppqi|Cq\n|d| ` µ\n˙\n“\nÿ\ni:cpqi,d qą0\nlog\nˆ\n1 ` cpqi, d q\nµppqi|Cq\n˙\n` | q|log\nˆ µ\n|d| ` µ\n˙\n`\n|q|ÿ\ni“1\nlog pppqi|Cqq\nThe last term of the equation above depends only on the query a nd collection, therefore it\ncan be ignored to rank documents, which leads to :\nlog pppq|θdqq “\nÿ\ni:cpqi,d qą0\n„\nlog\nˆ\n1 ` cpqi, d q\nµppqi|Cq\n˙\n` | q|log\nˆ µ\nµ ` | d|\n˙\n8.2 Why equation (5) instead of equation (7) :\nAs we said previously, usually language model RSV functions are presented using a sum that\ngoes through terms that appear in both the document and the qu ery, as in equation (7). The\nreason is that this formulation is more compatible with the u sage of an inverted index. In fact\nan inverted index just associates to a term, all documents wi th non null term frequency: this is\ncalled the posting list. When using inverted index, the matc hing computation have to compute\npartial RSV sum for all documents found in posting lists, in p arallel. If the formula only requires\nterms to have non null frequency in documents as in formula (7 ) the matching algorithm is\ntrivial. If the formula need all terms of the query, as in form ula (5) then some partial matching\nsums has to be corrected during matching computation and the matching algorithm is much\nmore complex and less efﬁcient.\nIn our experimental system, we do not use any inverted index: we store the all index in a\ndirect structure in main memory. Hence, matching is done by s imply browsing all documents\nand computing RSV strait using the formula.\nThe models we presented that used statistical translation a nd word Embedding cannot be\nwritten with only a sum over q X d as in equation (7) : this is why we used equation (5) to give\nthe expression of the rank. Hence the implementation using a inverted ﬁl is more problematic,\nand for example, cannot be done in a simple way in T errier. Tha t is the reason why we could\nnot use T errier for these experiments.\n8.3 T errier’s Dirichlet Language Model :\nMoreover during the experiment, when we tried to obtain the s ame results as T errier, we noticed\nthat they did not implement exactly equation (5) to compute t he score of each document. Below\nis the formula they used to compute the score of documents :\nlog pppq|θdqq “\nÿ\ni:cpqi,d qą0\n„\nlog\nˆ\n1 ` cpqi, d q\nµppqi|Cq\n˙\n` log\nˆ µ\nµ ` | d|\n˙\n(15)\nCompared to equation (5) where the quantity log\n´\nµ\nµ `|d|\n¯\nis computed |q| times, this formula\ncomputes it only |q X d| times. Since log\n´\nµ\nµ `|d|\n¯\nă 0, equation(15) gives a little bit more\nimportance to query terms that do not appear in documents. Fo r our experiment, the Dirichlet\nlanguage model was implemented using equation (5) but we als o checked that we obtained the\nsame results as T errier when computing equation (15).\n8.4 Some details about the implemented Information Retriev al System\nIn order to have the same results as T errier, our IRS performe d the following pre-processing on\nthe collection :\n• W e replaced with a space all the characters that were not inte gers or letters, for example\nthe term pre-processing is broken down into the two terms pre and processing.\n• W e also replaced majuscule letters with their minuscule equ ivalent\n• W e deleted terms that contained more than 4 digits\n• W e deleted terms that had more than 3 consecutive identical c haracters\nAlso, when we implemented equation (13) ( the ranking formul a of the WETLM), we de-\ncided to remove the normalization terms when pcospqi, θ dq or ppqi|Cq were equal to 0 :\nppqi|θdq “\n$\n’\n’\n’\n&\n’\n’\n’\n%\n|d|\nµ `|d| pcospqi|θdq ` µ\nµ `|d| ppqi|Cq if pcospqi|θdq ‰ 0 and ppqi|Cq ‰ 0\npcospqi|θdq if pcospqi|θdq ‰ 0 and ppqi|Cq “ 0\nppqi|Cq if pcospqi|θdq “ 0 and ppqi|Cq ‰ 0\n(16)\nW e decided to do so in order to avoid underweighting the trans lation probability of a query\nterm that is not in the collection and conversely to avoid und erweighting the smoothing term\nof associated to a word that is in the collection but does not h ave any similar terms in the\nconsidered document.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.783730149269104
    },
    {
      "name": "Word embedding",
      "score": 0.6902539134025574
    },
    {
      "name": "Natural language processing",
      "score": 0.6892392039299011
    },
    {
      "name": "Word (group theory)",
      "score": 0.6850278973579407
    },
    {
      "name": "Embedding",
      "score": 0.6670275926589966
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6160928010940552
    },
    {
      "name": "Machine translation",
      "score": 0.5822029709815979
    },
    {
      "name": "Translation (biology)",
      "score": 0.5556989312171936
    },
    {
      "name": "Cosine similarity",
      "score": 0.5544885993003845
    },
    {
      "name": "Language model",
      "score": 0.5263156890869141
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5089576244354248
    },
    {
      "name": "Task (project management)",
      "score": 0.49894261360168457
    },
    {
      "name": "Semantic similarity",
      "score": 0.43014946579933167
    },
    {
      "name": "Speech recognition",
      "score": 0.33248502016067505
    },
    {
      "name": "Linguistics",
      "score": 0.15359711647033691
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.10676345229148865
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    }
  ],
  "cited_by": 5
}