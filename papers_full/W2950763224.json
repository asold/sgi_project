{
    "title": "PageRank without hyperlinks: Structural re-ranking using links induced by language models",
    "url": "https://openalex.org/W2950763224",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4282048637",
            "name": "Kurland, Oren",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288524351",
            "name": "Lee, Lillian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2787893582",
        "https://openalex.org/W2066867064",
        "https://openalex.org/W2146442558",
        "https://openalex.org/W2133576408",
        "https://openalex.org/W2130395434",
        "https://openalex.org/W2138621811",
        "https://openalex.org/W1514403774",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W2114512077",
        "https://openalex.org/W1988914905",
        "https://openalex.org/W2144270295",
        "https://openalex.org/W2099194852",
        "https://openalex.org/W1525595230",
        "https://openalex.org/W2095368471",
        "https://openalex.org/W2074449313",
        "https://openalex.org/W3101913037",
        "https://openalex.org/W1514324734",
        "https://openalex.org/W2136542423",
        "https://openalex.org/W1540124269",
        "https://openalex.org/W2127086485",
        "https://openalex.org/W1993972354",
        "https://openalex.org/W2053549370",
        "https://openalex.org/W2111557120",
        "https://openalex.org/W2027445772",
        "https://openalex.org/W1494864219",
        "https://openalex.org/W2066636486",
        "https://openalex.org/W2000569744",
        "https://openalex.org/W1585620735",
        "https://openalex.org/W2114524997",
        "https://openalex.org/W2061198046"
    ],
    "abstract": "Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural re-ranking approach to ad hoc information retrieval: we reorder the documents in an initially retrieved set by exploiting asymmetric relationships between them. Specifically, we consider generation links, which indicate that the language model induced from one document assigns high probability to the text of another; in doing so, we take care to prevent bias against long documents. We study a number of re-ranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks.",
    "full_text": "arXiv:cs/0601045v1  [cs.IR]  11 Jan 2006\nPageRank without Hyperlinks: Structural Re­Ranking\nusing Links Induced by Language Models\nOren Kurland 1, 3\nkurland@cs.cornell.edu\n1. Computer Science Department, Cornell University , Ithaca NY 14853, U.S.A.\n2. Language T echnologies Institute, Carnegie Mellon University , Pittsburgh P A 15213, U.S.A.\n3. Computer Science Department, Carnegie Mellon University , Pittsburgh P A 15213, U.S.A.\nLillian Lee 1, 2, 3\nllee@cs.cornell.edu\nABSTRACT\nInspired by the PageRank and HITS (hubs and authorities)\nalgorithms for Web search, we propose a structural re-rank-\ning approach to ad hoc information retrieval: we reorder the\ndocuments in an initially retrieved set by exploiting asym-\nmetric relationships between them. Speciﬁcally, we consid er\ngeneration links , which indicate that the language model in-\nduced from one document assigns high probability to the\ntext of another; in doing so, we take care to prevent bias\nagainst long documents. We study a number of re-ranking\ncriteria based on measures of centrality in the graphs formed\nby generation links, and show that integrating centrality i nto\nstandard language-model-based retrieval is quite eﬀectiv e at\nimproving precision at top ranks.\nCategories and Subject Descriptors: H.3.3 [Informa-\ntion Search and Retrieval]: Retrieval models\nGeneral Terms: Algorithms, Experimentation\nKeywords: language modeling, PageRank, HITS, hubs,\nauthorities, social networks, high-accuracy retrieval, g raph-\nbased retrieval, structural re-ranking\n1. INTRODUCTION\nInformation retrieval systems capable of achieving high\nprecision at the top ranks of the returned results would be\nof obvious beneﬁt to human users, and could also aid pseudo-\nfeedback approaches, question-answering systems, and oth er\napplications that use IR engines for pre-processing purpos es\n[31, 35, 32]. But crafting such systems remains a key re-\nsearch challenge.\nThe PageRank Web-search algorithm [1] uses explicitly-\nindicated inter-document relationships as an additional s ource\nof information beyond textual content, computing which\ndocuments are the most central. Here, we consider adapting\nthis idea to corpora in which explicit links between docu-\nments do not exist.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee.\nSIGIR’05, August 15–19, 2005, Salvador, Brazil.\nCopyright 2005 ACM 1-59593-034-5/05/0008 ...$5.00.\nHow should we form links in a non-hypertext setting?\nWhile previous work in summarization has applied Page-\nRank to cosine-based links [4], we draw on research demon-\nstrating the success of using language models to improve IR\nperformance in general [30, 2] and to model inter-document\nrelationships in particular [16]. Speciﬁcally, we employ gen-\neration links , which are based on the probability assigned by\nthe language model induced from one document to the term\nsequence comprising another. 1 Our use of such links echoes\nthe standard language-model-based ranking principle, ﬁrs t\nintroduced in [30], that a document is relevant to the extent\nthat its corresponding language model assigns high proba-\nbility to the query. However, given that we are working with\nmultiple documents rather than a single query, we employ\na technique that compensates for length bias in estimating\ngeneration probabilities.\nWe note that the analogy between hyperlinks and gener-\nation links is not perfect. In particular, one can attribute\nmuch of the success of link-based Web-search algorithms to\nthe fact that hyperlinks are (often) human-provided certi-\nﬁcations that two pages are truly related [13]. In contrast,\nautomatically-induced generation links are surely a noisi er\nsource of information. T o compensate, we advocate an ap-\nproach (used elsewhere as well [39, 10, 13, 20, 37, 22]) that\nwe term structural re-ranking : we use inter-document rela-\ntionships to compute an ordering not of the entire corpus,\nbut of a (possibly unranked) set of documents produced\nby an initial retrieval method. This set should provide a\nreasonable ratio of relevant to non-relevant documents, an d\nthus form a good foundation for our algorithms. Note that\nour approach diﬀers in spirit from pseudo-feedback-based\nmethods [31], which deﬁne a model based on the initially\nretrieved documents expressly in order to re-rank the entir e\ncorpus. Indeed, since the quality of the initially retrieve d\nresults plays a major role in determining the eﬀectiveness\nof pseudo-feedback-based algorithms [35], our methods can\npotentially serve to greatly enhance the input to them.\nT o compute centrality values for a given generation graph,\nwe propose a number of methods, including variants of Page-\nRank [1] and HITS (a.k.a. hubs and authorities) [13]. Com-\nparisons on various TREC datasets against numerous base-\nlines (including use of cosine-based links and re-ranking e m-\n1 While the term “generate” is convenient, we do not think\nof a “generator” document or language model as literally\n“creating” others. Other work further discusses this issue\nand proposes alternate terminology (e.g., “render”) [17].\nploying only document-speciﬁc characteristics) show that\nlanguage-model-based re-ranking using centrality as a for m\nof “document prior” is indeed successful at moving relevant\ndocuments in the initial retrieval results higher up in the\nlist.\n2. STRUCTURAL RE­RANKING\nThroughout this section, we assume that the following\nhave been ﬁxed: the corpus C (in which each document has\nbeen assigned a unique numerical ID); the query q; the set\nDinit ⊆ C of top documents returned by some initial re-\ntrieval algorithm in response to q (this is the set upon which\nre-ranking is performed); and the value of an ancestry pa-\nrameter α that pertains to our graph construction process.\nF or each document d ∈ C , pd(·) denotes the smoothed\nunigram language model induced from d (estimation details\nappear in Section 2.4). We use g and o to distinguish be-\ntween a document treated as a “generator” and a document\ntreated as “oﬀspring”, that is, something that is generated\n(details below).\nWe use the notation ( V, w t) for weighted directed graphs:\nV is the set of vertices and w t : V × V → { y ∈ ℜ : y ≥ 0}\nis the edge-weight function . Thus, there is a directed edge\nbetween every ordered pair of vertices, but w t may assign\nzero weight to some edges. We write w t(v1 → v2) to denote\nthe value of w t on edge ( v1, v 2).\n2.1 Generation Graphs\nOur use of language models to form links can be moti-\nvated by considering the following two documents:\nd1: T oronto Sheﬃeld Salvador\nd2: Salvador Salvador Salvador\nKnowing that d2 is important (i.e., central or relevant) would\nprovide strong evidence that d1 is at least somewhat impor-\ntant. However, knowing that d1 is very important does not\nallow us to conclude that d2 is, since the importance of d1\nmight stem from its ﬁrst two terms. Using language models\ninduced from documents enables us to capture this asymme-\ntry in how centrality is propagated: we allow a document\nd to receive support for centrality status from a document\no only to the extent that pd(o) is relatively large. (If o is\nnot in fact important, the support it provides may not be\nsigniﬁcant.) Note that ranking documents by pd(q), as ﬁrst\nproposed by Ponte and Croft [30], can be considered a vari-\nation of this principle.\nWe are thus led to the following deﬁnitions.\nDefinition 1. The top α generators of a document d ∈\nDinit, denoted T opGen(d), is the set of α documents g ∈\nDinit −{d} that yield the highest pg (d), where ties are broken\nby document ID. (We suppress α in our notation for clarity.)\nDefinition 2. The oﬀspring of a document d ∈ Dinit are\nthose documents that d is a top generator of, i.e., the set\n{o ∈ Dinit : d ∈ T opGen(o)}.\nNote that multiple documents can share oﬀspring, and that\nit is possible for a document to have no oﬀspring.\nWe can encode top-generation relationships using either\nof two generation graphs GU = ( Dinit, w tU ) and GW =\n(Dinit, w tW ), where for o, g ∈ Dinit ,\nw tU (o → g) =\n{\n1 if g ∈ T opGen(o),\n0 otherwise;\nw tW (o → g) =\n{\npg (o) if g ∈ T opGen(o),\n0 otherwise .\nThus, in both graphs, positive-weight edges lead only from\noﬀspring to their respective top α generators; but GU treats\n(edges to) the top generators of o uniformly, whereas GW\ndiﬀerentially weights them by the probability their induced\nlanguage models assign to o.\nSome of our algorithms require “smoothed” versions of\nthese graphs, in which all edges (including self-loops) hav e\nnon-zero weight, to work correctly. T o be speciﬁc, we employ\nPageRank’s [1] smoothing technique.\nDefinition 3. Given an edge-weighted directed graph G =\n(Dinit, w t) and smoothing parameter λ ∈ [0, 1), the smoothed\ngraph G[λ ] = ( Dinit, w t[λ ]) has edge weights deﬁned as fol-\nlows: for every o, g ∈ Dinit.\nw t[λ ](o → g) = (1 − λ ) · 1\n|Dinit| + λ · w t(o → g)∑\ng′∈Dinit\nw t(o → g′) ,\nThe weights of all edges leading out of any given node in G[λ ]\nsum to 1 and thus may be treated as transition probabilities.\nWith these concepts in hand, we can now phrase our\ncentrality-determination task as follows: given a generat ion\ngraph, compute for each node (i.e., document) how much\ncentrality is “transferred” to it from other nodes — by our\nedge-weight deﬁnitions, centrality therefore correspond s to\nthe degree to which a document is responsible for “generat-\ning” (perhaps indirectly) the other documents in the initia lly\nretrieved set. We now consider diﬀerent ways to formalize\nthis notion of transferrence of centrality.\n2.2 Computing Graph Centrality\nA straightforward way to deﬁne the centrality of a docu-\nment d with respect to a given graph G = ( Dinit, w t) is to\nset it to d’s weighted in-degree, which we call its inﬂux :\nC enI (d; G)\ndef\n=\n∑\no∈Dinit\nw t(o → d). (1)\nThe Uniform Inﬂux algorithm sets G = GU , so that the\nonly thing that matters is how many oﬀspring d has; it is\nthus reminiscent of the journal impact factor function from\nbibliometrics [5], which computes normalized counts of ex-\nplicit citation links. The Weighted Inﬂux algorithm sets\nG = GW , so that the generation probabilities that d assigns\nto its oﬀspring are factored in as well.\nAs previously noted by Pinski and Narin in their work\non inﬂuence weights [29], one intuition not accounted for by\nweighted in-degree methods is that a document with even\na great many oﬀspring should not be considered central (or\nrelevant) if those oﬀspring are themselves very non-centra l.\nWe can easily modify Equation 1 to model this intuition; we\nsimply scale the evidence from a particular oﬀspring doc-\nument by that oﬀspring’s centrality, thus arriving at the\nfollowing recursive equation:\nCenRI (d; G)\ndef\n=\n∑\no∈Dinit\nw t(o → d) · CenRI (o; G), (2)\nwhere we also require that ∑\nd∈Dinit\nCenRI (d; G) = 1. Un-\nfortunately, for arbitrary GU and GW , Equation 2 may not\nhave a unique solution or even any solution at all under the\nnormalization constraint just given; however, a unique so-\nlution is guaranteed to exist for their PageRank-smoothed\nversions.2 By analogy with the two inﬂux algorithms given\nabove, then, we have the Recursive Uniform Inﬂux al-\ngorithm, which sets G = G[λ ]\nU and is a direct analog of\nPageRank, and the Recursive Weighted Inﬂux algorithm,\nwhich sets G = G[λ ]\nW .\n2.3 Incorporating Initial Scores\nThe centrality scores presented above can be used in iso-\nlation as criteria by which to rank the documents in Dinit .\nHowever, if available, it might be useful to incorporate mor e\ninformation from the initial retrieval engine to help handl e\ncases where centrality and relevance are not strongly corre -\nlated. (Recall that it participates in any case by specifyin g\nthe set Dinit.) In our experiments, we explore one concrete\ninstantiation of this approach: we apply language-model-\nbased retrieval [30, 2] to determine Dinit, and consider the\nfollowing family of re-ranking criteria:\nC en(d; G) · pd(q), (3)\nwhere d ∈ D init and C en is one of the centrality functions\ndeﬁned in the previous section. This gives rise to the al-\ngorithms Uniform Inﬂux +LM, Weighted Inﬂux +LM,\nRecursive Uniform Inﬂux +LM, and Recursive Weighted\nInﬂux +LM.\nIncidentally, our choosing pd(q) as initial score function\nhas the interesting consequence that it suggests interpret ing\nC en(d; G) as a document “prior” — in fact, Laﬀerty and\nZhai write, “with hypertext, [a document prior] might be\nthe distribution calculated using the ‘PageRank’ scheme”\n[18]. We will return to this idea later.\n2.4 Estimating Generation Probabilities: Length\nand Entropy Effects\nGeneration probabilities form the basis for the graphs on\nwhich our algorithms are deﬁned. This section describes our\nmethod for estimating these probabilities.\nLet tf( w ∈ x) denote the number of times the term w\noccurs in the text or text collection x. What is often called\nthe maximum-likelihood estimate (MLE) of w with respect\nto x is deﬁned as\n˜p M LE\nx (w)\ndef\n= tf(w ∈ x)\n∑\nw′ tf(w′ ∈ x) .\nSome prior work in language-model-based retrieval [22, 40]\nemploys a Dirichlet-smoothed version:\n˜p [µ ]\nx (w)\ndef\n= tf(w ∈ x) + µ · ˜p M LE\nC (w)\n∑\nw′ tf(w′ ∈ x) + µ ;\nthe smoothing parameter µ controls the degree of reliance on\nrelative frequencies in the corpus rather than on the counts\nin x. Both estimates just described are typically extended\n2The edge weights correspond to the transition probabili-\nties for a Markov chain that is aperiodic and irreducible,\nand hence has a unique stationary distribution [8] that can\nbe computed by a variety of means [34, 6, 7]. In our exper-\niments, power iteration converged very quickly.\nto distributions over term sequences by assuming that terms\nare independent: for an n-term text sequence w1w2 · · · wn ,\npM LE\nx (w1w2 · · · wn)\ndef\n=\nn∏\nj=1\n˜p M LE\nx (wj );\np[µ ]\nx (w1w2 · · · wn)\ndef\n=\nn∏\nj=1\n˜p [µ ]\nx (wj ).\nAnother estimation approach, which we adopt, incorporates\nthe Kullback-Leibler divergence D between document lan-\nguage models [16, 17] (see also previously proposed ranking\nprinciples [26, 18]): unless otherwise speciﬁed, for docum ent\nd and word sequence s (in our setting, either a document or\nthe query), we set pd(s) to\npKL,µ\nd (s)\ndef\n= exp\n(\n−D\n(\n˜p M LE\ns (·)\n⏐\n⏐\n⏐\n⏐\n⏐\n⏐ ˜p [µ ]\nd (·)\n))\n. (4)\nEquation 4 has some useful properties. We can show that\npKL,µ\nd (s) = ( p[µ ]\nd (s))\n1\n|s|\n  \nterm A\n· exp(H (˜p M LE\ns (·)))  \nterm B\n,\nwhere H is the entropy function. Now, observe that for\nboth pM LE\nx (·) and p[µ ]\nx (·), longer text sequences tend to be\nassigned lower probabilities; this would correspond to an\nunmotivated reduction of weights for edges out of long docu-\nments in the graph GW . However, T erm A length-normalizes\np[µ ]\nd (s) via the geometric mean , which has helped amelio-\nrate numerical problems in previous work [19]. Addition-\nally, term B raises the generation probability for texts wit h\nhigh-entropy MLE term distributions. High entropy may be\ncorrelated with a larger number of unique terms — for exam-\nple, we get an entropy of 0 for the document “Salvador Sal-\nvador Salvador” but log 3 for “T oronto Sheﬃeld Salvador”\n— which, in turn, has previously been suggested as a cue for\nrelevance [33, 11]. Hence, generators of documents inducin g\nhigh-entropy language models may be good candidates for\ncentrality status. (We hasten to point out, though, that for\nthe algorithms based on smoothed graphs (Deﬁnition 3), the\nentropy term cancels out due to our normalization of edge\nweights.)\n3. RELA TED WORK\nWork on structural re-ranking in traditional ad hoc in-\nformation retrieval has mainly focused on query-dependent\nclustering, wherein one seeks to compute and exploit a clus-\ntering of the initial retrieval results [39, 10, 20, 37, 22].\nClusters represent structure within a document set, but do\nnot directly induce an obvious single criterion or principl e\nby which to rank documents; for instance, they have been\nused to improve rankings indirectly by serving as smooth-\ning mechanisms [22]. Interestingly, some centrality measu res\nhave been previously employed to produce clusterings [36].\nThere has been increasing use of techniques based on\ngraphs induced by implicit relationships between document s\nor other linguistic items [9, 3, 12, 4, 24, 28, 38]. The work\nin the domain of text summarization [4, 24] most resembles\nours, in that it also computes centrality on graphs (althoug h\nthe nodes correspond to sentences or terms instead of doc-\numents). Perhaps the main contrast with our work is that\nlinks were not induced by generation probabilities; Sectio n\n4.2 presents the results of experiments studying the relati ve\nmerits of our particular choice of link deﬁnition.\nOur centrality scores constitute a relationship-based re-\nranking criterion that can serve as a bias aﬀecting the initi al\nretrieval engine’s scores, as in Equation 3. Alternative bi ases\nthat are based on individual documents alone have also been\ninvestigated. F unctions incorporating document or averag e\nword length [11, 14, 25] are applicable in our setting; we\nreport on experiments with (variants of) document length\nin Section 4.2. Other previously suggested biases that may\nbe somewhat less appropriate for general domains include\ndocument source [25] and creation time [21], and webpage\nhyperlink in-degree and URL form [15].\n4. EV ALU A TION\n4.1 Experimental Setting\nThe objective of structural re-ranking is to (re-)order an\ninitially-retrieved document set Dinit so as to improve preci-\nsion at the very top ranks of the ﬁnal results. Therefore, we\nemployed the following three evaluation metrics: the preci -\nsion of the top 5 documents (prec@5), the precision of the\ntop 10 documents (prec@10), and the mean reciprocal rank\nof the ﬁrst relevant document (MRR) [32].\nWe are interested in the general validity of the various\nstructural re-ranking methods we have proposed. We be-\nlieve that a good way to emphasize the eﬀectiveness (or lack\nthereof) of the underlying principles is to downplay the rol e\nof parameter tuning. Therefore, we made the following de-\nsign decisions, with the eﬀect that the performance numbers\nwe report are purposely not necessarily the best achievable\nby exhaustive parameter search :\n• The initial ranking that created the set Dinit was built\naccording to the function pKL,µ\nd (q) where the value of\nµ was chosen to optimize the non-interpolated average\nprecision of the top 1000 retrieved documents. This\nis not one of our evaluation metrics, but is a reason-\nable general-purpose optimization criterion. (In fact,\nresults with this initial ranking turned out to be statis-\ntically indistinguishable from the results obtained by\noptimizing with respect to the actual evaluation met-\nrics, although of course they were lower in absolute\nterms.)\n• We only optimized settings for α (the ancestry parame-\nter controlling the number of top generators considered\nfor each document) and λ (the edge-weight smoothing\nfactor) with respect to precision among the top 5 docu-\nments, not with respect to all three evaluation metrics\nemployed.\nThe search ranges for the latter two parameters were:\nα : 4 , 9, 19, . . . , |Dinit | − 1\nλ : 0 , 0. 05, 0. 1, 0. 2, . . . , 0. 9, 0. 95\nAs it turned out, for many instances (except for the Weighted\nInﬂux algorithm), the optimal value of α with respect to\nprecision at 5 was either 4 or 9, suggesting that a relatively\nsmall number of generators per document should be consid-\nered when constructing the graph. In contrast, λ exhibited\nsubstantial variance in optimal value for precision at 5 in\nsome of our datasets. We set |Dinit |, the number of initially-\nretrieved documents, to 50 in all results reported below (si m-\nilar performance patterns were obtained when |Dinit | = 100).\nThe remaining details are as follows. We conducted our\nexperiments on the following four TREC corpora:\ncorpus # of docs queries disk(s)\nAP89 84,678 1-46,48-50 1\nAP 242,918 51-64, 66-150 1-3\nWSJ 173,252 151-200 1-2\nTREC8 528,155 401-450 4-5\n(AP89 is a subset of AP containing articles just from the\nyear 1989). All documents and queries (in our case, TREC-\ntopic titles) were stemmed using the Porter stemmer and to-\nkenized, but no other pre-processing steps were applied. We\nused the Lemur toolkit [27] for language-model estimation.\nStatistically-signiﬁcant diﬀerences in performance were de-\ntermined using the two-sided Wilcoxon test at a conﬁdence\nlevel of 95%.\n4.2 Results\nIn the tables that follow, we use the following abbrevia-\ntions for algorithm names.\nU-In Uniform Inﬂux\nW-In Weighted Inﬂux\nR-U-In Recursive Uniform Inﬂux\nR-W-In Recursive Weighted Inﬂux\nU-In+LM Uniform Inﬂux +LM\nW-In+LM Weighted Inﬂux +LM\nR-U-In+LM Recursive Uniform Inﬂux +LM\nR-W-In+LM Recursive Weighted Inﬂux +LM\n4.2.1 Primary evaluations\nOur main experimental results are presented in T able 1.\nThe ﬁrst three rows specify reference-comparison data. The\ninitial ranking was, as described above, produced using pKL,µ\nd (q)\nwith µ chosen to optimize for non-interpolated precision at\n1000. The empirical upper bound on structural re-ranking ,\nwhich applies to any algorithm that re-ranks Dinit, indicates\nthe performance that would be achieved if all the relevant\ndocuments within the initial ﬁfty were placed at the top\nof the retrieval list: note that these bounds indicate that\nthe initial rankings for AP89 are quite worse than those for\nthe other three corpora. We also computed an optimized\nbaseline for each metric m and test corpus C; this consists\nof ranking all the documents (not just those in Dinit) by\npKL,µ\nd (q), with µ chosen to yield the best m-results on C. As\na sanity check, we observe that the performance of the initia l\nretrieval method is always below that of the corresponding\noptimized baseline (though not statistically distinguish able\nfrom it).\nThe ﬁrst question we are interested in is how our struc-\ntural re-ranking algorithms taken as a whole do. As shown\nin T able 1, our methods improve upon the initial ranking\nin many cases, speciﬁcally, roughly 2/3 of the 96 relevant\ncomparisons (8 centrality-based algorithms × 4 corpora × 3\nevaluation metrics). An even more gratifying observation i s\nthat T able 1 shows (via italics and boldface) that in many\ncases, our algorithms, even though optimized for precision\nat 5, can outperform a language model optimized for a dif-\nferent (albeit related) metric m even when performance is\nmeasured with respect to m; see, for example, the results\nfor precision at 10 on the AP corpus.\nCloser examination of the results in T able 1 reveals that\nin about 60% of the 48 relevant comparisons, our algorithms\nnot only are at least as eﬀective when applied to the graph\nGW as when applied to GU , but often yield better perfor-\nmance results; the comparison between Recursive Weighted\nAP89 AP WSJ TREC8\nprec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR\nupper bound 63. 7 53. 1 75. 5 87. 6 78. 8 93. 0 89. 6 80. 0 100. 0 94. 4 85. 0 98. 0\ninit. ranking 28. 3 26. 5 52. 3 45. 7 43. 2 59. 6 54. 8 48. 4 76. 2 50. 0 45. 6 69. 1\nopt. baselines 30. 0 27. 4 54.3 46. 5 43. 9 63. 5 56. 0 49. 4 77. 2 51. 2 46. 4 69.6\nU-In 29. 6 27 . 8 39. 5 o 50 . 9 49 . 0 i\no\n66.3 50. 0 46. 6 66. 7 50. 0 45. 0 62. 0\nW-In 31 . 3 29 . 6 46. 8 51 . 3 48 . 7 i 64 . 4 52. 0 47. 8 63. 3 o 49. 2 43. 4 63. 7\nU-In+LM 33.5 27. 0 46. 5 51 . 3 i 49.4 i\no\n63. 2 56 . 4 49. 2 73. 6 52 . 8 52.0 i\no\n66. 6\nW-In+LM 31 . 7 27 . 6 48. 4 51 . 1 i 48 . 4 i\no\n63. 0 57 . 2 50 . 0 77. 2 51 . 6 49 . 6 i 64. 5\nR-U-In 31 . 3 28 . 9 46. 4 51 . 5 48 . 9 i 63. 4 53. 6 49 . 6 68. 5 52 . 0 44. 6 66. 5\nR-W-In 32 . 2 29 . 6 40. 5 o 52 . 1 i 49 . 1 i\no\n63 . 9 54. 0 49. 2 70. 2 52 . 4 44. 6 66. 5\nR-U-In+LM 33 . 0 29 . 3 45. 8 52 . 1 i\no\n49 . 2 i\no\n64 . 3 58.8 i 51.0 i 78.6 55 . 6 46. 0 68. 4\nR-W-In+LM 33.5 29.8 46. 0 52.9 i\no\n49 . 0 i\no\n62. 6 58.8 i 50 . 6 78.6 56.0 45. 8 67. 6\nTable 1: Primary experimental results, showing algorithm p erformance with respect to our 12 evaluation\nsettings (3 performance metrics × 4 corpora). For each evaluation setting, improvements over the optimized\nbaselines are given in italics; statistically signiﬁcant d iﬀerences between our structural re-ranking algorithms\nand the initial ranking and optimized baselines are indicat ed by i and o respectively; bold highlights the best\nresults over all ten algorithms.\nNotice that even though the structural re-ranking algorith ms were optimized for prec@5 only (and produce\nthe best results for this metric), they still perform well wi th respect to the other two metrics.\nInﬂux (R-W-In) and Recursive Uniform Inﬂux (R-U-In) is a\ngood example. These results imply that it is a bit better to\nexplicitly incorporate generation probabilities into the edge\nweights of our generation graphs than to treat all the top\ngenerators of a document equally.\nAnother observation we can draw from T able 1 is that\nadding in query-generation probabilities as weights on the\ncentrality scores (see Equation 3) tends to enhance perfor-\nmance. This can be seen by comparing rows labeled with\nsome algorithm abbreviation “X” against the correspond-\ning rows labeled “X+LM”: about 80% of the 48 relevant\ncomparisons exhibit this improvement. Most of the coun-\nterexamples occur in settings involving precision at 10 and\nMRR, which we did not optimize our algorithms for.\nSimilarly, by comparing “Y”-labeled rows with “R-Y”-\nlabeled ones, we see that in about 70% of the 48 relevant\ncomparisons, it is better to use the recursive formulation o f\nEquation 2, where the centrality of a document is aﬀected\nby the centrality of its oﬀspring, than to ignore oﬀspring\ncentrality as is done by Equation 1.\nPerhaps not surprisingly, then, the Recursive Uniform In-\nﬂux +LM and Recursive Weighted Inﬂux +LM algorithms,\nwhich combine the two preferred features just described (re -\ncursive centrality computation and use of the initial searc h\nengine’s score function) appear to be our best performing\nalgorithms: working from a starting point below the op-\ntimized baselines, they improve the initial retrieval set t o\nyield results that even at their worst, are not only clearly\nbetter than the initial ranking for precision at 5 and 10, but\nare also merely statistically indistinguishable from the o pti-\nmized baselines. Moreover, in one setting (AP , precision at\n10) they actually produce statistically signiﬁcant improv e-\nments over the optimized baseline even though they were\nnot optimized for that evaluation metric.\nIt is interesting to note that the relative performance of\nour algorithms does not seem to depend strongly on the\nquality of the initial ranking, in the following sense. The\naverage percentage of relevant documents among the 50 that\nare initially retrieved is 21%, 35 . 5%, 33 . 3% and 30 . 3% for\nAP89, AP , WSJ and TREC8, respectively, but the relative\nimprovements for precision at 5 and 10 that our algorithms\nachieve with respect to the initial ranking are almost alway s\nhigher on AP89 than on WSJ or TREC8.\n4.2.2 Links based on the vector­space model\nWe have advocated the use of generation relationships\nto deﬁne centrality, where these asymmetric relationships\nare based on language-model probabilities. However, other\ninter-document relationships have been previously exploi ted\nin information retrieval. Perhaps the most well-known is\nvector-space proximity , with the cosine frequently used as\n(symmetric) closeness metric; indeed, as mentioned above,\nprevious work in summarization [4] has used the cosine to\ndetermine centrality in ways very similar to the ones we have\nconsidered. It is thus important to examine whether the\nperformance improvements we have achieved can be repro-\nduced, or even surpassed, by the use of vector-space-based\nlinks rather than language-model-based generation links.\nT o run this evaluation, we simply modiﬁed Deﬁnition 1\nand all eight of our structural re-ranking algorithms to use\nthe cosine of the angle between log tf.idf document vectors,\nrather than language-model probabilities, to form the ba-\nsis for determining the edge weights of our graphs. (Note\nthat the fact that the cosine is symmetric does not imply\nthat edges ( v1, v 2) and ( v2, v 1) get the same weight even\nin our non-smoothed graphs — document d1 being a top\n“generator” of d2 with respect to the cosine does not imply\nthe reverse.) It should be observed that the language-model\nweights on centrality scores (i.e., the pd(q) term in Equation\n3, on which the “+ LM” algorithms are based) were not re-\nplaced with cosine values, which makes sense since we want\nour comparison to focus on the eﬀect of diﬀerent means of\ncomputing graph-based centrality.\nT able 2 depicts the relative performance diﬀerences be-\ntween using our language-model-based graphs and graphs\ninduced using vector-space proximity in the manner just\ndescribed. F or each choice of algorithm, evaluation mea-\nsure, and dataset, we indicate which formulation, if any,\nU-In W-In U-In+LM W-In+LM R-U-In R-W-In R-U-In+LM R-W-In+L M\nprec @5 □ □ □\nAP89 prec @10 ♦ ♦\nMRR □ □ □ □ □ □\nprec @5 ♦ ♦ ♦ ♦ ♦ ♦ ♦ ♦\nAP prec @10 ♦ ♦ ♦ ♦ ♦ ♦ ♦ ♦\nMRR ♦ ♦ ♦ ♦ ♦\nprec @5 ♦\nWSJ prec @10 ♦ ♦\nMRR □\nprec @5 ♦ ♦ ♦ ♦ ♦ ♦\nTREC8 prec @10 ♦ ♦ ♦ ♦\nMRR □ □\nTable 2: Structural re-ranking based on language models (LM ) vs. structural re-ranking based on cosine-\nmeasured vector-space proximity (VEC). W e indicate the set tings in which the relative diﬀerence was at least\n5% with either a “ ♦ ” (LM superior) or a “ □ ” (VEC superior).\nAP89 AP WSJ TREC8\nprec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR\nuniform (= init) 28. 3 26. 5 52. 3 45. 7 43. 2 59. 6 54. 8 48. 4 76. 2 50. 0 45. 6 69. 1\nW-In 31 . 7 27 . 6 48. 4 51 . 1 ∗ 48 . 4 ∗ 63.0 57 . 2 50 . 0 77 . 2 51 . 6 49.6∗ 64. 5\nR-W-In 33.5 29.8 46. 0 52.9∗ 49.0∗ 62 . 6 58.8∗ 50.6 78.6 56.0 45 . 8 67. 6\nlength 29 . 1 24. 3 50. 8 41. 6 41. 4 55. 3 44. 4∗ 42. 4∗ 64. 6∗ 47. 2 41. 4 64. 2\nlog(length) 30 . 4 27 . 0 52 . 5 45. 3 43. 2 60 . 6 57 . 2 49 . 0 69. 8∗ 49. 6 46 . 8 69 . 2\nentropy 30 . 0 26. 5 52.6 46 . 1 42. 5 60 . 8 56 . 8 48 . 6 71. 1∗ 49. 6 46 . 8 71.7∗\nuniqT erms 27. 4 24. 8 52. 3 42. 0 41. 3 56. 2 50. 0 44. 6 68. 8 49. 2 44. 2 71 . 2\nlog(uniqT erms) 30 . 4 27 . 0 52 . 5 45 . 9 42. 3 60 . 8 57 . 2 49 . 0 70. 0∗ 49. 6 47 . 2 70 . 0\nTable 3: Comparison between our use of language-model-base d structural-centrality scores in Equation 3 vs.\nnon-structural re-ranking heuristics. For each evaluatio n setting, italics mark improvements over the default\nbaseline of uniform centrality scores, stars (*) indicate s tatistically signiﬁcant diﬀerences with this default\nbaseline, and bold highlights the best results over all eigh t algorithms.\nresulted in at least 5% relative improvement with respect to\nthe other. As can be seen, in at least three of our four cor-\npora, our language-modeling approach seems to be a more\neﬀective basis for determining document centrality than th e\nvector-space/cosine. We hasten to point out, though, that\nin most instances, vector-space proximity yielded better p er-\nformance than the corresponding baselines (the results are\nomitted since the precise numerical comparison does not\nyield additional information); this ﬁnding provides furth er\nsupport to the idea that the overall structural re-ranking\napproach is a ﬂexible and eﬀective paradigm that can incor-\nporate diﬀerent types of inter-document relationships whe n\nappropriate.\n4.2.3 Inducing centrality with the HITS algorithm\nOne well-known alternative method for computing cen-\ntrality in a graph is the HITS algorithm [13], originally pro -\nposed for Web search. There has been some work utilizing\nit for text summarization in non-Web domains as well [23].\nThe reason we have not yet discussed it in detail is that\nit diﬀers conceptually from our proposed algorithms in an\nimportant way: two diﬀerent notions of centrality are iden-\ntiﬁed, represented by hub and authority scores. While the\nconcepts of hubs and authorities are highly suitable for Web -\nsearch scenarios, it is less clear whether it is useful in our\nsetting to distinguish between the two.\nAs a preliminary investigation, we experimented with us-\ning hub and authority scores as measures of centrality on\nthe generation graphs we built. Space constraints preclude\na detailed discussion, but the results may be summarized\nas follows. We found that authority scores yielded better\nperformance than hub scores, and that the results were gen-\nerally at least as good as or better than those for the opti-\nmized baselines. However, they were slightly inferior in se v-\neral cases to those of the corresponding inﬂux algorithms.\nThus, it seems that our method for graph construction can\nsupport a variety of diﬀerent algorithms, but that the HITS-\nstyle hubs/authorities distinction may not be eﬀective for\nthe task we have addressed.\n4.2.4 Non­structural re­ranking\nSo far, we have discussed the use of graph-based centrality\nas a re-ranking criterion, the idea being that relationship s\nbetween documents can serve as an additional source of in-\nformation. Our best empirical results seem to be produced\nby using the weighted formulation given in Equation 3 from\nSection 2.3:\nC en(d; G) · pd(q).\nSince, as noted above, in this equation C en(d; G) can be\nregarded as a “prior” on documents, it is natural to ask\nwhether other previously-proposed biases on generation pr ob-\nabilities might prove similarly useful. The comparison is e s-\npecially interesting because these biases have tended to be\nisolated-document heuristics; we thus refer to their use as a\nreplacement for C en(d; G) as “non-structural re-ranking”.\nDocument length has been employed several times in the\npast to model the intuition that longer texts contain more\ninformation [11, 14, 25]. We reﬁne this hypothesis to disen-\ntangle several distinct notions of information: the number of\ntokens in a document, the distribution of these tokens, and\nthe number of types (“Salvador Salvador Salvador” contains\nthree tokens but only one type). Thus, as substitutions for\ncentrality in the above expression, we consider not only doc -\nument length, but also the entropy of the term distribution\nand the number of unique terms (used as the basis for piv-\noted unique normalization in [33]). As baseline, we took the\ninitial retrieval results; note that doing so corresponds t o\nusing a uniform bias, or, equivalently, using no bias at all.\nAs can be seen in T able 3, taking the log of token or\ntype count is an improvement over using the raw frequencies,\noften yielding above-baseline performance. The entropy is\nmore eﬀective than raw frequency of either tokens or types,\nand in two cases leads to the best performance overall. How-\never, in the majority of settings, structural re-ranking gi ves\nthe highest accuracies.\n4.2.5 Re­ranking vs. ranking\nWe posed our centrality-computation techniques as meth-\nods for improving the results returned by an initial retriev al\nengine, and showed that they are successful at accomplish-\ning this goal. But one can ask whether it is necessary to\nrestrict our attention to an initial pool Dinit; that is, would\nwe expect similarly good results if we based our generation\ngraphs on the entire corpus? As it happens, preliminary ex-\nperiments with the Recursive Uniform Inﬂux +LM and Re-\ncursive Weighted Inﬂux +LM algorithms on two full corpora\n(AP89 and LA combined with FR) showed that one would\nbe better oﬀ sticking with the standard language-modeling\napproach if no pre-ﬁltering of documents is available.\nWe do not see this ﬁnding as surprising, for our intuition is\nthat in the re-ranking case, there is a more direct connectio n\nbetween centrality and relevance since we can assume that\nrelevant documents comprise a reasonable fraction of the\ninitial retrieval results.\n5. CONCLUSION\nWe have proposed and evaluated a number of methods for\nstructural re-ranking using inter-document generation re la-\ntionships based on language models. Our main experiments\nshowed that even non-optimized instantiations of our over-\nall approach yield results rivaling those of optimized base-\nlines. F urther analysis revealed that generation relation -\nships seem more eﬀective within our centrality-computatio n\nframework than relationships based on vector-space proxim -\nity do, and that using inter-document relationships seems\nto be a promising alternative to employing the isolated-\ndocument heuristics we implemented (several of which were\nnovel to this study). Based on our results, we believe that\nexploring other methods for combining statistical languag e\nmodels and explicitly graph-based techniques is a fruitful\nline for future research.\nAcknowledgments. We thank James Allan, Bruce Croft,\nCarmel Domshlak, Jon Kleinberg, F ernando Pereira and\nthe anonymous reviewers for valuable discussions and com-\nments. We also thank CMU for its hospitality during the\nyear. This paper is based upon work supported in part\nby the National Science F oundation under grant no. IIS-\n0329064 and CCR-0122581; SRI International under sub-\ncontract no. 03-000211 on their project funded by the De-\npartment of the Interior’s National Business Center; and an\nAlfred P . Sloan Research F ellowship. Any opinions, ﬁndings ,\nand conclusions or recommendations expressed are those of\nthe authors and do not necessarily reﬂect the views or oﬃ-\ncial policies, either expressed or implied, of any sponsori ng\ninstitutions, the U.S. government, or any other entity.\n6. REFERENCES\n[1] Sergey Brin and Lawrence Page. The anatomy of a\nlarge-scale hypertextual web search engine. In\nProceedings of the 7th International World Wide Web\nConference, pages 107–117, 1998.\n[2] W. Bruce Croft and John Laﬀerty, editors. Language\nModeling for Information Retrieval . Number 13 in\nInformation Retrieval Book Series. Kluwer, 2003.\n[3] Inderjit Dhillon. Co-clustering documents and words\nusing bipartite spectral graph partitioning. In\nProceedings of the Seventh ACM SIGKDD Conference ,\npages 269–274, 2001.\n[4] G¨ une¸ s Erkan and Dragomir R. Radev. LexRank:\nGraph-based lexical centrality as salience in text\nsummarization. Journal of Artiﬁcial Intelligence\nResearch, 22:457–479, 2004.\n[5] Eugene Garﬁeld. Citation analysis as a tool in journal\nevaluation. Science, 178:471–479, 1972.\n[6] Gene H. Golub and Charles F. Van Loan. Matrix\nComputations. The Johns Hopkins University Press,\nthird edition, 1996.\n[7] Winfried K. Grassmann, Michael I. T aksar, and\nDaniel P . Heyman. Regenerative analysis and steady\nstate distributions for Markov chains. Operations\nResearch, 33(5):1107–1116, 1985.\n[8] Geoﬀrey R. Grimmett and David R. Stirzaker.\nProbability and Random Processes . Oxford Science\nPublications, third edition, 2001.\n[9] Vasileios Hatzivassiloglou and Kathleen McKeown.\nPredicting the semantic orientation of adjectives. In\nProceedings of the 35th ACL/8th EACL , pages\n174–181, 1997.\n[10] Marti A. Hearst and Jan O. Pedersen. Reexamining\nthe cluster hypothesis: Scatter/Gather on retrieval\nresults. In Proceedings of SIGIR , 1996.\n[11] Djoerd Hiemstra and Wessel Kraaij. Twenty-One at\nTREC7: Ad hoc and cross-language track. In\nProceedings of the Seventh Text Retrieval Conference\n(TREC-7), pages 227–238, 1999.\n[12] Thorsten Joachims. T ransductive learning via spectra l\ngraph partitioning. In Proceedings of ICML , 2003.\n[13] Jon Kleinberg. Authoritative sources in a hyperlinked\nenvironment. Journal of the ACM , 46:604–632, 1999.\n[14] Wessel Kraaij and Thijs Westerveld. TNO-UT at\nTREC9: How diﬀerent are web documents? In\nProceedings of the Ninth Text Retrieval Conference\n(TREC-9), pages 665–671, 2001.\n[15] Wessel Kraaij, Thijs Westerveld, and Djoerd Hiemstra.\nThe importance of prior probabilities for entry page\nsearch. In Proceedings of SIGIR , pages 27–34, 2002.\n[16] Oren Kurland and Lillian Lee. Corpus structure,\nlanguage models, and ad hoc information retrieval. In\nProceedings of SIGIR , pages 194–201, 2004.\n[17] Oren Kurland, Lillian Lee, and Carmel Domshlak.\nBetter than the real thing? Iterative pseudo-query\nprocessing using cluster-based language models. In\nProceedings of SIGIR , 2005.\n[18] John D. Laﬀerty and Chengxiang Zhai. Document\nlanguage models, query models, and risk minimization\nfor information retrieval. In Proceedings of SIGIR ,\npages 111–119, 2001.\n[19] Victor Lavrenko, James Allan, Edward DeGuzman,\nDaniel LaFlamme, Veera Pollard, and Steven Thomas.\nRelevance models for topic detection and tracking. In\nProceedings of the Human Language Technology\nConference (HLT), pages 104–110, 2002.\n[20] Anton Leuski. Evaluating document clustering for\ninteractive information retrieval. In Proceedings of the\ntenth International Conference on Information and\nKnowledge Managment (CIKM) , pages 33–40, 2001.\n[21] Xiaoyan Li and W. Bruce Croft. Time-based language\nmodels. In Proceedings of the 12th International\nConference on Information and Knowledge\nManagment (CIKM) , pages 469–475, 2003.\n[22] Xiaoyong Liu and W. Bruce Croft. Cluster-based\nretrieval using language models. In Proceedings of\nSIGIR, pages 186–193, 2004.\n[23] Rada Mihalcea. Graph-based ranking algorithms for\nsentence extraction, applied to text summarization. In\nThe Companion Volume to the Proceedings of the\n42nd Annual Meeting of the Association for\nComputational Linguistics , pages 170–173, 2004.\n[24] Rada Mihalcea and Paul T arau. T extRank: Bringing\norder into texts. In Proceedings of EMNLP , pages\n404–411, 2004. Poster.\n[25] David R. H. Miller, Tim Leek, and Richard M.\nSchwartz. A hidden Markov model information\nretrieval system. In Proceedings of SIGIR , pages\n214–221, 1999.\n[26] Kenney Ng. A maximum likelihood ratio information\nretrieval model. In Proceedings of the Eighth Text\nRetrieval Conference (TREC-8) , pages 483–492, 2000.\n[27] Paul Ogilvie and Jamie Callan. Experiments using the\nLEMUR toolkit. In Proceedings of the Tenth Text\nRetrieval Conference (TREC-10) , pages 103–108,\n2001.\n[28] Bo Pang and Lillian Lee. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In Proceedings of the ACL ,\npages 271–278, 2004.\n[29] Gabriel Pinski and F rancis Narin. Citation inﬂuence\nfor journal aggregates of scientiﬁc publications:\nTheory, with application to the literature of physics.\nInformation Processing and Management , 12:297–312,\n1976.\n[30] Jay M. Ponte and W. Bruce Croft. A language\nmodeling approach to information retrieval. In\nProceedings of SIGIR , pages 275–281, 1998.\n[31] Ian Ruthven and Mounia Lalmas. A survey on the use\nof relevance feedback for information access systems.\nKnowledge Engineering Review , 18(2):95–145, 2003.\n[32] Chirag Shah and W. Bruce Croft. Evaluating high\naccuracy retrieval techniques. In Proceedings of\nSIGIR, pages 2–9, 2004.\n[33] Amit Singhal, Chris Buckley, and Mandar Mitra.\nPivoted document length normalization. In\nProceedings of SIGIR , pages 21–29, 1996.\n[34] William J. Stewart. Introduction to the numerical\nsolution of Markov chains . Princeton University Press,\n1994.\n[35] T ao T ao and ChengXiang Zhai. A two-stage mixture\nmodel for pseudo feedback. In Proceedings of the 27th\nSIGIR, pages 486–487, 2004. Poster.\n[36] Naftali Tishby and Noam Slonim. Data clustering by\nMarkovian relaxation and the information bottleneck\nmethod. In Advances in Neural Information\nProcessing Systems (NIPS) 14 , pages 640–646, 2000.\n[37] Anastasios T ombros, Robert Villa, and C.J. van\nRijsbergen. The eﬀectiveness of query-speciﬁc\nhierarchic clustering in information retrieval.\nInformation Processing and Management ,\n38(4):559–582, 2002.\n[38] Kristina T outanova, Christopher D. Manning, and\nAndrew Y. Ng. Learning random walk models for\ninducing word dependency distributions. In\nProceedings of the International Conference on\nMachine Learning , 2004.\n[39] Peter Willett. Query speciﬁc automatic document\nclassiﬁcation. International Forum on Information\nand Documentation , 10(2):28–32, 1985.\n[40] Chengxiang Zhai and John D. Laﬀerty. A study of\nsmoothing methods for language models applied to ad\nhoc information retrieval. In Proceedings of SIGIR ,\npages 334–342, 2001."
}