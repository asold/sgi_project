{
    "title": "SwinNet: Swin Transformer Drives Edge-Aware RGB-D and RGB-T Salient Object Detection",
    "url": "https://openalex.org/W3212645988",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2121869397",
            "name": "Zhengyi Liu",
            "affiliations": [
                "Anhui University"
            ]
        },
        {
            "id": "https://openalex.org/A3214365789",
            "name": "Yacheng Tan",
            "affiliations": [
                "Anhui University"
            ]
        },
        {
            "id": "https://openalex.org/A2008737687",
            "name": "Qian He",
            "affiliations": [
                "Anhui University"
            ]
        },
        {
            "id": "https://openalex.org/A2097394777",
            "name": "Yun Xiao",
            "affiliations": [
                "Anhui University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2793668851",
        "https://openalex.org/W2923601983",
        "https://openalex.org/W3010722397",
        "https://openalex.org/W3018276607",
        "https://openalex.org/W2963897031",
        "https://openalex.org/W4214561053",
        "https://openalex.org/W3023562424",
        "https://openalex.org/W2615981376",
        "https://openalex.org/W2923266522",
        "https://openalex.org/W2948937967",
        "https://openalex.org/W3159018159",
        "https://openalex.org/W3046565475",
        "https://openalex.org/W6787721334",
        "https://openalex.org/W6795500503",
        "https://openalex.org/W3131182250",
        "https://openalex.org/W2989161706",
        "https://openalex.org/W3084740725",
        "https://openalex.org/W2967085153",
        "https://openalex.org/W3108948422",
        "https://openalex.org/W3087221416",
        "https://openalex.org/W3090469840",
        "https://openalex.org/W3183781492",
        "https://openalex.org/W3162275167",
        "https://openalex.org/W2607011617",
        "https://openalex.org/W6638992375",
        "https://openalex.org/W2990201021",
        "https://openalex.org/W2121378474",
        "https://openalex.org/W2014328297",
        "https://openalex.org/W2804743778",
        "https://openalex.org/W3125394276",
        "https://openalex.org/W3120524049",
        "https://openalex.org/W2765934933",
        "https://openalex.org/W3127711404",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2969377765",
        "https://openalex.org/W3006465601",
        "https://openalex.org/W6784364555",
        "https://openalex.org/W3034320133",
        "https://openalex.org/W3097053213",
        "https://openalex.org/W3170173308",
        "https://openalex.org/W3011305844",
        "https://openalex.org/W3080081801",
        "https://openalex.org/W3035687312",
        "https://openalex.org/W3114152269",
        "https://openalex.org/W3108608656",
        "https://openalex.org/W3035357085",
        "https://openalex.org/W3096966254",
        "https://openalex.org/W3173882198",
        "https://openalex.org/W3126725132",
        "https://openalex.org/W6790540925",
        "https://openalex.org/W3031719679",
        "https://openalex.org/W3049194477",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2787530955",
        "https://openalex.org/W2942049721",
        "https://openalex.org/W2887486131",
        "https://openalex.org/W2985335644",
        "https://openalex.org/W2945809413",
        "https://openalex.org/W6780499500",
        "https://openalex.org/W2995936506",
        "https://openalex.org/W3047800102",
        "https://openalex.org/W3166092877",
        "https://openalex.org/W3164802490",
        "https://openalex.org/W3185043317",
        "https://openalex.org/W3188963955",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W6796245125",
        "https://openalex.org/W2461758788",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2145023731",
        "https://openalex.org/W20683899",
        "https://openalex.org/W1976409045",
        "https://openalex.org/W1993713494",
        "https://openalex.org/W1966025376",
        "https://openalex.org/W2957414648",
        "https://openalex.org/W3002301267",
        "https://openalex.org/W3108812909",
        "https://openalex.org/W3108421143",
        "https://openalex.org/W6796469369",
        "https://openalex.org/W1772076007",
        "https://openalex.org/W2963868681",
        "https://openalex.org/W2100470808",
        "https://openalex.org/W2963529609",
        "https://openalex.org/W1982075130",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W3010616503",
        "https://openalex.org/W3086388316",
        "https://openalex.org/W3035633116",
        "https://openalex.org/W3035284915",
        "https://openalex.org/W3140528754",
        "https://openalex.org/W3134912427",
        "https://openalex.org/W3135874576",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2928165649",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W3097336090",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3023991476",
        "https://openalex.org/W3126152110",
        "https://openalex.org/W3102864715",
        "https://openalex.org/W3209732904",
        "https://openalex.org/W4308291551",
        "https://openalex.org/W3101839051",
        "https://openalex.org/W4301860726",
        "https://openalex.org/W3162942069",
        "https://openalex.org/W3104979525",
        "https://openalex.org/W4213078714",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3168825659",
        "https://openalex.org/W4287642479",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W1854404533",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3114738484",
        "https://openalex.org/W4250533120",
        "https://openalex.org/W3171757599"
    ],
    "abstract": "Convolutional neural networks (CNNs) are good at extracting contexture\\nfeatures within certain receptive fields, while transformers can model the\\nglobal long-range dependency features. By absorbing the advantage of\\ntransformer and the merit of CNN, Swin Transformer shows strong feature\\nrepresentation ability. Based on it, we propose a cross-modality fusion model\\nSwinNet for RGB-D and RGB-T salient object detection. It is driven by Swin\\nTransformer to extract the hierarchical features, boosted by attention\\nmechanism to bridge the gap between two modalities, and guided by edge\\ninformation to sharp the contour of salient object. To be specific, two-stream\\nSwin Transformer encoder first extracts multi-modality features, and then\\nspatial alignment and channel re-calibration module is presented to optimize\\nintra-level cross-modality features. To clarify the fuzzy boundary, edge-guided\\ndecoder achieves inter-level cross-modality fusion under the guidance of edge\\nfeatures. The proposed model outperforms the state-of-the-art models on RGB-D\\nand RGB-T datasets, showing that it provides more insight into the\\ncross-modality complementarity task.\\n",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 1\nSwinNet: Swin Transformer Drives Edge-Aware\nRGB-D and RGB-T Salient Object Detection\nZhengyi Liu*, Yacheng Tan, Qian He and Yun Xiao\nAbstract—Convolutional neural networks (CNNs) are good at\nextracting contexture features within certain receptive ﬁelds,\nwhile transformers can model the global long-range dependency\nfeatures. By absorbing the advantage of transformer and the\nmerit of CNN, Swin Transformer shows strong feature represen-\ntation ability. Based on it, we propose a cross-modality fusion\nmodel, SwinNet, for RGB-D and RGB-T salient object detection.\nIt is driven by Swin Transformer to extract the hierarchical\nfeatures, boosted by attention mechanism to bridge the gap\nbetween two modalities, and guided by edge information to\nsharp the contour of salient object. To be speciﬁc, two-stream\nSwin Transformer encoder ﬁrst extracts multi-modality features,\nand then spatial alignment and channel re-calibration mod-\nule is presented to optimize intra-level cross-modality features.\nTo clarify the fuzzy boundary, edge-guided decoder achieves\ninter-level cross-modality fusion under the guidance of edge\nfeatures. The proposed model outperforms the state-of-the-\nart models on RGB-D and RGB-T datasets, showing that it\nprovides more insight into the cross-modality complementarity\ntask.https://github.com/liuzywen/SwinNet\nIndex Terms—transformer, salient object detection, RGB-D,\nRGB-T, multi-modality\nI. I NTRODUCTION\nSalient object detection (SOD) simulates the visual attention\nmechanism to capture the prominent object. As described\nin the SOD review[1], SOD has been extended from RGB\nimage[2], [3], [4] to RGB-D image[5], [6], a group of\nimages[7], [8] and video[9], [10]. Recently, SOD in RGB-\nT image[11], light ﬁeld image[12], [13], [14], high-resolution\nimage[15], [16], optical remote sensing image[17], [18], [19]\nand 360◦omnidirectional image[20], [21] have been gradually\nresearched. SOD can beneﬁt many image and video processing\ntasks, such as image segmentation [22], [23], tracking [24],\n[25], [26], retrieval [27], compression [28], cropping [29],\n[30], retargeting[31], quality assessment [32] and activity\nprediction[33].\nWhen the light is insufﬁcient or the background is cluttered\nin a scene, SOD is still a challenge issue. With the widespread\nuse of depth cameras and infrared imaging devices, the depth\nThis work was supported by National Natural Science Foundation of\nChina (62006002) and Natural Science Foundation of Anhui Province\n(1908085MF182).(Corresponding author: Zhengyi Liu)\nZhengyi Liu, Yacheng Tan and Qian He are with Key Laboratory of\nIntelligent Computing and Signal Processing of Ministry of Education, School\nof Computer Science and Technology, Anhui University, Hefei, China(e-mail:\nliuzywen@ahu.edu.cn,1084043983@qq.com,1819469871@qq.com).\nYun Xiao is with Key Laboratory of Intelligent Computing and Signal\nProcessing of Ministry of Education, School of Artiﬁcial Intelligence, Anhui\nUniversity, Hefei, China(e-mail: 280240406@qq.com).\nCopyright ©2021 IEEE. Personal use of this material is permitted. However,\npermission to use this material for any other purposes must be obtained from\nthe IEEE by sending an email to pubs-permissions@ieee.org.\nor thermal infrared information as the supplementary modality\nhas shown the advantages to SOD performance improvements,\nbecause the depth image can provide the more geometry\ninformation and the thermal image can capture the radiated\nheat of objects especially under adverse weather and lighting\nconditions. Nevertheless, how to effectively implement cross-\nmodality information fusion is still challenging, which can\nsigniﬁcantly affect the achievement of robust performance.\nIn the past few years, convolutional neural networks (CNNs)\nhave achieved milestones in RGB-D and RGB-T SOD. How-\never, CNN gathers information from neighborhood pixels and\nloses spatial information due to pooling operation. It is not\neasy to learn global long-range semantic information inter-\naction well. Recently, Swin Transformer[34] is proposed. It\nimplements pairwise entity interactions within a local window\nby multi-head self-attention, and establishes long-range depen-\ndency across windows by shifted windowing scheme. Features\nextracted from transformer have more global attributes than\nthose from CNN. By absorbing the locality, translation invari-\nance and hierarchical merits of CNN, Swin Transformer can be\nused as backbone network to extract hierarchical information\nof each modality. The features from different modalities show\nthe different attribution. They consistently display the common\nsalient position in the spatial aspect, and respectively show\nthe different salient content in the channel aspect, so spatial\nalignment and channel re-calibration module is designed to\nboost the extracted features based on attention mechanism. In\naddition, SOD task is essentially a pixel-level dense prediction\ntask. After the feature extraction of encoder, the multi-level\nfeatures with different receptive ﬁeld and spatial resolution\nneed to be progressively combined by upsampling and skip\nconnection. In the decoding process, shallow-level features\nexhibit the detailed boundary information, and meanwhile it\nalso brings some background noises. Therefore, edge-aware\nmodule is presented to extract the edge feature, and further to\nguide the decoder for both suppressing the shallow-layer noise\nand reﬁning the contour of objects.\nOur main contributions can be summarized as follows:\n• A novel SOD model (SwinNet) for both RGB-D and\nRGB-T tasks built upon the Swin Transformer backbone\nis proposed. It extracts discriminative features from Swin\nTransformer backbone which absorbs the local advantage\nof convolution neural network and the long-range depen-\ndency merit of transformer, outperforming the state-of-\nthe-art (SOTA) RGB-D and RGB-T SOD models.\n• A newly designed spatial alignment and channel re-\ncalibration module is used to optimize the features of each\nmodality based on attention mechanism, achieving intra-\narXiv:2204.05585v1  [cs.CV]  12 Apr 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 2\nlayer cross-modality fusion from the spatial and channel\naspects.\n• The proposed edge-guided decoder achieves inter-layer\ncross-modal fusion under the guidance of edge-aware\nmodule, generating the sharper contour.\nII. R ELATED WORKS\nA. RGB-D salient object detection\nSalient object detection has achieved the massive improve-\nment by combining the other modality with color modality.\nDepth image is exactly a good supplement, because it provides\nmore reliable spatial structure information and insensitive to\nthe variations of the environment lights and colors.\nCong et al.[35] introduces depth information in the initial-\nization, reﬁnement and optimization of saliency map, achiev-\ning transfer from existing RGB SOD models to RGB-D\nSOD models. In these years, attention mechanism[36], [37],\n[38], [39], [40], [41], multi-task learning[42], [43], [44],\n[45], knowledge distillation[46], graph neural networks[47],\nneural architecture search[48], 3D convolutional neural\nnetworks[49], self-supervised learning[50], generative adver-\nsarial networks[51], disentanglement and reconstruction[52]\nare applied to solve SOD task. The intrinsic defect of CNN\nlimits above methods in learning global long-range depen-\ndencies. Visual Saliency Transformer (VST)[6] propagates\nlong-range dependencies across modalities by Scaled Dot-\nProduct Attention[53] between the queries from one modality\nwith the keys of the other modality. It also designs reverse\nT2T to decode and introduces edge detection to improve the\nperformance. Motivated by its success, we introduce Swin\nTransformer as backbone to enhance the feature representa-\ntion, and then use transformer encoder and CNN decoder to\ncomplete the SOD task.\nB. RGB-T salient object detection\nThe thermal image can capture the radiated heat of objects,\nand it is insensitive to lighting and weather conditions, and\nsuitable for handling scenes captured under adverse condi-\ntions, for example, total darkness environment, foggy weather,\nand cluttered backgrounds. Therefore, thermal image is a\npromising supplement to the RGB image for SOD. In earlier\nyears, RGB-T SOD adopts machine learning methods, for\nexample, SVM[54], ranking models[55], [56], [57] and graph\nlearning[58]. With the development of CNN, Tu et al.[59]\npropose a baseline model which combines CNN with attention\nmechanism. Zhang et al.[60], [61] propose two end-to-end\nCNN based RGB-T SOD models to achieve multi-scale, multi-\nmodality and multi-level fusion. ECFFNet[11] achieves more\neffectively cross-modality fusion, and enhances salient object\nboundaries by a bilateral reversal fusion of foreground and\nbackground information. MIDD[62] proposes multi-interactive\ndual-decoder to integrate the multi-level interactions of dual\nmodalities and global contexts. MMNet[63] simulates visual\ncolor stage doctrine to fuse cross-modal features in stages,\nand designs bi-directional multi-scale decoder to capture both\nlocal and global information. CGFNet[64] adopts the guidance\nmanner of one modality on the other modality to fuse two\nmodalities. CSRNet[65] uses context-guided cross modality\nfusion module to fuse two modalities, and designs a stacked\nreﬁnement network to reﬁne the segmentation results. Pushed\nby the global merit of Transformer in computer vision, we\npropose transformer based method to detect the salient object\nin RGB-T images.\nC. Transformer\nVaswani et al.[53] ﬁrst proposes transformer with stacked\nmulti-head self-attention and point-wise feed-forward layers\nin machine translation task. Recently, inspired by successful\nViT[66], transformer variants emerge explosively. T2T[66]\nprogressively structurizes the image to tokens by recursively\naggregating neighboring tokens into one token. CvT[67]\nadds convolutional layers into the multi-head self-attention.\nPVT[68] introduces a progressive shrinking pyramid to reduce\nthe sequence length of the transformer. DPT[69] assembles\ntokens from multiple stages of the vision transformer and\nprogressively combines them into full-resolution predictions\nusing a convolutional decoder. Swin transformer[34] designs\nthe shifted window-based multi-head attentions to reduce\nthe computation cost. CAT[70] alternately applies attention\ninner patch and between patches to maintain the performance\nwith lower computational cost and builds a cross attention\nhierarchical network. Due to the perfect performance of Swin\nTransformer, it is used as the backbone network.\nIII. P ROPOSED METHOD\nA. Overview\nThe overall framework of the proposed model is illustrated\nin Fig.1, which consists of a two-stream backbone, a spatial\nalignment and channel re-calibration module, an edge-aware\nmodule and an edge-guided decoder. Note that since RGB-D\nand RGB-T SOD are the same multi-modality fusion tasks, for\nbrevity, below we only elaborate the implementation detail of\nRGB-D SOD task, because that of RGB-T is the same.\nB. Two-stream Swin Transformer backbone\nSwin Transformer has the ﬂexibility to model at various\nscales and has linear computational complexity with respect\nto image size[34]. We adopt two Swin Transformers to extract\nhierarchical features from multi-modality image pairs. Con-\nsidering the complexity and efﬁciency, Swin-B version[34] is\nadopted.\nEach Swin Transformer ﬁrst splits the input single-modality\nimage into non-overlapping patches by a patch embedding.\nThe feature of each patch in color stream is set as a concate-\nnation of the raw pixel RGB values, while that in depth stream\nis set as a concatenation of three copied depth values. Then,\nthey are fed into the multi-stage feature transformation. With\nthe increasing depth of the network, the number of tokens\nis gradually reduced by patch merging layers to produce the\nhierarchical representation of each modality, which can be\ndenoted as {ST c\ni }4\ni=1 and {ST d\ni }4\ni=1, respectively.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 3\nFig. 1. An overview of our proposed SwinNet. It consists of a two-stream backbone, a spatial alignment and channel re-calibration module, an edge-aware\nmodule and an edge-guided decoder. Multi-modal hierarchical features from two-stream backbone will be fed into spatial alignment and channel re-calibration\nmodules to generate the enhanced features Fc\ni and Fd\ni (i=1,··· , 4). Besides, edge feature is generated from edge-aware module which process the shallow-layer\nfeatures of the depth backbone. At last, in the edge-guided decoder, enhanced features and edge feature are combined to generate saliency map.\nC. Spatial alignment and channel re-calibration module\nOn one hand, since the position of salient objects in multi-\nmodality image pairs should be the same, the features from\ndifferent modalities need to be aligned at ﬁrst to show the\ncommon salient position. On the other hand, since RGB image\nshows more appearance and texture information, and depth\nimage exhibits more spatial cue, the features from different\nmodalities are different in the importance of feature channels,\nand the multi-modality features need to be re-calibrated to em-\nphasize their respective salient content. Therefore, the spatial\nalignment and channel re-calibration module is proposed. It\nﬁrst aligns two modalities in spatial part, and then recalibrates\nrespective channel part to pay more attention to the salient\ncontent in each modality.\nSpeciﬁcally, given the color features ST c\ni and depth feature\nST d\ni at a certain hierarchy i ∈{1, ··· , 4}, we ﬁrst compute\ntheir common spatial attention map SAi as:\nSAi = SA(ST c\ni ×ST d\ni ) (1)\nwhere “×” means element-wise multiplication operation, and\nSA(·) denotes spatial attention operation which is deﬁned as:\nSA(x) =Sigmoid(Conv3(CGMP (x))) (2)\nwhere CGMP (·) means global max pooling operation along\nchannel direction, Conv3(·) represents the convolution oper-\nation with the kernel size 3 ×3, and Sigmoid(·) denotes the\nsigmoid activation function.\nNext, the common spatial attention map is served as the\nweight of color feature and depth feature to achieve the spatial\nalignment of both modalities by:\nST 1c\ni = SAi ×ST c\ni\nST 1d\ni = SAi ×ST d\ni\n(3)\nThird, the aligned features in spatial part ST 1l\ni(l ∈{c, d})\nare performed channel attention respectively, to generate chan-\nnel attention map which shows more weights on the more\nsalient content in each modality by:\nCAc\ni = CA(ST 1c\ni)\nCAd\ni = CA(ST 1d\ni) (4)\nwhere CA(·) denotes channel attention operation which is\ndeﬁned as:\nCA(x) =Sigmoid(Conv1(GMP (x))) (5)\nwhere GMP (·) means the global max pooling operation,\nConv1 represents the convolution operation with the kernel\nsize 1×1.\nLast, each channel attention map is multiplied with original\nfeature to achieve the channel re-calibration.\nFc\ni = CAc\ni ×ST c\ni\nFd\ni = CAd\ni ×ST d\ni\n(6)\nAfter the spatial alignment and channel re-calibration mod-\nule, the enhanced features Fl\ni (l ∈{c, d}) achieve the position\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 4\nalignment and channel re-calibration, which show the stronger\nrepresentation ability.\nD. Edge-aware module\nAs we all known, high-layer features express more semantic\ninformation, while shallow-layer features carry more details.\nMeanwhile, salient objects are more likely to exhibit pop-\nout structure in the depth image[71]. It is easy to depict the\nobject contours by depth contrast. Therefore, the shallow-layer\nfeatures of the depth backbone are used to produce the edge\nfeature.\nSpeciﬁcally, ST d\ni (i = 1, 2, 3) are performed 1 ×1 convolu-\ntional operation and upsampling operation to generate three\nfeatures with the same size, and then they are concatenated to\ngenerate edge feature.\nFe = Concat(Conv1(ST d\n1 ), Up2(Conv1(ST d\n2 )), Up4(Conv1(Std\n3)))\n(7)\nwhere Upx(·) denotes x×upsampling operation, and\nConcat(·) means the concatenation operation.\nNext, the obtained edge feature is performed a channel\nattention and a residual connection to generate the clearer edge\ninformation by:\nF′\ne = Fe ×CA(BConv (Fe)) +Fe (8)\nwhere BConv (·) represents convolutional operation with ker-\nnel size 3 ×3 followed by a batch normalization layer and\na ReLU activation function, and “+” means element-wise\naddition operation.\nThe edge-aware module outputs the edge features F′\ne which\nwill be used to guide the decoding process of the model and\nenhance the details.\nE. Edge-guided decoder\nAfter spatial alignment and channel re-calibration and edge\nfeature extraction, decoder combines the enhanced hierarchical\nfeatures of different modalities with the edge features to\nproduce the edge-guided salient feature.\nSpeciﬁcally, the aligned and re-calibrated color and depth\nfeatures from two modalities Fc\ni and Fd\ni at a certain hierarchy\ni ∈{1, ··· , 4}are fused by the addition, multiplication and\nconcatenation operation by:\nFi = Concat((Fd\ni + Fc\ni ), (Fd\ni ×Fc\ni )) (9)\nNext, according to the decoding idea widely used in U-Net\nframework[72], the high-level fused feature is progressively\naggregated into the shallow-layer fused features by:\nFFi =\n{\nFi + Conv3(Up2(FFi+1)), i = 1, 2, 3\nFi, i = 4 (10)\nAt last, edge feature from edge-aware module is combined\nwith fused feature to generate the edge-guided salient feature\nFs.\nFs = Concat(F′\ne, FF1) (11)\nF . Loss function\nThe loss function L is deﬁned as:\nL = Le(Se) +Ls(S) (12)\nwhere Le and Ls denote edge loss and saliency loss, respec-\ntively.\n1) Edge loss: The edge map is generated from edge-aware\nmodule. Speciﬁcally, edge feature F′\ne is fed into a convolution\nlayer and a upsampling layer to generate edge map Se by:\nSe = Up4(Conv3(F′\ne)) (13)\nThe edge ground truth can be easily got from saliency\nmap ground truth by Canny edge detector [73]. It is used to\nsupervise the edge map Se. The edge loss Le adopts the cross-\nentropy loss, and it is deﬁned as:\nLe(Se) =−\n∑\nj∈Z+\nlogPr (yj = 1|Se) −\n∑\nj∈Z−\nlogPr (yj = 0|Se)\n(14)\nwhere Z+ and Z−denote the edge pixels set and background\npixels set respectively. Pr (yj = 1|Se) is the prediction map\nin which each value denotes the edge conﬁdence for the pixel.\n2) Saliency loss: The ﬁnal saliency map can be gener-\nated from edge-guided decoder. Speciﬁcally, the edge-guided\nsalient feature Fs is fed into a convolution layer and a\nupsampling layer to generate saliency map S by:\nS = Up4(Conv3(Fs)) (15)\nThe saliency loss Ls adopts the cross-entropy loss, and it\nis deﬁned as:\nLs(S) =−\n∑\nj∈Y+\nlogPr (yj = 1|S) −\n∑\nj∈Y−\nlogPr (yj = 0|S) (16)\nwhere Y+ and Y−denote the salient region pixels set and non-\nsalient pixels set respectively. Pr (yj = 1|S) is the prediction\nmap in which each value denotes the salient region conﬁdence\nfor the pixel.\nIV. E XPERIMENTS\nA. Datasets and evaluation metrics\n1) Datasets: For RGB-D SOD, we evaluate the pro-\nposed method on several challenging RGB-D SOD datasets.\nNLPR [74] includes 1,000 images with single or multiple\nsalient objects. NJU2K [75] consists of 2,003 stereo image\npairs and ground-truth maps with different objects, complex\nand challenging scenes. STERE [76] incorporates 1,000 pairs\nof binocular images downloaded from the Internet. DES [77]\nhas 135 indoor images collected by Microsoft Kinect. SIP [78]\ncontains 1,000 high-resolution images of multiple salient per-\nsons. DUT [79] contains 1,200 images captured by Lytro\ncamera in real life scenes. For the sake of fair comparison, we\nuse the same training dataset as in [78], [80], which consists\nof 1,485 images from the NJU2K dataset and 700 images from\nthe NLPR dataset. The remaining images are used for testing.\nIn addition, on the DUT dataset, we follow the same protocols\nas in [79], [81], [46], [45], [44] to add additional 800 pairs\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 5\nfrom DUT for training and test on the remaining 400 pairs.\nIn summary, our training set contains 2,185 paired RGB and\ndepth images, but when testing is conducted on DUT, our\ntraining set contains 2,985 paired ones.\nFor RGB-T SOD, we evaluate the proposed method on\nthree RGB-T SOD datasets. VT821[56] contains 821 manually\nregistered image pairs. VT1000[58] contains 1,000 RGB-T\nimage pairs captured with highly aligned RGB and thermal\ncameras. VT5000[59] contains 5,000 pairs of high-resolution,\nhigh-diversity and low-deviation RGB-T images. For the sake\nof fair comparison, we use the same training dataset as in [62],\n[82], [11], which consists of 2,500 image pairs in VT5000. The\nrest image pairs are used for testing.\n2) Evaluation Metrics: We adopt widely used metrics to\nevaluate the performance of our model and SOTA RGB-D\nand RGB-T SOD models. They are the precision-recall (PR)\ncurve [83], S-measure [84], F-measure [85], E-measure [86]\nand mean absolute error (MAE) [87]. Speciﬁcally, the PR\ncurve plots precision and recall values by setting a series\nof thresholds on the saliency maps to get the binary masks\nand further comparing them with the ground truth maps. The\nS-measure can evaluate both region-aware and object-aware\nstructural similarity between saliency map and ground truth.\nThe F-measure is the weighted harmonic mean of precision\nand recall, which can evaluate the overall performance. The\nE-measure simultaneously captures global statistics and local\npixel matching information. The MAE measures the average\nof the per-pixel absolute difference between the saliency maps\nand the ground truth maps. In our experiment, E-measure and\nF-measure adopt the adaptive values.\nB. Implementation details\nDuring the training and testing phase, the input RGB, depth\nand thermal images are resized to 384 ×384. Since the depth\nimage is single-channel data, it is copied to form three-channel\nimage which is the same as RGB and thermal images. Multiple\nenhancement strategies are used for all training images, i.e.,\nrandom ﬂipping, rotating and border clipping. Parameters\nof the backbone network are initialized with the pretrained\nparameters of Swin-B network[34]. The rest of parameters are\ninitialized to PyTorch default settings. We employ the Adam\noptimizer [88] to train our network with a batch size of 3\nand an initial learning rate 5e-5, and the learning rate will be\ndivided by 10 every 100 epochs. Our model is trained on a\nmachine with a single NVIDIA RTX 2080Ti GPU. The model\nconverges within 200 epochs, which takes nearly 26 hours.\nC. Comparisons with SOTAs\n1) RGB-D SOD: For RGB-D SOD, our model is\ncompared with several SOTA RGB-D SOD algorithms,\nincluding D3Net [78], ASIF-Net[36], ICNet [89],\nDCMF [52], DRLF [90], SSF [43], SSMA [38], A2dele [46],\nUC-Net [91], JL-DCF[92], CoNet [44], DANet [81],\nEBFSP[93],CDNet[94], HAINet[95], RD3D[49], DSA2F[48],\nMMNet[63] and VST[6]. To ensure the fairness of the\ncomparison results, the saliency maps of the evaluation are\nprovided by the authors or generated by running source\ncodes.\nQuantitative Evaluation. Fig.2 shows the comparison re-\nsults on PR curve. Table.I shows the quantitative comparison\nresults of four evaluation metrics. As can be clearly observed\nfrom ﬁgure that our curves are signiﬁcant better than the others\non NLPR, NJU2K, STERE, SIP and DUT datasets, slightly on\nDES dataset. It beneﬁts from the choose of backbone, spatial\nalignment and channel re-calibration of two modalities and\nedge guidance. Meanwhile, the table also gives the consistent\nresults. The performance is improved with a large margin on\nNLPR, NJU2K, STERE, SIP and DUT datasets, and has a little\neffectiveness on DES dataset. Compared with transformer-\nbased method VST[6], S-measure, F-measure, E-measure and\nMAE are improved about 0.007, 0.017, 0.010 and 0.005 on\naverage. The PR curve and evaluation metrics all verify the\neffectiveness and advantages of our proposed method in RGB-\nD SOD task.\nQualitative Evaluation. To make the qualitative compar-\nisons, we show some visual examples in Fig.3. It can be\nobserved that our method has the better detection results than\nother methods in some challenging cases: similar foreground\nand background (1st-2nd rows), complex scene (3rd-4th rows),\ndepth image with low quality ( 5th-6th rows), small object\n(7th-8th rows) and multiple objects ( 9th-10th rows). In ad-\ndition, our approach can produce more ﬁne-grained details as\nhighlighted in the salient region ( 11th-12th rows). The visual\nexamples indicate that our approach can better locate salient\nobjects and produce more accurate saliency maps.\n2) RGB-T SOD: For RGB-T SOD, our model is\ncompared with some SOTA RGB-T SOD algorithms,\nincluding MTMR[56], M3S-NIR[55], SGDL[58], ADF[59],\nECFFNet[11], MIDD[62], MMNet[63], CSRNet[65],\nCGFNet[64]. To ensure the fairness of the comparison results,\nthe saliency maps of the evaluation are provided by the\nauthors or generated by running source codes.\nQuantitative Evaluation. Fig.4 shows the comparison re-\nsults on PR curve. Table.II shows the quantitative comparison\nresults of four evaluation metrics. As can be clearly found\nfrom ﬁgure that our curves are very high, which means that\nour method is superior to the others with a large margin.\nFurthermore, from the table, we can see that all the evaluation\nmetrics are the best and our performance is signiﬁcantly\nimproved. The PR curve and evaluation metrics all verify the\neffectiveness and advantages of our proposed method in RGB-\nT SOD task.\nQualitative Evaluation. To make the qualitative compar-\nisons, we show some visual examples in Fig.5. It can be\nobserved that our method has the better detection results than\nother methods in some challenging cases: similar foreground\nand background ( 1st row), complex scene ( 2nd row), poor\nilluminance ( 3rd row), low contrast of thermal image ( 4th\nrow), small object ( 5th row) and multiple objects ( 6th row).\nIn addition, our approach is robust to noise disturbance, which\ncan be seen in the 7th row. These all indicate that our approach\ncan better adapt to different scenes, and work well by cross-\nmodality fusion.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 6\n(a) NLPR dataset (b) NJU2K dataset (c) STERE dataset\n(d) DES dataset (e) SIP dataset (f) DUT dataset\nFig. 2. P-R curves comparison of different models on six RGB-D datasets. Our SwinNet represented by red solid line outperforms SOTA models.\nTABLE I\nS-MEASURE , ADAPTIVE F-MEASURE , ADAPTIVE E-MEASURE , MAE COMPARISONS WITH DIFFERENT RGB-D MODELS . THE BEST RESULT IS IN BOLD .\nDatasets Metric D3Net ASIF-Net ICNet DCMF DRLF SSF SSMA A2dele UC-Net JL-DCF CoNet DANet EBFSP CDNet HAINet RD3D DSA2F MMNet VST SwinNet\nTNNLS20 TCYB20 TIP20 TIP20 TIP20 CVPR20 CVPR20 CVPR20 CVPR20 CVPR20 ECCV20 ECCV20 TMM21 TIP21 TIP21 AAAI21 CVPR21 TCSVT21 arXiv Ours\nNLPR\nS↑ .912 .909 .923 .900 .903 .914 .915 .896 .920 .925 .908 .920 .915 .902 .924 .930 .918 .925 .931 .941\nFβ ↑ .861 .869 .870 .839 .843 .875 .853 .878 .890 .878 .846 .875 .897 .848 .897 .892 .892 .889 .886 .908\nEξ ↑ .944 .944 .944 .933 .936 .949 .938 .945 .953 .953 .934 .951 .952 .935 .957 .958 .950 .950 .954 .967\nMAE↓ .030 .029 .028 .035 .032 .026 .030 .028 .025 .022 .031 .027 .026 .032 .024 .022 .024 .024 .023 .018\nNJU2K\nS↑ .901 .891 .894 .889 .886 .899 .894 .869 .897 .902 .895 .899 .903 .885 .912 .916 .904 .911 .922 .935\nFβ ↑ .865 .877 .868 .859 .849 .886 .865 .874 .889 .885 .872 .871 .894 .866 .900 .901 .898 .900 .899 .922\nEξ ↑ .914 .907 .905 .897 .901 .913 .896 .897 .903 .913 .912 .908 .907 .911 .922 .918 .922 .919 .914 .934\nMAE↓ .046 .047 .052 .052 .055 .043 .053 .051 .043 .041 .046 .045 .039 .048 .038 .036 .039 .038 .034 .027\nSTERE\nS↑ .899 .874 .903 .883 .888 .887 .890 .878 .903 .903 .905 .901 .900 .896 .907 .911 .897 .891 .913 .919\nFβ ↑ .859 .852 .865 .841 .845 .867 .855 .874 .885 .869 .884 .868 .870 .873 .885 .886 .893 .880 .878 .893\nEξ ↑ .920 .908 .915 .904 .915 .921 .907 .915 .922 .919 .927 .921 .912 .922 .925 .927 .927 .924 .917 .929\nMAE↓ .046 .051 .045 .054 .050 .046 .051 .044 .039 .040 .037 .043 .045 .042 .040 .037 .039 .045 .038 .033\nDES\nS↑ .898 .934 .920 .877 .895 .905 .941 .885 .933 .931 .911 .924 .937 .875 .935 .935 .916 .830 .943 .945\nFβ ↑ .870 .915 .889 .820 .868 .876 .906 .865 .917 .900 .861 .899 .913 .839 .924 .917 .901 .746 .917 .926\nEξ ↑ .951 .974 .959 .923 .954 .948 .974 .922 .974 .969 .945 .968 .974 .921 .974 .975 .955 .893 .979 .980\nMAE↓ .031 .019 .027 .040 .030 .025 .021 .028 .018 .020 .027 .023 .018 .034 .018 .019 .023 .058 .017 .016\nSIP\nS↑ .860 .857 .854 .859 .850 .868 .872 .826 .875 .880 .858 .875 .885 .823 .880 .885 .862 .836 .904 .911\nFβ ↑ .835 .847 .836 .819 .813 .851 .854 .825 .868 .873 .842 .855 .869 .805 .875 .874 .865 .839 .895 .912\nEξ ↑ .902 .895 .899 .898 .891 .911 .911 .892 .913 .921 .909 .914 .917 .880 .919 .920 .908 .882 .937 .943\nMAE↓ .063 .061 .069 .068 .071 .056 .057 .070 .051 .049 .063 .054 .049 .076 .053 .048 .057 .075 .040 .035\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 7\nFig. 3. Visual comparison with SOTA RGB-D models. Our SwinNet is outstanding in some challenging cases: similar foreground and background (1 st-\n2ndrows), complex scene (3 rd-4throws), depth image with low quality (5 th-6throws), small object (7 th-8throws), multiple objects (9 th-10throws) and\nﬁne-grained object ( 11th-12throws).\n(a) VT821 dataset (b) VT1000 dataset (c) VT5000 dataset\nFig. 4. P-R curves comparison of different models on three RGB-T datasets. Our SwinNet represented by red solid line outperforms SOTA models.\nD. Ablation studies\nWe conduct ablation studies on RGB-D SOD to verify all\nof components.\n1) The effectiveness of Swin Transformer backbone: We\nreplaces Swin Transformer backbone with some CNN back-\nbones (e.g., ResNet-50[96], Res2Net-50[97], ResNet-101[96],\nRes50+ViT16[98]) and transformer backbones (e.g., T2T-\n14[66] and PVT-M[68]) to check the effectiveness of back-\nbones. From Table. III, we can ﬁnd that the use of Swin\nTransformer signiﬁcantly improves the detection performance.\nIt proﬁts from the integration of locality merit of CNN and\nglobal-aware ability of transformer. We also give some visual\ncomparison of ResNet101 and Swin Transformer in Fig. 6.\nFrom the left to the right, there are RGB image, depth image,\nground truth (GT), the color feature in the fourth layer of\nResNet (ResNet-C4), the color feature in the fourth layer of\nSwin Transformer (Swin-C4), the depth feature in the fourth\nlayer of ResNet (ResNet-D4), the depth feature in the fourth\nlayer of Swin Transformer (Swin-D4), the prediction saliency\nmap of ResNet (ResNet-Pred), the prediction saliency map of\nSwin Transformer (Swin-Pred). From the Fig. 6 (a)(b), we\ncan discover that ResNet-C4 is interior to Swin-C4, so as to\ngenerate the blurry prediction saliency map. From the Fig. 6\n(c)(d), we can ﬁnd that some small objects are ignored in\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 8\nTABLE II\nS-MEASURE , ADAPTIVE F-MEASURE , ADAPTIVE E-MEASURE , MAE COMPARISONS WITH DIFFERENT RGB-T MODELS . THE BEST RESULT IS IN BOLD .\nDatasets Metric MTMR M3S-NIR SGDL ADF MIDD ECFFNet MMNet CSRNet CGFNet SwinNet\nIGTA18 MIPR19 TMM19 Arxiv20 TIP21 TCSVT21 TCSVT21 TCSVT21 TCSVT21 Ours\nVT821\nSα ↑ .725 .723 .765 .810 .871 .877 .875 .885 .881 .904\nFβ ↑ .662 .734 .730 .716 .804 .810 .798 .830 .845 .847\nEϵ ↑ .815 .859 .847 .842 .895 .902 .893 .908 .912 .926\nMAE↓ .108 .140 .085 .077 .045 .034 .040 .038 .038 .030\nVT1000\nSα ↑ .706 .726 .787 .910 .915 .923 .917 .918 .923 .938\nFβ ↑ .715 .717 .764 .847 .882 .876 .863 .877 .906 .896\nEϵ ↑ .836 .827 .856 .921 .933 .930 .924 .925 .944 .947\nMAE↓ .119 .145 .090 .034 .027 .021 .027 .024 .023 .018\nVT5000\nSα ↑ .680 .652 .750 .863 .867 .874 .864 .868 .883 .912\nFβ ↑ .595 .575 .672 .778 .801 .806 .785 .810 .851 .865\nEϵ ↑ .795 .780 .824 .891 .897 .906 .890 .905 .922 .942\nMAE↓ .114 .168 .089 .048 .043 .038 .043 .042 .035 .026\nFig. 5. Visual comparison with SOTA RGB-T models. Our SwinNet is outstanding in some challenging cases: similar foreground and background ( 1strow),\ncomplex scene ( 2ndrow), poor illuminance ( 3rdrow), low contrast of thermal image ( 4throw), small object ( 5throw), multiple objects ( 6throw) and noise\ndisturbance object ( 7throw).\nResNet-D4. It may be caused by the larger receptive ﬁeld\nin convolution neural network. Equipped with the long-range\ndependency merit, Swin-D4 shows salient features with more\nintegrity. Certainly, Swin-Pred shows the better result than\nResNet-Pred.\n2) The effectiveness of spatial alignment and channel re-\ncalibration module: Fig. 7 shows visual comparison of some\nablation studies. From left to right, there are RGB image,\ndepth image, ground truth saliency map, prediction saliency\nmap in the ﬁrst line. In other lines, there are features in\ndifferent layers, corresponding with the color features from\nbackbones {ST c\ni }4\ni=1, the color features after spatial alignment\nand channel re-calibration module {Fc\ni }4\ni=1, the depth features\nfrom backbones {ST d\ni }4\ni=1, the depth features after spatial\nalignment and channel re-calibration module {Fd\ni }4\ni=1, the\nfeatures from decoder {FFi}4\ni=1. Last, a group of features\nin the decoder without edge guidance are shown in the last\nline.\nFrom the comparison between Fig. 7 (a) and (b), we ﬁnd\nthat the color features after spatial alignment and channel re-\ncalibration are puriﬁed and the noises are obviously reduced,\nespecially in the ﬁrst column. From the comparison between\nFig. 7 (c) and (d), we ﬁnd that the depth features with the help\nof color features are close to ground truth, and salient region\nare misjudged less, especially in the second and third column.\nFurthermore, we replace spatial alignment and channel re-\ncalibration module with Depth-enhanced Module (DEM) in\nBBS-Net [39] which consists of the similar channel attention\nand spatial attention but no alignment operation to verify the\neffectiveness of spatial alignment and channel re-calibration\nmodule. From Table. IV, we can see that our S-measure, F-\nmeasure and E-measure and MAE wins about 0.006, 0.006,\n0.005 and 0.002 when compared with DEM. The proposed\nspatial alignment and channel re-calibration module enhances\nthe feature representation of color and depth images by the\nintra-layer interaction between two modalities and attentional\nweight assignment.\n3) The effectiveness of edge guidance: We remove the\nedge guidance in the decoder to verify its effectiveness. From\nFig. 7 (e) and (f), we can ﬁnd the use of edge features\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 9\nTABLE III\nEFFECTIVENESS ANALYSIS OF BACKBONE NETWORK , INCLUDING RESNET-50, R ES2NET-50, R ESNET-101, R ESNET-50+V IT16, T2T-14, PVT-M AND\nSWIN -B. T HE BEST RESULT IS IN BOLD .\nBackbone NLPR NJU2K STERE SIP\nS↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓\nResNet-50 .925 .878 .948 .026 .911 .894 .916 .039 .896 .872 .920 .044 .892 .879 .926 .046\nRes2Net-50 .905 .813 .924 .036 .897 .840 .891 .054 .880 .816 .892 .061 .870 .840 .905 .065\nResNet-101 .924 .884 .955 .024 .920 .904 .922 .034 .885 .861 .918 .049 .897 .888 .931 .043\nRes-50+ViT16 .932 .892 .960 .021 .922 .904 .918 .033 .903 .869 .917 .041 .894 .891 .930 .046\nT2T-14 .928 .880 .958 .022 .915 .893 .919 .037 .894 .856 .918 .044 .897 .887 .931 .045\nPVT-M .925 .879 .956 .023 .917 .898 .921 .036 .901 .869 .922 .042 .893 .888 .932 .043\nSwin-B .941 .908 .967 .018 .935 .922 .934 .027 .919 .893 .929 .033 .911 .912 .943 .035\nFig. 6. Visual comparison between ResNet and Swin Transformer. From the left to the right, there are RGB image, depth image, ground truth (GT), the\ncolor feature in the fourth layer of ResNet (ResNet-C4), the color feature in the fourth layer of Swin Transformer (Swin-C4), the depth feature in the fourth\nlayer of ResNet (ResNet-D4), the depth feature in the fourth layer of Swin Transformer (Swin-D4), the prediction saliency map of ResNet (ResNet-Pred), the\nprediction saliency map of Swin Transformer (Swin-Pred).\nTABLE IV\nEFFECTIVENESS ANALYSIS OF SPATIAL ALIGNMENT AND CHANNEL RE -CALIBRATION MODULE . DEM DENOTES THE MODEL WITH DEPTH -ENHANCED\nMODULE IN BBS-N ET [39] INSTEAD OF OUR SPATIAL ALIGNMENT AND CHANNEL RE -CALIBRATION MODULE . THE BEST RESULT IS IN BOLD .\nVariant NLPR NJU2K STERE SIP\nS↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓\nDEM .936 .903 .964 .019 .930 .918 .929 .028 .914 .888 .928 .034 .900 .901 .933 .040\nOurs .941 .908 .967 .018 .935 .922 .934 .027 .919 .893 .929 .033 .911 .912 .943 .035\nenhances the detail of detected objects. Meanwhile, from\nTable. V, we can also see that our S-measure, F-measure\nand E-measure and MAE are improved about 0.004, 0.009,\n0.006 and 0.003, respectively. It can further illustrate that edge\nguidance improves the performance of our proposed model to\nsome extend.\n4) The effectiveness of each modality:To verify the con-\ntribution of each modality, we conduct the ablation study.\nFrom Table.VI we can see that depth plays an obvious role\nin improving the performance from the ﬁrst and third lines.\nMeanwhile, we also observe that depth information is interior\nto color cue in SOD performance when comparing the ﬁrst and\nsecond lines. Especially, in STERE dataset, depth information\nplays a negative role because there are some depth images with\nlow quality. The third line denoted as fusion result achieves\nthe best results in a whole.\n5) Model complexity analysis: Model size of SwinNet is\n198.7M parameters. Its computation cost is about 124.3G\nFLOPs and inference speed is about 10 FPS including all\nthe IO and preprocessing. Its complexity is high. From Ta-\nble.VII, we can ﬁnd the computation cost mainly exists in\nSwin Transformer backbone. SwinNet-fuse denotes SwinNet\nremoving spatial alignment and channel re-calibration mod-\nule, SwinNet-edge denotes SwinNet removing edge-aware\nmodule, SwinNet-decoder denotes SwinNet removing edge-\nguided decoder. Spatial alignment and channel re-calibration\nmodule and edge-aware module nearly spend no computation\ncost. Edge-guided decoder cost a little computation due to\nsome convolution operations during upsampling process. The\nmajority of cost exists in two Swin Transformer backbones.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 10\nTABLE V\nEFFECTIVENESS ANALYSIS OF EDGE -GUIDED DECODER . THE BEST RESULT IS IN BOLD .\nVariant NLPR NJU2K STERE SIP\nS↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓\nWithout edge .938 .901 .963 .020 .928 .911 .922 .031 .919 .887 .927 .034 .905 .900 .937 .040\nOurs .941 .908 .967 .018 .935 .922 .934 .027 .919 .893 .929 .033 .911 .912 .943 .035\nTABLE VI\nABLATION STUDY ABOUT INDEPENDENT MODALITY IN RGB-D SOD. T HE BEST RESULT IS IN BOLD .\nVariant NLPR NJU2K STERE SIP\nS↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓ S↑ Fβ↑ Eξ ↑ MAE↓\nRGB .932 .869 .955 .024 .921 .901 .919 .036 .923 .898 .926 .034 .902 .890 .933 .042\nDepth .896 .837 .937 .034 .888 .862 .899 .051 .768 .744 .855 .093 .884 .881 .923 .050\nRGB+Depth .941 .908 .967 .018 .935 .922 .934 .027 .919 .893 .929 .033 .911 .912 .943 .035\nFig. 7. Visual comparison about the effectiveness of spatial alignment and\nchannel re-calibration module and edge guidance.\nTABLE VII\nABLATION STUDY ABOUT MODEL SIZE AND COMPUTATION COST .\nMethods SwinNet SwinNet-fuse SwinNet-edge SwinNet-decoder\nParams(M) 198.7 198.3 198.4 173.6\nFLOPs(G) 124.3 124.3 122.4 88.9\nV. C ONCLUSIONS\nInspired by the success of transformer, it is introduced to\ndrive RGB-D and RGB-T SOD. SwinNet achieves SOTA per-\nformance, in which Swin Transformer absorbs the local merit\nof CNN and global advantage to encode hierarchical features,\nspatial alignment and channel re-calibration module enhances\nthe intra-layer cross-modality features, edge-guided decoder\nstrengths the inter-layer cross-modality fusion. Supervised by\nedge and saliency map, SwinNet works excellent on public\ndatabases. Increasing accuracy also brings about a reduction\nin speed. In the future, we will discuss the lightweight design.\nREFERENCES\n[1] R. Cong, J. Lei, H. Fu, M.-M. Cheng, W. Lin, and Q. Huang, “Review\nof visual saliency detection with comprehensive information,” IEEE\nTransactions on circuits and Systems for Video Technology, vol. 29,\nno. 10, pp. 2941–2959, 2018.\n[2] X. Hu, C.-W. Fu, L. Zhu, T. Wang, and P.-A. Heng, “SAC-Net: Spatial\nattenuation context for salient object detection,” IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 31, no. 3, pp. 1079–\n1090, 2020.\n[3] Z. Tu, Y . Ma, C. Li, J. Tang, and B. Luo, “Edge-guided non-local fully\nconvolutional network for salient object detection,” IEEE transactions\non circuits and systems for video technology, vol. 31, no. 2, pp. 582–593,\n2020.\n[4] L. Wang, R. Chen, L. Zhu, H. Xie, and X. Li, “Deep sub-region network\nfor salient object detection,” IEEE Transactions on Circuits and Systems\nfor Video Technology, vol. 31, no. 2, pp. 728–741, 2020.\n[5] Z. Liu, S. Shi, Q. Duan, W. Zhang, and P. Zhao, “Salient object\ndetection for RGB-D image by single stream recurrent convolution\nneural network,” Neurocomputing, vol. 363, pp. 46–57, 2019.\n[6] N. Liu, N. Zhang, K. Wan, J. Han, and L. Shao, “Visual Saliency\nTransformer,” arXiv preprint arXiv:2104.12099, 2021.\n[7] G. Gao, W. Zhao, Q. Liu, and Y . Wang, “Co-Saliency Detection with Co-\nAttention Fully Convolutional Network,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 31, no. 3, pp. 877–889, 2020.\n[8] J. Han, G. Cheng, Z. Li, and D. Zhang, “A uniﬁed metric learning-based\nframework for co-saliency detection,”IEEE Transactions on Circuits and\nSystems for Video Technology, vol. 28, no. 10, pp. 2473–2483, 2017.\n[9] F. Guo, W. Wang, Z. Shen, J. Shen, L. Shao, and D. Tao, “Motion-\naware rapid video saliency detection,” IEEE Transactions on Circuits\nand Systems for Video Technology, vol. 30, no. 12, pp. 4887–4898, 2019.\n[10] M. Xu, B. Liu, P. Fu, J. Li, Y . H. Hu, and S. Feng, “Video salient\nobject detection via robust seeds extraction and multi-graphs manifold\npropagation,” IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 30, no. 7, pp. 2191–2206, 2019.\n[11] W. Zhou, Q. Guo, J. Lei, L. Yu, and J.-N. Hwang, “ECFFNet: effective\nand consistent feature fusion network for RGB-T salient object detec-\ntion,” IEEE Transactions on Circuits and Systems for Video Technology,\n2021.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 11\n[12] Q. Zhang, S. Wang, X. Wang, Z. Sun, S. Kwong, and J. Jiang, “A multi-\ntask collaborative network for light ﬁeld salient object detection,” IEEE\nTransactions on Circuits and Systems for Video Technology, vol. 31,\nno. 5, pp. 1849–1861, 2020.\n[13] Y . Piao, Z. Rong, S. Xu, M. Zhang, and H. Lu, “DUT-LFSaliency:\nVersatile Dataset and Light Field-to-RGB Saliency Detection,” arXiv\npreprint arXiv:2012.15124, 2020.\n[14] Y . Zhang, L. Zhang, W. Hamidouche, and O. Deforges, “CMA-Net:\nA Cascaded Mutual Attention Network for Light Field Salient Object\nDetection,” arXiv preprint arXiv:2105.00949, 2021.\n[15] P. Zhang, W. Liu, Y . Zeng, Y . Lei, and H. Lu, “Looking for the\ndetail and context devils: High-resolution salient object detection,” IEEE\nTransactions on Image Processing, vol. 30, pp. 3204–3216, 2021.\n[16] Y . Zeng, P. Zhang, J. Zhang, Z. Lin, and H. Lu, “Towards high-resolution\nsalient object detection,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 7234–7243.\n[17] C. Li, R. Cong, C. Guo, H. Li, C. Zhang, F. Zheng, and Y . Zhao, “A\nparallel down-up fusion network for salient object detection in optical\nremote sensing images,” Neurocomputing, vol. 415, pp. 411–420, 2020.\n[18] C. Li, R. Cong, J. Hou, S. Zhang, Y . Qian, and S. Kwong, “Nested\nnetwork with two-stream pyramid for salient object detection in optical\nremote sensing images,” IEEE Transactions on Geoscience and Remote\nSensing, vol. 57, no. 11, pp. 9156–9166, 2019.\n[19] Q. Zhang, R. Cong, C. Li, M.-M. Cheng, Y . Fang, X. Cao, Y . Zhao,\nand S. Kwong, “Dense attention ﬂuid network for salient object detec-\ntion in optical remote sensing images,” IEEE Transactions on Image\nProcessing, vol. 30, pp. 1305–1317, 2020.\n[20] G. Ma, S. Li, C. Chen, A. Hao, and H. Qin, “Stage-wise salient object\ndetection in 360 ◦ omnidirectional image via object-level semantical\nsaliency ranking,” IEEE Transactions on Visualization and Computer\nGraphics, vol. 26, no. 12, pp. 3535–3545, 2020.\n[21] M. Huang, Z. Liu, G. Li, X. Zhou, and O. Le Meur, “FANet: Features\nAdaptation Network for 360 ◦ Omnidirectional Salient Object Detec-\ntion,” IEEE Signal Processing Letters, vol. 27, pp. 1819–1823, 2020.\n[22] S. K. Yarlagadda, D. M. Montserrat, D. Guerra, C. J. Boushey, D. A.\nKerr, and F. Zhu, “Saliency-Aware Class-Agnostic Food Image Segmen-\ntation,” arXiv preprint arXiv:2102.06882, 2021.\n[23] H. Huang, M. Cai, L. Lin, J. Zheng, X. Mao, X. Qian, Z. Peng, J. Zhou,\nY . Iwamoto, X.-H. Han et al., “Graph-based Pyramid Global Con-\ntext Reasoning with a Saliency-aware Projection for COVID-19 Lung\nInfections Segmentation,” in ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2021, pp. 1050–1054.\n[24] C. Ma, Z. Miao, X.-P. Zhang, and M. Li, “A saliency prior context\nmodel for real-time object tracking,” IEEE Transactions on Multimedia,\nvol. 19, no. 11, pp. 2415–2424, 2017.\n[25] S. Hong, T. You, S. Kwak, and B. Han, “Online tracking by learning\ndiscriminative saliency map with convolutional neural network,” in\nInternational conference on machine learning, 2015, pp. 597–606.\n[26] P. Zhang, W. Liu, D. Wang, Y . Lei, H. Wang, and H. Lu, “Non-\nrigid object tracking via deep multi-scale spatial-temporal discriminative\nsaliency maps,” Pattern Recognition, vol. 100, p. 107130, 2020.\n[27] Y . Gao, M. Shi, D. Tao, and C. Xu, “Database saliency for fast image\nretrieval,” IEEE Transactions on Multimedia, vol. 17, no. 3, pp. 359–\n369, 2015.\n[28] Q.-G. Ji, Z.-D. Fang, Z.-H. Xie, and Z.-M. Lu, “Video abstraction based\non the visual attention model and online clustering,” Signal Processing:\nImage Communication, vol. 28, no. 3, pp. 241–253, 2013.\n[29] W. Wang, J. Shen, and H. Ling, “A deep network solution for attention\nand aesthetics aware photo cropping,” IEEE transactions on pattern\nanalysis and machine intelligence, vol. 41, no. 7, pp. 1531–1544, 2018.\n[30] Y . Xu, W. Xu, M. Wang, L. Li, G. Sang, P. Wei, and L. Zhu, “Saliency\naware image cropping with latent region pair,” Expert Systems with\nApplications, vol. 171, p. 114596, 2021.\n[31] M. Ahmadi, N. Karimi, and S. Samavi, “Context-aware saliency de-\ntection for image retargeting using convolutional neural networks,”\nMultimedia Tools and Applications, vol. 80, no. 8, pp. 11 917–11 941,\n2021.\n[32] Q. Jiang, F. Shao, W. Lin, K. Gu, G. Jiang, and H. Sun, “Optimizing mul-\ntistage discriminative dictionaries for blind image quality assessment,”\nIEEE Transactions on Multimedia, vol. 20, no. 8, pp. 2035–2048, 2017.\n[33] Z. Weng, W. Li, and Z. Jin, “Human activity prediction using saliency-\naware motion enhancement and weighted LSTM network,” EURASIP\nJournal on Image and Video Processing, vol. 2021, no. 1, pp. 1–23,\n2021.\n[34] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030, 2021.\n[35] R. Cong, J. Lei, H. Fu, J. Hou, Q. Huang, and S. Kwong, “Going from\nRGB to RGBD saliency: A depth-guided transformation model,” IEEE\ntransactions on cybernetics, vol. 50, no. 8, pp. 3627–3639, 2019.\n[36] C. Li, R. Cong, S. Kwong, J. Hou, H. Fu, G. Zhu, D. Zhang, and\nQ. Huang, “ASIF-Net: Attention steered interweave fusion network for\nRGB-D salient object detection,” IEEE transactions on cybernetics,\nvol. 51, no. 1, pp. 88–100, 2020.\n[37] N. Liu, N. Zhang, L. Shao, and J. Han, “Learning Selective Mutual\nAttention and Contrast for RGB-D Saliency Detection,” arXiv preprint\narXiv:2010.05537, 2020.\n[38] N. Liu, N. Zhang, and J. Han, “Learning Selective Self-Mutual Attention\nfor RGB-D Saliency Detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n13 756–13 765.\n[39] D.-P. Fan, Y . Zhai, A. Borji, J. Yang, and L. Shao, “BBS-Net: RGB-D\nsalient object detection with a bifurcated backbone strategy network,” in\nEuropean Conference on Computer Vision. Springer, 2020, pp. 275–\n292.\n[40] W. Zhang, Y . Jiang, K. Fu, and Q. Zhao, “BTS-Net: Bi-Directional\nTransfer-And-Selection Network for RGB-D Salient Object Detection,”\nin 2021 IEEE International Conference on Multimedia and Expo\n(ICME). IEEE, 2021, pp. 1–6.\n[41] Z. Chen, R. Cong, Q. Xu, and Q. Huang, “DPANet: Depth potentiality-\naware gated attention network for RGB-D salient object detection,”IEEE\nTransactions on Image Processing, 2020.\n[42] J. Wu, W. Zhou, T. Luo, L. Yu, and J. Lei, “Multiscale multilevel\ncontext and multimodal fusion for RGB-D salient object detection,”\nSignal Processing, vol. 178, p. 107766, 2021.\n[43] M. Zhang, W. Ren, Y . Piao, Z. Rong, and H. Lu, “Select, Supplement and\nFocus for RGB-D Saliency Detection,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n3472–3481.\n[44] W. Ji, J. Li, M. Zhang, Y . Piao, and H. Lu, “Accurate RGB-D salient\nobject detection via collaborative learning,” in Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XVIII 16. Springer, 2020, pp. 52–69.\n[45] C. Li, R. Cong, Y . Piao, Q. Xu, and C. C. Loy, “RGB-D salient object\ndetection with cross-modality modulation and selection,” in European\nConference on Computer Vision. Springer, 2020, pp. 225–241.\n[46] Y . Piao, Z. Rong, M. Zhang, W. Ren, and H. Lu, “A2dele: Adaptive and\nAttentive Depth Distiller for Efﬁcient RGB-D Salient Object Detection,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 9060–9069.\n[47] A. Luo, X. Li, F. Yang, Z. Jiao, H. Cheng, and S. Lyu, “Cascade\ngraph neural networks for rgb-d salient object detection,” in European\nConference on Computer Vision. Springer, 2020, pp. 346–364.\n[48] P. Sun, W. Zhang, H. Wang, S. Li, and X. Li, “Deep RGB-D Saliency\nDetection with Depth-Sensitive Attention and Automatic Multi-Modal\nFusion,” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 1407–1417.\n[49] Q. Chen, Z. Liu, Y . Zhang, K. Fu, Q. Zhao, and H. Du, “RGB-D Salient\nObject Detection via 3D Convolutional Neural Networks,” AAAI, 2021.\n[50] X. Zhao, Y . Pang, L. Zhang, H. Lu, and X. Ruan, “Self-Supervised\nRepresentation Learning for RGB-D Salient Object Detection,” arXiv\npreprint arXiv:2101.12482, 2021.\n[51] B. Jiang, Z. Zhou, X. Wang, J. Tang, and B. Luo, “cmSalGAN: RGB-\nD Salient Object Detection with Cross-View Generative Adversarial\nNetworks,” IEEE Transactions on Multimedia, 2020.\n[52] H. Chen, Y . Deng, Y . Li, T.-Y . Hung, and G. Lin, “RGBD salient object\ndetection via disentangled cross-modal fusion,” IEEE Transactions on\nImage Processing, vol. 29, pp. 8407–8416, 2020.\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, vol. 30. Curran Associates,\nInc., 2017, p. 5998–6008.\n[54] Y . Ma, D. Sun, Q. Meng, Z. Ding, and C. Li, “Learning multiscale deep\nfeatures and SVM regressors for adaptive RGB-T saliency detection,” in\n2017 10th International Symposium on Computational Intelligence and\nDesign (ISCID), vol. 1. IEEE, 2017, pp. 389–392.\n[55] Z. Tu, T. Xia, C. Li, Y . Lu, and J. Tang, “M3S-NIR: Multi-modal multi-\nscale noise-insensitive ranking for RGB-T saliency detection,” in 2019\nIEEE Conference on Multimedia Information Processing and Retrieval\n(MIPR). IEEE, 2019, pp. 141–146.\nJOURNAL OF LATEX CLASS FILES, VOL. 18, NO. 9, NOVEMBER 2021 12\n[56] G. Wang, C. Li, Y . Ma, A. Zheng, J. Tang, and B. Luo, “RGB-T saliency\ndetection benchmark: Dataset, baselines, analysis and a novel approach,”\nin Chinese Conference on Image and Graphics Technologies. Springer,\n2018, pp. 359–369.\n[57] J. Tang, D. Fan, X. Wang, Z. Tu, and C. Li, “RGBT salient object\ndetection: benchmark and a novel cooperative ranking approach,” IEEE\nTransactions on Circuits and Systems for Video Technology, vol. 30,\nno. 12, pp. 4421–4433, 2019.\n[58] Z. Tu, T. Xia, C. Li, X. Wang, Y . Ma, and J. Tang, “RGB-T image\nsaliency detection via collaborative graph learning,” IEEE Transactions\non Multimedia, vol. 22, no. 1, pp. 160–173, 2019.\n[59] Z. Tu, Y . Ma, Z. Li, C. Li, J. Xu, and Y . Liu, “RGBT salient\nobject detection: A large-scale dataset and benchmark,” arXiv preprint\narXiv:2007.03262, 2020.\n[60] Q. Zhang, N. Huang, L. Yao, D. Zhang, C. Shan, and J. Han, “RGB-\nT salient object detection via fusing multi-level CNN features,” IEEE\nTransactions on Image Processing, vol. 29, pp. 3321–3335, 2019.\n[61] Q. Zhang, T. Xiao, N. Huang, D. Zhang, and J. Han, “Revisiting Feature\nFusion for RGB-T Salient Object Detection,” IEEE Transactions on\nCircuits and Systems for Video Technology, 2020.\n[62] Z. Tu, Z. Li, C. Li, Y . Lang, and J. Tang, “Multi-Interactive Dual-\nDecoder for RGB-Thermal Salient Object Detection,” IEEE Transac-\ntions on Image Processing, vol. 30, pp. 5678–5691, 2021.\n[63] W. Gao, G. Liao, S. Ma, G. Li, Y . Liang, and W. Lin, “Uniﬁed\nInformation Fusion Network for Multi-Modal RGB-D and RGB-T\nSalient Object Detection,” IEEE Transactions on Circuits and Systems\nfor Video Technology, 2021.\n[64] J. Wang, K. Song, Y . Bao, L. Huang, and Y . Yan, “CGFNet: Cross-\nGuided Fusion Network for RGB-T Salient Object Detection,” IEEE\nTransactions on Circuits and Systems for Video Technology, 2021.\n[65] F. Huo, X. Zhu, L. Zhang, Q. Liu, and Y . Shu, “Efﬁcient Context-Guided\nStacked Reﬁnement Network for RGB-T Salient Object Detection,”\nIEEE Transactions on Circuits and Systems for Video Technology, 2021.\n[66] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, “Tokens-to-token vit: Training vision transformers from scratch\non imagenet,” arXiv preprint arXiv:2101.11986, 2021.\n[67] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“Cvt: Introducing convolutions to vision transformers,” arXiv preprint\narXiv:2103.15808, 2021.\n[68] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, “Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,” arXiv preprint arXiv:2102.12122 ,\n2021.\n[69] R. Ranftl, A. Bochkovskiy, and V . Koltun, “Vision transformers for dense\nprediction,” arXiv preprint arXiv:2103.13413, 2021.\n[70] H. Lin, X. Cheng, X. Wu, F. Yang, D. Shen, Z. Wang, Q. Song, and\nW. Yuan, “CAT: Cross Attention in Vision Transformer,”arXiv preprint\narXiv:2106.05786, 2021.\n[71] D. Feng, N. Barnes, S. You, and C. McCarthy, “Local background\nenclosure for RGB-D salient object detection,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2016, pp.\n2343–2350.\n[72] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in International Confer-\nence on Medical image computing and computer-assisted intervention.\nSpringer, 2015, pp. 234–241.\n[73] J. Canny, “A computational approach to edge detection,” IEEE Trans-\nactions on pattern analysis and machine intelligence, vol. 8, no. 6, pp.\n679–698, 1986.\n[74] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, “Rgbd salient object\ndetection: a benchmark and algorithms,” in European conference on\ncomputer vision. Springer, 2014, pp. 92–109.\n[75] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, “Depth saliency based\non anisotropic center-surround difference,” in 2014 IEEE international\nconference on image processing (ICIP). IEEE, 2014, pp. 1115–1119.\n[76] Y . Niu, Y . Geng, X. Li, and F. Liu, “Leveraging stereopsis for saliency\nanalysis,” in 2012 IEEE Conference on Computer Vision and Pattern\nRecognition. IEEE, 2012, pp. 454–461.\n[77] Y . Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, “Depth enhanced\nsaliency detection method,” in Proceedings of international conference\non internet multimedia computing and service, 2014, pp. 23–27.\n[78] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, “Rethinking\nRGB-D Salient Object Detection: Models, Data Sets, and Large-Scale\nBenchmarks,” IEEE Transactions on Neural Networks and Learning\nSystems, 2020.\n[79] Y . Piao, W. Ji, J. Li, M. Zhang, and H. Lu, “Depth-induced multi-\nscale recurrent attention network for saliency detection,” in Proceedings\nof the IEEE International Conference on Computer Vision, 2019, pp.\n7254–7263.\n[80] S. Chen and Y . Fu, “Progressively guided alternate reﬁnement network\nfor RGB-D salient object detection,” in European Conference on Com-\nputer Vision. Springer, 2020, pp. 520–538.\n[81] X. Zhao, L. Zhang, Y . Pang, H. Lu, and L. Zhang, “A single stream\nnetwork for robust and real-time RGB-D salient object detection,” in\nEuropean Conference on Computer Vision. Springer, 2020, pp. 646–\n662.\n[82] Z. Tu, Z. Li, C. Li, and J. Tang, “Multi-interactive encoder-decoder net-\nwork for rgbt salient object detection,”arXiv preprint arXiv:2005.02315,\n2020.\n[83] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:\nA benchmark,” IEEE transactions on image processing, vol. 24, no. 12,\npp. 5706–5722, 2015.\n[84] D.-P. Fan, M.-M. Cheng, Y . Liu, T. Li, and A. Borji, “Structure-measure:\nA new way to evaluate foreground maps,” in Proceedings of the IEEE\ninternational conference on computer vision, 2017, pp. 4548–4557.\n[85] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, “Frequency-tuned\nsalient region detection,” in 2009 IEEE conference on computer vision\nand pattern recognition. IEEE, 2009, pp. 1597–1604.\n[86] D.-P. Fan, C. Gong, Y . Cao, B. Ren, M.-M. Cheng, and A. Borji,\n“Enhanced-alignment measure for binary foreground map evaluation,”\narXiv preprint arXiv:1805.10421, 2018.\n[87] F. Perazzi, P. Kr ¨ahenb¨uhl, Y . Pritch, and A. Hornung, “Saliency ﬁlters:\nContrast based ﬁltering for salient region detection,” in 2012 IEEE\nconference on computer vision and pattern recognition. IEEE, 2012,\npp. 733–740.\n[88] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[89] G. Li, Z. Liu, and H. Ling, “ICNet: Information Conversion Network for\nRGB-D Based Salient Object Detection,” IEEE Transactions on Image\nProcessing, vol. 29, pp. 4873–4884, 2020.\n[90] X. Wang, S. Li, C. Chen, Y . Fang, A. Hao, and H. Qin, “Data-level\nrecombination and lightweight fusion scheme for RGB-D salient object\ndetection,” IEEE Transactions on Image Processing, vol. 30, pp. 458–\n471, 2020.\n[91] J. Zhang, D.-P. Fan, Y . Dai, S. Anwar, F. S. Saleh, T. Zhang, and\nN. Barnes, “UC-Net: uncertainty inspired RGB-D saliency detection via\nconditional variational autoencoders,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n8582–8591.\n[92] K. Fu, D.-P. Fan, G.-P. Ji, and Q. Zhao, “JL-DCF: Joint learning and\ndensely-cooperative fusion framework for rgb-d salient object detection,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2020, pp. 3052–3062.\n[93] N. Huang, Y . Yang, D. Zhang, Q. Zhang, and J. Han, “Employing\nBilinear Fusion and Saliency Prior Information for RGB-D Salient\nObject Detection,” IEEE Transactions on Multimedia, 2021.\n[94] W.-D. Jin, J. Xu, Q. Han, Y . Zhang, and M.-M. Cheng, “CDNet:\nComplementary Depth Network for RGB-D Salient Object Detection,”\nIEEE Transactions on Image Processing, vol. 30, pp. 3376–3390, 2021.\n[95] G. Li, Z. Liu, M. Chen, Z. Bai, W. Lin, and H. Ling, “Hierarchical\nAlternate Interaction Network for RGB-D Salient Object Detection,”\nIEEE Transactions on Image Processing, vol. 30, pp. 3528–3542, 2021.\n[96] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[97] S. Gao, M.-M. Cheng, K. Zhao, X.-Y . Zhang, M.-H. Yang, and P. H. Torr,\n“Res2Net: A new multi-scale backbone architecture,” IEEE transactions\non pattern analysis and machine intelligence, 2019.\n[98] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306, 2021."
}