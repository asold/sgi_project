{
  "title": "Tree Recurrent Neural Networks with Application to Language Modeling.",
  "url": "https://openalex.org/W2227882979",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2101736369",
      "name": "Xingxing Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103875444",
      "name": "Liang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2019867208",
      "name": "Mirella Lapata",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2250489405",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W71795751",
    "https://openalex.org/W1499864241",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2251433671",
    "https://openalex.org/W2115221470",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2103305545",
    "https://openalex.org/W2108425242",
    "https://openalex.org/W1971844566",
    "https://openalex.org/W1772447446",
    "https://openalex.org/W1808032177",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W2950797609",
    "https://openalex.org/W2166905217",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W1843946026",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W1607229519",
    "https://openalex.org/W1530801890",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2400801499"
  ],
  "abstract": "In this paper we develop a recurrent neural network (TreeRNN), which is designed to predict a tree rather than a linear sequence as is the case in conventional recurrent neural networks. Our model defines the probability of a sentence by estimating the generation probability of its dependency tree. We construct the tree incrementally by generating the left and right dependents of a node whose probability is computed using recurrent neural networks with shared hidden layers. Application of our model to two language modeling tasks shows that it outperforms or performs on par with related models.",
  "full_text": "Top-down Tree Long Short-Term Memory Networks\nXingxing Zhang, Liang Luand Mirella Lapata\nSchool of Informatics, University of Edinburgh\n10 Crichton Street, Edinburgh EH8 9AB, UK\n{x.zhang,liang.lu}@ed.ac.uk,mlap@inf.ed.ac.uk\nAbstract\nLong Short-Term Memory (LSTM) networks,\na type of recurrent neural network with a\nmore complex computational unit, have been\nsuccessfully applied to a variety of sequence\nmodeling tasks. In this paper we develop Tree\nLong Short-Term Memory (T REE LSTM), a\nneural network model based on LSTM, which\nis designed to predict a tree rather than a lin-\near sequence. T REE LSTM deﬁnes the prob-\nability of a sentence by estimating the gener-\nation probability of its dependency tree. At\neach time step, a node is generated based\non the representation of the generated sub-\ntree. We further enhance the modeling power\nof TREE LSTM by explicitly representing the\ncorrelations between left and right depen-\ndents. Application of our model to the MSR\nsentence completion challenge achieves re-\nsults beyond the current state of the art. We\nalso report results on dependency parsing\nreranking achieving competitive performance.\n1 Introduction\nNeural language models have been gaining increas-\ning attention as a competitive alternative to n-grams.\nThe main idea is to represent each word using a\nreal-valued feature vector capturing the contexts in\nwhich it occurs. The conditional probability of the\nnext word is then modeled as a smooth function of\nthe feature vectors of the preceding words and the\nnext word. In essence, similar representations are\nlearned for words found in similar contexts result-\ning in similar predictions for the next word. Previ-\nous approaches have mainly employed feed-forward\n(Bengio et al., 2003; Mnih and Hinton, 2007) and\nrecurrent neural networks (Mikolov et al., 2010;\nMikolov, 2012) in order to map the feature vec-\ntors of the context words to the distribution for the\nnext word. Recently, RNNs with Long Short-Term\nMemory (LSTM) units (Hochreiter and Schmidhu-\nber, 1997; Hochreiter, 1998) have emerged as a pop-\nular architecture due to their strong ability to capture\nlong-term dependencies. LSTMs have been success-\nfully applied to a variety of tasks ranging from ma-\nchine translation (Sutskever et al., 2014), to speech\nrecognition (Graves et al., 2013), and image descrip-\ntion generation (Vinyals et al., 2015).\nDespite superior performance in many applica-\ntions, neural language models essentially predict se-\nquences of words. Many NLP tasks, however, ex-\nploit syntactic information operating over tree struc-\ntures (e.g., dependency or constituent trees). In this\npaper we develop a novel neural network model\nwhich combines the advantages of the LSTM archi-\ntecture and syntactic structure. Our model estimates\nthe probability of a sentence by estimating the gen-\neration probability of its dependency tree. Instead\nof explicitly encoding tree structure as a set of fea-\ntures, we use four LSTM networks to model four\ntypes of dependency edges which altogether specify\nhow the tree is built. At each time step, one LSTM is\nactivated which predicts the next word conditioned\non the sub-tree generated so far. To learn the repre-\nsentations of the conditioned sub-tree, we force the\nfour LSTMs to share their hidden layers. Our model\nis also capable of generating trees just by sampling\nfrom a trained model and can be seamlessly inte-\ngrated with text generation applications.\narXiv:1511.00060v3  [cs.CL]  3 Apr 2016\nOur approach is related to but ultimately differ-\nent from recursive neural networks (Pollack, 1990)\na class of models which operate on structured in-\nputs. Given a (binary) parse tree, they recursively\ngenerate parent representations in a bottom-up fash-\nion, by combining tokens to produce representations\nfor phrases, and eventually the whole sentence. The\nlearned representations can be then used in classi-\nﬁcation tasks such as sentiment analysis (Socher et\nal., 2011b) and paraphrase detection (Socher et al.,\n2011a). Tai et al. (2015) learn distributed representa-\ntions over syntactic trees by generalizing the LSTM\narchitecture to tree-structured network topologies.\nThe key feature of our model is not so much that\nit can learn semantic representations of phrases or\nsentences, but its ability to predict tree structure and\nestimate its probability.\nSyntactic language models have a long history\nin NLP dating back to Chelba and Jelinek (2000)\n(see also Roark (2001) and Charniak (2001)). These\nmodels differ in how grammar structures in a parsing\ntree are used when predicting the next word. Other\nwork develops dependency-based language models\nfor speciﬁc applications such as machine translation\n(Shen et al., 2008; Zhang, 2009; Sennrich, 2015),\nspeech recognition (Chelba et al., 1997) or sentence\ncompletion (Gubbins and Vlachos, 2013). All in-\nstances of these models apply Markov assumptions\non the dependency tree, and adopt standard n-gram\nsmoothing methods for reliable parameter estima-\ntion. Emami et al. (2003) and Sennrich (2015) esti-\nmate the parameters of a structured language model\nusing feed-forward neural networks (Bengio et al.,\n2003). Mirowski and Vlachos (2015) re-implement\nthe model of Gubbins and Vlachos (2013) with\nRNNs. They view sentences as sequences of words\nover a tree. While they ignore the tree structures\nthemselves, we model them explicitly.\nOur model shares with other structured-based lan-\nguage models the ability to take dependency infor-\nmation into account. It differs in the following re-\nspects: (a) it does not artiﬁcially restrict the depth\nof the dependencies it considers and can thus be\nviewed as an inﬁnite order dependency language\nmodel; (b) it not only estimates the probability of a\nstring but is also capable of generating dependency\ntrees; (c) ﬁnally, contrary to previous dependency-\nbased language models which encode syntactic in-\nformation as features, our model takes tree structure\ninto account more directly via representing different\ntypes of dependency edges explicitly using LSTMs.\nTherefore, there is no need to manually determine\nwhich dependency tree features should be used or\nhow large the feature embeddings should be.\nWe evaluate our model on the MSR sentence com-\npletion challenge, a benchmark language modeling\ndataset. Our results outperform the best published\nresults on this dataset. Since our model is a general\ntree estimator, we also use it to rerank the top K de-\npendency trees from the (second order) MSTPasrser\nand obtain performance on par with recently pro-\nposed dependency parsers.\n2 Tree Long Short-Term Memory\nNetworks\nWe seek to estimate the probability of a sentence by\nestimating the generation probability of its depen-\ndency tree. Syntactic information in our model is\nrepresented in the form of dependency paths. In the\nfollowing, we ﬁrst describe our deﬁnition of depen-\ndency path and based on it explain how the proba-\nbility of a sentence is estimated.\n2.1 Dependency Path\nGenerally speaking, a dependency path is the path\nbetween ROOT and w consisting of the nodes on\nthe path and the edges connecting them. To rep-\nresent dependency paths, we introduce four types\nof edges which essentially deﬁne the “shape” of a\ndependency tree. Let w0 denote a node in a tree\nand w1,w2,..., wn its left dependents. As shown in\nFigure 1, L EFT edge is the edge between w0 and\nits ﬁrst left dependent denoted as (w0,w1). Let wk\n(with 1 < k ≤n) denote a non-ﬁrst left dependent\nof w0. The edge from wk−1 to wk is a N X-LEFT\nedge (NX stands for NEXT ), where wk−1 is the right\nadjacent sibling of wk. Note that the NX-LEFT edge\n(wk−1,wk) replaces edge (w0,wk) (illustrated with a\ndashed line in Figure 1) in the original dependency\ntree. The modiﬁcation allows information to ﬂow\nfrom w0 to wk through w1,..., wk−1 rather than di-\nrectly from w0 to wk. R IGHT and NX-RIGHT edges\nare deﬁned analogously for right dependents.\nGiven these four types of edges, dependency\npaths (denoted as D(w)) can be deﬁned as follows\nw0\nw1wk−1wkwn LEFT\nNX-LEFT\nFigure 1: LEFT and N X-LEFT edges. Dotted line between\nw1 and wk−1 (also between wk and wn) indicate that there may\nbe ≥0 nodes inbetween.\nbearing in mind that the ﬁrst right dependent of\nROOT is its only dependent and that wp denotes the\nparent of w. We use (... ) to denote a sequence,\nwhere () is an empty sequence and ∥is an operator\nfor concatenating two sequences.\n(1) if w is ROOT , then D(w) = ()\n(2) if w is a left dependentof wp\n(a) if w is the ﬁrst left dependent, then\nD(w) =D(wp)∥(⟨wp, LEFT ⟩)\n(b) if w is not the ﬁrst left dependent andws is\nits right adjacent sibling, then\nD(w) =D(ws)∥(⟨ws, NX-LEFT ⟩)\n(3) if w is a right dependentof wp\n(a) if w is the ﬁrst right dependent, then\nD(w) =D(wp)∥(⟨wp, RIGHT ⟩)\n(b) if w is not the ﬁrst right dependent and ws\nis its left adjacent sibling, then\nD(w) =D(ws)∥(⟨ws, NX-RIGHT ⟩)\nA dependency tree can be represented by the set of\nits dependency paths which in turn can be used to\nreconstruct the original tree.1\nDependency paths for the ﬁrst two levels\nof the tree in Figure 2 are as follows (ig-\nnoring for the moment the subscripts which\nwe explain in the next section). D(sold) =\n(⟨ROOT , RIGHT ⟩) (see deﬁnitions (1) and (3a)),\nD(year) = D(sold)∥(⟨sold, LEFT ⟩) (see (2a)),\nD(manufacturer) = D(year)∥(⟨year, NX-LEFT ⟩)\n(see (2b)), D(cars) = D(sold)∥(⟨sold, RIGHT ⟩)\n(see (3a)), D(in) = D(cars)∥(⟨cars, NX-RIGHT ⟩)\n(according to (3b)).\n2.2 Tree Probability\nThe core problem in syntax-based language model-\ning is to estimate the probability of sentence S given\n1Throughout this paper we assume all dependency trees are\nprojective.\nROOT\nsold1\nmanufacturer3\nThe9 luxury8 auto7\nyear2\nlast6\ncars4\n1,21410\nin5\nU.S.11\nthe12\nFigure 2: Dependency tree of the sentence The luxury auto\nmanufacturer last year sold 1,214 cars in the U.S.Subscripts\nindicate breadth-ﬁrst traversal. ROOT has only one dependent\n(i.e., sold) which we view as its ﬁrst right dependent.\nits corresponding tree T , P(S|T ). We view the prob-\nability computation of a dependency tree as a gener-\nation process. Speciﬁcally, we assume dependency\ntrees are constructed top-down, in a breadth-ﬁrst\nmanner. Generation starts at the ROOT node. For\neach node at each level, ﬁrst its left dependents are\ngenerated from closest to farthest and then the right\ndependents (again from closest to farthest). The\nsame process is applied to the next node at the same\nlevel or a node at the next level. Figure 2 shows the\nbreadth-ﬁrst traversal of a dependency tree.\nUnder the assumption that each word w in a de-\npendency tree is only conditioned on its dependency\npath, the probability of a sentence S given its depen-\ndency tree T is:\nP(S|T ) = ∏\nw∈BFS(T )\\ROOT\nP(w|D(w)) (1)\nwhere D(w) is the dependency path of w. Note that\neach word w is visited according to its breadth-ﬁrst\nsearch order (BFS(T)) and the probability of ROOT\nis ignored since every tree has one. The role of\nROOT in a dependency tree is the same as the begin\nof sentence token (BOS) in a sentence. When com-\nputing P(S|T ) (or P(S)), the probability of ROOT (or\nBOS) is ignored (we assume it always exists), but is\nused to predict other words. We explain in the next\nsection how TREE LSTM estimates P(w|D(w)).\n2.3 Tree LSTMs\nA dependency path D(w) is subtree which we de-\nnote as a sequence of ⟨word, edge-type⟩tuples. Our\nw0\nw1w2w3 w4 w5 w6\nGenerated by four LSTMswith tied Weand tied Who\nw0\nw1w2w3\nw0w1w2\nw4 w5 w6\nw0 w4 w5GEN-L\nGEN-NX-LGEN-NX-L GEN-R\nGEN-NX-R GEN-NX-R\nFigure 3: Generation process of left ( w1,w2,w3) and right\n(w4,w5,w6) dependents of tree nodewo (top) using four LSTMs\n(GEN-L, GEN-R, GEN-NX-L and GEN-NX-R). The model can\nhandle an arbitrary number of dependents due to G EN-NX-L\nand GEN-NX-R.\ninnovation is to learn the representation ofD(w) us-\ning four LSTMs. The four LSTMs (G EN-L, G EN-\nR, G EN-NX-L and G EN-NX-R) are used to repre-\nsent the four types of edges (L EFT , R IGHT , N X-\nLEFT and N X-RIGHT ) introduced earlier. G EN,\nNX, L and R are shorthands for GENERATE , NEXT,\nLEFT and R IGHT . At each time step, an LSTM is\nchosen according to an edge-type; then the LSTM\ntakes a word as input and predicts/generates its de-\npendent or sibling. This process can be also viewed\nas adding an edge and a node to a tree. Speciﬁ-\ncally, LSTMs G EN-L and G EN-R are used to gen-\nerate the ﬁrst left and right dependent of a node\n(w1 and w4 in Figure 3). So, these two LSTMs\nare responsible for going deeper in a tree. While\nGEN-NX-L and G EN-NX-R generate the remain-\ning left/right dependents and therefore go wider in\na tree. As shown in Figure 3, w2 and w3 are gener-\nated by G EN-NX-L, whereas w5 and w6 are gener-\nated by GEN-NX-R. Note that the model can handle\nany number of left or right dependents by applying\nGEN-NX-L or G EN-NX-R multiple times.\nWe assume time steps correspond to the steps\ntaken by the breadth-ﬁrst traversal of the depen-\ndency tree and the sentence has length n. At\ntime step t (1 ≤t ≤n), let ⟨wt′,zt ⟩denote the last\ntuple in D(wt ). Subscripts t and t′ denote the\nbreadth-ﬁrst search order of wt and wt′, respectively.\nzt ∈{LEFT , RIGHT , NX-LEFT , NX-RIGHT } is the\nedge type (see the deﬁnitions in Section 2.1). Let\nWe ∈Rs×|V |denote the word embedding matrix and\nWho ∈R|V |×d the output matrix of our model, where\n|V |is the vocabulary size,s the word embedding size\nand d the hidden unit size. We use tied We and tied\nWho for the four LSTMs to reduce the number of pa-\nrameters in our model. The four LSTMs also share\ntheir hidden states. Let H ∈Rd×(n+1) denote the\nshared hidden states of all time steps and e(wt ) the\none-hot vector of wt . Then, H[:,t] represents D(wt )\nat time step t, and the computation2 is:\nxt = We ·e(wt′) (2a)\nht = LSTMzt (xt ,H[:,t′]) (2b)\nH[:,t] =ht (2c)\nyt = Who ·ht (2d)\nwhere the initial hidden state H[:,0] is initialized to\na vector of small values such as 0.01. According to\nEquation (2b), the model selects an LSTM based on\nedge type zt . We describe the details of LSTM zt in\nthe next paragraph. The probability of wt given its\ndependency path D(wt ) is estimated by a softmax\nfunction:\nP(wt |D(wt )) = exp(yt,wt )\n∑\n|V |\nk′=1 exp(yt,k′)\n(3)\nWe must point out that although we use four jointly\ntrained LSTMs to encode the hidden states, the train-\ning and inference complexity of our model is no dif-\nferent from a regular LSTM, since at each time step\nonly one LSTM is working.\nWe implement LSTM z in Equation (2b) using a\ndeep LSTM (to simplify notation, from now on we\nwrite z instead of zt ). The inputs at time step t\nare xt and ht′ (the hidden state of an earlier time\nstep t′) and the output is ht (the hidden state of cur-\nrent time step). Let L denote the layer number of\nLSTMz and ˆhl\nt the internal hidden state of the l-th\nlayer of the LSTMz at time step t, where xt is ˆh0\nt and\nht′ is ˆhL\nt′. The LSTM architecture introduces mul-\ntiplicative gates and memory cells ˆcl\nt (at l-th layer)\nin order to address the vanishing gradient problem\nwhich makes it difﬁcult for the standard RNN model\nto learn long-distance correlations in a sequence.\nHere, ˆcl\nt is a linear combination of the current input\nsignal ut and an earlier memory cell ˆcl\nt′. How much\ninput information ut will ﬂow into ˆcl\nt is controlled\n2We ignore all bias terms for notational simplicity.\nw0\nw0w1w2\nw4 w5 w6\nw0 w4 w5GEN-L\nGEN-NX-LGEN-NX-L GEN-R\nGEN-NX-R GEN-NX-R\nw1w2w3\nLD LD\nFigure 4: Generation of left and right dependents of node w0\naccording to LDTREE LSTM.\nby input gate it and how much of the earlier mem-\nory cell ˆcl\nt′ will be forgotten is controlled by forget\ngate ft . This process is computed as follows:\nut = tanh(Wz,l\nux ·ˆhl−1\nt +Wz,l\nuh ·ˆhl\nt′) (4a)\nit = σ(Wz,l\nix ·ˆhl−1\nt +Wz,l\nih ·ˆhl\nt′) (4b)\nft = σ(Wz,l\nf x ·ˆhl−1\nt +Wz,l\nf h ·ˆhl\nt′) (4c)\nˆcl\nt = ft ⊙ˆcl\nt′+it ⊙ut (4d)\nwhere Wz,l\nux ∈Rd×d (Wz,l\nux ∈Rd×s when l = 1) and\nWz,l\nuh ∈Rd×d are weight matrices for ut , Wz,l\nix and\nWz,l\nih are weight matrices for it and Wz,l\nf x, and Wz,l\nf h\nare weight matrices for ft . σ is a sigmoid function\nand ⊙the element-wise product.\nOutput gate ot controls how much information of\nthe cell ˆcl\nt can be seen by other modules:\not = σ(Wz,l\nox ·ˆhl−1\nt +Wz,l\noh ·ˆhl\nt′) (5a)\nˆhl\nt = ot ⊙tanh(ˆcl\nt ) (5b)\nApplication of the above process to all layersL, will\nyield ˆhL\nt , which is ht . Note that in implementation,\nall ˆcl\nt and ˆhl\nt (1 ≤l ≤L) at time step t are stored,\nalthough we only care about ˆhL\nt (ht ).\n2.4 Left Dependent Tree LSTMs\nTREE LSTM computes P(w|D(w)) based on the de-\npendency path D(w), which ignores the interaction\nbetween left and right dependents on the same level.\nIn many cases, T REE LSTM will use a verb to pre-\ndict its object directly without knowing its subject.\nFor example, in Figure 2, TREE LSTM uses ⟨ROOT ,\nRIGHT ⟩and ⟨sold, RIGHT ⟩to predict cars. This in-\nformation is unfortunately not speciﬁc tocars (many\nthings can be sold, e.g., chocolates, candy). Consid-\nering manufacturer, the left dependent ofsold would\nhelp predict cars more accurately.\nIn order to jointly take left and right dependents\ninto account, we employ yet another LSTM, which\ngoes from the furthest left dependent to the closest\nleft dependent (L D is a shorthand for left depen-\ndent). As shown in Figure 4, L D LSTM learns the\nrepresentation of all left dependents of a node w0;\nthis representation is then used to predict the ﬁrst\nright dependent of the same node. Non-ﬁrst right de-\npendents can also leverage the representation of left\ndependents, since this information is injected into\nthe hidden state of the ﬁrst right dependent and can\npercolate all the way. Note that in order to retain the\ngeneration capability of our model (Section 3.4), we\nonly allow right dependents to leverage left depen-\ndents (they are generated before right dependents).\nThe computation of the L DTREE LSTM is al-\nmost the same as in T REE LSTM except when\nzt = GEN-R. In this case, let vt be the cor-\nresponding left dependent sequence with length\nK (vt = (w3,w2,w1) in Figure 4). Then, the hidden\nstate (qk) of vt at each time step k is:\nmk = We ·e(vt,k) (6a)\nqk = LSTMLD(mk,qk−1) (6b)\nwhere qK is the representation for all left depen-\ndents. Then, the computation of the current hid-\nden state becomes (see Equation (2) for the original\ncomputation):\nrt =\n[We ·e(wt′)\nqK\n]\n(7a)\nht = LSTMGEN-R (rt ,H[:,t′]) (7b)\nwhere qK serves as additional input for LSTMGEN-R .\nAll other computational details are the same as in\nTreeLSTM (see Section 2.3).\n2.5 Model Training\nOn small scale datasets we employ Negative Log-\nlikelihood (NLL) as our training objective for both\nTREE LSTM and L DTREE LSTM:\nLNLL(θ) =−1\n|S|∑\nS∈S\nlogP(S|T ) (8)\nwhere S is a sentence in the training set S, T is the\ndependency tree of S and P(S|T ) is deﬁned as in\nEquation (1).\nOn large scale datasets (e.g., with vocabulary\nsize of 65K), computing the output layer activa-\ntions and the softmax function with NLL would\nbecome prohibitively expensive. Instead, we em-\nploy Noise Contrastive Estimation (NCE; Gutmann\nand Hyv¨arinen (2012), Mnih and Teh (2012)) which\ntreats the normalization term ˆZ in ˆP(w|D(wt )) =\nexp(Who[w,:]·ht )\nˆZ as constant. The intuition behind NCE\nis to discriminate between samples from a data dis-\ntribution ˆP(w|D(wt )) and a known noise distribu-\ntion Pn(w) via binary logistic regression. Assuming\nthat noise words are k times more frequent than real\nwords in the training set (Mnih and Teh, 2012), then\nthe probability of a word w being from our model\nPd(w,D(wt )) is\nˆP(w|D(wt ))\nˆP(w|D(wt ))+kPn(w) . We apply NCE to\nlarge vocabulary models with the following training\nobjective:\nLNCE(θ) =− 1\n|S|∑\nT ∈S\n|T |\n∑\nt=1\n(\nlogPd(wt ,D(wt ))\n+\nk\n∑\nj=1\nlog[1 −Pd( ˜wt, j,D(wt ))]\n)\nwhere ˜wt, j is a word sampled from the noise distri-\nbution Pn(w). We use smoothed unigram frequen-\ncies (exponentiating by 0.75) as the noise distribu-\ntion Pn(w) (Mikolov et al., 2013b). We initialize\nln ˆZ = 9 as suggested in Chen et al. (2015), but in-\nstead of keeping it ﬁxed we also learnˆZ during train-\ning (Vaswani et al., 2013). We setk = 20.\n3 Experiments\nWe assess the performance of our model on two\ntasks: the Microsoft Research (MSR) sentence com-\npletion challenge (Zweig and Burges, 2012), and de-\npendency parsing reranking. We also demonstrate\nthe tree generation capability of our models. In the\nfollowing, we ﬁrst present details on model train-\ning and then present our results. We implemented\nour models using the Torch library (Collobert et\nal., 2011) and our code is available at https://\ngithub.com/XingxingZhang/td-treelstm.\n3.1 Training Details\nWe trained our model with back propagation\nthrough time (Rumelhart et al., 1988) on an Nvidia\nGPU Card with a mini-batch size of 64. The ob-\njective (NLL or NCE) was minimized by stochastic\ngradient descent. Model parameters were uniformly\ninitialized in [−0.1,0.1]. We used the NCE objec-\ntive on the MSR sentence completion task (due to\nthe large size of this dataset) and the NLL objec-\ntive on dependency parsing reranking. We used an\ninitial learning rate of 1.0 for all experiments and\nwhen there was no signiﬁcant improvement in log-\nlikelihood on the validation set, the learning rate was\ndivided by 2 per epoch until convergence (Mikolov\net al., 2010). To alleviate the exploding gradients\nproblem, we rescaled the gradient g when the gradi-\nent norm ||g||> 5 and set g = 5g\n||g|| (Pascanu et al.,\n2013; Sutskever et al., 2014). Dropout (Srivastava\net al., 2014) was applied to the 2-layer TREE LSTM\nand L DTREE LSTM models. The word embedding\nsize was set to s = d/2 where d is the hidden unit\nsize.\n3.2 Microsoft Sentence Completion Challenge\nThe task in the MSR Sentence Completion Chal-\nlenge (Zweig and Burges, 2012) is to select the\ncorrect missing word for 1,040 SAT-style test sen-\ntences when presented with ﬁve candidate comple-\ntions. The training set contains 522 novels from\nthe Project Gutenberg which we preprocessed as fol-\nlows. After removing headers and footers from the\nﬁles, we tokenized and parsed the dataset into de-\npendency trees with the Stanford Core NLP toolkit\n(Manning et al., 2014). The resulting training set\ncontained 49M words. We converted all words to\nlower case and replaced those occurring ﬁve times\nor less with UNK. The resulting vocabulary size\nwas 65,346 words. We randomly sampled 4,000\nsentences from the training set as our validation set.\nThe literature describes two main approaches to\nthe sentence completion task based on word vectors\nand language models. In vector-based approaches,\nall words in the sentence and the ﬁve candidate\nwords are represented by a vector; the candidate\nwhich has the highest average similarity with the\nsentence words is selected as the answer. For lan-\nguage model-based methods, the LM computes the\nprobability of a test sentence with each of the ﬁve\ncandidate words, and picks the candidate comple-\ntion which gives the highest probability. Our model\nbelongs to this class of models.\nModel d |θ| Accuracy\nWord Vector based Models\nLSA — — 49.0\nSkip-gram 640 102M 48.0\nIVLBL 600 96.0M 55.5\nLanguage Models\nKN5 — — 40.0\nUDepNgram — — 48.3\nLDepNgram — — 50.0\nRNN 300 48.1M 45.0\nRNNME 300 1120M 49.3\ndepRNN+3gram 100 1014M 53.5\nldepRNN+4gram 200 1029M 50.7\nLBL 300 48.0M 54.7\nLSTM 300 29.9M 55.00\nLSTM 400 40.2M 57.02\nLSTM 450 45.3M 55.96\nBidirectional LSTM 200 33.2M 48.46\nBidirectional LSTM 300 50.1M 49.90\nBidirectional LSTM 400 67.3M 48.65\nModel Combinations\nRNNMEs — — 55.4\nSkip-gram + RNNMEs — — 58.9\nOur Models\nTREE LSTM 300 31.6M 55.29\nLDTREE LSTM 300 32.5M 57.79\nTREE LSTM 400 43.1M 56.73\nLDTREE LSTM 400 44.7M 60.67\nTable 1:Model accuracy on the MSR sentence completion task.\nThe results of KN5, RNNME and RNNMEs are reported in\nMikolov (2012), LSA and RNN in Zweig et al. (2012), UDep-\nNgram and LDepNgram in Gubbins and Vlachos (2013), de-\npRNN+3gram and depRNN+4gram in Mirowski and Vlachos\n(2015), LBL in Mnih and Teh (2012), Skip-gram and Skip-\ngram+RNNMEs in Mikolov et al. (2013a), andIVLBL in Mnih\nand Kavukcuoglu (2013); d is the hidden size and |θ|the num-\nber of parameters in a model.\nTable 1 presents a summary of our results to-\ngether with previoulsy published results. The best\nperforming word vector model is IVLBL (Mnih and\nKavukcuoglu, 2013) with an accuracy of 55.5, while\nthe best performing single language model is LBL\n(Mnih and Teh, 2012) with an accuracy of 54.7.\nBoth approaches are based on the log-bilinear lan-\nguage model (Mnih and Hinton, 2007). A combi-\nnation of several recurrent neural networks and the\nskip-gram model holds the state of the art with an\naccuracy of 58.9 (Mikolov et al., 2013b). To fairly\ncompare with existing models, we restrict the layer\nParser Development Test\nUAS LAS UAS LAS\nMSTParser-2nd 92.20 88.78 91.63 88.44\nTREE LSTM 92.51 89.07 91.79 88.53\nTREE LSTM* 92.64 89.09 91.97 88.69\nLDTREE LSTM 92.66 89.14 91.99 88.69\nNN parser* 92.00 89.70 91.80 89.60\nS-LSTM* 93.20 90.90 93.10 90.90\nTable 2:Performance of T REE LSTM and L DTREE LSTM on\nreranking the top dependency trees produced by the 2nd order\nMSTParser (McDonald and Pereira, 2006). Results for the NN\nand S-LSTM parsers are reported in Chen and Manning (2014)\nand Dyer et al. (2015), respectively. * indicates that the model\nis initialized with pre-trained word vectors.\nsize of our models to 1. We observe that LDTREE L-\nSTM consistently outperforms T REE LSTM, which\nindicates the importance of modeling the interac-\ntion between left and right dependents. In fact,\nLDTREE LSTM ( d = 400) achieves a new state-of-\nthe-art on this task, despite being a single model.\nWe also implement LSTM and bidirectional LSTM\nlanguage models. 3 An LSTM with d = 400 out-\nperforms its smaller counterpart (d = 300), however\nperformance decreases with d = 450. The bidirec-\ntional LSTM is worse than the LSTM (see Mnih\nand Teh (2012) for a similar observation). The\nbest performing LSTM is worse than a L DTREE L-\nSTM ( d = 300). The input and output embeddings\n(We and Who) dominate the number of parame-\nters in all neural models except for RNNME, de-\npRNN+3gram and ldepRNN+4gram, which include\na ME model that contains 1 billion sparse n-gram\nfeatures (Mikolov, 2012; Mirowski and Vlachos,\n2015). The number of parameters in T REE LSTM\nand LDTREE LSTM is not much larger compared to\nLSTM due to the tied We and Who matrices.\n3.3 Dependency Parsing\nIn this section we demonstrate that our model can\nbe also used for parse reranking. This is not possi-\nble for sequence-based language models since they\ncannot estimate the probability of a tree. We use\nour models to rerank the top K dependency trees\nproduced by the second order MSTParser (McDon-\n3LSTMs and BiLSTMs were also trained with NCE\n(s = d/2; hyperparameters were tuned on the development set).\nald and Pereira, 2006). 4 We follow closely the ex-\nperimental setup of Chen and Manning (2014) and\nDyer et al. (2015). Speciﬁcally, we trained T REE L-\nSTM and L DTREE LSTM on Penn Treebank sec-\ntions 2–21. We used section 22 for development and\nsection 23 for testing. We adopted the Stanford ba-\nsic dependency representations (De Marneffe et al.,\n2006); part-of-speech tags were predicted with the\nStanford Tagger (Toutanova et al., 2003). We trained\nTREE LSTM and L DTREE LSTM as language mod-\nels (singletons were replaced with UNK) and did\nnot use any POS tags, dependency labels or com-\nposition features, whereas these features are used in\nChen and Manning (2014) and Dyer et al. (2015).\nWe tuned d, the number of layers, and K on the de-\nvelopment set.\nTable 2 reports unlabeled attachment scores\n(UAS) and labeled attachment scores (LAS) for\nthe MSTParser, T REE LSTM ( d = 300, 1 layer,\nK = 2), and L DTREE LSTM ( d = 200, 2 layers,\nK = 4). We also include the performance of two\nneural network-based dependency parsers; Chen and\nManning (2014) use a neural network classiﬁer to\npredict the correct transition (NN parser); Dyer et\nal. (2015) also implement a transition-based depen-\ndency parser using LSTMs to represent the contents\nof the stack and buffer in a continuous space. As can\nbe seen, both TREE LSTM and L DTREE LSTM out-\nperform the baseline MSTParser, with L DTREE L-\nSTM performing best. We also initialized the word\nembedding matrix We with pre-trained GLOVE vec-\ntors (Pennington et al., 2014). We obtained a slight\nimprovement over T REE LSTM (T REE LSTM* in\nTable 2; d = 200, 2 layer, K = 4) but no im-\nprovement over LDTREE LSTM. Finally, notice that\nLDTREE LSTM is slightly better than the NN parser\nin terms of UAS but worse than the S-LSTM parser.\nIn the future, we would like to extend our model so\nthat it takes labeled dependency information into ac-\ncount.\n3.4 Tree Generation\nThis section demonstrates how to use a trained\nLDTREE LSTM to generate tree samples. The gen-\neration starts at the ROOT node. At each time step t,\nfor each node wt , we add a new edge and node to\n4http://www.seas.upenn.edu/ strctlrn/MSTParser\nFigure 5: Generated dependency trees with L DTREE LSTM\ntrained on the PTB.\nthe tree. Unfortunately during generation, we do not\nknow which type of edge to add. We therefore use\nfour binary classiﬁers (A DD-LEFT , A DD-RIGHT ,\nADD-NX-LEFT and A DD-NX-RIGHT ) to predict\nwhether we should add a L EFT , R IGHT , N X-LEFT\nor N X-RIGHT edge.5 Then when a classiﬁer pre-\ndicts true, we use the corresponding LSTM to gener-\nate a new node by sampling from the predicted word\ndistribution in Equation (3). The four classiﬁers take\nthe previous hidden state H[:,t′] and the output em-\nbedding of the current node Who ·e(wt ) as features.6\nSpeciﬁcally, we use a trained L DTREE LSTM to\ngo through the training corpus and generate hidden\nstates and embeddings as input features; the corre-\nsponding class labels (true and false) are “read off”\nthe training dependency trees. We use two-layer rec-\ntiﬁer networks (Glorot et al., 2011) as the four clas-\nsiﬁers with a hidden size of 300. We use the same\nLDTREE LSTM model as in Section 3.3 to gener-\nate dependency trees. The classiﬁers were trained\nusing AdaGrad (Duchi et al., 2011) with a learning\nrate of 0.01. The accuracies of A DD-LEFT , A DD-\nRIGHT , A DD-NX-LEFT and A DD-NX-RIGHT are\n5It is possible to get rid of the four classiﬁers by adding\nSTART/STOP symbols when generating left and right depen-\ndents as in (Eisner, 1996). We refrained from doing this for\ncomputational reasons. For a sentence with N words, this ap-\nproach will lead to 2N additional START/STOP symbols (with\none START and one STOP symbol for each word). Conse-\nquently, the computational cost and memory consumption dur-\ning training will be three times as much rendering our model\nless scalable.\n6The input embeddings have lower dimensions and therefore\nresult in slightly worse classiﬁers.\n94.3%, 92.6%, 93.4% and 96.0%, respectively. Fig-\nure 5 shows examples of generated trees.\n4 Conclusions\nIn this paper we developed T REE LSTM (and\nLDTREE LSTM), a neural network model architec-\nture, which is designed to predict tree structures\nrather than linear sequences. Experimental results\non the MSR sentence completion task show that\nLDTREE LSTM is superior to sequential LSTMs.\nDependency parsing reranking experiments high-\nlight our model’s potential for dependency pars-\ning. Finally, the ability of our model to gener-\nate dependency trees holds promise for text gen-\neration applications such as sentence compression\nand simpliﬁcation (Filippova et al., 2015). Although\nour experiments have focused exclusively on depen-\ndency trees, there is nothing inherent in our formu-\nlation that disallows its application to other types of\ntree structure such as constituent trees or even tax-\nonomies.\nAcknowledgments\nWe would like to thank Adam Lopez, Frank Keller,\nIain Murray, Li Dong, Brian Roark, and the NAACL\nreviewers for their valuable feedback. Xingxing\nZhang gratefully acknowledges the ﬁnancial sup-\nport of the China Scholarship Council (CSC). Liang\nLu is funded by the UK EPSRC Programme Grant\nEP/I031022/1, Natural Speech Technology (NST).\nReferences\n[Bengio et al.2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Janvin. 2003. A neural\nprobabilistic language model. The Journal of Machine\nLearning Research, 3:1137–1155.\n[Charniak2001] Eugene Charniak. 2001. Immediate-\nhead parsing for language models. In Proceedings of\nthe 39th Annual Meeting on Association for Compu-\ntational Linguistics , pages 124–131. Association for\nComputational Linguistics.\n[Chelba and Jelinek2000] Ciprian Chelba and Frederick\nJelinek. 2000. Structured language modeling. Com-\nputer Speech and Language, 14(4):283–332.\n[Chelba et al.1997] Ciprian Chelba, David Engle, Freder-\nick Jelinek, Victor Jimenez, Sanjeev Khudanpur, Lidia\nMangu, Harry Printz, Eric Ristad, Ronald Rosenfeld,\nAndreas Stolcke, et al. 1997. Structure and per-\nformance of a dependency language model. In EU-\nROSPEECH. Citeseer.\n[Chen and Manning2014] Danqi Chen and Christopher\nManning. 2014. A fast and accurate dependency\nparser using neural networks. In Proceedings of\nthe 2014 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP) , pages 740–750,\nDoha, Qatar, October. Association for Computational\nLinguistics.\n[Chen et al.2015] X Chen, X Liu, MJF Gales, and\nPC Woodland. 2015. Recurrent neural network lan-\nguage model training with noise contrastive estimation\nfor speech recognition. In In 40th IEEE International\nConference on Accoustics, Speech and Signal Process-\ning, pages 5401–5405, Brisbane, Australia.\n[Collobert et al.2011] Ronan Collobert, Koray\nKavukcuoglu, and Cl ´ement Farabet. 2011. Torch7:\nA matlab-like environment for machine learning. In\nBigLearn, NIPS Workshop , number EPFL-CONF-\n192376.\n[De Marneffe et al.2006] Marie-Catherine De Marneffe,\nBill MacCartney, Christopher D Manning, et al. 2006.\nGenerating typed dependency parses from phrase\nstructure parses. In Proceedings of LREC, volume 6,\npages 449–454.\n[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram\nSinger. 2011. Adaptive subgradient methods for on-\nline learning and stochastic optimization. The Journal\nof Machine Learning Research, 12:2121–2159.\n[Dyer et al.2015] Chris Dyer, Miguel Ballesteros, Wang\nLing, Austin Matthews, and Noah A. Smith. 2015.\nTransition-based dependency parsing with stack long\nshort-term memory. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long Pa-\npers), pages 334–343, Beijing, China, July. Associa-\ntion for Computational Linguistics.\n[Eisner1996] Jason M Eisner. 1996. Three new prob-\nabilistic models for dependency parsing: An explo-\nration. In Proceedings of the 16th conference on Com-\nputational linguistics-Volume 1, pages 340–345. Asso-\nciation for Computational Linguistics.\n[Emami et al.2003] Ahmad Emami, Peng Xu, and Fred-\nerick Jelinek. 2003. Using a connectionist model in\na syntactical based language model. In Proceedings\nof the IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, pages 372–375, Hong\nKong, China.\n[Filippova et al.2015] Katja Filippova, Enrique Alfon-\nseca, Carlos A Colmenares, Lukasz Kaiser, and Oriol\nVinyals. 2015. Sentence compression by deletion\nwith lstms. In EMNLP, pages 360–368.\n[Glorot et al.2011] Xavier Glorot, Antoine Bordes, and\nYoshua Bengio. 2011. Deep sparse rectiﬁer neural\nnetworks. In International Conference on Artiﬁcial In-\ntelligence and Statistics, pages 315–323.\n[Graves et al.2013] Alan Graves, Abdel-rahman Mo-\nhamed, and Geoffrey Hinton. 2013. Speech recogni-\ntion with deep recurrent neural networks. InAcoustics,\nSpeech and Signal Processing (ICASSP), 2013 IEEE\nInternational Conference on, pages 6645–6649. IEEE.\n[Gubbins and Vlachos2013] Joseph Gubbins and Andreas\nVlachos. 2013. Dependency language models for sen-\ntence completion. In EMNLP, pages 1405–1410, Seat-\ntle, Washington, USA, October. Association for Com-\nputational Linguistics.\n[Gutmann and Hyv¨arinen2012] Michael U Gutmann and\nAapo Hyv¨arinen. 2012. Noise-contrastive estimation\nof unnormalized statistical models, with applications\nto natural image statistics. The Journal of Machine\nLearning Research, 13(1):307–361.\n[Hochreiter and Schmidhuber1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. 1997. Long short-term memory.\nNeural computation, 9(8):1735–1780.\n[Hochreiter1998] Sepp Hochreiter. 1998. Vanishing gra-\ndient problem during learning recurrent neural nets\nand problem solutions. International Journal of Un-\ncertainty, Fuzziness and Knowledge-based Systems ,\n6(2):107–116.\n[Manning et al.2014] Christopher D Manning, Mihai Sur-\ndeanu, John Bauer, Jenny Finkel, Steven J Bethard,\nand David McClosky. 2014. The stanford corenlp\nnatural language processing toolkit. In Proceedings\nof 52nd Annual Meeting of the Association for Com-\nputational Linguistics: System Demonstrations, pages\n55–60.\n[McDonald and Pereira2006] Ryan T McDonald and Fer-\nnando CN Pereira. 2006. Online learning of approxi-\nmate dependency parsing algorithms. In EACL.\n[Mikolov et al.2010] Tomas Mikolov, Martin Karaﬁ ´at,\nLukas Burget, Jan Cernock `y, and Sanjeev Khudan-\npur. 2010. Recurrent neural network based language\nmodel. In INTERSPEECH 2010, 11th Annual Confer-\nence of the International Speech Communication As-\nsociation, Makuhari, Chiba, Japan, September 26-30,\n2010, pages 1045–1048.\n[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg\nCorrado, and Jeffrey Dean. 2013a. Efﬁcient esti-\nmation of word representations in vector space. In\nProceedings of the 2013 International Conference on\nLearning Representations, Scottsdale, Arizona, USA.\n[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,\nKai Chen, Greg S Corrado, and Jeff Dean. 2013b.\nDistributed representations of words and phrases and\ntheir compositionality. In Advances in Neural Infor-\nmation Processing Systems 26, pages 3111–3119.\n[Mikolov2012] Tomas Mikolov. 2012. Statistical Lan-\nguage Models based on Neural Networks . Ph.D. the-\nsis, Brno University of Technology.\n[Mirowski and Vlachos2015] Piotr Mirowski and An-\ndreas Vlachos. 2015. Dependency recurrent neural\nlanguage models for sentence completion. In ACL,\npages 511–517, Beijing, China, July. Association for\nComputational Linguistics.\n[Mnih and Hinton2007] Andriy Mnih and Geoffrey Hin-\nton. 2007. Three new graphical models for statistical\nlanguage modelling. In Proceedings of the 24th In-\nternational Conference on Machine Learning , pages\n641–648.\n[Mnih and Kavukcuoglu2013] Andriy Mnih and Koray\nKavukcuoglu. 2013. Learning word embeddings efﬁ-\nciently with noise-contrastive estimation. In Advances\nin Neural Information Processing Systems 26 , pages\n2265–2273.\n[Mnih and Teh2012] Andriy Mnih and Yee Whye Teh.\n2012. A fast and simple algorithm for training neural\nprobabilistic language models. In Proceedings of the\n29th International Conference on Machine Learning ,\npages 1751–1758, Edinburgh, Scotland.\n[Pascanu et al.2013] Razvan Pascanu, Tomas Mikolov,\nand Yoshua Bengio. 2013. On the difﬁculty of train-\ning recurrent neural networks. In Proceedings of the\n31st International Conference on Machine Learning ,\npages 1310–1318, Atlanta, Georgia, USA.\n[Pennington et al.2014] Jeffrey Pennington, Richard\nSocher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. EMNLP,\n12:1532–1543.\n[Pollack1990] Jordan B. Pollack. 1990. Recursive dis-\ntributed representations. Artiﬁcial Intelligence , 1–\n2(46):77–105.\n[Roark2001] Brian Roark. 2001. Probabilistic top-down\nparsing and language modeling. Computational lin-\nguistics, 27(2):249–276.\n[Rumelhart et al.1988] David E Rumelhart, Geoffrey E\nHinton, and Ronald J Williams. 1988. Learning repre-\nsentations by back-propagating errors. Cognitive mod-\neling, 5:3.\n[Sennrich2015] Rico Sennrich. 2015. Modelling and op-\ntimizing on syntactic n-grams for statistical machine\ntranslation. Transactions of the Association for Com-\nputational Linguistics, 3:169–182.\n[Shen et al.2008] Libin Shen, Jinxi Xu, and Ralph\nWeischedel. 2008. A new string-to-dependency ma-\nchine translation algorithm with a target dependency\nlanguage model. In Proceedings of ACL-08: HLT ,\npages 577–585, Columbus, Ohio, USA.\n[Socher et al.2011a] Richard Socher, Eric H. Huang, Jef-\nfrey Pennington, Christopher D. Manning, and An-\ndrew Ng. 2011a. Dynamic pooling and unfolding\nrecursive autoencoders for paraphrase detection. In\nAdvances in Neural Information Processing Systems ,\npages 801–809.\n[Socher et al.2011b] Richard Socher, Jeffrey Pennington,\nEric H. Huang, Andrew Y . Ng, and Christopher D.\nManning. 2011b. Semi-supervised recursive autoen-\ncoders for predicting sentiment distributions. In Pro-\nceedings of the 2011 Conference on Empirical Meth-\nods in Natural Language Processing, pages 151–161,\nEdinburgh, Scotland, UK.\n[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hin-\nton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A simple way to pre-\nvent neural networks from overﬁtting. The Journal of\nMachine Learning Research, 15(1):1929–1958.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc VV Le. 2014. Sequence to sequence learning\nwith neural networks. In Advances in Neural Informa-\ntion Processing Systems, pages 3104–3112.\n[Tai et al.2015] Kai Sheng Tai, Richard Socher, and\nChristopher D. Manning. 2015. Improved semantic\nrepresentations from tree-structured long short-term\nmemory networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguis-\ntics and the 7th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers) ,\npages 1556–1566, Beijing, China, July. Association\nfor Computational Linguistics.\n[Toutanova et al.2003] Kristina Toutanova, Dan Klein,\nChristopher D Manning, and Yoram Singer. 2003.\nFeature-rich part-of-speech tagging with a cyclic de-\npendency network. In Proceedings of the 2003 Con-\nference of the North American Chapter of the Associ-\nation for Computational Linguistics on Human Lan-\nguage Technology-Volume 1, pages 173–180. Associa-\ntion for Computational Linguistics.\n[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,\nVictoria Fossum, and David Chiang. 2013. Decod-\ning with large-scale neural language models improves\ntranslation. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 1387–1392, Seattle, Washington, USA.\n[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,\nSamy Bengio, and Dumitru Erhan. 2015. Show and\ntell: A neural image caption generator. In The IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, Boston, Massachusetts, USA.\n[Zhang2009] Ying Zhang. 2009. Structured language\nmodels for statistical machine translation . Ph.D. the-\nsis, Johns Hopkins University.\n[Zweig and Burges2012] Geoffrey Zweig and Chris J.C.\nBurges. 2012. A challenge set for advancing language\nmodeling. In Proceedings of the NAACL-HLT 2012\nWorkshop: Will We Ever Really Replace the N-gram\nModel? On the Future of Language Modeling for HLT,\npages 29–36, Montr´eal, Canada.\n[Zweig et al.2012] Geoffrey Zweig, John C Platt, Christo-\npher Meek, Christopher JC Burges, Ainur Yessenalina,\nand Qiang Liu. 2012. Computational approaches\nto sentence completion. In Proceedings of the 50th\nAnnual Meeting of the Association for Computational\nLinguistics: Long Papers-Volume 1 , pages 601–610.\nAssociation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7796207070350647
    },
    {
      "name": "Recurrent neural network",
      "score": 0.741577684879303
    },
    {
      "name": "Tree (set theory)",
      "score": 0.6964678168296814
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6723581552505493
    },
    {
      "name": "Language model",
      "score": 0.6034567952156067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5972424745559692
    },
    {
      "name": "Artificial neural network",
      "score": 0.5474135279655457
    },
    {
      "name": "Construct (python library)",
      "score": 0.5143047571182251
    },
    {
      "name": "Sentence",
      "score": 0.48975634574890137
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4838232398033142
    },
    {
      "name": "Decision tree model",
      "score": 0.44591498374938965
    },
    {
      "name": "Machine learning",
      "score": 0.4425075650215149
    },
    {
      "name": "Node (physics)",
      "score": 0.4397198557853699
    },
    {
      "name": "Decision tree",
      "score": 0.34501907229423523
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32185807824134827
    },
    {
      "name": "Mathematics",
      "score": 0.12135109305381775
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}