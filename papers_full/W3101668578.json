{
  "title": "Multi-Unit Transformers for Neural Machine Translation",
  "url": "https://openalex.org/W3101668578",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2274786728",
      "name": "Jian-hao Yan",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133392087",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2807880213",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2913637113",
    "https://openalex.org/W2997110224",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W2946462349",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964048171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3004484424",
    "https://openalex.org/W3080183383",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2912934387",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2970550739",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2998681865",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2970682957",
    "https://openalex.org/W2962822108",
    "https://openalex.org/W2989571009",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2888519496",
    "https://openalex.org/W2984489779"
  ],
  "abstract": "Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention. In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units. Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity. Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units. Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%). In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters. These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1047–1059,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n1047\nMulti-Unit Transformers for Neural Machine Translation\nJianhao Yan Fandong Meng Jie Zhou\nPattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China\n{elliottyan,fandongmeng,withtomzhou}@tencent.com\nAbstract\nTransformer models (Vaswani et al., 2017)\nachieve remarkable success in Neural Ma-\nchine Translation. Many efforts have been de-\nvoted to deepening the Transformer by stack-\ning several units (i.e., a combination of Multi-\nhead Attentions and FFN) in a cascade, while\nthe investigation over multiple parallel units\ndraws little attention. In this paper, we pro-\npose the Multi-Unit TransformErs ( MUTE),\nwhich aim to promote the expressiveness of\nthe Transformer by introducing diverse and\ncomplementary units. Speciﬁcally, we use sev-\neral parallel units and show that modeling with\nmultiple units improves model performance\nand introduces diversity. Further, to better\nleverage the advantage of the multi-unit set-\nting, we design biased module and sequen-\ntial dependency that guide and encourage com-\nplementariness among different units. Exper-\nimental results on three machine translation\ntasks, the NIST Chinese-to-English, WMT’14\nEnglish-to-German and WMT’18 Chinese-to-\nEnglish, show that the MUTE models signif-\nicantly outperform the Transformer-Base, by\nup to +1.52, +1.90 and +1.10 BLEU points,\nwith only a mild drop in inference speed (about\n3.1%). In addition, our methods also surpass\nthe Transformer-Big model, with only 54% of\nits parameters. These results demonstrate the\neffectiveness of the MUTE, as well as its ef-\nﬁciency in both the inference process and pa-\nrameter usage. 1\n1 Introduction\nTransformer based models (Vaswani et al., 2017)\nhave been proven to be very effective in build-\ning the state-of-the-art Neural Machine Transla-\ntion (NMT) systems via neural networks and atten-\ntion mechanism (Sutskever et al., 2014; Bahdanau\n1Code is available at https://github.com/Ellio\nttYan/Multi Unit Transformer\net al., 2014). Following the standard Sequence-to-\nSequence architecture, Transformer models consist\nof two essential components, namely the encoder\nand decoder, which rely on stacking several identi-\ncal layers, i.e., multihead attentions and position-\nwise feed-forward network.\nMultihead attentions and position-wise feed-\nforward network, together as a basic unit, plays an\nessential role in the success of Transformer mod-\nels. Some researchers (Bapna et al., 2018; Wang\net al., 2019a) propose to improve the model capac-\nity by stacking this basic unit many times, i.e., deep\nTransformers, and achieve promising results. Nev-\nertheless, as an orthogonal direction, investigation\nover multiple parallel units draws little attention.\nCompared with single unit models, multiple par-\nallel unit layout is more expressive to capture com-\nplex information ﬂow (Tao et al.; Meng et al., 2019;\nLi et al., 2018, 2019) in two aspects. First, this\nmultiple-unit layout boosts the model by its varied\nfeature space composition and different attentions\nover inputs. With this diversity, multi-unit models\nadvance in expressiveness. Second, for the multi-\nunit setting, one unit could mitigate the deﬁciency\nof other units and compose a more expressive net-\nwork, in a complementary way.\nIn this paper, we propose the Multi-Unit\nTransformErs ( MUTE), which aim to promote\nthe expressiveness of transformer models by intro-\nducing diverse and complementary parallel units.\nMerely combining multiple identical units in par-\nallel improves model capability and diversity by\nits varied feature compositions. Furthermore, in-\nspired by the well-studied bagging (Breiman, 1996)\nand gradient boosting algorithms (Friedman, 2001)\nin the machine learning ﬁeld, we design biased\nunits with a sequential dependency to further boost\nmodel performance. Speciﬁcally, with the help of\na module named bias module, we apply different\nkinds of noises to form biased inputs for corre-\n1048\nsponding units. By doing so, we explicitly estab-\nlish information gaps among units and guide them\nto learn from each other. Moreover, to better lever-\nage the power of complementariness, we introduce\nsequential ordering into the multi-unit setting, and\nforce each unit to learn the residual of its preceding\naccumulation.\nWe evaluate our methods on three widely\nused Neural Machine Translation datasets, NIST\nChinese-English, WMT’14 English-German and\nWMT’18 Chinese-English. Experimental results\nshow that our multi-unit model yields an improve-\nment of +1.52, +1.90 and +1.10 BLEU points,\nover the baseline model (Transformer-Base) for\nthree tasks with different sizes, respectively. Our\nmodel even outperforms the Transformer-Big on\nthe WMT’14 English-German by 0.7 BLEU points\nwith only 54% of parameters. Moreover, as an inter-\nesting side effect, our model only introduces mild\ninference speed decrease (about 3.1%) compared\nwith the Transformer-Base model, and is faster than\nthe Transformer-Big model.\nThe contributions of this paper are threefold:\n• We propose the Multi-Unit TransformErs\n(MUTE), to promote the expressiveness of\nTransformer models by introducing diverse\nand complementary parallel units.\n• Aside from learning with identical units, we\nextend the MUTE by introducing bias mod-\nule and sequential ordering to further model\nthe diversity and complementariness among\ndifferent units.\n• Experimental results show that our mod-\nels substantially surpass baseline models in\nthree NMT datasets, NIST Chinese-English,\nWMT’14 English-German and WMT’18\nChinese-English. In addition, our models also\nshow high efﬁciency in both the inference\nspeed and parameter usage, compared with\nTransformer baselines.\n2 Transformer Architecture\nThe Transformer Architecture (Vaswani et al.,\n2017) for Neural Machine Translation (NMT)\ngenerally adopts the standard encoder-decoder\nparadigm. In contrast to RNN architectures, the\nTransformer stacks several identical self-attention\nbased layers instead of recurrent units for better par-\nallelization. Speciﬁcally, given an input sequence\nX = {x1,x2,··· ,xn}in source language (e.g.,\nEnglish), the model is asked to predict its corre-\nsponding translation Y = {y1,y2,··· ,ym}in tar-\nget language (e.g., German).\nEncoder. Digging into the details of the model, a\nTransformer encoder consists of Ne stacked layers,\nwhere each layer consists of two sub-layers, a mul-\ntihead self-attention sub-layer and a position-wise\nfeed-forward network (FFN) sub-layer.\nsk = SelfAttn(Xk) + Xk, (1)\nFe(Xk) = sk + FFN(sk), (2)\nwhere Xk ∈Rn×d and Fe(Xk) ∈Rn×d denote\nthe inputs and outputs of the k-th encoder layer,\nrespectively, and dis the hidden dimension.\nDecoder. The decoder follows a similar architec-\nture, with an additional multihead cross-attention\nsub-layer for each of Nd decoder layers.\nsk = SelfAttn(Yk) + Yk, (3)\nck = CrossAttn(sk,Fe(XNe )) + sk, (4)\nFd(Yk) = ck + FFN(ck), (5)\nwhere Yk ∈Rm×d and Fd(Yk) ∈Rm×d repre-\nsent the inputs and outputs of k-th decoder layer.\nHere, we omit layer norms among sub-layers for\nsimplicity. We take the bundle of cascading sub-\ncomponents (i.e., attention modules and FFN) as\na unit, and refer to the original Transformer and\nits variants with such cascade units as Single-Unit\nTransformer. For ease of reading, we refer to a\nsingle unit of encoder and decoder as Fe and Fd\nin the following sections.\nThen, the probability P(Y|X) is produced with\nanother Softmax layer on top of decoder outputs,\nP(Y|X) = Softmax(Ws ·Fd(YNd ) + b), (6)\nwhere Ws ∈Rd×|V|and b∈R|V|are learnable pa-\nrameters, and |V|denotes the size of target vocab-\nulary. Then, a cross-entropy objective is computed\nby,\nLCE =\n∑\nt∈(1,m)\nYtlog P(Yt|X), (7)\nwhere trepresents the t-th step for decoding phase.\n3 Model Layout\n3.1 MUTE\nIn this section, we brieﬂy describe the Multi-Unit\nTransformer (MUTE), shown in Figure 1(a). As\nmentioned before, we take the bundle of cascading\nsub-components (i.e., attention modules and FFN)\nas a single unit. By combining units in parallel, we\nhave a basic Multi-Unit Transformer (MUTE) layer.\n1049\nMulti-Head \nAttention …..\nFFN\nMulti-Head \nAttention\nFFN\nLayer output\nLayer Input\nMulti-Head \nAttention …..\nFFN\nMulti-Head \nAttention\nFFN\nLayer output\nLayer Input\nSwapping Disorder Multi-Head \nAttention …..\nFFN\nMulti-Head \nAttention\nFFN\nLayer output\nLayer Input\nSwapping Disorder\nAutoShuﬄe AutoShuﬄe\n…..\n…..\nCausal Causal…..\n( a ) Multi-Unit ( b ) Biased Multi-Unit ( c ) Sequentially Biased Multi-Unit\nFigure 1: Layer architecture for Multi-Unit Transformer. White arrow indicates model change from Multi-Unit to\nBiased Multi-Unit, to Sequentially Biased Multi-Unit, with dashed lines representing the newly added modules.\nThen, we follow the standard usage by stacking\nseveral MUTE layers to constitute our encoder and\ndecoder.\nIn general, the encoder and decoder of the Trans-\nformer network share a similar architecture and can\nbe improved with the same techniques. Without\nlosing generality, we take the encoder as an ex-\nample to further illustrate the MUTE. Given input\nXk of k-th layer, we feed it into I identical units\n{F1,··· ,Fi,··· ,FI}with different learnable pa-\nrameters.\nsk\ni = SelfAttni(Xk) + Xk, (8)\nFe\ni (Xk) = sk\ni + FFNi(sk\ni), (9)\nwhere idenotes the i-th unit. After collecting out-\nputs for all Iunits, we combine them by a weighted\nsum,\nFe(Xk) =\n∑\ni∈(1,I)\nαi ·Fe\ni (Xk), (10)\nwhere αi ∈R1 represents the learnable weight for\nthe i-th unit (Section 5.8) and Fe(Xk) ∈Rn×d is\nthe ﬁnal output for the k-th layer.\n3.2 Biased MUTE\nThe multi-unit setting for Transformer resembles\nthe well-known ensemble techniques in machine\nlearning ﬁelds, in that it also combines several dif-\nferent modules into one and aims for better per-\nformance. In that perspective, borrowed from the\nidea of bagging (Breiman, 1996), we propose to\nuse biased units instead of identical units, which\nresults in creating information gaps among units\nand makes them learn from each other.\nMore speciﬁcally, in training, we introduce a\nBias-Module to create biased units, as shown in\nFigure 1(b). For each layer, instead of giving the\nsame inputs Xk ∈Rn×d to all units, we transform\neach input with corresponding type of noises (e.g.,\nswap, reorder, mask), in order to force the model\nto focus on different parts of inputs:\nXk\ni = Biasi(Xk), (11)\nFe(Xk) =\n∑\ni∈(1,I)\nαi ·Fe\ni (Xk\ni ), (12)\nwhere Biasi denotes the noise function for i-th unit.\nThe noise operations 2 we investigated include,\n• Swapping, randomly swap two input embed-\ndings up to a certain range (i.e., 3).\n• Disorder, randomly permutate a subsequence\nwithin a certain length (i.e., 3).\n• Masking, randomly replace one input embed-\nding with a learnable mask embedding.\nNote that, the identity mapping (i.e., no noise) can\nbe seen as a special case of bias module and is\nincluded in our model design.\nAdditionally, to get deterministic outputs, we\ndisable the noises in the testing phase, which brings\nin the inconsistency between training and testing.\nHence, we propose a switch mechanism with a\nsample rate pβ that determines whether to enable\nthe bias module in training. This mechanism forces\nthe model to adapt to golden inputs and mitigate\nthe aforementioned inconsistency.\n2To avoid a distortion of input sequence, each operation is\nperformed only once for corresponding bias module.\n1050\n3.3 Sequentially Biased MUTE\nAlthough the bias module guides units of learning\nfrom each other by formulating such information\ngaps, it still lacks explicit complementarity model-\ning, i.e., mitigating these gaps. Here, based on the\nBiased MUTE, we propose a novel method to ex-\nplicitly introducing a deep connection among units\nby utilizing the power of order (Figure 1(c)).\nSequential Dependency. Given the outputs from\nbiased units Fe\ni (Xk\ni ), we permutate these out-\nputs by a certain ordering function p(i) (e.g., i∈\n{1,2,3,4}to p(i) ∈{4,2,3,1}),\n{Ge\ni = Fe\np(i)(Xk\np(i))|i∈(1,I)}, (13)\nwhere Ge\ni is the i-th permutated output. The imple-\nmentation of p(i) will be illustrated later.\nThen, we explicitly model the complementari-\nness among units by introducing sequential depen-\ndency. Speciﬁcally, we compute an accumulated\nsequence {ˆGe\ni|i∈(1,I)}over the permutated out-\nputs Ge\ni,\nˆGe\ni = ˆGe\ni−1 + Ge\ni, (14)\nwhere ˆGe\ni ∈ Rn×d is the i-th accumulated out-\nput, and ˆGe\n0 = 0 . Through this sequential de-\npendency, each permutated output Ge\ni learns the\nresidual of previousaccumulated outputs ˆGe\ni−1 (He\net al., 2016) and serves as a complement to previous\naccumulated outputs.\nFinally, we normalize this accumulated sequence\nto keep the output norm stable and fuse all accumu-\nlated outputs.\nFe(Xk) =\n∑\ni∈(1,I)\nαi ·\nˆGe\ni\ni . (15)\nAutoshufﬂe. Until now, we have modeled the\nsequential dependency between each of the units.\nThe only problem left is how to gather a proper\nordering of units. We propose to use the AutoShuf-\nﬂe Network (Lyu et al., 2019). Mathematically,\nshufﬂing with speciﬁc order equals to a multiplica-\ntion by a permutation matrix (i.e., every row and\ncolumn contains precisely a single 1 with 0s else-\nwhere). Nevertheless, a permutation matrix only\ncontains discrete values and can not be optimized\nby gradient descent. Therefore, we use a contin-\nuous matrix M ∈RI×I with non-negative values\nMi,j ∈(0,1),i,j ∈(1,I) to approximate the dis-\ncrete permutation matrix.\nParticularly, M is regarded as a learnable matrix\nand is used to multiply the outputs of units,\n[··· ; Fe\np(i)(Xk\np(i)); ···] = M⊤×[··· ; Fe\ni (Xk\ni ); ···],\n(16)\nwhere [·; ·] means the concatenation operation.\nTo ensure M remains an approximation for the\npermutation matrix during training, we normalize\nM after each optimization step.\nMi,j = max(Mi,j,0), (17)\nMi,j = Mi,j∑\nˆiMˆi,j\n,Mi,j = Mi,j∑\nˆj Mi,ˆj\n. (18)\nThen, we introduce a Lipschitz continuous non-\nconvex penalty, as proposed in Lyu et al. (2019) to\nguarantee M converge to a permutation matrix.\nLp =\nI∑\ni=1\n[\nI∑\nj=1\n|Mi,j|−(\nI∑\nj=1\nM2\ni,j)\n1\n2 ]\n+\nI∑\nj=1\n[\nI∑\ni=1\n|Mi,j|−(\nI∑\ni=1\nM2\ni,j)\n1\n2 ],\n(19)\nPlease refer to Appendix D for proof and other\ndetails.\nFinally, the penalty is added to cross-entropy\nloss deﬁned in equantion (7) as our ﬁnal objective,\nL= LCE + λ\n∑\nk\nLk\np, (20)\nwhere λis a hyperparameter to balance two objec-\ntives and Lk\np is the penalty for the k-th layer.\n4 Experimental Settings\nIn this section, we elaborate our experimental\nsetup on three widely-studied Neural Machine\nTranslation tasks, NIST Chinese-English (Zh-En),\nWMT’14 English-German (En-De) and WMT’18\nChinese-English.\nDatasets. For NIST Zh-En task, we use 1.25M\nsentences extracted from LDC corpora 3. To val-\nidate the performance of our model, we use the\nNIST 2006 (MT06) test set with 1664 sentences as\nour validation set. Then, the NIST 2002 (MT02),\n2003 (MT03), 2004 (MT04), 2008 (MT08) test sets\nare used as our test sets, which contain 878, 919,\n1788 and 1357 sentences, respectively.\nFor the WMT’14 En-De task, following the same\nsetting in Vaswani et al. (2017), we use 4.5M pre-\nprocessed data, which has been tokenized and split\nusing byte pair encoded (BPE) (Sennrich et al.,\n3The corpora include LDC2002E18, LDC2003E07,\nLDC2003E14, Hansards portion of LDC2004T07,\nLDC2004T08 and LDC2005T06.\n1051\nSystem #Param. Valid MT02 MT03 MT04 MT08 ∆\nExisting NMT systems\nWang et al. (2018) (Base) 95M 45.47 46.31 45.30 46.45 35.66 −\nCheng et al. (2018) (Base) 95M 45.78 45.96 45.51 46.49 36.08 −\nCheng et al. (2019) (Base) 95M 46.95 47.06 46.48 47.39 37.38 −\nBaseline NMT systems\nTransformer (Base) 95M 45.79 46.34 45.32 47.92 36.16 ref\nTransformer + Relative (Base) 95M 46.57 46.64 45.67 47.26 38.03 +0.53\nTransformer + Relative (Big) 277M 46.52 47.23 46.43 48.35 37.31 +0.86\nOur NMT systems\nMUTE (Base, 4 Units) 152M 47.23† 47.38† 46.24† 47.81† 38.48 +1.12\n+ Bias (Base, 4 Units) 152M 47.45†† 47.40†† 47.11†† 48.44†† 38.31 +1.44\n+ Bias + Seq. (Base, 4 Units) 152M 47.80†† 47.72†† 46.60†† 48.30†† 38.70 +1.52\nTable 1: Case-insensitive BLEU scores (%) of NIST Chinses-English (Zh-En) task. For all models with MUTE,\nwe use four units. #Params. means the number of learnable parameters in the model. ∆ denotes the average\nBLEU improvement over dev set and test sets, compared with the “Transformer (Base)”. Bold represents the best\nperformance. “†”: signiﬁcantly better than “Transformer + Relative (Base)” (p <0.05); “††”: signiﬁcantly better\nthan “Transformer + Relative (Base)” (p< 0.01).\n2016) with 32k merge operations and a shared vo-\ncabulary for English and German. We use new-\nstest2013 as our validation set and newstest2014\nas our test set, which contain 3000 and 3003 sen-\ntences, respectively.\nFor the WMT’18 Zh-En task, we use 18.4M\npreprocessed data, which is also tokenized and split\nusing byte pair encoded (BPE) (Sennrich et al.,\n2016). We use newstest2017 as our validation set\nand newstest2018 as our test set, which contains\n2001 and 3981 sentences, respectively.\nEvaluation. For evaluation, we train all the mod-\nels with maximum 150k/300k/300k steps for NIST\nZh-En, WMT En-De and WMT Zh-En, respec-\ntively, and we select the model which performs\nthe best on the validation set and report its per-\nformance on the test sets. We measure the case-\ninsensitive/case-sensitive BLEU scores usingmulti-\nbleu.perl 4 with the statistical signiﬁcance test\n(Koehn, 2004) 5 for NIST Zh-En and WMT’14\nEn-De, respectively. For WMT’18 Zh-En, we use\ncase sensitive BLEU scores calculated by Moses\nmteval-v13a.pl script 6 .\n4https://github.com/moses-smt/mosesde\ncoder/blob/master/scripts/generic/multi-\nbleu.perl\n5https://github.com/moses-smt/mosesde\ncoder/blob/master/scripts/analysis/boots\ntrap-hypothesis-difference-significance.\npl\n6https://github.com/moses-smt/mosesde\ncoder/blob/master/scripts/generic/mteval\n-v13a.pl\nModel and Hyper-parameters. For all our\nexperiments, we basically follow two model\nsettings illustrated in (Vaswani et al., 2017),\nnamely Transformer-Base and Transformer-Big. In\nTransformer-Base, we use 512 as hidden size, 2048\nas ﬁlter size and 8 heads in multihead attention. In\nTransformer-Big, we use 1024 as hidden size, 4096\nas ﬁlter size, and 16 heads in multihead attention.\nBesides, since noise types like swapping and\nreordering are of no effect on Transformer mod-\nels with absolute position information, the MUTE\nmodels are implemented using relative position in-\nformation (Shaw et al., 2018). In addition, we only\napply multi-unit methods to encoders, provided\nthat the encoder is more crucial to model perfor-\nmance (Wang et al., 2019a). All experiments on\nMUTE models are conducted with Transformer-\nBase setting. For the basic MUTE model, we use\nfour identity units. As for the Biased MUTE and\nSequentially Biased MUTE, we use four units in-\ncluding one identity unit, one swapping unit, one\ndisorder unit and one masking unit. The sample\nrate pβ is set to 0.85. For more implementation de-\ntails and experiments on sample rate, please refer\nto Appendix A and B.\n5 Results\nThrough experiments, we ﬁrst evaluate our model\nperformance (Section 5.1 and 5.2). Then, we an-\nalyze how each part of our model works (Section\n5.4 and 5.6). Finally, we conduct experiments to\nfurther understand the behavior of our models (Sec-\n1052\nSystem #Param.BLEU ∆\nExisting NMT systems\nVaswani et al. (2017) (Base) 81M27.3 ref.\nBapna et al. (2018) (Base, 16L) 137M28.0 -\nWang et al. (2019a) (Base, 30L)137M29.3 -\nVaswani et al. (2017) (Big) 251M28.4 -\nChen et al. (2018) (Big) 379M28.5 -\nHe et al. (2018) (Big) *251M29.0 -\nShaw et al. (2018) (Big) *251M29.2 -\nOtt et al. (2018) (Big) 251M29.3 -\nExisting Multi-Unit Style NMT Systems\nAhmed et al. (2017) (Base) *81M28.4 -\nLi et al. (2019) (Base) 118M28.5 -\nLi et al. (2018) (Base) *81M28.5 -\nBaseline NMT systems\nTransformer (Base) 81M27.4 ref.\nTransformer + Relative (Base) 81M28.2 +0.8\nTransformer + Relative (Big) 251M28.6 +1.2\nOur NMT systems\nMUTE (Base, 4 Units) 130M28.8†† +1.4\n+ Bias (Base, 4 Units) 130M29.1†† +1.7\n+ Bias + Seq. (Base, 4 Units) 130M29.3†† +1.9\nTable 2: Case-sensitive BLEU scores (%) of WMT’14\nEnglish-German (En-De) task. ∆ denotes the improve-\nment over newstest2014, compared with Transformer\n(Base). * denotes an estimated value. “ ††”: sig-\nniﬁcantly better than “Transformer + Relative (Base)”\n(p< 0.01).\ntion 5.7 and 5.8).\n5.1 Results on NIST Chinses-English\nAs shown in Table 1, we list the performance of our\nre-implemented Transformer baselines and our ap-\nproaches. We also list several existing strong NMT\nsystems reported in previous work to validate the ef-\nfectiveness of our models. By investigating results\nin Table 1, we have the following observations.\nFirst, compared with existing NMT systems, our\nre-implemented Transformers are strong baselines.\nSecond, all of our approaches substantially out-\nperform our baselines, with improvement ranging\nfrom 1.12 to 1.52 BLEU points. Comparing our\nmethods to “Transformer + Relative (Base)”, our\nbest approach (i.e., “MUTE + Bias + Seq. (Base, 4\nUnits)”) still achieves a signiﬁcant improvement of\n1.0 BLEU points on multiple test sets.\nThird, among our approaches, we ﬁnd that, even\nthough our basic MUTE model has already sur-\npassed existing strong NMT systems and our base-\nlines, the bias module and sequential dependency\ncan further boost the performance (i.e., from +1.12\nto +1.52), which demonstrates that introducing\ncomplementariness does help the Multi-Unit Trans-\nformers.\nFourth, we ﬁnd it interesting that compared\nwith the “Transformer + Relative (Big)”, the basic\n“MUTE (Base, 4 Units)” can achieve better BLEU\nperformance with only 54% of parameters, which\nindicates that our multi-unit approaches can lever-\nage parameters more effectively and efﬁciently.\n5.2 Results on WMT’14 English-German\nThe results on WMT’14 En-De are shown in Ta-\nble 2. We list several competitive NMT systems\nfor comparison, which are divided into models\nbased on Transformer-Base and models based on\nTransformer-Big.\nFirst of all, our models show signiﬁcant\nBLEU improvements over two baselines in the\nTransformer-Base setting, ranging from +1.4 to\n+1.9 for “Transformer (Base)” and from +0.6 to\n+1.1 for “Transformer+Relative (Base)”. That\nproves our methods perform consistently across\nlanguages and are still useful in large scale datasets.\nNext, among our own NMT methods, the sequen-\ntially biased model further improves the BLEU per-\nformance over our strong Multi-Unit model (from\n28.8 to 29.3), which is consistent with our ﬁndings\nin the Zh-En7 task and further proves the power of\ncomplementariness.\nFinally, compared with the existing NMT sys-\ntems, we ﬁnd that our models achieve comparable\n/ better performance with much fewer parameters.\nThe only exception is (Wang et al., 2019a), which\nlearns a very deep (30 layers) Transformer. We\nregard these deep Transformer methods as orthogo-\nnal methods to ours, and it can be integrated with\nour MUTE models in future work. Additionally,\nwe list several systems related to our multi-unit\nsetting (“Existing Multi-Unit Style NMT Systems”),\nwith diversity modeling or features space compo-\nsition. As shown, MUTE models also outperform\nthese methods, demonstrating the superiority of our\nmethods in diverse and complementary modeling.\n5.3 Results on WMT’18 Chinese-English\nIn this section, we represent our results on WMT18\nChinese-English. The results are shown in Table\n7We ﬁnd that improvements on the larger scale WMT’14\nEn-De dataset is bigger than that on NIST Zh-En. We attribute\nthis phenomenon to the overﬁtting problem caused by the\nsmall Zh-En dataset.\n1053\nSystem #Param.BLEU ∆\nTransformer (Base) 81M 23.4 ref.\nTransformer + Relative (Base) 81M 23.8 +0.4\nMUTE + Bias + Seq. 130M 24.5 +1.1\nTable 3: Case-sensitive BLEU scores (%) of WMT’18\nChinese-English (Zh-En) task.\nMethods BLEU Score\nMUTE + Bias + Seq. 47.80\n- identity unit. 47.01 (-0.79)\n- swapping unit. 47.13 (-0.67)\n- disorder unit. 47.15 (-0.65)\n- mask unit. 47.36 (-0.44)\n- bias module 47.28 (-0.52)\n- sequential dependency 47.45 (-0.35)\nTable 4: Ablation study for BLEU scores (%) over the\nNIST Zh-En validation set.\n3. As we can see, our MUTE model still strongly\noutperforms the baseline “Transformer (Base)” and\n“Transformer+Relative (Base)” with +1.1 and +0.7\nBLEU points. Noting that WMT’18 Zh-En has a\nmuch larger dataset (18.4M), and these ﬁndings\nproves that our model perform consistently well\nwith different size of datasets.\n5.4 Ablation Study\nIn this section, we conduct the ablation study to\nverify each part of our proposed model. The results\nare shown in Table 4. From our strongest Sequen-\ntially Biased model, we remove each of the four\ndifferent units to validate which unit contributes the\nmost to the performance. Then, we remove the bias\nmodule and sequential dependency independently\nto investigate each module’s behavior.\nWe come to the following conclusions:\n(1) All units make substantial contributions to\n“MUTE + Bias + Seq.”, ranging from 0.44 to 0.79,\nproving the effectiveness of our design.\n(2) Among all units, the identity unit contributes\nmost to our performance, which is consistent with\nour intuition that the identity unit should be respon-\nsible most for complementing other biased units.\n(3) The bias module and sequential dependency\nboth contribute much to our Multi-Unit Transform-\ners. We ﬁnd it intriguing that, without the bias mod-\nule, sequential dependency only provides marginal\nimprovements. We conjecture that the complemen-\ntary effect becomes minimal with no information\ngap among units.\nMethods BLEU Score\nTransformer + Relative 44.83\n+ average last 5 saves 44.97\n+ average 5 seeds Fails\nMUTE 45.43\nMUTE + Bias + Seq. 45.82\nTable 5: Comparison with averaging checkpoints in\nterms of BLEU scores (%) over the NIST Zh-En\ndatasets. The reported BLEU scores is the average of\nall test sets.\n5.5 Comparison with Averaging Checkpoints\nAs we mentioned before, our MUTE models are in-\nspired by ensembling methods. Therefore, it is nec-\nessary to compare our model with representative\nensemble methods, e.g., averaging model check-\npoints. Here, the comparison results of our MUTE\nmodel and averaging checkpoints in NIST Zh-En\nare shown in Table 5. Since averaging checkpoints\noften leads to better generalization, we report the\naverage BLEU scores over all test sets.\nWe adopt two settings, namely averaging the\nlast several (i.e., 5) checkpoints and averaging over\nmodels initialized with different seeds. The exper-\niment with different initialization seeds fails. We\nconjecture the reason is that different seeds make\nmodels fall in different sub-optimals, and brutally\ncombining them together makes the model perform\nbadly. Then we average checkpoints over the last 5\nsaves, which gives us 44.97 BLEU points, which\nonly outperforms the best checkpoint marginally\n(+0.14 in average), and MUTE performs much bet-\nter (45.43 and 45.82 BLEU points). Speciﬁcially,\nour naive MUTE model suprasses the averaginig\ncheckpoint method, and the sequential ordering\nand bias module enable a better interaction over\ndifferent units.\n5.6 Quantitative Analysis of Model Diversity\nHere, we empirically investigate which granularity\nshould be used for better diversity among units. To\nverify the impact of multiple units compared with\nthe single unit, we evaluate three different models:\n• 4 Self. + 4 FFN , the model with four differ-\nent self-attention modules and four different\nFFNs.\n• 4 Self. + 1 FFN , the model with four self-\nattention modules and one shared FFN.\n• 1 Self. + 4 FFN , the model with one shared\nself-attention module and four different FFNs.\n1054\nMethods Att Pos.Att Sub.FFN Sub.\nMUTE + Bias + Seq.0.475 0.602 0.460\n4 Self. + 4 FFN 0.447 0.576 0.449\n4 Self. + 1 FFN 0.420 0.520 0.404\n1 Self. + 4 FFN - - 0.381\nTable 6: Quantitative analysis on diversity scores. Att\nPos., Att Sub. and FFN Sub. denote the diversity\nscores for self-attention weights, self-attention outputs\nand FFN outputs, respectively. A larger score means\nbetter diversity.\nTo control variables, these models include neither\nbias module nor sequential dependency.\nFor each model, we evaluate the diversity among\nunits for three outputs: (1) the outputs of self-\nattention modules, (2) the attention weights of self-\nattention modules, (3) the outputs of FFN modules.\nThe diversity scores are computed by the exponen-\ntial of the negative cosine distance among units, the\nsame as proposed in (Li et al., 2018).\nDIV = exp(− o⊤\ni oj\n|oi|·|oj|), (21)\nwhere DIV ∈(0,1) represents the diversity score\nfor module outputs oi ∈Rd and oj ∈Rd. The\nresults are shown in Table 6.\nAbove all, we ﬁnd that multiple FFN layers can\nintroduce diversity. As seen, “1 Self. + 4 FFN” pro-\nduces a 0.381 diversity score on “FFN Sub.”. Since\nwe use a shared self-attention layer, which brings\nno diversity in the input-side of FFN layers, the dif-\nference is only brought by different FFN modules.\nWe think the reason is that the RELU activation\ninside the FFN module serves as selective attention\nto ﬁlter out input information.\nNext, “4 Self. + 1 FFN” achieves 0.420 and\n0.520 diversity scores for self-attention modules,\nwhich indicates that multiple self-attention mod-\nules focus on different parts of the input sequence\nand lead to diverse outputs.\nThen, “4 Self. + 4 FFN” has higher diversity\nscores than “4 Self. + 1 FFN” and “1 Self. + 4\nFFN”, which veriﬁes our choice of using a combi-\nnation of self-attention module and FFN as a basic\nunit.\nFinally, concerning the diversity scores among\nall models, we ﬁnd that our full model with biased\ninputs and sequential dependency achieves the best\ndiversity scores for all three outputs, which also\nachieves the best BLEU scores in previous experi-\nments.\n1 2 3 4 5 6\nNumber of Units\n45.5\n46.0\n46.5\n47.0\n47.5\n48.0BLEU\nMulti-Unit\nTransformer-Base\n(a) Effects on BLEU\n1 2 3 4 5 6\nNumber of Units\n800\n820\n840\n860\n880\n900\n920Speed\nMulti-Unit\nTransformer-Base (b) Effects on Speed\nFigure 2: The effect on the number of units: (a) BLEU\nscores (%) against number of units used in the MUTE\nmodel. (b) Inference speed (tokens per second) on\nGPU against number of units used in the MUTE model.\nExperiments are conducted on the valid set of NIST Zh-\nEn with decoding batch size = 30 and beam size = 4.\n5.7 Effects on the Number of Units\nAnother concern is how the MUTE models perform\nwhen increasing the number of units. Thus, in this\nsection, we empirically investigate the effects on\nthe number of units, in terms of model performance\nand inference speed. Here we use the basic Multi-\nUnit Transformer8.\nAs shown in Figure 2(a) and 2(b), increasing the\nnumber of units from 1 to 6 yields consistent BLEU\nimprovement (from 46.5 to 47.5) with only mild in-\nference speed decrease (from 890 tokens/sec to 830\ntokens/sec). Besides, our model with four unit used\nin other experiments is faster than Transfomrer-\nBig (863 tokens/sec vs 838 tokens/sec). These\nresults prove the computational efﬁciency of our\nMUTE model. We attribute this mild speed de-\ncrease (about 3.1% for four units and 6.7% for six\nunits) for two reasons. First, the multi-unit model\nis naturally easy for parallelization. Each unit can\nbe computed independently without waiting for\nother functions to ﬁnish. Second, we only widen\nthe encoder, which is only computed once for each\nsentence translation.\n5.8 Visualization\nWe also present a visualization example of the\nlearnable weights αfor units, shown in Figure 3.\nAs we can see, learnable weights αshow simi-\nlar trends in “MUTE” and “MUTE + Bias”. The\nweights for each unit within the same layer fall in\na similar range (0.2 to 0.35), dispelling the worries\nthat the biased units may be omitted or skipped.\nAs for the “MUTE + Bias + Seq.”, the weight dis-\ntribution is very different. The weights nearly in-\ncrease progressively when the unit index increases.\n8The number of combinations of different noise types is\nexponentially large. Moreover, introducing sequential depen-\ndency and bias module hardly affects the inference speed.\n1055\nBecause the model weights are learned with back-\npropagation, the larger the weight, the more the\nmodel favors the corresponding unit. Thus, to some\nextent, this phenomenon demonstrates that the lat-\nter accumulated outputs are more potent than the\npreceding ones, and therefore, the latter single unit\noutput complements the previous accumulation.\n6 Related Work\nRecently Transformer-based models (Vaswani\net al., 2017; Ott et al., 2018; Wang et al., 2019a)\nbecome the de facto methods in Neural Machine\nTranslation, owing to high parallelism and large\nmodel capacity.\nSome researchers devise new modules to im-\nprove the Transformer model, including combin-\ning the transformer unit with convolution networks\n(Wu et al., 2019; Zhao et al., 2019; Lioutas and\nGuo, 2020), improving the self-attention architec-\nture(Fonollosa et al., 2019; Wang et al., 2019b; Hao\net al., 2019), and deepening the Transformer archi-\ntecture by dense connections(Wang et al., 2019a).\nSince our multi-unit framework makes no limita-\ntion about its unit, these models can be easily inte-\ngrated into our multi-unit framework.\nThere are also some works utilizing the power\nof multiple modules to capture complex feature\nrepresentations in NMT. Shazeer et al. (2017) use\na vast network and a sparse gated function to se-\nlect from multiple experts (i.e., MLPs). Ahmed\net al. (2017) train a weighted Transformer by re-\nplacing the multi-head attention by self-attention\nbranches. Nevertheless, these models ignore the\nmodeling of relations among different modules.\nThen, some multihead attention variants (Li et al.,\n2018, 2019) introduce modeling of diversity or in-\nteraction among heads. However, complementari-\nness is not taken into account in their approaches.\nOur MUTE models differ from their methods in\ntwo aspects. First, we use a powerful unit with a\nstrong performance in diversity (Section 5.6). Sec-\nond, we explicitly model the complementariness\nwith bias module and sequential dependency.\n7 Conclusion\nIn this paper, we propose Multi-Unit Transformers\nfor NMT to improve the expressiveness by intro-\nducing diverse and complementary units. In addi-\ntion, we propose two novel techniques, namely bias\nmodule and sequential dependency to further im-\nprove the diversity and complementariness among\n1 2 3 4\nIndex of Units\n654321\nIndex of Layers\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n(a) MUTE\n1 2 3 4\nIndex of Units\n654321\nIndex of Layers\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325\n0.350\n (b) MUTE + Bias\n1 2 3 4\nIndex of Units (Accumulated)\n654321\nIndex of Layers\n0.0\n0.2\n0.4\n0.6\n0.8\n(c) MUTE + Bias + Seq.\nFigure 3: Visualization of our models’ learnable\nweights for units in each layer.\nunits. Experimental results show that our methods\ncan signiﬁcantly outperform the baseline methods\nand achieve comparable / better performance com-\npared with existing strong NMT systems. In the\nmeantime, our methods use much fewer parameters\nand only introduce mild inference speed degrada-\ntion, which proves the efﬁciency of our models.\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer net-\nwork for machine translation. arXiv preprint\narXiv:1711.02132.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nAnkur Bapna, Mia Xu Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural ma-\nchine translation models with transparent attention.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3028–3033.\nLeo Breiman. 1996. Bagging predictors. Machine\nlearning, 24(2):123–140.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\net al. 2018. The best of both worlds: Combining\nrecent advances in neural machine translation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 76–86.\n1056\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333.\nYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie\nZhai, and Yang Liu. 2018. Towards robust neural\nmachine translation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1756–\n1766.\nJos´e AR Fonollosa, Noe Casas, and Marta R\nCosta-juss`a. 2019. Joint source-target self at-\ntention with locality constraints. arXiv preprint\narXiv:1905.06596.\nJerome H Friedman. 2001. Greedy function approx-\nimation: a gradient boosting machine. Annals of\nstatistics, pages 1189–1232.\nJie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang,\nand Zhaopeng Tu. 2019. Multi-granularity self-\nattention for neural machine translation. arXiv\npreprint arXiv:1909.02222.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7944–7954.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander M. Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proc. ACL.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing, pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nJian Li, Zhaopeng Tu, Baosong Yang, Michael R Lyu,\nand Tong Zhang. 2018. Multi-head attention with\ndisagreement regularization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2897–2903.\nJian Li, Xing Wang, Baosong Yang, Shuming Shi,\nMichael R Lyu, and Zhaopeng Tu. 2019. Neu-\nron interaction based representation composition\nfor neural machine translation. arXiv preprint\narXiv:1911.09877.\nVasileios Lioutas and Yuhong Guo. 2020. Time-\naware large kernel convolutions. arXiv preprint\narXiv:2002.03184.\nJiancheng Lyu, Shuai Zhang, Yingyong Qi, and Jack\nXin. 2019. Autoshufﬂenet: Learning permutation\nmatrices via an exact lipschitz continuous penalty in\ndeep convolutional neural networks. arXiv preprint\narXiv:1901.08624.\nFandong Meng, Jinchao Zhang, Yang Liu, and Jie\nZhou. 2019. Multi-zone unit for recurrent neural net-\nworks. arXiv preprint arXiv:1911.07184.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nChongyang Tao, Shen Gao, Mingyue Shang, Wei Wu,\nDongyan Zhao, and Rui Yan. Get the point of my\nutterance! learning towards effective responses with\nmulti-head attention mechanism.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F Wong, and Lidia S Chao.\n2019a. Learning deep transformer models for ma-\nchine translation. arXiv preprint arXiv:1906.01787.\nXing Wang, Zhaopeng Tu, Longyue Wang, and\nShuming Shi. 2019b. Self-attention with struc-\ntural position representations. arXiv preprint\narXiv:1909.00383.\n1057\nXinyi Wang, Hieu Pham, Zihang Dai, and Graham\nNeubig. 2018. Switchout: an efﬁcient data aug-\nmentation algorithm for neural machine translation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n856–861.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan\nZhang, and Liangchen Luo. 2019. Muse: Parallel\nmulti-scale attention for sequence to sequence learn-\ning. arXiv preprint arXiv:1911.09483.\n1058\nA Implementation Details\nAll models are trained using Opennmt-py frame-\nwork (Klein et al., 2017). The batch size for each\nGPU is set to 4096 tokens. The beam size is set to 4,\nand the length penalty is 0.6 among all experiments.\nFor the WMT’14 En-De and WMT’18 Zh-En task,\nall experiments are conducted using 4 NVIDIA\nTesla V100 GPUs, while we use 2 GPUs for the\nNIST Zh-En task. That gives us about 8k/16k/16k\ntokens per update for NIST Zh-En/WMT’14 En-\nDe/WMT’18 Zh-En. All models are optimized\nusing Adam (Kingma and Ba, 2014) with β1 = 0.9\nand β2 = 0 .998, and learning rate is set to 1.0\nfor all experiments. Label smoothing is set to 0.1\nfor all three tasks. We use dropout of 0.4/0.2/0.1\nfor NIST Zh-en/WMT En-de/WMT Zh-en, respec-\ntively. All our Transformer models contain 6 en-\ncoder layers and 6 decoder layers, following the\nstandard setting in (Vaswani et al., 2017).\nAll the hyperparameters are empirically set\nby our previous experiences or manually tuned.\nThe criterion for selecting hyperparameters is the\nBLEU score on validation sets for both tasks. The\naverage runtimes are one GPU day for NIST Zh-En\nand 4 GPU days for WMT’14 En-De and WMT’18\nZh-EN.\nB Effect of Sample Rate\nAs we mentioned above, we use sample rate to mit-\nigate the incosistency between training and testing,\nand force the model to adapt to golden inputs. Here,\nwe conduct hyper-parameter search for sample rate\nwith 4 empirically set values, 0.5, 0.75, 0.85, 1.0.\nThe results are shown in Figure 4.\nAs seen, we make the following observations:\n1. The trends of BLEU scores for sequentially\nbiased model and biased model are very similar,\nwhen we increases the sample rate pβ. The best\nperformance for both models appear when the sam-\nple rate is 0.85. Therefore, we set sample rate to\n0.85 among all of our experiments. 2. Introducing\nsample rate is essential for both biased model and\nsequentially biased model. Compared with alway\ninjecting noises, i.e., the sample ratepβ = 1.0, con-\nsistent improvements is observed for both models.\nBLEU scores increase from 47.27 to 47.48 (+0.21)\nfor biased model, and from 47.56 to 47.80 (+0.24)\nfor sequentially biased model.\nThe aforementioned observations prove that in-\ntroducing sample rate when using bias module is an\nessential way to mitigate the inconsistency between\n0.5 0.6 0.7 0.8 0.9 1.0\nSample Rate\n46.0\n46.5\n47.0\n47.5\n48.0\n48.5BLEU\nSequential Biased Multi-Unit\nBiased Multi-Unit\nFigure 4: BLEU scores (%) changes on NIST Zh-En\nvalid set when increasing the sample rate pβ. Bigger\nvalue means a better chance to inject noises. 1.0 for\nsample rate means always injecting noises.\ntraining and testing.\nC Case Study\nWe also provide a visualization example to show\nhow our proposed methods improve complemen-\ntariness among units. As shown in Figure 5, the\ntop one is the attention weights of Basic Multi-Unit\nmodel. The attention weights for each of the units\nmainly focus on same parts of the inputs. In con-\ntrast to this phenomenon, the biased model and\nsequentially biased model attend with more diverse\nweights. Moreover, we ﬁnd that the sequentially\nbiased model pays more attention to what the pre-\nvious unit miss, which proves that our proposed\nmethod can actually encourage complementariness.\nD Simple Proof for Lipschitz Penalty\nHere we give a intuitive proof why the Lipschitz\npenalty (Lyu et al., 2019) would lead our learnable\nmatrix M towards a permutation matrix, i.e., every\nrow and column contains precisely a single 1 with\n0s elsewhere.\nRecall the penalty for matrix M,\nLp =\nI∑\ni=1\n[\nI∑\nj=1\n|Mi,j|−(\nI∑\nj=1\nM2\ni,j)\n1\n2 ]\n+\nI∑\nj=1\n[\nI∑\ni=1\n|Mi,j|−(\nI∑\ni=1\nM2\ni,j)\n1\n2 ].\n(22)\nBy the Cauchy-Schwarz inequality, we have\n(\nI∑\nj=1\n|Mi,j|) −(\nI∑\nj=1\nM2\ni,j)\n1\n2 ≥0,∀i, (23)\n1059\n12341234\n<\nbàogào\n>\nxi nshì\n,\n2005nián\n3yuè\n24rì\n,\nzh ngguó\ndì\n21\ncì\nnánjí\nk ochá@@\nduì\nyuánm n\nwánchéng\nrènwù\nf nhuí\nshàngh i\n,\nc\ncì\nk ochá\nlìshí\n15@@\n1\ntián\n,\nhángchéng\n26@@\n500\nh il\n.\n1234\n0.0\n0.2\n0.0\n0.2\n0.4\n0.0\n0.2\n0.4\n0.0 0.2 0.4 0.6 0.8 1.00.0\n0.2\n0.4\n0.6\n0.8\n1.0Index of Unit\nFigure 5: A visualization of example attention weights of the Multi-Unit, Biased Multi-Unit and Sequentially\nBiased Multi-Unitmodels, top to bottom. We ﬁlter out the weights smaller than 0.05 for ease of understanding.\nwith the equality holds if and only if there exists\nmaximum one 1 for each row,\n|{j : Mi,j ̸= 0}<= 1|,∀i. (24)\nThen, in conjunction with our normalization that\nperserves,\nMi,j ≥0,∀i,j; (25)\nI∑\nj=1\nMi,j = 1,∀i;\nI∑\ni=1\nMi,j = 1,∀j, (26)\nthe equality only holds if and only if there exists\nonly one 1 for each row,\n|{j : Mi,j ̸= 0}= 1|,∀i. (27)\nLikewise,\n(\nI∑\ni=1\n|Mi,j|) −(\nI∑\ni=1\nM2\ni,j)\n1\n2 ≥0,∀j, (28)\nwith equality if and only if |{j : Mi,j ̸= 0 }=\n1|,∀i. Therefore, when the penalty (22) becomes\n0, M converges to a permutaion matrix. For more\ndetailed mathematical proof about the Lipschitz\npenalty, we refer readers to Lyu et al. (2019).",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8361040353775024
    },
    {
      "name": "Transformer",
      "score": 0.7885274887084961
    },
    {
      "name": "Computer science",
      "score": 0.7707607746124268
    },
    {
      "name": "Inference",
      "score": 0.6034860014915466
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5523347854614258
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.49067485332489014
    },
    {
      "name": "Cascade",
      "score": 0.4427075684070587
    },
    {
      "name": "Machine learning",
      "score": 0.3815244436264038
    },
    {
      "name": "Natural language processing",
      "score": 0.3656524419784546
    },
    {
      "name": "Computer engineering",
      "score": 0.3258627653121948
    },
    {
      "name": "Voltage",
      "score": 0.17939335107803345
    },
    {
      "name": "Engineering",
      "score": 0.10769811272621155
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 21
}