{
  "title": "Building Probabilistic Models for Natural Language",
  "url": "https://openalex.org/W1512277306",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Chen, Stanley F.",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W177072266",
    "https://openalex.org/W2439178139",
    "https://openalex.org/W2002221802",
    "https://openalex.org/W1541301615",
    "https://openalex.org/W1825362480",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W1982944197",
    "https://openalex.org/W2079145130",
    "https://openalex.org/W1970961429",
    "https://openalex.org/W1582219060",
    "https://openalex.org/W2137638032",
    "https://openalex.org/W1972099155",
    "https://openalex.org/W2028770325",
    "https://openalex.org/W1573416751",
    "https://openalex.org/W2137194186",
    "https://openalex.org/W2341171179",
    "https://openalex.org/W2060108852",
    "https://openalex.org/W2138584836",
    "https://openalex.org/W2117652747",
    "https://openalex.org/W2083398114",
    "https://openalex.org/W1489181569",
    "https://openalex.org/W26104448",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W1536631629",
    "https://openalex.org/W2047706513",
    "https://openalex.org/W2144679335",
    "https://openalex.org/W2170120409",
    "https://openalex.org/W1974094162",
    "https://openalex.org/W1543107604",
    "https://openalex.org/W1978470410",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W1519253855",
    "https://openalex.org/W2054658115",
    "https://openalex.org/W2126163471",
    "https://openalex.org/W2138389163",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W2063918473",
    "https://openalex.org/W103303497",
    "https://openalex.org/W2084084380",
    "https://openalex.org/W2061079066",
    "https://openalex.org/W2059800182",
    "https://openalex.org/W2611071497",
    "https://openalex.org/W2152810530",
    "https://openalex.org/W2135625884",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2168929382",
    "https://openalex.org/W2046419776",
    "https://openalex.org/W2110190189",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W2078950386",
    "https://openalex.org/W2061271742",
    "https://openalex.org/W2953145006",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2022597264",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2159782014",
    "https://openalex.org/W2056933273",
    "https://openalex.org/W1575431606",
    "https://openalex.org/W1601728146",
    "https://openalex.org/W1638203394",
    "https://openalex.org/W2129139611",
    "https://openalex.org/W2005097301",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1987409251",
    "https://openalex.org/W1924403233"
  ],
  "abstract": "In this thesis, we investigate three problems involving the probabilistic modeling of language: smoothing n-gram models, statistical grammar induction, and bilingual sentence alignment. These three problems employ models at three different levels of language; they involve word-based, constituent-based, and sentence-based models, respectively. We describe techniques for improving the modeling of language at each of these levels, and surpass the performance of existing algorithms for each problem. We approach the three problems using three different frameworks. We relate each of these frameworks to the Bayesian paradigm, and show why each framework used was appropriate for the given problem. Finally, we show how our research addresses two central issues in probabilistic modeling: the sparse data problem and the problem of inducing hidden structure.",
  "full_text": "arXiv:cmp-lg/9606014v1  11 Jun 1996\nBuilding Probabilistic Models for Natural\nLanguage\nA thesis presented\nby\nStanley F. Chen\nto\nThe Division of Applied Sciences\nin partial fulﬁllment of the requirements\nfor the degree of\nDoctor of Philosophy\nin the subject of\nComputer Science\nHarvard University\nCambridge, Massachusetts\nMay 1996\nc⃝ 1996 by Stanley F. Chen\nAll rights reserved.\nii\nAbstract\nBuilding models of language is a central task in natural lang uage processing. Tradition-\nally, language has been modeled with manually-constructed grammars that describe which\nstrings are grammatical and which are not; however, with the recent availability of massive\namounts of on-line text, statistically-trained models are an attractive alternative. These\nmodels are generally probabilistic, yielding a score reﬂec ting sentence frequency instead of\na binary grammaticality judgement. Probabilistic models o f language are a fundamental\ntool in speech recognition for resolving acoustically ambi guous utterances. For example,\nwe prefer the transcription forbear to four bear as the former string is far more frequent\nin English text. Probabilistic models also have applicatio n in optical character recognition,\nhandwriting recognition, spelling correction, part-of-s peech tagging, and machine transla-\ntion.\nIn this thesis, we investigate three problems involving the probabilistic modeling of lan-\nguage: smoothing n-gram models, statistical grammar induction, and bilingua l sentence\nalignment. These three problems employ models at three diﬀer ent levels of language; they\ninvolve word-based, constituent-based, and sentence-bas ed models, respectively. We de-\nscribe techniques for improving the modeling of language at each of these levels, and surpass\nthe performance of existing algorithms for each problem. We approach the three problems\nusing three diﬀerent frameworks. We relate each of these fram eworks to the Bayesian\nparadigm, and show why each framework used was appropriate f or the given problem. Fi-\nnally, we show how our research addresses two central issues in probabilistic modeling: the\nsparse data problem and the problem of inducing hidden struc ture.\niii\nAcknowledgements\nI didn’t realize graduate school was going to be this tough. I had a bad case of hubris coming\nin, and it took me about six years to get it under control enoug h for me to graduate. It\ncould have taken longer, but fortunately my advisor Stuart S hieber was there to yell at me.\nI think the turning point was the beginning of my seventh year when Stuart said, “You\nknow this is your last year, don’t you?”\nI want to thank Stuart for giving me the freedom to try my crazy ideas even though in\nhindsight they were pretty stupid, for helping me ﬁnally lea rn how to do research correctly,\nfor teaching me how to write, and for teaching me the importan ce of coming up with a\n“story.” I publicly apologize to Stuart for not having liste ned to him earlier in my graduate\ncareer: Yes, Professor Shieber, you were right all along. Fi nally, I want to thank Stuart for\nthe vaunted “full meal” paradigm of thesis writing, and the q uote “Thirty-ﬁve cents and\nthe big picture will get you a cup of coﬀee in Harvard Square.”\nNext, I would like to thank Barbara Grosz for being my advisor my ﬁrst two years and\nfor general support for the rest of them. Again, I appreciate the freedom that Barbara gave\nme to pursue my interests, and I don’t hold a grudge for receiv ing the fourth-lowest grade\nin CS 280 my year because I now realize my ﬁnal paper was crap. ( At the time, I thought\nit was pretty deep.) I would also like to thank the rest of my co mmittee, Leslie Valiant and\nDavid Mumford, for their feedback on my research and on my the sis, and for allowing me\nto graduate.\nMy summers at the IBM T.J. Watson Research Center were extrem ely valuable to my\ngraduate career. Thanks to Peter Brown, Stephen DellaPietr a, Vincent DellaPietra, and\nRobert Mercer for teaching me all I know about statistical na tural language processing, and\nfor getting me that fellowship. Thanks to Peter Brown for bei ng the best manager I’ve ever\nhad. Thanks to the rest of the IBM crew for making it a blast: Ad am Berger, Eric Dunn,\nJohn Gillett (for quarters), Meredith Goldsmith, Jan Hajic , Josh Koppelman (for nothing),\nRay Lau (for rmon), David Magerman, Adwait Ratnaparkhi, Philip Resnik, Jeﬀ R eynar,\nMike Schultz, and everybody else.\nI would like to thank my undergraduate advisor, Fred Thompso n, for helping me get\nstarted on this whole artiﬁcial intelligence thing. He taug ht me how to be cynical, and was\nthe ﬁrst to try to cure my hubris: he gave me a C in Artiﬁcial Int elligence, but apparently\nI just didn’t get it.\nWithout my Caltech friends, life would not have been nearly s o bearable. Thanks to\nRay Sidney and Satomi Okazaki for inviting me over for dinner all the time and for the\nR. incident, and thanks to Meera’s brother Bugoo for making s uch a fool of Ray. Thanks\nto Donald Finnell for being so easy to wail on, to Oscar Dur` an for being Guatamalan, to\nTracy Fu for having the edge, to Chris Tully and Paul Rubinov f or being hairy, and to Jared\nBronski for the C. thing.\niv\nThanks also go to my fellow graduate students for making the d epartment a great place\nto be: Ellie Baker, Vanja Buvac, Rebecca Hwa, Lillian Lee (fo r messing with my mind),\nKaren Lochbaum, Christine Nakatani, Ted Nesson, Wheeler Ru ml (for being Wheeler),\nKathy Ryall (for not washing pots), and Nadia Shalaby (for ch oosing where to eat). Thanks\nto Andrew Kehler, the Golden Boy of NLP, for being my oﬃcemate and for exposing me\nto fajitas, beer, “Damn!”, and “What’s up with that?”. Thank s to Joshua Goodman for\nteaching me all I know about women, not!, for pretending he kn ows everything about\neverything, and for his theory of natural selection through the seat-belt law.\nOf course, no acknowledgements could be complete without me ntioning my idol, Jon\nChristensen, better known as Slacker Boy. Thanks for the cou ch and the best naps I’ve had\nin grad school, for being so very charming and pretending to h ave integrity, for teaching me\nabout the “I’d love to, but . . . ” conversational macro, and fo r revolutionizing the harem\nsystem through the “Miss Tuesday Night” concept. Thanks als o go to Jon for giving me\nthe chance to whip him in arm wrestling even though he outweig hs me by over ten pounds.\nLast but not least, thanks to Jon for having given me the honor of knowing him; these\nmoments I will treasure forever.\nI would like to thank my family: my sisters for giving me perso nal advice which never\nturned out to be any good, and my parents for their support and love. Like Stuart, they\nwere right all along, and like Stuart, I didn’t listen.\nI am grateful for the ﬁnancial support that I have received ov er these years. My research\nwas supported in part by a National Science Foundation Gradu ate Student Fellowship,\nan IBM Graduate Student Fellowship, US West grant cs1412120 20, the letter Q and the\nnumber 5, and National Science Foundation grants IRI-91-57 996, IRI-93-50192, and CDA-\n94-01024 along with matching grants from the Digital Equipm ent Corporation and the\nXerox Corporation.\nFinally, I would like to thank myself. If it weren’t for me, I d on’t think I would have\nmade it. I want to thank myself especially for writing the label program for automatically\nplacing labels next to lines in graphs made by gnuplot, without which many of the graphs\nin this thesis would not have been possible. It uses neural ne ts and fuzzy logic.\nInspirational Quotes\n“ ‘... no one knows what’s in your heart, only you, your heart a nd your brain,\nthat’s all we have to battle Time...’ ” — Marathon Man , William Goldman\n“... Babe replied, ‘It hurts too much, I’m burning up inside, ’ and that made\nNurmi angry, ‘Of course you’re burning up inside, you’re sup posed to burn up\ninside, and you keep going, you burst through the pain barrie r...’ ” — Marathon\nMan, William Goldman\n“Don’t hit the wall. Run through it.” — Gatorade advertiseme nt\n“Test your faith daily.” — Nike advertisement\n“... but for my own quite possibly perverse reasons I prefer t hose scientists who\ndrive toward daunting goals with nerves steeled against fai lure and a readiness\nto accept pain, as much to test their own character as to parti cipate in the\nscientiﬁc culture.” — Naturalist, Edward O. Wilson\n“Be your own pig.” — anonymous\nv\nContents\n1 Introduction 2\n1.1 Models of Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Applications for Probabilistic Models . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Problem Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.4 Bayesian Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.5 Sparse Data and Inducing Hidden Structure . . . . . . . . . . . . . . . . . . 10\n2 Smoothing n-Gram Models 11\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2 Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2.1 Additive Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.2.2 Good-Turing Estimate . . . . . . . . . . . . . . . . . . . . . . . . . . 1 6\n2.2.3 Jelinek-Mercer Smoothing . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2.4 Katz Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.2.5 Church-Gale Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . 2 1\n2.2.6 Bayesian Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3 Novel Smoothing Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.1 Method average-count . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.3.2 Method one-count . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.4 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.4.1 Smoothing Implementations . . . . . . . . . . . . . . . . . . . . . . . 25\n2.4.2 Implementation Architecture . . . . . . . . . . . . . . . . . . . . . . 30\n2.4.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.4.4 Parameter Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.5.1 Overall Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.5.2 Count-by-Count Analysis . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.5.3 Accuracy of the Good-Turing Estimate for Zero Counts . . . . . . . 45\n2.5.4 Church-Gale Smoothing versus Linear Interpolation . . . . . . . . . 45\n2.5.5 Held-out versus Deleted Interpolation . . . . . . . . . . . . . . . . . 47\n2.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 8\n2.6.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3 Bayesian Grammar Induction for Language Modeling 52\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.1.1 Probabilistic Context-Free Grammars . . . . . . . . . . . . . . . . . 52\n3.1.2 Probabilistic Context-Free Grammars and n-Gram Models . . . . . 55\nvi\n3.2 Grammar Induction as Search . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.2.1 Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n3.2.2 Description Lengths . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2\n3.2.3 The Minimum Description Length Principle . . . . . . . . . . . . . . 65\n3.3 Algorithm Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n3.3.1 Evaluating the Objective Function . . . . . . . . . . . . . . . . . . . 70\n3.3.2 Parameter Training . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n3.3.3 Constraining Moves . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.3.4 Post-Pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.3.5 Algorithm Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n3.4 Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n3.4.1 Bayesian Grammar Induction . . . . . . . . . . . . . . . . . . . . . . 74\n3.4.2 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n3.5 Algorithm Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n3.5.1 Grammar Speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . 7 8\n3.5.2 Move Set and Triggers . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n3.5.3 Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n3.5.4 Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n3.5.5 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n3.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 06\n3.7.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n4 Aligning Sentences in Bilingual Text 108\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n4.1.1 Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n4.1.2 Algorithm Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1\n4.2 The Alignment Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 12\n4.2.1 The Alignment Framework . . . . . . . . . . . . . . . . . . . . . . . 11 2\n4.2.2 The Basic Translation Model . . . . . . . . . . . . . . . . . . . . . . 115\n4.2.3 The Complete Translation Model . . . . . . . . . . . . . . . . . . . . 117\n4.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n4.3.1 Evaluating the Probability of a Sentence Bead . . . . . . . . . . . . 119\n4.3.2 Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 0\n4.3.3 Parameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 23\n4.3.4 Parameter Estimation Framework . . . . . . . . . . . . . . . . . . . 124\n4.3.5 Parameter Estimation Details . . . . . . . . . . . . . . . . . . . . . . 125\n4.3.6 Cognates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n4.3.7 Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n4.3.8 Deletion Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n4.3.9 Subdividing a Corpus for Parallelization . . . . . . . . . . . . . . . . 130\n4.3.10 Algorithm Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 0\n4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2\n4.4.1 Lexical Correspondences . . . . . . . . . . . . . . . . . . . . . . . . . 133\n4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 34\nvii\n5 Conclusion 136\n5.1 Bayesian Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n5.1.1 Smoothing n-Gram Models . . . . . . . . . . . . . . . . . . . . . . . 138\n5.1.2 Bayesian Grammar Induction . . . . . . . . . . . . . . . . . . . . . . 139\n5.1.3 Bilingual Sentence Alignment . . . . . . . . . . . . . . . . . . . . . . 140\n5.2 Sparse Data and Inducing Hidden Structure . . . . . . . . . . . . . . . . . . 141\n5.2.1 Sparse Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n5.2.2 Inducing Hidden Structure . . . . . . . . . . . . . . . . . . . . . . . 142\nA Sample of Lexical Correspondences Acquired During Bilingual Sentence\nAlignment 144\nA.1 Poor Correspondences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nviii\nList of Figures\n1.1 Parse tree for Max threw the ball beyond the fence . . . . . . . . . . . . . . . 4\n2.1 λ values for old and new bucketing schemes for Jelinek-Mercer smoothing . 23\n2.2 Outline of our Church-Gale trigram implementation . . . . . . . . . . . . . 29\n2.3 Performance relative to baseline method of katz and new-avg-count with\nrespect to parameters δ and cmin, respectively, over several training set sizes 32\n2.4 Eﬀect of kn on Katz smoothing . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.5 Eﬀect of cmin and cmb on Church-Gale smoothing . . . . . . . . . . . . . . . 34\n2.6 Baseline cross-entropy on test data . . . . . . . . . . . . . . . . . . . . . . . 36\n2.7 Trigram model on TIPSTER data; relative performance of v arious methods\nwith respect to baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 6\n2.8 Bigram model on TIPSTER data; relative performance of va rious methods\nwith respect to baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 7\n2.9 Bigram and trigram models on Brown corpus; relative perf ormance of various\nmethods with respect to baseline . . . . . . . . . . . . . . . . . . . . . . . . 37\n2.10 Bigram and trigram models on Wall Street Journal corpus ; relative perfor-\nmance of various methods with respect to baseline . . . . . . . . . . . . . . 38\n2.11 Average corrected counts for bigram and trigram models , 1M words training\ndata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.12 Average corrected counts for bigram and trigram models , 200M words train-\ning data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.13 Expected over actual counts for various algorithms, bi gram and trigram mod-\nels, 1M words training data . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1\n2.14 Expected over actual counts for various algorithms, bi gram and trigram mod-\nels, 200M words training data . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.15 Relative performance at each count for various algorit hms, bigram and tri-\ngram models, 1M words training data . . . . . . . . . . . . . . . . . . . . . 42\n2.16 Relative performance at each count for various algorit hms, bigram and tri-\ngram models, 200M words training data . . . . . . . . . . . . . . . . . . . . 42\n2.17 Fraction of entropy devoted to various counts over many training sizes, base-\nline smoothing, bigram and trigram models . . . . . . . . . . . . . . . . . . 43\n2.18 Average count assigned to n-grams with zero count for various n1 and N,\nactual, bigram and trigram models . . . . . . . . . . . . . . . . . . . . . . . 44\n2.19 Average count assigned to n-grams with zero count for various n1 and N,\npredicted by Good-Turing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 4\n2.20 Corrected count assigned to zero counts by Church-Gale for all buckets, bi-\ngram and trigram models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nix\n2.21 Corrected count assigned to various counts by Church-G ale for all buckets,\nbigram and trigram models . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n2.22 Held-out versus deleted interpolation on TIPSTER data , relative perfor-\nmance with respect to baseline, bigram and trigram models . . . . . . . . . 47\n3.1 Parse tree for a cat hit the tree . . . . . . . . . . . . . . . . . . . . . . . . . 54\n3.2 Parse of the dog barks using a bigram-equivalent grammar . . . . . . . . . . 55\n3.3 Initial Viterbi parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n3.4 Predicted Viterbi parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n3.5 Outline of search algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n3.6 Example class hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n3.7 Triggering concatenation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n3.8 After concatenation/triggering classing . . . . . . . . . . . . . . . . . . . . . 81\n3.9 After classing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n3.10 Triggering and applying the repetition move . . . . . . . . . . . . . . . . . . 83\n3.11 Without smoothing rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n3.12 With smoothing rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n3.13 ǫ-smoothing rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n3.14 After smoothing triggering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n3.15 Before and after specialization . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n3.16 Before and after repetition specialization . . . . . . . . . . . . . . . . . . . . 87\n3.17 Smoothing rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n3.18 Before and after specialization . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n3.19 Typical parse-tree structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.20 Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n3.21 Before and after concatenation . . . . . . . . . . . . . . . . . . . . . . . . . 98\n3.22 Sample grammar used to generate data . . . . . . . . . . . . . . . . . . . . 100\n3.23 Performance versus model size, English-like artiﬁcia l grammar . . . . . . . . 101\n3.24 Performance versus model size, WSJ-like artiﬁcial gra mmar . . . . . . . . . 102\n3.25 Performance versus model size, part-of-speech sequen ces . . . . . . . . . . . 102\n3.26 Expansions of symbols A with highest frequency p(A) . . . . . . . . . . . . 103\n3.27 Grammar induced with Lari and Young algorithm . . . . . . . . . . . . . . 104\n3.28 Execution time versus training data size . . . . . . . . . . . . . . . . . . . . 105\n3.29 Memory usage versus training data size . . . . . . . . . . . . . . . . . . . . 105\n4.1 Two-to-one sentence alignment . . . . . . . . . . . . . . . . . . . . . . . . . 109\n4.2 A bilingual corpus fragment . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n4.3 A bilingual corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n4.4 An alignment error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n4.5 Another alignment error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\nx\nList of Tables\n2.1 Perplexity results reported by Katz and N´ adas on 100 tes t sentences . . . . 21\n2.2 Perplexity results reported by MacKay and Peto on three t est sets . . . . . 22\n2.3 Implementation diﬃculty of various methods in terms of l ines of C++ code 26\n2.4 Eﬀect on speech recognition performance of typical entro py diﬀerences found\nbetween smoothing methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 0\n3.1 Initial hypothesis grammar . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n3.2 English-like artiﬁcial grammar . . . . . . . . . . . . . . . . . . . . . . . . . 100\n3.3 Wall Street Journal-like artiﬁcial grammar . . . . . . . . . . . . . . . . . . 101\n3.4 English sentence part-of-speech sequences . . . . . . . . . . . . . . . . . . . 101\n3.5 Number of parameters and training time of each algorithm . . . . . . . . . 104\n1\nChapter 1\nIntroduction\nIn this thesis, we describe novel techniques for building pr obabilistic models of language.\nWe investigate three distinct problems involving such mode ls, and improve the state-of-the-\nart in each task. In addition, we show how the techniques deve loped in this work address\ntwo central problems in probabilistic modeling.\nIn this chapter, we describe what probabilistic models of la nguage are, and demonstrate\nhow such models play an important role in many applications. We introduce the three\nproblems examined and explain how they are related. Finally , we summarize the basic\nconclusions of this work.\nChapters 2–4 describe in detail the work on each of the three t asks: smoothing n-gram\nmodels, Bayesian grammar induction, and bilingual sentenc e alignment. Chapter 5 presents\nthe conclusions of this thesis.\n1.1 Models of Language\nA model of language is simply a description of language. In its simpl est form, it may just be\na representation of the list of the sentences belonging to a l anguage; more complex models\nmay also try to describe the structure and meaning underlyin g sentences in a language.\nHistorically, attempts to model language have fallen in two general categories. The older\nand more familiar types of models are the grammars that were ﬁ rst developed in the ﬁeld of\nlinguistics. In more recent years, shallow probabilistic m odels for use in applications such\nas speech recognition have gained common usage. It is these s hallow probabilistic models\nthat we study in this thesis. In this section, we introduce an d contrast these two types of\nmodels.\nTraditionally, language has been modeled through grammars . In linguistics, it was\nobserved that language is structured in a rather constraine d hierarchical manner; there seem\nto be a fairly small number of primitive building blocks that can be combined together in\na limited number of ways to create the widely diverse forms th at are found in language.\nFor example, at the very lowest level of written English we ha ve the letter. Letters can be\ncombined to form words. Words in turn can be combined to form p hrases, such as a noun\nphrase, e.g., John Smith or a boat , or a prepositional phrase, e.g., above the table . These\n2\nphrases in turn can be combined to create sentences, which in turn can be used to build\nparagraphs, and so on.\nGrammars can be used to describe such hierarchical structure in a succ inct manner\n(Chomsky, 1964). A grammar consists of rules that describe a llowable ways of combining\nstructures at one level to form structures at the next higher level. For example, we may\nhave a grammar rule of the form:\nNoun-Phrase → Determiner Noun\nwhich is generally abbreviated as\nNP → D N\nThis represents the observation that a determiner ( e.g., a or the) followed by a noun can\nform a noun phrase. By combining the previous rule with the ru les\nD → a | the\nN → boat | cat | tree\ndescribing that a determiner can be formed by the words a or the and a noun can be\nformed by the words boat, cat, or tree, we have that strings such as a boat or the tree are\nnoun phrases.\nA grammar is a collection of rules like these that describe ho w to form high-level struc-\ntures such as sentences from low-level structures such as wo rds. Using this representation,\none can attempt to describe the set of all sentences in a langu age, and much work in lin-\nguistics is devoted to this goal, though using grammar repre sentations much richer than the\none described above.\nSuch grammars for language have wide application, most nota bly in the ﬁeld of natural-\nlanguage processing .1 The ﬁeld of natural-language processing deals with buildin g auto-\nmated systems that are able to process language in some way. F or example, one of the\ngoals of the ﬁeld is natural-language understanding , or being able to build systems that can\nunderstand human-friendly input such as What is the capital of North Dakota instead of\nonly computer-friendly input such as ﬁnd X : capital(X, “North Dakota”) .\nGrammars are useful models of language for natural language processing because they\nprovide insight into the structure of sentences, which aids in determining their meanings.\nFor example, in most systems the ﬁrst step in processing a sen tence is to parse the sentence\nto produce a parse tree. We display the parse tree for Max threw the ball beyond the fence\nin Figure 1.1. The parse tree shows what rules in the grammar n eed to be applied to form\nthe top-level structure, in this case a sentence, from the gi ven lowest-level structures, in this\n1 The term natural language is used to distinguish languages used for human communicati on such as\nEnglish or Urdu from languages used with machines such as Bas ic or Lisp. In this thesis, we use the term\nlanguage to mean only natural languages.\n3\nS\n✟✟✟✟✟✟\n❍❍❍❍❍❍\nNP\nPN\nMax\nVP\n✟✟✟✟✟\n❍❍❍❍❍\nVP\n✟✟✟ ❍❍❍\nV\nthrew\nNP\n✟✟ ❍❍\nD\nthe\nN\nball\nPP\n✟✟✟ ❍❍❍\nP\nbeyond\nNP\n✟✟ ❍❍\nD\nthe\nN\nfence\nFigure 1.1: Parse tree for Max threw the ball beyond the fence\ncase words. In this parse tree, the top three nodes represent the applications of the rules\nS → NP VP\nNP → PN\nVP → VP PP\nstating that a noun phrase followed by a verb phrase can form a sentence, a proper noun\ncan form a noun phrase, and a verb phrase followed by a preposi tional phrase can form a\nverb phrase.\nSubstrings of the sentence that are exactly spanned by nodes in the parse tree are\nintended to correspond to units that are relevant in determi ning the meaning of the sentence,\nand are called constituents. For example, the phrases Max, the ball , threw the ball , and Max\nthrew the ball beyond the fence are all constituents, while the ball beyond and threw the are\nnot. To give another example, the phrase the ball beyond the fence , while meaningful, is not\na constituent because in this sentence the phrase beyond the fence describes the throw, not\nthe ball. Thus, we see how grammars model not only which sente nces belong to a language,\nbut also the structure that underlies the meaning behind lan guage.\nWhile grammars have been the prevalent tool in modeling lang uage for a long time, it\nis generally accepted that building grammars that can handl e unrestricted language is at\nleast many years away. Instead, interest has shifted away fr om natural language under-\nstanding toward applications that do not require such a rich model of language. The most\nprominent of these applications is speech recognition, the task of constructing systems that\ncan automatically transcribe human speech.\nIn speech recognition, a model of language is used to help dis ambiguate acoustically\nambiguous utterances. For example, consider the task of tra nscribing an acoustic signal\ncorresponding to the string\nhe is too forbearing\n4\nPossible transcriptions include the following:\nT1 = he is too forbearing\nT2 = he is two four baring\nWhile both strings have the same pronunciation, we prefer th e ﬁrst transcription because\nit is the one that is more likely to occur in language. Hence, w e see how models that reﬂect\nthe frequencies of diﬀerent strings in language are useful in speech recognition.\nSuch models typically take forms very diﬀerent than linguist ic grammars. For example,\na common shallow probabilistic model is the bigram model. With a bigram model, the\nprobability of the sentence he is too forbearing is expressed as\np(he)p(is|he)p(too|is)p(forbearing|too)\nEach probability p(wi|wi−1) attempts to reﬂect how often the word wi follows the word wi−1\nin language, and these probabilities are estimated by takin g statistics on large amounts of\ntext.\nThere are many signiﬁcant diﬀerences between the shallow mod els used in speech recog-\nnition and the grammatical models used in linguistics and na tural language processing\nbesides their disparate representations. In linguistics, one attempts to build grammars that\ncorrespond exactly to the set of grammatical sentences. In speech recognition, one attempts\nto model how frequently strings are spoken, regardless of gr ammaticality. In linguistics and\nnatural language processing, one is concerned with buildin g parse trees that reveal the\nmeanings of sentences. In speech recognition, there is usua lly no need for structural anal-\nysis or any other deep processing of language. In linguistic s, models are not probabilistic\nas one is only trying to express a binary grammaticality judg ement. In speech recognition,\nmodels are almost exclusively probabilistic in order to exp ress frequencies.\nFinally, linguistic grammars have traditionally been manu ally constructed. A linguist\nusually designs grammars without any automated aid. In cont rast, models for speech recog-\nnition are built by taking statistics on large corpora of tex t. Such models have a great\nnumber of probabilities that need to be estimated, and this e stimation is only practical\nthrough the automated analysis of on-line text.\n1.2 Applications for Probabilistic Models\nProbabilistic models of language are not only valuable in sp eech recognition, but they are\nalso useful in applications as diverse as spelling correcti on, machine translation, and part-of-\nspeech tagging. These and other applications can be placed i n a single common framework\n(Bahl et al. , 1983), the source-channel model used in information theory (Shannon, 1948).\nIn this section, we explain how speech recognition can be pla ced in this framework, and\nthen explain how other applications are just variations on t his theme.\nThe task of speech recognition can be framed as follows: for a n acoustic signal A corre-\n5\nsponding to a sentence, we want to ﬁnd the most probable trans cription T , i.e., to ﬁnd\nT = arg max\nT\np(T |A)\nHowever, building accurate models of p(T |A) directly is beyond current know-how; 2 instead,\none applies Bayes’ rule to get the relation:\nT = arg max\nT\np(T )p(A|T )\np(A) = arg max\nT\np(T )p(A|T ) (1.1)\nThe probability distribution p(T ) is called a language model and describes how probable\nor frequent each sentence T is in language. The distribution p(A|T ) is called an acous-\ntic model and describes which acoustic signals A are likely realizations of a sentence T .\nThe language model p(T ) corresponds to the probabilistic model of language for spe ech\nrecognition discussed in the preceding section.\nThe source-channel model in information theory describes the problem of recove ring\ninformation that has been sent over a noisy channel. One has a model of the information\nsource, p(I), and a model of the noisy channel p(O|I) describing the likely outputs O of the\nchannel given an input I. (For a perfect channel, we would just have that p(O|I) = 1 for\nO = I and p(O|I) = 0 otherwise.) The task is to recover the original message I sent over\nthe channel given the noisy output O received at the other end. This can be phrased as\nﬁnding the message I with highest probability given O, or ﬁnding\nI = arg max\nI\np(I|O) = arg max\nI\np(I)p(O|I)\np(O) = arg max\nI\np(I)p(O|I) (1.2)\nWe can see an analogy with the task of speech recognition. The information source in this\ncase is a person generating the text of a sentence according t o the distribution p(T ). The\nnoisy channel corresponds to the process of a person convert ing this sentence from text to\nspeech according to p(A|T ). Finally, the goal is to recover the original text given the output\nof this noisy channel. While it may not be intuitive to refer t o a channel that converts text\nto speech as a noisy channel, the mathematics are identical.\nThe source-channel model is a powerful paradigm because it c ombines the model of the\nsource and the model of the channel in an elegant and eﬃcaciou s manner. For example,\nconsider the previous example of an acoustic utterance A corresponding to the sentence he\nis too forbearing and the possible transcriptions:\nT1 = he is too forbearing\nT2 = he is two four baring\nHere we have two transcriptions with identical pronunciati ons ( p(A|T1) ≈ p(A|T2)), but\nbecause the former sentence is much more common ( p(T1) ≫ p(T2)) we get p(T1)p(A|T1) ≫\n2Brown et al. (1993) present an explanation of why estimating p(T |A) is diﬃcult.\n6\np(T2)p(A|T2) and thus prefer transcription T1. On the other hand, consider\nT3 = he is very forbearing\nIn this case, we have two transcriptions with very similar fr equencies ( p(T1) ≈ p(T3)), but\nbecause T1 has a much higher acoustic score ( p(A|T1) ≫ p(A|T3)) we again prefer T1. Thus,\nwe see that the source-channel model combines acoustic and l anguage model information\neﬀectively to prefer transcriptions that both are likely to o ccur in language and match the\nacoustic signal well.\nThe source-channel model can be extended to many other appli cations besides speech\nrecognition by just varying the channel model used (Brown et al. , 1992b). In optical char-\nacter recognition and handwriting recognition (Hull, 1992 ; Srihari and Baltus, 1992), the\nchannel can be interpreted as converting from text to image d ata instead of from text to\nspeech, yielding the equation\nT = arg max\nT\np(T )p(image|T ).\nIn spelling correction (Kernighan et al. , 1990), the channel can be interpreted as an imper-\nfect typist that converts perfect text T to noisy text Tn with spelling mistakes, yielding\nT = arg max\nT\np(T )p(Tn|T ).\nIn machine translation (Brown et al. , 1990), the channel can be interpreted as a translator\nthat converts text T in one language into text Tf in a foreign language, yielding\nT = arg max\nT\np(T )p(Tf |T ). (1.3)\nIn each of these cases, we try to recover the original text T given the output of a noisy\nchannel, whether the noisy channel outputs image data, text with spelling errors, or text\nin a foreign language.\nBy varying the source model, we can extend the source-channe l model to further appli-\ncations. In part-of-speech tagging (Church, 1988), one att empts to label words in sentences\nwith their part-of-speech. We can apply the source-channel model by taking the source\nto generate part-of-speech sequences Tpos corresponding to sentences, and taking the chan-\nnel to convert part-of-speech sequences Tpos to sentences T that are consistent with that\npart-of-speech sequence, yielding\nTpos = arg max\nTpos\np(Tpos)p(T |Tpos).\nIn this case, we try to recover the original part-of-speech s equence Tpos given the text output\nof the noisy channel. The same techniques used to build model s p(T ) for regular text can\nbe used to build models p(Tpos) for part-of-speech sequences.\nNotice that in all of these applications it is necessary to bu ild a source language model,\n7\neither p(T ) or p(Tpos). Because of the importance of this task, this topic has beco me its\nown ﬁeld, language modeling . Notice that the term language modeling is used speciﬁcally\nto refer to source language models such as p(T ); we use the term models of language to\ninclude more general models such as channel models or lingui stic grammars. The ﬁrst two\nproblems we examine in this thesis are concerned with improv ing language modeling. The\nthird problem is concerned with a model very similar to a chan nel model, in particular the\ntranslation model p(Tf |T ) in equation (1.3).\n1.3 Problem Domains\nThe three problems that we have selected investigate the tas k of modeling language at three\ndiﬀerent levels: words, constituents, and sentences.\nFirst, we consider the problem of smoothing n-gram language models (Shannon, 1951).\nSuch models are dominant in language modeling, yielding the best current performance.\nIn such models, the probability of a sentence is expressed th rough the probability of each\nword in the sentence; such models are word-based models. The construction of an n-gram\nmodel is straightforward, except for the issue of smoothing, a technique used when there is\ninsuﬃcient data to estimate probabilities accurately. In t his thesis, we introduce two novel\nsmoothing methods that outperform existing methods, and pr esent an extensive analysis of\nprevious techniques.\nNext, we consider the task of statistically inducing a gramm atical language model from\ntext. While it seems logical to use the grammatical models de veloped in linguistics for\nprobabilistic language modeling, previous attempts at thi s have not yielded strong results.\nInstead, we attempt to statistically induce a grammar from a large corpus of text. In\ngrammatical language models, the probability of a sentence is expressed through the proba-\nbilities of the constituents within the sentence, and thus c an be considered constituent-based.\nThough yet to perform as well as word-based models, grammati cal models oﬀer the best\nhope for signiﬁcantly improving language modeling accurac y. We introduce a novel gram-\nmar induction algorithm based on the minimum description length principle (Rissanen,\n1978) that surpasses the performance of existing algorithm s.\nThe third problem deals with the task of bilingual sentence alignment . There exist many\ncorpora that contain equivalent text in multiple languages . For example, the Hansard corpus\ncontains the Canadian parliament proceedings in both Engli sh and French. Multilingual\ncorpora are useful for automatically building tools for mac hine translation such as bilingual\ndictionaries. However, current algorithms for building su ch tools require the speciﬁcation\nof which sentence(s) in one language translate to each sente nce in the other language,\nand this information is typically not included by human tran slators. Bilingual sentence\nalignment is the task of automatically producing this information. Th is turns out to be\na diﬃcult problem as a sentence in one language does not alway s correspond to a single\nsentence in the other language. Sentence alignment can be ap proached within the source-\nchannel framework using equation (1.3) as in machine transl ation; however, in this work\nwe use a slightly diﬀerent framework and express the translat ion model p(Tf |T ) as a joint\ndistribution p(T, Tf ). As sentence alignment is concerned only with aligning tex t at the\n8\nsentence level, the models used are sentence-based. We design a sentence-based translation\nmodel that leads to an eﬃcient and accurate alignment algori thm that outperforms previous\nalgorithms.\nFinally, we discuss how our work on these three problems forw ards research in probabilis-\ntic modeling. We compare the strategies used for building mo dels in these three diﬀerent\ndomains from a Bayesian perspective, and demonstrate why di ﬀerent strategies are appro-\npriate for diﬀerent domains. In addition, we show how the tech niques we have developed\naddress two central issues in probabilistic modeling.\n1.4 Bayesian Modeling\nThe Bayesian framework is an elegant and very general framew ork for probabilistic model-\ning. We explain the Bayesian framework through an example: c onsider the task of inducing\na grammar G from some data or observations O. In the Bayesian framework, one attempts\nto ﬁnd the grammar G that has the highest probability given the data O, i.e., to ﬁnd\nG = arg max\nG\np(G|O).\nAs it is diﬃcult to estimate p(G|O) directly, we apply Bayes’ rule to get\nG = arg max\nG\np(O|G)p(G)\np(O) = arg max\nG\np(O|G)p(G). (1.4)\nThe term p(O|G) describes the probability assigned to the data by the gramm ar, and is a\nmeasure of how well the grammar models the data. The term p(G) describes our a priori\nnotion of how likely a given grammar G is.3 This division between model accuracy and the\nprior belief of model likelihood is a natural way of modulari zing the grammar induction\nproblem.\nWhile each of the three problems we investigated can be addre ssed within the Bayesian\nframework, we instead selected three dissimilar approache s. For the grammar induction\nproblem, we apply the Bayesian framework in a straightforwa rd manner. For the sentence\nalignment problem, we use ad hoc methods that can be loosely interpreted as Bayesian in\nnature. While the Bayesian framework is well-suited to sent ence alignment, the use of ad\nhoc methods greatly simpliﬁed implementation at little or no co st in terms of performance.\nFinally, for smoothing n-gram models we use non-Bayesian methods. It is unclear how t o\nselect a prior distribution over smoothed n-gram models, and we have found that it is more\neﬀective to optimize performance directly than to optimize p erformance through examining\ndiﬀerent prior distributions. We conclude that while the Bay esian framework is elegant and\ngeneral, in practice less elegant methods are often eﬀective .\n3While equation (1.4) is very similar to equation (1.1) descr ibing the source-channel model for speech\nrecognition, this equation diﬀers in that p(G) is called a prior distribution and is built using a priori\ninformation. The analogous term p(T ) in equation (1.1) is called a language model and is built using\nmodeling techniques.\n9\n1.5 Sparse Data and Inducing Hidden Structure\nTwo issues that form a recurring theme in probabilistic mode ling are the sparse data prob-\nlem and the problem of inducing hidden structure ; these are perhaps the two most important\nissues in probabilistic modeling today.\nThe sparse data problem refers to the situation when there is insuﬃcient dat a to train\none’s model accurately. This problem is ubiquitous in stati stical modeling; the models that\nperform well tend to be very large and thus require a great dea l of data to train. There\nare two main approaches to addressing this problem. First, o ne can use the technique\nof smoothing, which describes methods for accurately estimating probab ilities in the pres-\nence of sparse data. Secondly, one can consider techniques f or building compact models.\nCompact models have fewer parameters to train and thus requi re less data.\nThe problem of inducing hidden structure describes the task of building models that\nexpress structure not overtly present in the training data. To give an example, consider the\nbigram model mentioned earlier, where the probability of th e sentence he is too forbearing\nis expressed as\np(he)p(is|he)p(too|is)p(forbearing|too)\nExpressing the probability of a sentence in terms of the prob ability of each word in the sen-\ntence conditioned on the immediately preceding word does no t seem particularly felicitous.\nIntuitively, it seems likely that by capturing the structur e underlying language as is done\nin linguistics, one may be able to build superior models. We c all this structure hidden as\nit is not explicitly demarcated in text. 4 To date, bigram models and similar models greatly\noutperform models that attempt to model hidden structure, b ut methods that induce hid-\nden structure oﬀer perhaps the best hope for producing models that signiﬁcantly improve\nthe current state-of-the-art.\nIn this thesis, we present several techniques that help addr ess these two central issues\nin probabilistic modeling. For the sparse data problem, we g ive novel techniques for both\nsmoothing and for constructing compact models. In addition , we present novel techniques\nfor inducing hidden structure that are not only eﬀective but e ﬃcient as well.\n4There is some data that has been manually annotated with this information, e.g., the Penn Treebank.\nHowever, manual annotation is expensive and thus only a limi ted amount of such data is available.\n10\nChapter 2\nSmoothing n-Gram Models\nIn this chapter, we describe work on the task of smoothing n-gram models (Chen and\nGoodman, 1996). 1 Of the three structural levels at which we model language in t his thesis,\nthis represents work at the word level. We introduce two nove l smoothing techniques\nthat signiﬁcantly outperform all existing techniques on tr igram models, and that perform\ncompetitively on bigram models. We present an extensive emp irical comparison of existing\nsmoothing techniques, which was previously lacking in the l iterature.\n2.1 Introduction\nAs mentioned in Chapter 1, language models are a staple in many domains including speech\nrecognition, optical character recognition, handwriting recognition, machine translation,\nand spelling correction. A language model is a probability distribution p(s) over strings s\nthat attempts to reﬂect how frequently a string s occurs as a sentence. For example, for a\nlanguage model describing spoken language, we might have p(hello) ≈ 0.01 since perhaps\none out of every hundred sentences a person speaks is hello. On the other hand, we would\nhave p(chicken funky overload ketchup ) ≈ 0 and p(asbestos gallops gallantly ) ≈ 0 since it\nis extremely unlikely anyone would utter either string. Not ice that unlike in linguistics,\ngrammaticality is irrelevant in language modeling. Even th ough the string asbestos gallops\ngallantly is grammatical, we still assign it a near-zero probability. Also, notice that in\nlanguage modeling we are only interested in the frequency wi th which a string occurs as\na complete sentence. For instance, we have p(you today ) ≈ 0 even though the string you\ntoday occurs frequently in spoken language, as in how are you today .\nBy far the most widely used language models are n-gram language models. We introduce\nthese models by considering the case n = 2; these models are called bigram models. First,\nwe notice that for a sentence s composed of the words w1 · · · wl, without loss of generality\nwe can express p(s) as\np(s) = p(w1)p(w2|w1)p(w3|w1w2) · · · p(wl|w1 · · · wl−1) =\nl∏\ni=1\np(wi|w1 · · · wi−1)\n1 This research was joint work with Joshua Goodman.\n11\nIn bigram models, we make the approximation that the probabi lity of a word only depends\non the identity of the immediately preceding word, giving us\np(s) =\nl∏\ni=1\np(wi|w1 · · · wi−1) ≈\nl∏\ni=1\np(wi|wi−1) (2.1)\nTo make p(wi|wi−1) meaningful for i = 1, we can pad the beginning of the sentence with\na distinguished token wbos; that is, we pretend w0 is wbos. In addition, to make the sum\nof the probabilities of all strings ∑\ns p(s) equal 1, it is necessary to place a distinguished\ntoken weos at the end of sentences and to include this in the product in eq uation (2.1). 2 For\nexample, to calculate p(John read a book ) we would take\np(John read a book ) = p(John|wbos)p(read|John)p(a|read)p(book|a)p(weos|book)\nTo estimate p(wi|wi−1), the frequency with which the word wi occurs given that the last\nword is wi−1, we can simply count how often the bigram wi−1wi occurs in some text and\nnormalize; that is, we can take\np(wi|wi−1) = c(wi−1wi)∑\nwi c(wi−1wi) (2.2)\nwhere c(wi−1wi) denotes the number of times the bigram wi−1wi occurs in the given\ntext.3 The text available for building a model is called training data . For n-gram mod-\nels, the amount of training data used is typically many milli ons of words. The estimate\nfor p(wi|wi−1) given in equation (2.2) is called the maximum likelihood (ML) estimate of\np(wi|wi−1), because this assignment of probabilities yields the bigr am model that assigns\nthe highest probability to the training data of all possible bigram models. 4\nFor n-gram models where n > 2, instead of conditioning the probability of a word on\nthe identity of just the preceding word, we condition this pr obability on the identity of the\n2Without this, consider the total probability associated wi th one-word strings. We have\n∑\ns=w1\np(s) =\n∑\nw1\np(w1|wbos) = 1\nThat is, the probabilities associated with one-word string s alone sum to 1. Similarly, without this device we\nwould have that the total probability of strings of exactly l ength k is 1 for all k > 0, giving us\n∑\ns\np(s) =\n∞∑\nl=1\n∑\nl(s)=l\np(s) =\n∞∑\nl=1\n1 = ∞\n3The expression\n∑\nwi\nc(wi−1wi) in equation (2.2) can also be expressed as simply c(wi−1), the number\nof times the word wi−1 occurs. However, we generally use the summation form as this highlights the fact\nthat this expression is used for normalization.\n4The probability of some data given a language model is just th e product of the probabilities of each\nsentence in the data. For training data S composed of the sentences ( s1, . . . , s lS ), we have p(S) =\n∏ lS\ni=1 p(si).\n12\nlast n − 1 words. Generalizing equation (2.1) to n > 2, we get\np(s) =\nl∏\ni=1\np(wi|wi−1\ni−n+1)\nwhere wj\ni denotes the words wi · · · wj .5 To estimate the probabilities p(wi|wi−1\ni−n+1), the\nanalogous equation to equation (2.2) is\np(wi|wi−1\ni−n+1) = c(wi\ni−n+1)∑\nwi c(wi\ni−n+1) (2.3)\nIn practice, the largest n in wide use is n = 3; this model is referred to as a trigram model.\nLet us consider a small example. Let our training data S be composed of the three\nsentences\n(John read Moby Dick , Mary read a diﬀerent book , she read a book by Cher )\nand let us calculate p(John read a book ) for the maximum likelihood bigram model. We\nhave\np(John|wbos) = c(wbosJohn)∑\nw c(wbosw) = 1\n3\np(read|John) = c(John read )∑\nw c(John w) = 1\n1\np(a|read) = c(read a )∑\nw c(read w) = 2\n3\np(book|a) = c(a book )∑\nw c(a w) = 1\n2\np(weos|book) = c(book weos)∑\nw c(book w) = 1\n2\ngiving us\np(John read a book ) = p(John|wbos)p(read|John)p(a|read)p(book|a)p(weos|book)\n= 1\n3 × 1 × 2\n3 × 1\n2 × 1\n2 ≈ 0.06\nNow, consider the sentence Moby read a book . We have\np(read|Moby) = c(Moby read )∑\nw c(Moby w) = 0\n1\nso we will get p(Moby read a book ) = 0. Obviously, this is an underestimate for the proba-\nbility p(Moby read a book ) as there is some probability that the sentence occurs. To show\n5 Instead of padding the beginning of sentences with a single wbos as in a bigram model, we need to pad\nsentences with n − 1 wbos’s for an n-gram model.\n13\nwhy it is important that this probability should be given a no nzero value, we turn to the\nprimary application for language models, speech recognition. As described in Chapter 1,\nin speech recognition one attempts to ﬁnd the sentence s that maximizes p(A|s)p(s) for a\ngiven acoustic signal A. If p(s) is zero, then p(A|s)p(s) will be zero and the string s will\nnever be considered as a transcription, regardless of how un ambiguous the acoustic signal\nis. Thus, whenever a string s such that p(s) = 0 occurs during a speech recognition task,\nan error will be made. Assigning all strings a nonzero probab ility helps prevent errors in\nspeech recognition.\nSmoothing is used to address this problem. The term smoothing describes techniques\nfor adjusting the maximum likelihood estimate of probabili ties (as in equations (2.2) and\n(2.3)) to produce more accurate probabilities. Typically, smoothing methods prevent any\nprobability from being zero, but they also attempt to improv e the accuracy of the model as\na whole. Whenever a probability is estimated from few counts , smoothing has the potential\nto signiﬁcantly improve estimation. For instance, from the three occurrences of the word\nread in the above example we have that the maximum likelihood esti mate of the probability\nthat the word a follows the word read is 2\n3 . As this estimate is based on three counts,\nwe do not have great conﬁdence in this estimate and intuitive ly, it is a gross overestimate.\nSmoothing would typically greatly lower this estimate.\nThe name smoothing comes from the fact that these techniques tend to make distri -\nbutions more uniform, which can be viewed as making them smoo ther. Typically, very\nlow probabilities such as zero probabilities are adjusted u pward, and high probabilities are\nadjusted downward.\nTo give an example, one simple smoothing technique is to pret end each bigram occurs\nonce more than it actually does (Lidstone, 1920; Johnson, 19 32; Jeﬀreys, 1948), yielding\np+1(wi|wi−1) = c(wi−1wi) + 1∑\nw[c(wi−1wi) + 1] = c(wi−1wi) + 1∑\nw c(wi−1wi) + |V | (2.4)\nwhere V is the vocabulary, the set of all words being considered. 6 Let us reconsider the\nprevious example using this new distribution, and let us tak e our vocabulary V to be the\nset of all words occurring in the training data S, so that we have |V | = 11.\nFor the sentence John read a book , we have\np(John read a book ) = p(John|wbos)p(read|John)p(a|read)p(book|a)p(weos|book)\n= 2\n14 × 2\n12 × 3\n14 × 2\n13 × 2\n13 ≈ 0.0001\nIn other words, we estimate that the sentence John read a book occurs about once every ten\nthousand sentences. This is much more reasonable than the ma ximum likelihood estimate\nof 0.06, or about once every seventeen sentences. For the sen tence Moby read a book , we\n6 Notice that if V is taken to be inﬁnite, the denominator is inﬁnite and all pro babilities are set to zero.\nIn practice, vocabularies are typically ﬁxed to be tens of th ousands of words. All words not in the vocabulary\nare mapped to a single distinguished word, usually called th e unknown word.\n14\nhave\np(Moby read a book ) = p(Moby|wbos)p(read|Moby)p(a|read)p(book|a)p(weos|book)\n= 1\n14 × 1\n12 × 3\n14 × 2\n13 × 2\n13 ≈ 0.00003\nAgain, this is more reasonable than the zero probability ass igned by the maximum likelihood\nmodel.\nWhile smoothing is a central issue in language modeling, the literature lacks a deﬁnitive\ncomparison between the many existing techniques. Previous studies (Nadas, 1984; Katz,\n1987; Church and Gale, 1991; MacKay and Peto, 1995) only comp are a small number of\nmethods (typically two) on a single corpus and using a single training data size. As a result,\nit is currently diﬃcult for a researcher to intelligently ch oose between smoothing schemes.\nIn this work, we carry out an extensive empirical comparison of the most widely used\nsmoothing techniques, including those described by Jeline k and Mercer (1980), Katz(1987),\nand Church and Gale (1991). We carry out experiments over man y training data sizes on\nvaried corpora using both bigram and trigram models. We demo nstrate that the relative\nperformance of techniques depends greatly on training data size and n-gram order. For\nexample, for bigram models produced from large training set s Church-Gale smoothing has\nsuperior performance, while Katz smoothing performs best o n bigram models produced\nfrom smaller data. For the methods with parameters that can b e tuned to improve per-\nformance, we perform an automated search for optimal values and show that sub-optimal\nparameter selection can signiﬁcantly decrease performanc e. To our knowledge, this is the\nﬁrst smoothing work that systematically investigates any o f these issues.\nIn addition, we introduce two novel smoothing techniques: t he ﬁrst belonging to the\nclass of smoothing models described by Jelinek and Mercer, t he second a very simple linear\ninterpolation method. While being relatively simple to imp lement, we show that these\nmethods yield good performance in bigram models and superio r performance in trigram\nmodels.\nWe take the performance of a method m to be its cross-entropy on test data\n1\nNT\nlT∑\ni=1\n− log2 pm(ti)\nwhere pm(ti) denotes the language model produced with method m and where the test\ndata T is composed of sentences ( t1, . . . , t lT ) and contains a total of NT words. The cross-\nentropy, which is sometimes referred to as just entropy, is inversely related to the average\nprobability a model assigns to sentences in the test data, an d it is generally assumed that\nlower entropy correlates with better performance in applic ations. Sometimes the entropy\nis reported in terms of a perplexity value; an entropy of H is equivalent to a perplexity of\n2H . The perplexity can be interpreted as the inverse ( 1\np ) of the average probability ( p) with\nwhich words are predicted by a model. Typical perplexities y ielded by n-gram models on\nEnglish text range from about 50 to several hundred, dependi ng on the type of text.\nIn addition to evaluating the overall performance of variou s smoothing techniques, we\n15\nprovide more detailed analyses of performance. We examine t he performance of diﬀerent\nalgorithms on n-grams with particular numbers of counts in the training dat a; we ﬁnd\nthat Katz and Church-Gale smoothing most accurately smooth n-grams with large counts,\nwhile our two novel methods are best for small counts. We calc ulate the relative impact\non performance of small counts and large counts for diﬀerent t raining set sizes and n-gram\norders, and use this data to explain the variation in perform ance of diﬀerent algorithms in\ndiﬀerent situations. Finally, we discuss several miscellan eous points including how Church-\nGale smoothing compares to linear interpolation, and how de leted interpolation compares\nwith held-out interpolation.\n2.2 Previous Work\n2.2.1 Additive Smoothing\nThe simplest type of smoothing used in practice is additive smoothing (Lidstone, 1920;\nJohnson, 1932; Jeﬀreys, 1948), which is just a generalizatio n of the smoothing given in\nequation (2.4). Instead of pretending each n-gram occurs once more than it does, we\npretend it occurs δ times more than it does, where typically 0 < δ ≤ 1, i.e.,\npadd(wi|wi−1\ni−n+1) = c(wi\ni−n+1) + δ\n∑\nwi c(wi\ni−n+1) + δ|V | (2.5)\nLidstone and Jeﬀreys advocate taking δ = 1. Gale and Church (1990; 1994) have argued\nthat this method generally performs poorly.\n2.2.2 Good-Turing Estimate\nThe Good-Turing estimate (Good, 1953) is central to many smo othing techniques. The\nGood-Turing estimate states that for any n-gram that occurs r times, we should pretend\nthat it occurs r∗ times where\nr∗ = ( r + 1) nr+1\nnr\n(2.6)\nand where nr is the number of n-grams that occur exactly r times in the training data. To\nconvert this count to a probability, we just normalize: for a n n-gram α with r counts, we\ntake\npGT(α) = r∗\nN (2.7)\nwhere N is the total number of counts in the distribution.\nTo derive this estimate, assume that there are a total of s diﬀerent n-grams α1, . . . , α s\nand that their true probabilities or frequencies are p1, . . . , p s, respectively. Let c(αi) denote\nthe number of times the n-gram αi occurs in the given training data. Now, we wish to\ncalculate the true probability of an n-gram αi that occurs r times; we can interpret this as\n16\ncalculating E(pi|c(αi) = r), where E denotes expected value. This can be expanded as\nE(pi|c(αi) = r) =\ns∑\nj=1\np(i = j|c(αi) = r)pj (2.8)\nThe probability p(i = j|c(αi) = r) is the probability that a randomly selected n-gram αi\nwith r counts is actually the jth n-gram αj . This is just\np(i = j|c(αi) = r) = p(c(αj ) = r)∑ s\nj=1 p(c(αj ) = r) =\n(N\nr\n)\npr\nj (1 − pj)N−r\n∑ s\nj=1\n(N\nr\n)\npr\nj (1 − pj )N−r = pr\nj (1 − pj )N−r\n∑ s\nj=1 pr\nj (1 − pj)N−r\nwhere N = ∑ s\ni=1 c(αi), the total number of counts. Substituting this into equati on (2.8),\nwe get\nE(pi|c(αi) = r) =\n∑ s\nj=1 pr+1\nj (1 − pj)N−r\n∑ s\nj=1 pr\nj (1 − pj)N−r (2.9)\nThen, consider EN (nr), the expected number of n-grams with exactly r counts given\nthat there are a total of N counts. This is equal to the sum of the probability that each\nn-gram has exactly r counts:\nEN (nr) =\ns∑\ni=1\np(c(αi) = r) =\ns∑\ni=1\n(\nN\nr\n)\npr\ni (1 − pi)N−r\nWe can substitute this expression into equation (2.9) to yie ld\nE(pi|c(αi) = r) = r + 1\nN + 1\nEN+1(nr+1)\nEN (nr)\nThis is an estimate for the expected probability of an n-gram αi with r counts; to express\nthis in terms of a corrected count r∗ we use equation (2.7) to get\nr∗ = Np(αi) = N r + 1\nN + 1\nEN+1(nr+1)\nEN (nr) ≈ (r + 1) nr+1\nnr\nNotice that the approximations EN (nr) ≈ nr and N\nN+1 EN+1(nr+1) ≈ nr+1 are used in the\nabove equation. In other words, we use the empirical values o f nr to estimate what their\nexpected values are.\nThe Good-Turing estimate yields absurd values when nr = 0; it is generally necessary\nto “smooth” the nr, e.g., to adjust the nr so that they are all above zero. Recently,\nGale and Sampson (1995) have proposed a simple and eﬀective al gorithm for smoothing\nthese values.\nIn practice, the Good-Turing estimate is not used by itself f or n-gram smoothing, be-\ncause it does not include the interpolation of higher-order models with lower-order models\nnecessary for good performance, as discussed in the next sec tion. However, it is used as a\ntool in several smoothing techniques.\n17\n2.2.3 Jelinek-Mercer Smoothing\nConsider the case of constructing a bigram model on training data where we have that\nc(burnish the ) = 0 and c(burnish thou ) = 0. Then, according to both additive smoothing\nand the Good-Turing estimate, we will have\np(the|burnish) = p(thou|burnish)\nHowever, intuitively we should have\np(the|burnish) > p (thou|burnish)\nbecause the word the is much more common than the word thou. To capture this behavior,\nwe can interpolate the bigram model with a unigram model. A unigram model is just a\n1-gram model, which corresponds to conditioning the probab ility of a word on no other\nwords. That is, the unigram probability of a word just reﬂect s its frequency in text. For\nexample, the maximum likelihood unigram model is\npML(wi) = c(wi)\n∑\nwi c(wi)\nWe can linearly interpolate a bigram model and unigram model as follows:\npinterp(wi|wi−1) = λ p ML(wi|wi−1) + (1 − λ) pML(wi)\nwhere 0 ≤ λ ≤ 1. Because pML(the|burnish) = pML(thou|burnish) = 0 while pML(the) ≫\npML(thou), we will have that\npinterp(the|burnish) > p interp(thou|burnish)\nas desired.\nIn general, it is useful to linearly interpolate higher-ord er n-gram models with lower-\norder n-gram models, because when there is insuﬃcient data to estim ate a probability in the\nhigher-order model, the lower-order model can often provid e useful information. A general\nclass of interpolated models is described by Jelinek and Mer cer (1980). An elegant way of\nperforming this interpolation is given by Brown et al. (1992a) as follows\npinterp(wi|wi−1\ni−n+1) = λwi−1\ni−n+1\npML(wi|wi−1\ni−n+1) + (1 − λwi−1\ni−n+1\n) pinterp(wi|wi−1\ni−n+2)\nThe nth-order smoothed model is deﬁned recursively as a linear in terpolation between the\nnth-order maximum likelihood model and the ( n − 1)th-order smoothed model. To end\nthe recursion, we can take the smoothed 1st-order model to be the maximum likelihood\ndistribution, or we can take the smoothed 0th-order model to be the uniform distribution\npunif(wi) = 1\n|V |\n18\nGiven ﬁxed pML, it is possible to search eﬃciently for the λwi−1\ni−n+1\nthat maximize the\nprobability of some data using the Baum-Welch algorithm (Ba um, 1972). To yield mean-\ningful results, the data used to estimate the λwi−1\ni−n+1\nneed to be disjoint from the data used\nto calculate the pML.7 In held-out interpolation , one reserves a section of the training data\nfor this purpose. Alternatively, Jelinek and Mercer descri be a technique called deleted in-\nterpolation where diﬀerent parts of the training data rotate in training e ither the pML or\nthe λwi−1\ni−n+1\n; the results are then averaged.\nTraining each parameter λwi−1\ni−n+1\nindependently is not generally felicitous; we would\nneed an enormous amount of data to train so many independent p arameters accurately.\nInstead, Jelinek and Mercer suggest dividing the λwi−1\ni−n+1\ninto a moderate number of sets,\nand constraining all λwi−1\ni−n+1\nin the same set to be equal, thereby reducing the number of\nindependent parameters to be estimated. Ideally, we should tie together those λwi−1\ni−n+1\nthat\nwe have an a priori reason to believe should have similar values. Bahl et al. (1983) suggest\nchoosing these sets of λwi−1\ni−n+1\naccording to ∑\nwi c(wi\ni−n+1), the total number of counts in\nthe higher-order distribution being interpolated. The gen eral idea is that this total count\nshould correlate with how strongly the higher-order distri bution should be weighted. That\nis, the higher this count the higher λwi−1\ni−n+1\nshould be. Distributions with the same number\nof total counts should have similar interpolation constant s. More speciﬁcally, Bahl et al.\nsuggest dividing the range of possible total count values in to some number of partitions,\nand to constrain all λwi−1\ni−n+1\nassociated with the same partition to have the same value.\nThis process of dividing n-grams up into partitions and training parameters independ ently\nfor each partition is referred to as bucketing.\n2.2.4 Katz Smoothing\nThe other smoothing technique besides Jelinek-Mercer smoo thing used widely in speech\nrecognition is due to Katz (1987). Katz smoothing (1987) ext ends the intuitions of Good-\nTuring by adding the interpolation of higher-order models w ith lower-order models.\nWe ﬁrst describe Katz smoothing for bigram models. In Katz sm oothing, for every count\nr > 0 a discount ratio dr is calculated, and any bigram with r > 0 counts is assigned a\ncorrected count of drr counts. Then, to calculate a given conditional distributio n p(wi|wi−1),\nthe nonzero counts are discounted according to dr, and the counts subtracted from the\nnonzero counts in that distribution are assigned to the bigr ams with zero counts. These\ncounts assigned to the zero-count bigrams are distributed p roportionally to the next lower-\norder n-gram model, i.e., the unigram model.\nIn other words, if the original count of a bigram c(wi\ni−1) is r, we calculate its corrected\ncount as follows:\nckatz(wi\ni−1) =\n{\ndrr if r > 0\nα p katz(wi) if r = 0 (2.10)\nwhere α is chosen such that the total number of counts in the distribu tion ∑\nwi ckatz(wi\ni−1)\n7When the same data is used to estimate both, setting all λwi− 1\ni− n+1\nto one yields the optimal result.\n19\nis unchanged, i.e., ∑\nwi ckatz(wi\ni−1) = ∑\nwi c(wi\ni−1). To calculate pkatz(wi|wi−1) from the\ncorrected count, we just normalize:\npkatz(wi|wi−1) = ckatz(wi\ni−1)∑\nwi ckatz(wi\ni−1)\nThe dr are calculated as follows: large counts are taken to be relia ble, so they are not\ndiscounted. In particular, Katz takes dr = 1 for all r > k for some k, where Katz suggests\nk = 5. The discount ratios for the lower counts r ≤ k are derived from the Good-Turing\nestimate applied to the global bigram distribution; that is , the nr in equation (2.6) denote\nthe total numbers of bigrams that occur exactly r times in the training data. These dr are\nchosen in such a way that the resulting discounts are proport ional to the discounts predicted\nby the Good-Turing estimate, and such that the total number o f counts discounted in the\nglobal bigram distribution is equal to the total number of co unts that should be assigned to\nbigrams with zero counts according to the Good-Turing estim ate.8 The former constraint\ncorresponds to the equation\n1 − dr = µ(1 − r∗\nr )\nfor all 1 ≤ r ≤ k for some constant µ. Good-Turing estimates that the total number of\ncounts that should be assigned to bigrams with zero counts is n00∗ = n0 n1\nn0\n= n1, so the\nsecond constraint corresponds to the equation\nk∑\nr=1\nnr(1 − dr)r = n1\nThe unique solution to these equations is given by\ndr =\nr∗\nr − (k+1)nk+1\nn1\n1 − (k+1)nk+1\nn1\nKatz smoothing for higher-order n-gram models is deﬁned analogously. As we can see\nin equation (2.10), the bigram model is deﬁned in terms of the unigram model; in general,\nthe Katz n-gram model is deﬁned in terms of the Katz ( n − 1)-gram model, similar to\nJelinek-Mercer smoothing. To end the recursion, the Katz un igram model is taken to be\nthe maximum likelihood unigram model:\npkatz(wi) = pML(wi) = c(wi)∑\nwi c(wi)\nRecall that we mentioned in Section 2.2.2 that it is usually n ecessary to smooth nr\nwhen using the Good-Turing estimate, e.g., for those nr that are very low. However, in\n8 In the normal Good-Turing estimate, the number of counts dis counted from n-grams with nonzero\ncounts happens to be equal to the number of counts assigned to n-grams with zero counts. Thus, the\nnormalization constant for a smoothed distribution is iden tical to that of the original distribution. In Katz\nsmoothing, Katz tries to achieve a similar eﬀect except thro ugh discounting only counts r ≤ k.\n20\nJelinek-Mercer N´ adas Katz\nbigram 118 119 117\ntrigram 89 91 88\nTable 2.1: Perplexity results reported by Katz and N´ adas on 100 test sentences\nKatz smoothing this is not essential because the Good-Turin g estimate is only used for\nsmall counts r ≤ k, and nr is generally fairly high for these values of r.\nKatz compares his algorithm with an unspeciﬁed version of Je linek-Mercer deleted esti-\nmation and with N´ adas smoothing (Nadas, 1984) using 750,00 0 words of training data from\nan oﬃce correspondence database. The perplexities display ed in Table 2.1 are reported\nfor a test set of 100 sentences. (Recall that smaller perplex ities are desirable.) Katz con-\ncludes that his algorithm performs at least as well as Jeline k-Mercer smoothing and N´ adas\nsmoothing.\n2.2.5 Church-Gale Smoothing\nChurch and Gale (1991) describe a smoothing method that like Katz’s, combines the Good-\nTuring estimate with a method for merging the information fr om lower-order models and\nhigher-order models.\nWe describe this method for bigram models. To motivate this m ethod, consider using\nthe Good-Turing estimate directly to build a bigram distrib ution. For each bigram with\ncount r, we would assign a corrected count of r∗ = ( r + 1) nr+1\nnr . As noted in Section 2.2.3,\nthis has the undesirable eﬀect of giving all bigrams with zero count the same corrected\ncount; instead, unigram frequencies should be taken into ac count. Consider the corrected\ncount assigned by an interpolative model to a bigram wi\ni−1 with zero counts. In such a\nmodel, we would have\np(wi|wi−1) ∝ p(wi)\nfor a bigram with zero counts. To convert this probability to a count, we multiply by the\ntotal number of counts in the distribution to get\np(wi|wi−1)\n∑\nwi\nc(wi\ni−1) ∝ p(wi)\n∑\nwi\nc(wi\ni−1) = p(wi)c(wi−1) ∝ p(wi)p(wi−1)\nThus, p(wi−1)p(wi) may be a good indicator of the corrected count of a bigram wi\ni−1 with\nzero counts.\nIn Church-Gale smoothing, bigrams wi\ni−1 are partitioned or bucketed according to the\nvalue of pML(wi−1)pML(wi). That is, they divide the range of possible pML(wi−1)pML(wi)\nvalues into a number of partitions, and all bigrams associat ed with the same subrange are\nconsidered to be in the same bucket. Then, each bucket is trea ted as a distinct probability\ndistribution and Good-Turing estimation is performed with in each. For a bigram in bucket\n21\ntest set Jelinek-Mercer MacKay-Peto\nsize (words) 3 λ’s 15 λ’s 150 λ’s\n260,000 79.60 79.90\n243,000 89.57 88.47 88.91 89.06\n116,000 91.82 92.28\nTable 2.2: Perplexity results reported by MacKay and Peto on three test sets\nb with rb counts, we calculate its corrected count r∗\nb as\nr∗\nb = ( rb + 1) nb,r+1\nnb,r\nwhere the counts nb,r include only those bigrams within bucket b.\nChurch and Gale partition the range of possible pML(wi−1)pML(wi) values into about 35\nbuckets, with three buckets in each factor of 10. To smooth th e nb,r for the Good-Turing\nestimate, they use a smoother by Shirey and Hastie (1988).\nWhile extensive empirical analysis is reported, they prese nt only a single entropy result,\ncomparing the above smoothing technique with another smoot hing method introduced in\ntheir paper, extended deleted estimation .\n2.2.6 Bayesian Smoothing\nSeveral smoothing techniques are motivated within a Bayesi an framework. A prior distri-\nbution over smoothed distributions is selected, and this pr ior is used to somehow arrive at\na ﬁnal smoothed distribution. For example, Nadas (1984) sel ects smoothed probabilities to\nbe their mean a posteriori value given the prior distribution.\nNadas (1984) hypothesizes a prior distribution from the fam ily of beta functions. The\nreported experimental results are presented in Table 2.1. ( The same results are reported in\nthe Katz and N´ adas papers.) These results indicate that N´ a das smoothing performs slightly\nworse than Katz and Jelinek-Mercer smoothing.\nMacKay and Peto (1995) use Dirichlet priors in an attempt to m otivate the linear in-\nterpolation used in Jelinek-Mercer smoothing. They compar e their method with Jelinek-\nMercer smoothing for a single training set of about two milli on words. For Jelinek-Mercer\nsmoothing, deleted interpolation was used dividing the cor pus up into six sections. The\nparameters λ were bucketed as suggested by Bahl et al. (1983), and three diﬀerent bucket-\ning granularities were tried. They report results for three diﬀerent test sets; these results\nare displayed in Table 2.2. These results indicate that MacK ay-Peto smoothing performs\nslightly worse than Jelinek-Mercer smoothing.\n2.3 Novel Smoothing Techniques\nOf the great many novel methods that we have tried, two techni ques have performed espe-\ncially well.\n22\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 10 100 1000 10000 100000\nlambda\nnumber of counts in distribution\nold bucketing\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.001 0.01 0.1 1 10 100\nlambda\naverage non-zero count in distribution minus one\nnew bucketing\nFigure 2.1: λ values for old and new bucketing schemes for Jelinek-Mercer smoothing; each\npoint represents a single bucket\n2.3.1 Method average-count\nThis scheme is an instance of Jelinek-Mercer smoothing. Rec all that one takes\npinterp(wi|wi−1\ni−n+1) = λwi−1\ni−n+1\npML(wi|wi−1\ni−n+1) + (1 − λwi−1\ni−n+1\n) pinterp(wi|wi−1\ni−n+2),\nwhere Bahl et al. suggest that the λwi−1\ni−n+1\nare bucketed according to ∑\nwi c(wi\ni−n+1), the\ntotal number of counts in the higher-order distribution. We have found that partitioning\nthe λwi−1\ni−n+1\naccording to the average number of counts per nonzero elemen t\n∑\nwi\nc(wi\ni−n+1)\n|wi:c(wi\ni−n+1)>0|\nyields better results.\nIntuitively, the less sparse the data for estimating pML(wi|wi−1\ni−n+1), the larger λwi−1\ni−n+1\nshould be. While the larger the total number of counts in a dis tribution the less sparse the\ndistribution tends to be, this measure ignores the allocati on of counts between words. For\nexample, we would consider a distribution with ten counts di stributed evenly among ten\nwords to be much more sparse than a distribution with ten coun ts all on a single word. The\naverage number of counts per word seems to more directly expr ess the concept of sparseness.\nIn Figure 2.1, we graph the value of λ assigned to each bucket under the original and\nnew bucketing schemes on identical data. The x-axis in each graph represents the criteria\nused for bucketing. Notice that the new bucketing scheme res ults in a much tighter plot,\nindicating that it is better at grouping together distribut ions with similar behavior.\nOne can use the Good-Turing estimate to partially explain th is behavior. As mentioned\nin Section 2.2.4, the Good-Turing estimate states that the n umber of counts that should\nbe devoted to n-grams with zero counts is n1, the number of n-grams in the distribution\nwith exactly one count. This is equivalent to assigning a tot al probability of n1\nN to n-grams\nwith zero counts, where N is the total number of counts in the distribution. Notice tha t\nthe value 1 − λwi−1\ni−n+1\nin Jelinek-Mercer smoothing is roughly proportional to the total\nprobability assigned to n-grams with zero counts: for n-grams wi\ni−n+1 with zero count we\n23\nhave pML(wi|wi−1\ni−n+1) = 0 so\npinterp(wi|wi−1\ni−n+1) = (1 − λwi−1\ni−n+1\n) pinterp(wi|wi−1\ni−n+2)\nThus, it seems reasonable that we want to satisfy the relatio n\n1 − λwi−1\ni−n+1\n∝ n1\nN\nwhere in this case n1 = |wi : c(wi\ni−n+1) = 1 | and N = ∑\nwi c(wi\ni−n+1).9\nOur goal in choosing a bucketing scheme is to bucket n-grams that should have similar\nλ values. Hence, given the above analysis we should bucket n-grams wi\ni−n+1 according to\nthe value of\nn1\nN = |wi : c(wi\ni−n+1) = 1 |∑\nwi c(wi\ni−n+1) (2.11)\nThis is very similar to the inverse of\n∑\nwi c(wi\ni−n+1)\n|wi : c(wi\ni−n+1) > 0|,\nthe actual value we use to bucket with. Instead of looking at t he number of n-grams with\nexactly one count, we use the number of n-grams with nonzero counts. Notice that it does\nnot matter much whether we bucket according to a value or acco rding to its inverse; the\nsame n-grams are grouped together.\nA natural experiment to try is to bucket according to the expr ession given in equa-\ntion (2.11). However, using this expression and variations , we were unable to surpass the\nperformance of the given bucketing scheme.\n2.3.2 Method one-count\nThis technique combines two intuitions. First, MacKay and P eto (1995) show that by using\na Dirichlet prior as a prior distribution over possible smoo thed distributions, we get (roughly\nspeaking) a model of the form\npone(wi|wi−1\ni−n+1) = c(wi\ni−n+1) + αpone(wi|wi−1\ni−n+2)\n∑\nwi c(wi\ni−n+1) + α\nwhere α is constant across n-grams. This is similar to additive smoothing, except that\ninstead of adding the same number of counts to each n-gram, we add counts proportional\n9Notice that n1 and N have diﬀerent meanings in this context from those found in Ka tz smoothing and\nChurch-Gale smoothing. While in each of these cases we use th e Good-Turing estimate, we apply the estimate\nto diﬀerent distributions. In Katz, we apply the Good-Turin g estimate to the global n-gram distribution,\nso that n1 represents the total number of n-grams with exactly one count and N represents the total count\nof n-grams. Church-Gale is similar to Katz except that n-grams are partitioned into a number of buckets.\nHowever, in this context we apply the Good-Turing estimate t o a conditional distribution p(wi|wi−1\ni−n+1) for\nsome ﬁxed wi−1\ni−n+1. Thus, n1 represents the number of n-grams wi\ni−n+1 beginning with wi−1\ni−n+1 with exactly\none count, and N represents the total count of n-grams wi\ni−n+1 beginning with wi−1\ni−n+1.\n24\nto the probability yielded by the next lower-order distribu tion. The parameter α represents\nthe total number of counts that we add to the distribution.\nSecondly, using a similar analysis as in the last section, th e Good-Turing estimate can\nbe interpreted as stating that the number of extra counts α should be proportional to n1,\nthe number of n-grams with exactly one count in the given distribution. Thu s, instead of\ntaking α to be constant across n-grams wi\ni−n+1, we take it to be a function of n1 = |wi :\nc(wi\ni−n+1) = 1 |. We have found that taking\nα = γ (n1 + β) (2.12)\nworks well, where β and γ are constants. Notice that higher-order models are deﬁned\nrecursively in terms of lower-order models.\nGiven the results mentioned in the last section, a natural ex periment to try is to take\nα to be a function of the number of nonzero counts in the distribution, as opposed to the\nnumber of one counts. However, attempts in this vein failed to yield super ior results.\n2.4 Experimental Methodology\nIn our experiments, we compare our novel smoothing methods w ith the most widely-used\nsmoothing techniques in language modeling: additive smoot hing, Jelinek-Mercer smoothing,\nand Katz smoothing. For Jelinek-Mercer smoothing, we try bo th held-out interpolation\nand deleted interpolation. In addition, we have also implem ented Church-Gale smoothing,\nas this has never been compared against popular techniques. We do not consider N´ adas\nsmoothing or MacKay-Peto smoothing as they are not widely us ed and as previous results\nindicate that they do not perform as well as other methods.\nAs a baseline method, we choose a simple instance of Jelinek- Mercer smoothing, one\nthat uses much fewer parameters than is typically used in rea l applications.\n2.4.1 Smoothing Implementations\nIn this section, we discuss the details of our implementatio ns of various smoothing tech-\nniques. The titles of the following sections include the mne monic we use to refer to the\nimplementations in later sections. We use the mnemonic when we are referring to our spe-\nciﬁc implementation of a smoothing method, as opposed to the algorithm in general. For\neach method, we mention the parameters that can be tuned to op timize performance; in\ngeneral, any variable mentioned is a tunable parameter.\nTo give an informal estimate of the diﬃculty of implementati on of each method, in Table\n2.3 we display the number of lines of C++ code in each implemen tation excluding the core\ncode common across techniques.\n10 For interp-baseline, we used the interp-held-out code as it is just a special case. Written anew, it\nprobably would have been about 50 lines.\n25\nMethod Lines\nplus-one 40\nplus-delta 40\nkatz 300\nchurch-gale 1000\ninterp-held-out 400\ninterp-del-int 400\nnew-avg-count 400\nnew-one-count 50\ninterp-baseline10 400\nTable 2.3: Implementation diﬃculty of various methods in te rms of lines of C++ code\nAdditive Smoothing (plus-one, plus-delta)\nWe consider two versions of additive smoothing. Referring t o equation (2.5) in Section\n2.2.1, we ﬁx δ = 1 in plus-one smoothing. In plus-delta, we consider any δ. (The values\nof parameters such as δ are determined through training on held-out data.)\nJelinek-Mercer Smoothing (interp-held-out, interp-del-int)\nRecall that higher-order models are deﬁned recursively in t erms of lower-order models.\nWe end the recursion by taking the 0th-order distribution to be the uniform distribution\npunif(wi) = 1 /|V |.\nWe bucket the λwi−1\ni−n+1\naccording to ∑\nwi c(wi\ni−n+1) as suggested by Bahl et al . Intu-\nitively, each bucket should be made as small as possible, to o nly group together the most\nsimilar n-grams, while remaining large enough to accurately estimat e the associated pa-\nrameters. We make the assumption that whether a bucket is lar ge enough for accurate\nparameter estimation depends on how many n-grams that fall in that bucket occur in the\ndata used to train the λ’s. We bucket in a such a way that a minimum of cmin n-grams fall\nin each bucket. We start from the lowest possible value of ∑\nwi c(wi\ni−n+1) ( i.e., zero) and\nput increasing values of ∑\nwi c(wi\ni−n+1) into the same bucket until this minimum count is\nreached. We repeat this process until all possible values of ∑\nwi c(wi\ni−n+1) are bucketed. If\nthe last bucket has fewer than cmin counts, we merge it with the preceding bucket. Histor-\nically, this process is called the wall of bricks (Magerman, 1994). We use separate buckets\nfor each n-gram model being interpolated.\nIn performing this bucketing, we create an array containing how many n-grams occur\nfor each value of ∑\nwi c(wi\ni−n+1) up to some maximum value of ∑\nwi c(wi\ni−n+1), which we\ncall ctop. For n-grams wi−1\ni−n+1 with ∑\nwi c(wi\ni−n+1) > c top, we pretend ∑\nwi c(wi\ni−n+1) = ctop\nfor bucketing purposes.\nAs mentioned in Section 2.2.3, the λ’s can be trained eﬃciently using the Baum-Welch\nalgorithm. Given initial values for the λ’s, the Baum-Welch algorithm adjusts these param-\neters iteratively to minimize the entropy of some data. The a lgorithm generally decreases\nthe entropy with each iteration, and guarantees not to incre ase it. We set all λ’s initially\n26\nto the value λ0. We terminate the algorithm when the entropy per word change s less than\nδstop bits between iterations.\nWe implemented two versions of Jelinek-Mercer smoothing, o ne using held-out interpo-\nlation and one using deleted interpolation. In interp-held-out, the λ’s are trained using\nheld-out interpolation on one of the development test sets. In interp-del-int, the λ’s are\ntrained using the relaxed deleted interpolation technique described by Jelinek and Mercer,\nwhere one word is deleted at a time. In interp-del-int, we bucket an n-gram according\nto its count before deletion, as this turned out to signiﬁcan tly improve performance. We\nhypothesize that this is because this causes an n-gram to be placed in the same bucket dur-\ning training as in evaluation, allowing the λ’s to be meaningfully geared toward individual\nn-grams.\nKatz Smoothing (katz)\nReferring to Section 2.2.4, instead of a single k we allow a diﬀerent kn for each n-gram\nmodel being interpolated.\nRecall that higher-order models are deﬁned recursively in t erms of lower-order models,\nand that the recursion is ended by taking the unigram distrib ution to be the maximum\nlikelihood distribution. While using the maximum likeliho od unigram distribution works\nwell in practice, this choice is not well-suited to our work. In practice, the vocabulary\nV is usually chosen to include only those words that occur in th e training data, so that\npML(wi) > 0 for all wi ∈ V . This assures that the probabilities of all n-grams are nonzero.\nHowever, in this work we do not satisfy the constraint that al l words in the vocabulary\noccur in the training data. We run experiments using many tra ining set sizes, and we use\na ﬁxed vocabulary across all runs so that results between siz es are comparable. Not all\nwords in the vocabulary will occur in the smaller training se ts. Thus, unless we smooth\nthe unigram distribution we may have n-gram probabilities that are zero, which could lead\nto an inﬁnite cross-entropy on test data. To address this iss ue, we smooth the unigram\ndistribution in Katz smoothing using additive smoothing; w e call the additive constant δ.11\nIn the algorithm as described in the original paper, no proba bility is assigned to n-grams\nwith zero counts in a conditional distribution p(wi|wi−1\ni−n+1) if there are no n-grams wi\ni−n+1\nthat occur between 1 and kn times in that distribution. This can lead to an inﬁnite cross -\nentropy on test data. To address this, whenever there are no c ounts between 1 and kn in\na conditional distribution, we give the zero-count n-grams a total of β counts, and increase\nthe normalization constant appropriately.\n11In Jelinek-Mercer smoothing, we address this issue by endin g the model recursion with a 0th-order model\ninstead of a unigram model, and taking the 0th-order model to be a uniform distribution. We tried a similar\ntack with Katz smoothing, but the natural way of interpolati ng a unigram model with a uniform model\nin the Katzian paradigm led to poor results. We tried additiv e smoothing instead, which is equivalent to\ninterpolating with a uniform distribution using the Jeline k-Mercer paradigm, and this worked well.\n27\nChurch-Gale Smoothing (church-gale)\nWhile Church and Gale use the maximum likelihood unigram dis tribution, we instead\nsmooth the unigram distribution using Good-Turing (withou t bucketing) as this seems\nmore consistent with the spirit of the algorithm. This shoul d not aﬀect performance much,\nas the unigram probabilities are used only for bucketing pur poses.12\nWe use a diﬀerent bucketing scheme than that described by Chur ch and Gale. For\na bigram model, they divide the range of possible values of p(wi−1)p(wi) into about 35\nbuckets, with three buckets per factor of 10. However, this b ucketing strategy is not ideal\nas bigrams are not distributed uniformly among diﬀerent orde rs of magnitude. Furthermore,\nthey provide analysis that indicates that they had suﬃcient data to distinguish between at\nleast 1200 diﬀerent probabilities to be assigned to bigrams w ith zero counts; this is evidence\nthat using signiﬁcantly more than 35 buckets might yield bet ter performance. Hence, we\nchose to use wall of bricks bucketing as in our implementation of Jelinek-Mercer smoot hing.\nWe ﬁrst do as Church and Gale do and partition the range of poss ible p(wi−1)p(wi)\nvalues using some constant number of buckets per order of mag nitude, except instead of\nusing a total of 35 buckets we use some very large number of buc kets, cmb. Instead of calling\nthese partitions buckets we call them minibuckets, as we lump together these minibuckets\nto form our ﬁnal buckets using the wall of bricks technique. W e group together minibuckets\nso that at least cmin n-grams with nonzero count fall in each bucket.\nTo smooth the counts nr needed for the Good-Turing estimate, we use the technique\ndescribed by Gale and Sampson (1995). This technique assign s a total probability of n1\nN\nto n-grams with zero counts, as dictated by the Good-Turing esti mate. However, it is\npossible that n1 = N in which case no probability is assigned to nonzero counts. A s this is\nunacceptable, we modify the algorithm so that in this case, w e assign a total probability of\npn1=N < 1 to zero counts. In addition, it is possible that n1 = 0 in which case no probability\nis assigned to zero counts. In this case, we instead assign a t otal probability of pn1=0 > 0\nto zero counts.\nFinally, the original paper describes only bigram smoothin g in detail; extending this\nmethod to trigram models is ambiguous. In particular, it is u nclear whether to bucket\ntrigrams according to p(wi−1\ni−2)p(wi) or p(wi−1\ni−2)p(wi|wi−1). We choose the former value;\nwhile the latter value may yield better performance as it is a better estimate of p(wi\ni−2),13\nour belief is that it is much more diﬃcult to implement and it r equires a great deal more\ncomputation.\nWe outline the algorithm we use to construct a trigram model i n Figure 2.2. The\ntime complexity of the algorithm is roughly O(cnz + cp), where cnz denotes the number\n12 We observed an interesting phenomenon when smoothing the un igram distribution with Good-Turing.\nWe construct a vocabulary V by collecting all words in a corpus that occur at least k times, for some value\nk. For training sets that include the majority of a corpus, the re will be unnaturally few words occurring\nfewer than k times, since most of these words have been weeded out of the vo cabulary. This results in nr\nthat can yield odd corrected counts r∗. For example, for a given cutoﬀ k we may get nk ≫ nk−1 so that\n(k − 1)∗ is overly high. We have not found that this phenomenon signiﬁ cantly aﬀects performance.\n13 Referring to the analysis given in Section 2.2.5, the former choice roughly corresponds to interpolating\nthe trigram model with a unigram model, while the latter choi ce corresponds to interpolating the trigram\nmodel with a bigram model.\n28\n; count how many trigrams with nonzero counts fall in each mini bucket\nfor each trigram wi\ni−2 with c(wi\ni−2) > 0 do\nincrement the count for the minibucket bm that wi\ni−2 falls in\ngroup minibuckets bm into buckets b using the wall of bricks technique\n; calculate number of trigrams in each bucket by looping over a ll values of p(wi−1\ni−2)p(wi)\n; this is used later to calculate number of trigrams with zero c ounts in each bucket\n; the ﬁrst two loops loop over all possible values of p(wi−1\ni−2), the third loop is for p(wi)\nfor each bigram bucket b2 do\nfor each count r2 with nb2,r2 > 0 do\nfor each count r1 with nr1 > 0 in the unigram distribution do\nbegin\nb := the bucket a trigram wi\ni−2 falls in if c(wi−1\ni−2) = r2 and c(wi) = r1\nincrement the count for b by the number of trigrams wi\ni−2 such that\nc(wi−1\ni−2) = r2 and c(wi) = r1, i.e.|wi−1\ni−2 : c(wi−1\ni−2) = r2| × | wi : c(wi) = r1|\nend\n; calculate counts nb,r for each bucket b and count r > 0\nfor each trigram wi\ni−2 with c(wi\ni−2) > 0 do\ncalculate the bucket b that wi\ni−2 falls in, and increase nb,r for r = c(wi\ni−2)\ncalculate nb,0 by subtracting ∑ ∞\nr=1 nb,r from the total number of trigrams in b\nsmooth the nb,r values using the Gale-Sampson algorithm\n; calculate normalization constants ∑\nwi cGT(wi\ni−2) for each wi−1\ni−2\nfor each bigram bucket b2 do\nfor each count r2 with nb2,r2 > 0 do\ncalculate normalization constant Nb2,r2 for a bigram wi−1\ni−2 in bucket b2\nwith c(wi−1\ni−2) = r2 given that c(wi\ni−2) = 0 for all wi\nfor each bigram wi−1\ni−2 with c(wi−1\ni−2) > 0 do\ncalculate its normalization constant ∑\nwi cGT(wi\ni−2) by calculating its diﬀerence\nfrom Nb2,r2 where wi−1\ni−2 falls in bucket b2 and c(wi−1\ni−2) = r2; this can be done\nby looping through all trigrams wi\ni−2 with c(wi\ni−2) > 0\nFigure 2.2: Outline of our Church-Gale trigram implementat ion\n29\nof trigrams with nonzero counts and cp denotes the number of diﬀerent possible values of\np(wi−1\ni−2)p(wi). The term cp comes from the fact that to calculate the total number of n-\ngrams in a bucket b (which is needed to eﬃciently calculate nb,0), it is necessary to loop\nover all possible values of p(wi−1\ni−2)p(wi). We take advantage of the fact that the number\nof possible values for p(wi−1\ni−2) is at most the total number of diﬀerent bigram counts in\neach bigram bucket |nb,r : nb,r > 0, b a bigram bucket |, as each ( b, r) pair corresponds to\na potentially diﬀerent corrected count that can be assigned t o a bigram. Similarly, the\nnumber of possible values for p(wi) is at most the total number of diﬀerent unigram counts\n|nr : nr > 0, for the unigram distribution |.\nNow, consider the analogous algorithm except bucketing usi ng p(wi−1\ni−2)p(wi|wi−1). The\nfactor in cp from p(wi−1\ni−2) remains the same, but the number of diﬀerent values for p(wi|wi−1)\nis much larger than the number of diﬀerent values for p(wi). The number of diﬀerent values\nfor p(wi|wi−1) is roughly equal to the number of diﬀerent bigrams with nonze ro counts,\nwhile the number of diﬀerent values for p(wi) is at most the number of diﬀerent unigram\ncounts. Thus, this alternate bucketing scheme is much more expensi ve computationally.\nNovel Smoothing Methods (new-avg-count, new-one-count)\nThe implementation of smoothing method average-count, new-avg-count, is identical to\ninterp-held-out except that instead of bucketing the λwi−1\ni−2\naccording to ∑\nwi c(wi\ni−n+1),\nwe bucket according to\n∑\nwi\nc(wi\ni−n+1)\n|wi:c(wi\ni−n+1)>0| as described in Section 2.3.1.\nIn the implementation of smoothing method one-count, new-one-count, we have dif-\nferent parameters βn and γn in equation (2.12) for each n-gram model being interpolated.\nAlso, recall that higher-order models are deﬁned recursive ly in terms of lower-order models.\nWe end the recursion by taking the 0th-order distribution to be the uniform distribution\npunif(wi) = 1 /|V |.\nBaseline Smoothing (interp-baseline)\nFor our baseline smoothing method, we use Jelinek-Mercer sm oothing with held-out inter-\npolation where for each n-gram model being interpolated we constrain all λwi−1\ni−n+1\nin the\nmodel to be equal to a single value λn, i.e.,\npbase(wi|wi−1\ni−n+1) = λn pML(wi|wi−1\ni−n+1) + (1 − λn) pbase(wi|wi−1\ni−n+2).\nThis is identical to interp-held-out where cmin is set to ∞, so that there is only a single\nbucket for each n-gram model.\n2.4.2 Implementation Architecture\nIn this section, we give an overview of the entire implementa tion. The coding was done in\nC++. Each of the implementations of the individual smoothin g techniques were linked into\na single program, to help ensure uniformity in the methodolo gy used with each smoothing\ntechnique.\n30\nFor large training sets, it is diﬃcult to ﬁt an entire bigram o r trigram model into a\nmoderate amount of memory. Thus, we chose not to do the straig htforward implementation\nof ﬁrst building an entire smoothed n-gram model in memory and then evaluating it.\nInstead, notice that for a given test set, it is only necessar y to build that part of the\nsmoothed n-gram model that is applicable to the test set. To take advant age of this ob-\nservation, we ﬁrst process the training set by taking counts of all n-grams up to the target\nn, and we sort these n-grams into an order suitable for future processing. We then iterate\nthrough these n-gram counts, extracting those counts that are relevant for evaluating the\ntest data (or the held-out data used to optimize parameter va lues). We use these extracted\ncounts to build the smoothed n-gram model on only the relevant data. For some smoothing\nalgorithms ( katz and church-gale), it is necessary to make additional passes through the\nn-gram counts to collect other statistics.\nIn some experiments, we use very large test sets; in this case the above algorithm is\nnot practical as a too large fraction of the total model is nee ded to evaluate the test data.\nFor these experiments, we process the test data in the same wa y as the training data, by\ntaking counts of all relevant n-grams and sorting them. We then iterate through both the\ntraining n-gram counts and test n-gram counts simultaneously, repeatedly building a small\nsection of the smoothed n-gram model, evaluating it on the associated test data, and t hen\ndiscarding the partial model.\nIn our implementation, we include a general multidimension al search engine for auto-\nmatically searching for optimal parameter values for each s moothing technique. We use the\nimplementation of Powell’s search algorithm (Brent, 1973) given in Numerical Recipes in C\n(Press et al. , 1988, pp. 309–317). Powell’s algorithm does not require th e calculation of the\ngradient. It involves successive searches along vectors in the multidimensional search space.\n2.4.3 Data\nWe used the Penn treebank and TIPSTER corpora distributed by the Linguistic Data\nConsortium. From the treebank, we extracted text from the ta gged Brown corpus, yielding\nabout one million words. From TIPSTER, we used the Associate d Press (AP), Wall Street\nJournal (WSJ), and San Jose Mercury News (SJM) data, yieldin g 123, 84, and 43 million\nwords respectively. We created two distinct vocabularies, one for the Brown corpus and\none for the TIPSTER data. The former vocabulary contains all 53,850 words occurring in\nBrown; the latter vocabulary consists of the 65,173 words oc curring at least 70 times in\nTIPSTER.\nFor each experiment, we selected three segments of held-out data along with the segment\nof training data. These four segments were chosen to be adjac ent in the original corpus\nand disjoint, the held-out segments preceding the training to facilitate the use of common\nheld-out data with varying training data sizes. The ﬁrst hel d-out segment was used as\nthe test data for performance evaluation, and the other two h eld-out segments were used\nas development test data for optimizing the parameters of ea ch smoothing method. In\nexperiments with multiple runs on the same training data siz e, the data segments of each\nrun are completely disjoint.\n31\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n0.001 0.01 0.1 1 10 100 1000\ndifference in test cross-entropy from baseline (bits/token)\ndelta\nperformance of katz with respect to delta\n100 sent\n1,000 sent\n10,000 sent\n50,000 sent\n-0.14\n-0.13\n-0.12\n-0.11\n-0.1\n-0.09\n-0.08\n-0.07\n-0.06\n-0.05\n1 10 100 1000 10000 100000\ndifference in test cross-entropy from baseline (bits/token)\nminimum number of counts per bucket\nperformance of new-avg-count with respect to c-min\n100 sent\n10,000 sent\n1,000,000 sent\n10,000,000 sent\nFigure 2.3: Performance relative to baseline method of katz and new-avg-count with\nrespect to parameters δ and cmin, respectively, over several training set sizes\nEach piece of held-out data was chosen to be roughly 50,000 wo rds. This decision does\nnot reﬂect practice well. For example, if the training set si ze is less than 50,000 words then\nit is not realistic to have this much development test data av ailable. However, we made\nthis choice to prevent us having to optimize the training ver sus held-out data tradeoﬀ for\neach data size. In addition, the development test data is use d to optimize typically very\nfew parameters, so in practice small held-out sets are gener ally adequate, and perhaps can\nbe avoided altogether with techniques such as deleted estim ation.\n2.4.4 Parameter Setting\nIn Figure 2.3, we show how the values of the parameters δ and cmin aﬀect the performance\nof methods katz and new-avg-count, respectively, over several training data sizes. Notice\nthat poor parameter setting can lead to very signiﬁcant loss es in performance. In Figure\n2.3, we see diﬀerences in entropy from several hundredths of a bit to over a bit. Also,\nwe see that the optimal value of a parameter varies with train ing set size. Thus, it is\nimportant to optimize parameter values to meaningfully com pare smoothing techniques,\nand this optimization should be speciﬁc to the given trainin g set size.\nIn each experiment we ran except as noted below, optimal valu es for the parameters of\nthe given method were searched for using Powell’s search alg orithm. Parameters were chosen\nto optimize the cross-entropy of the ﬁrst of the two developm ent test sets associated with\nthe given training set. For katz and church-gale, we did not perform the parameter search\nfor training sets over 50,000 sentences due to resource cons traints, and instead manually\nextrapolated parameter values from optimal values found on smaller data sizes.\nFor instances of Jelinek-Mercer smoothing, the λ’s were trained using the Baum-Welch\nalgorithm on the second development test set; all other para meters were optimized using\nPowell’s algorithm on the ﬁrst development test set. More sp eciﬁcally, to evaluate the en-\ntropy associated with a given set of (non- λ) parameters in Powell’s search, we ﬁrst optimize\nthe λ’s on the second test set.\n32\nTo constrain the parameter search in our main battery of expe riments, we searched\nonly those parameters that were found to aﬀect performance si gniﬁcantly, as indicated\nthrough preliminary experiments over several data sizes. I n each run of these preliminary\nexperiments, we ﬁxed all parameters but one to some reasonab le value, and used Powell’s\nalgorithm to search on the single free parameter. We recorde d the entropy of the test data\nfor each parameter value considered by Powell’s algorithm. If the range of test data entropy\nover this search was much smaller than the typical diﬀerence i n entropies between diﬀerent\nalgorithms, we considered it safe not to perform the search o ver this parameter in the later\nexperiments. For each parameter, we tried three diﬀerent tra ining sets: 20,000 words from\nthe WSJ corpus, 1M words from the Brown corpus, and 3M words fr om the WSJ corpus.\nWe assumed that all parameters are signiﬁcant for the method s plus-one, plus-lambda,\nand new-one-count. We describe the results for the other algorithms below.\nJelinek-Mercer Smoothing ( interp-held-out, interp-del-int, new-avg-count,\ninterp-baseline)\nThe parameter λ0, the initial value of the λ’s in the Baum-Welch search, aﬀected entropy by\nless than 0.001 bits. Thus, we decided not to search over this parameter in later experiments.\nWe ﬁx λ0 to be 0.5.\nThe parameter δstop, controlling when to terminate the Baum-Welch search, aﬀect ed\nentropy by less than 0.002 bits. We ﬁx δstop to be 0.001 bits.\nThe parameter ctop, the top count considered in bucketing, aﬀected entropy by up to\n0.006 bits, which is signiﬁcant. However, we found that in ge neral the entropy is lower for\nhigher values of ctop; this range of 0.006 bits is mainly due to setting ctop too low. We ﬁx\nctop to be a fairly large value, 100,000.\nThe parameter cmin, the minimum number of counts in each bucket, aﬀected entropy\nby up to 0.07 bits, which is signiﬁcant. Thus, we search over t his parameter in later\nexperiments.\nKatz Smoothing (katz)\nThe parameters kn, specifying the count above which counts are not discounted , aﬀected\nentropy by up to 0.01 bits, which is signiﬁcant. However, we f ound that the larger the kn,\nthe better the performance. In Figure 2.4, we display the ent ropy on the Brown corpus for\ndiﬀerent values of k1, k2, and k3. However, for large k there will be counts r such that the\nassociated discount ratio dr takes on an unreasonable value, such as a nonpositive value o r\na value above one. We take kn to be as large as possible such that the dr take on reasonable\nvalues.\nThe parameter β, describing how many counts are given to n-grams with zero counts if\nno counts in a distribution are discounted, aﬀected the entro py by less than 0.001 bits. We\nﬁx β to be 1.\nThe parameter δ, the constant used for the additive smoothing of the unigram distri-\nbution, aﬀected entropy by up to 0.02 bits, which is signiﬁcan t. Thus, we search over this\nparameter in later experiments. For large training sets (ov er 50,000 sentences), we do not\n33\n9.638\n9.639\n9.64\n9.641\n9.642\n9.643\n9.644\n9.645\n9.646\n9.647\n4 6 8 10 12 14 16 18\ncross entropy of test data (bits/token)\nparameter value\nk1\nk2\nk3\nFigure 2.4: Eﬀect of kn on Katz smoothing\n8.41\n8.42\n8.43\n8.44\n8.45\n8.46\n8.47\n8.48\n8.49\n0 500 1000 1500 2000 2500 3000 3500\ncross entropy of test data (bits/token)\nc_min\n8.41\n8.415\n8.42\n8.425\n8.43\n8.435\n8.44\n500 1000 1500 2000 2500 3000 3500\ncross entropy of test data (bits/token)\nc_mb\nFigure 2.5: Eﬀect of cmin and cmb on Church-Gale smoothing\nperform the search due to time constraints. Instead, we choo se its value by manually extrap-\nolating from the optimal values found on smaller training se ts. For example, for TIPSTER\nwe found that δ = 0 .0011 × l0.7\nS ﬁts the optimal values found for smaller training sets well,\nwhere lS denotes the number of sentences in the training data.\nChurch-Gale Smoothing (church-gale)\nThe parameter pn1=0, the probability assigned to zero counts if there are no one- counts in\na distribution, aﬀected the entropy not at all. We ﬁx pn1=0 to be 0.01.\nThe parameter pn1=N , the probability assigned to zero counts if all counts in a di stri-\nbution are one-counts, aﬀected the entropy by up to 0.2 bits. T hus, we search over this\nparameter in later experiments. However, for training sets over 50,000 sentences, due to\ntime constraints we do not perform parameter search for church-gale. We noticed that\nfor larger training sets this parameter does not seem to have a large eﬀect (0.002 bits on\nthe 3M words of WSJ), and the optimal value tends to be very clo se to 1. Thus, for large\ntraining sets we ﬁx pn1=N to be 0.995.\n34\nThe parameters cmin, the minimum number of counts per bucket, and cmb, the number\nof minibuckets, both aﬀected the entropy a great deal (over 0. 5 bits). Thus, we search\nover these parameters in later experiments. However, the se arch space for both of these\nparameters is very bumpy, so it is unclear how eﬀective the sea rch process is. In Figure\n2.5, we display the entropy on test data for various values of cmin and cmb when training\non 3M words of WSJ. The search algorithm will ﬁnd a local minim um, but we will have no\nguarantee on the global quality of this minimum given the nat ure of the search space.\nAs mentioned above, for training sets over 50,000 sentences , due to time constraints we\ndo not perform parameter search for church-gale. Fortunately, for larger training sets cmin\nand cmb seem to have a smaller eﬀect (0.07 and 0.03 bits, respectively , on the 3M words\nof WSJ). For training sets over 50,000 sentences, we just gue ss reasonable values for these\nparameters: we ﬁx cmin to be 500 and cmb to be 100,000. For very large data sets, due to\nmemory constraints we take cmin to be lS\n200 to limit the number of buckets created.\n2.5 Results\nIn this section, we present the results of our experiments. F irst, we present the performance\nof various algorithms for diﬀerent training set sizes on diﬀer ent corpora for both bigram and\ntrigram models. We demonstrate that the relative performan ce of smoothing methods varies\nsigniﬁcantly over training sizes and n-gram order, and we show which methods perform best\nin diﬀerent situations. We ﬁnd that katz performs best for bigram models produced from\nmoderately-sized data sets, church-gale performs best for bigram models produced from\nlarge data sets, and our novel methods new-avg-count and new-one-count perform best\nfor trigram models.\nThen, we present a more detailed analysis of performance, ra ting diﬀerent techniques\non how well they perform on n-grams with a particular count in the training data, e.g.,\nn-grams that have occurred exactly once in the training data. We ﬁnd that katz and\nchurch-gale most accurately smooth n-grams with large counts, while new-avg-count and\nnew-one-count are best for small counts. We then show the relative impact on performance\nof small counts and large counts for diﬀerent training set siz es and n-gram orders, and\nuse this data to explain the variation in performance of diﬀer ent algorithms in diﬀerent\nsituations.\nFinally, we examine three miscellaneous points: the accura cy of the Good-Turing es-\ntimate in smoothing n-grams with zero counts, how Church-Gale smoothing compare s to\nlinear interpolation, and how deleted interpolation compa res with held-out interpolation.\n2.5.1 Overall Results\nIn Figure 2.6, we display the performance of the interp-baseline method for bigram and\ntrigram models on TIPSTER, Brown, and the WSJ subset of TIPST ER. In Figures 2.7–\n2.10, we display the relative performance of various smooth ing techniques with respect to\nthe baseline method on these corpora, as measured by diﬀerenc e in entropy. In the graphs\non the left of Figures 2.6–2.8, each point represents an aver age over ten runs; the error bars\n35\n7.5\n8\n8.5\n9\n9.5\n10\n10.5\n11\n11.5\n100 1000 10000\ncross-entropy of test data (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nTIPSTER bigram\nTIPSTER trigram\nWSJ bigram\nWSJ trigram\n6.5\n7\n7.5\n8\n8.5\n9\n9.5\n10\n10.5\n11\n11.5\n100 1000 10000 100000 1e+06 1e+07\ncross-entropy of test data (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size\nTIPSTER bigram\nTIPSTER trigram\nBrown bigram\nBrown trigram\nWSJ bigram\nWSJ trigram\nFigure 2.6: Baseline cross-entropy on test data; graph on le ft displays averages over ten\nruns for training sets up to 50,000 sentences, graph on right displays single runs for training\nsets up to 10,000,000 sentences\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n0.04\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nFigure 2.7: Trigram model on TIPSTER data; relative perform ance of various methods\nwith respect to baseline; graphs on left display averages ov er ten runs for training sets up\nto 50,000 sentences, graphs on right display single runs for training sets up to 10,000,000\nsentences; top graphs show all algorithms, bottom graphs zo om in on those methods that\nperform better than the baseline method\n36\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n5\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nkatz, interp-held-out, interp-del-int, new-avg-count, new-one-count (see below)\nchurch-gale\nplus-delta\nplus-one\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\naverage over ten runs at each size, up to 50,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000 100000 1e+06 1e+07\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nsingle run at each size, up to 10,000,000 sentences\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\nchurch-gale\nFigure 2.8: Bigram model on TIPSTER data; relative performa nce of various methods\nwith respect to baseline; graphs on left display averages ov er ten runs for training sets up\nto 50,000 sentences, graphs on right display single runs for training sets up to 10,000,000\nsentences; top graphs show all algorithms, bottom graphs zo om in on those methods that\nperform better than the baseline method\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~21 words/sentence)\nbigram model\nchurch-gale\ninterp-del-int\nkatz\ninterp-held-out\nnew-avg-count new-one-count\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~21 words/sentence)\ntrigram model\ninterp-del-int\nkatz\ninterp-held-out\nnew-avg-count\nnew-one-count\nFigure 2.9: Bigram and trigram models on Brown corpus; relat ive performance of various\nmethods with respect to baseline\n37\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000 100000 1e+06\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nbigram model\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nchurch-gale\nchurch-gale\n-0.18\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n0.04\n100 1000 10000 100000 1e+06\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\ntrigram model\nnew-one-count\ninterp-held-out\nnew-avg-count\ninterp-del-int\nkatz\nkatz\nFigure 2.10: Bigram and trigram models on Wall Street Journa l corpus; relative performance\nof various methods with respect to baseline\nrepresent the empirical standard deviation over these runs . Due to resource limitations,\nwe only performed multiple runs for data sets of 50,000 sente nces or less. Each point on\nthe graphs on the right represents a single run, but we consid er training set sizes up to\nthe amount of data available, e.g., up to 250M words on TIPSTER. The graphs on the\nbottom of Figures 2.7–2.8 are close-ups of the graphs above, focusing on those algorithms\nthat perform better than the baseline. We ran interp-del-int only on training sets up\nto 50,000 sentences due to time constraints. To give an idea o f how these cross-entropy\ndiﬀerences translate to perplexity, each 0.014 bits corresp ond roughly to a 1% change in\nperplexity.\nFrom these graphs, we see that additive smoothing performs p oorly and that the meth-\nods katz and interp-held-out consistently perform well, with katz performing the best\nof all algorithms on small bigram training sets. The impleme ntation church-gale performs\npoorly except on large bigram training sets, where it perfor ms the best. The novel meth-\nods new-avg-count and new-one-count perform well uniformly across training data sizes,\nand are superior for trigram models. Notice that while perfo rmance is relatively consistent\nacross corpora, it varies widely with respect to training se t size and n-gram order.\n2.5.2 Count-by-Count Analysis\nTo paint a more detailed picture of performance, we consider the performance of diﬀerent\nmodels on only those n-grams in the test data that have exactly r counts in the training\ndata, for small values of r. This analysis provides information as to whether a model as signs\nthe correct amount of probability to categories such as n-grams with zero counts, n-grams\nwith low counts, or n-grams with high counts. In these experiments, we use about 1 0 million\nwords of test data.\nFirst, we consider whether various smoothing methods assig n on average the correct\ndiscounted count r∗ for a given count r. The discounted count r∗ generally varies for\ndiﬀerent n-grams; we only consider its average value here. (Recall tha t the corrected count\n38\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35\naverage corrected count\noriginal count\nbigram model\npredicted by ML\nactual\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35\naverage corrected count\noriginal count\ntrigram model\npredicted by ML\nactual\nFigure 2.11: Average corrected counts for bigram and trigra m models, 1M words training\ndata\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35\naverage corrected count\noriginal count\nbigram model\npredicted by ML\nactual\n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35\naverage corrected count\noriginal count\ntrigram model\npredicted by ML\nactual\nFigure 2.12: Average corrected counts for bigram and trigra m models, 200M words training\ndata\n39\nof an n-gram is proportional to the probability assigned by a model to that n-gram; in\nparticular, in this discussion we assume that the probabili ty assigned to an n-gram wi\ni−n+1\nwith r counts is just its normalized corrected count r∗\nN , where the normalization constant\nN is equal to the original total count ∑\nwi c(wi\ni−n+1).) To calculate how closely a model\ncomes to assigning the correct average r∗, we compare the expected value of the number of\ntimes n-grams with r counts occur in the test data with the actual number of times t hese\nn-grams occur. When the expected and actual counts agree, thi s corresponds to assigning\nthe correct average value of r∗.\nWe can estimate the actual correct average r∗\n0 for a given count r by using the following\nformula:\nr∗\n0 = r × actual number of n-grams with r counts in the test data\nexpected number according to the maximum likelihood model\nThe maximum likelihood model represents the case where we ta ke the corrected count to\njust be the original count. In Figure 2.11, we display the des ired average corrected count\nfor each count less than 40, for 1M words of training data from TIPSTER.\nThe last point in the graph corresponds to the average discou nt for that count and all\nhigher counts. (This property holds for later graphs, that t he last point corresponds to that\ncount and all higher counts.) The solid line corresponds to t he maximum likelihood model\nwhere the corrected count is taken to be equal to the original count. In Figure 2.12, we\ndisplay the same graph except for 200M words of training data from TIPSTER.\nIn Figures 2.13 and 2.14, we display how close various smooth ing methods came to\nthe desired average corrected count, again using 1M and 200M words of training data\nfrom TIPSTER. For each model, we graph the ratio of the actual average corrected count\nassigned by the model to the ideal average corrected count. F or the zero count case, we\nexclude those n-grams wi\ni−n+1 that occur in distributions that have a total of zero counts,\ni.e., ∑\nwi c(wi\ni−n+1) = 0. For these n-grams, the corrected count should be zero since the\ntotal count is zero. (These n-grams are also excluded in later graphs.)\nWe see that all of the algorithms tested tend to assign slight ly too little probability\nto n-grams with zero counts, and signiﬁcantly too much probabil ity to n-grams with one\ncount. For high counts, the algorithms tend to assign counts closer to the correct average\ncount on the larger training set than on the smaller training set. This eﬀect did not hold\nfor low counts.\nThe algorithms katz and church-gale consistently come closest to assigning the correct\naverage amount of probability to larger counts, with katz doing especially well. Thus, we\nconclude that the Good-Turing estimate is a useful tool for a ccurately estimating the desired\naverage corrected count, as both Katz and Church-Gale smoot hing use this estimate. 14\nIn contrast, we see that methods involving linear interpola tion are not as accurate on\nlarge counts, overdiscounting large counts in three of the e xperiments, and underdiscounting\nlarge counts in the 1M word bigram experiment. Roughly speak ing, linear interpolation\ncorresponds to linear discounting; that is, a corrected cou nt r∗ is about λ times the original\n14This only applies to Katz if a large k is used, as counts above k are not discounted.\n40\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n0 5 10 15 20 25 30 35\nexpected count / actual count\ncount\nbigram model\ninterp-baseline\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n0 5 10 15 20 25 30 35\nexpected count / actual count\ncount\ntrigram model\ninterp-baseline\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\nFigure 2.13: Expected over actual counts for various algori thms, bigram and trigram models,\n1M words training data\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n0 5 10 15 20 25 30 35\nexpected count / actual count\ncount\nbigram model\ninterp-baseline\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n0 5 10 15 20 25 30 35\nexpected count / actual count\ncount\ntrigram model\ninterp-baseline\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\nFigure 2.14: Expected over actual counts for various algori thms, bigram and trigram models,\n200M words training data\n41\n-0.25\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0 5 10 15 20 25 30 35\ndifference in test cross-entropy from baseline (bits/token)\ncount\nbigram model\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\n-0.4\n-0.3\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0 5 10 15 20 25 30 35\ndifference in test cross-entropy from baseline (bits/token)\ncount\ntrigram model\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\nFigure 2.15: Relative performance at each count for various algorithms, bigram and trigram\nmodels, 1M words training data\n-0.8\n-0.7\n-0.6\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0\n0.1\n0 5 10 15 20 25 30 35\ndifference in test cross-entropy from baseline (bits/token)\ncount\nbigram model\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\n-0.4\n-0.3\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\n0 5 10 15 20 25 30 35\ndifference in test cross-entropy from baseline (bits/token)\ncount\ntrigram model\nchurch-gale\ninterp-held-out\nkatz\nnew-avg-count\nnew-one-count\nFigure 2.16: Relative performance at each count for various algorithms, bigram and trigram\nmodels, 200M words training data\ncount r in Jelinek-Mercer smoothing. Referring to Figures 2.11 and 2.12, it is clear that the\ndesired average corrected count is not a constant multiplie d by the original count; a more\naccurate description is ﬁxed discounting , that the corrected count is the original count less\na constant.\nThe above analysis only considers whether an algorithm yiel ds the desired average cor-\nrected count; it does not provide insight into whether an alg orithm varies the corrected\ncount in diﬀerent distributions in a felicitous manner. For e xample, the Good-Turing esti-\nmate predicts that one should assign a total probability of n1\nN to n-grams with zero counts;\nobviously, this value varies from distribution to distribu tion. In Figures 2.15 and 2.16, we\ndisplay a measure that we call bang-for-the-buck that reﬂects how well a smoothing algo-\nrithm varies the corrected count r∗ of a given count r in diﬀerent distributions. To explain\nthis measure, we ﬁrst consider the measure of just taking the total entropy assigned in the\ntest data to n-grams with a given count r. Presumably, the smaller the entropy assigned\n42\n0\n0.2\n0.4\n0.6\n0.8\n1\n100 1000 10000 100000 1e+06 1e+07\ncumulative fraction of total entropy\nsentences of training data (~25 words/sentence)\nbigram model\nr=0\nr<=1\nr<=2\nr<=5\nr<=10\nr<=15\nr<=inf\n0\n0.2\n0.4\n0.6\n0.8\n1\n100 1000 10000 100000 1e+06 1e+07\ncumulative fraction of total entropy\nsentences of training data (~25 words/sentence)\ntrigram model\nr=0\nr<=1\nr<=2 r<=5\nr<=10\nr<=15\nr<=inf\nFigure 2.17: Fraction of entropy devoted to various counts o ver many training sizes, baseline\nsmoothing, bigram and trigram models\nto these n-grams, the better a smoothing algorithm is at estimating th ese corrected counts.\nHowever, an algorithm that assigns a higher average correct ed count will tend to have a\nlower entropy. We want to factor out this eﬀect, and we do this b y normalizing the average\ncorrected count of each algorithm to the same value before ca lculating the entropy. We\ncall this measure bang-for-the-buck as it reﬂects the relative performance of each algorithm\ngiven that they all assign the same amount of probability to a given count. In Figures 2.15\nand 2.16, we display the bang-for-the-buck per word of vario us algorithms relative to the\nbaseline method; as this score is an entropy value, the lower the score, the better.\nFor larger counts, Katz and Church-Gale yield superior bang -for-the-buck. We hypoth-\nesize that this is because the linear discounting used by oth er methods is a poor way to\ndiscount large counts. On small nonzero counts, Katz smooth ing does relatively poorly.\nWe hypothesize that this is because Katz smoothing does not p erform any interpolation\nwith lower-order models for these counts, while other metho ds do. It seems likely that\nlower-order models still provide useful information if cou nts are low but nonzero. The best\nmethods for modeling zero counts are our two novel methods.\nThe method church-gale performs especially poorly on zero counts in trigram models .\nThis can be attributed to the implementation choice discuss ed in Section 15. We chose to\nimplement a version of the algorithm analogous to interpola ting the trigram model directly\nwith a unigram model, as opposed to a version analogous to int erpolating the trigram model\nwith a bigram model. (As discussed, it is unclear whether the latter version is practical.)\nGiven the above analysis, it is relevant to note what fractio n of the total entropy of\nthe test data is associated with n-grams of diﬀerent counts. In Figure 2.17, we display\nthis information for diﬀerent training set sizes for bigram a nd trigram models. A line\nlabelled r ≤ k graphs the fraction of the entropy devoted to n-grams with up to k counts.\nFor instance, the region below the lowest line is the fractio n of the entropy devoted to zero\ncounts (excluding those zero counts that occur in distribut ions with a total of zero counts; as\nmentioned before we treat these n-grams separately). The fraction of the entropy devoted\n43\n0\n2\n4\n6\n8\n10\n12\n14\n0 2 4 6 8 10 12 14 16 18\ndesired counts devoted to zero count n-grams\nN = total counts in distribution\nbigram model\nn_1=0\nn_1=1n_1=2n_1=4\nn_1=6\nn_1=8\n0\n2\n4\n6\n8\n10\n12\n14\n0 2 4 6 8 10 12 14 16 18\ndesired counts devoted to zero count n-grams\nN = total counts in distribution\ntrigram model\nn_1=0\nn_1=1\nn_1=2\nn_1=4\nn_1=6\nn_1=8\nFigure 2.18: Average count assigned to n-grams with zero count for various n1 and N,\nactual, bigram and trigram models\n0\n2\n4\n6\n8\n10\n12\n14\n0 2 4 6 8 10 12 14 16 18\ndesired counts devoted to zero count n-grams\nN = total counts in distribution\nn_1=0\nn_1=1\nn_1=2\nn_1=4\nn_1=6\nn_1=8\nFigure 2.19: Average count assigned to n-grams with zero count for various n1 and N,\npredicted by Good-Turing\nto zero count n-grams occurring in zero count distributions is represente d by the region\nabove the top line in the graph.\nThis data explains some of the variation in the relative perf ormance of diﬀerent algo-\nrithms over diﬀerent training set sizes and between bigram an d trigram models. Our novel\nmethods get most of their performance gain relative to other methods from their perfor-\nmance on zero counts. Because zero counts are more frequent i n trigram models, our novel\nmethods perform especially well on these models. Furthermo re, because zero counts are less\nfrequent in large training sets, our methods do not do as well from a relative perspective on\nlarger data. On the other hand, Katz smoothing and Church-Ga le smoothing do especially\nwell on large counts. Thus, they yield better performance on bigram models and on large\ntraining sets.\n44\n2.5.3 Accuracy of the Good-Turing Estimate for Zero Counts\nBecause the Good-Turing estimate is a fundamental tool in sm oothing, it is interesting to\ntest its accuracy empirically. In this section, we describe experiments investigating how well\nthe Good-Turing estimate assigns probabilities to n-grams with zero counts in conditional\nbigram and trigram distributions. We consider zero counts i n particular because zero-count\nn-grams contribute a very sizable fraction of the total entro py, as shown in Figure 2.17.\nThe Good-Turing estimate predicts that the total probabili ty assigned to n-grams with\nzero counts should be n1\nN , the number of one-counts in a distribution divided by the to tal\nnumber of counts in the distribution. In terms of corrected c ounts, this corresponds to\nassigning a total of n1 counts to n-grams with zero counts. 15 We can calculate the desired\naverage corrected count for a given n1 and N by using a similar analysis as in Section 2.5.2,\ncomparing the expected number and actual number of zero-cou nt n-grams in test data. In\nFigure 2.18, we display the desired total number of correcte d counts assigned to zero counts\nfor various values of n1 and N, for 1M words of TIPSTER training data.\nEach line in the graph corresponds to a diﬀerent value of n1. The x-axis corresponds to\nN, and the y-axis corresponds to the desired count to assign to n-grams with zero counts.\nIf the Good-Turing estimate were exactly accurate, then we w ould have a horizontal line\nfor each n1 at the level y = n1, as displayed in Figure 2.19. However, we see that the lines\nare not very horizontal for smaller N, and that asymptotically they seem to level out at a\nvalue signiﬁcantly larger than n1.\nWe hypothesize that this is because the assumption made by Go od-Turing that suc-\ncessive n-grams are independent is incorrect. The derivation of the G ood-Turing estimate\nrelies on the observation that for an event with probability p, the probability that it will\noccur r times in N trials is\n(N\nr\n)\npr(1 − p)N−r. However, this only holds if each trial is inde-\npendent. Clearly, language has decidedly clumpy behavior. For example, a given word has\na higher chance of occurring given that it has occurred recen tly.16 Thus, the actual number\nof one-counts is probably lower than what would be expected i f independence were to hold,\nso the probability given to zero counts should be larger than n1\nN .\n2.5.4 Church-Gale Smoothing versus Linear Interpolation\nChurch-Gale smoothing incorporates the information from l ower-order models into higher-\norder models through its bucketing mechanism, unlike other smoothing methods that use\nlinear interpolation. In this section, we present empirica l results on how these two diﬀerent\ntechniques compare.\nFirst, we compare how Church-Gale smoothing and linear inte rpolation assign corrected\ncounts to zero counts. In Figure 2.20, we present the correct ed count of zero counts for\neach bucket in a Church-Gale run. In linear interpolation, t he corrected count of an n-\ngram with zero counts is proportional to the probability ass igned to that n-gram in the\n15As in Section 2.3.1, we use n1 and N to refer to counts in a conditional distribution p(wi|wi−1\ni−n+1) for a\nﬁxed wi−1\ni−n+1, as opposed to counts in the global n-gram distribution.\n16This observation is taken advantage of in dynamic language modeling (Kuhn, 1988; Rosenfeld and\nHuang, 1992; Rosenfeld, 1994a).\n45\n1e-06\n1e-05\n0.0001\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n1e-13 1e-12 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05\ncorrected count\nbucketing value\nbigram model\nshape predicted by linear interpolation\nchurch-gale\n1e-07\n1e-06\n1e-05\n0.0001\n0.001\n0.01\n0.1\n1\n10\n100\n1000\n1e-14 1e-13 1e-12 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05\ncorrected count\nbucketing value\ntrigram model\nshape predicted by linear interpolation\nchurch-gale\nFigure 2.20: Corrected count assigned to zero counts by Chur ch-Gale for all buckets, bigram\nand trigram models\n0\n2\n4\n6\n8\n10\n1e-13 1e-12 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05\ncorrected count\nbucketing value\nbigram model\nr=1\nr=2\nr=3\nr=5\nr=10\nshape predicted by linear interpolation\n0\n2\n4\n6\n8\n10\n1e-14 1e-13 1e-12 1e-11 1e-10 1e-09 1e-08 1e-07 1e-06 1e-05\ncorrected count\nbucketing value\ntrigram model\nr=1\nr=2\nr=3\nr=5\nr=10\nshape predicted by linear interpolation\nFigure 2.21: Corrected count assigned to various counts by C hurch-Gale for all buckets,\nbigram and trigram models\n46\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\nbigram model\ninterp-held-out\ninterp-del-int\n-0.16\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\n100 1000 10000\ndifference in test cross-entropy from baseline (bits/token)\nsentences of training data (~25 words/sentence)\ntrigram model\ninterp-held-out\ninterp-del-int\nFigure 2.22: Held-out versus deleted interpolation on TIPS TER data, relative performance\nwith respect to baseline, bigram and trigram models\nnext lower-order model. The x-axis of the graph represents the value used for bucketing in\nChurch-Gale, which is proportional to the probability assi gned to an n-gram by the next\nlower-order model. Thus, if Church-Gale smoothing assigns corrected counts to n-grams\nwith zero counts similarly to linear interpolation, the gra ph will be a line with slope 1 (given\nthat both axes are logarithmic). The actual graph is not far f rom this situation; the solid\nlines in the graphs are lines with slope 1. Thus, even though C hurch-Gale smoothing is\nvery far removed from linear interpolation on the surface, f or zero counts their behaviors\nare rather similar.\nIn Figure 2.21, we display the corrected counts for Church-G ale smoothing for n-grams\nwith larger counts. For these counts, the curves are very diﬀe rent from what would be\nyielded with linear interpolation. (The shapes of the curve s consistent with linear inter-\npolation are diﬀerent from those found in the previous ﬁgure b ecause the y-axis is linear\ninstead of logarithmic scale.)\n2.5.5 Held-out versus Deleted Interpolation\nIn this section, we compare the held-out and deleted interpo lation variations of Jelinek-\nMercer smoothing. Referring to Figure 2.22, we notice that t he method interp-del-int\nperforms signiﬁcantly worse than interp-held-out on TIPSTER data, though they diﬀer\nonly in that the former method uses deleted interpolation wh ile the latter method uses\nheld-out interpolation. Similar results hold for the other corpora, as shown in the earlier\nFigures 2.7–2.10.\nHowever, the implementation interp-del-int does not completely characterize the\ntechnique of deleted interpolation as we do not vary the size of the chunks that are deleted.\nIn particular, we made the choice of deleting only a single wo rd at a time for implementation\nease; we hypothesize that deleting larger chunks would lead to more similar performance to\ninterp-held-out.\nAs mentioned earlier, language tends to have clumpy behavio r. Held-out data external\n47\nto the training data will tend to be more diﬀerent from the trai ning data than data that is\ndeleted from the middle of the training data. As our evaluati on test data is also external\nto the training data (as is the case in applications), λ’s trained from held-out data should\nbetter characterize the evaluation test data. However, the larger the chunks deleted in\ndeleted interpolation, the more the deleted data behaves li ke held-out data. For example,\nif we delete half of the data at a time, this is very similar to t he held-out data situation.\nThus, larger chunks should yield better performance than th at achieved by deleting one\nword at a time.\nHowever, for large training sets the computational expense of deleted interpolation be-\ncomes a factor. In particular, the computation required is l inear in the training data size.\nFor held-out interpolation, the computation is linear in th e size of the held-out data. Be-\ncause there are relatively few λ’s, these parameters can be trained reliably using a fairly\nsmall amount of data. Furthermore, for large training sets i t matters little that held-out\ninterpolation requires some data to be reserved for trainin g λ’s while in deleted interpola-\ntion no data needs to be reserved for this purpose. Thus, for l arge training sets, held-out\ninterpolation seems the sensible choice.\n2.6 Discussion\nSmoothing is a fundamental technique for statistical model ing, important not only for lan-\nguage modeling but for many other applications as well, e.g., prepositional phrase attach-\nment (Collins and Brooks, 1995), part-of-speech tagging (C hurch, 1988), and stochastic\nparsing (Magerman, 1994). Whenever data sparsity is an issu e (and it always is), smooth-\ning has the potential to improve performance with moderate e ﬀort. Thus, thorough studies\nof smoothing can beneﬁt the research community a great deal.\nTo our knowledge, this is the ﬁrst empirical comparison of sm oothing techniques in\nlanguage modeling of such scope: no other study has systemat ically examined multiple\ntraining data sizes, corpora, or has performed parameter op timization. We show that in\norder to completely characterize the relative performance of two techniques, it is necessary\nto consider multiple training set sizes and to try both bigra m and trigram models. We show\nthat sub-optimal parameter selection can also signiﬁcantl y aﬀect relative performance.\nMultiple runs should be performed whenever possible to disc over whether any calculated\ndiﬀerences are statistically signiﬁcant; it is unclear whet her previously reported results in\nthe literature are reliable given that they are based on sing le runs and given the variances\nfound in this work. For example, we found that the standard de viation of the average\nperformance of Katz smoothing relative to the baseline meth od is about 0.005 bits for ten\nruns. Extrapolating to a single run, we expect a standard dev iation of about\n√\n10 ×0.005 ≈\n0.016 bits, which translates to about a 1% diﬀerence in perplexi ty. In the N´ adas and\nKatz papers, diﬀerences in perplexity between algorithms of about 1% are reported for a\nsingle test set of 100 sentences. MacKay and Peto present per plexity diﬀerences between\nalgorithms of signiﬁcantly less than 1%.\nOf the techniques studied, we have found that Katz smoothing performs best for bigram\nmodels produced from small training sets, while Church-Gal e performs best for bigram\n48\nmodels produced from large training sets. This is a new resul t; Church-Gale smoothing has\nnever previously been empirically compared with any of the p opular smoothing techniques\nfor language modeling. Our novel methods average-count and one-count are superior for\ntrigram models and perform well in bigram models; method one-count yields marginally\nworse performance but is extremely easy to implement.\nFurthermore, we provide a count-by-count analysis of the pe rformance of diﬀerent\nsmoothing techniques. By analyzing how frequently diﬀerent counts occur in a given do-\nmain, we can make rough predictions on the relative performa nce of diﬀerent algorithms\nin that domain. For example, this analysis lends insight int o how diﬀerent algorithms will\nperform on training sizes and n-gram orders other than those we tested.\nHowever, it is extremely important to note that in this work p erformance is measured\nsolely through the cross-entropy of test data. This choice w as made because it is fairly in-\nexpensive to evaluate cross-entropy, which enabled us to ru n experiments of such scale. Yet\nit is unclear how entropy diﬀerences translate to diﬀerences i n performance in real-world\napplications such as speech recognition. While entropy gen erally correlates with perfor-\nmance in applications, small diﬀerences in entropy have an un predictable eﬀect; sometimes\na reduction in entropy can lead to an increase in application error-rate, e.g., as reported\nby Iyer et al. (1994). In other words, entropy by no means completely chara cterizes appli-\ncation performance. Furthermore, it is not unlikely that re lative smoothing performance\nresults found in one application will not translate to other applications. Thus, to accu-\nrately estimate the eﬀect of smoothing in a given application , it is probably necessary to\nrun experiments using that particular application.\nHowever, we can guess how smoothing might aﬀect application p erformance by extrap-\nolating from existing results. For example, Isotani and Mat sunaga (1994) present the error\nrate of a speech recognition system using three diﬀerent lang uage models. As they also\nreport the entropies of these models, we can linearly extrap olate to estimate how much the\ndiﬀerences in entropy typically found between smoothing met hods aﬀect speech recognition\nperformance. In Table 2.4, we list typical entropy diﬀerence s found between smoothing\nmethods, where the “best” methods refer to interp-held-out, katz, new-avg-count, and\nnew-one-count. We also display how these entropy diﬀerences aﬀect applicati on perfor-\nmance as extrapolated from the Isotani data. The row labelle d original lists the error rate\nof the model tested by Isotani and Matsunaga with the highest entropy; the lower rows list\nthe extrapolated error rate if the model entropy were decrea sed by the prescribed amount.\nIn Table 2.4, we also display a similar analysis using data gi ven by Rosenfeld (1994b). This\nanalysis suggests that smoothing does not matter much as lon g as one uses a “good” imple-\nmentation of one of the better algorithms, e.g., those algorithms that perform signiﬁcantly\nbetter than the baseline; it is more likely that the diﬀerence s between the best and worst\nalgorithms are signiﬁcant.\nWe have found that it is surprisingly diﬃcult to design a “goo d” implementation of\nan existing algorithm. Given the description of our impleme ntations, it is clear that there\nare usually many choices that need to be made in implementing a given algorithm; most\nsmoothing techniques are incompletely speciﬁed in the lite rature. For example, as pointed\nout in Section 2.4.1, in certain cases Katz smoothing as orig inally described can assign\n49\nIsotani and Matsunaga\nentropy sentence error rate decrease in error rate\noriginal 48.7\n-0.05 bits 48.2 1.0%\n-0.10 bits 47.6 2.3%\n-0.15 bits 46.7 4.1%\nRosenfeld\nentropy word error rate decrease in error rate\noriginal 19.9\n-0.05 bits 19.7 1.0%\n-0.10 bits 19.5 2.0%\n-0.15 bits 19.3 3.0%\n0.05 bits ≥ typical entropy diﬀerence between best methods\n0.10 bits ≈ maximum entropy diﬀerence between best methods\n0.15 bits ≈ typical entropy diﬀerence between best methods and baseline method\nTable 2.4: Eﬀect on speech recognition performance of typica l entropy diﬀerences found\nbetween smoothing methods\nprobabilities of zero, which is undesirable as this leads to an inﬁnite entropy. We needed to\nperform a fair amount of tuning for each algorithm before we g uessed our implementation\nwas a reasonable representative of the algorithm. Poor choi ces often led to very signiﬁcant\ndiﬀerences in performance.\nFinally, we point out that because of the variation in the per formance of diﬀerent\nsmoothing methods and the variation in the performance of di ﬀerent implementations of\nthe same smoothing method ( e.g., from parameter setting), it is vital to specify the ex-\nact smoothing technique and implementation of that techniq ue used when referencing the\nperformance of an n-gram model. For example, the Katz and N´ adas papers describ e com-\nparisons of their algorithms with “Jelinek-Mercer” smooth ing, but they do not specify the\nbucketing scheme used or the granularity used in deleted int erpolation. Without this in-\nformation, it is impossible to determine whether their comp arisons are meaningful. More\ngenerally, there has been much work comparing the performan ce of various models with\nthat of n-gram models where the type of smoothing used is not speciﬁed , e.g., work by\nMcCandless and Glass (1993) and Carroll (1995). Again, with out this information we can-\nnot tell if the comparisons are signiﬁcant.\n2.6.1 Future Work\nPerhaps the most important work that needs to be done is to see how diﬀerent smoothing\ntechniques perform in actual applications. This would reve al how entropy diﬀerences relate\n50\nto performance in diﬀerent applications, and would indicate whether it is worthwhile to\ncontinue work in smoothing, given the largest entropy diﬀere nces we are likely to achieve.\nAlso, as mentioned before smoothing is used in other languag e tasks such as prepositional\nphrase attachment, part-of-speech tagging, and stochasti c parsing. It would be interesting\nto see whether our results extend to domains other than langu age modeling.\nSome smoothing algorithms that we did not consider that woul d be interesting to com-\npare against are those from the ﬁeld of data compression , which includes the subﬁeld of\ntext compression (Bell et al. , 1990). However, smoothing algorithms for data compressio n\nhave diﬀerent requirements from those used for language mode ling. In data compression, it\nis essential that smoothed models can be built extremely qui ckly and using a minimum of\nmemory. In language modeling, these requirements are not ne arly as strict.\nAs far as designing additional smoothing methods that surpa ss existing techniques, there\nwere many avenues that we did not pursue. Hybrid smoothing me thods look especially\npromising. As we found diﬀerent methods to be superior for big ram and trigram models,\nit may be advantageous to use diﬀerent smoothing methods in th e diﬀerent n-gram models\nthat are interpolated together. Furthermore, in our count- by-count analysis we found that\ndiﬀerent algorithms were superior on low versus high counts. Using diﬀerent algorithms for\nlow and high counts may be another way to improve performance .\n51\nChapter 3\nBayesian Grammar Induction for\nLanguage Modeling\nIn this chapter, we describe a corpus-based induction algor ithm for probabilistic context-free\ngrammars (Chen, 1995) that signiﬁcantly outperforms the gr ammar induction algorithm\nintroduced by Lari and Young (1990), the most widely-used al gorithm for probabilistic\ngrammar induction. In addition, it outperforms n-gram models on data generated with\nmedium-sized probabilistic context-free grammars, thoug h not on naturally-occurring data.\nOf the three structural levels at which we model language in t his thesis, this represents\nwork at the constituent level.\n3.1 Introduction\nWhile n-gram models currently yield the best performance in langua ge modeling, they\nseem to have obvious deﬁciencies. For instance, n-gram language models can only capture\ndependencies within an n-word window, where currently the largest practical n for natural\nlanguage is three, and many dependencies in natural languag e occur beyond a three-word\nwindow. In addition, n-gram models are extremely large, thus making them diﬃcult t o\nimplement eﬃciently in memory-constrained applications.\nAn appealing alternative is grammar-based language models . Grammar has long been\nthe representation of language used in linguistics and natu ral language processing, and\nintuitively such models capture properties of language tha t n-gram models cannot. For\nexample, it has been shown that grammatical language models can express long-distance\ndependencies (Lari and Young, 1990; Resnik, 1992; Schabes, 1992). Furthermore, grammat-\nical models have the potential to be more compact while achie ving equivalent performance\nas n-gram models (Brown et al. , 1992b). To demonstrate these points, we introduce the\ngrammar formalism we use, probabilistic context-free grammars (PCFG).\n3.1.1 Probabilistic Context-Free Grammars\nWe ﬁrst give a brief introduction to (non-probabilistic) co ntext-free grammars (Chomsky,\n1964). As mentioned in the introduction, grammars consist o f rules that describe how\n52\nstructures at one level of language combine to form structur es at the next higher level. For\nexample, consider the following grammar: 1\nS → NP VP\nVP → V NP\nNP → D N\nD → a | the\nN → boat | cat | tree\nV → hit | missed\nThis third through ﬁfth rules state that a noun phrase can be c omposed of a determiner\nfollowed by a noun, a determiner may be formed by the words a or the, and a noun may be\nformed by the words boat, cat, or tree. Thus, we have that strings such as a cat or the boat\nare noun phrases. Applying the other rules in the grammar, we see that strings such as a\nboat missed the tree or the cat hit the boat are sentences. Grammars provide a compact and\nelegant way for representing a set of strings.\nThe above grammar is considered context-free because there is a single symbol on the\nleft-hand side of each rule; there are grammar formalisms th at allow multiple symbols.\nThe symbols at the lowest level of the grammar such as a, hit, and tree are called the\nterminal symbols of the grammar. In all of the grammars we consider, the termin al symbols\ncorrespond to words. The other symbols in the grammar such as S, NP, and D are called\nnonterminal symbols.\nFor every grammar, a particular nonterminal symbol is chose n to be the sentential\nsymbol. The sentential symbol determines the set of strings the gra mmar is intended to\ndescribe; a grammar is said to accept a string if the string forms an instance of the sentential\nsymbol. For example, if in the above example we take the sente ntial symbol to be S, then\nthe grammar accepts the string a cat hit the tree but not the string the tree. The sentential\nsymbol is usually taken to be the symbol corresponding to the highest level of structure in\nthe grammar, and in language domains it usually corresponds to the linguistic concept of a\nsentence. In this work, we always name the sentential symbol S and it is always meant to\ncorrespond to a sentence (as opposed to a lower- or higher-le vel linguistic structure).\nA probabilistic context-free grammar (Solomonoﬀ, 1959) is a context-free gr ammar that\nnot just describes a set of strings, but also assigns probabi lities to these strings. 2 A prob-\nability is associated with each rule in the grammar, such tha t the sum of the probabilities\n1The following abbreviations are used in this work:\nS = sentence\nVP = verb phrase\nNP = noun phrase\nD = determiner\nN = noun\nV = verb\n2Actually, a probabilistic context-free grammar also assig ns probabilities to strings not accepted by the\ngrammar; this probability is just zero.\n53\nS\n✟✟✟\n❍❍❍\nNP\n✟✟ ❍❍\nD\na\nN\ncat\nVP\n✟✟ ❍❍\nV\nhit\nNP\n✟✟ ❍❍\nD\nthe\nN\ntree\nFigure 3.1: Parse tree for a cat hit the tree\nof all rules expanding a given symbol is equal to one. 3 This probability represents the\nfrequency with which the rule is applied to expand the symbol on its left-hand side. For\nexample, the following is a probabilistic context-free gra mmar:\nS → NP VP (1.0)\nVP → V NP (1.0)\nNP → D N (1.0)\nD → a (0.6)\nD → the (0.4)\nN → boat (0.5)\nN → cat (0.3)\nN → tree (0.2)\nV → hit (0.7)\nV → missed (0.3)\nTo explain how a probabilistic context-free grammar assign s probabilities to strings, we\nﬁrst need to describe how such a grammar assigns probabiliti es to parse trees. A parse tree\nof a string displays the grammar rules that are applied to for m the sentential symbol from\nthe string. For example, a parse tree of a cat hit the tree is displayed in Figure 3.1. Each\nnon-leaf node in the tree represents the application of a gra mmar rule. For instance, the\ntop node represents the application of the S → NP VP rule. The probability assigned to\na parse is simply the product of the probabilities associate d with each rule in the parse.\nThe probability assigned to the parse in Figure 3.1 is 0 .6 × 0.3 × 0.7 × 0.4 × 0.2 = 0 .01008,\nthe terms corresponding to the rules D → a, N → cat, V → hit, D → the, and N → tree,\nrespectively. All other rules used in the parse have probabi lity 1. The probability assigned\nto a string is the sum of the probabilities of all of its parses ; it is possible for a sentence to\nhave more than a single parse. 4\n54\nS\n✟✟✟\n❍❍❍\nthe Sthe\n✟✟✟ ❍❍❍\ndog Sdog\n✟✟ ❍❍\nbarks Sbarks\nweos\nFigure 3.2: Parse of the dog barks using a bigram-equivalent grammar\n3.1.2 Probabilistic Context-Free Grammars and n-Gram Models\nIn this section, we discuss the relationship between probab ilistic context-free grammars and\nn-gram models. First, we note that n-gram models are actually instances of probabilistic\ncontext-free grammars. For example, consider a bigram mode l with probabilities p(wi|wi−1).\nThis model can be expressed using a grammar with |T | + 1 nonterminal symbols, where T\nis the set of all terminal symbols, i.e., the set of all words. We have the sentential symbol\nS and a nonterminal symbol S w for each word w ∈ T . A symbol S w can be interpreted as\nrepresenting the state of having the word w immediately to the left. The grammar consists\nof all rules\nS → wi Swi (p(wi|wbos))\nSwi−1 → wi Swi (p(wi|wi−1))\nSwi−1 → weos (p(weos|wi−1))\nfor wi−1, wi ∈ T where wbos and weos are the beginning- and end-of-sentence tokens. The\nvalues in parentheses are the probabilities associated wit h each rule expressed in terms of\nthe probabilities of the corresponding bigram model. This g rammar assigns the identical\nprobabilities to strings as the original bigram model. For e xample, consider the sentence\nthe dog barks . The only parse of this sentence under the above grammar is di splayed in\nFigure 3.2. The probability of a parse is the product of the pr obabilities of each rule used,\nand going from top to bottom we get\np(the|wbos)p(dog|the)p(barks|dog)p(weos|barks)\nwhich is identical to the probability assigned by a bigram mo del.\nNot only can probabilistic context-free grammars model the same local dependencies as\nn-gram models, but they have the potential to model long-dist ances dependencies beyond\nthe scope of n-gram models. To demonstrate this, consider the sentence\nJohn read the boy a story.\nIn a trigram model, the word story is assumed to depend only on the phrase boy a. However,\n3This assures (except for some pathological cases) that the p robabilities assigned to strings sum to one.\n4A good introduction to probabilistic context-free grammar s has been written by Jelinek et al. (1992).\n55\nthere is a strong dependence between the words read and story. We can model this using\nthe following grammar fragment:\nSread → NPsubj\nread VPread\nVPread → Vread NPi-obj\nread NPd-obj\nread\nVread → read\nNPstory → Dstory Nstory\nNstory → story\nNPd-obj\nread → NPstory\nThe symbols with subscript read are symbols that we restrict to only occur in sentences with\nthe main verb read. The symbols with subscript story are symbols that we restrict to only\noccur in noun phrases whose head word is story. The superscripts on the NP’s represent\nthe diﬀerent roles a noun phrase can play in a sentence. The pro bability associated with the\nlast rule represents the probability that the word story is the head of the direct object of\nthe verb read; this captures the long-distance dependency present betwe en these two words.\nProbabilistic context-free grammars can express dependen cies between words arbitrarily far\napart.\nThus, we see that probabilistic context-free grammars are a more powerful formalism\nthan n-gram models, and thus have the potential for superior perfo rmance. Furthermore,\ngrammars also have the potential to be more compact than n-gram models while achieving\nequivalent performance, because grammars can express classing, or the grouping together\nof similar words. For example, consider the words corporal and sergeant. These words have\nvery similar bigram behaviors: for most words w we have p(w|corporal) ≈ p(w|sergeant) and\np(corporal|w) ≈ p(sergeant|w). However, in bigram models the probabilities associated w ith\nthese two words are estimated completely independently. In a grammatical representation,\nit is possible instead to introduce a symbol, say A, that corresponds to both words, i.e., to\nhave\nA → sergeant | corporal.\nWe can then have a single set of bigram probabilities p(w|A) and p(A|w) for the symbol\nA, instead of a separate set for each word. Notice that this doe s not preclude having some\nbigram probabilities speciﬁc to either corporal or sergeant, in the cases their behavior diﬀer.\nBecause grammars can class together similar words, equivalent performance to n-gram\nmodels can be achieved with much smaller models (Brown et al. , 1992b).\nNotice that when we use the term grammar, we are talking of its formal meaning,\ni.e., a collection of rules that describe how to build sentences f rom words. This contrasts\nwith the connotation of the term grammar in linguistics, of a representation that describes\nlinguistically meaningful concepts such as noun phrases an d verb phrases. The symbols\nin the grammars we consider do not generally have any relatio n to linguistic constituents.\nThus, the grammars we consider would not be applicable to the task of parsing for natural\nlanguage processing, where grammars are used to build struc ture useful for determining\n56\nsentence meaning. 5\nThere have been attempts to use linguistic grammars for lang uage modeling (Newell,\n1973; Woods et al. , 1976). However, such attempts have been unsuccessful. The se manually-\ndesigned grammars cannot approach the coverage achieved by algorithms that statistically\nanalyze millions of words of text. Furthermore, linguistic grammars are geared toward\ncompactly describing language; in language modeling the go al is to describe language in a\nprobabilistically accurate manner. Models with large numb ers of parameters like n-gram\nmodels are better suited to this task.\nIn this work, our goal was to design an algorithm that induces grammars somewhere\nbetween the rich grammars of linguistics and the ﬂat grammar s corresponding to n-gram\nmodels, grammars that have the structure for modeling long- distance dependencies as well\nas the size for modeling speciﬁc n-gram-like dependencies. In addition, we desired the\ngrammars to still be signiﬁcantly more compact than compara ble n-gram models.\nWe produced a grammar induction algorithm that largely sati sﬁed these goals. In ex-\nperiments, it signiﬁcantly outperforms the most widely-us ed grammar induction algorithm,\nthe Lari and Young algorithm, and on artiﬁcially-generated corpora it outperforms n-gram\nmodels. However, on naturally occurring data n-gram models are still superior. The algo-\nrithm induces a probabilistic context-free grammar throug h a greedy heuristic search within\na Bayesian framework, and it reﬁnes this grammar with a post- pass using the Inside-Outside\nalgorithm. The algorithm does not require the training data to be manually annotated in\nany way.6\n3.2 Grammar Induction as Search\nGrammar induction can be framed as a search problem, and has b een framed as such\nalmost without exception in past research (Angluin and Smit h, 1983). The search space is\ntaken to be some class of grammars; for example, in our work we search within the space of\nprobabilistic context-free grammars. We search for a gramm ar that optimizes some quantity,\nreferred to as the objective function. In grammar induction, the objective function generally\ncontains a factor that reﬂects how accurately the grammar mo dels the training data.\nMost work in language modeling, including n-gram models and the Inside-Outside al-\ngorithm, falls under the maximum likelihood paradigm. In this paradigm, the objective\nfunction is taken to be the likelihood or probability of the t raining data given the grammar.\n5 Probabilistic context-free grammars have been argued to be inappropriate for modeling natural language\nbecause they cannot model lexical dependencies as do n-gram models (Resnik, 1992; Schabes, 1992). As we\nhave shown that n-gram models are instances of probabilistic context-free g rammars, this is obviously not\nstrictly true. A more accurate statement is that the context -free grammars traditionally used for parsing\nare not appropriate for language modeling. These grammars t ypically have a small set of nonterminal\nsymbols, e.g., { S, NP, VP, . . . }, and grammars with few nonterminal symbols cannot express m any lexical\ndependencies. However, with expanded symbol sets it is poss ible to express these dependencies, e.g., as in\nthe example given earlier in this section where we qualify no nterminal symbols with their head words.\n6Some grammar induction algorithms require that the trainin g data be annotated with parse tree infor-\nmation (Pereira and Schabes, 1992; Magerman, 1994). Howeve r, these algorithms tend to be geared toward\nparsing instead of language modeling. It is expensive to man ually annotate data, and it is not practical to\nannotate the amount of data typically used in language model ing.\n57\nThat is, we try to ﬁnd the grammar\nG = arg max\nG\np(O|G)\nwhere O denotes the training data or observations. The probability of the training data is\nthe product of the probability of each sentence in the traini ng data, i.e.,\np(O|G) =\nn∏\ni=1\np(oi|G)\nif the training data O is composed of the sentences {o1, . . . , o n}. The probability of a\nsentence p(oi|G) is straightforward to calculate for a probabilistic gramm ar G.\nHowever, the optimal grammar under this objective function is one that generates only\nsentences in the training data and no other sentences. In par ticular, the optimal grammar\nconsists exactly of all rules of the form S → oi, each such rule having probability c(oi)/n\nwhere c(oi) is the number of times the sentence oi occurs in the training data. Obviously,\nthis grammar is a poor model of language at large even though i t assigns a high probability\nto the training data; this phenomenon is called overﬁtting the training data.\nIn n-gram models and work with the Inside-Outside algorithm (La ri and Young, 1990;\nLari and Young, 1991; Pereira and Schabes, 1992), this issue is evaded because all of the\nmodels considered are of a ﬁxed size, so that the “optimal” gr ammar cannot be expressed. 7\nHowever, in our work we do not wish to limit the size of the gram mars considered.\nWe can address this issue elegantly by using a Bayesian frame work instead of a maximum\nlikelihood framework. As touched on in Chapter 1, in the Baye sian framework one attempts\nto ﬁnd the grammar G with highest probability given the data p(G|O), as opposed to the\ngrammar that yields the highest probability of the data p(O|G) as in maximum likelihood.\nIntuitively, ﬁnding the most probable grammar is more corre ct than ﬁnding the grammar\nthat maximizes the probability of the data.\nLooking at the mathematics, in the Bayesian framework we try to ﬁnd\nG = arg max\nG\np(G|O).\nAs it is unclear how to estimate p(G|O) directly, we apply Bayes’ Rule and get\nG = arg max\nG\np(O|G)p(G)\np(O) = arg max\nG\np(O|G)p(G) (3.1)\nwhere p(G) denotes the prior probability of a grammar G. The prior probability p(G) is\nsupposed to reﬂect our a priori notion of how frequently the grammar G appears in the\ngiven domain.\nNotice that the Bayesian framework is equivalent to the maxi mum likelihood framework\nif we take p(G) to be a uniform distribution. However, it is mathematicall y improper to\n7As seen in Chapter 2, even though n-gram models cannot express the optimal grammar there is sti ll a\ngrave overﬁtting problem, which is addressed through smoot hing.\n58\nhave a uniform distribution over a countably inﬁnite set, su ch as the set of all context-free\ngrammars. We give an informal argument describing its mathe matical impossibility and\nrelate this to why the maximum likelihood approach tends to o verﬁt training data.\nConsider selecting a context-free grammar randomly using a uniform distribution over\nall context-free grammars. Now, let us deﬁne a size for each grammar; for example, we\ncan take the number of characters in the textual description of a grammar to be its size.\nThen, notice that for any value k, there is a zero probability of choosing a grammar of size\nless than k, since there are an inﬁnite number of grammars of size larger than k but only\na ﬁnite number of grammars smaller than k. Hence, in some sense the “average” grammar\naccording to the uniform distribution is inﬁnite in size, an d this relates to why a uniform\ndistribution is mathematically improper. In addition, thi s is related to why the maximum\nlikelihood approach prefers overlarge, overﬁtting gramma rs, as the uniform prior assigns far\ntoo much probability to large grammars.\nInstead, we argue that taking a minimum description length (MDL) principle (Rissanen,\n1978) prior is desirable. The minimum description length pr inciple states that one should\nselect a grammar G that minimizes the sum of l(G), the length of the description of the\ngrammar, and l(O|G), the length of the description of the data given the grammar . We will\nlater give a detailed description of what these lengths mean ; for now, suﬃce it to say that\nthis corresponds to taking a prior of the form\np(G) = 2 −l(G)\nwhere l(G) is the length of the grammar G in bits. For example, we can take l(G) to be\nthe length of a textual description of the grammar.\nIntuitively, this prior is appealing because it captures th e intuition behind Occam’s\nRazor, that simpler (or smaller) grammars are preferable ov er complex (or larger) grammars.\nClearly, the prior p(G) assigns higher probabilities to smaller grammars. Howeve r, this prior\nextends Occam’s Razor by providing a concrete way to trade oﬀ the size of a grammar with\nhow accurately the grammar models the data. In particular, w e try to ﬁnd the grammar\nthat maximizes p(G)p(O|G). The term p(G) favors grammars that are small, and the term\np(O|G) favors grammars that model the training data well.\nThis preference for small grammars over large addresses the problem of overﬁtting.\nThe optimal grammar under the maximum likelihood paradigm w ill be given a poor score\nbecause its prior probability p(G) will be very small, given its vast size. Instead, the optima l\ngrammar under MDL will be a compromise between size and model ing accuracy.\nBecause coding theory plays a key role in the future discussion, we digress at this p oint to\nintroduce some basic concepts in the ﬁeld. Coding theory for ms the basis of the descriptions\nused in the minimum description length principle, and it can be used to tie together the\nMDL principle with the Bayesian framework.\n3.2.1 Coding\nCoding can be thought of as the study of storing information compact ly. In particular, we\nare interested in representing information just using a bin ary alphabet, 0’s and 1’s, as is\n59\nnecessary for storing information in a computer. Coding jus t describes ways of mapping\ninformation into strings of binary digits or bits in a one-to -one manner, so that the original\ninformation can be reconstructed from the associated bit st ring.\nFor example, consider the task of coding the outcome of a coin ﬂip. A sensible code\nis to map tails to the bit string 0, and to map heads to the bit st ring 1 (or vice versa).\nAnother possibility is to map both heads and tails to 0. This i s an invalid code, because it is\nimpossible to reconstruct whether a coin ﬂip was heads or tai ls from the yielded bit string.\nIn general, distinct outcomes must be mapped to distinct bit strings; that is, mappings need\nto be one-to-one. Another possible code is to map heads to the bit string 00, and to map\ntails to the bit string 11. This is a valid code, but it is ineﬃc ient as it codes the information\nusing two bits when one will do.\nCodes can be used to store arbitrarily complex information. For example, the ASCII\nconvention maps letters of the alphabet to eight-bit string s. In this convention, the text\nhi would be mapped to the sixteen-bit string 0110100001101001 . By using the ASCII\nconvention, any data that can be expressed through text can b e mapped to bit strings.\nFor obvious reasons, coding theory is concerned with ﬁnding ways to code information\nwith as few bits as possible. For example, coding theory is at the core of the ﬁeld of data\ncompression. We now describe how to code data optimally, moving from simp le examples\nto more complex ones.\nFixed-Length Coding\nFirst, consider the case of coding the outcome of a single coi n ﬂip, which we showed earlier\ncan be coded using a single bit. There are two possible outcom es to a coin ﬂip, and there\nare two possible values for a single bit, so it is possible to ﬁ nd a one-to-one mapping from\noutcomes to bit values. Now, consider coding the outcome of k coin ﬂips. Intuitively, this\nshould be codable using k bits, and in fact it can. There are 2 k possible outcomes to k coin\nﬂips, and there are 2 k possible values of a k-bit string, so again we can ﬁnd a one-to-one\nmapping. In general, to code any information that has exactl y 2 k possible values, we need\nat most k bits. Alternatively, we can phrase this as: to code informat ion with n possible\nvalues, we need at most ⌈log2 n⌉ bits. For example, in New York Lotto, which involves\npicking 6 distinct numbers from the values 1 through 48, ther e are\n(48\n6\n)\n= 12 , 271, 512\npossible combinations. Then, we need at most ⌈log212, 271, 512⌉ = 24 bits to code a New\nYork Lotto ticket. Notice that in this discussion we ignore h ow diﬃcult it is to construct\nthe coder and decoder; to write a program that maps Lotto tick ets to and from distinct\n24-bit strings is not trivial. It usually possible to ﬁnd ine ﬃcient codes that are much easier\nto code and decode. For example, we could just store a Lotto ti cket as text using the ASCII\nconvention.\nVariable-Length Coding\nWhile the preceding analysis is optimal if we require that al l of the bit strings mapped to are\nof the same length, in most cases one can do better on average i f outcomes can be mapped\nto bit strings of diﬀerent lengths. In particular, if certain outcomes are more frequent than\n60\nothers, these should be mapped to shorter bit strings. While this may cause infrequent\nstrings to be mapped to longer bit strings than in a ﬁxed-leng th coding, this is more than\nmade up by the savings from shorter bit strings since the shor ter strings correspond to more\nfrequent outcomes. 8\nFor example, let us consider the coding of the information of which of three consecutive\ncoin ﬂips, if any, is the ﬁrst one to be heads. There are four po ssible outcomes: the ﬁrst\nﬂip, the second ﬂip, the third ﬂip, or none of them. Thus, we ca n code the outcome using\ntwo bits with a ﬁxed-length code. Now, let us consider a diﬀere nt coding, where we just use\nthree bits to code the outcome of each of the three coin ﬂips in order, using 0 to mean tails\nand 1 heads. This is a valid coding, since we can still recover which of the ﬂips yields the\nﬁrst head, but obviously this coding is less eﬃcient than the previous one because it uses\nthree bits instead of two. However, notice that in some cases some of the bits in this coding\nare superﬂuous. For example, if the ﬁrst bit is 1, then we know the earliest ﬂip to be heads\nis the ﬁrst one regardless of the later ﬂips, so there is no nee d to include the last two bits.\nLikewise, if the ﬁrst bit is 0 and the second bit is 1, we do not n eed to include the last bit\nbecause we know the earliest ﬂip to be heads is the second one. Instead of a ﬁxed-length\ncode, we can assign the bit strings 1, 01, 001, and 000 to the fo ur outcomes. Notice that\nthe probabilities of these four outcomes are 0.5, 0.25, 0.12 5, and 0.125, if the coin is fair.\nThus, on average we expect to use 0 .5 × 1 + 0 .25 × 2 + 0 .125 × 3 + 0 .125 × 3 = 1 .75 bits\nto code an outcome, taking into account the relative frequen cies of each outcome. This is\nsuperior to the average of two bits yielded by the optimal ﬁxe d-length code. 9\nIn general, if each outcome has a probability of the form 2 −k for k integer, then it\nis provably optimal to assign an outcome with probability 2 −k a bit string of length k.\nAlternatively phrased: given that the probabilities of all outcomes are of the form 2 −k, k ∈\nN , an outcome with probability p is optimally coded using log 2\n1\np bits. Thus, the code\ndescribed in the last example is optimal, as log 2\n1\n0.5 = 1, log 2\n1\n0.25 = 2, and log 2\n1\n0.125 = 3.\nNotice that in the case there are 2 k equiprobable outcomes, this formula just comes out to\na ﬁxed-length code of length k.\nNow, consider the case of coding probabilities that are not n egative powers of two. For\nexample, let us code the outcome of a single toss of a fair 3-si ded die. Intuitively, we\nwant to assign codeword lengths that are appropriate for pro babilities that are negative\npowers of two that are near to the actual probabilities of eac h outcome. In fact, there is\nan algorithm for performing this assignment in an optimal wa y, namely Huﬀman coding\n(Huﬀman, 1952). In this case, Huﬀman coding yields the codewor ds 0, 10, and 11. Clearly,\nthe codeword lengths do not follow the relation that an outco me with probability p has\n8 We see this principle followed in language: most common word s have short spellings, and long expres-\nsions that are used frequently in some context are often abbr eviated.\n9One may ask why we could not use an even shorter code, e.g., the bit strings 0, 1, 00, and 01. The reason\nis that it is required that codes are unambiguous even when us ed to code multiple trials consecutively. For\nexample, if we coded two of the above trials consecutively wi th this new code and yielded the bit string 000,\nwe would not be able to tell whether this should be interprete d as (0)(00) or (00)(0). One way to assure\nunambiguity is to require that no codeword is a preﬁx of anoth er, as is satisﬁed by the original code given\nin the example.\n61\ncodeword length log 2\n1\np as in this case these values are not integers. 10\nHowever, consider the case where instead of coding a single t oss, we are coding k tosses\nof a fair 3-sided die. Notice that there are 3 k possible outcomes, and as mentioned earlier\nwe can code this using ⌈log2 3k⌉ bits using a ﬁxed-length code. Hence, on average each coin\ntoss requires ⌈log2 3k⌉\nk = ⌈k log2 3⌉\nk bits. As k grows large, this approaches the value log 2 3. By\ncoding multiple outcomes jointly, we approach the limit whe re each individual outcome (of\nprobability p = 1\n3 ) can be coded on average using log2 1\np = log 2\n1\n1\n3\n= log23 bits; this is the\nsame relation we found when all probabilities were negative powers of two.\nThis result extends to the case where not all outcomes are equ iprobable; instead of ﬁxed-\nlength coding for the joint trials, we can use Huﬀman coding of the joint trials to approach\nthis limit. In general, if a particular outcome has probabil ity p, in the limit of coding a\nlarge number of trials, each of those outcomes will take on av erage log 2\n1\np bits to code in the\noptimal coding (Shannon, 1948; Cover and King, 1978). In fac t, this limit can be realized in\npractice with an eﬃcient algorithm called arithmetic coding (Pasco, 1976; Rissanen, 1976).\n3.2.2 Description Lengths\nNow, let us return to the minimum description length princip le and the meaning of a\ndescription. Recall that MDL states that one should minimize the sum of l(G), the length\nof the description of the grammar, and l(O|G), the length of the description of the data\ngiven the grammar. A description simply refers to the bit str ing that is used to code the\ngiven information. Notice that we do not care what the actual bit string that composes a\ndescription is; we are only concerned with its length.\nFirst, let us consider l(O|G). Typically, G is a probabilistic grammar that assigns\nprobabilities to sentences p(oi|G), and we can calculate the probability of the training\ndata as p(O|G) = ∏ n\ni=1 p(oi|G) where the training data O is composed of the sentences\n{oi, . . . , o n}. Then, using the result that an outcome with probability p can be coded\noptimally with log 2\n1\np bits, we get that taking l(O|G) to be log 2\n1\np(O|G) should yield the\nlowest lengths on average.\nNotice that for the MDL principle to be meaningful, we need to use an optimal coding\nas opposed to some arbitrary ineﬃcient coding. There are man y descriptions of a given\npiece of data. For example, for any description of some data g iven a grammar, we can\ncreate additional descriptions of the same data by just padd ing the end of the original\ndescription with 0’s. Clearly, it is easy to make descriptio ns arbitrarily long. However, it is\nnot possible to make descriptions arbitrarily compact. The re is a lower bound to the length\nof the description of any piece of data (Solomonoﬀ, 1960; Solo monoﬀ, 1964; Kolmogorov,\n1965), and we can use this lower bound to deﬁne a meaningful de scription length for a piece\nof data. This is why we choose an optimal coding for calculati ng l(O|G). This dictum\nof optimal coding extends as well to calculating l(G), the length of the description of a\n10However, Huﬀman coding does guarantee that on average outco mes are assigned codewords at most one\nbit longer than what is dictated by the log 2\n1\np relation. To see how this bound can be achieved simply, we\ncan just round down each probability to the next lower negati ve power of two, and assign codeword lengths\nas described earlier.\n62\ngrammar.\nThus, it is not appropriate to use textual descriptions of gr ammars as mentioned in Sec-\ntion 3.2, as this is rather ineﬃcient. For example, consider the following textual description\nof a grammar segment:\nNP->D N\nD->a|the\nN->boat|cat|tree\nThis textual description is 34 characters long (including c arriage returns), which translates\nto 272 bits under the ASCII convention of eight bits per chara cter. We can achieve a\nsigniﬁcantly smaller description using a more complex enco ding, where the grammar is\ncoded in three distinct sections:\n• We code the list of terminal symbols as text:\na the boat cat tree\nwhich comes to 20 characters including the carriage return.\n• We code the number of nonterminal symbols and the number of gr ammar rules also\nas text:\n3 3\nwhich comes to 4 characters including the carriage return. N otice that the names of\nnonterminal symbols are not relevant in describing a gramma r; these symbols can be\nrenamed arbitrarily without aﬀecting what strings the gramm ar generates.\n• Finally, we code the list of grammar rules, where each gramma r rule is coded in several\nparts:\n– The nonterminal symbol on the left-hand side of a rule can be c oded using two\nbits, as there are a total of three nonterminal symbols.\n– To code whether a rule is of the form A → a1a2 · · · or of the form A → a1|a2| · · · ,\nwe use a single bit. (In this example, we do not consider rules combining both\nforms.)\n– To code how many symbols are on the right-hand side of a rule, w e use three\nbits. With three bits we can code up to a length of eight; if a ru le is longer it\ncan be split into multiple rules.\n– To code each symbol on the right-hand side of a rule, we use thr ee bits to code\nwhich of the eight possible symbols it is (three nonterminal , ﬁve terminal).\nUnder this coding, the ﬁrst two rules each take 12 bits, and th e third takes 15 bits.\n63\nThis comes to a total of 24 characters for the ﬁrst two section s, or 192 bits under the\nASCII convention, and 39 bits for the last section, yielding a total of 231 bits. This is\nsigniﬁcantly less than the 272 bits of a naive textual encodi ng. Using more advanced tech-\nniques that will be described later, grammar descriptions c an be made even more compact.\nIn addition, the grammars we will be using later will be proba bilistic, so we will also have\nto code probability values.\nJust as we used p(O|G) to calculate l(O|G), we can use the prior probability p(G)\nmentioned in equation (3.1) to give us insight into l(G). According to coding theory, to\ncalculate the optimal length l(G) of a grammar G we need to know the probability of\nthe grammar p(G). An alternative approach to explicitly designing encodin gs like above\nis instead to design a prior probability p(G) and to deﬁne an encoding such that l(G) =\nlog2\n1\np(G) just as we did for l(O|G). However, unlike p(O|G) the distribution p(G) is not\nstraightforward to estimate. Furthermore, it is important to note that in order for a coding\nto be optimal ( i.e., produce the shortest descriptions on average), the underl ying probability\ndistribution must be accurate. For some distribution p(G), we know that using log 2\n1\np(G)\nbits to code a grammar G is optimal only if p(G) is the correct underlying distribution on\ngrammars.\nFor instance, consider the example given in Section 3.2.1 of coding which of three con-\nsecutive coin ﬂips is the ﬁrst to turn up heads. A ﬁxed-length code requires two bits\nto code this, and we showed that by assigning the codewords 1, 01, 001, and 000 to\nthe outcomes: ﬁrst ﬂip, second ﬂip, third ﬂip, and no ﬂip, res pectively, we can achieve\nan improved average of 1.75 bits, assuming the coin is fair. H owever, consider a biased\ncoin whose probability of heads is 1\n4 . Then, the frequencies of the four outcomes become\n1/4, 3/16, 9/64, and 27/64, respectively, and this yields an average codeword length of\n(1/4 × 1) + (3/16 × 2) + (9/64 × 3) + (27/64 × 3) = 2 .3125 bits, which is signiﬁcantly worse\nthan the ﬁxed-length code. Thus, we see that for a coding to be eﬃcient we must have an\naccurate model of the data.\nApplying this observation to coding grammars, we see that de riving the lengths l(G) of\ngrammars from a prior p(G) is no better than estimating l(G) directly; we have no guarantee\nthat the prior p(G) we choose is at all accurate. However, this relationship do es provide\nus with another perspective with which to view grammar encod ings. For every grammar\nencoding describing grammar lengths l(G) there is an associated prior p(G) = 2 −l(G), and\nwe should choose encodings that lead to priors p(G) that are good models of grammar\nfrequency. For example, for grammars G we perceive to be typical, i.e., to have high\nprobability p(G), we want l(G) to be low. In other words, we want typical grammars to\nhave short descriptions. Hence, referring to the two gramma r encodings given earlier, as\nthe latter grammar encoding assigns shorter descriptions t o typical grammars than the\nnaive encoding, 11 we conclude that in some sense the latter encoding correspon ds to a more\n11Actually, this is not clear. For smaller grammars, the compl ex encoding should be more eﬃcient since,\nfor example, it can code symbol identities using a small numb er of bits while in a text representation a symbol\nis represented using a minimum of one character, or eight bit s. For large grammars, text encodings may\nbe more eﬃcient since they can express variable-length enco dings of symbol identities, while the complex\nencoding assumes ﬁxed-length encodings of symbol identiti es.\n64\naccurate prior probability on grammars.\n3.2.3 The Minimum Description Length Principle\nAs touched on in the last section, the observation that an obj ect with probability p should\nbe coded using log 2\n1\np bits gives us a way to equate probabilities and description l engths, and\nthis is the key in showing the relation between the minimum de scription length principle\nand the Bayesian framework. Under the Bayesian framework, w e want to ﬁnd the grammar\nG = arg max\nG\np(O|G)p(G).\nUnder the minimum description length principle, we want to ﬁ nd the grammar\nG = arg min\nG\n[l(O|G) + l(G)].\nThen, we get that\nG = arg max\nG\np(O|G)p(G)\n= arg min\nG\n[− log2 p(O|G)p(G)]\n= arg min\nG\n[log2\n1\np(O|G) + log2\n1\np(G)]\n= arg min\nG\n[lp(O|G) + lp(G)]\nwhere lp(α) denotes the length of α under the optimal coding given p. Thus, any problem\nframed in the Bayesian framework can be converted to an equiv alent problem under MDL, by\njust taking the description lengths to be the optimal ones di ctated by the given probabilities.\nLikewise, any problem framed under MDL can be converted to an equivalent one in the\nBayesian framework, by choosing the probability distribut ions that would yield the given\ndescription lengths. For example, it is easy to see that the B ayesian prior corresponding to\nthe MDL principle is p(G) = 2 −l(G), as touched on earlier.\nThus, from a mathematical point of view, the minimum descrip tion length principle\ndoes not give us anything above the Bayesian framework. Howe ver, from a paradigmatic\nperspective, MDL provides two important ideas.\nFirstly, MDL gives us a new perspective for creating prior di stributions on grammars. By\nnoticing that any grammar encoding scheme implicitly descr ibes a probability distribution\np(G) = 2 −l(G), we can create priors by just designing encoding schemes. Fo r example,\nboth of the grammar encoding schemes used in Section 3.2.2 le ad to prior distributions\nrather diﬀerent from those usually found in probability theo ry. Viewing prior distributions\nin terms of encodings extends the toolbox one has for designi ng prior distributions. In\naddition, one can mix and match conventional prior distribu tions from probability theory\nwith those stemming from an encoding perspective.\nSecondly, it has been observed that “MDL-style” priors of th e form p(G) = 2 −l(G) can be\n65\ngood models of the real world (Solomonoﬀ, 1964; Rissanen, 197 8; Li and Vit´ anyi, 1993). 12\nTo demonstrate this, let us consider some examples of real-w orld data. Let us say you see\none hundred ﬂips of a coin, and each time it turns up heads. Cle arly, you expect a head with\nvery high probability on the next toss. 13 Or, let us say you peek at someone’s computer\nterminal and see the following numbers output: 2, 3, 5, 7, . . . , 83, 89. Then, you expect\nthe next number to be output to be 97 with very high probabilit y. Or, let us say you look\nat some text and notice that after each of the ten occurrences of the word Gizzard’s the\nword Gulch appears immediately afterwards. Then, if you see the word Gizzard’s again\nyou expect the word Gulch will follow with high probability. In general, when you noti ce a\npattern in some data in the real world, you expect the pattern to continue in later samples\nof the same type of data.\nThis behavior can be captured with an MDL-style prior. In par ticular, we can capture\nthis behavior by choosing a prior that assigns high probabil ities to data that can be described\nwith short programs. By programs, we mean programs written in a computer language such\nas Pascal or Lisp. For example, let us take our programming la nguage to be a Pascal-like\npseudo-code. Now, consider estimating the probability tha t a coin turns up heads on the\nnext toss, given that all hundred previous tosses of the coin yielded heads. That is, we want\nto estimate\np(h|100 h’s) = p(100 h’s, h)∑\nx={h,t} p(100 h’s, x) = p(101 h’s)\np(101 h’s) +p(100 h’s, t).\nIntuitively, this probability should be high, so we want\np(101h’s)> p (100h’s, 1t).\nA program that outputs 101 h’s is signiﬁcantly shorter than a program that outputs 100\nh’s and a t. For example, for the former we might have\nfor i := 1 to 101 do\nprint \"h\";\nwhile for the latter we might have\nfor i := 1 to 100 do\nprint \"h\";\nprint \"t\";\nThus, by assigning higher probabilities to data that can be g enerated with shorter programs,\nwe get the desired behavior on this example.\n12Closely related to the minimum description length principl e is the universal a priori probability. The\nuniversal a priori probability can be shown to dominate all enumerable prior di stributions by a constant. The\nminimum description length principle can be thought of as a s impliﬁcation of this elegant but incomputable\nuniversal distribution. A thorough discussion of this topi c is given by Li and Vit´ anyi (1993).\n13 If you knew the coin was fair, then you would still expect the next toss to be heads with probability\n0.5, as in the canonical grade school example. However, it is rare that you know with absolute certainty\nthat a coin is fair.\n66\nSimilarly, for the case of predicting the next output given t he preceding outputs 2, 3, 5,\n7, . . . , 89, we want that\np(2, 3, 5, . . . , 89, 97) > p (2, 3, 5, . . . , 89, x)\nfor x ̸= 97. Again, a program that generates the former will general ly be shorter than one\nthat generates the latter. For example, we might have\nfor i := 2 to 97 do\n<code for printing out i if it is prime>\nas opposed to\nfor i := 2 to 89 do\n<code for printing out i if it is prime>\nprint x;\nFor the example where the word Gulch always follows the word Gizzard’s the ten times\nthe word Gizzard’s occurs, and where we want to estimate the probability that th e word\nGulch follows Gizzard’s in its next occurrence, consider a program that encodes text using\na bigram-like model. Assume that for eﬃciency, the program o nly explicitly codes those\nbigram probabilities that are non-zero, as only a small frac tion of all bigrams occur in\npractice. To model the case where Gulch does follow Gizzard’s in its next occurrence, we\nonly need to code a single nonzero probability of the form p(x|Gizzard’s), i.e., for x = Gulch.\nHowever, if a diﬀerent word follows Gizzard’s, to model this new data we need an additional\nnonzero probability of the form p(x|Gizzard’s). Thus, presumably the program (including\nthe description of its bigram model) coding this latter case will be larger than the one\ncoding the former case. Thus, by assigning higher probabili ty to data generated with\nsmaller programs, we get the desired behavior of predicting Gulch with high probability.\nNow, notice that using a prior of the form p(G) = 2 −l(G) results in this behavior if we\njust replace grammars G with programs Gp. That is, we can express the probability p(O)\nof some data or observations O as\np(O) =\n∑\nGp\np(O, Gp) =\n∑\nGp\np(Gp)p(O|Gp) =\n∑\noutput(Gp ) = O\np(Gp)\nwhere we have p(O|Gp) = 1 if the output of program Gp is O and p(O|Gp) = 0 otherwise.\nSubstituting in the prior on programs p(Gp) = 2 −l(Gp), we get\np(O) =\n∑\noutput(Gp ) = O\n2−l(Gp)\nwhich gives us that data that can be described with shorter pr ograms have higher proba-\nbility.\nWhile the MDL-style prior p(Gp) = 2 −l(Gp ) yields this nice behavior, there are several\nprovisos. First of all, notice that this prior is not appropr iate for making precise predictions.\n67\nFor example, while in the above examples we make arguments ab out the relative magnitude\nof diﬀerent probabilities, it would be folly to try to nail dow n actual probabilities and expect\nthem to be accurate. Also, notice that we used data sets of non -trivial size; this is because\nthe inaccuracy of this type of prior is especially marked for small data sets. For example,\nconsider the case of predicting the next value in the sequenc e 2, 3, 5, 7. In this case, it is\nunlikely that the shortest program that outputs this sequen ce is of the form\nfor i := 2 to 7 do\n<code for printing out i if it is prime>\nand the argument given earlier for the longer sequence of pri mes does not hold. Instead, a\nshorter program would be\nprint \"2, 3, 5, 7\";\nHence, for this short sequence of primes it is unclear whethe r the MDL-style prior would\npredict 11 with high probability, even though intuitively t his is the correct prediction.\nBoth of these issues are related to the fact that there are man y diﬀerent programming\nlanguages we could use to describe programs, and that the sam e program in diﬀerent lan-\nguages may have very diﬀerent lengths. Thus, the speciﬁc beha vior of the prior depends\ngreatly on the language used. However, for large pieces of da ta the relative diﬀerences in\nprogram length between programming languages becomes smal ler. For example, if a pro-\ngram is 10 lines in Lisp and 1,000 lines in Basic, this is a rela tively large diﬀerence. However,\na 100,010-line Lisp program and a 101,000-line Basic progra m are nearly the same length\nfrom a relative perspective. 14 Thus, for large pieces of data the prior will yield qualitati vely\nsimilar results independent of programming language.\nIn any case, we choose an MDL-style prior in this work because of the observation that\nby assigning higher probabilities to smaller programs we ge t a very rich behavior that seems\nto model the real-world fairly well. However, instead of con sidering a general programming\nlanguage, we tailor our description language to one that des cribes only probabilistic context-\nfree grammars. Considering a restricted language simpliﬁe s the search problem a great\ndeal, and context-free grammars are able to express many of t he important properties\nof language. Furthermore, we observed above that a general M DL prior cannot make\nquantitatively accurate predictions. In this work, we atte mpt to tailor the prior so that\nmeaningful quantitative predictions can be made in the lang uage domain.\nTo summarize, we treat grammar induction as a search for the g rammar G with the\nhighest probability given the data or observations O, which is equivalent to ﬁnding the\ngrammar G that maximizes the objective function p(O|G)p(G), the likelihood of the training\ndata multiplied by the prior probability of the grammar. We t ake the prior p(G) to be 2 −l(G)\nas dictated by the minimum description length principle. Wh ile this framework does not\n14 For any two Turing-machine-equivalent languages, there ex ists a constant c such that any program in\none language, say of length l bits, can be duplicated in the other language using at most l + c bits. The\ngeneral idea behind the proof is that you can just write an int erpreter (of length c bits) for the former\nlanguage in the latter language.\n68\nrestrict us to a particular grammar formalism, in this work w e consider only probabilistic\ncontext-free grammars, as it is a fairly simple, yet express ive, representation. We describe\nour search strategy in Section 3.3. We describe what encodin g scheme we use to calculate\nl(G) in Section 3.5.\n3.3 Algorithm Outline\nWe assume a simple greedy search strategy. 15 We maintain a single hypothesis grammar\nthat is initialized to a small, trivial grammar. We then try t o ﬁnd a modiﬁcation to the\nhypothesis grammar, such as the addition of a grammar rule, t hat results in a grammar\nwith a higher score on the objective function. When we ﬁnd a su perior grammar, we make\nthis the new hypothesis grammar. We repeat this process unti l we can no longer ﬁnd a\nmodiﬁcation that improves the current hypothesis grammar.\nFor our initial grammar, we choose a grammar that can generat e any string, to assure\nthat the grammar assigns a nonzero probability to the traini ng data. 16 At the highest level\nof the grammar, we have the rules\nS → SX (1 − ǫ)\nS → X ( ǫ)\nexpressing that a sentence S is a sequence of X’s. The quantit ies in parentheses are the prob-\nabilities associated with the given rules; we describe ǫ and other rule probability parameters\nin detail in Section 3.5.1.\nThen, we have rules\nX → A (p(A))\nfor every nonterminal symbol A ̸= S,X in the grammar. Combined with the earlier rules,\nwe have that a sentence is composed of a sequence of independe ntly generated nonterminal\nsymbols. We maintain this property throughout the search pr ocess; that is, for every\nsymbol A that we add to the grammar, we also add a rule X → A. This assures that the\nsentential symbol can expand to every symbol; otherwise, ad ding a symbol will not aﬀect\nthe probabilities that a grammar assigns to strings.\nTo complete the initial grammar, we have rules\nAα → α (1)\nfor every terminal symbol or word α. That is, we have a nonterminal symbol expanding\nexclusively to each terminal symbol. With the above rules, t he sentential symbol can expand\n15 While searches that maintain a population of hypotheses can yield better performance, it is unclear\nhow to eﬃciently maintain multiple hypotheses in this domai n because each hypothesis is a grammar that\ncan potentially be very large. However, stochastic searche s such as simulated annealing could be practical,\nthough we have not tested them.\n16 Otherwise, the objective function will be zero, and unless t here is a single move that would cause the\nobjective function to be nonzero, the gradient will also be z ero, thus making it diﬃcult to search intelligently.\n69\nS → SX (1 − ǫ)\nS → X ( ǫ)\nX → A (p(A)) ∀ A ∈ N − {S, X}\nAα → α (1) ∀ α ∈ T\nN = the set of all nonterminal symbols\nT = the set of all terminal symbols\nProbabilities for each rule are in parentheses.\nTable 3.1: Initial hypothesis grammar\nto every possible sequence of words. (For every symbol Aα, there will be an accompanying\nrule X → Aα.) The initial grammar is summarized in Table 3.1.\nWe use the term move set to describe the set of modiﬁcations we consider to the curren t\nhypothesis grammar to hopefully produce a superior grammar . Our move set includes the\nfollowing moves:\nMove 1: Create a rule of the form A → BC (concatenation)\nMove 2: Create a rule of the form A → B|C (classing)\nFor any context-free grammar, it is possible to express a wea kly equivalent grammar using\nonly rules of these forms. As mentioned before, with each new symbol A we also create a\nrule X → A. We describe the move set in more detail in Section 3.5.2. 17\n3.3.1 Evaluating the Objective Function\nConsider the task of calculating the objective function p(O|G)p(G) for some grammar G.\nCalculating p(G) = 2 −l(G) turns out to be inexpensive; however, calculating p(O|G) requires\nevaluating the probability p(oi|G) for each sentence oi in the training data, which entails\nparsing each sentence in the training data. We cannot aﬀord to parse the training data for\neach grammar considered; indeed, to ever be practical for la rge data sets, it seems likely\nthat we can only aﬀord to parse the data once.\nTo achieve this goal, we employ several approximations. Fir st, notice that we do not\never need to calculate the actual value of the objective func tion; we need only to be able to\ndistinguish when a move applied to the current hypothesis gr ammar produces a grammar\nthat has a higher score on the objective function. That is, we need only to be able to\ncalculate the diﬀerence in the objective function resulting from a move. This can be d one\neﬃciently if we can quickly approximate how the probability of the training data changes\nwhen a move is applied.\nTo make this possible, we approximate the probability of the training data p(O|G) by\nthe probability of the single most probable parse, or Viterbi parse, of the training data.\n17In this chapter, we will use the symbols A, B, . . . and symbols of the form Aα, Bα, . . . to denote general\nnonterminal symbols, i.e., nonterminal symbols other than S and X.\n70\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nABob\nBob\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nS\n✟✟✟ ✟\n❍❍❍ ❍\nS\n✟✟ ❍❍\nS\nX\nAMary\nMary\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nFigure 3.3: Initial Viterbi parse\nS\n✟✟✟\n❍❍❍\nS\nX\nABob\nBob\nX\nB\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\nX\nAMary\nMary\nX\nB\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nFigure 3.4: Predicted Viterbi parse\n71\nFurthermore, instead of recalculating the Viterbi parse of the training data from scratch\nwhen a move is applied, we use heuristics to predict how a move will change the Viterbi\nparse. For example, consider the case where the training dat a consists of the two sentences\nO = {Bob talks slowly , Mary talks slowly }\nIn Figure 3.3, we display the Viterbi parse of this data under the initial hypothesis grammar\nof our algorithm.\nNow, let us consider the move of adding the rule\nB → Atalks Aslowly\nto the initial grammar (as well as the concomitant rule X → B). A reasonable heuristic\nfor predicting how the Viterbi parse will change is to replac e adjacent X’s that expand to\nAtalks and Aslowly respectively with a single X that expands to B, as displayed in Figure 3.4.\nThis is the actual heuristic we use for moves of the form A → BC , and we have analogous\nheuristics for each move in our move set. By predicting the di ﬀerences in the Viterbi parse\nresulting from a move, we can quickly estimate the change in t he probability of the training\ndata.\nNotice that our predicted Viterbi parse can stray a great dea l from the actual Viterbi\nparse, as errors can accumulate as move after move is applied . To minimize these eﬀects,\nwe process the training data incrementally. Using our initi al hypothesis grammar, we parse\nthe ﬁrst sentence of the training data and search for the opti mal grammar over just that one\nsentence using the described search framework. We use the re sulting grammar to parse the\nsecond sentence, and then search for the optimal grammar ove r the ﬁrst two sentences using\nthe last grammar as the starting point. We repeat this proces s, parsing the next sentence\nusing the best grammar found on the previous sentences and th en searching for the best\ngrammar taking into account this new sentence, until the ent ire training corpus is covered.\nDelaying the parsing of a sentence until all of the previous s entences are processed should\nyield more accurate Viterbi parses during the search proces s than if we simply parse the\nwhole corpus with the initial hypothesis grammar. In additi on, we still achieve the goal of\nparsing each sentence but once.\n3.3.2 Parameter Training\nIn this section, we describe how the parameters of our gramma r, the probabilities associated\nwith each grammar rule, are set. Ideally, in evaluating the o bjective function for a particular\ngrammar we should use its optimal parameter settings given t he training data, as this is the\nfull score that the given grammar can achieve. However, sear ching for optimal parameter\nvalues is extremely expensive computationally. Instead, w e grossly approximate the optimal\nvalues by deterministically setting parameters based on th e Viterbi parse of the training\ndata parsed so far. We rely on the post-pass, described later , to reﬁne parameter values.\nReferring to the rules in Table 3.1, the parameter ǫ is set to an arbitrary small constant.\nRoughly speaking, the values of the parameters p(A) are set to the frequency of the X → A\n72\nreduction in the Viterbi parse of the data seen so far and the r emaining symbols are set to\nexpand uniformly among their possible expansions. This iss ue is discussed in more detail\nin Section 3.5.1.\n3.3.3 Constraining Moves\nConsider the move of creating a rule of the form A → BC . This corresponds to k3 diﬀerent\nspeciﬁc rules that might be created, where k is the current number of nonterminal symbols\nin the grammar. As it is too computationally expensive to con sider each of these rules at\nevery point in the search, we use heuristics to constrain whi ch moves are appraised.\nFor the left-hand side of a rule, we always create a new symbol . This heuristic selects\nthe optimal choice the vast majority of the time; however, un der this constraint the moves\ndescribed earlier in this section cannot yield arbitrary co ntext-free languages. A symbol\ncan only be deﬁned in terms of symbols created earlier in the s earch process, so recursion\ncannot be introduced into the grammar. To partially address this, we add the move\nMove 3: Create a rule of the form A → AB|B\nThis creates a symbol that expands to an arbitrary number of B’s. With this iteration\nmove, we can construct grammars that generate arbitrary reg ular languages. In Section\n3.5.5, we discuss moves that extend our coverage to arbitrar y context-free languages.\nTo constrain the symbols we consider on the right-hand side o f a new rule, we use what\nwe call triggers.18 A trigger is a conﬁguration in the Viterbi parse of a sentence that is\nindicative that a particular move might lead to a better gram mar. For example, in Figure\n3.3 the fact that the symbols Atalks and Aslowly occur adjacently is indicative that it could\nbe proﬁtable to create a rule B → AtalksAslowly. We have developed a set of triggers for\neach move in our move set, and only consider a speciﬁc move if i t is triggered somewhere\nin the current Viterbi parse.\n3.3.4 Post-Pass\nA conspicuous shortcoming in our search framework is that th e grammars in our search\nspace are fairly unexpressive. Firstly, recall that our gra mmars model a sentence as a\nsequence of independently generated symbols; however, in l anguage there is a large depen-\ndence between adjacent constituents. Furthermore, the onl y free parameters in our search\nare the parameters p(A); all symbols besides S and X are ﬁxed to expand uniformly. Th ese\nchoices were necessary to make the search tractable.\nTo address these issues, we use an Inside-Outside algorithm post-pass. Our methodology\nis derived from that described by Lari and Young (1990). We cr eate n new nonterminal\nsymbols {X1, . . . , X n}, and create all rules of the form:\nXi → Xj Xk i, j, k ∈ { 1, . . . , n }\nXi → A i ∈ { 1, . . . , n }, A ∈ Nold − {S, X}\n18This is not to be confused with the use of the term triggers in dynamic language modeling.\n73\nNold denotes the set of nonterminal symbols acquired in the initi al grammar induction phase,\nand X1 is taken to be the new sentential symbol. These new rules repl ace the ﬁrst three\nrules listed in Table 3.1. The parameters of these rules are i nitialized randomly. Using this\ngrammar as the starting point, we run the Inside-Outside alg orithm on the training data\nuntil convergence.\nIn other words, instead of using the naive S → SX | X rule to attach symbols together\nin parsing data, we now use the Xi rules and depend on the Inside-Outside algorithm\nto train these randomly initialized rules intelligently. T his post-pass allows us to express\ndependencies between adjacent symbols. In addition, it all ows us to train parameters that\nwere ﬁxed during the initial grammar induction phase.\n3.3.5 Algorithm Summary\nWe summarize the algorithm, excluding the post-pass, in Fig ure 3.5. In Section 3.4, we\nrelate our algorithm to previous work on grammar induction. In Section 3.5, we ﬂesh out\nthe details of the algorithm, including the move set, the enc oding of the grammar used to\ncalculate the objective function, and the parsing algorith m used. In addition, we describe\nextensions to the basic algorithm that we have implemented.\n3.4 Previous Work\n3.4.1 Bayesian Grammar Induction\nWork by Solomonoﬀ (1960; 1964) is the ﬁrst to lay out the gener al Bayesian grammar\ninduction framework that we use. Solomonoﬀ points out the re lation between encodings and\nprior probabilities, and using this relation describes obj ective functions for several induction\nproblems including probabilistic context-free grammar in duction. Solomonoﬀ evaluates the\nobjective function manually for a few example grammars to de monstrate the viability of the\ngiven objective function, but does not specify an algorithm for automatically searching for\ngood grammars. Solomonoﬀ’s work can be seen as a precursor to the minimum description\nlength principle, and would in fact lead to the closely relat ed universal a priori probability\n(Solomonoﬀ, 1960; Solomonoﬀ, 1964; Li and Vit´ anyi, 1993).\nCook et al. (1976) present a probabilistic context-free grammar induc tion algorithm\nthat employs a similar framework. While not formally Bayesi an, their objective function\nstrongly resembles a Bayesian objective function. In parti cular, their objective function is\na weighted sum of the complexity of a grammar and the discrepancy between the grammar\nand the training data. The ﬁrst term is analogous to a prior on grammars, and the second is\nanalogous to the probability of the data given the grammar. H owever, the actual measures\nused for complexity and discrepancy are rather dissimilar f rom those used in this work.\nTheir initial hypothesis grammar consists of the sententia l symbol expanding to every\nsentence in the training data and only those strings. This co ntrasts to the simple overgen-\nerating grammar we use for our initial grammar. For Cook et al. , initially the length of the\ngrammar is large and the length of the data given the grammar i s small, while the converse\nis true for our approach. We hypothesize that neither approa ch is inherently superior; in\n74\n; G holds current hypothesis grammar\n; P holds best parse for each sentence seen so far\nG := initial hypothesis grammar\nP := ǫ\n; training data is composed of sentences (o1, . . . , o n)\nfor i := 1 to n do\nbegin\n; calculate best parse Pi of current sentence and append\n; to P, the list of best parses\nPi := best parse of sentence oi under grammar G\nP := append(P, Pi)\n; T holds the list of triggers yet to be checked\nT := set of triggers in Pi\nwhile T ̸= ǫ do\nbegin\n; pick the ﬁrst trigger t in T and remove from T\nt := ﬁrst(T )\nT := remove(T , t)\n; check if associated move is proﬁtable, if so, apply\nm := move associated with trigger t\nGm := grammar yielded if move m is applied to the grammar G,\nincluding parameter re-estimation\nPm := best parse yielded if move m is applied to G\n∆ := change in objective function if G becomes Gm and P becomes Pm\nif ∆ > 0 then\nbegin\nG := Gm\nP := Pm\nT := append(T , new triggers in P)\nend\nend\nend\nFigure 3.5: Outline of search algorithm\n75\nthe former approach one just chooses moves that tend to compa ct the grammar, while in\nthe latter approach one chooses moves that tend to compact th e data.\nThe move set used by Cook et al. includes: substitution, which is analogous to our\nconcatenation move except that instead of a new symbol being created on the left-hand\nside existing symbols can be used as well; disjunction, which is analogous to our classing\nrule; a move for removing inaccessible productions; and a mo ve for merging two symbols\ninto one. They describe a greedy heuristic search strategy, and present results on small\ndata sets, the largest being tens of sentences. Their work is geared toward ﬁnding elegant\ngrammars as opposed to ﬁnding good language models, and thus they do not present any\nlanguage modeling results.\nStolcke and Omohundro (1994) also present a similar algorit hm. Again, there are several\nsigniﬁcant diﬀerences from our approach. They adhere to the B ayesian framework as we do;\nhowever, their prior on grammars is divided into two diﬀerent terms: a prior on grammar\nrules, and a prior on the probabilities associated with gram mar rules. For the former, they\nuse an MDL-like prior p(G) = c−l(G) where c is varied during the search process. For the\nlatter, they use a Dirichlet prior. In our work, both grammar rules and rule probabilities are\nexpressed within the MDL framework. In addition, like Cook et al. , Stolcke and Omohundro\nchoose an initial grammar where the sentential symbol expan ds to every sentence in the\ntraining data and only those strings.\nThe most important diﬀerences between this work and ours conc ern the move set and\nsearch strategy. Stolcke and Omohundro describe only two mo ves: a move for merging\ntwo nonterminal symbols into one, and a move named chunking that is analogous to our\nconcatenation move. As their search strategy, they use a beam search , which requires\nmaintaining multiple hypothesis grammars. They describe h ow this is necessary because\nwith their move set, often several moves must be made in conju nction to improve the\nobjective function. We have addressed this problem in our wo rk by using a rich move set\n(see Section 3.5.2); we have complex moves that hopefully co rrespond to those move tuples\nof Stolcke and Omohundro that often lead to improvements in t he objective function. In\naddition, at each point in the search they consider every pos sible move, as opposed to\nusing the triggering heuristics we use to constrain the move s considered. Because of these\ndiﬀerences, we assume our algorithm is signiﬁcantly more eﬃc ient than theirs. They do not\npresent any results on data sets approaching the sizes that w e used, and like Cook et al. ,\nthey do not present any language modeling results.\n3.4.2 Other Approaches\nThe most widely-used tool in probabilistic grammar inducti on is the Inside-Outside algo-\nrithm (Baker, 1979), a special case of the Expectation-Maxi mization algorithm (Dempster\net al. , 1977). The Inside-Outside algorithm takes a probabilisti c context-free grammar and\nadjusts its probabilities iteratively to attempt to maximi ze the probability the grammar as-\nsigns to some training data. It is a hill-climbing search; it generally improves the probability\nof the training data in each iteration and is guaranteed not t o lower the probability.\nLari and Young (1990; 1991) have devised a grammar induction algorithm centered on\n76\nthe Inside-Outside algorithm. In this approach, the initia l grammar consists of a very large\nset of rules over some ﬁxed number of nonterminal symbols. Pr obabilities are initialized\nrandomly, and the Inside-Outside algorithm is used to prune away extraneous rules by\nsetting their probabilities to near zero; the intention is t hat this process reveals the correct\ngrammar. In Section 3.6, we give a more detailed description . Lari and Young present\nresults on various training corpora, with some success. In o ur experiments, we replicate the\nLari and Young algorithm for comparison purposes.\nPereira and Schabes (1992) extend the Lari and Young work by t raining on corpora\nthat have been manually parsed. They use the manual annotati on to constrain the Inside-\nOutside training. However, their goal was parsing as oppose d to language modeling, so no\nlanguage modeling results are reported.\nCarroll (1995) describes a heuristic algorithm for grammar induction that employs the\nInside-Outside algorithm extensively. Carroll restricts the grammars he considers to a\ntype of probabilistic dependency grammars, which are a subset of probabilistic context-free\ngrammars. In particular, he only considers grammars where t here is one nonterminal symbol\n¯A associated with each terminal symbol A and no other nonterminal symbols. Furthermore,\nall rules expanding a nonterminal symbol ¯A must have the corresponding terminal symbol\nA somewhere on the right-hand side.\nCarroll begins with a seed grammar that is manually construc ted. The training corpus\nis parsed a sentence at a time, and he has heuristics for addin g new grammar rules if a\nsentence is unparsable with the current grammar. The Inside -Outside algorithm is used\nduring this process as well as afterwards to reﬁne rule proba bilities. In addition, there are\nmanually-constructed constraints on the new rules that can be created.\nCarroll reports results for building language models for pa rt-of-speech sequences cor-\nresponding to sentences. Training on 300,000 words/part-o f-speech tags from the Brown\nCorpus, he reports slightly better perplexities on test dat a than trigram part-of-speech tag\nmodels on the ∼ 99% of the sentences the grammar can parse. In addition, by li nearly in-\nterpolating the grammatical model and the trigram model, he achieves a better perplexity\non the entire test set than the trigram model alone.\nMcCandless and Glass (1993) present a heuristic grammar ind uction algorithm that\ndoes not use the Inside-Outside algorithm. They begin with a grammar consisting of the\nsentential symbol expanding to every sentence, and they hav e a single move for improving\nthe grammar, a move that combines classing and concatenatio n. However, they do not take\na Bayesian approach in determining which moves to take. Inst ead, classing is based on how\nsimilar the bigram distributions of two symbols are, and con catenation is based on how\nfrequently symbols occur adjacently. For evaluation, they build an n-gram symbol model\nusing the symbols induced. That is, instead of predicting th e next word based on the last\nn − 1 words, they predict the next word based on the last n − 1 symbols. With these n-gram\nsymbol models, they achieve slightly better perplexity on t est data than the corresponding\nn-gram word models.\n77\n3.5 Algorithm Details\n3.5.1 Grammar Speciﬁcation\nIn this section, we describe in detail the forms of the gramma r rules we consider and we\ndiscuss how rules are assigned probabilities.\nRecall the structure of the grammar we use as described in Sec tion 3.3. We have rules\nexpressing that a sentence S is a sequence of X’s\nS → SX (1 − ǫ)\nS → X ( ǫ)\nwhere the quantity in parentheses is the probability associ ated with the given rule. We\ntake ǫ to be an arbitrarily small constant so that it can be safely ig nored in the objective\nfunction calculation. 19\nThen, we have a rule of the form\nX → A (p(A))\nfor each nonterminal symbol A ̸= S, X. To calculate p(A), we use the frequency with which\nthe associated rule has been used in past parses. We keep trac k of c(X → A), the number\nof times the rule X → A is used in the current best parse P (see Figure 3.5), and we just\nnormalize this value to yield p(A) as follows:\np(A) = c(X → A)\n∑\nA c(X → A)\nFinally, we have rules that deﬁne the expansions of the nonte rminal symbols besides S\nand X. We restrict such nonterminal symbols to expand in exac tly one of four ways:\n• expansion to a terminal symbol: A → a\n• concatenation of two nonterminal symbols: A → BC\n• classing of two nonterminal symbols: A → B|C\n• repetition of a nonterminal symbol: A → AB|B\nFor instance, we do not allow a symbol A → a|BC that expands to both a terminal symbol\nand a concatenation of two nonterminal symbols.\nWhile composing rules of the ﬁrst three forms is suﬃcient to d escribe any context-free\nlanguage, the move set we use cannot introduce recursion int o the grammar. We add the\nfourth form to model a simple but common instance of recursio n. Even with this extra\n19 In a parse tree, the latter rule can be applied at most once whi le the former rule can be applied many\ntimes. Thus, the probability contributed to a parse by these rules is of the form (1 − ǫ)kǫ. For small ǫ, this\nexpression is very nearly equal to just ǫ, a constant. As we are only concerned with changes in the objective\nfunction, constant expressions can be ignored.\n78\nA1\n✟✟✟ ❍❍❍\nA2\n✟✟ ❍❍\nA4\n✟✟ ❍❍\nA6 A7\nA5\nA3\nFigure 3.6: Example class hierarchy\nform, we can still only model regular languages; in Section 3 .5.5 we describe extensions that\nrelease this restriction.\nFor rules of the ﬁrst two forms, the probability associated w ith the rule is 1 − ps, where\nps is the probability associated with smoothing rules, which will be discussed in the next\nsection.\nFor classing rules, we choose probabilities to form a unifor m distribution over all symbols\nthe class can expand to, as deﬁned as follows. First, notice t hat while a given classing rule\nonly classes two symbols, by composing several of these rule s you can eﬀectively class an\narbitrary number of symbols. For example, consider the rule s\nA1 → A2|A3\nA2 → A4|A5\nA4 → A6|A7\nwhich we can express using a tree as in Figure 3.6. We can see th at A1 classes together\nthe symbols A3, A5, A6, and A7 at its leaves. Then, instead of assigning 0.5 probability to\nA1 expanding to each of A2 and A3, we assign 0.25 probability to A1 expanding to each\nof A3, A5, A6, and A7. That is, we assign a uniform distribution on all symbols the class\nrecursively expands to at its leaves, not a uniform distribu tion on the symbols that the\nclass immediately expands to. (In this example, we assume th at A3, A5, A6, and A7 are\nnot classing symbols themselves.) Then, to satisfy these le af expansion probabilities, we\nhave that A2 expands to A4 with probability 2/3 and A5 with probability 1/3, and A1\nexpands to A2 with probability 3/4 and A3 with probability 1/4. Notice that this uniform\nleaf probability constraint assigns consistent probabili ties among diﬀerent symbols. The\nprobabilities in the class hierarchy we set to satisfy unifo rm leaf probabilities for A1 are\nthe same we use to satisfy the uniform leaf probabilities for A2. In actuality, the preceding\ndiscussion is not quite accurate as we multiply each of the pr obabilities by 1 − ps as in the\nnon-classing rules, to allow for smoothing.\nFor the repetition rule, while the rule A → AB|B is accurate in terms of the strings\nexpanded to, it is inaccurate in terms of the way we assign pro babilities to expansions. In\nparticular, the probability we assign to the symbol A expanding to exactly n B ’s is\np(A ⇒∗ Bn) = pMDL(n) = 6\nπ2\n1\nn[log2(n + 1)]2\nwhere pMDL is the universal MDL prior over the natural numbers (Rissane n, 1989). We\n79\nchoose this parameterization because it prevents us from ne eding to estimate the probability\nof the A → AB expansion versus the A → B expansion, and because in some sense it is\nthe most conservative distribution one can take, in that it a symptotically assigns as high\nprobabilities to large n as possible. Again, in actuality the above probabilities sh ould be\nmultiplied by 1 − ps for smoothing.\nNotice that we have minimized the number of parameters we nee d to estimate; this is\nto simplify the search task. So far, the only parameters we ha ve described are the p(A)\nassociated with rules of the form X → A, which are estimated deterministically from the\nbest parse P, and ps, the probability assigned to smoothing rules. In Section 3. 5.5, we\ndescribe extensions where we consider richer parameteriza tions.\n3.5.2 Move Set and Triggers\nIn this section, we list the moves that we use to adjust the cur rent hypothesis grammar,\nand the patterns in the current best parse that trigger the co nsideration of a particular\ninstance of a move form. These moves all take on the form of add ing a rule to the grammar;\nin Section 3.5.5 we consider rules of diﬀerent forms. For the r ule added to the grammar,\nwe generate a new, unique symbol A for the left-hand side of the rule and also create a\nconcomitant rule of the form X → A as mentioned in Section 3.3. Recall that whether a\nmove is taken depends on whether it improves the objective fu nction, and that in order to\nestimate this eﬀect we approximate how the best parse of previ ous data changes. In this\nsection, we also describe for each move how we approximate it s eﬀect on the best parse if\nthe move is applied.\nFor future reference, we use the notation Aα to refer to a nonterminal symbol that\nexpands exclusively to the string α. For example, the symbol ABob expands to the word\nBob and no other strings.\nConcatenation Rules\nTo trigger the creation of rules of the form A → BC , we look for two adjacent instances of\nthe symbol X expanding to the symbols B and C respectively. (Recall that the sentential\nsymbol S expands to a sequence of X’s.) For example, in Figure 3.7, the following rules are\ntriggered: A → ABobAtalks, A → AtalksAslowly, and A → AMaryAtalks. To approximate how\nthe best parse changes with the creation of a rule A → BC , we simply replace all adjacent\npairs of X’s expanding to B and C with a single X expanding to A. In this example, if the\nrule A1 → AtalksAslowly is actually created, then we would estimate the best parse to be as\nin Figure 3.8.\nClassing Rules\nTo trigger the creation of rules of the form A → B|C, we look for cases where by forming the\nclassing rule, we can more compactly express the grammar. Fo r example, consider Figure\n3.8. These parses trigger concatenation rules of the form A → ABobA1 and A → AMaryA1.\nNow, if we create a rule of the form A2 → ABob|AMary, instead of creating two diﬀerent\n80\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nABob\nBob\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nS\n✟✟✟ ✟\n❍❍❍ ❍\nS\n✟✟ ❍❍\nS\nX\nAMary\nMary\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nFigure 3.7: Triggering concatenation\nS\n✟✟✟\n❍❍❍\nS\nX\nABob\nBob\nX\nA1\n✟✟ ✟❍❍ ❍\nAtalks\ntalks\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\nX\nAMary\nMary\nX\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nFigure 3.8: After concatenation/triggering classing\nS\n✟✟✟\n❍❍❍\nS\nX\nA2\nABob\nBob\nX\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\nX\nA2\nAMary\nMary\nX\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nFigure 3.9: After classing\n81\nconcatenation rules to model these two sentences, we can cre ate a single rule of the form\nA → A2A1. Thus, adding a classing rule can enable us to create fewer ru les to model a\ngiven piece of data. In this instance, the cost of creating th e classing rule may oﬀset the\ngain in creating one fewer concatenation rule; however, a cl assing rule can be proﬁtable if\nit saves rules in more than one place in the grammar.\nIn particular, whenever there are two triggered rules that d iﬀer by only one symbol, we\nconsider classing the symbols that they diﬀer by. Triggered r ules that do not improve the\nobjective function by themselves may become proﬁtable afte r classing, as a class can reduce\nthe number of new rules that need to be created. Recall that th e prior on grammars assigns\nlower probabilities to larger grammars, so the fewer rules c reated, the better.\nIn the above example, we consider building the class A2 → ABob|AMary because we have\ntwo triggered concatenation rules that only diﬀer in that ABob is replaced with AMary in\nthe latter rule. To approximate how the best parse changes if a classing rule is created,\nwe apply the classing rule wherever the associated triggeri ng rules would be applied. In\nthis example, if we create the classing rule the current pars e would be updated to be as in\nFigure 3.9.\nHowever, notice that if there exists a rule A3 → ABob|AJohn, then by classing together A3\nand AMary we can get the same aﬀect as classing together ABob and AMary: we will only need\nto create a single concatenation rule to describe the two sen tences in the previous example.\nSimilarly, if A3 belongs to a class A4, then we can get the same eﬀect by classing together\nA4 and AMary. Thus, when two triggered rules diﬀer by a symbol, instead of c onsidering\nclassing just those two symbols, we should consider classin g each class that each of those\ntwo symbols recursively belong two. However, this is too exp ensive computationally. For\nexample, if there are ten triggered rules of the form A → AαA1 for ten diﬀerent symbols\nAα and each Aα belongs to ten classes on average, then there are roughly\n(10·10\n2\n)\n≈ 5000\npairs of symbols that we could consider classing.\nTo address this issue, we use heuristics to reduce the number of classings we consider.\nIn particular, for a set of triggered rules that only diﬀer in a single position where the\nsymbols occurring in that position are {Aα}, we try to ﬁnd a minimal set of classes that\nall of the Aα recursively belong to, and only try exhaustive pairwise cla ssing among that\nminimal set. We use a greedy algorithm to search for this set; we explain this algorithm\nusing an example. Consider the case where we have triggered r ules of the form A → AαA1\nfor Aα = {ABob, AJohn, AMary, Athe macaw, Aa parrot, Aa frog}. Initially, we take the minimal\ncovering set to be just the set of all of the symbols:\n{ABob, AJohn, AMary, Athe macaw, Aa parrot, Aa frog}\nThen, we try to ﬁnd classes that multiple elements belong to. Say we notice that there is\nan existing symbol ABob|John that expands to ABob|AJohn. We group these two symbols by\nreplacing the two symbols with the new symbol:\n{ABob|John, AMary, Athe macaw, Aa parrot, Aa frog}\nUsing this new set, we again try to ﬁnd symbols that multiple e lements belong to. Let’s\n82\nS\n✟✟✟ ❍❍❍\nS\n✟✟ ❍❍\nS\nX\nAho\nho\nX\nAho\nho\nX\nAho\nho\nS\nX\nA\n✟✟✟ ❍❍❍\nA\n✟✟ ❍❍\nA\nAho\nho\nAho\nho\nAho\nho\nFigure 3.10: Triggering and applying the repetition move\nsay we ﬁnd an existing symbol Athe macaw|a parrot, giving us\n{ABob|John, AMary, Athe macaw|a parrot, Aa frog}\nand a symbol Athe macaw|a parrot|a frog giving us\n{ABob|John, AMary, Athe macaw|a parrot|a frog}\nThen, if we can ﬁnd no more classes that multiple elements bel ong to, we take this to be the\nminimal covering set and consider all possible pairwise cla ssings of these three elements.\nNotice that this algorithm attempts to ﬁnd the natural group ings of the elements in the\nlist as expressed through existing classes, and only tries t o class together these higher-level\nclasses. This is a reasonable heuristic in selecting new cla ssings to consider.\nTo constrain what groupings are performed, we only consider those groupings that are\nproﬁtable in terms of the objective function. For instance, in the above example we only\ngroup together the symbols ABob and AJohn into the symbol ABob | John if creating the rule\nA → ABob | JohnA1 is more proﬁtable (or less unproﬁtable) in terms of the objec tive function\nthan creating the rules A → ABobA1 and A′ → AJohnA1.\nRepetition Rules\nTo make rules of the form A → AB|B, we look for multiple ( i.e., at least three) adjacent\ninstances of the symbol X expanding to the symbol B. For example, the parse on the left\nof Figure 3.10 triggers a rule of the form A → AAho|Aho. To approximate how the best\nparse changes with the creation of a rule A → AB|B, we replace all chains of at least three\nconsecutive X’s expanding to B’s with a single X expanding to A. In this example, if the\nrepetition rule is actually created, then we would estimate the best parse to be the parse\non the right in Figure 3.10.\n83\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\n✟✟✟ ✟\n❍❍❍ ❍\nS\n✟✟ ❍❍\nS\nX\nABob\nBob\nX\nAtalks\ntalks\nX\nAquickly\nquickly\nFigure 3.11: Without smoothing rules\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nAquickly\nquickly\nFigure 3.12: With smoothing rules\nSmoothing Rules\nIn this section, we describe the smoothing rules alluded to e arlier. Just as smoothing\nimproves the accuracy of n-gram models, smoothing can improve grammatical models by\nassigning nonzero probabilities to phenomena with no count s. More importantly, they cause\ntext to be parsed in a way so as to provide informative trigger s for producing new rules.\nFor example, consider the case where the only concatenation rules in the grammar are\nA1 → AtalksAslowly\nA2 → ABobA1\nThen, if we see the sentences Bob talks slowly and Bob talks quickly , they will be parsed\nas in Figure 3.11. While the concatenation rules can be used t o parse the ﬁrst sentence,\nthey do not apply to the second sentence. However, on the surf ace these sentences are very\nsimilar and it is desirable to be able to capture this similar ity. Consider adding the rule\nAslowly → Aquickly to the grammar. Then, we can parse the two sentences as in Figu re 3.12,\ncapturing the similarity in structure of the two sentences.\n84\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nǫ\nFigure 3.13: ǫ-smoothing rules\nFurthermore, we can use the parse on the right as a trigger. Fo r example, we might\nconsider creating the rules\nA′\n1 → AtalksAquickly\nA′\n2 → ABobA′\n1\nreﬂecting the similarity in structure between the two const ructions. The rule Aslowly →\nAquickly helps us capture the parallel nature of similar constructio ns in both the best parse\nand the grammar.\nIn the grammar, we have a rule of the form A → B for all nonterminal symbols A, B ̸=\nS, X, and we call these smoothing rules. They are implicitly created whenever a new\nnonterminal symbol is created. We assign them very low proba bilities so that they are used\ninfrequently. They are only used in the most probable parse i f without them few grammar\nrules can be applied to the given text, but with them many rule s can be applied, as in the\nabove example. This prevents smoothing rules from indicati ng a parallel nature between\noverly dissimilar constructions.\nIn addition, we also have smoothing rules of the form A → ǫ for every nonterminal\nsymbol A ̸= S, X. These can capture the situation where two constructio ns are identical\nexcept that a word is deleted in one. We display possible pars es of the sentences Bob talks\nslowly and Bob talks in Figure 3.13.\nWe assign probability ps\n2 pG(B) to smoothing rules A → B, and probability ps\n2 to smooth-\ning rules A → ǫ. We take the distribution pG(B) to be diﬀerent from the probabilities p(B)\nassociated with rules of the form X → B. The probability p(B) reﬂects how frequently\na symbol occurs in text, and it is unclear this is an accurate r eﬂection of how frequently\na symbol occurs in a smoothing rule. Instead, we guess that a b etter reﬂection of this\nfrequency is the frequency with which a symbol occurs in the grammar; as smoothing rule\noccurrences trigger rule creations, these two quantities s hould correlate. Thus, we take\npG(B) = cG(B)∑\nB cG(B)\nwhere cG(B) is the number of times the symbol B occurs in grammar rules. The parameter\n85\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\nX\nA′\n2\n✟✟✟\n❍❍❍\nABob\nBob\nA′\n1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAquickly\nquickly\nFigure 3.14: After smoothing triggering\nps is set arbitrarily; in most experiments, we used the value 0. 01.\nWhen smoothing rules appear in the best parse P, they trigger rule creations in the\nway described in the examples given earlier in this section. In particular, we try to build\nthe smallest set of concatenation rules such that the given t ext can be parsed without the\nsmoothing rule. Thus, for the Bob talks quickly/Bob talks slowly example we try to build\nthe symbols A′\n1 and A′\n2 deﬁned earlier. To approximate how the best parse is aﬀected, we\njust use the heuristics given for concatenation rules; for t his example, this yields the parse\nin Figure 3.14. In Section 3.5.5, we describe other types of m oves that we can trigger with\nsmoothing rules.\nNotice that in creating multiple rules, we pay quite a penalt y in the objective function\nfrom the term favoring smaller grammars. To make this move mo re favorable, we have\nadded an encoding to our encoding scheme that describes thes e types of moves compactly.\nThis is described in Section 3.5.3.\nSpecialization\nIn the last section, we discussed a mechanism for handling th e case where a symbol is too\nspeciﬁc. For example, if we have a symbol that expands only to a string Bob talks slowly\n(ignoring smoothing rules), by applying smoothing rules th is symbol can expand to all\nstrings of the form Bob talks α. Furthermore, the application of a smoothing rule triggers\nthe creation of rules that expand to these other strings.\nOn the other hand, it may be possible that a symbol is too gener al. For example,\nconsider the rules\nA1 → Amouse|Arat\nA2 → AtheA1\nA3 → A2Asqueaks\nThe symbol A3 expands to the strings the mouse squeaks and the rat squeaks , but suppose\nonly the former string appears in text. We can create a specialization of the symbol A3 by\ncreating the rules\nA′\n2 → AtheAmouse\nA′\n3 → A′\n2Asqueaks\n86\nS\nX\nA3\n✟✟✟✟\n❍❍❍❍\nA2\n✟✟ ❍❍\nAthe\nthe\nA1\nAmouse\nmouse\nAsqueaks\nsqueaks\nS\nX\nA′\n3\n✟✟✟✟\n❍❍❍❍\nA′\n2\n✟✟ ❍❍\nAthe\nthe\nAmouse\nmouse\nAsqueaks\nsqueaks\nFigure 3.15: Before and after specialization\nS\nX\nA\n✟✟✟ ❍❍❍\nA\n✟✟ ❍❍\nA\nBho\nho\nBho\nho\nBho\nho\nS\nX\nA′′\n3\n✟✟✟ ❍❍❍\nA′′\n2\n✟✟ ❍❍\nBho\nho\nBho\nho\nBho\nho\nFigure 3.16: Before and after repetition specialization\nThe new symbol A′\n3 expands only to the string the mouse squeaks . Notice the similarity\nbetween this move and the move triggered by the smoothing rul e; the only diﬀerence is\nthat instead of being triggered whenever a smoothing rule oc curs, this move is triggered\nwhenever a classing rule occurs. Parses of the mouse squeaks before and after the creation\nof these specialization rules are displayed in Figure 3.15. Like with the smoothing rules, we\ntry to create the minimal number of rules so that the given tex t can be parsed without the\nclass expansion. Also like the smoothing rules, there is a sp ecial encoding in the encoding\nscheme to make these moves more favorable.\nIt is also possible to specialize repetition rules. For exam ple, consider the repetition rule\nA → ABho|Bho expressing that the symbol A can expand to any number of Bho’s. However,\nsuppose that ho always occurs exactly three times. Then, it seems reasonabl e to create the\nrules\nA′′\n2 → BhoBho\nA′′\n3 → A′′\n2Bho\n87\nso that we have a new symbol A′′\n3 that expands exactly to the string ho ho ho . Parses\nof ho ho ho before and after the creation of these specialization rules are displayed in\nFigure 3.16. Such specializations are triggered whenever a repetition symbol occurs in P.\nThey involve the creation of concatenation rules that gener ate the repeated symbol the\nappropriate number of times, like above.\nSummary\nWe have moves in our move set for building concatenation, cla ssing, and repetition grammar\nrules. These operations are the building blocks for regular languages, and as mentioned in\nSection 3.3.3 our algorithm can only create grammars that de scribe regular languages. This\nis because whenever we create a new rule, we create a new symbo l to be placed on its\nleft-hand side. In addition, we have no moves for modifying e xisting rules. Thus, it is\nimpossible to introduce recursion into the grammar, except for the recursion present in the\nrepetition rule.\nIn addition, we have moves for generalizing and specializin g existing symbols. Smoothing\nrules provide the trigger for creating symbols that general ize the set of strings existing\nsymbols expand to. Classing and repetition rules provide th e trigger for creating symbols\nthat specialize the set of strings existing symbols expand t o.\nWhile this forms a rich set of moves for constraining grammar s, there are some obvious\nshortcomings. For example, there are no moves for modifying existing rules or deleting\nrules, or for changing the set of strings a symbol expands to. Also, there are very few free\nparameters in the grammar; we may be able to do better by allow ing class expansions to\nhave probabilities that are trained. In Section 3.5.5, we de scribe extensions such as these\nthat we have experimented with.\n3.5.3 Encoding\nIn this section, we describe the encoding scheme that we use t o describe grammars. This\ndetermines the length l(G) of a grammar G, which is used to calculate the prior p(G) =\n2−l(G) on grammars, which is a term in our objective function.\nWhile one can encode grammars using simple methods such as te xtual description, we\nargue that it is important to use compact encodings as touche d on in Section 3.2.2. First\nof all, we want the prior p(G) = 2 −l(G) on grammars that is associated with an encoding\nl(G) to be an accurate prior; that is, p(G) should model grammar frequencies accurately.\nJust as good language models assign high probabilities to tr aining data, good priors should\nassign high probabilities to typical or frequent grammars. This corresponds to assigning\nshort lengths to typical grammars.\nFurthermore, the compactness of the encoding dictates how m uch data is needed as\nevidence to create a new grammar rule. To clarify, let us view the objective function from\nthe MDL perspective, i.e., as l(G)+l(O|G), the length of the grammar added to the length of\nthe data given the grammar, recalling that l(O|G) is simply log 2\n1\np(O|G) . Adding a grammar\nrule increases the length l(G) by some amount, say δ, so in order for a new grammar rule\nto improve the objective function its application must resu lt in a decrease in the length of\n88\nthe data of at least δ. The more compactly we can encode grammar rules, the smaller δ will\nbe, and the less a rule needs to compress the training data in o rder to be proﬁtable. This\ncorresponds to decreasing the amount of evidence necessary to induce a grammar rule, e.g.,\ndecreasing the number of times the symbols B and C need to occur next to each other to\nmake the creation of the rule A → BC proﬁtable.\nBefore we describe the encoding proper, we ﬁrst describe how we encode positive integers\nwith no upper limit, such as the number of symbols in the gramm ar. One option is to just\nset an arbitrary bound and to use a ﬁxed-length code, e.g., to code integers using 32 bits\nas in a programming language. However, it is inelegant to set a bound, and this encoding\nis ineﬃcient for small integers. Instead, we use the encodin g associated with the universal\nMDL prior over the natural numbers pMDL(n) (Rissanen, 1989) mentioned in Section 3.5.1,\nwhere\npMDL(n) = 6\nπ2\n1\nn[log2(n + 1)]2\nWe take the length l(n) of an integer n to be log 2\n1\npMDL(n) . This assigns shorter lengths to\nsmaller integers as is intuitive; in addition, it assigns as short lengths as possible asymptot-\nically to large integers.\nWe now describe the encoding. Recall that we are only concern ed with description\nlengths, as opposed to actual descriptions; thus, we only describe l engths here. The encoding\nis as follows:\n• First, we encode the list of all terminal symbols. How this is done is not important\nassuming the size of this list remains constant. We are only c oncerned with changes\nin grammar size, as we are only concerned with calculating changes in the objective\nfunction.\n• Then, we encode the number ns of nonterminal symbols excluding S and X using the\nuniversal MDL prior.\n• For each nonterminal symbol A ̸= S, X, we code the following:\n– We code the count c(A) (using the universal MDL prior) used to calculate p(A) =\nc(A)∑\nA c(A) , the probability associated with the rule X → A.\n– We code the type of the symbol, e.g., whether it is a concatenation rule, a classing\nrule, or a repetition rule. In all there are eight types (some of which we have yet\nto describe), so we use three bits to code this.\n– For each rule type, we have a diﬀerent way of coding the right-h and side of the\nrule, which will be described below.\nThe symbol on the left-hand side of each rule is given implici tly by the order in which the\nsymbols are listed. That is, the ﬁrst symbol listed is A1, the second A2, and so on up to\nAns . Notice that each symbol expands using exactly one of eight p ossible forms; we do not\nhave to consider listing multiple rules for a given symbol.\n89\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nAquickly\nquickly\nFigure 3.17: Smoothing rules\nWe do not have to list the rules expanding S or X because they ca n be determined\nimplicitly from the list of nonterminal symbols. Likewise, all smoothing rules can be de-\ntermined implicitly. Furthermore, all probabilities asso ciated with the grammar can be\ndetermined from just the form of the grammar, except for the s moothing probability ps and\nthe probabilities p(A) associated with the rules X → A. The probabilities p(A) are coded\nexplicitly above. We assume the probability ps is of constant size.\nBelow, we describe the eight diﬀerent rule types and how we cod e the right-hand side\nof each.\nexpansion to a terminal (A → a) Since none of these rules are created during the search\nprocess, it is not important how we code these rules assuming their size is constant.\nRecall that we are only concerned with changes in grammar length.\nconcatenation rule (A → BC ) We restrict concatenation rules to have exactly two sym-\nbols on the right-hand side, so we need not code concatenatio n length; we need only\ncode the identities of the two symbols. We constrain these tw o symbols on the right-\nhand side to be nonterminal symbols; this is not restrictive since there is a nonterminal\nsymbol expanding exclusively to each terminal symbol. As th ere are ns nonterminal\nsymbols, we can use log 2 ns bits to code the identity of each of the two symbols. In\nall rule types, to code a symbol identity we use log 2 ns bits.\nclassing rule (A → B|C) Again, we need only code two symbol identities, and we code\neach using log 2 ns bits.\nrepetition rule (A → AB|B) We need only code the identity of the B symbol to uniquely\nidentify this type of rule, and we code this using log 2 ns bits.\nderived rule This is the encoding tailored to the move triggered by the app lication of a\nsmoothing rule, as described in Section 3.5.2. We can identi fy the set of rules that are\ncreated in such a move with the following information: the sy mbol underneath which\n90\nthe smoothing rule was applied, the location of the applicat ion of the smoothing rule,\nand the symbol the smoothing rule expanded to. For instance, consider the example\ngiven in Section 3.5.2; we re-display this in Figure 3.17. Or iginally, the following rules\nexist:\nA1 → AtalksAslowly\nA2 → ABobA1\nand the smoothing rule triggers the creation of the followin g rules:\nA′\n1 → AtalksAquickly\nA′\n2 → ABobA′\n1\nWe can describe the new symbol A′\n2 as follows: it is just like the symbol A2, except\nwhere A2 expands to Aslowly, A′\n2 expands to Aquickly instead. The deﬁnition of A′\n1 can\nimplicitly be determined from this deﬁnition of A′\n2. Thus, to encode A′\n2, we need to\nencode A2, the location of Aslowly, and the symbol Aquickly. To code the two symbols,\nwe use log 2 ns bits for each as before. To code the location, in Figure 3.17 w e see that\nthere are ﬁve internal nodes in the parse tree headed by A2 (if no smoothing rules\nare applied). A smoothing rule can be applied at any of these ﬁ ve nodes. Thus, we\nneed log 2 5 bits to code the location of the application of the smoothin g rule. This\nsize will vary with the symbol under which the smoothing rule occurs. In general, we\nneed a total of 2 log 2 ns + log 2(# locations ) bits to code the set of rules triggered by\na smoothing rule. We call this type of encoding a derived rule, since it derives the\ndeﬁnition of one symbol from the deﬁnition of another.\ndeletion-derived rule This is identical to the derived rule just described, except that it\ncorresponds to the application of an ǫ-smoothing rule A → ǫ instead of a regular\nsmoothing rule A → B. In this case, we deﬁne a new symbol as equal to an existing\nsymbol except that one of its subsymbols is deleted ( i.e., is replaced with ǫ). To code\na deletion-derived rule, we just need to code the original sy mbol (log 2 ns bits) and\nthe location of the deletion (log 2(# locations ) bits); we do not have to code a second\nsymbol.\nspecialization rule This can be viewed as identical to the derived rule, except th at it\ncorresponds to the application of a classing rule as opposed to a smoothing rule. We\nwill deﬁne a new symbol as equal to an existing symbol, except that instead of re-\nplacing a subsymbol with an arbitrary symbol as in a derived r ule, we replace the\nsubsymbol with a symbol that the subsymbol expands to. For in stance, consider the\nexample given in Section 3.5.2; we re-display this in Figure 3.18. We can deﬁne the\nsymbol A′\n3 to be equal to A3, except that the symbol A1 is replaced with the symbol\nAmouse. This rule can be coded in the same way as a derived rule. Howev er, we have\nan additional constraint not present with derived rules: we know that the symbol\nthat is used to replace the original symbol at a given locatio n is a specialization of\nthe original symbol. That is, the original symbol expands to the replacing symbol via\nsome number of classing rules. We can code the replacing symb ol taking advantage\nof this observation; if the original symbol at the given loca tion expands to a total of\n91\nS\nX\nA3\n✟✟✟✟\n❍❍❍❍\nA2\n✟✟ ❍❍\nAthe\nthe\nA1\nAmouse\nmouse\nAsqueaks\nsqueaks\nS\nX\nA′\n3\n✟✟✟✟\n❍❍❍❍\nA′\n2\n✟✟ ❍❍\nAthe\nthe\nAmouse\nmouse\nAsqueaks\nsqueaks\nFigure 3.18: Before and after specialization\nc diﬀerent symbols via classing, then we can code the replacing symbol using log 2 c\nbits. Furthermore, we also have a constraint on the location not present in derived\nrules: we know that the original symbol at that location must be a classing sym-\nbol. Thus, instead of coding the location with log 2(# locations ) bits, we can code it\nwith log 2(# class locations ) bits. In summary, we can code specialization rules using\nlog2 ns + log2(# class locations ) + log 2 c bits.\nrepetition specialization rule This is identical to the specialization rule, except instea d\nof dealing with classing rules it is concerned with repetiti on rules. Notice that a\nrepetition rule A → AB|B can just be viewed as a classing rule of the form A →\nB|BB|BBB | · · ·. Thus, this rule can be coded in a similar manner to a regular\nspecialization rule. Instead of coding the location using l og2(# class locations ) bits,\nwe code it using log 2(# repeat locations ) bits. Instead of coding the replacing symbol\nusing log 2 c bits, we code the number of repetitions using the universal M DL prior.\n3.5.4 Parsing\nTo calculate the most probable parse of a sentence given the c urrent hypothesis grammar,\nwe use a probabilistic chart parser (Younger, 1967; Jelinek et al. , 1992). In chart parsing,\none ﬁlls in a chart composed of cells, where each cell represents a span in the sentence to\nbe parsed. If the sentence is composed of the words w1 · · · wm, then there is a cell for each\ni and j such that 1 ≤ i ≤ j ≤ m corresponding to the span wi · · · wj. Each cell is ﬁlled\nwith the set of symbols that can expand to the associated span wi · · · wj . For example,\nif the sentence is accepted under the grammar, then the symbo l S will occur in the cell\ncorresponding to w1 · · · wm. The cells can be ﬁlled in an eﬃcient manner with dynamic\nprogramming (Bellman, 1957). Performing probabilistic chart parsing just requires some\nextra bookkeeping; the algorithm is essentially the same. 20\n20This is only true when trying to calculate the most probable p arse of a sentence. In some applications,\none attempts to ﬁnd the total probability of a sentence, whic h involves summing the probabilities of all of its\n92\n.\n.\n.\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nA1\n.\n.\n.\nX\nA2\n.\n.\n.\nX\nA3\n.\n.\n.\nFigure 3.19: Typical parse-tree structure\nHowever, straightforward parsing is not eﬃcient given that we have smoothing rules of\nthe form A → B and A → ǫ for all nonterminal symbols A, B ̸= S, X. With these rules, it\nis possible for any symbol to expand to any span of a sentence; each cell in the chart will\nbe ﬁlled with every symbol in the grammar. Consequently, nai ve parsing with smoothing\nrules achieves the absolute worst-case time bounds for char t parsing. This is unacceptable\nin this application.\nInstead, we have heuristics for restricting the applicatio n of smoothing rules. We ﬁrst\nparse the sentence without using any smoothing rules. This y ields a parse of the form\ndisplayed in Figure 3.19. Then, we make the assumption that a pplying smoothing rules to\nthe structure below the Ai in the diagram is not proﬁtable; smoothing rules improve the\nprobability of a parse the most if they enable grammar rules t o apply in places where none\napplied before. We take the Ai to be the primitive units in the sentence, and only allow\nsmoothing rules to be applied immediately above these units . We re-parse the sequence\nA1A2 · · · given these smoothing rules, yielding a new best parse. We re peat this process\nwith this new best parse, until the best parse is unchanging. At some point, smoothing\nrules will not aﬀect the most probable parse.\n3.5.5 Extensions\nAfter completing the algorithm described in the previous pa rt of this section (Chen, 1995),\nwhich we will refer to as the basic algorithm , we experimented with diﬀerent extensions\nto arrive at what we refer to as the extended algorithm . In this section, we describe the\ndiﬀerences between the basic and extended algorithms.\nparses; in this case probabilistic chart parsing is somewha t more involved than normal chart parsing because\nof the diﬃculty in calculating probabilities when some type s of recursions are present in the grammar.\n93\nConcatenation\nIn the basic algorithm, we restrict concatenation rules A → A1A2 to have exactly two\nsymbols on the right-hand side. In the extended algorithm, w e allow an arbitrary number\nof symbols A → A1A2 · · ·. However, we do not enhance the move set by adding a move that\nconcatenates arbitrary k-tuples instead of just pairs, as the bookkeeping and comput ation\nrequired for this would be very expensive. Instead, we look f or opportunities to unfold\nshorter concatenation rules. For example, if we have two rul es\nA1 → A2A3\nA3 → A4A5\nand we notice that the symbol A3 occurs nowhere else in the grammar, then we can replace\nthis pair of rules with the single rule\nA1 → A2A4A5\nWe can unfold the deﬁnition of A3 into the ﬁrst rule to yield the longer concatenation rule.\nBy allowing long concatenations, we make it possible to expr ess new concatenations more\ncompactly, which is advantageous with respect to the object ive function.\nTo handle this extension in our encoding scheme, instead of j ust needing to code two\nsymbol identities as in the basic algorithm, we ﬁrst encode t he number of symbols k in the\nconcatenation using the universal MDL prior. Then, we encod e the k symbol indentities.\nClassing\nIn the basic algorithm, we restrict classing rules A → A1|A2 to have exactly two symbols on\nthe right-hand side. In the extended algorithm, we allow an a rbitrary number of symbols\nA → A1|A2| · · · . To support this representation, we add a new move to the move set. In\nthe basic algorithm, the only classing move is to create a new rule A → A1|A2. In the\nextended algorithm, we also allow the move of adding a new mem ber to an existing class.\nBoth of these moves fulﬁll the purpose of placing symbols int o a common class; the one\nthat is chosen in a particular situation is determined by whi ch is preferred by the objective\nfunction.\nWith this new move, it becomes possible to build grammars tha t express arbitrary\ncontext-free languages, instead of just regular languages . In the basic algorithm, it is\nimpossible to create recursion (except in the repetition ru le) because symbols can only be\ndeﬁned in terms of symbols created earlier in the search proc ess. Using this new move,\nwe can place into the deﬁnition of a symbol a symbol created su bsequently, thus enabling\nrecursion.\nAnother diﬀerence in the extended algorithm is that we train t he probabilities associated\nwith classing rules. In the basic algorithm, for a classing r ule A → A1|A2 we set the\nprobabilities p(A → Ai) of A expanding to each Ai in a deterministic way depending only\non the form of the grammar as described in Section 3.5.2. In th e extended algorithm, we\ntrain the probabilities p(A → Ai) by counting the frequency of each reduction A → Ai in\n94\nthe current best parse P of the training data. We take\np(A → Ai) = cA(Ai)∑\ni cA(Ai)\n(ignoring the factor of 1 −ps for handling smoothing rules) where cA(Ai) denotes the number\nof times the reduction A → Ai is used in P. By training class probabilities, it should be\npossible to build more accurate models of the training data.\nIn the encoding, instead of just needing to code two symbol id entities as in the basic\nalgorithm, we ﬁrst encode the number of symbols k in the class using the universal MDL\nprior. Then, we encode the k symbol indentities. We need also to encode the counts cA(Ai)\nfor each Ai; we do this by ﬁrst coding the total number of counts cA = ∑ k\ni=1 cA(Ai) using\nthe universal MDL prior. There are\n(cA+k−1\nk−1\n)\npossible ways to distribute cA counts among\nk elements;21 thus, we need log 2\n(cA+k−1\nk−1\n)\nbits to specify the values cA(Ai) given cA and k.\nSmoothing Rules\nThere are two modiﬁcations we make with respect to smoothing rules. First, we add\ninsertion smoothing rules, which can be thought of as the complement of deletion or ǫ-\nsmoothing rules. While deletion rules enable a symbol that e xpands to the string Bob talks\nslowly to also parse the string Bob talks as described in Section 3.5.2, insertion rules allow\nthe converse: they enable a symbol that expands to Bob talks to also parse the string Bob\ntalks slowly .\nInsertion rules are of the form A → AB for all nonterminal symbols A, B ̸= S, X. Such\na rule “inserts” a B immediately after an A. The probability associated with this rule is\nps\n3 pG(B) where pG(B) is deﬁned as in Section 3.5.2, and the probability of smooth ing rules\nA → B and A → ǫ are reduced to ps\n3 pG(B) and ps\n3 , respectively. The occurrence of an\ninsertion rule A → AB triggers the creation of a concatenation rule concatenatin g A and\nB.\nThe second modiﬁcation deals with the moves that are trigger ed by the occurrence of\na smoothing rule. Because the extended algorithm contains a move for adding symbols to\nclasses instead of just a move for creating classes, in the ex tended algorithm smoothing\nrules can trigger a move we could not consider in the basic alg orithm. Consider the rules\nA0 → Aquickly|Aslowly\nA1 → AtalksA0\nA2 → ABobA1\ndeﬁning a symbol A2 that expands to the strings Bob talks quickly and Bob talks slowly .\nNow, consider encountering a new string Bob talks well that we parse using the symbol A2\n21To see this, consider a row of cA white balls. By inserting k−1 black balls into this row of white balls, we\ncan represent a partitioning of the cA white balls into k bins: the black balls separate each pair of adjacent\nbins. Each of these partitionings correspond to a diﬀerent w ay of dividing cA counts among k elements. The\ntotal number of partitionings is equal to the number of diﬀer ent ways of placing the k − 1 black balls among\nthe total of cA + k − 1 balls in the row, or\n(cA+k−1\nk−1\n)\n.\n95\nS\nX\nA2\n✟✟✟\n❍❍❍\nABob\nBob\nA1\n✟✟ ❍❍\nAtalks\ntalks\nA0\nAwell\nwell\nFigure 3.20: Generalization\nand the smoothing rule A0 → Awell, as displayed in Figure 3.20. In the basic algorithm,\nthis triggers the possible creation of the rules\nA′\n1 → AtalksAwell\nA′\n2 → ABobA′\n1\ndescribing a symbol A′\n2 that expands just to the string Bob talks well . However, with the\nability to add new members to classes, we can instead just tri gger the addition of Awell to\nthe class A0 yielding\nA0 → Aquickly|Aslowly|Awell\nInstead of having a symbol A2 expanding to Bob talks quickly and Bob talks slowly and a\nsymbol A′\n2 expanding to Bob talks well , this new move yields a single symbol A2 expanding\nto all three strings. This is intuitively a more appropriate action, and results in a more\ncompact grammar.\nGrammar Compaction\nIn the basic algorithm, the move set consists entirely of mov es that create new rules, i.e.,\nmoves that expand the grammar (and hopefully compress the tr aining data). In the ex-\ntended algorithm, we consider moves that compact the gramma r (and leave the data the\nsame size or perhaps even enlarge the training data). These m oves help correct for the\ngreedy nature of the search strategy, by providing a mechani sm for reducing the number of\ngrammar rules. For example, if we have the following grammar rules\nA1 → AtalksAslowly\nA2 → ABobA1\nA′\n1 → AtalksAquickly\nA′\n2 → ABobA′\n1\n96\nit may be proﬁtable to replace the above rules with the rules\nA0 → Aquickly|Aslowly\nA′′\n1 → AtalksA0\nA′′\n2 → ABobA′′\n1\nMore generally, we search for rules that diﬀer by a single symb ol, and attempt to merge\nthese rules.\nTo constrain what rules we consider merging, we only conside r merging rules that expand\nsymbols that occur in a common class. This constraint is nece ssary because there are many\nconstructions that are on the surface very similar that have diﬀerent meanings. For example,\nthe strings a can and John can diﬀer by only one word, but are unrelated in meaning. We\nrestrict rule merging to symbols that are in a common class be cause hopefully symbols that\nhave been placed in the same class are semantically related.\nIn general, for any class A → A1|A2| · · ·, we attempt to create new symbols merging\nmultiple Ai whose deﬁnitions diﬀer by only a single symbol. Whenever we ma ke such\na symbol, we substitute it into the right-hand side of the cla ssing rule. If through this\nmerging we yield a classing rule A → B with only a single symbol on the right-hand side,\nwe merge the two symbols A and B into a single symbol.\nSymbol Encoding\nIn the basic algorithm, we encode symbol identities using lo g2 ns bits, where ns is the total\nnumber of nonterminal symbols. However, recall that coding theory states that a ﬁxed-\nlength coding such as this is an optimal coding only if all sym bols are equiprobable; in\ngeneral, a symbol with frequency p should be coded with log 2\n1\np bits. Intuitively, it seems\nreasonable to code rules involving frequent symbols such as Athe with fewer bits than rules\ninvolving rare symbols such as Ahippopotamus.\nWe calculate the frequency pG(A) of each symbol A in the grammar as cG(A)∑\nA cG(A) , where\ncG(A) is the number of times the symbol A occurs in the grammar. We code a symbol A\nusing log 2\n1\npG(A) bits. However, notice that we will not know cG(A) until the end of the\ngrammar description, but these values are needed to code the grammar. To resolve this\nproblem, we explicitly code the values of cG(A) for all A before we code the grammar rules.\nWe ﬁrst code cG = ∑ cG(A), the total number of symbol occurrences in the grammar, usi ng\nthe universal MDL prior. Then, there are\n(cG+ns−1\nns−1\n)\nways to distribute cG counts among ns\nelements, so we just need log 2\n(cG+ns−1\nns−1\n)\nbits to code the values of cG(A) given cG and ns.22\nMaintaining the Best Parse\nAs the move set grows in complexity, it becomes more diﬃcult f rom an implementational\nstandpoint to maintain an accurate estimate of the most prob able parse P. Furthermore,\nthe amount of memory needed to store P grows linearly in the training data size, so for\n22Using an adaptive coding method, it may be possible to encode symbols even more compactly. However,\nthe gain in compactness probably does not warrant the additi onal complexity of implementation.\n97\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nABob\nBob\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\nX\nABob\nBob\nX\nA1\n✟✟✟ ❍❍❍\nAtalks\ntalks\nAslowly\nslowly\nFigure 3.21: Before and after concatenation\nlarge training sets it may be impractical to store P entirely in memory, as is necessary for\ngood performance. In the extended algorithm, instead of kee ping track of P explicitly, we\njust estimate the counts of all salient events in P.\nFor example, to calculate whether it is proﬁtable to create a concatenation rule A → BC ,\nwe need to know the number of times two adjacent X’s expand to t he symbols B and C,\nrespectively. Let us call this quantity c(B, C ). We need to keep track of counts c(B, C )\nfor all symbols B and C. Likewise, there are similar counts that we need to keep trac k of\nfor the other types of moves. In the extended algorithm, we ju st record these counts as\nopposed to the actual parse P.\nUnfortunately, while less expensive this system is also les s accurate. To update counts\nwhen new sentences are parsed is straightforward; however, errors are introduced when-\never we apply a move to previous sentences. For example, cons ider Figure 3.21 depicting\nthe parse tree of Bob talks slowly before and after the creation of the concatenation rule\nA1 → AtalksAslowly. After the move, we know to set c(Atalks, Aslowly) to zero as the concate-\nnation rule will be applied at each relevant location; howev er, it is more diﬃcult to update\noverlapping counts. For example, for the sentence in the exa mple we should decrement\nc(ABob, Atalks) and increment c(ABob, A1). If we maintain P explicitly, this is straightfor-\nward; however, if we are only maintaining counts on P, the necessary information to update\nthese counts correctly is generally not available (unless e nough counts are available to com-\npletely reconstruct P). We use heuristics to update counts as best we can given avai lable\ninformation.\n3.6 Results\nTo evaluate our algorithm, we compare the performance of our algorithm to that of n-gram\nmodels and the Lari and Young algorithm.\nFor n-gram models, we tried n = 1 , . . . , 10 for each domain. To smooth the n-gram\nmodels, we use a popular version of Jelinek-Mercer smoothin g (Jelinek and Mercer, 1980;\nBahl et al. , 1983), namely the version that we refer to as interp-held-out described in\n98\nSection 2.4.1.\nIn the Lari and Young algorithm, the initial grammar is taken to be a probabilistic\ncontext-free grammar consisting of all Chomsky normal form rules over n nonterminal\nsymbols {X1, . . . X n} for some n, that is, all rules\nXi → Xj Xk i, j, k ∈ { 1, . . . , n }\nXi → a i ∈ { 1, . . . , n }, a ∈ T\nwhere T denotes the set of terminal symbols in the domain. All rule pr obabilities are\ninitialized randomly. From this starting point, the Inside -Outside algorithm is run until the\naverage entropy per word on the training data changes less th an a certain amount between\niterations; in this work, we take this amount to be 0.001 bits .\nFor smoothing the grammar yielded by the Lari and Young algor ithm, we interpolate the\nexpansion distribution of each symbol with a uniform distri bution; that is, for a grammar\nrule A → α we take its smoothed probability ps(A → α) to be\nps(A → α) = (1 − λ)pb(A → α) + λ 1\nn3 + n|T |\nwhere pb(A → α) denotes its probability before smoothing. The value n3 + n|T | is the\nnumber of rules expanding a symbol under the Lari and Young me thodology. The parameter\nλ is trained through the Inside-Outside algorithm on held-ou t data. This smoothing is also\nperformed on the grammar yielded by the Inside-Outside post -pass of our algorithm. For\neach domain, we tried n = 3 , . . . , 10.\nBecause of the computational demands of our algorithm, it is currently impractical to\napply it to large vocabulary or large training set problems. However, we present the results\nof our algorithm in three medium-sized domains. In each case , we use 4500 sentences for\ntraining, with 500 of these sentences held out for smoothing . We test on 500 sentences, and\nmeasure performance by the entropy of the test data.\nIn the ﬁrst two domains, we created the training and test data artiﬁcially so as to have\nan ideal grammar in hand to benchmark results. In particular , we used a probabilistic\ncontext-free grammar to generate the data. In the ﬁrst domai n, we created this grammar\nby hand; this simple English-like grammar is displayed in Fi gure 3.22. The numbers in\nparentheses are the probabilities associated with each rul e; rules without listed probabilities\nare equiprobable. In the second domain, we derived the gramm ar from manually parsed\ntext. From a million words of parsed Wall Street Journal data from the Penn treebank, we\nextracted the 20 most frequently occurring symbols, and the 10 most frequently occurring\nrules expanding each of these symbols. For each symbol that o ccurred on the right-hand\nside of a rule that was not one of the most frequent 20 symbols, we created a rule that\nexpanded that symbol to a unique terminal symbol. After remo ving unreachable rules, this\nyielded a grammar of roughly 30 nonterminals, 120 terminals , and 160 rules. Parameters\nwere set to reﬂect the frequency of the corresponding rule in the parsed corpus.\nFor the third domain, we took English text and reduced the siz e of the vocabulary by\nmapping each word to its part-of-speech tag. We used tagged W all Street Journal text from\n99\nS → NP VP (1.0)\nNP → D N (0.5)\n| PN (0.3)\n| NP PP (0.2)\nVP → V0 (0.2)\n| V1 NP (0.4)\n| V2 NP NP (0.2)\n| VP PP (0.2)\nPP → P NP (1.0)\nD → a | the\nN → car | bus | boy | girl\nPN → Joe | John | Mary\nP → on | at | in | over\nV0 → cried | yelled | ate\nV1 → hit | slapped | hurt\nV2 → gave | presented\nFigure 3.22: Sample grammar used to generate data\nbest entropy entropy relative\nn (bits/word) to n-gram\nideal grammar 2.30 −6.5%\nextended algorithm 8 2.31 −6.1%\nbasic algorithm 7 2.38 −3.3%\nn-gram model 4 2.46\nLari and Young 9 2.60 +5.7%\nTable 3.2: English-like artiﬁcial grammar\nthe Penn treebank, which has a tag set size of about ﬁfty. To re duce computation time, we\nonly used sentences with at most twenty words.\nIn Tables 3.2–3.4, we summarize our results. The ideal grammar denotes the grammar\nused to generate the training and test data. The rows basic algorithm and extended al-\ngorithm describe the two versions of our algorithm. For each algorit hm, we list the best\nperformance achieved over all n tried, and the best n column states which value realized this\nperformance. For n-gram models, n represents the order of the n-gram model; for the other\nalgorithms, n represents the number of nonterminal symbols used with the I nside-Outside\nalgorithm.23 In Figures 3.23–3.25, we show the complete data, the perform ance of each\nalgorithm at each n for each of the three domains.\nWe achieve a moderate but signiﬁcant improvement in perform ance over n-gram models\n23The data presented in Tables 3.2–3.4 diﬀer slightly from the corresponding data presented in an earlier\npaper (Chen, 1995). Part of this diﬀerence is due to the fact t hat we used a diﬀerent random starting\npoint for the Inside-Outside post-pass of our algorithm. In addition, we used a diﬀerent data set for the\npart-of-speech domain.\n100\nbest entropy entropy relative\nn (bits/word) to n-gram\nideal grammar 4.13 −10.4%\nextended algorithm 7 4.41 −4.3%\nbasic algorithm 9 4.41 −4.3%\nn-gram model 4 4.61\nLari and Young 9 4.64 +0.7%\nTable 3.3: Wall Street Journal-like artiﬁcial grammar\nbest entropy entropy relative\nn (bits/word) to n-gram\nn-gram model 8 3.00\nbasic algorithm 8 3.12 +4.0%\nextended algorithm 7 3.13 +4.3%\nLari and Young 9 3.60 +20.0%\nTable 3.4: English sentence part-of-speech sequences\n2.2\n2.4\n2.6\n2.8\n3\n3.2\n3.4\n3.6\n3.8\n4\n4.2\n1 2 3 4 5 6 7 8 9 10\nentropy (bits/word)\nn (model order for n-gram/no. nonterminals for IO)\nn-gram\nlari/young\nbasicextended\nideal\nFigure 3.23: Performance versus model size, English-like a rtiﬁcial grammar\n101\n4\n4.2\n4.4\n4.6\n4.8\n5\n5.2\n5.4\n5.6\n1 2 3 4 5 6 7 8 9 10\nentropy (bits/word)\nn (model order for n-gram/no. nonterminals for IO)\nn-gram\nlari/young\nbasic\nextended\nideal\nFigure 3.24: Performance versus model size, WSJ-like artiﬁ cial grammar\n3\n3.2\n3.4\n3.6\n3.8\n4\n4.2\n4.4\n4.6\n1 2 3 4 5 6 7 8 9 10\nentropy (bits/word)\nn (model order for n-gram/no. nonterminals for IO)\nn-gram\nlari/young\nbasic\nextended\nFigure 3.25: Performance versus model size, part-of-speec h sequences\n102\nA1 ⇒∗ (on|over) John\nA2 ⇒∗ in (Mary|the girl )\nA3 ⇒∗ a bus\nA4 ⇒∗ (the boy |Joe|a car |Mary|a girl |a boy ) at\nA5 ⇒∗ (a girl |a boy |the bus |Mary|the girl |John|the car ) ( hurt|slapped|hit) the car\nA6 ⇒∗ (the bus |Mary|the girl ) cried\n.\n.\n.\n.\n.\n. (13 prepositional phrases)\n.\n.\n.\nA20 ⇒∗ (a girl |a boy |the bus |Mary|the girl |John|the car ) slapped (the boy |a bus )\nA21 ⇒∗ at John\nA22 ⇒∗ (a girl |a boy |the bus |Mary|the girl |John|the car ) ( hurt|slapped|hit) the bus\nFigure 3.26: Expansions of symbols A with highest frequency p(A)\nand the Lari and Young algorithm in the ﬁrst two domains, whil e in the part-of-speech\ndomain we are outperformed by n-gram models but we vastly outperform the Lari and\nYoung algorithm.\nComparing the two versions of our algorithm, we ﬁnd that the e xtended algorithm\nperforms signiﬁcantly better than the basic algorithm on th e English-like artiﬁcial text,\nand performs marginally better for most n on the WSJ-like artiﬁcial text. In the part-of-\nspeech domain, the two algorithms perform almost identical ly for most n.\nIn Figure 3.26, we display a sample of the grammar induced in t he English-like artiﬁcial\ndomain. The ﬁgure displays the expansions of the symbols A with the highest probabilities\np(A), which is proportional to how frequently the reduction X → A is used. In some\nsense, these symbols are the most frequently occurring symb ols. Each row corresponds to a\ndiﬀerent symbol, listed in decreasing frequency starting fr om the most frequent symbol in\nthe grammar. The expression displayed expresses all possib le strings the symbol expands\nto; it does not reﬂect the actual grammar rules with that symb ol on the left-hand side. For\nexample, the most frequent symbol in the grammar expands to t he strings on John and over\nJohn, and the ﬁfth-most frequent symbol expands to the strings a girl hurt the car , a boy\nhurt the car , etc. Except for the fourth symbol, all of these symbols expa nd to strings that\nare constituents according to the original grammar. Thus, i n this domain our algorithm is\nable to capture some of the structure present in the original grammar.\nIn Figure 3.27, we display the grammar induced by the Lari and Young algorithm with\nnine nonterminal symbols in the English-like artiﬁcial dom ain. We display only those rules\nwith probability above 0.01; rule probabilities are shown i n parentheses. Unlike in Figure\n3.26 where we list all strings a symbol expands to, in this ﬁgure we list the most frequent\nrules a symbol expands with. This grammar does a reasonable job of g rouping together\nsimilar terminal symbols. However, it does less well at reco gnizing higher-level structures in\nthe grammar. Most symbols in the induced grammar do not match well with the symbols\n103\nA1 → hit (0.08) | hurt (0.08) | slapped (0.09) | presented (0.06) | gave (0.06)\n| over (0.12) | on (0.12) | in (0.12) | at (0.12)\n| A1 A2 (0.03) | A1 A8 (0.04) | A5 A1 (0.05)\nA2 → Mary (0.07) | Joe (0.24) | John (0.09)\n| A2 A5 (0.04) | A4 A3 (0.12) | A4 A6 (0.42)\nA3 → girl (0.03) | car (0.42) | boy (0.31) | bus (0.15)\n| A3 A5 (0.03) | A6 A5 (0.05)\nA4 → the (0.50) | a (0.50)\nA5 → yelled (0.04) | cried (0.04) | ate (0.04)\n| A1 A2 (0.21) | A1 A7 (0.19) | A1 A8 (0.23) | A1 A9 (0.21) | A5 A5 (0.04)\nA6 → girl (0.26) | car (0.23) | boy (0.24) | bus (0.26)\nA7 → Mary (0.09) | Joe (0.06) | John (0.19)\n| A2 A5 (0.02) | A4 A3 (0.03) | A4 A6 (0.55) | A7 A5 (0.02) | A8 A5 (0.04)\nA8 → Mary (0.17) | Joe (0.07) | John (0.09)\n| A4 A3 (0.04) | A4 A6 (0.58) | A8 A5 (0.05)\nA9 → A2 A5 (0.24) | A7 A5 (0.27) | A8 A5 (0.37) | A9 A5 (0.08) | A9 A7 (0.03)\nFigure 3.27: Grammar induced with Lari and Young algorithm\nWSJ-like n entropy no. time\nartiﬁcial (bits/word) params (sec)\nn-gram model 3 4.61 15000 50\nLari and Young 9 4.64 2000 30000\nbasic alg./ﬁrst pass 800 1000\nbasic alg./post-pass 5 4.56 4000 5000\nTable 3.5: Number of parameters and training time of each alg orithm\nin the original grammar. For example, the symbol A3 groups together nouns with nouns\nfollowed by a verb taking no arguments. Hence, we see that our algorithm is clearly better\nthan the Lari and Young algorithm at capturing relevant stru cture.\nIn Table 3.5, we display a sample of the number of parameters a nd execution time (on\na Decstation 5000/33) associated with each algorithm. We ch oose n to yield approximately\nequivalent performance for each algorithm. The ﬁrst pass row refers to the main grammar\ninduction phase of our algorithm, and the post-pass row refers to the Inside-Outside post-\npass.\nNotice that our algorithm produces a signiﬁcantly more comp act model than the n-gram\nmodel, while running signiﬁcantly faster than the Lari and Y oung algorithm even though\nboth algorithms employ the Inside-Outside algorithm. Part of this discrepancy is due to\nthe fact that we require a smaller number of new nonterminal s ymbols to achieve equivalent\nperformance, but we have also found that our post-pass conve rges more quickly even given\nthe same number of nonterminal symbols.\nIn Figures 3.28 and 3.29, we display the execution time and me mory usage of the main\ngrammar induction algorithm on various amounts of training data. Both of these graphs\n104\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n0 5000 10000 15000 20000 25000 30000 35000 40000\nexecution time (seconds)\ntraining data (tokens)\nFigure 3.28: Execution time versus training data size\n0\n2\n4\n6\n8\n10\n12\n14\n16\n0 5000 10000 15000 20000 25000 30000 35000 40000\nmemory usage (Mbytes)\ntraining data (tokens)\nFigure 3.29: Memory usage versus training data size\n105\nare nearly linear. 24\n3.7 Discussion\nOur algorithm consistently outperformed the Lari and Young algorithm in these experi-\nments. One perspective of this result can be taken noticing t hat both algorithms use the\nInside-Outside algorithm as a ﬁnal step. As the Inside-Outs ide algorithm is a hill-climbing\nalgorithm, it can be interpreted as ﬁnding the nearest local minimum in the search space\nto the initial grammar. In the Lari and Young framework, this initial grammar is chosen\nessentially randomly. In our framework, this initial gramm ar is chosen intelligently using a\nBayesian search. Thus, it is not surprising our algorithm ou tperforms the Lari and Young\nalgorithm.\nFrom a diﬀerent perspective, the main mechanism of the Lari an d Young algorithm for\nselecting grammar rules is the hill-climbing search of the I nside-Outside algorithm. If we\nview this in terms of a move set, the basic move of the Lari and Y oung algorithm is to\nadjust rule probabilities. In our algorithm, we use a rich mo ve set that corresponds to\nsemantically meaningful notions. We have moves that can exp licitly create new rules and\nnonterminal symbols unlike Lari and Young, moves that expre ss concepts such as classing,\nspecialization, and generalization. While both algorithm s employ greedy searches and can\nthus be interpreted as ﬁnding nearby local minima, our resul ts demonstrate that by using\na richer move set this constraint is much less serious. There have been several results\ndemonstrating the severity of the local minima problem for t he Inside-Outside algorithm\n(Chen et al. , 1993; de Marcken, 1995).\nIn terms of eﬃciency, the algorithms diﬀer signiﬁcantly beca use of the diﬀerent ways\nrules are selected. In the Lari and Young algorithm, one star ts with a large grammar and\nuses the Inside-Outside algorithm to prune away unwanted ru les by setting their proba-\nbilities to near zero. This approach scales poorly because t he grammar worked with is\nsubstantially larger than the desired target grammar, and u sing large grammars has a great\ncomputational expense. It is impractical to use more than te ns of nonterminal symbols with\nthe Lari and Young approach because the number of grammar rul es is cubic in the number\nof nonterminal symbols.\nIn contrast, our algorithm begins with a small grammar and ad ds rules incrementally.\nThe working grammar only contains rules perceived as worthy and is thus not unnecessarily\nlarge. In our algorithm, we can build grammars with hundreds of nonterminal symbols, and\nstill the execution time of the algorithm is dominated by the Inside-Outside post-pass.\nOutperforming n-gram models in the ﬁrst two domains demonstrates that our al gorithm\nis able to take advantage of the grammatical structure prese nt in data. For example, unlike\n24The jump in the graph of memory usage betwen 30,000 and 35,000 tokens is an artifact of the training\ndata used to produce the graph. At this point in the training d ata, there are many long sequences of a single\ntoken. It turns out that the number of possible ways to parse a long sequence of a single token is very large.\nFor example, if we denote the repeated token as a, then any symbol that can expand to ak for some k can\nbe used to parse any substring of length k in the long sequence of a’s. The jump in memory is due to the\nadditional memory needed to store the parse chart.\n106\nn-gram models our grammar can express classing and can handle long-distance dependen-\ncies. However, the superiority of n-gram models in the part-of-speech domain indicates that\nto be competitive in modeling naturally-occurring data, it is necessary to model colloca-\ntional information accurately. We need to modify our algori thm to more aggressively model\nn-gram information.\n3.7.1 Contribution\nThis research represents a step forward in the quest for deve loping grammar-based language\nmodels for natural language. We consistently outperform th e Lari and Young algorithm\nacross domains and outperform n-gram language models in medium-sized artiﬁcial domains.\nThe algorithm runs in near-linear time and space in terms of t he amount of training data\nand the grammars induced are relatively compact, so the algo rithm scales well.\nWe demonstrate the viability of the Bayesian approach for gr ammar induction, and we\nshow that the minimum description length principle is a usef ul paradigm for building prior\ndistributions for grammars. A minimum description length a pproach is crucial for methods\nthat do not limit the size of grammars; this approach favors s maller grammars, which is\nnecessary for preventing overﬁtting.\nWe describe an eﬃcient framework for performing grammar ind uction. We present\nan algorithm that parses each sentence only once, and we use t he concept of triggers to\nconstrain the set of moves considered at each point. We descr ibe a rich move set for\nmanipulating grammars in intuitive ways, and this enables o ur search to be eﬀective despite\nits greedy nature.\nFurthermore, this induction framework is not restricted to probabilistic context-free\ngrammars. For example, notice that we do not parameterize re petition rules strictly within\nthe PCFG paradigm. More complex grammar formalisms can be co nsidered without chang-\ning the computational complexity of the algorithm; we just n eed to enhance the move set.\n107\nChapter 4\nAligning Sentences in Bilingual\nText\nIn this chapter, we describe an algorithm for aligning sente nces with their translations in\na bilingual corpus (Chen, 1993). In experiments with the Han sard Canadian parliament\nproceedings, our algorithm yields signiﬁcantly better acc uracy than previous algorithms.\nIn addition, it is eﬃcient, robust, language-independent, and parallelizable. Of the three\nstructural levels at which we model language in this thesis, this represents work at the\nsentence level.\n4.1 Introduction\nA bilingual corpus is a corpus of text replicated in two languages. For example, the Hansard\nbilingual corpus contains the Canadian parliament proceed ings in both English and French.\nBilingual corpora have proven useful in many tasks, includi ng machine translation (Brown et\nal., 1990; Sadler, 1989), sense disambiguation (Brown et al. , 1991a; Dagan et al. , 1991; Gale\net al. , 1992), and bilingual lexicography (Klavans and Tzoukerma nn, 1990; Warwick and\nRussell, 1990).\nFor example, a bilingual corpus can be used to automatically construct a bilingual\ndictionary. A bilingual dictionary can be expressed as a pro babilistic model p(f|e) of how\nfrequently a particular word e in one language, say English, translates to a particular wor d\nf in another language, say French. Intuitively, one should be able to recover such a model\nfrom a bilingual corpus. For example, if a human translator w ere to mark exactly which\nFrench words correspond to which English words, we would be a ble to count how often a\ngiven English word e translates to each French word f and then normalize to get p(f|e),\ni.e.,\np(f|e) = c(e, f )\n∑\nf c(e, f )\nwhere c(e, f ) denotes how often the word e translates to f. However, this word alignment\ninformation is not typically included in a bilingual corpus .\nConsider the case where instead of knowing which words correspond to each other, we\n108\nEnglish ( ⃗E)\nE1 Hon. members opposite scoﬀ\nat the freeze suggested by this\nparty; to them it is laughable.\nFrench ( ⃗F )\nF1 Les d´ eput´ es d’en face se\nmoquent du gel que a propos´ e\nnotre parti.\nF2 Pour eux, c’est une mesure\nrisible.\nFigure 4.1: Two-to-one sentence alignment\njust know which sentences correspond to each other. Then, it is still possible to build an\napproximate model of how frequently words translate to each other by using an analogous\nequation to above:\np′(f|e) = c′(e, f )\n∑\nf c′(e, f )\nwhere c′(e, f ) denotes how frequently the words e and f occur in aligned sentences . For\nwords e and f that are mutual translations, c′(e, f ) will be high since every time e occurs\nin an English sentence f will also occur in the corresponding French sentence. For wo rds\ne and f that are not translations, while c′(e, f ) may be above zero it will most probably\nnot be very high since it is very unlikely that unrelated word s regularly co-occur in aligned\nsentences.1 Thus, with sentence alignment information we can build an ap proximate word\ntranslation model p′(f|e). Furthermore, we can use this approximate model p′(f|e) to\nbootstrap the construction of an even more accurate model of word translation. Given\nsentence alignment information and a model p′(f|e) of which words translate to each other,\nwe can produce a relatively accurate word alignment. Then, u sing the procedure described\nin the last paragraph of counting aligned word pairs we can pr oduce an improved word-to-\nword translation model.\nHence, sentence alignment is a useful step in processing a bi lingual corpus. All of the\napplications mentioned earlier such as machine translatio n and sense disambiguation require\nbilingual corpora that are sentence-aligned. As human tran slators typically do not include\nsentence alignment information when creating bilingual co rpora, automatic algorithms for\nsentence alignment are immensely useful.\nIn this work, we describe an accurate and eﬃcient algorithm f or bilingual sentence\nalignment. The task is diﬃcult because sentences frequentl y do not align one-to-one. For\nexample, in Figure 4.1 we show an example of two-to-one align ment. In addition, there\nare often deletions in one of the supposedly parallel corpor a of a bilingual corpus. These\ndeletions can be substantial; in the version of the Canadian Hansard corpus we worked with,\nthere are many deletions of several thousand sentences and o ne deletion of over 90,000\nsentences. Such anomalies are not uncommon in very large cor pora. Large corpora are\noften stored as as sizable set of smaller ﬁles, some of which m ay be accidentally deleted or\n1This is not quite accurate. For example, the count c′(e, “le”) may be high for many English words e\njust because the word le occurs in most French sentences. However, this eﬀect can be c orrected for, such as\ndescribed in Section 4.4.1.\n109\ntransposed.\n4.1.1 Previous Work\nThe ﬁrst sentence alignment algorithms successfully appli ed to large bilingual corpora are\nthose of Brown et al. (1991b) and Gale and Church (1991; 1993). Brown et al. base align-\nment solely on the number of words in each sentence; the actua l identities of words are\nignored. The general idea is that the closer in length two sen tences are, the more likely\nthey align. They construct a probabilistic model of alignme nt and select the alignment\nwith the highest probability under this model. The paramete rs of their model include\np(eafb), the probability that a English sentences align with b French sentences, and p(lf |le),\nthe probability that an English sentence (or sentences) con taining le words translates to a\nFrench sentence (or sentences) containing lf words. These parameters are estimated sta-\ntistically from bilingual text. To search for the most proba ble sentence alignment under\nthis alignment model, dynamic programming (Bellman, 1957) , an eﬃcient and exhaustive\nsearch method, is used.\nBecause dynamic programming requires time quadratic in the number of sentences to be\naligned and a bilingual corpus can be many millions of senten ces, it is not practical to align a\nlarge corpus as a single unit. The computation required is dr astically reduced if the bilingual\ncorpus can be subdivided into smaller chunks. Brown et al. use anchors to perform this\nsubdivision. An anchor is a piece of text of easily recognizable form likely to be pre sent\nat the same location in both of the parallel corpora of a bilin gual corpus. For example,\nBrown et al. notice that comments such as Author = Mr. Cossitt and Time = (1415)\nare interspersed in the English text of the Hansard corpus an d corresponding comments\nare present in the French text. Dynamic programming is ﬁrst u sed to determine which\nanchors align with each other, and then dynamic programming is used again to align the\ntext between anchors.\nThe Gale and Church algorithm is similar to the Brown algorit hm except that instead\nof basing alignment on the number of words in sentences, alignment is based on the number\nof characters in sentences. In addition, instead of using a probabilistic model and searching\nfor the alignment with the highest probability, they assign lengths to diﬀerent alignments\nand search for the alignment with the smallest length. 2 Dynamic programming is again\nused to search for the best alignment. Large corpora are assu med to be already subdivided\ninto smaller chunks.\nWhile these algorithms have achieved remarkably good perfo rmance, there is deﬁnite\nroom for improvement. For example, consider the excerpt fro m the Hansard corpus depicted\nin Figure 4.2. Length-based algorithms do not particularly favor aligning Yes with Oui over\nNon or aligning Mr. McInnis with M. McInnis over M. Saunders . Thus, such algorithms\ncan easily misalign passages like these by an even number of s entences if there are sentences\nmissing in one of the languages. Constructions in one langua ge that translate to a very\n2This can be interpreted as just working in description space instead of probability space. As described\nin Section 3.2.3, these two spaces are in some sense equivale nt.\n110\nEnglish ( ⃗E)\nE1 Mr. McInnis?\nE2 Yes.\nE3 Mr. Saunders?\nE4 No.\nE5 Mr. Cossitt?\nE6 Yes.\nFrench ( ⃗F )\nF1 M. McInnis?\nF2 Oui.\nF3 M. Saunders?\nF4 Non.\nF5 M. Cossitt?\nF6 Oui.\nFigure 4.2: A bilingual corpus fragment\ndiﬀerent number of words in the other language may also cause e rrors. In general, length-\nbased algorithms are not robust; they can align unrelated te xt because word identities are\nignored.\nAlignment algorithms that take advantage of lexical inform ation oﬀer a potential for\nhigher accuracy. Previous work includes algorithms by Kay a nd R¨ oscheisen (1993) and\nCatizone et al. (1989). Kay and R¨ oscheisen perform alignment using a relax ation paradigm.\nThey keep track of all possible sentence pairs that may align to each other. Initially, this\nset is very large; it is just constrained by the observation t hat a sentence in one language\nis probably aligned with a sentence in the other language wit h the same relative position\nin the corpus. For example, an English sentence halfway thro ugh the Hansard English\ncorpus is probably aligned to a French sentence near the midp oint of the Hansard French\ncorpus. Given this set of possible alignment pairs, word tra nslations are induced based\non distributional information. Using these induced word tr anslations, the set of possible\nalignment pairs is pruned, which then yields new word transl ations, etc. This process is\nrepeated until convergence. However, previous lexically- based algorithms have not proved\neﬃcient enough to be suitable for large corpora. The largest corpus aligned by Kay and\nR¨ oscheisen contains 1,000 sentences in each language; exi sting bilingual corpora have many\nmillions of sentences.\n4.1.2 Algorithm Overview\nWe describe a fast and accurate algorithm for sentence align ment that uses lexical informa-\ntion. Like Brown et al. , we build a sentence-based translation model and ﬁnd the ali gnment\nwith the highest probability given the model. However, unli ke Brown et al. the translation\nmodel makes use of a word-to-word translation model. We boot strap these models using a\nsmall amount of pre-aligned text; the models then reﬁne them selves on the ﬂy during the\nalignment process. The search strategy used is dynamic prog ramming with thresholding.\nBecause of thresholding, the search is linear in the length o f the corpus so that a corpus\nneed not be subdivided into smaller chunks.\nIn addition, the search strategy includes a separate mechan ism for handling large dele-\ntions in one of the corpora of a bilingual corpus. When a delet ion is present, thresholding\nis not eﬀective and dynamic programming requires time quadra tic in the length of the dele-\ntion to identify its extent, which is unacceptable for large deletions. Instead, we have a\n111\nmechanism that keys oﬀ of rare words to locate the bounds of a d eletion in time linear in\nthe length of the deletion. This deletion recovery mechanis m can also be used to subdivide\na corpus into small chunks; this enables the parallelizatio n of our algorithm.\n4.2 The Alignment Model\n4.2.1 The Alignment Framework\nIn this section, we present the general framework of our algo rithm, that of building a\nprobabilistic translation model and ﬁnding the alignment t hat yields the highest probability\nunder this model.\nMore speciﬁcally, we try to ﬁnd the alignment ⃗A with the highest probability given the\nbilingual corpus, i.e.,\n⃗A = arg max\n⃗A\np( ⃗A| ⃗E, ⃗F ) (4.1)\nwhere ⃗E and ⃗F denote the English corpus and French corpus, respectively. (In this paper,\nwe assume the two languages being aligned are English and Fre nch; however, none of the\ndiscussion is speciﬁc to this language pair, except for the d iscussion of cognates in Section\n4.3.6.) For now, we take an alignment ⃗A to be a list of integers representing which sentence\nin the French corpus is the ﬁrst to align with each successive sentence in the English corpus.\nFor example, the alignment ⃗A = (1 , 2, 4, 5, . . . ) aligns the ﬁrst English sentence with the ﬁrst\nFrench sentence, the second English sentence with the secon d and third French sentences,\nthe third English sentence with the fourth French sentence, etc.\nWe manipulate equation (4.1) into a more intuitive form:\n⃗A = arg max\n⃗A\np( ⃗A| ⃗E, ⃗F )\n= arg max\n⃗A\np( ⃗A, ⃗E, ⃗F )\n∑\n⃗A p( ⃗A, ⃗E, ⃗F )\n= arg max\n⃗A\np( ⃗A, ⃗E, ⃗F )\n= arg max\n⃗A\np( ⃗A, ⃗F | ⃗E)p( ⃗E)\n= arg max\n⃗A\np( ⃗A, ⃗F | ⃗E)\nThe probability p( ⃗A, ⃗F | ⃗E) represents the probability that the English corpus ⃗E translates\nto the French corpus ⃗F with alignment ⃗A. Making the assumption that successive sentences\ntranslate independently of each other, we can re-express p( ⃗A, ⃗F | ⃗E) as\np( ⃗A, ⃗F | ⃗E) =\nl( ⃗E)∏\ni=1\np(F Ai+1−1\nAi |Ei) (4.2)\nwhere Ei denotes the ith sentence in the English corpus, F j\ni denotes the ith through jth\n112\nEnglish ( ⃗E)\nE1 That is what the consumers\nare interested in and that is\nwhat the party is interested in.\nE2 Hon. members opposite scoﬀ\nat the freeze suggested by this\nparty; to them it is laughable.\nFrench ( ⃗F )\nF1 Voil` a ce qui int´ eresse le\nconsommateur et voil` a ce qui\nint´ eresse notre parti.\nF2 Les d´ eput´ es d’en face se\nmoquent du gel que a propos´ e\nnotre parti.\nF3 Pour eux, c’est une mesure\nrisible.\nFigure 4.3: A bilingual corpus\nsentences in the French corpus, Ai denotes the index of the ﬁrst French sentence aligning to\nthe ith English sentence in alignment ⃗A, and l( ⃗E) denotes the number of sentences in the\nEnglish corpus. 3 We refer to the distribution p(F j\ni |E) as a translation model, as it describes\nhow likely an English sentence E translates to the French sentences F j\ni .\nWith an accurate translation model p(F j\ni |E), we can use the relation\n⃗A = arg max\n⃗A\np( ⃗A, ⃗F | ⃗E) = arg max\n⃗A\nl( ⃗E)∏\ni=1\np(F Ai+1−1\nAi |Ei)\nto perform accurate sentence alignment. To give an example, consider the bilingual corpus\n( ⃗E, ⃗F ) displayed in Figure 4.3. Now, consider the alignment ⃗A1 = (1 , 2) aligning sentence\nE1 to sentence F1 and sentence E2 to sentences F2 and F3. We have\np( ⃗A1, ⃗F | ⃗E) = p(F1|E1)p(F 3\n2 |E2),\nThis value should be relatively large, since F1 is a good translation of E1 and F 3\n2 is a good\ntranslation of E2. Another possible alignment ⃗A2 = (1 , 1) aligns sentence E1 to nothing\nand sentence E2 to F1, F2, and F3. We get\np( ⃗A2, ⃗F | ⃗E) = p(ǫ|E1)p(F 3\n1 |E2)\nThis value should be fairly low, as ǫ is a poor translation of E1 and F 3\n1 is a poor translation\nof E2. Hence, if our translation model p(F j\ni |E) is accurate we will have\np( ⃗A1, ⃗F | ⃗E) ≫ p( ⃗A2, ⃗F | ⃗E)\nIn general, the more sentences that are mapped to their trans lations in an alignment ⃗A, the\n3 In this discussion and future discussion, we only consider a lignments ⃗A that are consistent with ⃗E and\n⃗F . For example, we do not consider alignments of incorrect len gth or alignments that refer to sentences\nbeyond the end of the French corpus. Obviously, for inconsis tent alignments ⃗A we have p( ⃗A, ⃗F | ⃗E) = 0.\nFurthermore, in equation (4.2) Al( ⃗E)+1 is implicitly taken to be l( ⃗F ) + 1; this enforces the constraint that\nthe last English sentence aligns with French sentences endi ng in the last French sentence.\n113\nhigher the value of p( ⃗A, ⃗F | ⃗E).\nHowever, because our translation model is expressed in term s of a conditional distribu-\ntion p(F j\ni |E), the above framework is not amenable to the situation where a French sentence\ncorresponds to multiple English sentences. Hence, we use a s lightly diﬀerent framework.\nWe view a bilingual corpus as a sequence of sentence beads (Brown et al. , 1991b), where a\nsentence bead corresponds to an irreducible group of senten ces that align with each other.\nFor example, the correct alignment ⃗A1 of the bilingual corpus in Figure 4.3 consists of the\nsentence bead [ E1, F1] followed by the sentence bead [ E2, F 3\n2 ]. Instead of expressing an\nalignment ⃗A as a list of sentence indices in the French corpus, we express an alignment ⃗A as\na list of pair of indices (( Ae\n1, Af\n1 ), (Ae\n2, Af\n2 ), . . . ), the indices representing which English and\nFrench sentence begin each successive sentence bead. Under this new convention, we have\nthat ⃗A1 = ((1 , 1), (2, 2)). Unlike the previous framework, this framework is symme tric and\ncan handle the case where a French sentence aligns with zero o r multiple English sentences.\nIn this framework, instead of taking\n⃗A = arg max\n⃗A\np( ⃗A, ⃗F | ⃗E) = arg max\n⃗A\nl( ⃗E)∏\ni=1\np(F Ai+1−1\nAi |Ei)\nwe take\n⃗A = arg max\n⃗A\np( ⃗A, ⃗E, ⃗F ) = arg max\n⃗A\npA-len(l( ⃗A))\nl( ⃗A)∏\ni=1\np([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n]) (4.3)\nwhere l( ⃗A) denotes the number of sentence beads in ⃗A and pA-len(l) denotes the probability\nthat an alignment contains exactly l sentence beads. The term pA-len(l( ⃗A)) is necessary for\nnormalization purposes; otherwise we would not have ∑\n⃗A, ⃗E, ⃗F p( ⃗A, ⃗E, ⃗F ) = 1. 4 Instead of\nhaving a conditional translation model p(F j\ni |E), our translation model is now expressed as\na distribution p([Ej\ni , F l\nk]) representing the frequencies of sentence beads [ Ej\ni , F l\nk].\n4To show this, notice that for any l we have that\n∑\nx1,...,xl\nl∏\ni=1\np(xi) =\n∑\nx1\np(x1)\n∑\nx2\np(x2) · · ·\n∑\nxl\np(xl) = 1 · 1 · · · · · 1 = 1\nApplying this relation to equation (4.3), we have that\n∑\n⃗A, ⃗E, ⃗F\np( ⃗A, ⃗E, ⃗F ) =\n∑\n⃗A, ⃗E, ⃗F\npA-len(l( ⃗A))\nl( ⃗A)∏\ni=1\np([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])\n=\n∑\nl\npA-len(l)\n∑\nl( ⃗A)=l, ⃗E, ⃗F\nl∏\ni=1\np([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])\n=\n∑\nl\npA-len(l)\n= 1\nWithout the term pA-len(l( ⃗A)) the sum is inﬁnite.\n114\n4.2.2 The Basic Translation Model\nAs we can see from equation (4.3), the key to accurate alignme nt in our framework is\ncoming up with an accurate translation model p([Ej\ni , F l\nk]). For this translation model, we\ndesire the simplest model that incorporates lexical inform ation eﬀectively. We describe our\nmodel in terms of a series of increasingly complex models. In this section, we consider\nonly sentence beads [ E, F ] containing a single English sentence E = e1 · · · el(E) and single\nFrench sentence F = f1 · · · fl(F ). As a starting point, consider a model that assumes that\nall individual words are independent, i.e., a model where the probability of some text is the\nproduct of the probabilities of each word in the text. More sp eciﬁcally, we can take\np′([E, F ]) = pe-len(l(E))pf-len(l(F ))\nl(E)∏\ni=1\npe(ei)\nl(F )∏\nj=1\npf (fj )\nwhere pe-len(l) is the probability that an English sentence is l words long, pf-len(l) is the\nprobability that a French sentence is l words long, pe(ei) is the frequency of the word ei in\nEnglish, and pf (fj) is the frequency of the word fj in French. The terms pe-len(l(E)) and\npf-len(l(F )) are necessary to make ∑\nE,F p′([E, F ]) = 1 (as in equation (4.3)). 5 For example,\nwe have that\np′([do you speak French , parlez-vous fran¸ cais]) =\npe-len(4)pf-len(3)pe(do)pe(you)pe(speak)pe(French)pf (parlez)pf (vous)pf (fran¸ cais)\nClearly, this is a poor translation model because it takes th e English sentence and French\nsentence to be independent, so it does not assign higher prob abilities to sentence pairs that\nare translations. For example, it would assign about the sam e probabilities to the following\ntwo sentence beads:\n[do you speak French , parlez-vous fran¸ cais]\n[did they eat German , parlez-vous fran¸ cais]\nTo capture the dependence between individual English words and individual French\nwords, we assign probabilities to word pairs in addition to j ust single words. For two words\ne and f that are mutual translations, instead of having the two term s pe(e) and pf (f) in\nthe above equation we would like a single term p(e, f ) that is substantially larger than\npe(e)pf (f). To this end, we introduce the concept of a word bead. A word bead is either\na single English word, a single French word, or a single Engli sh word and a single French\nword. We refer to these as 1:0, 0:1, and 1:1 word beads, respec tively. Instead of modeling a\npair of sentences as a list of independent words, we model sen tences as a list of word beads,\nusing the 1:1 word beads to capture the dependence between En glish and French words. To\n5This model can be considered somewhat similar to the alignme nt model used by Brown et al.. As the\nprobability of words are taken to be independent, these prob abilities do not depend on sentence alignment\nand can be ignored, as in Brown. However, unlike Brown we also take sentence lengths to be independent;\na model closer to Brown would express sentence lengths as a jo int probability plen(l(E), l(F )) that assigned\nhigher probabilities to sentence pairs with similar length s.\n115\naddress the issue that corresponding English and French wor ds may not occur in identical\npositions in their respective sentences, we abstract over t he positions of words in sentences;\nwe consider sentences to be unordered multisets 6 of words, so that 1:1 word beads can pair\nwords from arbitrary positions in sentences.\nAs a ﬁrst cut at this behavior, consider the following “model ”:\np′′(¯b) = pb-len(l(¯b))\nl(¯b)∏\ni=1\npb(bi)\nwhere ¯b = {b1, . . . , b l(¯b)} is a multiset of word beads, pb-len(l) is the probability that an En-\nglish sentence and a French sentence contain l word beads, and pb(bi) denotes the frequency\nof the word bead bi. This simple model captures lexical dependencies between E nglish and\nFrench sentences. For example, we might express the sentenc e bead\n[do you speak French , parlez-vous fran¸ cais]\nwith the word beading\n¯b = {[do], [you, vous], [speak, parlez], [French, fran¸ cais]}\nwith probability\np′′(¯b) = pb-len(4)pb([do])pb([you, vous])pb([speak, parlez])pb([French, fran¸ cais])\nIf pb([e, f ]) ≫ pb([e])pb([f]) for words e and f that are mutual translations, then word\nbeadings of sentence pairs that contain many words that are m utual translations can have\nmuch higher probability than word beadings of unrelated sen tence pairs.\nHowever, this “model” p′′(¯b) does not satisfy the constraint that ∑\n¯b p′′(¯b) = 1. To see\nthis, consider the case that the ordering of beads is signiﬁc ant so that instead of having\na multiset ¯b = {b1, . . . , b l(¯b)}, we have a list ⃗b = ( b1, . . . , b l(⃗b)). For this case, we have\n∑\n⃗b p′′(⃗b) = 1 (as for equation (4.3)). Then, because our beadings ¯b are actually unordered,\nmultiple terms in this last sum that are just diﬀerent orderin gs of the same multiset will\nbe collapsed to a single term in our actual summation. Hence, the true ∑\n¯b p′′(¯b) will be\nsubstantially less than one. To force this model to sum to one , we simply normalize to\nretain the qualitative aspects of the model. We take\np(¯b) = pb-len(l(¯b))\nNl(¯b)\nl(¯b)∏\ni=1\npb(bi)\nwhere\nNl =\n∑\nl(¯b)=l\nl∏\ni=1\npb(bi) (4.4)\n6A multiset is a set in which a given element can occur more than once.\n116\nTo derive the probabilities of sentence beads p([E, F ]) from the probabilities of word\nbeadings p(¯b), we need to consider the issue of word ordering. A beading ¯b describes an\nunordered multiset of English and French words, while sentences are ordered sequences of\nwords. We need to model word ordering, and ideally the probab ility of a sentence bead\nshould depend on the ordering of its component words. For exa mple, the sentence John\nate Fido should have a higher probability of aligning with the senten ce Jean a mang´ e Fido\nthan with the sentence Fido a mang´ e Jean . However, modeling how word order mutates\nunder translation is notoriously diﬃcult (Brown et al. , 1993), and it is unclear how much\nimprovement in accuracy an accurate model of word order woul d provide. Hence, we ignore\nthis issue and take all word orderings to be equiprobable. Le t O(E) denote the number of\ndistinct ways of ordering the words in a sentence E.7 Then, we take\np([E, F ]|¯b) = 1\nO(E)O(F ) (4.5)\nfor those sentence beads [ E, F ] consistent with ¯b, i.e., those sentence beads containing the\nsame words as ¯b. (For inconsistent beads [ E, F ], we have p([E, F ]|¯b) = 0.)\nThis gives us\np([E, F ], ¯b) = p(¯b)p([E, F ]|¯b) = pb-len(l(¯b))\nNl(¯b)O(E)O(F )\nl(¯b)∏\ni=1\npb(bi)\nTo get the total probability p([E, F ]) of a sentence bead, we need to sum over all beadings\n¯b consistent with [ E, F ], giving us\np([E, F ]) =\n∑\n¯b∼[E,F ]\np([E, F ], ¯b) =\n∑\n¯b∼[E,F ]\npb-len(l(¯b))\nNl(¯b)O(E)O(F )\nl(¯b)∏\ni=1\npb(bi) (4.6)\nwhere ¯b ∼ [E, F ] denotes ¯b being consistent with [ E, F ].\n4.2.3 The Complete Translation Model\nIn this section, we extend the translation model to other typ es of sentence beads besides\nbeads that contain a single English and French sentence. Lik e Brown et al. , we only consider\nsentence beads consisting of one English sentence, one Fren ch sentence, one English sentence\nand one French sentence, two English sentences and one Frenc h sentence, and one English\nsentence and two French sentences. We refer to these as 1:0, 0 :1, 1:1, 2:1, and 1:2 sentence\nbeads, respectively.\n7 For a sentence E containing words that are all distinct, we just have O(E) = l(E)!. More generally,\nwe have that O(E) =\n(l(E)\n{m(ei)}\n)\n, where {m(ei)} denotes the multiplicities of the diﬀerent words ei in the\nsentence.\n117\nFor 1:1 sentence beads, we take\np([E, F ]) = p(1 : 1)\n∑\n¯b∼[E,F ]\np1:1(l(¯b))\nN1:1\nl(¯b)O(E)O(F )\nl(¯b)∏\ni=1\npb(bi) (4.7)\nThis diﬀers from equation (4.6) in that we have added the term p(1 : 1) representing the\nprobability or frequency of 1:1 sentence beads; this term is necessary for normalization\npurposes now that we consider other types of sentence beads. In addition, we now refer\nto pb-len as p1:1 and to Nl as N1:1\nl as there will be analogous terms for the other types of\nsentence beads.\nTo model 1:0 sentence beads, we use a similar equation except that we only need to\nconsider 1:0 word beads ( i.e., individual English words), and we do not need to sum over\nbeadings since there is only one word beading consistent wit h a 1:0 sentence bead. We take\np([E]) = p(1 : 0) p1:0(l(E))\nN1:0\nl(E)O(E)\nl(E)∏\ni=1\npe(ei) (4.8)\nInstead of the distribution pb(bi) of word bead frequencies, we have the distribution pe(ei)\nof English word frequencies. We use an analogous equation fo r 0:1 sentence beads.\nFor 2:1 sentence beads, we take\np([Ei+1\ni , F ]) = p(2 : 1)\n∑\n¯b∼[Ei+1\ni ,F ]\np2:1(l(¯b))\nN2:1\nl(¯b)O(Ei)O(Ei+1)O(F )\nl(¯b)∏\ni=1\npb(bi) (4.9)\nWe use an analogous equation for 1:2 sentence beads. 8\n4.3 Implementation\nIn this section, we describe our implementation of the align ment algorithm. We describe\nhow we train the parameters or probabilities associated wit h the translation model, and how\nwe perform the search for the best alignment. In addition, we describe approximations that\nwe use to make the algorithm computationally tractable. We h ypothesize that because our\ntranslation model incorporates lexical information stron gly, correct alignments are tremen-\ndously more probable than incorrect alignments so that mode rate errors in calculation will\n8To be more consistent with 1:1 sentence beads, in equation (4 .9) instead of the expression O(Ei)O(Ei+1)\nwe should have the expression O(Ei+1\ni )(l + 1) where l = l(Ei) + l(Ei+1). There are O(Ei+1\ni ) diﬀerent ways\nto order the l English words in ¯b, and there are l + 1 diﬀerent places to divide the list of l English words\ninto two sentences (assuming we allow sentences of length ze ro). Thus, instead of equation (4.5) as for 1:1\nsentence beads, we have\np([Ei+1\ni , F ]|¯b) = 1\nO(Ei+1\ni )O(F )(l + 1)\nif we take all of these possibilities to be equiprobable. How ever, we choose the expression O(Ei)O(Ei+1)\nbecause then the contribution of this word ordering factor i s independent of alignment and can be ignored.\nMathematically, we account for this choice through the norm alization constants N2:1\nl ; see Section 4.3.2.\n118\nnot greatly aﬀect results.\n4.3.1 Evaluating the Probability of a Sentence Bead\nThe probability of a 0:1 or 1:0 sentence bead can be calculate d eﬃciently using equation\n(4.8) in Section 4.2.3. To evaluate the probabilities of oth er types of sentence beads exactly\nrequires a sum over a vast number of possible word beadings. W e make the gross approxi-\nmation that this sum is roughly equal to the maximum term in th e sum. For example, for\n1:1 sentence beads we have\np([E, F ]) = p(1 : 1)\n∑\n¯b∼[E,F ]\np1:1(l(¯b))\nN1:1\nl(¯b)O(E)O(F )\nl(¯b)∏\ni=1\npb(bi)\n≈ p(1 : 1) max¯b∼[E,F ]\n{ p1:1(l(¯b))\nN1:1\nl(¯b)O(E)O(F )\nl(¯b)∏\ni=1\npb(bi)}\nEven with this approximation, the calculation of p([E, F ]) is still expensive since it\nrequires a search for the most probable beading. We use a gree dy heuristic to perform this\nsearch; the heuristic is not guaranteed to ﬁnd the most proba ble beading. We begin with\nevery word in its own bead. We then ﬁnd the 0:1 bead and 1:0 bead that, when replaced\nwith a 1:1 word bead, results in the greatest increase in the p robability of the beading. We\nrepeat this process until we can no longer ﬁnd a 0:1 and 1:0 bea d pair that when replaced\nwould increase the beading’s probability.\nFor example, consider the sentence bead\n[do you speak French , parlez-vous fran¸ cais]\nTo search for the most probable word beading of this sentence bead, we begin with each\nword in its own word bead:\n¯b = {[do], [you], [speak], [French], [parlez], [vous], [fran¸ cais]}\nThen, we ﬁnd the pair of word beads that when replaced with a 1: 1 bead results in the\nlargest increase in p(¯b); suppose this pair is [ French] and [ fran¸ cais]. We substitute in the\n1:1 bead yielding\n¯b = {[do], [you], [speak], [French, fran¸ cais], [parlez], [vous]}\nWe repeat this process until there are no more pairs that are p roﬁtable to replace; this is\napt to yield a beading such as\n¯b = {[do], [you, vous], [speak, parlez], [French, fran¸ cais]}\nThis greedy search for the most probable beading can be perfo rmed in time roughly linear\nin the number of words in the involved sentences, as long as th e probability distribution\n119\npb([e, f ]) is fairly sparse, that is, as long as for most words e, there are few words f such\nthat pb([e, f ]) > 0. To perform this search eﬃciently, for each English word e we maintain\na list of all French words f such that pb([e, f ]) > 0. In addition, we maintain a list for each\nFrench word storing information about the current sentence . (For expository purposes, we\nassume the sentence bead contains a single English and Frenc h sentence.) Then, for a given\nsentence bead we do as follows:\n• For each word e in the English sentence, we take all associated beads [ e, f ] with\npb([e, f ]) > 0 and append these beads to the list associated with the word f.\n• We take the lists associated with each word f in the French sentence, and merge them\ninto a single list. We sort the resulting list according to th e increase in probability\nassociated with each bead if substituted into the beading.\nWith this procedure, we can ﬁnd all applicable beads [ e, f ] with nonzero probability in\nnear-linear time in sentence length. With the sorted list of beads, performing the greedy\nsearch eﬃciently is fairly straightforward.\n4.3.2 Normalization\nThe exact evaluation of the normalization constants Nl is very expensive. For example, for\n1:0 sentence beads we have that\nN1:0\nl =\n∑\n¯e={e1,...,el}\nl∏\ni=1\npe(ei)\nThis is identical to the normalization for 1:1 sentence bead s given in equation (4.4), except\nthat we restrict word beads to be single English words as 1:0 s entence beads only contain\nEnglish. It is impractical to sum over all sets of words {e1, . . . , e l}. Furthermore, we con-\ntinually re-estimate the parameters pe(ei) during the alignment process, so the exact value\nof N1:0\nl is constantly changing. Hence, we only approximate the norm alization constants\nNl.\nLet us ﬁrst consider the constants N1:0\nl . Notice that when we sum over ordered lists ⃗ e\ninstead of unordered sets ¯e, we have the relation\n∑\n⃗ e=(e1,...,el)\nl∏\ni=1\npe(ei) = 1\nLet O(¯e) be the number of distinct orderings of the elements in the (m ulti-)set ¯e =\n{b1, . . . , b l}; this is equal to the number of diﬀerent lists ⃗ ethat can be formed using all\nof the words in ¯ e. Then, we have\n∑\n¯e={e1,...,el}\nO(¯e)\nl∏\ni=1\npe(ei) = 1\n120\nWe make the approximation that O(¯e) = l! for all ¯ e. This approximation is exact for all\nsets E containing no duplicate words. This gives us\nN1:0\nl =\n∑\n¯e={e1,...,el}\nl∏\ni=1\npe(ei) = 1\nl!\n∑\n¯e={e1,...,el}\nl!\nl∏\ni=1\npe(ei) ≈ 1\nl!\n∑\n¯e={e1,...,el}\nO(¯e)\nl∏\ni=1\npe(ei) = 1\nl!\nWe use analogous approximations for N0:1\nl and N1:1\nl .\nNow, let us consider the constants N2:1\nl . These need to be calculated diﬀerently from\nthe above constants. To show this, we contrast the 2:1 senten ce bead case with the 1:1\nsentence bead case given in Section 4.2.2. For 1:1 sentence b eads, we have\np([E, F ]|¯b) = 1\nO(E)O(F )\nand this results in the expression O(E)O(F ) in equation (4.7). The analogous expression\nfor 2:1 sentence beads in equation (4.9) is O(Ei)O(Ei+1)O(F ). However, the distribution\np([Ei+1\ni , F ]|¯b) = 1\nO(Ei)O(Ei+1)O(F )\nis not proper in that ∑\nEi+1\ni ,F p([Ei+1\ni , F ]|¯b) ̸= 1; as described in Section 4.2.3, the expression\nO(Ei)O(Ei+1)O(F ) in not equal to the number of diﬀerent 2:1 sentence beads corr esponding\nto ¯b. Thus, the derivation for 1:1 sentence beads does not hold fo r 2:1 sentence beads and\nwe need to calculate the normalization constants diﬀerently .\nInstead, we choose N2:1\nl so that the probabilities p([Ei+1\ni , F ], ¯b) sum correctly. In par-\nticular, we want to choose N2:1\nl such that\n∑\nEi+1\ni ,F,l(¯b)=l\np([Ei+1\ni , F ], ¯b) = p(2 : 1) p2:1(l)\nas p(2 : 1) p2:1(l) is the amount of probability allocated by the model for word beadings of\nlength l of 2:1 sentence beads. Substituting in equation (4.9), we ge t\n∑\n¯b∼[Ei+1\ni ,F ],l(¯b)=l\np(2 : 1) p2:1(l)\nN2:1\nl O(Ei)O(Ei+1)O(F )\nl∏\ni=1\npb(bi) = p(2 : 1) p2:1(l)\nRearranging, we get\nN2:1\nl =\n∑\n¯b∼[Ei+1\ni ,F ],l(¯b)=l\n1\nO(Ei)O(Ei+1)O(F )\nl∏\ni=1\npb(bi)\nNow, we re-express the sum over Ei, Ei+1, and F as a sum over unordered sets ¯ ei =\n121\n{e1, . . . , e l(Ei)}, ¯ei+1 = {e′\n1, . . . , e ′\nl(Ei+1)}, and ¯f = {f1, . . . , f l(F )}, giving us\nN2:1\nl =\n∑\n¯b∼[¯ei,¯ei+1, ¯f],l(¯b)=l\nO(¯ei)O(¯ei+1)O( ¯f)\nO(¯ei)O(¯ei+1)O( ¯f)\nl∏\ni=1\npb(bi) =\n∑\n¯b∼[¯ei,¯ei+1, ¯f],l(¯b)=l\nl∏\ni=1\npb(bi)\nThen, let us consider how many diﬀerent [¯ ei, ¯ei+1, ¯f] are consistent with a given word beading\n¯b. There is only a single way to allocate the French words in ¯b to get ¯f; however, there\nare many ways of dividing the English words in ¯b to get ¯ei and ¯ei+1. In particular, there\nare 2 ne(¯b) ways of doing this, where ne(¯b) denotes the number of English words in ¯b. Each\nof the ne(¯b) English words in ¯b can be placed in either of the two English sets. Using this\nobservation, we have that\nN2:1\nl =\n∑\nl(¯b)=l\n2ne(¯b)\nl∏\ni=1\npb(bi)\nTo evaluate this equation, we use several approximations. T he ﬁrst approximation we\nmake is that pb(bi) is a uniform distribution, i.e., pb(b) = 1\nB for all b where B is the total\nnumber of diﬀerent word beads. This gives us\nN2:1\nl ≈\n∑\nl(¯b)=l\n2ne(¯b)\nl∏\ni=1\n1\nB = 1\nBl\n∑\nl(¯b)=l\n2ne(¯b)\nThen, let bl(n) be the number of bead sets ¯b of size l containing exactly n English words.\nWe can rewrite the preceding sum as\nN2:1\nl ≈ 1\nBl\nl∑\nn=0\nbl(n)2n (4.10)\nTo approximate bl(n), let B+e be the number of diﬀerent word beads containing English\nwords ( i.e., 1:0 and 1:1 beads), and let B−e be the number of diﬀerent word beads not\ncontaining English words ( i.e., 0:1 beads), so that B = B+e + B−e. Notice that the number\nof bead lists (as opposed to sets) b′\nl(n) of length l containing n English words is\nb′\nl(n) =\n(\nl\nn\n)\nBn\n+e Bl−n\n−e\nTo estimate the number of bead sets bl(n), we use the same approximation used in calcu-\nlating N1:0\nl and simply divide by l!. This gives us\nbl(n) ≈ 1\nl!\n(\nl\nn\n)\nBn\n+e Bl−n\n−e\nSubstituting this into equation (4.10), we get\nN2:1\nl ≈ 1\nBll!\nl∑\nn=0\n(\nl\nn\n)\nBn\n+eBl−n\n−e 2n = 1\nBll!\nl∑\nn=0\n(\nl\nn\n)\n(2B+e)nBl−n\n−e\n122\nUsing the binomial identity ( x + y)l = ∑ l\nn=0\n(l\nn\n)\nxnyl−n, we get\nN2:1\nl ≈ 1\nBll! (2B+e + B−e)l = 1\nl! (2B+e + B−e\nB )l = 1\nl! (B+e + B\nB )l = (1 + B+e\nB )l\nl!\nWe use an analogous approximation for N1:2\nl .\n4.3.3 Parameterization\nTo model the parameters pA-len(L) in equation (4.3) representing the probability that a\nbilingual corpus is L sentence beads in length, we assume a uniform distribution; 9 it is\nunclear what a priori information we have on the length of a corpus. This allows us t o\nignore the term, since this length will not aﬀect the probabil ity of an alignment.\nWe model sentence length (in beads) using a Poisson distribu tion, i.e.,\np1:0(l) = λl\n1:0\nl! eλ1:0\n(4.11)\nfor some λ1:0, and we have analogous equations for the other types of sente nce beads. To\nprevent the possibility of some of the λ’s being assigned unnaturally small or large values\nduring the training process to speciﬁcally model very short or very long sentences, we tie\ntogether the λ values for the diﬀerent types of sentence beads. We take\nλ1:0 = λ0:1 = λ1:1\n2 = λ2:1\n3 = λ1:2\n3 (4.12)\nIn modeling the frequency of word beads, there are three dist inct distributions we need\nto model: the distribution pe(ei) of 1:0 word beads in 1:0 sentence beads, the distribution\npf (fi) of 0:1 word beads in 0:1 sentence beads, and the distributio n of all word beads pb(bi)\nin 1:1, 2:1, and 1:2 sentence beads. 10 To reduce the number of independent parameters\nwe need to estimate, we tie these distributions together. We take pe(ei) and pf (fi) to be\nidentical to pb(bi), except restricted to the relevant subset of word beads and normalized\nappropriately, i.e.,\npe(e) = pb([e])∑\ne pb([e])\nand\npf (f) = pb([f])∑\nf pb([f])\nwhere [ e] and [ f] denote 1:0 and 0:1 word beads, respectively.\nTo further reduce the number of parameters, we convert all wo rds to lowercase. For\nexample, we consider the words May and may to be identical.\n9To be precise, we assume a uniform distribution over some arb itrarily large ﬁnite range, as one cannot\nhave a uniform distribution over a countably inﬁnite set.\n10Conceivably, we could consider using three diﬀerent distri butions pb(bi) for 1:1, 2:1, and 1:2 sentence\nbeads. However, we assume these distributions are identica l to reduce the number of parameters.\n123\n4.3.4 Parameter Estimation Framework\nThe basic method we use for estimating the parameters or prob abilities of our model is\nto just take counts on previously aligned data and to normali ze. For example, to estimate\np(1 : 0), the probability or frequency of 1:0 sentence beads, w e count the total number of 1:0\nsentence beads in previously aligned data and divide by the t otal number of sentence beads.\nTo bootstrap the model, we ﬁrst take counts on a small amount o f data that has been aligned\nby hand or by some other algorithm. Once the model has been boo tstrapped, it can align\nsentences by itself, and we can take counts on the data alread y aligned by the algorithm to\nimprove the parameter estimates for aligning future data. F or the Hansard corpus, we have\nfound that one hundred sentence pairs are suﬃcient to bootst rap the alignment model.\nThis method can be considered to be a variation of the Viterbi version of the expectation-\nmaximization (EM) algorithm (Dempster et al. , 1977). In the EM algorithm, an expectation\nphase, where counts on the corpus are taken using the current estimates of the parameters,\nis alternated with a maximization phase, where parameters are re-estimated based on the\ncounts just taken. Improved parameters lead to improved cou nts, which lead to even more\naccurate parameters. In the incremental version of the EM al gorithm we use, instead\nof re-estimating parameters after each complete pass throu gh the corpus, we re-estimate\nparameters after each sentence. By re-estimating paramete rs continually as we take counts\non the corpus, we can align later sections of the corpus more r eliably based on the alignment\nof earlier sections. We can align a corpus with only a single p ass, simultaneously producing\nalignments and updating the model as we proceed.\nHowever, to align a corpus in a single pass, the model must be f airly accurate before\nstarting or else the beginning of the corpus will be poorly al igned. Hence, after bootstrap-\nping the translation model on one hundred sentence pairs and before starting the one pass\nthrough the entire corpus to produce our ﬁnal alignment, we ﬁ rst reﬁne the translation\nmodel by using the algorithm to align a chunk of the unaligned target bilingual corpus. In\nexperiments with the Hansard corpus, we train on 20,000 unal igned sentence pairs before\nperforming the ﬁnal alignment.\nBecause the search algorithm considers many partial alignm ents simultaneously, it is not\nobvious how to determine when it is certain that a particular sentence bead will be part of\nthe ﬁnal alignment and thus can be trained on. To elaborate, o ur search algorithm maintains\na set of partial alignments, each partial alignment represe nting a possible alignment between\nsome preﬁx of the English corpus and some preﬁx of the French c orpus. These hypothesis\nalignments are extended incrementally during the search pr ocess. To address the problem\nof determining which sentence beads can be trained on, we kee p track of the longest partial\nalignment common to all partial alignments currently being considered. It is assured that\nthis common partial alignment will be part of the ﬁnal alignm ent. Hence, whenever a\nsentence bead is added to this common alignment we use it to tr ain on. The point in the\ncorpus at the end of this common alignment is called the conﬂuence point .\n124\n4.3.5 Parameter Estimation Details\nOne issue with the framework described in the last section is that in a straightforward\nimplementation, probabilities that are initialized to zer o will remain zero during the training\nprocess. An object with zero probability will never occur in an alignment, and thus it will\nnever receive any counts. The probability of an object is jus t its count normalized, so\nsuch an object will always have probability zero. Thus, it is important to initialize all\nprobabilities to nonzero values. Unless otherwise speciﬁe d, we achieve this by setting all\ncounts initially to 1.\nWe now describe in detail how we estimate speciﬁc parameters . To estimate word bead\nfrequencies pb(b), we maintain a count c(b) for each word bead b reﬂecting the number of\ntimes that word bead has occurred in the most probable word be ading of a sentence bead.\nMore speciﬁcally, given some aligned data to train on, we ﬁrs t ﬁnd the most probable word\nbeading of each sentence bead in the alignment using the curr ent model, and we then use\nthese most probable word beadings to update word bead counts . We take\npb(b) = c(b)\n∑\nb c(b)\nFor 0:1 and 1:0 word beads, we initialize the counts c(b) to 1. For 1:1 word beads, we\ninitialize these counts to zero; this is because our algorit hm for searching for the most\nprobable word beading of a sentence bead will not be eﬃcient u nless pb([e, f ]) is sparse, as\ndescribed in Section 4.3.1. Instead, we use a heuristic for i nitializing particular c([e, f ]) to\nnonzero values during the training process; whenever we see a 0:1 and a 1:0 word bead occur\nin the most probable beading of a sentence bead, we initializ e the count of the corresponding\n1:1 word bead to a small value. 11 This heuristic is eﬀective for constraining the number of\n1:1 word beads with nonzero probability.\nTo estimate the sentence length parameters λ, we divide the number of word beads\nin the most probable beadings of the previously aligned sent ences by the total number\nof sentences. This gives us the mean number of word beads per s entence. In a Poisson\ndistribution, the mean coincides with the value of the λ parameter. (Recall that we model\nsentence length with a Poisson distribution as in equation ( 4.11).) We take λ1:0 to be this\nmean value, and the other λ parameters can be calculated using equation (4.12). For the\nsituation before we have any counts, we set λ1:0 to an arbitrary constant; we chose the value\n7.\nTo estimate the probabilities p(1 : 0), p(0 : 1), p(1 : 1), p(2 : 1), and p(1 : 2) of each\ntype of sentence bead, we count the number of times each type o f bead occurred in the\npreviously aligned data and divide by the total number of sen tence beads. These counts\nare initialized to 1.\n11 The particular value we use is\nne+nf\n2nenf\n, where ne denotes the number of 1:0 word beads in the beading,\nand nf denotes the number of 0:1 word beads. This can be thought of as dividing\nne+nf\n2 counts evenly\namong all nenf bead pairs.\n125\n4.3.6 Cognates\nThere are many words that possess the same spelling in two diﬀe rent languages. For exam-\nple, punctuation, numbers, and proper names generally have the same spellings in English\nand French. Such words are members of a class called cognates (Simard et al. , 1992).\nBecause identically spelled words can be recognized automa tically and are frequently trans-\nlations of each other, it is sensible to use this a priori information in initializing word bead\nfrequencies. To this end, we initialize to 1 the count of all 1 :1 word beads that contain\nwords that are spelled identically.\n4.3.7 Search\nIt is natural to use dynamic programming to search for the bes t alignment; one can ﬁnd the\nmost probable of an exponential number of alignments using q uadratic time and memory.\nUsing the same perspective as Gale and Church (1993), we view alignment as a “shortest\npath” problem. Recall that we try to ﬁnd the alignment ⃗A such that\n⃗A = arg max\n⃗A\np( ⃗A, ⃗E, ⃗F ) = arg max\n⃗A\npA-len(l( ⃗A))\nl( ⃗A)∏\ni=1\np([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])\nManipulating this equation, we get\n⃗A = arg max\n⃗A\np( ⃗A, ⃗E, ⃗F )\n= arg min\n⃗A\n− ln p( ⃗A, ⃗E, ⃗F )\n= arg min\n⃗A\n− ln{pA-len(l( ⃗A))\nl( ⃗A)∏\ni=1\np([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])}\n= arg min\n⃗A\n{− ln pA-len(l( ⃗A)) +\nl( ⃗A)∑\ni=1\n− ln p([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])}\n= arg min\n⃗A\nl( ⃗A)∑\ni=1\n− ln p([E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n])\n(Recall that we take pA-len(l) to be a uniform distribution.) In other words, ﬁnding the\nalignment with highest probability is equivalent to ﬁnding the alignment that minimizes\nthe sum of the negative logarithms of the probabilities of th e sentence beads that compose\nthe alignment. 12 Thus, by assigning to each sentence bead a “length” equal to t he nega-\ntive logarithm of its probability, the sentence alignment p roblem is reduced to ﬁnding the\nshortest path from the beginning of a corpus to its end.\nThe shortest path problem has a well-known dynamic programm ing solution. We eval-\nuate a lattice D(i, j) representing the shortest distance from the beginning of t he corpus\n12This transformation is equivalent to the transformation be tween probability space and length space\ngiven in Section 3.2.3. (Recall that − ln p = ln 1\np .)\n126\nto the ith English sentence and jth French sentence. The distance D(i, j) is equal to the\nlength of the most probable alignment aligning the ﬁrst i and j sentences of the English\nand French corpora, respectively. This lattice can be calcu lated eﬃciently using a simple\nrecurrence relation; in this case, the recurrence relation is:\nD(i, j) = min\n\n\n\n\n\n\n\n\n\n\n\n\n\nD(i − 1, j) + − ln p([Ei])\nD(i, j − 1) + − ln p([Fj ])\nD(i − 1, j − 1) + − ln p([Ei, Fj ])\nD(i − 2, j − 1) + − ln p([Ei\ni−1, Fj ])\nD(i − 1, j − 2) + − ln p([Ei, F j\nj−1])\n(4.13)\nIn other words, the most probable alignment of the ﬁrst i and j English and French sentences\ncan be expressed in terms of the most probable alignment of so me preﬁx of these sentences\nextended by a single sentence bead. The rows in the equation c orrespond to 1:0, 0:1, 1:1,\n2:1, and 1:2 sentence beads, respectively. The value D(0, 0) is taken to be zero. The value\nD(l( ⃗E), l( ⃗F )) represents the shortest distance through the whole corpu s, and it is possible to\nrecover the alignment corresponding to this shortest path t hrough some simple bookkeeping.\nIntuitively, this search can be viewed as maintaining a set o f partial alignments and\nextending them incrementally. We ﬁll in the lattice in incre asing diagonals, where the kth\ndiagonal consists of all cells D(i, j) such that i + j = k; the kth diagonal corresponds to\nalignments containing a total of k sentences. Each cell D(i, j) corresponds to the most\nprobable alignment ending at the ith and jth sentence in the English and French corpora.\nWe can consider the alignments corresponding to the D(i, j) in the current diagonal i+j = k\nto be the set of current partial alignments. Filling in the la ttice in increasing diagonals can\nbe considered as extending the current partial alignments i ncrementally.\nNotice that this algorithm is quadratic in the number of sent ences in the bilingual\ncorpora, as we need to ﬁll in a lattice with l( ⃗E)l( ⃗F ) cells. Given the size of existing bilingual\ncorpora and the computation necessary to evaluate the proba bility of a sentence bead, a\nquadratic algorithm is too proﬂigate. However, we can reap g reat savings in computation\nthrough intelligent thresholding. Instead of evaluating t he entire lattice D(i, j), we ignore\nparts of the lattice that look as if they correspond to poor al ignments. By considering only\na subset of all possible alignments, we reduce the computati on to a linear one.\nMore speciﬁcally, we notice that the length D(i, j) of an alignment preﬁx is proportional\nto the number of sentences in the alignment preﬁx i + j. Hence, it is reasonable to compare\nthe lengths of two partial alignments if they contain the sam e number of sentences. We\nprune all alignment preﬁxes that have a substantially lower probability than the most\nprobable alignment preﬁx of the same length. That is, whenev er D(i, j) > D (i′, j′) + c\nfor some i′, j′ and constant c where i + j = i′ + j′, we set D(i, j) to ∞. This discards\nfrom consideration all alignments that begin by aligning th e ﬁrst i English sentences with\nthe ﬁrst j French sentences. We evaluate the array D(i, j) diagonal by diagonal, so that\ni + j increases monotonically. For c, we use the value 500, and with the Hansard corpus\nthis resulted in an average search beam width through the dyn amic programming lattice of\nabout thirty; that is, on average we evaluated thirty diﬀeren t D(i, j) such that i + j = k for\neach value k.\n127\n4.3.8 Deletion Identiﬁcation\nThe dynamic programming framework described above can hand le the case when there\nis a small deletion in one of the corpora of a bilingual corpus . However, the framework\nis ineﬀective for deletions larger than hundreds of sentence s; the thresholding mechanism\nis unreliable in this situation. When the deletion point is r eached, the search algorithm\nwill attempt to extend the current partial alignments with s entence beads that align unre-\nlated English and French sentences. There will be no correct alignment with signiﬁcantly\nhigher probability to provide a meaningful standard with wh ich to threshold against. Any\nthresholding that occurs will be due to the random variation in alignment probabilities.\nOne solution is to not threshold when a deletion is detected. However, this is also\nimpractical since dynamic programming is quadratic withou t thresholding and deletions\ncan be many thousands of sentences long. Thus, we handle long deletions outside of the\ndynamic programming framework.\nTo detect the beginning of a deletion, we use the conﬂuence point mentioned in Section\n4.3.4 as an indicator. Recall that the conﬂuence point is the point at the end of the longest\npartial alignment common to all current hypothesis alignme nts.13 The distance (in sen-\ntences) from the conﬂuence point to the diagonal in the latti ce D(i, j) currently being ﬁlled\nin can be thought of as representing the uncertainty in align ment at the current location\nin the lattice. In the usual case, there is one correct alignm ent that receives vastly greater\nprobability than other alignments, and thresholding is ver y aggressive so this distance is\nsmall. However, when there is a large deletion in one of the pa rallel corpora, consistent\nlexical correspondences disappear so no one alignment has a much higher probability than\nthe others. Thus, there will be little thresholding and the d istance from the conﬂuence\npoint to the frontier of the lattice will become large. When t his distance reaches a certain\nvalue, we take this to indicate the beginning of a deletion.\nIn thresholding with c = 500 on the Hansard corpus, we have found that the conﬂuence\npoint is typically about thirty sentences away from the fron tier of D(i, j). Whenever the\nconﬂuence point lags 400 sentences behind the frontier, we a ssume a deletion is present.\nTo identify the end of a deletion, we search for the occurrenc e of infrequent words\nthat are mutual translations. We search linearly through bo th corpora simultaneously. All\noccurrences of words whose frequency is below a certain valu e are recorded in a hash table;\nwith the Hansard corpus we logged words occurring ten or fewe r times previously. Whenever\nwe notice the occurrence of a rare word e in one corpus and its translation f in the other\n(i.e., pb([e, f ]) > 0), we take this as a candidate location for the end of the dele tion.\nTo give an example, assume that the current conﬂuence point i s located after the ith\nEnglish sentence and jth French sentence, and that we are currently calculating th e diagonal\nin D consisting of alignments containing a total of i + j + 400 sentences. Since this frontier\nis 400 sentences away from the conﬂuence point, we assume a de letion is present. We\n13To calculate the conﬂuence point, we keep track of all senten ce beads currently belonging to an active\npartial alignment. Whenever a sentence bead becomes the onl y active bead crossing a particular diagonal\nin the distance lattice D, i.e., the only active sentence bead [ Ei2\ni1 , F j2\nj1 ] such that i1 + j1 ≤ k and i2 + j2 > k\nfor some k, then we know all active partial alignments include that sen tence bead and we can move the\nconﬂuence point ahead of that sentence bead.\n128\niterate through the English and French corpora simultaneou sly starting from the ith and\njth sentences, respectively, logging all rare words. Suppos e the following rare words occur:\nlanguage sentence # word\nEnglish i + 7 Socratic\nEnglish i + 63 epidermis\nFrench j + 127 indemnisation\nEnglish i + 388 Topeka\nFrench j + 416 gypsophile\nEnglish i + 472 solecism\nFrench j + 513 socratique\n.\n.\n.\n.\n.\n.\n.\n.\n.\nWhen we reach the 513th French sentence after the conﬂuence p oint, we observe the word\nsocratique which is a translation of the word Socratic found in the 7th English sentence\nafter the conﬂuence point. (We assume that we have pb([Socratic, socratique]) > 0.) We\nthen take the ( i + 7)th and ( j + 513)th English and French sentences to be a candidate\nlocation for the end of the deletion.\nWe test the correctness of a candidate location using a two-s tage process for eﬃciency.\nFirst, we calculate the probability of the sentence bead com posed of the two sentences\ncontaining the two rare words. If this is “suﬃciently high,” we then examine the forty\nsentences following the occurrence of the rare word in each o f the two parallel corpora. We\nuse dynamic programming to ﬁnd the probability of the best al ignment of these two blocks\nof sentences. If this probability is also “suﬃciently high” we take the candidate location\nto be the end of the deletion. Because it is extremely unlikel y that there are two very\nsimilar sets of forty sentences in a corpus, this deletion id entiﬁcation algorithm is robust.\nIn addition, because we key oﬀ of rare words in searching for t he end of a deletion, deletion\nidentiﬁcation requires time linear in the length of the dele tion.\nWe consider the probability pactual of an alignment to be “suﬃciently high” if its score\nis a certain fraction f of the highest possible score given just the English sentenc es in the\nsegment. More speciﬁcally, we use the following equation to calculate f:\nf = (− ln pactual) − (− ln pmin)\n(− ln pmax) − (− ln pmin)\nTo calculate pmax, we calculate the French sentences that would yield the high est possible\nalignment score given the English sentences in the alignmen t; these sentences can be con-\nstructed by just taking the most probable word-to-word tran slation for each word in the\nEnglish sentences. The probability of the alignment of thes e optimal French sentences with\nthe given English sentences is pmax. The probability pmin is taken to be the probability\nassigned to the alignment where the sentences are aligned en tirely with 0:1 and 1:0 sentence\nbeads; this approximates the lowest possible achievable sc ore.\nWe take this quotient f to represent the quality of an alignment. For the initial sen tence-\nto-sentence comparison, we took the fraction 0.57 to be “suﬃ ciently high.” For the align-\n129\nment of the next forty sentences, we took the fraction 0.4 to b e “suﬃciently high.” These\nvalues were arrived at empirically, by trying several value s on several deletion points in the\nHansard corpus and choosing the value with the best subjecti ve performance. Reasonable\nperformance can be achieved with values within a couple tent hs of these values; higher or\nlower values may be used to improve alignment precision or re call.14\nBecause we key oﬀ of rare words in recovering from deletions, it is possible to overshoot\nthe true recovery point by a signiﬁcant amount. To correct fo r this, after we ﬁnd a location\nfor the end of a deletion using the mechanisms described prev iously, we backtrack through\nthe corpus. We take the ten preceding sentences in each corpu s from the recovery point,\nand ﬁnd the probability of their alignment. If this probabil ity is “suﬃciently high,” we\nmove the recovery point back and repeat the process. We take t he fraction 0.4 using the\nmeasure described in the last paragraph to be “suﬃciently hi gh.”\n4.3.9 Subdividing a Corpus for Parallelization\nSentence alignment is a task that seems well-suited to paral lelization, since the alignment\nof diﬀerent sections of a bilingual corpus are basically inde pendent. However, to parallelize\nsentence alignment it is necessary to be able to divide a bili ngual corpus into many sections\naccurately. That is, division points in the two corpora of a b ilingual corpus must correspond\nto identical points in the text. Our deletion recovery mecha nism can be used for this\npurpose. We start at the beginning of each corpus in the bilin gual corpus, and skip some\nnumber of sentences in each corpus. The number of sentences w e skip is the number of\nsentences we want in each subdivision of the bilingual corpu s. We then employ the deletion\nrecovery mechanism to ﬁnd a subsequent point in the two corpo ra that align, and we take\nthis to be the end of the subdivision. We repeat this process t o divide the whole corpus\ninto small sections.\n4.3.10 Algorithm Summary\nWe summarize the algorithm below, not including paralleliz ation.\ninitialize all counts and parameters\n; bootstrap the model by training on manually-aligned data\n; ([ EAe\n2−1\nAe\n1\n, F\nAf\n2 −1\nAf\n1\n], . . . , [E\nAe\nL+1−1\nAe\nL\n, F\nAf\nL+1−1\nAf\nL\n] )\nfor i = 1 to L do\nbegin\n¯b := most probable word beading of [ E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n]\n14We use precision to describe the fraction of sentence beads returned by the al gorithm that represent\ncorrect alignments; recall describes the fraction of all correct sentence beads that ar e found by the algorithm.\nIn these measures, we only consider sentence beads containi ng both English and French sentences as these\nare the beads most useful in applications.\n130\nupdate counts and parameters based on ¯b\nend\n; this is the main loop where we ﬁll the D(i, j) lattice\n; ( iconf, jconf) holds indices of the English and French sentences at conﬂuen ce point\n; k holds value of current diagonal being ﬁlled; diagonal is all D(i, j) with k = i + j\niconf := 1\njconf := 1\nD(0, 0) := 0\nfor k = 2 to l( ⃗E) + l( ⃗F ) do\nbegin\n; check for long deletion\nif k − (iconf + jconf) > 400 then\nbegin\n(iend, jend) := location of end of deletion (see Section 4.3.8)\nk := iend + jend\n(iconf, jconf) := ( iend, jend)\nend\n; ﬁll in diagonal in D array\n; Dbest holds the best score in the diagonal, used for thresholding p urposes\nDbest := ∞\n; only ﬁll in cells in diagonal corresponding to extending a no nthresholded alignment\nfor all i, j ≥ 1 such that i + j = k and\n∃i′, j′ with i − 2 ≤ i′ ≤ i, j − 2 ≤ j′ ≤ j, D(i′, j′) < ∞ do\nbegin\nD(i, j) := expression given in equation (4.13)\nif D(i, j) < D best then\nDbest := D(i, j)\nend\n; threshold items in diagonal\nfor all i, j ≥ 1 such that i + j = k and D(i, j) < ∞ do\nif D(i, j) < D best − 500 then\nD(i, j) := ∞\n; update conﬂuence point (see Section 4.3.8)\nif conﬂuence point has moved then\nbegin\n(iconf, jconf) := new location of conﬂuence point\nfor each sentence bead [ E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n] moved behind conﬂuence point do\nbegin\n¯b := most probable word beading of [ E\nAe\ni+1−1\nAe\ni\n, F\nAf\ni+1−1\nAf\ni\n]\nupdate counts and parameters based on ¯b\nend\n131\nE1 If there is some evidence that\nit . . . and I will see that it\ndoes.\nE2 \\SCM{} Translation \\ECM{}\nF1 Si on peut prouver que elle\n. . . je verrais ` a ce que elle se\ny conforme. \\SCM{}\nLanguage = French \\ECM{}\nF2 \\SCM{} Paragraph \\ECM{}\nFigure 4.4: An alignment error\nend\nend\n4.4 Results\nUsing this algorithm, we have aligned three large English/F rench corpora. We have aligned\na corpus of 3,000,000 sentences (of both English and French) of the Canadian Hansards,\na corpus of 1,000,000 sentences of newer Hansard proceeding s, and a corpus of 2,000,000\nsentences of proceedings from the European Economic Commun ity. In each case, we ﬁrst\nbootstrapped the translation model by training on 100 previ ously aligned sentence pairs.\nWe then trained the model further on 20,000 (unaligned) sent ences of the target corpus.\nBecause of the very low error rates involved, instead of dire ct sampling we decided to\nestimate our error on the old Hansard corpus through compari son with the alignment found\nby Brown et al. on the same corpus. We manually inspected over 500 locations where the\ntwo alignments diﬀered to estimate our error rate on the align ments disagreed upon. Taking\nthe error rate of the Brown alignment to be 0.6%, we estimated the overall error rate of our\nalignment to be 0.4%.\nIn addition, in the Brown alignment approximately 10% of the corpus was discarded\nbecause of indications that it would be diﬃcult to align. The ir error rate of 0.6% holds on\nthe remaining sentences. Our error rate of 0.4% holds on the e ntire corpus. Gale reports\nan approximate error rate of 2% on a diﬀerent body of Hansard da ta with no discarding,\nand an error rate of 0.4% if 20% of the sentences can be discard ed.\nHence, with our algorithm we can achieve at least as high accu racy as the Brown and\nGale algorithms without discarding any data. This is especially signiﬁcant since, p resum-\nably, the sentences discarded by the Brown and Gale algorith ms are those sentences most\ndiﬃcult to align.\nTo give an idea of the nature of the errors our algorithm makes , we randomly sampled\n300 alignments from the newer Hansard corpus. The two errors we found are displayed in\nFigures 4.4 and 4.5. In the ﬁrst error, E1 was aligned with F1 and E2 was aligned with\nF2. The correct alignment maps E1 and E2 to F1 and F2 to nothing. In the second error,\nE1 was aligned with F1 and F2 was aligned to nothing. The correct alignment maps E1\nto both F1 and F2. Both of these errors could have been avoided with improved s entence\nboundary detection.\nThe rate of alignment ranged from 2,000 to 5,000 sentences of both English and French\nper hour on an IBM RS/6000 530H workstation. Using the techni que described in section\n132\nE1 Motion No. 22 that Bill C-84\nbe amended in . . . and\nsubstituting the following\ntherefor : second anniversary\nof.\nF1 Motion No 22 que on modiﬁe\nle projet de loi C-84 . . . et en\nla rempla¸ cant par ce qui suit :\n‘ 18.\nF2 Deux ans apr` es : ’.\nFigure 4.5: Another alignment error\n4.3.9, we subdivided corpora into small sections (20,000 se ntences) and aligned sections in\nparallel. While it required on the order of 500 machine-hour s to align the newer Hansard\ncorpus, it took only 1.5 days of real time to complete the job o n ﬁfteen machines.\n4.4.1 Lexical Correspondences\nOne of the by-products of alignment is the distribution pb(b) of word bead frequencies. For\n1:1 word beads b = [ e, f ], the probability pb(b) can be interpreted as a measure of how\nstrongly the words e and f translate to each other. Hence, pb(b) in some sense represents a\nprobabilistic word-to-word bilingual dictionary.\nFor example, we can use the following measure t(e, f ) as an indication of how strong\nthe words e and f translate to each other: 15\nt(e, f ) = ln pb([e, f ])\npe(e)pf (f)\nWe divide pb([e, f ]) by the frequencies of the individual words to correct for t he eﬀect that\npb([e, f ]) will tend to be higher for higher frequency words e and f. Thus, t(e, f ) should not\nbe skewed by the frequency of the individual words. Notice th at t(e, f ) is roughly equal to\nthe gain in the (logarithm of the) probability of a word beadi ng if word beads [ e] and [ f]\nare replaced with the bead [ e, f ]. Thus, t(e, f ) dictates the order in which 1:1 word beads\nare applied in the search for the most probable word beading o f a sentence bead described\nin Section 4.3.1.\nIn Appendix A, for a group of randomly sampled English words w e list the French words\nthat translate most strongly to them according to this measu re. In general, the correspon-\ndences are fairly accurate. However, for some common prepos itions the correspondences are\nrather poor; examples of this are also listed in Appendix A. P repositions sometimes occur\n15This measure is closely related to the measure\nMIF (x, y) = pX,Y (x, y)\npX (x)pY (y)\nreferred to as mutual information by Magerman and Marcus (1990). This is not to be confused with the\nmore common deﬁnition of mutual information:\nI(X; Y ) =\n∑\nx,y\np(x, y) log p(x, y)\np(x)p(y)\n133\nin situations in which they have no good translation, and man y prepositions have numerous\ntranslations.16\nIn many lists, the “French” word with the exact same spelling as the English word\noccurs near the top of the list. (A word is considered to be “Fr ench” if it occurs at any\npoint in the French corpus.) This is due to the initializatio n described in Section 4.3.6 we\nperform for cognates.\nNotice that we are capable of acquiring strong lexical corre spondences between non-\ncognates. Preliminary experiments indicate that cognate i nitialization does not signiﬁcantly\naﬀect alignment accuracy. Hence, our alignment algorithm is applicable to languages with\ndiﬀering alphabets.\n4.5 Discussion\nWe have described an accurate, robust, and eﬃcient algorith m for sentence alignment.\nThe algorithm can handle large deletions in text, it is langu age independent, and it is\nparallelizable. It requires a minimum of human interventio n; for each language pair 100\nsentences need to be aligned by hand to bootstrap the transla tion model. Unlike previous\nalgorithms, our algorithm does not require that the bilingu al corpus be predivided into\nsmall chunks or that one can identify markers in the text that make this subdivision easier.\nOur algorithm produces a probabilistic bilingual dictiona ry, and it can take advantage of\ncognate correspondences, if present.\nThe use of lexical information requires a great computation al cost. Even with numerous\napproximations, this algorithm is tens of times slower than the length based algorithms of\nBrown et al. and Gale and Church. This is acceptable given available comp uting power\nand given that alignment is a one-time cost. It is unclear, th ough, whether more powerful\nmodels are worth pursuing.\nOne limitation of the algorithm is that it only considers ali gning a single sentence to zero,\none, or two sentences in the other language. It may be useful t o extend the set of sentence\nbeads to include 2:2 or 1:3 alignments, for example. In addit ion, we do not consider sentence\nordering transpositions between the two corpora. For examp le, the case where the ﬁrst\nsentence in an English corpus translates to the second sente nce in a French corpus and the\nsecond English sentence translates to the ﬁrst French sente nce cannot be handled correctly\nby our algorithm. At best, our algorithm will align one pair o f sentences correctly and align\neach of the remaining two sentences to nothing. However, whi le extending the algorithm\nin these ways can potentially reduce the error rate by allowi ng the algorithm to express a\nwider range of alignments, it may actually increase error ra te because the algorithm must\nconsider a larger set of possible alignments and thus become s more susceptible to random\nerror.\nThus, before adding extensions like these it may be wise to st rengthen the translation\n16It has been suggested that removing closed-class words befo re alignment may improve performance;\nthese words do not provide much reliable alignment informat ion and are often assigned spurious translations\nby our algorithm (Shieber, 1996).\n134\nmodel, which should improve performance in general anyway. For example, one natural\nextension to the translation model is to account for word ord ering. Brown et al. (1993)\ndescribe several such possible models. However, substanti ally greater computing power\nis required before these approaches can become practical, a nd there is not much room for\nfurther improvements in accuracy. In addition, parameter e stimation becomes more diﬃcult\nwith larger models.\n135\nChapter 5\nConclusion\nIn this thesis, we have presented techniques for modeling la nguage at the word level, the\nconstituent level, and the sentence level. At each level, we have developed methods that\nsurpass the performance of existing methods.\nAt the word level, we examined the task of smoothing n-gram language models. While\nsmoothing is a fundamental technique in statistical modeli ng, the literature lacks any sort\nof systematic comparison of smoothing techniques for langu age tasks. We present an ex-\ntensive empirical comparison of the most widely-used smoot hing algorithms for n-gram\nlanguage models, the current standard in language modeling . We considered several is-\nsues not considered in previous work, such as how training da ta size, n-gram order, and\nparameter optimization aﬀect performance. In addition, we i ntroduced two novel smooth-\ning techniques that surpass all previous techniques on trig ram models and that perform\nwell on bigram models. We provide some detailed analysis tha t helps explain the relative\nperformance of diﬀerent algorithms.\nAt the constituent level, we investigated grammar inductio n for language modeling.\nWhile yet to achieve comparable performance, grammar-base d models are a promising al-\nternative to n-gram models as they can express both short and long-distanc e dependencies\nand they have the potential to be more compact than equivalen t n-gram models. We in-\ntroduced a probabilistic context-free grammar induction a lgorithm that uses the Bayesian\nframework and the minimum description length principle. By using a rich move set and\nthe technique of triggering, our search algorithm is eﬃcien t and eﬀective. We demonstrated\nthat our algorithm signiﬁcantly outperforms the Lari and Yo ung induction algorithm, the\nmost widely-used algorithm for probabilistic grammar indu ction. In addition, we were able\nto surpass the performance of n-gram models on artiﬁcially-generated data.\nAt the sentence level, we examined bilingual sentence align ment. Bilingual sentence\nalignment is a necessary step in processing a bilingual corp us for use in many applica-\ntions. Previous algorithms suitable for large corpora igno re word identity information and\njust consider sentence length; we introduce an algorithm th at uses lexical information eﬃ-\ncient enough for large bilingual corpora. Furthermore, our algorithm is robust, language-\nindependent, and parallelizable. We surpass all previousl y reported accuracy rates on the\nHansard corpus, the most widely-used corpus in machine tran slation research.\n136\nIt is interesting to note that for these three tasks, we use th ree very diﬀerent frameworks.\nIn the next section, we explain what these frameworks are and how they relate to the\nBayesian framework. We argue that each framework used is app ropriate for the associated\nproblem. Finally, we show how our work on these three problem s address two central issues\nin probabilistic modeling: the sparse data problem and the p roblem of inducing hidden\nstructure.\n5.1 Bayesian Modeling\nIn our work on grammar induction, we use the Bayesian framewo rk. We attempt to ﬁnd\nthe grammar G with the largest probability given the training data, or obs ervations, O.\nApplying Bayes’ rule we get\nG = arg max\nG\np(G|O) = arg max\nG\np(G)p(O|G)\np(O) = arg max\nG\np(G)p(O|G) (5.1)\nWe search for the grammar G that maximizes the objective function p(G)p(O|G).\nThe Bayesian framework is a very elegant and general framewo rk. In the objective\nfunction, the term describing how well a grammar models the d ata, p(O|G), is separate from\nthe term describing our a priori notion of how likely a grammar is, p(G). Furthermore,\nbecause we express the target grammar in a static manner inst ead of an algorithmic manner,\nwe have a separation between the objective function and the s earch strategy. Thus, we can\nswitch around prior distributions or search strategies wit hout changing other parts of an\nalgorithm. The Bayesian framework modularizes search prob lems in a general and logical\nway.\nHowever, notice that we could have framed both n-gram smoothing and sentence align-\nment in the Bayesian framework as well, but we chose not to. To express smoothing in the\nBayesian framework, we can use an analogous equation to equa tion (5.1), e.g.,\nM = arg max\nM\np(M|O) = arg max\nM\np(M)p(O|M)\np(O) = arg max\nM\np(M)p(O|M)\nwhere M denotes a smoothed n-gram model. We can design a prior p(M) over smoothed\nn-gram models and search for the model M that maximizes p(M)p(O|M).1 Instead, most\nexisting smoothing algorithms as well as our novel algorith ms involve a straightforward\nmapping from training data to a smoothed model. 2\nIn sentence alignment, from aligned data ( ⃗E, ⃗F ) we build a model pt(Ej\ni ; F l\nk) of the\nfrequency with which sentences Ej\ni and sentences F l\nk occur as mutual translations in a\n1Actually, a slightly diﬀerent Bayesian formulation is more appropriate for smoothing, as will be men-\ntioned in Section 5.1.1.\n2This is not quite true; Jelinek-Mercer smoothing uses the Ba um-Welch algorithm to perform a maximum\nlikelihood search for λ values. In addition, we performed automated parameter opti mization in a maximum\nlikelihood manner.\n137\nbilingual corpus. To frame this in the Bayesian framework, w e can use the equation\npt = arg max\npt\np(pt| ⃗E, ⃗F ) = arg max\npt\np(pt)p( ⃗E, ⃗F |pt)\np( ⃗E, ⃗F )\n= arg max\npt\np(pt)p( ⃗E, ⃗F |pt)\nWe could devise a prior distribution p(pt) over possible translation models and attempt\nto ﬁnd the model pt that maximizes p(pt)p( ⃗E, ⃗F |pt). Instead, we use a variation of the\nExpectation-Maximization algorithm to perform a determin istic maximum-likelihood search\nfor the model pt.\nThus, while we could have used the Bayesian framework in each of the three problems\nwe addressed, we instead used three diﬀerent approaches. Bel ow, we examine each problem\nin turn and argue why the chosen approach was appropriate for the given problem.\n5.1.1 Smoothing n-Gram Models\nFirst, we note that the Bayesian formulation given previous ly of ﬁnding the most probable\nmodel\nMbest = arg max\nM\np(M|O)\nis not appropriate for the smoothing problem. The reason the most probable model Mbest is\nsigniﬁcant is because we expect it to be a good model of data th at will be seen in the future,\ne.g., data that is seen during the actual use of an application. In other words, we ﬁnd Mbest\nbecause we expect that p(Of |Mbest) is a good model of future data Of . However, notice\nthat ﬁnding Mbest is actually superﬂuous; what we are really trying to ﬁnd is ju st p(Of |O),\na model of what future data will be like given our training dat a. This is a more accurate\nBayesian formulation of the smoothing problem (and of model ing problems in general); the\nidentity of Mbest is not important in itself.\nUsing this perspective, we can explain why smoothing is gene rally necessary in n-gram\nmodeling and other types of statistical modeling. Expressi ng p(Of |O) in terms of models\nM, we get\np(Of |O) =\n∑\nM\np(Of , M |O) =\n∑\nM\np(Of |M, O)p(M|O) =\n∑\nM\np(Of |M)p(M|O)\nAccording to this perspective, instead of predicting futur e data using p(Of |Mbest) for just\nthe most probable model Mbest, we should actually sum p(Of |M) over all models M, weigh-\ning each model by its probability given the training data. Ho wever, performing a sum over\nall models is generally impractical. Instead, one might con sider trying to approximate this\nsum with its maximal term max M p(Of |M)p(M|O). However, the identity of this term\ndepends on Of , data that has not been seen yet. Thus, just using p(Of |Mgood ) for some\ngood model Mgood, such as the most probable model Mbest, may be the best we can do\nin practice. Smoothing can be interpreted as a method for cor recting this gap between\ntheory and reality, between p(Of |O) and p(Of |Mgood). Viewed from this perspective, most\nsmoothing algorithms for n-gram models do not even use an intermediate model Mgood, but\n138\njust estimate the distribution p(Of |O) directly from counts in the training data. 3\nWe argue that the Bayesian approach is not attractive for the smoothing problem from a\nmethodological perspective. In the Bayesian framework, th e nature of the prior probability\np(M) is the largest factor in determining performance. The dist ribution p(M) should reﬂect\nhow frequently smoothed n-gram models M occur in the real world. However, it is unclear\nwhat a priori information we have pertaining to the frequency of diﬀerent s moothed n-gram\nmodels; we have little or no intuition on this topic. In addit ion, adjusting p(M) to try to\nyield an accurate distribution p(Of |O) is a rather indirect process.\nFor smoothing, it is much easier to estimate p(Of |O) directly. We have insight into\nwhat a smoothed distribution p(Of |O) should look like given the counts in the training\ndata. For example, we know that an n-gram with zero counts should be given some small\nnonzero corrected count, and that an n-gram with r > 0 counts should be given a corrected\ncount slightly less than r, so that there will be counts available for zero-count n-grams.\nThese corrected counts lead directly to a model p(Of |O). In addition, detailed performance\nanalyses such as the analysis described in Section 2.5 lend t hemselves well to improving\nalgorithms that estimate p(Of |O) directly.\nThere are several existing Bayesian smoothing methods (Nad as, 1984; MacKay and\nPeto, 1995; Ristad, 1995), but none perform particularly we ll or are in wide use.\n5.1.2 Bayesian Grammar Induction\nIn grammar induction, we have a very diﬀerent situation from t hat found in smoothing. In\nsmoothing, we have intuitions on how to estimate p(Of |O), but little intuition of how to\nestimate p(M). In grammar induction, we have the opposite situation. In t his case, the\ndistribution p(Of |O) has a very complex nature. In Section 3.2.3, we give example s that\nhint at the complexity of this distribution. For example, if we see a sequence of numbers\nin some text that happen to be consecutive prime numbers, peo ple know how to predict\nfuture numbers in the sequence with high probability. In Sec tion 3.2.3, we explain how\ncomplex behaviors such as this can be modeled by using the Bay esian abstraction. We take\nthe probability p(O) of some data O to be\np(O) =\n∑\nGp\np(O, Gp) =\n∑\nGp\np(Gp)p(O|Gp) =\n∑\noutput(Gp ) = O\np(Gp)\nwhere Gp represents a program; that is, we view data as being the outpu t of some pro-\ngram. By assigning higher probabilities to shorter program s, we get the desirable behavior\nthat complex patterns in data can be modeled. In the grammar i nduction task, we just\nrestrict programs Gp to those that correspond to grammars G. In this domain, we have\ninsight into the nature of the prior probability p(G), which takes a fairly simple form, while\np(Of |O) takes a very complicated form. Thus, unlike smoothing, we ﬁ nd that the Bayesian\nperspective is appropriate for grammar induction.\n3Alternatively, we can just view Mgood as being the maximum likelihood n-gram model.\n139\n5.1.3 Bilingual Sentence Alignment\nIn sentence alignment, the situation is more similar to gram mar induction than smoothing.\nWe have a complex distribution p(Of |O), or using our alignment notation, p( ⃗Ef , ⃗Ff | ⃗E, ⃗F ).\nInstead of modeling p( ⃗Ef , ⃗Ff | ⃗E, ⃗F ) directly, it is more reasonable to use the Bayesian frame-\nwork and to design a prior on translation models using the min imum description length\nprinciple. This would yield some desirable behaviors; for e xample, in the description of\nthe full translation model we would include the description of the word-to-word translation\nmodel pb(b), which we can describe by listing all word beads b with nonzero probability.\nA minimum description length objective function would favo r models with fewer nonzero-\nprobability word beads, thus discouraging words from havin g superﬂuous translations in\nthe model.\nIn actuality, we decided to use a maximum likelihood approac h, which is equivalent to\nusing the Bayesian approach with a uniform prior over transl ation models. Speciﬁcally, we\nused a variation of the Expectation-Maximization (EM) algo rithm, which is a hill-climbing\nsearch on the likelihood of the training data. However, we us ed an incremental variation\nof the algorithm. Typically, the EM algorithm is an iterativ e algorithm, where in each\niteration the entire training data is processed. At the end o f each iteration, parameters are\nre-estimated so that the likelihood of the data increases (o r does not decrease, at least). In\nour incremental version, we make a single pass through the tr aining data, and we re-estimate\nparameters after each sentence bead. Thus, we do not perform a true maximum likelihood\nsearch; we just do something roughly equivalent to a single i teration in a conventional EM\nsearch.\nWhile we make this choice partially because a full EM search i s too expensive computa-\ntionally, we also believe that a full EM search would yield po orer results. Notice that with\neach iteration of EM performed, the current hypothesis mode l ﬁts closer to the training\ndata. Recall that maximum likelihood models tend to overﬁt t raining data, as the uniform\nprior assigns too much probability to large models. Thus, ad ditional EM iterations will\neventually lead to more and more overﬁtting. By only doing so mething akin to a single\niteration of EM, we avoid the overﬁtting problem. 4\nOne way of viewing this is that we express a prior distributio n over models procedurally.\nInstead of having an explicit prior that prefers smaller mod els, we avoid overﬁtting through\nheuristics in our search strategy. This violates the separa tion between the objective function\nand the search strategy found in the Bayesian framework. On t he other hand, it signiﬁcantly\ndecreases the complexity of the implementation. In the Baye sian framework, one needs to\ndesign an explicit prior over models and to perform a search f or the most probable model;\nthese tasks are expensive both from a design perspective and computationally. Further-\nmore, in sentence alignment it is not necessary to have a trem endously accurate translation\nmodel, as lexical information usually provides a great deal of distinction between correct\nand incorrect alignments. The use of ad hoc methods is not likely to decrease performance\n4Better performance may be achieved by using held-out data to determine when overﬁtting begins, and\nstopping the EM search at that point. However, additional EM passes are expensive computationally and\nour single-pass approach performs well.\n140\nsigniﬁcantly. Thus, we argue that the use of ad hoc methods such as procedural priors is\njustiﬁed for sentence alignment as it saves a great deal of eﬀo rt and computation without\nloss in performance.\nIn summary, while the Bayesian framework provides a princip led and eﬀective approach\nto many tasks, it is best to adjust the framework to the exigen cies of the particular problem.\nIn grammar induction, an explicit Bayesian implementation proved worthwhile, while in\nsentence alignment less rigorous methods performed well. F inally, for smoothing we argue\nthat non-Bayesian methods can be more eﬀective.\n5.2 Sparse Data and Inducing Hidden Structure\nAs mentioned in Chapter 1, perhaps the two most important iss ues in probabilistic modeling\nare the sparse data problem and the problem of inducing hidde n structure. The sparse data\nproblem describes the situation of having insuﬃcient data t o train one’s model accurately.\nThe problem of inducing hidden structure refers to the task o f building models that capture\nstructure not explicitly present in the training data, e.g., the grammatical structure that we\ncapture in our work in grammar induction. It is widely though t that models that capture\nhidden structure can ultimately outperform the shallow mod els that currently oﬀer the best\nperformance. In this thesis, we present several techniques that address these two central\nissues in probabilistic modeling.\n5.2.1 Sparse Data\nApproaches to the sparse data problem can be grouped into two general categories. The\nﬁrst type of approach is smoothing, where one takes existing models and investigates tech-\nniques for more accurately assigning probabilities in the p resence of sparse data. Our work\non smoothing n-gram models greatly forwards the literature on smoothing f or the most\nfrequently used probabilistic models for language. We clar ify the relative performance of\ndiﬀerent smoothing algorithms on a variety of data sets, faci litating the selection of smooth-\ning algorithms for diﬀerent applications; no thorough empir ical study existed before. In\naddition, we provide two new smoothing techniques that outp erform existing techniques.\nThe second general approach to the sparse data problem is the use of compact models.\nCompact models contain fewer probabilities that need to be e stimated and hence require less\ndata to train. While n-gram models are yet to be outperformed by more compact model s,\nthis approach to the sparse data problem seems to be the most p romising as smoothing\nmost likely will yield limited gains. 5 As mentioned in Section 3.1.2, probabilistic grammars\noﬀer the potential for achieving performance equivalent to t hat of n-gram models with much\nsmaller models. In our work on Bayesian grammar induction, w e introduce a novel algorithm\n5There has been some success in combining the two approaches. For example, Brown et al. (1992b) show\nhow class-based n-gram models can achieve performance near to that of n-gram models using much fewer\nparameters. They then show that by linearly interpolating o r smoothing conventional n-gram models with\nclass-based n-gram models, it is possible to achieve performance slightl y superior to that of conventional\nn-gram models alone.\n141\nfor grammar induction that employs the minimum description length principle, under which\nthe objective function used in the grammar search process ex plicitly favors compact models.\nIn experiments on artiﬁcially-generated data, we can achie ve performance equivalent to that\nof n-gram models using a probabilistic grammar that is many time s smaller, as shown in\nSection 3.6. While unable to outperform n-gram models on naturally-occurring data, this\nwork represents very real progress in constructing compact models.\n5.2.2 Inducing Hidden Structure\nModels that capture the hidden structure underlying langua ge have the potential to outper-\nform shallow models such as n-gram models. Not only does our work in grammar induction\nforward research in building compact models as described ab ove, it also demonstrates tech-\nniques for inducing hidden structure. (Clearly, the proble ms of sparse data and inducing\nhidden structure are not completely orthogonal, as taking a dvantage of hidden structure\ncan lead to more compact models.) In Section 3.6, we show how o n artiﬁcially-generated\ntext our grammar induction algorithm is able to capture much of the structure present in\nthe grammar used to generate the text, demonstrating that ou r algorithm can eﬀectively\nextract hidden structure from data.\nOur work in bilingual sentence alignment also addresses the problem of inducing hid-\nden structure. Given a raw bilingual corpus, sentence align ment involves recovering the\nhidden mapping between the two texts that speciﬁes the sente nce(s) in one language that\ntranslate to each sentence in the other language. To this end , our alignment algorithm also\ncalculates a rough mapping between individual words in sent ences in the two languages.\nUnlike in grammar induction, the model used to induce this hi dden structure is a fairly\nshallow model; the model is just used to annotate data with th e extracted structural infor-\nmation. This annotated data can then be used to train structu red models, as in work by\nBrown et al. (1990).\nIn this work on hidden structure, we place an emphasis on the u se of eﬃcient algorithms.\nA major issue in inducing hidden structure is constraining t he search process. Because the\nstructure is hidden, it is diﬃcult to select which structures to consider creati ng, and as a\nresult many algorithms dealing with hidden structure induc tion are ineﬃcient because they\ndo not adequately constrain the search space. Our algorithm s for grammar induction and\nbilingual sentence alignment are both near-linear; both ar e far more eﬃcient than all other\nalgorithms (involving hidden structure induction) that oﬀe r comparable performance.\nWe achieve this eﬃciency through data-driven heuristics th at constrain the set of hy-\npotheses considered in the search process and through heuri stics that allow hypotheses to\nbe evaluated very quickly. In grammar induction, we introdu ce the concept of triggers,\nor particular patterns in the data that indicate that the cre ation of certain rules may be\nfavorable. Triggers reduce the number of grammars consider ed to a manageable amount. In\naddition, to evaluate the objective function eﬃciently, we use sensible heuristics to estimate\nthe most probable parse of the training data given the curren t grammar and to estimate\nthe optimal values of rule probabilities.\nIn sentence alignment, we use thresholding to reduce the com putation of the dynamic\n142\nprogramming lattice from quadratic to linear in data size. T o enable eﬃcient evaluation\nof hypothesis alignments, we use heuristics to constrain wh ich word beads have nonzero\nprobability. As mentioned in Section 4.3.5, limiting the nu mber of such word beads greatly\nsimpliﬁes the search for the most probable beading of a sente nce bead. We believe that\ndata-driven heuristics such as the ones that we have employe d are crucial for making hidden\nstructure induction eﬃcient enough for large data sets.\nIn conclusion, this thesis represents a very signiﬁcant ste p forward towards addressing\ntwo central issues in probabilistic modeling: the sparse da ta problem and the problem of in-\nducing hidden structure. We introduce novel techniques for smoothing and for constructing\ncompact models, as well as novel and eﬃcient techniques for i nducing hidden structure.\n143\nAppendix A\nSample of Lexical Correspondences\nAcquired During Bilingual\nSentence Alignment\nIn this section, we list randomly-sampled lexical correspo ndences acquired during the align-\nment of 20,000 sentences pairs. We only list words occurring at least ten times. The numbers\nadjacent to the English words are the number of times those En glish words occurred in the\ncorpus. The number next to each French word f is the value\nt(e, f ) = ln pb([e, f ])\npe(e)pf (f)\nfor the English word e above; this is a measure of how strongly the English word and F rench\nword translate to each other.\nquality (27)\nqualit´ e 11.69\nqualitatives 11.46\neaux 9.52\nkeeps (16)\nengag´ ee 9.18\ncontinue 8.61\nque 4.66\nform (70)\nforme 10.11\ntrouveraient 8.72\nsorte 7.18\nobligations 6.69\najouter 6.49\nsous 5.03\nune 4.96\navons 3.97\nhouses (20)\nmaisons 11.47\nchambres 10.77\nhabitations 9.41\nmaison 9.34\ndomiciliaire 9.17\nlogements 9.14\nparlementaires 8.01\nacheter 7.69\n144\nthroughout (33)\ntravers 9.51\nagriculteurs 8.58\nlong 8.56\ntoute 7.87\ndans 7.34\norganismes 7.28\no` u 6.83\ndurant 6.55\nmoyens 5.98\ntout 5.53\nminimum (27)\nminimum 12.44\nminimal 12.43\nminimale 11.80\nminimaux 11.42\nminimums 11.19\nminimales 11.03\n´ etudi´ ees 8.95\nmoins 6.61\njusque 5.87\navec 4.87\nenterprises (21)\nentreprises 10.21\npoursuite 8.92\nfaciliter 8.87\nimportance 6.78\nmˆ eme 4.94\nune 4.48\nappear (35)\ncomparaˆitre 9.66\nsemblent 9.36\nsemble 8.92\nvoulons 7.36\nfrais 6.73\n-t- 5.26\ndelivery (17)\nlivraison 11.75\nlivraisons 10.57\nmodiﬁ´ ees 9.57\ncependant 7.72\navant 7.28\n; 6.37\nstocks (17)\nstocks 11.75\nr´ eserves 9.54\nbancs 9.51\nvaleurs 8.75\nactions 8.19\nexercer 7.78\nsteadily (14)\nconstamment 8.21\ncesse 8.18\ncombined (13)\njoints 9.91\ndeux 7.67\nespecially (25)\nsurtout 11.20\nparticuli` erement 10.99\nsp´ ecialement 10.18\nparticulier 9.55\nnotamment 9.43\npr´ ecis´ ement 7.65\nassurer 6.17\nqui 4.74\nﬂoor (30)\nplancher 11.61\nparole 9.42\nlocataires 8.66\nchambre 7.45\nlosing (19)\nperdons 10.54\nperd 9.92\nperdre 9.43\n145\nnew (134)\nnew 11.19\nnouveaux 11.15\nnouvelles 10.74\nnouvelle 10.70\nnouveau 10.54\nnouvel 10.30\nd´ emocrate 9.59\nneuves 9.46\nd´ emocrates 9.17\nneuf 8.64\nmanner (43)\nfa¸ con 8.59\nmani` ere 8.27\ntoucher 7.06\nquoi 6.07\navaient 5.77\nm. 5.76\navec 5.75\ndestin´ ees 5.70\n-t- 5.43\naussi 4.99\nA.1 Poor Correspondences\nIn this section, we list some selected English words for whic h the acquired lexical correspon-\ndences are not overly appropriate.\nthe (19379)\nthe 7.42\nla 6.56\n` a 5.65\nau 5.23\nencourag´ e 5.17\ncette 4.96\nprescrit 4.93\nproﬁtable 4.92\nparenchymes 4.90\ntenu 4.86\nat (2292)\nat 7.66\nheures 6.89\nentrepos´ es 6.60\nheure 6.39\nconformit´ e 5.98\nrapatri´ e 5.95\nimmobilis´ es 5.93\nlors 5.89\nvoyant 5.89\nrespectifs 5.88\non (1577)\ncommenter 6.96\nau 6.84\ndevrai 6.70\nvisites 6.63\naﬀreusement 6.43\nsoul` eve 6.15\nattaquant 5.98\nvinicole 5.97\nvictime 5.87\nensuite 5.84\nof (23032)\nof 7.91\nrappel 6.63\nfermeture 5.56\nhistoriques 5.51\ndes 5.12\ndemanderais 4.87\nordre 4.77\nentendu 4.77\npr´ esider 4.64\nr` egne 4.63\nReferences\n[Angluin and Smith1983] D. Angluin and C.H. Smith. 1983. Ind uctive inference: theory\nand methods. ACM Computing Surveys , 15:237–269.\n[Bahl et al. 1983] Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer . 1983. A maximum\n146\nlikelihood approach to continuous speech recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , PAMI-5(2):179–190, March.\n[Baker1979] James K. Baker. 1979. Trainable grammars for sp eech recognition. In Pro-\nceedings of the Spring Conference of the Acoustical Society o f America , pages 547–550,\nBoston, MA, June.\n[Baum1972] L.E. Baum. 1972. An inequality and associated ma ximization technique in\nstatistical estimation of probabilistic functions of a Mar kov process. Inequalities, 3:1–8.\n[Bell et al. 1990] Timothy C. Bell, John G. Cleary, and Ian H. Witten. 1990 . Text Compres-\nsion. Prentice Hall, Englewood Cliﬀs, N.J.\n[Bellman1957] Richard Bellman. 1957. Dynamic Programming. Princeton University Press,\nPrinceton N.J.\n[Brent1973] Richard P. Brent. 1973. Algorithms for Minimization without Derivatives .\nPrentice-Hall, Englewood Cliﬀs, NJ.\n[Brown et al. 1990] Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vi ncent J. DellaPi-\netra, Frederick Jelinek, John D. Laﬀerty, Robert L. Mercer, a nd Paul S. Roossin. 1990.\nA statistical approach to machine translation. Computational Linguistics , 16(2):79–85,\nJune.\n[Brown et al. 1991a] Peter F. Brown, Stephen A. DellaPietra, Vincent J. De llaPietra, and\nRobert L. Mercer. 1991a. Word sense disambiguation using st atistical methods. In\nProceedings 29th Annual Meeting of the ACL , pages 265–270, Berkeley, CA, June.\n[Brown et al. 1991b] Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. 1991b. Aligning\nsentences in parallel corpora. In Proceedings 29th Annual Meeting of the ACL , pages\n169–176, Berkeley, CA, June.\n[Brown et al. 1992a] Peter F. Brown, Stephen A. DellaPietra, Vincent J. De llaPietra, Jen-\nnifer C. Lai, and Robert L. Mercer. 1992a. An estimate of an up per bound for the\nentropy of English. Computational Linguistics , 18(1):31–40, March.\n[Brown et al. 1992b] Peter F. Brown, Vincent J. DellaPietra, Peter V. deSo uza, Jennifer C.\nLai, and Robert L. Mercer. 1992b. Class-based n-gram models of natural language.\nComputational Linguistics , 18(4):467–479, December.\n[Brown et al. 1993] Peter F. Brown, Stephen A. DellaPietra, Vincent J. Del laPietra, and\nRobert L. Mercer. 1993. The mathematics of statistical mach ine translation: Parameter\nestimation. Computational Linguistics , 19(2):263–312.\n[Carroll1995] Glenn Carroll. 1995. Learning Probabilistic Grammars for Language Modeling .\nPh.D. thesis, Brown University, May.\n[Catizone et al. 1989] Roberta Catizone, Graham Russell, and Susan Warwick. 1989. De-\nriving translation data from bilingual texts. In Proceedings of the First International\nAcquisition Workshop, Detroit, Michigan, August.\n147\n[Chen and Goodman1996] Stanley F. Chen and Joshua T. Goodman . 1996. An empirical\nstudy of smoothing techniques for language modeling. In Proceedings of the 34th Annual\nMeeting of the ACL , Santa Cruz, California, June. To appear.\n[Chen et al. 1993] Stanley F. Chen, Andrew S. Kehler, and Stuart M. Shiebe r. 1993. Exper-\niments in stochastic grammar inference with simulated anne aling and the inside-outside\nalgorithm. Unpublished report.\n[Chen1993] Stanley F. Chen. 1993. Aligning sentences in bil ingual corpora using lexical in-\nformation. In Proceedings of the 31st Annual Meeting of the ACL , pages 9–16, Columbus,\nOhio, June.\n[Chen1995] Stanley F. Chen. 1995. Bayesian grammar inducti on for language modeling.\nIn Proceedings of the 33rd Annual Meeting of the ACL , pages 228–235, Cambridge,\nMassachusetts, June.\n[Chomsky1964] Noam Chomsky. 1964. Syntactic Structures . Mouton.\n[Church and Gale1991] Kenneth W. Church and William A. Gale. 1991. A comparison of\nthe enhanced Good-Turing and deleted estimation methods fo r estimating probabilities\nof English bigrams. Computer Speech and Language , 5:19–54.\n[Church1988] Kenneth Church. 1988. A stochastic parts prog ram and noun phrase parser for\nunrestricted text. In Proceedings of the Second Conference on Applied Natural Lang uage\nProcessing, pages 136–143.\n[Collins and Brooks1995] Michael Collins and James Brooks. 1995. Prepositional phrase at-\ntachment through a backed-oﬀ model. In David Yarowsky and Ke nneth Church, editors,\nProceedings of the Third Workshop on Very Large Corpora , pages 27–38, Cambridge,\nMA, June.\n[Cook et al. 1976] Craig M. Cook, Azriel Rosenfeld, and Alan R. Aronson. 1 976. Grammat-\nical inference by hill climbing. Information Sciences , 10:59–80.\n[Cover and King1978] T.M. Cover and R.C. King. 1978. A conver gent gambling estimate of\nthe entropy of English. IEEE Transactions on Information Theory , 24(4):413–421.\n[Dagan et al. 1991] Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two lan guages are more\ninformative than one. In Proceedings of the 29th Annual Meeting of the ACL , pages\n130–137.\n[de Marcken1995] Carl de Marcken. 1995. Lexical heads, phra se structure, and the induction\nof grammar. In Proceedings of the Third Workshop on Very Large Corpora , Cambridge,\nMA, June.\n[Dempster et al. 1977] A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maxim um like-\nlihood from incomplete data via the EM algorithm. Journal of the Royal Statistical\nSociety, 39(B):1–38.\n[Gale and Church1990] William A. Gale and Kenneth W. Church. 1990. Estimation pro-\ncedures for language context: poor estimates are worse than none. In COMPSTAT,\nProceedings in Computational Statistics, 9th Symposium , pages 69–74, Dubrovnik, Yu-\ngoslavia, September.\n148\n[Gale and Church1991] William A. Gale and Kenneth W. Church. 1991. A program for\naligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting of\nthe ACL , Berkeley, California, June.\n[Gale and Church1993] William A. Gale and Kenneth W. Church. 1993. A program for\naligning sentences in bilingual corpora. Computational Linguistics , 19(1):75–102.\n[Gale and Church1994] William A. Gale and Kenneth W. Church. 1994. What’s wrong\nwith adding one? In N. Oostdijk and P. de Haan, editors, Corpus-Based Research into\nLanguage. Rodolpi, Amsterdam.\n[Gale and Sampson1995] William A. Gale and Geoﬀrey Sampson. 1 995. Good-Turing fre-\nquency estimation without tears. Journal of Quantitative Linguistics , 2(3). To appear.\n[Gale et al. 1992] William A. Gale, Kenneth W. Church, and David Yarowsky . 1992. Using\nbilingual materials to develop word sense disambiguation m ethods. In Proceedings of the\nFourth International Conference on Theoretical and Methodolo gical Issues in Machine\nTranslation, pages 101–112, Montr´ eal, Canada, June.\n[Good1953] I.J. Good. 1953. The population frequencies of s pecies and the estimation of\npopulation parameters. Biometrika, 40(3 and 4):237–264.\n[Huﬀman1952] D.A. Huﬀman. 1952. A method for the construction of minimum redundancy\ncodes. Proceedings of the IRE , 40:1098–1101.\n[Hull1992] Jonathon Hull. 1992. Combining syntactic knowl edge and visual text recognition:\nA hidden Markov model for part of speech tagging in a word reco gnition algorithm. In\nAAAI Symposium: Probabilistic Approaches to Natural Langu age, pages 77–83.\n[Isotani and Matsunaga1994] R. Isotani and S. Matsunaga. 19 94. Speech recognition using\na stochastic language model integrating local and global co nstraints. In Proceedings of\nthe Human Language Technology Workshop , pages 88–93, March.\n[Iyer et al. 1994] R. Iyer, M. Ostendorf, and J.R. Rohlicek. 1994. Langua ge modeling with\nsentence-level mixtures. In Proceedings of the Human Language Technology Workshop ,\npages 82–87, March.\n[Jeﬀreys1948] H. Jeﬀreys. 1948. Theory of Probability . Clarendon Press, Oxford, second\nedition.\n[Jelinek and Mercer1980] Frederick Jelinek and Robert L. Me rcer. 1980. Interpolated esti-\nmation of Markov source parameters from sparse data. In Proceedings of the Workshop\non Pattern Recognition in Practice , Amsterdam, The Netherlands: North-Holland, May.\n[Jelinek et al. 1992] Frederick Jelinek, John D. Laﬀerty, and Robert L. Merce r. 1992. Basic\nmethods of probabilistic context-free grammars. In Speech Recognition and Understand-\ning: Recent Advances, Trends, and Applications. Proceedin gs of the NATO Advanced\nStudy Institute , pages 345–360, Cetraro, Italy.\n[Johnson1932] W.E. Johnson. 1932. Probability: deductive and inductive problems. Mind,\n41:421–423.\n149\n[Katz1987] Slava M. Katz. 1987. Estimation of probabilitie s from sparse data for the lan-\nguage model component of a speech recognizer. IEEE Transactions on Acoustics, Speech\nand Signal Processing , ASSP-35(3):400–401, March.\n[Kay and R¨ oscheisen1993] Martin Kay and Martin R¨ oscheisen. 1993. Text-translation align-\nment. Computational Linguistics , 19(1):121–142.\n[Kernighan et al. 1990] M.D. Kernighan, K.W. Church, and W.A. Gale. 1990. A spe lling\ncorrection program based on a noisy channel model. In Proceedings of the Thirteenth\nInternational Conference on Computational Linguistics , pages 205–210.\n[Klavans and Tzoukermann1990] Judith Klavans and Evelyne T zoukermann. 1990. The\nbicord system. In COLING-90, pages 174–179, Helsinki, Finland, August.\n[Kolmogorov1965] A.N. Kolmogorov. 1965. Three approaches to the quantitative deﬁnition\nof information. Problems in Information Transmission , 1(1):1–7.\n[Kuhn1988] R. Kuhn. 1988. Speech recognition and the freque ncy of recently used words:\nA modiﬁed markov model for natural language. In 12th International Conference on\nComputational Linguistics , pages 348–350, Budapest, August.\n[Lari and Young1990] K. Lari and S.J. Young. 1990. The estima tion of stochastic context-\nfree grammars using the inside-outside algorithm. Computer Speech and Language ,\n4:35–56.\n[Lari and Young1991] K. Lari and S.J. Young. 1991. Applicati ons of stochastic context-free\ngrammars using the inside-outside algorithm. Computer Speech and Language , 5:237–\n257.\n[Li and Vit´ anyi1993] Ming Li and Paul Vit´ anyi. 1993. An Introduction to Kolmogorov Com-\nplexity and its Applications . Springer-Verlag.\n[Lidstone1920] G.J. Lidstone. 1920. Note on the general cas e of the Bayes-Laplace formula\nfor inductive or a posteriori probabilities. Transactions of the Faculty of Actuaries ,\n8:182–192.\n[MacKay and Peto1995] David J. C. MacKay and Linda C. Peto. 19 95. A hierarchical\nDirichlet language model. Natural Language Engineering , 1(3):1–19.\n[Magerman and Marcus1990] David M. Magerman and Mitchell P. Marcus. 1990. Parsing\na natural language using mutual information statistics. In Proceedings of the AAAI ,\nBoston, MA.\n[Magerman1994] David M. Magerman. 1994. Natural Language Parsing as Statistical Pat-\ntern Recognition. Ph.D. thesis, Stanford University, February.\n[McCandless and Glass1993] Michael K. McCandless and James R. Glass. 1993. Empir-\nical acquisition of word and phrase classes in the ATIS domai n. In Third European\nConference on Speech Communication and Technology , Berlin, Germany, September.\n[Nadas1984] Arthur Nadas. 1984. Estimation of probabiliti es in the language model of the\nIBM speech recognition system. IEEE Transactions on Acoustics, Speech and Signal\nProcessing, ASSP-32(4):859–861, August.\n150\n[Newell1973] A. Newell. 1973. Speech Understanding Systems: Final Report of a Study\nGroup. North-Holland, Amsterdam.\n[Pasco1976] R. Pasco. 1976. Source coding algorithms for fast data compression . Ph.D.\nthesis, Stanford University.\n[Pereira and Schabes1992] Fernando Pereira and Yves Schabe s. 1992. Inside-outside reesti-\nmation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of\nthe ACL , pages 128–135, Newark, Delaware.\n[Press et al. 1988] W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Ve tterling. 1988.\nNumerical Recipes in C . Cambridge University Press, Cambridge.\n[Resnik1992] Philip Resnik. 1992. Probabilistic tree-adj oining grammar as a framework\nfor statistical natural language processing. In Proceedings of the 14th International\nConference on Computational Linguistics .\n[Rissanen1976] J. Rissanen. 1976. Generalized Kraft inequ ality and arithmetic coding. IBM\nJournal of Research and Development , 20:198.\n[Rissanen1978] J. Rissanen. 1978. Modeling by the shortest data description. Automatica,\n14:465–471.\n[Rissanen1989] J. Rissanen. 1989. Stochastical Complexity and Statistical Inquiry . World\nScientiﬁc Publishing Company.\n[Ristad1995] Eric Sven Ristad. 1995. A natural law of succes sion. Technical Report CS-\nTR-495-95, Princeton University.\n[Rosenfeld and Huang1992] R. Rosenfeld and X.D. Huang. 1992 . Improvements in stochas-\ntic language modeling. In Proceedings of the DARPA Speech and Natural Language\nWorkshop, February.\n[Rosenfeld1994a] Ronald Rosenfeld. 1994a. Adaptive Statistical Language Modeling: a Max-\nimum Entropy Approach . Ph.D. thesis, Carnegie Mellon University, April.\n[Rosenfeld1994b] Ronald Rosenfeld. 1994b. A hybrid approa ch to adaptive statistical lan-\nguage modeling. In Proceedings of the Human Language Technology Workshop , pages\n76–81, March.\n[Sadler1989] V. Sadler. 1989. The Bilingual Knowledge Bank – A New Conceptual Basis for\nMT. BSO/Research, Utrecht.\n[Schabes1992] Yves Schabes. 1992. Stochastic lexicalized tree-adjoining grammars. In Pro-\nceedings of the 14th International Conference on Computationa l Linguistics .\n[Shannon1948] C.E. Shannon. 1948. A mathematical theory of communication. Bell Sys-\ntems Technical Journal , 27:379–423,623–656.\n[Shannon1951] C.E. Shannon. 1951. Prediction and entropy o f printed English. Bell Systems\nTechnical Journal, 30:50–64, January.\n[Shieber1996] Stuart M. Shieber. 1996. Personal communica tion.\n151\n[Simard et al. 1992] M. Simard, G. Foster, and P. Isabelle. 1992. Using cogn ates to align\nsentences in bilingual corpora. In Fourth International Conference on Theoretical and\nMethodological Issues in Machine Translation (TMI-92) , Montreal, Canada.\n[Solomonoﬀ1959] R.J. Solomonoﬀ. 1959. A progress report on ma chines to learn to trans-\nlate languages and retrieve information. In Advances in Documnetation and Library\nSciences, volume III, pages 941–953. Interscience, New York.\n[Solomonoﬀ1960] R.J. Solomonoﬀ. 1960. A preliminary report o n a general theory of induc-\ntive inference. Technical Report ZTB-138, Zator Company, C ambridge, MA, November.\n[Solomonoﬀ1964] R.J. Solomonoﬀ. 1964. A formal theory of indu ctive inference. Informa-\ntion and Control , 7:1–22, 224–254, March, June.\n[Srihari and Baltus1992] Rohini Srihari and Charlotte Balt us. 1992. Combining statistical\nand syntactic methods in recognizing handwritten sentence s. In AAAI Symposium:\nProbabilistic Approaches to Natural Language , pages 121–127.\n[Stolcke and Omohundro1994] Andreas Stolcke and Stephen Om ohundro. 1994. Best-ﬁrst\nmodel merging for hidden Markov model induction. Technical Report TR-94-003, In-\nternational Computer Science Institute, Berkeley, CA.\n[Warwick and Russell1990] Susan Warwick and Graham Russell . 1990. Bilingual concor-\ndancing and bilingual lexicography. In EURALEX 4th International Congress , M´ alaga,\nSpain.\n[Woods et al. 1976] W. Woods, M. Bates, G. Brown, B. Bruce, C. Cook, J. Klovs tad,\nJ. Makhoul, B. Nash-Webber, R. Schwartz, J. Wolf, and V. Zue. 1976. Speech un-\nderstanding systems: ﬁnal report, November 1974–October 1 976. Bolt Beranek and\nNewman Inc., Boston.\n[Younger1967] D.H. Younger. 1967. Recognition and parsing of context free languages in\ntime n3. Information and Control , 10:198–208.\n152",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7802748680114746
    },
    {
      "name": "Probabilistic logic",
      "score": 0.7056905627250671
    },
    {
      "name": "Language model",
      "score": 0.6136184930801392
    },
    {
      "name": "Artificial intelligence",
      "score": 0.589505672454834
    },
    {
      "name": "Statistical model",
      "score": 0.5852113366127014
    },
    {
      "name": "Smoothing",
      "score": 0.5780137181282043
    },
    {
      "name": "Grammar",
      "score": 0.5196574926376343
    },
    {
      "name": "Sentence",
      "score": 0.5148619413375854
    },
    {
      "name": "Natural language",
      "score": 0.4959739148616791
    },
    {
      "name": "Natural language processing",
      "score": 0.4811793267726898
    },
    {
      "name": "Probabilistic relevance model",
      "score": 0.4501122236251831
    },
    {
      "name": "Word (group theory)",
      "score": 0.43302756547927856
    },
    {
      "name": "Modeling language",
      "score": 0.4203530550003052
    },
    {
      "name": "Machine learning",
      "score": 0.345670223236084
    },
    {
      "name": "Probabilistic analysis of algorithms",
      "score": 0.1849633753299713
    },
    {
      "name": "Programming language",
      "score": 0.14282488822937012
    },
    {
      "name": "Linguistics",
      "score": 0.12267675995826721
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Software",
      "score": 0.0
    }
  ],
  "cited_by": 93
}