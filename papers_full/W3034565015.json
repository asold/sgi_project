{
    "title": "Unsupervised Domain Adaptation of a Pretrained Cross-Lingual Language Model",
    "url": "https://openalex.org/W3034565015",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2126452540",
            "name": "Juntao Li",
            "affiliations": [
                "Peking University",
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2740518062",
            "name": "Ruidan He",
            "affiliations": [
                "Alibaba Group (Cayman Islands)"
            ]
        },
        {
            "id": "https://openalex.org/A2132756360",
            "name": "Hai Ye",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2146810117",
            "name": "Hwee Tou Ng",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2160800796",
            "name": "Lidong Bing",
            "affiliations": [
                "Alibaba Group (Cayman Islands)"
            ]
        },
        {
            "id": "https://openalex.org/A2109109241",
            "name": "Rui Yan",
            "affiliations": [
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964266061",
        "https://openalex.org/W2952614664",
        "https://openalex.org/W2016630033",
        "https://openalex.org/W2963308086",
        "https://openalex.org/W4299579390",
        "https://openalex.org/W2952492414",
        "https://openalex.org/W2803832867",
        "https://openalex.org/W2250904672",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950733326",
        "https://openalex.org/W2963729324",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2251805974",
        "https://openalex.org/W2963768241",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2952289666",
        "https://openalex.org/W2887997457",
        "https://openalex.org/W2762022354"
    ],
    "abstract": "Recent research indicates that pretraining cross-lingual language models on large-scale unlabeled texts yields significant performance improvements over various cross-lingual and low-resource tasks. Through training on one hundred languages and terabytes of texts, cross-lingual language models have proven to be effective in leveraging high-resource languages to enhance low-resource language processing and outperform monolingual models. In this paper, we further investigate the cross-lingual and cross-domain (CLCD) setting when a pretrained cross-lingual language model needs to adapt to new domains. Specifically, we propose a novel unsupervised feature decomposition method that can automatically extract domain-specific features and domain-invariant features from the entangled pretrained cross-lingual representations, given unlabeled raw texts in the source language. Our proposed model leverages mutual information estimation to decompose the representations computed by a cross-lingual model into domain-invariant and domain-specific parts. Experimental results show that our proposed method achieves significant performance improvements over the state-of-the-art pretrained cross-lingual language model in the CLCD setting.",
    "full_text": "Unsupervised Domain Adaptation of\na Pretrained Cross-Lingual Language Model\u0003\nJuntao Li1;2y , Ruidan He3 , Hai Ye2 , Hwee Tou Ng2 , Lidong Bing3 and Rui Yan1\n1Center for Data Science, Academy for Advanced Interdisciplinary Studies, Peking University\n2Department of Computer Science, National University of Singapore\n3DAMO Academy, Alibaba Group\nlijuntao@pku.edu.cn, ruidan.he@alibaba-inc.com, fyeh, nghtg@comp.nus.edu.sg,\nl.bing@alibaba-inc.com, ruiyan@pku.edu.cn\nAbstract\nRecent research indicates that pretraining cross-\nlingual language models on large-scale unlabeled\ntexts yields signiﬁcant performance improvements\nover various cross-lingual and low-resource tasks.\nThrough training on one hundred languages and\nterabytes of texts, cross-lingual language mod-\nels have proven to be effective in leveraging\nhigh-resource languages to enhance low-resource\nlanguage processing and outperform monolingual\nmodels. In this paper, we further investigate\nthe cross-lingual and cross-domain (CLCD) setting\nwhen a pretrained cross-lingual language model\nneeds to adapt to new domains. Speciﬁcally,\nwe propose a novel unsupervised feature decom-\nposition method that can automatically extract\ndomain-speciﬁc features and domain-invariant fea-\ntures from the entangled pretrained cross-lingual\nrepresentations, given unlabeled raw texts in the\nsource language. Our proposed model lever-\nages mutual information estimation to decompose\nthe representations computed by a cross-lingual\nmodel into domain-invariant and domain-speciﬁc\nparts. Experimental results show that our proposed\nmethod achieves signiﬁcant performance improve-\nments over the state-of-the-art pretrained cross-\nlingual language model in the CLCD setting.\n1 Introduction\nRecent progress in deep learning beneﬁts a variety of NLP\ntasks and leads to signiﬁcant performance improvements\nwhen large-scale annotated datasets are available. For high-\nresource languages, e.g., English, it is feasible for many tasks\nto collect sufﬁcient labeled data to build deep neural mod-\nels. However, for many languages, there might not exist\nenough data in most cases to make full use of the advances of\ndeep neural models. As such, various cross-lingual transfer\nlearning methods have been proposed to utilize labeled data\n\u0003This work was supported by Alibaba Group through Alibaba\nInnovative Research (AIR) Program.\nyThis work was done when Juntao Li was an intern at the Na-\ntional University of Singapore.\nfrom high-resource languages to construct deep models for\nlow-resource languages [Kim et al., 2019; Lin et al., 2019;\nHe et al., 2019; Vuli´c et al., 2019]. Nonetheless, most cross-\nlingual transfer learning research focuses on mitigating the\ndiscrimination of languages, while leaving the domain gap\nless explored. In this study, we concentrate on a more chal-\nlenging setting, i.e., cross-lingual and cross-domain (CLCD)\ntransfer, where in-domain labeled data in the source language\nis not available.\nConventionally, cross-lingual methods mainly rely on ex-\ntracting language-invariant features from data to transfer\nknowledge learned from the source language to the target lan-\nguage. One straightforward method is weight sharing, which\ndirectly reuses the model parameters trained on the source\nlanguage to the target language, by mapping an input text to a\nshared embedding space beforehand. However, previous re-\nsearch [Chen et al., 2018] revealed that weight sharing is not\nsufﬁcient for extracting language-invariant features that can\ngeneralize well across languages. As a result, a language-\nadversarial training strategy was proposed to extract invari-\nant features across languages, using non-parallel unlabeled\ntexts from each language. Such a strategy performs well for\nthe bilingual transfer setting but is not suitable for extracting\nlanguage-invariant features from multiple languages, since\nfeatures shared by all source languages might be too sparse\nto retain useful information.\nRecently, pretrained cross-lingual language models at\nscale, e.g., multilingual BERT[Devlin et al., 2019] and XLM\n[Conneau and Lample, 2019; Conneau et al., 2019], show\nvery competitive performance over various cross-lingual\ntasks, and even outperform pretrained monolingual mod-\nels on low-resource languages. Through employing paral-\nlel texts (unlabeled for any speciﬁc task) and shared sub-\nword vocabulary over all languages, these pretrained cross-\nlingual models can effectively encode input texts from mul-\ntiple languages to one single representation space, which is\na feature space shared by multiple languages (more than one\nhundred). While generalizing well for extracting language-\ninvariant features, cross-lingual pretraining methods have\nno speciﬁc strategy for extracting domain-invariant features.\nIn our CLCD setting, both language-invariant and domain-\ninvariant features need to be extracted.\nTo address the aforementioned limitation of cross-lingual\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3672\npretrained models [Conneau et al., 2019] in CLCD scenarios,\nwe propose an unsupervised feature decomposition (UFD)\nmethod, which only leverages unlabeled data in the source\nlanguage. Speciﬁcally, our proposed method is inspired by\nthe recently proposed unsupervised representation learning\nmethod [Hjelm et al., 2019] and can simultaneously extract\ndomain-invariant features and domain-speciﬁc features by\ncombining mutual information maximization and minimiza-\ntion. Compared to previous cross-lingual transfer learning\nmethods, our proposed model maintains the merits of cross-\nlingual pretrained models, i.e., generalizing well for over\na hundred languages, and only needs unlabeled data in the\nsource language for domain adaptation, which is suitable for\nmore cross-lingual transfer scenarios.\nWe evaluate our model on a benchmark cross-lingual sen-\ntiment classiﬁcation dataset, i.e., Amazon Review [Pretten-\nhofer and Stein, 2010 ], which involves multiple languages\nand domains. Experimental results indicate that, with the\nenhancement of the pretrained XLM cross-lingual language\nmodel, our proposed UFD model (trained on some unlabeled\nraw texts in the source language) along with a simple lin-\near classiﬁer (trained on a small labeled dataset in the source\nlanguage and the source domain) outperforms state-of-the-art\nmodels that have access to strong cross-lingual supervision\n(e.g., commercial MT systems) or labeled datasets in mul-\ntiple source languages. Furthermore, incorporating our pro-\nposed UFD strategy with an unlabeled set of 150K instances\nin the source language leads to continuous gains over the\nstrong pretrained XLM model that is trained on one hundred\nlanguages and terabytes of texts. Extensive experiments fur-\nther demonstrate that unsupervised feature decomposition on\na pretrained cross-lingual language model outperforms a pre-\ntrained domain-speciﬁc language model trained on over 100\nmillion sentences.\n2 Related Work\nCross-lingual transfer learning (CLTL) has long been inves-\ntigated [Yarowsky et al., 2001] and is still one of the fron-\ntiers of natural language processing [Chen et al., 2019 ].\nThrough utilizing rich annotated data in high-resource lan-\nguages, CLTL signiﬁcantly alleviates the challenge of scarce\ntraining data in low-resource languages. Conventionally,\nCLTL mainly focuses on resources that are available for\ntransferring, e.g., collecting parallel texts between two lan-\nguages to directly transfer model built in a rich-resource lan-\nguage to a low-resource one [Pham et al., 2015] or construct-\ning annotated data in the target language by machine trans-\nlation systems [Xu and Yang, 2017 ]. Subsequently, with\nthe success of deep learning, cross-lingual word embeddings\nare proposed to learn the shared representation space at the\nfundamental level and can beneﬁt various downstream tasks\n[Artetxe et al., 2018; Conneau et al., 2018a]. Later, a cross-\nlingual sentence representation is also proposed [Conneau\net al., 2018b ]. Chen et al. [2018] designed a language-\nadversarial training strategy to extract language-invariant fea-\ntures that can directly transfer to the target language.\nAnother direction is pretraining cross-lingual [Conneau\nand Lample, 2019 ] or multilingual language models [De-\nvlin et al., 2019]. Beneﬁting from the large-scale training\ntexts and model size, these pretraining methods have changed\nthe face of cross-lingual transfer learning. Empirical results\ndemonstrate that representation space shared by one hundred\nlanguages can signiﬁcantly outperform the language-speciﬁc\npretrained models [Conneau et al., 2019]. As language ad-\nversarial training will lead to sparse language-invariant rep-\nresentations when multiple languages are involved [Chen et\nal., 2019], we follow the line of cross-lingual language model\npretraining. Unlike previous pretraining methods, we focus\non domain adaptation of these pretrained models. To main-\ntain the generalization ability of the cross-lingual pretrained\nmodel, we mainly consider the unsupervised domain adap-\ntation setting. The work most related to ours is proposed\nfor unsupervised representation learning [Hjelm et al., 2019],\nwhich is primarily used for visual representation learning.\n3 Model\nIn this section, we ﬁrst deﬁne the problem discussed in this\npaper and then describe the proposed method in detail.\n3.1 Problem Deﬁnition & Model Overview\nIn this paper, we consider a setting where we only have a la-\nbeled set Ds;s of a speciﬁc language and a speciﬁc domain\nwhich we call source language and source domain, and we\nwant to train a classiﬁer to be tested on a set Dt;t of a dif-\nferent language and a different domain which we call target\nlanguage and target domain. We also assume access to some\nunlabeled raw data Ds;u of multiple domains including the\ntarget domain from the source language during the training\nphase, which is usually feasible in practical applications. We\ncall this setting unsupervised cross-lingual and cross-domain\n(CLCD) adaptation.\nAs illustrated in Figure 1, the proposed method consists\nof three components: a pretrained multilingual embedding\nmodule which embeds the input document into a language-\ninvariant representation, an unsupervised feature decomposi-\ntion (UFD) module which extracts domain-invariant features\nand domain-speciﬁc features from the entangled language-\ninvariant representation, and a task-speciﬁc module trained\non the extracted domain-invariant and domain-speciﬁc fea-\ntures. We adopt XLM 1 [Conneau and Lample, 2019 ] as\nthe multilingual embedding module in our method, which\nhas been pretrained by large-scale parallel and monolingual\ndata from various languages and is the current state-of-the-\nart cross-lingual language model. We describe the other two\nmodules and the training process in the following subsec-\ntions.\n3.2 Unsupervised Feature Decomposition\nMutual Information Estimation\nBefore elaborating on the proposed unsupervised feature\ndecomposition module, we ﬁrst present some preliminary\nknowledge on mutual information estimation, which is em-\nployed in the training objectives of UFD. Mutual informa-\ntion (MI) is growing in popularity as an objective function\n1The latest version XLM-R is adopted, which is trained on over\none hundred languages and 2.5 terabytes of texts.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3673\nFigure 1: Our unsupervised domain adaptation model, where Min-\nMI and Max-MI refer to MI maximization and minimization. The\nmiddle-left part is the feature extractor Fs and the right is Fp.\nin unsupervised representation learning. It measures how in-\nformative one variable is of another variable. In the context\nof unsupervised representation learning, MI maximization is\nusually adopted such that the encoded representation maxi-\nmally encodes information of the original data. MI is difﬁcult\nto compute, particularly in continuous and high-dimensional\nsettings, and therefore various estimation approaches have\nbeen proposed.\nIn our method, we adopt a recently proposed neural estima-\ntion approach [Belghazi et al., 2018], which estimates MI of\ntwo continuous random variables X and Y by training a net-\nwork to distinguish between samples coming from their joint\ndistribution, J, and the product of their marginal distributions,\nM. This estimation utilizes a lower-bound of MI based on\nthe Donsker-Varadhan representation (DV) of KL-divergence\n[Donsker and Varadhan, 1983],\nI(X; Y) :=DKL(JjjM) \u0015bIDV (X; Y)\n:= EJ[T!(x;y)] \u0000log EM[eT!(x;y)]\n(1)\nwhere T! is a discrimination function parameterized by a\nneural network with learnable parameters!. It maps a sample\nfrom space X \u0002Y to a real value in R. Through maximiz-\ning bIDV , T! is encouraged to distinguish between samples\ndrawn from J and M by assigning the former large values\nwhile the latter small ones.\nProposed Method\nLet X 2 Rd denote the language-invariant representation\ngenerated by the pretrained multilingual embedding module.\nIt is then fed into the proposed UFD module as input. As\nshown in Figure 1, we introduce two feature extractors: the\ndomain-invariant extractorFs (i.e., the two-layer feedforward\nnetwork with ReLU activation on the left), and the domain-\nspeciﬁc extractor Fp (i.e., the two-layer network on the right).\nWe denote the extracted features as Fs(X) and Fp(X) re-\nspectively. Note that for Fs, we add residual connections to\nbetter maintain domain-invariant attributes from X.\nSpeciﬁcally, Fs aims to extract domain-invariant features\nfrom the language-invariant representation in an unsuper-\nvised manner. Since the multilingual embedding module is\npretrained on open domain datasets from over one hundred\nlanguages, presumably the generated language-invariant rep-\nresentations should contain certain attributes that can be gen-\neralized across domains. When Fs is trained on multiple do-\nmains with jointly maximizing MI between the inputs and\noutputs of each domain, it is encouraged to retain the shared\nfeatures among those domains from the language-invariant\nrepresentations. In this way, Fs is forced to pass domain-\ninvariant information from X to Fs(X).\nWe utilize the neural network-based estimator as presented\nin Equation (1) for computing MI. In our case, as Fs(X) is\ndependent on X, we can simplify the DV-based MI estimator\nto a Jensen-Shannon MI estimator as suggested in [Hjelm et\nal., 2019]:\nbIJSD (X; Fs(X)) :=EP[\u0000sp(\u0000T!(x;Fs(x)))]\n\u0000EP\u0002eP[sp(T!(x\n0\n;Fs(x)))]\n(2)\nwhere x is an input embedding with empirical probabil-\nity distribution P. As Fs(x) is directly computed from x,\n(x;Fs(x)) can be regarded as a sample drawn from the joint\ndistribution of X and Fs(X). x\n0\ncorresponds to an input\nembedding from eP = P, i.e., x\n0\nis computed from a ran-\ndom sample drawn from the same input distribution, such that\n(x\n0\n;Fs(x)) is drawn from the product of marginal distribu-\ntions. sp(z) = log(1 +ez) is the softplus activation function.\nThe training objective of Fs is to maximize the MI on Xand\nFs(X) and the loss is formulated as follows:\nLs(!s; s) =\u0000bIJSD (X;Fs(X)) (3)\nwhere !s denotes the parameters of the discrimination net-\nwork in the estimator and  s denotes the parameters of Fs.\nTo facilitate learning of domain-invariant features, we also\npropose to maximize the MI onFs(X) and the corresponding\nintermediate representation (ﬁrst layer output) F0\ns(X), and\nthe training loss is as follows:\nLr(!r; s) =\u0000bIJSD (F0\ns(X);Fs(X)) (4)\nwhere !r denotes the parameters of the discriminator network\nin the estimator.\nRecall that the objective ofFp is to extract domain-speciﬁc\nfeatures, which is supposed to be exclusive and independent\nof domain-invariant features. We propose to minimize the MI\nbetween features extracted by Fs and Fp, and the training\nloss is formulated as follows:\nLp(!p; s; p) =bIJSD (Fs(X);Fp(X)) (5)\nwhere  p denotes the parameters of Fp. !p denotes the pa-\nrameters of the discrimination network in MI estimator.\nThe training objective of the proposed UFD component is\nthus to minimize the overall loss as follows:\nLUFD = \u000bLs + \fLr + \rLp (6)\nwhere \u000b, \f, and \rare hyperparameters to balance the effects\nof sub-losses.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3674\nDatasets English\nBooks DVD Music\n#Documents 8,898,041 1,097,592 1,697,533\n#Sentences 101,061,948 16,447,191 21,062,292\n#Words 1,302,754,313 194,145,510 277,987,802\nAvg length 146.4 176.9 163.8\nTable 1: Statistics of domain-speciﬁc raw texts.\n3.3 Task-Speciﬁc Module\nIn the task-speciﬁc module, we ﬁrst employ a linear layer that\nmaps the concatenation of the domain-invariant and domain-\nspeciﬁc features in R2d into a vector representation in Rd.\nA simple feedforward layer with softmax activation is then\nemployed on this mapped vector representation to output the\ntask label. We train this module onDs;s and the cross-entropy\nloss denoted as Lt is utilized as the training objective.\n3.4 Training\nNote that the parameters of the multilingual embedding mod-\nule are pretrained and set to be frozen in the entire train-\ning process. We ﬁrst optimize the parameters of UFD, i.e.,\nfb!s;b!r;b!p; b s; b pgby minimizing LUFD on the unlabeled\nset Ds;u. Once the UFD module is trained, we ﬁx its param-\neters and train the task-speciﬁc module by minimizing Lt on\nthe labeled set Ds;s.\n4 Experimental Setting\n4.1 Datasets\nWe conduct experiments on the multi-lingual and multi-\ndomain Amazon review dataset [Prettenhofer and Stein,\n2010], which serves as a benchmark in previous cross-lingual\nsentiment analysis research and also supports cross-lingual\nand cross-domain evaluation. This dataset includes texts in\nfour languages, i.e., English, German, French, and Japanese,\nand each language contains three domains, i.e., Books, DVD,\nand Music. There are a training set and a test set for each\ndomain in each language and both consist of 1,000 positive\nreviews and 1,000 negative reviews.\nIn our CLCD evaluation, we treat English as the only\nsource language and attempt to adapt to the other three lan-\nguages. As each language contains three domains, we can\nconstruct 3 \u00022 CLCD source-target pairs between English\nand a speciﬁc target language. Therefore, we have 18 CLCD\nsource-target pairs in total considering all three target lan-\nguages. During training, we ﬁrst utilize some unlabeled raw\ndata from the source language for optimizing the proposed\nUFD. Then, the training set from the source language and\nsource domain is used for training the task-speciﬁc module.\nDuring testing, the model is evaluated on the test set of the\ntarget language and target domain.\nWe draw samples from 3 larger unannotated datasets\nof Books, DVD, and Music domains released in [He and\nMcAuley, 2016]. The statistics of the three datasets are given\nin Table 1. We randomly sample 50K documents from each\ndomain as the unlabeled domain-speciﬁc set in the source lan-\nguage (i.e., English) to be utilized during training. To encour-\nage Fs to capture domain-shared features, we utilize domain-\nspeciﬁc unlabeled sets from all domains (50K\u00023) in training\nthe UFD module. We also show the change in performance\nwhen varying the number of unlabeled samples in Section 5.\n4.2 Baselines\nWe denote our proposed model as XLM-UFD, and we com-\npare it with the following baselines:\nCL-RL [Xiao and Guo, 2013] is a cross-lingual word rep-\nresentation learning method, which learns the connection be-\ntween two languages by sharing part of the word vectors.\nBi-PV [Pham et al., 2015] attempts to learn paragraph vec-\ntors in a bilingual context setting by sharing the distributed\nrepresentations of unannotated parallel data from different\nlanguages.\nCLDFA [Xu and Yang, 2017] is a cross-lingual distillation\nmethod which leverages a parallel corpus of documents. An\nadversarial feature adaptation strategy is applied for reduc-\ning the mismatch between the labeled data and the unlabeled\nparallel document.\nMAN-MOE [Chen et al., 2019 ] addresses the multi-\nlingual transfer setting, i.e., there are multiple source lan-\nguages with labeled data. Building upon a language-\nadversarial training module, this model utilizes a mixture-of-\nexperts (MOE) module to dynamically combine private fea-\ntures of different languages.\nThe above four baselines were originally proposed for\nadaptation in a cross-lingual setting, e.g., adapting from\nEnglish-Books to German-Books. We report their ofﬁcial re-\nsults released in the original papers, which can be regarded\nas upper bounds for their CLCD performances. Note that the\nsetting of MAN-MOE is different, where N to 1 adaption is\nperformed, i.e., from N source languages to one target lan-\nguage. Thus, its cross-lingual performance cannot be simply\nviewed as the upper bound of its CLCD performance. We\nretrain the model in the CLCD setting as another baseline de-\nscribed later. For the baselines described below, they are all\ntrained in the CLCD setting.\nADAN [Chen et al., 2018] exploits adversarial training to\nreduce the representation discrepancy between the encoded\nsource and target embeddings.\nMAN-MOE-D is the version of MAN-MOE trained in a\nCLCD setting. As this speciﬁc model performs N to 1 adap-\ntation, it can adapt from multiple source domains from the\nsame source language to a speciﬁc target domain and target\nlanguage. In our experiments, MAN-MOE-D utilizes two\nsource domains from the same source language. For exam-\nple, when the target language and domain are German-Books,\nMAN-MOE-D takes labeled set from both English-DVD and\nEnglish-Music during training.\nMulti-BPE combines the pretrained multilingual byte-\npair embeddings in 275 languages [Heinzerling and Strube,\n2018]2 with the task-speciﬁc classiﬁer used in our proposed\nmodel to perform CLCD adaptation. This model is used to\ncalibrate the performance of the subword embeddings shared\nacross multiple languages.\n2https://nlp.h-its.org/bpemb/multi/\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3675\nModel German French Japanese\nBooks D\nVD Music Avg Books D\nVD Music Avg Books D\nVD Music Avg\nCL-RL 79.9 77.1\n77.3 78.1 78.3 74.8\n78.7 77.3 71.1 73.1\n74.4 72.9\nBi-PV 79.5 78.6\n82.5 80.2 84.3 79.6\n80.1 81.3 71.8 75.4\n75.5 74.2\nCLDFA 84.0 83.1\n79.0 82.0 83.4 82.6\n83.3 83.1 77.4 80.5\n76.5 78.1\nMAN-MOE 82.4 78.8\n77.2 79.5 81.1 84.3\n80.9 82.1 62.8 69.1\n72.6 68.2\nADAN 82.7 77.1\n79.2 79.6 75.9 75.2\n73.8 74.9 72.5 72.3\n74.3 73.0\nMAN-MOE-D 82.8 80.1\n81.6 81.5 83.0 85.5\n82.0 83.5 70.5 76.0\n70.8 72.4\nMulti-BPE 51.0 53.4\n53.0 52.5 50.5 51.4\n51.1 51.0 50.0 49.8\n50.0 49.9\nDLM 52.1 53.7\n53.3 53.0 57.4 51.5\n55.2 54.7 52.8 51.5\n50.8 51.7\nXLM 80.4 84.9\n79.3 81.5 86.4 86.3\n83.2 85.3 81.7 81.6\n84.1 82.5\nXLM-UFD 89.2 86.4\n88.8 88.1 89.5 89.4\n89.1 89.3 83.8 84.5\n85.2 84.5\nXLM* 86.3 81.2\n84.5 84.0 90.6 86.9\n87.6 88.4 82.9 85.0\n87.0 85.0\nTable 2: Overall comparison of classiﬁcation accuracy between our proposed model and baseline models. The upper part refers to the\naccuracy reported in previous studies in a cross-lingual setting while the middle part refers to our implemented models trained in a CLCD\nsetting. XLM* denotes the XLM model trained on source language target domain labeled data. We report the average values of three runs.\nDLM is a pretrained domain-speciﬁc language model3 im-\nplemented with the code of XLM 4 [Conneau and Lample,\n2019]. It employs the pretrained multilingual byte-pair em-\nbeddings as the initialized representations of input texts to\nmitigate the gap between the source language and the target\nlanguage. This model is used to study the effect of leveraging\nlarge scale domain-speciﬁc unlabeled texts.\nXLM refers to the model where we simply add a feedfor-\nward layer with softmax activation as the output layer on top\nof pretrained XLM [Conneau et al., 2019].\n4.3 Training Details\nThe hidden dimension of XLM is 1024. The input and out-\nput dimensions of the feedforward layers in both Fs and Fp\nare 1024. The discriminator of T!s, T!r , and T!p share the\nsame model structure as suggested in previous work [Hjelm\net al., 2019], i.e., the discriminator consists of two feedfor-\nward layers with ReLU activation. The input and output di-\nmensions of the ﬁrst feedforward layer in the discriminator\nare 2048 and 1024. The input and output dimensions of the\nsecond feedforward layer are 1024 and 1. The input dimen-\nsion of the single-layer task-speciﬁc classiﬁers is 1024. All\ntrainable parameters are initialized from a uniform distribu-\ntion [\u00000:1;0:1].\nWe utilize 100 labeled data in the target language and target\ndomain as the validation set, which is used for hyperparame-\nter tuning and model selection during training. The hyperpa-\nrameters are tuned on the validation set of a speciﬁc source-\ntarget pair, and are then ﬁxed in all experiments of XLM-\nUFD. Speciﬁcally, both UFD and the task-speciﬁc module\nare optimized by Adam [Kingma and Ba, 2014] with a learn-\ning rate of 1 \u000210\u00004. The batch size of training UFD and the\ntask-speciﬁc module are set to 16 and 8, respectively. The\nweights \u000b, \f, \r in Equation (6) are set to 1, 0.3, and 1, re-\nspectively. During training, the model that achieves the best\nperformance (lowest loss) on the validation set is saved for\nevaluation purpose.\n3Trained with the datasets presented in Table 1.\n4https://github.com/facebookresearch/XLM\n5 Results\nTable 2 presents the model comparison results and Table 3\nshows the results of different ablation tests on XLM-UFD.\nClassiﬁcation accuracy is used as the evaluation metric.\n5.1 Model Comparison\nIn Table 2, the top 4 models are trained in a cross-lingual\nsetting, and the middle 6 models are trained in a CLCD set-\nting. We repeat the experiment on each source-target pair for\n3 times with different random seeds and record the average\nresult on each pair. Each reported result for models trained\nin the CLCD setting is the average result of the adaptation\nperformance from two source domains in English. For exam-\nple, a result under German-Books is the average of adaptation\naccuracies from English-DVD and English-Music.\nWe make the following observations from Table 2. (1)\nXLM-UFD achieves signiﬁcantly better results over all base-\nlines across all settings. It even substantially outperforms\nbaselines trained in a cross-lingual setting with parallel texts\nfrom source and target languages such as CLDFA, which is\na much less challenging setting. (2) One interesting ﬁnd-\ning is that MAN-MOE-D performs better than MAN-MOE.\nOne possible reason is that MAN-MOE involves multiple\nsource languages while invariant features shared by multi-\nple languages might be too sparse to maintain enough infor-\nmation for extracting task-speciﬁc features. (3) Among the\npretrained models, multilingual byte pair embeddings (Multi-\nBPE) only achieves low performance. With the enhancement\nof large-scale domain-speciﬁc unlabeled text, the domain-\nspeciﬁc language model (DLM) taking the multilingual byte\npair embeddings as input obtains observable performance\ngains but still has much room for improvement. Beneﬁting\nfrom the large-scale training data and network size, XLM is\nable to perform better than the state-of-the-art task-speciﬁc\nmodels such as CLDFA and MAN-MOE-D on French and\nJapanese. When combined with the proposed UFD, signif-\nicant performance gains are observed on XLM. This points\nout that domain adaptation is necessary for pretrained multi-\nlingual language models when applied to a speciﬁc task.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3676\nSettings German French Japanese\nBooks D\nVD Music Avg Books D\nVD Music Avg Books D\nVD Music Avg\nBasic Model XLM-MI 80.4 84.9\n79.3 81.5 86.4 86.3\n83.2 85.3 81.7 81.6\n84.1 82.5\nModel Ablation\nMax-MI 84.5 82.0\n81.9 82.8 81.1 83.2\n81.8 82.0 81.8 80.6\n81.5 81.3\nMax-Min-MI 88.4 85.9\n87.3 87.2 88.0 88.4\n88.3 88.2 84.4 83.3 85.0\n84.2\n2Max-Min-MI 89.2 86.4\n88.8 88.1 89.5 89.4\n89.1 89.3 83.8 84.5 85.2\n84.5\nUnlabeled Data\nSize\n1K\u00023 87.2 85.4\n86.4 86.4 88.6 88.3\n87.0 88.0 78.9 80.0\n81.0 80.0\n2K\u00023 86.7 84.4\n85.6 85.6 87.9 88.1\n83.8 86.6 84.2 83.4 84.4\n84.0\n5K\u00023 89.0 86.0\n86.9 87.4 87.8 89.1\n86.9 87.9 83.0 83.8\n82.2 82.9\n10K\u00023 88.5 86.3\n88.1 87.6 88.8 88.8\n88.3 88.6 83.7 84.4 85.5\n84.5\n50K\u00023 89.2 86.4\n88.8 88.1 89.5 89.4\n89.1 89.3 83.8 84.5 85.2 84.5\nTable 3: Classiﬁcation accuracy of an ablation study and using different sizes of unlabeled data in the source language (i.e., English).\n5.2 Ablation Study\nTo determine the effect of each module of XLM-UFD, we\nconduct a thorough model ablation. As presented in Ta-\nble 3, we ﬁrst examine the domain-invariant feature ex-\ntractor along with MI maximization between the language-\ninvariant features from the multilingual embedding module\nand the extracted domain-invariant features, namely Max-\nMI. Classiﬁcation accuracy shows that Max-MI with only\ndomain-invariant features enhances the performance of XLM\non German but leads to decreased performance on French\nand Japanese. Through supplementing the domain-speciﬁc\nfeature extractor and the Min-MI objective (i.e., Lp), Max-\nMin-MI has a noticeable performance increase over Max-\nMI and outperforms XLM, which conﬁrms that unsupervised\nfeature decomposition can support dynamic domain-speciﬁc\nand domain-invariant feature combination and improve the\ntask performance. With the enhancement of the intermediate\nMax-MI objective (i.e.,Lr) between the intermediate features\nand the output of domain-invariant feature extractor, 2Max-\nMin-MI achieves signiﬁcant performance improvement over\nMax-MI, and it is used as the full model for conducting\nother comparison and ablation. In addition, we investigate\nthe effect of unlabeled data size of the source language used\nduring training. It can be seen from Table 3 that the set-\nting with 5K\u00023 unlabeled raw texts already yields a very\npromising performance. Further increasing the number of\nunlabeled examples continuously improves the model perfor-\nmance on French and German. When the unlabeled data size\nis larger than 10K\u00023, the performance improvement becomes\nmarginal on Japanese but continues on German and French.\n5.3 Visualization\nTo intuitively understand the process of domain-invariant fea-\nture and domain-speciﬁc feature extraction, we also give the\nt-SNE plots [Maaten and Hinton, 2008 ] of the UFD mod-\nule at the tenth epoch. Speciﬁcally, we sample ﬁve thousand\nraw texts from the source domain and target language. Each\nraw text is processed by XLM, and the following domain-\ninvariant feature extractor and domain-speciﬁc feature extrac-\ntor, respectively. As presented in Figure 2, each data point\nin the plots represents an input text. We can observe from\nthe left plot that the domain-invariant features and domain-\nspeciﬁc features of input texts have a clear border that can be\ndistinguished, which suggests that mutual information min-\nimization can force the two extractors to exclusively extract\ntwo sets of features. The right plot in Figure 2 demonstrates\nFigure 2: t-SNE plots, where the left ﬁgure refers to domain-\ninvariant features and domain-speciﬁc features of input texts, and\nthe right ﬁgure corresponds to domain-invariant features of input\ntexts and language-invariant representations from XLM.\nthat the domain-invariant features and the language-invariant\nrepresentations from XLM are partly entangled, which can\nbe explained by the fact that maximizing mutual information\nbetween them can force the domain-invariant extractor to re-\ntain useful features from the language-invariant representa-\ntions that are shared among different domains.\n6 Conclusions and Future Work\nIn this paper, we propose a simple but effective unsuper-\nvised feature decomposition module that extends the pre-\ntrained cross-lingual model to a more useful CLCD sce-\nnario. Through introducing the mutual information maxi-\nmization and minimization objectives in representation learn-\ning, our proposed method can automatically extract domain-\ninvariant and domain-speciﬁc features from the language-\ninvariant cross-lingual space, by using only a small unla-\nbeled dataset from the source language during training. Ex-\nperimental results indicate that, with the enhancement of the\nproposed module, the cross-lingual language model XLM\nachieves continuous improvements, which leads to new state-\nof-the-art results on the Amazon review benchmark dataset in\na CLCD setting. In the future, we will explore the effect of\nour proposed unsupervised feature decomposition model on\nother pretrained models and downstream tasks.\nAcknowledgments\nThe authors gratefully acknowledge the assistance of Qingyu\nTan in this work.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3677\nReferences\n[Artetxe et al., 2018] Mikel Artetxe, Gorka Labaka, and\nEneko Agirre. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embeddings.\nIn ACL, pages 789–798, 2018.\n[Belghazi et al., 2018] Mohamed Ishmael Belghazi, Aristide\nBaratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, De-\nvon Hjelm, and Aaron Courville. Mutual information neu-\nral estimation. In ICML, pages 530–539, 2018.\n[Chen et al., 2018] Xilun Chen, Yu Sun, Ben Athiwaratkun,\nClaire Cardie, and Kilian Weinberger. Adversarial deep\naveraging networks for cross-lingual sentiment classiﬁca-\ntion. TACL, 6:557–570, 2018.\n[Chen et al., 2019] Xilun Chen, Ahmed Hassan Awadallah,\nHany Hassan, Wei Wang, and Claire Cardie. Multi-source\ncross-lingual model transfer: Learning what to share. In\nACL, 2019.\n[Conneau and Lample, 2019] Alexis Conneau and Guil-\nlaume Lample. Cross-lingual language model pretraining.\nIn NeurIPS, 2019.\n[Conneau et al., 2018a] Alexis Conneau, Guillaume Lam-\nple, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv ´e\nJ´egou. Word translation without parallel data. In ICLR,\n2018.\n[Conneau et al., 2018b] Alexis Conneau, Ruty Rinott, Guil-\nlaume Lample, Adina Williams, Samuel Bowman, Holger\nSchwenk, and Veselin Stoyanov. Xnli: Evaluating cross-\nlingual sentence representations. In EMNLP, pages 2475–\n2485, 2018.\n[Conneau et al., 2019] Alexis Conneau, Kartikay Khandel-\nwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\nzek, Francisco Guzm ´an, Edouard Grave, Myle Ott, Luke\nZettlemoyer, and Veselin Stoyanov. Unsupervised cross-\nlingual representation learning at scale. arXiv preprint\narXiv:1911.02116, 2019.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In ACL, pages 4171–4186, 2019.\n[Donsker and Varadhan, 1983] Monroe D Donsker and\nSR Srinivasa Varadhan. Asymptotic evaluation of cer-\ntain markov process expectations for large time. IV.\nCommunications on Pure and Applied Mathematics,\n36(2):183–212, 1983.\n[He and McAuley, 2016] Ruining He and Julian McAuley.\nUps and downs: Modeling the visual evolution of fash-\nion trends with one-class collaborative ﬁltering. In WWW,\npages 507–517, 2016.\n[He et al., 2019] Junxian He, Zhisong Zhang, Taylor Berg-\nKiripatrick, and Graham Neubig. Cross-lingual syntactic\ntransfer through unsupervised adaptation of invertible pro-\njections. arXiv preprint arXiv:1906.02656, 2019.\n[Heinzerling and Strube, 2018] Benjamin Heinzerling and\nMichael Strube. Bpemb: Tokenization-free pre-trained\nsubword embeddings in 275 languages. In LREC, 2018.\n[Hjelm et al., 2019] R Devon Hjelm, Alex Fedorov, Samuel\nLavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representa-\ntions by mutual information estimation and maximization.\nIn ICLR, 2019.\n[Kim et al., 2019] Yunsu Kim, Yingbo Gao, and Hermann\nNey. Effective cross-lingual transfer of neural machine\ntranslation models without shared vocabularies. arXiv\npreprint arXiv:1905.05475, 2019.\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[Lin et al., 2019] Yu-Hsiang Lin, Chian-Yu Chen, Jean Lee,\nZirui Li, Yuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, et al. Choos-\ning transfer languages for cross-lingual learning. arXiv\npreprint arXiv:1905.12688, 2019.\n[Maaten and Hinton, 2008] Laurens van der Maaten and Ge-\noffrey Hinton. Visualizing data using t-sne. JMLR,\n9(Nov):2579–2605, 2008.\n[Pham et al., 2015] Hieu Pham, Thang Luong, and Christo-\npher Manning. Learning distributed representations for\nmultilingual text sequences. In Proceedings of the 1st\nWorkshop on Vector Space Modeling for Natural Lan-\nguage Processing, pages 88–94, 2015.\n[Prettenhofer and Stein, 2010] Peter Prettenhofer and Benno\nStein. Cross-language text classiﬁcation using structural\ncorrespondence learning. In ACL, pages 1118–1127, 2010.\n[Vuli´c et al., 2019] Ivan Vuli´c, Simone Paolo Ponzetto, and\nGoran Glavaˇs. Multilingual and cross-lingual graded lexi-\ncal entailment. In ACL, pages 4963–4974, 2019.\n[Xiao and Guo, 2013] Min Xiao and Yuhong Guo. Semi-\nsupervised representation learning for cross-lingual text\nclassiﬁcation. In EMNLP, pages 1465–1475, 2013.\n[Xu and Yang, 2017] Ruochen Xu and Yiming Yang. Cross-\nlingual distillation for text classiﬁcation. In ACL, pages\n1415–1425, 2017.\n[Yarowskyet al., 2001] David Yarowsky, Grace Ngai, and\nRichard Wicentowski. Inducing multilingual text analy-\nsis tools via robust projection across aligned corpora. In\nHLT, pages 1–8, 2001.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3678"
}