{
    "title": "Facilitating Large Language Model Russian Adaptation with Learned Embedding Propagation",
    "url": "https://openalex.org/W4406228222",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2420327904",
            "name": "Mikhail Tikhomirov",
            "affiliations": [
                "Lomonosov Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A5114083695",
            "name": "Daniil Chernyshov",
            "affiliations": [
                "Lomonosov Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A2420327904",
            "name": "Mikhail Tikhomirov",
            "affiliations": [
                "Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A5114083695",
            "name": "Daniil Chernyshov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4378465252",
        "https://openalex.org/W4384264777",
        "https://openalex.org/W4401043490",
        "https://openalex.org/W4385750324",
        "https://openalex.org/W4402670787",
        "https://openalex.org/W4401042701",
        "https://openalex.org/W4390961444",
        "https://openalex.org/W4391272853",
        "https://openalex.org/W2963500732",
        "https://openalex.org/W2946676565",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W4221143270",
        "https://openalex.org/W3112784227",
        "https://openalex.org/W4396234795",
        "https://openalex.org/W4403535679",
        "https://openalex.org/W4366327277",
        "https://openalex.org/W4402667110",
        "https://openalex.org/W4399026160",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W4383180661",
        "https://openalex.org/W4404781847",
        "https://openalex.org/W4377297670",
        "https://openalex.org/W4401306886",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4310998073",
        "https://openalex.org/W3035916712",
        "https://openalex.org/W4394708144",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4387561528",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4390784371",
        "https://openalex.org/W4385573195",
        "https://openalex.org/W3177240766"
    ],
    "abstract": "Background: Recent advancements in large language model (LLM) technologies have introduced powerful open-source instruction-tuned LLMs that match the text generation quality of leading models like GPT-4. Despite accelerating LLM adoption in sensitive-information environments, the lack of disclosed training data hinders replication and makes these achievements exclusive to specific models. Purpose: Given the multilingual nature of the latest iteration of open-source LLMs, the benefits of training language-specific LLMs diminish, leaving computational efficiency as the sole guaranteed advantage of this computationally-expensive procedure. This work aims to address the language-adaptation limitations posed by restricted access to high-quality instruction-tuning data, offering a more cost-effective pipeline. Method: To tackle language-adaptation challenges, we introduce Learned Embedding Propagation (LEP), a novel method with lower training data requirements and minimal disruption of existing LLM knowledge. LEP employs an innovative embedding propagation technique, bypassing the need for instruction-tuning and directly integrating new language knowledge into any instruct-tuned LLM variant. Additionally, we developed Darumeru, a new benchmark for evaluating text generation robustness during training, specifically tailored for Russian adaptation. Results: We applied the LEP method to adapt LLaMa-3-8B and Mistral-7B for Russian, testing four different vocabulary adaptation scenarios. Evaluation demonstrates that LEP achieves competitive performance levels, comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. Further improvements were observed through self-calibration and additional instruction-tuning steps, enhancing task-solving capabilities beyond the original models. Conclusion: LEP offers a viable and efficient alternative to traditional language-specific instruction-tuning, significantly reducing the costs associated with language adaptation while maintaining or surpassing the performance benchmarks set by contemporary LLMs.",
    "full_text": "130\nJLE  |  Vol. 10  |  No. 4  |  2024\nJOURNAL OF LANGUAGE & EDUCATION\n| Research Papers\nFacilitating Large Language Model \nRussian Adaptation with Learned \nEmbedding Propagation\nMikhail Tikhomirov , Daniil Chernyshev \nLomonosov Moscow State University, Moscow, Russia\nABSTRACT\nBackground: Recent advancements in large language model (LLM) technologies have introduced \npowerful open-source instruction-tuned LLMs that match the text generation quality of leading \nmodels like GPT-4. Despite accelerating LLM adoption in sensitive-information environments, \nthe lack of disclosed training data hinders replication and makes these achievements exclusive \nto specific models.\nPurpose: Given the multilingual nature of the latest iteration of open-source LLMs, the benefits \nof training language-specific LLMs diminish, leaving computational efficiency as the sole \nguaranteed advantage of this computationally-expensive procedure. This work aims to address \nthe language-adaptation limitations posed by restricted access to high-quality instruction-\ntuning data, offering a more cost-effective pipeline.\nMethod: To tackle language-adaptation challenges, we introduce Learned Embedding \nPropagation (LEP), a novel method with lower training data requirements and minimal \ndisruption of existing LLM knowledge. LEP employs an innovative embedding propagation \ntechnique, bypassing the need for instruction-tuning and directly integrating new language \nknowledge into any instruct-tuned LLM variant. Additionally, we developed Darumeru, a new \nbenchmark for evaluating text generation robustness during training, specifically tailored for \nRussian adaptation.\nResults: We applied the LEP method to adapt LLaMa-3-8B and Mistral-7B for Russian, testing \nfour different vocabulary adaptation scenarios. Evaluation demonstrates that LEP achieves \ncompetitive performance levels, comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. Further \nimprovements were observed through self-calibration and additional instruction-tuning steps, \nenhancing task-solving capabilities beyond the original models.\nConclusion: LEP offers a viable and efficient alternative to traditional language-specific \ninstruction-tuning, significantly reducing the costs associated with language adaptation while \nmaintaining or surpassing the performance benchmarks set by contemporary LLMs.\nKEYWORDS\nlarge language model, llama, language adaptation, natural language generation\nINTRODUCTION\nEmergence of universal instruct-tuned \nlarge language models (LLM) such as \nChatGPT (Ouyang, 2022) has substan-\ntially accelerated the development of \nnatural language processing technolo-\ngies. However, despite the remarkable \nachievements in zero-shot task solving, \nthe close-source nature of such models \nprevented their adoption in the areas \nwith sensitive or exclusive information \nwhere any risk of data-leak jeopardiz-\nes the integrity of the business process. \nAs a result the rising demand for open-\nsource alternatives drove the research-\ners to derive methods for knowledge \ndistillation of state-of-the-art LLMs. One \nof the first approaches was Alpaca (Taori, \n2023) which used ChatGPT to synthesize \nthe instruct-tuning data for open-source \nfoundation LLM LLaMA (Touvron, 2023a). \nWhile Alpaca was far from state-of-the-\nart this inspired the creation of more ad-\nvanced schemes like BactrianX (Li, 2023) \nthat augmented the synthesis process \nwith cross-lingual machine translation \nwhich in turn enabled training of open-\nhttps://doi.org/10.17323/jle.2024.22224\nCitation: Tikhomirov, M., Chernyshev, D. \n(2024). Facilitating Large Language \nModel Russian adaptation with learned \nembedding propagation. Journal of \nLanguage and Education, 10(4), 130-145.  \nhttps://doi.org/10.17323/jle.2024.22224\nCorrespondence: \nMikhail Tikhomirov,  \ntikhomirov.mm@gmail.com\nReceived: August 15, 2024\nAccepted: December 16, 2024\nPublished: December 30, 2024\nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n131\n| Research Papers\nsource multilingual chatbots. However, with release of GPT-\n4 (Achiam, 2023) which excelled in multilingual setting it be-\ncame possible to integrate the explicit translation step into \ninstruction synthesis pipeline thus increasing accessibility of \nknowledge distillation. This has led to creation of series lan-\nguage-specialized instruction-tunes of open-source LLMs \nsuch as Saiga (Gusev, 2023), PolyLM (Wei, 2023), Vikhr (Niko-\nlich, 2024), LLAMMAS (Kuulmets, 2024). \nWith increasing instruction synthesis quality the open-source \nlanguage-specific LLMs were closing the gap with the state-\nof-the-art closed-source solutions eventually hitting the \nperformance ceiling of conventional instruction-tuning (Cui, \n2023) due to low utilization of inherent English contextual \nknowledge which is dominant in state-of-the-art pre-trained \nopen-source LLMs (Touvron, 2023b; Jiang, 2023; Dubey, \n2024). As a possible solution researchers (Zhu, 2023; Li, \n2024; Chai, 2024) proposed enriching the instruction-tuning \ndatasets with translation tasks which are designed to align \nnew language knowledge with the existing English semantic \nrepresentations. However, it was shown by Ranaldi (2023) \nand Husain (2024) that the cause of alignment issue is likely \nto lie with the inefficiency of tokenization algorithm which \ncan be addressed either by building a new language-specific \ntoken vocabulary or by recycling the English tokens for Ro-\nmanized language representation. \nInspired by works of Lakew (2018), Kuratov (2019), Rust \n(2021) & Yang (2022) on vocabulary adaptation for encod-\ner models Cui et al. (2023) proposed language-specific \ncontinued pre-training pipeline for full LLM language ad-\naptation which paired with instruct-tuning on synthesized \nexamples allowed to create Chinese LLaMa, the first open-\nsource model to reach the performance level of ChatGPT \nwith substantially improved computation efficiency thanks \nto Chinese-adapted tokenization vocabulary. This approach \nwas studied in detail by Tikhomirov (2023) for LLaMa-2 (Tou-\nvron, 2023b) adaptation to Russian language and it was \nshown that semantic alignment efficiency can be further \nimproved with morphologically accurate tokenization algo-\nrithm. Moreover, the full LLM language adaptation pipeline \nwas shown by Nguyen (2023) to outperform state-of-the-art \nclosed-source counterparts on low-resource languages due \nto their bias towards popular languages. \nWhile the current iteration of language adaptation algo-\nrithm is relatively cost-efficient, the benefit of developing \nlanguage adapted LLMs is falling amid the rapid develop-\nment of LLM technology and multilingual specialization of \nopen-source options. At the same time it becomes common \nto release instruction-tuned models (Jiang, 2023; Dubey \n2024) that perform on par with closed-source state-of-the-\nart counterparts without disclosing the instruction-tuning \ndata the quality of which is the major factor of resulting \nLLM task-solving capabilities (Zhou 2024). Collecting data of \nsuch quality requires a considerable investment in human \nannotation to an extent that only large organizations can \nafford creation of such datasets (Dubey 2024). If a language \nspecific counterpart of a high quality instruction dataset is \nunavailable the result of full language adaptation will only \nhave the benefit of higher computational performance as an \ninferior instruction-tuning data will lead to inferior task-solv-\ning performance. \nTo cut the language adaptation costs and enable direct lan-\nguage adaptation of instruction-tuned LLM we propose an \nupdated pipeline for language adaptation, Learned Embed-\nding Propagation. Unlike the original full LLM language ad-\nFigure 1\nPerformance Comparison of Proposed Adaptation Method on Darumeru Benchmark\n\nMikhail Tikhomirov , Daniil Chernyshev\n132\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\naptation pipeline (Cui, 2023), our method requires less data \nand computational resources due to limited pre-training im-\npact on model parameters which is compensated by novel \nad-hoc embedding propagation procedure that allows to \nskip the instruction-tuning step and instead implant the new \nlanguage knowledge directly into any existing instruct-tuned \nvariant. To further facilitate the Russian adaptation we de-\nveloped a new lightweight benchmark for train-time evalua-\ntion of LLM text generation robustness, Darumeru. We test \nLearned Embedding Propagation pipeline on Mistral-7B and \nLLaMa-3-8B LLMs for 4 Russian tokenization variants. The \nevaluation results (Figure 1) demonstrate that despite lower \nparametrization our language-adaptation method manages \nnot only to regain the original quality of the instruction tune \nbut in some cases even outperform it by a significant mar-\ngin.  Additional case-study experiments on improving the \nbest language-adapted models with continued instruct-tun-\ning and self-calibration also confirm the superiority of our \nlanguage-adapted models, pushing their performance be-\nyond existing counterparts. \nMETHOD\nModel Language Adaptation\nFollowing the previous work on LLM lingual adaptation (Cui, \n2023; Tikhomirov, 2023) we first optimize model vocabulary \nfor better alignment with Russian language morphology \nand then continue the pre-training process on a large cor-\npora of Russian texts of various genres and topics.\nFormally the model adaptation consists of 3 steps:\n1. Tokenization training;\n2. Model embedding initialization;\n3. Continued pre-training of new embeddings (both input \nand output).\nTokenization training\nSince there are no best practices for vocabulary optimiza-\ntion we consider 4 options for tokenization training:\nBPE - fully substituting the tokenization vocabulary by re-\nbuilding the BPE tokenization algorithm (Vries, 2021), which \nis used in the majority of state-of-the-art LLMs.\nUnigram - fully substituting the tokenization vocabulary \nwith morphologically accurate tokenization obtained with \nUnigram algorithm (Tikhomirov, 2023).\nExtension - extending the original BPE vocabulary by first \nbuilding a new BPE vocabulary for Russian corpora and then \nmerging it with the original (Cui, 2023).\nOptimization - refactoring the existing BPE vocabulary by re-\nducing it to the most common 50% tokens of Russian corpo-\nra and then subsequent Extension to the original size. (con-\nsidered only for LLMs with extensive English vocabulary).\nEmbedding Initialization\nPrevious work on LLM language adaptation (Cui, 2023; Tik-\nhomirov, 2023; Nguyen, 2023) found simple averaging of \nembeddings of overlapping subtokens to be a sufficient \nsolution for embedding initialization. Formally, given em-\nbedding vectors of old  and new  tokenization vocabularies \nthe new embeddings are initialized as the following:\n (1)\n (2)\nwhere  is the original tokenization function, is token in new \nvocabulary,  is a token in original vocabulary. \nWhile there are more advanced initialization techniques, re-\ncent studies on design choices for LLM language adaptation \n(Tejaswi, 2024) concluded that embedding averaging has \nthe best expected adaptation quality and the performance \ngap with task-tailored methods is within standard deviation \nof task evaluation protocol. Therefore for all experiments we \nuse the described subtoken averaging embedding initializa-\ntion strategy.\nContinued Pre-Training\nThe main issue with embedding initialization is that despite \nintroduction of new tokens the LLM retains the habit to use \nthe tokens that were present in the original tokenization. As \na result the model computational performance of text gen-\neration remains the same as the model tends to use more \ntokens per word than it is expected while also misinterpret-\ning the new tokens due to homonymy of token context. \nTo alleviate the issue the common tactic is to train the new-\nly initialized embeddings on adaptation language corpora \nusing the same pre-training task as LLM, which is causal lan-\nguage modeling. In this task the input text is broken into \nsequences of tokens of increasing size all of which start from \nthe beginning and the model is asked to predict for each \nsequence the next possible token. The model optimization is \ndone using simple cross-entropy loss thus any text corpora \ncan be used for the pre-training task. \nContinued pre-training of embeddings only allows the mod-\nel to tailor those embeddings for inner semantics thus redis-\ntributing the existing language knowledge among the newly \nintroduced tokens. However, some researchers (Cui, 2023; \nTikhomirov 2024) argued that pre-training embeddings only \nmay be insufficient for proper model-vocabulary alignment \nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n133\n| Research Papers\nand intermediate model layers must be also trained. On the \nother hand, increasing the number of trained model param-\neters reduces the training process stability which in turn \nsubstantially raises the data size requirements and compu-\ntational costs of training procedure. As the middle ground \nwe complement embedding pre-training with a post-train-\ning layer alignment procedure that recycles existing fine-\ntunes of the adapted model.\nLearned Embedding Propagation\nThe issue of cost-efficient knowledge transfer for language \nadapted models has been studied before in the context of \nencoder models. To solve the absence of task-tuning data-\nset in the target language Artetxe et al. (2019) proposed a \nsimple algorithm for transferring task-solving knowledge to \nBERT models: \n1. Pre-train the full language model from scratch on avail-\nable large monolingual text corpora (e.g English) using \nlanguage modeling training objective (for BERT it is \nmasked language modeling);\n2. Create a copy of the pre-trained model and replace the \nembeddings of the original with new embeddings for \nthe target language;\n3. Continue the pre-training of the modified original on \ntarget language monolingual corpora for model em-\nbeddings while freezing (not updating) all other layers \nusing the same training objective;\n4. Fine-tune the copy on the downstream task dataset \nwhile keeping the embeddings frozen;\n5. Swap the embeddings of the fine-tuned copy with em-\nbeddings of the original model obtained after continued \npre-training on the target language corpora.\nThe major advantage of the described algorithm is that the \ncontinued pre-training step requires much less data than \ninitial pre-training from scratch as it requires training only \na fraction of model parameters which reduces model opti-\nmization task complexity and thus has faster convergence \n(Kaplan, 2020). The main hypothesis is that task-solving \nknowledge is language agnostic and it was confirmed in the \noriginal experiments (Artetxe, 2019) for natural language \nunderstanding and document classification tasks. Howev-\ner, the authors noted that fine-tuning on downstream tasks \nwith frozen embeddings is not enough for proper embed-\nding swap alignment and additional embedding transforma-\ntions or special embedding utilization penalties are required \nto maximize the efficiency of target language vocabulary \nprocessing. As a possible solution to the embedding align-\nment problem Chen et al. (2023) proposed using a special \npre-training regime with active embedding forgetting to \nforce the language model to accumulate the knowledge \nin intermediate layers. The downside of such an approach \nis that we must have full control on the initial pre-training \nwhich is not possible for state-of-the-art LLMs obtained by \ntraining on high quality proprietary datasets with immense \ncomputational budget. \nWe argue that embedding swap alignment can be achieved \nwithout special training procedures by leveraging the \nfine-tuning parameter update trajectory. Ilharco et al. (2023) \nshowed that the fine-tuning trajectory may be approximat -\ned with linear transformations of base model parameters \nwhich can be derived from parameter decomposition of \nfine-tuned variants. Therefore, by finding appropriate linear \ntransformations for embedding parameters we can approx-\nimate the results of a full language adaptation pipeline with-\nout involving the instruction-tuning dataset.\nFormally, let I, O be the input and output embeddings of \nLLM and W a pseudo-linear approximation of composition \nof intermediate LLM layers: \n (3)\nDenote D, U as linear embedding transformations that align \noriginal embeddings with the fine-tuned layers:\n(4)\nSince our target language embedding initialization strategy \naverages the embeddings of overlapping tokens in I base and  \nObase we can formalize the initialization process with vocabu-\nlary transformation operation :\n(5)\nFollowing the logic described above the fine-tune of lan-\nguage adapted base model LLMru→inst. can be represented as \nthe following:\n (6)\nNow by assuming that the optimal \n  we ar-\nrive at the final equation for propagation of continued pre-\ntrained embeddings \n :\n \n (7)\nThe remaining variables \n  are determined by cho-\nsen assumptions about embedding alignment properties. In \nour experiments we consider 3 options:\n1. Direct embedding swap\n2. Overlapping token correction\n3. Vocabulary conversion projection\nMikhail Tikhomirov , Daniil Chernyshev\n134\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\nDirect embedding swap\nConsidering that most state-of-the-art LLMs are trained on \nmultilingual datasets, it can be expected that their inner \nrepresentations are tailored for language-agnostic text pro-\ncessing. Similarly to the original works on embedding-based \nknowledge transfer for encoder models we assume that the \nembedding layer carry only conceptual information i.e. we \nsuppose \n  is an identity matrix.\nOverlapping Token Correction\nSince the considered LLMs are initially designed for multilin-\ngual text generation they have a basic set of the most com-\nmon tokens for popular languages such as russian. The idea \nis to find the union C = tokensold∩tokensnew of the original \ntokensold and language-adapted tokens new vocabularies and \nuse this subset to reduce IX , OX to the common components \nof embedding initialization IX/com , OX/com where \nThis allows to approximate the embedding projections as \n:\n , (9)\n , (10)\n. (11)\nwhere idx(t) is a function that maps token t to its respective \nposition in the embedding matrix. It must be noted that IX/com \n, OX/com matrices are likely to be not invertible and thus their \ninversion must be approximated with least squares problem \nsolvers.\nVocabulary Conversion Projection\nSince embedding initialization transformation T ru is univer-\nsal for both base and fine-tuned models we can derive an al-\nternative equation for obtaining language-adapted instruc-\ntion-tuned LLM:\n (12)\nBy assuming that both variants of instruction-tune adapta-\ntion are equivalent \n  we obtain the fol-\nlowing formulae for embedding alignment:\n  (13)\n (14) \nSimilarly to the previous alignment method the calculation \nof transformation matrices involves least square problem \nsolvers for finding the pseudo-inversion of non-invertible \n1 https://github.com/NLP-Core-Team/mmlu_ru\nmatrices. This is the main reason why vocabulary transfor-\nmation Tru should not be isolated. The pilot experiments \nshowed that such simplification increases the error margin \nof alignment transformations which lowers the quality of \nembedding propagation procedure.\nDarumeru Benchmark\nExisting LLM benchmarks for Russian language (Fenogeno-\nva, 2024) do not expose the testing data labels for local eval-\nuation. On one hand such an initiative is reasonable amid \nthe rising trend of training on test data which renders the \nLLM ranking results meaningless. On the other hand hid-\nden test labels means that the evaluation requires having \nan online connection to the benchmark system which pre-\nvents evaluation in offline computational environments thus \npostponing the evaluation until the end of training session. \nMoreover lack of access to test labels makes it impossible \nto classify the type of prediction errors thus limiting the \npost-training quality analysis.\nTo address the issue we developed a new benchmark frame-\nwork that focuses on quick and informative LLM text gener-\nation quality evaluation. This benchmark consists of combi-\nnations of open splits of datasets from MERA (Fenogenova, \n2024), mmlu_ru / mmlu_en, RuCoLA (Mikhailov, 2022), as \nwell as new datasets for text generation assessment - 17 da-\ntasets total. A more detailed description of each dataset is \ngiven in the following sections. \nFramework\nThe evaluation framework utilizes message format to ensure \ncompatibility with both pre-trained and instruction-tuned \nLLMs. This means that all task data for the models is convert-\ned into a sequence of “user role”-”message content” pairs, \nfrom which the final prompt is constructed.  The framework \nsupports tasks that require estimating the probability of the \nnext token, generation, or logsoftmax for the entire gener-\nated sequence. The evaluation can be carried out directly in \na conventional Transformers model training environment or \nvia VLLM specialized model inference servers.\nDaruMERA and DaruMMLU\nWe composed DaruMERA  from the following MERA data-\nsets: MultiQ, PARus, RCB, RWSD, USE, , ruOpenBookQA, ru-\nWorldTree. For better language understanding evaluation \nwe also added validation split of RuCoLA dataset.\nFor DaruMMLU part we separated ruMMLU (MERA) and \ncomplemented it with MMLU datasets from the NLP-Core-\nTeam repository1.\nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n135\n| Research Papers\nThere are several changes to the original datasets:\n1. MultiQ version was augmented with additional gold an-\nswers. The existing labels do not correspond in form to \nthe questions, as they were extracted from the text with-\nout proper preprocessing. The augmentation process \nconsisted of passing the question and reference answer \npairs to LLaMa-3-70B-Instruct model to rephrase the an-\nswer in accordance with the question. \n2. The ruMMLU version differs from the similar one in NLP-\nCore-Team repository in that it has few-shot examples \ncommon to all queries, regardless of the domain, and \nalso uses not one fixed template, but several options as \ninstructions. \n3. When calculating PARus, for each example the same \nexample was generated, but with a different order of \noptions, and only the case when the model predicts the \ncorrect option for both the direct and reverse order was \nconsidered a success.\nTo measure the performance on PARus, RWSD, MMLU da-\ntasets we used accuracy metric. For RCB, ruOpenBookQA \nand ruWorldTree we averaged accuracy and F1-macro. For \nRuCoLa we used average of accuracy and Matthews Corre-\nlation Coefficient (MCC). For MultiQ we used the average of \nF1 and exact match metrics. For USE the normalized total \ngrade was used. \nDaruSum\nMost of the evaluation tasks aim to measure the model’s \ntext comprehension capabilities and global contextual \nknowledge which is required for proper prompt processing. \nHowever for text generation the model must be also capa-\nble of filtering the input text for the query relevant content \nto ensure that the user would receive the desired answer \nregardless of input format or size. Text summarization is the \nperfect evaluation task for such a case as it requires both \nfiltering the input content and composing the answer from \nthe salient fragments. \nThere are two summarization settings: extractive and ab-\nstractive. Extractive summarization is a task of sentence \nsaliency ranking where the summary is obtained by taking \ntop-k ranked sentences. Abstractive summarization on the \nother hand is a text generation task where saliency ranking \nis integrated in the token sampling process as the model \nguides itself toward the most concise summary. While the \nabstractive setting has the higher preference it is hard to \ndistinguish automatically the suboptimal content filtering \nfrom the text generation errors. At the same time constrain-\ning the text generation process to input fragments such as \nsentences basically reduces the task to extractive summari-\nzation. Thus to evaluate content filtering accuracy and text \n2 https://github.com/tatsu-lab/alpaca_eval\ngeneration quality it is sufficient to evaluate the abstractive \nsummarization in free and constrained generation settings.\nFor the summarization dataset we chose Gazeta (Gusev, \n2020) which has established itself as the standard for Rus-\nsian automatic summarization evaluation. To improve the \naccuracy of evaluation procedure we derived an example fil-\ntering protocol that all reference summary content can be in-\nferred from the input document. Since LLaMa-3-70B showed \nhigh human agreement in LLM evaluation 2 we employed it \nas the example correctness evaluator and tasked it to find \nall citations that support the summary sentence. We filtered \nout all examples that had more than 20% of unsupported \nsummary sentences and mapped found citations to docu-\nment sentences, thus producing accurate extractive labels. \nTo adapt the task for a few-shot setting which is limited by \ncontext window limitations we compressed the documents \nby dropping the paragraphs that had no extractive summa-\nry labels. To account for LLM text generation length variance \n(Dubois, 2024) as the metric for abstractive and extractive \nsettings we chose average of ROUGE-1 and ROUGE-2 recall \nand R-precision respectively.\nDaruCopy\nWhen replacing the LLM vocabulary it is important that it \nlearns to fully utilize new tokens.  The input token embed-\ndings are responsible for conveying the text meaning which \ncan be evaluated by natural language understanding tasks \nsuch as MMLU. In contrast, the output token embeddings \nare used to find the closest semantic meaning to the current \nneural network state which depends on contextual history. \nAs a consequence, in creative tasks this state is unstable and \nLLM tends to generate rarer tokens. At the same time, in the \ntasks where the LLM is required to reuse the input context \nthe network state is expected to fall into semantic clusters \nof tokens that are present in the input sequence. Following \nthat logic by prompting the LLM to produce a copy of the \ninput text we can evaluate its token generation efficiency. \nWe used Wikipedia articles of different genres to collect copy \ntask datasets for English and Russian languages involving 2 \ncopy settings: sentence-wise and paragraph-wise. The for-\nmer setting assesses the LLM alignment with tokenization \nalgorithm which is calculated as the ratio of the length of \nthe original text to the generated text in tokens. In para-\ngraph setting we evaluate the overall text generation sta-\nbility by measuring the percentage of generations in which \nthe ratio of longest common subsequence (lcs) tokens to all \nparagraph tokens is greater than 99% (1% is left for spacing \nerrors). Deviation from 99% amid the high sentence copy \nscores indicates that the model tends to confuse tokens and \nthus can hallucinate context in creative tasks which is the \nmajor reliability concern for practical applications.\nMikhail Tikhomirov , Daniil Chernyshev\n136\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\nBenchmark Parameters\nWhen calculating the benchmark metrics, the following \nparameters were set: batch size 8, sequence length 4096, \n5-shot for foundation models and zero-shot for instruct \nmodels.\nExperiment Setting\nWe conducted adaptation experiments with two models: \nMistral-7B-v0.1 (Jiang, 2023) and LLaMa-3-8B (Dubey, 2024).\nContinued Pre-Training\nTraining dataset for tokenization and continued pre-training \nconsists of documents from the following domains: Russian \nWikipedia, English Wikipedia, Habrahabr, Pikabu, Fiction, \nNews, Educational literature.\nThe documents were deduplicated using Locality Sensitive \nHashing Minhash algorithm. We removed metadata, links, \ncomment sections and badly formatted documents to im-\nprove vocabulary distribution and reduce the number of \ngrammatically incorrect examples. To reduce the semantic \nnoise we restricted the vocabulary to Cyrillic and Latin lan-\nguages and stripped non-standard symbols like emoji or \nlogograms (e.g. Chinese characters) using UTF-8 normaliza-\ntion.\nFor training, texts were sampled with increased weights for \nWikipedia, educational and scientific literature. Additionally, \nto feed texts into the language model, we ensured that each \nsample began either with a new document or with a new \nparagraph.\nTokenization parameters. We trained BPE and Unigram \ntokenizers with 32000 and 128000 tokens for Mistral-7B and \nLLaMa-3-8B respectively. For Extended tokenizer, we extend-\ned the original tokenizers to 55328 and 174816 tokens using \nnew Russian-adapted BPE vocabularies for corresponding \nmodels. Since LLaMa-3-8B tokenization vocabulary is likely \nto be extensive we created an Optimized version, where we \nshrunk the original BPE vocabulary to 64000 tokens and then \nmerged with top 64000 most common tokens from new BPE \nvocabulary, resulting in 114504 tokens.\nHyperparameters. During continued pre-training we used \nthe following hyperparameters: Total Batch Size: 256; Block \nSize: 1024; Weight Decay: 0.1; Scheduler: Cosine; Warmup \nSteps: 100; Epochs: 1.\nWe tested 4 different learning rates: 2e-5, 5e-5, 1e-4, 2e-4 \nfor each model and tokenization on 20% of all continued \npre-training dataset. Based on benchmark results, we chose \na learning rate equal to 1e-4 for all Mistral-7B models, and \n3 https://huggingface.co/datasets/IlyaGusev/saiga_scored\nlearning rate equal to 2e-4 for LLaMa-3-8B models. It is \nimportant to note that the efficiency of model adaptation \nshowed a significant dependence on the learning rate, es-\npecially for LLaMa-3-8B based models. \nCase Study: Self-Calibration\nFor the cases of full vocabulary substitution where the mod-\nel learns to rewire all new embeddings virtually from scratch \nthe propagation process may have lower efficiency as the \ndifference between instruct-tuned and language-adapted \nembeddings may be dramatic. The logical solution is to syn-\nthesize self-instruct data using the original instruct-tuned \nLLM and then use it to calibrate the language-adapted ver-\nsion. To generate the examples, we used prompts from Sai-\nga instruction dataset and used greedy decoding to get the \nmost likely answer from instruct-tuned LLM viewpoint. Then \nwe asked LLaMa-3-70B to evaluate the quality of synthesized \npairs in terms of grammar and relevance on a 5-point grad-\ning scale. All examples that received a score less than 4 were \ndiscarded which left us 13531 calibration examples. \nSince calibration examples are native for LLM inner semantic \nrepresentations there is a risk that instead of alignment the \nmodel may revert back to the original tokenization behav-\nior which prioritizes smaller but more familiar tokenization \nchunks. To avert such a scenario we leverage the fact that all \nmodern LLMs are pre-trained on Wikipedia articles in such \na manner that their embedding representations are aligned \nwith Wikipedia concepts. By asking the fine-tuned model to \nrepeat a Wikipedia article token by token we force the model \nto recall its pre-training memory and thus to propagate the \nactivation signals respective to the concepts in the article \nto embeddings of optimal tokens of new tokenization. Fol-\nlowing that logic we supplemented the self-instruct dataset \nwith 10000 article-copy task examples, obtained from the \npart of Wikipedia that has no overlap with our pre-training \nor benchmark datasets.\nWe found the following LoRA-tuning settings to be optimal \nfor calibration procedure: Rank: 8; Alpha: 1; Learning Rate: \n2.5e-5; Weight Decay: 0.1; LoRa target modules: first and \nlast transformer layers; LoRa modules to save: lm_head, em-\nbed_tokens; Max Sequence Length: 8096 (i.e. max context \nlength); Total Batch Size: 64; Epochs: 1.\nCase Study: Continued Instruction-Tuning Calibration\nIn addition to the self-calibration experiments, we decided \nto test how continued instruction-tuning on the high-quali-\nty Russian instruction dataset would affect the final perfor -\nmance. For this experiment we choose Saiga3 dataset which \nis considered to be the best open-source option for Russian \nlanguage. We also investigated the impact of adding a small \nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n137\n| Research Papers\nnumber (2000) of special instructions to the dataset, the \npurpose of which is to copy a large text from Wikipedia\nTo fine-tune the models we used LoRA adapters with Sai-\nga-recommended hyperparameter settings which is the \nfollowing: Rank: 32; Alpha: 16; Learning Rate: 5e-5; Weight \nDecay: 0.05; LoRa target modules: attention, mlp; LoRa \nmodules to save: lm_head; Max Sequence Length: 4096; To-\ntal Batch Size: 128; Epochs: 1.\nRESULTS\nOpen-source LLM Benchmark\nTo establish a baseline we benchmarked popular in-\nstruct-tuned LLMs (see Table 1): Openchat 3.5, LLaMa-3 (in -\nstruct) (Dubey, 2024), Saiga (Gusev, 2023), Vikhr (Nikolich, \n2024), Qwen-2, Mistral Nemo (Jiang, 2023). As expected the \nlargest model, Mistral Nemo, has the highest zero-shot per-\nformance. Smaller counterparts have the same score mar-\ngin. However, Qwen-2 7B manages to outperform Mistral \nNemo in MMLU tasks while falling behind on text genera-\ntion robustness tests of DaruSum and DaruCopy. Vikhr-5.2 \nsimilarly has the same score on DaruMERA as Mistral Nemo. \nConsidering the LLM scaling laws (Kaplan, 2020) and the \nperformance gap with state-of-the-art sub-10B parameter \nLLM, LLaMa-3, this observations suggest that some parts of \nMMLU and MERA datasets were leaked to training data of \nVikhr-5.2 and Qwen-2 7B.\nVocabulary Adaptation and Continued Pre-\nTraining\nFollowing our initial benchmark results we focused on \nRussian adaptation of the foundation models of the most \nperformant instruct-tunes: Mistral-7B and LLaMa-3-8B. To \nevaluate the language-adaption results we used few-shot \nin-context-learning as the models are not used to interpret-\ning the instructions directly.\nFigure 2 shows the Darumeru score dynamic throughout \nthe continued pre-training process. In case of Mistral-7B the \nvocabulary substitution methods such as BPE and Unigram \nalmost exhaust the training examples converging to the op-\ntimum at the final 10k training steps. In contrast LLaMa-3-8B \nis more robust to vocabulary adaptation methods as they \nall tend to converge in the middle of a training session at \n20-30k steps. Since the full dataset size is 96 GB we can con-\nclude that 40 GB of texts is the minimum required for the \ngood performance of Russian adapted embeddings.\nIn Table 2 we report the detailed results of the best perform-\ning checkpoints. As expected, vocabulary extension methods \nsuch as Extended and Optimized have the lowest optimiza-\ntion difficulty as they show the highest language-adaptation \nscores. For Mistral-7B all language adaptations significantly \noutperform the original foundation, however the difference \nbetween their tokenization efficiency (symbols per token) \nand average task-performance may be considered margin-\nal. For LLaMa-3-8B only Extended variants managed to reach \nTable 1\nDarumeru Zero-Shot Evaluation Results for Popular Open-Source Instruct-Tuned Models\nModel Micro-Avg DaruMMLU DaruMERA DaruSum DaruCopy \n(EN)\nDaruCopy \n(RU)\nOpenchat 3.5 \n(Mistral-7B) 0,607 0,543 0,526 0,322 0,999 0,917\nLLaMa-3-8B (Instruct) 0,610 0,571 0,510 0,322 1,000 0,972\nSaiga (LLaMa-3-8B) 0,608 0,574 0,514 0,320 0,995 0,939\nVikhr-5.2 (Mistral-7B) 0,587 0,494 0,573 0,308 0,959 0,693\nQwen-2 7B 0,613 0,624 0,548 0,300 0,938 0,842\nMistral Nemo (12B) 0,639 0,592 0,576 0,320 0,998 0,924\nOurs\nOpenchat 3.5 + \nLEP-Extended + \ncalibration (best)\n0,632 0,541 0,563 0,321 1,000 0,989\nLLaMa-3-8B (Instruct) \n+ LEP-Extended + \ncalibration (best)\n0,618 0,565 0,521 0,339 1,000 0,984\nMikhail Tikhomirov , Daniil Chernyshev\n138\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\nthe original LLM benchmark scores mainly falling behind on \nDaruMMLU tasks. Most tokenization-efficient variants, BPE \nand Unigram, considerably lag behind, losing in DaruMERA \nand DaruSum. We assume that vocabulary substitution in \ncase of BPE and Unigram has a major impact on language \nunderstanding and that in their case continued pre-training \nof embeddings-only is not sufficient for proper semantic \nalignment and additional tuning procedures are required.\nLearned Embedding Propagation\nThe results of complete Learned Embedding Propagation \n(LEP) are reported in Table 3. For each adapted vocabulary \nconstruction option (BPE, Unigram, Extended and Opti-\nmized) we test 3 methods: Direct Embedding Swap ( Swap), \nOverlapping Token Correction ( Overlap) and Vocabulary \nConversion (Conversion). For embedding donor model we \nused best continued pre-training checkpoints (see Table 2).\nFor Mistral-7B and OpenChat 3.5 the embedding propa-\ngation results have large variance depending on the cho-\nsen tokenization algorithm for Russian vocabulary. In case \nof BPE, which is the same algorithm used for the original, \nthe trained embedding for new vocabulary has the highest \nalignment with instruct-tuned counterpart in case of direct \nembedding swap. In case of more morphologically correct \nRussian tokenization, Unigram, overlap projection has the \nhighest average task performance. However, if we look at \ngroup-wise scores it becomes evident that conversion is a \nbetter option as it leads in every task but DaruCopy (Ru) \nwhere all unigram conversion variants are experiencing is-\nsues. The conventional vocabulary extension also leans to-\nwards conversion projection and has the best overall task \nperformance among all vocabularies even outperforming \nthe original OpenChat 3.5.\nFor LLaMa-3-8B embedding conversion is more straightfor-\nward. For all tokenization variants the conversion projec-\ntion yields the best results, however, unlike the Mistral-7B \nLEP none of embedding propagations manage to reach the \noriginal LLaMa-3-8B (instruct) quality. The significant perfor-\nmance degradation is observed among all task groups with \nDaruCopy taking the biggest hit. Moreover, despite being \nthe original tokenization algorithm, BPE-build Russian vo-\ncabulary has the lowest embedding compatibility with in-\nstruction-tune having the largest score gap. While the vo-\ncabulary Optimized variant has lower vocabulary size limit it \nmaintains the same quality level as Extended and compar-\ning the conversion projections the former has better Daru-\nSum and DaruCopy solving capabilities.\nThere are several implications of the observations. First is \nthat the Vocabulary Conversion LEP algorithm is likely to be \nthe most efficient solution for the majority of embedding \nprojection scenarios in some cases even being sufficient for \nrecovering the original instruction-tuned performance. Sec-\nFigure 2\nMicro Average Benchmark Score Dynamic throughout Training\n\nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n139\n| Research Papers\nTable 2\nDarumeru Few-Shot Evaluation Results for Best Language-Adaptation Checkpoints\nModel Vocab Symbols per \ntoken Micro-Avg DaruMMLU DaruMERA DaruSum DaruCopy \n(EN)\nDaruCopy \n(RU)\nMistral-7B original 2,44 0,604 0,545 0,504 0,307 1,000 1,000\nBPE 3,76 0,616 0,528 0,537 0,316 0,995 0,984\nUnigram 3,78 0,614 0,516 0,544 0,311 0,995 0,960\nExtended 3,77 0,617 0,538 0,532 0,314 1,000 0,995\nLLaMa-3-8B original 2,89 0,629 0,582 0,547 0,326 0,980 0,982\nBPE 4,40 0,618 0,561 0,532 0,321 1,000 0,963\nUnigram 4,35 0,609 0,560 0,517 0,316 1,000 0,951\nExtended 3,78 0,627 0,560 0,550 0,325 0,980 0,983\nOptimized 3,40 0,620 0,552 0,536 0,323 0,981 0,989\nTable 3\nDarumeru Zero-Shot evaluation Results for Learned Embedding Propagation Methods\nVocab LEP method Micro-Avg DaruMMLU DaruMERA DaruSum DaruCopy \n(En)\nDaruCopy \n(Ru)\nOpenChat-3.5\nBPE Swap 0,587 0,528 0,526 0,277 0,988 0,829\nOverlap 0,584 0,525 0,523 0,281 0,986 0,818\nConversion 0,583 0,526 0,524 0,284 0,993 0,791\nUnigram Swap 0,556 0,517 0,517 0,282 0,985 0,614\nOverlap 0,572 0,514 0,534 0,297 0,981 0,68\nConversion 0,565 0,515 0,519 0,301 0,999 0,651\nExtended Swap 0,608 0,535 0,540 0,298 0,999 0,907\nOverlap 0,607 0,535 0,539 0,307 0,999 0,898\nConversion 0,609 0,535 0,541 0,306 0,999 0,909\nLLaMa-3-8B (instruct)\nBPE Swap 0,565 0,544 0,486 0,317 0,999 0,729\nOverlap 0,569 0,546 0,489 0,314 0,999 0,753\nConversion 0,570 0,546 0,490 0,318 0,999 0,754\nUnigram Swap 0,582 0,545 0,488 0,313 0,999 0,865\nOverlap 0,580 0,545 0,482 0,314 0,999 0,876\nConversion 0,584 0,545 0,488 0,315 0,994 0,889\nExtended Swap 0,592 0,557 0,498 0,319 0,969 0,921\nOverlap 0,597 0,556 0,504 0,321 0,964 0,936\nConversion 0,597 0,556 0,501 0,318 0,994 0,921\nOptimized Swap 0,594 0,554 0,499 0,327 0,970 0,928\nOverlap 0,586 0,553 0,495 0,323 0,925 0,925\nConversion 0,598 0,555 0,500 0,324 0,995 0,928\nMikhail Tikhomirov , Daniil Chernyshev\n140\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\nondly, while Unigram tokenization vocabulary may be con-\nsidered morphologically correct for Russian language it is \ninferior to Extended and Optimization options as it requires \nfull vocabulary substitution, which, considering unstable \nBPE performance, creates the largest disparity between em-\nbedding and inner-layer semantic representation. The to-\nkens removed in the Optimized variant seem to be unimpor-\ntant for Russian task-solving capabilities as it manages to \noutperform Extended tokenization which completely retains \nthe original vocabulary. The performance gap in LEP LLaMa-\n3-8B (instruct) is likely to be the consequence of proprietary \ninstruction-tuning dataset which was large enough to align \nthe embedding semantics with instruction-following tasks \n(Dubey, 2024). Another hypothesis is that the original LLM \nunderwent human preference alignment procedure which \naims to block text generation of harmful answers at the cost \nof necessary reasoning limitations and as a consequence \nhas a habit of blocking potential malicious semantics orig-\ninating from input embeddings which in turn inhibits text \ncomprehension capabilities. \nCase Study: Self-Calibration\nIn self-calibration experiments we focused on closing \nthe gap of best LEP LLaMa-3-8B instruct models (Table 4, \nself-calibration). As expected the performance of DaruCopy \ntasks improved substantially, practically reaching the perfect \nreliability levels. DaruSum also saw the improvements as the \nimproved citation capabilities are beneficial for composing \nconcise summaries. Other tasks however did not improve \nmuch and in the case of the weakest vocabulary adaptation, \nUnigram, saw a significant decline in benchmark scores. \nWe suspect that the self-calibration data promotes closed-\nmind reasoning as training on the most probable answers \nbiases the model towards generic vocabulary which had the \nhighest frequency in the training data. As a consequence the \ncomprehension of rarer domain-specific concepts which are \npresent in MMLU and MERA datasets may be inhibited due \nto increased tendency of using more common language. \nThe issue can be alleviated by more complex example sam-\npling procedures such a beam search or multi-candidate \ngeneration with post-generation ranking with larger state-\nof-the-art LLMs such as GPT-4 or LLaMa-3-405B.\nCase Study: Continued Instruction-Tuning \nCalibration\nOur experiments on continued instruction-tuning calibra-\ntion approach, presented in Table 4, showed that the addi-\ntionally fine-tuned LEP adapted models achieve and in some \ncases outperform the original models. Adding 2000 instruc-\ntions for copying long texts to the instructional dataset has \na positive effect in almost all cases. Moreover, the obtained \n4 https://huggingface.co/spaces/Vikhrmodels/arenahardlb\n5 https://lmarena.ai/\nmodels are more effective when used in the Russian lan-\nguage, and the loss of initial knowledge in the case of our \nmethod is minimal, compared to conventional instruct-tun-\ning.\nExamples\nWe also investigated how the models’ responses changed \ndepending on the stage: original model, LEP, LEP + calibra-\ntion (Figure 3).\nFrom the example, it can be seen that the original model \ndid not correctly perceive the question at all. The LEP mod-\nel already answers more correctly, but does not take into \naccount that this is a phraseological unit. The calibrated \nmodel already answers the question most correctly among \nthe three versions of the model, paying attention to the true \nmeaning of the phrase.\nDISCUSSION\nLLM Benchmark Results for Russian Language\nResults presented in Table 1 demonstrate that fine-tuning \nof open-source state-of-the-art LLMs on Russian focused in-\nstruction datasets commonly leads to performance drops in \nlanguage understanding. This phenomenon was initially ob-\nserved within Ru-Arena-General4 and Chatbot Arena5 bench-\nmarks, however, due to their open-question format it was \nhard to separate generation errors from bad user prompt-\ning. Closed-question benchmarks such as MERA (Fenogeno-\nva, 2024), which was used as the basis of Darumeru, can not \nreliably detect language processing degradation due to the \npossibility of benchmark hacking. Benchmark hacking is a \nprocedure of fine-tuning on benchmark solutions or similar \ndata which is viewed as a variant of cheating in the context \nof LLM benchmarks. Usually developers of LLM models do \nnot intend to resort to such poor practice and on the contra-\nry make an additional effort to remove any possible bench-\nmark data from the overall LLM training data pool. At the \nsame time detecting benchmark related data-leaks is a la-\nbor-intensive task as it requires checking training data not \njust for exact matches but also for any possible paraphrases \nwhich includes translating examples to other languages. \nOur Darumeru benchmark addresses the limitation of \nclosed-question format with newly introduced tasks for \ntext summarization (DaruSum) and tokenization diagnostic \n(DaruCopy). DaruSum requires two crucial task-solving ele-\nments, proper text analysis and good text writing skills. Any \nperformance drops in this benchmark subset indicate prob-\nlems with text understanding or text generation. DaruCopy \ndistinguishes between the two by exclusively evaluating the \nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n141\n| Research Papers\nlatter by reducing the task to explicitly broadcasting the \noriginal context without any analysis or paraphrasing. Con-\nsequently, lower DaruCopy scores indicate a reasoning con-\nflict within the LLM logic as the model fails to follow simplest \ntask directive of text copying. These two subsets of Daru-\nmeru benchmark show that LLaMa-3-8B is a more reliable \nchoice for Russian processing tasks than Saiga or Vikhr-5.2 \ndespite their Russian language specialization which con-\ntrasts with the results of MERA benchmark (Fenogenova, \n2024). While MERA results of Saiga lie within standard de-\nviation the results of Vikhr-5.2 clearly suggest the case of \nbenchmark hacking.\nLanguage Adaptation Strategy\nDuring development of our LLM Russian adaptation pipe-\nline we made several design choices which were explored \nin previous works. First of all, we assumed that tokenization \nknowledge and the ability to use new tokens is stored in in-\nput embeddings and LM head layers of LLM. Several works \n(Cui, 2023; Tikhomirov, 2023; Nikolich 2024; Nguyen, 2024) \ndemonstrated that language-adaptation of these subset of \nlayers only is insufficient for proper language understand -\ning and thus subsequent instruction-tuning of such models \nleads to suboptimal results. At the same time it was shown \n(Tikhomirov, 2024) that there is no significant difference \nbetween language-adaptation of all-layers and dual-stage \napproach, when embedding and LM-head training process \nis complemented with subsequent training of other layers. \nResults reported in Table 2 reinforce this claim as the first \nstage of dual-stage approach proves to be efficient enough \nto substantially improve Russian language comprehension \nof Mistral-7B model. However, LLaMa-3-8B post-adaptation \nscores suggest that the necessity of inner-layer training is \ndictated by the original LLM Russian linguistic skills which \nare effectively captured by the DaruMERA subset of our \nbenchmark. Learned Embedding Propagation procedure re-\nsults (see Table 3) also reflect this observation as Mistral-7B \nshowed highest language-knowledge transfer efficiency. \nWhether layer discrepancy can be alleviated by instruc-\ntion-tuning we explored in our calibration experiments. In-\nstruction-tuning on target language often improves token \nutilization and boosts language comprehension (Gusev, \nTable 4\nBenchmark Results for Model Calibration Schemes of Conversion LEP Models\nModel Fine-tuning \ndata Micro-Avg DaruMMLU DaruMERA DaruSum DaruCopy \n(EN)\nDaruCopy \n(RU)\nOpenChat 3.5\nOriginal model - 0,607 0,543 0,526 0,322 0,999 0,917\nsaiga d7 0,611 0,540 0,528 0,325 0,999 0,945\n+copy task 0,615 0,541 0,524 0,324 1,000 0,995\nUnigram - 0,565 0,515 0,519 0,301 0,999 0,651\nsaiga d7 0,599 0,532 0,556 0,316 0,999 0,754\n+copy task 0,630 0,530 0,559 0,321 1,000 0,999\nExtended - 0,609 0,535 0,541 0,306 0,999 0,909\nsaiga d7 0,616 0,543 0,566 0,319 0,999 0,845\n+copy task 0,632 0,541 0,563 0,321 1,000 0,989\nLLaMa-3-8B instruct \nOriginal model - 0,610 0,571 0,510 0,322 1,000 0,972\nsaiga d7 0,615 0,576 0,512 0,329 1,000 0,983\n+copy task 0,616 0,575 0,513 0,332 1,000 0,995\nExtended - 0,597 0,556 0,501 0,318 0,994 0,921\nself-calibration 0,606 0,552 0,512 0,321 1,000 0,958\nsaiga d7 0,614 0,568 0,519 0,338 0,995 0,961\n+copy task 0,618 0,565 0,521 0,339 1,000 0,984\nOptimized - 0,598 0,555 0,500 0,324 0,995 0,928\nself-calibration 0,601 0,550 0,501 0,325 1,000 0,95\nsaiga d7 0,611 0,555 0,515 0,336 1,000 0,971\n+copy task 0,617 0,555 0,522 0,339 1,000 0,989\nMikhail Tikhomirov , Daniil Chernyshev\n142\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\n2023; Wei, 2023; Nikolich, 2024). We see a similar trend in Ta-\nble 4. By training the original non-adapted instruction-tuned \nversions of LLMs on Saiga dataset (Gusev, 2023) we en-\nhanced Russian task-solving capabilities which boosted \nbenchmark scores. Applying the same procedure to our LEP \nmodels (saiga d7) we retain the positive effect at increased \nrates with the scores higher than of the original models \nwhich were the subjects of LEP knowledge transfer. The \ndrawback of instruction-tuning on Russian instruction data-\nsets is that we inevitably disturb the original knowledge that \nwas gained in prior training (Tejaswi, 2024). We attempted \nto address the issue by training on the answers generated \nby the original LLM (self-calibration) rather than using the \noriginal references from the Saiga dataset. However for LLa-\nMa-3-8B instruct we did not see noticeable improvement in \nany LLM capabilities besides tokenization utilization (Daru-\nCopy). This result is likely due to lack of generation quality of \nour self-calibration synthesized examples which during our \nmanual inspection revealed to carry much simpler Russian \nlanguage logic and vocabulary. Considering that Saiga is a \nprime example of GPT-4 reference synthesis (Taori, 2024) \nwe hypothesize that by utilizing more advanced sampling \ntechniques and better example quality evaluation protocols \nwe may collect a reference dataset with the similar features \nwithout employment of other datasets or third-party mod-\nels.\nLIMITATIONS\nDespite the broad applicability of our method, this study has \nseveral limitations. First, the method requires that not only \ninstructional versions of LLM but also their foundational \nversions be available, which is not always the case. Second-\nly, in the case of languages using hieroglyphs, initialization \nafter tokenizer replacement can be quite weak due to lack \nof shared tokens and it is not known how much adaptation \nof embeddings can help with this. Another important point \nFigure 3\nAn Example of Generation Using the OpenChat-3.5 Model and Its Adapted Versions\n\nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n143\n| Research Papers\nis that the focus of the knowledge transfer procedure was \non preserving the original knowledge of the target model \nwhich is why the possible volume of transferred knowledge \nmay be insufficient. However, since the methodology effec-\ntively adapts the model to the language, it is always possible \nto conduct an additional stage of continuous pretraining to \nacquire new knowledge.\nCONCLUSION\nIn this paper, we proposed Learned Embedding Propagation \n(LEP), an improved approach to large language model (LLM) \nlanguage adaptation that has minimal impact on LLM inher-\nent knowledge while enabling transferring the language-ad-\naptation knowledge directly to any instruct-tuned version, \nincluding the proprietary. Focussing on cost-efficiency of \nour method we derived 3 ad-hoc approaches for the em-\nbedding propagation: Direct Embedding Swap, Overlapping \nToken Correction and Vocabulary Conversion. To facilitate \nthe development process of optimal Russian adaptation we \nintroduced Darumeru, a train-time benchmark which fo-\ncuses on text generation reliability. By analyzing the bench-\nmark performance of popular instruction-tune LLMs and 4 \nvocabulary adaptation options we derived a recipe for the \nmost cost-efficient procedure. Using the recipe and the pro-\nposed LEP methods we built language-adapted variants of \nsub-9B parameter state-of-the-art instruction-tuned LLMs, \nOpenchat-3.5 and LLaMa-3-8B (Instruct). The evaluation \nresults demonstrated that the Vocabulary Conversion LEP \nvariants reproduce the performance levels of the original in-\nstruction-tuned LLM and in the case of OpenChat–3.5 even \noutperform while having all benefits of improved computa-\ntional efficiency. To close the remaining gaps in task-solv -\ning performance we conducted case-study experiments on \nself-calibration and continued instruct-tuning alignment \napproaches which concluded with further language com-\nprehension improvements and new benchmark records. \nThe obtained results open new prospects for LLM language \nadaptation enabling cost-efficient utilization of any instruc-\ntion-tuned models regardless of openness of their fine-tun-\ning data with all the merits of the original version.\nAll our models, benchmark and framework are open source \nand available under the original model licenses.\nACKNOWLEDGEMENT\nThe work of Mikhail Tikhomirov was supported by Noncom-\nmercial Foundation for Support of Science and Education \n”INTELLECT”. The work of Daniil Chernyshev was supported \nby Noncommercial Foundation for Support of Science and \nEducation ”INTELLECT”.  The research was carried out using \nthe MSU-270 supercomputer of Lomonosov Moscow State \nUniversity.\nDECLARATION OF COMPETITING \nINTEREST\nNone declared.\nAUTHOR CONTRIBUTIONS\nMikhail Tikhomirov:  Conceptualization; Investigation; \nMethodology; Project administration; Software; Writing – \noriginal draft.\nDaniil Chernyshev: Conceptualization; Data curation; For-\nmal analysis; Methodology; Validation; Visualization; Writ-\ning – original draft; Writing – review & editing.\nREFERENCES\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., & Hashimoto, T. B. (2023). Stanford alpaca: An instruc-\ntion-following llama model. https://github.com/tatsu-lab/stanford_alpaca\nLi, H., Koto, F., Wu, M., Aji, A. F., & Baldwin, T. (2023). Bactrian-x: Multilingual replicable instruction-following models with low-\nrank adaptation. arXiv preprint arXiv:2305.15011. https://doi.org/10.48550/arXiv.2305.15011\nWei, X., Wei, H., Lin, H., Li, T., Zhang, P., Ren, X., Li, M., Wan, Y., Cao, Z., Xie, B., Hu, T., Li, S., Hui, B., Yu, B., Liu, D., Yang, B., \n& Xie, J. (2023). Polylm: An open source polyglot large language model. arXiv:2307.06018. https://doi.org/10.48550/arX-\niv.2307.06018\nGusev, I. (2023). rulm: A toolkit for training neural language models. https://github.com/IlyaGusev/rulm.\nKuulmets, H. A., Purason, T., Luhtaru, A., & Fishel, M. (2024, June). Teaching Llama a new language through cross-lingual \nknowledge transfer. In Findings of the Association for Computational Linguistics: NAACL 2024 (pp. 3309-3325). Association for \nComputational Linguistics. https://doi.org/10.18653/v1/2024.findings-naacl.210\nZhu, W., Lv, Y., Dong, Q., Yuan, F., Xu, J., Huang, S., Kong, L., & Li, L. (2023). Extrapolating large language models to non-english by \naligning languages. arXiv:2308.04948. https://doi.org/10.48550/arXiv.2308.04948\nRanaldi, L., Pucci, G., & Freitas, A. (2023). Empowering cross-lingual abilities of instruction-tuned large language models by transla-\ntion-following demonstrations. arXiv:2308.14186. https://doi.org/10.48550/arXiv.2308.14186\nMikhail Tikhomirov , Daniil Chernyshev\n144\nJLE  |  Vol. 10  |  No. 4  |  2024\n| Research Papers\nLi, C., Wang, S., Zhang, J., & Zong, C. (2024, June). Improving in-context learning of multilingual generative language models \nwith cross-lingual alignment. Proceedings of the 2024 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies (vol. 1: Long Papers, pp. 8051-8069). Association for Computational \nLinguistics. 10.18653/v1/2024.naacl-long.445\nChai, L., Yang, J., Sun, T., Guo, H., Liu, J., Wang, B., Liang, X., Bai, J., Li, T., Peng, Q., & Li, Z. (2024). xcot: Cross-lingual instruction tun-\ning for cross-lingual chain-of-thought reasoning. arXiv preprint arXiv:2401.07037. https://doi.org/10.48550/arXiv.2401.07037\nHusain, J. A., Dabre, R., Kumar, A., Puduppully, R., & Kunchukuttan, A. (2024). RomanSetu: Efficiently unlocking multilingual ca-\npabilities of Large Language Models models via Romanization. arXiv:2401.14280. https://doi.org/10.48550/arXiv.2401.14280\nLakew, S. M., Erofeeva, A., Negri, M., Federico, M., & Turchi, M. (2018). Transfer learning in multilingual neural machine trans-\nlation with dynamic vocabulary. Proceedings of the 15th International Conference on Spoken Language Translation (pp. 54-61). \nInternational Conference on Spoken Language Translation. https://doi.org/10.48550/arXiv.1811.01137\nKuratov, Y., & Arkhipov, M. (2019). Adaptation of deep bidirectional multilingual transformers for Russian language. \nKomp’juternaja Lingvistika i Intellektual’nye Tehnologii (pp. 333-339). Komp’juternaja Lingvistika i Intellektual’nye Tehnologii.  \nhttps://doi.org/10.48550/arXiv.1905.07213\nRust, P., Pfeiffer, J., Vulić, I., Ruder, S., & Gurevych, I. (2021, August). How good is your tokenizer? On the monolingual per-\nformance of multilingual language models. Proceedings of the 59th Annual Meeting of the Association for Computational \nLinguistics and the 11th International Joint Conference on Natural Language Processing (vol. 1: Long Papers, pp. 3118-3135). \nAssociation for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-long.243\nYang, Z., Xu, Z., Cui, Y., Wang, B., Lin, M., Wu, D., & Chen, Z. (2022, October). CINO: A Chinese minority pre-trained Language \nModel. In Proceedings of the 29th International Conference on Computational Linguistics (pp. 3937-3949). International Com-\nmittee on Computational Linguistics. https://doi.org/10.48550/arXiv.2202.13558\nVries, W., & Nissim, M. (2021, August). As good as new. How to successfully recycle English GPT-2 to make models for other \nlanguages. Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (pp. 836-846). Association for Computa-\ntional Linguistics. https://doi.org/10.18653/v1/2021.findings-acl.74\nTikhomirov, M., & Chernyshev, D. (2023). Impact of tokenization on LLaMa Russian adaptation. 2023 Ivannikov Ispras Open \nConference (pp. 163-168). IEEE. http://dx.doi.org/10.1109/ISPRAS60948.2023.10508177\nTikhomirov, M., & Chernyshev, D. (2024). Improving Large Language Model Russian adaptation with preliminary vocabulary \noptimization. Lobachevskii Journal of Mathematics, 45, 3211-3219. 10.1134/S1995080224604120. \nCui, Y., Yang, Z., & Yao, X. (2023). Efficient and effective text encoding for Chinese llama and alpaca. arXiv:2304.08177.  \nhttps://doi.org/10.48550/arXiv.2304.08177\nNguyen, X. P., Zhang, W., Li, X., Aljunied, M., Tan, Q., Cheng, L., Chen, G., Deng, Y., Yang, S., Liu, C., Zhang, H., & Bing, L. (2023). \nSeaLLMs-Large Language Models for Southeast Asia. arXiv:2312.00738. https://doi.org/10.48550/arXiv.2312.00738\nNikolich, A., Korolev, K., & Shelmanov, A. (2024). Vikhr: The family of open-source instruction-tuned Large Language Models for \nRussian. arXiv preprint arXiv:2405.13929. https://doi.org/10.48550/arXiv.2405.13929\nArtetxe, M., Ruder, S., & Yogatama, D. (2020). On the cross-lingual transferability of monolingual representations. Proceed-\nings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics (pp. \n4623–4637). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.421\nChen, Y., Marchisio, K., Raileanu, R., Adelani, D., Saito Stenetorp, P. L. E., Riedel, S., & Artetxe, M. (2023). Improving language \nplasticity via pretraining with active forgetting. Advances in Neural Information Processing Systems, 36, 31543-31557.  \nhttps://doi.org/10.48550/arXiv.2307.01163\nTejaswi, A., Gupta, N., & Choi, E. (2024). Exploring design choices for building language-specific LLMs. arXiv preprint arX-\niv:2406.14670. https://doi.org/10.48550/arXiv.2406.14670\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., & Levy, O. \n(2024). Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36. https://doi.org/10.48550/\narXiv.2305.11206\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., ... & Ganapathy, R. (2024). The Llama 3 Herd of Models. arX-\niv:2407.21783. https://doi.org/10.48550/arXiv.2407.21783\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling \nlaws for neural language models. arXiv:2001.08361. https://doi.org/10.48550/arXiv.2001.08361\nIlharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., & Farhadi, A. Editing models with task arithmetic. The \nEleventh International Conference on Learning Representations. International Conference on Learning Representations.  \nhttps://doi.org/10.48550/arXiv.2212.04089\nFACILITATING LLM RUSSIAN ADAPTATION WITH LEP\nJLE  |  Vol. 10  |  No. 4  |  2024\n145\n| Research Papers\nGusev, I. (2020). Dataset for automatic summarization of Russian news. In Artificial Intelligence and Natural Language:  \n9th Conference (Proceedings 9, pp. 122-134). Springer International Publishing. https://doi.org/10.1007/978-3-030-59082-6_9\nDubois, Y., Galambosi, B., Liang, P., & Hashimoto, T. B. (2024). Length-controlled alpacaeval: A simple way to debias automatic \nevaluators. arXiv:2404.04475. https://doi.org/10.48550/arXiv.2404.04475\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C.,  Agarwal, S., Slama, K., Ray, A., Schulman, J., \nHilton, J., Kelton, F., Miller, L., Simens, M., Askell, A,, Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language \nmodels to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730-27744. \nhttps://doi.org/10.48550/arXiv.2203.02155\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F.,  \nRodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023a). Llama: Open and efficient foundation language models. arX-\niv:2302.13971. https://doi.org/10.48550/arXiv.2302.13971\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023b). Llama 2: Open foundation and fine-\ntuned chat models. arXiv:2307.09288. https://doi.org/10.48550/arXiv.2307.09288\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L.,  Bressand, F., Lengyel, G., Lample, G., \nSaulnier, L., Lavaud, L. R., Lachaux, M., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., & Sayed, W. E. (2023). Mistral 7B. \narXiv:2310.06825. https://doi.org/10.48550/arXiv.2310.06825\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). Gpt-4 technical report. arX-\niv:2303.08774. https://doi.org/10.48550/arXiv.2303.08774\nFenogenova A. et al. (2024). Mera: A comprehensive LLM evaluation in Russian. arXiv:2401.04531. https://doi.org/10.48550/arX-\niv.2401.04531\nMikhailov, V., Shamardina, T., Ryabinin, M., Pestova, A., Smurov, I., & Artemova, E. (2022). RuCoLA: Russian corpus of linguistic \nacceptability. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 5207-5227). Asso-\nciation for Computational Linguistics. https://doi.org/10.18653/v1/2022.emnlp-main.348"
}