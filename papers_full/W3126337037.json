{
    "title": "Video Transformer Network",
    "url": "https://openalex.org/W3126337037",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3081480175",
            "name": "Daniel Neimark",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2785111538",
            "name": "Omri Bar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3081207565",
            "name": "Maya Zohar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3036358170",
            "name": "Dotan Asselmann",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3081480175",
            "name": "Daniel Neimark",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2785111538",
            "name": "Omri Bar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3081207565",
            "name": "Maya Zohar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3036358170",
            "name": "Dotan Asselmann",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6724944384",
        "https://openalex.org/W2145287260",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W3118473641",
        "https://openalex.org/W2096733369",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6631456553",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W6786708909",
        "https://openalex.org/W3101367838",
        "https://openalex.org/W3034572008",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W639708223",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W6762876521",
        "https://openalex.org/W2553594924",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W1983364832",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W6955071965",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W2955874753",
        "https://openalex.org/W2990152177",
        "https://openalex.org/W2962711930",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2962876901",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W2995684093",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W1947481528",
        "https://openalex.org/W3099206234",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains $16.1\\times$ faster and runs $5.1\\times$ faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring $1.5\\times$ fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md",
    "full_text": "Video Transformer Network\nDaniel Neimark Omri Bar Maya Zohar Dotan Asselmann\nTheator\n{danieln, omri, maya, dotan}@theator.io\nAbstract\nThis paper presents VTN, a transformer-based frame-\nwork for video recognition. Inspired by recent developments\nin vision transformers, we ditch the standard approach in\nvideo action recognition that relies on 3D ConvNets and\nintroduce a method that classiﬁes actions by attending\nto the entire video sequence information. Our approach\nis generic and builds on top of any given 2D spatial\nnetwork. In terms of wall runtime, it trains 16.1×faster\nand runs 5.1×faster during inference while maintaining\ncompetitive accuracy compared to other state-of-the-art\nmethods. It enables whole video analysis, via a single\nend-to-end pass, while requiring 1.5×fewer GFLOPs. We\nreport competitive results on Kinetics-400 and Moments\nin Time benchmarks and present an ablation study of\nVTN properties and the trade-off between accuracy and\ninference speed. We hope our approach will serve as a\nnew baseline and start a fresh line of research in the video\nrecognition domain. Code and models are available at:\nhttps://github.com/bomri/SlowFast/blob/\nmaster/projects/vtn/README.md.\n1. Introduction\nAttention matters. For almost a decade, ConvNets have\nruled the computer vision ﬁeld [21, 7]. Applying deep\nConvNets produced state-of-the-art results in many visual\nrecognition tasks, i.e., image classiﬁcation [30, 18, 32], ob-\nject detection [16, 15, 26], semantic segmentation [23], ob-\nject instance segmentation [17], face recognition [31, 28]\nand video action recognition [9, 36, 3, 37, 13, 12]. But, re-\ncently this domination is starting to crack as transformer-\nbased models are showing promising results in many of\nthese tasks [10, 2, 33, 38, 40, 14].\nVideo recognition tasks also rely heavily on ConvNets.\nIn order to handle the temporal dimension, the fundamen-\ntal approach is to use 3D ConvNets [5, 3, 4]. In contrast to\nother studies that add the temporal dimension straight from\nthe input clip level, we aim to move apart from 3D net-\nworks. We use state-of-the-art 2D architectures to learn the\nspatial feature representations and add the temporal infor-\nFigure 1. Video Transformer Network architecture. Connecting\nthree modules: A 2D spatial backbone (f(x)), used for feature ex-\ntraction. Followed by a temporal attention-based encoder (Long-\nformer in this work), that uses the feature vectors ( φi) combined\nwith a position encoding. The [CLS] token is processed by a clas-\nsiﬁcation MLP head to get the ﬁnal class prediction.\nmation later in the data ﬂow by using attention mechanisms\non top of the resulting features. Our approach input only\nRGB video frames and without any bells and whistles (e.g.,\noptical ﬂow, streams lateral connections, multi-scale infer-\nence, multi-view inference, longer clips ﬁne-tuning, etc.)\nachieves comparable results to other state-of-the-art mod-\nels.\nVideo recognition is a perfect candidate for Transform-\ners. Similar to language modeling, in which the input words\nor characters are represented as a sequence of tokens [35],\nvideos are represented as a sequence of images (frames).\nHowever, this similarity is also a limitation when it comes\nto processing long sequences. Like long documents, long\nvideos are hard to process. Even a 10 seconds video, such\nas those in the Kinetics-400 benchmark [20], are processed\nin recent studies as short, ˜2 seconds, clips.\nBut how does this clip-based inference would work on\nmuch longer videos (i.e., movie ﬁlms, sports events, or sur-\ngical procedures)? It seems counterintuitive that the infor-\narXiv:2102.00719v3  [cs.CV]  17 Aug 2021\nFigure 2. Extracting 16 frames evenly from a video of the abseiling category in the Kinetics-400 dataset [20]. Analyzing the video’s full\ncontext and attending to the relevant parts is much more intuitive than analyzing several clips built around speciﬁc frames, as many of these\nframes might lead to false predictions.\nmation in a video of hours, or even a few minutes, can be\ngrasped using only a snippet clip of a few seconds. Nev-\nertheless, current networks are not designed to share long-\nterm information across the full video.\nVTN’s temporal processing component is based on a\nLongformer [1]. This type of transformer-based model can\nprocess a long sequence of thousands of tokens. The atten-\ntion mechanism proposed by the Longformer makes it fea-\nsible to go beyond short clip processing and maintain global\nattention, which attends to all tokens in the input sequence.\nIn addition to long sequence processing, we also explore\nan important trade-off in machine learning – speed vs. ac-\ncuracy. Our framework demonstrates a superior balance\nof this trade-off, both during training and also at inference\ntime. In training, even though wall runtime per epoch is\neither equal or greater, compared to other networks, our ap-\nproach requires much fewer passes of the training dataset\nto reach its maximum performance; end-to-end, compared\nto state-or-the-art networks, this results in a 16.1×faster\ntraining. At inference time, our approach can handle both\nmulti-view and full video analysis while maintaining simi-\nlar accuracy. In contrast, other networks’ performance sig-\nniﬁcantly decreases when analyzing the full video in a sin-\ngle pass. In terms of GFLOPS x Views, their inference cost\nis considerably higher than those of VTN, which concludes\nto a 1.5×fewer GFLOPS and a 5.1×faster validation wall\nruntime.\nOur framework’s structure components are modular\n(Fig. 1). First, the 2D spatial backbone can be replaced\nwith any given network. The attention-based module can\nstack up more layers, more heads or can be set to a differ-\nent Transformers model that can process long sequences.\nFinally, the classiﬁcation head can be modiﬁed to facilitate\ndifferent video-based tasks, like temporal action localiza-\ntion.\n2. Related Work\nSpatial-temporal networks. Most recent studies in video\nrecognition suggested architectures that are based on 3D\nConvNets [19, 34]. In [5], a two-stream architecture was\nused, one stream for RGB inputs and another for Optical\nFlow (OF) inputs. Residual connections are inserted into\nthe two-stream architecture to allow a direct link between\nRGB and OF layers. The idea of inﬂating 2D ConvNets into\ntheir 3D counterpart (I3D) was introduced in [3]. I3D takes\n2D ConvNets and expands its layers into 3D. Therefore it\nallows to leverage pre-trained state-of-the-art image recog-\nnition architectures in the spatial-temporal domain and ap-\nply them for video-based tasks.\nNon-local Neural Networks (NLN) [37] introduced a\nnon-local operation, a type of self-attention, that computes\nresponses based on relationships between different loca-\ntions in the input signal. NLN demonstrated that the core\nattention mechanism in Transformers can produce good re-\nsults on video tasks, however it is conﬁned to processing\nonly short clips. In order to extract long temporal context,\n[39] introduced a long-term feature bank that acts as the\nentire video memory and a Feature Bank Operator (FBO)\nthat computes interactions between short-term and long-\nterm features. However, it requires precomputed features,\nand it is not efﬁcient enough to support end-to-end training\nof the feature extraction backbone.\nSlowFast [13] explored a network architecture that op-\nerates in two pathways and different frame rates. Lateral\nconnections fuse the information between the slow pathway,\nfocused on the spatial information, and the fast pathway fo-\ncused on temporal information.\nThe X3D study [12] builds on top of SlowFast. It argues\nthat in contrast to image classiﬁcation architectures, which\nhave been developed via a rigorous evolution, the video\narchitectures have not been explored in detail, and histor-\nically are based on expanding image-based networks to ﬁt\nthe temporal domain. X3D introduces a set of networks that\nprogressively expand in different axes, e.g., temporal, frame\nrate, spatial, width, bottleneck width, and depth. Compared\nto SlowFast, it offers a lightweight network (in terms of\nGFLOPS and parameters) with similar performance.\nTransformers in computer vision. The Transformers ar-\nchitecture [35] reached state-of-the-art results in many NLP\ntasks, making it the de-facto standard. Recently, Transform-\ners are starting to disrupt the ﬁeld of computer vision, which\ntraditionally depends on deep ConvNets. Studies like ViT\nand DeiT for image classiﬁcation [10, 33], DETR for ob-\nject detection and panoptic segmentation [2], and VisTR for\nmodel pretrain\n(ImageNet accuracy (%)) top-1 top-5\nR50-VTN ImageNet (76.2 [25]) 71.2 90.0\nR101-VTN ImageNet (77.4 [25]) 72.1 90.3\nDeiT-B-VTN ImageNet (81.8 [33]) 75.5 92.2\nDeiT-BD-VTN ImageNet (83.4 [33]) 75.6 92.4\nViT-B-VTN ImageNet-21K (84.0 [10]) 78.6 93.7\nViT-B-VTN† ImageNet-21K (84.0 [10]) 79.8 94.2\nTable 1. VTN performance on Kinetics-400 validation set for dif-\nferent backbone variations. A full video inference is used. We\nshow top-1 and top-5 accuracy. We report what pre-training was\ndone for each backbone and the related single-crop top-1 accuracy\non ImageNet. (†) Training with extensive data augmentation.\nvideo instance segmentation [38] are some examples show-\ning promising results when using Transformers in the com-\nputer vision ﬁeld. Binding these results with the sequential\nnature of video makes it a perfect match for Transformers.\nApplying Transformers on long sequences. BERT [8]\nand its optimized version RoBERTa [22] are transformer-\nbased language representation models. They are pre-trained\non large unlabeled text and later ﬁne-tuned on a given target\ntask. With minimal modiﬁcation, they achieve state-of-the-\nart results on a variety of NLP tasks.\nOne signiﬁcant limitation of these models, and Trans-\nformers in general, is their ability to process long se-\nquences. This is due to the self-attention operation, which\nhas a complexity of O(n2) per layer ( n is sequence\nlength) [35].\nLongformer [1] addresses this problem and enables\nlengthy document processing by introducing an attention\nmechanism with a complexity of O(n). This attention\nmechanism combines a local-context self-attention, per-\nformed by a sliding window, and task-speciﬁc global atten-\ntion.\nSimilar to ConvNets, stacking up multiple windowed at-\ntention layers results in a larger receptive ﬁeld. This prop-\nerty of Longformer gives it the ability to integrate informa-\ntion across the entire sequence. The global attention part\nfocuses on pre-selected tokens (like the [CLS] token) and\ncan attend to all other tokens across the input sequence.\n3. Video Transformer Network\nVideo Transformer Network (VTN) is a generic frame-\nwork for video recognition. It operates with a single stream\nof data, from the frames level up to the objective task head.\nIn the scope of this study, we demonstrate our approach us-\ning the action recognition task by classifying an input video\nto the correct action category.\nThe architecture of VTN is modular and composed of\nthree consecutive parts. A 2D spatial feature extraction\nmodel (spatial backbone), a temporal attention-based en-\ncoder, and a classiﬁcation MLP head. Fig. 1 demonstrates\nour architecture layout.\nVTN is scalable in terms of video length during infer-\nence, and enables the processing of very long sequences.\nDue to memory limitation, we suggest several types of in-\nference methods. (1) Processing the entire video in an end-\nto-end manner. (2) Processing the video frames in chunks,\nextracting features ﬁrst, and then applying them to the tem-\nporal attention-based encoder. (3) Extracting all frames’\nfeatures in advance and then feed them to the temporal en-\ncoder.\n3.1. Spatial backbone\nThe spatial backbone operates as a learned feature ex-\ntraction module. It can be any network that works on\n2D images, either deep or shallow, pre-trained or not,\nconvolutional- or transformers-based. And its weights can\nbe ﬁxed (pre-trained) or trained during the learning process.\n3.2. Temporal attention-based encoder\nAs suggested by [35], we use a Transformer model ar-\nchitecture that applies attention mechanisms to make global\ndependencies in a sequence data. However, Transformers\nare limited by the number of tokens they can process at the\nsame time. This limits their ability to process long inputs,\nsuch as videos, and incorporate connections between distant\ninformation.\nIn this work, we propose to process the entire video at\nonce during inference. We use an efﬁcient variant of self-\nattention, that is not all-pairwise, called Longformer [1].\nLongformer operates using sliding window attention that\nenables a linear computation complexity. The sequence of\nfeature vectors of dimension dbackbone (Sec. 3.1) is fed to the\nLongformer encoder. These vectors act as the 1D tokens\nembedding in the standard Transformer setup.\nLike in BERT [8] we add a special classiﬁcation token\n([CLS]) in front of the features sequence. After propagat-\ning the sequence through the Longformer layers, we use the\nﬁnal state of the features related to this classiﬁcation token\nas the ﬁnal representation of the video and apply it to the\ngiven classiﬁcation task head. Longformer also maintains\nglobal attention on that special [CLS] token.\n3.3. Classiﬁcation MLP head\nSimilar to [10], the classiﬁcation token (Sec. 3.2) is pro-\ncessed with an MLP head to provide a ﬁnal predicted cat-\negory. The MLP head contains two linear layers with a\nGELU non-linearity and Dropout between them. The input\ntoken representation is ﬁrst processed with a Layer normal-\nization.\n3.4. Looking beyond a short clip context\nThe common approach in recent studies for video action\nrecognition uses 3D-based networks. During inference, due\n# layers top-1 top-5\n1 78.6 93.4\n3 78.6 93.7\n6 78.5 93.6\n12 78.3 93.3\n(a) Depth: Comparing\ndifferent numbers of at-\ntention layers in the Long-\nformer.\nPE shufﬂe top-1 top-5\nlearned - 78.4 93.5\nlearned ✓ 78.8 93.6\nﬁxed - 78.3 93.7\nﬁxed ✓ 78.5 93.7\nno - 78.6 93.7\nno ✓ 78.9 93.7\n(b) Positional embedding: Evaluating the im-\npact of different types of PE methods, with\nand without shufﬂing the input frames. We\nconducted this experiment with an older ex-\nperimental setup using a different learning rate\nscheduler, so the results of thelearned without\nshufﬂe are slightly different from what we re-\nport in other tables: 78.4% vs. 78.6%.\ntemporal footprint # frames top-1 top-5\n2.56 16 78.2 93.4\n2.56 32 78.2 93.6\n5.12 16 78.6 93.4\n5.12 32 78.5 93.5\n10.0 16 78.0 93.3\n(c) Temporal footprint: Comparing the im-\npact of temporal footprint size (in seconds)\nand the number of frames in a clip.\nﬁnetune top-1 top-5\n- 71.6 90.3\n✓ 78.6 93.7\n(d) Finetune: Training\na ViT-B-VTN with three\nattention layers with and\nwithout ﬁne-tuning the 2D\nbackbone. All other hy-\nperparameters remain the\nsame.\nTable 2. Ablation experiments on Kinetics-400. The results are top-1 and top-5 accuracy (%) on the validation set using the full video\ninference approach.\nto the addition of a temporal dimension, these networks are\nlimited by memory and runtime to clips of a small spatial\nscale and a low number of frames. In [3], the authors use the\nwhole video during inference, averaging predictions tempo-\nrally. More recent studies that achieved state-of-the-art re-\nsults processed numerous, but relatively short, clips during\ninference. In [37], inference is done by sampling ten clips\nevenly from the full-length video and average the softmax\nscores to achieve the ﬁnal prediction. SlowFast [13] follows\nthe same practice and introduces the term “view” – a tem-\nporal clip with a spatial crop. SlowFast uses ten temporal\nclips with three spatial crops at inference time; thus, 30 dif-\nferent views are averaged for the ﬁnal prediction. X3D [12]\nfollows the same practice, but in addition, it uses larger spa-\ntial scales to achieve its best results on 30 different views.\nThis common practice of multi-view inference is some-\nwhat counterintuitive, especially when handling long\nvideos. A more intuitive way is to “look” at the entire video\ncontext before deciding on the action, rather than viewing\nonly small portions of it. Fig. 2 shows 16 frames extracted\nevenly from a video of the abseiling category. The actual\naction is obscured or not visible in several parts of the video;\nthis might lead to a false action prediction in many views.\nThe potential in focusing on the segments in the video that\nare most relevant is a powerful ability. However, full video\ninference produces poor performance in methods that were\ntrained using short clips (Table 3 and 4). In addition, it is\nalso limited in practice due to hardware, memory, and run-\ntime aspects.\n4. Video Action Recognition with VTN\nIn order to evaluate our approach and the impact of con-\ntext attention on video action recognition, we use several\nspatial backbones pre-trained on 2D images.\nViT-B-VTN. Combining the state-of-the-art image classi-\nﬁcation model, ViT-Base [10], as the backbone in VTN. We\nuse a ViT-Base network that was pre-trained on ImageNet-\n21K. Using ViT as the backbone for VTN produces an end-\nto-end transformers-based network that uses attention both\nfor the spatial and temporal domains.\nR50/101-VTN. As a comparison, we also use a standard\n2D ResNet-50 and ResNet-101 networks [18], pre-trained\non ImageNet.\nDeiT-B/BD/Ti-VTN. Since ViT-Base was trained on\nImageNet-21K we also want to compare VTN by using sim-\nilar networks trained on ImageNet. We use the recent work\nof [33] and apply DeiT-Tiny, DeiT-Base, and DeiT-Base-\nDistilled as the backbone for VTN.\n4.1. Implementation Details\nTraining. The spatial backbones we use were pre-trained\non either ImageNet or ImageNet-21k. The Longformer and\nthe MLP classiﬁcation head were randomly initialized from\na normal distribution with zero mean and 0.02 std. We train\nthe model end-to-end using video clips. These clips are\nformed by choosing a random frame as the starting point,\nthen sampling 2.56 or 5.12 seconds as the video’s temporal\nfootprint. The ﬁnal clip frames are subsampled uniformly\nto a ﬁxed number of frames N(N = 16, 32), depending on\nthe setup.\nFor the spatial domain, we randomly resize the shorter\nside of all the frames in the clip to a [256, 320] scale and\nrandomly crop all frames to 224 ×224. Horizontal ﬂip is\nalso applied randomly on the entire clip.\nThe ablation experiments were done on a 4-GPU ma-\nchine. Using a batch size of 16 for the ViT-VTN (on\n16 frames per clip input) and a batch size of 32 for the\nR50/101-VTN. We use an SGD optimizer with an initial\nlearning rate of 10−3 and a different learning rate reduction\nFigure 3. Illustrating all the single-head ﬁrst attention layer weights of the [CLS] token vs. 16 frames pulled evenly from a video. High\nweight values are represented by a warm color (yellow) while low values by a cold color (blue). The video’s segments in which abseiling\ncategory properties are shown (e.g., shackle, rope) exhibit higher weight values compared to segments in which non-relevant information\nappears (e.g., shoes, people). The model prediction is abseiling for this video.\nFigure 4. Evaluating the inﬂuence of attention on the training\n(solid line) and validation (dashed line) curves for Kinetics-400.\nA similar ViT-B-VTN with three Longformer layers is trained for\nboth cases, and we modify the attention heads between a learned\none (red) and a ﬁxed uniform version (blue).\npolicy, steps-based for the ViT-VTN versions and cosine\nschedule decay for the R50/101-VTN versions. In order to\nreport the wall runtime, we use an 8-V100-GPU machine.\nSince we use 2D models as the spatial backbone, we can\nmanipulate the input clip shape xclip ∈RB×C×T×H×W by\nstacking all frames from all clips within a batch to create a\nsingle frames batch of shape x ∈R(B·T)×C×H×W . Thus,\nduring training, we propagate all batch frames in a single\nforward-backward pass.\nFor the Longformer, we use an effective attention win-\ndow of size 32, which was applied for each layer. Two other\nhyperparameters are the dimensions set for the Hidden size\nand the FFN inner hidden size. These are a direct derivative\nof the spatial backbone. Therefore, in R50/101-VTN we\nuse 2048 and 4096, respectively, and for ViT-B-VTN we\nuse 768 and 3072, respectively. In addition, we apply At-\ntention Dropout with a probability of 0.1. We also explore\nthe impact of the number of Longformer layers.\nThe positional embedding (PE) information is only rele-\nvant for the temporal attention-based encoder (Fig. 1). We\nexplore three positional embedding approaches (Table 2b):\n(1) Learned positional embedding - since a clip is repre-\nsented using frames taken from the full video sequence, we\ncan learn an embedding that uses as input the frame loca-\ntion (index) in the original video, giving the Transformer\ninformation regarding the position of the clip in the entire\nsequence; (2) Fixed absolute encoding - we use a similar\nmethod to the one in DETR [2], and modiﬁed it to work on\nthe temporal axis only; and (3) No positional embedding -\nno information is added in the temporal dimension, but we\nstill use the global position to mark the special[CLS] token\nposition.\nInference. In order to show a comparison between differ-\nent models, we use both the common practice of infer-\nence in multi-views and a full video inference approach\n(Sec. 3.4).\nIn the multi-view approach, we sample 10 clips evenly\nfrom the video. For each clip, we ﬁrst resize the shorter\nside to 256, then take three crops of size 224 ×224 from\nthe left, center, and right. The result is 30 views per video,\nand the ﬁnal prediction is an average of all views’ softmax\nscores.\nIn the full video inference approach, we read all the\nframes in the video. Then, we align them for batching pur-\nposes, by either sub- or up-sampling, to 250 frames uni-\nformly. In the spatial domain, we resize the shorter side to\n256 and take a center crop of size 224 ×224.\n5. Experiments\n5.1. Ablation Experiments on Kinetics-400\nKinetics-400 dataset. The original Kinetics-400 dataset\n[20] consists of 246,535 training videos and 19,761 vali-\ndation videos. Each video is labeled with one of 400 hu-\nman action categories, curated from YouTube videos. Since\nsome YouTube links are expired, we could only download\n234,584 of the original dataset, thus missing 11,951 videos\nfrom the training set, which are about 5%. This leads to a\nslight drop in performance of about 0.5%1.\n1https://github.com/facebookresearch/\nvideo-nonlocal-net/blob/master/DATASET.md\nmodel training wall\nruntime (minutes)\n# training\nepochs\nvalidation wall\nruntime (minutes)\ninference\napproach\nparams\n(M) top-1 top-5\nI3D* 30 - 84 multi-view 28 73.5 [11] 90.8 [11]\nNL I3D (our impl.) 68 50 150 multi-view 54 74.1 91.7\nNL I3D (our impl.) 68 50 31 full-video 54 72.1 90.5\nSlowFast-8X8-R50* 70 196 [13] 140 multi-view 35 77.0 [11] 92.6 [11]\nSlowFast-8X8-R50* 70 196 [13] 26 full-video 35 68.4 87.1\nSlowFast-16X8-R101* 220 196 [13] 244 multi-view 60 78.9 [11] 93.5 [11]\nR50-VTN 62 40 32 full-video 168 71.2 90.0\nR101-VTN 110 40 32 full-video 187 72.1 90.3\nDeiT-Ti-VTN (3 layers) 52 60 30 full-video 10 67.8 87.5\nViT-B-VTN (1 layer) 107 25 48 full-video 96 78.6 93.4\nViT-B-VTN (3 layers) 130 25 52 full-video 114 78.6 93.7\nViT-B-VTN (3 layers)† 130 35 52 full-video 114 79.8 94.2\nTable 3. To measure the overall time needed to train each model, we observe how long it takes to train a single epoch and how many epochs\nare required to achieve the best performance. We compare these numbers to the validation top-1 and top-5 accuracy on Kinetics-400 and the\nnumber of parameters per model. To measure the training wall runtime, we ran a single epoch for each model, on the same 8-V100-GPU\nmachine, with a 16GB memory per GPU. The models marked by (*) were taken from the PySlowFast GitHub repository [11]. We report\nthe accuracy as written in the Model Zoo, which was done using the 30 multi-view inference approach. To measure the wall runtime, we\nused the code base of PySlowFast. To calculate the SlowFast-16X8-R101 time on the same GPU machine, we used a batch size of 16.\nThe number of epochs is reported, when possible, based on the original model paper. All other models, including the NL I3D, are trained\nusing our codebase and evaluated with a full video inference approach. ( †) The model in the last row was trained with extensive data\naugmentation.\nFigure 5. Kinetics-400 learning curves for our implementation of\nNL I3D (blue)vs. DeiT-B-VTN (red). We show the top-1 accuracy\nfor the train set (solid line) and the validation set (dash line). Top-\n1 accuracy during training is calculated based on a single random\nclip, while during validation we use the full video inference ap-\nproach. DeiT-B-VTN shows high performance in every step of the\ntraining and validation process. It reaches its best accuracy after\nonly 25 epochs compared to the NL I3D that needs 50 epochs.\nIn the validation set, we are missing one video. To test\nour data’s validity and compare it to previous studies, we\nevaluated the SlowFast-8X8-R50 model, published in PyS-\nlowFast[11], on our validation data. We got 76.45% top1-\naccuracy vs. the reported 77%, thus a drop of 0.55%. This\ndrop might be related to different FFmpeg encoding and\nrescaling of the videos. From this point forward, when com-\nparing to other networks, we report results taken from the\noriginal studies except when we evaluate them on the full\nvideo inference in which we use our validation set. All our\napproach results are reported based on our validation set.\nSpatial backbone variations. We start by examining how\ndifferent spatial backbone architectures impact VTN perfor-\nmance. Table 1 shows a comparison of different VTN vari-\nants and the pretrain dataset the backbone was ﬁrst trained\non. ViT-B-VTN is the best performing model and reaches\n78.6% top-1 accuracy and 93.7% top-5 accuracy. The pre-\ntraining dataset is important. Using the same ViT backbone,\nonly changing between DeiT (pre-trained on ImageNet) and\nViT (pre-trained on ImageNet-21K) we get an improvement\nin the results.\nLongformer depth. Next, we explore how the number of\nattention layers impacts the performance. Each layer has 12\nattention heads and the backbone is ViT-B. Table 2a shows\nthe validation top-1 and top-5 accuracy for 1, 3, 6, and 12 at-\ntention layers. The comparison shows that the difference in\nperformance is small. This is counterintuitive to the fact that\ndeeper is better. It might be related to the fact that Kinetics-\n400 videos are relatively short, around 10 seconds. We be-\nlieve that processing longer videos will beneﬁt from a large\nreceptive ﬁeld obtained by using a deeper Longformer.\nLongformer positional embedding. In Table 2b we com-\npare three different positional embedding methods, focus-\ning on learned, ﬁxed, and no positional embedding. All ver-\nsions are done with a ViT-B-VTN, a temporal footprint of\n5.12 seconds, and a clip size of 16 frames. Surprisingly,\nmodel inference\napproach\n# frames per\ninference view # of views test crop\nsize\ninference\nGFLOPs top-1\nNL I3D (our impl.) multi-view 32 30 224 2,625 74.1\nNL I3D (our impl.) full-video 250 1 224 1,266 72.2\nSlowFast-8X8-R50* multi-view 32 30 256 1,971 77.0 [11]\nSlowFast-8X8-R50* full-video 250 1 256 517 68.4\nSlowFast-16X8-R101* multi-view 64 30 256 6,390 78.9 [11]\nSlowFast-16X8-R101* full-video 250 1 256 838 -\nR50-VTN (3 layers) multi-view 16 30 224 2,106 70.9\nR50-VTN (3 layers) full-video 250 1 224 1,059 71.2\nR101-VTN (3 layers) multi-view 16 30 224 3,895 72.0\nR101-VTN (3 layers) full-video 250 1 224 1,989 72.1\nViT-B-VTN (1 layer) multi-view 16 30 224 8,095 78.5\nViT-B-VTN (1 layer) full-video 250 1 224 4,214 78.6\nViT-B-VTN (3 layers) multi-view 16 30 224 8,113 78.6\nViT-B-VTN (3 layers) full-video 250 1 224 4,218 78.6\nTable 4. Comparing the number of GFLOPs during inference. The models marked by (*) were taken from the PySlowFast GitHub\nrepository [11]. We reproduced the SlowFast-8X8-R50 results by using the repository and our Kinetics-400 validation set and got 76.45%\ncompared to the reported value of 77%. When running this model using a full video inference approach, we get a signiﬁcant drop in\nperformance of about 8%. We did not run the SlowFast-16X8-R101 because it was not published. The inference GFLOPs is reported by\nmultiplying the number of views with the GFLOPs calculated per view. ViT-B-VTN with one layer achieves 78.6% top-1 accuracy, a 0.3%\ndrop compared to SlowFast-16X8-R101 while using 1.5×fewer GFLOPS.\nthe one without any positional embedding achieved slightly\nbetter results than the ﬁxed and learned versions.\nAs this is an interesting result, we also use the same\ntrained models and evaluate them after randomly shufﬂing\nthe input frames only in the validation set videos. This is\ndone by ﬁrst taking the unshufﬂed frame embeddings, then\nshufﬂe their order, and ﬁnally add the positional embedding.\nThis raised another surprising ﬁnding, in which the shufﬂe\nversion gives better results, reaching 78.9% top-1 accuracy\non the no positional embedding version. Even in the case\nof learned embeddings it does not have a diminishing ef-\nfect. Similar to the Longformer depth, we believe that this\nmight be related to the relatively short videos in Kinetics-\n400, and longer sequences might beneﬁt more from posi-\ntional information. We also argue that this could mean that\nKinetics-400 is primarily a static frame, appearance based\nclassiﬁcation problem rather than a motion problem [29].\nTemporal footprint and number of frames in a clip. We\nalso explore the effect of using longer clips in the temporal\ndomain and compare a temporal footprint of 2.56 vs. 5.12\nseconds. And also how the number of frames in the clip im-\npact the network performance. The comparison is done on\na ViT-B-VTN with one attention layer in the Longformer.\nTable 2c shows that top-1 and top-5 accuracy are similar,\nimplying that VTN is agnostic to these hyperparameters.\nFinetune the 2D spatial backbone. Instead of ﬁne-tuning\nthe spatial backbone, by continuing the back-propagation\nprocess, when training VTN, we can use a frozen 2D net-\nwork solely for feature extraction. Table 2d shows the vali-\ndation accuracy when training a ViT-B-VTN with three at-\ntention layers with and without also training the backbone.\nFine-tuning the backbone improves the results by 7% in\nKinetics-400 top-1 accuracy.\nDoes attention matter? A key component in our approach\nis the impact of attention functionally on the way VTN per-\nceives the full video sequence. To convey this impact we\ntrain two VTN networks, using three layers in the Long-\nformer, but with a single head for each layer. In one net-\nwork the head is trained as usual, while in the second net-\nwork instead of computing attention based on query/key dot\nproducts and softmax, we replace the attention matrix with\na hard-coded uniform distribution that is not updated during\nback-propagation.\nFig. 4 shows the learning curves of these two networks.\nAlthough the training has a similar trend, the learned atten-\ntion performs better. In contrast, the validation of the uni-\nform attention collapses after a few epochs demonstrating\npoor generalization of that network. Further, we visualize\nthe [CLS] token attention weights by processing the same\nvideo from Fig. 2 with the single-head trained network and\ndepicted, in Fig. 3, all the weights of the ﬁrst attention layer\naligned to the video’s frames. Interestingly, the weights are\nmuch higher in segments related to the abseiling category.\nIn Appendix A. we show a few more examples.\nTraining and validation runtime. An interesting observa-\ntion we make concerns the training and validation wall run-\ntime of our approach. Although our networks have more pa-\nrameters, and therefore, are longer to train and test, they are\nactually much faster to converge and reach their best perfor-\nmance earlier. Since they are evaluated using a single view\nof all video frames, they are also faster during validation.\nTable 3 shows a comparison of different models and several\nmodel pretrain MiT version inputs top-1 top-5\nResNet50 ImageNet v1 RGB 27.2 [24] 51.7 [24]\nI3D ImageNet v1 RGB + OF 29.5 [24] 56.1 [24]\nAssembelNet-50 [27] Kinetics400 v1 RGB + OF 33.9 60.9\nAssembelNet-101 [27] Kinetics400 v1 RGB + OF 34.3 62.7\nI3D ImageNet v2 RGB 28.4* 54.5*\nNL I3D (our impl.) Kinetics400 v2 RGB 30.1 57.3\nViT-B-VTN Kinetics400 v2 RGB 37.4 65.3\nViT-B-VTN (w/ shufﬂe) Kinetics400 v2 RGB 37.4 65.4\nTable 5. Comparison with the state-of-the-art on MiT-v1 and MiT-v2. The results marked by (*) are based on MiT GitHub repository3.\nVTN variants. Compared to the state-of-the-art SlowFast\nmodel, our ViT-B-VTN with one layer achieves almost the\nsame results but completes an epoch faster while requiring\nfewer epochs. This accumulates to a 16.1×faster end-to-\nend training. The validation wall runtime is also5.1×faster\ndue to the full video inference approach.\nTo better demonstrate the fast convergence of our ap-\nproach, we wanted to show an apples-to-apples compari-\nson of different training and evaluating curves for various\nmodels. However, since other methods use the multi-view\ninference only post-training, but use a single view evalua-\ntion while training their models, this was hard to achieve.\nThus, to show such comparison and give the reader addi-\ntional visual information, we trained a NL I3D (pre-trained\non ImageNet) with a full video inference protocol during\nvalidation (using our codebase and reproduced the original\nmodel results). We compare it to DeiT-B-VTN which was\nalso pre-trained on ImageNet. Fig. 5 shows that the VTN-\nbased network converges to better results much faster than\nthe NL I3D and enables a much faster training process com-\npared to 3D-based networks.\nData augmentation. Recent studies showed that data\naugmentation signiﬁcantly improves the performance of\ntransformers-based models [33]. To demonstrate its im-\npact on VTN, we apply extensive data augmentation as sug-\ngested in DeiT [33] and RandAugment [6]. Table 3 shows\nthat our method reaches 79.8% top-1 accuracy, a 1.2% im-\nprovement vs. the same model trained without such aug-\nmentations. Training with augmentations requires 10 more\nepochs but didn’t impact the training wall runtime.\nFinal inference computational complexity. Finally, we\nexamine what is the ﬁnal inference computational com-\nplexity for various models by measuring GFLOPs. Al-\nthough other models need to evaluate multiple views to\nreach their highest performance, ViT-B-VTN performs al-\nmost the same for both inference protocols. Table 4 shows a\nsigniﬁcant drop of about 8% when evaluating the SlowFast-\n8X8-R50 model using the full video approach. In contrast,\nViT-B-VTN maintains the same performance while requir-\ning, end-to-end, fewer GFLOPs at inference.\n5.2. Experiments on Moments in Time\nThe Moments in Time (MiT) dataset is a large-scale col-\nlection of short (3 seconds) videos [24]. MiT is a chal-\nlenging dataset, with state-of-the-art results just above 34%\ntop-1 accuracy [27]. In this work, we use MiT-v2, con-\nsisting of 727,305 training videos and 30,500 validation\nvideos. Each video is labeled with one of 305 classes of\ndynamic events. Although previous studies worked on MiT-\nv1 (802,264 training videos, 33,900 validation videos, 339\nclasses), this dataset is no longer available. In Table 5, we\nshow the results of various models on MiT-v1 and MiT-\nv2. Since the relation between v1 and v2 in terms of per-\nformance was not established and thus unknown, we also\ntrained our implementation of NL I3D on MiT-v2 using\nRGB inputs and achieved comparable results to those of\nI3D (RGB+OF) published on MiT-v1 [24]. Furthermore,\nViT-B-VTN achieves the highest top-1 accuracy on MiT-v2\nwhile using only RGB frames as input.\n6. Conclusion\nWe presented a modular transformer-based framework\nfor video recognition tasks. Our approach introduces an\nefﬁcient way to evaluate videos at scale, both in terms of\ncomputational resources and wall runtime. It allows full\nvideo processing during test time, making it more suitable\nfor dealing with long videos. Although current video clas-\nsiﬁcation benchmarks are not ideal for testing long-term\nvideo processing ability, hopefully, in the future, when such\ndatasets become available, models like VTN will show even\nlarger improvements compared to 3D ConvNets.\nAcknowledgements. We thank Ross Girshick for provid-\ning valuable feedback on this manuscript and for helpful\nsuggestions on several experiments.\nReferences\n[1] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\n3https://github.com/zhoubolei/moments_models\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020.\n[3] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6299–6308, 2017.\n[4] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Sey-\nbold, David A Ross, Jia Deng, and Rahul Sukthankar. Re-\nthinking the faster r-cnn architecture for temporal action lo-\ncalization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1130–1139,\n2018.\n[5] R Christoph and Feichtenhofer Axel Pinz. Spatiotemporal\nresidual networks for video action recognition. Advances in\nNeural Information Processing Systems , pages 3468–3476,\n2016.\n[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702–703, 2020.\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota, June 2019. Associa-\ntion for Computational Linguistics.\n[9] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko,\nand Trevor Darrell. Long-term recurrent convolutional net-\nworks for visual recognition and description. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2625–2634, 2015.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021.\n[11] Haoqi Fan, Yanghao Li, Bo Xiong, Wan-Yen Lo, and\nChristoph Feichtenhofer. Pyslowfast. https://github.\ncom/facebookresearch/slowfast, 2020.\n[12] Christoph Feichtenhofer. X3d: Expanding architectures for\nefﬁcient video recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 203–213, 2020.\n[13] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6202–6211, 2019.\n[14] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 244–253, 2019.\n[15] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015.\n[16] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n580–587, 2014.\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961–2969, 2017.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[19] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-\ntional neural networks for human action recognition. IEEE\ntransactions on pattern analysis and machine intelligence ,\n35(1):221–231, 2012.\n[20] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017.\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. Advances in neural information processing systems ,\n25:1097–1105, 2012.\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\n[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431–3440, 2015.\n[24] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-\nmakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown,\nQuanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments\nin time dataset: one million videos for event understanding.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 42(2):502–508, 2019.\n[25] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017.\n[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: towards real-time object detection with region\nproposal networks. IEEE transactions on pattern analysis\nand machine intelligence, 39(6):1137–1149, 2016.\n[27] Michael S. Ryoo, AJ Piergiovanni, Mingxing Tan, and\nAnelia Angelova. Assemblenet: Searching for multi-stream\nneural connectivity in video architectures. In International\nConference on Learning Representations, 2020.\n[28] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A uniﬁed embedding for face recognition and clus-\ntering. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 815–823, 2015.\n[29] Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj\nGoswami, Matt Feiszli, and Lorenzo Torresani. Only time\ncan tell: Discovering temporal data for temporal modeling.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 535–544, 2021.\n[30] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InInter-\nnational Conference on Learning Representations, 2015.\n[31] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior\nWolf. Deepface: Closing the gap to human-level perfor-\nmance in face veriﬁcation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n1701–1708, 2014.\n[32] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114. PMLR,\n2019.\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020.\n[34] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. InProceedings of the IEEE inter-\nnational conference on computer vision , pages 4489–4497,\n2015.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors,Advances in Neural Infor-\nmation Processing Systems , volume 30, pages 5998–6008.\nCurran Associates, Inc., 2017.\n[36] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-\nworks: Towards good practices for deep action recognition.\nIn European conference on computer vision , pages 20–36.\nSpringer, 2016.\n[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018.\n[38] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-\nend video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020.\n[39] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-\ning He, Philipp Krahenbuhl, and Ross Girshick. Long-term\nfeature banks for detailed video understanding. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 284–293, 2019.\n[40] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 8739–\n8748, 2018.\nAppendix\nA. Additional Qualitative Results\n(a) Label: Tai chi. Prediction: Tai chi.\n(b) Label: Chopping wood. Prediction: Chopping wood.\n(c) Label: Archery. Prediction: Archery.\n(d) Label: Throwing discus. Prediction: Flying kite.\n(e) Label: Surﬁng water. Prediction: Parasailing.\nFigure 6. Additional qualitative examples of why attention matters. Similar to Fig. 3, we illustrate the [CLS] token weights of the ﬁrst\nattention layer. We show some examples for successful predictions (a, b, c) and some of the failure modes of our approach (d, e)."
}