{
  "title": "Users do not trust recommendations from a large language model more than AI-sourced snippets",
  "url": "https://openalex.org/W4403236652",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2903585633",
      "name": "Melanie J. McGrath",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2104813500",
      "name": "Patrick S Cooper",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A1697611169",
      "name": "Andreas Duenser",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2903585633",
      "name": "Melanie J. McGrath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104813500",
      "name": "Patrick S Cooper",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1697611169",
      "name": "Andreas Duenser",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4389134268",
    "https://openalex.org/W1976862272",
    "https://openalex.org/W2143074722",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2044140042",
    "https://openalex.org/W3121596465",
    "https://openalex.org/W4243342770",
    "https://openalex.org/W1936883342",
    "https://openalex.org/W2779206865",
    "https://openalex.org/W2063052894",
    "https://openalex.org/W6779385450"
  ],
  "abstract": "Background The ability of large language models to generate general purpose natural language represents a significant step forward in creating systems able to augment a range of human endeavors. However, concerns have been raised about the potential for misplaced trust in the potentially hallucinatory outputs of these models. Objectives The study reported in this paper is a preliminary exploration of whether trust in the content of output generated by an LLM may be inflated in relation to other forms of ecologically valid, AI-sourced information. Method Participants were presented with a series of general knowledge questions and a recommended answer from an AI-assistant that had either been generated by an ChatGPT-3 or sourced by Google’s AI-powered featured snippets function. We also systematically varied whether the AI-assistant’s advice was accurate or inaccurate. Results Trust and reliance in LLM-generated recommendations were not significantly higher than that of recommendations from a non-LLM source. While accuracy of the recommendations resulted in a significant reduction in trust, this did not differ significantly by AI-application. Conclusion Using three predefined general knowledge tasks and fixed recommendation sets from the AI-assistant, we did not find evidence that trust in LLM-generated output is artificially inflated, or that people are more likely to miscalibrate their trust in this novel technology than another commonly drawn on form of AI-sourced information.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6196421384811401
    },
    {
      "name": "World Wide Web",
      "score": 0.4714718163013458
    },
    {
      "name": "Language model",
      "score": 0.426928848028183
    },
    {
      "name": "Information retrieval",
      "score": 0.3701644837856293
    },
    {
      "name": "Data science",
      "score": 0.3520488440990448
    },
    {
      "name": "Natural language processing",
      "score": 0.3179819583892822
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    }
  ]
}