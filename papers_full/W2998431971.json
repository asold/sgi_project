{
  "title": "An Iterative Polishing Framework Based on Quality Aware Masked Language Model for Chinese Poetry Generation",
  "url": "https://openalex.org/W2998431971",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2096913926",
      "name": "Liming Deng",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2095685898",
      "name": "Jie Wang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2228408486",
      "name": "Hang-Ming Liang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098099518",
      "name": "Hui Chen",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108751901",
      "name": "Zhiqiang Xie",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2123513742",
      "name": "Bojin Zhuang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2000707899",
      "name": "Shaojun Wang",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2047540789",
      "name": "Jing Xiao",
      "affiliations": [
        "Ping An (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2096913926",
      "name": "Liming Deng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095685898",
      "name": "Jie Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228408486",
      "name": "Hang-Ming Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098099518",
      "name": "Hui Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123513742",
      "name": "Bojin Zhuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000707899",
      "name": "Shaojun Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W1992626852",
    "https://openalex.org/W6731121046",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W12732426",
    "https://openalex.org/W2889924956",
    "https://openalex.org/W2146053064",
    "https://openalex.org/W6631501603",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2474839258",
    "https://openalex.org/W2116787772",
    "https://openalex.org/W6605869828",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2769117444",
    "https://openalex.org/W2337912830",
    "https://openalex.org/W2115221470",
    "https://openalex.org/W2026189632",
    "https://openalex.org/W2950272601",
    "https://openalex.org/W4288562606",
    "https://openalex.org/W197120736",
    "https://openalex.org/W2563845258",
    "https://openalex.org/W4210984920",
    "https://openalex.org/W2494229718",
    "https://openalex.org/W4295803813",
    "https://openalex.org/W2963227255",
    "https://openalex.org/W4255139273",
    "https://openalex.org/W2950909321",
    "https://openalex.org/W2572589325",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963558617",
    "https://openalex.org/W2807738734",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1525595230"
  ],
  "abstract": "Owing to its unique literal and aesthetical characteristics, automatic generation of Chinese poetry is still challenging in Artificial Intelligence, which can hardly be straightforwardly realized by end-to-end methods. In this paper, we propose a novel iterative polishing framework for highly qualified Chinese poetry generation. In the first stage, an encoder-decoder structure is utilized to generate a poem draft. Afterwards, our proposed Quality-Aware Masked Language Model (QA-MLM) is employed to polish the draft towards higher quality in terms of linguistics and literalness. Based on a multi-task learning scheme, QA-MLM is able to determine whether polishing is needed based on the poem draft. Furthermore, QA-MLM is able to localize improper characters of the poem draft and substitute with newly predicted ones accordingly. Benefited from the masked language model structure, QA-MLM incorporates global context information into the polishing process, which can obtain more appropriate polishing results than the unidirectional sequential decoding. Moreover, the iterative polishing process will be terminated automatically when QA-MLM regards the processed poem as a qualified one. Both human and automatic evaluation have been conducted, and the results demonstrate that our approach is effective to improve the performance of encoder-decoder structure.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nAn Iterative Polishing Framework Based on Quality\nAware Masked Language Model for Chinese Poetry Generation\nLiming Deng,1 Jie Wang,1 Hangming Liang,1 Hui Chen,1\nZhiqiang Xie,3∗ Bojin Zhuang,1 Shaojun Wang,1 Jing Xiao2\n1Ping An Technology\n2Ping An Insurance (Group) Company of China\n3University of Science and Technology of China\ndengliming777@pingan.com.cn, photonicsjay@163.com\nAbstract\nOwing to its unique literal and aesthetical characteristics, au-\ntomatic generation of Chinese poetry is still challenging in\nArtiﬁcial Intelligence, which can hardly be straightforwardly\nrealized by end-to-end methods. In this paper, we propose a\nnovel iterative polishing framework for highly qualiﬁed Chi-\nnese poetry generation. In the ﬁrst stage, an encoder-decoder\nstructure is utilized to generate a poem draft. Afterwards,\nour proposed Quality-Aware Masked Language Model (QA-\nMLM) is employed to polish the draft towards higher quality\nin terms of linguistics and literalness. Based on a multi-task\nlearning scheme,QA-MLM is able to determine whether pol-\nishing is needed based on the poem draft. Furthermore,QA-\nMLM is able to localize improper characters of the poem\ndraft and substitute with newly predicted ones accordingly.\nBeneﬁted from the masked language model structure, QA-\nMLM incorporates global context information into the pol-\nishing process, which can obtain more appropriate polishing\nresults than the unidirectional sequential decoding. Moreover,\nthe iterative polishing process will be terminated automati-\ncally whenQA-MLM regards the processed poem as a qual-\niﬁed one. Both human and automatic evaluation have been\nconducted, and the results demonstrate that our approach\nis effective to improve the performance of encoder-decoder\nstructure.\nIntroduction\nChinese Poetry, originated from people’s production and\nlife, has a long history. The poetry is developed from few\ncharacters, vague rules to some ﬁxed characters and lines\nwith stable rules and forms. The rules like tonal pattern,\nrhyme scheme lead to poems easy to be read and remem-\nbered. The great poems, which touch millions of people at\nheart across the space and time, should unify the concise\nform, reﬁned language and rich content together to guaran-\ntee the long-term prosperity. Writing great poems are not\neasy, which require strong desire for poets to express their\nfeelings, views or thoughts and then to choose characters\nand build sentence carefully.\n∗This work was done when Zhiqiang Xie was at Ping An Tech-\nnology\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nPoets are always regarded as genius with great talents and\nwell trained in writing poems. It is hard to write a poem\nfor ordinary people, let alone to computers. Although many\nworks (Gerv´as 2001; Ghazvininejad et al. 2016; Yi et al.\n2018; Li et al. 2018) have been conducted for automatic po-\netry generation and poetic rules and forms can be learned\npartially, the large gaps remain in the meaningfulness and\ncoherence of generated poems.\nIn this paper, we focus on the automatic Chinese poetry\ngeneration and aim to ﬁll these gaps. We notice that poets\nwould ﬁrst write a poem draft and then polish the draft many\ntimes to a perfect one. There is a popular story about pol-\nishing poem by Dao Jia, a famous poet in Tang Dynasty,\nwho inﬂuences many later poets in polishing their poems\nintensively. Motivated by the writing poem process of po-\nets, we aim to imitate this process and improve the coher-\nence and meaningfulness of primitive poems. However, it is\nchallenging for computer algorithms to automatically polish\nthe poem draft to an excellent one. The computer algorithms\nare unable to choose the characters and sentences like poets\nwith intuition and comprehensive understanding of the char-\nacters, which are only good at calculating the probability of\ncharacters and picking up ones with maximum probability\nfrom vocabulary. There are three key issues to be addressed\nfor the polishing framework.\n• Whether the text need to be polished, and when should we\nstop the iterative polishing process?\n• Which characters in the text are improper and need to be\nreplaced with better ones?\n• How to obtain the better ones?\nTo address these key issues and further improve the qual-\nity of generated poem, we propose a Quality-Aware Masked\nLanguage Model (QA-MLM) to implement an iterative pol-\nishing process. To the best of our knowledge, this is the ﬁrst\nwork to solve the three key issues in polishing framework\nwith one elegant model.\nOur idea originates from the BERT (Devlin et al. 2018)\nwith two-task learning schema, and we modify the tasks to\naware of text quality and further obtain appropriate charac-\nters to replace the low quality characters in the text. With\nthese two tasks, we can polish the generated poem draft it-\n7643\neratively, and the polishing process will be terminated auto-\nmatically. The main contributions of this paper are summa-\nrized as follows:\n• Our proposed model QA-MLM, a novel application of\nBERT for poem reﬁnement, which can judge the quality\nof poem and polish the bad characters in the poem iter-\natively. The polish process will be terminated automati-\ncally until the polished poem is regarded as a qualiﬁed\none by our model.\n• The QA-MLM model can obtain high quality characters\nby incorporating both left and right context information,\nand overcomes the weakness of the unidirectional decod-\ning that only consider one side context information for the\nnext character prediction.\n• A two-stage poem generation has been proposed to\nstrengthen the meaningfulness and coherence of gener-\nated poems. On the ﬁrst stage, the encoder-decoder struc-\nture has been utilized to generate the poem draft. Specif-\nically, the pre-trained BERT and the transformer de-\ncoder has been utilized for poem draft generation. The\npoem draft is further polished by our proposedQA-MLM\nmodel on the second stage.\nRelated Work\nAutomatic poetry generation has been investigated for\ndecades. The early work focus on improving the grammat-\nical rules and complying with poetic forms with template-\nbased methods (Gerv´as 2001; Wu, Tosa, and Nakatsu 2009)\nand evolution algorithms (Manurung 2004; Zhou, Y ou, and\nDing 2010). The statistical machine translation methods\n(He, Zhou, and Jiang 2012) and text summarization meth-\nods (Y an et al. 2013) have also been utilized for generating\nmore natural and ﬂexible poems.\nAs neural network demonstrates powerful ability for nat-\nural language representation (Bengio et al. 2003; Goldberg\n2017), different neural network structures have been uti-\nlized for poem generation and shown great advances. The\nmain structures are developed from vanilla recurrent neu-\nral network (Zhang and Lapata 2014) to bi-directional long\nshort term memory network and bi-directional gated recur-\nrent unit network (Wang et al. 2016; Yi, Li, and Sun 2017).\nThe poem generation is widely interpreted as a sequence-to-\nsequence problem, which utilize the encoder-decoder frame-\nwork to encode the previous sequence and generate the later\nsequence with the decoder (Wang, Luo, and Wang 2016;\nYi, Li, and Sun 2017). To strengthen the relation between\nthe encoder and decoder, the attention mechanism has been\nincorporated for poem generation (Wang, Luo, and Wang\n2016; Zhang et al. 2017; Yi, Li, and Sun 2017). Besides,\nsome tricks like working memory mechanism and salient-\nclue mechanism (Yi et al. 2018; Yi, Li, and Sun 2018) have\nbeen proposed to improve the coherence in meanings and\ntopics for poem generation. Recently, the conditional varia-\ntional autoencoder (C-V AE) has been utilized to generate the\npoem (Y ang et al. 2017; Li et al. 2018). The C-V AE can ob-\ntain high quality poem with topic coherence to some extent.\nSome bad cases are also reported by (Li et al. 2018).\nAll the previous methods are generating the poem directly\nwithout any reﬁnement. The most similar work to our pa-\nper is i,poet (Y an 2016), which has implemented an polish-\ning framework via encoding the writing intents repetitively\nto rewrite the poem. This work empirically polishes all the\ncharacters that generated at previous iterative step, and as-\nsumes that the further encoding of the writing intents would\nenhance the theme consistency. Another similar work is the\nDeliberation Network(Xia et al. 2017), which has utilized a\nsecond decoder to generate the ﬁnal sequence with the addi-\ntional input of sequence that generated by the ﬁrst decoder.\nFollowed by theDeliberation Network, a more recent work\n(Zhang et al. 2019) has employed transformer decoder to\nimplement the two-decoder polishing process for text sum-\nmarization. All these methods fail to sense the text quality\nand regard rewriting the whole sequence as polishing pro-\ncess, which are inefﬁciency and may replace qualiﬁed char-\nacters with low-quality ones. Besides, the polishing process\nin these methods are heavily coupled with the initial draft\ngeneration process, which refers to not only the generated\ndrafts but also the additional information that has been uti-\nlized in the draft generation. Therefore, these polishing pro-\ncess cannot be utilized to polish the text drafts that generated\nseparately by other models.\nBy contrast, our proposed QA-MLM model is different\nto the previous works signiﬁcantly and implements the iter-\natively polishing process like humans. Our model can ﬁrst\naware of the text quality and decide whether the text need\nto be polished. Furthermore, our model can aware of the\nlow-quality characters and pick them up to be polished with\nboth left and right context information. Since our model can\nsense the text quality, the iterative polish step will be ter-\nminated automatically once the polished text has been re-\ngarded as qualiﬁed. Our model only polishes a small part of\nlow-quality characters instead of rewriting the whole text.\nThe polishing process implemented by our model is inde-\npendent to the draft generation process, which can polish\nthe draft generated by other separate model. In this paper,\nwe apply our proposed QA-MLM model for Chinese po-\netry polishing, which can signiﬁcantly improve the quality\nof poem particular in terms of meaningfulness and coher-\nence. We will introduce our approach in the following sec-\ntions.\nModel Design\nOverview\nWe focus on the generation and polishing of quatrain, which\nis a famous kind of Chinese classic poetry with strict con-\nstraints of poetic rules and forms. In general, the quatrain is\nwith four poem lines, the number of characters for each line\nis equal, which is either ﬁve or seven. The tone level and\nrhythm are constrained with speciﬁc patterns (Wang 2002).\nWe follow the text-to-text generation paradigm (Wang et al.\n2016; Li et al. 2018) and generate the poem line by line.\nThe ﬁrst poem line is generated by keywords or topic words,\nand the following lines are generated by the preceding lines\nor their combinations. The key task turns into designing a\nproper model to generate the following lines with given key-\n7644\n  \nKeyword\np1,p2, ... , pi\np1,p2, ... , pi\n, ... ,p end\n, ... ,p end\np1,p2, ... , i , ... ,p end\np1,p2, ... ,p t, ... , pend\nPosition i\nPosition end\n[PRED]\nPoem draft \ngeneration\nIterative polishing\nFigure 1: An overview of our poem generation approach,\nincluding poem draft generation and iterative polishing pro-\ncess. 1⃝ Predict the character position with worst quality,\n2⃝ Mask the worst quality character,3⃝ Predict the masked\ncharacter and update it into the text accordingly,4⃝ Repeat\nthe previous steps until no character needs to be polished.\nwords or preceding lines.\nInspired by the real procedure of writing poems for po-\nets, we generate the poem lines with two stages. The poem\ndraft lines are ﬁrst generated with encoder-decoder frame-\nwork and then polished with our proposedQA-MLM. The\noverall structure of our approach can be shown in Figure 1.\nInput Representation\nOur input representation is similar to the embedding meth-\nods in BERT (Devlin et al. 2018). In addition to sum the to-\nken, segment and position embeddings as the representation,\nwe also add the tonal and rhythm embeddings into the input\nrepresentation. The tone of each character is either Ping (the\nlevel tone) or Ze (the downward tone) (Wang 2002). We en-\ncode the tone with three categories due to some tokens like\ncomma without any tone. The rhythm of last character for\neach poem line will be encoded and we utilize the Thirteen\nRhyme.\n1 With the tone and rhythm are embedded into the\nrepresentation, we can improve the poeticness of generated\npoem signiﬁcantly without sacriﬁce much of the poem qual-\nity. The visualization of our input representation is given in\nFigure 2.\n1Classify the ﬁnal vowels into thirteen categories according to\nthe rhyme table.\n෭ ᆙ ḕ ᅖ ኞ ᔳ ᅶ >6(3@Input\nToken\nEmbeddings\nSegment\nEmbeddings\nPosition\nEmbeddings\nTone\nEmbeddings\nRhyme\nEmbeddings\nE෭ Eᆙ Eḕ Eᅖ Eኞ Eᔳ Eᅶ E>6(3@\nE$ E$ E$ E$ E$ E$ E$ E$\nE\u0014 E\u0015 E\u0016 E\u0017 E\u0018 E\u0019 E\u001a E\u001b\nE= E= E3 E3 E3 E= E3 E1\nE5 E5 E5 E5 E5 E5 E$1 E5\n++++++++\n++++++++\n++++++++\n++++++++\n>&/6@\nE>&/6@\nE$\nE\u0013\nE1\nE5\n+\n+\n+\n+\nFigure 2: The input representation. The input embeddings\nis the sum of the token embeddings, segmentation embed-\ndings, position embeddings and tone embeddings as well\nas rhyme embeddings. TheE\nN and ER represent the token\nwithout tone or no need to embed the rhyme respectively.\nPoem Draft Generation\nThe poem draft can be generated via encoder-decoder struc-\nture. This structure learns the relation between the target se-\nquence t = {t\n1,t2,...,t n} and the source sequence s =\n{s1,s2,...,s m}. The generation probability P(t|s;Θ) can\nbe obtained by Equation (1), where theΘ is model parame-\nters and learned from the sequence pairs(s,t) ∈ (S,T).W e\nmaximize the objective function in Equation (2).\nP(ˆt|s;Θ) =\nn∏\nj=1\nP(ˆtj|t<j,s;Θ) (1)\nLcharacter =\n∑\n(s,t)∈(S,T)\nlogP(ˆt|s;Θ) (2)\nAs shown in Figure 3, the BERT is utilized as the encoder\nto represent the source sequence s with vector h, and the\nrepresentation vectorhis then fed to a two-layer transformer\ndecoder to generate the target sequenceˆt(Devlin et al. 2018;\nZhang et al. 2019). The source sequence can be a keyword\nor poem lines, and the target sequence is the poem line that\nwe want to generate. All or part of previous sequence have\nbeen utilized as source sequence by previous works (Wang\net al. 2016; Yi, Li, and Sun 2017). After carefully consid-\nering the relevance among poem lines and without making\nthe generation system complicated, we build three different\nmodels with the same structure for each poem line genera-\ntion, namely:key2one, one2one, andtwo2one.\nThe key2one model is utilized to generate the ﬁrst poem\nline with the input of keyword. Theone2one model is em-\nployed to generate the second poem line and the fourth poem\nline due to the similar relevance with their preceding poem\nlines. As for generating the third poem line, we consider\nboth the ﬁrst and second poem lines via thetwo2one model.\nThe whole poem draft generation process can be visualized\nin the upper part of Figure 1.\nIterative Polishing\nThere is an obvious deﬁciency for the aforementioned\nencoder-decoder method (Xia et al. 2017). During the de-\n7645\n(PEHGGLQJV\u0003\u000bWRNHQ\u000f\u0003VHJPHQW\u000f\u0003\nSRVLWLRQ\u000f\u0003WRQH\u0003OHYHO\u000f\u0003UK\\PH\f\ns1s1 s2s2 … snsn\n%(57\u0003\u000b(QFRGHU\f\n…\nh1 h2 hn…\n7UDQVIRUPHU\u0003\u000b'HFRGHU\f\u0003\u0003\u0003\u0003\u0003[\u0003\u0015\nt1t1 t2t2 t3t3 … tntn\n$WWHQWLRQ\n…\nFigure 3: The sequence-to-sequence generation with BERT and Transformer decoder.\ncoding, the character generated sequentially is affected by\nthe previous characters and ignores the inﬂuence of subse-\nquent characters, as demonstrated in Equation (1). There-\nfore, an iterative polishing framework which can utilize both\nprevious and subsequent context information to polish cen-\nter character is critical to obtain a semantic consistency and\ncoherence poem.\nWe propose a quality aware masked language model (QA-\nMLM) to implement the iterative polishing process. The\nquality aware reﬂects the model capability of distinguish-\ning the character quality and deciding which character need\nto be polished. Besides, our model can decide whether the\ntext need to be polished and when should we stop the iter-\native polishing process. The masked language model is to\nmask the low quality character ﬁrst and then predict another\ncharacter by referring the two-side context information. The\npredicted character is assumed to be with better semantic\nconsistency and coherence due to the consideration of both\nthe previous and subsequent information. Inspired by (De-\nvlin et al. 2018), we train theQA-MLM with two predic-\ntion tasks and apply it to polish the generated poem draft, as\ndescribed in the following subsections.\nPrediction Tasks In order to provide reasonable solutions\nto the aforementioned three key issues in polishing frame-\nwork, we design a quality prediction task and a masked lan-\nguage model task based on BERT (Devlin et al. 2018). Un-\nlike the BERT, which learns from multi-task for context rep-\nresentation, our proposed QA-MLM predicts the positions\nof low quality characters and then replaces the low qual-\nity characters with newly predicted ones for text reﬁnement.\nThe structure of QA-MLM can be visualized in Figure 4.\nThe quality prediction task is to predict the character posi-\ntions that the characters are with low quality. We regard the\noriginal poem lines from poetry corpus are the gold stan-\ndards, and any changing of the original poem lines would\nhurt the quality of the poem. Therefore, we randomly re-\nplace the characters in original poem line with random to-\nkens. We denote s\ng as the original poem line, and the sc\nas the changed poem line. The positions that have been\nreplaced can be denoted as p =[ pi1,pi2,...,p ir], where\nir < n, and the real characters that have been replaced\nare si =[ si1,si2,...,s ir]. The number of replaced posi-\ntions r reﬂects the learning capacity of theQA-MLM that\nhow many mismatched characters can be learned. A largerr\nallows a more powerful model for polishing the bad poem\nBERT\n[CLS] s1s1 snsn…\nE[CLS] E1 Ei En\n[CLS] ……\n…\n… …\nPosition prediction\nBERT\n[CLS] …\nE[CLS] E1 Ei En\n[CLS] ……\n…\n……\nToken prediction\ns1s1 MASK snsn\nˆtit1 tn\nsi\nt1 ti tn\nMask character\nˆpi\nˆpi\nQuality Aware\nMasked Language Model\nFigure 4: The structure of quality aware masked language\nmodel.\nlines intensively. However, the large r would lead to the\nchanged poem lines very bad and increases the training dif-\nﬁcult. It should be careful to choose an appropriater con-\nsidering both the model capacity and learning quality.\nIn this work, each poem line is randomly replaced2 ac-\ncording to the following rules:\n• 60% of the time: replace one character with ran-\ndom token, eg., the original poem line sg =\n[s1,s2,s3,s4,s5,s6,s7] (n =7 , for example) is changed\nto sc =[ s1,s2,si1,s4,s5,s6,s7] and the position la-\nbel is p =3 , then the masked poem line is sm =\n[s1,s2,[MASK],s4,s5,s6,s7].\n2Refers to both the positions and the tokens are randomly se-\nlected\n7646\n• 20% of the time: replace two characters with\nrandom tokens, eg., the original poem line\nsg =[ s1,s2,s3,s4,s5,s6,s7] is changed to\nsc =[ s1,si1,s3,s4,s5,si2,s7] and the position\nlabel is p =[ 2 ,6], then the masked poem line is\nsm =[ s1,[MASK],s3,s4,s5,[MASK],s7].\n• 20% of the time: keep the poem line unchanged, in this\nsituation we set the position label as 0, namelysg = sc\nand p =0 , there is no need to mask the poem linesc and\nno need to polish poem linesc, which inferspend =0 .\nLearning and Inference We can jointly learning the\naforementioned two tasks to train QA-MLM by minimiz-\ning the following loss functions:\nLossq = −\n∑\nsc∈Sc\nir∑\nj=i1\nlogP(ˆpj = pj|sc;Θ) (3)\nLossm = −\n∑\n(sm,sg)∈(Sm,Sg)\nir∑\nj=i1\nlogP(ˆsm,j = sg,j |sm,̸=j;Θ) (4)\nLosstotal =Lossm +λLossq (5)\nAfter learning our proposedQA-MLM over the constructed\npoem corpus (Sg,Sc,Sm), we can utilize theQA-MLM to\npolish the poem draft. At the beginning of polishing process,\nthe QA-MLM predicts the character position that with worst\ncharacter quality. If the predicted positionp\ni is equal topend\n(in our settingpend =0 ), which means that all characters in\nthe draft are qualiﬁed enough and no more any polishing,\notherwise the character identiﬁed with the worst quality will\nbe masked in the draft, and the masked draft will be fur-\nther utilized for a better character prediction viaQA-MLM\nmodel. The predicted character is regarded as more appro-\npriate than the masked character due to the incorporation\nof two-side context information during the prediction. Thus,\nwe replace the masked character in sequence draft with the\npredicted character, and one polishing step is ﬁnished. By re-\npeating the above polishing step, the sequence draft can be\niteratively polished many times until the end indicatorp\nend\nis predicted. At this time, the iterative polishing process will\nbe terminated automatically, and the sequence draft has been\npolished completely. The iterative polishing process can be\nshown in Algorithm 1.\nAlgorithm 1 :Iterative Polishing withQA-MLM\n1: Perform iterative polishing on sequence draft sd =\n[s1,s2,...,si,...,sn]\n2: Predict bad character positionpi = QA(sd)\n3: while pi ̸= pend do\n4: si ←[MASK] ,sd =[ s1,s2,...,[MASK],...,sn]\n5: Predict the new characterˆsi = MLM(sd)\n6: sd ← [s1,s2,...,ˆsi,...,sn]\n7: pi ← QA(sd)\n8: return polished sd\nThe sequence draft can be a poem line or several poem\nlines and even a whole poem, our approach is capable of pol-\nishing all of them. In this work, we polish the whole poem\ntogether, which incorporates the whole context information\nfor inappropriate character prediction, and the inappropriate\ncharacters will be replaced with highly qualiﬁed characters\nto obtain semantic consistency and coherence.\nExperiments and Evaluations\nExperimental Setup\nIn this paper, we concentrate on the generation of Chinese\nquatrain with seven ﬁxed characters for each line. Our po-\netry corpus is consist of poems from Tang Dynasty, Song\nDynasty, Yuan Dynasty, Ming Dynasty and Qing Dynasty.\nAbout 130,525 poems with total 905,790 number of poem\nlines are ﬁltered from the poetry corpus. Each ﬁltered poem\ncontain four or multiple of four poem lines, and each poem\nline with seven characters. These poems are randomly split\ninto three part for model training (90%), validation (5%)\nand testing (5%). Three models ( key2one, one2one, and\ntwo2one) trained by different sequence-to-sequence pairs\nare utilized to generate the poem draft lines.\nThe BERT is selected as the encoder with 12 layers and\ninitialized with the parameters pre-trained by (Devlin et al.\n2018). The 2-layer transformer decoder is selected for the\npoem generation. After the poem draft has been generated,\nthe QA-MLM is proposed for the polishing. The aware of\npoem quality is implemented by the quality task to predict\nthe position character that with worst semantic quality. In ad-\ndition to the current total 28 positions for the seven-character\nquatrain, an end positionp\nend =0 is added to indicate the\nend of iterative polishing. The character located by the qual-\nity prediction task will be masked and then replaced with\nnewly predicted one by masked language model task. The\nquality prediction task and the masked language model task\nare based on the 12-layer BERT and learned jointly.\nThe conventional RNN encoder-decoder structure with at-\ntention mechanism (AS2S) (Wang et al. 2016) and a more\nrecent work CV AE-D(Li et al. 2018) are selected as the\nbaselines for poem draft generation. Besides, we also im-\nplement a more powerful encoder-decoder structure with\npre-trained BERT and transformer decoder (B&T) for poem\ndraft generation. The poem drafts are generated with the in-\nput of keywords or writing intents, and we follow the key-\nwords extraction method adopted by AS2S (Wang et al.\n2016), which cuts the poem lines into several word segmen-\ntations by Jieba (Sun 2012) and then utilizes theTextRank\n(Mihalcea and Tarau 2004) method to select keyword with\nthe highest score. The poems generated by the aforemen-\ntioned three models are further polished with the proposed\nQA-MLM. Both the automatical evaluation criteria and hu-\nman evaluations have been conducted. The following sub-\nsection will introduce the detail about the evaluations.\nEvaluation Metrics\nIt is difﬁcult for computer to estimate the quality of poem.\nTherefore, we utilize both automatic evaluation metrics and\nhuman judgements for model comparisons. The automatic\nevaluation metrics including BLEU (Papineni et al. 2002),\n7647\nTable 1: The BLEU score results on different generated poem line with same extracted keyword or previous poem lines.BL-1\nand BL-2 are the BLEU scores on unigrams and bigrams.\nModel key →1 1 → 2 1&2 → 3 3 → 4 Average\nBL-1 BL-2 BL-1 BL-2 BL-1 BL-2 BL-1 BL-2 BL-1 BL-2\nAS2S 0.072 0.047 0.016 0.005 0.019 0.006 0.021 0.007 0.032 0.016\nAS2S-P 0.074 0.047 0.026 0.009 0.030 0.010 0.036 0.012 0.042 0.020\nCV AE-D 0.109 0.059 0.013 0.005 0.015 0.005 0.019 0.006 0.039 0.019\nCV AE-D-P 0.105 0.057 0.015 0.005 0.016 0.006 0.021 0.007 0.039 0.019\nB&T 0.102 0.050 0.028 0.010 0.036 0.014 0.035 0.013 0.050 0.022\nB&T-P 0.100 0.050 0.028 0.009 0.034 0.013 0.033 0.012 0.049 0.021\nTable 2: The evaluation results.Sim12 refers to the similarity between ﬁrst poem line and second poem line;Sim34 refers to\nthe similarity between the third poem line and the fourth poem line;Sim2L refers to the similarity between ﬁrst two poem lines\nand the last two poem lines;TA. and RA. are the tone level predicted accuracy and the rhythm predicted accuracy respectively;\nCon., Flu., Mea., andPoe. represent theConsistency, Fluency, Meaningfulness and Poeticness respectively.\nModel Automatic Evaluation Human Evaluation\nSim12 Sim34 Sim2L TA. RA. Con. Flu. Mea. Poe.\nAS2S 0.479 0.487 0.648 0.539 0.121 2.46 2.37 2.35 2.28\nAS2S-P 0.484 0.495 0.650 0.517 0.124 2.64 2.63 2.59 2.59\nCV AE-D 0.494 0.500 0.651 0.521 0.086 2.62 2.50 2.55 2.42\nCV AE-D-P 0.499 0.507 0.653 0.524 0.091 2.75 2.72 2.74 2.64\nB&T 0.500 0.516 0.640 0.976 0.956 3.01 2.99 3.06 2.88\nB&T-P 0.502 0.519 0.642 0.962 0.841 3.14 3.19 3.24 3.09\nSimilarity (Wieting et al. 2015), tone accuracy and rhythm\naccuracy are adopted in this paper.\nThe BLEU is designed for machine translation and also\nwidely adopted by many previous works (Zhang and Lap-\nata 2014; Li et al. 2018) in poem generation. The BLEU is\nto measure the overlapping of characters between the gener-\nated sentence and the referred sentence. Unlike the machine\ntranslation, the generated sentence can be signiﬁcantly dif-\nferent from the referred sentence but also regarded as high\nquality by human judgements. The poem generation is more\nrelated to creativity and the generated poem may far away\nfrom the referred poem, which may lead to the comparison\nof BLEU score is trivial. Therefore, we compare the BLEU\nscore on one sentence instead of the whole poem. Each sen-\ntence is generated by different approaches with the same\nkeyword or sentence input.\nThe Similarity is aimed to automatically measure the co-\nherency or consistency among poem lines. The embedding\nof characters can partially reﬂect the similarities and we ac-\ncumulate the embeddings of all the characters for each poem\nline for sentence-level embeddings (Wieting et al. 2015).\nThen, the similarity between two poem lines can be calcu-\nlated by the cosine function on the sentence-level embed-\ndings.\nThe Tone Accuracyand Rhythm Accuracyare also em-\nployed for the evaluation. The tone accuracy is the percent-\nage that the tone level (Ping or Ze) is predicted correct to all\nthe generated samples, and the rhythm accuracy is similar\nabout the last character of each poem line that the rhyme is\npredicted correct.\nThe Human Evaluation is inevitable for poem evalua-\ntion, which is more reliable and credible than the automatic\nevaluation metrics. Twenty well educated annotators are in-\nvited to evaluate the generated poems in four dimensions,\nnamely Consistency, Fluency, Meaningfulness and Poetic-\nness (Zhang and Lapata 2014; Li et al. 2018). Each dimen-\nsion is rated using the 1 to 5 scale to represent from bad\nquality to excellent. Each model generates one thousand po-\nems and the poems are divided equally into twenty pieces.\nTo reduce the individual scoring bias, the poems rated by\neach participant are from all models, but the participant has\nno information about the model that each poem belongs to.\nTherefore, we can obtain 6000 (20×6×50) poem ratings.\nResults and Discussions\nThe BLEU scores are compared in Table 1. The compared\nmodels are shown in the ﬁrst column of the table, where\nthe sufﬁx -P indicates that the poems generated by previ-\nous models have been polished by QA-MLM. The key-\nwords and poem lines are extracted from test dataset with\none thousand poems. In general, the BLEU scores ofCV AE-\nD are higher than AS2S but lower than B&T, which par-\ntially reﬂects the generation performance of these models.\nThe polishing process improves the BLEU scores forAS2S\nmodel while has no obvious improvement or even a bit hurt\nof BLEU scores for CV AE-Dand B&T. This is probably\ndue to the creativity and diversity properties in poem gener-\nation. The quality of generated poem is not proportional to\nthe BLEU score when the BLEU score is comparative high.\nThe BLEU score should be referred conservative during the\nquality evaluation for poetry generation.\nThe other automatic evaluation results and human score\nresults can be found in Table 2. These evaluation results are\nalso based on one thousand poems from test dataset. The\n7648\n\u0013\u0015\u000e\b\u0014\u0005\u000f\u0002\nIn the lonely spring breeze, birds are violently \nﬂuttering their wings,\n\u0003\u0003\n\b\u0016\u000b\t\u0017\f\u001a\nThe autumn wind blows the rain with delicate fragrance, \noverﬂowing the courtyard lightly.\n\u0003\u0003\u0012ᦩ\rԅ\u0011\u0004\u0007\u001b\nYou’d like to know the old things \nin order to come back late\u000f\n\u0001\u0001\u0006\u0003\u0018\f\u0010ᰜ\u0019\u001a\nOnly the ﬂowers and the happiness of ﬁshing.\n\u0013\u0015\u000e\b\u0014\u0005\u000f\u0002\nIn the lonely spring breeze, birds are violently \nﬂuttering their wings,\n\u0001\u0001\n\b\u0016\u000b\t\u0017\f\u001a\nThe autumn wind blows the rain with delicate fragrance, \noverﬂowing the courtyard lightly.\n\u0003\n\u0003\u0001\u0012Ꭳ\rࢵ\u001b\u0007\u0004\u0011\nYou’d like to know the news from homeland \nwhile come back late \u000f\n\u0001\u0001\u0006\u0003\u0018\f\u0010ռ\u0019\u001a\nOnly the ﬂowers and their fragrance remaining\u0011\nFigure 5: The example of poem draft and the polished poem. The left poem is the poem draft generated byB&T and the right\npoem is the poem iteratively polished byQA-MLM. The iterative polishing is automatically terminated after 3 polish steps.\nkeywords are extracted from these poems and utilized to\ngenerate poem drafts byAS2S, CV AE-Dand B&T. All the\npoem drafts generated by this three encoder-decoder struc-\nture models have been further polished by our proposedQA-\nMLM model. We can ﬁnd that the polish procedure im-\nproves the scores ofSimilarity criteria on all the compared\nencoder-decoder structure models, which demonstrates the\neffectiveness of our proposed QA-MLM for the improve-\nment of sematic coherence and theme consistency. The tone\nlevel and rhythm forAS2S and CV AE-Dmodels seem ran-\ndomly. By contrast, the accuracy of tone level and rhythm\nfor B&T are signiﬁcantly higher than the other two base-\nlines, which demonstrates the embedding-based method is\neffective to control the tone level and rhythm. The polishing\nframework on the B&T hurts a bit accuracy of tone level\nand rhythm, which sacriﬁces the tone level and rhythm con-\nstraints to obtain better semantic meaning and coherence.\nThe human evaluations are consistent with the automatic\nevaluation metrics. As for the poem draft generators, the\nCV AE-Doutperforms the AS2S on all the quality aspects\nevaluated by our annotators, which is consistent with the\nresults from (Li et al. 2018). However, the poem genera-\ntion performance ofB&T is better thanCV AE-D, which is\nprobability due to the powerful text representation ability of\nBERT or transformer.\nAbove all, our proposedQA-MLM can further improve\nthe qualities (consistency, ﬂuency, meaningfulness and poet-\nicness ) of poems generated by all the three aforementioned\nencoder-decoder structure models, which demonstrates that\nthe quality prediction task is effective to locate the bad char-\nacters and the masked language model task is also effec-\ntive to obtain better predictions when referring to the global\ncontext information. Therefore, the unidirectional sequential\ndecoding deﬁciency of the encoder-decoder structure can be\nlargely saved by our proposedQA-MLM for poetry reﬁne-\nment.\nAlthough the improvements brought by theQA-MLM in\nautomatic evaluation metrics seem trivial, the improvements\nare signiﬁcant in the human evaluation results. It is easy to\nunderstand that the polishing process only update a small\npart of characters, and the improvements will be averaged\non all the characters for automatic evaluation metrics. How-\never, human can understand and notice the signiﬁcant differ-\nence of the changed characters to the whole poem context.\nEven one character changed would lead to a big improve-\nment for poetry quality. We can notice the difference from\nthe example in Figure 5.\nConclusion\nIn this paper, we present an iterative polishing framework\nfor Chinese poetry generation by imitating the real poem\nwriting process. Following the famous encoder-decoder\nparadigm, a pre-trained BERT encoder and a transformer\ndecoder are combined to generate poem drafts. Then, poem\npolishing is accomplished by a multifunctionalQA-MLM,\nwhich can improve poem quality in terms of semantics, syn-\ntactics and literary. Based on the multi-task learning, the\ntrained QA-MLM is able to aware of the poem quality and\nlocate improper characters. Besides, theQA-MLM is capa-\nble of predicting better ones to replace the improper charac-\nters by synthesizing the all-round poem context information.\nMoreover, the QA-MLM will automatically terminate the\niterative polishing process when the polished draft is classi-\nﬁed as qualiﬁed.\nBoth automatic evaluation and human scores demonstrate\nthat our proposed approach is effective in Chinese poetry\ngeneration. Our model can automatically modify prelim-\ninary poems to elegant ones while keeping their original\nintents. Even though our proposed QA-MLM polishing\nframework is concentrated on Chinese poetry generation in\nthis work, this new text reﬁnement approach can be extended\nto other natural language generation areas.\nAcknowledgments\nWe thank Haoshen Fan, Weijing Huang, Mingkuo Ji and\nShaopeng Ma for helpful discussions.\n7649\nReferences\nBengio, Y .; Ducharme, R.; Vincent, P .; and Jauvin, C. 2003.\nA neural probabilistic language model.Journal of machine\nlearning research3(Feb):1137–1155.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nGerv´as, P . 2001. An expert system for the composition\nof formal spanish poetry. Knowledge-Based Systems 14(3-\n4):181–188.\nGhazvininejad, M.; Shi, X.; Choi, Y .; and Knight, K. 2016.\nGenerating topical poetry. InProceedings of the 2016 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 1183–1191.\nGoldberg, Y . 2017. Neural network methods for natural lan-\nguage processing. Synthesis Lectures on Human Language\nTechnologies 10(1):1–309.\nHe, J.; Zhou, M.; and Jiang, L. 2012. Generating chinese\nclassical poems with statistical machine translation models.\nIn AAAI.\nLi, J.; Song, Y .; Zhang, H.; Chen, D.; Shi, S.; Zhao, D.; and\nY an, R. 2018. Generating classical chinese poems via con-\nditional variational autoencoder and adversarial training. In\nProceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, 3890–3900.\nManurung, H. 2004. An evolutionary algorithm approach to\npoetry generation.\nMihalcea, R., and Tarau, P . 2004. Textrank: Bringing order\ninto text. In Proceedings of the 2004 conference on empiri-\ncal methods in natural language processing, 404–411.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. J. 2002.\nBleu: a method for automatic evaluation of machine transla-\ntion. In Proc Meeting of the Association for Computational\nLinguistics.\nSun, J. 2012. ‘jieba’chinese word segmentation tool.\nWang, Z.; He, W.; Wu, H.; Wu, H.; Li, W.; Wang, H.; and\nChen, E. 2016. Chinese poetry generation with planning\nbased neural network.arXiv preprint arXiv:1610.09889.\nWang, Q.; Luo, T.; and Wang, D. 2016. Can machine gen-\nerate traditional chinese poetry? a feigenbaum test. InInter-\nnational Conference on Brain Inspired Cognitive Systems,\n34–46. Springer.\nWang, L. 2002. A summary of rhyming constraints of chi-\nnese poems.\nWieting, J.; Bansal, M.; Gimpel, K.; and Livescu, K. 2015.\nTowards universal paraphrastic sentence embeddings.arXiv\npreprint arXiv:1511.08198.\nWu, X.; Tosa, N.; and Nakatsu, R. 2009. New hitch haiku:\nAn interactive renku poem composition supporting tool ap-\nplied for sightseeing navigation system. In Natkin, S., and\nDupire, J., eds., Entertainment Computing – ICEC 2009 ,\n191–196. Berlin, Heidelberg: Springer Berlin Heidelberg.\nXia, Y .; Tian, F.; Wu, L.; Lin, J.; Qin, T.; Y u, N.; and Liu,\nT.-Y . 2017. Deliberation networks: Sequence generation be-\nyond one-pass decoding. InAdvances in Neural Information\nProcessing Systems, 1784–1794.\nY an, R.; Jiang, H.; Lapata, M.; Lin, S.-D.; Lv, X.; and Li,\nX. 2013. i, poet: Automatic chinese poetry composition\nthrough a generative summarization framework under con-\nstrained optimization. InIJCAI, 2197–2203.\nY an, R. 2016. i, poet: Automatic poetry composition\nthrough recurrent neural networks with iterative polishing\nschema. In IJCAI, 2238–2244.\nY ang, X.; Lin, X.; Suo, S.; and Li, M. 2017. Generating\nthematic chinese poetry with conditional variational autoen-\ncoder. CoRR.\nYi, X.; Sun, M.; Li, R.; and Y ang, Z. 2018. Chinese poetry\ngeneration with a working memory model. arXiv preprint\narXiv:1809.04306.\nYi, X.; Li, R.; and Sun, M. 2017. Generating chinese classi-\ncal poems with rnn encoder-decoder. InChinese Computa-\ntional Linguistics and Natural Language Processing Based\non Naturally Annotated Big Data. Springer. 211–223.\nYi, X.; Li, R.; and Sun, M. 2018. Chinese poetry gen-\neration with a salient-clue mechanism. arXiv preprint\narXiv:1809.04313.\nZhang, X., and Lapata, M. 2014. Chinese poetry gen-\neration with recurrent neural networks. In Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 670–680.\nZhang, J.; Feng, Y .; Wang, D.; Wang, Y .; Abel, A.; Zhang,\nS.; and Zhang, A. 2017. Flexible and creative chinese\npoetry generation using neural memory. arXiv preprint\narXiv:1705.03773.\nZhang, H.; Gong, Y .; Y an, Y .; Duan, N.; Xu, J.; Wang, J.;\nGong, M.; and Zhou, M. 2019. Pretraining-based natural\nlanguage generation for text summarization.arXiv preprint\narXiv:1902.09243.\nZhou, C.-L.; Y ou, W.; and Ding, X. 2010. Genetic algorithm\nand its implementation of automatic generation of chinese\nsongci. Journal of Software21(3):427–437.\n7650",
  "topic": "Polishing",
  "concepts": [
    {
      "name": "Polishing",
      "score": 0.8362672328948975
    },
    {
      "name": "Computer science",
      "score": 0.7094822525978088
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6337653398513794
    },
    {
      "name": "Poetry",
      "score": 0.5885316729545593
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5331308841705322
    },
    {
      "name": "Process (computing)",
      "score": 0.5208335518836975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47228723764419556
    },
    {
      "name": "Natural language processing",
      "score": 0.4567497968673706
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4330666661262512
    },
    {
      "name": "Iterative and incremental development",
      "score": 0.4296574592590332
    },
    {
      "name": "Encoder",
      "score": 0.4214060306549072
    },
    {
      "name": "Engineering drawing",
      "score": 0.35490891337394714
    },
    {
      "name": "Linguistics",
      "score": 0.2748228907585144
    },
    {
      "name": "Engineering",
      "score": 0.166030615568161
    },
    {
      "name": "Programming language",
      "score": 0.11184853315353394
    },
    {
      "name": "Software engineering",
      "score": 0.11163640022277832
    },
    {
      "name": "Mathematics",
      "score": 0.10145431756973267
    },
    {
      "name": "History",
      "score": 0.08565950393676758
    },
    {
      "name": "Mechanical engineering",
      "score": 0.07522028684616089
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}