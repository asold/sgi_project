{
  "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
  "url": "https://openalex.org/W4385571886",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2891250805",
      "name": "Damai Dai",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A2113540864",
      "name": "Yutao Sun",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A2323301275",
      "name": "Yaru Hao",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A2099570760",
      "name": "Shuming Ma",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A2441688708",
      "name": "Zhifang Sui",
      "affiliations": [
        "King Center"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "King Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W4320516905",
    "https://openalex.org/W4287547489",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4297412003",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1988720110",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4311726128",
    "https://openalex.org/W4310509152",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W2325963232",
    "https://openalex.org/W4225920301",
    "https://openalex.org/W2169384417",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4311001646",
    "https://openalex.org/W2114524997"
  ],
  "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at https://aka.ms/icl.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4005‚Äì4019\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nWhy Can GPT Learn In-Context?\nLanguage Models Implicitly Perform Gradient Descent as Meta-Optimizers\nDamai Dai‚Ä†‚àó, Yutao Sun ‚à•‚àó, Li Dong ‚Ä°, Yaru Hao ‚Ä°, Shuming Ma ‚Ä°, Zhifang Sui ‚Ä†, Furu Wei ‚Ä°\n‚Ä†MOE Key Lab of Computational Linguistics, Peking University\n‚à•Tsinghua University ‚Ä°Microsoft Research\n{daidamai,szf}@pku.edu.cn\n{lidong1,fuwei}@microsoft.com\nAbstract\nLarge pretrained language models have shown\nsurprising in-context learning (ICL) ability.\nWith a few demonstration input-label pairs,\nthey can predict the label for an unseen input\nwithout parameter updates. Despite the great\nsuccess in performance, its working mecha-\nnism still remains an open question. In this\npaper, we explain language models as meta-\noptimizers and understand in-context learning\nas implicit finetuning. Theoretically, we fig-\nure out that Transformer attention has a dual\nform of gradient descent. On top of it, we un-\nderstand ICL as follows: GPT first produces\nmeta-gradients according to the demonstration\nexamples, and then these meta-gradients are ap-\nplied to the original GPT to build an ICL model.\nWe comprehensively compare the behaviors of\nin-context learning and explicit finetuning on\nreal tasks to provide empirical evidence that\nsupports our understanding. Experimental re-\nsults show that in-context learning behaves sim-\nilarly to explicit finetuning from multiple per-\nspectives. Inspired by the dual form between\nTransformer attention and gradient descent, we\ndesign a momentum-based attention by analogy\nwith gradient descent with momentum. The im-\nproved performance over vanilla attention fur-\nther supports our understanding from another\nperspective, and more importantly, shows the\npotential to utilize our understanding for fu-\nture model design. The code is available at\nhttps://aka.ms/icl.\n1 Introduction\nIn recent years, large pretrained language models,\nespecially in Transformer-based architectures (e.g.,\nGPT; Brown et al. 2020), have shown strong emer-\ngent in-context learning (ICL) ability (Wei et al.,\n2022; Dong et al., 2023). Different from finetuning\nwhich needs additional parameter updates, ICL just\nneeds several demonstration examples prepended\n‚àóContribution during internship at Microsoft Research.\n(Sentence, ?)\n‚Ä¶ \nDemonstration Examples Query Example\nFeed-Forward Network\nSelf-Attention\n(Sentence1, Answer1) (Sentence2, Answer2)\nMeta-Gradients\nAnswer\n‚Ä¶ GPT\nIn-Context Learning\nFinetuning\nGPT\n(Sentence1, Answer1) \nGPT\n(Sentence2, Answer2) \nBack-Propagation\nForward \nComputation\nùö´ùö´ùëæùëæùêÖùêÖùêÖùêÖ\nùö´ùö´ùëæùëæùêàùêàùêàùêàùêàùêà\nGradients\nDual \nView\nFigure 1: According to the demonstration examples,\nGPT produces meta-gradients for in-context learning\n(ICL) through forward computation. ICL works by ap-\nplying these meta-gradients to the model through at-\ntention. The meta-optimization process of ICL shares\na dual view with finetuning that explicitly updates the\nmodel parameters with back-propagated gradients.\nbefore the query input, and then the model can pre-\ndict labels for unseen inputs. On numerous down-\nstream tasks, large GPT models can achieve sur-\nprising performance, which even exceeds smaller\nmodels with supervised finetuning. However, al-\nthough ICL has achieved great performance, its\nworking mechanism is still an open question to be\ninvestigated.\nIn this paper, we explain in-context learning as a\nprocess of meta-optimization and analyze connec-\ntions between GPT-based in-context learning and\nfinetuning. Concentrating on the attention mod-\nules, we figure out that the Transformer attention\nhas a dual form of gradient descent. On top of\nit, we propose a novel perspective to explain in-\n4005\ncontext learning: (1) a pretrained GPT serves as\na meta-optimizer; (2) it produces meta-gradients\naccording to the demonstration examples through\nforward computation; (3) the meta-gradients are\napplied to the original language model through at-\ntention to build an ICL model. As illustrated in\nFigure 1, in-context learning and explicit finetun-\ning share a dual view of gradient descent, where\nICL produces meta-gradients through forward com-\nputation, while finetuning computes gradients by\nback-propagation. Therefore, it is reasonable to un-\nderstand in-context learning as implicit finetuning.\nIn order to provide empirical evidence to sup-\nport our understanding, we conduct comprehensive\nexperiments based on real tasks. On six classi-\nfication tasks, we compare the model predictions,\nattention outputs, attention weights to query tokens,\nand attention weights to training tokens between\nin-context learning and finetuning. Experimental\nresults validate that the behavior of in-context learn-\ning is similar to explicit finetuning from multiple\nperspectives. These results are strong evidence\nto prove the reasonability of our understanding of\nin-context learning as implicit finetuning.\nFurther, inspired by the dual form between Trans-\nformer attention and gradient descent, we design\na momentum-based attention, which regards the\nattention values as meta-gradients and applies the\nmomentum mechanism (Polyak, 1964; Sutskever\net al., 2013) to them. Experiments on both lan-\nguage modeling and in-context learning show that\nour momentum-based attention consistently outper-\nforms vanilla attention, which supports our under-\nstanding of meta-optimization again from another\nperspective. We note that beyond this preliminary\nattempt, our understanding may have more poten-\ntial to enlighten model design, which is worth in-\nvestigating in the future.\nOur contributions are summarized as follows:\n‚Ä¢ We figure out a dual form between Trans-\nformer attention and gradient descent, and ex-\nplain ICL as a process of meta-optimization.\n‚Ä¢ We analyze connections between in-context\nlearning and explicit finetuning and propose\nto understand ICL as implicit finetuning.\n‚Ä¢ We provide several lines of empirical evidence\nto prove that ICL and explicit finetuning be-\nhave similarly from multiple perspectives.\n‚Ä¢ We design a momentum-based attention and\nvalidate its effectiveness, which supports our\nunderstanding of meta-optimization again and\nshows the potential of our understanding to\nenlighten future model design.\n2 Background\n2.1 In-Context Learning with GPT\nIn this paper, we focus on ICL for classifica-\ntion tasks using GPT (Brown et al., 2020). A\nGPT model is stacked with L identical Trans-\nformer (Vaswani et al., 2017) decoder layers where\neach layer consists of an attention module and a\nfeed-forward network. For a classification task,\ngiven a query input text x and a candidate an-\nswer set Y = {y1,y2,...,y m}, we need to pre-\ndict a label ÀÜy conditional on n demonstration\nexamples C = {(x‚Ä≤\n1,y‚Ä≤\n1),(x‚Ä≤\n2,y‚Ä≤\n2),..., (x‚Ä≤\nn,y‚Ä≤\nn)},\nwhere (x‚Ä≤\ni,y‚Ä≤\ni) is an input-label pair different from\nthe query one. Formally, given a GPT model M,\nwe first compute the probability of each answer yj:\nPM(yj |C,x). (1)\nSince the label space is restricted for classifica-\ntion, we predict the final answer ÀÜy by selecting\nthe answer with the highest probability from the\ncandidate answer set Y:\nÀÜy= arg max\nyj\nPM(yj |C,x). (2)\nIn practice, we usually use a pre-defined template\nto format the demonstrations and prepend them\nbefore the query input. Let T(¬∑) be the function\nthat formats an example, e.g.:\nT(x,y) = Sentence: x.Sentiment: y. (3)\nThe contextual model input I is organized like\nT(x‚Ä≤\n1,y‚Ä≤\n1) T(x‚Ä≤\n2,y‚Ä≤\n2) ...T(x‚Ä≤\nn,y‚Ä≤\nn) T(x,_). (4)\nFeeding this contextual input into M, the probabil-\nity of an answer yj is computed as\nlj = M(I) ¬∑eyj , (5)\nPM(yj |C,x) = softmax(lj), (6)\nwhere M(I) denotes the output hidden state at the\nlast token position; eyj denotes the output word\nembedding of yj; and lj is the logit corresponding\nto the j-th answer.\n4006\n2.2 Dual Form Between Attention and Linear\nLayers Optimized by Gradient Descent\nThe idea in this paper to explain language models\nas meta-optimizers is inspired by Aizerman et al.\n(1964); Irie et al. (2022). They present that linear\nlayers optimized by gradient descent have a dual\nform of linear attention. Let W0,‚àÜW ‚ààRdout√ódin\nbe the initialized parameter matrix and the update\nmatrix, respectively, and x ‚ààRdin be the input rep-\nresentation. A linear layer optimized by gradient\ndescent can be formulated as\nF(x) = (W0 + ‚àÜW) x. (7)\nIn the back-propagation algorithm, ‚àÜW is com-\nputed by accumulating the outer products of his-\ntoric input representations x‚Ä≤T\ni ‚ààRdin and the error\nsignals ei ‚ààRdout of their corresponding outputs:\n‚àÜW =\n‚àë\ni\nei ‚äóx‚Ä≤\ni, (8)\nwhere ei is derived from the historic output gradi-\nents by multiplying ‚àíŒ≥, the negative learning rate.\nCombing Equation (7) and Equation (8), we can\nderive the dual form of linear layers optimized by\ngradient descent:\nF(x) = (W0 + ‚àÜW) x\n=W0x + ‚àÜWx\n=W0x +\n‚àë\ni\n(\nei ‚äóx‚Ä≤\ni\n)\nx\n=W0x +\n‚àë\ni\nei\n(\nx‚Ä≤T\ni x\n)\n=W0x + LinearAttn\n(\nE,X‚Ä≤,x\n)\n,\n(9)\nwhere LinearAttn(V,K, q) denotes the linear at-\ntention operation, in which we regard the historic\noutput error signals Eas values, the historic inputs\nX‚Ä≤as keys, and the current input x as the query.\n3 Understanding In-Context Learning\n(ICL) as Implicit Finetuning\nWe first qualitatively analyze the Transformer atten-\ntion under a relaxed linear attention form to figure\nout a dual form between it and gradient descent.\nThen, we compare in-context learning with explicit\nfinetuning to analyze connections between these\ntwo optimization forms. Based on these theoreti-\ncal findings, we propose to understand in-context\nlearning as implicit finetuning.\n3.1 Understanding Transformer Attention as\nMeta-Optimization\nLet x ‚ààRd be the input representation of a query\ntoken t, and q = WQx ‚ààRd‚Ä≤\nbe the attention\nquery vector. In the ICL setting, the attention result\nof a head is formulated as\nFICL(q) = Attn(V,K, q)\n=WV [X‚Ä≤; X] softmax\n(\n(WK[X‚Ä≤; X])T q‚àö\nd\n)\n, (10)\nwhere WQ,WK,WV ‚ààRd‚Ä≤√ód are the projection\nmatrices for computing the attention queries, keys,\nand values, respectively;\n‚àö\nddenotes the scaling\nfactor; Xdenotes the input representations of query\ntokens before t; X‚Ä≤denotes the input representa-\ntions of the demonstration tokens; and [X‚Ä≤; X] de-\nnotes the matrix concatenation. For ease of qualita-\ntive analysis, we approximate the standard attention\nto relaxed linear attention by removing the softmax\noperation and the scaling factor:\nFICL(q) ‚âàWV [X‚Ä≤; X]\n(\nWK[X‚Ä≤; X]\n)T\nq\n= WV X(WKX)T q + WV X‚Ä≤(\nWKX‚Ä≤)T\nq\n= ÀúFICL(q).\n(11)\nWe define WZSL = WV X(WKX)T as the ini-\ntialized parameters to be updated since WZSLq is\nthe attention result in the zero-shot learning (ZSL)\nsetting, where no demonstrations are given. Fol-\nlowing the reverse direction of Equation (9), we\nderive a dual form of the Transformer attention:\nÀúFICL(q) = WZSLq + WV X‚Ä≤(\nWKX‚Ä≤)T\nq\n=WZSLq + LinearAttn\n(\nWV X‚Ä≤,WKX‚Ä≤,q\n)\n=WZSLq +\n‚àë\ni\nWV x‚Ä≤\ni\n((\nWKx‚Ä≤\ni\n)T\nq\n)\n=WZSLq +\n‚àë\ni\n(\n(WV x‚Ä≤\ni) ‚äó\n(\nWKx‚Ä≤\ni\n))\nq\n=WZSLq + ‚àÜWICLq\n= (WZSL + ‚àÜWICL) q.\n(12)\nAs shown in the above equations, the attention to\nthe demonstration tokens is equivalent to param-\neter updates ‚àÜWICL that take effect on WZSL. In\naddition, by analogy with E in Equation (9), we\nregard WV X‚Ä≤as meta-gradients, which are used to\ncompute the update matrix ‚àÜWICL.\nIn summary, we explain in-context learning as a\nprocess of meta-optimization: (1) a pretrained GPT\nmodel serves as a meta-optimizer; (2) it produces\nmeta-gradients according to the demonstration ex-\namples through forward computation; (3) through\nattention, the meta-gradients are applied to the orig-\ninal language model to build an ICL model.\n4007\n3.2 Comparing ICL with Finetuning\nBased on the above understanding of in-context\nlearning, we further compare the meta-optimization\nof in-context learning with the explicit optimiza-\ntion of finetuning to analyze connections between\nthem. Considering that ICL directly takes effect\non only the attention keys and values, we design\na specific finetuning setting as the compared base-\nline, which also updates only the parameters for the\nkey and value projection. Also in the relaxed linear\nattention form, the attention result of a finetuned\nhead is formulated as\nÀúFFT(q) = (WV + ‚àÜWV )XXT (WK + ‚àÜWK)T q\n= (WZSL + ‚àÜWFT) q, (13)\nwhere ‚àÜWK and ‚àÜWV denote the parameter up-\ndates to WK and WV , respectively, which are\nacquired by back-propagation from task-specific\ntraining objectives; and ‚àÜWFT is the updates to\nWZSL introduced by finetuning.\nFor a more fair comparison with in-context learn-\ning, we further restrict the finetuning setting as fol-\nlows: (1) we specify the training examples as the\ndemonstration examples for in-context learning;\n(2) we train each example for only one step in the\nsame order as demonstrated for in-context learning;\n(3) we format each training example with the same\ntemplate used for ICL T(x‚Ä≤\ni,y‚Ä≤\ni) and use the causal\nlanguage modeling objective for finetuning.\nComparing in-context learning and this finetun-\ning setting, we find that ICL has many properties\nin common with finetuning. We organize these\ncommon properties into the following four aspects.\nBoth Perform Gradient Descent Comparing\nEquation (12) and Equation (13), we find that both\nin-context learning and finetuning introduce up-\ndates (‚àÜWICL v.s. ‚àÜWFT) to WZSL, which drive\nfrom implicit and explicit gradient descent, respec-\ntively. The main difference is that ICL produces\nmeta-gradients by forward computation while fine-\ntuning acquires real gradients by back-propagation.\nSame Training Information The meta-gradients\nof ICL are produced according to the demonstration\nexamples. The gradients of finetuning are also\nderived from the same training examples. That is\nto say, in-context learning and finetuning share the\nsame source of training information.\nSame Causal Order of Training Examples In-\ncontext learning and our finetuning setting share\nthe same causal order of training examples. ICL\nuses decoder-only Transformers so the subsequent\ntokens in the demonstrations will not affect the pre-\nceding ones. For our finetuning setting, we use the\nsame order of training examples and train only one\nepoch, so we can also guarantee that the subsequent\nexamples have no effect on the preceding ones.\nBoth Aim at Attention Compared with zero-\nshot learning, the direct effect of in-context learn-\ning and our finetuning are both restricted to the\ncomputation of attention keys and values. For ICL,\nthe model parameters are unchanged and it encodes\ndemonstration information into additional keys and\nvalues to change the attention behavior. For finetun-\ning, due to our restriction, the training information\ncan be introduced to only the projection matrices\nfor attention keys and values as well.\nConsidering the above common properties be-\ntween in-context learning and finetuning, we show\nthat it is reasonable to understand in-context learn-\ning as implicit finetuning. In the rest of this paper,\nwe compare ICL and explicit finetuning empirically\nfrom multiple perspectives to provide quantitative\nresults to support this understanding.\n4 Experiments\n4.1 Experimental Settings\nWe analyze two off-the-shelf pretrained GPT mod-\nels with 1.3 billion and 2.7 billion model parame-\nters, respectively, which are released by fairseq 1.\nIn the rest of this paper, we call them GPT 1.3B and\nGPT 2.7B for short. All experiments are conducted\non NVIDIA V100 GPUs with 32 GB memory.\nFor each task, we use the same template to for-\nmat examples for zero-shot learning (ZSL), fine-\ntuning (FT), and in-context learning (ICL). Details\nof the templates used for each task are provided\nin Appendix A. The answer prediction processes\nfor ZSL and finetuning are the same with ICL as\ndescribed in Section 2.1, except that they do not\nhave demonstration examples.\nFor in-context learning, we fix the max num-\nber of demonstration examples to 32 and tune the\nrandom seed for each task to find a set of demon-\nstration examples that achieves the best validation\nperformance. For explicit finetuning, we use the\nsame demonstration examples for in-context learn-\ning as the training examples and use SGD as the\noptimizer. For a fair comparison, we fine-tune the\n1https://github.com/facebookresearch/fairseq\n4008\nSST2 SST5 MR Subj AGNews CB\n# Validation Examples 872 1101 1066 2000 7600 56\n# Label Types 2 5 2 2 4 3\nZSL Accuracy (GPT 1.3B) 70.5 39.3 65.9 72.6 46.3 37.5\nFT Accuracy (GPT 1.3B) 73.9 39.5 73.0 77.8 65.3 55.4\nICL Accuracy (GPT 1.3B) 92.7 45.0 89.0 90.0 79.2 57.1\nZSL Accuracy (GPT 2.7B) 71.4 35.9 60.9 75.2 39.8 42.9\nFT Accuracy (GPT 2.7B) 76.9 39.1 80.0 86.1 65.7 57.1\nICL Accuracy (GPT 2.7B) 95.0 46.5 91.3 90.3 80.3 55.4\nTable 1: Statistics of six classification datasets (rows 1-2) and validation accuracy in the zero-shot learning (ZSL),\nfinetuning (FT), and in-context learning (ICL) settings on these datasets (rows 3-8).\nModel SST2 SST5 MR Subj AGNews CB Average\nGPT 1.3B 91.84 66.67 97.08 87.17 83.08 87.50 85.56\nGPT 2.7B 96.83 71.60 95.83 87.63 84.44 100.00 89.39\nTable 2: Rec2FTP for two GPT models on six datasets. From the perspective of model prediction, ICL can cover\nmost of the correct behavior of finetuning.\nmodel for only one epoch and the training exam-\nples are provided in the same order as demonstrated\nfor in-context learning. We tune the learning rate\nfor finetuning and select the one that achieves the\nbest validation performance. Details of the search\nrange and selected value for the random seeds and\nlearning rates are shown in Appendix B.\n4.2 Evaluation Datasets\nWe compare in-context learning and finetuning\nbased on six datasets spanning three sorts of\nclassification tasks. SST2 (Socher et al., 2013),\nSST5 (Socher et al., 2013), MR (Pang and\nLee, 2005) and Subj (Pang and Lee, 2004) are\nfour datasets for sentiment classification; AG-\nNews (Zhang et al., 2015) is a topic classification\ndataset; and CB (De Marneffe et al., 2019) is used\nfor natural language inference. Statistics of the\nnumber of validation examples and label types are\nsummarized in Table 1.\nFor reference, we present the validation accuracy\nin the ZSL, finetuning, and ICL settings on six\nclassification datasets in Table 1. Compared with\nZSL, ICL and finetuning both achieve considerable\nimprovements, which means the optimizations they\nmake are both helpful to these downstream tasks.\n4.3 ICL Covers Most of Correct Predictions\nof Finetuning\nWe compute a recall to finetuning prediction\n(Rec2FTP) to measure ICL can cover how much\nbehavior of finetuning from the perspective of the\nmodel prediction. We first count NFT>ZSL, the\nnumber of query examples that finetuning can pre-\ndict correctly but ZSL cannot. Then, among these\nexamples, we countN(FT>ZSL)‚àß(ICL>ZSL), the num-\nber that ICL can also predict correctly. Finally, we\ncompute the Rec2FTP score as\nN(FT>ZSL)‚àß(ICL>ZSL)\nNFT>ZSL\n.\nA higher Rec2FTP score suggests that ICL cov-\ners more correct behavior of finetuning from the\nperspective of the model prediction.\nWe show the Rec2FTP scores for two GPT mod-\nels on six datasets in Table 2. As shown in the table,\non average, ICL can correctly predict more than\n85% of the examples that finetuning can correct\nfrom ZSL. These results indicate that from the per-\nspective of model prediction, ICL can cover most\nof the correct behavior of finetuning.\n4.4 ICL Tends to Change Attention Outputs\nin the Same Direction as Finetuning\nFrom the perspective of representation, we com-\npute a similarity of the attention output updates\n(SimAOU) to measure the similarity between the\nupdates that ICL and finetuning make. For a query\nexample, let h(l)\nX denote the normalized output rep-\n4009\nModel Metric SST2 SST5 MR Subj AGNews CB Average\nGPT 1.3B SimAOU (Random ‚àÜ) 0.002 0.003 0.001 0.002 0.002 0.003 0.002\nSimAOU (‚àÜFT) 0.110 0.080 0.222 0.191 0.281 0.234 0.186\nGPT 2.7B SimAOU (Random ‚àÜ) 0.000 -0.002 0.000 0.001 -0.002 0.000 -0.001\nSimAOU (‚àÜFT) 0.195 0.323 0.157 0.212 0.333 0.130 0.225\nTable 3: SimAOU for two GPT models on six datasets. ICL updates are much more similar to finetuning updates\nthan to random updates. From the perspective of representation, ICL tends to change attention output representations\nin the same direction as finetuning changes.\nModel Metric SST2 SST5 MR Subj AGNews CB Average\nGPT 1.3B SimAM (Before Finetuning) 0.555 0.391 0.398 0.378 0.152 0.152 0.338\nSimAM (After Finetuning) 0.585 0.404 0.498 0.490 0.496 0.177 0.442\nGPT 2.7B SimAM (Before Finetuning) 0.687 0.380 0.314 0.346 0.172 0.228 0.355\nSimAM (After Finetuning) 0.687 0.492 0.347 0.374 0.485 0.217 0.434\nTable 4: SimAM for two models on six datasets. From the perspective of attention behavior, compared with attention\nweights before finetuning, ICL is more inclined to generate similar attention weights to those after finetuning.\nresentation of the last token at the l-th attention\nlayer in setting X. The updates of ICL and fine-\ntuning compared with ZSL are h(l)\nICL ‚àíh(l)\nZSL and\nh(l)\nFT ‚àíh(l)\nZSL, respectively. We compute the cosine\nbetween these two updates to get SimAOU (‚àÜFT)\nat the l-th layer. A higher SimAOU (‚àÜFT) means\nICL is more inclined to update the attention output\nin the same direction as finetuning. For comparison,\nwe also compute a baseline metric called SimAOU\n(Random ‚àÜ) that computes the similarity between\nICL updates and randomly generated updates.\nWe present the SimAOU scores averaged across\nexamples and layers for two GPT models on six\ndatasets in Table 3. From the table, we find\nthat SimAOU (Random ‚àÜ) is always around zero,\nwhile SimAOU (‚àÜFT) remains much more positive.\nThese results indicate that ICL updates are much\nmore similar to finetuning updates than to random\nupdates. From the perspective of representation,\nwe prove that ICL tends to change the attention\noutputs in the same direction as finetuning.\n4.5 ICL Is Inclined to Generate Similar\nAttention Weights to Finetuning\nFrom the perspective of attention behavior, we com-\npute a similarity of the attention map (SimAM)\nto measure the similarity of the attention map to\nquery tokens for ICL and finetuning. For a query\nexample, let m(l,h)\nX denote the attention weights be-\nfore softmax of the last token at the h-th attention\nhead in thel-th attention layer in setting X. For ICL,\nwe omit the attention to the demonstration tokens\nand only monitor the attention weights to the query\ntokens. First, before finetuning, we compute the\ncosine between m(l,h)\nICL and m(l,h)\nZSL and then average\nthe similarity across attention heads to get SimAM\n(Before Finetuning) at each layer. Similarly, after\nfinetuning, we compute the cosine between m(l,h)\nICL\nand m(l,h)\nFT to get SimAM (After Finetuning). A\nhigher SimAM (After Finetuning) over SimAM\n(Before Finetuning) indicates that the attention be-\nhavior of ICL is more similar to a finetuned model\nthan a non-finetuned one.\nTable 4 demonstrates the SimAM scores aver-\naged across examples and layers for two GPT mod-\nels on six datasets. We observe that compared with\nattention weights before finetuning, ICL is more\ninclined to generate similar attention weights to\nattention weights after finetuning. Again, from the\nperspective of attention behavior, we prove that\nICL behaves similarly to finetuning.\n4.6 ICL and Finetuning Tend to Pay Similar\nAttention to Training Tokens\nSince we understand ICL as a process of meta-\noptimization, we also compare the attention to\ntraining tokens for ICL and finetuning with the\nKendall rank correlation coefficient (Kendall,\n1948). For a query example, let m(l)\nICL denote the\nICL attention weights to the demonstration tokens\n4010\nModel Metric SST2 SST5 MR Subj AGNews CB Average\nGPT 1.3B Kendall (ICL, Random) 0.000 -0.001 0.000 0.001 -0.001 0.000 0.000\nKendall (ICL, FT) 0.192 0.151 0.173 0.181 0.190 0.274 0.193\nGPT 2.7B Kendall (ICL, Random) -0.001 0.000 0.000 0.000 0.000 -0.001 0.000\nKendall (ICL, FT) 0.213 0.177 0.264 0.203 0.201 0.225 0.214\nTable 5: Kendall rank correlation coefficients for two GPT models on six datasets. Compared with random attention\nweights, ICL attention weights to training tokens are much more similar to finetuning attention weights.\nof the last query token in the l-th attention layer,\nwhich is summed across attention heads. For fine-\ntuning, we first record all the attention queries\nQ‚Ä≤(l,h) ‚ààRd‚Ä≤√óN of the training tokens, and then\nuse the inner product between them and the atten-\ntion query q(l,h) ‚àà Rd‚Ä≤\nof the last token in the\nquery example as the finetuning attention weights\nto the training tokens: m(l)\nFT = ‚àë\nh Q‚Ä≤(l,h)T\nq(l,h),\nwhich is also summed across attention heads. The\nKendall coefficient between m(l)\nICL and m(l)\nFT is com-\nputed as Kendall (ICL, FT) = Pc‚àíPd\nN(N‚àí1)/2 , where\nN denotes the number of training tokens, Pc de-\nnotes the number of concordant pairs, and Pd de-\nnotes the number of discordant pairs. A higher\nKendall coefficient means that the orders of atten-\ntion weights to training tokens of ICL and finetun-\ning are more similar. For comparison, we also com-\npute the Kendall coefficient between m(l)\nICL and ran-\ndomly generated attention weights m(l)\nRandom, which\nwe call Kendall (ICL, Random).\nTable 5 shows the Kendall correlation coeffi-\ncients averaged across examples and layers for two\nGPT models on six datasets. We find that Kendall\n(ICL, Random) is always near zero, while Kendall\n(ICL, FT) always maintains a distinctly positive\nvalue. These results suggest that ICL and finetun-\ning tend to pay similar attention to training tokens.\n5 Momentum-Based Attention Inspired\nby Dual Form of Transformer Attention\nWe have figured out the dual form between Trans-\nformer attention and gradient descent. As illus-\ntrated in Figure 2, inspired by this dual view,\nwe investigate whether we can utilize momen-\ntum (Polyak, 1964; Sutskever et al., 2013), a widely\nused technique for optimization algorithms, to im-\nprove Transformer attention.\nGradient descent with momentum averages gra-\nMomentum-Based\nAttention\nGradient Descent\nGradient Descent \nwith Momentum\nAttention\n(Dual Form)\n(Analogy)\nFigure 2: Inspired by the dual form between atten-\ntion and gradient descent, we introduce the momentum\nmechanism into Transformer attention by analogy with\ngradient descent with momentum.\ndients among timestamps:\nŒòt = Œòt‚àí1 ‚àíŒ≥\nt‚àí1‚àë\ni=1\nŒ∑t‚àíi‚àáfŒòi , (14)\nwhere Œ≥ is the learning rate and Œ∑ is a scalar be-\ntween 0 and 1. As stated in Section 3.1, the atten-\ntion values serve as meta-gradients. By analogy\nwith gradient descent with momentum, we try to\nuse Exponential Moving Average (EMA; Hunter\n1986) to average the attention values to build the\nmomentum-based attention:\nMoAttn(V,K, qt) = Attn(V,K, qt) + EMA(V)\n= V softmax(KT qt‚àö\nd\n) +\nt‚àí1‚àë\ni=1\nŒ∑t‚àíivi,\nwhere vi is the i-th attention value vector. The\nmomentum of attention value vectors explicitly\nstrengthens the recency bias of attention, which has\nbeen shown helpful for language modeling (Press\net al., 2022). Therefore, we assume that introducing\nmomentum into attention will contribute to faster\nconvergence and better performance.\nExperiments on Language Modeling First, we\nevaluate the effect of momentum-based attention\non language modeling. We train two GPT models\nwith 350M parameters from scratch, where one is\nthe vanilla Transformer, and another applies mo-\nmentum to attention. More training details are pro-\nvided in Appendix C. We evaluate the perplexity\n4011\nModel Train 1024 Valid256 Valid512 Valid1024\nTransformer 17.61 19.50 16.87 15.14\nTransformerMoAttn 17.55 19.37 16.73 15.02\nTable 6: Perplexity on the training set and validation sets with different input lengths for language modeling.\nMomentum-based attention achieves a consistent perplexity improvement compared with the vanilla Transformer.\nModel SST5 IMDB MR CB ARC-E PIQA Average\nTransformer 25.3 64.0 61.2 43.9 48.2 68.7 51.9\nTransformerMoAttn 27.4 70.3 64.8 46.8 50.0 69.0 54.7\nTable 7: Accuracy on six in-context learning datasets. Introducing momentum into attention improves the accuracy\nof the vanilla Transformer by 2.8 on average.\nof these two models on the training set and three\nvalidation sets with input lengths of 256, 512, and\n1024, respectively. The results are shown in Table 6.\nOn all of the validation sets, applying momentum\nto attention introduces a consistent perplexity im-\nprovement compared with the vanilla Transformer.\nExperiments on In-Context Learning We also\nevaluate the in-context learning ability of the above\nlanguage models to verify the effectiveness of\nmomentum-based attention on downstream tasks.\nWe consider six datasets for sentiment analysis\n(SST5 (Socher et al., 2013), IMDB (Maas et al.,\n2011), and MR (Pang and Lee, 2005)), natural lan-\nguage inference (CB (De Marneffe et al., 2019)),\nand multi-choice selection (ARC-E (Clark et al.,\n2018) and PIQA (Bisk et al., 2020)). For all of\nthese datasets, we use up to 32 examples as demon-\nstrations. As shown in Table 7, compared with\nvanilla Transformer, using momentum-based atten-\ntion achieves consistently higher accuracy on all of\nthese datasets.\nThe performance improvements on both lan-\nguage modeling and in-context learning prove our\ndeduction that introducing momentum will im-\nprove Transformer attention. From another perspec-\ntive, these results further support our understanding\nof Transformer attention as meta-optimization.\n6 Related Work\nRecently, some pieces of work have attempted to\nunderstand the inference mechanism of in-context\nlearning. Xie et al. (2022) explain in-context learn-\ning as implicit Bayesian inference. They state that\nin-context learning emerges when language mod-\nels can infer the shared latent concept among the\ndemonstration examples, which is learned during\npretraining. On another aspect, Olsson et al. (2022)\nfocus on specific modules in Transformers. They\nfind some induction heads in Transformers that re-\nfer to abstract patterns in previous sequences to\nhelp predict the next token. They indicate that\nthe induction heads drive the ability of in-context\nlearning. Different from them, we concentrate on\nthe learning algorithm of ICL and explain it as a\nprocess of meta-optimization.\nSome other work also studies the learning algo-\nrithm of ICL. As a case study, Garg et al. (2022)\nshow that Transformers can be trained to in-context\nlearn a class of linear functions and the perfor-\nmance is comparable to the least squares estimator.\nBased on linear regression, Aky√ºrek et al. (2022)\nprove that they can construct parameters of Trans-\nformers to implement gradient-descent-based learn-\ning algorithms. Further, they show that models\ntrained with an in-context learning objective tend\nto match the behavior of models computed by ex-\nplicit learning algorithms. Also based on regres-\nsion tasks, von Oswald et al. (2022) show that lin-\near attention-only Transformers with constructed\nparameters that implement gradient descent and\nmodels learned by an in-context learning objective\nare highly related. Compared with them, we are\nthe first ones to explain in-context learning in real\nscenarios. To be specific, (1) we analyze in-context\nlearning for off-the-shelf GPT models, instead of\nmodels trained from scratch by an ICL objective;\n(2) our experiments are based on real NLP tasks,\ninstead of toy ones like linear regression.\n7 Conclusion\nIn this paper, we aim to explain the working mech-\nanism of GPT-based ICL. Theoretically, we figure\n4012\nout a dual form between Transformer attention and\ngradient descent, and propose to understand ICL\nas a process of meta-optimization. Further, we\nanalyze connections between ICL and explicit fine-\ntuning and show the reasonability to regard ICL as\nimplicit finetuning. Empirically, we comprehen-\nsively compare ICL and finetuning based on six\nreal NLP tasks. The results prove that ICL behaves\nsimilarly to explicit finetuning from multiple per-\nspectives. Further, inspired by our understanding of\nmeta-optimization, we design a momentum-based\nattention that achieves consistent performance im-\nprovements over vanilla attention. We believe our\nunderstanding will have more potential to enlighten\nICL applications and model design in the future.\nLimitations\nAlthough the ability of in-context learning has been\nfound for different architectures (e.g., Transformer\nand LSTM), we consider only Transformer-based\nin-context learning in this paper because Trans-\nformer is the current mainstream architecture of\nNLP. However, as for in-context learning itself, fig-\nuring out how it works for other architectures is\nalso a meaningful problem, which we encourage to\nstudy in the future.\nAs for the dual form we point out between Trans-\nformer attention and gradient descent, we consider\na relaxed form of linear attention for qualitative\nanalysis. Although the experimental results sup-\nport our understanding well, the mechanism of stan-\ndard Transformer attention without approximation\nmay be more complex and should be studied more\nclearly in the future.\nAs for empirical experiments, our analysis needs\nto record a large number of intermediate results\n(e.g., attention output representations, and atten-\ntion weights to query tokens and demonstration\ntokens) for thousands of validation examples. Con-\nsidering the storage space and computational cost\nof analysis, we only analyze GPT models with up\nto 2.7B parameters and leave larger models such as\nGPT 13B for future work. In addition, for the clar-\nity of the problem definition and the convenience\nof experiments, our analysis is based on only clas-\nsification tasks. Although classification is a repre-\nsentative application of in-context learning, other\ntasks like multiple choice and open-ended genera-\ntion are not considered in this paper and could be\ninvestigated in the future.\nAcknowledgement\nDamai Dai and Zhifang Sui are supported by the\nNational Key Research and Development Program\nof China 2020AAA0106700 and NSFC project\nU19A2065.\nReferences\nMark A Aizerman, Emmanuil M Braverman, and Lev I\nRozonoer. 1964. Theoretical foundation of potential\nfunctions method in pattern recognition. Avtomatika\ni Telemekhanika, 25(6):917‚Äì936.\nEkin Aky√ºrek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2022. What learn-\ning algorithm is in-context learning? investigations\nwith linear models. CoRR, abs/2211.15661.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, pages 7432‚Äì7439. AAAI Press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107‚Äì124.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey for in-context learning.\nCoRR, abs/2301.00234.\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gre-\ngory Valiant. 2022. What can transformers learn\nin-context? A case study of simple function classes.\nCoRR, abs/2208.01066.\nJ Stuart Hunter. 1986. The exponentially weighted\nmoving average. Journal of quality technology ,\n18(4):203‚Äì210.\n4013\nKazuki Irie, R√≥bert Csord√°s, and J√ºrgen Schmidhuber.\n2022. The dual form of neural networks revisited:\nConnecting test time predictions to training patterns\nvia spotlights of attention. In International Confer-\nence on Machine Learning, ICML 2022, volume 162\nof Proceedings of Machine Learning Research, pages\n9639‚Äì9659. PMLR.\nMaurice George Kendall. 1948. Rank correlation meth-\nods.\nLouis Kirsch, James Harrison, Jascha Sohl-Dickstein,\nand Luke Metz. 2022. General-purpose in-context\nlearning by meta-learning transformers. CoRR,\nabs/2212.04458.\nLouis Kirsch and J√ºrgen Schmidhuber. 2021. Meta\nlearning backpropagation and improving it. In Ad-\nvances in Neural Information Processing Systems\n34: Annual Conference on Neural Information Pro-\ncessing Systems 2021, NeurIPS 2021, pages 14122‚Äì\n14134.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142‚Äì150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam\nMcCandlish, and Chris Olah. 2022. In-context learn-\ning and induction heads. CoRR, abs/2209.11895.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings\nof the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pages 271‚Äì278,\nBarcelona, Spain.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In ACL 2005, 43rd An-\nnual Meeting of the Association for Computational\nLinguistics, Proceedings of the Conference , pages\n115‚Äì124. The Association for Computer Linguistics.\nBoris T Polyak. 1964. Some methods of speeding up\nthe convergence of iteration methods. Ussr com-\nputational mathematics and mathematical physics ,\n4(5):1‚Äì17.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In The Tenth International\nConference on Learning Representations, ICLR 2022.\nOpenReview.net.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631‚Äì1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nIlya Sutskever, James Martens, George E. Dahl, and\nGeoffrey E. Hinton. 2013. On the importance of\ninitialization and momentum in deep learning. In\nProceedings of the 30th International Conference on\nMachine Learning, ICML 2013, volume 28 of JMLR\nWorkshop and Conference Proceedings, pages 1139‚Äì\n1147. JMLR.org.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 5998‚Äì6008. Curran Asso-\nciates, Inc.\nJohannes von Oswald, Eyvind Niklasson, Ettore Ran-\ndazzo, Jo√£o Sacramento, Alexander Mordvintsev, An-\ndrey Zhmoginov, and Max Vladymyrov. 2022. Trans-\nformers learn in-context by gradient descent. ArXiv\npreprint, abs/2212.07677.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. CoRR,\nabs/2206.07682.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022. OpenReview.net.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural\nInformation Processing Systems 2015 , pages 649‚Äì\n657.\n4014\nAppendix\nA Templates for In-Context Learning\nWe demonstrate the templates used to format exam-\nples and the candidate answer sets for six classifi-\ncation datasets used in our experiments in Table 8.\nB Hyper-Parameters for In-Context\nLearning and Finetuning\nWe perform grid search to find the best random seed\nfor ICL and the best learning rate for finetuning.\nThe search range for all the datasets is the same.\nFor random seeds, we search in {1,2,3,4,5,6,7}.\nFor learning rates, the search base values are\n{1,2,3,4,5,6,7,8,9}and we scale them to 0.1,\n0.01, 0.001, and 0.0001 times, i.e., we have9√ó4 =\n36 values to search. As an exception, for GPT 1.3B\nfinetuned on SST5, we perform a more fine-grained\nsearch and finally set its learning rate to 0.00016\nsince the finetuned model cannot outperform the\nzero-shot learning with the above 36 learning rates.\nIn Table 9, we present the details of the selected\nrandom seeds and learning rates for two GPT mod-\nels on six classification datasets.\nC Hyper-Parameters for Training\nLanguage Models from Scratch\nThe hyper-parameters for training two language\nmodels from scratch are summarized in Table 10.\n4015\nDataset Template Candidate Answer Set\nSST2 Sentence: {Sentence} { Negative, Positive }\nLabel: {Label}\nSST5 Sentence: {Sentence} { terrible, bad, neutral, good, great }\nLabel: {Label}\nMR Review: {Sentence} { Negative, Positive }\nSentiment: {Label}\nSubj Input: {Sentence} { objective, subjective }\nType: {Label}\nAGNews Classify the news articles into the categories\nof World, Sports, Business, and Technology.\n{ World, Sports, Business, Technology }\nNews: {Sentence}\nType: {Label}\nCB {Premise} { True, False, Neither }\nQuestion: {Hypothesis} True, False, or Nei-\nther?\nAnswer: {Label}\nTable 8: Formatting templates and candidate answer sets for six classification datasets.\nHyper-Parameter Dataset GPT 1.3B GPT 2.7B\nRandom Seed\nSST2 2 7\nSST5 5 5\nMR 5 1\nSubj 4 4\nAGNews 3 3\nCB 3 3\nLearning Rate\nSST2 0.0005 0.007\nSST5 0.00016 0.04\nMR 0.003 0.001\nSubj 0.003 0.002\nAGNews 0.2 0.2\nCB 0.08 0.01\nTable 9: Selected random seeds and learning rates for two GPT models on six classification datasets.\n4016\nHyper-parameter Value\nEmbedding & Hidden Dimension 1024\nFFN Inner Hidden Dimension 4096\nNumber of Attention Heads 16\nNumber of Transformer Layers 24\nNumber of Parameters 350M\nSequence Length 1024\nBatch Size 512K Tokens\nOptimizer Adam\nAdam Betas (0.9, 0.98)\nAdam Epsilon 1e-6\nMaximum Learning Rate 3e-4\nLearning Rate Scheduler Polynomial Decay\nTotal Training Steps 500K\nWarm-up Steps 20K\nGradient Clip Norm 2.0\nTable 10: Hyper-parameters for training two language models from scratch.\n4017\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nSection \"Limitations\"\n‚ñ° A2. Did you discuss any potential risks of your work?\nNot applicable. This paper focuses on the interpretability of models, so there are no potential risks\nfor our work.\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nSection \"Abstract\" and Section 1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nSections 4 & 5\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.1\n‚ñ° B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Fairseq is a widely-acceptable open-source library for NLP tasks, and we do not\nviolate its license.\n‚ñ° B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Our use of fairseq is consistent with their intended use.\n‚ñ° B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We use publicly available datasets that had been checked when they were created.\n‚ñ° B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. We provide the website of fairseq, and the mentioned information is included in it.\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nSection 4.2\nC ‚ñ°\u0013 Did you run computational experiments?\nSections 4 & 5\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSections 4 & 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4018\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSections 4 & 5, Appendices B & C\n‚ñ° C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Our analytical experiments do not focus on performance, but analysis of off-the-shelf\nmodels\n‚ñ°\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe did not use existing packages for preprocessing, normalization, or evaluation\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4019",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8174113035202026
    },
    {
      "name": "Gradient descent",
      "score": 0.6345497369766235
    },
    {
      "name": "Transformer",
      "score": 0.622343897819519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.612099289894104
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.5189575552940369
    },
    {
      "name": "Machine learning",
      "score": 0.4908457398414612
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4835912585258484
    },
    {
      "name": "Stochastic gradient descent",
      "score": 0.46681663393974304
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.4620911478996277
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.45046868920326233
    },
    {
      "name": "AKA",
      "score": 0.4330879747867584
    },
    {
      "name": "Empirical research",
      "score": 0.41603535413742065
    },
    {
      "name": "Artificial neural network",
      "score": 0.16180405020713806
    },
    {
      "name": "Mathematics",
      "score": 0.08735218644142151
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Library science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    }
  ],
  "cited_by": 125
}