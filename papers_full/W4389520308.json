{
    "title": "Pretraining Language Models with Text-Attributed Heterogeneous Graphs",
    "url": "https://openalex.org/W4389520308",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2008284395",
            "name": "Tao Zou",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2096565724",
            "name": "Le Yu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2097935698",
            "name": "Yifei Huang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2103194454",
            "name": "Leilei Sun",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2342401403",
            "name": "Bowen Du",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2366141641",
        "https://openalex.org/W2950894652",
        "https://openalex.org/W4290875442",
        "https://openalex.org/W4286889809",
        "https://openalex.org/W3212640459",
        "https://openalex.org/W3154091824",
        "https://openalex.org/W4385562555",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W3173025631",
        "https://openalex.org/W4221153690",
        "https://openalex.org/W2997461192",
        "https://openalex.org/W3004507689",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W4312106615",
        "https://openalex.org/W3004578093",
        "https://openalex.org/W4287547328",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3012871709",
        "https://openalex.org/W3156738579",
        "https://openalex.org/W4299547686",
        "https://openalex.org/W3100278010",
        "https://openalex.org/W4286907499",
        "https://openalex.org/W2987579681",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W4321648960",
        "https://openalex.org/W2962975498",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2965857891",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W4386566638",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2893359107",
        "https://openalex.org/W4297571622",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W3164473340",
        "https://openalex.org/W3100078588",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4290876396"
    ],
    "abstract": "In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a text augmentation strategy to enrich textless nodes with their neighborsâ€™ texts for handling the imbalance issue. We conduct link prediction and node classification tasks on three datasets from various domains. Experimental results demonstrate the superiority of our approach over existing methods and the rationality of each design. Our code is available at https://github.com/Hope-Rita/THLM.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10316â€“10333\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nPretraining Language Models with Text-Attributed Heterogeneous Graphs\nTao Zou1,âˆ—, Le Yu1,âˆ—, Yifei Huang1, LeiLei Sun1,â€ and Bowen Du1,2,3\n1SKLSDE Lab, Beihang University, Beijing, China\n2 Zhongguancun Laboratory, Beijing, China\n3 School of Transportation Science and Engineering, Beihang University, Beijing, China\n{zoutao, yule, yifeihuang, leileisun, dubowen}@buaa.edu.cn\nAbstract\nIn many real-world scenarios (e.g., academic\nnetworks, social platforms), different types of\nentities are not only associated with texts but\nalso connected by various relationships, which\ncan be abstracted as Text-Attributed Heteroge-\nneous Graphs (TAHGs). Current pretraining\ntasks for Language Models (LMs) primarily\nfocus on separately learning the textual infor-\nmation of each entity and overlook the cru-\ncial aspect of capturing topological connections\namong entities in TAHGs. In this paper, we\npresent a new pretraining framework for LMs\nthat explicitly considers the topological and\nheterogeneous information in TAHGs. Firstly,\nwe define a context graph as neighborhoods of\na target node within specific orders and pro-\npose a topology-aware pretraining task to pre-\ndict nodes involved in the context graph by\njointly optimizing an LM and an auxiliary het-\nerogeneous graph neural network. Secondly,\nbased on the observation that some nodes are\ntext-rich while others have little text, we de-\nvise a text augmentation strategy to enrich text-\nless nodes with their neighborsâ€™ texts for han-\ndling the imbalance issue. We conduct link pre-\ndiction and node classification tasks on three\ndatasets from various domains. Experimen-\ntal results demonstrate the superiority of our\napproach over existing methods and the ratio-\nnality of each design. Our code is available at\nhttps://github.com/Hope-Rita/THLM.\n1 Introduction\nPretrained Language Models (PLMs) (Devlin et al.,\n2019; Yang et al., 2019; Brown et al., 2020; Lan\net al., 2020) that built upon the Transformer archi-\ntecture (Vaswani et al., 2017) have been success-\nfully applied in various downstream tasks such as\nautomatic knowledge base construction (Bosselut\net al., 2019) and machine translation (Herzig et al.,\n2020). Due to the design of pretraining tasks (e.g.,\nâˆ—Equal Contribution\nâ€ Corresponding Author\nmasked language modeling (Devlin et al., 2019),\nnext-token prediction (Radford et al., 2018), autore-\ngressive blank infilling (Du et al., 2022)), PLMs\ncan learn general contextual representations from\ntexts in the large-scale unlabelled corpus.\nKeyword (textless): Brief terms\nGraph neural networks\nPaper (text-rich): Title&Abstract\nHeterogeneous Graph Transformer. Recent \nyears have witnessed the emergingâ€¦\nAuthor (textless): Name\nJure Leskovec\nfirst-order \nneighbors\nhigh-order \nneighbors\nFigure 1: As an instance of TAHG, an academic net-\nwork contains three types of nodes (papers, authors,\nand keywords) with textual descriptions as well as their\nmulti-relational connections.\nIn fact, texts not only carry semantic information\nbut also are correlated with each other, which could\nbe well represented by Text-Attributed Heteroge-\nneous Graphs (TAHGs) that include multi-typed\nnodes with textual descriptions as well as relations.\nSee Figure 1 for an example. Generally, TAHGs\nusually exhibit the following two challenges that\nare struggled to be handled by existing PLMs.\nAbundant Topological Information (C1). Both\nfirst- and higher-order connections exist in TAHGs\nand can reflect rich relationships. For instance, a pa-\nper can be linked to its references via first-order ci-\ntations and can also be correlated with other papers\nthrough high-order co-authorships. However, the\ncommonly used pretraining tasks (Radford et al.,\n2018; Devlin et al., 2019; Du et al., 2022) just learn\nfrom texts independently and thus ignore the con-\nnections among different texts. Although some\nrecent works have attempted to make PLMs aware\nof graph topology (Yasunaga et al., 2022; Chien\net al., 2022), they only consider first-order relation-\nships and fail to handle higher-order signals.\nImbalanced Textual Descriptions of Nodes (C2).\nIn TAHGs, nodes are heterogeneous and their car-\nried texts are often in different magnitudes. For\nexample, papers are described by both titles and\n10316\nabstracts (rich-text nodes), while authors and key-\nwords only have names or brief terms (textless\nnodes). Currently, how to pretrain LMs to com-\nprehensively capture the above characteristics of\nTAHGs still remains an open question.\nIn this paper, we propose a new pretraining\nframework to integrate both Topological and\nHeterogeneous information in TAHGs into LMs,\nnamely THLM. To address C1, we define a con-\ntext graph as the neighborhoods of the central node\nwithin Korders and design a topology-aware pre-\ntraining task (context graph prediction) to predict\nneighbors in the context graph. To be specific, we\nfirst obtain the contextual representation of the cen-\ntral node by feeding its texts into an LM and com-\npute the structural representation of nodes in the\ngiven TAHG by an auxiliary heterogeneous graph\nneural network. Then, we predict which nodes are\ninvolved in the context graph based on the repre-\nsentations, aiming to inject the multi-order topol-\nogy learning ability of graph neural networks into\nLMs. To tackle C2, we devise a text augmentation\nstrategy, which enriches the semantics of textless\nnodes with their neighborsâ€™ texts and encodes the\naugmented texts by LMs. We conduct extensive ex-\nperiments on three TAHGs from various domains\nto evaluate the model performance. Experimental\nresults show that our approach could consistently\noutperform the state-of-the-art on both link predic-\ntion and node classification tasks. We also provide\nan in-depth analysis of the context graph predic-\ntion pretraining task and text augmentation strategy.\nOur key contributions include:\nâ€¢ We investigate the problem of pretraining LMs\non a more complicated data structure, i.e.,\nTAHGs. Unlike most PLMs that can only\nlearn from the textual description of each\nnode, we present a new pretraining framework\nto enable LMs to capture the topological con-\nnections among different nodes.\nâ€¢ We introduce a topology-aware pretraining\ntask to predict nodes in the context graph of a\ntarget node. This task jointly optimizes an LM\nand an auxiliary heterogeneous graph neural\nnetwork, enabling the LMs to leverage both\nfirst- and high-order signals.\nâ€¢ We devise a text augmentation strategy to en-\nrich the semantics of textless nodes to mitigate\nthe text-imbalanced problem.\n2 Preliminaries\nA Pretrained Language Model (PLM) can map\nan input sequence X = (x1,x2,Â·Â·Â· ,xL) of Lto-\nkens into their contextual representations H =\n(h1,h2,Â·Â·Â· ,hL) with the design of pretraining\ntasks like masked language modeling (Devlin et al.,\n2019), next-token prediction (Radford et al., 2018),\nautoregressive blank infilling (Du et al., 2022). In\nthis work, we mainly focus on the encoder-only\nPLMs (e.g., BERT (Devlin et al., 2019), RoBERTa\n(Liu et al., 2019)) and leave the explorations of\nPLMs based on encoder-decoder or decoder-only\narchitecture in the future.\nA Text-Attributed Heterogeneous Graph\n(TAHG) (Shi et al., 2019) usually consists of multi-\ntyped nodes as well as different kinds of relations\nthat connect the nodes. Each node is also associ-\nated with textual descriptions of varying lengths.\nMathematically, a TAHG can be represented by\nG = (V,E,U,R,X), where V, E, Uand Rde-\nnote the set of nodes, edges, node types, and edge\ntypes, respectively. Each node v âˆˆ Vbelongs\nto type Ï•(v) âˆˆU and each edge eu,v has a type\nÏˆ(eu,v) âˆˆ R. X is the set of textual descrip-\ntions of nodes. Note that a TAHG should satisfy\n|U|+ |R|>2.\nExisting PLMs mainly focus on textual descrip-\ntions of each node separately, and thus fail to\ncapture the correlations among different nodes in\nTAHGs (as explained in Section 1). To address\nthis issue, we propose a new framework for pre-\ntraining LMs with TAHGs, aiming to obtain PLMs\nthat are aware of the graph topology as well as the\nheterogeneous information.\n3 Methodology\nFigure 2 shows the overall framework of our pro-\nposed approach, which mainly consists of two com-\nponents: topology-aware pretraining task and text\naugmentation strategy. Given a TAHG, the first\nmodule extracts the context graph for a target node\nand predicts which nodes are involved in the con-\ntext graph by jointly optimizing an LM and an\nauxiliary heterogeneous graph neural network. It\naims to enable PLMs to capture both first-order\nand high-order topological information in TAHGs.\nSince some nodes may have little textual descrip-\ntions in TAHGs, the second component is further\nintroduced to tackle the imbalanced textual descrip-\ntions of nodes, which enriches the semantics of\ntextless nodes by neighborsâ€™ texts. It is worth notic-\n10317\ning that after the pretraining stage, we discard the\nauxiliary heterogeneous graph neural network and\nonly apply the PLM for various downstream tasks.\n3.1 Topology-aware Pretraining Task\nTo tackle the drawback that most existing PLMs\ncannot capture the connections between nodes with\ntextual descriptions, some recent works have been\nproposed (Yasunaga et al., 2022; Chien et al., 2022).\nAlthough insightful, these methods solely focus on\nthe modeling of first-order connections between\nnodes while ignoring high-order signals, which\nare proved to be essential in fields like network\nanalysis (Grover and Leskovec, 2016; Cui et al.,\n2019), graph learning (Kipf and Welling, 2017;\nHamilton et al., 2017) and recommender system\n(Wang et al., 2019; He et al., 2020). To this end, we\npropose a topology-aware pretraining task (namely,\ncontext graph prediction) for helping LMs capture\nmulti-order connections among different nodes.\nContext Graph Extraction . We first illus-\ntrate the definition of the context graph of a\ntarget node. Let Nu be the set of first-order\nneighbors of node u in a given TAHG G =\n(V,E,U,R,X). The context graph GK\nu of node\nuis composed of neighbors that ucan reach within\nK orders (including node u itself) as well as\ntheir connections, which is represented by GK\nu =\n(VK\nu ,EK\nu ). VK\nu =\n{\nvâ€²|vâ€²âˆˆNv âˆ§vâˆˆVKâˆ’1\nu\n}\nâˆª\nVKâˆ’1\nu is the node set of GK\nu and EK\nu ={\n(uâ€²,vâ€²) âˆˆE|uâ€²âˆˆVK\nu âˆ§vâ€²âˆˆVK\nu\n}\n) denotes the\nedge set of GK\nu . It is obvious that V0\nu = {u}and\nV1\nu = Nu âˆª{u}. Based on the definition, we can\nextract the context graph of node ubased on the\ngiven TAHG G. Note that when K â‰¥2, the con-\ntext graph GK\nu will contain multi-order correlations\nbetween nodes, which provides an opportunity to\ncapture such information by learning from GK\nu .\nContext Graph Prediction. TAHGs not only\ncontain multiple types of nodes and relations but\nalso involve textual descriptions of nodes. Instead\nof pretraining on single texts like most PLMs do,\nwe present the Context Graph Prediction (CGP)\nfor pretraining LMs on TAHGs to capture the rich\ninformation. Since LMs have been shown to be\npowerful in modeling texts (Devlin et al., 2019;\nBrown et al., 2020), the objective of CGP is to\ninject the graph learning ability of graph neural\nnetworks (Bing et al., 2022) into LMs.\nSpecifically, we first utilize an auxiliary hetero-\ngeneous graph neural network to encode the input\nTAHG Gand obtain the representations of all the\nnodes in Vas follows,\nHG= fHGNN (G) âˆˆR|V|Ã—d, (1)\nwhere fHGNN (Â·) can be implemented by any ex-\nisting heterogeneous graph neural networks. dis\nthe hidden dimension. Then, we encode the textual\ndescription of target node uby an LM and derive\nits semantic representation by\nhu\nLM = MEAN(fLM (Xu)) âˆˆRd, (2)\nwhere fLM(Â·) can be realized by the existing LMs.\nBesides, to capture the heterogeneity of node u, we\nintroduce a projection header in the last layer of the\nPLM. Xu denotes the textual descriptions of node\nu. Next, we predict the probability that node vis\ninvolved in the context graph GK\nu of uvia a binary\nclassification task\nË†yu,v = sigmoid\n(\nhu\nLM\nâŠ¤WÏ•(v)HG\nv\n)\n, (3)\nwhere WÏ•(v) âˆˆRdÃ—d is a trainable transform ma-\ntrix for node type Ï•(v) âˆˆR. The ground truth\nyu,v = 1if GK\nu contains v, and 0 otherwise.\nPretraining Process. In this work, we use BERT\n(Devlin et al., 2019) and R-HGNN (Yu et al., 2022)\nto implement fLM(Â·) and fHGNN (Â·), respectively.\nSince it is intractable to predict the appearing prob-\nabilities of all the nodes vâˆˆV in Equation (3), we\nadopt negative sampling (Mikolov et al., 2013) to\njointly optimize fLM(Â·) and fHGNN (Â·). To gener-\nate positive samples, we uniformly sample kneigh-\nbors from a specific relation during each hop. The\nnegative ones are sampled from the remaining node\nset V\\V K\nu with a negative sampling ratio of 5 (i.e.,\nfive negative samples per positive sample). In ad-\ndition to the CGP task, we incorporate the widely\nused Masked Language Modeling (MLM) task to\nhelp LMs better handle texts. The final objective\nfunction for each node uâˆˆV is\nLu = LMLM\nu + LCGP\nu = âˆ’log P( ËœXu|Xu\\ËœXu)âˆ’\nâˆ‘\nvâˆˆVKu\nlog Ë†yu,v âˆ’\n5âˆ‘\ni=1\nEvâ€²\niâˆ¼Pn(V\\VKu ) log\n(\n1 âˆ’Ë†yu,vâ€²\ni\n)\n, (4)\nwhere ËœXu is the corrupted version of node uâ€™s\noriginal textual descriptions Xu with a 40% mask-\ning rate following (Wettig et al., 2023). Pn(Â·) de-\nnotes the normal noise distribution. Additionally,\nthe input feature of each node for the auxiliary het-\nerogeneous graph neural network is initialized by\n10318\nğ‘¢\nHeterogeneous Graph \nNeural Network\nLanguage Model\nğ‘£ âˆˆ ğ’±ğ‘¢ğ¾\n(positive sample)\nğ‘€ğ‘¢\ntextual representations:ğ’‰ğ¿ğ‘€\nğ‘¢\nğ‘£â€²~ğ‘ƒğ‘›(ğ’±\\ğ’±ğ‘¢ğ¾)\n(negative sample)\n+\nâˆ’\nğ‘¯ğ‘£â€²\nğ’¢\ntext-attributed \nheterogeneous graph ğ’¢\nâ„’ğ‘¢ğ¶ğºğ‘ƒbackpropagate gradients \nto ğ‘“(Î˜ğ¿ğ‘€,Î˜ğ»ğºğ‘ğ‘)\nğ‘¢\ncontext graph ğ’¢ğ‘¢ğ¾\ncontext graph \nextraction\nğ’¢\nğ‘¯ğ‘£\nğ’¢\nstructural embeddings\nğ‘‹ğ‘¢ ğ‘‹ğ‘1\nğ‘¢\nâ€¦\nâ€¦\ntextless nodes\nrich-text nodes\nğ‘‹ğ‘¢[CLS] [SEP] LM input\nText Augmentation Strategy\nFigure 2: Framework of the proposed approach.\nits semantic representation based on Equation (2)\n1, which is shown to be better than a randomly-\ninitialized trainable feature in the experiments.\n3.2 Text Augmentation Strategy\nAs discussed in Section 1, the textual descriptions\nof different types of nodes in TAHGs are varying\nwith different lengths, resulting in rich-text nodes\nand textless nodes. The exhaustive descriptions of\nrich-text nodes can well reveal their characteristics,\nwhile the brief descriptions of textless nodes are\ninsufficient to reflect their semantics and solely en-\ncoding such descriptions would lead to suboptimal\nperformance. Therefore, we devise a text augmen-\ntation strategy to tackle the imbalance issue, which\nfirst enriches the semantics of textless nodes by\ncombining the textual descriptions of their neigh-\nbors according to the connections in TAHGs and\nthen computes the augmented texts by LMs.\nTo be specific, for rich-text node u, we use\nits texts with special tokens (Devlin et al., 2019)\nas the input Mu, which is denoted as [CLS] Xu\n[SEP]. For textless node u, we concatenate its\ntexts and ksampled neighborsâ€™ texts as the input\nMu, i.e., [CLS] Xu [SEP] XN1u [SEP] ... [SEP]\nXNku [SEP],2 where Ni\nu represents the i-th sam-\npled neighbor of u. Furthermore, in the case of\nnodes lacking text information, we employ the con-\ncatenation of text sequences from neighbors. This\napproach enables the generation of significant se-\nmantic representations for such nodes, effectively\naddressing the issue of text imbalance. After the\naugmentation of texts, we change the input of Equa-\ntion (2) from Xu to Mu to obtain representation\n1Note that the initialization is executed only once by using\nthe official checkpoint of BERT (Devlin et al., 2019).\n2Among the neighbors in Nu, we select rich-text nodes in\npriority. Moreover, if the size of Nu is smaller or equal to k,\nwe will choose all the neighbors.\nhu\nLM with more semantics. We empirically find\nthat text augmentation strategy can bring nontrivial\nimprovements without a significant increment of\nthe modelâ€™s complexity.\n3.3 Fine-tuning in Downstream Tasks\nAfter the pretraining process, we discard the\nauxiliary heterogeneous graph neural network\nfHGNN (Â·) and solely apply the pretrained LM\nfLM(Â·) to generate the semantic representations of\nnodes based on Equation (2). We select two graph-\nrelated downstream tasks for evaluation including\nlink prediction and node classification. We em-\nploy various headers at the top of fLM(Â·) to make\nexhaustive comparisons, including MultiLayer Per-\nceptron (MLP), RGCN (Schlichtkrull et al., 2018),\nHetSANN (Hong et al., 2020), and R-HGNN (Yu\net al., 2022). For downstream tasks, fLM(Â·) is\nfrozen for efficiency and only the headers can be\nfine-tuned. Please refer to the Appendix A.2 for\ndetailed descriptions of the headers.\n4 Experiments\n4.1 Datasets and Baselines\nDatasets. We conduct experiments on three real-\nworld datasets from different domains, including\nthe academic network (OAG-Venue (Hu et al.,\n2020b)), book publication (GoodReads (Wan and\nMcAuley, 2018; Wan et al., 2019)), and patent ap-\nplication (Patents3). All the datasets have raw texts\non all types of nodes, whose detailed descriptions\nand statistics are shown in the Appendix A.1.\nCompared Methods. We compare THLM with\nseveral baselines to generate the representations\nof nodes and feed them into the headers for down-\nstream tasks. In particular, we select six methods to\n3https://www.uspto.gov/\n10319\ncompute the node representations: BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019) are\nwidely used PLMs; MetaPath (Dong et al., 2017)\nis a representative method for heterogeneous net-\nwork embedding; MetaPath+BERT combines the\ntextual and structural information as the represen-\ntations, LinkBERT (Yasunaga et al., 2022) and GI-\nANT (Chien et al., 2022) are first-order topology-\naware PLMs. Besides, we apply OAG-BERT (Liu\net al., 2022) to compare the performance of OAG-\nVenue. Detailed information about baselines is\nshown in the Appendix A.1. It is worth noticing\nthat LinkBERT and GIANT are designed for ho-\nmogeneous graphs instead of TAHGs. Hence, we\nmaintain the 2-order connections among rich-text\nnodes and remove the textless nodes to build ho-\nmogeneous graphs for these two methods for eval-\nuation. See Appendix A.6 for more details.\n4.2 Experimental Settings\nFollowing the official configuration of BERTbase\n(110M params, (Devlin et al., 2019)), we limit the\ninput length of the text to 512 tokens. For the\ncontext graph prediction task, the number of or-\nders Kin extracting context graphs is searched in\n[1,2,3,4]. For the text augmentation strategy, we\nsearch the number of neighbors k for concatena-\ntion in [1,2,3]. We load the weights in BERTbase\ncheckpoint released from Transformers tools4 for\ninitialization. For R-HGNN, we set the hidden di-\nmension of node representations and relation repre-\nsentations to 786 and 64, respectively. The number\nof attention heads is 8. We use the two-layered\nR-HGNN in the experiments. To optimize THLM,\nwe use AdamW (Loshchilov and Hutter, 2019) as\nthe optimizer with (Î²1,Î²2) = (0.9,0.999), weight\ndecay 0.01. For BERTbase, we warm up the learn-\ning rate for the first 8,000 steps up to 6e-5, then\nlinear decay it. For R-HGNN, the learning rate\nis set to 1e-4. We set the dropout rate (Srivastava\net al., 2014) of BERTbase and R-HGNN to 0.1. We\ntrain for 80,000 steps, and batch sizes of 32, 48,\nand 64 sequences with 512 tokens for OAG-Venue,\nGoodReads, and Patents, and with maximize uti-\nlization while meeting the device constraints. The\npretraining process took about three days on four\nGeForce RTX 3090 GPUs (24GB memory). For\ndownstream tasks, please see Appendix A.4 for\ndetailed settings of various headers.\n4https://huggingface.co/bert-base-cased\n4.3 Evaluation Tasks\nLink Prediction. On OAG-Venue, GoodReads,\nand Patents, the predictions are between paper-\nauthor, book-publisher, and patent-company, re-\nspectively. We use RMSE and MAE as evalua-\ntion metrics, whose descriptions are shown in Ap-\npendix A.3. Considering the large number of edges\non the datasets, we use a sampling strategy for\nlink prediction. Specifically, the ratio of the edges\nused for training, validation, and testing is 30%,\n10%, and 10% in all datasets. Each edge is as-\nsociated with five/one/one negative edge(s) in the\ntraining/validation/testing stage.\nNode Classification . We classify the cate-\ngory of papers, books, and patents in OAG-Venue,\nGoodReads, and Patents. We use Micro-Precision,\nMicro-Recall, Macro-Precision, Macro-Recall, and\nNDCG to evaluate the performance of different\nmodels. Descriptions of the five metrics are shown\nin Appendix A.3. Each paper in OAG-Venue only\nbelongs to one venue, which could be formalized\nas a multi-class classification problem. Each patent\nor each book is categorized into one or more labels,\nresulting in multi-label classification problems.\n4.4 Performance Comparison\nDue to space limitations, we present the perfor-\nmance on RMSE and MAE for link prediction,\nas well as Micro-Precision and Micro-Recall for\nnode classification, in Table 1. For the performance\non Macro-Precision, Macro-Recall, and NDCG on\nthree datasets in the node classification task, please\nrefer to Appendix A.5. From Table 1 and Appendix\nA.5, we have the following conclusions.\nFirstly, except for MetaPath, BERT and\nRoBERTa exhibit relatively poorer performance\nin link prediction across three datasets compared\nto other baselines. This suggests that incorpo-\nrating the structural information from the graph\ncan greatly enhance the performance of down-\nstream link prediction tasks. Moreover, RoBERTa\nachieves notable performance in node classification\nwhen compared to other baselines. This implies\nthat leveraging better linguistic representations can\nfurther improve the overall performance.\nSecondly, we observe that MetaPath, which\nsolely captures the network embeddings, performs\nthe worst performance among the evaluated meth-\nods. However, when MetaPath is combined with se-\nmantic information, it achieves comparable or even\nsuperior performance compared to RoBERTa. This\n10320\nTable 1: Performance of different methods on three datasets in two downstream tasks. The best and second-best\nperformances are boldfaced and underlined. *: THLM significantly outperforms the best baseline with p-value <\n0.05\nDatasets Model Link Prediction Node ClassificationRMSE MAE Micro-Precision(@1) Micro-Recall(@1)HetSANN RGCN R-HGNNHetSANN RGCN R-HGNNMLP HetSANN RGCN R_HGNNMLP HetSANN RGCN R-HGNN\nOAG-Veune\nBERT 0.1987 0.2149 0.18020.0648 0.0886 0.04470.2257 0.3146 0.3136 0.34730.2257 0.3146 0.3136 0.3473RoBERTa0.1931 0.2152 0.16890.0635 0.0814 0.04000.2527 0.31930.33410.35160.2527 0.31930.33410.3516MetaPath0.2199 0.2415 0.19460.0842 0.0972 0.05440.1132 0.2693 0.2851 0.30110.1132 0.2693 0.2851 0.3011MetaPath+BERT0.2213 0.21490.16510.09810.07340.03770.23070.3311 0.3317 0.34720.23070.3311 0.3317 0.3472LinkBERTâ‹† 0.1867 0.2229 0.17390.0628 0.0892 0.04240.2278 0.3108 0.3115 0.35080.2278 0.3108 0.3115 0.3508GIANTâ‹† 0.20450.2022 0.17090.0730 0.0761 0.04080.2280 0.3116 0.3074 0.32740.2280 0.3116 0.3074 0.3274OAG-BERT0.1918 0.2030 0.17720.0634 0.0744 0.03860.2577 0.3214 0.3152 0.34250.2577 0.3214 0.3152 0.3425THLM 0.1857âˆ— 0.1893âˆ—0.1591âˆ— 0.0614âˆ— 0.0722âˆ—0.0352âˆ—0.2637âˆ— 0.3409âˆ— 0.3398âˆ— 0.3575âˆ— 0.2637âˆ— 0.3409âˆ— 0.3398âˆ—0.3575âˆ—\nGoodReads\nBERT 0.1424 0.1738 0.11030.0408 0.0586 0.01900.7274 0.8238 0.8240 0.83960.6984 0.7909 0.7911 0.8061RobERTa0.1349 0.12680.10440.0360 0.02980.01890.73630.8271 0.83140.84040.70690.7941 0.79820.8069MetaPath0.1782 0.1740 0.15200.0639 0.0639 0.04700.1492 0.6448 0.6479 0.68830.1432 0.6190 0.6220 0.6608MetaPath+BERT0.1314 0.1195 0.14030.03250.0280 0.03000.7240 0.82580.8320 0.83960.6951 0.79280.7988 0.8061LinkBERTâ‹† 0.1471 0.1362 0.11350.0443 0.0396 0.02120.7131 0.8209 0.8259 0.83690.6846 0.7882 0.7930 0.8035GIANTâ‹† 0.13230.1179 0.10890.03750.02710.01910.7580 0.8250 0.8300 0.83910.7277 0.7921 0.7969 0.8057THLM 0.1206âˆ— 0.1159âˆ—0.1000âˆ— 0.0286âˆ— 0.0271âˆ—0.0162âˆ—0.7769âˆ— 0.8399âˆ— 0.8437âˆ— 0.8496âˆ— 0.7459âˆ— 0.8102âˆ— 0.8134âˆ—0.8157âˆ—\nPatents\nBERT 0.3274 0.3135 0.27640.1945 0.1829 0.12840.6248 0.6603 0.6910 0.64480.3791 0.4006 0.4192 0.3912RoBERTa0.3149 0.2926 0.25850.1836 0.1545 0.11190.6380 0.6735 0.7022 0.69850.38710.4087 0.4261 0.4238MetaPath0.4816 0.4842 0.48420.3372 0.3352 0.33530.1996 0.4385 0.4548 0.46540.1211 0.2660 0.2759 0.2824MetaPath+BERT0.2922 0.2840 0.23710.1483 0.14400.09440.6243 0.6583 0.6881 0.68770.3788 0.3994 0.4175 0.4173LinkBERTâ‹† 0.3080 0.3033 0.26010.1803 0.1738 0.11420.65040.67490.70480.70750.3946 0.40950.42770.4293GIANTâ‹† 0.27340.24540.22760.15370.1238 0.09760.6508 0.6709 0.6992 0.69390.3949 0.4071 0.4242 0.4210THLM 0.2522âˆ— 0.25130.2190âˆ— 0.1233âˆ— 0.1210âˆ—0.0848âˆ—0.7066âˆ— 0.7159âˆ— 0.7324âˆ— 0.7363âˆ— 0.4287âˆ— 0.4344âˆ— 0.4444âˆ—0.4467âˆ—\nhighlights the importance of incorporating both\nstructural information and textual representations\nfor each node to enhance overall performance.\nThird, we note that LinkBERT and GIANT\nachieve superior results in the majority of met-\nrics for link prediction. This highlights the ad-\nvantage of learning textual representations that\nconsider the graph structure. However, both GI-\nANT and LinkBERT may not yield satisfactory re-\nsults in node classification on the OAG-Venue and\nGoodReads. This could be attributed to two rea-\nsons: 1) these models primarily focus on first-order\ngraph topology while overlooking the importance\nof high-order structures, which are crucial in these\nscenarios; 2) these models are designed specifically\nfor homogeneous graphs and do not consider the\npresence of multiple types of relations within the\ngraph. Consequently, their effectiveness is limited\nin TAHGs and may impede their performance.\nMoreover, OAG-BERT demonstrates competi-\ntive results in link prediction and strong perfor-\nmance in node classification, thanks to its ability to\ncapture heterogeneity and topology during pretrain-\ning. This can be attributed to its capability to learn\nthe heterogeneity and topology of graphs. How-\never, it should be noted that OAG-BERT primar-\nily captures correlations between papers and their\nmetadata, such as authors and institutions, over-\nlooking high-order structural information. These\nfindings highlight the importance of considering\nboth graph structure and high-order relationships\nwhen developing models for graph-based tasks.\nFinally, THLM significantly outperforms the ex-\nisting models due to: 1) integrating multi-order\ngraph topology proximity into language models,\nwhich enables the model to capture a more com-\nprehensive understanding of the graph topology;\n2) enhancing the semantic representations for text-\nless nodes via aggregating the neighborsâ€™ textual\ndescriptions, that generates more informative rep-\nresentations for textless nodes.\n4.5 Analysis of Context Graph Prediction\nTo explore the impact of incorporating multi-order\ngraph topology into language models, we conduct\nseveral experiments. These experiments aim to in-\nvestigate the effects of both first- and high-order\ntopology information, as well as the modelâ€™s ability\nto capture structural information using R-HGNN.\nFor the remaining experiments on the analysis of\ndifferent components like CGP and the text aug-\nmentation strategy, we intentionally removed the\nMLM task to isolate its effects in THLM, namely\nTHLMâ‹† in Figure 3 and Table 2.\nEvaluation on Multi-order Topology Infor-\nmation. To assess the significance of multi-order\nneighborsâ€™ topology, we vary the number of or-\nders Kin extracting the context graph from 1 to 4.\nThe corresponding results are illustrated in Figure\n3. Besides, to examine the impact of high-order\nneighbors, we solely predict the 2-order neighbors\nin the context graph prediction task, as indicated\nby w/ 2-order CGP in Table 2.\nFrom the results, it is evident that THLM\n10321\nachieves superior performance when predicting\nmulti-order neighbors compared to solely predict-\ning 1-order or 2-order neighbors. This suggests\nthat modeling both first- and high-order structures\nenables LMs to acquire more comprehensive graph\ntopology. Additionally, we observe that THLM\nexhibits better results when Kis 2 in context graph\nprediction. However, its performance gradually\ndeclines as we predict neighbors in higher layers,\npotentially due to the reduced importance of topo-\nlogical information in those higher-order layers.\nMLP HetSANN RGCN R-HGNN\n0.69\n0.70\n0.71\n0.72\n0.73\n1 2 3 4\nPrecision\n(b) Performance on Patents\n0.72\n0.74\n0.76\n0.78\n0.80\n1 2 3 4\n0.82\n0.84\nPrecision\n(a) Performance on GoodReads\nFigure 3: Effects of learning multi-order topology infor-\nmation in TAHGs on node classification.\nTable 2: Evaluation of the ability to learn informative\nrepresentations via R-HGNN on node classification\nDatasets GCP MLP HetSANN RGCN R-HGNN\nOAG-Venue\nw/ MLP 0.2591 0.3195 0.3043 0.3379w/ RGCN0.27280.3323 0.3220 0.3547w/ 2-order CGP0.2609 0.3357 0.3121 0.3488w/ random feats0.2602 0.3271 0.3133 0.3487THLMâ‹† 0.26290.3383 0.3228 0.3554\nGoodReads\nw/ MLP 0.7528 0.8352 0.8376 0.8445w/ RGCN0.76080.8380 0.84110.8512w/ 2-order CGP0.7512 0.8319 0.8355 0.8431w/ random feats0.75230.83840.8406 0.8483THLMâ‹† 0.7549 0.83820.84250.8485\nPatents\nw/ MLP 0.6903 0.6963 0.7201 0.7208w/ RGCN0.6911 0.6986 0.7184 0.7218w/ 2-order CGP0.6827 0.6876 0.7057 0.7068w/ random feats0.6908 0.7001 0.7107 0.7198THLMâ‹† 0.6948 0.7050 0.7275 0.7280\nEvaluation on Learning Informative Node\nFeatures of R-HGNN. In this work, we adopt one\nof the state-of-the-art HGNNs, i.e., R-HGNN with\npre-initialized semantic features on nodes to obtain\nnode representations. To examine the importance\nof learning informative node representations and\ncomplex graph structure in R-HGNN, we conduct\nexperiments using two variants. Firstly, we replace\nR-HGNN with an MLP encoder or an alternative\nHGNN framework, i.e., RGCN (Schlichtkrull et al.,\n2018) in this experiment, denoted as w/ MLP and\nw/ RGCN respectively. Secondly, we substitute the\nsemantic node features with randomly initialized\ntrainable features, referred to as w/ random feats.\nThe performance results are presented in Table 2.\nFrom the obtained results, we deduce that both\nthe initial features and effective HGNNs contribute\nsignificantly to capturing graph topology and em-\nbedding informative node representations effec-\ntively. Firstly, unlike MLP, which fails to capture\nthe contextualized graph structure in the context\ngraph prediction task, RGCN allows for the em-\nbedding of fine-grained graph structural informa-\ntion, which facilitates better learning of the graph\ntopology. Furthermore, the utilization of effective\nHGNNs such as R-HGNN enables the embedding\nof expressive structural representations for nodes.\nSecondly, R-HGNN demonstrates its superior abil-\nity to learn more comprehensive graph structures\nfrom nodes compared to using randomly initialized\nfeatures. These findings underscore the importance\nof integrating both semantic and structural infor-\nmation to learn informative node representations.\n4.6 Analysis of Text Augmentation Strategy\nTable 3: Results on the node classification task in evalu-\nating the effectiveness of our text augmentation strategy.\nDataset Methods MLP HetSANN RGCN R-HGNN\nOAG-Venue\nneighbors-only0.2597 0.3274 0.3165 0.3495\ntextless-only0.2625 0.3290 0.3044 0.3516\nTAS(1-Neighbor)0.2611 0.3349 0.3201 0.3507\nTAS(2-Neighbor)0.2627 0.3380 0.3217 0.3549\nTAS(3-Neighbor)0.2629 0.3383 0.3228 0.3554\nGoodReads\nneighbors-only0.4855 0.7278 0.7132 0.7624\ntextless-only0.7453 0.8351 0.8397 0.8436\nTAS(1-Neighbor)0.7480 0.8353 0.8421 0.8469\nTAS(2-Neighbor)0.7547 0.83810.84260.8475\nTAS(3-Neighbor)0.7549 0.83820.84250.8485\nPatents\nneighbors-only0.69710.7040 0.7228 0.7224\ntextless-only0.6856 0.6923 0.7139 0.7164\nTAS(1-Neighbor)0.6959 0.7004 0.7211 0.7221\nTAS(2-Neighbor)0.69600.70500.7219 0.7233\nTAS(3-Neighbor)0.69480.7050 0.7275 0.7281\nTo explore the potential of enhancing semantic\ninformation for textless nodes through our text aug-\nmentation strategy, we design three experimental\nvariants. Firstly, we remove the text sequences of\ntextless nodes and solely rely on the texts of their\nneighbors as inputs, denoted as \"neighbors-only\".\nWe set the number of neighbors k as 3 for con-\ncatenation. Secondly, we only use the original text\ndescriptions of textless nodes to derive textual em-\nbeddings, namely \"textless-only\". Additionally, we\nemploy the text augmentation strategy by varying\nthe number of neighbors for concatenation from\n1 to 3, denoted as \"TAS(1-Neighbor)\", \"TAS(2-\nNeighbor)\", and \"TAS(3-Neighbor)\", respectively.\nFor all variants, we focus exclusively on the con-\ntext graph prediction task to isolate the effects of\n10322\nother factors. Due to space limitations, we present\nthe Micro-Precision(@1) metric for node classifi-\ncation in the experiments. Similar trends could be\nobserved across other metrics.\nFrom Table 3, we observe that both neighbors\nand textless nodes themselves are capable of learn-\ning the semantic information for textless nodes.\nHowever, relying solely on either of them may\nlead to insufficient textual representations for nodes.\nFurthermore, it is found that using texts from more\nneighbors can enhance the semantic quality of text-\nless nodes. Nevertheless, considering the limita-\ntions on the input sequence length of language mod-\nels, we observe that THLM achieves similar perfor-\nmance when the number ofkis increased beyond 2.\nTherefore, to strike a balance between performance\nand computational efficiency while accommodat-\ning sequence length limitations, we choose kas 3\nfor concatenation in the text augmentation strategy.\nTo ensure the reliability of our findings, we conduct\nthe task five times using different seeds ranging\nfrom 0 to 4. Remarkably, all obtained p-values are\nbelow 0.05, indicating statistical significance and\nconfirming the accuracy improvement achieved by\nour text augmentation strategy.\n4.7 Effects of Two Pretraining Tasks\nTo study the importance of two pretraining tasks for\ndownstream tasks, we use two variants of THLM\nto conduct the experiments, and the performance\nis shown in Figure 4. Specifically, THLM w/o\nCGP removes the context graph prediction task,\nwhich does not predict the context neighbors for the\ninput node. THLM w/o MLM reduces the masked\nlanguage modeling task, which ignores the textual\ndependencies in the sentences and only predicts\nthe multi-order graph topology in the pretraining\nprocess, i.e., by predicting the neighbors involved\nin the context graphs for input nodes.\nTHLM w/o CGP\n THLM w/o MLM\n THLM\n0.64\n0.66\n0.68\n0.70\n0.72\nMLP HetSANN RGCN R-HGNN\nPrecision\n(b) Performance on Patents\n0.74\n0.76\n0.78\n0.80\n0.82\nMLP HetSANN RGCN R-HGNN\n0.84Precision\n(a) Performance on GoodReads\nFigure 4: Importance of two pretraining tasks on the\nnode classification task.\nFrom Figure 4, we can conclude that THLM\nachieves the best performance when it employs\nboth two pretraining tasks for training. Removing\neither of these tasks leads to a decrease in the re-\nsults. In particular, the context graph prediction\ntask significantly contributes to the overall perfor-\nmance, demonstrating the substantial benefits of\nincorporating graph topology into our LM. Addi-\ntionally, the masked language modeling task helps\ncapture the semantics within texts better and fur-\nther enhances the model performance. Besides, we\nfind that THLM w/o MLM performs better than the\noriginal BERT on two datasets, which contributes\nto our text augmentation strategy for textless nodes.\nThis enhancement allows for better connectivity\nbetween the brief terms of textless nodes and their\nneighboring text sequences, resulting in improved\ncontextual understanding and representation in pre-\ntraining PLMs.\n5 Related work\n5.1 Pretrained Language Models\nThe objective of PLMs is to learn general represen-\ntations of texts from large and unlabeled corpora\nvia pretraining tasks, which could be applied to a\nvariety of downstream tasks. Pretraining tasks that\nmost PLMs widely used include 1) masked lan-\nguage modeling in BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019); 2) next token predic-\ntion in GPT models (Radford et al., 2018; Brown\net al., 2020); and 3) autoregressive blank infilling in\nGLM (Du et al., 2022). However, these tasks sep-\narately focus on the modeling within single texts\nand ignore the correlation among multiple texts.\nRecently, several works have been proposed to\ncapture the connections between different texts\nLevine et al. (2022); Chien et al. (2022); Yasunaga\net al. (2022). For example, Chien et al. (2022) inte-\ngrated the graph topology into LMs by predicting\nthe connected neighbors of each node. Yasunaga\net al. (2022) designed the document relation pre-\ndiction task to pretrain LMs, which aims to clas-\nsify the type of relation (contiguous, random, and\nlinked) existing between two input text segments.\nAlthough insightful, these methods just consider\nthe first-order connections between texts and can-\nnot leverage high-order signals, which may lead\nto suboptimal performance. In this paper, we aim\nto present a new pretraining framework for LMs\nto help them comprehensively capture multi-order\nrelationships as well as heterogeneous information\nin a more complicated data structure, i.e., TAHGs.\n10323\n5.2 Heterogeneous Graph Learning\nGraph Neural Networks (GNNs) (Kipf and Welling,\n2017; Hamilton et al., 2017) have gained much\nprogress in graph learning, which are extensively\napplied in modeling graph-structure data. Recently,\nmany researchers have attempted to extend GNNs\nto heterogeneous graphs (Zhang et al., 2019; Fu\net al., 2020; Hong et al., 2020; Yu et al., 2020; Hu\net al., 2020b; Lv et al., 2021), which are powerful\nin handling different types of nodes and relations as\nwell as the graph topological information. In this\nwork, we aim to inject the graph learning ability of\nheterogeneous graph neural networks into PLMs\nvia a topology-aware pretraining task.\n5.3 Text-rich Network Mining\nMany real-world scenarios (academic networks,\npatent graphs) can be represented by text-rich net-\nworks, where nodes are associated with rich text\ndescriptions. Existing methods for text-rich net-\nwork mining can be divided into two categories.\nThe first branch designs the cascade architecture\nto learn the textual information by Transformer\n(Vaswani et al., 2017) and network topology by\ngraph neural networks separately (Zhu et al., 2021;\nLi et al., 2021; Pang et al., 2022). Another group\nnests GNNs into LMs to collaboratively explore\nthe textual and topological information (Yang et al.,\n2021; Jin et al., 2022, 2023a,b). However, these\nworks either mainly focus on the homogeneous\ngraph or modify the architecture of LMs by incor-\nporating extra components. For example, Heter-\nformers (Jin et al., 2023b) is developed for text-\nrich heterogeneous networks, which aims to embed\nnodes with rich text and their one-hop neighbors\nby leveraging the power of both LMs and GNNs\nduring pretraining and downstream tasks. Different\nfrom these works, we learn about the more compli-\ncated TAHGs and employ auxiliary heterogeneous\ngraph neural networks to assist LMs in capturing\nthe rich information in TAHGs. After the pretrain-\ning, we discard the auxiliary networks and only\napply the pretrained LMs for downstream tasks\nwithout changing their original architectures.\n6 Conclusion\nIn this paper, we pretrained language models on\nmore complicated text-attributed heterogeneous\ngraphs, instead of plain texts. We proposed the\ncontext graph prediction task to inject the graph\nlearning ability of graph neural networks into LMs,\nwhich jointly optimizes an auxiliary graph neural\nnetwork and an LM to predict which nodes are\ninvolved in the context graph. To handle imbal-\nanced textual descriptions of different nodes, a text\naugmentation strategy was introduced, which en-\nriches the semantics of textless nodes by combin-\ning their neighborsâ€™ texts. Experimental results\non three datasets showed that our approach could\nsignificantly and consistently outperform existing\nmethods across two downstream tasks.\n7 Limitations\nIn this work, we pretrained language models on\nTAHGs and evaluated the model performance on\nlink prediction and node classification tasks. Al-\nthough our approach yielded substantial improve-\nments over baselines, there are still several promis-\ning directions for further investigation. Firstly, we\njust focused on pretraining encoder-only LMs, and\nit is necessary to validate whether encoder-decoder\nor decoder-only LMs can also benefit from the\nproposed pretraining task. Secondly, more down-\nstream tasks that are related to texts (e.g., retrieval\nand reranking) can be compared in the experiments.\nThirdly, it is interesting to explore the pretraining\nof LMs in larger scales on TAHGs.\n8 Acknowledgements\nThis work was supported by the National Nat-\nural Science Foundation of China (51991395,\n62272023), and the Fundamental Research Funds\nfor the Central Universities (No. YWF-23-L-717,\nNo. YWF-23-L-1203).\nReferences\nRui Bing, Guan Yuan, Mu Zhu, Fanrong Meng, Huifang\nMa, and Shaojie Qiao. 2022. Heterogeneous graph\nneural networks analysis: a survey of techniques,\nevaluations and applications. Artificial Intelligence\nReview, pages 1â€“40.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: commonsense transformers for auto-\nmatic knowledge graph construction. In ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 4762â€“4779. Association for Com-\nputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\n10324\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS 2020, December 6-12, 2020, virtual.\nEli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-\nFu Yu, Jiong Zhang, Olgica Milenkovic, and Inder-\njit S. Dhillon. 2022. Node feature extraction by self-\nsupervised multi-scale neighborhood prediction. In\nICLR 2022, Virtual Event, April 25-29, 2022.\nPeng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. 2019.\nA survey on network embedding. IEEE Trans. Knowl.\nData Eng., 31(5):833â€“852.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pages 4171â€“4186. Association for Compu-\ntational Linguistics.\nYuxiao Dong, Nitesh V . Chawla, and Ananthram Swami.\n2017. metapath2vec: Scalable representation learn-\ning for heterogeneous networks. In KDD, Halifax,\nNS, Canada, August 13 - 17, 2017, pages 135â€“144.\nACM.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\ngeneral language model pretraining with autoregres-\nsive blank infilling. In ACL 2022, Dublin, Ireland,\nMay 22-27, 2022 , pages 320â€“335. Association for\nComputational Linguistics.\nXinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King.\n2020. MAGNN: metapath aggregated graph neural\nnetwork for heterogeneous graph embedding. In\nWWW â€™20: The Web Conference, pages 2331â€“2341.\nACM / IW3C2.\nAditya Grover and Jure Leskovec. 2016. node2vec:\nScalable feature learning for networks. In KDD, San\nFrancisco, CA, USA, August 13-17, 2016, pages 855â€“\n864. ACM.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In NIPS 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 1024â€“1034.\nXiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-\nDong Zhang, and Meng Wang. 2020. Lightgcn: Sim-\nplifying and powering graph convolution network for\nrecommendation. In SIGIR, pages 639â€“648. ACM.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMÃ¼ller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table pars-\ning via pre-training. In ACL 2020, pages 4320â€“4333.\nAssociation for Computational Linguistics.\nHuiting Hong, Hantao Guo, Yucheng Lin, Xiaoqing\nYang, Zang Li, and Jieping Ye. 2020. An attention-\nbased graph neural network for heterogeneous struc-\ntural learning. In AAAI, pages 4132â€“4139. AAAI\nPress.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong,\nHongyu Ren, Bowen Liu, Michele Catasta, and Jure\nLeskovec. 2020a. Open graph benchmark: Datasets\nfor machine learning on graphs. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020b. Heterogeneous graph transformer. In\nWWW â€™20, Taipei, Taiwan, April 20-24, 2020, pages\n2704â€“2710. ACM / IW3C2.\nBowen Jin, Yu Zhang, Yu Meng, and Jiawei Han. 2023a.\nEdgeformers: Graph-empowered transformers for\nrepresentation learning on textual-edge networks.\nCoRR, abs/2302.11050.\nBowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. 2022.\nHeterformer: A transformer architecture for node\nrepresentation learning on heterogeneous text-rich\nnetworks. CoRR, abs/2205.10282.\nBowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. 2023b.\nHeterformer: Transformer-based deep node represen-\ntation learning on heterogeneous text-rich networks.\nIn KDD 2023, Long Beach, CA, USA, August 6-10,\n2023, pages 1020â€“1031. ACM.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020.\nYoav Levine, Noam Wies, Daniel Jannai, Dan Navon,\nYedid Hoshen, and Amnon Shashua. 2022. The\ninductive bias of in-context learning: Rethinking\npretraining example design. In ICLR 2022, Virtual\nEvent, April 25-29, 2022. OpenReview.net.\nChaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun,\nZheng Liu, Xing Xie, Tianqi Yang, Yanling Cui,\nLiangjie Zhang, and Qi Zhang. 2021. Adsgnn:\nBehavior-graph augmented relevance modeling in\nsponsored search. In SIGIR â€™21, Virtual Event,\nCanada, July 11-15, 2021, pages 223â€“232. ACM.\nXiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng\nZhang, Hongxia Yang, Yuxiao Dong, and Jie Tang.\n2022. OAG-BERT: towards a unified backbone lan-\nguage model for academic knowledge services. In\nKDD â€™22, Washington, DC, USA, August 14 - 18,\n2022, pages 3418â€“3428. ACM.\n10325\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net.\nQingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen,\nWenzheng Feng, Siming He, Chang Zhou, Jianguo\nJiang, Yuxiao Dong, and Jie Tang. 2021. Are we\nreally making much progress?: Revisiting, bench-\nmarking and refining heterogeneous graph neural net-\nworks. In KDD â€™21, Virtual Event, Singapore, August\n14-18, 2021, pages 1150â€“1160. ACM.\nTomÃ¡s Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In NIPS 2013. Lake Tahoe, Nevada, United\nStates, pages 3111â€“3119.\nBochen Pang, Chaozhuo Li, Yuming Liu, Jianxun Lian,\nJianan Zhao, Hao Sun, Weiwei Deng, Xing Xie, and\nQi Zhang. 2022. Improving relevance modeling via\nheterogeneous behavior graph learning in bing ads.\nIn KDD â€™22, Washington, DC, USA, August 14 - 18,\n2022, pages 3713â€“3721. ACM.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In ESWC 2018 , volume\n10843 of Lecture Notes in Computer Science, pages\n593â€“607. Springer.\nYu Shi, Jiaming Shen, Yuchen Li, Naijing Zhang, Xin-\nwei He, Zhengzhi Lou, Qi Zhu, Matthew Walker,\nMyunghwan Kim, and Jiawei Han. 2019. Discover-\ning hypernymy in text-rich heterogeneous informa-\ntion network by exploiting context granularity. In\nCIKM 2019, Beijing, China, November 3-7, 2019 ,\npages 599â€“608. ACM.\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. J. Mach. Learn. Res., 15(1):1929â€“\n1958.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 5998â€“6008.\nMengting Wan and Julian J. McAuley. 2018. Item rec-\nommendation on monotonic behavior chains. In\nProceedings of the 12th ACM Conference on Rec-\nommender Systems, RecSys 2018, Vancouver, BC,\nCanada, October 2-7, 2018, pages 86â€“94. ACM.\nMengting Wan, Rishabh Misra, Ndapa Nakashole, and\nJulian J. McAuley. 2019. Fine-grained spoiler detec-\ntion from large-scale review corpora. In ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1:\nLong Papers, pages 2605â€“2610. Association for Com-\nputational Linguistics.\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and\nTat-Seng Chua. 2019. Neural graph collaborative\nfiltering. In SIGIR, pages 165â€“174. ACM.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2023. Should you mask 15% in masked\nlanguage modeling? In EACL 2023, Dubrovnik,\nCroatia, May 2-6, 2023, pages 2977â€“2992. Associa-\ntion for Computational Linguistics.\nJunhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo\nLi, Defu Lian, Sanjay Agrawal, Amit Singh,\nGuangzhong Sun, and Xing Xie. 2021. Graphform-\ners: Gnn-nested transformers for representation learn-\ning on textual graph. In NeurIPS 2021, December\n6-14, 2021, virtual, pages 28798â€“28810.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS 2019,, pages 5754â€“\n5764.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links. In ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 8003â€“8016. Association for Com-\nputational Linguistics.\nLe Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng\nLv, and Hui Xiong. 2020. Hybrid micro/macro level\nconvolution for heterogeneous graph learning. CoRR,\nabs/2012.14722.\nLe Yu, Leilei Sun, Bowen Du, Chuanren Liu, Weifeng\nLv, and Hui Xiong. 2022. Heterogeneous graph rep-\nresentation learning with relation awareness. IEEE\nTransactions on Knowledge and Data Engineering.\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram\nSwami, and Nitesh V . Chawla. 2019. Heterogeneous\ngraph neural network. In KDD â€™19, pages 793â€“803.\nACM.\nJason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li,\nMarkus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei\nZhang, and Huasha Zhao. 2021. Textgnn: Improving\ntext encoder via graph neural network in sponsored\nsearch. In WWW â€™21, Ljubljana, Slovenia, April 19-\n23, 2021, pages 2848â€“2857. ACM / IW3C2.\n10326\nA Appendix\nA.1 Datasets and Baselines\nDatasets. Specific statistics of datasets are shown\nin Table 4 and detailed descriptions of datasets are\nshown as follows.\nâ€¢ OAG-Venue: OAG-Venue 5 is a heteroge-\nneous graph followed by Hu et al. (2020b),\nwhich includes papers (P), authors (A), fields\n(F) and institutions (I). Each paper is pub-\nlished in a single venue. We treat papers as\nrich-text nodes and extract the title and ab-\nstract parts as their text descriptions. Authors,\nfields, and institutions are regarded as textless\nnodes, whose text descriptions are composed\nof their definitions or names.\nâ€¢ GoodReads: Following (Wan and McAuley,\n2018; Wan et al., 2019), we receive a subset\nof GoodReads6, which contains books (B),\nauthors (A) and publishers (P). Each book is\ncategorized into one or more genres. We treat\nbooks as rich-text nodes and extract brief in-\ntroductions as their text descriptions. Authors\nand publishers are regarded as textless nodes,\nwhose text descriptions are their names.\nâ€¢ Patents: Patents is a heterogeneous graph\ncollected from the USPTO7, which contains\npatent documents (P), applicants (A) and\napplied companies (C). Each patent is as-\nsigned several International Patent Classifica-\ntion (IPC) codes. We treat patents as rich-text\nnodes and extract the title and abstract parts\nas their text descriptions. Applicants and com-\npanies use their names as text descriptions,\nregarded as textless nodes.\nBaselines. We compare our model with the fol-\nlowing baselines: BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) are popular encoder-\nonly pretraining language models. MetaPath (Dong\net al., 2017) leverages meta-path-based random\nwalks in the heterogeneous graph to generate node\nembeddings. MetaPath+BERT combines the tex-\ntual embeddings embedded from BERTbase and\nstructural representations learned from MetaPath as\nnode features. LinkBERT (Yasunaga et al., 2022)\ncaptures the dependencies across documents by\n5https://github.com/UCLA-DM/pyHGT\n6https://sites.google.com/eng.ucsd.edu/\nucsdbookgraph/home\n7https://www.uspto.gov/\npredicting the relation between two segments on\nWikipedia and BookCorpus. GIANT (Chien et al.,\n2022) extracts graph-aware node embeddings from\nraw text data via neighborhood prediction in the\ngraph. OAG-BERT (Liu et al., 2022) is a pre-\ntrained language model specialized in academic\nknowledge services, allowing for the incorporation\nof heterogeneous entities such as authors, institu-\ntions, and keywords into paper embeddings.\nA.2 Headers in Downstream Tasks\nWe apply four methods on downstream tasks,\nwhich could be shown as follows,\nâ€¢ MLP relies exclusively on node features as\ninput and uses the multilayer perceptron for\nprediction, which does not consider the graph\ninformation.\nâ€¢ RGCN incorporates the different relation-\nships among nodes by using transformation\nmatrices respectively in the knowledge graphs\n(Schlichtkrull et al., 2018).\nâ€¢ HetSANN aggregates different types of re-\nlations information from neighbors with a\ntype-aware attention mechanism (Hong et al.,\n2020).\nâ€¢ R-HGNN learns the relation-aware node rep-\nresentation by integrating fine-grained repre-\nsentation on each set of nodes within separate\nrelations, and semantic representations across\ndifferent relations (Yu et al., 2022).\nA.3 Evaluation Metrics\nSeven metrics are adopted to comprehensively eval-\nuate the performance of different models in link\nprediction and node classification. In link predic-\ntion, we use Root Mean Square Error (RMSE) and\nMean Absolute Error (MAE) metrics. In node clas-\nsification, we use Micro-Precision, Micro-Recall,\nMacro-Precision, Macro-Recall, and Normalized\nDiscounted Cumulative Gain (NDCG) metrics for\nevaluation. Details of the metrics are shown below.\nRMSE evaluates the predicted ability for truth\nvalues, which calculates the error between predic-\ntion results and truth values. Given the prediction\nfor all examples Ë†y = {Ë†y1,Ë†y2,Â·Â·Â· ,Ë†ym}, and the\ntruth data y= {y1,y2,Â·Â·Â· ,ym}, we calculate the\n10327\nTable 4: Statistics of the datasets.\nDatasets Nodes Edges Average\nText LengthCategory Classification\nSplit Sets\nLink Prediction\nSplit Sets\nOAG-Venue\n# Paper (P): 167,004\n# Author (A): 511,122\n# Field (F): 45,775\n# Institution (I): 9,090\n# P-F: 1,709,601\n# P-P: 864,019\n# A-I: 614,161\n# P-A: 480,104\nP: 243.497\nA: 5.667\nF: 3.690\nI: 5.882\n242\nTrain: 106,724\nValidation: 24,433\nTest: 35,847\nTrain: 144,030\nValidation: 48,010\nTest: 48,010\nGoodReads\n# Book (B): 364,115\n# Author (A): 154,418\n# Publisher (P): 40,135\n# B-A: 572,654\n# B-P: 466,626\nB: 163.577\nA: 4.100\nP: 5.120\n8\nTrain: 254,880\nValidation: 54,617\nTest: 54,618\nTrain: 139,988\nValidation: 46,662\nTest: 46,662\nPatents\n# Patent (P): 363,528\n# Applicant (A): 182,561\n# Company (C): 1,000\n# P-C: 367,598\n# P-A: 334,906\nP: 139.436\nA: 6.418\nC: 8.436\n565\nTrain: 254,469\nValidation: 54,529\nTest: 54,530\nTrain: 110,277\nValidation: 36,759\nTest: 36,759\ntotal RMSE as follows,\nRMSE(Ë†y,y) =\nîµªîµ«îµ«âˆš 1\nm\nmâˆ‘\ni=1\n( Ë†yi âˆ’yi)2.\nMAE measures the absolute errors between pre-\ndictions and truth values. Given the prediction for\nall examples Ë†y = {Ë†y1,Ë†y2,Â·Â·Â· ,Ë†ym}, and the truth\ndata y = {y1,y2,Â·Â·Â· ,ym}, we calculate the total\nMAE as follows,\nMAE(Ë†y,y) = 1\nm\nmâˆ‘\ni=1\n|Ë†yi âˆ’yi|.\nMicro-averaged precision measures the ability\nthat recognizes more relevant elements than irrel-\nevant ones in all classes. We select the top-K pre-\ndicted labels as predictions for each sample. Hence,\nMicro-Precision@K is the proportion of positive\npredictions that are correct over all classes, which\nis calculated by,\nMicro-Precision@K =\nâˆ‘\nciâˆˆC TP(ci)âˆ‘\nciâˆˆC TP(ci) +FP(ci),\nwhere TP(ci), FP(ci) is the number of true posi-\ntives, and false positives for class ci respectively.\nMicro-averaged recall evaluates the modelâ€™s abil-\nity in selecting all the relevant elements in all\nclasses. We select the top-K probability predicted\nlabels as predictions for each sample. Hence,\nMicro-Recall@K is the proportion of positive la-\nbels that are correctly predicted over all classes,\nwhich is calculated by,\nMicro-Recall@K =\nâˆ‘\nciâˆˆC TP(ci)âˆ‘\nciâˆˆC TP(ci) +FN(ci),\nwhere TP(ci), FN(ci) is the number of true posi-\ntives, and false negatives for class ci respectively.\nMacro-averaged precision reflects the average\nability to recognize the relevant elements rather\nthan irrelevant ones in each class. We select the\ntop-K probability predicted labels as predictions.\nHence, Macro-Precision@K is calculated by aver-\naging all the precision values of all classes,\nMacro-Precision@K =\nâˆ‘\nciâˆˆC P( Ë†S,S,c i)\n|C| ,\nwhere Ë†S, S represents the predicted values and\ntruth labels in the datasets, P( Ë†S,S,c i) is the preci-\nsion value of class ci.\nMacro-averaged recall evaluates the average abil-\nity to select all the relevant elements in each class.\nWe select the top-K probability predicted labels as\npredictions. Hence, Macro-Recall@K is calculated\nby averaging all the recall values of all classes,\nMacro-Recall@K =\nâˆ‘\nciâˆˆC R( Ë†S,S,c i)\n|C| ,\nwhere Ë†S, S represents the predicted values and\ntruth labels in the datasets, R( Ë†S,S,c i) is the recall\nvalue of class ci.\nNDCG measures the ranking quality by consid-\nering the orders of all labels. For each sample pi,\nNDCG is calculated by\nNDCG@K(pi) =\nâˆ‘K\nk=1\nÎ´( Ë†Sk\ni ,Si)\nlog2(k+1)\nâˆ‘min(K,|Si|)\nk=1\n1\nlog2(k+1)\n,\nwhere Ë†Sk\ni denotes the k-th predicted label of ex-\nample pi. Î´(v,S) is 1 when element vis in set S,\notherwise 0. We calculate the average NDCG of\nall examples as a metric.\n10328\nA.4 Detailed Settings in Downstream Tasks\nIn downstream tasks, we search the hidden di-\nmension of node representation for headers in\n[32,64,128,256,512]. For methods that use atten-\ntion mechanisms, (i.e., HetSANN and R-HGNN),\nthe number of attention heads is searched in\n[1,2,4,8,16]. The training process is following\nR-HGNN (Yu et al., 2022).\nA.5 Detailed Experimental Results\nWe show the Macro-Precision(@1) and Macro-\nRecall(@1) in the node classification task on three\ndatasets in Table 7. Since the values of NDCG(@1)\nare the same as Micro-Precision(@1), we do not\nshow duplicate results. Besides, since node classi-\nfication tasks on GoodReads and Patents belong to\nmulti-label node classification, we show the perfor-\nmance on five metrics when K is 3 and 5 in Table\n8 and Table 9 respectively.\nA.6 LinkBERT & GIANT\nIn our baselines, LinkBERT and GIANT are specif-\nically designed for homogeneous text-attributed\ngraphs, which cannot be directly applied in TAHGs.\nTo address this, we convert the TAHGs into homo-\ngeneous graphs that contain the set of rich-text\nnodes and their connections to ensure that all nodes\ncontain rich semantic information in the graphs.\nFor Patents and GoodReads, we extract the 2-order\nrelationships in the graph and discard the textless\nnodes along with their relative edges to construct\nthe homogeneous graphs. In the case of the OAG-\nVenue dataset, due to the high density of the second-\norder graph, we choose to construct a homogeneous\ngraph using a subset of crucial meta-path informa-\ntion to save the graph topology as much as possible.\nInspired by Yu et al. (2022). we utilize the meta-\npath \"P-F-P\" (Paper-Field-Paper) and the direct\nrelation \"P-P\" (Paper-Paper) to build the homoge-\nneous graph for conducting experiments.\nIn addition to previous experiments, we con-\nducted another experiment to capture the first-order\ninformation in the TAHGs while preserving the\ngraph topology as much as possible. Specifically,\nwe discard the heterogeneity of nodes and relation-\nships in the graph to build a homogeneous graph,\nand the results are shown in Table 5.\nFrom Table 5, it is evident that pretraining\nLinkBERT and GIANT on TAHGs solely for 1-\norder prediction may not yield optimal results.\nThere are two key reasons for this observation: 1)\nTable 5: Performance on node classification in\nLinkBERT and GIANT.\nDatasets Model Micro-Precision(@1)\nMLP HetSANN RGCN R-HGNN\nGoodReads\nLinkBERT(1-order)0.6790 0.8100 0.8044 0.8302\nGIANT(1-order)0.6967 0.8247 0.8284 0.8398\nTHLM 0.7769 0.8399 0.8437 0.8496\nPatents\nLinkBERT(1-order)0.5972 0.6421 0.6773 0.6734\nGIANT(1-order)0.4793 0.6234 0.6323 0.6391\nTHLM 0.7066 0.7159 0.7324 0.7363\ntextless nodes always lack sufficient textual content,\nleading to scarce semantic information. Hence, pre-\ndicting relationships between textless nodes and\ntheir neighbors becomes challenging for language\nmodels. 2) Apart from first-order neighbors, high-\norder neighbors provide more complex structure in-\nformation within the graph. By considering the re-\nlationships beyond the immediate neighbors, LMs\ncould capture the graph topology across nodes\nmore effectively and comprehensively. These find-\nings highlight the importance of considering both\nfirst-order and high-order structure information in\nTAHGs and addressing the challenges of limited\nsemantics on textless nodes. By tackling both prob-\nlems, our model can learn better in TAHGs.\nA.7 Effect of Distinguishing Treasured\nStructural Information\nMLP HetSANN RGCN R-HGNN\n0.69\n0.70\n0.71\n0.72\n0.73\n1 3 5 7\nPrecision\n(b) Performance on Patents\n0.3\n0.4\n0.5\n0.6\n0.7\n1 3 5 7\n0.8\nPrecision\n(a) Performance on GoodReads\nFigure 5: Precision on node classification with different\nnumbers in sampling negative candidates in the pretrain-\ning process.\nWe investigate the effect of treasured structural\ninformation in the TAHGs. Specifically, we solely\nchange the number of negative candidates for each\npositive entity in the context graph prediction task\nin [1,3,5,7] in the pretraining stage. We present\nthe performance of GoodReads and Patents on the\nMicro-Precision(@1) metric in the node classifica-\ntion task.\nFrom Figure 5, we could observe that the per-\nformance with a smaller number or larger number\nin sampling negative candidates would be worse.\nThis observation can be explained by two factors.\n10329\nFirstly, the model may receive limited structural in-\nformation when selecting a smaller number of nega-\ntive candidates, which hampers the modelâ€™s ability\nto understand the underlying topology structure\neffectively. Secondly, sampling a larger number\nof negative candidates may bring noise topologi-\ncal information and make it difficult to distinguish\nmeaningful patterns and relationships. Hence, the\noptimal performance is achieved when the num-\nber of sampled negative candidates falls within a\nproper range. By striking a balance between learn-\ning sufficient topological information and avoiding\nexcessive noise, the model can effectively capture\nthe graph structure and achieve better performance\nin downstream tasks.\nA.8 Performance on Large-scale Datasets\nIn our evaluation, we further test THLM on large-\nscale datasets (i.e., obgn-mag dataset (Hu et al.,\n2020a)) for the node classification task. The per-\nformance is shown in Table 6. We observe that\nTHLM demonstrates scalability to larger datasets,\noutperforming baselines such as LinkBERT and\nGIANT. This outcome highlights the effectiveness\nof THLM, particularly its superior performance on\nthe obgn-mag dataset.\nTable 6: The accuracy results for node classification on\nthe obgn-mag dataset.\nModel MLP HetSANN RGCN\nBERT 0.3754 0.5298 0.5484\nRoBERTa 0.3770 0.5300 0.5490\nLinkBERTâ‹† 0.3775 0.5230 0.5491\nGIANTâ‹† 0.3903 0.5184 0.5256\nTHLM 0.3933 0.5353 0.5517\n10330\nTable 7: Performance of different methods on three datasets in node classification. The best and second-best\nperformances are boldfaced and underlined.\nDatasets Model Macro-Precision(@1)â†‘ Macro-Recall(@1)â†‘\nMLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN\nOAG-Venue\nBERT 0.2110 0.3104 0.3119 0.3359 0.1992 0.3118 0.3060 0.3415\nRoBERTa 0.2429 0.3264 0.3258 0.3598 0.2387 0.3187 0.3169 0.3412\nMetaPath 0.0959 0.2593 0.2830 0.3005 0.0717 0.2731 0.2663 0.3019\nMetaPath+BERT0.2094 0.3202 0.3248 0.3363 0.1991 0.3150 0.3180 0.3368\nLinkBERTâ‹† 0.2054 0.2921 0.3014 0.3479 0.2060 0.3057 0.2902 0.3233\nGIANTâ‹† 0.2026 0.3078 0.3080 0.3381 0.2005 0.3097 0.2858 0.3188\nTHLM 0.2506 0.3375 0.3408 0.3562 0.2464 0.3330 0.3331 0.3537\nGoodReads\nBERT 0.7352 0.8273 0.8253 0.8421 0.7040 0.7960 0.7969 0.8112\nRoBERTa 0.7420 0.8290 0.8328 0.8428 0.7134 0.7994 0.8039 0.8120\nMetaPath 0.1786 0.6599 0.6560 0.6966 0.1371 0.6204 0.6225 0.6624\nMetaPath+BERT0.7285 0.8286 0.8356 0.8425 0.7015 0.7978 0.8026 0.8104\nLinkBERTâ‹† 0.7178 0.8239 0.8276 0.8389 0.6917 0.7932 0.7987 0.8091\nGIANTâ‹† 0.7622 0.8273 0.8329 0.8418 0.7331 0.7970 0.8018 0.8109\nTHLM 0.7798 0.8472 0.8493 0.8515 0.7516 0.8148 0.8184 0.8209\nPatents\nBERT 0.3526 0.3876 0.4073 0.2994 0.1587 0.1864 0.1795 0.1335\nRoBERTa 0.3262 0.3918 0.4185 0.4227 0.1506 0.1941 0.1801 0.1846\nMetaPath 0.0854 0.1894 0.1862 0.2059 0.0153 0.0941 0.0930 0.0946\nMetaPath+BERT0.3330 0.3827 0.4072 0.4263 0.1577 0.1866 0.1842 0.1929\nLinkBERTâ‹† 0.3458 0.3838 0.4182 0.4515 0.1649 0.1858 0.1884 0.1920\nGIANTâ‹† 0.3506 0.3904 0.4194 0.4327 0.1764 0.1995 0.1928 0.1944\nTHLM 0.4374 0.4364 0.4466 0.4974 0.2090 0.2128 0.2115 0.2281\n10331\nTable 8: Performance of different methods on GoodReads in node classification. The best and second-best\nperformances are boldfaced and underlined.\nMetric Model K=3 K=5\nMLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN\nMacro-Precision\nBERT 0.3402 0.3645 0.3520 0.3637 0.2146 0.2193 0.2095 0.2136\nRoBERTa 0.3418 0.3602 0.3552 0.3707 0.2145 0.2174 0.2104 0.2166\nMetaPath 0.1463 0.3374 0.3283 0.3417 0.1415 0.2187 0.2107 0.2142\nMetaPath+BERT0.3377 0.3676 0.3605 0.3749 0.2138 0.2202 0.2137 0.2182\nLinkBERTâ‹† 0.3350 0.3688 0.3543 0.3647 0.2118 0.2209 0.2114 0.2133\nGIANTâ‹† 0.3526 0.3671 0.3609 0.3702 0.2186 0.2187 0.2146 0.2189\nTHLM 0.3458 0.3753 0.3647 0.3717 0.2139 0.2232 0.2125 0.2136\nMacro-Recall\nBERT 0.9368 0.9766 0.9755 0.9804 0.9814 0.9941 0.9935 0.9947\nRoBERTa 0.9431 0.9791 0.9792 0.9819 0.9851 0.9948 0.9945 0.9952\nMetaPath 0.3950 0.8608 0.8585 0.8863 0.6461 0.9445 0.9395 0.9554\nMetaPath+BERT0.9357 0.9762 0.9788 0.9803 0.9806 0.9939 0.9949 0.9950\nLinkBERTâ‹† 0.9283 0.9756 0.9760 0.9785 0.9768 0.9938 0.9935 0.9942\nGIANTâ‹† 0.9502 0.9766 0.9775 0.9803 0.9862 0.9947 0.9945 0.9951\nTHLM 0.9615 0.9829 0.9846 0.9836 0.9899 0.9961 0.9959 0.9957\nMicro-Precision\nBERT 0.3252 0.3391 0.3386 0.3403 0.2046 0.2071 0.2070 0.2072\nRoBERTa 0.3274 0.3399 0.3399 0.3409 0.2053 0.2072 0.2072 0.2073\nMetaPath 0.1438 0.2999 0.2991 0.3086 0.1410 0.1976 0.1964 0.1995\nMetaPath+BERT0.3249 0.3389 0.3399 0.3404 0.2044 0.2071 0.2073 0.2073\nLinkBERTâ‹† 0.3222 0.3388 0.3388 0.3397 0.2037 0.2071 0.2070 0.2071\nGIANTâ‹† 0.3299 0.3390 0.3393 0.3404 0.2056 0.2072 0.2072 0.2073\nTHLM 0.3338 0.3413 0.3418 0.3414 0.2063 0.2075 0.2075 0.2074\nMicro-Recall\nBERT 0.9368 0.9767 0.9753 0.9802 0.9821 0.9942 0.9936 0.9947\nRoBERTa 0.9430 0.9790 0.9791 0.9820 0.9854 0.9948 0.9945 0.9954\nMetaPath 0.4142 0.8638 0.8614 0.8888 0.6767 0.9484 0.9427 0.9578\nMetaPath+BERT0.9357 0.9762 0.9791 0.9805 0.9813 0.9941 0.9952 0.9952\nLinkBERTâ‹† 0.9281 0.9758 0.9758 0.9785 0.9777 0.9941 0.9936 0.9943\nGIANTâ‹† 0.9502 0.9764 0.9774 0.9804 0.9868 0.9948 0.9946 0.9953\nTHLM 0.9613 0.9831 0.9846 0.9835 0.9902 0.9962 0.9960 0.9957\nNDCG\nBERT 0.8526 0.9164 0.9158 0.9252 0.8713 0.9236 0.9233 0.9312\nRoBERTa 0.8600 0.9192 0.9209 0.9266 0.8776 0.9257 0.9273 0.9321\nMetaPath 0.3008 0.7740 0.7735 0.8070 0.4089 0.8088 0.8072 0.8354\nMetaPath+BERT0.8507 0.9170 0.9214 0.9253 0.8695 0.9244 0.9280 0.9314\nLinkBERTâ‹† 0.8413 0.9149 0.9168 0.9231 0.8617 0.9224 0.9241 0.9295\nGIANTâ‹† 0.8732 0.9168 0.9195 0.9252 0.8883 0.9243 0.9266 0.9313\nTHLM 0.8879 0.9288 0.9310 0.9314 0.8998 0.9342 0.9357 0.9364\n10332\nTable 9: Performance of different methods on Patents in node classification. The best and second-best performances\nare boldfaced and underlined.\nMetric Model K=3 K=5\nMLP HetSANN RGCN R-HGNN MLP HetSANN RGCN R-HGNN\nMacro-Precision\nBERT 0.2012 0.2502 0.2634 0.2365 0.1425 0.1800 0.1883 0.1866\nRoBERTa 0.2010 0.2421 0.2648 0.2886 0.1414 0.1725 0.1836 0.2136\nMetaPath 0.0655 0.1321 0.1389 0.1579 0.0523 0.1062 0.1102 0.1234\nMetaPath+BERT0.2041 0.2541 0.2626 0.2914 0.1418 0.1818 0.1860 0.2098\nLinkBERTâ‹† 0.2144 0.2443 0.2715 0.2933 0.1490 0.1758 0.1877 0.2194\nGIANTâ‹† 0.2181 0.2459 0.2692 0.2854 0.1518 0.1799 0.1882 0.2137\nTHLM 0.2541 0.2671 0.2827 0.3133 0.1761 0.1864 0.1950 0.2300\nMacro-Recall\nBERT 0.3036 0.3553 0.3592 0.2765 0.3824 0.4326 0.4493 0.3619\nRoBERTa 0.3017 0.3598 0.3603 0.3618 0.3827 0.4430 0.4560 0.4526\nMetaPath 0.0335 0.1889 0.1949 0.1884 0.0484 0.2446 0.2543 0.2465\nMetaPath+BERT0.3027 0.3603 0.3610 0.3682 0.3810 0.4383 0.4522 0.4527\nLinkBERTâ‹† 0.3186 0.3597 0.3714 0.3699 0.4038 0.4483 0.4631 0.4568\nGIANTâ‹† 0.3344 0.3591 0.3713 0.3654 0.4131 0.4324 0.4616 0.4511\nTHLM 0.3933 0.4023 0.4067 0.4111 0.4890 0.4886 0.5038 0.4976\nMicro-Precision\nBERT 0.3502 0.3636 0.3785 0.3599 0.2426 0.2502 0.2590 0.2488\nRoBERTa 0.3566 0.3694 0.3845 0.3826 0.2472 0.2541 0.2626 0.2615\nMetaPath 0.1286 0.2580 0.2695 0.2729 0.0971 0.1874 0.1941 0.1962\nMetaPath+BERT0.3498 0.3646 0.3773 0.3775 0.2428 0.2507 0.2582 0.2584\nLinkBERTâ‹† 0.3609 0.3699 0.3841 0.3851 0.2494 0.2542 0.2625 0.2627\nGIANTâ‹† 0.3596 0.3656 0.3804 0.3775 0.2484 0.2505 0.2600 0.2583\nTHLM 0.3843 0.3863 0.3951 0.3959 0.2626 0.2627 0.2684 0.2686\nMicro-Recall\nBERT 0.6375 0.6618 0.6890 0.6552 0.7360 0.7591 0.7856 0.7548\nRoBERTa 0.6491 0.6724 0.6998 0.6964 0.7499 0.7709 0.7968 0.7933\nMetaPath 0.2341 0.4697 0.4905 0.4967 0.2945 0.5684 0.5888 0.5951\nMetaPath+BERT0.6367 0.6636 0.6868 0.6871 0.7367 0.7606 0.7834 0.7838\nLinkBERTâ‹† 0.6570 0.6734 0.6992 0.7010 0.7567 0.7711 0.7963 0.7970\nGIANTâ‹† 0.6546 0.6655 0.6923 0.6871 0.7537 0.7598 0.7888 0.7835\nTHLM 0.6996 0.7032 0.7192 0.7207 0.7967 0.7970 0.8144 0.8148\nNDCG\nBERT 0.6725 0.7025 0.7297 0.6921 0.7066 0.7353 0.7610 0.7262\nRoBERTa 0.6854 0.7140 0.7417 0.7387 0.7200 0.7470 0.7726 0.7697\nMetaPath 0.2467 0.4902 0.5103 0.5188 0.2734 0.5296 0.5487 0.5573\nMetaPath+BERT0.6717 0.7024 0.7272 0.7280 0.7066 0.7351 0.7586 0.7594\nLinkBERTâ‹† 0.6953 0.7154 0.7413 0.7444 0.7291 0.7478 0.7725 0.7748\nGIANTâ‹† 0.6936 0.7084 0.7354 0.7305 0.7275 0.7397 0.7665 0.7617\nTHLM 0.7442 0.7488 0.7652 0.7675 0.7752 0.7785 0.7950 0.7963\n10333"
}