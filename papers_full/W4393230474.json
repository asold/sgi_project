{
  "title": "Multi-scale cross-attention transformer encoder for event classification",
  "url": "https://openalex.org/W4393230474",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1978219055",
      "name": "A. Hammad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051379657",
      "name": "S. Moretti",
      "affiliations": [
        "University of Southampton",
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A2138771585",
      "name": "M. Nojiri",
      "affiliations": [
        "The Graduate University for Advanced Studies, SOKENDAI",
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2925870596",
    "https://openalex.org/W3085387025",
    "https://openalex.org/W3092470512",
    "https://openalex.org/W3203170081",
    "https://openalex.org/W2917841156",
    "https://openalex.org/W3048350339",
    "https://openalex.org/W3048858912",
    "https://openalex.org/W3048847482",
    "https://openalex.org/W3050460046",
    "https://openalex.org/W3090780589",
    "https://openalex.org/W3160795578",
    "https://openalex.org/W4309072449",
    "https://openalex.org/W3215932716",
    "https://openalex.org/W4375858554",
    "https://openalex.org/W4210949709",
    "https://openalex.org/W3202839300",
    "https://openalex.org/W3216335947",
    "https://openalex.org/W2883681588",
    "https://openalex.org/W2975089235",
    "https://openalex.org/W4308976358",
    "https://openalex.org/W2938291043",
    "https://openalex.org/W4221158843",
    "https://openalex.org/W4388456325",
    "https://openalex.org/W3047247603",
    "https://openalex.org/W4312075917",
    "https://openalex.org/W4367367885",
    "https://openalex.org/W4310235257",
    "https://openalex.org/W4382726249",
    "https://openalex.org/W2895846750",
    "https://openalex.org/W2915621743",
    "https://openalex.org/W2124075924",
    "https://openalex.org/W2149631226",
    "https://openalex.org/W1966905614",
    "https://openalex.org/W2134521987",
    "https://openalex.org/W3107519126",
    "https://openalex.org/W3217032783",
    "https://openalex.org/W2041251850",
    "https://openalex.org/W4386227328",
    "https://openalex.org/W2110999179",
    "https://openalex.org/W2124410456",
    "https://openalex.org/W1965311100",
    "https://openalex.org/W2258584306",
    "https://openalex.org/W2156379501",
    "https://openalex.org/W2113445936",
    "https://openalex.org/W1987435915",
    "https://openalex.org/W2100507804",
    "https://openalex.org/W2048987940",
    "https://openalex.org/W3037884215",
    "https://openalex.org/W3020995000",
    "https://openalex.org/W2116232996",
    "https://openalex.org/W2062007487",
    "https://openalex.org/W1652336819",
    "https://openalex.org/W2162496521",
    "https://openalex.org/W1841093778",
    "https://openalex.org/W1541193715",
    "https://openalex.org/W2165610102",
    "https://openalex.org/W2139067675",
    "https://openalex.org/W2021788065",
    "https://openalex.org/W2527177583",
    "https://openalex.org/W3037961236",
    "https://openalex.org/W2076214561",
    "https://openalex.org/W4234122927",
    "https://openalex.org/W2047792789",
    "https://openalex.org/W1690836630",
    "https://openalex.org/W2257617748",
    "https://openalex.org/W2325907229",
    "https://openalex.org/W2512944313",
    "https://openalex.org/W2563484019",
    "https://openalex.org/W2586557507",
    "https://openalex.org/W2792435888",
    "https://openalex.org/W2805994763",
    "https://openalex.org/W4387452997",
    "https://openalex.org/W4221140049",
    "https://openalex.org/W2491766731",
    "https://openalex.org/W2792015270",
    "https://openalex.org/W2787586057",
    "https://openalex.org/W2946595836",
    "https://openalex.org/W2019149276",
    "https://openalex.org/W4286214691",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W6600864179",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W4308844456",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W3102719046",
    "https://openalex.org/W3214035614",
    "https://openalex.org/W3105721292",
    "https://openalex.org/W3119030234",
    "https://openalex.org/W3101441767",
    "https://openalex.org/W3138131070",
    "https://openalex.org/W3098163999",
    "https://openalex.org/W3101047025",
    "https://openalex.org/W3112297169",
    "https://openalex.org/W3100509708",
    "https://openalex.org/W3104161180",
    "https://openalex.org/W3101501459",
    "https://openalex.org/W3104674222",
    "https://openalex.org/W3101867861"
  ],
  "abstract": "A bstract We deploy an advanced Machine Learning (ML) environment, leveraging a multi-scale cross-attention encoder for event classification, towards the identification of the gg → H → hh → $$ b\\overline{b}b\\overline{b} $$ <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"> <mml:mi>b</mml:mi> <mml:mover> <mml:mi>b</mml:mi> <mml:mo>¯</mml:mo> </mml:mover> <mml:mi>b</mml:mi> <mml:mover> <mml:mi>b</mml:mi> <mml:mo>¯</mml:mo> </mml:mover> </mml:math> process at the High Luminosity Large Hadron Collider (HL-LHC), where h is the discovered Standard Model (SM)-like Higgs boson and H a heavier version of it (with m H &gt; 2 m h ). In the ensuing boosted Higgs regime, the final state consists of two fat jets. Our multi-modal network can extract information from the jet substructure and the kinematics of the final state particles through self-attention transformer layers. The diverse learned information is subsequently integrated to improve classification performance using an additional transformer encoder with cross-attention heads. We showcase that our approach surpasses current alternative methods used to establish sensitivity to this process in performance, whether solely based on kinematic analysis or combining this with mainstream ML approaches. Then, we employ various interpretive methods to evaluate the network results, including attention map analysis and visual representation of Gradient-weighted Class Activation Mapping (Grad-CAM). Finally, we note that the proposed network is generic and can be applied to analyse any process carrying information at different scales. Our code is publicly available for generic use ( https://github.com/AHamamd150/Multi-Scale-Transformer-Encoder ).",
  "full_text": "JHEP03(2024)144\nPublished for SISSA by\n Springer\nReceived: January 13, 2024\nRevised: February 2, 2024\nAccepted: March 6, 2024\nPublished: March 26, 2024\nMulti-scale cross-attention transformer encoder for\nevent classification\nA. Hammad,a S. Morettib,c and M. Nojiria,d,e\naTheory Center, IPNS, KEK,\n1-1 Oho, Tsukuba, Ibaraki 305-0801, Japan\nbSchool of Physics and Astronomy, University of Southampton,\nHighfield, Southampton, U.K.\ncDepartment of Physics& Astronomy, Uppsala University,\nBox 516, SE-751 20 Uppsala, Sweden\ndThe Graduate University of Advanced Studies (Sokendai),\n1-1 Oho, Tsukuba, Ibaraki 305-0801, Japan\neKavli IPMU (WPI), University of Tokyo,\n5-1-5 Kashiwanoha, Kashiwa, Chiba 277-8583, Japan\nE-mail: hamed@post.kek.jp, S.Moretti@soton.ac.uk, nojiri@post.kek.jp\nAbstract: We deploy an advanced Machine Learning (ML) environment, leveraging a\nmulti-scale cross-attention encoder for event classification, towards the identification of the\ngg →H →hh →b¯bb¯b process at the High Luminosity Large Hadron Collider (HL-LHC),\nwhere h is the discovered Standard Model (SM)-like Higgs boson andH a heavier version\nof it (withmH > 2mh). In the ensuing boosted Higgs regime, the final state consists of\ntwo fat jets. Our multi-modal network can extract information from the jet substructure\nand the kinematics of the final state particles through self-attention transformer layers. The\ndiverse learned information is subsequently integrated to improve classification performance\nusing an additional transformer encoder with cross-attention heads. We showcase that our\napproach surpasses current alternative methods used to establish sensitivity to this process in\nperformance, whether solely based on kinematic analysis or combining this with mainstream\nML approaches. Then, we employ various interpretive methods to evaluate the network\nresults, including attention map analysis and visual representation of Gradient-weighted Class\nActivation Mapping (Grad-CAM). Finally, we note that the proposed network is generic\nand can be applied to analyse any process carrying information at different scales. Our\ncode is publicly available for generic use.1\nKeywords: Higgs Production, Jets and Jet Substructure, Multi-Higgs Models\nArXiv ePrint: 2401.00452\n1https://github.com/AHamamd150/Multi-Scale-Transformer-Encoder.\nOpen Access, © The Authors.\nArticle funded by SCOAP3. https://doi.org/10.1007/JHEP03(2024)144\nJHEP03(2024)144\nContents\n1 Introduction 1\n2 Transformer encoder 2\n2.1 Attention mechanism 3\n3 Physics example 5\n3.1 The 2HDM 6\n3.2 Analysis strategy 8\n3.3 Data pre-processing 10\n4 Results 14\n4.1 The influence of cross-attention 15\n5 Interpretation of the transformer encoder results 17\n5.1 Attention maps 18\n5.2 Grad-CAM 20\n6 Conclusion 22\nA Networks structure 23\n1 Introduction\nInformation about jet identification provides powerful insights into collision events and\ncan help to separate different physics processes originating these. This information can be\nextracted from the elementary particles localized inside a jet. Recently, various methods\nhave been used to exploit the substructure of a jet to probe new physics signatures using\nadvanced Machine Learning (ML) techniques [1–5].\nConversely, using the reconstructed kinematics from the final state jets for event classifi-\ncation spans the full phase space and exhibits large classification performance [6–18]. Such\nhigh-level kinematics (i.e., encoding the global features of the final state particles), possibly\ntogether with the knowledge of the properties of (known or assumed) resonant intermediate\nparticles, remains blind to the information encoded inside the final state jets.\nA possible way to extract information from both jet substructure and global jet kinematics\nis to concatenate the information extracted from a multi-modal network [19–24]. If the\nkinematics and jet substructure have different performances in event discrimination, the\nnetwork assigns higher weight values to the layers associated with kinematics as input, while\nallocating lower values to layers associated with local jet information as input. Such a simple\nconcatenation leads to an imbalance of the extracted information, within which the kinematic\ninformation generally dominates [25].\nIn this paper, we present a novel method for incorporating different-scale information\nextracted from both global kinematics and substructure of jets via a transformer encoder\n– 1 –\nJHEP03(2024)144\nwith a cross-attention layer. The model initially extracts the most relevant information\nfrom each dataset individually using self-attention layers before incorporating these using\na cross-attention layer. The method demonstrates a larger improvement in classification\nperformance compared to the simple concatenation method.\nTo assess our results, we analyze the learned information by the transformer layers through\nthe examination of the attention maps of the self- and cross-attention layers. Attention maps\nprovide information about the (most) important embedded particles the model focuses on\nwhen classifying signal and background events. However, they cannot highlight the region in\nthe feature (e.g., phase) space crucial for model classification. For this purpose, we utilize\nGradient-weighted Class Activation Mapping (Grad-CAM) to highlight the geometric region\nin theη−ϕ (detector) plane where the model focuses on classifying events.\nWe test our approach for the dominant decay channel of Higgs boson pairs with Standard\nModel properties (hh) produced at the LHC, that is, into fourb-(anti)quarks. This signal\nhas historically proved to be extremely challenging to extract owing to a significant QCD\nbackground. Lately, there have been several attempts to tackle this signature using both\nstandard [26–28] and ML [29] approaches. Furthermore, in the case that thehh intermediate\nstate emerges from the (resonant) decay of a heavier Higgs state (H), each of the two would\nbe (slim)b-jets produced by the twoh decays actually merge into one (fat) jet, as the two\nh states can be very boosted. The final states in the detectors little resemble the primary\nparton kinematics of the underlying physics in such case.\nThe plan of this paper is as follows. In the next section, we describe the basics of a\ntransformer encoder. Then, in section 3, we introduce the physics process that we use as\nan example. In section 4, we present our numerical results. In section 5, we interpret the\nclassification results using various methods. The section 6 is for conclusions. The details\nof our network structure can be found in the appendix.\n2 Transformer encoder\nTransformers were originally proposed as sequence-to-sequence models for machine trans-\nlation [30]. The main ingredient of the original transformer model is the encoder-decoder\nblock. However, the models using encoder block only often appear for event classification\nanalysis at the LHC [31–33].\nInherited by the word tokens in the original transformer model, transformer encoders\nare used to analyze events in terms of clouds of particles for High Energy Physics (HEP)\nanalysis [34, 35]. Particle clouds represent the final state particles as a permutation invariant\nsequence of particles. Such a representation has the ability to share the advantages of particle\nbased representations, especially the flexibility to include arbitrary features for each particle.\nThe motivation to apply transformer encoders to particle clouds stems from their inherent\nability to model interactions between particles irrespective of their spatial proximity. By\nleveraging self-attention mechanisms, transformer encoders enable each particle to dynamically\nweigh the influence of other particles within the entire cloud, thus capturing both local\nand global dependencies. This can potentially revolutionize the analysis of HEP systems,\nparticularly by offering a more holistic understanding of their behavior and interactions.\n– 2 –\nJHEP03(2024)144\nUnderstanding the scientific operation of transformer encoders in the context of particle\nclouds requires diving into the core components of these models. At the heart of the\ntransformer architecture is the attention mechanism, an algorithm that allows the model\nto focus on different parts of the input sequence when making predictions. An attention\nmechanism operates by assigning attention weights to different particles based on their\nrelevance to the current particle being processed. This allows the model to consider global\nrelationships and dependencies, enabling it to capture emergent behaviors, interactions,\nand patterns that may not be apparent in filter based methods, e.g., Convolutional Neural\nNetworks (CNNs), which mainly extract the local information.\n2.1 Attention mechanism\nThe attention mechanism is an essential component of transformer models, playing a crucial\nrole in capturing information and dependencies amongst particles. In the transformer\narchitecture, the attention mechanism enables the model to focus selectively on different parts\nof the input sequence, allowing for the modelling of complex relationships and dependencies.\nIn general, the attention mechanism operates by assigning different weights to different\nelements in the input sequence, emphasizing the more relevant parts while downplaying the\nless relevant ones.1 The attention mechanisms broadly span two types, as follows.\n• Self-attention is a more advanced form of attention where the model attends to\ndifferent positions in the input sequence to weight their importance concerning the\ncurrent position. In the context of the transformer model, self-attention allows each\nelement in the sequence to attend to all other elements, capturing both local and global\ndependencies. Attention scores are calculated and used to combine the values associated\nwith different positions linearly. The self-attention mechanism enables the model to\nconsider the entire context, making it particularly effective for tasks where long-range\ndependencies are crucial.\n• Cross-attention extends the self-attention mechanism to handle input sequences from\ndifferent sources. In the transformer architecture, it is often used when processing pairs\nof sequences of different structures. Cross-attention allows each element in the first\nsequence to attend to all other elements in the subsequent sequence. This facilitates\nmodelling the relationships between different modalities or extracting the relevant\ninformation from sequences with different scales.\nConsider the input data sets (xi,xj) that have first been passed by a linear fully connected\nNN layer to generate the weight matrices as follows:\nQi = WQ ·xi, K j = WK ·xj , V j = WV ·xj , (2.1)\nwhere K,Q and V are called key, query, and value vectors, respectively, and used to compute\nthe attention to the whole data set.\n1Dropping the less informative instances from the data can rectify the sparsity problem when using CNNs\nto analyze jet images.\n– 3 –\nJHEP03(2024)144\nScaled dot-product attention can then be defined as\nαij = softmax\n(\nQi ·KT\nj√\nd\n)\n= exp(Qi ·KT\nj /\n√\nd)\n∑\nj exp(Qi ·KT\nj /\n√\nd)\n, (2.2)\nwhile the attention output is computed as a weighted sum of the attention scores as\nZi =\n∑\nj\nαijVj . (2.3)\nThis is called self-attention if the attention is computed for the same data set, i.e.,xi = xj.\nThe weights matrices have the dimensions ofWi×i\nQ ,Wi×i\nK ,Wi×i\nV which mixes the features of the\ninput data and retain the dimension of the embedded input to the original one. In contrast,\nif the two input data sets differ, i.e.,xi ̸= xj, cross attention is needed. In this case, the\nweight matrices should have different dimensions withWi×i\nQ ,Wj×i\nK ,Wj×i\nV in order to calculate\nattention. Attention output is used to scale the input data set via a skip connection as\n˜xi = xi + Zi. (2.4)\nThe newly transformed data set˜xi indicates the attention importance of each element in the\ndata set to the whole elements in the set. Although the attention output mixes the input\nand feature tokens, the skip connection keeps the reference to the order of the original input\ndata set. In the subsequent sections of the paper, we use the modified particle tokens to\nprovide a straightforward interpretation of the results.\nAt its basic level, each transformer layer includes a multi-head attention, which combines\ndifferent attention heads, allowing for parallel multi-dimensional processing of the inputs.\nMulti-head attention is a key innovation in the transformer model architecture, enhancing\nexpressive power and capturing complex patterns in data by allowing the model to attend to\ndifferent aspects of the input sequence simultaneously. Therefore, this mechanism eases the\nunderstanding of varied and subtle connections within the data, offering a more thorough\nrepresentation.\nAs explained, a single set of attention weights is computed for the entire input sequence.\nMulti-head attention extends this concept by employing multiple attention heads, each\nresponsible for learning different aspects of the relationships within the data. Each attention\nhead independently processes the input sequence, producing a set of output values. These\noutputs are then linearly combined to form the final output of the multi-head attention layer.\nMathematically, ifh represents the number of attention heads, andheadi denotes the\nith attention head, the outputOis obtained by concatenating the outputs of each attention\nhead and linearly transforming these:\nO= CONCAT(head1,head2,··· headh) WO, (2.5)\nwith WO the learnable linear transformation matrix which has the dimensions ofW(h∗i)×i\nO to\nretain the same dimensions as the original input. This enables the model to capture different\naspects of relationships and dependencies simultaneously.\nThe choice of the number of attention heads, denoted ashi, is a crucial hyperparameter\nin designing a transformer model. Increasing the number of attention heads has several\n– 4 –\nJHEP03(2024)144\nimplications, such as enhancing the model’s capacity to capture complex relationships.\nIt is also important to mention that a higher number of attention heads also increases\ncomputational complexity. Training and inference times and memory requirements could\nincrease. Therefore, the number of attention heads should be balanced based on the task\nrequirements and available computational resources.\nIn this particular context, we present an innovative methodology aimed at integrating\ninputs characterized by distinct scales within a multi-modal transformer model featuring\ncross-attention layers. The schematic representation of the network architecture is shown\nin figure 1. Considering the specific case of the HEP process to be studied at the LHC,\ngg →H →hh→b¯bb¯b, the model dynamically adjusts three streams through self-attention\ntransformer layers, each devoted to analyzing the leading jet, second-leading jet and the\nreconstructed kinematics, respectively. At this juncture, the model independently extracts\npivotal information from each data set, leveraging self-attention mechanisms before their\ncollective processing through a cross-attention layer.\nThe main role of the cross-attention layer is to extract the local jet substructure in-\nformation effectively and incorporate it into the extracted kinematic information. Notably,\nthe adaptability of the cross-attention layer in merging information from one data set into\nanother affords flexibility in determining how to integrate the extracted information, providing\nthe option to accentuate jet information for enhancing kinematics. Once the most relevant\ninformation from the data sets is extracted and combined via the cross-attention layer, we\nfeed the output to fully connected NNs to analyze the captured information and compute the\nclassification probability. The inclusion of self-attention layers in the model holds significance,\nas it allows for the independent extraction of the most relevant information from each data\nset before their amalgamation using the cross-attention mechanism. This characteristic makes\nthe model proficient in analyzing multi-scale data characterized by intricate structures.\n3 Physics example\nWe analyse SM-like di-Higgs boson (hh) production at the HL-LHC (with an integrated\nluminosity of3000 fb−1) within the framework of the 2HDM. In the boosted regime, where\nthe di-Higgs boson is produced from an on-shell heavy Higgs,H, the final state features\ntwo fat jets, as illustrated in figure 2 by the two red cones therein. Currently, ATLAS\nanalysis [36] shows the limit on the production cross section of heavy scalar decaying to\ntwo fat Higgs jets. The given limit on the production cross section is prominent with95%\nC.L. to beσ(pp→H →hh) <50 fb formH = 1 TeV, and we expect the bound to further\nimprove at the HL-LHC.\nTherefore, to start with, in this section, we provide a brief review of the 2HDM with\ntype-II Yukawa couplings, focusing on the aspects that are relevant to our analysis. We then\ndescribe the strategy behind our numerical analysis, together with its constituent elements,\ni.e., the event generation and detector simulation procedures, as well as the signal and\nbackground properties, in terms of the overall kinematics and internal dynamics of jets.\nWe adopt different transformer encoder configurations to analyze the kinematics and jet\nsubstructure individually and efficiently combine the information from both of these.\n– 5 –\nJHEP03(2024)144\nTransformer layers \n(MHSA)\nMLP\nTransformer layers \n(MHCA)\nTransformer layers \n(MHSA)\nTransformer layers \n(MHSA)\nAdd() Layer\nFigure 1. Structure of the transformer model used. Here,Pj1,Pj2 are the number (dimnesion) of\nthe leading and second leading jet constituents while thePm’s are the number of the reconstructed\nparticles, namely,j1,j2, andH; Ej1, Ej2, andEm are number the corresponding features of each\ndataset. Also, MHSA stands for multi-heads self-attention layers, and MHCA stands for multi-heads\ncross-attention layers. Finally, theNi’s are the number of the used transformer layers. The transformer\nlayers are stacked and work sequentially, as pointed out by the black arrow.\nFigure 2. Feynman diagram for the signal process.\n3.1 The 2HDM\nThe 2HDM is an extension of the SM through a secondSU(2)L Higgs doublet with the\nsame quantum numbers under the SM symmetry gauge group [37, 38]. The most general\n2HDM Higgs potential is given by\nVϕ = m2\n11(ϕ†\n1ϕ1) + m2\n22(ϕ†\n2ϕ2) −\n[\nm2\n12(ϕ†\n1ϕ2) + h.c.\n]\n+ λ1(ϕ†\n1ϕ1)2 + λ2(ϕ†\n2ϕ2)2 + λ3(ϕ†\n1ϕ1)(ϕ†\n2ϕ2) + λ4(ϕ†\n1ϕ2)(ϕ†\n2ϕ1)\n+ 1\n2\n[\nλ5(ϕ†\n1ϕ2)2 +\n[\nλ6(ϕ†\n1ϕ1) + λ7(ϕ†\n2ϕ2)\n]\n(ϕ†\n1ϕ2) + H.c.\n]\n.\n(3.1)\n– 6 –\nJHEP03(2024)144\nThe potential structure allows for Flavor Changing Neutral Currents (FCNCs) at the\ntree level, which is strongly constrained by experimental measurements. Applying a global\nZ2 symmetry to the scalar potential, with(ϕ1,ϕ2) →(ϕ1,−ϕ2) transformations, prevents the\nexistence of such FCNC sources [39]. However, the most general Yukawa interaction violates\nsuch aZ2 symmetry, thus leading to potential FCNCs at the tree level, as pointed out in\nref. [40]. Therefore, to tame the latter, only specific Yukawa structures, known as Types [37],\nare allowed. Yet, to enable Electro-Weak Symmetry Breaking (EWSB) consistent with the\nmeasured particle spectrum of the SM, a softly brokenZ2 symmetry should eventually be\nenabled by requiring a small but non-vanishing termm2\n12(ϕ†\n1ϕ2) and settingλ6 = λ7 = 0.\n(Herein, softly means that the model still respects theZ2 symmetry at small distances\nthrough all orders of perturbation theory.) The soft massm2\n12 and λ5 are in general complex,\nthough [41, 42]. In the following, we will consider a real potential that thus preserves the CP\nsymmetry, IM(m2\n12) = Im(λ5) = 0. In such a configuration of the 2HDM, then 7 independent\nparameters remain, which are theλi’s, withi= 1,... 5, tan β = v2/v12 and m2\n12, from which\nthe physical parameters, i.e., Higgs boson masses and couplings, are obtained, with the\nconstraint that one of the two CP-even Higgs fields should be the discovered one with mass\nof 125GeV or so (which in our case is theh field). Finally, as mentioned already, we restrict\nour study to the Type-II among the possible Yukawa structures.\nThe tree level mass matrix squared for the Higgs fields can be obtained as\n(\nM2\n)\nij\n= ∂Vϕ\n∂hi∂hj\n⏐⏐⏐⏐⏐\nhi,j=0\n, (3.2)\nwhere thehi’s (i= 1,..., 4) are the four components of the complex doublet fields. Upon\nEWSB, three physical neutral scalars are obtained after diagonalizing the corresponding\nmass matrices, as intimated, two CP-even (scalar) ones (h,H) and a CP-odd (pseudoscalar)\none (A), with masses given by\nm2\nh,H = 1\n2\n[\nχ2\n11 + χ2\n22 ∓\n√\n(χ2\n11 −χ2\n22)2 + 4(χ2\n12)2\n]\n, (3.3)\nm2\nA = 2m2\n12\nsin 2β −λ5v2 , (3.4)\nwith\nχ2\n11 = m2\n12 tan β+ 2λ1v2 cos2 β, (3.5)\nχ2\n22 = m2\n12 cot β+ 2λ2v2 sin2 β, (3.6)\nχ2\n12 = −m2\n12 + 1\n2(λ3 + λ4 + λ5)v2 sin 2β, (3.7)\nwhere the VEVs satisfy the relationv = √v1 + v2 (with v being the SM one).3\nTo stay with the neutral Higgs sector, the imposed CP conservation only allows for tree\nlevel couplings between two massive gauge bosons and the CP-even Higgs states, while the\nCP-odd Higgs state can only couple to a gauge boson and a CP-even Higgs one. Furthermore,\n2With v1 and v2 being the Vacuum Expectation Values (VEVs) of the two Higgs doublets.\n3The other two Higgs states emerging from the 2HDM after EWSB are charged and are denoted byH±.\n– 7 –\nJHEP03(2024)144\nmH[GeV] λ1 λ2 λ3 λ4 λ5 tan β m2\n12[TeV2] cos(β−α) σprod[fb]\n600 1.80 0.23 1.75 −2.06 −1.09 5.00 −78.97 0.31 0.86\n800 3.20 0.25 1.75 −2.06 −1.29 4.00 −128.91 0.33 0.375\n1000 1.0 0.16 3.50 −2.06 −1.49 3.00 −302.92 0.37 0.11\n2000 1.0 0.14 2.75 −1.06 −1.97 5.00 −889.05 0.32 0.024\nTable 1.Input parameters for our four BPs. The last column shows the production cross section for\nthe processgg →H →hh.\nall neutral Higgs states can couple to fermions. The coupling strength of the neutral Higgs\nbosons to both matter and forces are parameterized in terms oftan β and another parameter,\nα, which is the mixing angle between the CP-even Higgs states [37]. Furthermore, the triple\nscalar coupling is independent of the Yukawa structure and is given by [43]\nλ(H,h,h) = − e cβ−α\n2mWsWs2\n2β\n[\n(2m2\nh + m2\nH)s2αs2β + (3s2α −s2β)m2\n12\n]\n, (3.8)\nwhere e is the electric charge ands,c are thesin and cos of the given angle.\nThe 2HDM free parameters are constrained from various theoretical considerations\nand experimental observations, as described in [44]. Thus, profiting from the scan results\nperformed therein, we adopt four Benchmark Points (BPs), withmH = 600,800,1000, and\n2000 GeV, that satisfy all the current bounds. In table 1, we show the parameters values of\nthese points while the last column shows the production cross sectionσprod of our target\nprocess (prior to the twoh →b¯b decays) at √s = 14 TeV.\n3.2 Analysis strategy\nWith the theoretical setup clarified, we now proceed to a phenomenological study of di-Higgs\nboson production, focusing on final states with two boosted fat jets. We align our analysis\nwith the boosted analysis presented in the latest ATLAS paper [36]. The primary background\ncontamination arises from QCD multijet processes, specificallypp→jjjj, contributing an\nestimated 90% of the total background, while the di-top process¯tt contributes at the10%\nlevel. Other background processes, including SMh, hh, and EW di-boson production, have\nbeen assessed to make negligible contributions to the selected event yields. Therefore, they\nare not included in our analysis. Given that BSM di-Hggs events suffer from huge background\ncontamination and it is not trivial to extract the signal information, we employ various\nconfigurations of transformer encoders for this analysis.\nCommencing with the analysis of the global information encoded in the high-level\nreconstructed kinematics of both the signal and relevant background events, we employ a\ntransformer encoder with multi-head self-attention to optimize the separation power between\nsignal and background events. However, the presence of similar (to the signal) kinematic\nstructures in some background processes poses a challenge to the classification efficiency of\nthis network. Additionally, the substantial cross section of the background events diminishes\nsignal significance, even after optimizing the cut on the output score.\n– 8 –\nJHEP03(2024)144\nWe use MadGraph5 [45] to estimate multi-parton amplitudes and to generate signal\nand background events for subsequent processing. Background processespp →bbbb4 and\npp→t¯t are computed at Leading Order (LO) while the Higgs production from gluon-gluon\nfusion is calculated at Next-to-LO (NLO) in QCD using an effective coupling calculated by\nSPheno [46, 47]. PYTHIA [48] is used for parton shower, hadronization, heavy flavor decays,\nand for adding the soft underlying event. The¯tt background is simulated at LO and up to\ntwo more jets with the matching scale20 GeV via the MLM method [49, 50].\nFollowing the event selection in ATLAS analysis [36], we use the DELPHES package [51]\nfor detector simulation. DELPHES parameterizes the detector response, including tracks,\ncalorimeter deposits, and high level objects such as isolated electrons, jets, taus, and Missing\nET (MET). We use the default ATLAS card for resolution, but the (fat)jets are reconstructed\nfrom the Eflow objects, combining tracks and calorimeter information. Fat jets are clustered\nusing the anti-kT algorithm [52, 53] with cone radiusR= 1 and, to ensure further suppression\nof pile-up noise, jet trimming is performed [54]. To enhance the network performance, one\nmay consider applying initial cuts on certain variables before inputting the distributions\ninto the network, aiming to amplify the signal and suppress the backgrounds. We follow\nthe pre-selection cuts outlined in [36], requiring two fat jets with a doubleb-tagging each.\nMoreover, each event must have at least two fat jets with radiusR= 1.0 and pT >450 GeV\nfor the leading jet andpT >250 GeV for the second leading jet. Each of the two fat jets is\nrequired to have a pseudorapidity|η(J)|<2.5, a lower mass cut ofm(J) >50 GeV, and a\nmass window of200 GeV is applied for themH reconstruction formH ≤1 TeV and relaxed\nfor higher masses to allow for more statistics. Unlike the ATLAS analysis, we do not consider\npile-up effects in this analysis. Moreover, ATLAS analysis uses a new neural-network-based\nb-tagging algorithm DL1r [55], while we use the Delphes b-tagging with flate effeciency of80%.\nIn addition to the global kinematical variables, we can utilize jet substructure to dis-\ntinguish between signal and background events. This naturally arises from the fact that\njets initiated by different particles exhibit distinct characteristics. While heavy boosted\nparticles, such asW±, Z and Higgs bosons, can result in jets with a distinctive multi-prong\nstructure, quark and gluon jets are unlikely to have such structure. Furthermore, the boosted\ncolour singlet particle is isolated in colour flow. Therefore, two b jets from Higgs decay are\ncolour-connected only among themselves, unlike QCD jets.\nConsequently, the features of the parent particles can be inferred from the structure of the\njet constituents. This information enables the recovery of various local details about events\nfrom different processes, serving as a discriminating variable between signal and background\nevents. The study of jet substructure to identify the parent particle initiating a jet, thereby\ndistinguishing jets initiated from heavy boosted particles from QCD jets was introduced\nin [56–70] (and references herein). Recently, improvement on jet identification continued by\nusing ML methods for jet image analysis [71–79], graph based analysis [80–82] or sequence\nbased analysis [83–88]. In this paper, we especially employ a multi-modal transformer encoder\nwith self-attention multi-heads to analyze the jet contents. The different modalities are\ndesigned to extract information from the leading and second-leading jet contents in parallel\nbefore a simple concatenation is performed for classification purposes. Without cross attention\n4We simulate the multi-jets QCD processes in Madgraph aspp → b¯bb¯b to speed up the event generation.\n– 9 –\nJHEP03(2024)144\n−0.2 0.0 0.2\nη′\n(pJ2)\n0\n5\n10\n15Events (Normalized)\nLeading jet\nSignal\npp → jjjj\npp → tt\n−0.4 −0.2 0.0 0.2 0.4\nφ′\n(pJ2)\n0\n2\n4\n6Events (Normalized)\nLeading jet\nSignal\npp → jjjj\npp → tt\n−0.2 0.0 0.2\nη′\n(pJ2)\n0\n5\n10\n15Events (Normalized)\nSecond leading jet\nSignal\npp → jjjj\npp → tt\n−0.4 −0.2 0.0 0.2\nφ′\n(pJ2)\n0\n2\n4\n6Events (Normalized)\nSecond leading jet\nSignal\npp → jjjj\npp → tt\n0 10 20\npT(pJ2) [GeV]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−5 −4 −3 −2 −1 0\nlog\n(\npTp\npTJ2\n)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n0 5 10 15 20\npT(pJ2) [GeV]\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−4 −2 0\nlog\n(\npTp\npTJ2\n)\n0.0\n0.5\n1.0\n1.5\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\nFigure 3. Left: distributions of 10000 leading jets features averaged over the jet constituents.\nRight: distributions of10000 second leading jets features averaged over the jet constituents. Signal\ndistributions are for the BP withmH = 1 TeV.\nto the high-level kinematical variable discussed next, the classification performance is based\nsolely on the information localized inside the fat jet. Integrating inputs of varying scales\nencompassing both kinematics and jet substructure information, we utilize a multi-modal\ntransformer encoder equipped with three streams and cross-attention head. The first and\nsecond streams process information from the leading and second-leading jet contents. Each\nof them features a transformer encoder with self-attention heads. Once important features\nare extracted from the jets, they are aggregated in an addition layer. The third stream,\ndedicated to high-level kinematics, employs a transformer encoder with self-attention heads.\nThe output from the addition layer and the final layer of the third transformer are fed into a\ncross-attention layer. This cross-attention layer is pivotal in connecting information extracted\nfrom the jet constituents to the corresponding kinematics, enhancing the overall classification\nperformance. To elucidate the impact of the cross-attention layer, we introduce a fourth model\nwherein we substitute the cross-attention layer with a straightforward concatenation layer.\n3.3 Data pre-processing\nParticle clouds enable configuring diverse data into the network, emphasizing the permutation\nsymmetry of inputs to yield a promising representation of jets. Initially, we pre-process the\ndata sets for the leading and second-leading jet contents up to50 constituents each. The\nparticles are arranged in descending order based on their transverse momentum. For events\nwith fewer constituents, the remaining positions are padded with zeros, ensuring conformity\nwith the stipulated count.5 For each instance of the jet contents we store4 features: pT,η,ϕ\nand log pT\npT(jet)\n[35]. Figure 3 shows the four features averaged over the number of jet contents\nfor 10000 events of the leading jet (left) and second leading jet (right).\n5We stress here that we use an attention mask such that the network performance is not affected by the\npadded events [30].\n– 10 –\nJHEP03(2024)144\nTo optimize the network discriminative accuracy, it is imperative to pre-process the jet\ncontents, ensuring the manifestation of a multi-prong structure specific to signal events. We\nuse the preprocessing steps that were introduced for jet image analysis. The preprocessing\nallows learning from small input data and considerably speeds up the learning process.6\nFor this purpose, the following transformations are applied before inputting the data into\nthe network.7\n• TranslationJet contents are shifted in theη−ϕ directions such that the jet axis is at\nthe center of theη−ϕ plane.\n• Rotation Rotation is executed to mitigate the stochastic nature inherent in the\ndecay angle concerning the(η−ϕ) coordinate system. This alignment is achieved\ncomprehensively by ascertaining the principal axis of the original data and subsequently\nrotation around the jet-energy centroid. This rotation ensures that the principal\naxis consistently aligns vertically. The rotation transformation is performed by first\ncomputing the leading eigenvector of the covariance matrix as the principle axis of the\njet. A rotating angle,θ, is then defined asarctan2\n(\nx1\nx2\n)\n, withx1,x2 are the first and\nsecond components of the eigenvector respectively. Finally, the rotating angle is used to\nrotate the(η−ϕ) coordinates of the jet constituents to new non-physical coordinates,\n(η′−ϕ′), in which the principle axis of the jet is always vertical.\n• Flipping Jet constituents are reflected over the vertical axis such that the right side\nof η′ always has the highest momentum. This ensures that the hardest radiation\nalways appears in similar locations, which can be exploited to enhance the classification\nperformance.\nAfter pre-processing transformations, input data sets for the leading and second leading\njets have the dimensions of(n,50,4), wheren is the number of events,50 is the number of\njet constituents, and4 is the number of pre-processed features.\nFigure 4 presents the cumulative average of50000 pT distributions for both the leading\n(upper row) and second-leading (lower row) jets. The impact of pre-processing transformations\nis evident in revealing the multi-prong structure characteristic of signal events, wherein the\nleading and second-leading subjets are localized in specific regions within the(η′−ϕ′) plane.\nIn contrast, subjets from QCD multi-jets exhibit a broad energy range, lacking a discernible\nprong structure. Conversely,¯tt events show a distinct three-prong structure attributable\nto the fully hadronic decays of the top quark. Notably, despite the multi-prong structure\nin ¯tt background events, their contribution to the overall background is merely10%. We\nwill see later that background rejection efficiency is high, thereforet¯t background can be\nimportant to estimate the accessibility of the signal.\nThe kinematics data sets have dimension(n,3,6) with n as the number of events,3 as\nthe number of reconstructed particles, leading, second leading jet, and heavy Higgs, and6\n6In principle, we can use the input data without the preprocessing, but this needs a large input data set\nand training for a long time [33].\n7Other than the rotation and flipping as proposed below, it is also possible to recluster the fat jet into\nsubjets and define the rotation and flipping based on the subjet locations.\n– 11 –\nJHEP03(2024)144\nFigure 4. For illustration purposes, we show the averagpT distribution of50000 events of the leading\n(second leading) fat jet contents in the upper raw (lower raw) after pre-processing steps for both signal\nand backgrounds. Theη′−ϕ′plane are divided into 80 bins in each direction between−1 to 1. The\nsignal events (left) are simulated for the BP withmH = 1 TeV and shown against the yield of thebbbb\n(center) and¯tt (right) background events. Here,X and Y ticks indicate the bin inη and ϕ direction.\nas the number of the kinematic variables for each reconstructed particle. The 6 kinematic\nvariables are massm, pT, η, ϕ, energy of the jet(E) and the rotation angle of the jet(θ).\nNote that we assign 5 inputs corresponding to the 4 momenta of the jet. Because of the\nkinematical constraintsp2 = m2 and pH = pJ1 + pJ2, there are only 8 physically independent\nobservable among 15 kinematical inputs. These additional inputs help the network to figure\nout relevant features for the classification.\nFigure 5 shows the normalized kinematic distributions for the signal point withmH =\n1 TeV and backgrounds. In addition to the reconstructed high-level kinematics, we incorporate\nthe θi distributions for the leading and second-leading jets (but not the heavy Higgs), which\nare the rotating angles of the leading and second leading jet contents.\nWe incorporate the data sets as input to the networks as the inputs to the first and second\ntransformer encoders have the dimensions of(n,50,4). Input to the third transformer encoder\nhas the dimension of(n,3,6). Once the data sets are pre-processed, we stack signal and\nbackground events in each data set separately, attaching labels ofY = 1 for the signal events\nand Y = 0 for the background events. During the training of the network, the model tries to\nminimize a categorical cross entropy loss function by minimizing the difference between the\nmodel prediction and the assigned labels. In this analysis, we use equal size data sets for\nsignal and background events for training with1 million events8 and 100000 event for test.\n8A major problem in any attention based transformer model which exhibits larger classification performance\nwith larger size training set.\n– 12 –\nJHEP03(2024)144\n100 200 300 400\nm(J1) [GeV]\n0.000\n0.005\n0.010\n0.015\n0.020\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n600 800 1000\npT(J1) [GeV]\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−2 0 2\nη(J1)\n0.0\n0.1\n0.2\n0.3\n0.4\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−2 0 2\nφ(J1)\n0.00\n0.05\n0.10\n0.15\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n1000 2000 3000 4000 5000\nE(J1) [GeV]\n0.000\n0.001\n0.002\n0.003\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−0.5 0.0 0.5 1.0\nsin(θJ1)\n0\n1\n2\n3Events (Normalized)\nSignal\npp → jjjj\npp → tt\n100 200 300\nm(J2) [GeV]\n0.000\n0.005\n0.010\n0.015\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n300 400 500 600 700\npT(J2) [GeV]\n0.000\n0.002\n0.004\n0.006\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−2 0 2\nη(J2)\n0.0\n0.1\n0.2\n0.3\n0.4\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−2 0 2\nφ(J2)\n0.00\n0.05\n0.10\n0.15\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n1000 2000 3000\nE(J2) [GeV]\n0.000\n0.001\n0.002\n0.003\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−0.5 0.0 0.5 1.0\nsin(θJ2)\n0\n1\n2\n3Events (Normalized)\nSignal\npp → jjjj\npp → tt\n800 900 1000 1100 1200\nm(J1J2) [GeV]\n0.000\n0.002\n0.004\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n0 500 1000\npT(J1J2) [GeV]\n0.000\n0.002\n0.004\n0.006\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−5 0 5\nη(J1J2)\n0.00\n0.05\n0.10\n0.15\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n−2 0 2\nφ(J1J2)\n0.00\n0.05\n0.10\n0.15\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\n1000 2000 3000 4000 5000 6000\nE(J1J2) [GeV]\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\nEvents (Normalized)\nSignal\npp → jjjj\npp → tt\nFigure 5. Kinematics distributions of 10000 events for the signal BP withmH =1TeV and the\ncorresponding backgrounds after applying the pre-selection cuts.\n– 13 –\nJHEP03(2024)144\n0.0 0.2 0.4 0.6 0.8 1.0\nSignal eﬃciency\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse positive rate\nAUC (Jets only (self-attention))= 84.4%\nAUC (Kinematics only (self-attention))= 91.6%\nAUC (Jets+Kinematics (self-attention))= 95.0%\nAUC (Jets+Kinematics (cross-attention))= 98.8%\n600 800 1000 1200 1400 1600 1800 2000\nmH [GeV]\n10−1\n100\n101\n102\n95% C.L. limit σ(gg → H → h1h1)[fb]\nHL-LHC, L= 3000 fb−1\nJets only (self-attention)\nKinematics only (self-attention)\nJets+Kinematics (self-attention)\nJets+Kinematics (cross-attention)\nFigure 6. Left: the Receiver Operating Characteristic (ROC) curves for the four networks for the\nsignal BP withmH = 1 TeV. Right: Estimated95% upper limit, withZA ≥2 from eq. (4.1), on the\ntotal cross section for the processgg →H →hh (having factored out the SM-likeh→b¯b decays) at\nthe HL-LHC (with an integrated luminosity of3000 fb−1) for different ML analyses. The band for each\nplot represents the upper and lower values for 5 independent training of different random number seeds.\n4 Results\nWe now present the analysis results for probing the signature of the heavy scalar in the\nprocess of boosted di-Higgs boson production,gg →H →hh→b¯bb¯b, at the HL-LHC (with\nan integrated luminosity of3000 fb−1). The discriminating power of each network measures\nhow well the signal and background may be characterised through their different features,\nall entangled together into several kinematic distributions and jet substructure information.\nFor this purpose, we utilize four different attention based transformer models which analyze\nthe reconstructed high level kinematics or the jet substructures individually via transformer\nencoders with self-attention mechanisms. Alternatively, we adopt two multi-modals trans-\nformer encoders to analyze the combined information of kinematics and jets substructure.\nIn the latter, we incorporate the different information using a simple concatenation layer or\ncross-attention layer. A full description of the used networks is in appendix A.\nThe classification performance of the utilized networks is presented in figure 6. In the\nleft plot, we showcase the ROC for the employed networks for a signal withmH = 1 TeV.\nThe multi-modal transformer encoder with cross-attention layers performs best, achieving\nan Area Under the Curve (AUC) of98.8%. In contrast, the transformer encoder trained\nsolely on the jet substructure information exhibits the lowest performance with an AUC\nof 84.4%. It is crucial to highlight the impact of the cross-attention layer, which enhances\nperformance by7% over the transformer model trained exclusively on kinematic information.\nReplacing the cross-attention layer with a simple concatenation layer results in a degradation\nof classification performance by approximately∼4%, as depicted by the green line in the plot.\nWe now illustrate the impact of our classifier onH →hh exclusion and discovery. In\norder to optimize the signal-to-background yield, we enforce a cut on the network output score\nby keeping only20 events of the background. With this choice, we alleviate the statistical\nerrors that may occur for lower background [89]. The optimized signal and background events\n– 14 –\nJHEP03(2024)144\nare used to derive the upper limit using the following formula [90]:\nZA =\n[\n2\n(\n(Ns + Nb) ln (Ns + Nb)(Nb + σ2\nb)\nN2\nb + (Ns + Nb)σ2\nb\n−N2\nb\nσ2\nb\nln(1 + σ2\nbNs\nNb(Nb + σ2\nb))\n)]1/2\n, (4.1)\nwith Ns and Nb being the number of signal and background events, respectively, and where\nσb characterizes the uncertainty in the background events chosen to be a conservative value\nof 10% [91]. In this approximation, one expects to exclude(discover) regions with a total\nsignificance of ZA > 2(5).\nThe network performance is subject to both training and statistical uncertainty from\nlimited training and testing samples. For example, the network performance can be influenced\nby the random partitioning of the training and test data sets, and the network performance\nvaries when repeating the training and test steps with new splits. We repeat the experiment for\nk times and report the results as bands between the highest and lowest values. In our results,\nwe usek= 5, and the bands represent the values of the different represented experiments.\nIn figure 6 (right), we show the estimated95% upper limit, namelyZA ≥2, on the\nproduction cross-section at the HL-LHC for heavy scalar mass ranges between600−2000 GeV.\nFor smaller mass, incorporating the kinematics and jet information via cross-attention layers\nyields the best performance among all other network configurations. For larger masses, the\nreconstructed kinematics of the signal show a clear difference from the background events.\nTherefore, the performance of the transformer models saturates. In fact, for the limit,\ne.g., mH = 2 TeV, the background events can be easily removed with a simple cut on the\nreconstructed distributions of the signal events, which exhibits a clear difference from the\nbackground distributions. The transformer network trained on the jet constituents only does\nnot show a large impact with varying heavy scalar mass.\n4.1 The influence of cross-attention\nTo evaluate the impact of the cross-attention layer on the classification performance, figure 7\npresents the attention output, as defined in eq. (2.4), for both the multi-modal transformer\nwith cross-attention trained on kinematics plus jet constituents and the transformer network\ntrained on kinematics only. In both networks, the attention output has dimensions of(3,6),\nwhere 3 represents the reconstructed particles (leading, second-leading jet, and heavy Higgs),\nand the last dimension accounts for the utilized features. We stress that the input to the\ncorss-attention layer in the network structure, shown in figure 2, is adjusted with the Query\nmatrix encodes the jet constituents. In contrast, the Key and Value matrices encoding the\nkinematics. Accordingly, the output of the cross-attention layers has the same dimensions as\nthe kinematics dataset. In principle, we have the freedom to choose whether to add the jet\ninformation to the kinematics by fixing the assigned Query, Key, and Value matrices, but we\nopted to incorporate the jet information into the high-level kinematics.\nFigure 7 displays the distributions of the attention output for each transformed particle\nindividually and averaged over the used features. The top row shows the attention output\nfor signal and background events using a transformer encoder trained on kinematics only.\nConversely, when the information of the jet constituents is included using the cross-attention\nlayer, the attention output distributions for background events are broader, and the signal\n– 15 –\nJHEP03(2024)144\n0.0 0.5 1.0\nZ( ˜J1)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics+Jets (Cross-attention)\nSignal\nBackground\n0.0 0.5 1.0\nZ( ˜J2)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics+Jets (Cross-attention)\nSignal\nBackground\n0.0 0.5 1.0\nZ( ˜H)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics+Jets (Cross-attention)\nSignal\nBackground\n0.0 0.5 1.0\nZ( ˜J1)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics (Self-attention)\nSignal\nBackground\n0.0 0.5 1.0\nZ( ˜J2)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics (Self-attention)\nSignal\nBackground\n0.0 0.5 1.0\nZ( ˜H)\n0\n250\n500\n750\n1000\n1250Test Events\nKinematics (Self-attention)\nSignal\nBackground\nFigure 7. Top: output of the self-attention layer when trained on kinematics only. Bottom: output\nof the cross-attention layer when trained on kinematics and jet information. In both cases, attention\noutput has the dimensions of (reconstructed particles×features), and for both plots we use 10000\ntest events and average over the features for the background and the signal point withmH = 1 TeV.\n˜J1,˜J2 and ˜H represent the transformed particles as in eq. (2.4).\nKinematics Kinematics +θ Jet str. + Kinematics Jet str. + Kinematics +θ\n91.01% 91.60% 97.23% 98.68%\nTable 2. Area Under the ROC (AUC) for the networks using Kinematics or Kinematics + Jet\nstructure information with/withoutθ.\ndistributions are narrower. The fact that background jets lack a multi-prong structure with\nbroader soft radiations influences the attention output for background events, increasing\nthe output variations in the feature space.\nFinally, we include, alongside the described kinematical information, also the rotation\nangle θ aligning the fat jet axis to theϕ direction after shifting the jetη and ϕ to the centre\nof theη−ϕ plane. This information allows the network to reconstruct the full events and\naccess the correlation of the jet shape to the other fat jet and the beam axis.\nIn table 2, we compare the AUC value of the network using Kinematical inputs (Kins),\nKins +θ, Kins + jet substructure inputs (Jet str.), Kins + Jet str. +θ. Addingθ to Kins\nimproves AUC by 0.59 while addingθ to Kins + Jet str. improves AUC by 1.45. This\nindicates the correlation between all inputs (Jet str.,θ, and Kins) is contributing to the\nsignal and background classification.\n– 16 –\nJHEP03(2024)144\n0.0 0.2 0.4 0.6 0.8 1.0\nSignal eﬃciency\n10−4\n10−3\n10−2\n10−1\n100\nFalse positive rate\nwithout θ\nFull\n50 100 150 200 250\nmJet(GeV)\n0.5\n0.6\n0.7\n0.8\n0.9\nSignal Eﬃciencty\nwithout θ\nFull\n400 500 600 700 800 900\nPTJet (GeV)\n0.2\n0.4\n0.6\n0.8\nSignal Eﬃciencty\nwithout θ\nFull\nFigure 8. Left: the ROC curve and error band of the full model usingθ input (blue) and the model\nwithout θ input (red). The ROC is obtained by using 20,000 signal and background testing events.\nThe error is estimated as in figure 6. Middle(Right): the signal efficiency as varyingmJ1 (pTJ1 ) for the\nbest training results. The ratio is calculated with a score cut of 80% of the signal efficiency for 20,000\nsignal samples. The efficiency (without) usingθ is shown by blue(red) bars indicating statistical errors\nwithout taking into training errors. The acceptance of the full model is higher than withoutθ input\nat mJ1 ∼mh( pJ1 ∼mH/2).\nIn figure 8 Left, we show the ROC curve of the network trained without theθ inputs\n(red) compared to the ROC curve of our coss-attention model (blue). The improvement in\nthe background rejection is a factor of four for a signal efficiency of 80%. Therefore, including\nθ results in a drastically increased performance. In figure 8 Middle and Right, we show the\nefficiency in rejecting background for the model with/withoutθ inputs. The model withθ\nhas higher efficiency atmJ1 ∼mh and pT ∼mH\n2 . In short, the model can focus more on the\nH →hh kinematics withθ inputs. We also looked for simple correlations amongθ and the\nother kinematical variables, such asηJ ϕJ, but did not find any apparent ones contributing\nto the selection improvement, consistent with insignificant improvement by addingθ to the\nmodel using Kins. This indicates that the network is not merely utilizing the transformer\noutput of the jet substructure individually. Instead, it tunes the jet substructure analysis\nunder the condition of global kinematical information. The correlations within the internal\nstructures of the jet will be investigated in future publications.\n5 Interpretation of the transformer encoder results\nIn the following section, we discuss additional methods to interpret and analyze the results of\nthe transformer encoder with cross-attention, which has the best classification performance\nin figure 6. The interpretation methods are generic and can be further applied to other\nnetworks to interpret their results. As attention-based transformer models excel in capturing\nintricate spatial relationships and global context within data, their interpretability becomes\nparamount. Interpretation methods for attention-based transformers aim to elucidate the\nvisual cues, features, and regions that contribute significantly to the model’s predictions.\nCommon interpretation methods are\n• Attention Maps:attention maps visualize the focus of the model by highlighting the\nparticles in the cloud that receive higher attention. These maps provide a direct view\ninto which particles are considered most relevant for prediction, facilitating an intuitive\nunderstanding of the model’s decision-making process [92].\n– 17 –\nJHEP03(2024)144\n• Grad-CAM It generates class-specific activation maps by weighting the gradients of\nthe predicted class score with respect to the final transformer layer [93]. This technique\nhighlights the regions in the feature space that are crucial for the model’s classification\ndecision and thus can provide a geometrical interpretation,η−ϕ plane, of the learned\ninformation by the network.\n• Saliency MapsSaliency maps for transformer models are a form of interpretability\ntechnique used to understand and visualize the importance of different parts of the input\nsequence concerning the model’s predictions. Saliency maps highlight the regions of the\ninput that most significantly influence the model’s output, providing insights into the\nmodel’s decision-making process [94–96]. By examining the saliency map, users can gain\ninsights into which parts of the input sequence are crucial for the model’s predictions.\n• Layer-wise Relevance Propagation (LRP)The primary goal of LRP is to assign\nrelevance scores to input features, indicating their contribution to the model’s out-\nput [97]. However, it’s worth noting that LRP has limitations, and its effectiveness\ncan vary depending on the specific neural network architecture and the nature of the\ntask. Different variants of LRP have been proposed to address specific challenges and\nimprove its applicability to various models.\nThe interpretation of attention-based transformer models is pivotal for unlocking their\nfull potential and ensuring their responsible deployment in real-world applications. Among\nall the mentioned methods, we adopt the attention maps and Grad-CAM to interpret the\nlearned information using the transformer model.\n5.1 Attention maps\nAttention maps serve as a bridge between the abstract nature of neural network computations\nand the desired interpretability. These maps visualize the attention scores assigned to each\nparticle token in the input sequence, providing a representation of where the model focuses\nits attention during processing.\nThe analysis of the attention maps highlights the particle tokens that receive higher\nattention scores, indicating their significance in the model’s decision. Also, it reveals how\nparticle tokens relate to each other. For instance, it highlights the information extracted from\nthe jet constituents relevant to the reconstructed objects. Importantly, examining attention\nmaps can pinpoint areas where the model might struggle or make mistakes.\nIn this context, we utilize attention maps to analyze the acquired information from the\nlast transformer layer of the transformed jet constituents. Our focus centres on the output\nof the network shown in figure 1. We begin by examining the attention maps of the Add()\nlayer, which contains information about the jet substructure. In this case, the attention\nmaps denoted asαij in eq. (2.2), have dimensions of(nheads,50,50), where 50 represents\nthe number of constituents in the jet, andnheads denotes the number of self-attention heads.\nWe takenheads = 5 , see appendix A for detail.\nFigure 9 displays the values of the attention maps for each attention head individually,\nwith signal events in the top row and background events in the bottom row. Given that jet\nconstituents are originally ordered by their momentum, the X and Y axes ticks represent the\n– 18 –\nJHEP03(2024)144\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\nSelf Attention head-1\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\nSelf Attention head-2\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\nSelf Attention head-3\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\nSelf Attention head-4\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\nSelf Attention head-5\n0 20 40\nÆ i\n0\n10\n20\n30\n40\nÆ j\n10 ° 1\n2 £ 10 ° 2\n3 £ 10 ° 2\n4 £ 10 ° 2\n6 £ 10 ° 2\n2 £ 10 ° 2\n3 £ 10 ° 2\n4 £ 10 ° 2\n6 £ 10 ° 2\nFigure 9. Attention maps of the last self-attention transformer layer, which processes the jet\nsubstructure for the signal (top) and backgrounds (bottom) for a 120K test event.\n˜c1 ˜c2 ˜c3 ˜c4 ˜c5 ˜c6 ˜c7 ˜c8 ˜c9 ˜c10 ˜c11 ˜c12 ˜c13 ˜c14 ˜c15 ˜c16 ˜c17 ˜c18 ˜c19 ˜c20\n˜J1\n˜J2\n˜H\n0.91 0.91 0.91 0.89 0.87 0.87 0.85 0.84 0.82 0.83 0.8 0.79 0.81 0.8 0.79 0.78 0.77 0.76 0.76 0.76\n0.91 0.91 0.92 0.9 0.88 0.88 0.86 0.85 0.83 0.83 0.81 0.8 0.82 0.8 0.79 0.78 0.77 0.76 0.76 0.76\n0.98 0.98 1 0.96 0.94 0.94 0.91 0.88 0.86 0.87 0.84 0.81 0.85 0.82 0.81 0.79 0.78 0.76 0.76 0.75\nCross Attention maps\n˜c1 ˜c2 ˜c3 ˜c4 ˜c5 ˜c6 ˜c7 ˜c8 ˜c9 ˜c10 ˜c11 ˜c12 ˜c13 ˜c14 ˜c15 ˜c16 ˜c17 ˜c18 ˜c19 ˜c20\n˜J1\n˜J2\n˜H\n0.94 0.93 0.96 0.92 0.91 0.91 0.88 0.85 0.82 0.8 0.78 0.76 0.8 0.78 0.77 0.75 0.74 0.74 0.73 0.72\n0.97 0.97 1 0.96 0.95 0.94 0.91 0.88 0.84 0.81 0.78 0.76 0.81 0.79 0.78 0.75 0.74 0.74 0.73 0.72\n0.85 0.85 0.85 0.85 0.85 0.85 0.85 0.85 0.85 0.85 0.86 0.86 0.86 0.86 0.86 0.87 0.87 0.87 0.88 0.89\nFigure 10. Cross-attention maps of the cross-attention transformer layer averaged over the 8\ncross-attention heads, which processes the jet substructure and the event kinematics for the signal\n(top) and backgrounds (bottom) for a 120K test event. The X-axis shows the attention score for the\nfirst transformed20th jet contents while the Y-axis shows the attention score for the transformed\nreconstructed final state particles.\nattention values of the transformed jet constituents in descending order (where the zero tick\nrepresents the leading transformed jet constituent particle). The attention map values reveal\nthat the model concentrates on the leading and second-leading jet constituents to identify\nevents as signal-like, particularly evident in attention heads1,2,4,and 5. In fact, this reflects\nthe efficiency of the network to capture the two-prong structure of the signal events. On\nthe other hand, the network assigns high attention to the wide momentum orders of the jet\nconstituents when the network identifies the input as a background event. We stress here\n– 19 –\nJHEP03(2024)144\nthat, while the transformer layers intermingle particle and feature tokens, the skip connection\nstill preserves the order of the attention output in relation to the original input data.\nThe attention maps for background events exhibit significant agreement with the jet\nsubstructure of the background events presented in figure 4. To this end, through an analysis\nof the attention scores from the last transformer layer of the jet constituents, we confirm that\nthe transformer model adeptly extracts the correct multi-prong structure of signal events.\nMeanwhile, for background events dominated by QCD processes, the model exhibits high\nattention across a wide momentum range of jet constituents.\nThe attention maps of the cross-attention layer illustrate the attention scores between\nthe jet constituents and the reconstructed particles, including the leading and second-leading\njets and the heavy Higgs. The dimension of the attention score in the cross-attention layer\nis (nheads,3,50), where3 represents the number of reconstructed particles,50 is the number\nof jet constituents, andnheads is the number of cross-attention heads, set at8. Figure 10\ndisplays the cross-attention maps for signal events (top) and background events (bottom),\naveraged over the used cross-attention heads.\nThe cross-attention maps for signal events exhibit a stronger correlation between the\nhighest momentum transformed jet constituents and the heavy Higgs. In contrast, the\nHeavy Higgs displays a flat attention pattern with jet constituents of different momenta for\nthe background events. Indeed, the results from the cross-attention maps, along with the\ncross-attention output shown in figure 7, provide a comprehensive overview of the impact of\nthe cross-attention layer. This layer effectively assigns information from the jet constituents\nto the kinematics of the reconstructed particles to enhance the classification performance.\n5.2 Grad-CAM\nGrad-CAM is a technique designed to visualize and interpret the decisions made by DNN\nmodels. It builds upon the idea of class activation maps (CAMs) [98, 99] but extends it to\nmodels with arbitrary architectures. The primary objective of Grad-CAM is to highlight\nthe important regions in a transformed input features space,˜η−˜ϕ plane, that contribute\nto the prediction of a specific class [92].\nLet Fk(˜η, ˜ϕ) represent the activation of the final transformer layer for thekth event. The\ngradient of the predicted class score (Yc) with respect to the activation output is computed as:\n∂Yc\n∂Fk\n(5.1)\nThis gradient is then globally averaged to obtain the importance weights (α) as\nαk(˜η, ˜ϕ) = 1\nZ\n∑ ∂Yc\n∂Fk(˜η, ˜ϕ,˜pT)\n(5.2)\nwhere Z is the size of the feature activations, and the sum runs over the jet constituents.\n˜η, ˜ϕ and ˜pT are the transformed features. The final Grad-CAM heatmap is a weighted\nsum of gradients as\nGrad-CAM(˜η, ˜ϕ) = 1\nk\n∑\nk\nαk(˜η, ˜ϕ)Fk(˜η, ˜ϕ,˜pT) (5.3)\n– 20 –\nJHEP03(2024)144\nFigure 11.Grad-CAM results for 5000 test events of the transformer model with cross-attention. Left:\npT distribution of the jet constituents when events are predicted as signal events (top) and background\nevents(bottom)inthe η-ϕplane. Notethattheasymmetricpatternisduetotheflippingtransformation\nin the pre-processing steps in which all constituents with larger momentum are reflected in the positive\nη direction. Right: heat-map of the Grad-CAM results in transformed˜η-˜ϕ plane, as in eq. (2.4).\nThis heatmap highlights the regions of the input image that contribute the most to\nthe prediction of the target class.\nIn general, it operates by utilizing the gradient information flowing into the final trans-\nformer layer in the following way: during the forward pass, the neural network processes the\ninput particle cloud, and the activations of the final transformer layer are obtained. The\ngradients of the predicted class score with respect to the final transformer layer activation are\ncomputed during the backward pass. The gradients are then used to calculate the importance\nof the activation map. These importance scores are essentially the weights assigned to each\nspatial location of the final transformer layer. The weighted sum of the particle tokens\nis computed, creating the Grad-CAM. This map highlights the regions that contributed\nthe most to the final prediction. Additionally, upsampling is often employed to match the\nGrad-CAM dimensions with the original input features.\nTo visualize the geometrical interpretation of the learned information from the jet\nconstituents, we utilize Grad-CAM on the final self-attention layer of the jets, specifically, the\nAdd() Layer depicted in figure 1. The results are shown in figure 11 for 5000 test images. The\nleft panel illustrates thepT distribution of the predicted events as signal (top) or background\n(bottom). Signal events are considered for the benchmark point withmH = 1 TeV. The\nright panel displays the heat map of the Grad-CAM output for the predicted signal (top)\nand the predicted background (bottom).\n– 21 –\nJHEP03(2024)144\nThe visualization of the heat map clarifies that the transformer model focuses on the\ntwo-prong structure to classify the input event as a signal. On the other hand, it relies on\nthe soft-radiation pattern to classify the input event as a background event. Interestingly, we\nfound that the result highlights that the model focuses on the positiveη direction to make\npredictions, which is due to the flipping transformation done in the pre-processing step.\nWhile Grad-CAM has the power to explain the considered regions in the feature space\nfor the network predictions, one of its drawbacks is that it relies on gradient information\nfrom the final transformer layer. In cases where global context is crucial for decision-making,\nGrad-CAM may not capture long-range dependencies effectively. Moreover, Grad-CAM\nmight be sensitive to small changes in the input, potentially making it less robust in the\npresence of adversarial examples.\n6 Conclusion\nIn conclusion, this paper introduces an innovative method for enhancing event classification by\neffectively incorporating information from both global kinematics and substructure of jets in an\nevent. Conventional approaches, using simple concatenation to combine the event information,\nhave limitations, especially for scenarios where kinematical structures dominate. Specifically,\nthe proposed method utilizes a transformer encoder with cross-attention layers, enabling the\nextraction of different scale information from both global kinematics and jet substructure.\nThe results demonstrate a substantial improvement in classification performance compared\nto traditional concatenation methods. Indeed, the analysis of the learned information,\nconducted through attention maps and a Grad-CAM algorithm for visual representation,\nprovides valuable insights into the model focus on important particles and geometric regions\nin the transformed˜η−˜ϕ plane that is crucial for event classification.\nWe have validated this approach by focusing on the dominant decay channel, i.e., into\nfour b-jets, of SM-like Higgs boson pairs produced in the resonant decay of a heavier CP-even\nHiggs state, at the HL-LHC. This challenging scenario involves merging slimb-jets into a\nfatjet, due to the boosted nature of the lighter Higgs states so that the possibility of accessing\npartonic dynamics is apparently lost at the detector level. Furthermore, this occurs in an\nenvironment rich in tracks and calorimetric information not directly pertaining to the hard\nscattering sought, as typical of this CERN machine upgrade. Therefore, all these aspects\nadd complexity to the classification task. Despite these challenges, the proposed method\neffectively addresses the intricacies of the final state in the detectors, ultimately outperforming\nmainstream signal selection procedures, whether based solely on kinematical analysis or less\nadvanced ML tools. In the broader context, this research contributes to utilizing advanced jet\nidentification techniques for global event reconstruction towards the understanding of collision\nevents consisting of dynamics acting at various physics scales. Thus, the proposed method\noffers a promising avenue for improving the accuracy and efficiency of event classification in\npotentially many more complex scenarios encountered in high energy physics experiments.\n– 22 –\nJHEP03(2024)144\nAcknowledgments\nThe work of SM is supported in part through the NExT Institute, the Knut and Alice\nWallenberg Foundation under the Grant No. KAW 2017.0100 (SHIFT) and the STFC\nConsolidated Grant No. ST/L000296/1. AH and MN are funded by grant number 22H05113,\n“Foundation of Machine Learning Physics”, Grant in Aid for Transformative Research Areas\nand 22K03626, Grant-in-Aid for Scientific Research (C).\nA Networks structure\nIn this study, we employed four transformer encoders with distinct configurations to analyze\nvarious datasets. For all networks, we configured an output layer with two neurons and\napplied softmax activation. Additionally, we utilized the Adam optimizer [100] to minimize\nthe sparse categorical cross-entropy loss function [101], setting the learning rate at0.005.\nOur training dataset comprised one million samples, with 20,000 allocated for validation and\n100,000 for testing. The training batches were adjusted to a size of 500.\nFollowing data preprocessing, we obtained three datasets: one for the leading jet contents\nwith dimensions of(50,4), where50 represents the number of jet constituents and4 denotes\nthe considered features (pT, η, ϕ, log(pT)). We also use the same information for the second\nleading jet contents. The last dataset for event kinematics with dimensions of(3,6), where3\ncorresponds to the leading jet, the second leading jet and the (reconstructed) heavy Higgs\nboson while the6 represents the used kinematics as in figure 5. The structure of the different\nnetworks is the following:\n• A two-stream self-attention transformer encoder is employed for jet substructure. The\nnetwork takes two separate data sets for the leading and second-leading jet constituents\nas input, processed through two distinct transformer layers. Each transformer layer\nis repeated three times. These transformer layers consist of five self-attention heads\noperating in parallel. The output from the attention heads is then integrated with the\noriginal input data via a skip connection layer [102]. The resulting output from the\nskip connection is flattened and forwarded to two fully connected layers with 128 and\n4 neurons, respectively, using the GELU activation function [103]. The output from\nthe final fully connected layer is subsequently combined with the self-attention output\nthrough a second skip connection layer.9 The final output of the transformer layer\nundergoes a normalization layer and has the same dimension as the input dataset. The\nnormalized output from each transformer layer is combined through an addition layer.\nThis output then passes through a Multi-Layer Perceptron (MLP) comprising two fully\nconnected layers with dimensions 128 and 64, employing the GELU activation function.\nFollowing each fully connected layer, a dropout layer with a dropout rate of20% is\napplied. The output is then passed to the output layer for classification. The model is\ntrained for 30 epochs with a batch of size500 in 1421 seconds.\n• A single-stream self-attention transformer encoder is employed for kinematics analysis.\nThe network exclusively utilizes the kinematics dataset as input. To achieve this, we\n9Skip connection is of the utmost importance to stabilize the gradient flow of the model.\n– 23 –\nJHEP03(2024)144\nadopt the identical structure of the self-attention transformer encoder designed for jet\nsubstructure but with a singular stream. The model is trained for 30 epochs with a\nbatch of size500 in 1390 seconds.\n• A three-stream transformer encoder is employed to analyze the leading and subleading\njet constituents and the reconstructed kinematics. In this approach, we adjust the\ntransformer layers for the leading and subleading jets from the first network, while\nthe transformer layers for the kinematics are adapted from the latter network. The\noutput of the self-attention transformer encoder layers for jet constituents is added\nvia an addition layer. The resulting output from the addition layer, along with the\noutput from the self-attention transformer layers of the kinematics, is then fed to a\ncross-attention transformer layer. This cross-attention transformer layer is repeated\ntwice, and the output has the same dimensions as the input kinematics dataset, i.e.,\n(3,6). Subsequently, this output passes through a MLP consisting of two fully connected\nlayers with dimensions 128 and 64, utilizing the GELU activation function. After each\nfully connected layer, a dropout layer with a dropout rate of20% is applied. The\nresulting output is then forwarded to the output layer for classification. The model is\ntrained for 30 epochs with a batch of size500 in 1576 seconds.\n• The final network is configured to mirror the three-stream transformer encoder, with\nthe only modification being the substitution of the cross-attention transformer layers\nwith a single concatenation layer. The model is trained for 30 epochs with a batch of\nsize 500 in 1282 seconds.\nFor training of all models, we use two NVIDIA RTX A6000 GPU cards using the Tensorflow\nmirror strategy with the utilization of80% and 30% for the first and second cards, respectively,\nand memory consumption of96% (48 GB) of both cards.\nOpen Access. This article is distributed under the terms of the Creative Commons\nAttribution License (CC-BY4.0), which permits any use, distribution and reproduction in\nany medium, provided the original author(s) and source are credited.\nReferences\n[1] A. Chakraborty, S.H. Lim and M.M. Nojiri,Interpretable deep learning for two-prong jet\nclassification with jet spectra, JHEP 07 (2019) 135 [arXiv:1904.02092] [INSPIRE].\n[2] Y.-L. Chung, S.-C. Hsu and B. Nachman,Disentangling boosted Higgs boson production modes\nwith machine learning, 2021JINST 16 P07002 [arXiv:2009.05930] [INSPIRE].\n[3] J. Guo, J. Li, T. Li and R. Zhang,Boosted Higgs boson jet reconstruction via a graph neural\nnetwork, Phys. Rev. D103 (2021) 116025 [arXiv:2010.05464] [INSPIRE].\n[4] C.K. Khosa and S. Marzani,Higgs boson tagging with the Lund jet plane, Phys. Rev. D104\n(2021) 055043 [arXiv:2105.03989] [INSPIRE].\n[5] K. Datta, A. Larkoski and B. Nachman,Automating the construction of jet observables with\nmachine learning, Phys. Rev. D100 (2019) 095016 [arXiv:1902.07180] [INSPIRE].\n– 24 –\nJHEP03(2024)144\n[6] D. Cogollo et al.,Deep learning analysis of the inverse seesaw in a3-3-1 model at the LHC,\nPhys. Lett. B811 (2020) 135931 [arXiv:2008.03409] [INSPIRE].\n[7] M. Grossi, J. Novak, B. Kersevan and D. Rebuzzi,Comparing traditional and deep-learning\ntechniques of kinematic reconstruction for polarization discrimination in vector boson scattering,\nEur. Phys. J. C80 (2020) 1144 [arXiv:2008.05316] [INSPIRE].\n[8] V.S. Ngairangbam, A. Bhardwaj, P. Konar and A.K. Nayak,Invisible Higgs search through\nvector boson fusion: a deep learning approach, Eur. Phys. J. C80 (2020) 1055\n[arXiv:2008.05434] [INSPIRE].\n[9] C. Englert et al.,Sensing Higgs boson cascade decays through memory, Phys. Rev. D102 (2020)\n095027 [arXiv:2008.08611] [INSPIRE].\n[10] F.F. Freitas, J. Gonçalves, A.P. Morais and R. Pasechnik,Phenomenology of vector-like leptons\nwith deep learning at the Large Hadron Collider, JHEP 01 (2021) 076 [arXiv:2010.01307]\n[INSPIRE].\n[11] A. Stakia et al.,Advances in multi-variate analysis methods for new physics searches at the\nLarge Hadron Collider, Rev. Phys.7 (2021) 100063 [arXiv:2105.07530] [INSPIRE].\n[12] F. Jorge et al.,Top squark signal significance enhancement by different machine learning\nalgorithms, Int. J. Mod. Phys. A37 (2022) 2250197 [arXiv:2106.06813] [INSPIRE].\n[13] J. Ren et al.,Detecting an axion-like particle with machine learning at the LHC, JHEP 11\n(2021) 138 [arXiv:2106.07018] [INSPIRE].\n[14] D. Alvestad et al.,Beyond cuts in small signal scenarios: enhanced sneutrino detectability using\nmachine learning, Eur. Phys. J. C83 (2023) 379 [arXiv:2108.03125] [INSPIRE].\n[15] S. Jung, Z. Liu, L.-T. Wang and K.-P. Xie,Probing Higgs boson exotic decays at the LHC with\nmachine learning, Phys. Rev. D105 (2022) 035008 [arXiv:2109.03294] [INSPIRE].\n[16] M. Drees, M. Shi and Z. Zhang,Machine learning optimized search for theZ′ from U(1)Lµ−Lτ\nat the LHC, arXiv:2109.07674 [INSPIRE].\n[17] A.S. Cornell et al.,Boosted decision trees in the era of new physics: a smuon analysis case\nstudy, JHEP 04 (2022) 015 [arXiv:2109.11815] [INSPIRE].\n[18] X.C. Vidal, L.D. Maroñas and Á.D. Suárez,How to use machine learning to improve the\ndiscrimination between signal and background at particle colliders, Appl. Sciences11 (2021)\n11076 [arXiv:2110.15099] [INSPIRE].\n[19] J. Lin, M. Freytsis, I. Moult and B. Nachman,Boosting H →b¯b with machine learning, JHEP\n10 (2018) 101 [arXiv:1807.10768] [INSPIRE].\n[20] E.A. Moreno et al.,Interaction networks for the identification of boostedH →bb decays, Phys.\nRev. D102 (2020) 012010 [arXiv:1909.12285] [INSPIRE].\n[21] Y.-L. Chung, K. Cheung and S.-C. Hsu,Sensitivity of two-Higgs-doublet models on Higgs-pair\nproduction viab¯bb¯b final state, Phys. Rev. D106 (2022) 095015 [arXiv:2207.09602] [INSPIRE].\n[22] J.H. Kim et al.,Portraying double Higgs at the Large Hadron Collider, JHEP 09 (2019) 047\n[arXiv:1904.08549] [INSPIRE].\n[23] L. Huang et al.,Portraying double Higgs at the Large Hadron Collider II, JHEP 08 (2022) 114\n[arXiv:2203.11951] [INSPIRE].\n[24] W. Esmail, A. Hammad and S. Moretti,Sharpening theA→Z(∗)h signature of the type-II\n2HDM at the LHC through advanced machine learning, JHEP 11 (2023) 020\n[arXiv:2305.13781] [INSPIRE].\n– 25 –\nJHEP03(2024)144\n[25] K. Ban, K. Kong, M. Park and S.C. Park,Exploring the synergy of kinematics and dynamics\nfor collider physics, arXiv:2311.16674 [INSPIRE].\n[26] A. Chakraborty et al.,Revisiting jet clustering algorithms for new Higgs boson searches in\nhadronic final states, Eur. Phys. J. C82 (2022) 346 [arXiv:2008.02499] [INSPIRE].\n[27] A. Chakraborty et al.,Re-evaluating jet reconstruction techniques for new Higgs boson searches,\nPoS ICHEP2022 (2022) 503 [arXiv:2212.02246] [INSPIRE].\n[28] A. Chakraborty et al.,Fat b-jet analyses using old and new clustering algorithms in new Higgs\nboson searches at the LHC, Eur. Phys. J. C83 (2023) 347 [arXiv:2303.05189] [INSPIRE].\n[29] G. Cerro et al.,Spectral clustering for jet reconstruction, PoS ICHEP2022 (2022) 771\n[arXiv:2211.10164] [INSPIRE].\n[30] A. Vaswani et al.,Attention is all you need, in the proceedings of the31st international\nconference on neural information processing systems, (2017) [arXiv:1706.03762] [INSPIRE].\n[31] B. Käch, D. Krücker and I. Melzer-Pellmann,Point cloud generation using transformer\nencoders and normalising flows, arXiv:2211.13623 [INSPIRE].\n[32] T. Finke, M. Krämer, A. Mück and J. Tönshoff,Learning the language of QCD jets with\ntransformers, JHEP 06 (2023) 184 [arXiv:2303.07364] [INSPIRE].\n[33] H. Qu, C. Li and S. Qian,Particle transformer for jet tagging, arXiv:2202.03772 [INSPIRE].\n[34] P.T. Komiske, E.M. Metodiev and J. Thaler,Energy flow networks: deep sets for particle jets,\nJHEP 01 (2019) 121 [arXiv:1810.05165] [INSPIRE].\n[35] H. Qu and L. Gouskos,ParticleNet: jet tagging via particle clouds, Phys. Rev. D101 (2020)\n056019 [arXiv:1902.08570] [INSPIRE].\n[36] ATLAS collaboration, Search for resonant pair production of Higgs bosons in theb¯bb¯b final\nstate usingpp collisions at√s= 13 TeV with the ATLAS detector, Phys. Rev. D105 (2022)\n092002 [arXiv:2202.07288] [INSPIRE].\n[37] G.C. Branco et al.,Theory and phenomenology of two-Higgs-doublet models, Phys. Rept.516\n(2012) 1 [arXiv:1106.0034] [INSPIRE].\n[38] T.D. Lee,A theory of spontaneous T violation, Phys. Rev. D8 (1973) 1226 [INSPIRE].\n[39] S.L. Glashow and S. Weinberg,Natural conservation laws for neutral currents, Phys. Rev. D15\n(1977) 1958 [INSPIRE].\n[40] I.F. Ginzburg and M. Krawczyk,Symmetries of two Higgs doublet model and CP violation,\nPhys. Rev. D72 (2005) 115013 [hep-ph/0408011] [INSPIRE].\n[41] S. Antusch, O. Fischer, A. Hammad and C. Scherb,Testing CP properties of extra Higgs states\nat the HL-LHC, JHEP 03 (2021) 200 [arXiv:2011.10388] [INSPIRE].\n[42] S. Antusch, O. Fischer, A. Hammad and C. Scherb,Explaining excesses in four-leptons at the\nLHC with a double peak from a CP violating two Higgs doublet model, JHEP 08 (2022) 224\n[arXiv:2112.00921] [INSPIRE].\n[43] A. Arhrib et al.,Double neutral Higgs production in the two-Higgs doublet model at the LHC,\nJHEP 08 (2009) 035 [arXiv:0906.0387] [INSPIRE].\n[44] A. Hammad, M. Park, R. Ramos and P. Saha,Exploration of parameter spaces assisted by\nmachine learning, Comput. Phys. Commun.293 (2023) 108902 [arXiv:2207.09959] [INSPIRE].\n– 26 –\nJHEP03(2024)144\n[45] J. Alwall et al.,The automated computation of tree-level and next-to-leading order differential\ncross sections, and their matching to parton shower simulations, JHEP 07 (2014) 079\n[arXiv:1405.0301] [INSPIRE].\n[46] W. Porod,SPheno, a program for calculating supersymmetric spectra, SUSY particle decays and\nSUSY particle production ate+e− colliders, Comput. Phys. Commun.153 (2003) 275\n[hep-ph/0301101] [INSPIRE].\n[47] W. Porod and F. Staub,SPheno 3.1: extensions including flavour, CP-phases and models\nbeyond the MSSM, Comput. Phys. Commun.183 (2012) 2458 [arXiv:1104.1573] [INSPIRE].\n[48] T. Sjostrand, S. Mrenna and P.Z. Skands,PYTHIA 6.4 physics and manual, JHEP 05 (2006)\n026 [hep-ph/0603175] [INSPIRE].\n[49] J. Alwall et al.,Comparative study of various algorithms for the merging of parton showers and\nmatrix elements in hadronic collisions, Eur. Phys. J. C53 (2008) 473 [arXiv:0706.2569]\n[INSPIRE].\n[50] M.L. Mangano, M. Moretti, F. Piccinini and M. Treccani,Matching matrix elements and\nshower evolution for top-quark production in hadronic collisions, JHEP 01 (2007) 013\n[hep-ph/0611129] [INSPIRE].\n[51] DELPHES 3collaboration, DELPHES 3, a modular framework for fast simulation of a\ngeneric collider experiment, JHEP 02 (2014) 057 [arXiv:1307.6346] [INSPIRE].\n[52] M. Cacciari, G.P. Salam and G. Soyez,The anti-kt jet clustering algorithm, JHEP 04 (2008)\n063 [arXiv:0802.1189] [INSPIRE].\n[53] S. Catani, Y.L. Dokshitzer, M.H. Seymour and B.R. Webber,Longitudinally invariantKt\nclustering algorithms for hadron hadron collisions, Nucl. Phys. B406 (1993) 187 [INSPIRE].\n[54] D. Krohn, J. Thaler and L.-T. Wang,Jet trimming, JHEP 02 (2010) 084 [arXiv:0912.1342]\n[INSPIRE].\n[55] ATLAS collaboration, ATLAS flavour-tagging algorithms for the LHC run2 pp collision\ndataset, Eur. Phys. J. C83 (2023) 681 [arXiv:2211.16345] [INSPIRE].\n[56] J.M. Butterworth, A.R. Davison, M. Rubin and G.P. Salam,Jet substructure as a new Higgs\nsearch channel at the LHC, Phys. Rev. Lett.100 (2008) 242001 [arXiv:0802.2470] [INSPIRE].\n[57] D.E. Kaplan, K. Rehermann, M.D. Schwartz and B. Tweedie,Top tagging: a method for\nidentifying boosted hadronically decaying top quarks, Phys. Rev. Lett.101 (2008) 142001\n[arXiv:0806.0848] [INSPIRE].\n[58] Y. Cui, Z. Han and M.D. Schwartz,W-jet tagging: optimizing the identification of boosted\nhadronically-decaying W bosons, Phys. Rev. D83 (2011) 074023 [arXiv:1012.2077] [INSPIRE].\n[59] T. Plehn, M. Spannowsky and M. Takeuchi,How to improve top tagging, Phys. Rev. D85\n(2012) 034029 [arXiv:1111.5034] [INSPIRE].\n[60] D.E. Soper and M. Spannowsky,Finding top quarks with shower deconstruction, Phys. Rev. D\n87 (2013) 054012 [arXiv:1211.3140] [INSPIRE].\n[61] C. Anders et al.,Benchmarking an even better top tagger algorithm, Phys. Rev. D89 (2014)\n074047 [arXiv:1312.1504] [INSPIRE].\n[62] G. Kasieczka et al.,Resonance searches with an updated top tagger, JHEP 06 (2015) 203\n[arXiv:1503.05921] [INSPIRE].\n[63] J. Thaler and K. Van Tilburg,Identifying boosted objects withN-subjettiness, JHEP 03 (2011)\n015 [arXiv:1011.2268] [INSPIRE].\n– 27 –\nJHEP03(2024)144\n[64] J. Thaler and K. Van Tilburg,Maximizing boosted top identification by minimizing\nN-subjettiness, JHEP 02 (2012) 093 [arXiv:1108.2701] [INSPIRE].\n[65] A.J. Larkoski, G.P. Salam and J. Thaler,Energy correlation functions for jet substructure,\nJHEP 06 (2013) 108 [arXiv:1305.0007] [INSPIRE].\n[66] I. Moult, L. Necib and J. Thaler,New angles on energy correlation functions, JHEP 12 (2016)\n153 [arXiv:1609.07483] [INSPIRE].\n[67] A.J. Larkoski, S. Marzani, G. Soyez and J. Thaler,Soft drop, JHEP 05 (2014) 146\n[arXiv:1402.2657] [INSPIRE].\n[68] A. Abdesselam et al.,Boosted objects: a probe of beyond the Standard Model physics, Eur. Phys.\nJ. C 71 (2011) 1661 [arXiv:1012.5412] [INSPIRE].\n[69] A. Altheimer et al.,Jet substructure at the Tevatron and LHC: new results, new tools, new\nbenchmarks, J. Phys. G39 (2012) 063001 [arXiv:1201.0008] [INSPIRE].\n[70] A. Altheimer et al.,Boosted objects and jet substructure at the LHC. Report of BOOST2012,\nheld at IFIC Valencia,23rd–27th of July2012, Eur. Phys. J. C74 (2014) 2792\n[arXiv:1311.2708] [INSPIRE].\n[71] J. Cogan, M. Kagan, E. Strauss and A. Schwarztman,Jet-images: computer vision inspired\ntechniques for jet tagging, JHEP 02 (2015) 118 [arXiv:1407.5675] [INSPIRE].\n[72] L.G. Almeida et al.,Playing tag with ANN: boosted top identification with pattern recognition,\nJHEP 07 (2015) 086 [arXiv:1501.05968] [INSPIRE].\n[73] L. de Oliveira et al.,Jet-images — deep learning edition, JHEP 07 (2016) 069\n[arXiv:1511.05190] [INSPIRE].\n[74] P. Baldi et al.,Jet substructure classification in high-energy physics with deep neural networks,\nPhys. Rev. D93 (2016) 094034 [arXiv:1603.09349] [INSPIRE].\n[75] J. Barnard, E.N. Dawe, M.J. Dolan and N. Rajcic,Parton shower uncertainties in jet\nsubstructure analyses with deep neural networks, Phys. Rev. D95 (2017) 014018\n[arXiv:1609.00607] [INSPIRE].\n[76] P.T. Komiske, E.M. Metodiev and M.D. Schwartz,Deep learning in color: towards automated\nquark/gluon jet discrimination, JHEP 01 (2017) 110 [arXiv:1612.01551] [INSPIRE].\n[77] G. Kasieczka, T. Plehn, M. Russell and T. Schell,Deep-learning top taggers or the end of\nQCD?, JHEP 05 (2017) 006 [arXiv:1701.08784] [INSPIRE].\n[78] S. Macaluso and D. Shih,Pulling out all the tops with computer vision and deep learning,\nJHEP 10 (2018) 121 [arXiv:1803.00107] [INSPIRE].\n[79] S. Choi, S.J. Lee and M. Perelstein,Infrared safety of a neural-net top tagging algorithm, JHEP\n02 (2019) 132 [arXiv:1806.01263] [INSPIRE].\n[80] F. Mokhtar, R. Kansal and J. Duarte,Do graph neural networks learn traditional jet\nsubstructure?, in the proceedings of the36th conference on neural information processing\nsystems: workshop on machine learning and the physical sciences, (2022) [arXiv:2211.09912]\n[INSPIRE].\n[81] F. Ma, F. Liu and W. Li,Jet tagging algorithm of graph network with Haar pooling message\npassing, Phys. Rev. D108 (2023) 072007 [arXiv:2210.13869] [INSPIRE].\n[82] S. Gong et al.,An efficient Lorentz equivariant graph neural network for jet tagging, JHEP 07\n(2022) 030 [arXiv:2201.08187] [INSPIRE].\n– 28 –\nJHEP03(2024)144\n[83] D. Guest et al.,Jet flavor classification in high-energy physics with deep neural networks, Phys.\nRev. D94 (2016) 112002 [arXiv:1607.08633] [INSPIRE].\n[84] J. Pearkes, W. Fedorko, A. Lister and C. Gay,Jet constituents for deep neural network based\ntop quark tagging, arXiv:1704.02124 [INSPIRE].\n[85] S. Egan et al.,Long Short-Term Memory (LSTM) networks with jet constituents for boosted top\ntagging at the LHC, arXiv:1711.09059 [INSPIRE].\n[86] K. Fraser and M.D. Schwartz,Jet charge and machine learning, JHEP 10 (2018) 093\n[arXiv:1803.08066] [INSPIRE].\n[87] A. Butter, G. Kasieczka, T. Plehn and M. Russell,Deep-learned top tagging with a Lorentz\nlayer, SciPost Phys.5 (2018) 028 [arXiv:1707.08966] [INSPIRE].\n[88] G. Kasieczka, N. Kiefer, T. Plehn and J.M. Thompson,Quark-gluon tagging: machine learning\nvs detector, SciPost Phys.6 (2019) 069 [arXiv:1812.09223] [INSPIRE].\n[89] G. Cowan, K. Cranmer, E. Gross and O. Vitells,Asymptotic formulae for likelihood-based tests\nof new physics, Eur. Phys. J. C71 (2011) 1554 [Erratum ibid.73 (2013) 2501]\n[arXiv:1007.1727] [INSPIRE].\n[90] LHC Dark Matter Working Groupcollaboration, LHC dark matter working group:\nnext-generation spin-0 dark matter models, Phys. Dark Univ.27 (2020) 100351\n[arXiv:1810.09420] [INSPIRE].\n[91] E. Arganda, A. Delgado, R.A. Morales and M. Quirós,LHC search strategy for squarks in\nHiggsino-LSP scenarios with leptons and b-jets in the final state, Particles 5 (2022) 265\n[arXiv:2206.05977] [INSPIRE].\n[92] H. Chefer, S. Gur and L. Wolf,Transformer interpretability beyond attention visualization, in\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, (2021),\np. 782 [arXiv:2012.09838].\n[93] R.R. Selvaraju et al.,Grad-CAM: visual explanations from deep networks via gradient-based\nlocalization, inProceedings of the IEEE international conference on computer vision, (2017),\np. 618 [arXiv:1610.02391].\n[94] Y. Huang et al.,SSiT: saliency-guided self-supervised image transformer for diabetic retinopathy\ngrading, arXiv:2210.10969.\n[95] N. Duong-Trung, D.-M. Nguyen and D. Le-Phuoc,Temporal saliency detection towards\nexplainable transformer-based timeseries forecasting, arXiv:2212.07771.\n[96] C. Lu, H. Zhu and P. Koniusz,From saliency to DINO: saliency-guided vision transformer for\nfew-shot keypoint detection, arXiv:2304.03140.\n[97] A. Binder et al.,Layer-wise relevance propagation for neural networks with local\nrenormalization layers, inArtificial neural networks and machine learning — ICANN2016:\n25th international conference on artificial neural networks, Barcelona, Spain,6–9 September\n2016, Proceedings, part II25, Springer, (2016), p. 63 [arXiv:1604.00825].\n[98] I. Cherepanov, A. Ulmer, J.G. Joewono and J. Kohlhammer,Visualization of class activation\nmaps to explain AI classification of network packet captures, in2022 IEEE symposium on\nvisualization for cyber security (VizSec), IEEE (2022), p. 1\n[DOI:10.1109/VizSec56996.2022.9941392] [arXiv:2209.02045]\n[99] B. Zhou et al.,Learning deep features for discriminative localization, inProceedings of the IEEE\nconference on computer vision and pattern recognition, (2016), p. 2921 [arXiv:1512.04150].\n– 29 –\nJHEP03(2024)144\n[100] D.P. Kingma and J. Ba,Adam: a method for stochastic optimization, arXiv:1412.6980\n[INSPIRE].\n[101] J. Terven, D.M. Cordova-Esparza, A. Ramirez-Pedraza and E.A. Chavez-Urbiola,Loss\nfunctions and metrics in deep learning, arXiv:2307.02694.\n[102] Z. Lai et al.,Rethinking skip connections in encoder-decoder networks for monocular depth\nestimation, arXiv:2208.13441.\n[103] D. Hendrycks and K. Gimpel,Gaussian Error Linear Units (GELUs), arXiv:1606.08415\n[INSPIRE].\n– 30 –",
  "topic": "Physics",
  "concepts": [
    {
      "name": "Physics",
      "score": 0.8422698378562927
    },
    {
      "name": "Encoder",
      "score": 0.47088274359703064
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4328887462615967
    },
    {
      "name": "Event (particle physics)",
      "score": 0.4311453700065613
    },
    {
      "name": "Computer science",
      "score": 0.1370539665222168
    },
    {
      "name": "Quantum mechanics",
      "score": 0.10287025570869446
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I138728355",
      "name": "High Energy Accelerator Research Organization",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I43439940",
      "name": "University of Southampton",
      "country": "GB"
    }
  ]
}