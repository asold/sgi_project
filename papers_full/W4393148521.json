{
  "title": "Temporal Correlation Vision Transformer for Video Person Re-Identification",
  "url": "https://openalex.org/W4393148521",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5030109120",
      "name": "Pengfei Wu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5100350774",
      "name": "Le Wang",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5113827261",
      "name": "Sanping Zhou",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A5081114810",
      "name": "Gang Hua",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019248683",
      "name": "Changyin Sun",
      "affiliations": [
        "Anhui University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3185002841",
    "https://openalex.org/W4312372290",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W4327673107",
    "https://openalex.org/W2798329462",
    "https://openalex.org/W2798874329",
    "https://openalex.org/W2779003141",
    "https://openalex.org/W2955983623",
    "https://openalex.org/W6683411478",
    "https://openalex.org/W2044986361",
    "https://openalex.org/W1932380673",
    "https://openalex.org/W3195939161",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W6631782140",
    "https://openalex.org/W3043322005",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W3139508020",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W3159326306",
    "https://openalex.org/W3042662552",
    "https://openalex.org/W2948383821",
    "https://openalex.org/W2796364723",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2986451890",
    "https://openalex.org/W2901249123",
    "https://openalex.org/W2794704438",
    "https://openalex.org/W3155265626",
    "https://openalex.org/W3134396714",
    "https://openalex.org/W2214352687",
    "https://openalex.org/W2463071499",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W2783855081",
    "https://openalex.org/W6622321702",
    "https://openalex.org/W2798794112",
    "https://openalex.org/W4312794873",
    "https://openalex.org/W3106643287",
    "https://openalex.org/W3035486808",
    "https://openalex.org/W3035252826",
    "https://openalex.org/W29474918",
    "https://openalex.org/W2905632499",
    "https://openalex.org/W3010031195",
    "https://openalex.org/W3168146779",
    "https://openalex.org/W2520433280",
    "https://openalex.org/W2901617102",
    "https://openalex.org/W6743440100",
    "https://openalex.org/W2622829582",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W760855798",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W4287268097",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W3096748692",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W4386076556",
    "https://openalex.org/W4231095098",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4304892542",
    "https://openalex.org/W3168915470",
    "https://openalex.org/W3034417718",
    "https://openalex.org/W2963078173",
    "https://openalex.org/W3180143550",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W3106853541",
    "https://openalex.org/W4287022599",
    "https://openalex.org/W2964140013",
    "https://openalex.org/W3107901913",
    "https://openalex.org/W3170294077"
  ],
  "abstract": "Video Person Re-Identification (Re-ID) is a task of retrieving persons from multi-camera surveillance systems. Despite the progress made in leveraging spatio-temporal information in videos, occlusion in dense crowds still hinders further progress. To address this issue, we propose a Temporal Correlation Vision Transformer (TCViT) for video person Re-ID. TCViT consists of a Temporal Correlation Attention (TCA) module and a Learnable Temporal Aggregation (LTA) module. The TCA module is designed to reduce the impact of non-target persons by relative state, while the LTA module is used to aggregate frame-level features based on their completeness. Specifically, TCA is a parameter-free module that first aligns frame-level features to restore semantic coherence in videos and then enhances the features of the target person according to temporal correlation. Additionally, unlike previous methods that treat each frame equally with a pooling layer, LTA introduces a lightweight learnable module to weigh and aggregate frame-level features under the guidance of a classification score. Extensive experiments on four prevalent benchmarks demonstrate that our method achieves state-of-the-art performance in video Re-ID.",
  "full_text": "Temporal Correlation Vision Transformer for Video Person Re-Identification\nPengfei Wu1, Le Wang1*, Sanping Zhou1, Gang Hua3, Changyin Sun2\n1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,\nNational Engineering Research Center for Visual Information and Applications,\nand Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University\n2School of Artificial Intelligence, Anhui University\n3Wormpex AI Research\nxjtustuwpf@stu.xjtu.edu.cn, {lewang,spzhou}@xjtu.edu.cn, ganghua@gmail.com, cysun@ahu.edu.cn\nAbstract\nVideo Person Re-Identification (Re-ID) is a task of retriev-\ning persons from multi-camera surveillance systems. Despite\nthe progress made in leveraging spatio-temporal informa-\ntion in videos, occlusion in dense crowds still hinders fur-\nther progress. To address this issue, we propose a Tempo-\nral Correlation Vision Transformer (TCViT) for video person\nRe-ID. TCViT consists of a Temporal Correlation Attention\n(TCA) module and a Learnable Temporal Aggregation (LTA)\nmodule. The TCA module is designed to reduce the impact\nof non-target persons by relative state, while the LTA mod-\nule is used to aggregate frame-level features based on their\ncompleteness. Specifically, TCA is a parameter-free module\nthat first aligns frame-level features to restore semantic co-\nherence in videos and then enhances the features of the tar-\nget person according to temporal correlation. Additionally,\nunlike previous methods that treat each frame equally with a\npooling layer, LTA introduces a lightweight learnable module\nto weigh and aggregate frame-level features under the guid-\nance of a classification score. Extensive experiments on four\nprevalent benchmarks demonstrate that our method achieves\nstate-of-the-art performance in video Re-ID.\nIntroduction\nVideo Person Re-Identification (Re-ID) is a key compo-\nnent of surveillance systems (Zheng et al. 2016; Chen et al.\n2018a; Wang et al. 2014). It is different from image-based\nRe-ID as it retrieves persons from video sequences, thus pro-\nviding extra temporal clues. With the rise of deep learning\ntechniques, there has been considerable progress in video\nperson Re-ID. However, it is still challenging due to the\nocclusion caused by dense crowds. Therefore, the focus of\nvideo person Re-ID research is on how to exploit temporal\ninformation without the interference of occlusion.\nThe existing methods (Liu et al. 2021a; Yang et al. 2020;\nYan et al. 2020; He et al. 2021b; Gu et al. 2020) can\nbe divided into two categories. The first is the one-stage\nmethod (Liu et al. 2021a; Yang et al. 2020; Yan et al. 2020;\nHe et al. 2021b; Gu et al. 2020), which utilizes 3D convolu-\ntion or graph neural networks to learn spatial-temporal infor-\nmation from videos. As mentioned in (Wu et al. 2022), 3D\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a)\n (b)\n(c)\n Relati\nvely static\nRelatively dynamic\nTa\nrget\n Person\nNon-Target \nPerson\nFigure 1: Examples of occlusion cases in video person Re-\nID. (a) The target person is the occluded one, while the non-\ntarget person approaches from another direction. (b) The tar-\nget person is the occluder, while the non-target person passes\nthe target at a faster pace. (c) The target person is in the cen-\nter of the frames and relatively static to the bounding box.\nconvolution-based methods are often affected by misalign-\nment of adjacent frames and the occlusion problem. Fur-\nthermore, graph neural networks (Liu et al. 2021a) usually\nrequire an additional pose estimation network to model the\nbody relationships of the target person across frames. The\nsecond type is the two-stage method (Wu et al. 2022; Hou\net al. 2019), which first extracts frame-level features sepa-\nrately and then performs temporal feature aggregation. This\nkind of method uses attention mechanisms or generative ad-\nversarial networks (GANs) (Hou et al. 2019) to reduce the\nimpact of occlusion within frames, but usually treats video\nframes equally during the aggregation stage.\nAs seen in Figure 1 (c), each frame in videos is cropped\naccording to the bounding box generated by the video per-\nson detector (Girshick 2015; Xu, Hrustic, and Vivet 2020),\nwhich ensures that the target person is always in the center of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6083\neach frame and relatively static to the bounding box. On the\nother hand, when the non-target person approaches from an-\nother direction or passes the target person at a faster pace, the\nnon-target person is relatively dynamic to the bounding box.\nAs illustrated in the first row of Figure 1 (a) and (b), it is dif-\nficult to determine whether the target person is an occluder\nor an occluded one based on a single frame. However, by re-\nferring to other frames and considering their relative state, it\ncan be inferred that the target person is the occluded one in\n(a) and the occluder in (b). Thus, we can use the relative state\nto mine information about the target person and reduce the\nimpact of most occlusion cases (e.g., the non-target person\nis relatively dynamic to the bounding box).\nMotivated by the above observations, we propose a Tem-\nporal Correlation Vision Transformer (TCViT) to tackle the\nocclusion problem. This two-stage approach first extracts\nframe-level features using Vision Transformer (ViT) and\nthen enhances the features of the target person based on their\nrelative state. Thereafter, frame-level features are aggregated\nas the final video representation based on their complete-\nness. To do this, we introduce a parameter-free Temporal\nCorrelation Attention (TCA) module, which aligns the video\nframes using a kernel correlation filtering algorithm and then\nboosts the target person’s portion in the frame-level fea-\ntures according to temporal correlation. Additionally, unlike\nother two-stage approaches that treat video frames equally,\nwe employ a lightweight Learnable Temporal Aggregation\n(LTA) module to weigh and aggregate frame-level features\nbased on classification scores.\nWe conduct extensive experiments on four prevalent\ndatasets to evaluate our method. The results demonstrate that\nour method achieves competitive performance with state-\nof-the-art methods, validating its effectiveness. In summary,\nour contribution is threefold.\n• We propose a Temporal Correlation Vision Trans-\nformer (TCViT) for video person Re-ID, which exploits\nrelative state to learn robust features from the target per-\nson and aggregate them based on completeness.\n• We design a parameter-free Temporal Correlation At-\ntention (TCA) module to solve the occlusion problem,\nwhich first aligns the frame-level features by the correla-\ntion filter and then re-weights them according to temporal\ncorrelation.\n• We design a lightweight Learnable Temporal Aggrega-\ntion (LTA) module to replace the equal treatment strat-\negy, which weighs and aggregates frame-level features\nunder the guidance of classification scores.\nRelated Work\nVideo Person Re-ID.Along with the achievement in image-\nbased Re-ID (Chen et al. 2018b; Sun et al. 2018; Zheng et al.\n2019; Zhang et al. 2019; Kalayeh et al. 2018), much progress\nhas been made in video-based Re-ID. Existing video Re-ID\nmethods (Bai et al. 2022; Zhou et al. 2017; Aich et al. 2021;\nMcLaughlin, del Rincon, and Miller 2016; Yang et al. 2020;\nSong et al. 2018) mainly focus on exploiting spatio-temporal\nclues in videos. Widely used techniques, such as optical\nflow (McLaughlin, del Rincon, and Miller 2016; Chung,\nTahboub, and Delp 2017; Chen et al. 2020), recurrent neural\nnetworks (Zhou et al. 2017; McLaughlin, del Rincon, and\nMiller 2016), graph convolution (Yang et al. 2020; Yan et al.\n2020), and 3D convolution (Gu et al. 2020; Li, Zhang, and\nHuang 2019), are employed to model spatio-temporal rela-\ntions. However, occlusion and misalignment problems often\ncorrupt the learned features.\nRecently, some methods (Gu et al. 2020; Hou et al. 2019,\n2020) have been proposed to tackle misalignment and occlu-\nsion issues. Gu et al.(Gu et al. 2020) reconstruct the feature\nmaps of its adjacent frames to the central frame to ensure\nfeature alignment. Hou et al. (Hou et al. 2019) use infor-\nmation from whole frames to restore occluded body parts\nin occluded frames. However, these approaches can address\nonly one of the misalignment and occlusion problems. An-\nother line of work (He et al. 2021b; Liu et al. 2021a; Yan\net al. 2020) exploits the correlation between frames to ad-\ndress occlusion and misalignment implicitly. For example,\nLiu et al.(Liu et al. 2021a) employ a keypoint estimator to\nextract local features from body parts and interact with cor-\nresponding ones across frames. He et al. (He et al. 2021b)\ndivide the feature map extracted by CNN into several hori-\nzontal parts and then pay dense attention to multi-scale and\nmulti-granularity local features under the guidance of global\nfeatures. However, the above methods ignore the fixed body\nstructure of humans and the different relative states between\nthe target person and the occlusion. In contrast, we jointly\naddress the problems of occlusion and misalignment and dif-\nferentiate the target person from occlusion by their different\nrelative states.\nCorrelation Filtering. Correlation filters are widely ex-\nplored in object tracking (Henriques et al. 2014, 2012;\nBolme et al. 2010; Dai et al. 2019; Ma et al. 2015). Bolme\net al. (Bolme et al. 2010) learn a minimum output sum of\nthe squared error filter and update it with average mov-\ning. Henriques et al. (Henriques et al. 2012) augment the\ntraining samples by cyclic shift and speed up the algorithm\nwith the kernel method. Based on it, KCF (Henriques et al.\n2014) exploits the histogram of oriented gradients (HOG)\nfeature (Dalal and Triggs 2005) to improve the accuracy\nof the tracker. Zhang et al. (Zhang et al. 2014) model the\nscale change and learn the filters with context information.\nDanelljan et al.(Danelljan et al. 2014) learn an adaptive cor-\nrelation filter and adopt the color attributes of the target ob-\nject to object tracking. As for the proposed method, we di-\nrectly perform correlation filtering on RGB images and use\nthe correlation filter to calculate the deviation of the target\nperson across frames so as to tackle the problem of mis-\nalignment efficiently.\nTemporal Correlation Vision Transformer\nThe framework of our proposed TCViT, as shown in Fig-\nure 2, consists of TCA and LTA modules. The encoder di-\nvides each frame into patches and generates frame-level fea-\ntures. Meanwhile, the TCA module exploits the correlation\nfiltering algorithm to align the patches and then enhances the\npatches of the target person based on temporal correlation.\nSubsequently, under the guidance of classification scores,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6084\nmiL xentL triL\n61.204\n3\nTemporal C\norrelation Attention ModuleVideo clip \nL l\nLearnable Te\nmporal Aggregation Module\nl\nm s\neL\nmiL xen\ntL tr\niL\n68.988\n7\nTempo\nral Correlation Attention ModuleVideo clip Learnable Temporal Aggregation Module\nmseL\nL ll\nResidual\n connection\nAverage \npooli\nng\nCosi\nne \nsimilarity\nTempo\nral correlations ls\nSigmoi\nd\nMaximum \nresp\nonse\ntI Y\n Features l\ntZ\nDevia\ntion\nAlig\nned features\nAlig\nned 1tI \nl\ntZPatc\nh rolling\n1tI  1tM \nFigure 2: Framework of the proposed Temporal Correlation Vision Transformer (TCViT) for video person Re-ID.\nthe lightweight LTA module is introduced to weight and ag-\ngregate frame-level features.\nVision Transformer Layers\nInspired by the success of Vision Transformer (ViT) in im-\nage person Re-ID (He et al. 2021a), we use ViT to represent\nframes as features. Given a video sequence V = {It}T\nt=1\ncontaining T frames, It ∈ RH×W×C is the t-th frame,\nwhere H, W, and C denote its height, width, and chan-\nnels, respectively. The ViT encoder first splits each frame\ninto N fixed-size patches It = {p1\nt , p2\nt , . . . ,pN\nt }. Then, an\nextra learnable class token is prepended to patches and posi-\ntion embedding P ∈R(N+1)×D and camera index embed-\nding C ∈R(N+1)×D (He et al. 2021a) are applied. The input\nof the encoder is thus described as follows:\nXt =\n\u0002\npcls\nt ; f\n\u0000\np1\nt\n\u0001\n; f\n\u0000\np2\nt\n\u0001\n; . . .; f\n\u0000\npN\nt\n\u0001\u0003\n+ P + C, (1)\nwhere f(·) is a linear mapping function that maps each patch\nto a D-dimensional feature vector, and pcls\nt ∈ RD is the\nclass token. The ViT encoder composed of L layers extracts\nfeatures layer by layer. We denote the frame-level feature\nprocess by layer l as Zl\nt = {zcls\nt ; z1\nt ; z2\nt ; . . .; zN\nt }, where\nzcls\nt and zn\nt are D-dimensional feature vectors of the class\ntoken and patches, respectively.\nTemporal Correlation Attention Module\nAs mentioned in the Introduction, the target person is rela-\ntively static to the bounding box, providing a clue to enhance\nthe feature of the target person. Drawing inspiration from the\nprevious method (Hou et al. 2019), we apply temporal aver-\nage pooling on frame-level features to focus on the relatively\nstatic part of the video. We then calculate the cosine similar-\nity between the frame-level features and the pooled one to\nmeasure their temporal correlation separately. Patches with\nlow temporal correlation correspond to relatively dynamic\nparts (i.e., the non-target person), while those with high tem-\nporal correlation correspond to relatively static parts (i.e.,the\ntarget person).\nHowever, the inadequate detector leads to misalignment\nbetween adjacent frames, resulting in an issue during tem-\nporal average pooling where the features of the target per-\nson can be confused by the misaligned background. To align\nthe frame-level features, we use the kernelized correlation\nMaximum \nresp\nonse\ntI Y\n Features l\ntZ\n Aligned fe\natures ZPatc\nh rolling\n1tI  1tM  Deviatio\nn\nFigure 3: Diagram of Filter-based Patch Alignment\nfilter (KCF) (Henriques et al. 2014) to recover semantic co-\nherence, which first calculates the cross-frame position de-\nviation and then rolls patches in the frame-level features for\nalignment.\nFilter-based Patch Alignment. Following KCF (Henriques\net al. 2014), we transform the deviation calculation into a\nridge regression problem with the kernel method. The goal\nis to obtain a filter with the highest response to a certain part\nof the target person. Therefore, the deviation across frames\ncan be obtained by comparing the maximum response points\nof the filter on adjacent frames. For this reason, the first step\nis to calculate a filter with the highest response to the center\nof the frame It.\nSuppose that Y ∈ RH×W is the response map of the filter\non frame It, as shown in the first row of Figure 3. The cen-\nter of the map is the highest response, marked as one, and\nthe response gradually decreases to zero from the center to\nthe corners. Following KCF, we use It to initialize the fil-\nter and calculate the self-correlation Kt,t ∈ RH×W with a\nGaussian kernel as:\nKt,t = exp\n \n− 2\nσ2\n\u0012\n∥It∥2 − F−1\n\u0010P\nC( ˆIt\n∗\n⊙ ˆIt)\n\u0011\u0013!\n, (2)\nwhere ⊙ is the\nelement-wise product, F−1(·) denotes the in-\nverse discrete Fourier transform, and P\nC(·) means summa-\ntion along the channel dimension. Moreover, σ is the band-\nwidth parameter, ˆIt means the discrete Fourier transforma-\ntion of It, and ˆIt\n∗\nis the complex conjugate of ˆIt.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6085\nAlgorithm 1: Deviation calculation procedure\nInput: Video V = {It}T\nt=1, response map Y\nOutput: Deviations (ex, ey)\n1: Initial filter α with I1 by Eq. (3);\n2: for t = 2, t≤ T do\n3: Compute the response Mt+1 of filter α on It+1 by\nEq. (5)\n4: Obtain the deviations (ex\nt+1, ey\nt+1) by Eq. (6)\n5: Based on deviations, roll the patches of featureZl\nt for\nalignment.\n6: end for\n7: return Aligned features ¯ZlBased on Kt,t and the regression target Y , the filter α\ncan be computed as follows:\nα = F(Y )\nF(Kt,t) + λ, (3)\nwhere F(·) represents the discrete Fourier transformation\nand λ is a regularization parameter. The filterα ∈ RH×W is\nmost responsive to the center of It. To obtain the deviation\nacross frames, we compute the response map of the filter on\nthe next frame It+1. We first compute the cross-correlation\nKt,t+1 between It and It+1 as follows:\nKt,t+1 = exp\n \n− 1\nσ2\n\u0012\n∥It∥2 + ∥It+1∥2\n−2F−1\n\u0010X\nC\n( ˆIt\n∗\n⊙ ˆIt+1)\n\u0011\u0013!\n.\n(4)\nThen, the response Mt+1 ∈ RH×W of the filter α on\nframe It+1 can be calculated as follows:\nMt+1 = F−1 \u0000\nF(Kt,t+1) ⊙ α\n\u0001\n. (5)\nThe location of the maximum response point of Mt+1,\ndenoted as (Mx\nt+1, My\nt+1), corresponds to the center of\nframe It. Accordingly, the deviation can be calculated as:\n(ex\nt+1, ey\nt+1) = (Mx\nt+1 − W/2, My\nt+1 − H/2), (6)\nwhere ex\nt+1 and ey\nt+1 are horizontal and vertical deviations,\nrespectively. Based on it, TCA rolls patches (except for class\ntoken) in feature maps for alignment, as shown in the second\nrow of Figure 3. This operation ensures that frame-level fea-\nture maps have the same semantics along the same spatial\nregion, thus restoring semantic coherence. The entire pro-\ncess of the deviation calculation is outlined in Algorithm 1\nto facilitate comprehension.\nCorrelation Attention. As illustrated in Figure 4, the fea-\ntures aligned after layer l are represented as ¯Zl\nt, and tempo-\nral average pooling is applied to them to obtainV ∈ RN×D:\nV = 1\nT\nTX\nt=1\n¯Zl\nt. (7)\nAs mentioned above, averaging over the temporal dimen-\nsion focuses on the relatively static parts. We can determine\nResidual connect\nion\nAverage \npooli\nng\nCosi\nne \nsimilarity\nTe\nmporal correlations\nSigmoid\nl\ntZ ts\nmiL xentL tr\niL\nTo\nken weights\nFigure 4: Diagram of Correlation Attention\nwhether a patch is important (i.e., belonging to the target\nperson) by comparing it with the pooled ones. To do this,\nwe calculate the cosine similarity between ¯Zl\nt and V as fol-\nlows:\nst =\n¯Zl\nt\n∥ ¯Zl\nt∥ ⊗ V\n∥V ∥, (8)\nwhere ⊗ denotes the inner product. Here, st ∈ RN , which\nlies between 0 and 1, estimates the temporal correlation\nof each patch. Applying Sigmoid(st) to the input features,\nTCA will return frame-level features that focus more on the\ntarget person as follows:\nZl\nt = ¯Zl\nt + ¯Zl\nt ⊙ Sigmoid(st). (9)\nDuring the alignment operation, we initial filter with the\nfirst frame. However, in some cases, the first frame may not\nbe an accurate detection result. Aligning subsequent frames\nto it can cause the body structure of the target person to be\ndistorted (e.g., patches of the legs rolling up to the upper\nbody). Our aim is to make sure that the features have the\nsame semantic in the same spatial region so that patches with\nthe same semantic can be merged together during temporal\naverage pooling. Therefore, the disruption of body structure\ndoes not reduce performance, and the original patch arrange-\nment will be restored for the subsequent layer after the TCA\nprocess.\nLearnable Temporal Aggregation Module\nViT, when used in conjunction with the TCA module, en-\ncodes each frame as a frame-level feature. In the traditional\napproach, the class tokens zcls\nt are separated from ZL\nt and\nthen averaged over time to generate the final video represen-\ntation. However, in crowded scenes, some frames may be\noccluded, resulting in only a partial view of the target per-\nson. It is more appropriate to assign a higher weight to com-\nplete frames compared to occluded ones, rather than treating\nall frames equally. To this end, a lightweight LTA module is\nintroduced to weigh and aggregate each frame-level feature.\nThe illustration in Figure 2 provides an overview of our\nLTA. Specifically, the class tokens Zcls ∈ RT×D are sepa-\nrated from the output of the last layer in ViT. Subsequently,\na linear layer and a RELU layer are used to downsample the\nchannel dimension:\nF = GELU(MLP(Zcls)). (10)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6086\nHere, F ∈ RT×D/r compresses the information of class\ntokens by downsampling it at a rate of r. We reshape F into\na vector f ∈ RT·D/r and compute the weight w ∈ RT of\nclass tokens as follows:\nw = MLP(f),\nv = 1\nT\nX\nT\n(Zcls ⊙ Sigmoid(w)),\n(11)\nwhere P\nT (·) means summation along time dimension. Sub-\nsequently, LTA multiplies Zcls with Sigmoid(w) and ag-\ngregates over time to obtain the final video representation\nv ∈ RD.\nIt is insufficient to learn the weights only through cas-\ncaded linear layers. To better guide the training of LTA, we\ncalculate classification scores for Zcls with a classification\nhead and use the cross entropy loss as the classification loss\nfor training. Generally, partially occluded frames have lower\nclassification scores due to incomplete information about the\ntarget person, while complete frames tend to have higher\nclassification scores. Taking this into account, we apply a\nMean Squared Error (MSE) loss to measure the discrepancy\nbetween x and the classification score:\nc = MLP(Zcls),\nLmse(c, w) =\nX\nT\n∥y(c) − w∥2, (12)\nwhere c ∈ RT×M is the classification score, M is the num-\nber of classes. The function y(c) ∈ RT selects a score cor-\nresponding to the ground truth from each row of c.\nLast, with the help of the classification scores, LTA can\nefficiently aggregate frame-level features to create an infor-\nmative representation of the video.\nObjective Function\nWe adopt the triplet loss (Hermans, Beyer, and Leibe 2017)\nLtri and mutual information loss (Hjelm et al. 2019) Lmi\nto guide the training of frame-level feature maps, similar\nto (Bai et al. 2022; Yan et al. 2020). Additionally, we ap-\nply the MSE loss Lmse in Eq. (12) and cross entropy loss\nLxent to improve the learned weights:\nLf = Ltri(Zcls) +Lmi(Zcls) +Lxent(c) +Lmse(c, w). (13)\nAfter the process of LTA, Ltri and Lxent are adopted to\nguide the training of the video representation v:\nLv = Ltri(v) + Lxent(v). (14)\nThe overall objective function of our TCF is a combina-\ntion of the above two losses:\nL = Lv + ∥Lv∥\n∥Lf ∥Lf . (15)\nExperiments\nDatasets and Settings\nDatasets. We evaluate our method on three datasets, i.e.,\nMARS (Zheng et al. 2016), LS-VID (Li et al. 2019), and\niLiDS-VID (Wang et al. 2014), for video person Re-ID and\nMethods GFs Para. Infer. LS-VID\nTime mAP rank-1\nbase. 11.04 86.3 1.0x 80.5 88.2\n+TCA w/o A. 11.04 86.3 1.03x 81.0 88.7\n+TCA 11.04 86.3 1.40x 81.4 89.2\n+LTA w/o M. 11.05 87.2 1.02x 81.0 88.8\n+LTA 11.05 87.2 1.02x 82.0 89.3\nTCViT 11.05 87.2 1.44x 83.1 90.1\nTable 1: Component analysis of TCViT on LS-VID. A. and\nM. means align operation and MSE loss separately.\none dataset, i.e., VVeRI-901-trial (Zhao et al. 2021) for\nvideo vehicle Re-ID. More details are introduced in supple-\nmentary materials.\nImplementation Details. The ViT-base (l = 12) is adopted\nas the baseline for our TCViT, the same as the CA ViT (Wu\net al. 2022). During training, we randomly select 8 identi-\nties and sample 4 sequences for each identity. We follow the\nrestricted random sampling strategy (Li et al. 2018), which\nevenly divides the video sequence into 8 chunks and ran-\ndomly selects one frame per chunk. The frames are resized\nto 256 × 128 and augmented by random flipping and eras-\ning (Wang et al. 2018; Zhong et al. 2020). We set the re-\nduction rate r = 8 . Additionally, we add two TCA mod-\nules after the 7th and the 9th ViT layers separately. The\nAdam (Kingma and Ba 2014) optimizer with a weight de-\ncay of 0.001 is used as optimizer, and the learning rate is\ninitialized as 0.0005 with a cosine learning rate scheduler.\nThe model is trained for 90 epochs. During evaluation, we\nsplit each video sequence into multiple 8-frame video clips\nand obtain the video-level representation by averaging all\nextracted clip features. The mean Average Precision (mAP)\nand the Cumulative Matching Characteristics (CMC) are\nadopted as evaluation metrics.\nAblation Study\nFor a fair comparison, we build the baseline by degrading\nTCViT without TCA and LTA, which we refer to as “base.”\nfor simplicity. In addition, we compare the influence of TCA\nand LTA on inference time in Table 1.\nComponent Analysis of TCA. We evaluate the contribu-\ntions of TCA on LS-VID in Table 1. TCA is parameter-\nfree and does not add any computational costs, resulting in\na 0.9% / 1.0% increase in rank-1/ mAP over the baseline. In\nparticular, filter-based patch alignment is essential for TCA,\nand its absence leads to a decrease in performance.\nWe conduct experiments to investigate the organizational\nrationality of TCA by inserting two TCA modules into dif-\nferent positions. “Z3+Z5” means respectively inserting two\nTCA modules after the 3rd and 5th ViT layers. The combina-\ntion of “Z7 + Z9” achieves the best performance, as shown\nin Table 2 (a). In shallow layers, patches learn features from\nsimilar patches surrounding them. TCA weakens non-target\npersons’ and background patches, which hinders the relation\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6087\nMethods mAP rank-1\nTCViT 83.1 90.1\nZ3 + Z5 76.6 84.5\nZ5 + Z7 81.7 88.6\nZ7 + Z9 83.1 90.1\nZ9 + Z11 82.7 89.6\n(a) The results of TCViT with two TCA\nmodules at different positions.\nMethods mAP rank-1\nTCViT 83.1 90.1\nZ7 + Z9 + Z11 83.0 89.9\nZ7 + Z9 83.1 90.1\nZ7 + Z8 82.8 89.8\nZ7 82.6 89.3\n(b) The results of TCViT with a different num-\nber of TCA modules.\nMethods Para. mAP rank-1\nTCViT 87.02 83.1 90.1\nr = 2 87.24 82.1 89.1\nr = 4 87.09 82.6 89.6\nr = 8 87.02 83.1 90.1\nr = 16 86.98 82.9 89.8\n(c) The results of TCViT with different reduc-\ntion rate r in the LTA module.\nTable 2: Ablation study on LS-VID. More details are explained in the text.\nMethods Proc. LS-VID MARS iLiDS-VID\nmAP rank-1 mAP rank-1 rank-1 rank-5\nGLTR(Li et al. 2019) ICCV2019 44.3 63.1 78.5 87.0 86.0 98\nVRSTC(Hou et al. 2019) CVPR2019 82.3 88.5 83.4 95.5\nM3D(Li, Zhang, and Huang 2019) AAAI2019 40.1 57.7 74.1 84.4 74.0 94.3\nAP3D(Gu et al. 2020) ECCV2020 73.2 84.5 85.1 90.1 88.7\nTCLNet(Hou et al. 2020) ECCV2020 70.3 81.5 85.1 89.8 86.6\nAFA(Chen et al. 2020) ECCV2020 82.9 90.2 88.5 96.8\nMGH(Yan et al. 2020) CVPR2020 85.8 90.0 85.6 97.1\nMG-RAFA(Zhang et al. 2020) CVPR2020 85.9 88.8 88.6 98\nBiCnet-TKS(Hou et al. 2021) CVPR2021 75.1 84.6 86.0 90.2\nGRL(Liu et al. 2021b) CVPR2021 84.8 91.0 96.7 98.3\nSTRF(Aich et al. 2021) ICCV2021 86.1 90.3 89.3\nSTMN(Eom et al. 2021) ICCV2021 69.2 82.1 84.5 90.5\nDenseIL(He et al. 2021b) ICCV2021 87.0 90.8 92.0 98.0\nSINet(Bai et al. 2022) CVPR2022 79.6 87.4 86.2 91.0 92.5\nCA ViT(Wu et al. 2022) ECCV2022 79.2 89.2 87.2 90.8 93.3 98.0\nTCViT 83.1 90.1 87.6 91.7 94.3 99.3\nTable 3: Performance comparison with state-of-the-art methods on LS-VID, MARS, and iLiDS-VID.\nMethods Sequence\nLength Proc. VVeRI-901\nmAP rank-1\nbase. 8 ICLR2021 63.5 56.7\nBiCnet-TKS 8 CVPR 2021 50.8 41.3\nAP3D 4 ECCV 2020 61.2 52.5\nToken shift 8 ICCV 2019 67.4 57.5\nCA ViT 8 ECCV 2022 65.6 60.0\nTCViT 4 65.0 58.7\n8 69.0 63.9\nTable 4: Comparison with state-of-the-arts on VVeRI-901.\nconstruction between patches in shallow layers. However, in\ndeep layers, only a few patches with important semantic in-\nformation are taken into account. Therefore, TCA improves\nperformance by weighting the deep frame-level features to\nfocus on the target person, which is in line with the findings\nof Chang et al. (Chang et al. 2023).\nWe also evaluate the influence of the number of TCA\nmodules, as illustrated in Table 2 (b). It can be seen that the\nperformance increases and reaches its peak with two sep-\narate TCA modules after the 7th and 9th layers. Stacking\nthree TCA modules does not bring any performance gain,\nand “Z7 + Z9” provides the best balance between perfor-\nmance and complexity.\nComponent Analysis of LTA.We evaluate the effectiveness\nof LTA, and the results are presented in Table 1. Replac-\ning the temporal average pooling with LTA has increased\nthe rank-1/mAP by 0.5%/0.6% over the baseline. Addition-\nally, when MSE loss is embedded to guide LTA training,\nperformance is further improved by 1.0%/0.5% rank-1/mAP\non LS-VID. LTA is a lightweight module that only intro-\nduces 1% extra parameters and negligible GFLOPs, yet has\nachieved a total improvement of 1.5%/1.1% rank-1/mAP.\nAs shown in Eq. (10), LTA compresses the class tokens at\na rate of r, and Table 2 (c) presents the results on LS-VID at\ndifferent reduction ratesr, i.e., r = 2/4/8/16. As the reduc-\ntion rate increases, the performance improves until it reaches\nits peak at r = 8. After that, further increasing the rate does\nnot bring any benefit, probably due to the information loss\ncaused by the excessive reduction rate.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6088\nInputBaseTCV\niT\nMARS iLi\nDS-VID\nMARS\nInput\niLi\nDS-VID\nACM\n(a) Visualization of Filter-based Patch Alignment. The first row is\nan example of misaligned videos, while the second is the video se-\nquence after alignment.\nInputBaseTCV\niT\nMARS iLi\nDS-VID\nMARS\nInput\niLi\nDS-VID\nACM\n(b) Visualization of Correlation Attention. The first row is two exam-\nples of videos under occlusion, while the second is the corresponding\ntemporal correlation st.\nFigure 5: Visualization of Filter-based Alignment and Correlation Attention in TCA.\nInp\nutBaseTCViT\nMARS iLiDS-VID\nMARS\nInp\nut\niLiDS-VID\nACM\nInp\nutFPR\nFigure 6: Visualization of the difference between the base-\nline and our TCViT framework.\nComparison with State-of-the-art Methods\nIn Table 3, we compare TCViT with existing state-of-the-art\nmethods on three prevalent video person Re-ID benchmarks.\n(1) LS-VID is one of the most challenging datasets. Our ap-\nproach achieves 83.1% in mAP, surpassing the previous best\nmethod CA ViT (Wu et al. 2022) by a considerable margin.\n(2) On MARS, we obtain a rank-1/mAP of 91.7%/87.6%,\noutperforming all other state-of-the-art methods. Most of the\nmethods are CNN-based, except CA ViT (Wu et al. 2022) and\nDenseIL (He et al. 2021b). Our method increases rank-1 by\n0.9% over the above two methods. As Wu et al. (Wu et al.\n2022) pointed out, the ID switch caused by the GMMCP\ntractor (Dehghan, Modiri Assari, and Shah 2015) and the\nmisalignment caused by the DPM detector (Felzenszwalb\net al. 2009) led to confusion in the video Re-ID task, re-\nsulting in a performance bottleneck on MARS. Neverthe-\nless, TCViT achieves the best performance among all other\nmethods. (3) iLiDS-VID is a small benchmark with only\n500 identities. TCViT still performs well on the small-scale\ndataset, showing robustness and effectiveness.\nTo test the generalizability of our TCViT, we conduct ex-\nperiments on VVeRI-901-trial (Zhao et al. 2021). As Ta-\nble 4 shows, TCViT outperforms the previous best meth-\nods AP3D and CA ViT by 6.2%/3.8% rank-1/mAP and\n3.9%/3.4% rank-1/mAP, respectively, when using the same\nsequence length. Additionally, the combination of TCA and\nLTA results in a 7.3%/5.5% rank-1/mAP improvement over\nthe baseline.\nVisualization Analysis\nVisualization of Filter-based Patch Alignment.We visual-\nize the results of the alignment operation. As shown in Fig-\nure 5 (a), the misaligned frames in the input sequence are\nhighlighted with red boxes. After the alignment operation,\nthe misaligned frames align with each other, as indicated by\nthe green boxes. This demonstrates that our TCA model ac-\ncurately calculates the deviation of the target person between\nframes and restores temporal coherence.\nVisualization of Temporal Correlations. In Figure 5 (b),\nwe visualize the temporal correlations of frames with occlu-\nsion. We sample two identities from the MARS dataset. The\ntop row shows the raw frames with the non-target person\nocclusion, while the bottom row shows the corresponding\ntemporal correlation st in Eq. (8). It is clear that when the\nnon-target persons are relatively dynamic to the bounding\nbox, patches belonging to them are assigned lower weights.\nIn contrast, the attention map is activated when the relatively\nstatic part (e.g., target person) appears. This demonstrates\nthat correlation attention is prone to focus on the relatively\nstatic part. When the non-target person comes from another\ndirection or passes the target person, TCA can weaken the\noccluded patches and enhance the valuable ones.\nVisualization of Activation Maps. We further evaluate the\nperformance of our method by comparing the activation map\nwith the baseline on MARS and iLiDS-VID. As marked by\nred boxes in the second row of Figure 6, the baseline model\ncannot distinguish the target person from the non-target per-\nson, while our TCViT framework can focus on the target\nperson even under severe occlusion.\nConclusion\nThis paper aims to develop a better representation of the\ntarget person, particularly when they are partially occluded\nfrom view. Our TCViT utilizes the relative state to differen-\ntiate the target person from occlusion and aggregates frame-\nlevel features based on completeness. With two lightweight\nTCA and LTA modules, our TCViT achieves competitive\nperformance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6089\nAcknowledgements\nThis work was supported partly by the National Key R&D\nProgram of China under Grant 2021YFB1714700, NSFC\nunder Grants 62088102 and 62106192, Natural Science\nFoundation of Shaanxi Province under Grant 2022JC41, and\nFundamental Research Funds for the Central Universities\nunder Grant XTR042021005.\nReferences\nAich, A.; Zheng, M.; Karanam, S.; Chen, T.; Roy-\nChowdhury, A. K.; and Wu, Z. 2021. Spatio-temporal\nrepresentation factorization for video-based person re-\nidentification. In ICCV, 152–162.\nBai, S.; Ma, B.; Chang, H.; Huang, R.; and Chen, X.\n2022. Salient-to-Broad Transition for Video Person Re-\nIdentification. In CVPR, 7339–7348.\nBolme, D. S.; Beveridge, J. R.; Draper, B. A.; and Lui, Y . M.\n2010. Visual object tracking using adaptive correlation fil-\nters. In CVPR, 2544–2550.\nChang, S.; Wang, P.; Lin, M.; Wang, F.; Zhang, D. J.; Jin,\nR.; and Shou, M. Z. 2023. Making Vision Transformers Ef-\nficient from A Token Sparsification View. In CVPR, 6195–\n6205.\nChen, D.; Li, H.; Xiao, T.; Yi, S.; and Wang, X. 2018a. Video\nperson re-identification with competitive snippet-similarity\naggregation and co-attentive snippet embedding. In CVPR,\n1169–1178.\nChen, D.; Xu, D.; Li, H.; Sebe, N.; and Wang, X. 2018b.\nGroup consistent similarity learning via deep crf for person\nre-identification. In CVPR, 8649–8658.\nChen, G.; Rao, Y .; Lu, J.; and Zhou, J. 2020. Temporal\ncoherence or temporal motion: Which is more critical for\nvideo-based person re-identification? In ECCV, 660–676.\nChung, D.; Tahboub, K.; and Delp, E. J. 2017. A Two\nStream Siamese Convolutional Neural Network for Person\nRe-identification. In ICCV, 1983–1991.\nDai, K.; Wang, D.; Lu, H.; Sun, C.; and Li, J. 2019. Visual\ntracking via adaptive spatially-regularized correlation filters.\nIn CVPR, 4670–4679.\nDalal, N.; and Triggs, B. 2005. Histograms of oriented gra-\ndients for human detection. In CVPR, 886–893.\nDanelljan, M.; Shahbaz Khan, F.; Felsberg, M.; and Van de\nWeijer, J. 2014. Adaptive color attributes for real-time visual\ntracking. In CVPR, 1090–1097.\nDehghan, A.; Modiri Assari, S.; and Shah, M. 2015.\nGmmcp tracker: Globally optimal generalized maximum\nmulti clique problem for multiple object tracking. In CVPR,\n4091–4099.\nEom, C.; Lee, G.; Lee, J.; and Ham, B. 2021. Video-based\nperson re-identification with spatial and temporal memory\nnetworks. In ICCV, 12036–12045.\nFelzenszwalb, P. F.; Girshick, R. B.; McAllester, D.; and\nRamanan, D. 2009. Object detection with discriminatively\ntrained part-based models. IEEE T-PAMI, 32(9): 1627–\n1645.\nGirshick, R. 2015. Fast R-CNN. In CVPR, 1440–1448.\nGu, X.; Chang, H.; Ma, B.; Zhang, H.; and Chen, X. 2020.\nAppearance-preserving 3D convolution for video-based per-\nson re-identification. In ECCV, 228–243.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang,\nW. 2021a. TransReID: Transformer-based object re-\nidentification. In ICCV, 15013–15022.\nHe, T.; Jin, X.; Shen, X.; Huang, J.; Chen, Z.; and Hua, X.-S.\n2021b. Dense Interaction Learning for Video-based Person\nRe-identification. In ICCV, 1490–1501.\nHenriques, J. F.; Caseiro, R.; Martins, P.; and Batista, J.\n2012. Exploiting the circulant structure of tracking-by-\ndetection with kernels. In ECCV, 702–715.\nHenriques, J. F.; Caseiro, R.; Martins, P.; and Batista, J.\n2014. High-speed tracking with kernelized correlation fil-\nters. IEEE T-PAMI, 37(3): 583–596.\nHermans, A.; Beyer, L.; and Leibe, B. 2017. In defense of\nthe triplet loss for person re-identification. arXiv preprint\narXiv:1703.07737.\nHjelm, R. D.; Fedorov, A.; Lavoie-Marchildon, S.; Grewal,\nK.; Bachman, P.; Trischler, A.; and Bengio, Y . 2019. Learn-\ning deep representations by mutual information estimation\nand maximization. In ICLR.\nHou, R.; Chang, H.; Ma, B.; Huang, R.; and Shan, S. 2021.\nBiCnet-TKS: Learning efficient spatial-temporal represen-\ntation for video person re-identification. In CVPR, 2014–\n2023.\nHou, R.; Chang, H.; Ma, B.; Shan, S.; and Chen, X.\n2020. Temporal complementary learning for video person\nre-identification. In ECCV, 388–405.\nHou, R.; Ma, B.; Chang, H.; Gu, X.; Shan, S.; and\nChen, X. 2019. VRSTC: Occlusion-free video person re-\nidentification. In CVPR, 7183–7192.\nKalayeh, M. M.; Basaran, E.; G¨okmen, M.; Kamasak, M. E.;\nand Shah, M. 2018. Human semantic parsing for person re-\nidentification. In CVPR, 1062–1071.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. In ICLR.\nLi, J.; Wang, J.; Tian, Q.; Gao, W.; and Zhang, S. 2019.\nGlobal-local temporal representations for video person re-\nidentification. In ICCV, 3958–3967.\nLi, J.; Zhang, S.; and Huang, T. 2019. Multi-scale 3D con-\nvolution network for video based person re-identification. In\nAAAI, 8618–8625.\nLi, S.; Bak, S.; Carr, P. W.; and Wang, X. 2018. Diversity\nRegularized Spatiotemporal Attention for Video-based Per-\nson Re-identification. In CVPR, 369–378.\nLiu, J.; Zha, Z.-J.; Wu, W.; Zheng, K.; and Sun, Q. 2021a.\nSpatial-Temporal Correlation and Topology Learning for\nPerson Re-Identification in Videos. In CVPR, 4370–4379.\nLiu, X.; Zhang, P.; Yu, C.; Lu, H.; and Yang, X. 2021b.\nWatching you: Global-guided reciprocal learning for video-\nbased person re-identification. In CVPR, 13334–13343.\nMa, C.; Huang, J.-B.; Yang, X.; and Yang, M.-H. 2015. Hier-\narchical convolutional features for visual tracking. InCVPR,\n3074–3082.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6090\nMcLaughlin, N.; del Rincon, J. M.; and Miller, P. 2016. Re-\ncurrent Convolutional Network for Video-Based Person Re-\nidentification. In CVPR, 1325–1334.\nSong, C.; Huang, Y .; Ouyang, W.; and Wang, L. 2018.\nMask-guided contrastive attention model for person re-\nidentification. In CVPR, 1179–1188.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018.\nBeyond part models: Person retrieval with refined part pool-\ning (and a strong convolutional baseline). In ECCV, 480–\n496.\nWang, T.; Gong, S.; Zhu, X.; and Wang, S. 2014. Person\nre-identification by video ranking. In ECCV, 688–703.\nWang, Y .; Wang, L.; You, Y .; Zou, X.; Chen, V .; Li, S.;\nHuang, G.; Hariharan, B.; and Weinberger, K. Q. 2018. Re-\nsource aware person re-identification across multiple resolu-\ntions. In CVPR, 8042–8051.\nWu, J.; He, L.; Liu, W.; Yang, Y .; Lei, Z.; Mei, T.; and Li,\nS. Z. 2022. CA ViT: Contextual Alignment Vision Trans-\nformer for Video Object Re-identification. In ECCV, 549–\n566.\nXu, Z.; Hrustic, E.; and Vivet, D. 2020. Centernet heatmap\npropagation for real-time video object detection. In ECCV,\n220–234.\nYan, Y .; Qin, J.; Chen, J.; Liu, L.; Zhu, F.; Tai, Y .; and Shao,\nL. 2020. Learning Multi-Granular Hypergraphs for Video-\nBased Person Re-Identification. In CVPR, 2899–2908.\nYang, J.; Zheng, W.-S.; Yang, Q.; Chen, Y .-C.; and Tian,\nQ. 2020. Spatial-Temporal Graph Convolutional Network\nfor Video-Based Person Re-Identification. In CVPR, 3289–\n3299.\nZhang, K.; Zhang, L.; Liu, Q.; Zhang, D.; and Yang, M.-H.\n2014. Fast visual tracking via dense spatio-temporal context\nlearning. In ECCV, 127–141.\nZhang, Z.; Lan, C.; Zeng, W.; and Chen, Z. 2019. Densely\nsemantically aligned person re-identification. In CVPR,\n667–676.\nZhang, Z.; Lan, C.; Zeng, W.; and Chen, Z. 2020. Multi-\ngranularity reference-aided attentive feature aggregation for\nvideo-based person re-identification. In CVPR, 10407–\n10416.\nZhao, J.; Qi, F.; Ren, G.; and Xu, L. 2021. PhD Learn-\ning: Learning with Pompeiu-hausdorff Distances for Video-\nbased Vehicle Re-Identification. In CVPR, 2225–2235.\nZheng, L.; Bie, Z.; Sun, Y .; Wang, J.; Su, C.; Wang, S.; and\nTian, Q. 2016. MARS: A video benchmark for large-scale\nperson re-identification. In ECCV, 868–884.\nZheng, M.; Karanam, S.; Wu, Z.; and Radke, R. J. 2019. Re-\nidentification with consistent attentive siamese networks. In\nCVPR, 5735–5744.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. InAAAI, 13001–13008.\nZhou, Z.; Huang, Y .; Wang, W.; Wang, L.; and Tan, T.\n2017. See the forest for the trees: Joint spatial and tem-\nporal recurrent neural networks for video-based person re-\nidentification. In CVPR, 4747–4756.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n6091",
  "topic": "Correlation",
  "concepts": [
    {
      "name": "Correlation",
      "score": 0.5467695593833923
    },
    {
      "name": "Computer vision",
      "score": 0.46495991945266724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45709720253944397
    },
    {
      "name": "Computer science",
      "score": 0.43784064054489136
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33211809396743774
    },
    {
      "name": "Mathematics",
      "score": 0.1781618297100067
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "cited_by": 7
}