{
  "title": "LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework",
  "url": "https://openalex.org/W4287854436",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2320201689",
      "name": "Mengjie Zhao",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2127241586",
      "name": "Fei Mi",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2111728759",
      "name": "Ya-sheng Wang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2138004159",
      "name": "Minglei Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2111997136",
      "name": "Hinrich Schuetze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2785787385",
    "https://openalex.org/W4206648492",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2101210369",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3100124407",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W2061863227",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2108126316",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2971204996",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4230030242",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2171671120",
    "https://openalex.org/W3035434938",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3171875349",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3111114607",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3200936406",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962852048",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3199377785",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3102950138",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3213458975",
    "https://openalex.org/W4206529673",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3049254243",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3173185981",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4236763394",
    "https://openalex.org/W1947674219",
    "https://openalex.org/W2132232498",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4294810635",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2152005244",
    "https://openalex.org/W2250600206",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W1773652845",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W151188873",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W3174429158",
    "https://openalex.org/W3017779903",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4206734067",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2996944542",
    "https://openalex.org/W2756978580",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W3035433211",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2251199578",
    "https://openalex.org/W3197770791",
    "https://openalex.org/W2899689163",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963742748",
    "https://openalex.org/W2951911250",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2906152891"
  ],
  "abstract": "Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shotlearners as crowdsourcing workers. The rationale is that crowdsourcing workers are in fact few-shot learners: They are shown a few illustrative examples to learn about a task and then start annotating. LMTurk employs few-shot learners built upon PLMs as workers. We show that the resulting annotations can be utilized to train models that solve the task well and are small enough to be deployable in practical scenarios. Active learning is integrated into LMTurk to reduce the amount of queries made to PLMs, minimizing the computational cost of running PLM inference passes. Altogether, LMTurk is an important step towards making effective use of current PLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 675 - 692\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLMTurk: Few-Shot Learners as Crowdsourcing Workers in a\nLanguage-Model-as-a-Service Framework\nMengjie Zhao† Fei Mi‡ Yasheng Wang‡ Minglei Li*\nXin Jiang‡ Qun Liu‡ Hinrich Schütze†\n†CIS, LMU Munich ‡Huawei Noah’s Ark Lab *Huawei Technologies Co., Ltd.\nmzhao@cis.lmu.de, {mifei2,wangyasheng,jiang.xin,qun.liu}@huawei.com\nAbstract\nVast efforts have been devoted to creating high-\nperformance few-shot learners, i.e., large-scale\npretrained language models (PLMs) that per-\nform well with little downstream task train-\ning data. Training PLMs has incurred signif-\nicant cost, but utilizing the few-shot learners\nis still challenging due to their enormous size.\nThis work focuses on a crucial question: How\nto make effective use of these few-shot learn-\ners? We propose LMTurk, a novel approach\nthat treats few-shot learners as crowdsourcing\nworkers. The rationale is that crowdsourcing\nworkers are in fact few-shot learners: They\nare shown a few illustrative examples to learn\nabout a task and then start annotating. LMTurk\nemploys few-shot learners built upon PLMs as\nworkers. We show that the resulting annota-\ntions can be utilized to train models that solve\nthe task well and are small enough to be deploy-\nable in practical scenarios. Active learning is\nintegrated into LMTurk to reduce the amount of\nqueries made to PLMs, minimizing the compu-\ntational cost of running PLM inference passes.\nAltogether, LMTurk is an important step to-\nwards making effective use of current PLMs.1\n1 Introduction\nEquipped with proliﬁc linguistic features (Liu et al.,\n2019; Tenney et al., 2019; Belinkov and Glass,\n2019; Rogers et al., 2020) and rich world knowl-\nedge (Petroni et al., 2019; Poerner et al., 2020;\nKassner et al., 2021), large-scale pretrained lan-\nguage models (PLMs) have been shown to be ver-\nsatile: They are now basic building blocks (Bom-\nmasani et al., 2021) of systems solving diverse NLP\ntasks in many languages (Wang et al., 2018, 2019;\nHu et al., 2020; Xu et al., 2020; Khashabi et al.,\n2021; Park et al., 2021; Adelani et al., 2021).\nRecent work shows that PLMs are effective\nfew-shot learners (Brown et al., 2020; Schick and\nSchütze, 2021b; Gao et al., 2021; Tam et al., 2021)\n1Resources are available at: github.com/lmturk\nS\nU\nA\nD\nSmall model S predicts unlabelled data U.\nSelect data D from U with active learning.\nLMTurkers A annotate and aggregate labels of D.\nTraining a new small model S.\nConverting a PLM to LMTurker with few-shot gold data G of task T.\nG\nFigure 1: LMTurk overview; best viewed in color. We\nfew-shot adapt PLMs to task T (left) and then use them\nas crowdsourcing workers in active learning. We show\nthat these PLM workers are effective in training a small\nmodel Sthrough a customized active learning loop\n(right). LMTurk is a novel way to take advantage of\nlarge-scale PLMs: It creates models small enough to be\ndeployed in resource-limited real-world settings.\nthrough priming (Brown et al., 2020; Tsimpoukelli\net al., 2021) or prompting (Li and Liang, 2021; Liu\net al., 2021b; Lester et al., 2021; Zhao and Schütze,\n2021). Developing few-shot learners is crucial be-\ncause current NLP systems require much more data\nthan humans (Yin et al., 2020). Few-shot learners\ntend to perform well; however, they still fall behind\nsystems trained with abundant data. Furthermore,\nthe enormous size of PLMs hinders their deploy-\nment in practice. For example, it is challenging\nto ﬁt the 11 billion T5-XXL (Raffel et al., 2020)\nmodel on a single regular GPU.\nOur goal in this paper is to devise methods that\nmake more effective use of current few-shot learn-\ners. This is crucial because an increasing number\n675\nof gigantic few-shot learners are trained; how to use\nthem effectively is thus an important question. In\nparticular, we want an alternative to hard-to-deploy\nhuge models. At the same time, we want to take\nfull advantage of the PLMs’ strengths: Their versa-\ntility ensures wide applicability across tasks; their\nvast store of knowledge about language and the\nworld (learned in pretraining) manifests in the data\nefﬁciency of few-shot learners, reducing labor and\ntime consumption in data annotation.\nIn this work, we propose LMTurk, Language\nModel as mechanical Turk. Our basic idea (see\nFigure 1) is that, for an NLP task T, we treat few-\nshot learners as non-expert workers , resembling\ncrowdsourcing workers that annotate resources for\nhuman language technology. We are inspired by the\nfact that we can view a crowdsourcing worker as a\ntype of few-shot learner: A few examples demon-\nstrating T teach her enough about T to conduct ef-\nfective annotation. For example, Snow et al. (2008)\ntrain workers with a few examples of annotating\nemotion; He et al. (2015) conduct short training\nsessions for workers before annotation; Lee et al.\n(2021) train workers with learning curricula.\nSnow et al. (2008) pioneered crowdsourcing in\nNLP (Howe et al., 2006; Howe, 2008), motivated\nby the high cost of TreeBank annotation (Marcus\net al., 1993; Miller et al., 1993). Crowdsourcing\norganizes human workers over the Web to annotate\ndata. Workers need not be experts to be effective,\nresulting in reduced per-label cost. Active learning\n(Hachey et al., 2005; Felder and Brent, 2009) can\nbe incorporated (Laws et al., 2011) to further de-\ncrease annotation cost, by lowering the number of\nlabels to be annotated. LMTurk treats PLM-based\nfew-shot learners as non-expert workers that pro-\nduce training sets, which are then used to train a\nsmall machine learning model Sspecialized for\nT. This scenario is analogous to active learning.\nWe achieve two beneﬁts: (i) low annotation cost\nbecause humans only need to annotate a few shots\nof data; (ii) solving practical NLP tasks with small\nmodels that are more real-world deployable.\nLMTurk resonates with Laws et al. (2011)’s ear-\nlier idea of combining crowdsourcing and active\nlearning. They consider human workers as “noisy\nannotators” while we explore the utilization of mod-\nern NLP few-shot learners (built upon machine\nlearning models) as workers – which have the ad-\nvantage of being free, instantly interactive, fast,\nresponsive, and non-stopping.\nOur contributions: (i) We propose LMTurk, a\nmethod that uses few-shot learners as crowdsourc-\ning workers. Figure 1 shows the overview of LM-\nTurk. (ii) We vary an array of important design\nchoices, identifying strengths and weaknesses of\nLMTurk. (iii) Unlike much work on active learning\nin a synthetic oracle setting, we develop methods\nfor handling the varying quality of annotation that\ndoes not come from an oracle. (iv) We extensively\nevaluate LMTurk on ﬁve datasets, showing that\nLMTurk can guide a small model Sto progres-\nsively improve on T. Scan then be deployed in\npractical scenarios. (v) This is the ﬁrst work show-\ning that few-shot learners give rise to effective NLP\nmodels through crowdsourcing and active learning\n– with the beneﬁts of low annotation cost and prac-\ntical deployability.\n2 Related Work\nFew-shot learners in NLP. Signiﬁcant progress\nhas been made in developing (Devlin et al., 2019;\nPeters et al., 2018; Yang et al., 2019; Brown\net al., 2020), understanding (Liu et al., 2019; Ten-\nney et al., 2019; Belinkov and Glass, 2019; He-\nwitt and Liang, 2019; Hewitt and Manning, 2019;\nZhao et al., 2020a; Rogers et al., 2020), and uti-\nlizing (Houlsby et al., 2019; Zhao et al., 2020b;\nBrown et al., 2020; Li and Liang, 2021; Schick\nand Schütze, 2021a; Lester et al., 2021; Mi et al.,\n2021a) PLMs. Brown et al. (2020), Schick and\nSchütze (2021a), and Liu et al. (2021b) show that\nPLMs can serve as data-efﬁcient few-shot learners,\nthrough priming or prompting (Liu et al., 2021a).\nFor example, GPT3 achieves near state-of-the-art\nperformance on COPA (Roemmele et al., 2011)\nwith only 32 annotated data.\nHowever, little to no work discusses or explores\nthe actual practical utility of these few-shot learn-\ners. We aim to develop effective methods of utiliz-\ning them in practical scenarios.\nCrowdsourcing has a long history in human\nlanguage technology (Alonso et al., 2008; Callison-\nBurch, 2009; Trautmann et al., 2020); specialized\nworkshops were organized (Callison-Burch and\nDredze, 2010; Paun and Hovy, 2019). It has numer-\nous applications (Yuen et al., 2011), but we focus\non its application as voting systems. To reduce per-\nlabel cost, crowdsourcing organizes non-expert hu-\nman workers distributed across the Web for annota-\ntion, instead of employing linguistic experts (Jami-\nson and Gurevych, 2015; Bhardwaj et al., 2019;\n676\nNangia et al., 2021). Snow et al. (2008) show\nthat averaging ten crowdsourced labels matches\nan expert-level label for recognizing textual entail-\nment (Dagan et al., 2006). Paun et al. (2018) show\nthat incorporating structure in annotation models is\nimportant. Measuring label disagreements is also\ncrucial (Dumitrache et al., 2021).\nLMTurk utilizes NLP few-shot learners as non-\nexpert workers. The few-shot training data can be\nviewed as the examples shown to humans before\nannotating. The process is free, fast, responsive,\nand non-stopping.\nActive learning (AL; Cohn et al. (1996); Settles\n(2009)) strives to reduce the number of examples\nto be annotated via identifying informative exam-\nples with acquisition functions. Settles and Craven\n(2008) evaluate AL algorithms for sequence label-\ning. Zhang et al. (2017); Shen et al. (2017); Sid-\ndhant and Lipton (2018) apply AL to deep neural\nnetworks. Simpson and Gurevych (2018) devise\na scalable Bayesian preference learning method\nfor identifying convincing arguments. Lee et al.\n(2020) propose to consider user feedback in AL\nsystems. Ein-Dor et al. (2020) explore AL for\nBERT. Schröder and Niekler (2020) review text\nclassiﬁcation with AL. Liang et al. (2020); Mar-\ngatina et al. (2021) integrate contrastive learning\ninto AL. Zhang and Plank (2021) identify examples\nwith datamap (Swayamdipta et al., 2020).\nWe incorporate AL in LMTurk to reduce the\namount of examples to be annotated by PLMs, re-\nducing the computational cost of running several in-\nference passes. This contributes to a more environ-\nmentally friendly (Strubell et al., 2019; Schwartz\net al., 2020; Patterson et al., 2021) scenario.\nPerhaps closest to our work, Yoo et al. (2021)\nconduct data augmentation via priming GPT3\nand Wang et al. (2021) mix human- and GPT3-\nannotated data, focusing on cost analysis. GPT3\nis utilized in a Language-Model-as-a-Service form\nby OpenAI, which is not free. 2 Also, strategies\nof priming GPT3 may not generalize well to other\nPLMs. For example, priming strategies have to\nadapt to GPT3’s maximum sequence length. How-\never, maximum sequence length – as a hyperpa-\nrameter – could vary across PLMs. In this work,\nwe prompt publicly available free PLMs. This also\nmakes the process more ﬂexible; for example, the\nPLM can be updated with gradient descent.\n2https://beta.openai.com/pricing\n3 LMTurk\n3.1 Training few-shot learners\nWe ﬁrst adapt a PLM to task T with a few-shot\nhuman-labeled gold dataset G= {Gtrain; Gdev} of\nT. This procedure mimics one of the initial but\ncrucial steps in crowdsourcing: A few example an-\nnotations are shown to the workers, demonstrating\nT; workers learn about the task and then start anno-\ntating (Snow et al., 2008; He et al., 2015; Roit et al.,\n2020; Trautmann et al., 2020; Lee et al., 2021).\nWe achieve this adaptation through P-Tuning\n(Liu et al., 2021b). Taking movie review classi-\nﬁcation as an example, the goal is to associate a\nbinary label yfrom {-1, +1} to an input sentence\nx = (x1,...,x n) where xi refers to a token. Un-\nlike ﬁnetuning and its variants (Devlin et al., 2019;\nHoulsby et al., 2019; Zhao et al., 2020b) that train\na classiﬁer head, P-Tuning reformulates a sentence\ninto a cloze-style query; the PLM is then requested\nto respond to the query with an answer selected\nfrom a list of candidates. Concretely, an input pair\n(x, y) = (“watching it leaves you giddy.”, -1)\nis reformulated to:\n“[v] watching it leaves you giddy. It is [MASK] .”\nin which the underlined tokens are prompting\nwords that give the model a hint about T. “[v]” –\nwhose trainable embedding vector is randomly ini-\ntialized – is a prompting token injecting extra free\nparameters. The PLM is then requested to pick a\nword from {“bad”, “good”} to ﬁll in the position of\n“[MASK]”. A mapping {“bad”→-1, “good”→+1}\nis used to transform the selected answer to a label\nsuch that standard evaluation measures like accu-\nracy can be computed. Prompting has been shown\nto effectively adapt a PLM to T with only a few\nannotations; see (Liu et al., 2021a) for a compre-\nhensive review of prompting. We refer to a PLM\nadapted to T as an LMTurker A.\nWe select prompting words and mappings based\non the small development set Gdev. §4.2 provides\ndetails on prompting and datasets.\n3.2 Aggregating annotations\nIndividual workers are subject to annotation bi-\nases (Snow et al., 2008); therefore, crowdsourcing\noften collects labels from several workers (Yuen\net al., 2011) for an example x and then aggregates\nthem for quality control (Alonso et al., 2008). It\nis straightforward to obtain a group of LMTurkers\n677\nA= {A1,...,A k}, by adapting the PLM to Twith\nkdifferent prompts. A querying sentence x is then\nannotated by every LMTurker, resulting in a list\nof labels y = [y1,...,y k]. We evaluate different\nmethods aggregating y to a single label ˆy.\nBestWorker. Among the kLMTurkers, we pick\nthe one performing best on the dev set Gdev.\nMajorityVoting. We select the most frequent\nlabel in y = [y1,...,y k] as ˆy.\nTo estimate an LMTurker’s conﬁdence on label\nyi, we compare the logits3 computed by the PLM:\nyi = arg max(logit(y1),..., logit(yN )),\nwhere N refers to the label set size, e.g., N=2 for\ny from {-1, +1}. We then can evaluate several\nmethods of aggregating annotations according to\nPLM logits.\nLogitVoting. We average the logits from all k\nLMTurkers {A1,...,A k}to compute ˆy:\nˆy = arg max(1\nk\n∑k\ni=1 logit(y1\ni ),..., 1\nk\n∑k\ni=1 logit(yN\ni )).\nWeightedLogitVoting. We use LMTurkers’ per-\nformance on Gdev to weight their logits and then\naggregate the predictions:\nˆy = arg max(∑k\ni=1 wilogit(y1\ni ),...,∑k\ni=1 wilogit(yN\ni ))\nwi = f(Ai,Gdev)/ ∑k\ni=1 f(Ai,Gdev)\nwhere f(Ai,Gdev) is the performance of the ith\nLMTurker Ai on Gdev.\nWe collect and aggregate annotations from ﬁve\nLMTurkers, i.e., we use k=5 in our experiments.\n3.3 Training a small model S\nAfter adapting LMTurkers to Tthrough prompting\nwith the few-shot gold dataset G, we next train\na small model Sspecialized to solve T. Though\nlarge PLMs are versatile and strong performers,\ntraining and inference are faster and more efﬁcient\nfor small models: They are more deployable in\nresource-restricted scenarios, e.g., on edge devices\n(Jiao et al., 2020).\nWe mimic pool-based active learning (AL; Set-\ntles (2009)) to train S. The motivation is to avoid\nfrequent querying of LMTurkers Abecause energy\nand time consumption of PLM inference is costly\nwhen the number of queries and |A|are large.\nConcretely, pool-based AL assumes a large col-\nlection of unlabeled data U= {x1,..., xM } for T.\n3Calibration can be conducted to further improve the esti-\nmation (Guo et al., 2017). We leave this to future work.\nSis ﬁrst trained with G= {Gtrain; Gdev}. After\nthat, a group of examples Bfrom Uis sampled\n(c.f. §3.3.1), which LMTurkers annotate. Next, the\nannotated and aggregated examples B′are concate-\nnated with Gto train S. The procedure is repeated\niteratively, such that the training data for Skeeps\nexpanding. We denote as Sj the model trained af-\nter the jth iteration. Note that Sis trained from\nscratch in each iteration (Cohn et al., 1994).\n3.3.1 AL acquisition function\nAt the beginning of the jth iteration, a straightfor-\nward strategy of sampling Bfrom Uis random\nsampling. AL promises to select a more informa-\ntive Bsuch that the trained Sj performs better, un-\nder the same budget. These strategies – or acquisi-\ntion functions – rely on Sj−1, i.e., Sfrom the previ-\nous iteration: Sj−1 is employed to inferUto obtain\nlabels and logits Pj−1 = {(y1,c1),..., (yM ,cM )};\neach ci contains the logits of the N labels; yi =\narg max(ci). We explore two common AL acquisi-\ntion functions: Entropy (Roy and McCallum, 2001)\nand LeastConﬁdent (Lewis and Gale, 1994).\nEntropy selects from Pj−1 examples with the\nlargest prediction entropy, computed usingc. Large\nentropy of an example x implies that Sj−1 is un-\nsure about which label to select; x is then a query\nmade to LMTurkers to obtain its label ˆy. (x,ˆy) is\nsubsequently added to Gtrain for training Sj.\nLeastConﬁdent selects from Pj−1 examples for\nwhich the maximum logit in c is the smallest. Se-\nlected examples are then annotated and added to\nGtrain for training Sj.\nOur AL setup is fairly standard, both in terms of\nacquisition functions and iterative enlargement by\nnew sampled data Bat iteration jlabeled by Sj−1.\n3.3.2 Considering annotation quality\nAs in any realistic AL scenario, annotations are not\nperfect: LMTurkers do not score perfectly on T.\nAs a result, annotation quality of LMTurkers needs\nto be taken into consideration before training Sj.\nDenoting the training data of Sj as Dj, we explore\na strategy of processing Dj, based on LMTurker\nlogits l.\nInstanceTresholding. We preserve examples\n(x,ˆy,l) ∈Dj for which entropy computed on l is\nsmallest. Gtrain is always preserved because it is\nhuman-labeled gold data. Note that this is different\nfrom the strategy of sampling B, where we select\nfrom Pj−1 examples to which Sj−1 is most unsure\n678\n(computed with c). We evaluate4 the effectiveness\nof processing Dj before training Sj in §5.6.\n3.4 Summary of LMTurk\nLMTurk can be viewed as intermediate between\nself training (Yarowsky, 1995; Abney, 2004; Lee\net al., 2013; Mi et al., 2021b) and AL. Unlike self\ntraining, LMTurk employsexternal models provide\nlabels to S. Different from the artiﬁcial setup used\nin many AL experiments, the provided labels do\nnot have oracle quality; so Smust use the annota-\ntions more carefully. We next conduct experiments\ninvestigating the effectiveness of LMTurk.\n4 Datasets and Setup\n4.1 Dataset\nWe evaluate LMTurk on ﬁve datasets: Binary\n(SST2) and ﬁne-grained (ﬁve classes) sentiment\nclassiﬁcation (SST5) with the Stanford Sentiment\nTreeBank (Socher et al., 2013); news article topic\nclassiﬁcation with the AG’s News Corpus (AG-\nNews; Zhang et al. (2015)); recognizing textual en-\ntailment (RTE; Dagan et al. (2006)); assessing lin-\nguistic acceptability (CoLA; Warstadt et al. (2019)).\nAppendix §A reports dataset statistics. SST2/SST5\nand AGNews are widely used in crowdsourcing\nand AL (Laws et al., 2011; Ein-Dor et al., 2020;\nMargatina et al., 2021; Zhang and Plank, 2021).\nRTE and CoLA assess the models’ ability to un-\nderstand textual entailment and linguistic phenom-\nena – as opposed to text categorization. We report\nMatthew’s correlation coefﬁcient for CoLA and\naccuracy for the others (Wang et al., 2018).\nFew-shot datasets. Recall LMTurk uses a small\nhuman-annotated dataset G= {Gtrain; Gdev}. De-\nnoting nas the number of shots per class, we sam-\nple Gn\ntrain and Gn\ndev for each of n ∈{8,16,32}.\nFor SST2, RTE, and CoLA, we use the train and\ndev sets of GLUE (Wang et al., 2018);Gn\ntrain and\nGn\ndev are sampled from the train set; the dev set is\nused as the test set. For SST5 and AGNews, we\nuse the ofﬁcial datasets; Gn\ntrain (Gn\ndev) is sampled\nfrom the train (dev) set; we report performance on\nthe test set. We repeat the sampling process with\nthree random seeds.\n4.2 Training setup\nBrown et al. (2020) show that large model size is\n4Motivated by Wang et al. (2017), we also investigate the\neffectiveness of weighting training examples. However, we\ndo not observe noticeable improvements of task performance.\nWe list more details in Appendix §E.\nSchick and Schütze (2021a,b) Gao et al. (2021) Ours\nSST2 n/a 93.0±0.6 93.08±0.62\nSST5 n/a 49.5±1.7 46.70±0.93\nRTE 69.8 71.1±5.3 70.88±1.70\nAGN. 86.3±0.0 n/a 87.71±0.07\nCoLA n/a 21.8±15.9 19.71±1.89\nTable 1: LMTurkers achieve comparable few-shot per-\nformance with the literature. We refer to PET results\nin Schick and Schütze (2021a,b) and results of Prompt-\nbased FT (auto) + demonstrations in Gao et al. (2021).\nnecessary for strong few-shot performance. We\nuse ALBERT-XXLarge-v2 (Lan et al., 2020) – of\nsize 223M parameters – as our large PLM, which is\nadapted to be an LMTurkerAof Twith G. With pa-\nrameter reuse, ALBERT-XXLarge-v2 outperforms\nlarger models like the 334M BERT-large (Devlin\net al., 2019). In contrast, Smust be small to be de-\nployable in practical scenarios. We use TinyBERT-\nGeneral-4L-312D (Jiao et al., 2020), which has\n14.5M parameters.\nWe train – with prompting – the large PLM with\nGfor 100 batch steps using batch size 16, AdamW\n(Loshchilov and Hutter, 2019) and learning rate\n5e-4 with linear decay. We prompt the large PLM\nﬁve times to obtain ﬁve LMTurkers; Appendix §C\nshows prompting details. At each iteration, we ﬁne-\ntune Sfor 20 epochs using batch size 32, Adam\n(Kingma and Ba, 2015) and learning rate 5e-5.\nEach experiment is run with three different ran-\ndom seeds. We use PyTorch (Paszke et al., 2019)\nand HuggingFace (Wolf et al., 2020).\n5 Experiment\n5.1 Few-shot performance (non-iterative)\nWe compare few-shot performance of LMTurkers\nand the small model Swhen only Gis used. LM-\nTurker performance is comparable to prior work\n(Schick and Schütze, 2021a,b; Gao et al., 2021) as\nshown in Table 1.\nFigure 2 compares performance of LMTurkers\nand S. Appendix §B Table 3 reports numeric val-\nues. LMTurkers perform clearly better than Son\nCoLA, SST5, AGNews, and SST2; e.g., for SST2,\nfor train/dev size 16, LMTurker accuracy is 93.08%\nvs. 75.83% for S. LMTurkers’ superiority overS\non RTE is modest. As an inference task, RTE\nis more challenging than classiﬁcation (e.g., AG-\nNews). We hypothesize that current few-shot learn-\ners require more data than G32 to process difﬁcult\ntasks better than S. Scaling up to even larger PLMs\nis also a promising direction (Brown et al., 2020;\n679\nFigure 2: Few-shot test set performance of LMTurkers\nand S. We use the few-shot gold datasets G8 (top), G16\n(middle), and G32 (bottom).\nLester et al., 2021).\nOverall, LMTurkers outperform Swith clear\nmargins, evidencing that their annotations can\nserve as supervisions for training S. We next con-\nduct iterative training to improve performance of\nSon T with supervisions from LMTurkers.\n5.2 Iterative training\nWe investigate the effectiveness of LMTurk by sim-\nulating scenarios analogous to active learning. Con-\ncretely, we compare three schemes of annotating\nthe sampled data Bat each annotation iteration j:\n• Active learning (AL). We use B’sgold labels\nto show how Sperforms with expert annota-\ntions. Gold labels are ideal, but costly because\nexpert annotators need to be employed.\n• Self training (ST). We use Sj−1, the model\ntrained in the previous iteration, to annotate\nB(Yarowsky, 1995; Abney, 2004; Lee et al.,\nFigure 3: Improving Swith active learning (blue), self\ntraining (orange), and LMTurk (green). Free markers\nat step zero show LMTurker performances; colors dis-\ntinguish random seeds. Three acquisition functions are:\nEntropy (•), LeastConﬁdent ( ■), random sampling\n(/enc-36). At iteration j, each experiment is repeated\nthree times; we show mean and standard deviation.\nAppendix Figure 9 visualizes more results.\n2013). ST trades supervision quality for an-\nnotation cost; no extra cost is introduced. Be-\ncause there is no external supervision, ST is\nexpected to be a baseline.\n• LMTurk. We query the LMTurkers to anno-\ntate B. LMTurkers are machine learning mod-\nels, so there is no human labor. Based on the\nﬁndings in Figure 2, LMTurker supervisions\n680\nare expected to have better quality than those\nof ST. Yet LMTurk could fall behind AL be-\ncause LMTurker labels are not gold labels.\nWhen sampling Bfrom Uat each iteration j,\nwe consider the strategies described in §3.3. We\nemploy Random for all three schemes and En-\ntropy/LeastConﬁdent for AL/LMTurk. Entropy\nand LeastConﬁdent rely on Sj−1. Regarding the\nnumber of sampled examples, we experiment with\n|B|=100 and |B|=400 for SST2, SST5, AGNews,\nCoLA. Due to RTE’s small size, we use |B|=20\nand |B|=100. We run for 15 iterations of improv-\ning S. To aggregate annotations from LMTurkers,\nwe use MajorityV oting (§3.2), which is widely used\nin crowdsourcing. See §5.3 for a comparison of\nvarious aggregation methods.\nFigure 3 compares AL, ST, and LMTurk.ST\n(orange) noticeably helps Sto perform progres-\nsively better on AGNews, e.g., when comparing\nS15 to S0 shown in the ﬁrst row, especially when\n|B|=400. However, we do not identify clear im-\nprovements when looking at other tasks. Except for\nRTE-G8, ST clearly falls behind AL and LMTurk.\nThis inferior performance meets our expectation\nbecause there is no external supervision assisting\nSto perform better on T. In what follows, we omit\nST for clearer visualization and discussion.\nAL (blue) performs the best in most experiments.\nHowever, this comes with extra costs that are not\nnegligible: At each iteration , human annotators\nneed to annotate 100–400 sentences.\nLMTurk (green) holds a position between AL\nand ST on AGNews, SST2, SST5, and CoLA.\nSomehow surprisingly, LMTurk performs almost\ncomparably to AL on SST2. Unlike AL, LMTurk\nrequires very little human labor; the only human\nannotation throughout the entire process is the few-\nshot gold datasetG. In contrast, AL has high human\nannotation cost, e.g., 1000–4000 examples by iter-\nation ten. LMTurk also shows clear performance\nimprovements over ST.\nResults on RTE are noisy; we conjecture this\nis due to its very small test set (277 examples).\nWe do not observe performance improvement of\nS along the iterations in experiment RTE- G32-\n|B|=100, likely due to saturated task performance:\nTinyBERT-General-4L-312D (S) achieves 66.6%\non RTE for the full train set (Jiao et al., 2020).\nComparing sampling strategies. Entropy ( •)\nand LeastConﬁdent ( ■) outperform random sam-\npling (/enc-36) in AGNews and SST2 with noticeable\nFigure 4: Comparing strategies of aggregating LM-\nTurker annotations. We compare LMTurk (green) with\nAL (blue). Strategies: LogitV oting (/enc-36), MajorityV ot-\ning (■), WeightedLogitV oting (♦), BestWorker (\u001a).\nAL uses gold labels without aggregation (•).\nmargins – for both AL and LMTurk, especially\nwhen |B|=400. They also surpass random sam-\npling when using LMTurk for SST5 and CoLA\nwith G8. In other words, Entropy and LeastCon-\nﬁdent assist LMTurk to achieve the same perfor-\nmance as of using random sampling, but with fewer\nannotations. For example in AGNews-G8-|B|=100,\nLeastConﬁdent at iteration six already achieves\ncomparable performance as random sampling at\niteration eleven. This is economically and environ-\nmentally beneﬁcial because the number of queries\nmade to LMTurkers, i.e., the cost of running infer-\nence passes on the array of large PLMs, is signiﬁ-\ncantly reduced.\nOverall, we show that LMTurk can be used to\ncreate datasets for training a specialized modelSof\nsolving T in practical scenarios. To reduce compu-\ntational cost, we use only Entropy in what follows.\n5.3 Design choice 1: Aggregation strategies\nFigure 4 compares effectiveness of different strate-\ngies of aggregating LMTurker annotations (§3.2).\nLooking at SST5 and AGNews results (top two\nimages), we observe that committee-style aggre-\ngation (LogitV oting (/enc-36), MajorityV oting (■), and\nWeightedLogitV oting (♦)) generally outperforms\nBestWorker (\u001a), which simply relies on the LM-\nTurker performing best on Gdev. LMTurkers per-\nform well on these two datasets as shown by the\nfree markers at iteration zero; ensembling their pre-\ndictions results in higher-quality datasets.\n681\n0 20 40 60 80 100\nIteration\n0.78\n0.80\n0.82\n0.85\n0.88\n0.90\n0.93\nAGNews; 32; | | = 100\nAL\nLMTurk\n0 100 200 300 400 500\nIteration\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95 SST2; 32; | | = 100\nAL\nLMTurk\nFigure 5: Running more iterations of improving Swith\nAL and LMTurk. Sampling strategy Entropy is used for\nboth methods; WeightedLogitV oting is used for aggre-\ngating LMTurker annotations.\nIn contrast, BestWorker ( \u001a) has stellar per-\nformance on RTE (bottom-left), outperforming\ncommittee-style aggregation. Note that even the\nLMTurkers do not perform really well in this ex-\nperiment, as shown by the free markers at itera-\ntion zero – some LMTurkers even perform worse\nthan S. Ensembling these low-quality annotations\nseems a worse option than simply relying on the\nbest LMTurker. For CoLA, we observe comparable\nperformance of different aggregation strategies.\n5.4 Design choice 2: More iterations\nWe hypothesize that AL performance is an upper\nbound for performance when Sis trained with LM-\nTurker annotations – recall that the AL annotations\nare gold labels. Figure 5 compares AL and LM-\nTurk when running 100 iterations of improving\nSon AGNews and 500 iterations on SST2. As\nexpected, AL outperforms LMTurk because the\npool of human-annotated data expands. The per-\nformance of Sprogressively approaches that of the\nLMTurkers; LMTurk performs comparably to AL\nin SST2, however, no human labor is required.\n5.5 Design choice 3: Distilling logits\nWe can view LMTurk as a kind of distillation (Hin-\nton et al., 2015): The ability of LMTurkers to solve\nT is progressively transferred to S. In this sec-\ntion, we explore the utility of distillation: We train\nSwith predicted logits5 instead of discrete labels\nfrom LMTurkers. Concretely, we train Sby re-\nducing the KL divergence between its predicted\nprobability distribution (over the label set) and the\nprobability distribution from LMTurkers.\n5Distilling with intermediate activations likely to further\nimprove performance of S. However, note that PLM inter-\nmediate activations are not always available in a Language-\nModel-as-a-Service framework.\nFigure 6: Performance of AL and LMTurk with discrete\nlabels (•) vs. with KL divergence (/enc-36). Entropy is used\nas the sampling strategy and WeightedLogitV oting\nis used to aggregate worker annotations.\nFigure 6 shows that training Swith KL diver-\ngence noticeably improves over discrete labels on\nAGNews and SST5. This is expected: AGNews\nand SST5 have larger label set size (four and ﬁve)\nsuch that the probability distribution over the la-\nbel set is more informative than that of the binary\nclassiﬁcation tasks SST2 and RTE.\n5.6 Design choice 4: Quality-based ﬁltering\nOne key difference between AL and LMTurk is\nthat LMTurkers are not oracles: Their labels are\nnot perfect. Hence, it is reasonable to consider\nprocessing the training data, denoted as Dj, for Sj,\ninstead of using it indiscriminately as in AL.\nInstanceTresholding (§3.3.2) preserves annota-\ntions in Dj for which LMTurkers have the smallest\nprediction entropy. Concretely, we rank all anno-\ntations (x,ˆy,l) ∈Dj by entropy(l) and then keep\nthe τ percent smallest. Note that we always pre-\nserve the human-labeled few-shot data Gtrain. We\nexperiment with τ ∈{10%, . . . , 90%, 100%}.\nFigure 7 left shows the performance of S; Fig-\nure 7 right tracks the status of Dj. To measure\nquality, we compute the accuracy of LMTurker an-\nnotations on Dj (compared to gold labels); see the\nlineplots and the left y-axis. We also report the size\nof Dj as scatter plots (right y-axis).\nWe observe that τ=10%, i.e., keeping only the\n10% most certain examples, gives the worst perfor-\nmance. This is most obvious at iteration three for\nSST2: The performance drops to near the majority\nbaseline (≈50%). This is because D3 is small and\n682\n0 2 4 6 8 10 12 14\nIteration\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nSST2; 8; | | = 100\nExp.\nAL\nLMTurk\nPreseR.\n0.1\n0.8\n0.9\n1.0\n0 2 4 6 8 10 12 14\nIteration\n0.80\n0.83\n0.85\n0.88\n0.90\n0.93\n0.95\n0.98\n1.00Dataset Quality\nSST2; 8; | | = 100\n0\n200\n400\n600\n800\n1000\n1200\n1400\nDataset Size\n0.1\n0.8\n0.9\n1.0\n0 2 4 6 8 10 12 14\nIteration\n0.28\n0.30\n0.33\n0.35\n0.38\n0.40\n0.43\n0.45\nSST5; 8; | | = 100\nExp.\nAL\nLMTurk\nPreseR.\n0.1\n0.8\n0.9\n1.0\n0 2 4 6 8 10 12 14\nIteration\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00Dataset Quality\nSST5; 8; | | = 100\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nDataset Size\n0.1\n0.8\n0.9\n1.0\n0 2 4 6 8 10 12 14\nIteration\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAGNews; 8; | | = 100\nExp.\nAL\nLMTurk\nPreseR.\n0.1\n0.8\n0.9\n1.0\n0 2 4 6 8 10 12 14\nIteration\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Dataset Quality\nAGNews; 8; | | = 100\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nDataset Size\n0.1\n0.8\n0.9\n1.0\nFigure 7: Training Swith examples for which LMTurk-\ners have low entropy. We report performance ofS(left),\nnumber and quality (measured by accuracy) of the pre-\nserved examples (right) at each iteration.\nunbalanced: It has eight negative (from Gtrain) and\n38 positive examples. However, using all the LM-\nTurker annotations (τ=100%) may not be optimal\neither. This is noticeable when looking at SST5:\nτ=90% and τ=80% are better options.\nWe see that there is a trade-off between Dj’s\nquality and size from Figure 7 right. Being con-\nservative, i.e., preserving only a handful of anno-\ntations from LMTurkers, results in a small, but\nhigh-quality Dj; using all the annotations indis-\ncriminately leads to a large Dj with low quality.\nThis experiment highlights a key difference be-\ntween LMTurk and AL: LMTurker annotations are\nnot perfect and taking the annotation quality into\nconsideration when training Sis crucial.\n6 Conclusion\nIn this work, our focus is the research question:\nHow to make effective use of current few-shot learn-\ners? We propose LMTurk, a simple yet effective\nmethod that considers PLM-based few-shot learn-\ners as non-expert annotators in crowdsourcing; ac-\ntive learning strategies are incorporated to reduce\nthe cost of annotation. We further show that pro-\ncessing the annotations from LMTurkers can be\nbeneﬁcial.\nFuture work may combine LMTurker annota-\ntions with human annotators in a human-in-the-\nloop setup (Monarch, 2021) to increase the overall\nutility of invested resources (Bai et al., 2021). Scal-\ning up to even larger PLMs likely to further boost\nmodel performances (Kaplan et al., 2020; Brown\net al., 2020) Applying LMTurk to multilingual few-\nshot learners (Zhao et al., 2021; Winata et al., 2021;\nLin et al., 2021) is also promising.\nAcknowledgements\nWe thank the anonymous reviewers for their in-\nsightful comments and suggestions. MZ and HS\nwere supported by the European Research Coun-\ncil (ERC# 740516) and the German Federal Min-\nistry of Education and Research (BMBF, grant\n#01IS18036A).\nReferences\nSteven Abney. 2004. Understanding the Yarowsky al-\ngorithm. Computational Linguistics, 30(3):365–395.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. MasakhaNER: Named Entity\nRecognition for African Languages. Transactions\nof the Association for Computational Linguistics ,\n9:1116–1131.\nOmar Alonso, Daniel E. Rose, and Benjamin Stewart.\n2008. Crowdsourcing for relevance evaluation. SI-\nGIR Forum, 42(2):9–15.\nFan Bai, Alan Ritter, and Wei Xu. 2021. Pre-train or an-\nnotate? domain adaptation with a constrained budget.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n683\n5002–5015, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nSangnie Bhardwaj, Samarth Aggarwal, and Mausam\nMausam. 2019. CaRB: A crowdsourced benchmark\nfor open IE. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 6262–6267, Hong Kong, China. Association\nfor Computational Linguistics.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nChris Callison-Burch. 2009. Fast, cheap, and creative:\nEvaluating translation quality using Amazon’s Me-\nchanical Turk. In Proceedings of the 2009 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 286–295, Singapore. Association\nfor Computational Linguistics.\nChris Callison-Burch and Mark Dredze, editors. 2010.\nProceedings of the NAACL HLT 2010 Workshop on\nCreating Speech and Language Data with Amazon’s\nMechanical Turk. Association for Computational Lin-\nguistics, Los Angeles.\nDavid Cohn, Les Atlas, and Richard Ladner. 1994. Im-\nproving generalization with active learning. Machine\nlearning, 15(2):201–221.\nDavid A Cohn, Zoubin Ghahramani, and Michael I\nJordan. 1996. Active learning with statistical models.\nJournal of artiﬁcial intelligence research, 4:129–145.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classiﬁcation,\nand Recognising Tectual Entailment, pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAnca Dumitrache, Oana Inel, Benjamin Timmermans,\nCarlos Ortiz, Robert-Jan Sips, Lora Aroyo, and Chris\nWelty. 2021. Empirical methodology for crowdsourc-\ning ground truth. Semantic Web, 12(3):1–19.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim. 2020.\nActive Learning for BERT: An Empirical Study. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7949–7962, Online. Association for Computa-\ntional Linguistics.\nRichard M Felder and Rebecca Brent. 2009. Active\nlearning: An introduction. ASQ higher education\nbrief, 2(4):1–5.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning, pages 1321–1330. PMLR.\nBen Hachey, Beatrice Alex, and Markus Becker. 2005.\nInvestigating the effects of selective sampling on the\nannotation task. In Proceedings of the Ninth Confer-\nence on Computational Natural Language Learning\n(CoNLL-2005), pages 144–151, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n643–653, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743, Hong Kong,\nChina. Association for Computational Linguistics.\n684\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efﬁcient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nJeff Howe. 2008. Crowdsourcing: How the power of\nthe crowd is driving the future of business. Random\nHouse.\nJeff Howe et al. 2006. The rise of crowdsourcing. Wired\nmagazine, 14(6):1–4.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research, pages\n4411–4421. PMLR.\nEmily Jamison and Iryna Gurevych. 2015. Noise or\nadditional information? leveraging crowdsource an-\nnotation item agreement for natural language tasks.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n291–297, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4163–\n4174, Online. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nDaniel Khashabi, Arman Cohan, Siamak Shakeri, Pe-\ndram Hosseini, Pouya Pezeshkpour, Malihe Alikhani,\nMoin Aminnaseri, Marzieh Bitaab, Faeze Brahman,\nSarik Ghazarian, Mozhdeh Gheini, Arman Kabiri,\nRabeeh Karimi Mahabagdi, Omid Memarrast, Ah-\nmadreza Mosallanezhad, Erfan Noury, Shahab Raji,\nMohammad Sadegh Rasooli, Sepideh Sadeghi, Er-\nfan Sadeqi Azer, Niloofar Saﬁ Samghabadi, Mahsa\nShafaei, Saber Sheybani, Ali Tazarv, and Yadollah\nYaghoobzadeh. 2021. ParsiNLU: A suite of language\nunderstanding challenges for Persian. Transactions\nof the Association for Computational Linguistics ,\n9:1147–1162.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nFlorian Laws, Christian Scheible, and Hinrich Schütze.\n2011. Active learning with Amazon Mechanical\nTurk. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1546–1556, Edinburgh, Scotland, UK. Associ-\nation for Computational Linguistics.\nDong-Hyun Lee et al. 2013. Pseudo-label: The simple\nand efﬁcient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in\nrepresentation learning, ICML, volume 3, page 896.\nJi-Ung Lee, Jan-Christoph Klie, and Iryna Gurevych.\n2021. Annotation curricula to implicitly train non-\nexpert annotators. arXiv preprint arXiv:2106.02382.\nJi-Ung Lee, Christian M. Meyer, and Iryna Gurevych.\n2020. Empowering Active Learning to Jointly Op-\ntimize System and User Demands. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4233–4247, On-\nline. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nDavid D Lewis and William A Gale. 1994. A sequential\nalgorithm for training text classiﬁers. In SIGIR’94,\npages 3–12. Springer.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\n685\nWeixin Liang, James Zou, and Zhou Yu. 2020. ALICE:\nActive learning with contrastive natural language ex-\nplanations. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4380–4391, Online. Association\nfor Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 650–663, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nFei Mi, Yitong Li, Yasheng Wang, Xin Jiang, and Qun\nLiu. 2021a. Cins: Comprehensive instruction for few-\nshot learning in task-oriented dialog systems. arXiv\npreprint arXiv:2109.04645.\nFei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai,\nMinlie Huang, and Boi Faltings. 2021b. Self-training\nimproves pre-training for few-shot learning in task-\noriented dialog systems. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1887–1898.\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and\nRoss T. Bunker. 1993. A semantic concordance.\nIn Human Language Technology: Proceedings of\na Workshop Held at Plainsboro, New Jersey, March\n21-24, 1993.\nRobert Munro Monarch. 2021. Human-in-the-Loop\nMachine Learning: Active learning and annotation\nfor human-centered AI. Simon and Schuster.\nNikita Nangia, Saku Sugawara, Harsh Trivedi, Alex\nWarstadt, Clara Vania, and Samuel R. Bowman. 2021.\nWhat ingredients make for an effective crowdsourc-\ning protocol for difﬁcult NLU data collection tasks?\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1221–1235, Online. Association for Computational\nLinguistics.\nSungjoon Park, Jihyung Moon, Sung-Dong Kim,\nWon Ik Cho, Jiyoon Han, Jangwon Park, Chisung\nSong, Junseong Kim, Yongsook Song, Tae Hwan\nOh, Joohong Lee, Juhyun Oh, Sungwon Lyu, Young\nkuk Jeong, Inkwon Lee, Sang gyu Seo, Dongjun Lee,\nHyunwoo Kim, Myeonghwa Lee, Seongbo Jang, Se-\nungwon Do, Sunkyoung Kim, Kyungtae Lim, Jong-\nwon Lee, Kyumin Park, Jamin Shin, Seonghyun\nKim, Lucy Park, Alice H. Oh, Jung-Woo Ha, and\nKyunghyun Cho. 2021. KLUE: Korean language\nunderstanding evaluation. ArXiv, abs/2105.09680.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems, volume 32. Curran Associates, Inc.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Carbon\nemissions and large neural network training. arXiv\npreprint arXiv:2104.10350.\nSilviu Paun, Bob Carpenter, Jon Chamberlain, Dirk\nHovy, Udo Kruschwitz, and Massimo Poesio. 2018.\nComparing Bayesian models of annotation. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:571–585.\nSilviu Paun and Dirk Hovy, editors. 2019. Proceedings\nof the First Workshop on Aggregating and Analysing\nCrowdsourced Annotations for NLP. Association for\nComputational Linguistics, Hong Kong, China.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n686\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efﬁcient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 803–818,\nOnline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In 2011 AAAI Spring Symposium Series.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nPaul Roit, Ayal Klein, Daniela Stepanov, Jonathan\nMamou, Julian Michael, Gabriel Stanovsky, Luke\nZettlemoyer, and Ido Dagan. 2020. Controlled\ncrowdsourcing for high-quality QA-SRL annotation.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7008–\n7013, Online. Association for Computational Lin-\nguistics.\nNicholas Roy and Andrew McCallum. 2001. Toward op-\ntimal active learning through monte carlo estimation\nof error reduction. ICML, Williamstown, 2:441–448.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nChristopher Schröder and Andreas Niekler. 2020.\nA survey of active learning for text classiﬁca-\ntion using deep neural networks. arXiv preprint\narXiv:2008.07267.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green AI. Communications of the\nACM, 63(12):54–63.\nBurr Settles. 2009. Active learning literature survey.\nBurr Settles and Mark Craven. 2008. An analysis of\nactive learning strategies for sequence labeling tasks.\nIn Proceedings of the 2008 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1070–1079, Honolulu, Hawaii. Association for Com-\nputational Linguistics.\nYanyao Shen, Hyokun Yun, Zachary Lipton, Yakov Kro-\nnrod, and Animashree Anandkumar. 2017. Deep\nactive learning for named entity recognition. In\nProceedings of the 2nd Workshop on Representa-\ntion Learning for NLP, pages 252–256, Vancouver,\nCanada. Association for Computational Linguistics.\nAditya Siddhant and Zachary C. Lipton. 2018. Deep\nBayesian active learning for natural language pro-\ncessing: Results of a large-scale empirical study.\nIn Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing, pages\n2904–2909, Brussels, Belgium. Association for Com-\nputational Linguistics.\nEdwin Simpson and Iryna Gurevych. 2018. Finding\nconvincing arguments using scalable Bayesian pref-\nerence learning. Transactions of the Association for\nComputational Linguistics, 6:357–371.\nRion Snow, Brendan O’Connor, Daniel Jurafsky, and\nAndrew Ng. 2008. Cheap and fast – but is it good?\nevaluating non-expert annotations for natural lan-\nguage tasks. In Proceedings of the 2008 Conference\non Empirical Methods in Natural Language Process-\ning, pages 254–263, Honolulu, Hawaii. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. Association for Computa-\ntional Linguistics.\n687\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4980–4991,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nDietrich Trautmann, Johannes Daxenberger, Christian\nStab, Hinrich Schütze, and Iryna Gurevych. 2020.\nFine-grained argument unit recognition and classi-\nﬁcation. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 34(05):9048–9056.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Advances in Neural Information Process-\ning Systems.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nRui Wang, Masao Utiyama, Lemao Liu, Kehai Chen,\nand Eiichiro Sumita. 2017. Instance weighting for\nneural machine translation domain adaptation. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1482–1488, Copenhagen, Denmark. Association for\nComputational Linguistics.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung. 2021.\nLanguage models are few-shot multilingual learners.\narXiv preprint arXiv:2109.07684.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A Chinese language understanding evalua-\ntion benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4762–4772, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32. Curran\nAssociates, Inc.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 189–196, Cambridge, Mas-\nsachusetts, USA. Association for Computational Lin-\nguistics.\nWenpeng Yin, Nazneen Fatema Rajani, Dragomir\nRadev, Richard Socher, and Caiming Xiong. 2020.\nUniversal natural language processing with limited\nannotations: Try few-shot textual entailment as a\nstart. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8229–8239, Online. Association for\nComputational Linguistics.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2225–2239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMan-Ching Yuen, Irwin King, and Kwong-Sak Leung.\n2011. A survey of crowdsourcing systems. In 2011\n688\nIEEE Third International Conference on Privacy, Se-\ncurity, Risk and Trust and 2011 IEEE Third Interna-\ntional Conference on Social Computing, pages 766–\n773.\nMike Zhang and Barbara Plank. 2021. Cartography ac-\ntive learning. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 395–\n406, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nYe Zhang, Matthew Lease, and Byron C. Wallace. 2017.\nActive discriminative text representation learning. In\nProceedings of the Thirty-First AAAI Conference on\nArtiﬁcial Intelligence, AAAI’17, page 3386–3392.\nAAAI Press.\nMengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh,\nand Hinrich Schütze. 2020a. Quantifying the con-\ntextualization of word representations with seman-\ntic class probing. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1219–1234, Online. Association for Computational\nLinguistics.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Schütze. 2020b. Masking as an efﬁcient alter-\nnative to ﬁnetuning for pretrained language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226–2241, Online. Association for Computa-\ntional Linguistics.\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and\nsoft prompting for multilingual models. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8547–8555,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nMengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli´c, Roi\nReichart, Anna Korhonen, and Hinrich Schütze. 2021.\nA closer look at few-shot crosslingual transfer: The\nchoice of shots matters. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5751–5767, Online. Association\nfor Computational Linguistics.\n689\nA Reproducibility Checklist\nA.1 Computing infrastructure\nWe use four Tesla V100 GPUs to prompt each of\nthe LMTurkers, and a single Tesla V100 GPU is\nused when ﬁnetuning the small model S.\nA.2 Datasets\nFor SST2, CoLA, and RTE, we use the\nofﬁcial datasets available on the benchmark\nwebsite gluebenchmark.com. We down-\nload SST5 dataset from nlp.stanford.edu/\nsentiment and AGNews from the link provided\nby Zhang et al. (2015).\nThe number of testing examples of each dataset\nis shown in Table 2. Note that for SST2, CoLA,\nand RTE, Gdev is sampled from the training set,\nand the dev set is used as the test set.\nCoLA SST5 RTE AGNews SST2\n1042 2210 277 7600 872\nTable 2: Number of testing examples.\nB Numerical Results\nTable 3 reports the numerical value of Figure 2.\nC Prompting Details\nFor each task, we list the ﬁve prompts employed to\nadapt a PLM to a LMTurker. “[v]” is a prompting\ntoken whose trainable embedding vector is ran-\ndomly initialized.\nFor SST5, we use following prompts:\n• “[v] x It is [MASK].”\n• “[v] x Such a [MASK] movie.”\n• “x [v] It is pretty [MASK].”\n• “It is [MASK] because x [v]”\n• “x So it is [MASK]. [v]”\nand the PLM picks a word from {“crap”, “bad”,\n“normal”, “good”, “perfect”}. to ﬁll the position of\n“[MASK]”. The mapping {“crap”→1, “bad”→2,\n“normal”→3, “good”→4, “perfect”→5 } is used\nto convert model predictions to numerical values.\nFor SST2, we use following prompts:\n• “[v] x It is [MASK].”\n• “[v] x Such a [MASK] movie.”\n• “x [v] It is pretty [MASK].”\n• “It is [MASK] because x [v]”\n• “x So it is [MASK]. [v]”\nand the PLM picks a word from {“bad”, “good”}\nto ﬁll the position of “[MASK]”. The mapping\n{“bad”→0, “good”→1} is used.\nFor AGNews, we use following prompts:\n• “[v] x It is about [MASK].”\n• “x [v] Topic: [MASK].”\n• “x [v] The text is about [MASK].”\n• “x Topic: [MASK]. [v]”\n• “x [v] [MASK].”\nand the PLM picks a word from {“world”,\n“sports”, “economy”, “technology”} to ﬁll the po-\nsition of “[MASK]”. The mapping {“world”→1,\n“sports”→2, “economy”→3, “technology”→4 }\nis used.\nFor CoLA, we use following prompts:\n• “[v] x It sounds [MASK].”\n• “[v] x The sentence is [MASK].”\n• “[v] x It is a [MASK] sentence.”\n• “x [v] [MASK].”\n• “[v] x [MASK].”\nand the PLM picks a word from {“wrong”, “ok”}\nto ﬁll the position of “[MASK]”. The mapping\n{“wrong”→0, “okay”→1} is used.\nFor RTE, we use following prompts:\n• “p Question: h? [v] Answer: [MASK].”\n• “p [SEP] h? [MASK]. [v]”\n• “p [SEP] h? [v] answer: [MASK].”\n• “p [SEP] In short h. [MASK]. [v]”\n• “[v] p [SEP] In short h. [MASK].”\nwhere p and h refer to premise and hypothesis. The\nPLM picks a word from {“No”, “Yes”} to ﬁll the\nposition of “[MASK]”. The mapping {“No”→0,\n“Yes”→1} is used.\n690\nG8 G16 G32\nWorkers S Workers S Workers S\n91.13±0.52 91.93±1.09 91.97±0.83\n91.63±0.68 93.08±0.62 91.70±1.78\nSST2 90.18±1.00 67.63±8.01 91.74±1.04 75.83±1.35 91.21±1.83 76.37±3.16\n90.83±0.58 90.79±0.47 91.13±0.24\n90.52±1.84 91.67±1.36 93.23±0.37\n41.37±1.55 45.16±2.13 45.91±0.96\n42.32±2.04 45.96±2.12 48.64±0.59\nSST5 40.57±2.70 28.47±1.61 46.70±0.93 34.97±1.51 50.53±0.94 33.47±2.79\n37.69±1.34 42.53±2.43 43.32±3.42\n38.05±2.60 42.96±0.69 45.72±1.43\n68.95±1.47 68.35±2.29 71.72±1.96\n54.99±3.76 57.64±3.23 58.48±3.59\nRTE 62.70±1.33 57.30±1.79 70.88±1.70 61.50±0.78 68.47±1.19 62.93±0.74\n50.42±2.07 58.60±1.62 59.33±4.72\n51.99±4.45 57.88±2.83 60.41±2.47\n75.39±5.25 83.06±0.83 84.92±0.28\n85.40±1.43 87.71±0.07 87.79±1.08\nAGNews 78.83±4.77 66.37±2.95 83.59±2.96 69.40±0.93 87.39±1.29 76.53±0.41\n85.07±1.09 87.69±0.04 87.17±0.67\n79.95±0.86 80.15±3.38 83.32±0.59\n0.14±1.43 11.81±7.82 19.88±3.30\n2.42±4.84 15.23±7.07 22.51±0.96\nCoLA 7.40±8.12 0.97±4.40 19.71±1.89 4.27±3.26 26.34±1.54 2.50±2.41\n9.91±7.98 17.14±2.48 18.15±0.63\n15.33±2.15 19.66±0.48 27.58±7.09\nTable 3: Few-shot performance of the ﬁve LMTurkers and the small modelS. Each experiment is repeated three\ntimes and we report mean and standard deviation.\nFigure 8: Weighting the training instances from LM-\nTurkers.\nD More Visualizations\nFigure 9 visualizes the performance of Swhen\ndifferent |G|and |B|are used.\nE Instance Weighting\nFollowing Wang et al. (2017), we associate each\nexample (x,ˆy,l) ∈Dj with weight 1-entropy(l)\nwhen computing the loss during training Sj. We\ncan interpret this weight as a measure of the cer-\ntainty of the LMTurkers ensemble.\nFigure 8 reports the performance of Swhen us-\ning instance weighting, however, the impacts are\nless noticeable.\n691\nFigure 9: Improving Swith active learning (blue), self training (orange), and LMTurk (green). Free markers at step\nzero show LMTurker performances; colors distinguish random seeds. Three acquisition functions are: Entropy (•),\nLeastConﬁdent (■), random sampling (/enc-36). At iteration j, each experiment is repeated three times; we show\nmean and standard deviation. We evaluate different |G|and |B|.\n692",
  "topic": "Crowdsourcing",
  "concepts": [
    {
      "name": "Crowdsourcing",
      "score": 0.9483279585838318
    },
    {
      "name": "Computer science",
      "score": 0.784338116645813
    },
    {
      "name": "Task (project management)",
      "score": 0.7440044283866882
    },
    {
      "name": "Shot (pellet)",
      "score": 0.7044133543968201
    },
    {
      "name": "Inference",
      "score": 0.6720900535583496
    },
    {
      "name": "Service (business)",
      "score": 0.5729108452796936
    },
    {
      "name": "One shot",
      "score": 0.5300353169441223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4696749150753021
    },
    {
      "name": "Language model",
      "score": 0.4493536949157715
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4172610640525818
    },
    {
      "name": "Machine learning",
      "score": 0.4132382571697235
    },
    {
      "name": "Data science",
      "score": 0.3689226508140564
    },
    {
      "name": "Human–computer interaction",
      "score": 0.36353832483291626
    },
    {
      "name": "World Wide Web",
      "score": 0.15790191292762756
    },
    {
      "name": "Engineering",
      "score": 0.1079966127872467
    },
    {
      "name": "Systems engineering",
      "score": 0.09231993556022644
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    }
  ],
  "cited_by": 7
}