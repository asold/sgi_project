{
    "title": "Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models",
    "url": "https://openalex.org/W2952127920",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2106762951",
            "name": "Fang Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286961604",
            "name": "Chung, Yu-An",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745591341",
            "name": "Glass, James",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2107740512",
        "https://openalex.org/W2964243274",
        "https://openalex.org/W2963975282",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2951418500",
        "https://openalex.org/W2963609956",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W2907262790",
        "https://openalex.org/W1570629387",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2107860279",
        "https://openalex.org/W2907916773",
        "https://openalex.org/W2130086727",
        "https://openalex.org/W2963300588",
        "https://openalex.org/W2129142580",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964281804",
        "https://openalex.org/W2901997113",
        "https://openalex.org/W2889028433",
        "https://openalex.org/W2091425152",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2926827382",
        "https://openalex.org/W854541894"
    ],
    "abstract": "Modern text-to-speech (TTS) systems are able to generate audio that sounds almost as natural as human speech. However, the bar of developing high-quality TTS systems remains high since a sizable set of studio-quality pairs is usually required. Compared to commercial data used to develop state-of-the-art systems, publicly available data are usually worse in terms of both quality and size. Audio generated by TTS systems trained on publicly available data tends to not only sound less natural, but also exhibits more background noise. In this work, we aim to lower TTS systems' reliance on high-quality data by providing them the textual knowledge extracted by deep pre-trained language models during training. In particular, we investigate the use of BERT to assist the training of Tacotron-2, a state of the art TTS consisting of an encoder and an attention-based decoder. BERT representations learned from large amounts of unlabeled text data are shown to contain very rich semantic and syntactic information about the input text, and have potential to be leveraged by a TTS system to compensate the lack of high-quality data. We incorporate BERT as a parallel branch to the Tacotron-2 encoder with its own attention head. For an input text, it is simultaneously passed into BERT and the Tacotron-2 encoder. The representations extracted by the two branches are concatenated and then fed to the decoder. As a preliminary study, although we have not found incorporating BERT into Tacotron-2 generates more natural or cleaner speech at a human-perceivable level, we observe improvements in other aspects such as the model is being significantly better at knowing when to stop decoding such that there is much less babbling at the end of the synthesized audio and faster convergence during training.",
    "full_text": "Towards Transfer Learning for End-to-End Speech Synthesis\nfrom Deep Pre-Trained Language Models\nWei Fang, Yu-An Chung, James Glass\nMIT Computer Science and Artiﬁcial Intelligence Laboratory\nCambridge, MA 02139, USA\n{weifang,andyyuan,glass}@mit.edu\nAbstract\nModern text-to-speech (TTS) systems are able to generate audio\nthat sounds almost as natural as human speech. However, the\nbar of developing high-quality TTS systems remains high since\na sizable set of studio-quality <text, audio> pairs is usually re-\nquired. Compared to commercial data used to develop state-\nof-the-art systems, publicly available data are usually worse\nin terms of both quality and size. Audio generated by TTS\nsystems trained on publicly available data tends to not only\nsound less natural, but also exhibits more background noise.\nIn this work, we aim to lower TTS systems’ reliance on high-\nquality data by providing them the textual knowledge extracted\nby deep pre-trained language models during training. In par-\nticular, we investigate the use of BERT to assist the training of\nTacotron-2, a state of the art TTS consisting of an encoder and\nan attention-based decoder. BERT representations learned from\nlarge amounts of unlabeled text data are shown to contain very\nrich semantic and syntactic information about the input text, and\nhave potential to be leveraged by a TTS system to compensate\nthe lack of high-quality data. We incorporate BERT as a par-\nallel branch to the Tacotron-2 encoder with its own attention\nhead. For an input text, it is simultaneously passed into BERT\nand the Tacotron-2 encoder. The representations extracted by\nthe two branches are concatenated and then fed to the decoder.\nAs a preliminary study, although we have not found incorpo-\nrating BERT into Tacotron-2 generates more natural or cleaner\nspeech at a human-perceivable level, we observe improvements\nin other aspects such as the model is being signiﬁcantly better\nat knowing when to stop decoding such that there is much less\nbabbling at the end of the synthesized audio and faster conver-\ngence during training.\nIndex Terms: speech synthesis, text-to-speech, transfer learn-\ning, pre-trained language models, semi-supervised learning\n1. Introduction\nEnd-to-end, deep learning-based approaches are causing a\nparadigm shift in the ﬁeld of text-to-speech (TTS) [1, 2, 3, 4, 5].\nUnlike traditional parametric TTS systems [6, 7], which typ-\nically pipeline a text front-end, a duration model, an acoustic\nfeature prediction model, and a vocoder, end-to-end neural TTS\napproaches integrate all these components into a single network\nand thus only require paired text and audio for training. This\nremoves the need of extensive domain expertise for designing\neach component individually, offering a much simpliﬁed voice\nbuilding pipeline. More importantly, they have been shown to\nbe capable of generating speech that sounds almost as natural\nas humans [4, 8].\nTTS systems that produce speech close to human quality,\nhowever, are usually trained on data collected by individual in-\nstitutions that are not publicly accessible. Due to their strictly-\ncontrolled conditions during the collection process, it is a com-\nmon belief that these internal data have higher quality than the\npublicly available ones. Such a belief is supported by the fact\nthat when training modern TTS models such as Tacotron-2 [4]\non publicly available dataset like LJ Speech [9], the synthesized\nspeech sounds less natural and exhibits more noise in the au-\ndio background than that produced by Tacotron-2 trained on\nGoogle’s internal data. Therefore, it is still unclear whether we\ncan achieve state-of-the-art TTS performance using only pub-\nlicly available data.\nRecently, deep pre-trained language models (LMs) [10, 11,\n12, 13] have shown to be capable of extracting textual repre-\nsentations that contain very rich syntactic and semantic infor-\nmation about the text sequences. By transferring the textual\nknowledge contained in these deep pre-trained LMs (e.g., by re-\nplacing the original text input with the extracted text features),\na simple downstream model is able to achieve state-of-the-art\nperformance on a wide range of natural language processing\ntasks such as natural language inference, sentiment analysis,\nand question answering, to name a few. These deep LMs are\nﬁrst trained on large amounts of unlabeled text data using self-\nsupervised objectives, and then ﬁne-tuned with task-speciﬁc\nlosses along with models for the downstream tasks.\nIn this work, we aim to leverage the textual knowledge con-\ntained in these deep pre-trained LMs to lower end-to-end TTS\nsystems’ reliance on high-quality data. Particularly, we investi-\ngate the use of BERT [13] for assisting the training of Tacotron-\n2 [4]. The backbone of Tacotron-2 is a sequence-to-sequence\nnetwork [14] with attention [15] that consists of an encoder and\na decoder. The goal of the encoder is to transform the input\ntext into robust sequential representations of text, which are\nthen consumed by the attention-based decoder for predicting\nthe spectral features. We enrich the textual information to be\nconsumed by the decoder by feeding the linguistic features ex-\ntracted by BERT from the same input text to the decoder as well\nalong with the original encoder representations.\nExisting work such as [16, 17] also attempts to make use\nof the textual knowledge learned from large text corpora to\nimprove TTS. However, they either focus on improving the\ndata efﬁciency of TTS training, i.e., to minimize the amount\nof paired text and audio for training, in a small-data regime,\nor the word embedding modules they use to provide textual\nknowledge have some limitations by nature, e.g., the word em-\nbeddings are trained based on very shallow language modeling\ntasks and are hence considered less powerful than the one we\nuse in this work.\narXiv:1906.07307v1  [cs.CL]  17 Jun 2019\nCharacter Embedding\nInput text\nConvolution Layers\nBi-directional LSTM\nLocation-sensitive\nAttention Autoregressive LSTM\nPre-Net\nPost-Net\nMel Spectrogram Stop\nWaveGlow V ocoder\nWaveform\nWordPiece Embedding\nTransformer layers\nLocation-sensitive\nAttention\nOriginal Tacotron2\nBERT Encoder\nEncoder Decoder\nFigure 1: Overall architecture of the proposed model. The bottom block depicts the original Tacotron-2, which is used as our base TTS\nsystem. Tacotron-2 consists of an encoder and an attention-based decoder. For an input text, the goal of the encoder is to transform\nit into robust textual representations that are then consumed by the attention-based decoder for predicting the spectral features. The\nupper part of the ﬁgure illustrates how we incorporate BERT into the TTS system. Speciﬁcally, BERT is fed with the same input text and\nextracts another sequence of textual representations of it. For each decoding time step, the decoder attends back to the two sequences\nof textual representations produced by the two branches (i.e., the Tacotron-2 encoder and BERT) with separate attention heads. The\ntwo generated attention context vectors are concatenated and fed to the decoder as the new input for decoding.\n2. Proposed Approach\nIn this section, we start with introducing BERT [13] and\nTacotron-2 [4]. We then present the proposed approach for in-\ncorporating BERT representations into the training of Tacotron-\n2. The proposed approach is illustrated in Figure 1.\n2.1. Tacotron-2\nTacotron-2 follows the sequence-to-sequence (seq2seq) with at-\ntention framework and functions as a spectral feature (e.g., mel\nspectrogram) prediction network. The predicted spectral fea-\ntures are then inverted by a separately trained vocoder into\ntime-domain waveforms. In this work, we modify the seq2seq\ncomponent by incorporating BERT as a parallel branch to the\nTacotron-2 encoder, and use WaveGlow [18] as the vocoder to\nsynthesize the audio waveforms.\nThe seq2seq component consists of two main building\nblocks: an encoder and an attention-based decoder. At a high\nlevel, the encoder takes a text sequence as input and transforms\nit into a sequence of textual representations. For each decoding\ntime step, the decoder conditions on these textual representa-\ntions and predicts a mel spectrogram frame.\nSpeciﬁcally, the encoder takes a character sequence as in-\nput, passes it through a stack of convolutional layers followed\nby a single-layer bidirectional LSTM, and the hidden states\nof the recurrent network are used as the encoded representa-\ntions. These representations will then be consumed by the\ndecoder. The decoder is an autoregressive LSTM with the\nlocation-sensitive attention [15] mechanism that summarizes\nthe encoded representations at each decoding time step. The\ninput to the decoder at each time step, which is the prediction\nfrom the previous step, is ﬁrst passed through a pre-net before\nbeing fed into the LSTM. Lastly, the output of the LSTM is pro-\ncessed by a convolutional post-net to predict the ﬁnal spectro-\ngram. The output of the LSTM is also fed into another network\nwith a sigmoid activation to determine when to stop decoding.\nDuring training, the mean squared error is calculated at the\nspectrogram prediction output, while the stop-token prediction\nis trained with binary cross-entropy loss. Additionally, teacher-\nforcing is used to train the recurrent decoder. During inference,\nthe ground truth targets are unknown. The decoder generates\nthe mel spectrogram in an autoregressive fashion as opposed to\nteacher-forcing. Generation is completed when the stop token\noutput exceeds a threshold of 0.5.\n2.2. BERT\nOur goal is to leverage rich textual knowledge contained in deep\npre-trained LMs to assist TTS training. To do so, we use BERT\nto transform the input text sequence into textual representations\nthat are in parallel to those extracted by the Tacotron-2 encoder,\nand provide both of them to the Tacotron-2 decoder. BERT is a\nTransformer-based [19] model trained on large amounts of text\nin an unsupervised manner. Below we brieﬂy summarize BERT.\nInput Representations. Unlike Tacotron-2 that takes char-\nacter sequence as input, the input to BERT is a sequence of\nsubword units that are usually referred to as WordPieces [20],\nwhere the tokenization is determined by a Byte-Pair Encoding\nprocess [21]. Since a Transformer is position-agnostic, it needs\npositional information encoded in their input, thus it uses learn-\nable positional embeddings for WordPiece tokens with length\nup to 512. Additionally, a special [CLS] token is added at the\nbeginning of each sequence so that the ﬁnal hidden state cor-\nresponding to this token represents the aggregate vector of the\nsequence.\nThe Transformer Model. The backbone of BERT is a multi-\nlayer bidirectional Transformer encoder, which consists of mul-\ntiple blocks of multi-head self-attention stacked together. In this\nwork we use the BERTBASE conﬁguration that contains 12\nattention blocks each with 12 attention heads and hidden size\nof 768.\nPre-Training. The BERT model is pre-trained using two un-\nsupervised objectives: masked language modeling, for which\nthe model has to predict a randomly masked out token, and next\nsentence prediction, where two sentences are packed as input to\nthe encoder and the aggregate vector is used to predict whether\nthey are adjacent sentences.\n2.3. Using BERT in Tacotron-2 Framework\nAs mentioned previously, the Tacotron-2 encoder aims to ex-\ntract robust sequential representations of the input text, the de-\ncoder then decodes the spectrogram by conditioning on these\ntextual representations. By drawing analogy to the traditional\nparametric TTS systems, the Tacotron-2 encoder can be viewed\nas the linguistic front-end and the decoder is similar to the sta-\ntistical acoustic prediction model.\nFrom this view, we propose to inject the textual informa-\ntion contained in the BERT representations to the Tacotron-2\ndecoder, so that it has access to the textual features from both\nthe Tacotron-2 encoder and BERT to make a spectral predic-\ntion. In practice, we feed the input text, which is ﬁrst tokenized\ninto WordPiece sequence, into BERT, and the representations\nfrom the last attention block is exposed to the decoder. The\ndecoder then attends back to both the Tacotron-2 encoder rep-\nresentations and BERT representations using separate location-\nsensitive attention heads in order to produce the corresponding\nattention context vectors. The two vectors are then concatenated\nbefore feeding into the autoregressive recurrent network.\nThere are other ways for incorporating external textual rep-\nresentations into the text front-end of a TTS system, for ex-\nample by concatenating the representations with the charac-\nter embedding sequence [16]. However, there usually exists a\nmismatch between the time resolutions of the two sequences\nand the existing solutions are not as intuitive as simply let-\nting the decoder attend to the representations extracted by both\nbranches.\nNote that although we use Tacotron-2 as the base TTS sys-\ntem in this work, our framework can be easily extended to other\nTTS systems as well.\n3. Experiments\n3.1. Dataset and Settings\nWe use LJ Speech [9], a public domain speech dataset consist-\ning of 13100 audio clips of a single speaker reading from non-\nﬁction books. The audio utterances vary in length from 1 to 10\nseconds, with a total length of about 24 hours of speech. They\nare encoded with 16-bit PCM with a sample rate of 22050Hz.\nThe dataset is split into train, validation, and test sets of 12500,\n100, and 500 clips respectively.\nWe use the implementation of Tacotron-2 by Nvidia 1 and\nkeep the default hyperparameters to train the baseline model.\n1https://github.com/NVIDIA/tacotron2\nTable 1: Performance of our model versus the baseline\nTacotron-2 model on several evaluation metrics.\nModel MCD 13 GPE FFE\nTacotron-2 21.88 0 .722 0 .740\nOurs 25.21 0 .720 0 .735\nFor our model we use the same conﬁguration for the Tacotron-2\ncomponent. The parameterization of the additional location-\nsensitive attention layer for attending to the BERT representa-\ntions is also kept the same. The entire model is trained end-\nto-end, thus the losses are also back-propagated into the BERT\nencoder to ﬁne-tune the textual representations. The Adam op-\ntimizer is used with a learning rate of 0.001 to learn the param-\neters.\n3.2. Evaluation Metrics\nTo measure performance, we use several metrics used in pre-\nvious works that correlate with voice quality and prosody. To\ncompare the generated audio to the reference audio, these met-\nrics are only calculated up to the length of the shorter audio\nsignal. The pitch and voicing metrics are computed using the\nYIN [22] pitch tracking algorithm.\nMean Cepstral Distortion (MCDK) [23]:\nMCDK = 1\nT\nT−1∑\nt=0\n√\nK∑\nk=1\n(ct,k −ˆct,k)2,\nwhere ct,k and ˆct,k are the k-th mel frequency cepstral coefﬁ-\ncient (MFCC) of the t-th frame from the reference and gener-\nated audio, respectively. The overall energy ct,0 is discarded in\nthis measure. We follow previous work and set K = 13.\nGross Pitch Error (GPE) [24]:\nGPE =\n∑\nt 1 [|pt −ˆpt|> 0.2pt]1 [vt]1 [ˆvt]∑\nt 1 [vt]1 [ˆvt] ,\nwhere pt and ˆpt are the pitch signals from the reference and gen-\nerated audio, vt and ˆvt are the voicing decisions from the refer-\nence and generated audio, and 1 denotes the indicator function.\nGPE measures the percentage of voiced frames that deviate by\nmore than 20% in the pitch signal of the generated audio com-\npared to the reference.\nF0 Frame Error (FFE) [25]:\nFFE = 1\nT\n∑\nt\n1 [|pt −ˆpt|> 0.2pt]1 [vt]1 [ˆvt] +1 [vt ̸= ˆvt].\nFollowing the deﬁnitions of GPE, FFE measures the percent-\nage of frames that either have a 20% pitch error or a differing\nvoicing decision between the generated and reference audio.\n3.3. Results and Observations\nTable 1 shows the performance of our model versus the baseline\nTacotron-2 model without the BERT encoder. All metrics cal-\nculate some form of error of the generated audio against the ref-\nerence audio, thus the lower the better. On the MCD 13 metric,\nour model performs a little worse than the baseline model, but\non the GPE and FFE metrics our model performs slightly better.\nOverall, there is not much difference in terms of these metrics\n0 20 40\n20\n30\n40\n50\nTraining steps (k)\nMCD13\nTacotron-2\nOurs\n0 10 20 30 40 50\n0.8\n0.9\nTraining steps (k)\nFFE\nTacotron-2\nOurs\nFigure 2: Plots for MCD 13 and FFE metrics as training steps increases.\n(a) Encoder Attention\n (b) BERT Attention\nFigure 3: Attention alignments for encoder and BERT encoder on a test phrase.\nat the end of training between both models. Additionally, we\ndo not ﬁnd a difference in naturalness for audios synthesized by\nthe two models at the end.\nHowever, if we visualize the performance metrics against\nthe number of training steps, as shown in Figure 2, we see that\nour model converges faster compared to the baseline Tacotron-\n2. This suggests that the addition of the learned representations\nof BERT may have helped our model learn quicker early on dur-\ning training. Even though the metrics converge to similar val-\nues at the end of training, our model still beneﬁts from the pre-\ntrained knowledge from BERT. We also discover a difference\nearly on in training in terms of the quality of the audio generated\nby the two models. The quality of the audio generated by our\nmodel is generally better than the baseline Tacotron-2, but the\ndiscrepancy disappears as training progresses. These results are\nto some extent similar to those reported in previous work [16],\nwhere semi-supervised learning with external text representa-\ntions and pre-training improves synthesis performance in low-\nresource settings but not in large-data regimes.\nFrom the learning curves, we can also see that the MCD 13\nmetric converges much quicker than the FFE metric and oscil-\nlates within a small range, and we do not observe much corre-\nlation of the MCD 13 metric with audio quality. We even ﬁnd\nthat models at early training stages that achieve relatively low\nMCD13 synthesize gibberish audio. Compared to MCD 13, we\nﬁnd that FFE offers a much better indication of audio quality.\nAdditionally, we observe a clear improvement in predicting\nwhen to stop generation with our model. The baseline Tacotron-\n2 often has trouble ending its decoding process, generating gib-\nberish noise at the end. Our model, on the other hand, almost\nnever exhibits this behavior.\nWe visualize the attention alignments of the decoder against\nboth attention layers of our model in Figure 3. From Figure 3a,\nwe see that the attention is a almost diagonal, meaning that the\ndecoder mostly focuses on the correct characters as the audio se-\nquence is generated. This pattern is also observed in the original\nTacotron [1], as well as the baseline Tacotron-2 model. From\nwhat we observe, this behavior is strongly correlated with syn-\nthesized audio quality, but unfortunately there isn’t a straight-\nforward method to quantify this behavior. On the other hand,\nin our model the attention layer that attends to the BERT repre-\nsentations, shown in Figure 3b, only has a rough diagonal pat-\ntern at the beginning of decoding. Its attention is more spread\nacross different time steps compared to the encoder attention\nalignments. We can also see from the ﬁgures that the values of\nthe attention are much lower for the BERT attention. Since the\nencoder attention patterns are similar, we hypothesize that the\nmodel still learns to map the text sequence to acoustic feature\nsequence mainly by the textual representations learned from the\nencoder. The BERT representations serve as additional infor-\nmation that the decoder uses to improve its prediction.\n4. Discussion and Future Work\nIn this work, we propose to exploit the textual representations\nfrom pre-trained deep LMs for improving end-to-end neural\nspeech synthesis. As a preliminary study, we have not found in-\ncorporating BERT into the Tacotron-2 framework signiﬁcantly\nimproves the quality of the synthesized audio; however, we do\nﬁnd that our approach improves the Tacotron-2 model in other\naspects such as faster convergence during training and the ﬁnal\nmodel is signiﬁcantly better at knowing when to stop decoding\nsuch that the synthesized audio has less babbling in the end.\nThis is only a preliminary work, and there is still a lot left\nto be studied. For instance, instead of utilizing only knowl-\nedge from unlabeled text data, we can also make use of large\namounts of unlabeled speech corpora [26] to assist the decoder\nin learning the acoustic representations and alignments. A po-\ntential method could be to initialize Tacotron decoder with a\npre-trained auto-regressive predictive coding model [27].\n5. References\n[1] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. Weiss, N. Jaitly,\nZ. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. Le, Y . Agiomyrgian-\nnakis, R. Clark, and R. Saurous, “Tacotron: Towards end-to-end\nspeech synthesis,” in Interspeech, 2017.\n[2] S. ¨O. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky,\nY . Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta, and\nM. Shoeybi, “Deep voice: Real-time neural text-to-speech,” in\nICML, 2017.\n[3] J. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner,\nA. Courville, and Y . Bengio, “Char2wav: End-to-end speech syn-\nthesis,” in ICLR Workshop, 2017.\n[4] J. Shen, R. Pang, R. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Skerrv-Ryan, R. Saurous,\nY . Agiomyrgiannakis, and Y . Wu, “Natural TTS synthesis by con-\nditioning wavenet on mel spectrogram predictions,” in ICASSP,\n2018.\n[5] W. Ping, K. Peng, and J. Chen, “Clarinet: Parallel wave generation\nin end-to-end text-to-speech,” inICLR, 2019.\n[6] H. Zen, K. Tokuda, and A. Black, “Statistical parametric speech\nsynthesis,” Speech Communication , vol. 51, no. 11, pp. 1039–\n1064, 2009.\n[7] P. Taylor, Text-to-speech synthesis. Cambridge University Press,\n2009.\n[8] N. Li, S. Liu, Y . Liu, S. Zhao, M. Liu, and M. Zhou, “Neural\nspeech synthesis with transformer network,” inAAAI, 2019.\n[9] K. Ito, “The LJ speech dataset,” https://keithito.com/\nLJ-Speech-Dataset/, 2017.\n[10] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,”\nin NAACL-HLT, 2018.\n[11] J. Howard and S. Ruder, “Universal language model ﬁne-tuning\nfor text classiﬁcation,” in ACL, 2018.\n[12] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Im-\nproving language understanding by generative pre-training,” Ope-\nnAI, Tech. Rep., 2018.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL-HLT, 2019.\n[14] I. Sutskever, O. Vinyals, and Q. Le, “Sequence to sequence learn-\ning with neural networks,” inNIPS, 2014.\n[15] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Bengio,\n“Attention-based models for speech recognition,” inNIPS, 2015.\n[16] Y .-A. Chung, Y . Wang, W.-N. Hsu, Y . Zhang, and R. Skerry-Ryan,\n“Semi-supervised training for improving data efﬁciency in end-to-\nend speech synthesis,” in ICASSP, 2019.\n[17] H. Ming, L. He, H. Guo, and F. Soong, “Feature reinforcement\nwith word embedding and parsing information in neural TTS,”\narXiv preprint arXiv:1901.00707, 2019.\n[18] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A ﬂow-based\ngenerative network for speech synthesis,” inICASSP, 2019.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” in NIPS, 2017.\n[20] Y . Wu, M. Schuster, Z. Chen, Q. Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, . Kaiser, S. Gouws, Y . Kato, T. Kudo,\nH. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young,\nJ. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean, “Google’s neural machine translation\nsystem: Bridging the gap between human and machine transla-\ntion,” arXiv preprint arXiv:1609.08144, 2016.\n[21] R. Sennrich, B. Haddow, and A. Birch, “Neural machine transla-\ntion of rare words with subword units,” inACL, 2016.\n[22] A. De Cheveign ´e and H. Kawahara, “YIN, a fundamental fre-\nquency estimator for speech and music,” The Journal of the\nAcoustical Society of America , vol. 111, no. 4, pp. 1917–1930,\n2002.\n[23] R. Kubichek, “Mel-cepstral distance measure for objective speech\nquality assessment,” in PacRim, 1993.\n[24] T. Nakatani, S. Amano, T. Irino, K. Ishizuka, and T. Kondo, “A\nmethod for fundamental frequency estimation and voicing deci-\nsion: Application to infant utterances recorded in real acoustical\nenvironments,” Speech Communication, vol. 50, no. 3, pp. 203–\n214, 2008.\n[25] W. Chu and A. Alwan, “Reducing f0 frame error of f0 tracking\nalgorithms under noisy conditions with an unvoiced/voiced clas-\nsiﬁcation frontend,” in ICASSP, 2009.\n[26] W.-N. Hsu, Y . Zhang, R. Weiss, Y .-A. Chung, Y . Wang, Y . Wu,\nand J. Glass, “Disentangling correlated speaker and noise for\nspeech synthesis via data augmentation and adversarial factoriza-\ntion,” in ICASSP, 2019.\n[27] Y .-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised\nautoregressive model for speech representation learning,” arXiv\npreprint arXiv:1904.03240, 2019."
}