{
  "title": "Evaluating Object Hallucination in Large Vision-Language Models",
  "url": "https://openalex.org/W4389523832",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101139584",
      "name": "Yifan Li",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2135553912",
      "name": "Yifan Du",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2102854352",
      "name": "Kun Zhou",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2132144181",
      "name": "Jinpeng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098250721",
      "name": "Zhao Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904565150",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W4296566403",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4365606129",
    "https://openalex.org/W2295158492",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W3158419028",
    "https://openalex.org/W4376122449",
    "https://openalex.org/W3202965415",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4382491206",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4312810944",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4375869762",
    "https://openalex.org/W3104279398",
    "https://openalex.org/W4367628410",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4386566667",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W2963890019"
  ],
  "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently proposed by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that they suffer from object hallucinations, i.e., they tend to generate objects inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issues. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently appear in the visual instructions or co-occur with the image objects are obviously prone to be hallucinated by LVLMs. Besides, we further design a polling-based query method called POPE for better evaluation of object hallucination. Experiment results show that our POPE can evaluate object hallucination in a more stable and flexible way.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292–305\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEvaluating Object Hallucination in Large Vision-Language Models\nYifan Li1,3∗, Yifan Du1,3∗, Kun Zhou2∗, Jinpeng Wang 4,\nWayne Xin Zhao2,3†and Ji-Rong Wen1, 2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China.\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n4Meituan Group\n{liyifan0925, yifandu1999, batmanfly}@gmail.com,\nfrancis_kun_zhou@163.com, wangjinpeng04@meituan.com, jrwen@ruc.edu.cn\nAbstract\nInspired by the superior language abilities of\nlarge language models (LLM), large vision-\nlanguage models (LVLM) have been recently\nproposed by integrating powerful LLMs for\nimproving the performance on complex multi-\nmodal tasks. Despite the promising progress\non LVLMs, we find that they suffer from ob-\nject hallucinations, i.e., they tend to generate\nobjects inconsistent with the target images in\nthe descriptions. To investigate it, this work\npresents the first systematic study on object\nhallucination of LVLMs. We conduct the eval-\nuation experiments on several representative\nLVLMs, and show that they mostly suffer from\nsevere object hallucination issues. We further\ndiscuss that the visual instructions may influ-\nence the hallucination, and find that: objects\nthat frequently appear in the visual instructions\nor co-occur with the image objects are obvi-\nously prone to be hallucinated by LVLMs. Be-\nsides, we further design a polling-based query\nmethod called POPE for better evaluation of\nobject hallucination. Experiment results show\nthat our POPE can evaluate object hallucination\nin a more stable and flexible way.\n1 Introduction\nLarge language models (LLMs) (Zhao et al., 2023)\nhave shown remarkable abilities to solve various\ncomplex tasks by following human instructions in\na zero-shot manner. The success of LLMs drives\nthe researchers to devise more powerful multi-\nmodal models based on the superior capacity of\nLLMs, to enhance the understanding of visual se-\nmantics (Alayrac et al., 2022; Li et al., 2023b). As\nan exemplified work, GPT-4 (OpenAI, 2023) has\nexhibited the exciting performance of LLMs on\nmultimodal tasks and scenarios.\nFollowing this line of research, a surge of stud-\nies (Zhu et al., 2023; Gao et al., 2023; Li et al.,\n*Equal contribution.\n†Corresponding author.\n2023a) have been proposed to enhance the vision-\nlanguage pre-trained model (VLPM) (Gan et al.,\n2022) by incorporating powerful LLMs (Touvron\net al., 2023; Chiang et al., 2023), which are called\nlarge vision-language model (LVLM). Typically, ex-\nisting work reuses the visual encoder in VLPMs to\nhandle image data, while replacing the original lan-\nguage encoder with LLMs. After vision-language\npre-training (Alayrac et al., 2022; Li et al., 2022b)\nand visual instruction tuning (Liu et al., 2023),\nLVLMs can fulfill complex tasks according to hu-\nman instructions, demonstrating strong capacities\nin solving various vision-language tasks, e.g., im-\nage captioning (Ordonez et al., 2011; Hodosh et al.,\n2015; Sharma et al., 2018; Agrawal et al., 2019)\nand visual question answering (Antol et al., 2015a;\nZhang et al., 2016; Goyal et al., 2017).\nDespite the success of LVLMs, previous work\nhas revealed that their main components, i.e.,\nLLMs and VLPMs, both suffer from hallucination.\nEspecially, LLMs tend to hallucinate unintended\ntext (Huang et al., 2021; Bang et al., 2023), and\nVLPMs might generate nonexistent objects in the\nimage (Biten et al., 2022) (termed as object hal-\nlucination). It is generally believed that the hal-\nlucination would degrade the model performance\nand greatly harm the user experiences in real-world\napplications (MacLeod et al., 2017; Ji et al., 2022).\nTherefore, it is natural to ask the question: does\nhallucination still exist in LVLMs? In this paper, we\nsystematically evaluate the issue of object halluci-\nnation in existing LVLMs, which refers to generat-\ning contents that are inconsistent with ground-truth\nobjects in the given image.\nTo conduct our study, we first use the CHAIR\n(Caption Hallucination Assessment with Image Rel-\nevance) metric (Rohrbach et al., 2018), and ex-\namine the hallucination degree of several repre-\nsentative LVLMs on the MSCOCO dataset. Our\npreliminary experiments (Table 1) show that most\nof LVLMs severely suffer from object hallucina-\n292\nThe image features a table with a variety\nof food items displayed in bowls. There\nare two bowls of food, one containing a\nmix of vegetables, such as broccoli and\ncarrots,a n d the other containing meat.\nThe bowl with vegetablesis placed closer\nto the front, while the meat bowl is\nsituated behind it. In addition to the main\ndishes, there is an apple placed on the\ntable, adding a touch of fruit to the meal. A\nbottle can also be seen on the table,\npossibly containing a beverage or\ncondiment. The table is neatly arranged,\nshowcasing the different food items in an\nappetizing manner.\nProvide a detailed description of the given\nimage.\nInstruction-based evaluation\nYes, there is a pear in the image.\nIs there a pear in the image?\nAdversarial settings \nPOPE\nYes, there is a knife in the image.\nIs there a knife in the image?\nPopular settings \nYes, there is a bottle in the image.\nIs there a bottle in the image?\nRandom settings \nFigure 1: Cases of object hallucination in LVLMs. Bold objects are ground-truth objects in the annotations and\nred objects are hallucinated objects by LVLMs. The left case is from the traditional instruction-based evaluation\nmethod, and the right cases are from three variants of POPE.\ntion, and are even more prone to hallucinate than\nsmall vision-language models. Besides, we find\nthat the existing object hallucination evaluation\nmethod may not be best suited for LVLMs and\nfurther propose a Polling-based Object Probing\nEvaluation (POPE) method. The basic idea is to\nconvert the evaluation of hallucination into a bi-\nnary classification task by prompting LVLMs with\nsimple Yes-or-No short questions about the prob-\ning objects (e.g., Is there a car in the image?). We\nshow that such a method is more stable and flexible.\nBesides, by using different object sampling strate-\ngies, we validate that existing LVLMs are prone\nto hallucinate objects which frequently appear or\nco-occur in the visual instruction dataset.\nOur main contributions are as follows: (1) We\nconduct an empirical study on object hallucination\nfor several representative LVLMs and find that they\nare highly affected by object hallucination. (2) We\ndiscuss the potential reasons behind this promblem,\ne.g., LVLMs tend to generate frequently appearing\nor co-occurring objects in the instruction corpora.\n(3) We propose an object hallucination evaluation\napproach called POPE, which is more stable and\ncan be easily extended to unannotated datasets.\n2 Background\n2.1 Large Vision-Language Model\nSince LLMs have been shown to be general task\nsolvers in a zero-shot/few-shot manner, a number\nof studies are devoted to improving VLPM by inte-\ngrating powerful LLMs for more accurate language\nunderstanding and generation (Zhu et al., 2023; Liu\net al., 2023; Dai et al., 2023a). In this paper, we re-\nfer to the enhanced VLPMs with the integration of\nLLMs as Large Vision-Language Models (LVLM).\nGenerally speaking, an LVLM consists of a vi-\nsion encoder, a language encoder ( i.e., an LLM),\nand a cross-modal alignment network. The training\nof LVLMs is generally composed of three major\nsteps. First, a vision encoder and a language en-\ncoder are pre-trained on large-scale unimodal data\n(i.e., image and text data, respectively). Second,\nthese two encoders are aligned through image-text\nalignment pre-training, which enables the LLM\nto generate a meaningful caption for a given im-\nage. Third, the aligned model is further fine-tuned\non image-text instructions, so that it can generate\nsatisfactory answers w.r.t. to a natural language\nquestion regarding a specific image. Note that in\nthe second and third steps, we can optionally fine-\ntune different components instead of performing\nfull-parameter fine-tuning.\nOnce the visual encoder and the LLM are well\naligned, the derived LVLM can demonstrate a su-\nperior visual understanding ability. It can not only\ngrasp the visual semantics of objects in the image,\nbut also deeply understand the linguistic seman-\ntics for these objects by leveraging the parametric\nknowledge in the LLM. Further, the LVLM can per-\nform complex reasoning over the related concepts\nabout these objects, thus achieving an improved\nperformance on a variety of multimodal tasks, e.g.,\nvisual question answering (VQA).\n2.2 Object Hallucination\nAlthough LVLMs are powerful in solving vision-\nlanguage tasks, they also suffer from the issue of\nobject hallucination as VLPMs. In the literature\n293\nof computer vision field (Rohrbach et al., 2018;\nBiten et al., 2022), object hallucination refers that\nthe model generating descriptions or captions that\ncontain objects which are inconsistent with or even\nabsent from the target image. In general, object\nhallucination can be defined at different seman-\ntic levels. The most straightforward way is to\ndefine it over the object level, while more fine-\ngrained definitions might be concerned with the\nattributes or characteristics of objects. In this work,\nwe focus on coarse-grained object hallucinations\nin the model-generated captions and leave fine-\ngrained object hallucinations such as the number,\nattributes, and positions of the object for future\nwork. We present an example of object halluci-\nnation in Figure 1, where the hallucinated object\n“meat bowl”,“bottle”, “beverage”, “condiment”\nare generated by the underlying LVLMs.\nThe hallucination phenomenon hinders the safe\nuse of LVLMs in real-world deployment, as it may\nresult in unexpected consequences caused by these\nhallucinated objects (MacLeod et al., 2017). For\nexample, due to an incorrect understanding of the\nexternal environment, an autonomous driving sys-\ntem would make wrong decisions when encounter-\ning unexpected events, which might lead to serious\nsafety issues. In order to mitigate these issues, this\nwork aims to study how object hallucination exists\nin LVLMs from an evaluation perspective.\n3 Object Hallucination in LVLMs\nIn this section, we evaluate the object hallucina-\ntion problem in popular LVLMs using an existing\nmethod. We first introduce the evaluation settings\nand then analyze the experimental results.\n3.1 Evaluation Settings\nCaption Hallucination Assessment with Image Rel-\nevance (CHAIR) (Rohrbach et al., 2018) is a pop-\nular metric for evaluating object hallucination in\nimage captioning tasks. Given the ground truth\nobjects in the image, CHAIR calculates the propor-\ntion of objects that appear in the caption but not\nthe image. Existing work commonly adopts its two\nvariants, i.e., CHAIRI and CHAIRS, which evalu-\nate the hallucination degree at the object instance\nlevel and sentence level respectively. They can be\nformulated as:\nCHAIRI = |{hallucinated objects}|\n|{all mentioned objects}|, (1)\nCHAIRS = |{captions with hallucinated objects}|\n|{all captions}| . (2)\nI Model CHAIRI CHAIRS Len\n-\nOSCARBase 7.1 13.0 -\nVinVLLarge 5.5 10.5 -\nOFALarge 4.7 8.9 -\nBLIPLarge 4.7 8.8 -\nI1\nmPLUG-Owl 14.8 25.4 35.8\nLLaV A 10.5 32.7 64.3\nMultiModal-GPT 11.1 15.0 11.6\nMiniGPT-4 6.7 9.5 24.7\nInstructBLIP 2.6 3.7 8.5\nI2\nmPLUG-Owl 30.2 76.8 98.5\nLLaV A 18.8 62.7 90.7\nMultiModal-GPT 18.2 36.2 45.7\nMiniGPT-4 9.2 31.5 116.2\nInstructBLIP 2.5 3.4 7.5\nTable 1: Results of CHAIR on VLPMs and LVLMs. I1\ndenotes “Generate a short caption of the image” and I2\ndenotes “Provide a brief description of the given image”.\nLen refers to the average length of generated captions.\nThe results of VLPMs (OSCAR, VinVL, BLIP, and\nOFA) are collected from Dai et al. (2023b). The best\nresults in each block are denoted in bold.\nWe select five recently released LVLMs, i.e.,\nmPLUG-Owl (Ye et al., 2023), LLaV A (Liu\net al., 2023), Multimodal-GPT (Gong et al.,\n2023), MiniGPT-4 (Zhu et al., 2023) and Instruct-\nBLIP (Dai et al., 2023a) and prompt them with\nfollowing instructions to generate captions about\nimages in MSCOCO (Lin et al., 2014):\n•I1: Generate a short caption of the image.\n•I2: Provide a brief description of the given\nimage.\nThen, we calculate CHAIR on these captions.\nWe leave more details about the introduction to the\ndataset and evaluated models in Appendix A.\n3.2 Evaluation Results\nSeverity of Hallucinations. As the evaluation re-\nsults illustrated in Table 1, most instruction-tuned\nLVLMs suffer from the object hallucination prob-\nlem, even more serious than small models, e.g.,\nLLaV A (32.7)v.s. OSCARbase (13.0) on CHAIRS\nusing Instruction 1. It indicates that object hallu-\ncination is an important problem for LVLMs and\ndeserves to be concerned about. As a comparison,\nInstructBLIP hallucinates less than other LVLMs.\nA possible reason is that its visual instructions are\ncollected from a wide variety of publicly avail-\nable datasets, which are relatively short. In con-\ntrast, other LVLMs mostly employ the visual in-\nstructions generated by unimodal LLMs (Liu et al.,\n2023). Such synthetic visual instructions are gener-\n294\nperson dining table chair car book bottle cup cat horse toilet0\n100\n200\n300\n400\n500\n600Hallucination times\nMiniGPT-4\nLLaVA\nMultiModal-GPT\nmPLUG-Owl\n(a) Hallucination times of top ten frequently appearing objects, whose frequencies decrease from right to left.\nperson chair cup bowl bottle laptop knife book fork vase0\n10\n20\n30\n40\n50\n60Hallucination times\nMiniGPT-4\nLLaVA\nMultiModal-GPT\nmPLUG-Owl\n(b) Hallucination times of top ten objects co-occurring with “dining table”, whose frequencies decrease from right to left.\nFigure 2: Hallucination times of frequently appearing/co-occurring objects in MSCOCO.\nally longer and more informative, but may involve\nunexpected descriptive information (hallucination\ninherent from LLMs) that is inconsistent with the\nimage, which could mislead LVLMs.\nDisadvantages of CHAIR. As Table 1 shows,\nthe evaluation results can be affected by other fac-\ntors, e.g., instruction designs and the length of cap-\ntions. Specifically, although the adopted two in-\nstructions have similar semantic meanings, LVLMs\nprompted by Instruction 2 can even result in dou-\nbled values of CHAIR metrics compared with those\nprompted by Instruction 1, and the performance\norder of some LVLMs also changes (e.g., CHAIRI\nvalues of LLaV A and MultiModal-GPT). It indi-\ncates the instability of the CHAIR metric when\ndifferent instructions are employed. Besides, as\nCHAIR requires to examine whether the mentioned\nobjects are hallucinated in the generated caption, it\nneeds complex human-crafted parsing rules to per-\nform exact matching, which has not been adapted\nto the special generation styles of LVLMs and may\nlead to misclassification errors.\nThus, it is necessary to consider a more suitable\nmethod that can stably and conveniently evaluate\nthe object hallucination problem in LVLMs.\n4 Influence of Instruction Data on Object\nHallucination\nConsidering their impressive performance on com-\nplex vision-language tasks (Chen et al., 2023; Bai\net al., 2023; Li et al., 2023a), it is counter-intuitive\nthat the hallucination problem of LVLMs is so\nsevere. Since smaller VLPMs suffer less from\nobject hallucination, it is possible that the visual\ninstruction-tuning process of LVLMs exacerbates\nobject hallucination. In this section, we investigate\nthe influence of the visual instruction data. We\nfirst make two basic hypotheses in Section 4.1 and\nthen conduct qualitative and quantitative analysis\nto verify them in Section 4.2 and Section 4.3.\n4.1 Hypotheses\nAs the visual instruction datasets of these LVLMs\nare mostly constructed based on MSCOCO (Lin\net al., 2014), they generally share a similar unbal-\nanced object distribution where top frequent ob-\njects occupy a major part of the dataset. After be-\ning fine-tuned on them, LVLMs may also be prone\nto generate (or hallucinate) frequently appearing\nobjects in MSCOCO. Additionally, the presence\nof frequently co-occurring object groups (e.g., lap-\ntop, mouse and keyboard) may also contribute to\n295\nModel\nHRA HRC(dining table)\n@10 @20 @30 @10 @20 @30\nmPLUG-Owl 0.5455 0.6591 0.7533 0.6608 0.7926 0.8253\nLLaV A 0.4620 0.5911 0.6796 0.5628 0.7329 0.8595\nMultiModal-GPT 0.4152 0.5399 0.6743 0.5742 0.7849 0.8961\nMiniGPT-4 0.4610 0.5758 0.7207 0.5600 0.6980 0.9145\nTable 2: Results on MSCOCO that quantify the correlations between the appearing/co-occurring frequency of\nobjects and the hallucination times of LVLMs.\nobject hallucination. LVLMs can be elicited by the\nexisting objects in the image to hallucinate other\nobjects that frequently co-occur with them. There-\nfore, we hypothesize that (1) LVLMs are prone to\nhallucinate frequently appearing objects in the vi-\nsual instruction datasets; (2) LVLMs are prone to\nhallucinate objects that frequently co-occur with\nground-truth objects in the image. We conduct qual-\nitative and quantitative analyses in the following\nparts to verify them.\n4.2 Qualitative Analysis\nWe first qualitatively analyze the correlation be-\ntween the appearance frequency and hallucination.\nFor the first hypothesis, we plot a bar chart be-\ntween the top ten frequently appearing objects in\nMSCOCO and their hallucination times in the vali-\ndation set of MSCOCO; for the second hypothesis,\nwe select the top ten frequently co-occurring ob-\njects with “dining table” and also plot a bar chart\nto show their hallucination times across images that\nreally contain “dining table”. We show the re-\nsults of MiniGPT-4, LLaV A, MultiModal-GPT and\nmPLUG-Owl in Figure 2. Obviously, with the\ndecreasing of the occurrence frequency of objects\n(from right to left), there is a notable decrease in the\nhallucination times for all four LVLMs. It reveals\nthat the frequently appearing and co-occurring ob-\njects in the visual instruction dataset are indeed\nmore likely to be hallucinated by LVLMs. To bet-\nter support our results, we also list the full statistics\nof all 80 COCO objects in Appendix B.\n4.3 Quantitative Analysis\nTo further consolidate the above findings, we em-\nploy the top- k hit ratio (HR@ k) to measure the\nconsistency between the appearance frequency and\nhallucination times of objects, which is defined as:\nHRA@k = 1\nn\nn∑\ni=1\nHit@k(i)\nHallucinated(i), (3)\nHRC@k(o) = 1\nm\nm∑\ni=1\nHit@k(i, o)\nHallucinated(i), (4)\nwhere HR A and HR C quantify the correlations\nbetween hallucination times and appearing and\nco-occurring frequency respectively. n is the to-\ntal number of images, Hallucinated(i) denotes the\nnumber of hallucinated objects in the i-th example,\nHit@k(i) denotes the number of top-k frequently\nappearing MSCOCO objects in Hallucinated(i),\nand Hit@k(i, o) denotes the number of top-k fre-\nquently co-occurring objects with the probing ob-\nject o in Hallucinated(i). Therefore, HR@ k can\nreflect the proportion of top-k frequently appearing\nor co-occurring objects in all hallucinated objects.\nWe present the HRA and HRC(dining table)\nof top 30 objects in Table 2 and leave\nHRC@(chair) and HRC@(car) in Appendix C.\nThe HRA@10 and HR C@10(dining table) of\nall LVLMs are near 0.5 and 0.6, respectively. It\nindicates that, on average, approximately half of\nthe hallucinated objects in each image belong to the\ntop 10 frequently appearing COCO objects, while\nmore than half are among the top 10 frequently co-\noccurring objects with the objects already present\nin the image. When we broaden our observation\nto the top 30 objects, this proportion continues to\nincrease. These findings further verify that LVLMs\nmostly hallucinate common objects in the visual\ninstruction data and inspire us to design three sam-\npling strategies in our evaluation pipeline.\n5 POPE\nIn this section, we devise Polling-based Object\nProbing Evaluation (POPE), a simple yet effective\napproach for evaluating hallucination in LVLMs.\nWe first provide an overview of POPE, and then\nevaluate the representative LVLMs with POPE. Fi-\nnally, we discuss the stability and scalability of our\nmethod, and also analyze the impact of hallucina-\n296\nperson, chair, umbrella,  \nsand, sea, …\nRandom: dog, apple, … \nPopular: table, knife, …\nAdversarial: surfboard, …\nsampling\nHuman annotation\nQ: Is there a person in the image?\nA: Yes.\nQ: Is there a chair in the image?\nA: Yes.\nQ: Is there an umbrella in the image?\nA: Yes.\nPolling questions\nQ: Is there a dog in the image?\nA: No.\nQ: Is there a table in the image?\nA: No.\nQ: Is there a surfboard in the image?\nA:No.\nSegmentation\nGround-truth objects\nNegative\nNonexistent objects\nSEEM\nAutomatic annotation\nFigure 3: Overview of the POPE pipeline. Given an input image, POPE first extracts ground-truth objects in\nthe image either from human annotations or with the help of automatic segmentation tools like SEEM. Then,\nPOPE conducts negative sampling for nonexistent objects in the image under Random/Popular/Adversarial settings.\nFinally, the ground-truth objects and nonexistent objects are formulated into question templates to poll LVLMs.\ntion on VQA task.\n5.1 Overview of POPE\nIn the empirical results of Section 3, we have re-\nvealed the severity of the object hallucination prob-\nlem in LVLMs and highlighted the limitations of\nthe existing evaluation method, e.g., sensitive to\ninstructions and biased to short captions. Besides,\nexisting methods mostly rely on parsing the gener-\nated captions to extract the predicted objects, which\nusually require human-crafted complex rules and\nare still inevitable to omit or misclassify objects.\nTherefore, we consider devising a more suitable\nmethod for the stable, fair and flexible object hal-\nlucination evaluation of LVLMs, namely polling-\nbased object probing evaluation (POPE). Specif-\nically, POPE formulates the evaluation of object\nhallucination as a binary classification task that\nprompts LVLMs to output “Yes” or “No”,e.g., “Is\nthere a chair in the image?”. In this way, by sam-\npling objects that LVLMs are prone to hallucinate,\nwe can construct a set of hard questions to poll\nLVLMs. As standard answers to these questions\nare just “Yes” or “No”, we can easily identify them\nwithout complex parsing rules, and avoid the influ-\nence of instruction designs and caption length, thus\nguaranteeing stability, fairness and flexibility.\nDefinition. Given an image caption dataset,\nPOPE focuses on constructing a set of triples, each\nof which consists of an image, multiple questions\nand their answers (“Yes” or “No”). The formulated\ndefinition of a triple can be described as:\n⟨x, {q(oi), ai}l\ni=1⟩, (5)\nwhere x denotes the image, q(oi) is the question\nprobing oi based on a template “Is there a/an\n<object> in the image?”, oi is the i-th object to be\nprobed, ai is the answer to the question (“Yes” or\n“No”) and l denotes the number of polling questions\nper image. oi can be obtained either from annota-\ntions or the results of automatic segmentation tools\nlike SEEM (Zou et al., 2023). We set the ratio be-\ntween ground-truth and nonexistent objects as 1:1\nfor label balance. After constructing the evaluation\ntriples, we can directly poll LVLMs with them and\ncollect the predicted answers.\nPipeline. The whole POPE pipeline is presented\nin Figure 3. After obtaining objects in the image,\nwe can start to building polling questions. Ques-\ntions whose answers are “Yes” can be directly built\nusing ground-truth objects, while questions with\nthe answer “No” can be built by sampling from\nnegative objects. Therefore, by devising differ-\nent sampling strategies, we can validate whether\nLVLMs are prone to hallucinate specific objects,\ne.g., frequently appearing or co-occurring objects\ndiscussed in Section 4. Thus, we devise the follow-\ning three sampling strategies:\n•Random Sampling: we randomly sample the\nobjects that do not exist in the image.\n•Popular Sampling: we select the top-k most\nfrequent objects in the whole image dastaset that\ndo not exist in the current image, where k = l\n2 .\n•Adversarial Sampling: we first rank all ob-\njects according to their co-occurring frequencies\nwith the ground-truth objects, and then select the\ntop-k frequent ones that do not exist in the image.\n297\nDataset POPE Model Accuracy Precision Recall F1 Score Yes (%)\nMSCOCO\nRandom\nmPLUG-Owl 53.30 51.71 99.53 68.06 96.23\nLLaV A 54.43 52.32 99.80 68.65 95.37\nMultiModal-GPT 50.03 50.02 100.00 66.68 99.97\nMiniGPT-4 77.83 75.38 82.67 78.86 54.83\nInstructBLIP 88.73 85.08 93.93 89.29 55.20\nPopular\nmPLUG-Owl 50.63 50.32 99.27 66.79 98.63\nLLaV A 52.43 51.25 99.80 67.72 97.37\nMultiModal-GPT 50.00 50.00 100.00 66.67 100.00\nMiniGPT-4 68.30 64.27 82.40 72.21 64.10\nInstructBLIP 81.37 75.07 93.93 83.45 62.57\nAdversarial\nmPLUG-Owl 50.67 50.34 99.33 66.82 98.67\nLLaV A 50.77 50.39 99.87 66.98 99.10\nMultiModal-GPT 50.00 50.00 100.00 66.67 100.00\nMiniGPT-4 66.60 62.45 83.27 71.37 66.67\nInstructBLIP 74.37 67.67 93.33 78.45 68.97\nTable 3: Results of LVLMs under three evaluation settings of POPE on the validation set of MSCOCO. Yes denotes\nthe proportion of answering “Yes” to the given question. The best results in each block are denoted in bold.\nUnder the above three settings, we can build the\nevaluation questions of different difficulty levels.\nWe evaluate previously mentioned LVLMs on them\nwith the following metrics.\nMetrics. We adopt Accuracy, Precision, Recall\nand F1 score as the evaluation metrics. Accuracy\nreflects the proportion of correctly answered ques-\ntions. Precision and Recall reflect the ratios of\ncorrectly answering questions whose answers are\n“Yes” or “No”, respectively. F1 score combines\nthe results of Precision and Recall and we select\nit as the major metric for evaluation. Besides, we\nalso report the ratio that LVLMs answer “Yes” as a\nreference to analyze the model behaviors.\n5.2 Evaluation on MSCOCO\nWe evaluate all the LVLMs with POPE built on\nthe validation set of MSCOCO (Lin et al., 2014).\nWe randomly select 500 images with more than\n3 ground-truth objects in the annotations and con-\nstruct 6 questions for each image (i.e., l = 6).\nThe results are presented in Table 3, where\nwe can obtain a similar conclusion as in Table 1\nthat InstructBLIP performs the best, while LLaV A,\nMultiModal-GPT and mPLUG-Owl suffer more se-\nvere hallucination problem, whose F1 Score are be-\nlow 70. It indicates that POPE can well estimate the\ndegree of the hallucination problem in LVLMs. Be-\nsides, we find that LLaV A, MultiModal-GPT and\nmPLUG-Owl are extremely prone to answer “Yes”\n(near 99%). It reveals that these three LVLMs are\nover confident, leading to lower accuracy on ques-\ntions with the answer “No”. Furthermore, the per-\nformance of LVLMs consistently decreases, from\nrandom settings, to popular and adversarial. It is\nconsistent with our findings in Section 4, as LVLMs\nare prone to hallucinate the frequently appearing\nand co-occurring objects.\n5.3 Advantages of POPE\nAs previously stated, the current approach for eval-\nuating object hallucination in LVLMs like CHAIR\nis instruction-based, which is hindered by LVLMs’\nsensitivity to prompts and requires object annota-\ntions and manually designed rules for evaluation.\nIn contrast, POPE is more stable to prompt forms\nand can be easily extended to unannotated datasets.\nIts probing result is also highly consistent with\nmodel’s caption.\nStability. Regardless of the variations in prompt\ntemplates, POPE requires LVLMs to answer sim-\nple closed-ended questions, which is less likely to\nintroduce ambiguity compared to instruction-based\nmethods. Such characteristic contributes to its sta-\nbility. To validate it, we evaluate LLaV A using both\nPOPE and CHAIRI with four different prompts for\neach. The evaluation results are presented in Ta-\nble 4. It can be observed that the standard deviation\n298\nPOPE CHAIR\nPrompt F1 Score Prompt CHAIRI\nIs there a <object> in the image? 68.65 Generate a short caption of the image. 10.50\nDoes the image contain a <object>? 66.83 Provide a brief description of the image. 18.80\nHave you noticed a <object> in the image? 66.67 Generate a concise description for the image. 14.60\nCan you see a <object> in the image? 67.58 Create a short textual summary for the image. 11.60\nAvg±Std. 67.43 ±0.78 13.88 ±3.22\nTable 4: Evaluation results of LLaV A on POPE and CHAIR with different prompt templates.\nDataset POPE Model Accuracy Precision Recall F1 Score F1 Score (Truth) Yes (%)\nMSCOCO\nRandom\nLLaV A 50.47 50.24 99.67 66.80 68.65 99.20\nMiniGPT-4 73.77 79.25 64.40 71.06 78.86 40.63\nInstructBLIP 86.60 80.74 96.13 89.29 89.27 59.53\nPopular\nLLaV A 50.00 50.00 99.27 66.50 67.72 99.27\nMiniGPT-4 67.80 68.80 65.13 66.92 72.21 47.33\nInstructBLIP 71.27 64.20 96.13 76.99 83.45 74.87\nAdversarial\nLLaV A 49.77 49.88 99.20 66.38 66.98 99.43\nMiniGPT-4 61.93 61.46 64.00 62.70 71.37 52.07\nInstructBLIP 62.53 57.50 96.13 71.96 78.45 83.60\nTable 5: SEEM-based POPE results of LVLM on MSCOCO. F1 Score (Truth) are the results of POPE using\nground-truth annotations, which are copied from Table 3. The best results in each block are denoted in bold.\nof the F1 score is significantly lower than CHAIRI,\nwhich confirms that POPE exhibits higher stability\nwhen faced with different prompts.\nScalability. As mentioned before, with the as-\nsistance of automatic segmentation tools, POPE\ncan be easily extended to datasets without annota-\ntions. To validate it, we adopt SEEM (Zou et al.,\n2023) to annotate images from three datasets (i.e.,\nMSCOCO, A-OKVQA (Schwenk et al., 2022) and\nGQA (Hudson and Manning, 2019)) and build\nPOPE based on the segmentation results. We evalu-\nate InstructBLIP, MiniGPT-4 and LLaV A on them\nand report the results in Table 5 and Table 11 (pre-\nsented in Appendix D). In Table 5, the perfor-\nmances of all LVLMs mostly follow the same trend\nas annotation-based POPE in Table 3, i.e., Ran-\ndom > Popular > Adversarial, and InstructBLIP >\nMiniGPT-4 > LLaVA. Such consistency indicates\nthe reliability of the SEEM-based POPE. Whereas,\nwe also notice the performance gap between the\ntwo settings, e.g., F1 Score 71.37 v.s. 62.70 for\nMiniGPT-4 under the Adversarial setting. This\nphenomenon can be attributed to the finer gran-\nularity of the segmentation results generated by\nSEEM, which makes the POPE more challenging.\nIn summary, when combined with automated seg-\nmentation tools, POPE can be easily extended to\nunannotated datasets and conduct effective evalua-\ntions on them.\nConsistency. A potential concern for POPE is\nwhether the Yes/No responses of LVLMs genuinely\nreflect their perception of objects. To validate this,\nwe measure the consistency between the POPE re-\nsponses and captions generated by LVLMs. Specif-\nically, we examine if objects that receive \"No\"\nresponses seldom appear in the captions, and if\nobjects frequently mentioned in captions usually\nreceive \"Yes\" answers. We collect data from In-\nstructBLIP and MiniGPT-4, given their relatively\nbalanced yes/no distributions. Our findings reveal\nthat out of the 1303 and 1445 objects that are given\n\"No\" responses by InstructBLIP and MiniGPT-4,\nmerely 0 and 5 of those objects were referenced\nin captions. Moreover, out of the 664 and 1034\nobjects mentioned in the captions by these models,\n664 and 961 respectively received a \"Yes\" verdict.\nSuch results underscore a robust correlation be-\ntween objects’ presence in captions and Yes/No\nresponses in POPE questions about them, validat-\ning the reliability of the POPE assessment.\n5.4 Impact of Hallucination on Vision Tasks\nAlthough existing LVLMs do suffer from signifi-\ncant object hallucination issues, it remains an open\nquestion whether these hallucinations have a strong\nimpact on other vision tasks. Therefore, we com-\npare their performance on POPE with VQA and\n299\nDataset Model POPE↑ VQA↑\nA-OKVQA\nInstructBLIP 87.20 59.68\nMiniGPT-4 72.47 38.69\nLLaV A 66.64 50.51\nGQA\nInstructBLIP 85.32 62.12\nMiniGPT-4 67.13 42.24\nLLaV A 66.56 47.60\nTable 6: Evaluation results of LVLMs on POPE and\nVQA. For VQA tasks, we report the VQA score on A-\nOKVQA and Accuracy on GQA. For POPE, we copy\nthe result under the random setting from Table 11.\nimage captioning tasks. For VQA tasks, we eval-\nuate the SEEM-based POPE and VQA scores of\nLVLMs on A-OKVQA and GQA datasets. Since\nLVLMs are prone to generate answers in an open-\nended manner, we utilize ChatGPT to help parse\nthe generated results to better evaluate the VQA\nperformance. The details of evaluation settings\nare presented in Appendix E. For image caption-\ning tasks, we evaluate the captions of 500 images\nin POPE with traditional metrics. The evaluation\nresults are left in Appendix F.\nThe evaluation results are shown in Table 6. In-\nstructBLIP performs the best under all settings,\nhighlighting the importance of instruction-tuning\non large visual instruction corpora. Note that since\nInstructBLIP has been trained on A-OKVQA, the\nresult should be considered with caution. Further-\nmore, despite MiniGPT-4 achieving a higher F1\nscore compared to LLaV A, its performance on\nVQA tasks is relatively poor. A possible reason\nis that the instruction dataset of MiniGPT-4 only\nderives from image caption data, while LLaV A uses\n158K visual instructions data involving complex\nvisual questions. The results imply that the degree\nof hallucination may not be always consistent with\nthe VQA performance and these two evaluation as-\npects are both important and should be considered\nin real-world applications.\n6 Conclusion\nIn this work, we conducted evaluation experiments\non several LVLMs and examined how they suffer\nfrom the object hallucination issue. By investigat-\ning the reasons for object hallucination, we empir-\nically revealed that the object distributions of the\nvisual instructions would affect the object halluci-\nnation of LVLMs. Besides, we also found that the\nexisting hallucination evaluation methods might be\naffected by the input instructions and the generated\ntext of LVLMs, thus leading to less reliable evalua-\ntion results. To address this issue, we proposed a\npolling-based query method called POPE, to pro-\nvide an improved evaluation approach for the object\nhallucination of LVLMs. Experimental results have\nshown that our proposed POPE can better evaluate\nthe object hallucination issue of LVLMs.\n7 Limitations\nDespite that we have made extensive explorations,\nthis work still has several limitations. First, we\nonly focus on the object hallucination problem in\nLVLMs, while do not consider other aspects that\ncan reflect the capacities of LVLMs. It means that\nthe current evaluation task cannot measure theover-\nall performance of LVLMs. In other words, if\nsome model got a higher score in our evaluation\nsetting, it does not necessarily indicate a stronger\noverall capacity than the one with a lower score.\nSecond, due to the limitation of computation re-\nsources, we have to evaluate all models on a part\nof the validation set for each dataset. The reported\nresults might be affected by the corresponding data\ndistribution, though we have carefully set up the\nexperiments. Third, our proposed POPE utilizes\na matching-based method to determine whether\nLVLMs answer “Yes” or “No”, while empirically,\nLVLMs may occasionally fail to provide answers\nexplicitly containing these words, which may lead\nto inaccurate evaluation results. Fourth, when com-\nbined with the automatic segmentation tool, the\nobjects would be annotated based on the label set\nby the tool, which may be inconsistent with the\ncollected human annotations, leading to a diver-\ngence in evaluation results. Finally, this work has\nonly compared a small number of LVLMs, without\nincluding some recently released or closed-source\nones. We leave the evaluation of more LVLMs as\nour future work.\nAlthough we have extensively discussed the hal-\nlucination issues of LVLMs, it does not indicate\nthat we hold an negative opinion on their progress.\nInstead, it will be a very promising direction to\ndevelop LVLMs by leveraging the powerful LLMs.\nThese models that were evaluated in this work\nhave been excellent demonstrations for this direc-\ntion. While, we do hope that our work can bring\nnew ideas or insights to develop more reliable and\nhuman-aligned LVLMs.\n300\nAcknowledgements\nThis work was partially supported by National Nat-\nural Science Foundation of China under Grant No.\n62222215, Beijing Natural Science Foundation un-\nder Grant No. L233008 and 4222027, and Beijing\nOutstanding Young Scientist Program under Grant\nNo. BJJWZYJH012019100020098. This research\nwas also supported by Meituan.\nReferences\nHarsh Agrawal, Peter Anderson, Karan Desai, Yufei\nWang, Xinlei Chen, Rishabh Jain, Mark Johnson,\nDhruv Batra, Devi Parikh, and Stefan Lee. 2019.\nnocaps: novel object captioning at scale. In 2019\nIEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October 27\n- November 2, 2019, pages 8947–8956. IEEE.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob L. Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand\nSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karén Si-\nmonyan. 2022. Flamingo: a visual language model\nfor few-shot learning. In NeurIPS.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015a. VQA: visual question an-\nswering. In ICCV, pages 2425–2433. IEEE Com-\nputer Society.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015b. VQA: visual question an-\nswering. In 2015 IEEE International Conference\non Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015, pages 2425–2433. IEEE Com-\nputer Society.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. CoRR,\nabs/2308.12966.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nAli Furkan Biten, Lluís Gómez, and Dimosthenis\nKaratzas. 2022. Let there be a clock on the beach:\nReducing object hallucination in image captioning.\nIn IEEE/CVF Winter Conference on Applications of\nComputer Vision, WACV 2022, Waikoloa, HI, USA,\nJanuary 3-8, 2022, pages 2473–2482. IEEE.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. 2023. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic. CoRR,\nabs/2306.15195.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023a. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500.\nWenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale\nFung. 2023b. Plausible may not be faithful: Probing\nobject hallucination in vision-language pre-training.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, EACL 2023, Dubrovnik, Croatia, May\n2-6, 2023, pages 2128–2140. Association for Com-\nputational Linguistics.\nZhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng\nLiu, and Jianfeng Gao. 2022. Vision-language pre-\ntraining: Basics, recent advances, and future trends.\nFound. Trends Comput. Graph. Vis. , 14(3-4):163–\n352.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Qiao. 2023.\nLlama-adapter V2: parameter-efficient visual instruc-\ntion model. CoRR, abs/2304.15010.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\nMiao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,\nPing Luo, and Kai Chen. 2023. Multimodal-gpt: A\nvision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nvisual question answering. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017 , pages\n6325–6334. IEEE Computer Society.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2015. Framing image description as a ranking task:\nData, models and evaluation metrics (extended ab-\nstract). In IJCAI, pages 4188–4192. AAAI Press.\nYi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng,\nand Bing Qin. 2021. The factual inconsistency prob-\nlem in abstractive text summarization: A survey.\nCoRR, abs/2104.14839.\n301\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6700–6709. Computer Vision Founda-\ntion / IEEE.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of hal-\nlucination in natural language generation. CoRR,\nabs/2202.03629.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023a. Otter: A\nmulti-modal model with in-context instruction tuning.\nCoRR, abs/2305.03726.\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil-\nvio Savarese, and Steven C. H. Hoi. 2022a. LA VIS:\nA library for language-vision intelligence. CoRR,\nabs/2209.09019.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023b. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. CoRR, abs/2301.12597.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022b. BLIP: bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 12888–12900.\nPMLR.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng\nGao. 2020. Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks. In Computer Vi-\nsion - ECCV 2020 - 16th European Conference, Glas-\ngow, UK, August 23-28, 2020, Proceedings, Part\nXXX, volume 12375 of Lecture Notes in Computer\nScience, pages 121–137. Springer.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\ncommon objects in context. In Computer Vision -\nECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V, volume 8693 of Lecture Notes in Computer\nScience, pages 740–755. Springer.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. CoRR,\nabs/2304.08485.\nHaley MacLeod, Cynthia L. Bennett, Meredith Ringel\nMorris, and Edward Cutrell. 2017. Understanding\nblind people’s experiences with computer-generated\ncaptions of social media images. In Proceedings\nof the 2017 CHI Conference on Human Factors in\nComputing Systems, Denver, CO, USA, May 06-11,\n2017, pages 5988–5999. ACM.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In NIPS, pages 1143–1151.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 4035–4045. Association\nfor Computational Linguistics.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-OKVQA: A benchmark for visual question answer-\ning using world knowledge. In Computer Vision -\nECCV 2022 - 17th European Conference, Tel Aviv,\nIsrael, October 23-27, 2022, Proceedings, Part VIII,\nvolume 13668 of Lecture Notes in Computer Science,\npages 146–162. Springer.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In ACL (1), pages 2556–2565. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. OFA: unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 23318–23340. PMLR.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming\nYan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong\nXu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang,\nand Fei Huang. 2023. mplug-owl: Modularization\nempowers large language models with multimodality.\nCoRR, abs/2304.14178.\nPeng Zhang, Yash Goyal, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2016. Yin and yang:\nBalancing and answering binary visual questions. In\nCVPR, pages 5014–5022. IEEE Computer Society.\n302\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 5579–5588.\nComputer Vision Foundation / IEEE.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. CoRR, abs/2304.10592.\nXueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie\nLi, Jianfeng Gao, and Yong Jae Lee. 2023. Seg-\nment everything everywhere all at once. CoRR,\nabs/2304.06718.\nA Details of Evaluation Settings\nDataset. MSCOCO (Lin et al., 2014) is a large-\nscale image recognition, segmentation, and caption-\ning dataset. Here, we randomly sample 2,000 im-\nages with annotations about contained objects and\nhuman-labeled captions from its validation set as\nour evaluation dataset. For computing the CHAIR\nmetric on MSCOCO, we follow the settings in\nRohrbach et al. (2018) which only considers 80\nobjects appearing in the MSCOCO segmentation\nchallenge.\nModels. The evaluated LVLMs basically con-\nsist of three parts: a visual encoder, an alignment\nmodel, and a large language model. All the above\nmodels have been tuned on collected visual in-\nstruction data. A detailed comparison (e.g., back-\nbones and trainable components) of these LVLMs\nis shown in Table 7. We also collect the evaluation\nresults of smaller VLPMs, i.e., OSCAR (Li et al.,\n2020), VinVL (Zhang et al., 2021), BLIP (Li et al.,\n2022b) and OFA (Wang et al., 2022) from Dai et al.\n(2023b) as baseline results.\nB Additional Qualitative Analysis Results\nTo better validate our hypotheses, We expand the\nanalysis scope to all 80 objects in MSCOCO and\npresent the result in this part.\nFor hypothesis (1), we present the cumulative\nproportions of the hallucination times of all 80\nCOCO objects in Table 8. The table demonstrates\nthat, across all models, the top 30 objects comprise\napproximately 70% of all hallucinated objects. For\nhypothesis (2), we present the cumulative propor-\ntions of the hallucination times of all COCO objects\nthat co-occur with dining tablein Table 9. We\nalso arrange these objects by their co-occurrence\nfrequency. Similarly, the top 20 objects comprise\nabout 80% of all hallucinated objects.\nC Additional Quantitative Analysis\nResults\nWe present the HRC results of two other common\nobjects, i.e., chair and car in Table 10, which\nshow a similar trend with Table 2.\nD Results of SEEM-based POPE on\nA-OKVQA and GQA\nWe adopt SEEM (Zou et al., 2023) to annotate im-\nages from A-OKVQA and GQA and build POPE\n303\nModel VE AN LLM\nPre-training Fine-tuning\nVE AN LLM VE AN LLM\nmPLUG-Owl ViT-L/14 Attention LLaMA 7B\nLLaV A ViT-L/14 Linear LLaMA 13B\nMultiModal-GPT ViT-L/14 Attention LLaMA 7B\nMiniGPT-4 ViT-G/14 Linear Vicuna 13B\nInstructBLIP ViT-G/14 Q-Former Vicuna 13B\nTable 7: Comparison of the evaluated LVLMs. VE, AN, LLM stand for Visual Encoder, Alignment Network and\nLarge Language Model, respectively.\n denotes frozen and\n denotes trainable. The fine-tuning of LLM in\nMultiModal-GPT and mPLUG-Owl is implemented by LoRA.\nModel\nAccumulative proportions (sorted by appearance frequency)\nTop 10 Top 20 Top 30 Top 40 Top 50 Top 60 Top 70 Top 80\nmPLUG-Owl 56.89 67.75 77.34 79.93 90.41 95.33 98.75 100.00\nLLaV A 48.47 60.29 69.74 72.80 81.46 90.26 96.69 100.00\nMultimodal-GPT 43.07 56.12 68.85 72.78 81.59 90.07 97.07 100.00\nMiniGPT-4 44.96 55.19 70.68 78.12 84.36 93.23 97.59 100.00\nTable 8: The accumulated proportions of the hallucination times of all 80 COCO objects. We arrange all objects by\ntheir frequency of occurrence.\nModel\nAccumulative proportions (sorted by co-occurrence frequency)\nTop 10 Top 20 Top 30 Top 40 Top 50 >Top 60\nmPLUG-Owl 71.78 82.82 86.81 92.94 99.08 100.00\nLLaV A 60.69 73.79 85.52 93.10 97.24 100.00\nMultimodal-GPT 53.50 75.16 85.99 96.82 98.09 100.00\nMiniGPT-4 64.00 80.00 92.00 94.00 96.00 100.00\nTable 9: The accumulated proportions of the hallucination times all objects that co-occur with dining table. We\narrange all objects by their co-occurrence frequency with dining table.\nbased on segmentation results. We evaluate In-\nstructBLIP, MiniGPT-4 and LLaV A. We also eval-\nuate a full-data supervised-tuned smaller model,\nBLIP (Li et al., 2022b) to better reflect the degree\nof hallucination. The evaluation results are pre-\nsented in Table 11.\nE ChatGPT-assisted VQA Evaluation.\nWe employ the VQA score (Antol et al., 2015b)\nto assess VQA tasks with the help of ChatGPT.\nConsidering that LVLMs generally produce open-\nended responses, we enlist ChatGPT to assess\nwhether the model’s reply aligns with potential\nanswers. The prompt we provide to ChatGPT is as\nfollows:\n•“You are an examiner who can judge whether\na student’s answer matches the correct answers.\nNext, I will provide you with 10 correct answers in\nthe form of a list and a student’s answer. Please\njudge whether the student’s answer matches one of\nthe 10 correct answers. If it matches, please output\nthe correct answer directly (must be an element\nin the list, if it matches multiple correct answers,\nplease output the most frequent occurrence in the\nlist); if not, please output <NAN> directly. Do\nNOT output anything else!\ncorrect answers:\nstudent answer:”\nF Results of Image Captioning\nThe MSCOCO captioning results of LVLMs are\nshowcased in Table 12. Generally, their captioning\nperformance aligns with the POPE assessments,\nsuggesting that object hallucination influences the\nefficacy of LVLMs in other vision tasks.\n304\nModel\nHRC(chair) HR C(car)\n@10 @20 @30 @10 @20 @30\nmPLUG-Owl 0.5926 0.7186 0.8201 0.7587 0.9136 0.9707\nLLaV A 0.5206 0.6830 0.8152 0.6870 0.8886 0.9188\nMultiModal-GPT 0.5732 0.7576 0.8811 0.6031 0.8482 0.8623\nMiniGPT-4 0.5701 0.7746 0.8581 0.6444 0.8278 0.9417\nTable 10: The HRC result of chair and car.\nDataset POPE Model Accuracy Precision Recall F1 Score Yes (%)\nA-OKVQA\nRandom\nBLIP 91.00 92.24 89.53 90.87 48.53\nLLaV A 50.16 50.08 99.53 66.64 99.37\nMiniGPT-4 74.47 78.63 67.20 72.47 42.73\nInstructBLIP 85.77 79.21 97.00 87.20 61.23\nPopular\nBLIP 88.40 87.55 89.53 88.53 51.13\nLLaV A 50.03 50.02 99.67 66.61 99.63\nMiniGPT-4 69.93 70.40 68.80 69.59 48.87\nInstructBLIP 75.03 67.39 97.00 79.53 71.97\nAdversarial\nBLIP 82.37 78.31 89.53 83.55 57.17\nLLaV A 50.13 50.07 99.67 66.65 99.53\nMiniGPT-4 64.33 63.62 66.93 65.24 52.60\nInstructBLIP 65.46 59.48 97.00 73.75 81.53\nGQA\nRandom\nBLIP 89.93 91.08 88.53 89.79 48.60\nLLaV A 50.17 50.08 99.20 66.56 99.03\nMiniGPT-4 71.33 78.67 58.53 67.13 37.20\nInstructBLIP 83.90 78.39 93.60 85.32 59.70\nPopular\nBLIP 86.63 85.34 88.47 86.87 51.83\nLLaV A 50.03 50.02 99.47 66.56 99.43\nMiniGPT-4 68.26 72.99 58.00 64.64 39.73\nInstructBLIP 71.87 65.24 93.60 76.89 71.73\nAdversarial\nBLIP 82.40 78.89 88.47 83.41 56.07\nLLaV A 49.77 49.88 99.20 66.38 99.43\nMiniGPT-4 64.23 66.29 57.93 61.83 43.70\nInstructBLIP 64.30 59.02 93.60 72.39 79.30\nTable 11: SEEM-based POPE results of LVLMs and BLIP on A-OKVQA and GQA. The probing objects are\nselected from the segmentation results of SEEM on these datasets. The best results in each block except BLIP are\ndenoted in bold. We employ the BLIP fine-tuned on VQAv2 from LA VIS (Li et al., 2022a).\nModel POPE BLEU-1 BLEU-2 METEOR ROUGE-L\nInstructBLIP 89.29 59.5 45.2 22.6 42.3\nLLaV A 68.65 22.0 13.9 19.8 22.4\nMiniGPT-4 78.86 41.1 28.8 25.6 44.7\nTable 12: MSCOCO caption results.\n305",
  "topic": "Hallucinating",
  "concepts": [
    {
      "name": "Hallucinating",
      "score": 0.960129976272583
    },
    {
      "name": "Object (grammar)",
      "score": 0.6850509643554688
    },
    {
      "name": "Computer science",
      "score": 0.6780345439910889
    },
    {
      "name": "Visual Hallucination",
      "score": 0.619194746017456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5836284160614014
    },
    {
      "name": "Polling",
      "score": 0.5308939814567566
    },
    {
      "name": "Computer vision",
      "score": 0.4917919933795929
    },
    {
      "name": "Natural language processing",
      "score": 0.3874214291572571
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3545963168144226
    },
    {
      "name": "Psychology",
      "score": 0.26126134395599365
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}