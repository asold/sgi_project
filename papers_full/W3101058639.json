{
  "title": "BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition",
  "url": "https://openalex.org/W3101058639",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3101113502",
      "name": "Elisa Terumi Rubel Schneider",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2968913254",
      "name": "João Vitor Andrioli de Souza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2414130666",
      "name": "Julien Knafou",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2133800650",
      "name": "Lucas Emanuel Silva e Oliveira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2531790239",
      "name": "Jenny Copara",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2968653913",
      "name": "Yohan Bonescki Gumiel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3035940256",
      "name": "Lucas Ferro Antunes de Oliveira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A729294037",
      "name": "Emerson Cabrera Paraiso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2234087575",
      "name": "Douglas Teodoro",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2395795625",
      "name": "Cláudia Maria Cabral Moro Barra",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287867546",
    "https://openalex.org/W3011718307",
    "https://openalex.org/W3006306665",
    "https://openalex.org/W2527896214",
    "https://openalex.org/W3003406981",
    "https://openalex.org/W2970228048",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2972483465",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2682408236",
    "https://openalex.org/W2976444281",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3091322135",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2345088727",
    "https://openalex.org/W2129404933",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W3007595536",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2989967505",
    "https://openalex.org/W2967989289",
    "https://openalex.org/W2493916176"
  ],
  "abstract": "With the growing number of electronic health record data, clinical NLP tasks have become increasingly relevant to unlock valuable information from unstructured clinical text. Although the performance of downstream NLP tasks, such as named-entity recognition (NER), in English corpus has recently improved by contextualised language models, less research is available for clinical texts in low resource languages. Our goal is to assess a deep contextual embedding model for Portuguese, so called BioBERTpt, to support clinical and biomedical NER. We transfer learned information encoded in a multilingual-BERT model to a corpora of clinical narratives and biomedical-scientific papers in Brazilian Portuguese. To evaluate the performance of BioBERTpt, we ran NER experiments on two annotated corpora containing clinical narratives and compared the results with existing BERT models. Our in-domain model outperformed the baseline model in F1-score by 2.72%, achieving higher performance in 11 out of 13 assessed entities. We demonstrate that enriching contextual embedding models with domain literature can play an important role in improving performance for specific NLP tasks. The transfer learning process enhanced the Portuguese biomedical NER model by reducing the necessity of labeled data and the demand for retraining a whole new model.",
  "full_text": "Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 65–72\nNovember 19, 2020.c⃝2020 Association for Computational Linguistics\n65\nBioBERTpt - A Portuguese Neural Language Model for Clinical Named\nEntity Recognition\nElisa Terumi Rubel Schneider1, Jo˜ao Vitor Andrioli de Souza1, Julien Knafou2,\nJenny Copara2, Lucas E. S. e Oliveira1, Yohan B. Gumiel1, Lucas F. A. de Oliveira1,\nDouglas Teodoro2, Emerson Cabrera Paraiso1 and Claudia Moro1\n1Pontif´ıcia Universidade Cat´olica do Paran´a, Brazil\n2University of Applied Sciences and Arts of Western Switzerland\n{elisa.rubel, joao.souza}@pucpr.edu.br, paraiso@ppgia.pucpr.br, c.moro@pucpr.br\nAbstract\nWith the growing number of electronic health\nrecord data, clinical NLP tasks have be-\ncome increasingly relevant to unlock valu-\nable information from unstructured clinical\ntext. Although the performance of down-\nstream NLP tasks, such as named-entity recog-\nnition (NER), in English corpus has recently\nimproved by contextualised language models,\nless research is available for clinical texts\nin low resource languages. Our goal is to\nassess a deep contextual embedding model\nfor Portuguese, so called BioBERTpt, to sup-\nport clinical and biomedical NER. We transfer\nlearned information encoded in a multilingual-\nBERT model to a corpora of clinical narratives\nand biomedical-scientiﬁc papers in Brazilian\nPortuguese. To evaluate the performance of\nBioBERTpt, we ran NER experiments on two\nannotated corpora containing clinical narra-\ntives and compared the results with existing\nBERT models. Our in-domain model out-\nperformed the baseline model in F1-score by\n2.72%, achieving higher performance in 11\nout of 13 assessed entities. We demonstrate\nthat enriching contextual embedding models\nwith domain literature can play an important\nrole in improving performance for speciﬁc\nNLP tasks. The transfer learning process en-\nhanced the Portuguese biomedical NER model\nby reducing the necessity of labeled data and\nthe demand for retraining a whole new model.\n1 Introduction\nDespite recent increases in the availability of ma-\nchine learning methods, extracting structured in-\nformation from large amounts of unstructured and\nnoisy clinical documents, as available in electronic\nhealth record (EHR) systems, is still a challenging\ntask. Patient’s EHR are ﬁlled with clinical concepts,\noften misspelled, abbreviated and represented by\na variety of synonyms. Nevertheless, they contain\nvaluable and detailed patient information (Lopes\net al., 2019). Natural language processing (NLP)\ntasks, such as Named Entity Recognition (NER),\nare used for acquiring knowledge from unstruc-\ntured texts, by recognizing meaningful entities in\ntext passages. In the clinical domain, NER can be\nused to identify clinical concepts, such as diseases,\nsigns, procedures and drugs, supporting other data\nanalysis as prediction of future clinical events, sum-\nmarization, and relation extraction between entities\n(e.g., drug-to-drug interaction).\nRule-based NER approaches, supported by dic-\ntionary resources, perform well in simple con-\ntexts (Eftimov et al., 2017). However, they are\nlimited to work with the complexity of clinical\ntexts. For complex corpora, machine learning ap-\nproaches, such as conditional random ﬁelds (CRF)\n(Lafferty et al., 2001) and, lately, a combination\nwith Bidirectional Long Short-Term Memory (BiL-\nSTM) models, have been proposed (Lample et al.,\n2016). These supervised approaches have a con-\nsiderable performance gain when trained on huge\namounts of labeled data. Neural network language\nmodels introduced the idea of deep learning into\nlanguage modeling by learning a distributed rep-\nresentation of words. These distributed word rep-\nresentations, trained on massive amounts of unan-\nnotated textual data, have been proved to provide\ngood lower dimension feature representations in a\nwide range of NLP tasks (Wang et al., 2020). The\nContinuous Bag-of-Words and Skip-gram models\n66\nproposed to reduce the computational complexity\nwere considered as a milestone in the development\nof the so-called word embeddings (Mikolov et al.,\n2013), followed by the Global Vector (GloVe) (Pen-\nnington et al., 2014) and the fastText (Bojanowski\net al., 2016) models.\nWhile these approaches work with a single\nglobal representation for each word, several\ncontext-dependent representations models have\nbeen recently proposed, such as embeddings from\nlanguage models (ELMo) (Peters et al., 2018), ﬂair\nembeddings (Akbik et al., 2018), the Universal\nLanguage Model Fine-tuning (ULMFit) (Howard\nand Ruder, 2018) and bidirectional encoder rep-\nresentations from transformers (BERT) (Devlin\net al., 2018). Contextual embedding models pre-\ntrained on large-scale unlabelled corpora, particu-\nlarly those supported by the transformer architec-\nture (Vaswani et al., 2017), reached the state-of-\nthe-art performance on many NLP tasks (Liu et al.,\n2020). Nevertheless, when applying the general\nword representation models in healthcare text min-\ning, the characteristics of clinical texts are not con-\nsidered, known to be noisy, with a different vocab-\nulary, expressions, and word distribution (Knake\net al., 2016). Therefore, contextual word embed-\nding models, like BERT, can be ﬁne-tuned, i.e.,\nhave their last layers updated to adapt to a spe-\nciﬁc domain, like clinical and biomedical, using\ndomain-speciﬁc training data. These transfer learn-\ning process allows the training of a general domain\nmodel with medical domain corpus, proving to be a\nviable technique to medical NLP tasks (Ranti et al.,\n2020).\nDespite the low availability of clinical narra-\ntives, given the sensitive nature of health data and\nprivacy concerns (Berman, 2002), several mod-\nels were trained on clinical and biomedical cor-\npora. In 2013, the word2vec model was trained on\nbiomedical corpora (Pyysalo et al., 2013), creating\na language model with high-quality vector space\nrepresentations. BioBERT (Lee et al., 2019) is a\nBERT model trained from scratch using PubMed\nand PubMed Central (PMC) scientiﬁc texts, reach-\ning the state-of-the-art results on some biomedical\nNLP tasks. Clinical BERT (Alsentzer et al., 2019)\ndemonstrated that the pre-trained model with clin-\nical data improved performance in three common\nclinical NLP tasks. Li et al. (2019) reached state-\nof-the-art for biomedical and clinical entity nor-\nmalization with a model trained using EHR data.\nDespite the essential contributions of contextual\nword embeddings on clinical NER, all these studies\nused English corpora. Indeed, there are few studies\nin lower resources languages for the clinical do-\nmain. In Portuguese, Lopes et al. (2019) proposed\na fastText model trained with clinical texts, which\nachieved higher results when compared to out-of-\ndomain embeddings. In a recent work, de Souza\net al. (2019) explored the CRF algorithm for the\nNER task on SemClinBr (Oliveira et al., 2020), the\nsame annotated corpus we used in this work. They\nclassiﬁed three clinical entities (Disorders, Proce-\ndures and Chemicals and Drugs) and some medical\ntext abbreviations, achieving promising results. A\nPortuguese clinical word embedding model were\ntrained using Skip-gram with negative sampling\nand evaluated on a downstream biomedical NLP\ntask for Urinary Tract Infection disease identiﬁca-\ntion (Oliveira et al., 2019). Their results showed\nthat larger, coarse-grained models achieve a slightly\nbetter outcome when compared with small, ﬁne-\ngrained models in the proposed task.\nAlthough these previous works achieved relevant\nresults, we have not found studies for clinical Por-\ntuguese using attention-based architectures, such\nas BERT, which have been achieving the state-of-\nthe-art for most of English NLP tasks. Even with\nthe existence of multilingual models, like BERT-\nmultilingual, it is important to investigate what\ncan be the contribution in creating a domain ﬁne-\ntuned model for a lower-resource language. As\ndemonstrated in the work of Peng et al. (2019),\npre-trained BERT models with biomedical and clin-\nical data achieves better results in the BLUE bench-\nmark for English. This leads us to believe that the\nsame is valid for Portuguese. Thus, the objective of\nthis work is to assess the performance of a domain\nspeciﬁc attention-based model, BioBERTpt, to sup-\nport NER tasks in Portuguese clinical narratives.\nWe intend to investigate how an in-domain model\ncan inﬂuence the performance of BERT-based mod-\nels for NER in clinical data. Also, as knowledge\nencoded in transformer-based language models can\nbe leveraged to several downstream NLP tasks, we\nrelease publicly the ﬁrst BERT-based model trained\non clinical data for Portuguese 1.\n2 Methods\nIn this section, we ﬁrst describe how BioBERTpt\nwas developed using clinical notes and scientiﬁc\n1https://github.com/HAILab-PUCPR/BioBERTpt\n67\nFigure 1: Clinical notes and scientiﬁc biomedical ab-\nstracts are fed to a pre-trained BERT multilingual\nmodel to create BioBERTpt(clin), BioBERTpt(bio) and\nBioBERTpt(all). These models are then used to extract\ninformation from Portuguese clinical notes, evaluated\nin the clinical NER corpora SemClinBr and CLINpt.\nabstracts. Next, we introduce the corpora used for\nthe NER tasks and the evaluation metrics used in\nour experiments.\n2.1 Development of BioBERTpt\nIn this paper, we ﬁne-tuned three BERT-based mod-\nels on Portuguese clinical and biomedical corpora,\ninitialized with multilingual BERT weights pro-\nvided by Devlin et al. (2018).\nWith the approval from the PUCPR Research\nEthics Committee with CAAE (Certiﬁcate of\npresentation for Ethical Appreciation), number\n51376015.4.0000.0020, we collected 2,100,546\nclinical notes from Brazilian hospitals, from 2002\nto 2018. All the clinical text have been properly\nde-identiﬁed, to respect patient’s privacy. This cor-\npus contains multi specialty information, including\ncardiology, nephrology and endocrinology, from\ndifferent types of clinical texts (narratives), such as\ndischarge summaries, nurse notes and ambulatory\nnotes. In total, the clinical notes contain 3.8 mil-\nlion sentences with 27.7 million words. Our clini-\ncal model was trained with this corpus, beneﬁting\nfrom the weights already trained in the multilingual\nBERT model.\nWe also trained a biomedical model, using titles\nand abstracts from Portuguese scientiﬁc papers pub-\nlished in Pubmed and in the Scielo (Scientiﬁc Elec-\ntronic Library Online)2, an integrated database that\ncontains Brazilian’s scientiﬁc journal publications\nin multidisciplinary areas such as health. These\ntexts were obtained from the Biomedical Trans-\nlation Task in the First Conference on Machine\nTranslation (WMT16), which evaluated the transla-\ntion of scientiﬁc abstracts between English, French,\nSpanish and Portuguese (Bojar et al., 2016). In this\nwork, we used only the Portuguese part, composed\nby documents from Scielo and Pubmed databases\nabout biological and health, resulting in 16.4 mil-\nlion words. The text corpora used for training our\nmodels are listed in Table 1.\nIn the preprocessing step, we split the notes and\nabstracts into sentences and tokenize them with the\ndefault BERT wordpiece tokenizer (Devlin et al.,\n2018). All models were trained for 5 epochs on a\nGPU GTX2080Ti Titan 12 GB, with the hyperpa-\nrameters: batch size as 4, learning rate as 2e-5 and\nblock size as 512. We used the PyTorch implemen-\ntation of Bert proposed by Hugging Face3.\nTo investigate how the domain can inﬂuence the\ntask performance, we trained: a) a model with the\nclinical data, from the narratives of Brazilian hospi-\ntals, b) a model with the biomedical data, from the\nscientiﬁc papers abstracts, and c) a full version, i.e.,\nusing both clinical and biomedical data. Through-\nout this paper, we will refer to these corresponding\nmodels as BioBERTpt(clin), BioBERTpt(bio) and\nBioBERTpt(all), respectively.\n2.2 NER experiments\nCorpora: In our ﬁrst NER experiment, we use\nSemClinBr (Oliveira et al., 2020), a semantically\nannotated corpus for Portuguese clinical NER, con-\ntaining 1,000 labeled clinical notes. This corpus\ncomprehended 100 UMLS semantic types, summa-\nrized in 13 groups of entities: Disorders, Chem-\nicals and Drugs, Medical Procedure, Diagnostic\nProcedure, Disease Or Syndrome, Findings, Health\nCare Activity, Laboratory or Test Result, Medical\nDevice, Pharmacologic Substance, Quantitative\nConcept, Sign or Symptom and Therapeutic or Pre-\nventive Procedure. Although SemClinBr supports\nIOB2 (aka BIO) and IOBES (aka BILOU) tagging\nschemes, we report our experiment in IOB2, widely\n2https://scielo.org/\n3https://github.com/huggingface/transformers\n68\nTable 1: List of text corpora used for BioBERTpt\nCorpus Source N o of sentences N o of words Domain\nClinical notes EHR from Brazilian hospitals 3.8 million 27.7 million Clinical\nScielo: Health area Literature titles and abstracts 532,920 12.4 million Biomedical\nScielo: Biological area Literature titles and abstracts 130,098 3.2 million Biomedical\nPubmed Literature titles 74,451 812,711 Biomedical\nused in the literature.\nFor the second NER experiment, we run our\nmodels in a small dataset with IOBES format, pro-\nposed by Lopes et al. (2019). This corpus is a col-\nlection of 281 Neurology clinical case descriptions,\nwith manually-annotated named entities, from now\non called CLINpt. These cases were collected from\na clinical journal published by the Portuguese So-\nciety of Neurology.\nExecution: Our experiments were performed\nwith holdout using a corpus split of 60% for train-\ning, 20% for validation and 20% for test. We used\nthe Hugging Face API, which provides the BertFor-\nTokenClassiﬁcation class. This class adds a token-\nlevel classiﬁer, a linear layer that uses the last hid-\nden state of the sequence. For both NER tasks we\nused this conﬁguration: AdamW optimizer, weight\ndecay as 0.01, batch size as 4, maximum length as\n256, learning rate as 3e-5, maximum epoch as 10,\nand the linear schedule that decreases the learning\nrate throughout the epochs with warmup as 0.1.\nEvaluation criteria: We evaluate the results us-\ning precision, recall and F1-score metrics. As in\nSemClinBr each entity can have more than one\nsemantic type associated (similar to a multi-label\nclassiﬁcation), we used the label-based metrics, an\nadaptation of existing single-label problem metrics,\nto measure the model general performance. We cal-\nculated the micro-average metric, when the score\nis computed globally over all instances and then\nover all class labels (Sorower, 2010).\nIn addition, we also analyzed statistical signiﬁ-\ncance between the F1-score of the models for all\nentities in SemClinBr. We deﬁned seven samples,\nwhere each one corresponds to a set of the F1-score\nvalues of all entities in the corpus, calculated for\neach respective model. As the Friedman test only\nindicates if there is a difference between the means\nof the samples, without identifying which sam-\nple(s) is(are) different from the set, we applied a\nWilcoxon signed-ranks pair-wise as post-test. The\nWilcoxon signed-rank test was calculated between\npairs of samples, in order to show which pairs of\nsamples have different means. The results are con-\nsidered statistically signiﬁcant for P value <.05.\nWe compare BioBERTpt with the already exist-\ning contextual models: BERT multilingual uncased,\nBERT multilingual cased, Portuguese BERT base\nand Portuguese BERT large. Both BERT multilin-\ngual are large versions and provide Portuguese lan-\nguage support, called in this work BERT multi(u)\nfor the uncased version and BERT multi(c) for the\ncased version. The Portuguese BERT models, pro-\nposed by Souza et al. (2019), are BERT-models\ntrained on the BrWaC (Brazilian Web as Corpus),\na large Portuguese corpus, with whole-word mask.\nWe used both base and large versions, called here\nBERT PT(b) and BERT PT(l), respectively. All\nthese word embeddings are out-of-domain, i.e.,\ntrained in general context corpora, like Wikipedia\nand books.\n3 Results\nTable 2 shows the average precision, recall and F1-\nscore values for all BERT models on SemClinBr\nand CLINpt corpora, where our in-domain models\noutperformed in the average scores.\nIn the SemClinBr corpus, BioBERTpt(bio) im-\nproved 0.1 in precision, BioBERTpt(all), 2.0 in\nrecall and 1.6 in F1-score, over the out-of-domain\nmodel with better performance. Full F1-score val-\nues for each entity are provided on our reposi-\ntory. Analyzing the performance by entity, the\nin-domain models in general were better at recall\nand F1-score. Our models obtained better results\nin precision for 4 entities, recall for 8 and F1-score\nfor 11. The out-of-domain models obtained bet-\nter results for 9 entities in precision, 5 in recall\nand 2 in F1-score. The results of the Friedman\ntest evidenced that there is a difference between\nsome models. The post-test Wilcoxon signed-ranks\npair-wise showed the statistical relevance between\nmodels over all entities, as shown in Figure 2.\n69\nTable 2: The average scores of the NER tasks, for\neach model evaluated. In bold, the best results\nCorpus / model Precision Recall F1\nSemClinBr\nBERT multi (u)a 0.623 0.566 0.588\nBERT multi (c)b 0.604 0.567 0.582\nBERT PT(b)c 0.595 0.587 0.585\nBERT PT(l)d 0.563 0.531 0.541\nBioBERTpt(bio) 0.624 0.586 0.602\nBioBERTpt(clin) 0.609 0.603 0.602\nBioBERTpt(all) 0.608 0.607 0.604\nCLINpt\nBiLSTM-CRF e 0.753 0.745 0.749\nBERT multi (u) 0.903 0.921 0.912\nBERT multi (c) 0.912 0.931 0.921\nBERT PT(b) 0.910 0.922 0.916\nBERT PT(l) 0.898 0.927 0.912\nBioBERTpt(bio) 0.917 0.925 0.921\nBioBERTpt(clin) 0.917 0.935 0.926\nBioBERTpt(all) 0.912 0.929 0.920\naBERT multilingual uncased\nbBERT multilingual cased\ncPortuguese BERT base\ndPortuguese BERT large\neBaseline from previous work Lopes et al. (2019), where\nthe authors used Fastext as word embeddings.\nBioBERTpt(all) had statistically higher results on\nF1-score than BERT multilingual uncased (P value\nas 0.04640), Portuguese BERT large ( P value as\n0.00298) and Portuguese BERT base (P value as\n0.01750). BioBERTpt(clin) had its performance\nstatistically higher in relation to Portuguese BERT\nlarge (0.00713) and Portuguese BERT base ( P\nvalue as 0.01075), and BioBERTpt(bio), in relation\nto Portuguese BERT large ( P value as 0.01750).\nAlso, BERT multilingual uncased had a signiﬁcant\nhigher performance in relation to Portuguese BERT\nlarge (P value as 0.03305).\nThe results on the CLINpt corpus, also presented\nin table 4, shows that BioBERTpt(clin) improved\nprecision in 0.5, recall in 0.4 and F1-score in 0.5.\nFigure 2: F1-scores of all entities from SemClinBr for\nevaluation of the models (Wilcoxon signed-ranks pair-\nwise post-test).\nDespite CLINpt cases are not representative of the\nusual clinical notes and narratives found in EHRs,\nour clinical model presented the best results. Al-\nthough with little improvement compared to BERT\nmultilingual cased, BioBERTpt(clin) reached the\nstate-of-the-art on this corpus for these three met-\nrics.\n4 Discussion\n4.1 Effect of domain\nOur results show that the in-domain models out-\nperform the general models in average precision,\nrecall and F1-score on the two Portuguese cor-\npora. These results are aligned with previous\nexperiments in English, where domain-speciﬁc\nmodels outperform generic models (Lee et al.,\n2019; Alsentzer et al., 2019; Li et al., 2019;\nPyysalo et al., 2013). BioBERTpt trained on\nclinical narratives had overall better performance\nwhen compared with the model trained only on\nbiomedical texts, reaching higher results for en-\ntities with more clinical-domain-speciﬁc vocabu-\nlary, such asLaboratory, Pharmacologic Substance\nand Chemical and Drugs. The better performance\nof BioBERTpt(clin) over BioBERTpt(bio) was ex-\npected, since the NER evaluation set only contains\nclinical narratives. Although we evaluated both\nschemes, IOBES and IOB2, we report only IOB2 as\nthere was no signiﬁcant difference between them.\nThe F1-score performance of the Chemical and\nDrugs entities was the most assertive for all mod-\nels, reaching 0.911 with BioBERTpt(clin). Due to\nspeciﬁc characteristics of each entity, such as granu-\nlarity, speciﬁcity and different vocabulary across in-\nstitutions, some entities achieved low performance,\nlike Laboratory, which reached only 0.453 as its\nhighest F1-score with BioBERTpt(clin). The use\nof imbalanced data can also affect the results, since\nthe entities with lower frequency have fewer and\n70\nTable 3: F1-score values for three SemClinBr group\nof entities, for comparison with baseline. In bold, the\nhighest values.\nEntity / Model Disorder Proced. a Drug\nCRF b 0.65 0.60 0.42\nBioBERTpt(bio) 0.79 0.69 0.89\nBioBERTpt(clin) 0.78 0.69 0.91\nBioBERTpt(all) 0.79 0.70 0.90\naProcedure\nbBaseline from previous work (de Souza et al., 2019)\nselected vocabulary, leading the models to achieve\nlower results or overﬁt the vocabulary vectors.\nBy evaluating BioBERTpt, we found that the do-\nmain can inﬂuence the performance of BERT-based\nmodels, particularly for domains with unique char-\nacteristics such as medical. Our in-domain models\nachieved higher results for the average metrics. As\nshown in the statistical tests, the results were sig-\nniﬁcant in relation to the BERT uncased model and\nthe Portuguese BERT versions.\n4.2 Effect of the contextualized language\nmodel\nBy providing a contextualized word representation\nand taking advantage of the transformer architec-\nture, BERT-based language models have become\na new paradigm for NLP tasks (Liu et al., 2020).\nThe use of BERT-base models in our work had a\npositive impact on the results when compared to\nprevious works with traditional machine learning\nalgorithms and word embeddings for NER in Por-\ntuguese clinical text (de Souza et al., 2019; Lopes\net al., 2019). For examples, de Souza et al. (2019)\nevaluated three groups of entities from the Sem-\nClinBr corpus using CRF, without any word em-\nbedding. AS shown in Table 3, they obtained for\nDisorder 0.65 of F1-score, compared to our 0.79;\nfor Procedure, they achieved 0.60 compared to our\n0.70 and for Drug, they achieved 0.42 compared to\nour 0.91. In the work of Lopes et al. (2019), where\nthe authors used BiLSTM-CRF plus fastText on the\nCLINpt corpus, they achieved 0.759 with their in-\ndomain model for micro F1-score, compared with\n0.926 with BioBERTpt(clin), as we can see in Table\n2. In general, all BERT-based models performed\nbetter in both corpora compared to the results of\nprevious works. Indeed, the generic BERT models\nperformed reasonably well on clinical NER tasks,\nprobably because they were trained with a consid-\nerable amount of data, which embraced most of the\nsemantics and syntax of the medical context.\n4.3 Effect of language\nAlthough the in-domain models performed bet-\nter than out-of-domain models, the generic Por-\ntuguese BERT models (Souza et al., 2019) were\noutperformed by the BERT multilingual versions.\nThe statistical analyses showed that the Portuguese\nBERT large version was signiﬁcantly outperformed\nnot only by the in-domain models, but also by the\nBERT multilingual uncased. This may be due to\na local minima problem or the catastrophic forget-\nting. As shown by Xu et al., catastrophic forgetting\ncan happen during ﬁne-tuning step, by overwriting\nprevious knowledge of the model with new dis-\ntinct knowledge, leading to a loss of information\non lower layers (Xu et al., 2019). This may have oc-\ncurred since the linguistic characteristics of clinical\ntexts are very different from the Portuguese corpus\nused during pre-training phase of Portuguese BERT.\nAs they were trained from a Web Corpus, collected\nusing a search engine with random pairs of content\nwords from 120,000 different Brazilian websites,\nmaybe the new data in the ﬁne-tuning process did\nnot adequately represented the knowledge included\nin the original training data. The catastrophic for-\ngetting probably occurred because the pre-trained\nmodel had to learn new input patterns, or needed to\nbe adapted to a very distinct environment. On the\nother hand, for the multilingual model, this effect is\nless noticeable due to the larger and more generic\ncorpus used for training.\n4.4 Clinical relevance\nThe World Health Organization (WHO) recently\nreleased a list of 13 urgent health challenges the\nworld will face over next decade, which highlights\na range of issues, including health care equity and\ntopping infectious diseases (WHO). To face these\nchallenges, access to quality health information\nis essential, specially considering the information\nprovided only in EHR’s clinical narratives.\nThe BERT-based models proposed in this study\nand publicly released will support clinical NLP\ntasks for Portuguese, a language with relative lower\nresources, in particular in the health domain. Ex-\ntracting structured information from a large amount\nof available clinical documents can provide health\ncare assistance and help in the clinical decision-\n71\nmaking process, supporting other biomedical tasks\nand contributing to the urgent health challenges for\nthe next decade 4.\n5 Conclusions and future work\nWe proposed a new publicly available Portuguese\nBERT-based model to support clinical and biomed-\nical NLP tasks. Our NER experiments showed that,\ncompared to out-of-domain contextual word em-\nbeddings, BioBERTpt reaches the state-of-the-art\non the CLINpt corpus. Additionally, it has better\nperformance for most entities analyzed on the Sem-\nClinBR corpus. Our preliminary results are aligned\nwith previous results in other languages, evidencing\nthat domain transfer learning can beneﬁt clinical\ntasks, in a statistically signiﬁcant way. In the fu-\nture, we would like to explore larger transformers-\nbased models in the clinical Portuguese domain\nand evaluate our model in different clinical NLP\ntasks, such as negation detection, summarization\nand de-identiﬁcation.\nAcknowledgments\nThis work is related to a project supported by the\nLeading House for the Latin American Region\n- Seed Money Grant (No.1922) - of the Centro\nLatinoamericano-Suizo de la Universidad de San\nGallen CLS-HSG. The authors also would like\nto thank Fundac ¸˜ao Arauc´aria, CAPES (Brazilian\nCoordination for the Improvement of Higher Ed-\nucation Personnel) and CNPq (Brazilian National\nCouncil of Scientiﬁc and Technologic Develop-\nment) for their support in this research.\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n1638–1649, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Asso-\nciation for Computational Linguistics.\n4https://www.who.int/news-room/photo-story/photo-\nstory-detail/urgent-health-challenges-for-the-next-decade\nJules Berman. 2002. Conﬁdentiality issues for medi-\ncal data miners. Artiﬁcial intelligence in medicine ,\n26:25–36.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2016. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Aur ´elie\nN´ev´eol, Mariana Neves, Martin Popel, Matt Post,\nRaphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 conference\non machine translation. In Proceedings of the\nFirst Conference on Machine Translation: Volume\n2, Shared Task Papers, pages 131–198, Berlin, Ger-\nmany. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nTome Eftimov, Barbara Korou ˇsi´c Seljak, and Pe-\nter Koro ˇsec. 2017. A rule-based named-entity\nrecognition method for knowledge extraction of\nevidence-based dietary recommendations. PLOS\nONE, 12(6):1–32.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\npages 328–339.\nLindsey Knake, Monika Ahuja, Erin McDonald, Kelli\nRyckman, Nancy Weathers, Todd Burstain, John Da-\ngle, Jeffrey Murray, and Prakash Nadkarni. 2016.\nQuality of ehr data extractions for studies of preterm\nbirth in a tertiary care center: Guidelines for obtain-\ning reliable data. BMC Pediatrics, 16.\nJohn Lafferty, Andrew Mccallum, and Fernando\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. pages 282–289.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\npages 260–270.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nFei Li, Yonghao Jin, Weisong Liu, Bhanu Pratap Singh\nRawat, Pengshan Cai, and Hong Yu. 2019. Fine-\ntuning bidirectional encoder representations from\ntransformers (bert)–based models on large-scale\nelectronic health record notes: An empirical study.\nJMIR Medical Informatics, 7.\n72\nQi Liu, Matt J. Kusner, and Phil Blunsom. 2020.\nA survey on contextual embeddings. ArXiv,\nabs/2003.07278.\nF´abio Lopes, C ´esar Teixeira, and Hugo\nGonc ¸alo Oliveira. 2019. Contributions to clini-\ncal named entity recognition in Portuguese. In\nProceedings of the 18th BioNLP Workshop and\nShared Task , pages 223–233, Florence, Italy.\nAssociation for Computational Linguistics.\nTomas Mikolov, Kai Chen, G.s Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. Proceedings of Workshop at\nICLR, 2013.\nLucas Oliveira, Yohan Gumiel, Lilian Cintho, Sa-\ndid Hasan, Deborah Carvalho, Claudia Moro, and\nArnon Santos. 2019. Learning portuguese clini-\ncal word embeddings: a multi-specialty and multi-\ninstitutional corpus of clinical narratives supporting\na downstream biomedical task.\nLucas Oliveira, Ana Peters, Adalniza Silva, Caroline\nGebeluca, Yohan Gumiel, Lilian Cintho, Deborah\nCarvalho, Sadid Hasan, and Claudia Moro. 2020.\nSemclinbr – a multi institutional and multi specialty\nsemantically annotated corpus for portuguese clini-\ncal nlp tasks.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on ten\nbenchmarking datasets. In Proceedings of the 2019\nWorkshop on Biomedical Natural Language Process-\ning (BioNLP 2019).\nJeffrey Pennington, Richard Socher, and Christoper\nManning. 2014. Glove: Global vectors for word rep-\nresentation. volume 14, pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations.\nSampo Pyysalo, F Ginter, Hans Moen, T Salakoski, and\nSophia Ananiadou. 2013. Distributional semantics\nresources for biomedical text processing. Proceed-\nings of Languages in Biology and Medicine.\nDaniel Ranti, Katie Hanss, Shan Zhao, Varun Arvind,\nJoseph Titano, Anthony Costa, and Eric Oermann.\n2020. The utility of general domain transfer learning\nfor medical language tasks.\nMohammad S. Sorower. 2010. A literature survey on\nalgorithms for multi-label learning.\nF´abio Souza, Rodrigo Nogueira, and Roberto Lotufo.\n2019. Portuguese named entity recognition using\nbert-crf.\nJo˜ao Vitor de Souza, Yohan Gumiel, Lucas Emanuel\nOliveira, and Claudia Maria Moro. 2019. Named\nentity recognition for clinical portuguese corpus\nwith conditional random ﬁelds and semantic groups.\nIn Anais Principais do XIX Simp ´osio Brasileiro\nde Computac ¸˜ao Aplicada `a Sa ´ude, pages 318–323,\nPorto Alegre, RS, Brasil. SBC.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nYuxuan Wang, Yutai Hou, Wanxiang Che, and Ting Liu.\n2020. From static to dynamic word representations:\na survey. International Journal of Machine Learn-\ning and Cybernetics.\nWHO. World health organization.\nY . Xu, X. Zhong, A. Yepes, and J. Lau. 2019. Forget\nme not: Reducing catastrophic forgetting for domain\nadaptation in reading comprehension.",
  "topic": "Portuguese",
  "concepts": [
    {
      "name": "Portuguese",
      "score": 0.6928577423095703
    },
    {
      "name": "Computer science",
      "score": 0.5472996234893799
    },
    {
      "name": "Artificial neural network",
      "score": 0.5190039873123169
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4434671998023987
    },
    {
      "name": "Natural language processing",
      "score": 0.43913912773132324
    },
    {
      "name": "Linguistics",
      "score": 0.4101320505142212
    },
    {
      "name": "Cognitive science",
      "score": 0.3443446159362793
    },
    {
      "name": "Humanities",
      "score": 0.3395349979400635
    },
    {
      "name": "Art",
      "score": 0.2912324070930481
    },
    {
      "name": "Psychology",
      "score": 0.2470737099647522
    },
    {
      "name": "Philosophy",
      "score": 0.24014532566070557
    }
  ]
}