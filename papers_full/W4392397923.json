{
  "title": "Performance of a Large Language Model on Japanese Emergency Medicine Board Certification Examinations",
  "url": "https://openalex.org/W4392397923",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2134797169",
      "name": "Yutaka Igarashi",
      "affiliations": [
        "Nippon Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A5104252330",
      "name": "Kyoichi Nakahara",
      "affiliations": [
        "Nippon Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A1975741911",
      "name": "Tatsuya Norii",
      "affiliations": [
        "University of New Mexico"
      ]
    },
    {
      "id": "https://openalex.org/A3109716198",
      "name": "Nodoka Miyake",
      "affiliations": [
        "Nippon Medical School"
      ]
    },
    {
      "id": "https://openalex.org/A2003218458",
      "name": "Takashi Tagami",
      "affiliations": [
        "Nippon Medical School Musashi Kosugi Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2198219586",
      "name": "Shoji Yokobori",
      "affiliations": [
        "Nippon Medical School"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134882440",
    "https://openalex.org/W3200098356",
    "https://openalex.org/W4321380696",
    "https://openalex.org/W4324308091",
    "https://openalex.org/W4323350039",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4381056727",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W4323038373",
    "https://openalex.org/W4323981476",
    "https://openalex.org/W4362511131",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4380786006",
    "https://openalex.org/W4380303971",
    "https://openalex.org/W4379966280",
    "https://openalex.org/W4372047097",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4377711218",
    "https://openalex.org/W4379467554",
    "https://openalex.org/W4367668444",
    "https://openalex.org/W4366750536",
    "https://openalex.org/W4377030497",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4379093714",
    "https://openalex.org/W4386596026",
    "https://openalex.org/W4379599010",
    "https://openalex.org/W4319868628",
    "https://openalex.org/W4366736258",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4321748993"
  ],
  "abstract": "An LLM performed satisfactorily on an emergency medicine board certification examination in Japanese and without images. However, factual errors in the responses highlight the need for physician oversight when using LLMs.",
  "full_text": "1 \n \nPerformance of a large language model on Japanese emergency medicine \nboard certification examinations \n \nYutaka Igarashi1, Kyoichi Nakahara1, Tatsuya Norii2, Nodoka Miyake1, Takashi \nTagami3, Shoji Yokobori1 \n \n1Department of Emergency and Critical Care Medicine, Nippon Medical School, \nTokyo, Japan \n2Department of Emergency Medicine, University of New Mexico, NM, United \nStates of America \n3Department of Emergency and Critical Care Medicine, Nippon Medical School \nMusashi-Kosugi Hospital, Kanagawa, Japan \n \n \nCorresponding author: \n \nYutaka Igarashi MD, PhD \nDepartment of Emergency and Critical Care Medicine \nNippon Medical School \n1-1-5, Sendagi, Bunkyo-ku, Tokyo, 113-8603, Japan \nTel: +81-3-3822-2131 \nFax: +81-3-3821-5102 \nEmail: igarashiy@nms.ac.jp \n \nRunning title: LLMs and the emergency specialist exam \n \n  \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n2 \n \nABSTRACT \nBackground Emergency physicians need a broad range of knowledge and \nskills to address critical medical, traumatic, and environmental conditions. \nArtificial intelligence (AI), including large language models (LLMs), has potential \napplications in healthcare settings; however, the performance of LLMs in \nemergency medicine remains unclear. \nMethods To evaluate the reliability of information provided by ChatGPT, an LLM \nwas given the questions set by the Japanese Association of Acute Medicine in \nits board certification examinations over a period of 5 years (2018–2022) and \nprogrammed to answer them twice. Statistical analysis was used to assess \nagreement of the two responses. \nResults The LLM successfully answered 465 of the 475 text-based questions, \nachieving an overall correct response rate of 62.3%. For questions without \nimages, the rate of correct answers was 65.9%. For questions with images that \nwere not explained to the LLM, the rate of correct answers was only 52.0%. The \nannual rates of correct answers to questions without images ranged from 56.3% \nto 78.8%. Accuracy was better for scenario-based questions (69.1%) than for \nstand-alone questions (62.1%). Agreement between the two responses was \nsubstantial (kappa = 0.70). Factual error accounted for 82% of the incorrectly \nanswered questions. \nConclusion An LLM performed satisfactorily on an emergency medicine board \ncertification examination in Japanese and without images. However, factual \nerrors in the responses highlight the need for physician oversight when using \nLLMs. \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n3 \n \n \nKey words: artificial intelligence, emergency medicine, language, medicine, \nspecialty boards  \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n4 \n \nIntroduction \n \nEmergency medicine encompasses prehospital assistance, disaster readiness, \nproficiency in basic and advanced resuscitation techniques, and management \nof critical medical, traumatic, and environmental conditions that demand \nimmediate attention in persons of all age groups.1 Thus, emergency physicians \nneed to have broad knowledge, advanced technical skills, the ability for rapid \ndecision-making, good communication skills, and extensive experience in \nresponding to various situations.  \n \nArtificial intelligence (AI) has the potential to reduce the burden of emergency \nphysicians by fulfilling many expected roles2-4. Large language models (LLMs) \nare an advanced form of AI that learns from large quantities of textual \ninformation and engages in natural interactions with humans. Although LLMs \nare not models specially trained in a particular domain, they can answer \nquestions related to medical expertise. For example, applications are expected \nin fields such as computer-aided diagnosis, data summarization, \ncommunication, and education.5-12  \n \nTo evaluate the performance of LLMs in applications requiring medical \nexpertise, studies have been conducted using medical examinations for \nundergraduates and postgraduates worldwide. These evaluations included \nnational medical licensing examinations in Japan13-15 and the United States10, 16 \nand board certification examinations in neurology,17 nephrology,18 family \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n5 \n \nmedicine,19 general surgery,20 neurosurgery,21, 22 orthopedic surgery,23 urology,24 \nplastic surgery,25 obstetrics and gynecology,26 anesthesiology,27 radiology,28 \ndermatology,29 ophthalmology,30 and otorhinolaryngology.31 Earlier versions of \nChatGPT had a correct answer rate of 30%–60%, i.e., mostly failing scores. \nHowever, performance has significantly improved since the release of \nChatGPT-4. In studies comparing ChatGPT-3.5 and 4, ChatGPT-4 achieved \nhigher scores in all examinations, with the score increasing by an average of \napproximately 20%, thus reaching the passing standard for many examinations.  \n \nHowever, except for cardiopulmonary resuscitation courses,32 there has been \nno evaluation of LLM performance in the field of emergency medicine. In \naddition, LLM performance on board certification examinations remains unclear. \nTherefore, we evaluated LLM performance on emergency medicine board \ncertification examinations administered by the Japanese Association of Acute \nMedicine (JAAM).  \n \nMethods \n \nJAAM board certification examinations \nThe board certification examinations for emergency medicine conducted by the \nJAAM are divided into 3 parts: career as an emergency physician (10 points), \nrecord of medical practice (10 points), and written examination (80 points). A \nscore of ≥70 points out of a total of 100 points is required to pass the board \ncertification examination.33 A score of 50 (62.5%) out of 70 points is required to \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n6 \n \npass the written examination. Each year, examinees are asked to answer 100 \nstand-alone and scenario-based questions by selecting 1 to 3 of the 5 options \nprovided. Inappropriate questions are eliminated, and correct answers are \npublished in the official JAAM journal. Since answers to older questions tend to \nchange because of guideline updates and the accumulation of new evidence, \nthe board certification examinations developed during a recent 5-year period \n(2018–2022) were used in this study. \n \nLLM \nChatGPT-4 (May 24, 2023 Version, OpenAI, San Francisco, CA, USA) was \nused to answer the questions. This LLM utilizes self-attention mechanisms and \nextensive training data to produce natural language responses in conversational \nsettings (Fig. 1). Similar studies excluded questions with images because \nChatGPT could not process images. However, some scenario-based medical \nquestions can be answered without using images by assuming the most \nprobable disease. If ChatGPT was unable to answer a question because of the \ninclusion of images, that question was excluded. Because ChatGPT sometimes \nprovides different answers to the same question, each question was entered \ninto the ChatGPT interface by 2 independent raters (K.N. and Y.I.) to estimate \naccuracy and assess self-agreement in ChatGPT’s response. A previous study \non LLMs used similar methods, making the LLM answer the same questions \ntwice and evaluating the agreement.27 \n \nThe primary outcome was the proportion of correct answers to the questions \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n7 \n \nwithout images, under the same conditions as those encountered by \nexaminees. Secondary outcomes included correct answers to all answerable \nquestions, those with images, and those with stand-alone and scenario-based \nitems. To assess robustness, agreement between the 2 sets of responses was \nevaluated using both kappa and simple agreement metrics. Since ChatGPT is \nlanguage-independent and provides answers based on information, use of the \nJapanese language has no impact on the answers, and all questions were input \nusing Japanese characters. \n \nCategories of ChatGPT errors \nAs in previous studies, incorrectly answered questions were classified into 4 \nprimary categories: comprehension, factualness, specificity, and inference.34 \n Comprehension errors account for failures in understanding the question \ncontext and intent and often occur in the presence of grammar mistakes or \nambiguity. For instance, misinterpretation of question intent was observed \nwhen the model failed to correctly address questions containing incorrect \ninterrogative pronouns.  \n Factualness errors arise when the model lacks the necessary supporting \nfacts to produce an accurate answer, which can be attributed to the model’s \nlimited knowledge of specific entities, attributes, or events. These errors are \nstraightforward and are a significant proportion of the model’s errors.  \n Specificity errors happen when the model fails to answer a problem at the \ncorrect level of specificity by providing answers that are too general or too \ndetailed. For example, the model might categorize different sub-genres of \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n8 \n \nmusic as distinct genres, incorrectly claiming that they are not in the same \ngenre.  \n Inference errors occur when the model, despite having the necessary \nknowledge, fails to reason effectively with the facts to derive the correct \nanswer. This can happen when the model struggles to make predictions \nbased on common sense or cannot correctly compare or analyze data. \nWe used these 4 categories to categorize the reasons for incorrect answers to \nquestions without images. \n \nStatistical analyses \nCategorical variables were analyzed with the chi-square test. A P value of <0.05 \nwas considered to indicate statistical significance. The rate of agreement \nbetween 2 sets of responses was calculated using Cohen's kappa statistic, and \nstatistical processing was conducted using R software version 4.3.0 (R \nFoundation for Statistical Computing, Vienna, Austria). The kappa accounts not \nonly for mere agreement but also for agreement by chance.35 \n \nResults \n \nOf the 475 questions with text choices, the LLM was able to answer 465 (Fig. \n2). Ten questions could not be answered because the LLM was unable to \nprocess information from the images. Of the 342 questions without images \nadministered to examinees and the LLM under the same conditions, 65.9% \nwere answered correctly by the LLM, and a passing score of 62.5% was \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n9 \n \nobtained. Of the 246 questions with images, 52.0% were answered correctly, \neven in the absence of any information about the image. This score was \nsignificantly lower than the score for questions without images (p < 0.001). Of \nall answerable questions, 62.3% were answered correctly.  \n \nFor questions without images, the annual rates of correct answers ranged from \n56.3% to 78.8% (Table 1). Moreover, the accuracy rate for scenario-based \nquestions without images was 69.1%, which was higher than the rate for stand-\nalone questions (62.1%), although the difference was not significant (p = 0.06) \n(Table 2). \n \nEach question was asked twice, and 72.5% elicited the same responses after 2 \nrepetitions. The kappa value was 0.70, indicating moderate agreement.35 \n \nOf the 233 incorrectly answered questions that did not include images, factual \nerrors accounted for 191 (82%), inference errors for 36 (15%), and \ncomprehension errors for 6 (3%).  \n \nDiscussion \n \nTo our knowledge, this is the first study to use an emergency medicine board \ncertification examination to evaluate an LLM. The findings demonstrated the \nconsiderable potential of LLMs to achieve a passing score on these \nexaminations. Additionally, we assessed the inferencing capabilities of LLMs, \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n10 \n \nincluding questions with images, and it nearly achieved a passing score for all \nanswerable questions. To evaluate its robustness, we conducted the \nexaminations twice and calculated the consistency rate, which showed \nmoderate agreement. \n \nThe LLM had a high inference capability for emergency scenarios. In this study, \nafter screening all questions, the scenario-based questions accounted for 25% \nof items. Of the 133 questions with images, 123 (92.5%) were answerable \nwithout image information, and 65 (52.8%) were answered correctly. Accuracy \nwas greater for scenario-based problems than for stand-alone questions. In \nmost previous studies, questions with images were excluded because the LLM \ncould not use information from images and answered questions under the \nconditions encountered by the examinees. However, this result may reflect the \nfact that questions include enough textual information that a single diagnosis \ncan be determined. Additionally, only 10% of incorrect answers resulted from \ninference errors. In a study of clinicopathological cases, which have been used \nsince the 1950s to evaluate differential diagnosis generators, an LLM \ndemonstrated a high clinical inference ability and provided correct diagnoses for \nthe differential diagnosis of 64% of challenging cases; in 39% of those, it \nprovided the most likely diagnosis.9 \n \nAlthough the performance of the present LLM was comparable to the level \nexpected of board-certified doctors in emergency medicine, most incorrect \nanswers were due to errors in factualness. Similar findings were observed in \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n11 \n \nother board certification examinations.27 In general, highly specialized models \nwith questions that are answerable by yes or no answers are more accurate,36 \nwhereas highly versatile models are less accurate.37 An LLM is a multifunctional \nmodel that has not been trained in a specific domain; however, it is possible to \nfine-tune these models on specific tasks or domains to improve their \nperformance in those areas. \n \nA concern in emergency medicine is the potential for patients to use LLMs for \nself-diagnosis, as an LLM tends to “write plausible sounding but incorrect or \nnonsensical answers.”38 It is possible for patients to be exposed to incorrect \nmedical knowledge generated by LLMs. Currently, AI errors in medical \nknowledge require intervention from physicians to correct, as AI itself has a \nlimited ability to self-correct errors. In contrast to another study, where self-\nagreement was close to 0.9,27 this study did not find a high level of agreement, \nindicating potential challenges in terms of robustness. It is important to exercise \ncaution with ChatGPT’s responses, as it may provide statements such as “I’m \nnot a doctor and can’t provide medical advice” or “Don’t delay seeking medical \nattention.” \n \nThis study has several limitations. First, the LLM may have already learned the \nquestions published. However, the LLM’s knowledge was based on information \npublished up to September and it performed best on the latest examinations, \ni.e., those after that date—no performance degradation was observed. Second, \nalthough ChatGPT-4 was trained in various languages, including Japanese, its \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n12 \n \nproficiency in non-English languages may not be equivalent to its proficiency in \nEnglish. Third, the research showed that inference errors accounted for a small \nproportion of errors; however, this does not necessarily mean the AI is good at \ninference. It could be that the types of questions asked did not sufficiently \nchallenge this skill. \n \nAcknowledgements \nYI, KN, and TN contributed to the design of the research. YI and KN collected \ndata. YI, KN, TN, TT, and SY contributed to the interpretation of the results. YI \nand KN wrote the original draft of the manuscript. TN, TT, and SY critically \nreviewed the manuscript for important intellectual content. All authors read and \napproved the final manuscript to be published.  \n \nConflict of interest  \nThe authors declare no conflict of interest for this article. \n \nData availability statement \nThe data that support the findings of this study are available upon request from \nthe corresponding author. \n \nEthics statement \nThis study only used publicly available data, did not include any patient \ninformation, and did not affect patient safety. Therefore, the requirement of \nreview by the Institutional Review Board was waived.   \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n13 \n \nReferences \n \n1. Tintinalli J, Ma O, Yealy D, et al. Tintinalli's Emergency Medicine, 9th ed.  \n2. Otaguro T , Tanaka H, Igarashi Y, et al. Machine Learning for Prediction of \nSuccessful Extubation of Mechanical Ventilated Patients in an Intensive Care \nUnit: A Retrospective O bservational Study. J Nippon Med Sch. 2021 Nov \n17;88(5):408-17. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/33692291. \n3. Igarashi Y, Nishimura K, Ogawa K, et al. Machine Learning Predicti on for \nSupplemental Oxygen Requirement in Patients with COVID-19. J Nippon Med \nSch. 2022 May 12;89(2):161 -8. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/34526457. \n4. Kitano S, Ogawa K, Igarashi Y, et al. Development of a Machine Learning Model \nto Predict Cardiac Arrest during Transport of Trauma Patients. J Nippon Med \nSch. 2023 May 30;90(2):186 -93. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36823128. \n5. . Will ChatGPT transform healthcare? Nat Med. 2023 Mar;29(3):505 -6. \nAvailable from: https://www.ncbi.nlm.nih.gov/pubmed/36918736. \n6. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient \nclinic letters. Lancet Digit Health. 2023 Apr;5(4):e179 -e81. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36894409. \n7. Ayers JW, Poliak A, Dredze M, et al. Comparing Physician and Artificial \nIntelligence Chatbot Responses to Patient Questions Posted to a Public Social \nMedia Forum. JAMA Intern Med. 2023 Jun 1;183(6):589 -96. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37115527. \n8. Dahdah JE, Kassab J, Helou MCE, Gaballa A, Sayles S, 3rd, Phelan MP . ChatGPT: \nA Valuable Tool for Emergency Medical Assistance. Ann Emerg Med. 2023 Jun \n16. Available from: https://www.ncbi.nlm.nih.gov/pubmed/37330721. \n9. Kanjee Z, Crowe B, Rodman A. Accuracy of a Generative Artificial Intelligence \nModel in a Complex Diagnostic Challenge. JAMA. 2023 Jun 15. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37318797. \n10. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: \nPotential for AI -assisted medical education using large language  models. \nPLOS Digit Health. 2023 Feb;2(2):e0000198. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36812645. \n11. Patel SB, Lam K. ChatGPT: the future of discharge summaries? Lancet Digit \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n14 \n \nHealth. 2023 Mar;5(3):e107 -e8. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36754724. \n12. Sabry Abdel -Messih M, Kamel Boulos MN. ChatGPT in Clinical Toxicology. \nJMIR Med Educ. 2023 Mar 8;9:e4 6876. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36867743. \n13. Kaneda Y, Tanimoto T , Ozaki A, Sato T , Takahashi K. Can ChatGPT Pass the \n2023 Japanese National Medical Licensing Examination? 2023. \n14. Kasai J, Kasai Y, Sakaguchi K, Yamada Y, Radev D. Evaluating gpt-4 and chatgpt \non japanese medical licensing examinations. arXiv preprint arXiv:230318027. \n2023. \n15. Takagi S, Watari T , Erabi A, Sakaguchi K. Performance of GPT-3.5 and GPT-4 \non the Japanese Medical Licensing Examination: Comparison Study. JMIR \nMedical Education. 2023;9(1):e48002. \n16. Gilson A, Safranek CW, Huang T, et al. How Does ChatGPT Perform on the \nUnited States Medical Licensing Examination? The Implications of Large \nLanguage Models for Medical Education and Knowledge Assessment. JMIR \nMed Educ. 2023 Feb 8;9:e45312. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36753318. \n17. Giannos P . Evaluating the limits of AI in medical specialisation: ChatGPT ’s \nperformance on the UK Neurology Specialty Certificate Examination. BMJ \nNeurology Open. 2023;5(1). \n18. Noda R, Izaki Y, Kitano F, Komatsu J, Ichikawa D, Shibagaki Y. Performance of \nChatGPT and Bard in Self -Assessment Questions for Nephrology Board \nRenewal. medRxiv. 2023:2023.06. 06.23291070. \n19. Weng T-L, Wang Y-M, Chang S, Chen T -J, Hwang S-J. ChatGPT failed Taiwan’s \nFamily Medicine Board Exam. Journal of the Chinese Medical \nAssociation:10.1097. \n20. Oh N, Choi GS, Lee WY. ChatGPT goes to the operating room: evaluating GPT-\n4 performance and its potential in surgical education and training in the era \nof large language models. Ann Surg Treat Res. 2023 May;104(5):269 -73. \nAvailable from: https://www.ncbi.nlm.nih.gov/pubmed/37179699. \n21. Ali R, Tang OY, Connolly ID, et al. Performance of ChatGPT , GPT-4, and Google \nbard on a neurosurgery oral boards preparation question bank. \nNeurosurgery. 2022:10.1227. \n22. Ali R, Tang OY, Connolly ID, et al. Performance of ChatGPT and GPT -4 on \nneurosurgery written board examinations. medRxiv. 2023:2023.03. \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n15 \n \n25.23287743. \n23. Lum ZC. Can artificial intelligence pass the American Board of Orthopaedic \nSurgery examination? Orthopaedic re sidents versus ChatGPT . Clinical \nOrthopaedics and Related Research®. 2022:10.1097. \n24. Huynh LM, Bonebrake BT , Schultis K, Quach A, Deibert CM. New Artificial \nIntelligence ChatGPT Performs Poorly on the 2022 Self -assessment Study \nProgram for Urology. Urolo gy Practice. 2023:10.1097/UPJ. \n0000000000000406. \n25. Gupta R, Herzog I, Park JB, et al. Performance of ChatGPT on the plastic \nsurgery inservice training examination. Aesthetic surgery journal. \n2023:sjad128. \n26. Li SW, Kemp MW, Logan SJS, et al. ChatGPT outscored human candidates in a \nvirtual objective structured clinical examination in obstetrics and gynecology. \nAm J Obstet Gynecol. 2023 Apr 22. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37088277. \n27. Shay D, Kumar B, Bellamy D, et al. Assessment of ChatGPT success with \nspecialty medical knowledge using anaesthesiology board examination \npractice questions. British journal of anaesthesia. 2023. \n28. Bhayana R, Krishna S, Bleakney RR. Performance of ChatGPT on a Radiology \nBoard-style Examination: Insights into Current Strengths and Limitations. \nRadiology. 2023 May 16:230582. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37191485. \n29. Passby L, Jenko N, Wernham A. Performance of ChatGPT on dermatology \nSpecialty Certificate Examination multiple choice questions. Clin Exp \nDermatol. 2023 Jun 2. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37264670. \n30. Teebagy S, Colwell L, Wood E, Yaghy A, Faustina M. Improved performance of \nChatGPT-4 on the OKAP exam: A comparative study with ChatGPT -3.5. \nmedRxiv. 2023:2023.04. 03.23287957. \n31. Hoch CC, Wollenberg B, Lu ers JC, et al. ChatGPT's quiz skills in different \notolaryngology subspecialties: an analysis of 2576 single -choice and \nmultiple-choice board certification preparation questions. Eur Arch \nOtorhinolaryngol. 2023 Jun 7. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/37285018. \n32. Fijacko N, Gosak L, Stiglic G, Picard CT , John Douma M. Can ChatGPT pass the \nlife support exams without entering the American heart association course? \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n16 \n \nResuscitation. 2 023 Apr;185:109732. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36775020. \n33. Japanese Association of Acute Medicine. Application for board -certified \ndoctor of emergency and critical care  medicine [cited 10 June 2023] \n[Internet]. [cited 2023 June 10]. Available from: \nhttps://www.jaam.jp/info/2021/info-20211124.html  \n34. Zheng S, Huang J, Chang KC -C. Why Does ChatGPT Fall Shor t in Answering \nQuestions Faithfully? arXiv preprint arXiv:230410513. 2023. \n35. McHugh ML. Interrater reliability: the kappa statistic. Biochem Med (Zagreb). \n2012;22(3):276-82. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/23092060. \n36. Jin Q, Dhingra B, Liu Z, Cohen W, Lu X. PubMedQA: A Dataset for Biomedical \nResearch Question Answering. Hong Kong, China: Association for \nComputational Linguistics; 2019 November. p.2567-77. \n37. Jin D, Pan E, Oufattole N, Weng W-H, Fang H, Szolovits P . What Disease Does \nThis Patient Have? A Large-Scale Open Domain Question Answering Dataset \nfrom Medical Exams. Applied Sciences. 2021;11(14):6421. Available from: \nhttps://www.mdpi.com/2076-3417/11/14/6421. \n38. Brainard J. Journals take up arms against AI -written text. Science. 2023 Feb \n24;379(6634):740-1. Available from: \nhttps://www.ncbi.nlm.nih.gov/pubmed/36821673. \n  \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\n17 \n \nFigure legends \n \nFigure 1. List of sample questions \nThe questions are scenario-based questions on sepsis. ChatGPT answered \neach question with an explanation. On the basis of the latest guidelines, \nquestions on infusion volume calculation and drug selection were correctly \nanswered. \n \nFigure 2. Question flowchart \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\nFig. 1 \n \n \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\nFig. 2 \n \n \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\nTable 1. Correct answers rates provided by ChatGPT-4 \n \nYear Questions without images Questions with images  Total \n2022 104/132 (78.8%) 18/56 (32.1%) 122/188 (71.3%) \n2021 67/118 (56.8%) 45/68 (66.2%) 112/186 (60.2%) \n2020 89/134 (66.4%) 22/50 (44.0%) 111/184 (62.0%) \n2019 101/140 (72.1%) 30/54 (55.6%) 131/194 (67.0%) \n2018 90/160 (56.3%) 13/18 (72.2%) 103/178 (60.7%) \nTotal 451/684 (65.9%) 128/246 (52.0%) 579/930 (62.3%) \n \n  \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)\nTable 2. Correct answers rates to stand-alone and scenario-based questions \n \nType Stand-alone Scenario-based \nQuestions without images 195/314 (62.1%) 256/ 370 (69.2%) \nQuestions with images 0/2 (0%) 128/244 (52.5%) \nTotal 195/316 (61.7%) 384/614 (62.5%) \n \n \n \n\u0000J\u0000o\u0000u\u0000r\u0000n\u0000a\u0000l\u0000 \u0000o\u0000f\u0000 \u0000N\u0000i\u0000p\u0000p\u0000o\u0000n\u0000 \u0000M\u0000e\u0000d\u0000i\u0000c\u0000a\u0000l\u0000 \u0000S\u0000c\u0000h\u0000o\u0000o\u0000l0\u0000\u0000J\u0000-\u0000S\u0000T\u0000A\u0000G\u0000E0\u0000\u0000A\u0000d\u0000v\u0000a\u0000n\u0000c\u0000e\u0000 \u0000P\u0000u\u0000b\u0000l\u0000i\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000M\u0000a\u0000r\u0000c\u0000h\u0000 \u00002\u0000,\u0000 \u00002\u00000\u00002\u00004\u0000)",
  "topic": "Certification",
  "concepts": [
    {
      "name": "Certification",
      "score": 0.6696585416793823
    },
    {
      "name": "Medical emergency",
      "score": 0.48809587955474854
    },
    {
      "name": "Medicine",
      "score": 0.4818635880947113
    },
    {
      "name": "Acute medicine",
      "score": 0.47431254386901855
    },
    {
      "name": "Health care",
      "score": 0.42347386479377747
    },
    {
      "name": "Intensive care medicine",
      "score": 0.1641048789024353
    },
    {
      "name": "Political science",
      "score": 0.11867356300354004
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80188885",
      "name": "Nippon Medical School",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I169521973",
      "name": "University of New Mexico",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210093346",
      "name": "Nippon Medical School Musashi Kosugi Hospital",
      "country": "JP"
    }
  ]
}