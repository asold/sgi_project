{
  "title": "LST-EMG-Net: Long short-term transformer feature fusion network for sEMG gesture recognition",
  "url": "https://openalex.org/W4323319976",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2112135474",
      "name": "Wenli Zhang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3108822944",
      "name": "Ting-Song Zhao",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103815782",
      "name": "Jianyi Zhang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110223640",
      "name": "Yufei Wang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2112135474",
      "name": "Wenli Zhang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3108822944",
      "name": "Ting-Song Zhao",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103815782",
      "name": "Jianyi Zhang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110223640",
      "name": "Yufei Wang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3092342532",
    "https://openalex.org/W4312532542",
    "https://openalex.org/W2516710120",
    "https://openalex.org/W6684671651",
    "https://openalex.org/W2082480549",
    "https://openalex.org/W2041345816",
    "https://openalex.org/W4220825763",
    "https://openalex.org/W4228997986",
    "https://openalex.org/W3114261751",
    "https://openalex.org/W2133644836",
    "https://openalex.org/W3203224907",
    "https://openalex.org/W4225892562",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3114693512",
    "https://openalex.org/W2898716605",
    "https://openalex.org/W2021356830",
    "https://openalex.org/W4282963349",
    "https://openalex.org/W4281255185",
    "https://openalex.org/W4313312686",
    "https://openalex.org/W2066459718",
    "https://openalex.org/W4289176422",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2344094861",
    "https://openalex.org/W2991089415",
    "https://openalex.org/W2591690220",
    "https://openalex.org/W4221167488",
    "https://openalex.org/W3157903123",
    "https://openalex.org/W2156341082",
    "https://openalex.org/W2082740732",
    "https://openalex.org/W2162212366",
    "https://openalex.org/W2901351566",
    "https://openalex.org/W2916667546",
    "https://openalex.org/W3092657362",
    "https://openalex.org/W2761881812",
    "https://openalex.org/W3044285175",
    "https://openalex.org/W2981877040",
    "https://openalex.org/W2762706434",
    "https://openalex.org/W3203308684",
    "https://openalex.org/W3005864656",
    "https://openalex.org/W2171188488",
    "https://openalex.org/W4221147059",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2003736918",
    "https://openalex.org/W2775447708",
    "https://openalex.org/W2765746460",
    "https://openalex.org/W2992504441",
    "https://openalex.org/W3097692841",
    "https://openalex.org/W3114397111",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W4295436641",
    "https://openalex.org/W2169931829",
    "https://openalex.org/W2129566274",
    "https://openalex.org/W4226444284",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "With the development of signal analysis technology and artificial intelligence, surface electromyography (sEMG) signal gesture recognition is widely used in rehabilitation therapy, human-computer interaction, and other fields. Deep learning has gradually become the mainstream technology for gesture recognition. It is necessary to consider the characteristics of the surface EMG signal when constructing the deep learning model. The surface electromyography signal is an information carrier that can reflect neuromuscular activity. Under the same circumstances, a longer signal segment contains more information about muscle activity, and a shorter segment contains less information about muscle activity. Thus, signals with longer segments are suitable for recognizing gestures that mobilize complex muscle activity, and signals with shorter segments are suitable for recognizing gestures that mobilize simple muscle activity. However, current deep learning models usually extract features from single-length signal segments. This can easily cause a mismatch between the amount of information in the features and the information needed to recognize gestures, which is not conducive to improving the accuracy and stability of recognition. Therefore, in this article, we develop a long short-term transformer feature fusion network (referred to as LST-EMG-Net) that considers the differences in the timing lengths of EMG segments required for the recognition of different gestures. LST-EMG-Net imports multichannel sEMG datasets into a long short-term encoder. The encoder extracts the sEMG signals’ long short-term features. Finally, we successfully fuse the features using a feature cross-attention module and output the gesture category. We evaluated LST-EMG-Net on multiple datasets based on sparse channels and high density. It reached 81.47, 88.24, and 98.95% accuracy on Ninapro DB2E2, DB5E3 partial gesture, and CapgMyo DB-c, respectively. Following the experiment, we demonstrated that LST-EMG-Net could increase the accuracy and stability of various gesture identification and recognition tasks better than existing networks.",
  "full_text": "fnbot-17-1127338 February 24, 2023 Time: 8:45 # 1\nTYPE Original Research\nPUBLISHED 28 February 2023\nDOI 10.3389/fnbot.2023.1127338\nOPEN ACCESS\nEDITED BY\nR. A. R. C. Gopura,\nUniversity of Moratuwa, Sri Lanka\nREVIEWED BY\nAsif Ali Laghari,\nSindh Madressatul Islam University, Pakistan\nYang Sun,\nShenyang Normal University, China\n*CORRESPONDENCE\nWenli Zhang\nzhangwenli@bjut.edu.cn\nRECEIVED 19 December 2022\nACCEPTED 14 February 2023\nPUBLISHED 28 February 2023\nCITATION\nZhang W, Zhao T, Zhang J and Wang Y (2023)\nLST-EMG-Net: Long short-term transformer\nfeature fusion network for sEMG gesture\nrecognition.\nFront. Neurorobot.17:1127338.\ndoi: 10.3389/fnbot.2023.1127338\nCOPYRIGHT\n© 2023 Zhang, Zhao, Zhang and Wang. This is\nan open-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with\nthese terms.\nLST-EMG-Net: Long short-term\ntransformer feature fusion\nnetwork for sEMG gesture\nrecognition\nWenli Zhang1*, Tingsong Zhao1, Jianyi Zhang2 and Yufei Wang1\n1Faculty of Information Technology, Beijing University of Technology, Beijing, China,2College of Art\nand Design, Beijing University of Technology, Beijing, China\nWith the development of signal analysis technology and artiﬁcial intelligence,\nsurface electromyography (sEMG) signal gesture recognition is widely used\nin rehabilitation therapy, human-computer interaction, and other ﬁelds. Deep\nlearning has gradually become the mainstream technology for gesture\nrecognition. It is necessary to consider the characteristics of the surface\nEMG signal when constructing the deep learning model. The surface\nelectromyography signal is an information carrier that can reﬂect neuromuscular\nactivity. Under the same circumstances, a longer signal segment contains\nmore information about muscle activity, and a shorter segment contains less\ninformation about muscle activity. Thus, signals with longer segments are suitable\nfor recognizing gestures that mobilize complex muscle activity, and signals with\nshorter segments are suitable for recognizing gestures that mobilize simple\nmuscle activity. However, current deep learning models usually extract features\nfrom single-length signal segments. This can easily cause a mismatch between\nthe amount of information in the features and the information needed to\nrecognize gestures, which is not conducive to improving the accuracy and\nstability of recognition. Therefore, in this article, we develop a long short-\nterm transformer feature fusion network (referred to as LST-EMG-Net) that\nconsiders the differences in the timing lengths of EMG segments required\nfor the recognition of different gestures. LST-EMG-Net imports multichannel\nsEMG datasets into a long short-term encoder. The encoder extracts the sEMG\nsignals’ long short-term features. Finally, we successfully fuse the features\nusing a feature cross-attention module and output the gesture category. We\nevaluated LST-EMG-Net on multiple datasets based on sparse channels and high\ndensity. It reached 81.47, 88.24, and 98.95% accuracy on Ninapro DB2E2, DB5E3\npartial gesture, and CapgMyo DB-c, respectively. Following the experiment, we\ndemonstrated that LST-EMG-Net could increase the accuracy and stability of\nvarious gesture identiﬁcation and recognition tasks better than existing networks.\nKEYWORDS\nsEMG signals, gesture recognition, multi-scale features, multi-head attention, stroke\nrehabilitation, human-computer interaction\nFrontiers in Neurorobotics 01 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 2\nZhang et al. 10.3389/fnbot.2023.1127338\n1. Introduction\nSurface electromyography signals are bioelectric signals\ngenerated during muscle contractions. sEMG signals can be\ncollected non-invasively, safely, and easily, and sEMG can directly\nreﬂect the state of muscle activity. By analyzing the sEMG,\ngestures can be accurately recognized. sEMG-based gesture\nrecognition methods have the advantages of being faster and\nmore environmentally independent than vision-based gesture\nrecognition methods (Oudah et al., 2020; Mujahid et al., 2021).\nTherefore, sEMG-based gesture recognition methods have strong\napplication possibilities in sectors related to human-computer\ninterfaces, including intelligent prosthetics (Cipriani et al.,\n2008), upper limb rehabilitation exoskeletons (Leonardis et al.,\n2015), robotic arm control (Wang et al., 2012), and others\n(Muri et al., 2013).\nAn sEMG-based gesture recognition framework generally\nconsists of four parts: Signal preprocessing, signal segmentation,\nfeature extraction, and gesture classiﬁcation mechanisms (Parajuli\net al., 2019). For traditional machine learning algorithms, the\nfeatures used for classiﬁcation are usually handcrafted by human\nexperts. Therefore, the quality of the feature set selected by\nthe experts directly determines the success or failure of the\nrecognition task. Numerous gesture recognition studies have\nused traditional classiﬁers for manual features. For example,\nsupport vector machines (SVMs) (Alseed and Tasoglu, 2022;\nBriouza et al., 2022), k-nearest neighbors (KNN) (Baygin et al.,\n2022), linear discriminant analysis (LDA) (Narayan, 2021), hidden\nMarkov models (HMMs) (Hu and Wang, 2020), and random\nforests (RF) (Xue et al., 2019; Jia et al., 2021) have made some\nprogress. However, the accuracy and stability of traditional learning\nalgorithms do not yet satisfy practical application requirements\nwhen applied to large-scale datasets consisting of larger numbers\nof hand gestures or wrist movements. Therefore, improving the\naccuracy and stability of hand gesture recognition is still a hot\nresearch topic.\nIn recent years, with the rapid development of artiﬁcial\nintelligence technology, deep learning has shown great potential in\nmedical rehabilitation ﬁelds such as physiological signal processing\n(Rim et al., 2020; Al-Saegh et al., 2021) and medical image imaging\n(Karim et al., 2022; Laghari et al., 2022). In gesture recognition\ntasks based on surface EMG signals, deep learning methods can\nautomatically learn the dependencies or intrinsic connections of\nthe amplitude changes at each sampling point in surface EMG\nsignals due to their deep network architectures. The dependencies\nand intrinsic connections can be considered the muscle activity\nfeatures that indirectly express forearm muscle activity conditions,\nand gesture information can be obtained under this condition. The\nfollowing research summarizes the feature extraction methods that\nhave been developed under diﬀerent model architectures for deep\nlearning algorithms.\n1.1. Related work\nDeep learning models outperform traditional machine\nlearning models, so many researchers use deep learning for\ngesture recognition. Convolutional neural networks (CNNs)\n(Atzori et al., 2016; Wei et al., 2019; Chen et al., 2020), recurrent\nneural networks (RNNs) (Vaswani et al., 2017; Hu et al., 2018;\nXia et al., 2018), and transformer-based gesture identiﬁcation\napproaches (Rahimian et al., 2021; Siddhad et al., 2022) are the\ncurrent prevalent deep learning gesture recognition algorithms.\nResearchers have conducted studies on CNN-based gesture\nrecognition methods (Atzori et al., 2016; Wei et al., 2019; Chen\net al., 2020). Atzori et al. (2016) ﬁrst applied a CNN to an sEMG\ngesture recognition task using only a shallow network constructed\nfrom four convolutional layers. The accuracy was comparable to\nthat of traditional machine learning gesture recognition methods.\nWei et al. (2019) proposed a multistream convolutional neural\nnetwork (MSCNN) with decomposition and fusion stages. The\nnetwork learned the correlations between gesture muscles, and\nit was evaluated on three benchmark databases. The results\nshowed that multistream CNNs outperformed simple CNNs and\nrandom forest classiﬁers, but the computational eﬀort of the\nmethod increased multiplicatively with the number of myoelectric\nchannels.\nSome researchers have combined recurrent neural networks\n(RNNs) with CNNs, using CNNs for feature extraction and RNNs\nfor modeling time dependencies (Vaswani et al., 2017; Hu et al.,\n2018; Xia et al., 2018). An RNN has all nodes connected in a\nchain-like manner, so it can handle short-term memorization tasks\nwell. For example, Xia et al. (2018) proposed the RCNN, a single-\nbranch deep structure with a CNN and RNN connected serially.\nThe CNN extracts the myoelectric local spatial features, and the\nRNN saves the local spatial features and eﬃciently passes them\nto the next moment to update the model weights. This network\nhas an advantage in learning complex motion features. However,\nthe RCNN can have a sharp decrease in recognition accuracy over\ntime compared to the CNN. Xia et al. (2018) tried to use large\nneural networks with more layers and parameters to improve the\nrobustness of the model to time variations. Nevertheless, problems\nsuch as a heavy training time burden and system recognition\ndelays are caused by the inability of RNNs to train in parallel.\nHu et al. (2018) proposed an attention-based hybrid CNN-RNN\nmodel. The model uses a CNN to extract spatial feature maps of\nsuccessive frames of sEMG signals and an RNN to further extract\ntemporal features from the feature maps. The model was able to\neﬀectively extract the temporal correlation of each channel of sparse\nmultichannel sEMG signals.\nIn recent years, after the transformer model (Vaswani\net al., 2017) was proposed, it attracted attention in natural\nlanguage processing and computer vision tasks. The transformer\nmodel entirely relies on self-attention, which can capture global\ndependencies in the input data to achieve parallel computation\nand improve computational eﬃciency. At the same time, self-\nattention can produce more interpretable models. For example,\nSiddhad et al. (2022) explored the eﬀectiveness of transformer\nnetworks for the classiﬁcation of raw EEG data. They used\nraw resting-state EEG data to classify people by age and\ngender, and the classiﬁcation results showed that the transformer\nnetwork was comparable in accuracy to state-of-the-art CNN and\nLSTM recognition with feature extraction. This proved that the\ntransformer network has excellent feature extraction capability\nfor time-series signals. Some researchers have already used\ntransformers for hand gesture recognition (Rahimian et al., 2021;\nMontazerin et al., 2022). For example, Rahimian et al. (2021) used\nFrontiers in Neurorobotics 02 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 3\nZhang et al. 10.3389/fnbot.2023.1127338\na vision-based transformer model architecture for the classiﬁcation\nand recognition of upper limb gestures, and the recognition\naccuracy on Ninapro DB2 Exercise B for 17 gestures reached\nstate-of-the-art performance at that time. Montazerin et al. (2022)\nalso proposed a transformer framework based on ViT for high-\ndensity sEMG gesture recognition with 128 arrayed electrodes,\nwhich can solve the time burden problem of RNN structures that\nare not trained in parallel. However, current transformer-based\ngesture recognition networks only apply the image classiﬁcation\nscheme to EMG recognition. The network structure is not\ndesigned according to the characteristics of gesture activity and\nsEMG signals.\nIn summary, gesture recognition is mainly implemented by\ndeep learning methods at this stage. Among them, the transformer\nmodel has become a hot research topic because of its self-attention\nstructure, which can extract sEMG signal temporal muscle activity\nfeatures well and performs well in gesture recognition. However,\ncurrent recognition methods still suﬀer from a mismatch between\nthe amount of information contained in the extracted features\nand the amount of information required to recognize gestures\nwhen implementing multicategory gesture recognition. The reason\nfor the mismatch problem is that there are diﬀerences in the\nstability exhibited in the sEMG signal due to the diﬀerent\nmuscle activity, muscle contraction changes, and muscle strength\nchanges mobilized (Farago et al., 2022; Li et al., 2022). The\nsEMG signals of more complex gestures are less stable, and\nsimpler gesture movements have better EMG signal stability. To\nrecognize complex gestures from less stable EMG signals, the\nlengths of the feature extraction segments need to increase to\nyield a suﬃcient amount of information for recognition (Nazmi\net al., 2017). However, most of the existing related works do\nnot consider the characteristic that the lengths of EMG signals\nare diﬀerent for diﬀerent gestures. They all intercept ﬁxed-length\nEMG signals for spatial and temporal feature extraction, which\nleads to a mismatch between the amount of feature information\nextracted by the designed models and the corresponding gestures\nand aﬀects the accuracy and robustness of the gesture recognition\nframework.\n1.2. Contributions\nTo address the above problems, it is necessary to propose\na gesture recognition method to extract moderate feature\ninformation from EMG sample segments. Therefore, we propose a\ngesture recognition method based on LST-EMG-Net. It can extract\nlong- and short-sequence features in sEMG windows and fuse them\neﬀectively to achieve high-accuracy recognition of complex and\nsimple gestures. The method proposed in this article makes the\nfollowing three contributions:\n(1) To address the mismatch between the feature information and\nrequired information in a multicategory gesture recognition\ntask, we propose a long short-term encoder and use the\nlinear projection in the encoder to construct a long-\nterm branch and a short-term branch. Then, each branch\nfeature is extracted by a long- or short-term subencoder\nto achieve multiscale time feature extraction and solve the\nproblem of redundant or insuﬃcient feature extraction. To\nfurther improve the feature quality, we use sEMG channel\nattention to dynamically set the weights of each channel of\nthe sEMG windows.\n(2) We propose a cross-attention module for long- and short-\nterm features from the encoder to fuse the long- and\nshort-term features eﬃciently. This module uses an\nattention-based approach to cross-learn one branch’s\nclassiﬁcation token and another branch’s patch tokens\nin the feature. This module can eﬀectively fuse the\nmuscle activity information and enhance the eﬃciency\nof feature fusion due to its low computational eﬀort. It\nﬁnally achieves the goal of improving the accuracy of hand\ngesture recognition.\n(3) To address the problem that individual sEMG signals\nare diﬃcult to collect in large quantities, we propose a\nsignal augmentation method based on sEMG windows.\nThis method adopts random windows and sEMG time\ndelays to augment the original sEMG windows and\nconstructs a training dataset together with the original\nEMG timing windows. This method reduces the burden of\ndata collection.\nThe remainder of the article is organized as follows. The dataset\nand the sEMG signal enhancement method utilized in this article\nare described in detail in Section “2. Materials and methods.”\nThe framework of LST-EMG-Net, including the motivation of the\nstudy and the submodule structure, is presented in Section “3.\nThe long short-term sEMG transformer feature fusion network\nframework.” The experimental environment of LST-EMG-Net and\nexperimental results are presented in Section “4. Experiments and\nresults.” Finally, the conclusions of this article are drawn in Section\n“5. Conclusion.”\n2. Materials and methods\n2.1. The datasets\nWe use two types of datasets, a sparse sEMG dataset and a\nhigh-density sEMG dataset, to evaluate our LST-EMG-Net. The\nsparse dataset includes Ninapro DB2 and DB5 (Atzori et al., 2012,\n2014a,b; Gijsberts et al., 2014; Du et al., 2017; Pizzolato et al.,\n2017). The high-density dataset is the public CapgMyo dataset\n(Du et al., 2017).\nSparse sEMG dataset: We use 17 basic wrist movements and\nisotonic hand conﬁgurations from the DB2 Exercise B subdataset\n(as shown in Figure 1A). In the DB2 dataset, the muscular\nactivities were measured using 12 active double-diﬀerential wireless\nelectrodes from a Delsys Trigno Wireless EMG system at a\nsampling frequency of 2 kHz. The DB5 dataset uses 18 gestures\nfrom the Exercise C subdataset that fully mobilize muscle activity\nand facilitate muscle recovery training (as shown in Figure 1B).\nThe DB5 dataset was taken from 10 healthy subjects. Its collection\ndevice was a pair of Thalmic Labs Myo (Myo) armbands. Each Myo\nhad eight single-channel electrodes, each with a sampling rate of\n200 Hz. The DB2 dataset and DB5 dataset collection rules were the\nsame. Each gesture was repeated six times, each acquisition had a\nFrontiers in Neurorobotics 03 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 4\nZhang et al. 10.3389/fnbot.2023.1127338\nFIGURE 1\nTypes of gestures in the datasets used in this manuscript. (A) Ninapro DB2 exercise B dataset 17 gestures. (B) Ninapro DB5 exercise C dataset 18\ngestures. (C) CapgMyo DB-c dataset 12 gestures.\n5 s activity signal, and each acquisition interval was 3 s. The 1st,\n3rd, 4th, and 6th repetitions of the gesture were used to construct\nthe training set, and the 2nd and 5th repetitions were used to build\nthe test set.\nHigh-density sEMG dataset: We used 12 basic ﬁnger\nmovements from the DB-C subdataset of CapgMyo (as shown\nin Figure 1C). The dataset was acquired at a sampling rate of\n1,000 Hz with an array of 8 2 ×8 diﬀerential electrodes to capture\nthe activity of the right peripheral forearm muscle groups. The\nCapgMyo dataset was obtained from 10 users who repeated several\nmovements 10 times, each lasting 3 s, followed by 7 s of rest.\nOdd-numbered repetitions were used to construct the training set,\nand even-numbered repetitions were used to build the test set.\n2.2. Preprocessing\nBefore performing the classiﬁcation task, the sEMG signals\nwere preprocessed. The sEMG signals were ﬁltered from power\nline interference before signal acquisition due to the built-in\n50 Hz trap ﬁlter in the sEMG sensor. We only used a blind\nsource separation process called fast independent component\nanalysis (Fast ICA) (Comon, 1992) on the raw signals to eliminate\ninterchannel crosstalk. Then, Z Score standard normalization was\nused to process the sEMG signals after ﬁltering the noise. Z Score\nnormalization of a channel is shown in Equation 1.\nF(xt) =xt −µ\nσ (1)\nWhere xt is the sEMG signal, µ is the mean value of the sEMG\nsignal and σ is the standard deviation of the sEMG signal.\nThis article uses the sliding-window method with overlap to\nsegment the normalized EMG signal to obtain the original EMG\ntiming window. The length of the sliding window is set according\nto the related work of Scheme and Englehart (2011). It is noted\nthat 300–800 ms is an acceptable latency. Considering the delay and\ncomputation volume, we set the window length of the Ninapro DB2\ndataset as 300 ms, its window distance as 10 ms, the window length\nof the Ninapro DB5 dataset as 500 ms, its window length as 100 ms,\nthe window length of the CapgMyo dataset as 60 ms and its window\ndistance as 10 ms.\n2.3. Signal augmentation based on sEMG\nwindows (SA)\nDue to the lack of a priori experience of the self-attention of\nthe transformer network, such as inductive bias and translational\ninvariance, the transformer model requires a larger dataset to reach\nconvergence. However, most current recognition methods require\nthe acquisition of individual sEMG signals to build recognition\nmodels, whereas the collection of sEMG data from a large number\nof individuals is diﬃcult to achieve because of the high time cost\nand muscle fatigue. Therefore, we propose a signal augmentation\nmethod based on sEMG windows to solve the above problem.\nThis method is used to obtain random windows and time-delay\nenhancement windows to increase the number of training samples.\nFirst, the original sEMG signals are input into the random\nwindow selection module. This module randomly selects the start\npoint of the window within each class of gesture action sequences\nand determines the end point based on the window length to\nobtain a random window for that type of gesture. This operation\nis repeated to obtain the random windows for all gesture actions.\nSecond, the transceiver delay and transmission interference of\nthe acquisition device (Liu et al., 2016) make it inevitable that sEMG\nwill miss some sample points, which impacts the sEMG recognition\nmodel’s robustness. Therefore, this step randomly selects a certain\npercentage of the original sEMG window to input to the time-\ndelay enhancement module. This model selects sequence An in\nthe original window randomly and selects sequence B at the next\nsampling moment (where the numbers of sampling points of\nsequence A and sequence B are the same); ﬁnally, sequence A is\ndeleted, and the sampling points of sequence B are copied to the\noriginal sequence A sampling moment to obtain the time-delay\nenhancement windows, as shown in Figure 2.\nThe proposed signal augmentation method expands the\ntraining samples by doubling the number of sEMG windows. Take\nFrontiers in Neurorobotics 04 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 5\nZhang et al. 10.3389/fnbot.2023.1127338\nFIGURE 2\nSchematic diagram of time-delay enhancement module.\nDB5 dataset subject ten as an example: we have 5,886 original EMG\nwindows initially; then, we obtain 2,943 random windows by the\nrandom window selection module and 2,943 time-delay enhanced\nwindows by the time-delay enhancement module. Finally, we\nobtain 11,772 windows.\n3. The long short-term sEMG\ntransformer feature fusion network\nframework\nIn the ﬁeld of gesture recognition, we often use the fractal\ndimension to calculate the complexity and stability of the EMG\nsignal (Naik and Nguyen, 2014). Gestures with low fractal\ndimensions are simple gestures whose signal stability is higher, and\nthe electrode position, muscle contraction, and muscle force change\nmore slowly in these types of gestures; they include single-ﬁnger\nﬂexion, multiﬁnger ﬂexion, and wrist translation (Namazi, 2019a).\nGestures with high fractal dimensions are complex gestures with\nlow signal smoothness. The electrode position, muscle contraction,\nand muscle force change more rapidly in these gestures, such\nas wrist-hand linkage and dynamic operations (e.g., grasping,\npressing, and tapping). In addition, when the subject increases\nthe force of the gesture, it also leads to an increase in the fractal\ndimension of the EMG signal (Menon et al., 2017; Namazi, 2019b),\nwhich in turn aﬀects the stability of the EMG signal. Therefore,\nwe need longer EMG sequence segments to extract high-quality\nfeatures from signals with low stability when recognizing complex\ngestures; we need shorter EMG sequence segments to recognize\nsimple gestures.\nOn the other hand, the transformer can capture longer\ndependent information in the temporal signal classiﬁcation task\ndue to its self-attention structure. However, current transformer\nnetworks are designed based on multihead attention; in this\nmethod, there is a lack of constraints between every pair of heads,\nwhich makes the output similar between network layers, eventually\nleading to the problem of attention collapse (Zhou et al., 2021) and\naﬀecting accuracy. Therefore, we propose LST-EMG-Net to extract\nthe sEMG features of both long-term and short-term segments in\nthe sEMG window to perform multitemporal feature extraction and\nfeature fusion for various-complexity gesture recognition tasks. To\nfurther improve the gesture recognition eﬀect, the network adds a\ntransposition matrix between the heads of multihead attention to\nsolve the attention collapse problem.\nThe overall structure of LST-EMG-Net is shown in Figure 3; it\nconsists of three parts: the long short-term encoder, feature cross-\nattention module, and gesture classiﬁcation module, of which the\nlong short-term encoder module and the feature cross-attention\nmodule correspond to contributions 1 and 2 of this article,\nrespectively.\nLong short-term encoder: This module takes as input a set of\nmultichannel sEMG window collections D ={(Xi, yi)}m\ni=1 . Then,\nthe input is given importance weights for each channel, and long-\nterm features ZL\nN and short-term features ZS\nM are extracted from the\nsEMG window. The temporal window setD consists of m windows;\nthe i window is denoted byXi ∈RHxW (1 ≤i ≤M), and the gesture\nlabel is denoted by yi. H is the number of EMG signal channels, and\nW is the number of sampling points per window.\nFeature cross-attention module: This module receives the long-\nterm features Z L\nN and short-term features ZS\nM extracted by the\nlong short-term encoder. The long-term features and short-term\nfeatures are cross-learned using scaled dot-product attention, and\nthe cross-learned long-term features Z L′\nN and short-term features\nZS′\nM are output.\nFrontiers in Neurorobotics 05 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 6\nZhang et al. 10.3389/fnbot.2023.1127338\nFIGURE 3\nThe proposed LST-EMG-Net structure. Among them, the sEmg channel attention, multi-head re-attention and the feature cross-attention module\n(yellow module) are the contribution points of this manuscript.\nGesture classiﬁcation module: This module receives the long-\nterm features Z L′\nN and short-term features ZS′\nM after cross-learning,\ncalculates the gesture category probabilities corresponding to\nthe long short-term features, fuses the gesture probabilities for\ndecision-level fusion and ﬁnally outputs the gesture categories.\n3.1. Long short-term encoder\nThis module mainly consists of three parts: sEMG channel\nattention, linear projection, and the long/short-term sub encoder.\n3.1.1. Surface electromyography channel\nattention (ECA)\nIt is commonly accepted in medical statistics that sEMG\nsignals from one muscle are statistically independent of those\nfrom neighboring muscles (Naik et al., 2007) and that speciﬁc\nmuscles play more critical roles in certain hand movements (Huang\net al., 2009). However, most of the previous methods extracted\ncorrelations between channels and gestures by constructing\nmultistream inputs with channel decomposition signals. As an\nMSCNN (Wei et al., 2019) assigns network input streams to\neach channel and fuses them, the computational eﬀort increases\nexponentially when the number of channels is high. Therefore,\nto reduce the computational eﬀort, we propose modularized\nmyoelectric channel attention based on scaled dot-product\nattention to perform correlation extraction of channels and gestures\nand dynamically adjust the channel weights according to the\ngestures, increasing the channel weights with strong correlations\nand decreasing the channel weights with weak correlations.\nFirst, the sEMG window X i converts each channel into K and\nQ. Then, we calculate the correlations between channels using\nscaled point multiplier attention and output the sEMG window\nwith channel weights Pi, as shown in Equation 2.\nPi =Xi +Softmax\n(\nAvgpooling(Q) ×Avgpooling(K)√\ndk\n)\nXi (2)\nFrontiers in Neurorobotics 06 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 7\nZhang et al. 10.3389/fnbot.2023.1127338\nwhere dk is the vector dimension, Avgpooling is the mean pooling\nlayer, Xi is the raw EMG window, and sotfmax is a normalized\nexponential function.\n3.1.2. Linear projection\nTo construct long-term and short-term branches for multiscale\nfeature extraction, this module slices Pi into long-term and\nshort-term segments, respectively, and performs linear projection\ninto long-term tokens pL\ni and short-term tokens pS\ni . The ﬁnal\nconstruction forms the set of long-term branching tokens zL\n0 . As in\nEquation 3, the set of short-term branching tokens zS\n0 is described\nin Equation 4.\nZS\n0 =[pS\ncls;pS\n1ES;pS\n2ES;. . .;pNS\nS ES]+ES\npos (3)\nZL\n0 =[pL\ncls;pL\n1EL;pL\n2EL;. . .;pL\nNL EL]+EL\npos (4)\nwhere ES and EL are linear projection matrices, ps\n1ps\n2....ps\nNS are\nshort-term tokens whose sizes are set to H ×SShort , pL\n1 pL\n2 ...pL\nNL are\nlong-term tokens whose sizes are set to H ×SLong , PS\ncls and PL\ncls\nare the classiﬁcation tokens of the short-term branch and long-\nterm branch, and ES\npos and EL\npos are the position embeddings of the\nshort-term branch and long-term branch, respectively.\n3.1.3. Multihead reattention (MHRA)\nThe long-/short-term subencoder mainly consists of multihead\nreattention (MHRA) and a multilayer perceptron (MLP). MHRA\nis the contribution of this module. MHRA collects complementary\ninformation about the interactions between multiple attentions by\nadding a transformation matrix θ ∈Rhead×head. MHRA enables\nindividual heads to observe the characteristics of the signal from\ndiﬀerent angles, eﬀectively solving the attentional collapse problem\n(Zhou et al., 2021), where head is the number of MHRA output\nheads.\nThis module extracts the long- and short-term features from\nthe set of long-term branch tokenszL\n0 and short-term branch tokens\nzS\n0 by MHRA and the MLP , respectively. The speciﬁc steps of the\nmodule are as follows.\nFirst, we compute the interpatch attention information by\ntransforming each patch in the output zS\n0 or zL\n0 of the linear\nprojection module into QKV , which is fed into the respective\nbranch’s encoder.\nRe−Attention(Q, K, V) =Norm(θT(Soft max(QKT /\n√\ndk)))V\n(5)\nwhere Norm is the layer norm normalization function, Q, K, and V\nare the query, key and value for the short-term branch, respectively,\nand dk is the vector dimension.\nNext, the reattention information from the MHRA module is\ninput to the MLP module, and the MHRA and MLP modules are\nconnected by means of residuals.\nFinally, the short-term sequence characteristic of the short-\nterm branch output is ZS\nM, as in Equation 6, and similarly, the long-\nterm sequence characteristic ZL\nN can be obtained with Equation 7.\nZS\nM=[zS\ncls;zS\n1;. . .;zS\nNS ] (6)\nZL\nN =[zL\ncls;zL\n1 ;. . .;zL\nNL ] (7)\nwhere zS\ncls and zL\ncls are the classiﬁcation tokens on the short- and\nlong-term features, respectively, z S\n1. . .zS\nNS are the patch tokens of\nthe short-term features; NS is the number of short-term patch\ntokens; zL\n1 . . .zL\nNL are the patch tokens of the long-term features; and\nNL is the number of long-term patch tokens.\nWe stack the long- and short-term sub-encoders, M and\nN, respectively, to construct the deep network and extract deep\nfeatures.\n3.2. Feature cross-attention (FCA)\nIn the ﬁelds of image classiﬁcation and object detection, a\nlarge number of researchers have proposed improved ideas for\nfeature fusion methods (Yu et al., 2020; Zheng et al., 2020), such\nas feature pyramid networks (FPNs) (Lin et al., 2017), ResNet\n(He et al., 2016) and adaptive spatial feature fusion (ASFF)\n(Liu et al., 2019). The above research proved that setting up\nan appropriate feature fusion strategy is beneﬁcial for improving\naccuracy. However, the current fusion methods are designed\nbased on the feature maps extracted by convolutional neural\nnetworks and are not applicable to the vector features extracted\nby the transformer model. Therefore, we propose the feature\ncross-attention module (FCA) to cross-learn the classiﬁcation\ntoken and patch tokens of two branches, which achieves the\neﬃcient fusion of long- and short-term features with less\ncomputational eﬀort.\nTaking the short-term branch as an example, the feature cross-\nattention module is speciﬁed in Figure 4.\nFirst, the short-term feature classiﬁcation token (CLS token)\nand the long-term feature patch tokens are aligned and stitched\ntogether as in Equations 8, 9:\nzS′\ncls =fS(zS\ncls) (8)\nzS′\n=Concat(zS′\ncls, zL\n1 , . . . ,zL\nNL ) (9)\nwhere f S( ·) is the feature alignment function and Concat is the\nsplicing operation.\nSecond, the FCA input z S′\ncls is cross-learned with z S′\nas in\nEquations 10–13.\nFCA(zS′\ncls, zS′\n) =soft max\n(\nqkT\n√\nd\n)\nv (10)\nwhere q and k are the query and key of the short-term features,\nd is the long-term patch token dimension, and Softmax is the\nnormalized exponential function.\nFinally, the feature cross-attention is extended to multiple\nheads, which is denoted as multihead feature cross-attention\n(MFCA); the multihead features are aligned backward, and their\noutput dimensions are kept consistent with the short-term feature\nclassiﬁcation token to obtain the short-term feature classiﬁcation\ntoken ZS′′\ncls after cross-learning, as in Equation 11.\nzS′′\ncls =gS(zS′\ncls +FCA(zS′\ncls, zS′\n)) (11)\nwhere g S( ·) is the reverse alignment function and z S′\nclsis the\nclassiﬁcation token before reverse alignment.\nFrontiers in Neurorobotics 07 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 8\nZhang et al. 10.3389/fnbot.2023.1127338\nFIGURE 4\nFeature cross-attention module.\nAt this point, the short-term feature after cross-learning is ZS′\nM\n,\nas in Equation 12, and similarly, the long-term feature after cross-\nlearning is ZL′\nN , as in Equation 13.\nZS′\nM=[zS′′\ncls;zS\n1;. . .;zS\nNS ] (12)\nZL′\nN =[zL′′\ncls;zL\n1 ;. . .;zL\nNL ] (13)\nSince the short-term feature classiﬁcation token learns the\nabstract information of the branches, interacting with the patch\ntokens at the other branches helps to include information at a\ndiﬀerent scale. The fused long-term and short-term features ZS′\nM\nand ZL′\nN are output to the gesture classiﬁcation model.\n3.3. Gesture classiﬁcation module\nThe short-term feature z S′′\ncls and long-term feature z L′′\ncls\nclassiﬁcation tokens are obtained, and the sum of the gesture scores\nof each branch is output to obtain the gesture category.\ngestures =LL(LayerNorm(zS′′\ncls)) +LL(LayerNorm(zL′′\ncls)) (14)\n4. Experiments and results\nOur experiments employed a deep learning framework on a\ncomputer platform for model training and testing. The computer\nhardware conﬁguration used was an Intel Core i7-8700K CPU\nprocessor (32 GB RAM) and a GeForce GTX 3090 GPU (24\nGB RAM). The operating system was Ubuntu 18.04.4LTS, and\nnetwork models were constructed, trained, and validated using the\nPython 3.6.5 programming language under the PyTorch 1.8.0 deep\nlearning framework. The cross-entropy loss was used to measure\nclassiﬁcation performance.\n4.1. LST-EMG-Net model training\nparameter setting\nWe evaluated diﬀerent variants of the LST-EMG-Net\narchitecture. For all model variants, we used the Adam optimizer\nto set the parameters to 0.9, 0.999, and the learning rate was\ncorrected using StepLR, with the step size set to 3 and gamma set\nto 0.5. We set the initial learning rate to 6e-4 with a batch size of\n512. The short-term patch length and long-term patch length were\ndynamically set according to the sEMG window length used for\nthe dataset. For the short-term branch, the short-term subencoder\ndepth was set to 1 (i.e., M =1), and the number of short-term\nsubencoder heads was set to eight. For the long-term branch,\nthe long-term subencoder depth was set to 2 (i.e., N =2), and\nthe number of long-term subencoder heads was set to eight. The\nfeature cross-attention depth =4 (i.e., L =4), and the number of\nfeature cross-attention heads =8. All models were trained under\nthese parameters until convergence.\n4.2. Ablation experiments\nIn this article, we evaluate the LST-EMG-Net model on three\ndatasets and describe the ablation experiments of the LST-EMG-\nNet model. This ablation experiment used the ViT model extended\nto dual streams as the baseline and added the gesture recognition\neﬀects of FCA, ECA, MHRA, and SA. The baseline model\nframework was used to remove the yellow module in Figure 3. We\nrecorded the average accuracy of all subjects on each dataset to form\nTable 1.\nLST-EMG-Net shows the best model results obtained by\nsimultaneously adding FCA, ECA, MHRA, and SA. Model 1\nimproves the average recognition accuracy by 2.78% compared\nto the baseline. This demonstrates that dual-stream information\nfusion helps improve accuracy. Model 2 improved the average\nrecognition accuracy by 2.73% on the three datasets compared\nto Model 1, with a 4.29% improvement on the CapgMyo DB-c\nhigh-density dataset. Because of the high number of channels of\nthe sEMG acquisition device in this dataset, 128 channels, and\nthe rich muscle activity information between channels, the ECA\nFrontiers in Neurorobotics 08 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 9\nZhang et al. 10.3389/fnbot.2023.1127338\nTABLE 1 Hand gesture recognition accuracy achieved on each dataset in the ablation experiments.\nModel name FCA ECA MHRA SA DB2 exercise B DB5 exercise C CapgMyo DB-c\nBaseline 76.25% 77.17% 91.60%\nModel 1 √ 78.13% (+1.88%) 83.00% (+5.83%) 92.24% (+0.64%)\nModel 2 √ √ 79.51% (+1.38%) 85.53% (+2.53%) 96.53% (+4.29%)\nModel 3 √ √ √ 80.62% (+1.11%) 86.96% (+1.43%) 98.95% (+2.32%)\nModel 4 √ √ √ √ 81.47% (+0.85%) 88.24% (+1.28%) 98.80% (−0.15%)\nThe bold value means the best recognition accuracy.\neﬀect is evident in this dataset. Model 3 improved the average\nrecognition accuracy by 1.62% compared with model 2. Because\nthe amount of sEMG data in this experiment was smaller and the\nnumber of required network layers was relatively shallow, with\nthe addition of MHRA, the network may perform better on more\nextensive sEMG data. Model 4 achieves an average recognition\naccuracy improvement of 0.66% compared to model 3. However,\non the CapgMyo DB-c high-density dataset, the original signal size\nreached saturation due to the high sampling rate and the number of\nchannels in this dataset. Therefore, compared to model 3, the model\naccuracy stabilized.\n4.3. Comparison experiment\nWe compare the proposed LST-EMG-Net with the existing\nMSCNN of the multistream CNN (Wei et al., 2019), the\nbidirectional temporal convolutional network (BiTCN) (Chen\net al., 2020), and TEMG based on the vision transformer (ViT)\n(Siddhad et al., 2022) on the above three EMG datasets.\n(1) LST-EMG-Net’s accuracy and inference time: The\nperformance is shown in Table 2.\nFrom Table 2, we can see that our method reaches the optimum\nresults on the three datasets of DB2 Exercise B, DB5 Exercise C,\nand CapgMyo DB-C, and the accuracy is improved by 2.70, 4.49,\nTABLE 2 Accuracies and inference times of LST-EMG-Net and the\ncomparison algorithms.\nDataset Model name Accuracy Inference\ntime\nDB2 exercise B MSCNN 71.89% 5.60 ms\nBiTCN 65.79% 5.75 ms\nTEMG 78.77% 1.09 ms\nLSTEMGNet [ours] 81.47% 6.47 ms\nDB5 exercise C MSCNN 79.14% 7.27 ms\nBiTCN 83.75% 7.29 ms\nTEMG 68.18% 1.18 ms\nLSTEMGNet [ours] 88.24% 6.36 ms\nCapgMyo DB-C MSCNN 86.67% 7.78 ms\nBiTCN 98.38% 7.30 ms\nTEMG 92.90% 1.12 ms\nLSTEMGNet [ours] 98.80% 6.32 ms\nThe bold value means the best recognition accuracy.\nand 0.42%, respectively, compared to the optimum comparison\nmethods.\nRegarding the recognition time aspect, we can also see from\nTable 2 that LST-EMG-Net not only has higher recognition\naccuracy but also outperforms the CNN-based MSCNN model\nand the RNN-based BiTCN model in terms of inference time.\nBoth LST-EMG-Net and TEMG are designed based on the\ntransformer model, but the diﬀerence is that LST-EMG-Net\nextends the transformer to a dual-ﬂow structure. Compared with\nthe single-stream structure of TEMG, the average recognition\naccuracy of LST-EMG-Net is 9.5% higher on the three datasets,\nwhich demonstrates improved recognition accuracy and stability.\nFurthermore, the dual-stream structure of LST-EMG-Net increases\nthe computational and parametric quantities of the model to a\ncertain extent. On average, it is 5.25 ms slower than TEMG, but\nboth can meet the requirements of real-time recognition.\n(2) LST-EMG-Net’s stability: To verify the stability of LST-EMG-\nNet in recognizing various types of gestures, we compare the\nﬂuctuation of the recognition accuracy of the method in this\narticle with that of MSCNN, BiTCN, and TEMG. We choose\nthe standard deviation (STD) as an indicator to measure the\nﬂuctuation of each gesture between subjects. Taking gesture\none as an example, the ﬂuctuation value is calculated as\nfollows in Equation 15.\nG1 =\n√∑n\ni=1 acci −acc\nn (15)\nwhere i is the subject number, acc i denotes the i-th subject\ngesture one accuracy, acc is the average gesture pne accuracy,\nand n is the number of subjects. A smaller ﬂuctuation value\nmeans that the gesture recognition is more stable, and we\ncalculate the average ﬂuctuation value of each gesture in the\nthree datasets, as shown in Table 3.\nThe experimental results in Table 3 show that the average\nﬂuctuation value of the proposed LST-EMG-Net is low for all kinds\nTABLE 3 Average ﬂuctuation values of LST-EMG-Net and the\ncomparison algorithms.\nModel name DB2\nexercise B\nDB5\nexercise C\nCapgMyo\nDB-C\nMSCNN 0.1795 0.1835 0.1252\nBiTCN 0.2232 0.1324 0.0396\nTEMG 0.1197 0.1392 0.1045\nLST-EMG-Net [ours] 0.1181 0.1098 0.0179\nThe bold value means the highest recognition stability.\nFrontiers in Neurorobotics 09 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 10\nZhang et al. 10.3389/fnbot.2023.1127338\nof gestures. It is suitable for recognition tasks because it learns the\ninformation of EMG sampling points with diﬀerent timing lengths,\nthus maintaining a relatively stable and high recognition rate for\ngestures of diﬀerent complexity.\n5. Conclusion\nCurrent research gives little attention to the problem of\nmatching the amount of information in features with the amount\nof information needed to recognize gestures. Here, we propose\nthe LST-EMG-Net-based sEMG gesture recognition method to\naddress the above problems; it is mainly composed of a long\nshort-term encoder and a feature cross-attention module. Our\nmethod maintains a high level of accuracy for all types of gesture\nrecognition in both sparse EMG datasets and high-density sEMG\ndatasets. It improves the stability of gesture recognition compared\nto other network structures.\nOur LST-EMG-Net framework can be applied well to recognize\nvarious types of gestures by subjects. Nevertheless, due to the\nindividual variability among subjects, LST-EMG-Net is diﬃcult\nto apply to the intersubject recognition of gestures and has a\nhigh burden of use for new subjects, which needs further study\nin clinical applications. In the future, we will improve the LST-\nEMG-Net framework to achieve intersubject gesture recognition\nfor controlling exoskeletons or other rehabilitation devices for\npost-surgical rehabilitation of stroke patients.\nData availability statement\nPublicly available datasets were analyzed in this study. This data\ncan be found here: http://ninapro.hevs.ch/node/7.\nEthics statement\nWritten informed consent was obtained from the individual(s)\nfor the publication of any potentially identiﬁable images or data\nincluded in this article.\nAuthor contributions\nWZ and TZ conceived the ideas and designed the methodology,\nimplemented the technical pipeline, conducted the experiments,\nand analyzed the results. JZ and YW provided the dataset for\nthe experiments. All authors discussed the manuscript, wrote the\nmanuscript, and gave ﬁnal approval for its publication.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAl-Saegh, A., Dawwd, S. A., and Abdul-Jabbar, J. M. (2021). Deep learning for motor\nimagery EEG-based classiﬁcation: A review.Biomed. Signal Process. Control63:102172.\ndoi: 10.1016/j.bspc.2020.102172\nAlseed, M. M., and Tasoglu, S. (2022). “Machine learning-enabled classiﬁcation of\nforearm sEMG signals to control robotic hands prostheses, ” inProceedings of the 2022\ninnovations in intelligent systems and applications conference (ASYU) (Antalya: IEEE).\ndoi: 10.1109/ASYU56188.2022.9925273\nAtzori, M., Cognolato, M., and Müller, H. (2016). Deep learning with convolutional\nneural networks applied to electromyography data: A resource for the classiﬁcation\nof movements for prosthetic hands. Front. Neurorobot. 10:9. doi: 10.3389/fnbot.2016.\n00009\nAtzori, M., Gijsberts, A., Castellini, C., Caputo, B., Hager, A. G. M., Elsig, S., et al.\n(2014a). Electromyography data for non-invasive naturally-controlled robotic hand\nprostheses. Sci. Data 1, 1–13.\nAtzori, M., Gijsberts, A., Kuzborskij, I., Elsig, S., Hager, A. G. M., Deriaz, O.,\net al. (2014b). Characterization of a benchmark database for myoelectric movement\nclassiﬁcation. IEEE Trans. Neural Syst. Rehabil. Eng. 23, 73–83. doi: 10.1109/TNSRE.\n2014.2328495\nAtzori, M., Gijsberts, A., Heynen, S., Hager, A. G. M., Deriaz, O., Van Der Smagt,\nP., et al. (2012). “Building the Ninapro database: A resource for the biorobotics\ncommunity, ” inProceedings of the 2012 4th IEEE RAS & EMBS international conference\non biomedical robotics and biomechatronics (BioRob) (Rome: IEEE), 1258–1265. doi:\n10.1109/BioRob.2012.6290287\nBaygin, M., Barua, P. D., Dogan, S., Tuncer, T., Key, S., Acharya, U. R., et al. (2022). A\nhand-modeled feature extraction-based learning network to detect grasps using sEMG\nsignal. Sensors 22:2007. doi: 10.3390/s22052007\nBriouza, S., Gritli, H., Khraief, N., Belghith, S., and Singh, D. (2022). “EMG signal\nclassiﬁcation for human hand rehabilitation via two machine learning techniques:\nKNN and SVM, ” inProceedings of the 2022 5th international conference on advanced\nsystems and emergent technologies (IC_ASET) (Hammamet: IEEE), 412–417. doi: 10.\n1109/IC_ASET53395.2022.9765856\nChen, H., Zhang, Y., Zhou, D., and Liu, H. (2020). “Improving gesture recognition\nby bidirectional temporal convolutional netwoks, ” in Proceedings of the international\nconference on robotics and rehabilitation intelligence (Singapore: Springer), 413–424.\ndoi: 10.1007/978-981-33-4932-2_30\nCipriani, C., Zaccone, F., Micera, S., and Carrozza, M. C. (2008). On the shared\ncontrol of an EMG-controlled prosthetic hand: Analysis of user–prosthesis interaction.\nIEEE Trans. Robot. 24, 170–184. doi: 10.1109/TRO.2007.910708\nComon, P. (1992). Independent component analysis . J-L.Lacoume. Higher-order\nstatistics. Amsterdam: Elsevier, 29–38. doi: 10.1109/TNN.2004.826380\nDu, Y., Wenguang, J., Wentao, W., and Geng, W. (2017). CapgMyo: A high\ndensity surface electromyography database for gesture recognition. Hangzhou: Zhejiang\nUniversity, 2017.\nFarago, E., Macisaac, D., Suk, M., and Chan, A. D. C. (2022). A review of techniques\nfor surface electromyography signal quality analysis. IEEE Rev. Biomed. Eng. 16,\n472–486. doi: 10.1109/RBME.2022.3164797\nFrontiers in Neurorobotics 10 frontiersin.org\nfnbot-17-1127338 February 24, 2023 Time: 8:45 # 11\nZhang et al. 10.3389/fnbot.2023.1127338\nGijsberts, A., Atzori, M., Castellini, C., Müller, H., and Caputo, B. (2014). Measuring\nmovement classiﬁcation performance with the movement error rate. IEEE Trans.\nNeural Syst. Rehabil. Eng.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” inProceedings of the IEEE conference on computer vision and pattern\nrecognition, Las Vegas, NV , 770–778. doi: 10.1109/CVPR.2016.90\nHu, Y., and Wang, Q. (2020). “A comprehensive evaluation of hidden Markov\nmodel for hand movement recognition with surface electromyography, ” inProceedings\nof the 2020 2nd international conference on robotics, intelligent control and artiﬁcial\nintelligence, New York, NY , 85–91. doi: 10.1145/3438872.3439060\nHu, Y., Wong, Y., Wei, W., Du, Y., Kankanhalli, M., and Geng, W. (2018). A novel\nattention-based hybrid CNN-RNN architecture for sEMG-based gesture recognition.\nPLoS One 13:e0206049. doi: 10.1371/journal.pone.0206049\nHuang, Y. Y., Low, K. H., and Lim, H. B. (2009). “Objective and quantitative\nassessment methodology of hand functions for rehabilitation, ” in Proceedings of the\n2008 IEEE international conference on robotics and biomimetics (Bangkok: IEEE),\n846–851. doi: 10.1109/ROBIO.2009.4913110\nJia, R., Y ang, L., Li, Y., and Xin, Z. (2021). “Gestures recognition of sEMG signal\nbased on Random Forest, ” in Proceedings of the 2021 IEEE 16th conference on\nindustrial electronics and applications (ICIEA) (Chengdu: IEEE), 1673–1678. doi: 10.\n3389/fnhum.2022.911204\nKarim, S., Qadir, A., Farooq, U., Shakir, M., and Laghari, A. (2022). Hyperspectral\nimaging: A review and trends towards medical imaging. Curr. Med. Imaging . doi:\n10.2174/1573405618666220519144358\nLaghari, A., Estrela, V., and Yin, S. (2022). How to collect and interpret\nmedical pictures captured in highly challenging environments that range from\nnanoscale to hyperspectral imaging. Curr. Med. Imaging . doi: 10.2174/15734056\n19666221228094228\nLeonardis, D., Barsotti, M., Loconsole, C., Solazzi, M., Troncossi, M., Mazzotti, C.,\net al. (2015). An EMG-controlled robotic hand exoskeleton for bilateral rehabilitation.\nIEEE Trans. Haptics 8, 140–151. doi: 10.1109/TOH.2015.2417570\nLi, X., Liu, J., Huang, Y., Wang, D., and Miao, Y. (2022). Human motion pattern\nrecognition and feature extraction: An approach using multi-information fusion.\nMicromachines 13:1205. doi: 10.3390/mi13081205\nLin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., and Belongie, S. (2017).\n“Feature pyramid networks for object detection, ” inProceedings of the IEEE conference\non computer vision and pattern recognition, 2117–2125. doi: 10.1109/CVPR.2017.106\nLiu, L., Chen, X., Lu, Z., Cao, S., Wu, D., and Zhang, X. (2016). Development of an\nEMG-ACC-based upper limb rehabilitation training system. IEEE Trans. Neural Syst.\nRehabil. Eng. 25, 244–253. doi: 10.1109/TNSRE.2016.2560906\nLiu, S., Huang, D., and Wang, Y. (2019). Learning spatial fusion for single-shot\nobject detection. arXiv [Preprint]. doi: 10.48550/arXiv.1911.09516\nMenon, R., Di Caterina, G., Lakany, H., Petropoulakis, L., Conway, B. A., and\nSoraghan, J. J. (2017). Study on interaction between temporal and spatial information\nin classiﬁcation of EMG signals for myoelectric prostheses. IEEE Trans. Neural Syst.\nRehabil. Eng. 25, 1832–1842. doi: 10.1109/TNSRE.2017.2687761\nMontazerin, M., Zabihi, S., Rahimian, E., Mohammadi, A., and Naderkhani, F.\n(2022). ViT-HGR: Vision transformer-based hand gesture recognition from high\ndensity surface EMG signals. arXiv [Preprint]. doi: 10.48550/arXiv.2201.10060\nMujahid, A., Awan, M. J., Y asin, A., Mohammed, M. A., Damaševi ˇcius, R.,\nMaskeli¯unas, R., et al. (2021). Real-time hand gesture recognition based on deep\nlearning YOLOv3 model. Appl. Sci. 11:4164. doi: 10.3390/app11094164\nMuri, F., Carbajal, C., Echenique, A. M., Fernández, H., and López, N. M. (2013).\nVirtual reality upper limb model controlled by EMG signals. J. Phys. Conf. Ser.\n477:012041. doi: 10.1088/1742-6596/477/1/012041\nNaik, G. R., and Nguyen, H. T. (2014). Nonnegative matrix factorization for the\nidentiﬁcation of EMG ﬁnger movements: Evaluation using matrix analysis. IEEE J.\nBiomed. Health Inform. 19, 478–485. doi: 10.1109/JBHI.2014.2326660\nNaik, G. R., Kumar, D. K., Weghorn, H., and Palaniswami, M. (2007). “Subtle hand\ngesture identiﬁcation for HCI using temporal decorrelation source separation BSS of\nsurface EMG, ” inProceedings of the 9th biennial conference of the Australian pattern\nrecognition society on digital image computing techniques and applications (DICTA\n2007) (Glenelg, SA: IEEE), 30–37. doi: 10.1109/DICTA.2007.4426772\nNamazi, H. (2019a). Decoding of hand gestures by fractal analysis of\nelectromyography (EMG) signal. Fractals 27:1950022. doi: 10.1142/S0218348X195\n00221\nNamazi, H. (2019b). Fractal-based classiﬁcation of electromyography (EMG) signal\nbetween ﬁngers and hand’s basic movements, functional movements, and force\npatterns. Fractals 27:1950050. doi: 10.1142/S0218348X19500506\nNarayan, Y. (2021). Hb vsEMG signal classiﬁcation with time domain and frequency\ndomain features using LDA and ANN classiﬁer. Mater. Today Proc. 37, 3226–3230.\nNazmi, N., Abdul Rahman, M. A., Y amamoto, S. I., Ahmad, S. A., Malarvili, M. B.,\nMazlan, S. A., et al. (2017). Assessment on stationarity of EMG signals with diﬀerent\nwindows size during isotonic contractions. Appl. Sci. 7:1050. doi: 10.3390/app7101050\nOudah, M., Al-Naji, A., and Chahl, J. (2020). Hand gesture recognition based\non computer vision: A review of techniques. J. Imaging 6:73. doi: 10.3390/\njimaging6080073\nParajuli, N., Sreenivasan, N., Bifulco, P., Cesarelli, M., Savino, S., Niola, V., et al.\n(2019). Real-time EMG based pattern recognition control for hand prostheses: A\nreview on existing methods, challenges and future implementation. Sensors 19:4596.\ndoi: 10.3390/s19204596\nPizzolato, S., Tagliapietra, L., Cognolato, M., Reggiani, M., Müller, H., and Atzori, M.\n(2017). Comparison of six electromyography acquisition setups on hand movement\nclassiﬁcation tasks. PLoS One 12:e0186132. doi: 10.1371/journal.pone.0186132\nRahimian, E., Zabihi, S., Asif, A., Farina, D., Atashzar, S. F., and Mohammadi, A.\n(2021). TEMGNet: Deep transformer-based decoding of Upperlimb sEMG for hand\ngestures recognition. arXiv [Preprint]. doi: 10.48550/arXiv.2109.12379\nRim, B., Sung, N. J., Min, S., and Hong, M. (2020). Deep learning in physiological\nsignal data: A survey. Sensors 20:969. doi: 10.3390/s20040969\nScheme, E., and Englehart, K. (2011). Electromyogram pattern recognition for\ncontrol of powered upper-limb prostheses: State of the art and challenges for clinical\nuse. J. Rehabil. Res. Dev. 48, 643–659. doi: 10.1682/JRRD.2010.09.0177\nSiddhad, G., Gupta, A., Dogra, D. P., and Roy, P. P. (2022). Eﬃcacy of transformer\nnetworks for classiﬁcation of raw EEG data. arXiv [Preprint]. doi: 10.48550/arXiv.\n2202.05170\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\net al. (2017). Attention is all you need. Adv. Neural Inform. Process. Syst. 30,\n5998–6008.\nWang, B., Y ang, C., and Xie, Q. (2012). “Human-machine interfaces based on EMG\nand Kinect applied to teleoperation of a mobile humanoid robot, ” inProceedings of the\n10th world congress on intelligent control and automation (Beijing: IEEE), 3903–3908.\ndoi: 10.1109/WCICA.2012.6359124\nWei, W., Wong, Y., Du, Y., Hu, Y., Kankanhalli, M., and Geng, W. (2019). A\nmulti-stream convolutional neural network for sEMG-based gesture recognition in\nmuscle-computer interface. Pattern Recogn. Lett. 119, 131–138. doi: 10.1016/j.patrec.\n2017.12.005\nXia, P., Hu, J., and Peng, Y. (2018). EMG-based estimation of limb movement\nusing deep learning with recurrent convolutional neural networks. Artif. Organs 42,\nE67–E77. doi: 10.1111/aor.13004\nXue, Y., Ji, X., Zhou, D., Li, J., and Ju, Z. (2019). SEMG-based human in-hand\nmotion recognition using nonlinear time series analysis and random forest. IEEE\nAccess 7, 176448–176457. doi: 10.1109/ACCESS.2019.2957668\nYu, J., Li, H., Yin, S. L., and Karim, S. (2020). Dynamic gesture recognition based\non deep learning in human-to-computer interfaces. J. Appl. Sci. Eng. 23, 31–38. doi:\n10.6180/jase.202003_23(1).0004\nZheng, D., Li, H., and Yin, S. (2020). Action recognition based on the modiﬁed\ntwostream CNN. Int. J. Math. Sci. Comp. 6, 15–23. doi: 10.5815/ijmsc.2020.06.03\nZhou, D., Kang, B., Jin, X., Y ang, L., Lian, X., Jiang, Z., et al. (2021). Deepvit: Towards\ndeeper vision transformer. arXiv [Preprint]. doi: 10.48550/arXiv.2103.11886\nFrontiers in Neurorobotics 11 frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7872188091278076
    },
    {
      "name": "Gesture",
      "score": 0.7338519096374512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6337773203849792
    },
    {
      "name": "Speech recognition",
      "score": 0.5636650323867798
    },
    {
      "name": "Gesture recognition",
      "score": 0.5584233999252319
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5479273796081543
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5065222978591919
    },
    {
      "name": "SIGNAL (programming language)",
      "score": 0.4936259984970093
    },
    {
      "name": "Deep learning",
      "score": 0.4811626374721527
    },
    {
      "name": "Electromyography",
      "score": 0.4811362326145172
    },
    {
      "name": "Encoder",
      "score": 0.4213739335536957
    },
    {
      "name": "Computer vision",
      "score": 0.3456661105155945
    },
    {
      "name": "Neuroscience",
      "score": 0.06682366132736206
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 19
}