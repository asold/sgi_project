{
    "title": "Evaluation of the Performance of Generative AI Large Language Models ChatGPT, Google Bard, and Microsoft Bing Chat in Supporting Evidence-Based Dentistry: Comparative Mixed Methods Study",
    "url": "https://openalex.org/W4388823522",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4361640142",
            "name": "Kostis Giannakopoulos",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A2438096541",
            "name": "Argyro Kavadella",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A5092793030",
            "name": "Anas Aaqel Salim",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A4318149572",
            "name": "Vassilis Stamatopoulos",
            "affiliations": [
                "Athena Research and Innovation Center In Information Communication & Knowledge Technologies"
            ]
        },
        {
            "id": "https://openalex.org/A2501876073",
            "name": "Eleftherios G. Kaklamanos",
            "affiliations": [
                "European University Cyprus",
                "Aristotle University of Thessaloniki",
                "Mohammed Bin Rashid University of Medicine and Health Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A4361640142",
            "name": "Kostis Giannakopoulos",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A2438096541",
            "name": "Argyro Kavadella",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A5092793030",
            "name": "Anas Aaqel Salim",
            "affiliations": [
                "European University Cyprus"
            ]
        },
        {
            "id": "https://openalex.org/A4318149572",
            "name": "Vassilis Stamatopoulos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2501876073",
            "name": "Eleftherios G. Kaklamanos",
            "affiliations": [
                "Mohammed Bin Rashid University of Medicine and Health Sciences",
                "European University Cyprus",
                "Aristotle University of Thessaloniki"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3215605706",
        "https://openalex.org/W2965207724",
        "https://openalex.org/W3111642447",
        "https://openalex.org/W3096543177",
        "https://openalex.org/W4283791207",
        "https://openalex.org/W2033786574",
        "https://openalex.org/W3206094211",
        "https://openalex.org/W4324387439",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W4362471626",
        "https://openalex.org/W4380995257",
        "https://openalex.org/W4367556998",
        "https://openalex.org/W3028484854",
        "https://openalex.org/W4319332372",
        "https://openalex.org/W4319341091",
        "https://openalex.org/W4319301633",
        "https://openalex.org/W3164718925",
        "https://openalex.org/W4362601804",
        "https://openalex.org/W2135400982",
        "https://openalex.org/W4296178189",
        "https://openalex.org/W4229442974",
        "https://openalex.org/W4366989878",
        "https://openalex.org/W4384824783",
        "https://openalex.org/W4377090863",
        "https://openalex.org/W4367051110",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4313447794",
        "https://openalex.org/W2084950908",
        "https://openalex.org/W4365451987",
        "https://openalex.org/W4387373660",
        "https://openalex.org/W4384944962",
        "https://openalex.org/W4378389568",
        "https://openalex.org/W4387307799",
        "https://openalex.org/W4377010360",
        "https://openalex.org/W3081359095",
        "https://openalex.org/W4385346108",
        "https://openalex.org/W4324140459",
        "https://openalex.org/W4378389266",
        "https://openalex.org/W4381480701",
        "https://openalex.org/W4387356888"
    ],
    "abstract": "Background The increasing application of generative artificial intelligence large language models (LLMs) in various fields, including dentistry, raises questions about their accuracy. Objective This study aims to comparatively evaluate the answers provided by 4 LLMs, namely Bard (Google LLC), ChatGPT-3.5 and ChatGPT-4 (OpenAI), and Bing Chat (Microsoft Corp), to clinically relevant questions from the field of dentistry. Methods The LLMs were queried with 20 open-type, clinical dentistry–related questions from different disciplines, developed by the respective faculty of the School of Dentistry, European University Cyprus. The LLMs’ answers were graded 0 (minimum) to 10 (maximum) points against strong, traditionally collected scientific evidence, such as guidelines and consensus statements, using a rubric, as if they were examination questions posed to students, by 2 experienced faculty members. The scores were statistically compared to identify the best-performing model using the Friedman and Wilcoxon tests. Moreover, the evaluators were asked to provide a qualitative evaluation of the comprehensiveness, scientific accuracy, clarity, and relevance of the LLMs’ answers. Results Overall, no statistically significant difference was detected between the scores given by the 2 evaluators; therefore, an average score was computed for every LLM. Although ChatGPT-4 statistically outperformed ChatGPT-3.5 (P=.008), Bing Chat (P=.049), and Bard (P=.045), all models occasionally exhibited inaccuracies, generality, outdated content, and a lack of source references. The evaluators noted instances where the LLMs delivered irrelevant information, vague answers, or information that was not fully accurate. Conclusions This study demonstrates that although LLMs hold promising potential as an aid in the implementation of evidence-based dentistry, their current limitations can lead to potentially harmful health care decisions if not used judiciously. Therefore, these tools should not replace the dentist’s critical thinking and in-depth understanding of the subject matter. Further research, clinical validation, and model improvements are necessary for these tools to be fully integrated into dental practice. Dental practitioners must be aware of the limitations of LLMs, as their imprudent use could potentially impact patient care. Regulatory measures should be established to oversee the use of these evolving technologies.",
    "full_text": "Original Paper\nEvaluation of the Performance of Generative AI Large Language\nModels ChatGPT, Google Bard, and Microsoft Bing Chat in\nSupporting Evidence-Based Dentistry: Comparative Mixed\nMethods Study\nKostis Giannakopoulos1, DDS, PhD; Argyro Kavadella1, BDentSc, MSc, PhD; Anas Aaqel Salim1, DDS, PhD; Vassilis\nStamatopoulos2, BSc, MSc; Eleftherios G Kaklamanos1,3,4, MSc, MA, DDS, PhD\n1School of Dentistry, European University Cyprus, Nicosia, Cyprus\n2Information Management Systems Institute, ATHENA Research and Innovation Center, Athens, Greece\n3School of Dentistry, Aristotle University of Thessaloniki, Thessaloniki, Greece\n4Mohammed Bin Rashid University of Medicine and Health Sciences, Dubai, United Arab Emirates\nCorresponding Author:\nKostis Giannakopoulos, DDS, PhD\nSchool of Dentistry\nEuropean University Cyprus\n6 Diogenis St\nEngomi\nNicosia, 2404\nCyprus\nPhone: 357 22559622\nFax: 357 22559622\nEmail: k.giannakopoulos@euc.ac.cy\nAbstract\nBackground: The increasing application of generative artificial intelligence large language models (LLMs) in various fields,\nincluding dentistry, raises questions about their accuracy.\nObjective: This study aims to comparatively evaluate the answers provided by 4 LLMs, namely Bard (Google LLC), ChatGPT-3.5\nand ChatGPT-4 (OpenAI), and Bing Chat (Microsoft Corp), to clinically relevant questions from the field of dentistry.\nMethods: The LLMs were queried with 20 open-type, clinical dentistry–related questions from different disciplines, developed\nby the respective faculty of the School of Dentistry, European University Cyprus. The LLMs’ answers were graded 0 (minimum)\nto 10 (maximum) points against strong, traditionally collected scientific evidence, such as guidelines and consensus statements,\nusing a rubric, as if they were examination questions posed to students, by 2 experienced faculty members. The scores were\nstatistically compared to identify the best-performing model using the Friedman and Wilcoxon tests. Moreover, the evaluators\nwere asked to provide a qualitative evaluation of the comprehensiveness, scientific accuracy, clarity, and relevance of the LLMs’\nanswers.\nResults: Overall, no statistically significant difference was detected between the scores given by the 2 evaluators; therefore, an\naverage score was computed for every LLM. Although ChatGPT-4 statistically outperformed ChatGPT-3.5 (P=.008), Bing Chat\n(P=.049), and Bard (P=.045), all models occasionally exhibited inaccuracies, generality, outdated content, and a lack of source\nreferences. The evaluators noted instances where the LLMs delivered irrelevant information, vague answers, or information that\nwas not fully accurate.\nConclusions: This study demonstrates that although LLMs hold promising potential as an aid in the implementation of\nevidence-based dentistry, their current limitations can lead to potentially harmful health care decisions if not used judiciously.\nTherefore, these tools should not replace the dentist’s critical thinking and in-depth understanding of the subject matter. Further\nresearch, clinical validation, and model improvements are necessary for these tools to be fully integrated into dental practice.\nDental practitioners must be aware of the limitations of LLMs, as their imprudent use could potentially impact patient care.\nRegulatory measures should be established to oversee the use of these evolving technologies.\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 1https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n(J Med Internet Res 2023;25:e51580) doi: 10.2196/51580\nKEYWORDS\nartificial intelligence; AI; large language models; generative pretrained transformers; evidence-based dentistry; ChatGPT; Google\nBard; Microsoft Bing; clinical practice; dental professional; dental practice; clinical decision-making; clinical practice guidelines\nIntroduction\nBackground\nArtificial intelligence (AI) dental applications and tools have\nexhibited exponential growth during the past few years, aiming\nto assist health care professionals in providing improved oral\nhealth care in a consistent manner. Currently, such tools can\nsupport image analysis; the interpretation of radiographs and\ndiagnoses made by neural networks; data synthesis; the\nprovision of information on materials and clinical techniques\nfor improved outcomes; patient record management; other\napplications in forensic dentistry, orthodontics, periodontology,\nand endodontics; caries diagnosis; treatment planning; and\npatient communication and interaction [1]. Using AI technology,\nclinical questions can be answered on a user’s mobile phone\nwithin seconds, and continuing educational updates can be\nconstant [1-7]. Through data synthesis, along with risk factors’\nand patterns’ identification, AI could potentially assist in the\nsystematic assessment of clinically relevant scientific evidence,\nwhich, when judiciously integrated with the dentist’s clinical\nexpertise in addition to the patient’s treatment needs and\npreferences, may support busy clinicians in overcoming the\nchallenges associated with the implementation of the\nevidence-based dentistry (EBD) approach to oral health care\n[8-10]. Thus, AI may be able to promote individualized\npatient-centered care and bolster a more efficient, reliable, and\nstandardized clinical practice [11].\nOn November 30, 2022, an exciting technological innovation\nin AI, Generative AI (GenAI), was introduced through the\nlaunch of ChatGPT (OpenAI Inc), a generative pretrained\ntransformer (GPT) that attracted 100 million users within the\nfirst 3 months of its launch, a historical number for an internet\napplication [12]. ChatGPT is a large language model (LLM)\nthat uses natural language processing, an area of AI that aims\nat enabling computers to understand natural language inputs\nusing a variety of techniques, such as machine learning [12,13].\nLLMs are neural networks trained on massive amounts of text\ndata from the internet (from Wikipedia, digitized books, articles,\nand webpages) with the aim of processing and generating\ncoherent, humanlike conversational responses based on the\ncontext of the input text (question or prompt) using\ndeep-learning algorithms and advanced modeling [13-15].\nModern LLMs use a neural architecture based on positional\nencoding and self-attention techniques to identify relationships\nwithin the input text and produce meaningful and relevant\nresponses [16]. They can answer follow-up questions, ask for\nclarifications, challenge incorrect statements, and reject\ninappropriate requests [15]. Furthermore, LLMs can be\nfine-tuned by human evaluators to improve their performance\non specific tasks or specialized applications, a process that\nincreases their usability, accuracy, and functionality [16,17].\nUnlike conventional search engines, the user does not have to\nbrowse, select, and click on a website to obtain an answer;\ninstead, the LLM’s output already collates all available and\nrelevant data from its database in a text response, making it a\nuser-friendly, time-efficient, and seemingly reliable tool. The\ncurrent free-access version of ChatGPT is based on the GPT-3.5\nlanguage model, and the newer version, GPT-4, is currently\navailable under the ChatGPT Plus paid subscription. Later, in\nFebruary 2023, Microsoft launched the Bing Chat AI chatbot,\nwhich uses the GPT-4 language model, whereas in March 2023,\nGoogle released the Bard chatbot, which was powered initially\nby Language Model for Dialogue Applications (LaMDA), its\nproprietary family of LLMs, and later by the Pathways Language\nModel (PaLM) 2 LLM.\nChatGPT-3.5 and the improved, subscription version\nChatGPT-4, compared with their competitors, are easy to use\nand available to everyone on OpenAI’s website. This widespread\naccessibility makes these bots a top choice for many users. By\ncontrast, although Bing Chat has its strengths, such as being\nsuitable for research, having live access to the internet, and\nhaving access to GPT-4, its limited accessibility is a drawback.\nBing Chat has a chat limit of 100 requests per day, which,\ncompared with ChatGPT’s 70 requests per hour, can be a\nbottleneck in a research study. This, in tandem with its\nsomewhat limited browser compatibility, makes it unsuitable\nfor everyday use. Google Bard also has live access to the internet\nbut is still in its early stages, both technologically and\ncommercially [18,19].\nChatGPT is the most studied LLM so far in education, research,\nand health care, with promising results and some valid concerns.\nBenefits in health care clinical practice could include cost\nsaving, documentation, personalized medicine, health literacy,\nand the streamlining of workflow, whereas in dentistry and oral\nhealth care, ChatGPT could be used as a supplementary tool\nfor better diagnosis and decision-making, data recording, image\nanalysis, disease prevention, and patient communication\n[14,17,20,21]. Rao et al [22] evaluated ChatGPT’s capacity for\nclinical decision support in radiology through the identification\nof appropriate imaging modalities for various clinical\npresentations of breast cancer screening and breast pain and\nconcluded that the integration of such AI tools into the clinical\nworkflow is feasible and efficient. The coupling of LLMs with\nEBD seems ideal, as dental professionals can have\nevidence-based, fact-driven, and patient-specific responses to\nclinical queries within seconds, an approach that could\npotentially enable the identification of treatment choices and\nthe decision-making process, lower the chances of mistakes,\nand enhance personalized dental care and practice efficiency.\nThe serious concerns raised about different aspects of GenAI\ntechnologies include the criteria and goals of the developers,\npersonal data protection and encryption vulnerability, and the\nvalidity of the information provided by these models [1,23].\nThe major question at present is which aspects of GenAI provide\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 2https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nreal benefits to society and which present potential problems\n[24,25]. In March 2023, Italy banned the use of ChatGPT owing\nto privacy concerns, as there was no secure way to protect\npersonal data and financial information could thus potentially\nbe stolen through this technology [26]. However, the ban was\nlifted after OpenAI met the demands of regulators regarding\nprivacy concerns [27]. ChatGPT is also banned in countries\nwith heavy internet control, such as North Korea, Iran, Russia,\nand China [26].\nFurthermore, there are several considerations regarding the use\nof GenAI in health care, such as the output’s accuracy; the\npossibility of unreliable responses, including the risk of\nhallucination, that is, the presentation of entirely wrong,\ninaccurate or even harmful responses and fabricated information\nas real; the risk of biased diagnoses; and ethical and legal issues.\nMajor drawbacks for health-related queries include the limited\nknowledge database (ChatGPT’s database at the time of the\nstudy did not extend beyond 2021), the inability to evaluate the\ncredibility of information retrieval sources, and the inability to\nintegrate external resources outside their databases (eg, scientific\njournals and textbooks) [1,12,14,20,28-30]. Considering the\nabovementioned limitations, it seems logical that despite the\ndata set and training provided to these models, they cannot\nreplace unique human intellectual abilities, and users must\nexercise caution and apply all means of evaluation, validation,\nand critical thinking to the information received.\nObjectives\nThis study aimed to compare the performance of currently\navailable GenAI LLMs in answering clinically relevant\nquestions from the field of dentistry by assessing their accuracy\nagainst traditional, evidence-based scientific dental resources.\nThe null hypothesis is that there is no difference in\ncomprehensiveness, scientific accuracy, clarity, and relevance\namong the 4 LLMs and between the 4 LLMs and the\nevidence-based scientific literature. By conducting this\ncomparative analysis, this study aimed to shed light on the\nadvantages and disadvantages of using LLMs in dental practice\nand initiate a debate about the role of AI technologies in EBD.\nThis study may be the first to evaluate the clinical use of\nChatGPT and similar chatbots as chairside dental assistants to\npromote EBD and clinical decision-making.\nMethods\nOverview\nA total of 20 questions relevant to clinical dentistry were asked\nto the 4 different LLMs. The questions were regarding common\nclinical issues related to different dental disciplines (Multimedia\nAppendix 1). The LLMs tested were (1) ChatGPT model\nGPT-3.5 (offered for free at the moment), (2) ChatGPT model\nGPT-4 (offered through ChatGPT Plus under subscription), (3)\nGoogle Bard, and (4) Microsoft Bing Chat. These LLMs appear\nto be the most popular and powerful chatbots in GenAI at the\nmoment.\nA pool of questions was developed by the faculty of the School\nof Dentistry, European University Cyprus, in the disciplines of\noral surgery and oral medicine and oral pathology,\nendodontology, operative dentistry, orthodontics,\nperiodontology, pediatric dentistry, oral and maxillofacial\nradiology, and prosthodontics. The specialists were asked to\nprovide questions that were clinically relevant and had answers\nthat were supported by strong evidence. The questions used\nwere agreed upon among the authors, through a consensus\nprocess, based on the following criteria: (1) they would be of\ninterest to the general dentist; therefore, questions on specific\nfields that can be answered solely by specialists were not\nconsidered; (2) they would cover a broad spectrum of dental\nprocedures performed in routine clinical practice, such as\noperative dentistry, radiology, prosthodontics, oral surgery, and\nperiodontology; and (3) they would have indisputable,\nunequivocal answers supported by scientific evidence. This\nevidence was provided by specialists. They were retrieved\nmainly from guidelines issued by scientific organizations and\nacademies; consensus statements; textbooks; professional and\neducational bodies, such as the Federation Dentaire\nInternationale (FDI) and the American Dental Association\n(ADA); medical libraries; and a PubMed database search for\nsystematic reviews in high-impact, peer-reviewed scientific\njournals. All pieces of evidence retrieved clearly addressed the\nquestions and were of the highest quality available [31]. They\nserved as the gold standard with which the LLMs’ responses\nwere compared.\nQuestions or prompts were written in scientific language using\nappropriate terminology and were open ended, requiring a\ntext-based response. Each question was asked once to each LLM\nby one of the authors, with no follow-up questions, rephrasing,\nor additional explanation in case of the LLM’s inability to\nanswer. It was also not asked for a second time by another\nauthor. By simulating scenarios in which oral health care\nprofessionals seek immediate assistance with single questions,\nour study mirrored real-world situations. This approach made\nit easier to assess how the LLMs could assist dentists in quick,\non-demand information retrieval and clarification, a valuable\nskill in health care practice.\nMoreover, limiting interactions to single queries allowed for a\nmore focused evaluation of the LLMs’ability to provide concise\nand relevant responses to complex queries without the need for\nreprompting, meaning that the process can be once-off and not\ntime consuming.\nThe answer to each question was evaluated and graded by 2\nexperienced faculty members of the School of Dentistry,\nEuropean University Cyprus, who were informed that they were\ngrading LLMs’ responses (authors KG and AAS). The first\nauthor is a coordinator of operative dentistry courses and holds\na graduate degree in advanced education in general dentistry\nand PhD in operative dentistry. The second author is a\ncoordinator of operative dentistry and critical appraisal of the\nliterature courses and holds a PhD in operative dentistry. The\nLLMs’ answers were graded 0 (minimum) to 10 (maximum)\npoints against a rubric (Multimedia Appendix 2). The evaluators\nwere blinded to the names of the LLM, as each LLM was\nreferred to by a letter; therefore, they were unaware of which\nLLM they were grading. The correct answer or “gold standard,”\nbased on which they were asked to evaluate the answers\nprovided by the LLMs, was given to the evaluators and was\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 3https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nallocated the maximum grade of 10/10. As the “gold standard”\nwas provided, no other calibration was required. A mixed\nmethods approach (quantitative and qualitative research) was\nused.\nQualitative Evaluation\nThe evaluators were asked to provide a qualitative evaluation\nof the LLMs’ responses in terms of their scientific accuracy,\ncomprehensiveness, clarity, and relevance in the form of free\ntext. Specifically, they were asked to provide explanatory\ncomments on the LLMs’ answers, which would document their\nchosen grade and would result from critically comparing the\nLLMs’ answers with the “gold standard.” In their analysis of\nthe answers, the evaluators could indicate the specific elements\nthat were false, irrelevant, outdated, or contradictory and their\neffect on clinical practice if they were actually applied by the\ndentist. Comments could include positive aspects of the answers,\nfor example, stating that the answers were detailed, accurate,\nand well articulated and addressed the subject sufficiently, as\nwell as negative aspects of the answers, for example, stating\nthat the answers were inaccurate, unclear, or incomplete; did\nnot match the “gold standard”; and, therefore, could not provide\nrelevant and scientifically correct guidance for an\nevidence-based practice.\nStatistical Analyses\nThe data were summarized by calculating indices of central\ntendency (mean and median values) and indices of variability\n(minimum and maximum values, SDs and SE of mean values,\nand coefficient of variation). To assess reliability, Cronbach α\nand intraclass correlation coefficient (ICC) were calculated. To\ntest whether there was a correlation between the scores of the\n2 evaluators, Pearson r and Spearman ρ were calculated.\nFurthermore, to test the differences between the scores,\nFriedman and Wilcoxon tests were performed. All statistical\nanalyses were performed using SPSS (version 29.0; IBM Corp),\nwhich was enhanced using the module Exact Tests (for\nperforming the Monte Carlo simulation method) [32]. The\nsignificance level in all hypotheses and testing procedures was\npredetermined at Cronbach α=.05 (P≤.05).\nEthical Considerations\nThe study does not involve any humans or animals. We have a\nconfirmation certificate of the President of the Institutional\nCommittee on Bioethics and Ethics of the European University\nCyprus that no ethical approval is needed for this project.\nResults\nOverview\nTable 1 presents the descriptive statistics for the scores given\nby the 2 evaluators for the answers provided by the 4 LLMs.\nBoth evaluators scored the answers of ChatGPT-4 as the best,\nfollowed by the answers of ChatGPT-3.5, Google Bard, and\nMicrosoft Bing Chat.\nMultimedia Appendix 3 presents the answers of the LLMs to\nthe 20 questions and a short description of the evidence that\nwas used as the gold standard against which the answers were\ngraded.\nThe interevaluator reliability, that is, the correlation between\nthe scores given by the 2 evaluators for the answers provided\nby the 4 LLMs, is presented in Table 2. Pearson r and Spearman\nρ revealed strong and statistically significant correlations\nbetween their scores, suggesting that the answers of the 4 LLMs\nwere evaluated in the same way. Similarly, Cronbach α and\nICC suggested high reliability. All Cronbach α values were >.6,\nand all ICCs were statistically significant (Table 2).\nCorroborating evidence was provided by Wilcoxon test, which\ndid not detect any statistically significant difference overall\nbetween the scores given by the 2 evaluators for the answers\nprovided by the 4 LLMs (Table 2), except for the scores given\nfor the answers provided by ChatGPT-4, between which a\nmarginally statistically significant difference was found\n(P=.049). Therefore, an average score was computed for the\nscores provided by the 2 evaluators for each LLM.\nFigure 1 presents the average scores for the answers provided\nby the 4 LLMs to each question. Table 3 presents the descriptive\nstatistics for the average scores for the answers provided by the\n4 LLMs. The answers of ChatGPT-4 were scored as the best,\nfollowed by the answers of ChatGPT-3.5, Google Bard, and\nMicrosoft Bing Chat.\nFriedman test revealed statistically significant differences\nbetween the average scores of the 4 LLMs (P=.046); therefore,\na series of pairwise Wilcoxon tests were performed. According\nto the Wilcoxon’s test results, a statistically significant\ndifference between the average scores of ChatGPT-3.5 and\nChatGPT-4 was noted (P=.008), and marginally statistically\nsignificant differences were noted between the average scores\nof ChatGPT-4 and Microsoft Bing Chat (P=.049) and between\nthe average scores of ChatGPT-4 and Google Bard (P=.045).\nNo other statistical differences were detected between the\naverage scores of the other LLMs (Table 4). On the basis of the\naforementioned statistics, the answers that scored the best were\nfrom ChatGPT-4 (average score=7.2), followed by those from\nChatGPT-3.5 (average score=5.9), Google Bard (average\nscore=5.7), and Microsoft Bing Chat (average score=5.4).\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 4https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 1. Descriptive statistics for the scores given by the 2 evaluators for the answers provided by the 4 large language models.\nMicrosoft Bing ChatGoogle BardOpenAI ChatGPT-4OpenAI ChatGPT-3.5\nEvaluator 2Evaluator 1Evaluator 2Evaluator 1Evaluator 2Evaluator 1Evaluator 2Evaluator 1\n01004221Minimum\n44777866Median\n1010101010101010Maximum\n5.3 (3.5; 0.8)5.6 (3.5; 0.8)5.6 (3.1; 0.7)5.8 (3.4; 0.8)6.7 (1.9; 0.4)7.7 (2.1; 0.5)6.1 (2.3; 0.5)5.8 (3.2; 0.7)Mean (SD; SE)\n66.563.955.959.429.126.938.155.0Coefficient of\nvariance (%)\nTable 2. Correlation between the scores (Pearson r and Spearman ρ), Cronbach α, intraclass correlation coefficient (ICC) for the scores, and Wilcoxon\nP value for the scores given by the 2 evaluators for the answers provided by the 4 large language models (LLMs).\nWilcoxon test\nP value\nP valuesICC aver-\nage\nP valuesICC sin-\ngle\nCronbach αP valuesSpearman ρP valuesPearson rLLMs\n.81.0050.719.0050.561.711.0040.620.007 a0.580OpenAI Chat-\nGPT-3.5\n.049.0060.659.0060.492.689.0070.586.010.536OpenAI Chat-\nGPT-4\n.75<.0010.877<.0010.782.873.0040.611<.0010.779Google Bard\n.47<.0010.919<.0010.850.917<.0010.744<.0010.847Microsoft\nBing Chat\naStatistically significant values are italicized.\nFigure 1. The average scores for the answers provided by the 4 large language models to each question.\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 5https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 3. Descriptive statistics for the average scores to the answers provided by the 4 large language model.\nMicrosoft Bing ChatGoogle BardOpenAI ChatGPT-4OpenAI ChatGPT-3.5\n0.50.03.52.0Minimum\n3.56.57.56.0Median\n10.010.010.09.0Maximum\n5.4 (3.4; 0.8)5.7 (3.1; 0.7)7.2 (1.8; 0.4)5.9 (2.4; 0.5)Mean (SD; SE)\n63.054.425.040.7Coefficient of variance (%)\nTable 4. Wilcoxon test P value for the average scores for the answers provided by the 4 large language models (LLMs).\nWilcoxon P valueLLM\n.008 aOpenAI ChatGPT-3.5 vs OpenAI ChatGPT-4\n.84OpenAI ChatGPT-3.5 vs Google Bard\n.63OpenAI ChatGPT-3.5 vs Microsoft Bing Chat\n.045OpenAI ChatGPT-4 vs Google Bard\n.049OpenAI ChatGPT-4 vs Microsoft Bing Chat\n.65Google Bard vs Microsoft Bing Chat\naStatistically significant values are italicized.\nQualitative Results\nOverview\nThe free-text qualitative comments of the evaluators were\nreviewed, analyzed, and grouped into key themes (Textbox 1).\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 6https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTextbox 1. Examples of the evaluators’ comments (exact copies).\nScientific correctness and relevance\n• “This is exactly the answer you are looking for” (Microsoft Bing Chat, operative dentistry question).\n• “Perfectly correct answer” (Microsoft Bing Chat, operative dentistry question).\n• “The answer is correct and it gives further details proving thorough knowledge of the topic” (Google Bard, endodontic question).\n• “This answer includes all the findings mentioned in the ESE guidelines and gives additional details regarding causes of RCT failure” (Open AI\nChatGPT-4, endodontic question).\n• “Additionally, answer No 8 even though not included in the answer key is also correct, so I would have given an additional mark” (Open AI\nChatGPT-4, prosthodontic question).\n• “It says to remove all carious tissue, that is a mistake (we expected the selective caries removal protocol)” (Open AI ChatGPT-3.5, operative\ndentistry question).\n• “The terminal point for chemo-mechanical preparation and obturation of a given root canal, does not depend on the obturation method applied\nor material used nor clinician’s experience and preferences!!!!!” (Open AI ChatGPT-3.5, endodontic question).\n• “Also does mistakes such as chlorhexidine mouthwash to reduce caries” (Open AI ChatGPT-4, operative dentistry question).\n• “This answer is focused on clinical findings without considering the radiographic follow up. Moreover, it mentions some causes of failure which\nis relevant information but doesn’t answer the question” (Google Bard, endodontic question).\n• “The answer is not specifically related to the question” (Google Bard, endodontic question).\n• “Half of the answer is correct, but not specific” (Open AI ChatGPT-3.5 and ChatGPT-4, operative dentistry question).\n• “Both answers from LLMA and LLMB are correct and similar” (Open AI ChatGPT-3.5 and ChatGPT-4, operative dentistry question).\nContent quality\n• “Well organized answer!” (OpenAI ChatGPT-3.5, operative dentistry question).\n• “This is the most comprehensive answer compared to the other 3. Provides regimens and doses” (OpenAI ChatGPT-3.5, oral surgery question).\n• “Vague answer” (Microsoft Bing Chat, oral surgery question).\n• “Correct answer but not complete” (Microsoft Bing Chat, endodontic question).\n• “The answer is incomplete” (Microsoft Bing Chat, endodontic question).\n• “Incomplete response, key points omitted” (OpenAI ChatGPT-3.5, OpenAI ChatGPT-4, and Microsoft Bing Chat, oral pathology question).\n• “It is on topic but not updated” (Microsoft Bing Chat, endodontic question).\n• “This answer is not thorough” (Microsoft Bing Chat, endodontic question).\n• “Few points are not mentioned clearly. Other points are not explained well” (OpenAI ChatGPT-4, prosthodontic question).\n• “Neutral answer. Two positive and two negative effects. So, no clear direction” (ChatGPT-4, orthodontic question).\n• “The answer here is not specific for answering the question” (OpenAI ChatGPT-4, operative dentistry question).\n• “It gives me the impression of not understanding the topic in depth” (OpenAI ChatGPT-4 & Google Bard, oral pathology question).\n• “I would give some marks but not full marks as the answer is very brief” (Microsoft Bing Chat, prosthodontic question).\nLanguage\n• “Very good answer, but directed to patients (not dentists)” (Google Bard, oral pathology question).\n• “It is not obvious that the answer is given by AI. It could be a dentist with a relatively good knowledge of the literature, although not completely\nup-to-date” (OpenAI ChatGPT-3.5, endodontic question).\n• “Gives the impression that it is a written informal response to an informational question” (Microsoft Bing Chat, endodontic question).\n• “The answer is directed to the patient rather than the dentist. This is not what we are looking for in this question” (Google Bard, operative dentistry\nand oral surgery question).\n• “Patients can understand well from this answer” (OpenAI ChatGPT-4, oral surgery question).\n• “This is a very similar answer to what many students may have actually answered” (Microsoft Bing Chat, prosthodontic question).\nScientific Correctness and Relevance\nIn general, the LLMs’ responses were scientifically correct and\nrelevant to the questions asked, and sometimes they were even\nsuperb. Occasionally, LLMs provided additional relevant content\noutside the immediate scope of the question, thus enriching the\nresponse. Unfortunately, the additional content was not always\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 7https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nbeneficial: “included information that was not asked” (all LLMs,\npediatric dentistry question). Scientifically incorrect, partially\ncorrect, or irrelevant answers were also noted. Similar answers\nfrom different LLMs were identified. Inability to provide an\nanswer was registered for an LLM on 2 occasions: “I’m unable\nto help, as I am only a language model and don’t have the ability\nto process and understand that” (Google Bard, oral surgery\nquestion) and “I’m a language model and don’t have the capacity\nto help with that” (Google Bard, prosthodontic question).\nScientifically incorrect answers were provided by Google Bard\nand Microsoft Bing for question 2, “what is the recommendation\nto treat a non-cavitated caries lesion that is limited to enamel\nand the outer third of dentin, on a proximal surface?” which\nwere graded 1/10 and 2.5/10, respectively. Although evidence\nappears in the guidelines and consensus statements of\ninternational organizations (eg, the FDI and ADA), Google\nBard and Microsoft Bing Chat, which claim to have access to\nthe internet, could not retrieve this information. ChatGPT-4 and\nChatGPT-3.5 answered the same question correctly, and both\nscored 9/10.\nContent Quality (Clarity, Comprehensiveness, and\nUp-to-Date Knowledge)\nThe evaluators commented on the quality of the responses,\nhighlighting some positive examples regarding the structure,\norganization, and clarity of the texts. An example of clear,\nupdated answers was noticed for question 8, “what is the\nrecommended age for a child’s first dental visit?” To this\nquestion, all LLMs correctly answered (graded 9/10) that the\nfirst visit should take place when the first primary tooth appears\nand up to 12 months of age, a recommendation that appears in\nboth the American Academy of Pediatric Dentistry and the ADA\nwebsites; however, no contradictory information appears on the\nweb, which could possibly confuse the LLMs. The evaluators\nalso noted that some responses were unclear; very brief; very\ngeneral; outdated; or did not include all the desired, important\npoints. For example, question 5, “What is the material of choice\nfor direct pulp capping (vital pulp therapy)?” for which both\nolder and updated guidelines exist, confused the LLMs, although\nthe updated guidelines were issued well before the knowledge\ncutoff date of September 2021 for ChatGPT. The only LLM\nthat clearly answered correctly was Google Bard (graded\n8.5/10), whereas Microsoft Bing Chat presented the older\nguidelines as recent guidelines (graded 0.5/10). Contradictory\nstatements within the same answer also appeared and were\ncommented on: “sealants cannot be placed in proximal surfaces\nand it mentions that before” (Google Bard, operative question).\nLanguage\nAccording to the context of the input text (scientifically\nformatted prompt), the LLMs generated responses in a similar\nformat (scientific language), but not always. We noted language\ndiscrepancies, such as “chewing surfaces of the back teeth”\ninstead of “occlusal surfaces of posterior teeth” (Google Bard,\noperative dentistry question), and the evaluators also noted these\nincompatibilities. They evaluated the language as being informal\nsometimes, where the answers seemed as though they were\ncomposed by a student or intended for the general public and\npatients.\nReferences were cited in Microsoft Bing Chat’s responses,\nalthough the authors did not specifically ask for them in their\nqueries, apparently because of the recognition of the input text’s\nformal, scientific language by the LLM, but these references\nwere not always accurate, as either they were either nonexistent\nor they redirected the reader to an irrelevant document: “I was\nnot able to find the reference mentioned in the answer” and\n“after following the link indicated in the answer the following\nreference was retrieved” (Microsoft Bing Chat, endodontic\nquestion).\nDiscussion\nPrincipal Findings and Explanations\nAlthough professional and scientific oral health care\norganizations strive to embed EBD into dental clinical practice\nthrough the development and dissemination of clinical practice\nguidelines, ongoing challenges such as rapid scientific and\ntechnological developments, outdated guidelines, a lack of\nevidence, and practice workflow obstruct successful\nimplementation [33]. The recent wave of GenAI chatbots,\ntheoretically capable of instantly generating evidence-based\nresponses to scientific queries and thus acting as the dentist’s\n“chairside personal scientific consultant,” appears to have the\npotential to be an ideal tool for the successful implementation\nand enhancement of EBD. To investigate this immersive\nopportunity, we evaluated 4 LLMs’responses to queries related\nto different dental procedures and clinical decision-making\nprocesses encountered in routine practice. The responses\ngenerated by ChatGPT-4 were provided the highest scores by\nthe evaluators (mean average score 7.2, SD 1.8; range 3.5-10),\nfollowed by those generated by ChatGPT-3.5, Google Bard,\nand Microsoft Bing Chat (mean average score 5.4, SD 3.4; range\n0.5-10), and the differences between the first LLM and the\nothers were statistically significant.\nChatGPT-4’s high score can be attributed to its large database,\nmore reliable availability, and extensive training. ChatGPT (and\nsimilar LLMs) is a natural language model trained on a vast\nand diverse amount of data using supervised fine-tuning, reward\nmodeling, and reinforcement learning to generate contextually\nrelevant and humanlike output in response to a text input\n(prompt, query, and statement) [28,34]. As with any process\nthat requires continuous training to improve and reduce its\nfailures, AI tools require large data sets to train themselves [35],\nand ChatGPT has been trained for a number of years using such\ndata sets. The first version of ChatGPT, trained on a massive\ndata set of internet-derived text, was launched by OpenAI in\nJune 2018, and a number of updated versions followed until\nJune 2020, when ChatGPT-3, a large and powerful model was\nreleased, including 175 billion parameters [13]. Continuous\ndevelopment and refinement of the model’s capabilities resulted\nin ChatGPT-3.5 in November 2022, followed by the latest\nmodel, ChatGPT-4, in March 2023.\nThe LLMs’ ranking in this study could reflect the differences\nbetween them in terms of their architecture, training data, and\nperformance characteristics, which impact their accuracy,\nrelevance, and suitability for different applications or use cases.\nIt should be noted that Google Bard and Microsoft Bing Chat\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 8https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nclaim to have live access to the internet, whereas the data set\nknowledge cutoff for ChatGPT is only September 2021.\nAlthough these LLMs are all language models and share\nsimilarities, they are based on different neural network\narchitectures: (1) ChatGPT is based on the GPT architecture, a\ndeep-learning technique that involves training the model on\nmassive data before fine-tuning it on specific tasks; (2) Google\nBard is based on Google’s LaMDA neural network architecture,\ndesigned to allow the model to better understand the context\nand generate accurate responses; and (3) Microsoft Bing Chat\nAI is based on a variety of learning models (including GPT-4),\ndepending on the specific task or application. The different\nnetwork architectures and differences in the amount and\ndiversity of training data result in the LLMs generating different\nresponses to identical questions and having different strengths,\nweaknesses, capabilities, and limitations overall, whereas\nsimilarities also exist. A study by Rudolph et al [36] that\ncompared the same chatbots as those in this study in terms of\ntheir use in higher education found the same results, with\nChatGPT-4 scoring the best, followed by ChatGPT-3.5 and then\nGoogle Bard and Microsoft Bing Chat.\nIn this study, all LLMs performed relatively well in answering\na range of clinically relevant questions (mean average score\nranging from 5.4 to 7.2 out of 10). Although ChatGPT-4’s\nanswers appeared superior, we consider this as reflecting the\nspecific conditions of this study, that is, the specific questions\nasked in a specific manner and at a specific time point. In\naddition, the evidence deduced from the quality comments can\nprove to be equally interesting and useful. Overall, the evaluators\nidentified examples of accurate, well-articulated responses,\nalthough in most cases, the responses were incomplete,\ncompared with traditional evidence. In several cases, however,\nthe machines were “hallucinating,” with the answers being\nmisleading or wrong, and these answers were presented in an\nindisputable, expert manner, making them something that could\nmisguide the clinician if they were unfamiliar with the recent\ndevelopments on the subject.\nUndeniably, LLMs possess no factual knowledge of dentistry,\nmedicine, or other sciences [12]; therefore, their errors and\ninconsistencies could be related to their operation processes.\nWhen asked a question, ChatGPT takes in the input text\nsequence; encodes it into numerical vectors using a process\ncalled “tokenization” (ie, breaking the text into words and\nsubwords); passes it through the transformer network, which\nuses attention mechanisms to weigh the importance of different\nparts of the input sequence; and generates a corresponding and\ncontextually relevant output sequence [37]. Any mishap in this\nprocess will result in an incorrect, an irrelevant, or a confusing\nresponse.\nAnother possible explanation for wrong or inaccurate answers\n(and their deviation from the established “gold standard”) could\nbe attributed to the fact that the prompts must be very specific\nfor the results to be accurate, as LLMs’ outputs are sensitive to\nthe level of detail in the question; therefore, some questions\nwere probably not phrased accurately enough for the LLMs to\ncorrectly perceive them [38]. In addition, in medical and dental\nAI, deficiencies in the representativeness of the training data\nsets (different for the different LLMs) may result in inadequate\nanswers [39]. For medical and dental questions, the LLMs need\naccess to specialized knowledge and high-quality and relevant\nscientific data, which they may not currently have, as they are\ntrained on general text data, possibly not including\ndomain-specific content [13]. In addition, LLMs are unable to\nunderstand the complex relationships between medical\nconditions and treatment options and provide relevant answers\n[17].\nComparison With Relevant Literature\nRao et al [22] used a similar research design to evaluate\nChatGPT’s capacity for clinical decision support in radiology\nvia the identification of appropriate imaging services for 2\nclinical presentations, namely the breast cancer screening and\nbreast pain, and compared ChatGPT’s responses with the\nAmerican College of Radiology Appropriateness Criteria\n(apparently used as the “gold standard”). ChatGPT scored high\nin open-ended questions (average 1.83 out of 2) and was\nimpressively accurate in responding to select all that apply\nprompts (on average, 88.9% correct responses) for breast cancer\nscreening. ChatGPT displayed more reasoning for open-ended\nprompts, where it often provided an extensive rationale for\nrecommending the specific imaging modality in accordance\nwith the American College of Radiology Appropriateness\nCriteria [22].\nThe evaluators’qualitative comments were of particular interest,\nas they reported instances where LLMs included additional\ncontent outside the immediate scope of the question or some\nvery brief, very general, and outdated content in their responses.\nFurthermore, incorrect references were cited, and partially\ncorrect, incorrect, confusing, or irrelevant answers were noted,\nas were 2 “no reply” answers from Google Bard. Such failures\nand shortcomings of LLMs have also been reported in the\nrelevant recent literature. Abstracts generated by ChatGPT were\nevaluated as “superficial and vague” [40], and responses to\nmedical questions “were not assumed as fully accurate and\nauthenticated” [13]. In a systematic review on ChatGPT’s\napplications in health care, Sallam [14] reported incorrect\ninformation in one-third of the records studied, inaccurate\nreferences in 16.7% of the records, misinformation in 8.3% of\nthe records, and overdetailed content in 8.3% of the records\n[14].\nFergus et al [15] evaluated ChatGPT-generated responses to\nchemistry assessment questions and concluded that the quality\nof the responses varied. For the answers of 10 (62%) out of the\n16 questions asked, mostly related to the application and\ninterpretation of knowledge, the evaluators assigned the grade\n0, as the answers were incorrect or there was no answer.\nInterestingly, 1 response was incorrect, although the correct\nanswer could be easily found on the internet [15]. Furthermore,\nas in our study, the evaluators commented that there were\ngeneral answers to some questions, omitted key points, and\nirrelevant additional information.\nPatel and Lam [41] described ChatGPT’s ability to produce a\npatient’s discharge summary and reported that the LLM added\nextra information to the summary that was not included in the\ninput prompt. Similarly, in a separate study testing ChatGPT’s\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 9https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nability to simplify radiology reports, key medical findings were\nreported as missing [42]. Vaishya et al [13] interacted with\nChatGPT and identified incorrect information in multiple places,\nfactual mistakes in responses to medical questions, and different\nresponses to the same questions with a lot of general\ninformation. LLMs can generate entirely wrong or inaccurate,\nbiased, or even harmful responses; fabricate information; and\npresent the fabricated information as real (“hallucinations”); all\nthese issues raise major concerns in health care practice,\nparticularly when reliable evidence is sought to inform clinical\npractice and the decision-making process [12,20,22,28,30].\nMago and Sharma [38] asked ChatGPT-3 80 questions on oral\nand maxillofacial radiology, related to anatomical landmarks,\noral and maxillofacial pathologies, and the radiographic features\nof pathologies, and the answers were evaluated by a\ndentomaxillofacial radiologist. They concluded that ChatGPT-3\nwas overall efficient and can be used as an adjunct when an oral\nradiologist requires additional information on pathologies;\nhowever, it cannot be the main reference source. ChatGPT-3\ndoes not provide the necessary details, and the data possess a\nrisk of infodemics and the possibility of medical errors [38].\nClinical Practice: Applications, Challenges,\nLimitations, and Future Directions of LLMs\nAlthough dental professionals are dedicated to providing the\nbest care for their patients, several challenges exist, resulting\nin clinicians not yet being fully aligned with the concept of\nEBD, which would facilitate clinical decision-making and\nimprove treatment outcomes in oral health care [43].\nUser-friendly and fast-growing LLMs may have the potential\nto become valuable tools in office practice and enhance\ndiagnostic accuracy, clinical decision-making, treatment\nplanning, patient communication, and oral health literacy\n[14,20]. Current research on LLMs mainly explores the\nChatGPT tool and is limited to education, research, scientific\nwriting, and patient information, whereas clinical perspectives\nhave a limited evidence level.\nIn respect to patients, patient-centered oral health care could be\nfurther promoted, with patients having access to information\nregarding their health status, thus empowering them to make\ninformed decisions. For example, Balel [44] concluded in his\nstudy that ChatGPT has significant potential as a tool for patient\ninformation in oral and maxillofacial surgery. However, patients\nshould correctly understand and interpret the information they\nobtain from the chatbot, and health care professionals should\nverify its accuracy [44]. Patients can describe their symptoms,\nask questions, and receive explanations, thus better\nunderstanding their treatment options and diagnoses; treatment\nplans may be tailored to the unique needs of each patient,\nimproving the patient-professional relationship [45]. However,\npatients’ easy and instantaneous access to medical information\n(or misinformation) may challenge professionals while\nconfronting their opinions and demands.\nChatGPT can offer personalized oral hygiene advice to help\npatients maintain good oral health, prevent common dental\nproblems, and increase their oral health literacy and awareness.\nIt can also provide postprocedure instructions and medication\nreminders, as well as offer relaxation techniques and coping\nstrategies to patients with stress [46].\nIn respect to clinicians and medical or dental professionals,\nLLMs, such as ChatGPT, could play a role in diagnosis and\ntreatment planning by analyzing patients’ symptoms, history,\nand clinical signs, thus serving as a clinical decision support\nsystem (eg, for oral diseases and rare pathologies) [47].\nIn the field of oral and maxillofacial surgery, LLMs could\ntransform perioperative care for patients and surgeons. When\nasked about relevant potential applications, GPT-4 included\npatient interaction, surgical planning and decision-making,\nassistance in clinical documentation (eg, writing of discharge\nletters), remote consultations, psychological support, and\nprotocol and guideline reminders [48].\nAmong specialist professionals, ChatGPT can serve as a\nplatform for knowledge sharing and collaboration by facilitating\ndiscussions on complex cases; enabling professionals to consider\ndiagnostic and treatment possibilities outside their routine\npractices, the sharing of research findings, and brainstorming;\nand providing a virtual space for exchanging expertise and best\npractices [45,49].\nAn important issue is that LLMs do not provide the sources of\nthe information they use, and this is a major problem, as\nverification is difficult, if not impossible, albeit necessary. This,\nin combination with the fact that LLMs were created by\ncommercial companies and without any governmental or other\ntype of legislation or control so far, may lead to information\nplatforms with unknown goals that are potentially against the\nbenefit of societies, public health, and safe and effective\nevidence-based treatment.\nTransparency (the capacity to attribute factual information to\nits source and openness of the sources), as well as all ethical\nand technical guidelines regulating the use of these machines\nand controlling their application, should be ruled by solid\nlegislation, which should be developed as soon as possible and\nserve, among other roles, as a scientific gatekeeper for\nevidence-based health care. In the margins of the EU-US Trade\nand Technology Council, a stakeholder panel named\n“Perspectives on Large AI Models” brought together EU and\nUS representatives, including the US Secretary of State Anthony\nBlinken; European Commission Executive Vice President\nMargrethe Vestager; and stakeholders representing industry,\nacademia, and civil society [50]. The need to prepare to address\nthe broader effects of AI on economies and societies and to\nregulate AI systems directly to ensure that AI benefits society\nhas also been stressed by the representatives of international\norganizations such as the International Monetary Fund [51].\nThe International Organization for Standardization (ISO) and\nthe International Electrotechnical Commission (IEC) have\nalready established concepts, terminology, and frameworks\nrelated to AI and machine learning [52,53]. Hopefully, a solid\nand detailed regulatory foundation will soon exist for AI\ntechnology [54].\nThe inherent limitations and weaknesses of LLMs reported in\nthis study, in line with the recent literature, include a lack of\nreliability (possible inaccurate, irrelevant, or inconsistent\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 10https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nresponses) and transparency (inability to attribute factual\ninformation to its source), possible outdated content, limited\ndatabase, inability to search the internet, and ethical and societal\nconcerns [12,14,22]. These shortcomings currently curtail the\nuse of LLMs as health care assistance tools, for which the LLMs\nshould be trained with high-quality, continuously updated, and\ndomain-specific data sets and thus become up-to-date, reliable,\nconsistent, and unbiased [12-14]. Before their implementation\nin evidence-based dental practice, LLMs should be clinically\nvalidated, and evidence demonstrating their clinical utility,\nefficacy, and applicability should be presented [1,14,20,55].\nFurthermore, the sources of information should be provided, at\nleast upon request, so that the dentist can evaluate the\ninformation, add to it from sources not referenced, and apply\ncritical thinking to it. Meanwhile, oral health care providers\nneed to learn how to improve the queries they ask LLMs so that\nthe latter will produce more relevant replies [28,34].\nThe future of GenAI LLMs will likely involve ongoing\ndevelopment and performance improvements, for example,\nthrough the expansion of their training data and refinement of\ntheir algorithms, which would help improve their performance\nand enhance their ability to generate more complex responses,\nsuch as those exhibiting reasoning or a deep understanding of\ncontext. A crucial factor for the future applications of LLMs in\ndentistry is training LLMs with dentistry-specific knowledge,\nsuch as teaching material from different sources and patient\nrecords and displaying different patterns and terminology,\nresulting in enhanced accuracy and relevance. Continuous\ntraining through machine learning and fine-tuning will update\nthe models’ content to include recent medical developments\nand knowledge [45]. In addition, the integration of ChatGPT\nand similar models into scientific databases, such as Web of\nScience, PubMed, and Scopus, would improve the quality and\naccuracy of responses to scientific questions; we propose that\nthis new version be named ChatGPT-Academic [44].\nIncorporating virtual and augmented reality into the LLMs will\nfundamentally alter diagnosis and treatment planning [45].\nMultimodal LLMs combining various types of input data, such\nas radiographs; biopsy microscopy images; text; audio input,\nsuch as patients’ narratives of history or symptoms; and video,\ncould lead to accurate diagnoses, as well as other applications\n[56,57]. Already, the new GPT-4 version accepts images\n(documents with photographs, diagrams, and screenshots) as\ninput queries [58].\nOn the basis of the aforementioned information, dentists still\nneed to be well educated and as updated as possible through all\nmeans of traditional evidence-based education. This would\nallow them to apply critical thinking to the information provided\nby LLMs, so it may be used in a positive way. Otherwise,\nclinicians may easily be misguided. Currently, irrespective of\nthe knowledge data set or training, LLMs do not seem to be\nable to replace unique human intellectual abilities. Any\nevaluation and use of this technology should be carried out with\nskepticism and a high level of critical thinking. We propose that\nhealth professionals and academicians should be cautious when\nusing ChatGPT and similar models, compare them with\nreputable sources, and consider them as a supplement to their\nclinical knowledge and experience [44,49]. Clinicians must be\nvery alert and apply all means of evaluation and criticism to the\ninformation provided before such tools are established as support\nfor clinical decision-making and EBD. This is in line with what\nChatGPT admitted: “while I can generate text that is related to\nscientific evidence and clinical decision-making, it is important\nto note that I am not a substitute for professional medical advice,\ndiagnosis, or treatment.” [59].\nStrengths and Limitations\nThis study has several strengths, the most important of which\nis that, to our knowledge, this is the first research study to show\nthat LLMs are related to EBD, which seems to be an excellent\ncombination, considering the clinical practice environment and\nthe capabilities of LLMs. Moreover, 4 LLMs were examined\nsimultaneously, which is a rare methodology, as almost all\nstudies retrieved investigated only 1 model, usually ChatGPT,\nas it was the first to appear for public use and the most\nprominent one. A third strength is that apart from the\nquantitative results, the study presents qualitative results (the\nevaluators’ quality comments), which offer detailed insights\ninto the LLMs’ performance and highlight some of the LLMs’\nlimitations.\nA limitation of our study could be that the questions were asked\nonly once, with no follow-up questions or requests for additional\nclarifications, which could have produced more relevant and\nless inaccurate answers. Consequently, the ability of the LLMs\nto generate evidence-based responses could have possibly been\nunderestimated. Because it has been reported that ChatGPT\nmay generate different responses to the same prompt if asked\nmultiple times (or to a slightly modified prompt), by different\nusers [15,40], or at different times [13], we chose not to\ncomplicate the research design by introducing additional\nparameters. In addition, limiting interactions to single queries\nallowed for a more focused evaluation of the LLMs’ ability to\nprovide concise and relevant responses to queries without the\nneed for reprompting, meaning that the process could be\nonce-off and not time consuming, thus mirroring real-world\nclinical practice.\nThe concept of “gold standards” could also be considered a\nlimitation, as guidelines and organizations’ recommendations\nmay differ within countries or continents and may not be\nuniversally accepted. We tried to address this by choosing\nconsensus and high-quality “gold standards,” which still may\nnot be universally applicable. Finally, it should be noted that\nthe answers reflect the LLMs’ performance at the time of\nresearch and that their performance may change over time,\nwhich is an inherent limitation of studies involving technological\ndevelopments.\nConclusions\nThe implementation of LLMs such as ChatGPT in\nevidence-based clinical practice looks promising; however,\nextensive research and clinical validation as well as model\nimprovements are needed to address their inherent weaknesses.\nUntil GenAI and LLMs reach their full potential, health care\nprofessionals should judiciously and critically use them to\ninform their clinical practice.\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 11https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nThe 4 LLMs evaluated herein in terms of their responses to\nclinically relevant questions performed rather well, with\nChatGPT-4 exhibiting the statistically significantly highest\nperformance and Microsoft Bing Chat exhibiting the lowest.\nIrrespective of the LLMs’ ranking, the evaluators identified\nsimilar advantages, weaknesses, and limitations, including\noccasional inaccuracies, errors, outdated or overgeneral content,\nand contradictory statements. Although the widespread use of\nLLMs offers an opportunity to reinforce the implementation of\nEBD, the current limitations suggest that imprudent use could\nresult in biased or potentially harmful health care decisions.\nAcknowledgments\nThis study was funded by the authors and the European University Cyprus and received no specific grant from any funding agency\nin the public, commercial, or not-for-profit sector. The authors declare that generative artificial intelligence was not used in the\nwriting of any portion the manuscript.\nData Availability\nAny data not appearing in this paper are available from the corresponding author upon reasonable request.\nAuthors' Contributions\nAll authors contributed equally to this study.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nClinical dentistry–related questions asked to the large language models.\n[DOCX File , 13 KB-Multimedia Appendix 1]\nMultimedia Appendix 2\nThe assessment rubric used to evaluate the questions.\n[DOCX File , 16 KB-Multimedia Appendix 2]\nMultimedia Appendix 3\nThe answers of the large language models (LLMs) to the questions asked and references to the scientific evidence. A summary\nof the information that the LLMs’ answers were graded against is provided for the convenience of the reader.\n[DOCX File , 102 KB-Multimedia Appendix 3]\nReferences\n1. Schwendicke F, Blatz M, Uribe S, Cheung W, Verma M, Linton J, et al. Artificial intelligence for dentistry, FDI artificial\nintelligence working group. FDI. 2023. URL: https://www.fdiworlddental.org/sites/default/files/2023-01/\nFDI%20ARTIFICIAL%20INTELLIGENCE%20WORKING%20GROUP%20WHITE%20PAPER_0.pdf [accessed\n2023-11-29]\n2. Seah J. ChatGPT and the future of dentistry. Dental Resource Asia. URL: https://dentalresourceasia.com/\nchatgpt-and-the-future-of-dentistry/ [accessed 2023-11-29]\n3. Carrillo-Perez F, Pecho OE, Morales JC, Paravina RD, Della Bona A, Ghinea R, et al. Applications of artificial intelligence\nin dentistry: a comprehensive review. J Esthet Restor Dent. 2022 Jan;34(1):259-280 [doi: 10.1111/jerd.12844] [Medline:\n34842324]\n4. Hung K, Montalvao C, Tanaka R, Kawai T, Bornstein MM. The use and performance of artificial intelligence applications\nin dental and maxillofacial radiology: a systematic review. Dentomaxillofac Radiol. 2020 Jan;49(1):20190107 [FREE Full\ntext] [doi: 10.1259/dmfr.20190107] [Medline: 31386555]\n5. Khanagar SB, Vishwanathaiah S, Naik S, A Al-Kheraif A, Devang Divakar D, Sarode SC, et al. Application and performance\nof artificial intelligence technology in forensic odontology - a systematic review. Leg Med (Tokyo). 2021 Mar;48:101826\n[doi: 10.1016/j.legalmed.2020.101826] [Medline: 33341601]\n6. Prados-Privado M, García Villalón J, Martínez-Martínez CH, Ivorra C, Prados-Frutos JC. Dental caries diagnosis and\ndetection using neural networks: a systematic review. J Clin Med. 2020 Nov 06;9(11):3579 [FREE Full text] [doi:\n10.3390/jcm9113579] [Medline: 33172056]\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 12https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n7. Islam NM, Laughter L, Sadid-Zadeh R, Smith C, Dolan TA, Crain G, et al. Adopting artificial intelligence in dental\neducation: a model for academic leadership and innovation. J Dent Educ. 2022 Nov;86(11):1545-1551 [doi:\n10.1002/jdd.13010] [Medline: 35781809]\n8. Clinical practice guidelines and dental evidence. American Dental Association. 2023. URL: https://www.ada.org/en/\nresources/research/science-and-research-institute/evidence-based-dental-research [accessed 2023-07-02]\n9. Evidence-based dentistry (EBD). FDI. URL: https://www.fdiworlddental.org/\nevidence-based-dentistry-ebd#:~:text=EBD%20is%20an%20approach%20to,patient's%20treatment%20needs%20and%20preferences\n[accessed 2023-07-02]\n10. McGlone P, Watt R, Sheiham A. Evidence-based dentistry: an overview of the challenges in changing professional practice.\nBr Dent J. 2001 Jun 23;190(12):636-639 [doi: 10.1038/sj.bdj.4801062] [Medline: 11453152]\n11. Mertens S, Krois J, Cantu AG, Arsiwala LT, Schwendicke F. Artificial intelligence for caries detection: randomized trial.\nJ Dent. 2021 Dec;115:103849 [doi: 10.1016/j.jdent.2021.103849] [Medline: 34656656]\n12. Eggmann F, Blatz MB. ChatGPT: chances and challenges for dentistry. Compend Contin Educ Dent. 2023 Apr;44(4):220-224\n[Medline: 37075729]\n13. Vaishya R, Misra A, Vaish A. ChatGPT: is this version good for healthcare and research? Diabetes Metab Syndr. 2023\nApr;17(4):102744 [doi: 10.1016/j.dsx.2023.102744] [Medline: 36989584]\n14. Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives\nand valid concerns. Healthcare (Basel). 2023 Mar 19;11(6):887 [FREE Full text] [doi: 10.3390/healthcare11060887]\n[Medline: 36981544]\n15. Fergus S, Botha M, Ostovar M. Evaluating academic answers generated using ChatGPT. J Chem Educ. 2023 Mar\n31;100(4):1672-1675 [doi: 10.1021/acs.jchemed.3c00087]\n16. Brynjolfsson E, Li D, Raymond LR. Generative AI at work. National Bureau of Economic Research. URL: http://www.\nnber.org/papers/w31161 [accessed 2023-11-29]\n17. Liu J, Wang C, Liu S. Utility of ChatGPT in clinical practice. J Med Internet Res. 2023 Jun 28;25:e48568 [FREE Full text]\n[doi: 10.2196/48568] [Medline: 37379067]\n18. Schade M. How do I use ChatGPT browse with Bing to search the web? OpenAI. URL: https://help.openai.com/en/articles/\n8077698-how-do-i-use-chatgpt-browse-with-bing-to-search-the-web [accessed 2023-11-29]\n19. Too many requests in 1 hour try again later: OpenAI ChatGPT fix. OpenAI. URL: https://www.wepc.com/tips/\ntoo-many-requests-in-1-hour-try-again-later-open-ai-chat-gpt/ [accessed 2023-11-29]\n20. Alhaidry HM, Fatani B, Alrayes JO, Almana AM, Alfhaed NK. ChatGPT in dentistry: a comprehensive review. Cureus.\n2023 Apr;15(4):e38317 [FREE Full text] [doi: 10.7759/cureus.38317] [Medline: 37266053]\n21. Milne-Ives M, de Cock C, Lim E, Shehadeh MH, de Pennington N, Mole G, et al. The effectiveness of artificial intelligence\nconversational agents in health care: systematic review. J Med Internet Res. 2020 Oct 22;22(10):e20346 [FREE Full text]\n[doi: 10.2196/20346] [Medline: 33090118]\n22. Rao A, Kim J, Kamineni M, Pang M, Lie W, Succi MD. Evaluating ChatGPT as an adjunct for radiologic decision-making.\nmedRxiv. Preprint posted online February 7, 2023. [FREE Full text] [doi: 10.1101/2023.02.02.23285399] [Medline:\n36798292]\n23. The Lancet Digital Health. ChatGPT: friend or foe? Lancet Digit Health. 2023 Mar;5(3):e102 [FREE Full text] [doi:\n10.1016/S2589-7500(23)00023-7] [Medline: 36754723]\n24. Oliver C. Married father kills himself after talking to AI chatbot for six weeks about his climate change fears. Daily Mail.\n2023 Mar. URL: https://www.dailymail.co.uk/news/article-11920801/\nMarried-father-kills-talking-AI-chatbot-six-weeks-climate-change-fears.html [accessed 2023-11-29]\n25. Hsu T, Lee MS. Can we no longer believe anything we see? The New York Times. 2023 Apr. URL: https://www.nytimes.com/\n2023/04/08/business/media/ai-generated-images.html [accessed 2023-11-29]\n26. Browne R. Italy became the first Western country to ban ChatGPT. Here’s what other countries are doing. CNBC. URL:\nhttps://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-are-doing.html[accessed 2023-11-29]\n27. Chan K. OpenAI: ChatGPT back in Italy after meeting watchdog demands. Associated Press News. URL: https://apnews.\ncom/article/chatgpt-openai-data-privacy-italy-b9ab3d12f2b2cfe493237fd2b9675e21 [accessed 2023-04-28]\n28. Korngiebel DM, Mooney SD. Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3)\nin healthcare delivery. NPJ Digit Med. 2021 Jun 03;4(1):93 [FREE Full text] [doi: 10.1038/s41746-021-00464-x] [Medline:\n34083689]\n29. GPT-4 product. OpenAI. 2023. URL: https://openai.com/product/gpt-4 [accessed 2023-04-19]\n30. Eggmann F, Weiger R, Zitzmann NU, Blatz MB. Implications of large language models such as ChatGPT for dental\nmedicine. J Esthet Restor Dent. 2023 Oct 05;35(7):1098-1102 [doi: 10.1111/jerd.13046] [Medline: 37017291]\n31. Balshem H, Helfand M, Schünemann HJ, Oxman AD, Kunz R, Brozek J, et al. GRADE guidelines: 3. Rating the quality\nof evidence. J Clin Epidemiol. 2011 Apr 01;64(4):401-406 [doi: 10.1016/j.jclinepi.2010.07.015] [Medline: 21208779]\n32. Mehta CR, Patel NR. IBM SPSS exact tests. IBM. 1996. URL: https://www.ibm.com/docs/en/SSLVMB_27.0.0/pdf/en/\nIBM_SPSS_Exact_Tests.pdf [accessed 2023-11-29]\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 13https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n33. Frantsve-Hawley J, Abt E, Carrasco-Labra A, Dawson T, Michaels M, Pahlke S, et al. Strategies for developing\nevidence-based clinical practice guidelines to foster implementation into dental practice. J Am Dent Assoc. 2022\nNov;153(11):1041-1052 [doi: 10.1016/j.adaj.2022.07.012] [Medline: 36127176]\n34. Sabzalieva E, Valentini A. ChatGPT and artificial intelligence in higher education. A quick start guide. United Nations\nEducational, Scientific and Cultural Organization. URL: https://www.iesalc.unesco.org/wp-content/uploads/2023/04/\nChatGPT-and-Artificial-Intelligence-in-higher-education-Quick-Start-guide_EN_FINAL.pdf [accessed 2023-11-29]\n35. Hemachandran K, Verma P, Pareek P, Arora N, Rajesh Kumar KV, Ahanger TA, et al. Artificial intelligence: a universal\nvirtual tool to augment tutoring in higher education. Comput Intell Neurosci. 2022;2022:1410448 [FREE Full text] [doi:\n10.1155/2022/1410448] [Medline: 35586099]\n36. Rudolph J, Tan S, Tan S. War of the chatbots: Bard, Bing Chat, ChatGPT, Ernie and beyond. J Appl Learn Teach.\n2023;6(1):1-26 [FREE Full text] [doi: 10.37074/jalt.2023.6.1.23]\n37. Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt [accessed 2023-11-29]\n38. Mago J, Sharma M. The potential usefulness of ChatGPT in oral and maxillofacial radiology. Cureus. 2023 Jul;15(7):e42133\n[FREE Full text] [doi: 10.7759/cureus.42133] [Medline: 37476297]\n39. Roganović J, Radenković M, Miličić B. Responsible use of artificial intelligence in dentistry: survey on dentists' and\nfinal-year undergraduates' perspectives. Healthcare (Basel). 2023 May 19;11(10):1480 [FREE Full text] [doi:\n10.3390/healthcare11101480] [Medline: 37239766]\n40. Gao CA, Howard FM, Markov NS, Dyer EC, Ramesh S, Luo Y, et al. Comparing scientific abstracts generated by ChatGPT\nto real abstracts with detectors and blinded human reviewers. NPJ Digit Med. 2023 Apr 26;6(1):75 [FREE Full text] [doi:\n10.1038/s41746-023-00819-6] [Medline: 37100871]\n41. Patel SB, Lam K. ChatGPT: the future of discharge summaries? Lancet Digit Health. 2023 Mar;5(3):e107-e108 [FREE\nFull text] [doi: 10.1016/S2589-7500(23)00021-3] [Medline: 36754724]\n42. Jeblick K, Schachtner B, Dexl J, Mittermeier A, Stüber AT, Topalis J, et al. ChatGPT makes medicine easy to swallow:\nan exploratory case study on simplified radiology reports. arXiv. Preprint posted online December 30, 2022 [FREE Full\ntext] [doi: 10.48550/arXiv.2212.14882]\n43. Kao RT. The challenges of transferring evidence-based dentistry into practice. J Evid Based Dent Pract. 2006\nMar;6(1):125-128 [doi: 10.1016/j.jebdp.2005.12.011] [Medline: 17138414]\n44. Balel Y. Can ChatGPT be used in oral and maxillofacial surgery? J Stomatol Oral Maxillofac Surg. 2023 Oct;124(5):101471\n[FREE Full text] [doi: 10.1016/j.jormas.2023.101471] [Medline: 37061037]\n45. de Souza LL, Lopes MA, Santos-Silva A, Vargas P. The potential of ChatGPT in oral medicine: a new era of patient care?\nOral Surg Oral Med Oral Pathol Oral Radiol (Forthcoming). 2023 Oct 05 [FREE Full text] [doi: 10.1016/j.oooo.2023.09.010]\n[Medline: 37968192]\n46. Tussie C. Transforming dentistry with ChatGPT: a guide to optimizing patient care. J Am Dent Assoc (Forthcoming). 2023\nJul 21 [FREE Full text] [doi: 10.1016/j.adaj.2023.06.003] [Medline: 37480927]\n47. Rai A, Sybil D, Shrivastava P. AI and clinicians. Br Dent J. 2023 May;234(10):711-712 [FREE Full text] [doi:\n10.1038/s41415-023-5926-2] [Medline: 37237182]\n48. Puladi B, Gsaxner C, Kleesiek J, Hölzle F, Röhrig R, Egger J. The impact and opportunities of large language models like\nChatGPT in oral and maxillofacial surgery: a narrative review. Int J Oral Maxillofac Surg (Forthcoming). 2023 Oct 03\n[FREE Full text] [doi: 10.1016/j.ijom.2023.09.005] [Medline: 37798200]\n49. Mello MM, Guha N. ChatGPT and physicians' malpractice risk. JAMA Health Forum. 2023 May 05;4(5):e231938 [FREE\nFull text] [doi: 10.1001/jamahealthforum.2023.1938] [Medline: 37200013]\n50. EU-US trade and technology council: panel discussion on large Artificial Intelligence models. European Commission.\nURL: https://digital-strategy.ec.europa.eu/en/news/\neu-us-trade-and-technology-council-panel-discussion-large-artificial-intelligence-models [accessed 2023-07-05]\n51. The power and perils of the “artificial hand”: considering AI through the ideas of Adam Smith. International Monetary\nFund. URL: https://www.imf.org/en/News/Articles/2023/06/05/sp060523-fdmd-ai-adamsmith [accessed 2023-07-09]\n52. Information technology — artificial intelligence — artificial intelligence concepts and terminology: ISO/IEC 22989:2022(en).\nInternational Organization for Standardization. URL: https://www.iso.org/obp/ui/en/#iso:std:iso-iec:22989:ed-1:v1:en\n[accessed 2023-07-29]\n53. Framework for Artificial Intelligence (AI) systems using machine learning (ML): ISO/IEC 23053:2022(en). International\nOrganization for Standardization. URL: https://www.iso.org/obp/ui/en/#iso:std:iso-iec:23053:ed-1:v1:en [accessed\n2023-11-29]\n54. EU AI Act: first regulation on artificial intelligence. European Parliament. URL: https://www.europarl.europa.eu/news/en/\nheadlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence [accessed 2023-11-29]\n55. Boon IS, Lim JS, Yap MH, Au Yong TP, Boon CS. Artificial intelligence and soft skills in radiation oncology: data versus\nwisdom. J Med Imaging Radiat Sci. 2020 Dec;51(4S):S114-S115 [doi: 10.1016/j.jmir.2020.08.011] [Medline: 32859543]\n56. Huang H, Zheng O, Wang D, Yin J, Wang Z, Ding S, et al. ChatGPT for shaping the future of dentistry: the potential of\nmulti-modal large language model. Int J Oral Sci. 2023 Jul 28;15(1):29 [FREE Full text] [doi: 10.1038/s41368-023-00239-y]\n[Medline: 37507396]\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 14https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n57. Ferres JM, Weeks WB, Chu LC, Rowe SP, Fishman EK. Beyond chatting: the opportunities and challenges of ChatGPT\nin medicine and radiology. Diagn Interv Imaging. 2023 Jun;104(6):263-264 [doi: 10.1016/j.diii.2023.02.006] [Medline:\n36925365]\n58. GPT-4. OpenAI. URL: https://openai.com/research/gpt-4 [accessed 2023-11-29]\n59. Dorri M. AI and clinical decision making. Br Dent J. 2023 May;234(10):711 [doi: 10.1038/s41415-023-5928-0] [Medline:\n37237181]\nAbbreviations\nACR: American College of Radiology\nADA: American Dental Association\nAI: artificial intelligence\nEBD: evidence-based dentistry\nFDI: Federation Dentaire Internationale\nGenAI: generative artificial intelligence\nGPT: generative pretrained transformer\nICC: intraclass correlation coefficient\nIEC: International Electrotechnical Commission\nISO: International Organization for Standardization\nLaMDA: Language Model for Dialogue Applications\nLLM: large language model\nPaLM: Pathways Language Model\nEdited by T Leung, G Eysenbach; submitted 04.08.23; peer-reviewed by H Spallek, C Wang; comments to author 14.09.23; revised\nversion received 15.10.23; accepted 20.11.23; published 28.12.23\nPlease cite as:\nGiannakopoulos K, Kavadella A, Aaqel Salim A, Stamatopoulos V, Kaklamanos EG\nEvaluation of the Performance of Generative AI Large Language Models ChatGPT, Google Bard, and Microsoft Bing Chat in\nSupporting Evidence-Based Dentistry: Comparative Mixed Methods Study\nJ Med Internet Res 2023;25:e51580\nURL: https://www.jmir.org/2023/1/e51580\ndoi: 10.2196/51580\nPMID: 38009003\n©Kostis Giannakopoulos, Argyro Kavadella, Anas Aaqel Salim, Vassilis Stamatopoulos, Eleftherios G Kaklamanos. Originally\npublished in the Journal of Medical Internet Research (https://www.jmir.org), 28.12.2023. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits\nunrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of\nMedical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on\nhttps://www.jmir.org/, as well as this copyright and license information must be included.\nJ Med Internet Res 2023 | vol. 25 | e51580 | p. 15https://www.jmir.org/2023/1/e51580\n(page number not for citation purposes)\nGiannakopoulos et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX"
}