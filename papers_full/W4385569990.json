{
    "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
    "url": "https://openalex.org/W4385569990",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5049198824",
            "name": "Ercong Nie",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "Munich Center for Machine Learning"
            ]
        },
        {
            "id": "https://openalex.org/A2131857791",
            "name": "Sheng Liang",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "Munich Center for Machine Learning"
            ]
        },
        {
            "id": "https://openalex.org/A2098897977",
            "name": "Helmut Schmid",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2035156685",
            "name": "Hinrich Schütze",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963826397",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W3100880133",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3021766370",
        "https://openalex.org/W3038047279",
        "https://openalex.org/W4224247062",
        "https://openalex.org/W3115830533",
        "https://openalex.org/W4385573782",
        "https://openalex.org/W2788448041",
        "https://openalex.org/W2917273768",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W4385571891",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4206529673",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4299574851",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4292420126",
        "https://openalex.org/W3100806282",
        "https://openalex.org/W4205523161",
        "https://openalex.org/W2963078909",
        "https://openalex.org/W2891177506",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W2739967986",
        "https://openalex.org/W4285122897",
        "https://openalex.org/W4283794478",
        "https://openalex.org/W1532325895",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W3168875417"
    ],
    "abstract": "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1%) and labeled settings (+16.3%). PARC also outperforms finetuning by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8320–8340\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCross-Lingual Retrieval Augmented Prompt for Low-Resource Languages\nErcong Nie⋆ 1,2 Sheng Liang⋆ 1,2 Helmut Schmid1 Hinrich Schütze1,2\n1Center for Information and Language Processing (CIS), LMU Munich, Germany\n2 Munich Center for Machine Learning (MCML), Munich, Germany\n{nie, shengliang}@cis.lmu.de\nAbstract\nMultilingual Pretrained Language Models\n(MPLMs) perform strongly in cross-lingual\ntransfer. We propose Prompts Augmented\nby Retrieval Crosslingually ( PARC) to im-\nprove zero-shot performance on low-resource\nlanguages (LRLs) by augmenting the context\nwith prompts consisting of semantically similar\nsentences retrieved from a high-resource lan-\nguage (HRL). PARC improves zero-shot perfor-\nmance on three downstream tasks (sentiment\nclassification, topic categorization, natural lan-\nguage inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language fami-\nlies in unlabeled (+5.1%) and labeled settings\n(+16.3%). PARC also outperforms finetuning\nby 3.7%. We find a significant positive cor-\nrelation between cross-lingual transfer perfor-\nmance on one side, and the similarity between\nhigh- and low-resource languages as well as the\namount of low-resource pretraining data on the\nother side. A robustness analysis suggests that\nPARC has the potential to achieve even stronger\nperformance with more powerful MPLMs.\n1 Introduction\nMultilingual pretrained language models (MPLMs)\n(Devlin et al., 2019; Conneau et al., 2020; Liu et al.,\n2020; Xue et al., 2021; Shliazhko et al., 2022),\npretrained on multilingual corpora with >100 lan-\nguages, exhibit strong multilinguality on down-\nstream tasks (Hu et al., 2020).\nLow-resource languages, for which little text\ndata is available for pretraining monolingual pre-\ntrained language models (PLMs), benefit from\nMPLMs. However, the lack of LRL data leads\nto an imbalanced language distribution in the\npretraining corpora of MPLMs (Wu and Dredze,\n2020). LRLs are therefore under-represented in\npretraining, resulting in bad performance. Further-\nmore, the scarcity of domain- or task-specific an-\nnotated data of LRLs makes it difficult to apply the\n⋆ Equal Contribution.\n(a) Retrieval from high-resource language corpora\n(b) Prediction with a retrieval-augmented prompt\nFigure 1: Main idea of PARC: we enhance zero-shot\nlearning for low-resource languages (LRLs) by cross-\nlingual retrieval from labeled/unlabeled high-resource\nlanguages (HRLs). (a) An LRL input sample is taken\nas query by the cross-lingual retriever to retrieve the\nsemantically most similar HRL sample from the HRL\ncorpus. The label of the retrieved HRL sample is ob-\ntained either from the corpus ( labeled setting) or by\nself-prediction (unlabeled setting). (b) The retrieved\nHRL sample together with its label and the input sample\nare reformulated as prompts. The cross-lingual retrieval-\naugmented prompt is created by concatenation and taken\nby the MPLM for prediction. Our experiments show\nthat PARC outperforms other zero-shot methods and\neven finetuning.\npretraining-finetuning paradigm to LRLs (Lauscher\net al., 2020). Given that the pretraining-finetuning\nparadigm always has a high demand for domain-\nspecific labeled data, another line of research –\nprompt-based learning – emerges, focusing on ex-\nploiting large pretrained language models by re-\nformulating the input. The prompt is designed to\n8320\nhelp PLMs “understand” the task better and “recall”\nwhat has been learned during the pretraining. In\nparticular, Brown et al. (2020) propose a simple in-\ncontext learning approach without any finetuning,\nwhich adds training examples as additional context\nto test examples. Instead of using random exam-\nples as context, KATE (Liu et al., 2022a) and SOUP\n(Liu et al., 2022b) retrieve semantically similar ex-\namples as prompt for monolingual in-context learn-\ning. The above mentioned prompt-based learning\ntechniques require no parameter updating, while\nthere is also work employing sampled similar ex-\namples for prompt-based funetuning (Gao et al.,\n2021). Unlike Brown et al. (2020) who created\nprompts with manually selected examples, these\napproaches augment the context by retrieving re-\nlated information from external corpora, allowing\nthe PLMs to capture more domain- or task-specific\nknowledge. The prompt-based method offers a new\nform of zero-shot or few-shot learning in multilin-\ngual NLP studies. It involves performing a specific\ntask using prompts, without labeled data in the\ntarget language and has the potential of being an\neffective method for LRLs lacking annotated data.\nOur work improves the zero-shot transfer learn-\ning performance of LRLs on three different classi-\nfication tasks by taking advantage of cross-lingual\ninformation retrieval and the multilinguality of\nMPLMs. Specifically, we retrieve semantically\nsimilar cross-lingual sentences as prompts and use\nthe cross-lingual retrieval information to benefit\nthe LRLs from the multilinguality of MPLMs and\nachieve better performance in the zero-shot set-\nting1. Our main contributions are: (1) We propose\nPrompts Augmented by Retrieval Crosslingually\n(PARC), a pipeline for integrating retrieved cross-\nlingual information into prompt engineering for\nzero-shot learning (Figure 1). (2) We conduct ex-\nperiments on several multilingual tasks, showing\nthat PARC improves the zero-shot performance on\nLRLs by retrieving examples from both labeled\nand unlabeled HRL corpora. (3) To find an optimal\nconfiguration of our PARC pipeline, we conduct\na comprehensive study on the variables that affect\nthe zero-shot performance: the number of prompts,\nthe choice of HRL, and the robustness w.r.t. other\nretrieval methods and MPLMs.\n1Different from the zero-shot cross-lingual transfer learn-\ning where MPLMs are finetuned on HRLs (Hu et al., 2020),\nour zero-shot setting does not involve finetuning. Details in\n§6.4\n2 Related Work\nRetrieval methods External knowledge ex-\ntracted by information retrieval is often leveraged\nto solve NLP tasks. Two types of representations\nhave been used for retrieval: (1) sparse bag-of-\nwords representations (Chen et al., 2017; Wang\net al., 2018), and (2) dense representation learned\nby neural networks (Qu et al., 2020). Dense repre-\nsentations come either from contextual token em-\nbeddings (May et al., 2019; Zhang et al., 2020)\nor from sentence encoders (Conneau et al., 2017;\nCer et al., 2018). Reimers and Gurevych (2019)\npropose sentence transformers to create seman-\ntically meaningful sentence embeddings by ap-\nplying siamese and triplet network structures to\ntransformer-based pretrained language models. By\nusing knowledge distillation, sentence transformers\ncan be expanded to support various languages as\nmultilingual sentence transformers (Reimers and\nGurevych, 2020), allowing for cross-lingual re-\ntrieval.\nRetrieval augmented prompt Brown et al.\n(2020) show that large-scale pretrained language\nmodels such as GPT-3 can learn to perform a task\nby putting examples of input-output pairs into the\ninput as context. The in-context learning method\nsimply concatenates the input with examples ran-\ndomly extracted from the training set. Recent stud-\nies (Gao et al., 2021; Liu et al., 2022a,b) augment\nthe prompts for pre-trained models by sampling\nsemantically similar examples. They apply the\nretrieval augmented method to discrete prompts,\nwhich are represented by tokens instead of vectors\nin a continuous space. They use them either for fine-\ntuning in few-shot settings or for zero-shot learning.\nChowdhury et al. (2022) use a similar kNN-based\nretrieval method for tuning the soft prompts in a\ncontinuous space with a standard supervised train-\ning setup. Previous work focused on monolingual\nretrieval-augmented prompts. Our work applies\ncross-lingual retrieval to discrete prompts in a sce-\nnario without parameter updating. To the best of\nour knowledge, our work is the first to investigate\nprompt learning augmented by cross-lingual re-\ntrieval.\nMultilingual prompt learning Despite the suc-\ncess of prompting in English, prompting in multilin-\ngual tasks has not been extensively studied. Winata\net al. (2021) show the multilingual skills of LMs\nmainly trained on English data in prompt learning\n8321\nby giving them a few English examples as context\nbut testing them on non-English data. Some recent\nworks investigate the prompt learning with multilin-\ngual PLMs (Zhao and Schütze, 2021; Huang et al.,\n2022). Unlike our work, they focus on finetuning\nor prompt tuning requiring parameter updating. We\napply our method to LRLs in a zero-shot setting\nwithout adjusting the model parameters.\n3 Methodology\nThis work aims to improve the performance of\nMPLMs on LRLs in the zero-shot setting by lever-\naging retrieved cross-lingual contents from HRLs.\nFor that, we design the PARC pipeline that can\nbe applied to labeled and unlabeled scenarios, i.e.,\nthe HRL information can be retrieved from either\nlabeled or unlabeled corpora.\nAs Figure 1 shows, the PARC pipeline consists\nof two steps: (a) Cross-lingual retrieval from high-\nresource language corpora, and (b) prediction with\na retrieval-augmented prompt. Figure 1 shows an\nexample: A Telugu input sentence from a senti-\nment classification task is firstly fed into the cross-\nlingual retriever to fetch the semantically closest\nsample from the HRL corpus, i.e. English in this\ncase. In the second step, the retrieved HRL sample\ntogether with its label and the LRL input sentence\nare transformed into a prompt. For prompt-based\nclassification, we need (i) a pattern which converts\nthe input sentence into a cloze-style question with\na mask token, and (ii) a representative word (called\nverbalizer) for each possible class. Converting the\nclassification task into a cloze-style question aligns\nseamlessly with the framework of our proposed\nPARC method, because it not only performs zero-\nshot learning well but, more significantly, facili-\ntates better integration of the retrieved cross-lingual\ncontexts.\nIn our example, we use the pattern P(X) =X◦\n“In summary, the product was [MASK]. ”\nto convert the retrieved English sentence into\n“Wonderful! Works as stated! In summary,\nthe product was [MASK].”, where ◦is the string\nconcatenation operator. A verbalizer such as {pos\n→“great”, neg →“terrible”}, which maps the orig-\ninal labels {pos, neg} onto words in the vocabulary,\nis then used to replace the [MASK] token with the\nverbalized label word “great”, standing for the cor-\nrect label pos of this sentence. We call the resulting\nEnglish sentence (in our example: “ Wonderful!\nWorks as stated! In summary, the product\nwas great.”) the “cross-lingual context”. At last,\nwe fill the same pattern with the input Telugu sen-\ntence and append it to the cross-lingual context.\nWe feed this cross-lingual retrieval augmented in-\nput to the MPLM. The MPLM returns for each of\nthe verbalizers its probability of being the masked\ntoken.\nMore formally, let XL\ni ∈DL be the input sam-\nple from the LRL test set, (XH\nj , yj) ∈DH\nlb and\nXH\nj ∈DH\nun denote the HRL data from the labeled\nand unlabeled corpora, respectively, where Xj is\nthe text sample and yj its class label from a la-\nbel set Y . As Eq. (1) shows, the cross-lingual\nretriever CLR takes the HRL corpora DH and a\ngiven LRL input sentenceXL\ni . It returns an ordered\nlist of HRL sentences DRi according to the seman-\ntic similarity. We then have(XRi\nk , yRi\nk ) ∈DRi\nlb and\nXRi\nk ∈DRi\nun for labeled and unlabeled scenarios,\nrespectively, where XRi\nk is the k-th most similar\nHRL sentence to the LRL input XL\ni .\nDRi = CLR(XL\ni , DH) (1)\nThe prompt pattern P(.) converts an HRL input\nsentence XRi\nk into a cloze-style form with a mask\ntoken. The verbalizer v(.) is a bijective mapping\nfrom the set of class labels Y to a set of verbalized\nwords V from the HRL vocabulary. We use the\nverbalized label word to fill in the mask token in\nthe prompt pattern, and construct the cross-lingual\ncontext Ci\nk for the input XL\ni with the k-th most\nsimilar HRL sample XRi\nk :\nCi\nk = P(XRi\nk , v(yRi\nk )) (2)\nThe cross-lingual context Ci\nk is then concate-\nnated with the prompted LRL input as the input I\nto the MPLM:\nIi = Ci\nk ◦P(XL\ni ) (3)\nThe MPLM M performs masked token predic-\ntion and returns the probabilities p = M(Ii) of all\ncandidate words for the masked token in Ii. We\npredict the class ˆy whose verbalizer v(ˆy) received\nthe highest probability from model M:\nˆy = arg max\ny∈Y\np(v(y)) (4)\nIn the labeled scenario, we obtain the correct\nlabel yRi\nk of the HRL sentence from DRi\nlb . In the\nunlabeled scenario, we predict the label using the\nsame prompt-based classification method without\n8322\ncross-lingual context, similar to Eq. (4). We call\nthis the self-prediction method:\nˆyRi\nk = arg max\ny∈Y\nM(P(XRi\nk ), v(y)) (5)\nIn order to use more cross-lingual information,\nwe retrieve the K most similar HRL samples. With\neach sample, we obtain verbalizer probabilities as\ndescribed above and retrieve the class whose ver-\nbalizer has the largest sum of probabilities. We\ncall this method the Bag-of-Retrieval (BoR) strat-\negy. We also tried concatenating the different cross-\nlingual contexts (CONC method), but the resulting\nperformance has been worse (see Table 15 in the\nAppendix).\n4 Experimental Setup\n4.1 Datasets\nBase Datasets Three representative classification\ntasks are selected for evaluation in this work: bi-\nnary sentiment analysis on Amazon product re-\nviews (Keung et al., 2020), topic classification on\nAG News texts (Zhang et al., 2015), and natural lan-\nguage inference on XNLI (Conneau et al., 2018).\nAmazon Reviewsdataset categorizes the shop-\nping reviews into 5 star ratings from 1 to 5. In order\nto satisfy a binary classification setting, we select\nthe reviews with rating 1 as negative (0) and 5 as\npositive (1) for our experiments. The following\npattern P(X) and verbalizer v are defined for an\ninput review text X:\n• P(X) =X ◦“All in all, it was [MASK].”\n• v(0) =“terrible”, v(1) =“great”\nAG Newsis a collection of more than 1 million\nnews articles for topic classification. The news\ntopic categories contained in the dataset are World\n(0), Sports (1) , Business (2), and Tech (3). The\npattern and verbalizers are as follows:\n• P(X) =“[MASK] News: ” ◦X\n• v(0) =“World”, v(1) =“Sports”,\nv(2) =“Business”, v(3) =“Tech”\nXNLI is a multilingual version of the MultiNLI\ndataset (Williams et al., 2018). We use a subset\nof the original XNLI dataset in our experiment.\nThe text in each data item consists of two parts.\nSentence A is the premise and sentence B is the\nhypothesis. The NLI task is to predict the type\nof inference between the given premise and hy-\npothesis among the three types: entailment (0),\nneutral (1) and contradiction (2). For a given\nsentence pair X1 and X2, we design the pattern\nand verbalizer as:\n• P(X1, X2) =X1 ◦“? [MASK],” ◦X2\n• v(0) = “Yes” , v(1) = “Maybe” , v(2) =\n“No”\nConstruction of Multilingual Parallel Test Sets\nParallel test datasets for evaluating cross-lingual\ntransfer performance on LRLs are rare. However,\nrecent research conducted by Hu et al. (2020); Liu\net al. (2022c) shows that automatically translated\ntest sets are useful for measuring cross-lingual per-\nformance. Hence, we adopt their methodology and\nconstruct datasets for different tasks by automati-\ncally translating English test sets to targeted LRLs.\nWe use the Python API of the Google Translate Sys-\ntem to implement the construction of multilingual\nparallel test sets in our experiment. We also vali-\ndate the translation effectiveness and quality. The\noriginal XNLI datasets include two low-resource\nlanguages that are used in our experiments (Swahili\nand Urdu), so we use them as the “gold” standard\nfor our translation validation. We compare the\ncross-lingual transfer performance on translation\ntest sets and original test sets of XNLI. We also\nmeasure the translation quality by using the orig-\ninal sets as gold standard. Through the validation\nconducted on these two languages within the XNLI\ntask, we infer that the translation method is effec-\ntive and could be generalized to other languages\nand tasks. Detailed results are shown in Appendix\n§A.\nFollowing Wu and Dredze (2020), we regard\nlanguages with a WikiSize2 of less than 7 as LRLs.\nWe construct a test set consisting of 10 LRLs in 6\nlanguage families: Indo-European (Afrikaans - af,\nUrdu - ur), Austronesian (Javanese - jv, Tagalog -\nta), Altaic (Mongolian - mn, Uzbek - uz), Dravidian\n(Tamil - tl and Telugu - te), Sino-Tibetan (Burmese\n- my), and Niger-Congo (Swahili - sw). Table 18 in\nthe Appendix shows more information on the test\nsets.\nHRL Corpora To retrieve rich and diverse in-\nformation, a large-scale general corpus or knowl-\nedge base in the different HRLs would be the ideal\n2WikiSize less than 7 means that the Wikipedia corpus of\nthe language is smaller than 0.177 GB.\n8323\nsentence retrieval pool. In practice, however, a\ntrade-off is necessary in order to save computa-\ntional resources. Following Wang et al. (2022), we\ntherefore use the task-specific labeled training set\nof English as the sentence pool in our experiments.\nThe selection of the HRL will be discussed in §6.2.\n4.2 Baseline\nWe compare PARC with the following baselines in\nboth labeled and unlabeled settings:\nMAJ The majority baseline. Since we construct\nthe test sets to be balanced, MAJ is equivalent to\nrandom guess.\nRandom We randomly retrieve a cross-lingual\nsentence as prompt, similarly to the simple in-\ncontext learning using examples without semantic\nsimilarity to the input (Brown et al., 2020).\nDirect The pattern filled with the input sample\nis directly fed to the MPLM for prediction, without\nadding cross-lingual context to the prompts.\nFinetune The MPLM is first finetuned with the\nretrieved high resource sentences. Then the low-\nresource test input is predicted by the finetuned\nMPLM. We use the Cross Entropy Loss as the\nobjective function for finetuning and AdamW for\noptimization with a learning rate of 1e-5. Since the\nfinetuning data is very limited, we only train for a\nsingle epoch to avoid overfitting.\nOur test sets are constructed by machine trans-\nlation. Therefore we cannot apply a translation\nbaseline, where we translate the input sample into\nthe high resource language before feeding it to the\nMPLM. The Appendix presents a different experi-\nment where we compare with a translation baseline.\n4.3 Models\nCross-Lingual Retriever The retrieval methods\nused in monolingual NLP are either based on sparse\nor dense representations. Sparse representations\nsuch as BM25 (Manning et al., 2008) which is\nbased on term frequency, cannot be used for cross-\nlingual retrieval as the shared words across dif-\nferent languages are normally scarce. Therefore\ndense representations from deep learning methods\nsuch as LASER (Artetxe and Schwenk, 2019) and\nsentence-BERT (Reimers and Gurevych, 2019) are\nmore suitable for our pipeline.\nWe choose the multilingual sentence trans-\nformer (Reimers and Gurevych, 2020) version\n“paraphrase-multilingual-mpnet-base-v2” as the re-\ntriever in our experiments. This multilingual re-\ntriever is based on XLM-R (Conneau et al., 2020)\nAmazon AGNews XNLI Avg.\nMAJ 50.0 25.0 33.3 36.1\nRandom 48.2 25.6 32.4 35.4\nDirect 53.8 36.3 33.1 41.1\nFinetune 68.6 57.9 34.5 53.7\nPARC-unlabeled 58.4 46.7 33.5 46.2\nPARC-labeled 68.9 67.6 35.8 57.4\nTable 1: Overview of results on three classification tasks.\nThe reported numbers are averaged across 10 evaluation\nLRLs. The number of prompts k=1 in relevant baselines\nand our methods for all three tasks.\nand trained on parallel data from 50+ languages\nby employing knowledge distillation. Through the\nmultilingual sentence transformer, sentences are\nrepresented by embeddings. We use the sentence\nembeddings to calculate the cosine similarity be-\ntween the LRL inputs and HRL sentences and rank\nthe most similar ones for retrieval. Robustness\nwith respect to other cross-lingual retrievers will\nbe discussed in §6.3.\nMultilingual Pretrained Language Model In\norder to solve cloze-style classification tasks, we\nuse the pretrained multilingual BERT model “bert-\nbase-multilingual-cased” (Devlin et al., 2019). It\ncontains 178M parameters and was trained on\nWikipedia corpora in 104 languages. In §6.3, we\nwill also explore XLM-R (Conneau et al., 2020),\nanother multilingual pretrained language model.\nAll the models mentioned above were imple-\nmented using the Huggingface Transformers li-\nbrary (Wolf et al., 2020).\n5 Results\nTable 1 presents an overview of the results on the\nthree tasks3. PARC outperforms the MAJ, Direct\nand Random baseline on all three tasks, in both la-\nbeled and unlabeled settings: When retrieving from\nunlabeled high-resource language corpora, the per-\nformance is improved by 4.6%, 10.4% and 0.4%\ncompared to Direct on Amazon Review, AG News,\nand XNLI respectively. When retrieving from la-\nbeled HRL corpora, the performance is improved\nby 15.1%, 31.3% and 2.7%. The Finetune base-\nline uses the label of retrieved examples for prompt-\nbased finetuning. Hence it is fair to compare it with\nPARC in the labeled setup rather than the unlabeled\none. PARC-labeled outperforms Finetune by 0.3%,\n9.7% and 1.3% on the three tasks respectively.\nAlthough our proposed methods perform better\n3k = 1unless otherwise specified.\n8324\nthan the baselines on all three tasks, the degree\nof improvement differs. A large improvement is\nfound on AG News, the topic categorization task,\nwhile XNLI only shows a slight improvement. An\nexplanation for this could be that the natural lan-\nguage inference task is more difficult than topic cat-\negorization, especially in a zero-shot setup. Also,\nprior work has shown that designing cloze-style pat-\nterns and searching the answer space for NLI tasks\n(Schick and Schütze, 2021; Webson and Pavlick,\n2022) is difficult.\nWe also find that PARC-labeled noticeably out-\nperforms PARC-unlabeled, indicating that the per-\nformance of self-prediction is limited by the ca-\npabilities of mBERT. More powerful MPLMs and\nbetter pattern designs might further improve the\nperformance.\nTo analyze the performance for every language\nin detail, we present the complete experimental re-\nsults for the topic categorization task on AG News\nin Table 2. Here, we use the BoR method to take\nadvantage of multiple retrieved HRL sentences. As\nexpected, PARC outperforms the Direct baseline\non all languages in both labeled and unlabeled set-\ntings.\nHowever, it is worth noting that the sensitivity to\ncross-lingual retrieval differs from language to lan-\nguage. For some LRLs, e.g. Urdu (Ur) and Uzbek\n(Uz), PARC’s improvement from cross-lingual re-\ntrieval is smaller. Others gain more, e.g. Javanese\n(Jv). Retrieving more samples increases the per-\nformance up to k=30 except for Telugu (Te) and\nSwahili (Sw) where the max is reached for k=20.\nWe now turn to the following two questions: 1)\nHow does k affect the performance on other tasks\nthan topic categorization? 2) Which LRLs profit\nmost from our PARC method and which HRLs are\nbest suited to retrieve prompts?\n6 Analysis\n6.1 Effect of k\nWe investigated how the performance changes as\nthe number of retrieved HRL samples k increases.\nAs shown in Figure 2, an abrupt accuracy increase\ncan be seen in both labeled and unlabeled scenarios\nby concatenating the most similar cross-lingual\nsample. In labeled scenarios, the performance\ntends to increase up to k=20 and then levels off.\nThis can be explained by the fact that later retrieved\nsamples are less similar to the input sample, so their\ncontribution as prompts decreases. In unlabeled\nFigure 2: Accuracy on three tasks with different k in\nthe labeled (LB) and unlabeled (UN) setup.\nscenarios, there is no clear improvement beyond\nk=1 except for AGNews(UN), where the accuracy\nincreases monotonically except for k=10. The per-\nformance of XNLI is less obviously influenced by\nthe value of k than binary sentiment analysis and\ntopic categorization. We assume that this could\nbe attributed to the difficulty of the inference task.\nUnlike the other two single sentence classification\ntasks, XNLI identifies the relationship between a\npair of sentences. Transferring knowledge about\nsentence relationships is more complicated and re-\nquires more samples to learn, in contrast to the\nother two tasks where semantic information from\nsimilar cross-lingual sentences can be transferred\ndirectly.\n6.2 Effect of Languages\nLauscher et al. (2020) pointed out that two lin-\nguistic factors exert crucial effects on cross-lingual\ntransfer performance: (1) the size of the pretraining\ncorpus for the target language and (2) the similar-\nity between the source and target language. In our\nstudy, we also consider a third factor: (3) the size of\nthe pretraining corpus for the source language. In\nthis section, we conduct a correlation analysis be-\ntween PARC’s cross-lingual transfer performance\nand the three language-related factors mentioned\nabove. To achieve that, we have to measure these\nfactors in a proper way at first. The size of the\npretraining corpus can be easily measured by the\nlog2 value of the Wikipedia size in MB, as we men-\ntioned in §4. Thus the remaining problem is how\nto properly represent language similarity.\n6.2.1 Measurement of Language Similarity\nMalaviya et al. (2017) and Littell et al. (2017) pro-\npose LANG2VEC from linguistic, typological, and\nphylogenetic perspectives. LANG2VEC employs\ndifferent vectors to represent various types of lin-\n8325\nEn Af Jv Mn My Sw Ta Te Tl Ur Uz Avg\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 52.5 41.8 27.4 42.5 32.2 31.3 31.5 33.0 31.6 46.9 44.8 36.3\nUN\nk=1 53.7 52.8 46.2 46.5 46.1 42.8 43.3 44.3 45.0 51.0 49.7 46.7\nk=3 55.8 53.6 46.2 47.1 48.2 44.9 44.5 46.3 47.1 52.6 51.0 48.1\nk=5 57.1 54.4 47.0 47.0 48.0 46.6 44.8 45.8 48.5 53.1 52.3 48.7\nk=10 57.5 55.3 46.3 46.4 47.6 45.6 44.1 46.7 47.7 53.0 51.4 48.4\nk=20 59.7 57.2 48.1 46.7 50.0 47.9 46.0 48.9 49.6 55.4 53.2 50.3\nk=30 60.1 57.4 49.0 47.4 51.1 49.2 47.1 48.7 50.1 56.5 54.4 51.1\nLB\nk=1 74.9 75.4 68.1 63.5 68.2 64.0 62.8 65.6 64.8 72.5 71.4 67.6\nk=3 77.1 77.1 69.6 65.6 71.1 67.6 65.6 68.4 65.9 74.6 74.4 70.0\nk=5 78.1 78.6 69.0 64.4 72.9 68.8 65.9 69.3 66.4 75.8 75.4 70.6\nk=10 78.7 79.4 70.5 67.0 72.9 68.3 66.6 70.7 67.2 76.6 75.9 71.5\nk=20 79.0 79.7 70.7 67.5 72.5 70.0 67.5 70.7 68.1 77.4 76.3 72.0\nk=30 79.0 79.7 71.3 67.6 72.8 69.9 68.1 71.1 69.4 77.2 76.7 72.4\nTable 2: Results of topic categorization task on AG News dataset. k is the number of retrieved cross-lingual sample.\nMAJ is the majority baseline. Avg is the average accuracy across 10 LRLs. En is the HRL for retrieval. BoR\nstrategy is adopted.\n(a) Zero-Shot Performance (Unlabeled)\n (b) Language Similarity\n (c) Zero-Shot Performance (labeled)\nFigure 3: Visualization of the correlation between zero-shot performance and language similarity, pretraining\ndata size of source and target language. On the X(Y)-axis are target(source) languages with an increasing order\nof pretraining data size from left(bottom) to right(top). (a) and (c) show the zero-shot performance with PARC-\nunlabeled and PARC-labeled on Amazon review task respectively. (b) shows the language similarity of each\npair.\nguistic features for different languages. Each lan-\nguage is encoded with 5 vectors corresponding to\ndifferent linguistic features including three typo-\nlogical features (syntax, phonology and phonetic\ninventory), phylogenetic and geographical features.\nIn typological vectors, each dimension represents\na linguistic property. For example, one dimension\nof the syntax vector represents the word order fea-\nture SVO. If a language has a SVO order, then its\nsyntax vector would have the value 1 on this di-\nmension. Missing values in the typological vectors\ncould have detrimental effects. Therefore we re-\nplace them with values predicted from the k most\nsimilar typological vectors (Malaviya et al., 2017).\nThe phylogenetic vector embodies the position of a\nlanguage in the world language family tree (Harald\net al., 2015), while the geographical vector contains\nthe position information of languages w.r.t. their\nspeakers.\nFollowing prior work (Rama et al., 2020), we\nconsider all 5 linguistic features when measuring\nthe language similarity: syntax (SYN), phonology\n(PHO), phonological inventory (INV), language\nfamily (FAM), and geography (GEO). Given these\ndifferent types of vectors, we calculate 5 cosine\nsimilarities for each pair of source language (i) and\ntarget language (j) and average them to get the final\nlanguage similarity sim(i, j):\nsim(i, j) = 1\n|F|\n∑\nf∈F\ns(vf (i), vf (j)) (6)\nwhere Fis the set of features, vf (i) and vf (j)\nstand for the language vectors representing the\nfeature f for i and j, and s(·) computes the min-\nmax normalized cosine similarity of the two vec-\ntors. The detailed cosine similarities between En-\nglish and 10 LRLs evaluated in our experiment are\nshown in Table 9 in Appendix §B.\n6.2.2 Correlation Analysis\nWe conduct a correlation analysis between cross-\nlingual performance and the three language factors\nmentioned above: language similarity between the\n8326\nUnlabeled Sim. source size target size\ncorr p corr p corr p\nSpearman 0.28 0.05 0.20 0.16* 0.31 0.03\nPearson 0.27 0.06* 0.22 0.12* 0.38 6e-03\nlabeled Sim. source size target size\ncorr p corr p corr p\nSpearman 0.42 2e-03 0.08 0.54* 0.44 1e-03\nPearson 0.41 3e-03 -3e-4 1.00* 0.46 8e-4\nTable 3: Correlations between Amazon review perfor-\nmance and three features. Sim.: language similarity\nbetween an LRL and an HRL; source (target) size: the\nlog of the data size (MB) of source (target). *: insignifi-\ncant result with a p value larger than 0.05.\nAmazon AGNews XNLI Avg.\nDirect 53.8 36.2 33.1 41.0\nUN\nmBERT+pooling 53.1 36.9 33.6 41.2\nmBERT+distiluse 54.7 38.4 34.0 42.3\nmBERT+paraphrase 59.6 46.7 33.7 46.7\nXLM-R+paraphrase 70.1 57.4 34.7 54.1\nmBERT+LaBSE 59.4 43.8 35.1 46.1\nLB\nmBERT+pooling 53.6 58.0 33.8 48.5\nmBERT+distiluse 62.8 63.8 34.6 53.7\nmBERT+paraphrase 72.9 67.6 36.8 59.1\nXLM-R+paraphrase 73.0 76.0 35.7 61.6\nmBERT+LaBSE 72.2 80.0 37.5 63.2\nTable 4: Accuracy with different models used in our\napproach. pooling: cosine similarity of the last hid-\nden states from the MPLM; distiluse: distiluse-base-\nmultilingual-cased-v2, sentence transformer of multilin-\ngual distilBERT; paraphrase: paraphrase-multilingual-\nmpnet-base-v2, sentence transformer of XLM-R. UN:\nunlabeled setup; LB: labeled setup.\nsource (retrieved) and target (input) language, pre-\ntraining data size of the source language and of the\ntarget language. We use the log value of Wikipedia\nsize to represent the size of pretraining corpus for\ntarget and source languages and sim(i, j) com-\nputed by Eq. (6) to represent the similarity between\nthe source and target language. Four other HRLs\n– Chinese, German, Hindi, Cebuano – are selected\nas source languages in addition to English. We\nmeasure the cross-lingual performance of PARC\non the Amazon product review task in both the la-\nbeled and the unlabeled settings. Full results can\nbe found in Appendix §D.2.\nTable 3 shows the outcome of the correlation\nanalysis. We observe a significant positive corre-\nlation between cross-lingual performance and lan-\nguage similarity as well as target language pretrain-\ning data size, in both the labeled and the unlabeled\nsetting. The correlation between performance and\nsource language size is not significant. Figure 3\nvisualizes the correlations and further clarifies the\nfindings by selecting 4 source languages and 4 tar-\nget languages and showing the cross-lingual perfor-\nmance and similarity between them.\nIg Sn Mt Co Sm\nDirect 30.3 32.1 29.8 32.6 30.4\nLB\nk=1 56.5 59.7 63.9 75.0 52.0\nk=3 58.1 61.4 65.2 78.2 54.1\nk=5 58.8 61.6 65.9 79.8 55.4\nUN\nk=1 36.6 37.3 39.1 42.6 34.4\nk=3 34.8 36.2 37.6 40.6 33.9\nk=5 34.8 35.3 37.2 40.4 34.1\nSt Haw Zu Ny Avg.\nDirect 30.4 27.1 34.4 29.8 30.8\nLB\nk=1 53.5 49.9 58.0 54.9 58.1\nk=3 55.5 49.7 58.5 57.0 59.7\nk=5 56.8 51.4 58.8 58.0 60.7\nUN\nk=1 36.3 31.6 35.6 35.3 36.5\nk=3 33.7 31.0 34.3 32.9 35.0\nk=5 34.2 30.6 34.0 32.0 34.7\nTable 5: Results of several unseen languages on a topic\ncategorization task (AG News dataset). Ig - Igbo, Sn -\nShona, Mt - Maltese, Co - Corsican, Sm - Samoan, St -\nSesotho, Haw - Hawaiian, Zu - Zulu, Ny - Chiechewa.\n6.3 Robustness\nIn this section, we test the robustness of the PARC\nmethod w.r.t. other cross-lingual retrievers and\nMPLMs as well as unseen languages.\n6.3.1 Retriever and MPLM\nApart from the multilingual sentence transformer\nbased on XLM-R (“paraphrase”) used in our pre-\nvious experiments, we explore several other types\nof cross-lingual retrievers: a “pooling” retriever\nwhich averages the last hidden states of the MPLM\nand computes the cosine similarity between these\npooled sentence representations; “distiluse” re-\ntriever, a sentence transformer based on multi-\nlingual distilBERT (Sanh et al., 2019); and the\n“LaBSE” retriever (Feng et al., 2020), a BERT-\nbased model trained for sentence embedding for\n109 languages. As an alternative to mBERT, we\nalso investigate the performance of XLM-R, which\nhas the same architecture as mBERT but is more\npowerful. We follow the setup described in §4.\nResults are shown in Table 4. We can find that\neven the worst combination – mBERT+pooling –\noutperforms the Direct baseline on average under\nboth labeled and unlabeled settings. If the retriever\nis replaced by a slightly more powerful one, such\nas the combination mBERT+distiluse, higher ac-\ncuracies in the unlabeled and labeled setting are\nachieved, suggesting that our proposed method\nPARC is robust w.r.t. other cross-lingual retriev-\ners. In the result of XLM-R+paraphrase, the ob-\nviously better performance of XLM-R in the un-\nlabeled setup shows that a stronger MPLM can\n8327\np1 p2 p3 p4 Avg\nen te en te en te en te en te\nFinetune\nDirect 84 76 83 70 86 67 85 73 85 74\nPARC-UN 84 – 65 ↓ 85↑ 62↓ 83↓ 60↓ 82↓ 64↓ 84↓ 67↓\nPARC-LB 83↓ 64↓ 83 – 64 ↓ 83↓ 64↓ 82↓ 70↓ 83↓ 69↓\nw/o Finetune\nDirect 54 53 59 54 54 50 53 51 55 52\nPARC-UN 59↑ 55↑ 55↓ 58↑ 52↓ 52↑ 53 – 52 ↑ 55 – 54 ↑\nPARC-LB 90↑ 82↑ 90↑ 82↑ 90↑ 82↑ 90↑ 82↑ 90↑ 82↑\nTable 6: Result of English and Telugu on Amazon review task using MPLMs with and without finetuning on English\ntrain set. UN: Unlabeled, LB: labeled. pi represents different prompt patterns.\nnoticeably improve the self-prediction. We expect\nthat an even better performance could be obtained\nby applying our proposed PARC approach to larger\nand/or more powerful MPLMs such as InfoXLM\n(Chi et al., 2021).\n6.3.2 Unseen Languages\nOur previous experiments show that the LRLs pre-\ntrained by MPLMs can benefit well from PARC.\nHowever, popular MPLMs are pretrained only on\napprox. 100 languages, accounting for a tiny part\nof all languages in the world ( ∼100/7000). We\nwonder if our proposed method could potentially\nbenefit a wider range of LRLs, so we apply PARC\nto several unseen LRLs, i.e. languages not included\nin the pretrained corpora of the MPLM. We con-\nduct experiments on a topic categorization task for\nnine unseen languages. The results in Table 5 show\nthat PARC is also effective for unseen LRLs. It\ncan be observed from the result that PARC is also\neffective for unseen LRL languages.\n6.4 Zero-shot Setting\nDifferent from the cross-lingual transfer paradigm\nwhere a MPLM is first finetuned on annotated train-\ning data of one language, and then directly applied\nto the test data of other languages for inference, our\nproposed approach is employed in the zero-shot set-\nting for LRLs, i.e., the model parameters are not ad-\njusted by finetuning with HRL data. Table 6 shows\nresults from a preliminary experiment where our\nPARC method combined with a finetuned MPLM\nfails to outperform the Direct baseline. When using\nfinetuned MPLM to evaluate with PARC, we do not\nsee sufficient performance improvement. However,\nwithout finetuning, PARC performs better in both\nunlabeled and labeled setup, and PARC-LB without\nfinetuning also outperforms it with finetuning.\n6.5 Qualitative Analysis\nTable 7 shows results of the PARC pipeline for an\nexample from the Amazon review task. The review\nAmazon Review\nCase #963\nInput:\n(Used with several loads of laundry. Gentle on the fabric\nand gentle on my skin.) pos\nRetrieved:\nR1: Hard to wash. The fur on top gets all over the sides in\nthe wash. :/ pos\nR2: Very nice and thick high quality towels. pos\nR3: Smelled really bad mold! I had to wash them before\nuse. neg\nPredictions: No retrieval -neg, k=1 - neg, k=3 -pos\nTable 7: A PARC pipeline example for Amazon review\ntask in the labeled setting.\nin Telugu is positive, but the class predicted without\ncross-lingual context is negative. The prediction\nstays the same when a single positive English sam-\nple is added as prompt context. When two more\nEnglish samples are added, the prediction becomes\ncorrect.\nThis case indicates that the retrieved cross-\nlingual samples help the MPLM make a correct\ndecision. Furthermore, more similar HRL samples\ncould rectify the deviation. More cases are shown\nin Table 10 and Table 11 in Appendix §C.\n7 Conclusion\nWe propose PARC, a pipeline that augments\nprompts for zero-shot learning on low resource\nlanguages by retrieving semantically similar cross-\nlingual sentences from HRL corpora. We test\nPARC on three classification tasks with parallel test\nsets across 10 LRLs, and it performs better than the\nbaselines in both unlabeled and labeled settings. In-\ncreasing the number of retrieved prompts improves\nperformance at first, but deteriorates it after a cer-\ntain point. A robustness study shows that PARC\nalso performs well with other cross-lingual retriev-\ners or MPLMs, suggesting potential applications\nof PARC to a wider scope of scenarios.\n8328\nLimitations\nThe PARC pipeline proposed in this work is de-\nsigned to improve the cross-lingual transfer perfor-\nmance for low-resource languages in a zero-shot\nsetting. We tested our method on different LRLs\ncontained in MPLMs and also investigate its effec-\ntiveness for several unseen languages. These are\nnot included in pretraining corpora of the MPLM\nbut use a seen script and share some subwords\nwith the seen languages. However, our proposed\nmethod is not applicable for unseen languages with\nnew scripts, which restricts its extension towards\na wider range of languages. Besides, PARC is a\nretrieval-based method. More time and computa-\ntional resources are required in the cross-lingual\nretrieval phase. Therefore, it is computationally\nless efficient to use PARC for inference.\nAcknowledgements\nThis work was supported by European Research\nCouncil (# 740516), Munich Center for Machine\nLearning (MCML) and China Scholarship Council\n(CSC).\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics,\n7:597–610.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder for english.\nIn Proceedings of the 2018 conference on empiri-\ncal methods in natural language processing: system\ndemonstrations, pages 169–174.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nJishnu Ray Chowdhury, Yong Zhuang, and Shuyi Wang.\n2022. Novelty controlled paraphrase generation with\nretrieval augmented conditional prompt tuning. In\nAAAI.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Conference on\nEmpirical Methods in Natural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Matthew Cer,\nN. Arivazhagan, and Wei Wang. 2020. Language-\nagnostic bert sentence embedding. In Annual Meet-\ning of the Association for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nHammarström Harald, Robert Forkel, Martin Haspel-\nmath, and Sebastian Bank. 2015. glottolog-data:\nGlottolog database 2.6.\n8329\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generalisa-\ntion. In International Conference on Machine Learn-\ning, pages 4411–4421. PMLR.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022. Zero-shot cross-\nlingual transfer of prompt-based tuning with a unified\nmultilingual prompt. ArXiv, abs/2202.11451.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual amazon reviews cor-\npus. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On\nthe limitations of zero-shot cross-lingual transfer\nwith multilingual transformers. arXiv preprint\narXiv:2005.00633.\nPatrick Littell, David R Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. Uriel\nand lang2vec: Representing languages as typological,\ngeographical, and phylogenetic vectors. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, volume 2, pages 8–14.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYanchen Liu, Timo Schick, and Hinrich Schütze. 2022b.\nSemantic-oriented unlabeled priming for large-scale\nlanguage models. arXiv preprint arXiv:2202.06133.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYongkang Liu, Shi Feng, Daling Wang, and Yifei\nZhang. 2022c. MulZDG: Multilingual code-\nswitching framework for zero-shot dialogue genera-\ntion. In Proceedings of the 29th International Confer-\nence on Computational Linguistics, pages 648–659,\nGyeongju, Republic of Korea. International Commit-\ntee on Computational Linguistics.\nChaitanya Malaviya, Graham Neubig, and Patrick Lit-\ntell. 2017. Learning language representations for\ntypology prediction. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\nCopenhagen, Denmark.\nChristopher D Manning, Prabhakar Raghavan, and Hin-\nrich Schütze. 2008. Introduction to information re-\ntrieval. Cambridge university press.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On mea-\nsuring social biases in sentence encoders. ArXiv,\nabs/1903.10561.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng\nWang. 2020. Rocketqa: An optimized training ap-\nproach to dense passage retrieval for open-domain\nquestion answering. In North American Chapter of\nthe Association for Computational Linguistics.\nTaraka Rama, Lisa Beinborn, and Steffen Eger. 2020.\nProbing multilingual bert for genetic and typological\nsignals. In International Conference on Computa-\ntional Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual. arXiv preprint arXiv:2204.07580.\nShuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu,\nSiqi Sun, Ruochen Xu, Chenguang Zhu, and Michael\nZeng. 2022. Training data is more valuable than you\nthink: A simple and effective method by retrieving\nfrom training data. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 3170–3179,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerald\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R3:\nReinforced ranker-reader for open-domain question\nanswering. In AAAI.\n8330\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung. 2021.\nLanguage models are few-shot multilingual learners.\nIn Proceedings of the 1st Workshop on Multilingual\nRepresentation Learning, pages 1–15, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore:\nEvaluating text generation with bert. ArXiv,\nabs/1904.09675.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. ArXiv, abs/1509.01626.\nMengjie Zhao and Hinrich Schütze. 2021. Discrete and\nsoft prompting for multilingual models. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8547–8555,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\n8331\nA Effect of Translations\nIn our experiment, we use multilingual parallel\ntest sets created by machine translation from En-\nglish to target low-resource languages. To explore\nthe effect of machine translation-created test sets,\nwe compare the cross-lingual transfer performance\non translation test sets and original test sets of\nXNLI. The original XNLI datasets include two low-\nresource languages that we used in our experiments,\ni.e., Swahili (sw) and Urdu (ur). We also measure\nthe translation quality by using the original sets as\ngold standard. The analysis results (Table 8) sug-\ngests that machine translated test sets are useful as\na proxy for evaluating cross-lingual performance\non LRLs.\nLanguages sw ur\nPerformance\nMT Acc. 34.00 33.92\nOV Acc. 34.07 33.87\nDiff 0.07 -0.05\nP-Value 0.85 0.92\nTranslation Quality\nBLEU 56.39 64.96\nchrF 49.58 59.89\nSim. 81.82 81.19\nTable 8: Comparison of performance on machine\ntranslation-created XNLI test sets (MT) and the original\nversion of XNLI test sets (OV) in sw and ur languages.\nBLEU & chrF scores and semantic similarities (Sim.)\nare computed to measure the translation quality of ma-\nchine translation-created test sets.\nB Language Features\nTable 9 shows the language features of all 10 LRLs\nevaluated in our experiments. Language similarity\nrefers to the similarity between each LRL and En-\nglish. SIM score is computed by Eq. (6). WikiSize\nis the log value of the Wikipedia size in MB.\nC Case Study\nTable 10 shows two examples from the Amazon\nReview task. We compare the predictions for three\nscenarios: no retrieval information (i.e., Direct\nbaseline, see §4.2), one retrieved sample, and three\nretrieved samples. Similarly, Table 11 shows the\nsame comparison on the AG News task.\nD Detailed Results\nD.1 Results for each task\nWe show the detailed experimental results for all\ntasks in Table 12 (Amazon reviews), Table 13 (AG\nNews) and Table 14 (XNLI), respectively.\nLang Language Similarity Wiki\nSizeSYN PHO INV FAM GEO SIM\nAf 84.9 60.3 38.4 50.4 33.1 53.4 6\nJv 48.0 39.2 52.7 0.0 0.0 28.0 5\nMn 31.0 100.0 39.4 0.0 56.8 45.4 5\nMy 17.4 80.3 100.0 0.0 37.6 47.1 5\nTa 28.9 60.3 51.5 0.0 72.7 42.7 7\nTe 36.0 56.2 31.3 0.0 45.2 33.7 7\nTl 35.0 70.5 26.7 0.0 38.8 34.2 6\nSw 27.0 87.0 62.1 0.0 57.2 46.6 5\nUr 50.2 72.0 47.1 12.6 62.5 48.9 7\nUz 39.8 75.6 24.1 0.0 73.7 42.6 6\nTable 9: List of language features of the 10 LRLs that\nwe evaluate.\nAmazon Review\nCase 1 #37\nInput:\n(Very dry on my hair.) neg\nRetrieved:\nR1: It’s a little bit too greasy in my opinion. Doesn’t really\nseem to soak into the hair very well. pos\nR2: The tiniest amount leaves my hair stringy and oily. neg\nR3: could smell this stuff all day but I don’t feel like it\nmoisturizes my skin enough, and my skin isn’t overly dry\nto begin with. pos\nPredictions: No retrieval -pos, k=1 - neg, k=3 -neg\nCase 2 #963\nInput:\n(Used with several loads of laundry. Gentle on the fabric\nand gentle on my skin.) pos\nRetrieved:\nR1: Hard to wash. The fur on top gets all over the sides in\nthe wash. :/ pos\nR2: Very nice and thick high quality towels. pos\nR3: Smelled really bad mold! I had to wash them before\nuse. neg\nPredictions: No retrieval -neg, k=1 - neg, k=3 -pos\nTable 10: PARC examples for Amazon Review task.\nD.2 Detailed data for Correlation Analysis\nTable 16 shows the detailed data used for corre-\nlation analysis of language similarity, high- and\nlow-resource language pretraining data size with\ncross-lingual performance in the unlabeled setting\nas well as labeled setting.\nD.3 Complete Results for Robustness Analysis\nTable 17 shows the results of each language using\ndifferent combinations of retriever and MPLM for\nvalidating the robustness on three tasks.\n8332\nAG News\nCase 1 #1939\nInput:\n(Flower Power A Japanese company has come up with a\nway to turn flowers into amplifiers. ) Tech\nRetrieved:\nR1: Japanese firms step up spending Japanese firms\ncontinue to spend on new equipment and production plants,\na survey finds, underlining a continuing recovery in the\nworld’s second-largest economy. Business\nR2: IBM, Honda deliver in-car speech-recognition\nnavigation system IBM and Honda have jointly developed\na hands-free and natural sounding in-vehicle speech-\nrecognition system that will be offered as standard equip-\nment on the 2005 Acura RL Tech\nR3: Scientists Make Phone That Turns Into a Sunflower\n(Reuters) Reuters - Scientists said on Monday they have\ncome up with a cell phone cover that will grow into a\nsunflower when thrown away. Tech\nPredictions: No retrieval -World, k=1 - Tech,\nk=3 -Tech\nCase 2 #1302\nInput:\n(Movies in a Snap: Netflix and TiV o Discuss Downloads\nBee Staff Writer. The high-tech terrain is shifting under-\nfoot amid rumblings of a new Silicon Valley alliance\nthat would allow the owners of TiV o Inc. ) Business\nRetrieved:\nR1: NETFLIX, TIVO HOOKUP CLOSE Netflix and\nTiV o are in late-stage talks on a partnership that would\nlet subscribers use the Internet to download Netflix\nmovies directly into their TiV o box, The Post has\nlearned. Business\nR2: TiV o and NetFlix: Picture-Perfect Duo? With TiV o\n(TIVO) and NetFlix (NFLX ) finally announcing a long-\nrumored partnership to launch a video-on-demand service\nsometime next year, investors smiled on the deal that will\nkeep the two popular, but under-fire, innovators ahead of\ncompetitors. Tech\nR3: New Treo and more unveiled at CTIA CTIA stands\nfor the Cellular Telecommunications and Internet\nAssociation. Each year they host two shows for the\nindustry. This week is their fall Wireless IT and Enter-\ntainment expo in San Francisco. Business\nPredictions: No retrieval -World, k=1 - Tech,\nk=3 -Business\nTable 11: PARC examples for AG News task\n8333\npattern 0 [X] [MASK]\npattern 1 It was [MASK]. [X]\npattern 2 [X] All in all, it was [MASK].\npattern 3 Just [MASK]! [X]\npattern 4 [X] In summary, the product is [MASK].\nen af ur\np0 p1 p2 p3 p4 p0 p1 p2 p3 p4 p0 p1 p2 p3 p4\nMAJ 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nDirect 50.5 54.3 58.9 53.7 52.6 53.3 50.7 50.4 49.8 51.5 49.9 51.7 54.6 49.9 50.3\nUnlabeled\nk=1 50.9 55.4 59.1 51.9 52.6 51.0 54.9 57.9 52.9 52.8 51.6 56.7 60.0 52.2 52.2\nk=3 50.7 53.7 57.7 50.8 50.4 50.4 52.5 56.2 50.7 51.0 51.3 52.9 57.1 50.8 50.9\nk=5 50.8 52.2 56.0 50.3 50.9 50.8 52.2 55.0 50.2 50.6 51.2 52.5 56.4 50.3 50.7\nk=10 50.7 51.9 56.0 50.0 50.6 50.7 52.0 55.8 50.2 50.7 51.4 52.4 55.5 50.0 50.3\nk=20 50.5 50.8 53.6 49.9 50.1 50.5 51.1 53.5 50.0 50.2 51.1 51.2 54.0 49.8 50.0\nlabeled\nk=1 60.0 82.4 82.4 82.3 82.4 66.0 79.0 79.2 79.2 79.2 57.0 80.4 80.6 80.6 80.6\nk=3 58.5 86.2 86.2 86.2 86.2 65.0 80.7 81.1 81.1 81.0 56.4 83.8 84.3 84.3 84.3\nk=5 57.3 87.2 87.2 87.2 87.2 65.4 82.7 82.9 82.9 82.8 56.2 84.6 85.0 85.0 85.0\nk=10 57.7 88.9 88.9 88.9 88.9 66.5 85.2 85.4 85.4 85.4 56.6 87.0 87.3 87.3 87.3\nk=20 56.4 89.5 89.5 89.5 89.5 64.3 85.3 85.7 85.7 85.6 55.4 87.6 87.9 87.9 88.0\nk=30 56.3 88.9 88.9 88.9 88.9 63.6 85.4 85.6 85.6 85.6 55.7 87.4 87.6 87.6 87.6\nsw te ta\np0 p1 p2 p3 p4 p0 p1 p2 p3 p4 p0 p1 p2 p3 p4\nMAJ 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nDirect 47.3 50.2 51.9 49.9 50.3 50.8 52.5 53.9 49.9 51.4 54.1 59.0 56.2 50.5 51.9\nUnlabeled\nk=1 51.4 50.4 50.5 50.5 50.1 51.6 54.8 57.5 52.3 52.1 57.1 55.3 57.2 52.6 51.6\nk=3 50.5 50.3 50.3 50.1 50.1 51.3 52.8 55.3 50.6 51.3 55.7 52.5 55.0 50.5 50.6\nk=5 50.6 50.1 50.0 50.1 50.1 51.6 51.7 54.0 50.4 50.3 56.1 51.4 54.0 50.1 50.1\nk=10 50.8 50.1 50.0 50.1 50.1 51.8 52.1 53.5 50.4 50.3 57.3 51.5 53.9 50.0 50.1\nk=20 50.5 50.1 50.0 50.1 50.1 51.4 50.6 52.9 50.0 50.0 56.9 50.5 52.9 50.0 50.0\nlabeled\nk=1 50.5 50.0 49.9 49.9 49.9 58.2 75.9 75.8 75.8 75.8 68.1 75.3 75.4 75.4 75.4\nk=3 51.0 54.1 54.1 54.1 54.1 58.0 78.4 78.4 78.4 78.4 70.2 79.1 79.3 79.3 79.2\nk=5 50.7 54.4 54.4 54.4 54.4 56.8 79.1 79.0 79.0 79.1 70.7 80.5 80.5 80.5 80.5\nk=10 51.3 55.5 55.5 55.5 55.5 57.2 81.3 81.6 81.6 81.6 70.9 83.7 83.9 83.9 83.9\nk=20 50.9 54.3 54.4 54.4 54.4 56.9 82.0 82.1 82.1 82.1 70.8 82.8 83.1 83.1 83.1\nk=30 50.7 54.3 54.3 54.3 54.3 56.8 82.0 82.0 82.0 82.0 70.5 83.3 83.5 83.4 83.4\nmn uz my\np0 p1 p2 p3 p4 p0 p1 p2 p3 p4 p0 p1 p2 p3 p4\nMAJ 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nDirect 49.1 49.7 51.4 49.7 50.0 48.5 50.2 52.4 49.7 51.2 54.4 56.1 56.1 50.5 52.6\nUnlabeled\nk=1 51.1 54.7 58.6 52.6 52.8 50.4 53.1 53.6 51.8 50.9 53.0 53.9 56.0 52.3 52.0\nk=3 50.2 53.2 56.4 51.0 51.1 50.5 51.9 52.1 50.2 50.3 53.0 51.5 55.0 51.2 50.7\nk=5 50.2 52.0 55.3 50.4 50.5 50.5 50.3 50.7 50.0 50.2 52.9 51.1 53.6 50.5 50.3\nk=10 50.4 52.2 56.3 50.6 50.5 50.6 50.3 50.6 50.1 50.0 53.4 51.1 54.2 50.2 50.1\nk=20 50.4 51.1 54.5 50.0 50.0 50.5 50.0 50.7 50.0 50.0 53.2 50.5 52.8 50.0 50.0\nlabeled\nk=1 60.8 74.9 74.9 74.9 74.9 56.0 65.0 64.7 64.7 64.7 65.3 73.9 73.8 73.8 73.8\nk=3 60.3 79.5 79.7 79.7 79.7 55.2 65.3 65.2 65.2 65.2 66.6 77.5 77.7 77.7 77.7\nk=5 59.7 80.6 80.6 80.6 80.6 55.5 66.1 66.0 66.0 65.8 65.8 78.6 78.9 78.9 78.9\nk=10 62.2 83.9 84.3 84.3 84.3 55.9 68.1 68.2 68.2 68.3 67.8 80.9 81.1 81.1 81.1\nk=20 60.3 82.5 83.2 83.2 83.2 53.8 67.0 67.1 67.1 67.1 67.4 81.8 81.8 81.8 81.8\nk=30 59.7 83.3 83.8 83.8 83.8 54.4 67.5 67.7 67.7 67.7 67.6 81.7 81.8 81.8 81.8\njv tl Avg.\np0 p1 p2 p3 p4 p0 p1 p2 p3 p4 p0 p1 p2 p3 p4\nMAJ 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\nDirect 50.9 52.3 54.1 50.1 52.3 49.6 50.4 51.9 50.0 51.2 50.8 52.5 53.8 50.3 51.4\nUnlabeled\nk=1 50.6 53.0 54.2 50.9 50.5 50.4 50.6 50.9 50.1 50.2 51.7 53.9 56.0 51.8 51.6\nk=3 50.2 51.7 53.5 50.4 50.3 50.0 50.3 50.3 50.2 50.0 51.2 52.1 54.4 50.6 50.6\nk=5 50.2 50.9 52.9 50.1 50.2 50.1 50.2 50.1 50.0 50.1 51.4 51.3 53.5 50.2 50.4\nk=10 50.1 50.7 52.5 49.9 50.0 50.2 50.0 50.3 50.0 50.0 51.6 51.3 53.5 50.1 50.2\nk=20 50.5 50.1 51.7 50.0 50.0 50.2 50.0 50.4 50.0 50.0 51.4 50.5 52.5 50.0 50.0\nlabeled\nk=1 54.1 59.3 59.3 59.3 59.3 52.4 55.4 55.4 55.4 55.4 58.9 70.1 68.9 70.1 70.1\nk=3 52.7 61.6 61.6 61.6 61.6 52.1 57.7 57.7 57.7 57.7 58.7 73.1 73.2 73.2 73.2\nk=5 52.8 61.5 61.5 61.5 61.5 51.6 60.2 60.2 60.2 60.1 58.4 74.1 74.2 74.2 74.2\nk=10 51.6 62.6 62.6 62.6 62.6 52.4 63.2 63.3 63.3 63.3 59.1 76.4 76.5 76.5 76.5\nk=20 51.6 61.5 61.5 61.5 61.5 51.5 62.8 62.9 62.9 62.9 58.1 76.1 76.3 76.3 76.3\nk=30 51.6 60.9 61.0 61.0 61.0 51.5 62.3 62.4 62.4 62.4 58.0 76.1 76.2 76.2 76.2\nTable 12: Results on Amazon reviews dataset.\n8334\npattern 0 [X] [MASK]\npattern 1 [MASK]: [X]\npattern 2 [MASK] News: [X]\npattern 3 [X] Category: [MASK]\nen af ur\np0 p1 p2 p3 p0 p1 p2 p3 p0 p1 p2 p3\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 52.5 47.8 47.3 53.0 41.8 41.3 40.2 57.8 27.4 32.4 33.0 53.5\nUnlabeled\nk=1 53.7 47.6 45.6 53.2 52.8 46.8 46.2 53.2 46.2 41.8 41.0 49.7\nk=3 55.8 47.6 43.4 54.3 53.6 46.5 44.3 54.3 46.2 40.5 38.2 49.9\nk=5 57.1 48.3 41.7 55.6 54.4 46.9 43.7 55.1 47.0 40.9 37.2 51.4\nk=10 57.5 45.7 41.9 55.3 55.3 44.6 42.3 55.6 46.3 38.3 35.3 51.9\nk=20 59.7 46.7 41.5 55.3 57.2 45.9 42.2 56.1 48.1 39.7 35.5 51.6\nlabeled\nk=1 74.9 83.5 83.8 83.8 75.4 81.2 82.9 82.7 68.1 76.9 78.8 78.7\nk=3 77.1 86.5 86.8 86.7 77.1 84.3 85.4 85.2 69.6 79.4 81.7 81.8\nk=5 78.1 87.7 88.0 87.9 78.6 86.8 87.1 87.1 69.0 79.9 82.7 82.7\nk=10 78.7 88.2 88.5 88.5 79.4 87.2 87.7 87.5 70.5 81.5 83.6 83.4\nk=20 79.0 89.1 89.4 89.4 79.7 87.4 87.8 87.5 70.7 81.6 83.3 83.2\nsw te ta\np0 p1 p2 p3 p0 p1 p2 p3 p0 p1 p2 p3\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 42.5 37.6 33.3 56.6 32.2 37.2 32.5 55.4 31.3 37.2 28.6 55.1\nUnlabeled\nk=1 46.5 42.1 42.0 46.4 46.1 41.5 43.3 48.6 42.8 41.6 39.2 47.6\nk=3 47.1 41.2 39.9 47.9 48.2 40.0 42.4 50.3 44.9 41.0 36.9 50.1\nk=5 47.0 41.5 39.3 48.6 48.0 40.4 41.0 52.4 46.6 39.8 36.0 50.9\nk=10 46.4 38.5 37.0 50.0 47.6 39.0 39.3 51.8 45.6 37.8 33.9 51.5\nk=20 46.7 39.1 36.9 49.9 50.0 40.1 39.7 51.6 47.9 38.8 34.7 52.5\nlabeled\nk=1 63.5 68.4 70.3 70.3 68.2 73.9 75.0 75.0 64.0 69.7 71.5 71.5\nk=3 65.6 70.8 72.3 72.4 71.1 77.6 78.2 78.2 67.6 74.4 75.7 75.7\nk=5 64.4 72.2 73.5 73.4 72.9 79.7 79.9 79.8 68.8 75.8 76.6 76.5\nk=10 67.0 72.5 74.1 73.9 72.9 79.9 80.0 80.0 68.3 76.5 77.2 77.1\nk=20 67.5 72.7 73.6 73.6 72.5 80.2 80.6 80.6 70.0 77.5 78.1 78.2\nmn uz my\np0 p1 p2 p3 p0 p1 p2 p3 p0 p1 p2 p3\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 31.5 30.9 32.0 47.3 33.0 37.5 33.8 50.7 31.6 37.4 33.7 51.9\nUnlabeled\nk=1 43.3 42.5 41.5 48.2 44.3 44.4 42.3 49.0 45.0 43.9 43.6 50.0\nk=3 44.5 41.2 40.5 51.1 46.3 42.2 40.7 50.9 47.1 44.5 41.7 53.7\nk=5 44.8 41.5 39.6 51.8 45.8 41.7 39.2 52.3 48.5 43.8 41.4 54.2\nk=10 44.1 39.7 38.0 53.3 46.7 39.7 37.9 53.4 47.7 41.4 40.0 54.4\nk=20 46.0 39.7 37.9 52.8 48.9 41.2 36.9 53.1 49.6 42.2 40.3 53.6\nlabeled\nk=1 62.8 70.9 72.7 72.8 65.6 71.5 73.2 73.3 64.8 76.2 77.4 77.2\nk=3 65.6 75.4 77.3 77.2 68.4 73.6 75.7 75.7 65.9 79.5 80.1 79.8\nk=5 65.9 75.8 78.0 77.9 69.3 76.1 77.9 77.8 66.4 81.4 82.5 81.8\nk=10 66.6 77.0 78.7 78.6 70.7 76.4 78.3 78.2 67.2 82.4 82.9 82.3\nk=20 67.5 77.4 78.2 78.0 70.7 77.3 78.8 78.7 68.1 83.1 83.6 83.3\njv tl Avg\np0 p1 p2 p3 p0 p1 p2 p3 p0 p1 p2 p3\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 46.9 39.3 38.0 59.3 44.8 44.4 42.6 60.4 37.8 38.4 36.2 50.9\nUnlabeled\nk=1 51.0 45.5 45.4 51.6 49.7 45.8 43.7 52.2 47.4 44.2 43.5 48.9\nk=3 52.6 44.6 42.0 53.5 51.0 45.3 42.7 54.0 48.8 43.6 41.9 50.3\nk=5 53.1 44.5 41.3 53.6 52.3 45.2 41.8 54.2 49.5 43.7 41.2 51.0\nk=10 53.0 42.4 39.9 54.0 51.4 44.0 39.8 54.9 49.2 41.7 39.7 51.2\nk=20 55.4 42.8 40.1 54.2 53.2 44.4 38.9 55.3 51.1 42.6 39.9 51.4\nlabeled\nk=1 72.5 77.8 79.1 79.1 71.4 76.6 78.9 79.0 68.3 74.6 75.9 75.9\nk=3 74.6 80.5 82.3 82.3 74.4 80.7 82.1 82.2 70.6 77.8 78.9 78.9\nk=5 75.8 81.3 82.8 82.8 75.4 81.2 83.4 83.5 71.3 79.1 80.2 80.1\nk=10 76.6 82.0 84.0 84.2 75.9 82.4 84.5 84.6 72.1 79.8 80.9 80.8\nk=20 77.4 82.8 84.6 84.8 76.3 82.8 84.0 84.0 72.6 80.4 81.1 81.1\nTable 13: Results on AG News dataset.\n8335\npattern 0 [X1] [MASK] [X2]\npattern 1 [X1]? [MASK], [X2] (Yes - No)\npattern 2 [X1]? [MASK], [X2] (Right - Wrong)\nen af ur sw\np0 p1 p2 p0 p1 p2 p0 p1 p2 p0 p1 p2\nMAJ 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3\nDirect 33.3 34.2 34.3 33.2 33.0 33.4 33.6 34.0 33.2 33.2 32.2 33.1\nUnlabeled\nk=1 34.1 33.7 34.5 34.0 34.1 33.7 32.4 35.3 32.7 33.5 33.7 33.7\nk=3 33.7 34.1 34.3 33.0 32.9 34.1 33.3 34.0 33.9 33.6 33.0 33.5\nk=5 31.9 33.7 34.3 32.5 32.8 33.9 31.2 34.1 33.6 33.2 32.7 32.9\nk=10 31.9 33.6 33.3 31.9 33.3 32.6 32.2 34.2 33.2 33.0 32.7 32.5\nk=20 32.0 34.4 33.3 31.6 33.6 34.1 31.6 34.4 33.9 33.1 33.1 32.0\nlabeled\nk=1 38.9 39.1 38.8 38.7 38.9 38.1 37.0 37.4 36.7 33.3 33.4 33.4\nk=3 39.2 39.1 38.6 37.9 37.9 37.4 37.0 37.8 36.8 33.7 33.5 33.7\nk=5 40.0 39.8 39.5 38.0 38.0 37.1 40.2 40.6 39.8 32.7 32.5 32.6\nk=10 41.5 41.6 40.9 41.1 41.1 40.5 42.0 42.4 41.0 33.7 33.7 34.1\nk=20 44.5 44.1 43.5 42.3 43.0 41.3 42.4 43.4 42.2 35.9 35.7 35.9\nte ta mn uz\np0 p1 p2 p0 p1 p2 p0 p1 p2 p0 p1 p2\nMAJ 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3\nDirect 31.9 33.0 33.2 32.4 34.1 32.9 33.0 32.7 32.6 33.3 33.3 32.9\nUnlabeled\nk=1 34.1 34.1 34.1 34.5 34.3 33.3 32.8 33.6 34.7 33.2 33.9 32.8\nk=3 32.8 34.9 33.4 33.7 34.7 34.2 32.2 34.5 33.7 32.3 34.5 33.4\nk=5 32.9 35.1 33.8 32.9 34.3 33.9 31.9 33.9 34.1 33.1 34.5 33.9\nk=10 32.0 34.1 32.7 32.3 34.7 32.5 30.8 34.1 32.5 32.8 33.9 32.6\nk=20 31.5 34.6 32.7 32.5 34.8 32.9 32.0 34.1 33.4 32.6 33.5 32.6\nlabeled\nk=1 37.8 38.1 37.7 37.7 38.0 37.0 36.5 36.5 36.5 35.5 34.8 35.0\nk=3 38.9 39.5 38.4 38.7 39.4 37.5 39.1 39.1 38.9 35.1 34.7 34.7\nk=5 37.5 37.1 35.9 38.3 38.7 36.3 37.1 36.9 36.9 36.0 35.9 35.9\nk=10 39.2 39.5 37.9 41.1 40.8 38.0 39.5 39.3 39.3 38.3 37.9 37.8\nk=20 41.2 41.5 39.3 42.7 43.1 39.7 40.3 40.2 40.0 40.0 39.9 39.6\nmy jv tl Avg\np0 p1 p2 p0 p1 p2 p0 p1 p2 p0 p1 p2\nMAJ 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3 33.3\nDirect 33.7 33.6 33.7 33.3 33.3 33.6 33.3 33.5 32.3 33.1 33.3 33.1\nUnlabeled\nk=1 33.3 33.5 33.8 32.4 32.0 33.3 33.8 32.7 32.8 33.4 33.7 33.5\nk=3 32.6 33.9 33.7 32.1 31.4 34.2 33.7 33.9 33.3 32.9 33.7 33.7\nk=5 32.5 34.3 33.6 32.4 31.6 34.3 34.1 33.5 32.1 32.7 33.6 33.6\nk=10 30.5 33.9 33.3 32.1 32.6 33.5 33.2 33.1 32.6 32.1 33.5 32.8\nk=20 30.9 33.5 32.7 30.8 33.6 34.7 32.9 32.5 33.1 32.0 33.6 33.2\nlabeled\nk=1 36.8 36.7 36.1 34.2 33.5 33.3 34.7 34.4 34.3 36.2 36.2 35.8\nk=3 36.7 36.9 36.2 34.6 33.9 33.9 35.7 35.7 35.7 36.7 36.8 36.3\nk=5 37.7 37.7 37.3 35.2 34.8 34.6 35.7 35.7 35.3 36.9 36.8 36.2\nk=10 39.5 39.3 38.1 34.7 34.4 33.6 37.2 36.9 36.9 38.6 38.5 37.7\nk=20 41.7 41.3 39.6 32.8 32.8 32.4 37.4 37.0 37.0 39.7 39.8 38.7\nTable 14: Results on XNLI dataset.\nEn Af Jv Mn My Sw Ta Te Tl Ur Uz Avg\nMAJ 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0 25.0\nDirect 52.5 41.8 27.4 42.5 32.2 31.3 31.5 33.0 31.6 46.9 44.8 36.3\nUN\nk=1 53.7 52.8 46.2 46.5 46.1 42.8 43.3 44.3 45.0 51.0 49.7 46.7\nk=3 BoR 55.8 53.6 46.2 47.1 48.2 44.9 44.5 46.3 47.1 52.6 51.0 48.1\nCONC 53.5 52.4 45.9 44.9 44.8 42.9 41.7 46.6 46.0 52.0 51.6 46.9\nk=5 BoR 57.1 54.4 47.0 47.0 48.0 46.6 44.8 45.8 48.5 53.1 52.3 48.7\nCONC 53.5 48.0 38.2 41.3 36.3 36.9 39.5 41.4 42.9 50.5 49.6 42.4\nk=10 BoR 57.5 55.3 46.3 46.4 47.6 45.6 44.1 46.7 47.7 53.0 51.4 48.4\nCONC 46.4 41.1 36.2 38.3 36.6 34.9 34.6 35.8 40.7 46.3 45.0 38.9\nk=20 BoR 59.7 57.2 48.1 46.7 50.0 47.9 46.0 48.9 49.6 55.4 53.2 50.3\nCONC 50.0 48.4 42.3 41.4 43.3 43.1 39.3 44.3 48.1 47.9 48.4 44.6\nk=30 BoR 60.1 57.4 49.0 47.4 51.1 49.2 47.1 48.7 50.1 56.5 54.4 51.1\nCONC 50.7 47.6 43.9 38.2 42.9 42.5 41.8 44.5 47.7 47.1 47.3 44.3\nLB\nk=1 74.9 75.4 68.1 63.5 68.2 64.0 62.8 65.6 64.8 72.5 71.4 67.6\nk=3 BoR 77.1 77.1 69.6 65.6 71.1 67.6 65.6 68.4 65.9 74.6 74.4 70.0\nCONC 75.6 74.8 67.3 63.1 60.3 59.0 60.5 67.1 65.9 73.3 72.4 66.4\nk=5 BoR 78.1 78.6 69.0 64.4 72.9 68.8 65.9 69.3 66.4 75.8 75.4 70.6\nCONC 74.6 66.5 48.2 53.9 44.9 45.4 52.1 59.5 56.0 70.9 63.6 56.1\nk=10 BoR 78.7 79.4 70.5 67.0 72.9 68.3 66.6 70.7 67.2 76.6 75.9 71.5\nCONC 61.2 52.7 43.2 48.0 44.5 42.5 41.3 45.0 50.1 62.3 56.7 48.6\nk=20 BoR 79.0 79.7 70.7 67.5 72.5 70.0 67.5 70.7 68.1 77.4 76.3 72.0\nCONC 67.4 65.1 55.8 55.6 57.6 58.3 51.2 61.0 62.8 66.4 66.0 60.0\nk=30 BoR 79.0 79.7 71.3 67.6 72.8 69.9 68.1 71.1 69.4 77.2 76.7 72.4\nCONC 72.8 71.1 62.1 57.0 61.6 60.4 57.9 67.9 64.6 71.6 69.3 64.3\nTable 15: Results of topic categorization task on AG News Dataset. k is the number of retrieved cross-lingual\nsample. MAJ is the majority baseline. Avg is the average accuracy across 10 LRLs. En is the HRL for retrieval.\nBoR refers to the Bag of Retrievalstrategy, CONC refers to the Concatenation strategy.\n8336\nPerformance Language Similarity WikiSize\nUnlabeled labeled SYN PHO INV FAM GEO SIM source target\nen-af 79.2 62.0 84.9 60.3 38.4 50.4 33.1 53.4 14 6\nen-ur 80.6 63.4 50.2 72.0 47.1 12.6 62.5 48.9 14 7\nen-sw 49.9 51.0 27.0 87.0 62.1 0.0 57.2 46.6 14 5\nen-te 75.8 60.1 36.0 56.2 31.3 0.0 45.2 33.7 14 7\nen-ta 75.4 60.2 28.9 60.3 51.5 0.0 72.7 42.7 14 7\nen-mn 74.9 62.9 31.0 100.0 39.4 0.0 56.8 45.4 14 5\nen-uz 64.7 54.9 39.8 75.6 24.1 0.0 73.7 42.6 14 6\nen-my 73.8 60.3 17.4 80.3 100.0 0.0 37.6 47.1 14 5\nen-jv 59.3 55.3 48.0 39.2 52.7 0.0 0.0 28.0 14 5\nen-tl 55.4 53.5 35.0 70.5 26.7 0.0 38.8 34.2 14 6\nde-af 71.6 56.5 87.1 33.1 90.3 77.2 43.1 66.2 12 6\nde-ur 77.5 58.5 50.7 68.3 45.8 15.4 72.6 50.6 12 7\nde-sw 50.6 48.9 29.5 33.1 36.2 0.0 66.7 33.1 12 5\nde-te 71.2 55.7 45.6 29.4 5.2 0.0 56.5 27.3 12 7\nde-ta 76.3 57.6 43.0 56.7 48.7 0.0 81.3 45.9 12 7\nde-mn 74.7 59.1 44.4 68.3 42.8 0.0 61.8 43.4 12 5\nde-uz 62.8 55.1 48.3 91.9 27.8 0.0 81.1 49.8 12 6\nde-my 72.0 59.3 31.3 29.9 63.9 0.0 47.5 34.5 12 5\nde-jv 60.0 50.9 41.5 14.4 32.5 0.0 10.3 19.8 12 5\nde-tl 54.5 52.1 48.1 42.1 0.0 0.0 50.8 28.2 12 6\nzh-af 70.4 58.6 53.9 9.5 25.2 0.0 12.1 20.1 11 6\nzh-ur 75.1 62.8 59.0 43.5 36.3 0.0 82.6 44.3 11 7\nzh-sw 53.9 51.5 5.7 33.1 27.0 0.0 27.6 18.7 11 5\nzh-te 72.4 60.3 49.9 29.4 4.5 0.0 86.7 34.1 11 7\nzh-ta 73.0 61.8 19.0 56.7 16.8 0.0 40.5 26.6 11 7\nzh-mn 71.6 60.4 56.5 43.5 8.7 0.0 99.0 41.5 11 5\nzh-uz 62.5 54.9 49.0 69.3 26.2 0.0 87.2 46.3 11 6\nzh-my 69.6 59.3 42.5 71.8 32.7 37.8 95.7 56.1 11 5\nzh-jv 59.8 54.3 41.1 42.1 31.4 0.0 85.1 39.9 11 5\nzh-tl 54.7 52.4 44.7 14.4 6.9 0.0 83.4 29.9 11 6\nhi-af 78.2 59.0 55.4 50.1 30.8 14.3 52.3 40.6 7 6\nhi-ur 80.0 57.8 100.0 88.1 73.0 100.0 99.9 92.2 7 7\nhi-sw 50.7 50.5 27.4 24.6 24.9 0.0 66.9 28.8 7 5\nhi-te 72.7 58.4 74.7 74.4 67.2 0.0 100.0 63.3 7 7\nhi-ta 74.2 57.0 48.9 50.1 36.8 0.0 75.8 42.3 7 7\nhi-mn 74.6 57.7 57.9 61.3 31.2 0.0 89.4 48.0 7 5\nhi-uz 64.0 50.8 57.8 64.8 45.6 0.0 97.2 53.1 7 6\nhi-my 74.3 58.7 36.7 46.7 37.5 0.0 97.6 43.7 7 5\nhi-jv 59.4 48.7 21.2 0.0 13.6 0.0 79.6 22.9 7 5\nhi-tl 56.6 52.9 73.1 59.8 41.3 0.0 98.2 54.5 7 6\nceb-af 63.9 58.1 42.4 44.1 52.5 0.0 8.9 29.6 11 6\nceb-ur 68.7 57.1 29.3 84.3 22.5 0.0 62.9 39.8 11 7\nceb-sw 53.4 49.2 33.0 16.1 76.3 0.0 12.0 27.5 11 5\nceb-te 69.3 59.0 4.8 98.6 17.9 0.0 75.9 39.4 11 7\nceb-ta 66.3 55.8 22.4 72.1 63.0 0.0 16.6 34.8 11 7\nceb-mn 65.9 59.7 16.5 55.0 37.6 0.0 79.3 37.7 11 5\nceb-uz 56.2 52.6 26.2 61.3 17.9 0.0 60.6 33.2 11 6\nceb-my 64.8 56.3 3.0 43.5 57.7 0.0 88.1 38.4 11 5\nceb-jv 57.1 51.2 60.2 17.1 70.0 54.8 97.6 59.9 11 5\nceb-tl 53.0 56.2 0.0 82.7 50.0 0.0 76.2 41.8 11 6\nTable 16: Detailed data of 50 source-target language pairs used for correlation analysis of language similarity,\nsource and target language pretraining data size with cross-lingual performance in unlabeled and labeled setup. Task\nperformance is measured on Amazon review task with k = 1.\n8337\nAmazon Review\nen af ur sw te ta mn uz my jv tl Avg\nUN\nmBERT+pooling 57.8 54.4 54.9 52.4 53.5 54.8 51.1 49.3 52.4 56.1 52.1 53.1\nmBERT+distiluse 63.1 60.1 61.0 46.1 50.1 50.0 59.9 55.2 56.7 57.2 50.1 54.7\nmBERT+paraphrase 69.3 63.8 67.1 51.4 62.2 61.4 61.1 56.6 62.9 55.6 54.0 59.6\nXLM-R+paraphrase 69.2 75.4 80.8 64.1 71.0 70.4 69.7 68.2 70.4 63.8 66.6 70.1\nLB\nmBERT+pooling 65.6 56.8 57.0 51.8 53.8 53.1 52.7 51.2 52.5 53.5 53.2 53.6\nmBERT+distiluse 80.4 76.0 80.0 51.2 48.9 50.0 77.9 57.7 70.7 60.5 55.4 62.8\nmBERT+paraphrase 87.2 82.9 85.0 54.4 79.0 80.5 80.6 66.0 78.9 61.5 60.2 72.9\nXLM-R+paraphrase 77.6 81.7 82.2 64.0 74.2 73.9 75.1 70.6 76.4 66.3 66.1 73.0\nAG News\nen af ur sw te ta mn uz my jv tl Avg\nUN\nmBERT+pooling 37.9 37.3 34.8 37.7 32.9 38.0 36.0 33.7 37.4 42.0 38.8 36.9\nmBERT+distiluse 43.3 43.5 38.8 40.6 25.4 29.1 39.7 39.6 42.7 42.0 42.9 38.4\nmBERT+paraphrase 53.7 52.8 46.2 46.5 46.1 42.8 43.3 44.3 45.0 51.0 49.7 46.7\nXLM-R+paraphrase 62.7 61.9 58.9 52.2 58.1 55.8 55.6 56.0 58.6 59.2 58.4 57.4\nLB\nmBERT+pooling 77.4 68.2 55.4 58.5 54.7 52.1 50.7 54.6 49.0 66.7 70.2 58.0\nmBERT+distiluse 85.1 82.0 76.0 65.5 25.3 28.7 70.8 64.4 71.3 77.8 76.5 63.8\nmBERT+paraphrase 74.9 75.4 68.1 63.5 68.2 64.0 62.8 65.6 64.8 72.5 71.4 67.6\nXLM-R+paraphrase 83.8 82.9 78.8 70.4 75.1 71.7 72.7 73.2 77.4 79.2 79.0 76.0\nXNLI\nen af ur sw te ta mn uz my jv tl Avg\nUN\nmBERT+pooling 34.7 34.3 34.4 33.2 33.9 33.5 34.3 33.3 33.3 32.9 32.7 33.6\nmBERT+distiluse 32.9 32.6 33.4 33.2 36.1 36.1 33.8 34.6 31.9 34.0 34.1 34.0\nmBERT+paraphrase 34.1 32.9 34.0 33.0 34.9 34.7 34.5 34.5 33.9 31.4 33.9 33.7\nXLM-R+paraphrase 35.5 33.7 34.0 32.3 35.0 36.5 38.1 34.7 35.1 33.5 34.1 34.7\nLB\nmBERT+pooling 35.5 34.1 34.0 35.3 33.3 34.1 35.7 32.8 33.1 33.5 32.3 33.8\nmBERT+distiluse 34.5 35.6 33.6 35.1 31.3 31.4 38.5 35.6 34.8 35.7 34.3 34.6\nmBERT+paraphrase 39.1 37.9 37.8 33.5 39.5 39.4 39.1 34.7 36.9 33.9 35.7 36.8\nXLM-R+paraphrase 36.8 35.7 35.0 32.8 37.5 37.5 37.3 36.7 37.5 32.8 33.9 35.7\nTable 17: Results of all languages using different combinations of retriever and MPLM for robustness analysis on\nAmazon review task (k = 5), AG News tasks (k = 1), and XNLI task (k = 3), respectively.\nTask Dataset Size #Label Languages\nSentiment Analysis Amazon Reviews 1000 2 af, ur, jv,\nTopic Categorization AG News 2000 4 ta, mn, uz,\nSentence Pair Classification XNLI 1500 3 tl, te, mn, sw\nTable 18: Overview of the test sets for the three tasks. Size refers to the number of samples for each LRL.\n8338\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe section after the conclustion and before the references.\n□\u0013 A2. Did you discuss any potential risks of your work?\nIn the limitation section.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and section 1 Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nNo AI writing assistants were used.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nModels used are introduced in Section 4.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n8339\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nExperimental setup is discussed in Section 4. Hyperparameter search is not applicable.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5 Results\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n8340"
}