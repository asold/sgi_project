{
  "title": "Transformer-Based Selective Super-resolution for Efficient Image Refinement",
  "url": "https://openalex.org/W4393147907",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2116085444",
      "name": "Tianyi Zhang",
      "affiliations": [
        "University of Minnesota System"
      ]
    },
    {
      "id": "https://openalex.org/A2229219563",
      "name": "Kishore Kasichainula",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2789561310",
      "name": "Yaoxin Zhuo",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2128754310",
      "name": "Baoxin Li",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A4202045199",
      "name": "Jae-sun Seo",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2098211329",
      "name": "Yu Cao",
      "affiliations": [
        "University of Minnesota System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4280534023",
    "https://openalex.org/W6660241312",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W54257720",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W4221166843",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W2735224642",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6797410494",
    "https://openalex.org/W3076169370",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W2747898905",
    "https://openalex.org/W3184190059",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W3016101116",
    "https://openalex.org/W2967400231",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4388481418",
    "https://openalex.org/W4379382445",
    "https://openalex.org/W3174531399",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4386083034",
    "https://openalex.org/W3035564946",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W2983339877",
    "https://openalex.org/W3204971388",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4384947796",
    "https://openalex.org/W3096739052",
    "https://openalex.org/W4239147634",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3198359486",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Conventional super-resolution methods suffer from two drawbacks: substantial computational cost in upscaling an entire large image, and the introduction of extraneous or potentially detrimental information for downstream computer vision tasks during the refinement of the background. To solve these issues, we propose a novel transformer-based algorithm, Selective Super-Resolution (SSR), which partitions images into non-overlapping tiles, selects tiles of interest at various scales with a pyramid architecture, and exclusively reconstructs these selected tiles with deep features. Experimental results on three datasets demonstrate the efficiency and robust performance of our approach for super-resolution. Compared to the state-of-the-art methods, the FID score is reduced from 26.78 to 10.41 with 40% reduction in computation cost for the BDD100K dataset.",
  "full_text": "Transformer-Based Selective Super-resolution for Efficient Image Refinement\nTianyi Zhang1, Kishore Kasichainula2, Yaoxin Zhuo2, Baoxin Li2, Jae-Sun Seo3, Yu Cao1\n1University of Minnesota\n2Arizona State University\n3Cornell Tech\nzhan9167@umn.edu, {kkasicha, yzhuo6, baoxin.li}@asu.edu, js3528@cornell.edu, yucao@umn.edu\nAbstract\nConventional super-resolution methods suffer from two\ndrawbacks: substantial computational cost in upscaling an en-\ntire large image, and the introduction of extraneous or po-\ntentially detrimental information for downstream computer\nvision tasks during the refinement of the background. To\nsolve these issues, we propose a novel transformer-based al-\ngorithm, Selective Super-Resolution (SSR), which partitions\nimages into non-overlapping tiles, selects tiles of interest at\nvarious scales with a pyramid architecture, and exclusively\nreconstructs these selected tiles with deep features. Exper-\nimental results on three datasets demonstrate the efficiency\nand robust performance of our approach for super-resolution.\nCompared to the state-of-the-art methods, the FID score is\nreduced from 26.78 to 10.41 with 40% reduction in computa-\ntion cost for the BDD100K dataset.\nIntroduction\nSuper-resolution (SR) is a fundamental task aimed at en-\nhancing image resolution by producing intricate details\nfrom low-resolution (LR) images. It supplies high-resolution\n(HR) images that are pivotal for downstream computer vi-\nsion tasks, such as object detection and image classifi-\ncation, with wide-ranging applications in the real world.\nFor instance, in the context of autonomous driving, higher-\nresolution images facilitate more precise and early object\ndetection, particularly for diminutive objects. Although vari-\nous super-resolution methods based on convolutional neural\nnetworks (CNNs) have been proposed, which enhance high-\nfrequency information through low-resolution image recon-\nstruction, their efficacy is impeded by a lack of long-range\ndependency integration.\nRecently, leveraging transformer-based architectures to\ncapture the extended contextual information, pioneering ef-\nforts like SwinIR (Liang et al. 2021) and HAT (Chen\net al. 2023), have achieved notable advancements in\nsuper-resolution. Nevertheless, two key issues persist with\nthese algorithms. Firstly, due to the substantial scale of\ntransformer-based networks, the computational demand be-\ncomes exceedingly high when reconstructing entire images,\nparticularly when input low-resolution image sizes are not\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The reconstruction of high-frequency background\ninformation in conventional SR methods (e.g., HAT) often\nresults in discrepancies in features compared to ground truth\nhigh-resolution (HR) images. SSR effectively resolves this\nissue by exclusively enhancing foreground pixels.\nsmall, such as 256×256. Secondly, even in state-of-the-\nart super-resolution approaches, the refined images fail to\nmatch the performance of ground truth HR images for typi-\ncal downstream tasks. To delve into the cause of this degra-\ndation, we conduct a comparison between features gener-\nated by the Inception model from refined images and orig-\ninal HR images. This analysis unveils that features derived\nfrom background pixels exhibit markedly different details\ncompared to the ground truth feature map, as depicted in\nFigure 1. This divergence suggests that overemphasizing\nbackground pixel details can introduce erroneous informa-\ntion and impede feature generation for downstream tasks.\nThis paper introduces a novel algorithm, Selective Super-\nResolution (SSR), designed to address these challenges.\nSpecifically, leveraging object location information, we par-\ntition images into non-overlapping tiles and employ a cost-\nefficient transformer-based network for tile selection. To en-\nsure comprehensive coverage of objects, a pyramid structure\nis devised for tile selection across multiple scales. In the fi-\nnal layer of this selection network, we integrate a Gumbel-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7305\nSoftmax layer (Jang, Gu, and Poole 2016) to make hard\ndecisions, subsequently directing positive tiles to an ensu-\ning super-resolution (SR) module. This module comprises\na convolution layer for shallow feature extraction, trans-\nformer blocks for deep feature extraction and an image\nreconstruction block. In contrast, negative tiles are recon-\nstructed directly from shallow features. This framework not\nonly reduces computation by extracting deep features solely\nfor positive tiles but also enhances image generation by\navoiding excessive background detail addition. To validate\nthe robustness of SSR, alongside common evaluation met-\nrics, such as Structural Similarity Index Measure (SSIM),\nFr´echet Inception Distance (FID), and Kernel Inception\nDistance (KID), we introduce a novel metric, inspired by\nKID and tailored to evaluate features from an object de-\ntection (OD) model, YOLO, called Kernel YOLO Distance\n(KYD). Our approach is experimented on three distinct\nimage datasets, BDD100K, MSRA10K, and COCO2017,\ndemonstrating both substantial computational reduction and\nimage generation improvement.\nTo summarize, our key contributions are as follows:\n• We design a low-cost Tile Selection module, employing\ntransformer blocks, to effectively extract desired object-\ncontaining tiles from images. The introduction of a pyra-\nmid structure ensures accurate selection of positive tiles.\n• By seamlessly integrating two transformer-based mod-\nules, Tile Selection (TS) and Tile Refinement (TR), our\nSSR efficiently reconstructs the input images by ex-\nclusively adding high-frequency information for object-\ncontaining tiles, effectively mitigating computational\ncosts and enhancing visual quality.\n• Through comprehensive experiments on three diverse\nimage datasets and various evaluation metrics, we show-\ncase SSR’s robust performance and specifically lower\nFID from 26.78 to 10.41 for BDD100K, accompanied by\na 40% reduction in computation cost.\nRelated Work\nSuper-Resolution\nIn this context, our primary focus revolves around single im-\nage super-resolution (SISR) techniques, without delving into\nmethodologies that reconstruct high-resolution images using\nmultiple input images. Previously, the SRCNN model (Dong\net al. 2014) based on convolutional neural networks (CNNs)\ninspires many works in SR (Tai, Yang, and Liu 2017; Niu\net al. 2020; Mei, Fan, and Zhou 2021). This seminal ap-\nproach employed bicubic interpolation and trained a three-\nlayer CNN for SR, achieving remarkable success. A diverse\nrange of CNN-based methodologies has emerged to map\nlow-resolution images to their high-resolution counterparts,\nleveraging distinct block designs, such as the incorporation\nof residual blocks (Lim et al. 2017). Moreover, harnessing\nthe unprecedented image generation capabilities of genera-\ntive adversarial networks (GANs) (Goodfellow et al. 2020).\nCertain studies have notably advanced the quality of gen-\nerated images, such as SRGAN (Ledig et al. 2017; Wang\net al. 2021, 2018; Zhang et al. 2019), which introduce adver-\nsarial learning to improve perceptual quality. Recent strides\nhave been witnessed in the adoption of the attention mech-\nanism, which excels in capturing long-range dependencies.\nThis mechanism has lately been adopted to further enhance\nSISR methodologies(Liang et al. 2021; Chen et al. 2023).\nTransformer in Computer Vision\nGiven the remarkable success of transformers in natural lan-\nguage processing (NLP) (Vaswani et al. 2017), this architec-\ntural paradigm is progressively permeating diverse computer\nvision tasks (Chu et al. 2021; Huang et al. 2021; Dong et al.\n2022; He et al. 2022; Zhang et al. 2023a,b). For instance,\nVision Transformer (ViT) divides input images into16 × 16\npatches, which are subsequently treated as tokens for the\napplication of the attention mechanism (Dosovitskiy et al.\n2020). For object detection (OD), DETR conceptualizes it\nas a direct set prediction issue and crafts a transformer-\nbased network (Carion et al. 2020). DINO introduces self-\nsupervised learning to propose a novel network rooted in\nthe ViT architecture (Caron et al. 2021). Swin Transformer\nintegrates window-based self-attention and shifted window-\nbased self-attention mechanisms, to reduce the computation\ncost by limiting the computation inside small windows (Liu\net al. 2021, 2022). By capturing long-range dependencies\nand facilitating enhanced visual representation, transformer-\nbased networks have exhibited superiority in various do-\nmains, including super-resolution (Liang et al. 2021; Chen\net al. 2023).\nMethodology\nThe overall architecture of our approach is illustrated in\nFigure 2. Selective Super-Resolution (SSR) network com-\nprises two fundamental modules: Tile Selection (TS) and\nTile Refinement (TR). Both modules are transformer-based\nnetworks, with TS being notably smaller in scale. Upon re-\nceiving an input image, the TS module initiates the process\nby partitioning it into non-overlapping tiles and then selects\npertinent tiles based on object location cues. The tiles con-\ntaining objects are directed through a computationally inten-\nsive block for intricate refinement, while the remaining tiles\ntraverse a cost-efficient block for straightforward upscaling.\nThis section is dedicated to a comprehensive discourse on all\nmodules, elucidating their specifics. We outline the precise\nalgorithm in Algorithm 1.\nTile Selection Module\nBy splitting the input images into4×4 non-overlapping tiles\nand utilizing the location information of objects in these im-\nages, this module classifies each tile by whether it contains\nobjects or not. Specifically, as shown in Figure 2, this mod-\nule consists of an image encoder and several tile classifiers.\nFirst, the embedding layer and the first transformer layer\nof the encoder embeds the low-resolution (LR) input image\nILR ∈ RHLR×WLR×CLR (HLR, WLR, and CLR are the in-\nput LR image height, width and the number of channels)\ninto representations r0 ∈ R\nHLR\np ×WLR\np ×C, where p is the\npatch size of each token and C is the number of channels\nfor each token. After that, three transformer layers, T L1,\nT L2, and T L3, generate representations of these tokens at\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7306\nFigure 2: Overall architecture of Selective Super-Resolution (SSR). Input low-resolution (LR) images are split into non-\noverlapping tiles and classified to two classes by Tile Selection (TS) module. And only positive tiles containing objects are\nintricately reconstructed by Positive Tile Refinement (PTR) path with deep features. The specific Residual Transformer Block\n(RTB) in the feature extraction module will be introduced in detail later.\nthree different scales. The transformer layer adopts the struc-\nture of Swin Transformer (Liu et al. 2021, 2022). Basically,\nwith the incorporation of window-based self-attention and\nshifted window-based self-attention mechanisms, this layer\ncan capture global contextual information with attention\ncomputation within small windows, which is much cheaper\nthan the traditional attention mechanism. Besides, with the\nfeature merging layer, we can obtain features at various\nscales to enable the implementation of pyramid structure.\nThe whole process is as follows,\nr1 = T L1(r0), r2 = T L2(r1), r3 = T L3(r2) (1)\nwhere r1 ∈ R\nHLR\n2p ×WLR\n2p ×2C, r2 ∈ R\nHLR\n4p ×WLR\n4p ×4C,\nr3 ∈ R\nHLR\n8p ×WLR\n8p ×8C are three generated representations\nfor classification. And, each token of these representations\ncorresponds to tiles of size 2p × 2p, 4p × 4p, and 8p × 8p,\nrespectively.\nTo classify these tiles, we adopt the cross-attention mech-\nanism by introducing a learnable classification token, de-\nnoted as c. We obtain the query matrix Q by applying one\nlinear layer to image featuresr1, r2 or r3 as Qi = riWq\ni , i∈\n1, 2, 3 while computing the key matrix and value matrix\nwith the classification token as equations Ki = cWk\ni , Vi =\ncWv\ni , i∈ 1, 2, 3, then the attention computation is expressed\nas follows,\nAi = softmax( QiKT\ni\n√\nd\n)V i ∀i ∈ 1, 2, 3 (2)\nNext, each of the three features undergoes individual\nprocessing through a multi-layer perceptron (MLP) and a\nGumbel-Softmax layer. This step facilitates making defini-\ntive classifications for the tile classes. This is crucial for the\nsubsequent refinement module to apply the corresponding\nnetwork. The process are as follows:\nsi = GumbelSoftmax (MLPi(Ai)) ∀i ∈ 1, 2, 3 (3)\nwhere MLPi denotes the output layer for theith feature em-\nbeddings.\nAccordingly to the network structure, after pooling the\ninstance segmentation label of the input images at three\ndifferent scales, we introduce a pyramid label that con-\ntains, y1 ∈ R\nHLR\n2p ×WLR\n2p ×1, y2 ∈ R\nHLR\n4p ×WLR\n4p ×1, and\ny3 ∈ R\nHLR\n8p ×WLR\n8p ×1, to supervise the training of TS mod-\nule by allocating positive labels to the tiles that contain ob-\njects. This hierarchical structure ensures the preservation of\na larger number of tiles and minimizes the loss of positive\ninstances. Finally, all tiles are divided into two groups to be\nprocessed by the subsequent refinement module.\nTile Refinement Module\nAs depicted in Figure 2, there are two Tile Refinement\n(TR) paths: Positive Tile Refinement (PTR) targeting object-\ncontaining tiles, and Negative Tile Refinement (NTR) for\ntiles with solely background pixels.\nPositive Tile Refinement. For positive tiles which con-\ntain objects, our refinement involves a transformer-based\nprocess for deep feature extraction and image reconstruc-\ntion. To be specific, for a given tile TLR ∈ R8p×8p×CLR,\nthe convolution layer first extract the shallow feature FS ∈\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7307\nAlgorithm 1: Selective Super-Resolution (SSR)\nInput: Low-resolution (LR) Image data ILR, tile class\nlabels y1, y2, y3, and ground truth high-resolution (HR)\nimage IGT\nParameter: loss weight α, learning rate η\n1: Initialize model parameters θ\n2: for each epoch t = 1, 2, ... do\n3: r1, r2, r3 = fθIE (ILR) ▷ Generate representations\nwith Image Encoder (IE)\n4: si = GumbelSoftmax (fθCL (ri)) ∀i ∈ 1, 2, 3 ▷\nClassify tiles with Classification Layer (CL)\n5: LTS (θ) =P3\ni=1(−yilog(si)−(1−yi)log(1−si))\n6: for each tile T1\nLR, T2\nLR, ..., TN\nLR do\n7: if sn\n3 = 1then ▷ Positive tiles\n8: Fn\nS = fθConv (Tn\nLR) ▷ Shallow feature\nextraction with convolution layer\n9: Fn\nD = fθFE (Fn\nS )) ▷ Deep Feature\nExtraction (FE)\n10: Tn\nHR = fθIR (Fn\nS + Fn\nD) ▷ HR Image\nReconstruction (IR)\n11: else ▷ Negative tiles\n12: Fn\nS = fθConv (Tn\nLR)\n13: Tn\nHR = fθIR (Fn\nS )\n14: end if\n15: end for\n16: Group all output tiles THR to entire images IHR\n17: LTR (θ) =||IHR − IGT ||1\n18: LSSR (θ) =LTS (θ) +αLTR (θ)\n19: θ ← θ − η∇θLSSR (θ) ▷ Update model\n20: end for\nR8p×8p×Cf , where Cf is the number of channels for fea-\ntures. Subsequently, a series of K residual transformer\nblocks (RTBs), based on the Swin transformer architecture,\nare employed to derive deep features FD ∈ R8p×8p×Cf as\nthe following equations:\nFi = RT Bi(Fi−1), i= 1, 2, ..., K, (4)\nFD = Conv(FK) (5)\nwhere F0 is the input shallow featureFS, RT Bi denotes the\ni-th RTB, and Conv is the final convolution layer.\nThe keypoint is the design of RTB. Figure 3 presents the\nspecific structure of RTB. Basically, it consists of a series\nof transformer layers and one convolution layer. Primarily,\nthe inclusion of an additional convolutional layer at the end\nserves to optimize the transformer more effectively, yield-\ning enhanced representations. This is due to the fact that\ndirect similarity comparisons across all tokens often intro-\nduce redundancy, as evidenced in various works(Zhang et al.\n2018; Liang et al. 2021; Li et al. 2023; Wu et al. 2021; Xiao\net al. 2021). Secondly, the skip connection within the RTB\nestablishes a link between features at different levels and the\nimage reconstruction block. This facilitates the aggregation\nof features from diverse levels, promoting the integration\nof multi-level information. Additionally, we adopt distinct\nFigure 3: Structure of RTB. The incorporation of a skip\nconnection and convolution layer in RTB contributes to im-\nproved feature learning capabilities.\nattention blocks proposed in (Chen et al. 2023) to activate\nmore pixels for high-resolution image reconstruction.\nAfter obtaining both the shallow featureFS and deep fea-\nture FD, we merge them to reconstruct HR tiles using the\nfollowing equation,\nTHR = IR(FS + FD) (6)\nwhere IR is the image reconstruction block. By transmit-\nting the shallow feature containing low-frequency informa-\ntion and the deep feature which highlights high-frequency\ndetails via a long skip connection, this module effectively\nconcentrates on reconstructing high-frequency information.\nAnd in this block, the sub-pixel convolution layer (Shi et al.\n2016) is employed to upsample the feature.\nNegative Tile Refinement. Since the intricate refinement\nfor the background introduce some irrelevant or even detri-\nmental features which degrade the image quality for down-\nstream tasks, we remove the transformer-based feature ex-\ntraction block and directly reconstruct the negative tiles with\nshallow features to obtain HR tiles with the same resolution\nas refined positive tiles.\nAfter obtaining HR tiles via both PTR and NTR, we con-\nsolidate them to produce the refined HR image.\nLoss Function\nFor the TS module, the loss is computed using cross-entropy.\nWith the introduction of a pyramid structure aimed at reduc-\ning false negative predictions (FN) and ensuring the selec-\ntion of all positive patches, the losses from the three scales\nare combined to formulate the final loss as follows:\nLTS =\n3X\ni=1\n(−yilog(si) − (1 − yi)log(1 − si)) (7)\nFor the TR module, we leverage L1 loss to quantify the\ndiscrepancy between the generated high-resolution image\nIHR and the ground truth high-resolution image IGT as fol-\nlows:\nLTR = ||IHR − IGT ||1 (8)\nFinally, the loss function of SSR is the weighted sum of\nthese two modules and the formulation is expressed as,\nLSSR = LTS + αLTR (9)\nwhere α is a hyper-parameter to adjust the weight of TS\nmodule in training.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7308\nExperimental Results\nDatasets and Experimental Settings\nWe evaluate the effectiveness of our SSR model on three\ndatasets, BDD100K (Yu et al. 2020), COCO 2017 (Lin\net al. 2014) and MSRA10K (Cheng et al. 2015). These\ndatasets provide segmentation or object detection labels, en-\nabling us to generate tile classification labels for tile se-\nlection. BDD100K is the most complex dataset focused\non autonomous driving, containing approximately 100,000\nhigh-quality images with an original resolution of 1280 ×\n720. Each image encompasses various objects, including\nvehicles, pedestrians, and eight other object categories.\nCOCO2017 comprises around 100,000 training images and\n5,000 validation images. Each image encompasses multiple\nobjects spanning 80 different classes. MSRA10K provides\n10000 images and their segmentation labels.\nFor all datasets, we resize them into low-resolution (LR)\nimages with resolution 256 ×256, and high-resolution (HR)\nimages with resolution1024×1024 by bicubic interpolation.\nThe embedding dimension of the Tile Selection (TS) module\nis 96 while it’s 180 for Tile Refinement (TR) module. We set\nthe learning rates to 0.00001. Each TL utilizes a depth of 2, a\nwindow size of 7, and 3 attention heads. We employ a patch\nsize of 2 for embedding, which corresponds to tile sizes of\n16 × 16, 32 × 32, and 64 × 64, yielding tile labels of 4 × 4,\n8 × 8, and 16 × 16 respectively. The weight parameter α\nfor the loss function is set to 1. The number of RTB is 6.\nAll experiments are conducted for 50 epochs on two Linux\nservers, each equipped with two NVIDIA A6000 GPUs.\nEvaluation Metrics. Besides the common metrics for im-\nage quality, like Peak Signal-to-Noise Ratio (PSNR), Struc-\ntural Similarity Index Measure (SSIM), Fr ´echet Inception\nDistance (FID), and Kernel Inception Distance (KID), we\nintroduce an additional metric named Kernel YOLO Dis-\ntance (KYD) to demonstrate the robustness of our approach.\nTo evaluate FID, we employ a pre-trained image classifica-\ntion model, Inceptionv3, to generate features and compare\nthe distributions of two image sets. For KYD, we focus on\nthe comparison of features for object detection (OD) by uti-\nlizing a pre-trained YOLOv8 model to generate features and\ncompute the kernel distance similarly to KID.\nFurthermore, we assess the performance of the TS mod-\nule using True Positive Rate (TPR) and the maximum F1\nscore (maxF) for accuracy, along with the average number\nof selected tiles for computation efficiency.\nTile Selection Results\nFor Tile Selection (TS), we explore various configurations\nto identify the optimal setup for our objectives. We adjusted\nthe patch size of the first embedding layer, which conse-\nquently influenced the number of transformer layers (TL)\nin the network. Specifically, by setting the patch size to 2,\n4, or 8, we achieved 5, 4, or 3 TLs respectively. We also\nexperimented with different loss functions, including pyra-\nmid labels at various scales and a single label. Additionally,\nwe introduced a max block at the end of the TS module\nto consolidate three predictions by selecting the maximum.\nThe results, presented in Table 1, highlighted several key\nModel TPR maxF #Tiles #MACs\nB (w/o pyramid) 0.9137 0.8807 62% 563.21M\nS (w/o pyramid) 0.9086 0.8522 65% 155.30M\nT (w/o pyramid) 0.8973 0.8225 66% 53.11M\nB (w/ pyramid) 0.9341 0.8967 62% 563.21M\nS (w/ pyramid) 0.9048 0.8327 66% 155.30M\nT (w/ pyramid) 0.8855 0.8384 62% 53.11M\nB (w/ max) 0.9692 0.8893 77% 563.21M\nS (w/ max) 0.9681 0.8572 79% 155.30M\nT (w/ max) 0.9666 0.8400 81% 53.11M\nTable 1: Results of TS module. Encode images with 5, 4, or 3\ntransformer layers, corresponding to TS-Base (B), TS-Small\n(S) and TS-Tiny (T) in this table. More layers provide better\nperformance with increasing computation. The adoption of\nthe pyramid structure for tile selection across different scales\nimproves the network’s effectiveness. The integration of the\nmax block further enhances the selection.\ninsights. Firstly, increasing the number of transformer lay-\ners enhanced performance, albeit at the cost of increased\ncomputation. Secondly, the pyramid structure consistently\nyielded better outcomes. Lastly, while the max block im-\nproved selection results, it also introduced additional com-\nputational overhead for the refinement module.\nSuper-Resolution (SR) Results\nAs shown in figure 1, by comparing the feature visualization\nresults of reconstructed images by SR methods with ground\ntruth HR images, we found the high-frequency refinement of\nconventional methods for background adversely impacts the\nimage quality. To substantiate this observation, we devised\ntwo new datasets: one by replacing foreground pixels and\nthe other by substituting background pixels with upscaled\nimages via bicubic interpolation. This simulation aimed to\nexplore the impact of high-frequency removal. Upon eval-\nuating the image quality of these datasets, we discerned\nan intriguing finding. Even the mere replacement of pix-\nels lacking high-frequency information for the background\nled to enhanced image quality. Figure 4 graphically illus-\ntrates this comparison. The top example juxtaposes gener-\nated foreground via SR with coarse background upscaled\nby bicubic interpolation, while the bottom image maintains\nhigh-frequency details exclusively in the background. The\ntop dataset exhibited superior image quality, as indicated by\nlower FID and higher SSIM. This quantitative assessment\nprovides further support for our SSR design.\nQuantitative Results. Table 2 shows the quantitative\ncomparison of our approach with the state-of-the-art meth-\nods (Liang et al. 2021; Chen et al. 2023) on three differ-\nent datasets, considering a 4× upscale of the input images.\nThe results clearly demonstrate the consistent superiority of\nour proposed method, SSR, in terms of various evaluation\nmetrics. This robust performance underlines SSR’s potential\nto generate images with improved features for downstream\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7309\nMethod BDD100K COCO 2017 MSRA10K\nPSNR↑ SSIM↑ FID↓ KID↓ KYD↓ SSIM↑ FID↓ KYD↓ SSIM↑ FID↓ KYD↓\nSwinIR 30.08 0.8757 24.87 0.0104 0.0092 0.8420 8.483 0.0036 0.9441 9.502 0.0035\nHAT 30.09 0.8750 26.78 0.0115 0.0096 0.8391 10.03 0.0037 0.9473 9.577 0.0030\nSSR (ours) 29.91 0.8833 10.41 0.0017 0.0041 0.8512 2.844 0.0006 0.9653 1.905 0.0002\nTable 2: Comparison of SR results. The table presents the outcomes for a 4×upscaling factor. For SwinIR and HAT, we utilize\nthe official tile-based implementation of HAT for equitable assessment. Both SwinIR and HAT demonstrate results inferior to\nour SSR in terms of image quality across all three datasets. Generated images by SSR can perform better for downstream tasks\nwith lower FID and KYD.\nFigure 4: By replacing either the background or foreground\npixels with bicubic-interpolated upscaled images, we effec-\ntively simulate the removal of high-frequency details from\nthese regions. The results illustrate that the high-frequency\ninformation of background introduced by SR module im-\npairs the generated image quality.\nMethod Scale SSIM FID FID KYD\nHAT ×2 0.8764 27.58 0.0123 0.0092\nSSR (ours) ×2 0.8890 10.28 0.0016 0.0037\nHAT ×3 0.8768 30.88 0.0150 0.0096\nSSR (ours) ×3 0.8918 10.80 0.0017 0.0027\nTable 3: Quantitative comparison with HAT for 2× and 3×\nup-sampling. SSR consistently outperforms HAT across all\nevaluation metrics, demonstrating its superiority in refining\nimages at various scales.\ntasks such as image classification and object detection, as\nindicated by its lower FID and KYD.\nIn Table 3, we extend the comparison to 2× and 3×\nmagnification factors on BDD100K. Given SwinIR’s chal-\nlenges on this dataset, we focus on contrasting HAT with\nour method. The results showcased in this table demonstrate\nSSR’s proficiency across different scales.\nVisual Comparison. Figure 5 illustrates the visual com-\nparison across three datasets. Besides the ground truth (GT)\nMethod #Tiles SSIM FID KID KYD\nw/ max 62% 0.8833 10.41 0.0017 0.0041\nw/o max 77% 0.8835 10.62 0.0017 0.0041\nTable 4: With the max block, the visual quality of images\ngenerated by SSR can be slightly improved with additional\n15% computation overhead.\nDataset Method #Tiles #Params\n(M)\n#MACs\n(G)\nBDD100K HAT - 1675.99 191.06\nSSR 62% 1233.13 119.40\nCOCO 2017 HAT - 1675.99 191.06\nSSR 65% 1233.13 125.15\nMSRA10K HAT - 1675.99 191.06\nSSR 57% 1233.13 109.82\nTable 5: Computation cost comparison. Compared to HAT,\nSSR achieves about 40% computation reduction for all three\ndatasets. All metrics are evaluated for a 256 × 256 input.\nimages and the images generated by HAT and our SSR ap-\nproach, we also present the extracted features from these im-\nages. The features derived from our SSR-generated images\nexhibit a closer resemblance to GT images, emphasizing the\nfidelity of our approach.\nEfficiency. Actually, with more tiles selected by the max\nblock, the generated image quality remains quite similar, as\nshown in Table 4. So, we opt not to include this block in our\nfinal algorithm. For our SSR, only positive tiles are directed\nthrough the expensive SR module. In Table 5, we summa-\nrize the total number of parameters and computation cost\nfor three different datasets. It underscores the efficiency of\nSSR with about 40% computation reduction.\nAblation Study\nPre-training with ImageNet. Similar to other computer\nvision work, SSR can substantially benefit from a pre-\ntraining strategy employing a large dataset, such as Ima-\ngeNet. A comparison of results obtained with and without\npre-training highlights the enhancement in image quality,\nwith PSNR scores improving from 29.56 to 29.91. Figure\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7310\nFigure 5: Visual comparison with the state-of-the-art methods for 4× upscaling. We generate features with Inceptionv3 model\nand visualize them. The features extracted from SSR images exhibit a higher degree of similarity to the GT features.\nFigure 6: With pre-training, SSR can generate better images\ncompared to the outcomes without pre-training.\n6 presents one example for visual comparison.\nNegative Tile Refinement. For background pixels, high-\nfrequency information recovery is unnecessary. We experi-\nmented with two different designs: the first employs a sin-\ngle Upsample layer, while the second retains the convolu-\ntion layers from the Positive Tile Refinement (PTR) path\nand omits the transformer blocks. The results are shown in\nTable 6. Adding convolution layers in NTR marginally en-\nhances image quality. This enhancement can be attributed\nto the compensation for a few positive tiles that might be\nmissed by the selection module. However, this improvement\ncomes at the cost of increased 19.2% computational load.\nTherefore, it is a trade-off between the computation and im-\nage generation performance.\nMethod PSNR SSIM FID KID KYD\nw/o conv 29.91 0.8833 10.41 0.0017 0.0041\nw/ conv 30.03 0.8863 9.56 0.0013 0.0042\nTable 6: With convolution layers in the NTR path, SSR\nshowcases an improvement in image generation.\nConclusion\nIn this paper, we delve into the cause behind the im-\nage quality gap between generated images through conven-\ntional super-resolution (SR) techniques and high-resolution\n(HR) ground truth images. Our investigation reveals that\nthe high-frequency refinement of background pixels un-\ndermines the overall image feature generation for down-\nstream tasks. To address this challenge and concurrently re-\nduce computational overhead, we introduce a novel algo-\nrithm termed Selective Super-Resolution (SSR). By leverag-\ning a cost-efficient transformer-based network, we partition\nthe input low-resolution (LR) image into non-overlapping\ntiles, assessing the presence of objects within each tile.\nBy exclusively refining the high-frequency characteristics\nof object-containing tiles, we improve visual quality with\na lower computation cost. Our approach’s superior perfor-\nmance is substantiated across three distinct datasets employ-\ning diverse evaluation metrics. Specifically, experiments on\nBDD100K exhibit an improvement of FID from 26.78 to\n10.41 with 40% computation reduction.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7311\nAcknowledgments\nThis work is supported by the Center for the Co-Design of\nCognitive Systems (CoCoSys), one of seven centers in Joint\nUniversity Microelectronics Program 2.0 (JUMP 2.0), a\nSemiconductor Research Corporation (SRC) program spon-\nsored by the Defense Advanced Research Projects Agency\n(DARPA).\nReferences\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer International Publishing Cham.\nCaron, M.; Touvron, H.; Misra, I.; J ´egou, H.; Mairal, J.;\nBojanowski, P.; and Joulin, A. 2021. Emerging properties\nin self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n9650–9660.\nChen, X.; Wang, X.; Zhou, J.; Qiao, Y .; and Dong, C.\n2023. Activating more pixels in image super-resolution\ntransformer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 22367–22377.\nCheng, M.-M.; Mitra, N. J.; Huang, X.; Torr, P. H. S.; and\nHu, S.-M. 2015. Global Contrast based Salient Region De-\ntection. IEEE TPAMI, 37(3): 569–582.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021. Twins: Revisiting the design of\nspatial attention in vision transformers. Advances in Neural\nInformation Processing Systems, 34: 9355–9366.\nDong, C.; Loy, C. C.; He, K.; and Tang, X. 2014. Learning\na deep convolutional network for image super-resolution.\nIn Computer Vision–ECCV 2014: 13th European Confer-\nence, Zurich, Switzerland, September 6-12, 2014, Proceed-\nings, Part IV 13, 184–199. Springer.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2022. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 12124–12134.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2020. Generative adversarial networks. Communications of\nthe ACM, 63(11): 139–144.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll ´ar, P.; and Girshick,\nR. 2022. Masked autoencoders are scalable vision learners.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 16000–16009.\nHuang, Z.; Ben, Y .; Luo, G.; Cheng, P.; Yu, G.; and Fu, B.\n2021. Shuffle transformer: Rethinking spatial shuffle for vi-\nsion transformer. arXiv preprint arXiv:2106.03650.\nJang, E.; Gu, S.; and Poole, B. 2016. Categorical\nreparameterization with gumbel-softmax. arXiv preprint\narXiv:1611.01144.\nLedig, C.; Theis, L.; Husz´ar, F.; Caballero, J.; Cunningham,\nA.; Acosta, A.; Aitken, A.; Tejani, A.; Totz, J.; Wang, Z.;\net al. 2017. Photo-realistic single image super-resolution us-\ning a generative adversarial network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 4681–4690.\nLi, K.; Wang, Y .; Zhang, J.; Gao, P.; Song, G.; Liu, Y .; Li,\nH.; and Qiao, Y . 2023. Uniformer: Unifying convolution and\nself-attention for visual recognition. IEEE Transactions on\nPattern Analysis and Machine Intelligence.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021. Swinir: Image restoration using swin\ntransformer. In Proceedings of the IEEE/CVF international\nconference on computer vision, 1833–1844.\nLim, B.; Son, S.; Kim, H.; Nah, S.; and Mu Lee, K. 2017.\nEnhanced deep residual networks for single image super-\nresolution. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition workshops, 136–144.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, Z.; Hu, H.; Lin, Y .; Yao, Z.; Xie, Z.; Wei, Y .; Ning, J.;\nCao, Y .; Zhang, Z.; Dong, L.; et al. 2022. Swin transformer\nv2: Scaling up capacity and resolution. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 12009–12019.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012–10022.\nMei, Y .; Fan, Y .; and Zhou, Y . 2021. Image super-resolution\nwith non-local sparse attention. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3517–3526.\nNiu, B.; Wen, W.; Ren, W.; Zhang, X.; Yang, L.; Wang, S.;\nZhang, K.; Cao, X.; and Shen, H. 2020. Single image super-\nresolution via a holistic attention network. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XII 16, 191–\n207. Springer.\nShi, W.; Caballero, J.; Husz ´ar, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-time\nsingle image and video super-resolution using an efficient\nsub-pixel convolutional neural network. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, 1874–1883.\nTai, Y .; Yang, J.; and Liu, X. 2017. Image super-resolution\nvia deep recursive residual network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 3147–3155.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7312\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, X.; Xie, L.; Dong, C.; and Shan, Y . 2021. Real-\nesrgan: Training real-world blind super-resolution with pure\nsynthetic data. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, 1905–1914.\nWang, X.; Yu, K.; Wu, S.; Gu, J.; Liu, Y .; Dong, C.; Qiao,\nY .; and Change Loy, C. 2018. Esrgan: Enhanced super-\nresolution generative adversarial networks. In Proceedings\nof the European conference on computer vision (ECCV)\nworkshops, 0–0.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF international\nconference on computer vision, 22–31.\nXiao, T.; Singh, M.; Mintun, E.; Darrell, T.; Doll ´ar, P.; and\nGirshick, R. 2021. Early convolutions help transformers see\nbetter. Advances in neural information processing systems,\n34: 30392–30400.\nYu, F.; Chen, H.; Wang, X.; Xian, W.; Chen, Y .; Liu, F.; Mad-\nhavan, V .; and Darrell, T. 2020. BDD100K: A Diverse Driv-\ning Dataset for Heterogeneous Multitask Learning. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR).\nZhang, T.; Kasichainula, K.; Jee, D.-W.; Yeo, I.; Zhuo, Y .;\nLi, B.; Seo, J.-s.; and Cao, Y . 2023a. Improving the Effi-\nciency of CMOS Image Sensors through In-Sensor Selec-\ntive Attention. In 2023 IEEE International Symposium on\nCircuits and Systems (ISCAS), 1–4. IEEE.\nZhang, T.; Kasichainula, K.; Zhuo, Y .; Li, B.; Seo, J.-S.; and\nCao, Y . 2023b. Patch-based Selection and Refinement for\nEarly Object Detection. arXiv preprint arXiv:2311.02274.\nZhang, W.; Liu, Y .; Dong, C.; and Qiao, Y . 2019. Ranksr-\ngan: Generative adversarial networks with ranker for image\nsuper-resolution. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 3096–3105.\nZhang, Y .; Li, K.; Li, K.; Wang, L.; Zhong, B.; and Fu, Y .\n2018. Image super-resolution using very deep residual chan-\nnel attention networks. In Proceedings of the European con-\nference on computer vision (ECCV), 286–301.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7313",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5223844647407532
    },
    {
      "name": "Computer science",
      "score": 0.5057384967803955
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4646000564098358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4361749589443207
    },
    {
      "name": "Computer vision",
      "score": 0.3663521409034729
    },
    {
      "name": "Engineering",
      "score": 0.13782784342765808
    },
    {
      "name": "Electrical engineering",
      "score": 0.11745652556419373
    },
    {
      "name": "Voltage",
      "score": 0.06440714001655579
    }
  ]
}