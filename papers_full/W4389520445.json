{
  "title": "Learn Your Tokens: Word-Pooled Tokenization for Language Modeling",
  "url": "https://openalex.org/W4389520445",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2767883706",
      "name": "Avijit Thawani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2036193908",
      "name": "Saurabh Ghanekar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101902995",
      "name": "Xiaoyuan Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A704051514",
      "name": "Jay Pujara",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3198722157",
    "https://openalex.org/W3166790124",
    "https://openalex.org/W4376632433",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4293392373",
    "https://openalex.org/W4287854950",
    "https://openalex.org/W4224510354",
    "https://openalex.org/W3105643199",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W3088049945",
    "https://openalex.org/W4285596748",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W3154987757",
    "https://openalex.org/W4206068491",
    "https://openalex.org/W4226271314",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W4379538464",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3029337234",
    "https://openalex.org/W4285199167",
    "https://openalex.org/W4385571757",
    "https://openalex.org/W4300466035",
    "https://openalex.org/W4312533035",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3191780119",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3135427360"
  ],
  "abstract": "Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as ‘ing’ or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative ‘learn your tokens’ scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderately expressive and moderately fast end-to-end tokenizer outperform by over ‘300%‘ both subwords and byte/character models over the intrinsic language modeling metric of next-word prediction across datasets. It particularly outshines on rare words, outperforming by a factor of 30! We extensively study the language modeling setup for all three categories of tokenizers and theoretically analyze how our end-to-end models can also be a strong trade-off in efficiency and robustness.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9883–9893\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLearn Your Tokens:\nWord-Pooled Tokenization for Language Modeling\nAvijit Thawani\nthawani@usc.edu\nSaurabh Ghanekar\nUSC\nXiaoyuan Zhu\nUSC\nJay Pujara\nUSC / ISI\nAbstract\nLanguage models typically tokenize text\ninto subwords, using a deterministic, hand-\nengineered heuristic of combining characters\ninto longer surface-level strings such as ‘ing’\nor whole words. Recent literature has repeat-\nedly shown the limitations of such a tokeniza-\ntion strategy, particularly for documents not\nwritten in English and for representing num-\nbers. On the other extreme, byte/character-\nlevel language models are much less restricted\nbut suffer from increased sequence descrip-\ntion lengths and a subsequent quadratic ex-\npansion in self-attention computation. Recent\nattempts to compress and limit these context\nlengths with fixed size convolutions is help-\nful but completely ignores the word boundary.\nThis paper considers an alternative ‘learn your\ntokens’ scheme which utilizes the word bound-\nary to pool bytes/characters into word repre-\nsentations, which are fed to the primary lan-\nguage model, before again decoding individual\ncharacters/bytes per word in parallel. We find\nthat our moderately expressive and moderately\nfast end-to-end tokenizers outperform by over\n300% both subwords and byte/character mod-\nels over the intrinsic language modeling metric\nof next-word prediction across datasets. It par-\nticularly outshines on rare words, outperform-\ning by a factor of 30! We extensively study\nthe language modeling setup for all three cate-\ngories of tokenizers and theoretically analyze\nhow our end-to-end models can also be a strong\ntrade-off in efficiency and robustness. [Github].\n1 Introduction\nAlmost all natural language processing (NLP) be-\ngins with tokenization (Mielke et al., 2021). Se-\nquences of characters are (mostly deterministically)\nsegmented into discrete tokens, each of which has\na lookup embedding in an enormous vocabulary\nmatrix. Statistical NLP methods, similar to other\nforms of machine learning at the time, relied on\nfeature extraction from these tokens, in the form of\nEfficiency Expressivity Accuracy\nSubword High Low Mid\nByte/ Low High Low\nChar\neByte/ Mid Mid High\neChar\nTable 1: Trade-offs involved when choosing tokenizers:\nSubword vs Bytes/Characters vs eByte/eChar (ours).\nn-gram occurrences or part-of-speech tags or other\nrepresentations of syntax. All of these pipelines\nhave over time been replaced with end-to-end learn-\ning using recurrent neural networks (RNNs) or\ntransformers, however the tokenization schemes\nremain static, deterministic, and manually engi-\nneered.\nState-of-the-art approaches include subword to-\nkenization schemes such as WordPiece (Wu et al.,\n2016), Byte Pair Encoding or BPE (Sennrich et al.,\n2016), and Unigram (Kudo, 2018), all of which\nare statistical methods for preprocessing a large\nunlabeled corpus of text to yield a fixed vocabu-\nlary, midway between characters or bytes at one\nend and whole words at the other. This results in a\nconvenient trade-off in sequence description length\nwhile avoiding the UNK token, that is, a fallback\nmechanism for handling rare words. However, it is\nnot obvious why these hand-engineered algorithms\nwould be the optimal forms of tokenization and\nwhether there exists a possibility for end-to-end\nmodels to also include this crucial stage of the NLP\npipeline.\nRecent work has shown countless limitations\nwith subword embeddings. Several languages con-\ntain diverse morphological features whereas sub-\nword segmentation is mostly apt at only identifying\nsuffixes and prefixes (Clark et al., 2022). Technical\ndomains such as biomedical documents often need\nto pre-train their own tokenizer for improved vocab-\nulary (Boecking et al., 2022). Finally, numbers are\n9883\noften inconsistently segmented into subwords, lead-\ning to decreased arithmetic (Wallace et al., 2019)\nand estimation (Thawani et al., 2021b) skills. The\nextent of these numeric limitations is so dire that\nGPT-4 (OpenAI, 2023) has an explicit workaround\nof adding all numbers from 0 to 999 as individual\ntokens to the model’s vocabulary.\nRecently, several language models have been\nproposed which remove the tokenizer vocabulary\nentirely, beginning with a character (El Boukkouri\net al., 2020) or byte-level (Xue et al., 2022) vocab-\nulary and often compressing them into fixed units\nof around four tokens each (Tay et al., 2021; Yu\net al., 2023; Clark et al., 2022). While these zero-\nassumption methods are useful in compressing text\nand consequently expand context windows, they\ncompletely ignore the word boundary. Besides, the\nso-called ‘tokenizer-free’ byte-based models are\nnot entirely bias-free since the Unicode-8 encod-\ning they use is itself biased towards representing\nLatin scripts with a single byte each, whereas some\nscripts1 like Bammum (Africa), Meetei (India), and\nCherokee (North America) may require four bytes\nto represent a single character.\nThe concept of words is a fundamental feature of\nnearly all human languages, including those written\nin Chinese or Japanese scripts that do not explic-\nitly delineate words by whitespaces. This paper\nempirically studies the case where tokenizers lose\ntheir subword segmentation algorithms but utilize\nthe word boundary for a multi-level model with\nadded efficiency. More concretely, we use the word\nboundary to compress the base tokens of bytes or\ncharacters into word representations, which are\nthen fed into the underlying language model (here,\na small version of GPT (Radford et al., 2018)).\nOur end-to-end learned tokenization undoubt-\nedly has several limitations. It is not faster than\nsubwords. It does not allow characters/bytes within\none word to directly attend to those in another\nword. It relies on the word boundary, which is\nnot straightforward to find for most internet-scale\ndatasets. Nevertheless, we believe this empirical\ndeep-dive into tokenizers for language modeling\noffers the following contributions:\n1. We compare different tokenizer strategies for\nlanguage modeling on multiple facets and on\na fair footing across languages.\n2. We are the first to explicitly use word bound-\n1https://unicode.org/roadmaps/bmp/\nFigure 1: Overview of our proposed simple end-to-\nend tokenized autoregressive language model. A trans-\nformer encoder compresses the variable number of base\nunits (here, characters) into n=1 CLS tokens per word.\nDotted characters are the previously predicted tokens at\ninference, and when training they are the ground truth.\nary to compress an autoregressive language\nmodel’s base tokens.\n3. We report over 300% gains in language mod-\neling capabilities over multiple languages and\ndatasets, against both subwords and charac-\nter/byte models, and by a factor of 30 on rare\nwords.\n4. We theoretically analyze strengths and weak-\nnesses of our word-compressed tokenization\nscheme, which carries insights for the lan-\nguage modeling community.\nWe will publicly release all code (see supplemen-\ntary material) and checkpoints upon acceptance.\n2 Method\nFigure 1 pictorially depicts our proposed language\nmodel architecture. Our end-to-end tokenization\nstrategy is a straightforward word pooling method\nwhich uses a transformer encoder (Step 1) to pool\nin the base tokens (characters or bytes) into a fixed\nnumber of embeddings per word. This is analogous\nto how CLS embeddings are often used to pool in\nthe embeddings of an entire sentence or any text\n9884\nsequence in BERT-like transformer encoders. In\nour case, we have the equivalent of a fixed number\nof CLS tokens2 prepended to each word that store\nthe meaning of the entire word.\nNext, (Step 2) the pooled per-word embeddings\nare passed onto the main language model, in our\ncase, a vanilla transformer decoder like GPT (Rad-\nford et al., 2018). Finally, the contextualized word\nembeddings are fed through another transformer\ndecoder to autoregressively decode the next word,\none base unit (character or byte) at a time. Note\nthat we call this method an end-to-end ‘tokenizer’\nsince it compresses the many units into a few em-\nbeddings per word, just like subwords, except the\ncompression is learned from scratch. Finally, at de-\ncoding stage (Step 3), the contextualized word rep-\nresentations are unrolled with another transformer\ndecoder to autoregressively predict one base token\n(character/byte) at a time. 3\nNote how we achieve our purported trade-off\nbetween subwords and byte/character models. The\nCLS representations learnt are unconstrained by a\ndeterministic mapping as in subwords. They are\nalso efficient to compute and decode from, since the\nfirst and last steps only allow intra-word attention.\nFor a tokenizer-free model, roughly 80% of the\nmemory bottleneck4 is spent on tokens from one\nword attending to tokens on another word, which\nwe contest is of questionable importance relative\nto the overhead incurred.\nFormally, we begin with a sequence of words\nw0, w1, . . . , wn each of which is comprised of an\nordered set of base units (character/bytes) wi =\nc0\ni , c1\ni , . . . , cmi\ni where mi + 1 is the length of the\nith word. The task is autoregressive language\nmodeling, i.e. given the previously seen words\nw0, w1, . . . , wi−1 as well as the previously seen\nunits in wi (the current word): c0\ni , c1\ni , . . . , cj−1\ni pre-\ndict the next unit cj\ni .\nCharacter/byte level models ignore the word-\nboundary and directly model the task as:\ncj\ni = Decoder(c0\n0, . . . , cm0\n0 , c0\n1, . . . , c0\ni , . . . cj−1\ni )\n2This paper uses 4 CLS tokens per word, except in Section\n4.2 where we ablate with 1 CLS per word.\n3This autoregressive decoder can also be replaced by a\nnon-autoregressive transformer which emits the entire word in\nO(1) time. Our initial experiments with such a vanilla setup\nperformed much worse than autoregressive models (in line\nwith prior work), therefore we leave this to future work.\n4In Figure 2, this is the difference in blue attention blocks\ndepicted in Figure between Byte/Char-level models and our\nintra-word attention.\nSubword segmentation maps the base units de-\nterministically into fewer subwords per word, i.e. ,\nwi = c0\ni . . . cmi\ni →s0\ni . . . s\nm′\ni\ni where m′\ni ≤mi, the\nnumber of subwords that the ith word is decom-\nposed into. Following this determinsitc process, a\nsubword model predicts the next subword as:\nsj\ni = Decoder(s0\n0, . . . , s\nm′\n0\n0 , s0\n1, . . . , s0\ni , . . . sj−1\ni )\nOur end-to-end models instead follow a three-\nstep process to (1) pool base units into a fixed set\nof embeddings per word, (2) autoregressively pre-\ndicting the next word embedding, and (3) autore-\ngressively predicting individual unit embeddings\nper word:\nCLSi = Encoder(c0\ni , c1\ni , . . . , cmi\ni ) (1)\nCLS′\ni = Decoder(CLS0, CLS1, . . . ,CLSi−1) (2)\ncj\ni = Decoder(CLS′\ni\n⨁\nc0\ni , . . . , cj−1\ni ) (3)\nHere, Encoder refers to a transformer BERT-like\nencoder and Decoder refers to a transformer GPT-\nlike decoder. From an implementation standpoint,\nwe prefix a fixed number ( n = 1 or 4 in this pa-\nper) of CLS tokens to every word before passing it\nthrough a transformer encoder. The word-level con-\ntextualized representations obtained on the other\nend are collectively depicted here as wi.\nFigure 2 is a visualization of how our end-to-end\nmodel saves on self-attention computation bottle-\nneck by only allowing intra-word attention at the\nfirst step, before allowing contextualization of in-\nformation across the word boundary in step 2 us-\ning the base decoder model. Finally step 3 again\nrestricts the individual characters/bytes to be pre-\ndicted using only the single word-level predicted\nembeddings.5\n3 Experiments\nThere are numerous NLP tasks that can bene-\nfit from improved tokenization, such as Machine\nTranslation, Question Answering, and Text Clas-\nsification. However, the scope of our preliminary\nanalysis is not to cast a wide net over every down-\nstream application. Instead, we choose to analyze\nin depth the most commonly used pre-training task\nin NLP i.e. language modeling.\nWe pretrain autoregressive language models\nfrom scratch using different tokenizers described in\n5Note that our current implementation has minor devia-\ntions from the shown simplistic figure. Refer to Section 3.4\nfor details.\n9885\nFigure 2: Self-attention visualized across (1) Byte/Char-level models, (2) Subword/Word-level models, and (3) Our\nproposed end-to-end tokenization modules (word encoder; base LM decoder; word decoder) with character base.\nBlue blocks indicate self attention mask. @ symbol indicates a prepended CLS token per word.\nthe previous section, on different datasets described\nin Section 3.2.\n3.1 Models\nWe report results over the following tokenizers:\nSubword: a pretrained BPE vocabulary used by\nGPT-2 and GPT-3.Byte: a pretrained byte-level vo-\ncabulary as implemented in ByT5 Xue et al. (2022).\nCharacter: a corpus-specific vocabulary learnt\nfrom each dataset, with a fallback to UNK for\ncharacters unseen in training. eByte/eChar: Our\nend-to-end tokenized models which begin with the\nabove Byte/Character vocabularies, but are com-\npressed into CLS representations as described in\nSection 2.\nThere can be countless ways to make for a ‘fair’\ncomparison across tokenizers. We train all models\non all datasets for the same number of total epochs.\nWe also focus on letting the models access the same\ncontext window size, i.e. amount of information\navailable to predict the next set of tokens. Different\ntokenizers can use vastly different memory sizes\nto fit the same amount of information. This is\nanalogous to how the same book can be published\nDataset Size Words Chars/\n(MBs) (Mil.) Word\nEnglish 4.7 1.34 5.46\nFrench 5.1 1.55 5.18\nRussian 7.5 1.18 6.39\nNumeracy 6.6 1.35 5.09\nTable 2: Statistics for our language modeling datasets.\nSee Section 3.2 for more details.\nin different font sizes to choose between light and\nbulky books. We control for this information parity\nby fixing the number of characters in the available\ncontext to 192 for each tokenizer and each dataset.\nSubword models will then be allowed to access\n192//N subwords where N is the average number\nof characters per subword.\n3.2 Datasets\nOur proposed method requires access to a word\nboundary signal, which can either be obtained from\na clean natural language corpus, or by running a\npreprocessing pipeline on an unclean corpus to fil-\n9886\nTokenizer Acc Mem Params Acc Mem Params Acc Mem Params\n(%) (GBs) (Mil.) (%) (GBs) (Mil.) (%) (GBs) (Mil.)\nLanguage English French Russian\nSubword 14.37 0.55 76.8 41.20 1.50 76.8 8.31 1.49 76.8\nByte 13.69 0.53 25.7 17.39 0.54 25.7 12.76 0.53 25.7\nChar 13.68 0.54 26.3 16.95 0.53 25.7 10.01 0.54 26.1\neByte 44.17 3.84 38.7 46.44 6.01 38.7 35.00 4.92 38.7\neChar 42.94 2.94 39.2 47.06 3.59 38.7 37.15 3.95 39.0\nTable 3: Word Prediction Accuracies (Acc %) for different languages and tokenizers. See Section 4.1 for details.\nter out nonlinguistic tokens such as URLs or meta-\ndata. We chose the former to avoid confounding\nour results with a layer of preprocessing decisions.\nTherefore, our datasets are smaller but cleaner than\nthe large-scale mC4 and OSCAR datasets typically\nused for training large language models.\nOur choice of languages depended on the avail-\nability of a large enough corpus of clean data. We\nalso deliberately avoid Chinese and Japanese cor-\npora since segmenting them into words would re-\nquire an additional, possibly confounding step of\nsegmentation through an off-the-shelf model.\nConcretely, here are the four datasets we pre-\ntrain and evaluate our language models on:\n1. English: We randomly sample 10,000\nparagraphs from the comprehensions of\nSQuAD2.0 (Rajpurkar et al., 2016) dataset.\n2. French: We randomly sample 10,000\nparagraphs from the comprehensions of\nSQuAD_FR (Cattan et al., 2021) dataset.\n3. Russian: We randomly sample 10,000 para-\ngraphs from the reading passages of the\nSberQuAD (Efimov et al., 2020) dataset.\n4. Numeracy: We sample 60,000 rows of\nnumber-annotated sentences from Wiki-\nConvert (Thawani et al., 2021a), itself derived\nfrom the English Wikipedia. The task is to\nestimate these numbers approximately using\nthe preceding words as context.\nTable 2 presents statistics for the datasets that we\nuse. The average dataset consists of 7.4M charac-\nters (676 unique) and 1.4M words (102k unique).\n3.3 Metrics\nSince the models have different vocabularies, we\ncan not compare their perplexity scores. Instead,\nwe fix the number of context to be exactly192 char-\nacters and report the accuracy of predicting the next\nword (over held-out validation data from the same\ncorpus as the training set). When estimating num-\nbers, we report magnitude-based metrics that are\ntypically reported in the literature (Thawani et al.,\n2021a; Berg-Kirkpatrick and Spokoyny, 2020):\nthe order-of-magnitude Exponent Accuracy (EAcc:\nwhether the number of digits are the same in the\nground truth and predicted number) and Median\nAbsolute Percentage Error (MdAPE: median of\n100|x −y|/y where x is the prediction and y is the\nground truth number).\n3.4 Implementation\nEvery model (using a different tokenizer) is pre-\ntrained from scratch on every dataset described\nabove. We report the aforementioned metrics on\nthe individual test set from each corpus. Our\nbase language model is a decoder-only transformer\ncalled minGPT6 with 8 layers. For our end-to-end\nmodels, the main language model (Step 2) remains\nthe same - with 8 layers like the others, whereas the\nword-encoder (Step 1) and word-decoder (Step 3)\nare both shallow transformers (encoder and decoder\nrespectively) with 2 layers each. They use padding\ntokens to make each word of equal length for ease\nof training. We use trained absolute positional em-\nbeddings for all models, and the end-to-end models\nuse it thrice - one for each step. We pretrain all\nmodels on all datasets from scratch for 100 epochs.\nWe set the learning rate to 0.0001, batch size\nto 2, and block size to 192. We used AdamW as\nour optimizer and trained our models on NVIDIA\nA100-PCIe-40GB GPUs. With this configuration,\ntraining each model variant for 100 epochs took an\naverage of 52 hours.\n6https://github.com/karpathy/minGPT\n9887\nLang Tok 1 CLS 4 CLS ∆% ∆ Mem\nen eByte 31 44 42% 0.02\nen eChar 29 43 48% 0.02\nfr eByte 31 46 48% 0.02\nfr eChar 34 47 38% 0.02\nru eByte 26 35 35% 0.02\nru eChar 29 37 28% 0.02\nTable 4: Word Prediction Accuracies for different rep-\nresentative power (number of prefix CLS tokens) per\nword in our end-to-end byte/char-tokenized (Tok) mod-\nels. Up to 45% higher prediction scores are available\nfor a marginal increase in memory (Mem in GBs) of\nabout 20 MBs. See Section 4.2 for details.\n4 Results\n4.1 Main Results\nOur main results are summarized in Table 3. Next\nword prediction accuracies over different datasets\nshow that given a fixed context window, our end-\nto-end tokenized language models perform much\nbetter (up to 300% from 14% to 44% on English)\non all datasets than both the default BPE subwords\nas well as the tokenizer-free character and byte\nmodels. This does come at a doubling of GPU\nmemory requirements, due to the additional word-\nlevel modules in our architecture.\n4.2 Representation Power\nHere we ablate the representative power available\nfor word-pooling of character- or byte-level embed-\ndings. This hyperparameter is controlled simply by\nadding a different (yet fixed) number of prefix CLS\ntokens per word before encoding via a transformer.\nTable 4 shows the word prediction accuracies and\nrelative jumps when the number of prefix CLS to-\nkens per word is increased from 1 to 4. We notice\na huge jump for every model, with the trade-off in\nsequence description length. Note, however, that\nthe memory usage does not jump by more than 20\nMBs. Similarly, the number of parameters also in-\ncreases (not shown in table) by only 300K (0.7%)\nfor both eByte and eChar models.\n4.3 Predicting Rare Words\nOne of the primary motivations for subword tok-\nenization is their ability to compositionally create\nrarer words using other frequently occurring sub-\nwords. Wolleb et al. (2023) recently show that such\ncompositionality is a significant contribution to the\nTokenizer Rare Frequent\nSubword 0.11 7.20\nByte 0.00 4.36\nChar 0.28 9.84\neByte 5.90 42.90\neChar 6.78 44.17\nTable 5: Case study: Word Prediction Accuracies for\nRussian across tokenizers, stratified by Rare and Fre-\nquent words. See Section 4.3 for details.\nTokenizer % Num ↑ EAcc ↑ MdAPE ↓\nSubword 20.0 44.8 95.72\nByte 39.9 40.5 99.00\nChar 42.8 46.6 92.5\neByte 47.5 49.9 88.37\neChar 46.7 45.6 90.0\nTable 6: Number Estimation results on Numeracy\ndataset across tokenizers. % Num = the percentage\nof times the model predicts a number, over which the\nnext two metrics are calculated. EAcc = Exponent Ac-\ncuracy. MdAPE = Median Absolute Percentage Error.\nSee Section 4.4 for details.\nempirical performance gains achieved by subword\nmodels. Hence, we report in Table 5 the word pre-\ndiction accuracies for rare words (those seen less\nthan 10 times in the training dataset) as well as\nfrequent ones (those seen more than 45 times). We\nfind our end-to-end models outperform by a factor\nof 5-7 on frequent words and over 30 times on rare\nwords!\n4.4 Number Estimation\nWe further evaluate a representative subset of to-\nkenizers on WikiConvert number estimation task.\nTable 6 again reports that the ability of end-to-end-\ntokenized eByte/eChar is far better than both sub-\nword as well as Byte/Char models.\n[]\n5 Efficiency Analysis\nHere, we determine the theoretical training and\ninference/generation speed-up accessible by com-\npressing words using our end-to-end tokenizer as\nopposed to tokenizer-free methods, while also com-\nparing against the more efficient subword models.\n9888\nDataset En Fr Ru Ru (rare) Ru (freq) Numeracy [3]\nMetric Next Word Prediction Accuracy EAcc↑ MdAPE↓\nSubword 14.37 41.20 8.31 0.11 7.20 44.8 95.7\nByte 13.69 17.39 12.76 0.00 4.36 40.5 99.0\nChar 13.68 16.95 10.01 0.28 9.84 46.6 92.5\neByte 44.17 46.44 35.00 5.90 42.90 49.9 88.4\neChar 42.94 47.06 37.15 6.78 44.17 45.6 90.0\n5.1 Training Speed-up\nAssume M total memory budget available (say,\nin GBs) and a context window of T characters\nper batch. Also assume the tokenizer-free model\n(henceforth referred to as base/baseline) is a de-\ncoder transformer with L layers and D dimensions.\nThe memory footprint would most significantly de-\npend on the number of activations stored 7 which\ncan be estimated as M = LDBT 2 where B is the\nbatch size. Given a fixed memory budget of M and\nrequired context size of T characters, we can find\nour optimal batch size as:\nB = M\nLDT 2\nAssuming the training corpus comprises of N\ncharacters, the number of training iterations re-\nquired is:\nX = N\nBT = NDLT\nM\nNext, for subwords, a similar batch size can be\nestimated as:\nB′= M\nLDT 2/s2\nwhere s is the number of characters per subword\n(roughly 2.8 for our three languages). Substituting\nto find the number of training steps:\nX′= N\nB′T = NDLT\nMs2\nThe training speed-up of a subword model is there-\nfore estimated to be X/X′= s2 = 7.8x.\nFinally, we calculate the analogous number of\ntraining steps required for one epoch of our end-\nto-end character-tokenized model. We assume L/4\nword-encoder layers, L primary LM (word level)\nlayers, and L/4 word-decoder layers for simplicity\n7The other components such as parameter variables and\nbackpropagated gradients can be approximated away.\n(this is our default setup in this paper). Let B′′be\nthe optimal batch size that we wish to calculate and\nc be the average number of characters per word\n(roughly 5.5 for English). Note that we retain T\ncharacters as our context window, therefore the\naverage number of words per batch sequence will\nbe T/c. The memory footprint of activations would\nthen be (LDB′′Tc)/4 for the word encoder (and\nsame for word decoder) and (LDB′′T2)/(c2) for\nthe primary (word level) language model.\nThis leads to the optimal batch size:\nB′′= M\nLDT (c/2 + T/c2)\nand the number of training steps to be:\nX′′= N\nB′′T = NDL\nM (c/2 + T/c2)\nFinally, we estimate our proposed speedup in\ntotal training time as\nX/X′′= T\nc/2 + T/c2\nPlugging in c = 5.5 as a conservative number of\ncharacters per word8 and T = 192 context window\nlength, we get a 6.8x speed-up in training steps,\nwhich is only marginally less than the subword\nspeed-up (7.8x) relative to character level langauge\nmodel.\n5.2 Generation Speed-up\nAnother unique advantage of our end-to-end tok-\nenized model is in generation, which is also par-\nallelized per word. A character/byte model must\ngenerate one token at a time, then feed the predicted\ntoken back into the input and run the forward loop\nagain for autoregressively generating the next. As-\nsuming the L layers of a decoder take t seconds\nfor the forward iteration of GPT-like decoder, the\ngeneration speed for such a character based model\nwill be 1/t characters per second.\n8Real values: En 5.5, Fr 5.2, Ru 6.4.\n9889\nMethod Citation Compress? Generate? Learnt? Word level?\nGPT Radford et al. (2018) Lookup Yes No Yes\nByT5 Xue et al. (2022) None Yes No No\nMANTa Godey et al. (2022) Segment Yes Yes No\nRetVec Bursztein et al. (2023) Conv. No Yes Yes\nFastText Bojanowski et al. (2017) Conv. No Yes Yes\nELMo Peters et al. (2018) Conv. No Yes Yes\nCharBERT El Boukkouri et al. (2020) Conv. No Yes Yes\nCharFormer Tay et al. (2021) Conv. No Yes No\nLOBEF-nCF Sreedhar et al. (2022) None Yes Yes No\nLOBEF-WSF Sreedhar et al. (2022) None Yes Yes Yes\nCANINE Clark et al. (2022) Conv. Yes Yes No\nMegaByte Yu et al. (2023) Dense Yes Yes No\nOurs Attn. Yes Yes Yes\nTable 7: Literature Review of existing tokenization methods along several dimensions. Compress? Is the input\nstring chunked into bigger units? Generate? Whether or not the model can generate new unseen tokens? Learnt?\nIs the tokenization learnt end-to-end with other parameters? Word Boundary? Is the word boundary considered or\ntreated as just another token? Conv: Convolution. Attn: Attention.\nSubword models benefit from having tokens that\nare longer (roughly 2.8 characters/subword for the\nthree languages we consider), therefore they can\ngenerate at a speed of 2.8/t characters per second.\nWith a very coarse assumption, our end-to-end\ncharacter model with L/4 word-encoder layers and\nL decoder layers (ignore the L/4 word-decoder\nlayers for now) will require 5t/4 seconds to gener-\nate the representation of one word at a time. The\nnext step can then be parallelized (with trade-off in\nmemory consumption) to both autoregressively go\non generating the next word representation in an-\nother 5t/4 seconds, as well as autoregressively gen-\nerating one character at a time using this predicted\nword representation. This word-level decoder that\nemits characters currently hasL/4 layers so a crude\nassumption would mean t/4 seconds per character.\nTherefore, at steady state, the word-decoder will\ntake 5.5t/4 seconds to generate the average 5.5\ncharacters per word, while the next word will be\nready for decoding simultaneously in just 5t/4 sec-\nonds. Thus, the generation speed is 4/t characters\nper second, i.e. roughly 50% better than subwords\nand four times as fast as tokenizer-free models.\n6 Related Work\nSome recent work has challenged the subword tok-\nenization schemes. Table 7 highlights the different\nkinds of tokenizations existing in prior work and\npositions our work uniquely among them.\nCharacter/Byte-level ByT5 (Xue et al., 2022),\nCANINE (Clark et al., 2022), and SubChar (Si\net al., 2021) propose using very small fixed-length\nunits such as characters, bytes, or glyph strokes in-\nstead of dynamic-length subwords or words. This\noften comes at the expense of larger sequence\nlengths and more compute requirements, especially\nfor a transformer architecture which typically has\na complexity of O(n2) in number of input tokens.\nBeyond word level CodeBPE (Chirkova and\nTroshin, 2022) and Multi Word Expressions (Ku-\nmar and Thawani, 2022; Zaninello and Birch, 2020;\nRikters and Bojar, 2017) show promise in yet larger\ntokens that cross word boundaries, e.g., a vocab-\nulary with single tokens for the strings “for i in\nrange” or “New York City” respectively.\nVisual segmentation Yet another line of work\n(Rust et al., 2022; Salesky et al., 2021) renders\ntext as images before feeding them to CNNs, do-\ning away with tokenization altogether and showing\ngains in robustness to spelling or printing errors.\nLearnt subword segmentation Finally, some\nmethods (Mofijul Islam et al., 2022; Kaushal and\nMahowald, 2022; Pinter et al., 2021; Tay et al.,\n2021; Provilkov et al., 2020; Wang et al., 2021)\nparameterize the process of tokenization by pooling\ncharacter n-grams or randomly choosing one of the\nmany ways to segment a given word.\n9890\nA recent preprint on machine translation by\nSreedhar et al. (2022) proposes a method called\nWSF, perhaps closest to ours, except that they only\nuse the word boundary fusion at encoder stage. Our\nindependent analysis focuses on language model-\ning instead and also generates text in parallel using\nend-to-end attention based tokenization.\n7 Conclusion\nSubword tokenization is efficient but too rigid and\ndeterministic. Character/Byte-level models on the\nother hand are too expressive, which leads to inef-\nficient training and inference. We propose a word-\nboundary-informed tokenizer that efficiently and\nrobustly performs language modeling in a hierar-\nchical, end-to-end, learned model. We show that\nit outperforms by over 300% both extremes: sub-\nwords and character/byte models. We also analyze\nits trade-offs in training and inference efficiency.\nDespite its many flaws including reliance on a word\nboundary signal and moderate efficiency as well as\nmoderate expressiveness, we expect this prelimi-\nnary study to pose an interesting trade-off tokeniza-\ntion for truly end-to-end language modeling.\nOur code is released on Github.\n8 Limitations\nWe repeatedly highlight our work’s limitations\nthroughout the paper: our proposed end-to-end to-\nkenization is neither faster than subwords nor as\nexpressive as character/byte-level models. Instead,\nwe propose it as a reasonable trade-off between the\ntwo extremes.\nWe also do not cast a wide net on many down-\nstream tasks like machine translation or question\nanswering, hence lack a comparison with other\nmodels like CharFormer, CANINE, and Local Byte\nFusion. We instead focus solely on the intrinsic\nmetrics of language modeling, across multiple lan-\nguages as well as on number estimation.\nOur choice of languages is limited by availability\nof high quality ‘natural language’ corpora, unlike\nthe internet-scale language modeling data which\nwe observed are filled with examples of long strings\nlike URLs and unsegmented strings. We do not\nuse preprocessing pipelines (except simple punc-\ntuation cleanup) to avoid confounding results with\nthe choice of such heuristics. This unfortunately\nprevents us from experimenting with other high\nresource languages like Chinese and Japanese, cor-\npora of which do not implicitly have a whitespace\nboundary signal.\nTo summarize, we concede that our proposed\nend-to-end models are not yet ready for adoption\nat scale with large language models trained on raw\ninternet data. However, we expect our analyses\nto encourage insightful conversation in the com-\nmunity about (1) the spectrum between subwords\nand character/byte models, as well as on (2) the\nrole of word boundary as a meaningful signal in\ntokenization and language modeling.\n9 Acknowledgements\nThis work was funded by the Defense Ad-\nvanced Research Projects Agency with award\nN660011924033. The authors acknowledge the\nCenter for Advanced Research Computing (CARC)\nat the University of Southern California for provid-\ning computing resources that have contributed to\nthe research results reported within this publication.\nWe also thank Google for their generous support.\nWe appreciate the anonymous reviewers at EMNLP\n2023 for helping us refine this paper.\nReferences\nTaylor Berg-Kirkpatrick and Daniel Spokoyny. 2020.\nAn empirical investigation of contextualized number\nprediction. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4754–4764, Online. Association for\nComputational Linguistics.\nBenedikt Boecking, Naoto Usuyama, Shruthi Bannur,\nDaniel C Castro, Anton Schwaighofer, Stephanie Hy-\nland, Maria Wetscherek, Tristan Naumann, Aditya\nNori, Javier Alvarez-Valle, et al. 2022. Making\nthe most of text semantics to improve biomedical\nvision–language processing. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23–27, 2022, Proceedings, Part XXXVI,\npages 1–21. Springer.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the associa-\ntion for computational linguistics, 5:135–146.\nElie Bursztein, Marina Zhang, Owen Vallis, Xinyu Jia,\nand Alexey Kurakin. 2023. Retvec: Resilient and\nefficient text vectorizer.\nOralie Cattan, Christophe Servan, and Sophie Rosset.\n2021. On the Usability of Transformers-based mod-\nels for a French Question-Answering task. In Recent\nAdvances in Natural Language Processing (RANLP),\nVarna, Bulgaria.\nNadezhda Chirkova and Sergey Troshin. 2022.\nCodebpe: Investigating subtokenization options for\n9891\nlarge language model pretraining on source code. In\nDeep Learning for Code Workshop.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10:73–91.\nPavel Efimov, Andrey Chertok, Leonid Boytsov, and\nPavel Braslavski. 2020. Sberquad – russian reading\ncomprehension dataset: Description and analysis. In\nExperimental IR Meets Multilinguality, Multimodal-\nity, and Interaction, pages 3–15. Springer Interna-\ntional Publishing.\nHicham El Boukkouri, Olivier Ferret, Thomas Lavergne,\nHiroshi Noji, Pierre Zweigenbaum, and Jun’ichi Tsu-\njii. 2020. CharacterBERT: Reconciling ELMo and\nBERT for word-level open-vocabulary representa-\ntions from characters. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 6903–6915, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nNathan Godey, Roman Castagné, Éric de la Clergerie,\nand Benoît Sagot. 2022. MANTa: Efficient gradient-\nbased tokenization for end-to-end robust language\nmodeling. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, pages 2859–\n2870, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nAyush Kaushal and Kyle Mahowald. 2022. What do\ntokens know about their characters and how do they\nknow it? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2487–2507, Seattle, United States.\nAssociation for Computational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66–75,\nMelbourne, Australia. Association for Computational\nLinguistics.\nDipesh Kumar and Avijit Thawani. 2022. BPE beyond\nword boundary: How NOT to use multi word expres-\nsions in neural machine translation. In Proceedings\nof the Third Workshop on Insights from Negative\nResults in NLP, pages 172–179, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nSabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky,\nColin Raffel, Manan Dey, Matthias Gallé, Arun Raja,\nChenglei Si, Wilson Y Lee, Benoît Sagot, et al. 2021.\nBetween words and characters: A brief history of\nopen-vocabulary modeling and tokenization in nlp.\narXiv preprint arXiv:2112.10508.\nMd Mofijul Islam, Gustavo Aguilar, Pragaash Pon-\nnusamy, Clint Solomon Mathialagan, Chengyuan Ma,\nand Chenlei Guo. 2022. A vocabulary-free multilin-\ngual neural tokenizer for end-to-end task learning.\nIn Proceedings of the 7th Workshop on Representa-\ntion Learning for NLP, pages 91–99, Dublin, Ireland.\nAssociation for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nYuval Pinter, Amanda Stent, Mark Dredze, and Jacob\nEisenstein. 2021. Learning to look inside: Augment-\ning token-based encoders with character-level infor-\nmation.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nMat¯ıss Rikters and Ondˇrej Bojar. 2017. Paying attention\nto multi-word expressions in neural machine transla-\ntion. In Proceedings of Machine Translation Summit\nXVI: Research Track, pages 86–95, Nagoya Japan.\nPhillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Eliz-\nabeth Salesky, Miryam de Lhoneux, and Desmond\nElliott. 2022. Language modelling with pixels.\nElizabeth Salesky, David Etter, and Matt Post. 2021.\nRobust open-vocabulary translation from visual text\nrepresentations. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 7235–7252, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nChenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao\nQi, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun.\n2021. Shuowen-jiezi: Linguistically informed to-\nkenizers for chinese language model pretraining.\narXiv preprint arXiv:2106.00400.\n9892\nMakesh Narsimhan Sreedhar, Xiangpeng Wan, Yu-Jie\nCheng, and Junjie Hu. 2022. Local byte fusion for\nneural machine translation. ArXiv, abs/2205.11490.\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta,\nHyung Won Chung, Dara Bahri, Zhen Qin, Simon\nBaumgartner, Cong Yu, and Donald Metzler. 2021.\nCharformer: Fast character transformers via gradient-\nbased subword tokenization. In International Con-\nference on Learning Representations.\nAvijit Thawani, Jay Pujara, and Filip Ilievski. 2021a.\nNumeracy enhances the literacy of language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6960–6967, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021b. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nXinyi Wang, Sebastian Ruder, and Graham Neubig.\n2021. Multi-view subword regularization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n473–482, Online. Association for Computational Lin-\nguistics.\nBenoist Wolleb, Romain Silvestri, Giorgos Vernikos,\nLjiljana Dolamic, and Andrei Popescu-Belis. 2023.\nAssessing the importance of frequency versus com-\npositionality for subword-based tokenization in nmt.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nLili Yu, Dániel Simig, Colin Flaherty, Armen\nAghajanyan, Luke Zettlemoyer, and Mike Lewis.\n2023. Megabyte: Predicting million-byte se-\nquences with multiscale transformers. arXiv preprint\narXiv:2305.07185.\nAndrea Zaninello and Alexandra Birch. 2020. Multi-\nword expression aware neural machine translation. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 3816–3825, Marseille,\nFrance. European Language Resources Association.\n9893",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8521801233291626
    },
    {
      "name": "Language model",
      "score": 0.7777417898178101
    },
    {
      "name": "Word (group theory)",
      "score": 0.7162354588508606
    },
    {
      "name": "Byte",
      "score": 0.7124457359313965
    },
    {
      "name": "Lexical analysis",
      "score": 0.7112877368927002
    },
    {
      "name": "Natural language processing",
      "score": 0.5311248302459717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5061411261558533
    },
    {
      "name": "Heuristic",
      "score": 0.43195995688438416
    },
    {
      "name": "Parsing",
      "score": 0.42141789197921753
    },
    {
      "name": "Speech recognition",
      "score": 0.3757029175758362
    },
    {
      "name": "Linguistics",
      "score": 0.16995206475257874
    },
    {
      "name": "Programming language",
      "score": 0.12189227342605591
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}