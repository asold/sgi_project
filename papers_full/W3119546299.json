{
  "title": "AraGPT2: Pre-Trained Transformer for Arabic Language Generation",
  "url": "https://openalex.org/W3119546299",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5050749439",
      "name": "Wissam Antoun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050198833",
      "name": "Fady Baly",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088085383",
      "name": "Hazem Hajj",
      "affiliations": [
        "American University of Beirut"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098302716",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3092558113",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3118440692",
    "https://openalex.org/W2971016465",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W3128029819",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W1990501283",
    "https://openalex.org/W3121458077",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2945785363",
    "https://openalex.org/W2975901202",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3119349118",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W2562238432",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W3118942176",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W2963026768"
  ],
  "abstract": "Recently, pre-trained transformer-based architectures have proven to be very efficient at language modeling and understanding, given that they are trained on a large enough corpus. Applications in language generation for Arabic are still lagging in comparison to other NLP advances primarily due to the lack of advanced Arabic language generation models. In this paper, we develop the first advanced Arabic language generation model, AraGPT2, trained from scratch on a large Arabic corpus of internet text and news articles. Our largest model, AraGPT2-mega, has 1.46 billion parameters, which makes it the largest Arabic language model available. The Mega model was evaluated and showed success on different tasks including synthetic news generation, and zero-shot question answering. For text generation, our best model achieves a perplexity of 29.8 on held-out Wikipedia articles. A study conducted with human evaluators showed the significant success of AraGPT2-mega in generating news articles that are difficult to distinguish from articles written by humans. We thus develop and release an automatic discriminator model with a 98% percent accuracy in detecting model-generated text. The models are also publicly available, hoping to encourage new research directions and applications for Arabic NLP.",
  "full_text": "ARAGPT2: Pre-Trained Transformer\nfor Arabic Language Generation\nWissam Antoun and Fady Baly and Hazem Hajj\nAmerican University of Beirut\n{wfa07, fbg06, hh63}@aub.edu.lb\nAbstract\nRecently, pre-trained transformer-based archi-\ntectures have proven to be very efﬁcient at\nlanguage modeling and understanding, given\nthat they are trained on a large enough cor-\npus. Applications in language generation for\nArabic are still lagging in comparison to other\nNLP advances primarily due to the lack of\nadvanced Arabic language generation models.\nIn this paper, we develop the ﬁrst advanced\nArabic language generation model, AraGPT2,\ntrained from scratch on a large Arabic corpus\nof internet text and news articles. Our largest\nmodel, ARAGPT2- MEGA , has 1.46 billion pa-\nrameters, which makes it the largest Arabic\nlanguage model available. The MEGA model\nwas evaluated and showed success on different\ntasks including synthetic news generation, and\nzero-shot question answering. For text gener-\nation, our best model achieves a perplexity of\n29.8 on held-out Wikipedia articles. A study\nconducted with human evaluators showed the\nsigniﬁcant success of AraGPT2-mega in gen-\nerating news articles that are difﬁcult to dis-\ntinguish from articles written by humans. We\nthus develop and release an automatic discrim-\ninator model with a 98% percent accuracy in\ndetecting model-generated text. The models\nare also publicly available1, hoping to encour-\nage new research directions and applications\nfor Arabic NLP.\n1 Introduction\nFew years ago, Natural language processing\n(NLP) was revolutionized with the introduction\nof multi-head self-attention transformer architec-\nture (Vaswani et al., 2017). The transformer\nachieved superior performance compared to recur-\nrent neural networks several NLP tasks including\nmachine translation, sentence classiﬁcation with\n1Pretrained variants of ARAGPT2 (base, medium,\nlarge, mega) and discriminator are publicly available on\ngithub.com/aub-mind/arabert/tree/master/aragpt2\nBERT (Devlin et al., 2019), and ELECTRA (Clark\net al., 2020b), and sentence completion with GPT-\n2 (Radford et al., 2019), GROVER (Zellers et al.,\n2019), and CTRL (Keskar et al., 2019). Recent\nworks have shown that larger models pre-trained\non larger datasets can further improve performance\ni.e. RoBERTa (Liu et al., 2019), and XLM-R (Con-\nneau et al., 2019).\nOn the other hand, work on Arabic language\nmodeling has mostly targeted natural language\nunderstanding (NLU) by pre-training transformer-\nbased models using the Masked Language Model-\ning (MLM) task i.e. ARABERT (Antoun et al.,\n2020a). In contrast, Arabic text generation or\ncausal language modeling hasn’t received much\nattention. Few works such as hULMonA (ElJundi\net al., 2019) used next word prediction as a pre-\ntraining task in for transfer learning in Arabic text\nclassiﬁcation. (Khooli, 2020) and (Doiron, 2020)\nleveraged the existing GPT2 English model and\nadapted it for Arabic using text from the Arabic\nWikipedia dumps, which is sub-optimal for Arabic.\nIn this paper, the ﬁrst advanced language gener-\nation models built from the grounds up on Arabic\nlanguage have been developed. The process of pre-\ntraining ARAGPT2 , a GPT-2 transformer model\nfor the Arabic language is described. The model\ncomes in 4 size variants: base (135M2), medium\n(370M), large (792M) and mega (1.46B3), which\nallows the exploration ofARAGPT2 in multiple ap-\nplications with different data availability and com-\nputational constraints. The perplexity measure is\nused to automatically evaluate ARAGPT2 . Fur-\nthermore, a human-based evaluation is provided,\nwhich highlights the ability of ARAGPT2 to de-\nceive human evaluators. Finally, an ARAELEC-\nTRA (Antoun et al., 2020b) based detector is devel-\n2Million Parameters\n3Billion Parameters\narXiv:2012.15520v2  [cs.CL]  7 Mar 2021\noped and released. It is able to consistently identify\nnews articles written by ARAGPT2 . Making such\npowerful models publicly available to the Arabic\nresearch community enables research in rising Ara-\nbic NLP ﬁelds i.e Conversational Agents (Naous\net al., 2020), Detection of Automatic News Gener-\nation Detection (Harrag et al., 2020)...\nOur contributions can be summarized as follows:\n• A methodology to pre-train a billion-size\nGPT2 model on a large-scale Arabic corpus.\n• An automatic discriminator that achieves a\n98% accuracy in detecting model-generated\nsynthetic text.\n• The four variants of ARAGPT2 are released\non popular NLP libraries, along with the auto-\nmatic ARAGPT2 discriminator.\nThe rest of the paper is structured as follows.\nSection 2 provides a concise review of previous\nliterature on Arabic language modeling. Sec-\ntion 3 details the methodology used in developing\nARAGPT2 . Section 4 describes the experimental\nsetup, evaluation procedures and results. In addi-\ntion, the approach to build a machine-generated\ntext discriminator is presented in Section 5. Fi-\nnally, a conclusion of the work and implications\nare mentioned in Section 6.\n2 Related Works\n2.1 English and Non-Arabic Language\nmodeling\nGPT-1 (Radford et al., 2018) showed that Causal\nLanguage Modeling4 is an effective pre-training\ntechnique that improves a model’s generalization\ncapabilities. GPT-2 then showed that using a larger\nmodel trained on a larger dataset surpasses the\nstate-of-the-art of many tasks in a zero-shot setting,\nwhere a model solves a task without receiving any\ntraining on that task. Taking the scaling approach\nto the extreme led to the creation of GPT-3 (Brown\net al., 2020), with 175 billion parameter model,\nalso trained with CLM using terabytes of internet\ntext. GPT-3 explored the idea of few-shot learning,\nwhere a model is given examples from a new task\nas a text prompt, which unlocks new capabilities\nat test time. It was later shown that a carefully\ndesigned GPT-3 prompt allows the model to gener-\nate website designs, scramble/unscramble words...\n4This is the regular Language Modeling objective where\nthe model learns the probability of a word given the previ-\nous context. The CLM acronym is used to distinguish from\nmasked language modeling (MLM).\nThe advantage of scaling model sizes and training\ndatasets comes with drawbacks, particularly the\nhigh computational cost, in addition to the huge\ncorpora required for pre-training. It was estimated\nthat training GPT-2 and GPT-3 costs $43K and\n$4.6M respectively, without any hyper-parameter\ntuning. These drawbacks restricted the availabil-\nity of large pre-trained models to English mainly\nand a handful of other languages i.e. ruGPT35 for\nRussian, and Chinese 1.5B GPT2 (Zhang, 2019).\n2.2 Arabic Language modeling\nWork on Arabic causal language modeling has\nbeen mostly limited to automatic speech recogni-\ntion (ASR) systems. Since the language modeling\ncomponent in ASR systems is a key module that en-\nsures that the output text adheres with the statistical\nstructure of language. Work on Arabic language\nmodels in ASR systems has mostly relied on N-\ngrams language models. (Ali et al., 2014) built\nan N-grams language model (LM) using GALE\ntraining data transcripts of 1.4M words. More re-\ncent work in Arabic ASR implemented a recur-\nrent neural network as an LM, using 130M tokens,\nand achieved a perplexity of 481 compared to 436\nfor a 4-gram LM (Khurana et al., 2019). Hamed\net al. (2017) developed a code-switched Arabic-\nEnglish language model using tri-gram LM and\nprovided performance superior compared to two\nseparate monolingual LMs. The code-switched LM\nwas trained on 2.3M sentences or 13M words and\nachieved a perplexity of 275.\nWith the rising popularity of transfer learning in\nNLP, Arabic CLM was used as a pre-training task\nfor an Arabic universal LM, hULMonA (ElJundi\net al., 2019). The model was then ﬁne-tuned on\ndifferent downstream text classiﬁcation tasks. hUL-\nMonA is a 3 stack of AWD-LSTM6 layers (Howard\nand Ruder, 2018), trained on 600K Wikipedia arti-\ncle pre-segmented using the MADAMIRA Arabic\nmorphological analyzer and disambiguator (Pasha\net al., 2014).\nMasked Language Modeling (MLM) has been\nuseful as a pre-training task for several Arabic NLU\nmodels. Masked Language Modeling (MLM) is\na slightly different objective than CLM that re-\nquires a system to predict a masked word within\na sequence compared to CLM which predicts the\nmissing word at the end of a sequence. MLM\n5https://github.com/sberbank-ai/ru-gpts/\n6ASGD Weight-Dropped LSTM\nwas used in models such as ARABERT (An-\ntoun et al., 2020a), Arabic-BERT (Safaya et al.,\n2020), Arabic-ALBERT7, GigaBERT (Lan et al.,\n2020), MarBERT (Abdul-Mageed et al., 2020), and\nQARiB (Chowdhury et al., 2020). Only two works\nhave attempted to create an Arabic transformer\ncausal language model. Khooli (2020) and Doiron\n(2020) ﬁnetuned the OpenAI GPT2-base model on\nArabic Wikipedia, which was mainly trained on\nEnglish text. Doiron (2020) also continued training\non a collection of dialectal Arabic datasets, in order\nto create a dialectal Arabic GPT2. While this ap-\nproach has shown the capability to generate Arabic\ntext, it is sub-optimal for Arabic and is useful in\ncases where the training data is scarce.\nOur proposed model is hence, the ﬁrst Arabic\ntransformer-based causal language model trained\nfrom scratch on the largest Arabic corpora available\nat the time of writing.\n3 A RAGPT2: Methodology\nARAGPT2 is a stacked transformer-decoder model\ntrained using the causal language modeling objec-\ntive. The model is trained on 77GB of Arabic text.\nARAGPT2 comes in four variants as detailed in\nTable 1, with the smallest model, base, having the\nsame size as ARABERT-base which makes it ac-\ncessible for the larger part of researchers. Larger\nmodel variants (medium, large, xlarge) offer im-\nproved performance but are harder to ﬁne-tune and\ncomputationally more expensive. The ARAGPT2 -\ndetector is based on the pre-trained ARAELEC-\nTRA model ﬁne-tuned on the synthetically gener-\nated dataset. More details on the training procedure\nand dataset are provided in the following sections.\n3.1 Model\nARAGPT2 closely follows GPT2’s variant archi-\ntectures and training procedure. Table 1 shows the\ndifferent model sizes, number of heads, number\nof layers, parameter count, and optimizer used for\neach model variant. All models are trained with\ncontext sizes of 1024 tokens. The LAMB (You\net al., 2019) optimizer is used in the base and\nmedium models only, since it allows using large\nbatch sizes without worrying about training diver-\ngence. Using LAMB and Adam (Kingma and Ba,\n2014) to train the large and mega variants isn’t\npossible on TPUv3 due to the optimizer’s high\nmemory requirements, since memory cost scales\n7https://github.com/KUIS-AI-Lab/Arabic-ALBERT/\nlinearly with the number of parameters. The lim-\nitations were overcome by following the training\nprocedure of the GROVER model (Zellers et al.,\n2019) by using the Adafactor optimizer (Shazeer\nand Stern, 2018), which reduces memory require-\nments by factoring the second-order momentum\nparameters into a tensor product of two vectors.\nThe GROVER architecture was also used instead\nof GPT2’s, in which the layer normalization order\nin the transformer block is changed.\n3.2 Dataset\nThe training dataset is a collection of the publicly\navailable Arabic corpora listed below:\n• The unshufﬂed OSCAR corpus (Ortiz Su´arez\net al., 2020).\n• The Arabic Wikipedia dump from September\n2020.\n• The 1.5B words Arabic Corpus (El-Khair,\n2016).\n• The OSIAN corpus (Zeroual et al., 2019).\n• News articles provided by As-saﬁr newspaper.\nPreprocessing First, the corpus was ﬁltered by\nremoving short documents with less than 3 sen-\ntences, and documents with more than 20% re-\npeated sentences. URLs, emails, and user men-\ntions were also replaced with special tokens. All\ndiacritics, and elongations were removed as well,\nwhile punctuation and non-alphabetic characters\nwere padded with white-spaces. Moreover, the\n‘<|endoftext|>’ token is appended at the\nend of each document. The total dataset size is\n77GB with 8.8B words8. The majority of the train-\ning data is comprised of Arabic news article, which\nis mostly written in MSA. The corpus also contains\na small set of English words i.e. named entities,\nwhich are kept without lower-casing. Subsequently,\na Byte-level byte-pair-encoding (BPE) tokenizer is\ntrained with 64000 vocabulary size on all of our\npreprocessed dataset, using the optimized BPE im-\nplementation from the HuggingFace library (Wolf\net al., 2020). Finally, the BPE encoding is applied\non the preprocessed dataset, which results in a to-\ntal of 9.7M training examples with 1024 sub-word\ntokens each.\n8Word count was done after preprocessing, where white\nspace is inserted before and after punctuations, brackets, num-\nbers... which increased the total word count\nModel Size Architecture Context Size Emb. Size Heads Layers Optimizer\nBase 135M GPT2 1024 768 12 12 LAMB\nMedium 370M GPT2 1024 1024 16 24 LAMB\nLarge 792M GROVER 1024 1280 20 36 Adafactor\nMega 1.46B GROVER 1024 1536 24 48 Adafactor\nTable 1: ARAGPT2 model variants with sizes, architecture and optimizer\nModel Batch Size Learning Rate Steps Time (days) PPL\nBase 1792 1.27e-3 120K 1.5 55.8\nMedium* 80 3e-4 1M 23 45.7\nLarge 256 1e-4 220K 3 36.6\nMega 256 1e-4 780K 9 29.8\nTable 2: A RAGPT2 training details and validation perplexity. * Medium was trained on a TPUv3-8 with a small\nbatch size, since the model was not converging with a large batch size\n4 Experiments and Evaluation\n4.1 Pre-training Setup\nAll models were trained on a TPUv3-128 slice 9\nwith different batch sizes and the total number\nof steps as shown in Table 2. Base and mega\nwere trained for approximately 20 epochs, while\nmedium and large were trained for 10 and 6\nepochs respectively, due to TPU access limitations.\n4.2 Numerical Evaluation\nFor the validation dataset, the Arabic Wikipedia\narticles that were published after August 2020\nwere used, since older articles were included in\nthe September Wikipedia dump. The perplexity\nscore was selected as a numerical evaluation met-\nric since it measures the degree of ’uncertainty’ a\nmodel has assigning probabilities to the test text.\nTable 2 shows that, unsurprisingly, validation per-\nplexity keeps improving with larger model sizes.\nIn fact, the model is still under-ﬁtting the validation\nset from Wikipedia. The generation capabilities of\nthe different variants of ARAGPT2 is illustrated\nthrough the selected examples in Appendix A.\n4.3 Zero-Shot Evaluation\nDuring zero-shot task evaluation, the model is only\ngiven a natural language instruction to motivate\nand ground the task, without any back-propagation\nhappening. The task of searching and ﬁnding the\nbest input prompt, also known as “prompt engineer-\ning”, is hard. Since the search space is practically\ninﬁnite, and the performance is highly sensitive to\nchanges in the prompt. The zero-shot performance\nof ARAGPT2 -Mega is evaluated on two tasks,\n9TPUv3-128 has a total of 2TB of HBM memory with\n16GB per core. TPUs were freely provided by the TFRC\nprogram.\nquestion answering, and translation. ARAGPT2-\nMEGA correctly answers 25% of the trivia ques-\ntions but fails in English-to-Arabic translation. De-\ntails on the datasets, prompts, and evaluation are\npresented in Appendix B.\n4.4 Evaluating the Human Ability to Detect\nMachine-Generated Text\nThe gold standard for evaluating a model’s lan-\nguage generation capability is human evaluation.\nWe presented 74 Arabic-speaking subjects from\nvarious social media with a survey designed to\ntest the average-human ability to distinguish be-\ntween machine-generated and human-written text\nand thus testing the model’s ability to deceive a\nhuman subject. The survey had a total of 8 news\narticles, 4 machine-generated using ARAGPT2 -\nMega and 4 written by humans. Each category\nwas split into long and short text, which allows\nus to test the long-term generation coherency. In\naddition, the human evaluators are allowed to add\njustiﬁcation for each answer.\nThe survey results, Figure 1, show that\nARAGPT2 -Mega successfully fooled approx. 60%\nof the respondents, with longer passages having a\nhigher error rate than short passages. In the pro-\nvided explanations, some subjects relied on punc-\ntuation mistakes, coherence, and repetition issues,\nwhile others spotted factual inaccuracies. However,\nthe results also show that humans were misclassi-\nfying human-written 50% the time (chance level\nperformance), while also citing factual inconsis-\ntencies, grammatical errors, and unusual writing\nstyles10.\nThese surprising results show that ARAGPT2\ncan accurately generate human-like text while\n10Survey results are available on our GitHub repository.\nFigure 1: Survey results showing human error rates on\nmachine generated (left) and human written text (right)\nmaintaining grammatical correctness that can fool\nthe average reader. It should be noted that there ex-\nist some tools, i.e. the Giant Language model Test\nRoom (GLTR) (Gehrmann et al., 2019), that allows\nhumans to study the statistical distributional differ-\nences in text generated by GPT2-based models and\nhuman-written text. Figure 5 in Appendix C dis-\nplays a visualization of token-level information cre-\nated by GLTR with text generated by ARAGPT2\nand on human-written articles.\n5 Automatic Detection of Machine\nGenerated Text\nLarge language models could have a signiﬁcant so-\ncietal impact if used for malicious purposes, such\nas automating the generation of misleading news\narticles, fake reviews, or high-quality phishing\nmessages. The survey in Section 4.4, showcases\nthe failure of the average-human to consistently\ndetect machine-generated text, which motivates\nthe problem of automatic detection of ARAGPT2 -\ngenerated text. Related work on the detection of\nmachine-generated text by Jawahar et al. (2020) in-\ndicates that automatic detectors like the GROVER-\ndetector (Zellers et al., 2019) and the RoBERTA-\ndetector (Solaiman et al., 2019) have better suc-\ncess than human evaluators. In addition, previous\nwork on detecting Arabic GPT2 (Khooli, 2020)\nauto-generated tweets, achieved 98.7% accuracy,\nby ﬁne-tuning an ARABERTv0.1 (Antoun et al.,\n2020a) based classiﬁer (Harrag et al., 2020).\nOur detector is based on the pre-trained ARA-\nELECTRA (Antoun et al., 2020b) model, which\nwe ﬁne-tuned on a dataset created by combining\n1500 human-written news articles, with 1500 ar-\nticles generated by ARAGPT2 -Mega. For article\ngeneration, we only provided the model with a\nshort prompt of 25 words. We created two versions\nof the dataset, one with short texts (150 tokens)\nand one with long texts (500 tokens), in order to\nevaluate the impact of the text’s length.\nFine-tuned ARAELECTRA achieves 98.7% and\n94.9% F1-score on long and short text respec-\ntively11, which indicates that longer text is easier\nto detect than short text. The high scores achieved\nby ARAELECTRA can be explained by the fact\nthat machine-generated text tends to be more pre-\ndictable compared to human-written text (see Ap-\npendix C, Fig. 5). The difference in text predictabil-\nity can be easily exploited by a language model to\ndetect machine-generated text. Another contribut-\ning factor is that ARAELECTRA was pre-trained\non the exact same dataset as ARAGPT2.\n6 Conclusion\nARAGPT2 is the ﬁrst advanced Arabic language\ngeneration model based on the transformer archi-\ntecture. The model was trained on the largest pub-\nlicly available collection of ﬁltered Arabic corpora.\nThe model was evaluated using the perplexity mea-\nsure which measures how well a probability model\npredicts a sample. Results show that ARAGPT2\nis able to produce high quality Arabic text that is\ncoherent, grammatically correct and syntactically\nsound.\nIt is important to note thatARAGPT2 , like many\nML models, has ethical implications and can be\nused maliciously i.e. automatic fake news genera-\ntion, modeling the dataset inherent biases... To help\ndetect misuse of the model, a detector model that\nis tasked to detect output generated by ARAGPT2\nis also released. More importantly, our hopes that\npublicly releasing ARAGPT2 will open up doors\nfor new research possibilities for the Arabic NLP\ncommunity.\nAcknowledgments\nThis research was supported by the University Re-\nsearch Board (URB) at the American University\nof Beirut (AUB), and by the TFRC program for\nproviding free access to cloud TPUs. Many thanks\nto As-Saﬁr newspaper for the data access, and also\nthanks to Nick Doiron for the insightful discus-\nsions.\n11The trained model will be publicly available in our repos-\nitory\nReferences\nMuhammad Abdul-Mageed, Chiyu Zhang, Abdel-\nRahim Elmadany, and Lyle Ungar. 2020. To-\nward micro-dialect identiﬁcation in diaglossic and\ncode-switched environments. arXiv preprint\narXiv:2010.04900.\nAhmed Ali, Yifan Zhang, Patrick Cardinal, Najim Da-\nhak, Stephan V ogel, and James Glass. 2014. A com-\nplete kaldi recipe for building arabic speech recogni-\ntion systems. In 2014 IEEE spoken language tech-\nnology workshop (SLT), pages 525–529. IEEE.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020a.\nArabert: Transformer-based model for arabic lan-\nguage understanding. In LREC 2020 Workshop Lan-\nguage Resources and Evaluation Conference 11–16\nMay 2020, page 9.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020b.\nAraelectra: Pre-training text discriminators for ara-\nbic language understanding.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nShammur Absar Chowdhury, Ahmed Abdelali, Ka-\nreem Darwish, Jung Soon-Gyo, Joni Salminen, and\nBernard J. Jansen. 2020. Improving Arabic text cate-\ngorization using transformer training diversiﬁcation.\nIn Proceedings of the Fifth Arabic Natural Language\nProcessing Workshop, pages 226–236, Barcelona,\nSpain (Online). Association for Computational Lin-\nguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020a. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020b. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. arXiv preprint arXiv:2003.10555.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nNick Doiron. 2020. Making a mini gpt-2 with dialect\nprompts.\nIbrahim Abu El-Khair. 2016. 1.5 billion words arabic\ncorpus. arXiv preprint arXiv:1611.04033.\nObeida ElJundi, Wissam Antoun, Nour El Droubi,\nHazem Hajj, Wassim El-Hajj, and Khaled Shaban.\n2019. hulmona: The universal language model in\narabic. In Proceedings of the Fourth Arabic Natural\nLanguage Processing Workshop, pages 68–77.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M Rush. 2019. Gltr: Statistical detection\nand visualization of generated text. arXiv preprint\narXiv:1906.04043.\nInjy Hamed, Mohamed Elmahdy, and Slim Abdennad-\nher. 2017. Building a ﬁrst language model for code-\nswitch arabic-english. Procedia Computer Science,\n117:208–216.\nFouzi Harrag, Maria Dabbah, Kareem Darwish, and\nAhmed Abdelali. 2020. Bert transformer model for\ndetecting Arabic GPT2 auto-generated tweets. In\nProceedings of the Fifth Arabic Natural Language\nProcessing Workshop, pages 207–214, Barcelona,\nSpain (Online). Association for Computational Lin-\nguistics.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nGanesh Jawahar, Muhammad Abdul-Mageed, and\nLaks Lakshmanan, V .S. 2020. Automatic detection\nof machine generated text: A critical survey. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 2296–2309,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nAbed Khooli. 2020. gpt2-small-arabic.\nSameer Khurana, Ahmed Ali, and James Glass. 2019.\nDarts: Dialectal arabic transcription system. arXiv\npreprint arXiv:1909.12163.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nGigabert: Zero-shot transfer learning from english\nto arabic. In Proceedings of The 2020 Conference on\nEmpirical Methods on Natural Language Process-\ning (EMNLP).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHussein Mozannar, Elie Maamary, Karl El Hajal, and\nHazem Hajj. 2019. Neural Arabic question answer-\ning. In Proceedings of the Fourth Arabic Natu-\nral Language Processing Workshop, pages 108–118,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTarek Naous, Christian Hokayem, and Hazem Hajj.\n2020. Empathy-driven Arabic conversational chat-\nbot. In Proceedings of the Fifth Arabic Natu-\nral Language Processing Workshop , pages 58–68,\nBarcelona, Spain (Online). Association for Compu-\ntational Linguistics.\nPedro Javier Ortiz Su´arez, Laurent Romary, and Benoˆıt\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1703–1714, Online. Association for Computational\nLinguistics.\nArfath Pasha, Mohamed Al-Badrashiny, Mona Diab,\nAhmed El Kholy, Ramy Eskander, Nizar Habash,\nManoj Pooleery, Owen Rambow, and Ryan Roth.\n2014. MADAMIRA: A fast, comprehensive tool\nfor morphological analysis and disambiguation of\nArabic. In Proceedings of the Ninth International\nConference on Language Resources and Evalua-\ntion (LREC’14), pages 1094–1101, Reykjavik, Ice-\nland. European Language Resources Association\n(ELRA).\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Ope-\nnAI.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\nJ¨orgrg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Proceedings of the Eight Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’12), Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30:5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learn-\ning: Training bert in 76 minutes. arXiv preprint\narXiv:1904.00962.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in neural information processing\nsystems, pages 9054–9065.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. Osian: Open source inter-\nnational arabic news corpus-preparation and integra-\ntion into the clarin-infrastructure. In Proceedings\nof the Fourth Arabic Natural Language Processing\nWorkshop, pages 175–182.\nZhibo Zhang. 2019. Gpt2-ml: Gpt-2 for multiple\nlanguages. https://github.com/imcaspar/\ngpt2-ml.\nA Generated Samples from ARAGPT2\nPrompt \nأحدث حصول لقاح شركة \" أوكسفورد \" البريطانية على الموافقة في موطنه ، حالة من الارتياح في العالم ، نظرا إلى فعاليته في وقاية\nالجسم من وباء  كورونا   ، فضلا عن سعره المناسب وسهولة تخزينه اللافتة مقارنة بالتطعيمات الأخرى المتاحة في السوق الدولية . وبحسب  \nشبكة \" سكاي نيوز \" البريطانية فإن هذا الموافقة على هذا اللقاح تعني الشيء الكثير للعالم وليست مجرد خبر عادي .  \nGenerated Text 1 \nوقالت الشبببكة إن هذه هي المرة الأولى التي يحصببف فيها لقاح مضبباد لكورونا على موافقة منظمة الصببحة العالمية ، كما أنها المرة الأولى  \nالتي تحصببف فيها شببركة بريطانية على مثف هذه الموافقة منذ أكثر من 20  عاما . وأضببافت أن الشببركة حصببلت أيضببا على موافقة إدارة\nالغذاء والدواء الأميركية ( FDA  ) لقاحها  المضببباد لفيروي زيكا الذي تم تطويره بالتعاون مر شبببركة \" سلاكسبببو سبببميثكلاين \" ل دوية \nوشبركة \" سبانوفي أفنتي\" \" الفرنسبية للصبناعات الدوا.ية . وأشبارت إلى أنه لم يتم حتى اعن اععلان عن أي حالات إببابة بكورونا بين \nالبشبر في الولايات المتحدة أو سيرها من دول العالم . ونقلت الشببكة عن المدير التنفيذي لشبركة \" كلاكسبو سبميي كلاين \" قوله ن \" نحن \nسبعداء للغاية بحصبولنا على هذه الموافقة لأن كلي يعني أننا تمكنا من تحقيه هدفنا المتمثف في حماية أكبر عدد ممكن من الناي من اعببا بة  \nبفيروي كورونبا \" . وأضببببا  ن \" نب مبف أن نتمكن من إنتبا  المزيبد  من اللقباحبات المضببببادة لهبذا الفيروي وسيره من الأمراي المعبديبة  \nالمنتشرة في كمير أنحاء العالم خلال السنوات القليلة المقبلة \" \nيذكر أن فيروس كورونا المسبب  لملازمم  الربرا اسوسبل اللانةسبي  حو ألف الةيروسباي اللاب لالبي  اللتام اللانةسبب ج وح لاولف للا  ا ن \nعل  مسبلاو  الاال  مالوماي فيية  عن ملبفر حذا الةيروس وح لرا انلاةال، ج كما ح يولف لالاي  ويا ب أو مدباف ليوع لازل، ك لكن \nمراكم السبببيلرل عل  اسمراض والوياي  منتا بالوحياي الملالفل اسميركي  كاني يف أعلني فب ويي سبببابن من الربببتر اللارع أن لةالا  \nلالريبيا أنلاللا، ربرك    نوفارلاس   السبويسبري  أ بي فاعليلا، فب الوياي  من مرض ملازمم  الربرا اسوسبل اللانةسبي     MERS - CoV  )\nلف  اسلةال والبالغين الذين ياانونمن أعراض رببيت  بععراض احنةلونما ك وكاني منمم  اللبل  الاالمي  يف أعلني فب ربتر سببلامبر \nأيلول المادبب لاسبليل أول لال  وفال نالم  عن االباب  بةيروس كورونا فب المملك  الاربي  السباوفي  ج ليف لاوفب رلل يبل  من الامر  \n69 عاما كان ياانب من عفل أمراض مممن  لراء إلببابلا، بتذا الةيروس ك وكان الرلل يف نةل إل  مسببلارببة  الملص فيلببل اللا لببلببب  \nومركم اسبلاف فب مفين  الرياض باف رباور  بععراض لانةسبي  لافل أفي إل  ف ول، فب بيبوب  ولاوفب باف يومين من إف ال، المسبلاربة   \nك ويال الفكلاور عزء الالوان المفير الاا  للمكلا  اللانةيذع لمللس ومراء اللببببل  لفول مللس اللاااون ور يس الللن  ال ليلي  لمكافل   \nاسمراض المافي  إن، بناء عل  ما أعلنلا، منمم  اللببل  الاالمي  فةف لامي الموافة  عل  لل  ومارل اللببل  بالمملك  الاربي  السبباوفي  \nللامويفحا باللةاح الوايب \nGenerated Text 2 \nوقالت الشببكة إن هذه هي المرة الأولى التي يحصبف فيها لقاح مضباد لفيروي كوفيد- 19 على موافقة منظمة الصبحة العالمية ، ما يعني أنه\nسبيكون متاحا للجمير خلال الأسبابير القليلة المقبلة .وأضبافت أن هذا اللقاح الذي يحمف اسبم \" تاميفلو \" ( Tamiflu  ) تم تطويره بالتعاون\nمر شببركة \" سلاكسببو سببميي كلاين \" ( GlaxoSmithKline  ) وشببركة \" سببانوفي باسببتورSanofi Pasteur  ) الفرنسببية ، وهما\nشببركتان تابعتان لمجموعة \" أسببترا زينيكا \" ( AstraZeneca  ) الدوا.ية .وقال ر.ي\" قسببم الأمراي المعدية في منظمة الصببحة العالمية\nديفيبد نباببارو ن \" نحن مسببببرورون للغبايبة بحصببببول هبذا اللقباح على الموافقبة النهبا.يبة من قببف إدارة الأسبذيبة والعقباقير الأمريكيبة ( FDA   )\nومنظمة الصبحة العالميةWHO  ) \" . وأضبا  ن \" إنه نب  سبار لجمير العاملين في مجال الرعاية الصبحية والمرضبى الذين سبيسبتفيدون\nمن هذا اللقاح \" .من كهته ، قال الر.ي\" التنفيذي لشببركة \" سلاكسببوسببميي كلاين \" ، بروي أرمسببترونن ن \" إننا سببعداء كدا بهذا الخبر  \nالسبار ، ونحن واققون من أننا سبنتمكن من إنتا  كميات كبيرة من هذا اللقاح لتلبية الطلب العالمي المتزايد عليه \" .وتابر قا.لا ن \" لقد عملنا \nبشبكف وقيه مر وزارة الصبحة العامة في المملكة المتحدة لضبمان الحصبول على الموافقات التنظيمية اللازمة لهذا اللقاح ، ون مف أن نتمكن \nمن إنتاكه بكميات كافية لتلبية احتياكات المرضبى في كمير أنحاء العالم \" .يشبار إلى أن فيروي كورونا المسببب لمتلازمة الشبرق الأوسب  \nالتنفسبية ( MERS - CoV  ) هو أحد الفيروسبات التي تصبيب الجهاز التنفسبي ، ولا توكد حتى اعن على مسبتوى العالم معلومات دقيقة  \nعن مصبببببدر هبذا الفيروي ولا طرق انتقبالبه ، كمبا لا يوكبد تطعيم وقبا.ي أو مضببببباد حيوي لعلاكبه . لكن هنباا العبديبد من اعكراءات  \nوالاحتياطات التي يمكن اتخاكها للحد من خطر اعبببابة به ، مثف اعقلال من مخالطة المرضببى وسسببف اليدين بانتظاج وتجنب لم\" العين \nوالأنف والفم واسببتخداج المناديف عند السببعال أو العط\" وتغطية الفم والأنف أقناء السببعال أو العطاي وتغطية الأنف والفم عند العط\" أو \nالسبعال باسبتخداج منديف والتخلم منه مباشبرة في سبلة النفايات وعدج مشباركة اعخرين في أدوات الط عاج والشبرا  سير المسبتخدمة بعد \nسسببلها كيدا والحرع على سسببف اليدين بالصببابون والماء الداف  لمدة 20 قانية على الأقف قم شببطفها بالماء البارد وتجفيف اليدين بمنشببفة\nنظيفة والابتعاد عن أماكن الازدحاج والتجمعات البشبرية قدر اعمكان .يذكر أن منظمة الصبحة العالمية كانت قد أعلنت الأسببو  الماضبي أن \nعدد حالات اعببابة المككدة بفيروي كورونا الجديد ارتفر إلى  991 حالة ، توفي منها563 شبخصبا منذ اكتشبا  المري في شبهر سببتمبر\nأيلول  2012 . \nFigure 2: Random unseen context about coronavirus vaccine(top). Followed by two generated samples bu\nARAGPT2-mega. Generated text 1 ( topp = 0.95), Generated text 2 (topp = 1)\nPrompt \nيحكى أن مزارعا مخادعا قام ببيع بئر الماء الموجود في أرضه لجاره مقابل مبلغ كبير من المال ، وعندما جاء المزارع الذي اشترى البئر \nليستخدم الماء الموجود فيه في اليوم التالي قال له الرجل المخادع : اذهب من هنا أيها الرجل فأنا قد بعتك البئر لكنني لم  أبعك الماء الموجود \nفيه ، دهش الرجل مما سمع وتوجه إلى القاضي ليشتكي المزارع المخادع له بعد محاولات عديدة لإقناعه بأن البئر والماء الذي فيه من حقه  \nGenerated Text 1 \nوأن عليه أن يدفع المبلغ الذي اشتتتراه به نمنا للبئر ، لكن القاضتتي رفش الشتتكوى وقال للرجل المخادع  : لقد اشتتتريم منك البئر لكنك لم \nتشتتتر مني الماء فوفي يوم من اايام وبينما كان أحد المزارعين يجلع على إحدى شتترفات منزله المعلى على حديقى مليئى بااشتتجار رأى \nرجلا قادما نحوه وبيده كيع مليء بالحلوى فستتلم عليه وستتأله عن ستتبب مجيئه إليه فألبره الرجل بأنه يريد أن يععيه بعش الحلوى التي  \nيحبها ، فقال له المزارع : حستتنا ستتأععيك ما تريد بشتتر  واحد وهو أن لا تخبر أحدا بهذا اامر أبدا ، وافل الرجل على شتتر  المزارع  \nوأععاه الكيع المملوء بالحلوى ، وقبل أن يغادر المزرعى ستأله المزارع قالالا : هل تعلم يا ستيدي بأنني رجل فقير ولا أملك مالا احصتل  \nعلى الحلوى ؟ فأجابه الرجل : نعم أعلم ذلك ولكن المشتكلى هي أنني لا أستتعيع أن أععيك الحلوى دون أن تخبرني بذلك ، ضتحك المزارع  \nقليلا نم قال للرجل : إذا ألبرتني قبل أن تععيني الحلوى فلن أستتمل لك بألذ أي شتتيء منها حتى لو كان هذا الشتتيء هو نمن الحلوى التي  \nأععيتك إياها فألبر المزارع جاره بما حدث معه و لب منه أن يوصتتتتله إلى القاضتتتتي لعله يجد حلا لمشتتتتكلته ، ذهب جار المزارع إلى  \nالقاضتتي وألبره بما حدث مع جاره ، فستتأله القاضتتي مستتتغربا : لماذا فعلم ذلك ؟ أجابه الجار : انني أريد أن أعرل كيل حصتتلم على \nالحلوى دون أن تألذها مني ودون أن تععيها احد غيرك !وبالفعل عندما وصتل الجار إلى القاضتي  لب منه القاضتي أن يريه الصتندو   \nالذي يحتوي على الحلوى ، نظر القاضتتتي إلى الصتتتندو  فوجد بدالله صتتتندوقا  لر كبيرا مليئا بالحلوى ، تعجب القاضتتتي من محتويات  \nالصتتندو  لكنه ستترعان ما قرر أن يفتل الصتتندو  ليرى ماذا يوجد بدالله ، فتل القاضتتي الصتتندو  فوجده مملوءا بالحلوى وقد كتب عليه \nعبارة تقول : \" هذه الحلوى لمن يستتعيع الحصتول عليها فق\" \" ، استتغرل القاضتي من محتوى الصتندو  وقرر أن يعيد فتحه مرة ألرى  \nليجد بدالله صتندوقا نالاا مليئا بالحلوى أياتا ، ستأل القاضتي صتاحب الصتندوقين  عما يحتويه كل صتندو  فأجال صتاحب الصتندو  الاال   \nقالالا : إن هذين الصتتندوقين مخصتتصتتان للفقراء الذين لا يستتتعيعون الحصتتول على الحلوى ، أما الصتتندوقين اخلرين فهما مخصتتصتتان  \nللأغنياء الذين يستعيعون الحصول على تلك الحلوى ف \n \nPrompt \nيحكى أن ااستتتد ملك الغابى وأقوى ستتتكانها كان ذات يوم نالاما ، عندما بدأ فأر صتتتغير يعيش في الغابى نفستتتها بالركش حوله والقفز فوقه\nوإصتدار أصتوات مزعجى ، مما أقلل نوم ااستد ودفعه للاستتيقا  ، وعندما قام ااستد من نومه كان غاضتبا ، فوضتع قباتته الاتخمى فو   \nالفأر ، وزمجر وفتل فمه ينوي ابتلاع الفأر الصتغير بلقمى واحدة ، صتاا الفأر عندها بصتوت يرتجل من الخول راجيا أن يعفو ااستد عنه \n، وقال : \nGenerated Text 1 \n\" ستامحني يا ملك الغابى ، أرجوك ستامحني ، فأنا لا أستتعيع  العيش بدونك \" فأجال ااستد قالالا : \" أنا ملك الغابى ، وأنم أيها الفأر الصتغير  \nمجرد تابع لي ، ولا تستتعيع عمل أي شتيء دون إذني ، أما أنا فأستتعيع أن أعمل ما أشتاء وأعيش كما أشتاء ، ولكن بشتر  واحد وهو أن \nتستتاعدني على القاتتاء على جميع الفئران التي تعيش في الغابى \" ففقال الفأر : \" نعم ستتيدي ، ستتأفعل كل ما تأمرني به ، وستتأقاتتي على \nجميع الفئران الموجودة في الغابى ، وستتتي بك إلى هنا لتعيش معي وتاكل من  عامي وتشترل من مالاى وتنام في فراشتي \" فضتحك ااستد \nضتحكى عاليى نم قال للفأر : \" هل أنم موافل ؟ \" ، فقال الفأر : \" لقد ستمعم هذا الكلام منذ قليل فق\" ، ولكني لم أكن أعرل أنك ملك الغابى  \nوستتيدها ، وأن لك ستتلعانا علي الحيوانات كلها ، لذلك أرجو منك أن تستتمل لي بأن أعيش معك تحم ستتقل واحد وتحم حمايى ستتلعانك  \nوملكك \" ففرا ااستتد كايرا بالفكرة ، و لب من الفأر أن يذهب معه إلى بيته ليعيشتتا معا حياة ستتعيدة هانئى ، وبالفعل ذهب الفأر مع ااستتد \nإلى بيم ااستد الذي رحب به أجمل ترحيب ، وأععاه قععى كبيرة من الخشتب ليصتنع منها ستريرا كبيرا له ولزوجته وأولاده ، وهكذا عا   \nالفأر مع ااستد في ستعادة وهناء فوبعد فترة قصتيرة من الزمن ، مرلأ ااستد مرضتا شتديدا ، ولم يعد قادرا على تحمل المزيد من ااعباء  \nالملقاة على عاتقه ، فعلب من الفأر أن يستتاعده في حمل ستتريره الاقيل والذهال به إلى العبيب ، فوافل الفأر على الفور ، وذهب مستترعا \nإلى العبيب ليخبره بأنه مصتال بمرلأ لعير يحتا  إلى جراحى عاجلى ، لكنه رفش إجراء الجراحى لوفا من غاتب ااستد عليه وانتقامه  \nمنه ، لكن الفأر أصتتر على الذهال إلى العبيب لإجراء الجراحى ، وفي أنناء ذلك ستتمع صتتوت ااستتد يناديه ويقول له : \" ماذا تريد مني يا  \nملك الغابى ؟ \" ، فأجابه الفأر : \" أريد أن أحملك على  هري اذهب بك إلى العبيب حتى يشتتتخا حالتك ويصتتتل لك العلا  المناستتتب  \nلحالتك \" فنظر ااسد إلى الفأر مبتسما وقال له : \" هيا بنا يا صديقي العزيز لنذهب \nFigure 3: Random unseen contexts about children stories. Followed by a generated sample by A RAGPT2-mega\nwith topp = 0.95\nPrompt 1 (Cooking Recipes) \nفتة الدجاج اللبنانية طريقة التحضير نضع الدجاج في قدر من الماء مع البصل والهيل وورق الغار والملح والفلفل الأسود ونسلقه على نار \nمتوسطة حتى ينضج تماما . صفي الدجاج من المرق ونفتته ونضعه جانبا ثم نصفي المرق ونضعه في وعاء . نحمي  الزيت ونقلي فيه الخبز  \nحتى يصبح ذهبيا ومقرمشا .  \nGenerated Text \nنوزع شرائح الفليفلة الخضراء والحمراء والخضراء والبندورة المقشرة والمفرومة فوق الخبز ، ثم نرش فوقها القليل من البقدونس المفروم  \nوالصننورر المحم  . ندهن صنينية فرب رالقليل من الزيت ونرشنها رالقليل من الطحين ثم ندخل الصنينية للى فرب محمى مسنبقا على حرارة  \n180  درجة مئوية لمدة تتراوح رين15 و20 دقيقة أو حتى يحمر وجه الصننينية . نخرج الصننينية من الفرب ونترلها لتبرد قلي  ثم نقطعها\nونقدمها ساخنة . \n \nPrompt 2 (Short Conversation - Greeting) \nمرحبا ليف حالك ؟ \nGenerated Text \nاب شناء   رخير . اليوم جبتلمم لعبة حلوة لتير و هي عبارة عن ممموعة من المرا  الملونة رللواب مختلفة   لل ما عليك فعله هو الضنغط  \nعلى المرة التي تريد أب ترميها في المماب المخص  لها ثم اضغط عليها مرة أخرى \n \nPrompt 3 (Wikipedia-style) \nلبناب تعد الممهورية اللبنانية دولة عررية عاصنمتها ريرو  ، وتقع على الشناطا المطل على المزء الشنرقي للبحر الأريل المتوسنط ، وتعد\nواحدة من ألثر البلداب الممتظة رالسننماب ، ورالعودة للى تاريخها فهي  تعتبر موطنا لأقدم الحضننارا  البشننرية في الألفية الثالثة قبل المي د \nخاصننة المنطقة السنناحلية منها ، فمانت من أهم المرالز الثقافية والتمارية في ذلك الوقت ، ولاب لتضنناريسننها الوعرة دورا مهما على مر \nالتاريخ في لموء عدة جماعا  لليها ، وأعلنت لدولة جمهورية عام 1926  م، وفي عام 1946  م حصلت لبناب على استق لها رعد الانتداب\nالفرنسي . \nGenerated Text \n[١[]٢]\nالسنائدة في الب د فهي افسن م ، حيي يشنمل المسنلموب حوالي  96 % من لجمالي عدد السنماب ، ويتوزع الباقوب على الطوائف المسنيحية\nوالديانا  الأخرى ، ومن أررز المعالم السننياحية في لبناب مدينة ريرو  التي تعد عاصننمة الب د ومرلزها الثقافي والاقتصننادا ، لما يوجد \nرها العديد من المواقع الأثرية الهامة مثل مغارة جعيتا ، وقلعة رعلبك ، وررج ليفل ، ومتحف سنرسن، ، وقصر ريت الدين ال ا يعد واحدا من \nأقدم القصننور التاريخية في العالم ، وريرها المثير من المعالم المميزة٣]\nوالمناخ المعتدل صنيفا وشنتاء ، ل لك ف ب أرلا السنياح يفضنلوب قضناء عطلتهم الصنيفية في المناط، المبلية والسناحلية ل سنتمتاع رالمنا ر  \nالطبيعينة الخ رنة والمننا ر المنائينة الرائعنة ، رينمنا يفضنننننل البعل الخر التوجنه للى المندب والمنناط، النداخلينة للتمتع رمنناخهنا المعتندل  \nوالاستممام في أحضاب الطبيعة الساحرة \n \nFigure 4: Random unseen contexts on miscellaneous topics. Followed by a generated sample by ARAGPT2-mega\nwith topp = 0.95\nB Zero-Shot Learning\nB.1 Question Answering\nIn zero-shot factoid question answering, the infor-\nmation contained within the language model can\nbe queried. The model is tested on the Arabic\nexamples from the TyDiQA (Clark et al., 2020a)\nvalidation dataset (921 examples), and on the test\nset of ARCD (Mozannar et al., 2019) (702 exam-\nples). Hence, the model os provided with the fol-\nlowing prompt: “Answer the following question:”\n- “: ú\nÍA\u0010JË@ È@ \rñË@ \tá« I. k. \r@” , followed by the question,\nthen the phrase “The answer is” - “:ñë H. @ñm.Ì'@”. It is\nalso possible to append the phrase “in the year” -\n“ÐA« ú\n\t¯”, if the expected answer is a year, as shown\nin Table 3.\nTable 3: The input prompt for question answering\nÉÊÓ \táÓ \u0010é\u0010®Êg Èð\r@ \u0010I \tQ« ú\u0010æÓ : ú\nÍA\u0010JË@ È@ \rñË@ \tá« I. k. \r@\nÐA« ú\n\t¯ ñë H. @ñm.Ì'@ ? ZAÓYË@ ú \næAÓ \u0010éÊ\u0010KA\u0010¯ ú\n\t¯AK.\nAnswer the following question: When was the ﬁrst\nepisode of the series Buffy the Vampire Slayer shown?\nThe answer is in the year\nThe answer length is set to be the same as the\ngold answer length, and a repetition penalty is ap-\nplied as in CTRL (Keskar et al., 2019), which penal-\nizes the probability scores of previously generated\ntokens. A ‘no repeat tri-gram’ strategy that inhibits\nthe model from generating the same tri-gram more\nthan once has also been employed. Note that the\ncontext passage is not provided, which forces the\nmodel to rely only on the information gained dur-\ning pretraining.\nThe model achieves a 3.93% exact-match score\nand an F1-score of 14.51% on TyDiQA, and 4.07%\nexact-match score and 13.88% F1-score on ARCD.\nSince exact-match and F1-score misses answers\nthat are correct but are worded differently (as\nshown in Table 4). A subset of 500 answers from\nthe best TyDiQA run is selected, and scored man-\nually. Manual scoring shows that ARAGPT2 cor-\nrectly answered 24.6% of the questions. The model\nwas particularly good in countries and capitals ques-\ntion, year of birth and death, and some geography.\nYet it was failing mostly on questions about quan-\ntities i.e. population counts, area, age... The pre-\ndeﬁned answer length negatively affected the gen-\nerated answers in some cases, which is a limitation\nof the current approach.\nTable 4: Examples of correct answers that have zero\nexact match score.\nQuestion ? ÉK.ñ\tK XQ\t®Ë\r@ ñë\táÓ\nWho is Alfred Nobel?\nPredicted Answer ¨Q\u0010\u001e\tm×ð , \u0010IJ\nÓA\tJK\nYË@ ¨Q\u0010\u001e\tm×\nInventor of the dynamite, and the inventor of\nGround Truth ø\nYK\nñ ù\n\rKAJ\nÒJ\n» ð ¨Q\u0010\u001e\tm×ð Y\tJêÓ\nAn engineer and an inventor and a Swedish chemist\nQuestion ? ÐY\u0010®Ë@ \u0010èQºË ú\nÍðYË@ XAm\u001a\u0010'B@ A\u0010K ú\u0010æÓ\nWhen was the FIFA founded?\nPredicted Answer . Ð 1904\n1904 AD\nGround Truth 1904 ÐAªË@\táÓ ñK\nAÓ 21\n21 May of the year 1904\nQuestion ? A\tªK\nX PA\t«X@\r ñë \táÓ\nWho is Edgar Degas?\nPredicted AnswerÐA« \u001d\nPAK. ú\n\t¯ YËð , ú\næ\t\u001dQ\t¯ ú\nÎJ\nº\u0011\u0010\u001d \tàA\tJ\t¯ é\tK\r@\nHe is a French visual artist, born in\nGround Truth ú\næ\t\u001dQ\t¯ \u0010HAm\u001a\t' ð ÐAP ðú\nÎJ\nº\u0011\u0010\u001d \tàA\tJ\t¯\nVisual artist and painter and sculptor\nB.2 Translation\nA experiments has also been conducted to test the\ntranslation capability of ARAGPT2 by append-\ning the prompt “What is the translation of this\nsentence ?:” - “:? \u0010éÊÒm.Ì'@ è \tYë \u0010éÔg. Q\u0010K ù\n ë AÓ ” to the sen-\ntence from the source language, in order to induce\nthe translation behavior of the model. We then\napply greedy decoding to get the generated tar-\nget sentence. Evaluation is performed on 5000\nrandomly selected pairs from the English-Arabic\nTatoeba (Tiedemann, 2012) dataset. The model\nachieved only 1.32 BLEU score12. The low score is\ndue to the scarce representation of English words in\nthe vocabulary, since most words were split into sin-\ngle characters. Additionally, given that the prompt\ndesign greatly affects the model’s zero-shot perfor-\nmance, our prompt design might have been sub-\noptimal. Nevertheless, this negative result encour-\nages research into prompt engineering for Arabic\nlanguage models, which we leave as future work.\n12Using the sacrebleu scorer (Post, 2018)\nC GLTR Analysis and Visualizations\nGiant Language model Test Room\nThe histograms show some statistic about the text: Frac(p) describes the fraction of probability for the actual\nword divided by the maximum probability of any word at this position. The Top 10 entropy describes the\nentropy along the top 10 results for each word.\nTest-Model: gpt-2-small\nQuick start - select a demo text:\nmachine: GPT-2 small top_k 5 temp 1 machine: GPT-2 small top_k 40 temp .7\nmachine*: unicorn text (GPT2 large) human: NYTimes article human: academic text\nhuman: woodchuck :)\nor enter a text:\nanalyze\ntop k count\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90  \nfrac(p) histogram\n0510152025303540455055\n0 0.10.20.30.40.50.60.70.80.9 1\n1\n \ntop 10 entropy(p) histogram\n02468101214161820\n00.20.40.60.811.21.41.61.822.22.4\n1.55\nTop K  Frac P  Colors (top k):  1 0   1 0 0   1 0 0 0  \nﺷرطﻲ ﻓﯾﮭﺎ أﺻﯾب ، اﻟﻣﺣﺗﻠﺔ اﻟﻘدس ﻓﻲ دھس ﻋﻣﻠﯾﺔ ، أﻣس ، اﻟﻌﻣود رأس ﺣﻲ ﻣن (ﻋﺎﻣﺎ 23 )ﯾﻣﺔ ﻻ اﻟس ﻣﺣﻣود ﻣﺣﻣد اﻟﻣﻘدﺳﻲ اﻟﺷﺎب ﻧﻔذ \nﺣرس ﺷرطﺔ ﻋﻧﺎﺻر ﻣن ﻋددا ﺑﺳﯾﺎرﺗﮫ ﺻدم ﺳﯾﺎرة ﺳﺎﺋﻖ إن ﻓﯾﻠد روزن ﻣﯾﻛﻲ اﻹﺳراﺋﯾﻠﯾﺔ اﻟﺷرطﺔ ﺑﺎﺳم اﻟﻣﺗﺣدث وﻗﺎل .طﻔﯾﻔﺔ ﺑﺟروح إﺳراﺋﯾﻠﻲ \nھداﺳﺎ ﻣﺳﺗﺷﻔﻰ إﻟﻰ أﺛرھﺎ ﻋﻠﻰ ﻧﻘل طﻔﯾﻔﺔ ﺑﺟروح أﺣدھم إﺻﺎﺑﺔ إﻟﻰ أدى ﻣﺎ اﻟﻐرﺑﯾﺔ اﻟﻘدس ﻓﻲ ﯾﺎﻓﺎ ﺷﺎرع ﻓﻲ ورﯾﺔ ﺑد ﯾﻘوﻣون ﻛﺎﻧوا اﻟذﯾن اﻟﺣدود \n.اﻷﻗﺻﻰ اﻟﻣﺳﺟد ﺟﻧوب ﺳﻠوان ﺑﻠدة ﺳﻛﺎن ﻣن ﻓﻠﺳطﯾﻧﻲ ﺷﺎب ھو اﻟﻌﻣﻠﯾﺔ ﻣﻧﻔذ أن اﻹﺳراﺋﯾﻠﯾﺔ اﻹﻋﻼم وﺳﺎﺋل وذﻛرت .اﻟﻌﻼج ﻟﺗﻠﻘﻲ ﻛﺎرم ﻋﯾن \nTweet about GLTR \nMIT-IBM Watson AI lab and Harvard NLP \n \nﻛﺎﻧوا  اﻟذﯾن  اﻟﺣدود  ﺣرس  ﺷرطﺔ  ﻋﻧﺎﺻر  ﻣن  ﻋددا  ﺑﺳﯾﺎرﺗﮫ  ﺻدم  ﺳﯾﺎرة  ﺳﺎﺋﻖ  إن  روزﻧﻔﯾﻠد  ﻣﯾﻛﻲ  اﻹﺳراﺋﯾﻠﯾﺔ  اﻟﺷرطﺔ  ﺑﺎﺳم  اﻟﻣﺗﺣدث  وﻗﺎل  .طﻔﯾﻔﺔ  ﺑﺟروح  \nاﻟﻌﻼج  ﻟﺗﻠﻘﻲ  ﻛﺎرم  ﻋﯾن  ھداﺳﺎ  ﻣﺳﺗﺷﻔﻰ  إﻟﻰ  أﺛرھﺎ  ﻋﻠﻰ  ﻧﻘل  طﻔﯾﻔﺔ  ﺑﺟروح  أﺣدھم  إﺻﺎﺑﺔ  إﻟﻰ  أدى  ﻣﺎ  اﻟﻐرﺑﯾﺔ  اﻟﻘدس  ﻓﻲ  ﯾﺎﻓﺎ  ﺷﺎرع  ﻓﻲ  ﺑدورﯾﺔ  ﯾﻘوﻣون  \nاﻷﻗﺻﻰ  اﻟﻣﺳﺟد  ﺟﻧوب  ﺳﻠوان  ﺑﻠدة  ﺳﻛﺎن  ﻣن  ﻓﻠﺳطﯾﻧﻲ  ﺷﺎب  ھو  اﻟﻌﻣﻠﯾﺔ  ﻣﻧﻔذ  أن  اﻹﺳراﺋﯾﻠﯾﺔ  اﻹﻋﻼم  وﺳﺎﺋل  وذﻛرت ..\n(a) Text generated by ARAGPT2-Mega. The ﬁrst sentence is the human-written prompt\nGiant Language model Test Room\nThe histograms show some statistic about the text: Frac(p) describes the fraction of probability for the actual\nword divided by the maximum probability of any word at this position. The Top 10 entropy describes the\nentropy along the top 10 results for each word.\nTest-Model: gpt-2-small\nQuick start - select a demo text:\nmachine: GPT-2 small top_k 5 temp 1 machine: GPT-2 small top_k 40 temp .7\nmachine*: unicorn text (GPT2 large) human: NYTimes article human: academic text\nhuman: woodchuck :)\nor enter a text:\nanalyze\ntop k count\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90  \nfrac(p) histogram\n0510152025303540455055\n0 0.10.20.30.40.50.60.70.80.9 1\n0.423\n \ntop 10 entropy(p) histogram\n02468101214161820\n00.20.40.60.811.21.41.61.822.22.4\n1.6\n9\nTop K  Frac P  Colors (top k):  1 0   1 0 0   1 0 0 0  \nﺟدﯾﺔ اﻷﻛﺛر اﻻﺧﺗﺑﺎر أن ﻏﯾر ، ﻋﺳﻛرﯾﺔ أو ﺳﯾﺎﺳﯾﺔ ، اﻟﻣﻌﺎرﺿﺔ أو اﻟدوﻟﺔ ﻣﻊ ﺳواء ، اﻟﻣﺎﺿﯾﺔ اﻟﻔﺗرة ﻓﻲ واﺧﺗﺑﺎرات ﺗﺣدﯾﺎت ﺳﻠﺳﻠﺔ اﻷﻛراد واﺟﮫ \nاﻟﺷﻌب ﺣﻣﺎﯾﺔ وﺣدات ﻣﻘﺎﺗﻠو ﺗﻣﻛن ﺣﯾث ، (ﻛوﺑﺎﻧﻲ )اﻟﻌرب ﻋﯾن ﻓﻲ داﻋش -واﻟﺷﺎم اﻟﻌراق ﻓﻲ اﻹﺳﻼﻣﯾﺔ اﻟدوﻟﺔ ﺗﻧظﯾم ﺗﮭم ﻣواﺟﮫ ﻓﻲ ﻛﺎن \n، اﻟﻘرﯾﺑﺔ ﻣواﻗﻌﮭم ﺗﺣﺻﯾن ﻣﻊ ﺑﺎﻟﺗزاﻣن ، اﻟﻣﻧطﻘﺔ ﻋن ﺑﻌﺎده وإ داﻋش ﺗﻘدم ﻛﺳر ﻣن ، اﻟدوﻟﻲ اﻟﺗﺣﺎﻟف ﯾران وط ﻣرﻛﺔ اﻟﺑش ﻣﻊ ﺑﺎﻟﺗﻌﺎون ، اﻟﻛردﯾﺔ \nﺟﺑﮭﺔ � � ﻟﻠﺗﺻدي اﻷﻛراد اﻟﻣﻘﺎﺗﻠﯾن ﻣن اﻟﻌدﯾد اﻧطﻼق ﻣﺣطﺔ ﻓﻛﺎﻧت ﻋﻔرﯾن أﻣﺎ .اﻟﻌﯾن رأس إﻟﻰ وﺻوﻻ ودا ﻋﺎم أو اﻟﻘﺎﻣﺷﻠﻲ ﻓﻲ ﺳواء \n.ھراء واﻟز ﺑل ن ﻋن اﻟﺣﺻﺎر ﺗﺧﻔﯾف ﻓﻲ ﻣﮭﻣﺎ دورا وا وﻟﻌب ، رﻓﯾﺔ واﻷش ﻣﻘﺻود اﻟﺷﯾﺦ ﻣﺛل ، ﺣﻠب ﻓﻲ أﺣﯾﺎء ﺣﺗﻰ ، اﻟﻧﺻرة \nTweet about GLTR \nMIT-IBM Watson AI lab and Harvard NLP \n \nﻋﺎﻣودا  أو  اﻟﻘﺎﻣﺷﻠﻲ  ﻓﻲ  ﺳواء  ،  اﻟﻘرﯾﺑﺔ  ﻣواﻗﻌﮭم  ﺗﺣﺻﯾن  ﻣﻊ  ﺑﺎﻟﺗزاﻣن  ،  اﻟﻣﻧطﻘﺔ  ﻋن  وإﺑﻌﺎده  داﻋش  ﺗﻘدم  ﻛﺳر  ﻣن  ،  اﻟدوﻟﻲ  اﻟﺗﺣﺎﻟف  وطﯾران  اﻟﺑﺷﻣرﻛﺔ  \nاﻟﺷﯾﺦ  ﻣﺛل  ،  ﺣﻠب  ﻓﻲ  أﺣﯾﺎء  ﺣﺗﻰ  ،  اﻟﻧﺻرة  ﺟﺑﮭﺔ  ل  ﻟﻠﺗﺻدي  اﻷﻛراد  اﻟﻣﻘﺎﺗﻠﯾن  ﻣن  اﻟﻌدﯾد  اﻧطﻼق  ﻣﺣطﺔ  ﻓﻛﺎﻧت  ﻋﻔرﯾن  أﻣﺎ  . اﻟﻌﯾن  رأس  إﻟﻰ  وﺻوﻻ  \nواﻟزھراء  ﻧﺑل  ﻋن  اﻟﺣﺻﺎر  ﺗﺧﻔﯾف  ﻓﻲ  ﻣﮭﻣﺎ  دورا  وﻟﻌﺑوا  ،  واﻷﺷرﻓﯾﺔ  ﻣﻘﺻود . \n(b) Human-Written Text\nFigure 5: It is clear that the machine generated text in (a) is mostly green and yellow highlighted, while in the\nhuman-written text, (b), an increase in red and purple highlighted words can be noticed. P.S.: We use A RAGPT2-\nbase as the backend model in GLTR",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.884049654006958
    },
    {
      "name": "Computer science",
      "score": 0.8024708032608032
    },
    {
      "name": "Language model",
      "score": 0.7761704921722412
    },
    {
      "name": "Natural language processing",
      "score": 0.7278711795806885
    },
    {
      "name": "Transformer",
      "score": 0.642586886882782
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6327050924301147
    },
    {
      "name": "Arabic",
      "score": 0.603044331073761
    },
    {
      "name": "Text generation",
      "score": 0.43895405530929565
    },
    {
      "name": "Linguistics",
      "score": 0.2252558171749115
    },
    {
      "name": "Engineering",
      "score": 0.09863948822021484
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98635879",
      "name": "American University of Beirut",
      "country": "LB"
    }
  ],
  "cited_by": 15
}