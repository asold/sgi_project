{
  "title": "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models",
  "url": "https://openalex.org/W4402716457",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1953624896",
      "name": "Li Xin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2488310657",
      "name": "Wu Yunfei",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2351205817",
      "name": "Jiang XingHua",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2377661480",
      "name": "Guo Zhi-hao",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2356287441",
      "name": "Gong Mingming",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4229341557",
      "name": "Cao, Haoyu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2876721312",
      "name": "Liu, Yinsong",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2366664990",
      "name": "Jiang, Deqiang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2113166830",
      "name": "Sun Xing",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6852800892",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6779992872",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6783441721",
    "https://openalex.org/W6855795981",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3200439183",
    "https://openalex.org/W4304013646",
    "https://openalex.org/W6776700526",
    "https://openalex.org/W6804103469",
    "https://openalex.org/W6846178434",
    "https://openalex.org/W6849177959",
    "https://openalex.org/W6847278779",
    "https://openalex.org/W6851592950",
    "https://openalex.org/W6852609638",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W3120043490",
    "https://openalex.org/W4213213306",
    "https://openalex.org/W3004268082",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6846007759",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3106859150",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W3201693581",
    "https://openalex.org/W3169708801",
    "https://openalex.org/W4386065837",
    "https://openalex.org/W3128661784",
    "https://openalex.org/W2997154779",
    "https://openalex.org/W6787566904",
    "https://openalex.org/W6854695065",
    "https://openalex.org/W4389519972",
    "https://openalex.org/W6852060543",
    "https://openalex.org/W6853838016",
    "https://openalex.org/W3113328489",
    "https://openalex.org/W6851950068",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4287755086",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3087858202",
    "https://openalex.org/W3216533828",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385570934",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4383604278",
    "https://openalex.org/W3104953317",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4386141732",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W4376653374",
    "https://openalex.org/W4382766522"
  ],
  "abstract": "Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.",
  "full_text": "Enhancing Visual Document Understanding with Contrastive Learning in\nLarge Visual-Language Models\nXin Liâ€ *\nYunfei Wu* Xinghua Jiang Zhihao Guo Mingming Gong Haoyu Cao\nYinsong Liu Deqiang Jiang Xing Sun\nTencent YouTu Lab\n{fujikoli, marcowu, clarkjiang, nicholasguo, riemanngong, rechycao,\njasonysliu, dqiangjiang, winfredsun}@tencent.com\nAbstract\nRecently, the advent of Large Visual-Language Mod-\nels (LVLMs) has received increasing attention across var-\nious domains, particularly in the field of visual document\nunderstanding (VDU). Different from conventional vision-\nlanguage tasks, VDU is specifically concerned with text-\nrich scenarios containing abundant document elements.\nNevertheless, the importance of fine-grained features re-\nmains largely unexplored within the community of LVLMs,\nleading to suboptimal performance in text-rich scenarios.\nIn this paper, we abbreviate it as the fine-grained feature\ncollapse issue. With the aim of filling this gap, we pro-\npose a contrastive learning framework, termed Document\nObject COntrastive learning (DoCo), specifically tailored\nfor the downstream tasks of VDU. DoCo leverages an auxil-\niary multimodal encoder to obtain the features of document\nobjects and align them to the visual features generated by\nthe vision encoder of LVLM, which enhances visual repre-\nsentation in text-rich scenarios. It can represent that the\ncontrastive learning between the visual holistic representa-\ntions and the multimodal fine-grained features of document\nobjects can assist the vision encoder in acquiring more ef-\nfective visual cues, thereby enhancing the comprehension\nof text-rich documents in LVLMs. We also demonstrate that\nthe proposed DoCo serves as a plug-and-play pre-training\nmethod, which can be employed in the pre-training of vari-\nous LVLMs without inducing any increase in computational\ncomplexity during the inference process. Extensive experi-\nmental results on multiple benchmarks of VDU reveal that\nLVLMs equipped with our proposed DoCo can achieve su-\nperior performance and mitigate the gap between VDU and\ngeneric vision-language tasks.\n1. Introduction\nLarge Language Models (LLMs) [1, 3, 5, 26, 34] have\naroused increasing research interest due to their powerful\n*Equal contribution. â€ Contact person.\nImage\nBox\nText\nA document\nimage of ...\nA document\nimage of ...\n(a) Image-level instance discrimination (CLIP)\n(b) Document object discrimination (Our DoCo)\nImage\nEncoder\nA document\nimage of ...Text\nEncoder\nImage-level\nAlignment\nImage\nEncoder\nMulti-\nmodal\nEncoder\nDocument\nObject\nAlignment\nImage\nBox\nText\nImage\nBox\nText\nFigure 1. The motivation of the proposed DoCo. (a) Image-level\ninstance discrimination between visual and textual inputs, which\naims to learn the holistic representations but fails to extract fine-\ngrained features in text-rich scenarios. (b) Document object\ndiscrimination between visual and multimodal inputs, which en-\nhances the visual representation of image encoder in LVLMs and\nachieves the better visual understanding performance for VDU.\ntext generation and understanding capabilities. These mod-\nels can be aligned with user intent through fine-tuning in-\nstructions, demonstrating strong interactive capabilities and\npotential to serve as productivity-boosting intelligent assis-\ntants. However, the inherent limitation of these models is\ntheir confinement to the domain of textual data, render-\ning them incapable of processing other prevalent modal-\nities such as images, speech, and video. This signifi-\ncantly curtails the applicability of these models. To circum-\nvent this constraint, a set of Large Visual-Language Mod-\nels (LVLMs) [2, 16, 18, 40, 43] has been devised to enhance\nthe ability of LLMs to perceive and understand visual sig-\nnals. These LVLMs have shown immense potential in ad-\ndressing real-world visual challenges.\n1\narXiv:2402.19014v1  [cs.CV]  29 Feb 2024\nDespite these multimodal large models having com-\nmendable capabilities in multimodal understanding, their\nproficiency in text-rich scenarios, such as visual document\nunderstanding (VDU) tasks, remains limited due to insuf-\nficient specific training. To address this issue, recent ad-\nvancements in LVLMs [8, 38, 39, 41] have further ex-\nplored the performance of VDU in the community, which\nutilize large-scale document data for training or combine\nmultiple VDU tasks of multimodal understanding. For\nexample, mPLUG-DocOwl [38] constructs a comprehen-\nsive dataset dedicated to document image comprehension.\nLLaV AR [41] proposes incorporating a text recognition pre-\ntraining task to enhance the understanding of text-rich im-\nages. UniDoc [8] implements a unified multimodal instruc-\ntion tuning on the contributed large-scale instruction fol-\nlowing datasets. While these works have made significant\nstrides in text-rich scenarios, they still struggle with accu-\nrately capturing the textual details within images. As de-\npicted in Fig. 1(a), the vision encoders of these models are\ninitialized from CLIP-based [25] weights, which are pre-\ntrained through contrastive learning to foster cross-modality\nalignment between visual and textual inputs via image-level\ninstance discrimination. Nevertheless, these image-level\nrepresentations are not optimal for dense prediction tasks\nsuch as VDU and fail to extract fine-grained visual features.\nThis dilemma leads to the following question: how can\nthe exploration of fine-grained features enhance the perfor-\nmance of LVLMs in text-rich scenarios? We define it as the\nfine-grained collapse issue, which still lacks investigation in\nthe VDU field.\nIn this work, we introduce a novel Document Object\nCOntrastive learning (DoCo) tailored for this problem, as\nillustrated in Fig. 1(b). Concretely, we adopt a contrastive\nlearning approach, which leverages a multimodal encoder to\nobtain the multimodal features ( i.e., visual, layout and tex-\ntual) of document objects and align them to the visual fea-\ntures produced by the image encoder of LVLM. During the\npre-training phase, the image encoder processes the whole\nimage into a series of embeddings consistent with the vi-\nsual feature extraction of LVLMs. Beyond visual features,\nthe layout information along with the textual contents of the\ndocument objects derived from the OCR engine is embed-\nded to the multimodal encoder to capture the fine-grained\nfeatures. Subsequently, the output representations gener-\nated by the two encoders are trained to align in the man-\nner of contractive learning. In this way, the image encoder\nof LVLMs can acquire more effective visual cues from the\ndocument objects, thereby strengthening the understanding\nability of text-rich scenarios. Note that the yielded DoCo is\na plug-and-play pre-training method, which can be applied\nin the pre-training of various LVLMs without inducing any\nincrease in computational complexity during inference. To\nsum up, our main contributions are in the three folds:\nâ€¢ We investigate the importance of fine-grained features in\nVDU tasks and propose the fine-grained feature collapse\nissue. To our best knowledge, we are the first to research\nthe enhancement of visual information for LVLMs.\nâ€¢ We coin a novel DoCo tailored for the fine-grained fea-\nture collapse issue, which introduces a contrastive learn-\ning framework to assist LVLMs in acquiring more bene-\nficial visual features in text-rich scenarios and enhancing\nthe perception of document objects.\nâ€¢ Experimental results on various VDU datasets demon-\nstrate that our proposed method facilitates significant per-\nformance enhancements for LVLMs in text-rich scenes.\n2. Related Work\n2.1. Visual Document Understanding\nThe field of Visual Document Understanding (VDU) is con-\ncerned with the interpretation and comprehension of a wide\narray of digital-native or scanned documents, which in-\ncludes but is not limited to forms, tables, reports, and aca-\ndemic research papers. The techniques employed in VDU\ncan be broadly classified into two primary categories. The\ninitial category aims to address the task by integrating im-\nages and OCR annotations sourced from external OCR sys-\ntems [11, 12, 33, 36, 37]. Among these, LayoutLMv3 [12]\nutilizes a streamlined transformer-based architecture and\nloss functions that foster alignment between the patches and\nOCR. UDOP [33] employs a unified encoder to represent\nfeatures from both images and texts, thereby transforming\ninformation from these modalities into vision-text embed-\ndings. The second category investigates methods that di-\nrectly process the document images without external OCR\ntools. Donut [14] proposes a model that performs text read-\ning from document images as a pre-training task without the\nOCR engine involved. Pix2Struct [15] introduces the con-\ncept of screenshot parsing objectives, while MATCHA [17]\nincorporates chart derendering and math reasoning into the\nmodel training.\n2.2. Large Visual-Language Models\nLarge Language Models (LLMs), such as GPT-3 [3],\nPaLM [5], PaLM2 [1], BLOOM [26], and LLaMA [34],\nhave showcased remarkable zero-shot capabilities across a\nwide range of open-ended tasks. In recent years, researchers\nhave been exploring the integration of high-performance\nLLMs with Large Vision-Language Models (LVLMs) to\nenable a deeper understanding of visual inputs. BLIP-\n2 [16] aligns queried visual features with text using multi-\nple vision-language losses and employs a simple fully con-\nnected layer to feed the queried embedding into a frozen\nlanguage model. Mini-GPT4 [43], Qwen-VL [2] and\nmPLUG-Owl [40] retain the Q-Former architecture while\nreplacing the language model with a larger one. These mod-\n2\nImage\nEncoder\nMultimodal\nEncoder\nROI\nAggregation\nMLP\nDocument Object \nFeature Extraction\nDocument Object Contrastive Loss\nDocument Object \nFeature Aggregation\nDocument Object \nContrastive Learning\nğ…\"m\nğ…\"v\nğ…#m\nğ…#v\nImage\nEncoder\n Adaptor\nBox\nÂ·Â·Â·\n(b) Fine-tuning Phase\nQuestion: What country \nhas the highest BMI?\nImage + Box + Text\nImage\n(a) Pre-training Phase\nOCR\nShar e  of adults  tha t a r e  ove r w e ight o r  obe s e , 20 11 Our Wo rld\nin Data\nBeing  o v erweig ht is â€¦\nBMI g rea ter tha n o r equa l to  3 0 . BMI is a  perso n's weig h t in kilo g ra m s di v id ed b y  \nhis  or  he r  he ight in met ressq u a red\nUn ited  S ta tes 67.5%\nLiby a 60.9%\nCy p ru s 58.3%\n0% 10% 20% 30% 40% 50% 60%\nSo urce: WHO, Glo ba l Hea lth Observ a to ry OurWo rldlnDa ta .o rg/obe sity  Â· CC BY\n(ğ’™ğŸğŸ, ğ’šğŸğŸ, ğ’™ğŸğŸ, ğ’šğŸğŸ)\n(ğ’™ğŸğŸ, ğ’šğŸğŸ, ğ’™ğŸğŸ, ğ’šğŸğŸ)\n(ğ’™ğŸğŸ‘, ğ’šğŸğŸ‘, ğ’™ğŸğŸ‘, ğ’šğŸğŸ‘)\n(ğ’™ğŸğŸ’, ğ’šğŸğŸ’, ğ’™ğŸğŸ’, ğ’šğŸğŸ’)\n(ğ’™ğŸğ‘µ&ğŸ, ğ’šğŸğ‘µ&ğŸ,\nğ’™ğŸğ‘µ&ğŸ, ğ’šğŸğ‘µ&ğŸ)\nShare of adul ts \nthat â€¦\nBeing  o v erw eig ht  \nis d ef in ed  a s â€¦\nBM I g rea t er t ha n \nor  e qual  to â€¦\nsquared.\nShare of adul ts \nthatâ€¦\nÂ·Â·Â·\nÂ·Â·Â·\nÂ·Â·Â·\nÂ·Â·Â·\nÂ·Â·Â·\nLLM\nCross-Entropy Loss\nFm\nFv\nImage Fv ğ…\"v\nPatch\nEmbedding\nMultimodal\nEmbedding\nFigure 2. A schematic overview of our proposed DoCo. We aim to bridge the representation learning gap between visual features and\nmultimodal features by guiding the former to imitate the features extracted by the latter. Note the branch of multimodal feature extraction is\nremoved after pre-training, which indicates the computational complexity does not increase during the phase of fine-tuning and inference.\nels are then fine-tuned on meticulously collected instruction\ndata to enhance their performance. To extend the capabili-\nties of LVLMs to VDU, mPLUG-DocOwl [38] proposes a\nmodularized model based on mPLUG-Owl for OCR-free\ndocument understanding. UniDoc [8] employs a univer-\nsal large multimodal model to simultaneously perform text\ndetection, recognition, spotting, and understanding. URe-\nader [39] designs a shape-adaptive cropping module before\nthe encoder-decoder architecture to leverage the frozen low-\nresolution vision encoder for processing high-resolution im-\nages. These comprehensive methods underscore the poten-\ntial of LVLMs in tackling intricate vision-language tasks.\nHowever, the significance of fine-grained features, which\nare crucial for VDU, remains unexplored.\n2.3. Contrastive Learning\nContrastive learning methods have been extensively em-\nployed to derive meaningful representations, demonstrat-\ning their efficacy across a multitude of tasks such as image\nclassification [13, 25], object detection [32, 35], and im-\nage segmentation [4, 42]. The vision-language framework,\nCLIP [25], which is extensively adopted, capitalizes on con-\ntrastive learning and vast data quantities to align images and\ntexts in a semantic space, resulting in strong generalization\ncapabilities across a wide range of downstream tasks. Nev-\nertheless, the discrimination of the entire image between\nthe visual and textual information is not favorable for the\ncomprehension of visual documents due to the absence of\nfine-grained features. To address this issue, we introduce\na novel contrastive feature enhancement technique, which\neffectively augments the visual features within LVLMs, re-\nsulting in a robust capability of analyzing text-rich images.\n3. Methodology\n3.1. Overall Architecture\nThe overview of the proposed Document Object\nCOntrastive learning (DoCo) can be observed in Fig. 2,\nwhose goal is to align the multimodal features of document\nobjects to the image representations. Here, we employ\nvarious LVLMs ( i.e., Qwen-VL-Chat [2] and mPLUG-\nOwl [40]) to illustrate our fundamental design principle.\nDoCo mainly comprises two branches, the vision encoder,\nsituated at the top, and the multimodal encoder, positioned\nat the bottom. During the pre-training phase, an input\nimage undergoes processing by the visual encoder to obtain\na series of image embeddings. Concurrently, an Optical\nCharacter Recognition (OCR) engine is employed to parse\nthe bounding boxes and contents of texts, which are subse-\nquently sent to the multimodal encoder for the extraction\nof the fused features of document texts. Furthermore,\nthe output representations generated by both encoders\nare aligned at the document object level by the ROI\nAggregation module. Ultimately, the visual representations\nfrom the image encoder are enhanced by DoCo within\nthe contrastive learning paradigm, encompassing both\nIntra-Document Object Contrastive Learning (Intra-DoCo)\nand Inter-Document Object Contrastive Learning (Inter-\n3\nDoCo). During the fine-tuning phase, the multimodal\nfeature extraction branch is removed, while the enhanced\nimage features, in conjunction with the embeddings from\nadditional text input ( e.g., question), are fed into the\nvision-language adapter and subsequently into the LLM.\n3.2. Document Object Feature Extraction\nVisual features: Throughout the training and inference\nphases, the input image x is transformed by the process\nof patch embedding into a sequence of image embeddings\ndenoted as Fv. Subsequent to this procedure, the con-\nventional Vision Transformer (ViT) [6] initialized with the\npre-trained weights derived from CLIP [25] is employed\nto extract the embeddings to a collection of visual features\neFv =\n\b\nfv\n1,1, fv\n1,2, ...,fv\nHâ€²,Wâ€²\n\t\nâˆˆ R(Hâ€²Wâ€²)Ã—dv are produced,\nwhere (Hâ€²Wâ€²) denotes the patch size of the image and dv\nis the dimension of patch.\nMultimodal features: In this process, we aim to extract\nthe multimodal representations of document objects, which\namalgamates various embeddings to facilitate the feature\nextraction of each box via a multimodal encoder. Draw-\ning inspiration from recent pre-training frameworks for doc-\nument understanding, we adhere to LayoutLMv3 [12] to\nleverage the multimodal features Fm consisting of visual\nembeddings, layout embeddings and textual embeddings,\nas illustrated by the red, green and blue boxes in Fig. 2.\nFollowing this procedure, a set of multimodal embeddings\neFm =\n\b\nfm\n1 , fm\n2 , ...,fm\nN+1\n\t\nâˆˆ R(N+1)Ã—dm are generated\nby the multimodal encoder, where N signifies the number\nof boxes recognized by the OCR engine and dm means the\ndimension of the aggregated features of the objects. Spe-\ncially, a box possessing an identical resolution to the image\nis utilized to extract the global features, thereby resulting in\nthe quantity of the multimodal embeddings being N + 1.\nA comprehensive description of the multimodal feature ex-\ntraction is provided in the supplementary material.\n3.3. Document Object Feature Aggregation\nUpon the completion of the visual encoding phase, we at-\ntempt to accurately acquire the image features that corre-\nspond to a specified region of the document object. Never-\ntheless, the direct application of a traditional feature extrac-\ntion technique, such as ROIAlign [10] or ROIPooling [9],\nto a ViT backbone presents a challenge due to the disparate\nshapes of the output feature maps produced by CNN and\nViT. Given that the bounding box coordinates are expressed\nin pixel units, the bounding box edges frequently fail to\nalign with the patch boundaries. To address this issue, we\npropose a novelROI Aggregationmodule for the feature ex-\ntraction, which leverages the overlapped area between each\npatch and the specified region to the attention mask in the\nattention function.\nROI Aggregation:As depicted in Fig. 3, the patch features\n0.21\n0\n0\n0\n0\n0.23\n0\n0\n0\n0\n0.23\n0\n0\n0\n0\n0.08\n0\n0\n0\n0\n0\n0\n0\n0\n0\nV Q\nMatMul\nScale\nâŠ•\nSoftMax\nMask\nMatMul\nK\nÂ·Â·Â·\n<CLS>\nbi\nğ…\"v\nğŸ$!\n\"\nMici\nğ›!,!\n# ğ›!,$!\n#ğ›!,%\n# ğ›!,&\n# ğ›!,â€¦\n#\nğ›%,!\n# ğ›%,$!\n#ğ›%,%\n# ğ›%,&\n# ğ›%,â€¦\n#\nğ›&,!\n# ğ›&,$!\n#ğ›&,%\n# ğ›&,&\n# ğ›&,â€¦\n#\nğ›(!,!\n# ğ›(!,$!\n#ğ›(!,%\n# ğ›(!,&\n# ğ›(!,â€¦\n#\nğ›â€¦,!\n# ğ›â€¦,$!\n#ğ›â€¦,%\n# ğ›â€¦,&\n# ğ›â€¦,â€¦\n#\nBiaspadding\nğŒ& i\nFigure 3. The proposed ROI Aggregation. The dashed red grid\nrepresents the image patch features, and the solid green region de-\nnotes the bounding box region. The overlap between each patch\nand the given region is calculated and serves as the attention mask\nfor the visual aggregation of the document objects. Best viewed in\ncolor.\noutput by the image encoder are denoted as eFv. For the\ni-th box object, the corresponding bounding box is defined\nas bi =\n\b\nxi\n1, yi\n1, xi\n2, yi\n2\n\t\n. Initially, we calculate the over-\nlapped area between the specified region bi and each patch\nbp =\nn\nbp\n1,1, bp\n1,2, ...,bp\nHâ€²,Wâ€²\no\nâˆˆ R(Hâ€²Wâ€²)Ã—4, and normal-\nize it to Mi with the patch area:\nMi = Overlap(bi, bp)/Area(bp), (1)\nwhere Overlap and Area denote calculating the overlap\narea between two boxes and the area of the box, respec-\ntively. Mi âˆˆ R(Hâ€²Wâ€²)Ã—1 represents the attention bias of the\nvisual features towards the i-th document object. Subse-\nquently, we prepend a <CLS> token ci to the sequence of\npatch embeddings eFv, denoted as the reorganized sequencen\nci, eFv\no\nâˆˆ R(Hâ€²Wâ€²+1)Ã—dv , to aggregate the image rep-\nresentations of the document box. Note that Mi is then\npadding with zeros to fMi âˆˆ R(Hâ€²Wâ€²+1)Ã—(Hâ€²Wâ€²+1), which\nfunctions as the attention bias for the attention score. Ulti-\nmately, the reorganized sequence regarded asQ, K, and V,\nalong with fMi, is process by the self-attention layer:\nAttention(Q, K, V) =softmax( Q Â· KâŠ¤\nâˆšdv\n+ fMi) Â· V, (2)\nFollowing this procedure, the aggregated image features\nof the i-th box object are obtained, denoted as Ë†fv\ni , and a\nset of visual embeddings of the document boxes Ë†Fv =\n4\nn\nË†fv\n1 ,Ë†fv\n1 , ...,Ë†fv\nN+1\no\nâˆˆ R(N+1)Ã—dv are generated. Further-\nmore, two groups of MLP layers are employed to project\nthe multimodal representations into a uniform dimensional\nspace, thereby converting the dimension dm of eFm to the\ndimension dv of Ë†Fm =\nn\nË†fm\n1 ,Ë†fm\n1 , ...,Ë†fm\nN+1\no\nâˆˆ R(N+1)Ã—dv .\n3.4. Document Object Contrastive Learning\nIn an effort to incorporate the visual and multimodal fea-\ntures of document objects into a shared domain, we present\na novel DoCo designed to improve the visual representa-\ntions, which significantly contributes to the extraction of the\nvisual features in text-rich scenes. The proposed scheme\nconsists of two components: Intra-DoCo and Inter-DoCo,\nwhich respectively represent the learning of document ob-\nject representation within a single image and between two\ndistinct images.\nf\"!\n\" Â· f\"!\n# f\"!\n\" Â· f\"$\n# f\"!\n\" Â· f\"%\n# â€¦ f\"!\n\"\nÂ· f\"&'!\n#f\"!\n\"\nf\"&'!\n\"\nÂ· f\"!\n#\nf\"&'!\n\"\nÂ· f\"$\n#\nf\"&'!\n\"\nÂ· f\"%\n# â€¦ f\"&'!\n\"\nÂ· f\"&'!\n#f\"&'!\n\"\nf\"$\n\" Â· f\"!\n# f\"$\n\" Â· f\"$\n# f\"$\n\" Â· f\"%\n# â€¦ f\"$\n\"\nÂ· f\"&'!\n#f\"$\n\"\nf\"%\n\" Â· f\"!\n# f\"%\n\" Â· f\"$\n# f\"%\n\" Â· f\"%\n# â€¦ f\"%\n\"\nÂ· f\"&'!\n#f\"%\n\"\nâ€¦ â€¦ â€¦ â€¦ â€¦â€¦\nf\"!\n# f\"$\n# f\"%\n# â€¦ f\"&'!\n#\nğ’™ğŸ\nğ’™ğŸ\nF!\n\"\nÂ· F!\n#\nF!\n\"\nÂ· F$\n#\nF!\n\"\nÂ· F%\n# â€¦ F!\n\"\nÂ· F(\n#\nF!\n\"\nF(\n\"\nÂ· F!\n#\nF(\n\"\nÂ· F$\n#\nF(\n\"\nÂ· F%\n# â€¦ F(\n\"\nÂ· F(\n#\nF(\n\"\nF$\n\"\nÂ· F!\n#\nF$\n\"\nÂ· F$\n#\nF$\n\"\nÂ· F%\n# â€¦ F$\n\"\nÂ· F(\n#\nF$\n\"\nF%\n\"\nÂ· F!\n#\nF%\n\"\nÂ· F$\n#\nF%\n\"\nÂ· F%\n# â€¦ F%\n\"\nÂ· F(\n#\nF%\n\"\nâ€¦ â€¦ â€¦ â€¦ â€¦â€¦\nF!\n#\nF$\n#\nF%\n# â€¦ F(\n#\nAverage\nFeature\nğ’™ğŸ\nAverage\nFeature\nğ’™ğŸ\n(a) Intra-DoCo (b) Inter-DoCo\nğ’™ğŸ\nğ’™ğŸ\nğ’™. .\nğ’™. .\nÂ·Â·Â·\nÂ·Â·Â·\n Â·Â·Â·\nÂ·Â·Â·\nFigure 4. An illustrative view of DoCo. The red and purple boxes\nrepresent the aggregated visual and multimodal features of docu-\nment objects, respectively. Best viewed in color.\nIntra-DoCo: For the purpose of achieving more fine-\ngrained representations in text-rich scenes, we establish an\nalignment between visual and multimodal features at the\nlevel of document object within the image, as opposed to\ndiscriminating image and text features at the overall im-\nage level. As depicted in Fig. 4 (a), the visual and mul-\ntimodal features of boxes within an image are denoted as\nFv\nintra =\nn\nË†fv\n1 ,Ë†fv\n2 , ...,Ë†fv\nN+1\no\nâˆˆ R(N+1)Ã—dv and Fm\nintra =n\nË†fm\n1 ,Ë†fm\n2 , ...,Ë†fm\nN+1\no\nâˆˆ R(N+1)Ã—dv respectively. The ob-\njective of Intra-DoCo is to predict the actual pairings from\nthe (N + 1)Ã— (N + 1)possible (Fv\nintra, Fm\nintra) combi-\nnations across a batch, which aims to maximize the cosine\nsimilarity of the visual and multimodal embeddings of the\nN +1 pairs illustrated by the blue blocks of Fig. 4 (a), while\nsimultaneously minimizing the cosine similarity of the em-\nbeddings of the(N +1)2 âˆ’(N +1) incorrect pairings shown\nwith the white blocks in Fig. 4 (a). Given that thei-th visual\nrepresentations are Ë†fv\ni âˆˆ Fv\nintra, the corresponding positive\nmultimodal features areË†fm\ni âˆˆ Fm\nintra, and the negative mul-\ntimodal features are Ë†fm\nj âˆˆ Fm\nintra and i Ì¸= j, the optimiza-\ntion is then defined as the sum of minimizing the similarity\nof positive sample pairs and maximizing the similarity of\nnegative sample pairs:\nLx\nIntra-DoCo = âˆ’ 1\nN + 1\nN+1X\ni=1\nlog\nï£«\nï£­ esim(Ë†fv\ni ,Ë†fm\ni )\nPN+1\nj=1 esim(Ë†fv\ni ,Ë†fm\nj )\nï£¶\nï£¸, (3)\nwhere sim computes similarity scores between the pairs. In\na symmetrical manner, we also calculate the loss from j to\ni as Ly\nIntra-DoCo in accordance with the CLIP model. And\nIntra-DoCo is optimizd using the InfoNCE loss function:\nLIntra-DoCo = (Lx\nIntra-DoCo + Ly\nIntra-DoCo)/2. (4)\nInter-DoCo: In contrast to the conventional approach\nof learning contrastive representations with the image-\nlevel instance discrimination, Inter-DoCo emphasizes the\ncomprehensive perspective of document objects across di-\nverse images. As illustrated in Fig. 4 (b), given a batch\nof B images X = {x1, x2, ..., xB} âˆˆ RBÃ—HÃ—WÃ—C,\nthe extracted visual and multimodal features are repre-\nsented as Ë†Fv =\nn\nË†Fv\n1, Ë†Fv\n2, ...,Ë†Fv\nB\no\nâˆˆ RBÃ—(N+1)Ã—dv\nand Ë†Fm =\nn\nË†Fm\n1 , Ë†Fm\n2 , ...,Ë†Fm\nB\no\nâˆˆ RBÃ—(N+1)Ã—dv respec-\ntively. To derive the global features of document ob-\njects, these visual and multimodal features are averaged\nto Fv\ninter =\nn\nF\nv\n1, F\nv\n2, ...,F\nv\nB\no\nâˆˆ RBÃ—dv and Fm\ninter =n\nF\nm\n1 , F\nm\n2 , ...,F\nm\nB\no\nâˆˆ RBÃ—dv respectively. Similarly, the\ncomputation from i to j is defined as follows:\nLx\nInter-DoCo = âˆ’ 1\nB\nBX\ni=1\nlog\n \nesim(Fv\ni ,Fm\ni )\nPB\nj=1 esim(Fv\ni ,Fm\nj )\n!\n, (5)\nand Inter-DoCo is defined as:\nLInter-DoCo = (Lx\nInter-DoCo + Ly\nInter-DoCo)/2. (6)\nConsidering both Intra-DoCo and Inter-DoCo, the total loss\nof DoCo is formulated as:\nLDoCo = LIntra-DoCo + LInter-DoCo. (7)\n3.5. Training Strategies\nPre-training: During the pre-training phase, the multi-\nmodal backbone remains static, while the image encoder\nundergoes optimization via the DoCo loss function to en-\nhance the visual feature representations.\nFine-tuning: Subsequent to the pre-training phase, the\nweights of the enhanced image encoder are utilized for\nfine-tuning, and the multimodal encoder is discarded. We\nkeep the image encoder and LLM frozen during fine-tuning,\nand update the parameters of the adaptor. Note we experi-\nment with various LVLMs, the adaptor represents the gen-\neral module for vision and language feature alignment, that\nis Position-Aware Vision-Language Adapter for Qwen-VL-\nChat [2] and Visual Abstractor for mPLUG-Owl [40]. The\nprimary aim of the training process is to minimize the cross-\nentropy of the textual tokens.\n5\n4. Experiments\n4.1. Datasets\nPre-training datasets: In order to guarantee the richness\nof text within the pre-training datasets, we adhere to the\nmethodologies proposed by LLaV A [18] and LLaV AR [41]\nfor the construction of these datasets. Specifically, these\ndatasets are comprised of approximately 1.0 million image-\ntext pairs. This includes a subset of 0.6 million data\nfrom the CC3M dataset [28], utilized by LLaV A [18] dur-\ning the pre-training phase, and an additional subset of 0.4\nmillion data from the LAION dataset [27], employed by\nLLaV AR [41] in the same phase. It is important to note that\nthese aggregated datasets undergo processing with the OCR\ntool PaddleOCR [7] to extract bounding boxes and textual\ncontents, thereby optimizing our proposed DoCo model.\nDataset Task Fine-tuning\n(Amt)\nEvaluation\n(Amt)\nTextVQA [30] VQA 35K 5.7k\nDocVQA [21] VQA 39K 5.2k\nChartQA [20] VQA 28K 2.5k\nOCRVQAâˆ— [23] VQA 200K 100k\nInfoVQA [22] VQA 24K 3.3k\nKLC [31] KIE 14k 4.9k\nWTQ [24] Table 14k 4.3k\nTextCaps [29] Image Caption 91K 16k\nTable 1. Statistics of the fine-tuning and evaluation datasets.\nâ€œOCRVQAâˆ—â€ denotes that we sample 25% data from the origi-\nnal dataset. â€œAmtâ€ means â€œAmountâ€.\nFine-tuning datasets: During the fine-tuning phase,\nLVLMs undergo training utilizing an array of text-rich\ndatasets, which include TextVQA [30], DocVQA [21],\nChartQA [20], OCRVQA [23], InfoVQA [22], KLC [31],\nWTQ [24] and TextCaps [29]. In Tab. 1, we provide a sum-\nmary of these datasets, which collectively comprise approx-\nimately 0.4 million fine-tuning data.\nEvaluation datasets: We perform extensive experiments\non the benchmark datasets reported in Tab. 1. In alignment\nwith previous studies, the performance on DocVQA [21]\nand InfoVQA [22] is evaluated by the Average Normal-\nized Levenshtein Similarity (ANLS). For the KLC [31]\ndataset, the F1-score is utilized as the evaluation criterion.\nThe primary metric for WTQ [24], OCRVQA [23], and\nTextVQA [30] is accuracy. In the context of ChartQA [20],\na relaxed accuracy metric is employed. Finally, the\nTextCaps [29] dataset is assessed using the Consensus-\nbased Image Description Evaluation (CIDEr) metric.\n4.2. Implementation Details\nThe experimental framework utilizes Qwen-VL-Chat [2]\nand mPLUG-Owl [40] as foundational models to illustrate\nthe competencies of DoCo. The image encoder utilizes the\nVision Transformer (ViT) [6] architecture, initialized with\nthe pre-trained weights from the ViT-bigG [25] of Openclip.\nFor the multimodal encoder, the LayoutLMv3 LARGE [12]\nis employed to extract the multimodal features of docu-\nment objects, whose configuration incorporates a 24-layer\ntransformer encoder, 16 self-attention heads, a hidden size\nof 1,024, and an intermediate size of 4,096 for the feed-\nforward network. During the training and inference stages,\nthe images are resized to 448 2. All experiments are\nconducted on a computational platform consisting of 128\nNVIDIA A100 80GB GPUs. The models undergo pre-\ntraining with a cumulative batch size of 640 over the course\nof 1 epoch, followed by fine-tuning with a total batch size\nof 256 over the span of 5 epochs. The models are trained\nutilizing the AdamW optimizer with Î²1 = 0.9, Î²2 = 0.98,\neps = 1eâˆ’6. The cosine learning rate schedule is adopted,\nsetting the maximum learning rate at 2eâˆ’4 and minimum at\n1eâˆ’6, with a linear warm-up of 500 steps. The models in-\ncorporate a weight decay of 5eâˆ’2 and a gradient clipping of\n1.0 to ensure the stability of the training process.\n4.3. Comparison with State-of-the-arts\nThe performance outcomes of various LVLMs on differ-\nent benchmarks pertaining to visual document understand-\ning are consolidated in Tab. 2, from which we can see that\nDoCo significantly enhances the efficacy of LVLMs. In de-\ntail, the mean score of the Qwen-VL-Chat [2], when inte-\ngrated with DoCo, surpasses the score of the model pre-\ntrained by CLIP [25] by approximately 2%, which demon-\nstrates the critical role of fine-grained document features.\nTo further investigate the generalization benefits of the pro-\nposed framework, a series of experiments are conducted to\njuxtapose it with other SOTAs. When evaluating our DoCo\non mPLUG-Owl [40], a similar trajectory of performance\nenhancement is also discerned, which exceeds the version\nwith CLIP [25] by 1.8%. It is important to note that the\nsame datasets delineated in Sec. 4.1 and experimental set-\ntings described in Sec. 4.2 are utilized to train these mod-\nels to maintain a fair and consistent comparison. Conclu-\nsively, our DoCo consistently enhances the visual represen-\ntations of image encoder within LVLMs in text-rich scenar-\nios, thereby bolstering the performance of visual document\nunderstanding.\nFig. 5 exhibits an assortment of qualitative outcomes en-\ngendered by Qwen-VL-Chat [2] between the version pre-\ntrained with CLIP [25] to the version optimized by DoCo\nacross a diverse range of document images. DoCo demon-\nstrates proficiency in not only the extraction of relevant data\nfrom the document but also in providing corresponding re-\nsponses by concentrating on unique regions. The integra-\ntion of contrastive learning, coupled with the multimodal\nfeatures of document elements, enhances the efficacy of\n6\nMethod Resolution OCRVQA TextVQA DocVQA InfoVQA ChartQA KLC WTQ TextCaps\nMiniGPT-4â€¡ [43] 2242 11.5 18.7 3.0 13.3 4.3 - - -\nmPLUG-Owlâ€¡ [40] 2242 28.6 40.2 6.9 16.5 9.5 - - -\nQwen-VL [2] 4482 75.7 63.8 65.1 - 65.7 - - -\nQwen-VL-Chat [2] 4482 70.5 61.5 62.6 - 66.3 - - -\nmPLUG-DocOwl [38] - - 52.6 62.2 38.2 57.4 30.3 26.9 111.9\nLLaV AR(336) [41] 3362 23.8 48.5 11.6 - - - - -\nUReader [39] 2242 - 57.6 65.4 42.2 59.3 32.8 29.4 118.4\nQwen-VL-Chatâ€  4482 71.1 61.7 62.2 33.1 67.3 31.5 24.8 112.3\nQwen-VL-Chatâ€ â€  4482 73.2 63.6 64.8 34.9 68.9 33.8 26.9 114.5\nmPLUG-Owlâ€  4482 70.3 53.5 61.8 32.5 58.3 31.2 25.2 113.4\nmPLUG-Owlâ€ â€  4482 72.1 55.7 63.6 34.1 60.1 32.9 26.4 115.9\nTable 2. Comparison results of different LVLMs on various benchmarks of visual document understanding. We use â€œ â€¡â€ to refer to the\nresults fetched from [19]. The models with â€œ â€ â€ and â€œâ€ â€ â€ denote pre-training with CLIP and DoCo respectively, which are optimized with\nthe same datasets and experimental settings for a fair comparison.\nQuestion: What is the \"phone number\" given \non the slip?\nQwen-VL-Chatâ€ : 5041-102\nQwen-VL-Chatâ€ â€  : 496 -3454\nQuestion : Can you identify what is the percentage \nvalue of foreign visitor in 2017?\nQwen-VL-Chatâ€ : 80\nQwen-VL-Chatâ€ â€  : 20\nQuestion : Find which category is shown in yellow \ncolor?\nQwen-VL-Chatâ€ : Adobe Flash 2.53\nQwen-VL-Chatâ€ â€  : PDF\nQuestion : What is the deadline given for \nAANP?\nQwen-VL-Chatâ€ : june 2002\nQwen-VL-Chatâ€ â€  : October 2001\nQuestion : What percentage of B2B survey \nrespondents search for information on soci al \nmedia?\nQwen-VL-Chatâ€ : 41%\nQwen-VL-Chatâ€ â€  : 55%\nFigure 5. Qualitative results between CLIP (â€œâ€ â€) and DoCo (â€œâ€ â€ â€). Crucial regions are enlarged for clearer visualization.\nLVLMs, thereby facilitating a more thorough comprehen-\nsion in scenarios abundant with text. More visualizations\nare provided in the supplementary material.\n4.4. Ablation Study\nTo investigate the efficacy of various components within\nour proposed DoCo, an extensive series of ablation studies\nbased on the Qwen-VL-Chat [2] model are conducted on\nthe DocVQA [21] dataset, which are summarized in Tab. 3.\nEffect of Intra-DoCo and Inter-DoCo:The data presented\nin Tab. 3 substantiates the assertion that the integration of\nIntra-DoCo into Qwen-VL-Chat (referred to as â€œ-w/ In-\ntra&R&I&B&Tâ€) significantly enhances the performance\nby 1.7% on the DocVQA dataset, in comparison to the ver-\nsion without DoCo involved in (â€œ-w/o DoCoâ€). This implies\nthat the utilization of Intra-DoCo for pre-training LVLMs\nis more proficient for visual document understanding. The\nobserved performance enhancement can be ascribed to the\ncongruence between patch-level image features and their\ncorresponding multimodal contextual features, achieved\nthrough a fine-grained and localized scheme. The imple-\nmentation of Inter-DoCo into Qwen-VL-Chat (referred to as\nâ€œâ€ â€ â€) further amplifies the average accuracy by 0.9%, sug-\ngesting that the intricate interaction between images can as-\n7\nStep 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8 Step 9 Step 10 Step 11 Step 12\n3 3 6 - 7 4 1 - 0 8 2 0\n4 1 4 2 7 0 - 8 0 2 0\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nQuestion: What is the phone number \nof CARR SMITH?\nFigure 6. Visualization of the heat-maps and generated tokens in each step from CLIP (â€œ â€ â€) and DoCo (â€œ â€ â€ â€) based on Qwen-VL-Chat.\nCLIP yields attention maps exhibiting drift, which encounters the absence of fine-grained features. In contrast, our DoCo can accurately\npredict the attention position of the subsequent step with the context information, which demonstrates our method can assist the vision\nencoder in acquiring more effective visual cues in text-rich scenes. Best viewed in color.\nMethod IntraInter R A I B T DocVQA\nQwen-VL-Chatâˆ—\nw/o DoCo âœ— âœ— âœ— âœ—âœ— âœ— âœ— 62.2\nQwen-VL-Chatâˆ—\nw/ Intra&R&I&B&T âœ“ âœ— âœ“âœ— âœ“âœ“âœ“ 63.9\nQwen-VL-Chatâˆ—\nw/ DoCo&A&I&B&T âœ“ âœ“ âœ—âœ“ âœ“âœ“âœ“ 64.2\nQwen-VL-Chatâˆ—\nw/ DoCo&R&T âœ“ âœ“ âœ“âœ— âœ— âœ—âœ“ 63.7\nQwen-VL-Chatâˆ—\nw/ DoCo&R&B&T âœ“ âœ“ âœ“âœ— âœ—âœ“âœ“ 64.4\nQwen-VL-Chatâ€ â€  âœ“ âœ“ âœ“âœ— âœ“âœ“âœ“ 64.8\nTable 3. Ablation studies of DoCo based on Qwen-VL-Chat on\nDocVQA dataset. â€œIntraâ€ and â€œInterâ€ denote â€œIntra-DoCoâ€ and\nâ€œInter-DoCoâ€. â€œRâ€ and â€œAâ€ stand for â€œROI Aggregationâ€ and â€œAv-\nerage Aggregationâ€ respectively. â€œIâ€, â€œBâ€ and â€œTâ€ represent â€œIm-\nageâ€, â€œBoxâ€ and â€œTextâ€ modalities of document objects separately.\nâ€œw/oâ€ and â€œw/â€ are short for â€œwithoutâ€ and â€œwithâ€.\nsimilate contextual information, thereby helping the model\ndiscern textual details from a global standpoint. These re-\nsults further corroborate the efficacy of the proposed con-\ntrastive training pattern in enabling the vision encoder to\ncapture more potent visual cues, thereby enhancing the\ncomprehension of text-rich documents in LVLMs.\nEffect of ROI Aggregation: To verify the capability of\nanother core component ROI Aggregation, we juxtapose\nit with Average Aggregation, a technique that calculates\nthe arithmetic mean of the patch features that overlap with\nthe corresponding bounding boxes. The empirical findings,\nas delineated in Tab. 3, underscore the superiority of ROI\nAggregation (referred to as â€œ â€ â€ â€) over Average Aggrega-\ntion (referred to as â€œ-w/ DoCo&A&I&B&Tâ€), with an im-\nprovement of 0.6%. We hypothesize that the facilitation\nis derived from the inherent ability of ROI Aggregation\nto concentrate on the regions of interest, thereby collating\nmore salient features for the text information.\nEffect of various modalities: Here, we aim to authen-\nticate the contributions of various modalities towards the\nfine-grained features. Compared with the performance of\nthe model labeled as â€œ-w/o DoCoâ€, which is pre-trained via\nthe traditional image-text contrastive learning at the whole\nimage level akin to CLIP, the incorporation of the textual\nfeatures of document objects optimized in the DoCo man-\nner (referred to as â€œ-w/ DoCo&R&Tâ€) results in an im-\nprovement of 1.5% on the DocVQA dataset. This confirms\nthe benefit of alignment at the document objects level be-\ntween visual features derived from the image encoder and\nthe textual features from the multimodal encoder. Encour-\nagingly, we also discern that the introduction of the layout\nmodality (referred to as â€œ-w/ DoCo&R&B&Tâ€) and image\nmodality (referred to as â€œâ€ â€ â€) further augment performance\nby 0.7% and 0.4% respectively. These observations demon-\nstrate that the fine-grained representations furnished by the\nmultimodal encoder can assist the image encoder of LVLMs\nin obtaining more effective cues for text-rich scenarios.\n4.5. Further Analysis on DoCo\nTo provide intuitive examples for explaining the mechanism\nof our method, we visualize the heat-maps derived from the\nimage encoder and the generated tokens at each decoding\nphase. These models are pre-trained utilizing CLIP and\nDoCo, based on the Qwen-VL-Chat framework. As de-\npicted in Fig. 6, we surprisingly observe that the heat-maps\ngenerated by DoCo tend to consistently concentrate on the\nfine-grained textual features that have a contextual relation-\nship, which proves that DoCo can assist the image encoder\nin capturing more efficacious visual cues in text-rich sce-\nnarios. Upon the removal of DoCo, the activation maps\nproduced by the original CLIP are scattered, a condition\nthat is not favorable for the comprehension of visual doc-\numents. Taking Fig. 6 as a case study, the attention heat-\nmaps of CLIP from the decoding steps 1 to 2, 3 to 4, and\n8 to 9 exhibit significant drift and weak semantic interre-\nlation, leading to a deviation from the correct response and\nyielding unsatisfactory outcomes. These unstable behaviors\ncan be ascribed to the absence of fine-grained features that\nare generated by the whole image-level discrimination. By\n8\ncomparison, the model that has been pre-trained with DoCo\nis more adept at leveraging the context information to accu-\nrately predict the attention position of the subsequent step,\nwhich proves that aligning the multimodal features to the vi-\nsual representations in the document object level is indeed\na more preferable way to solve the fine-grained feature col-\nlapse problem. These observations underscore the ability of\nDoCo to guide LVLMs in capturing salient details in a com-\nprehensive manner, which is particularly crucial for tasks\ninvolving visual document understanding.\n5. Conclusion\nIn this paper, we present a novel contrastive learning\nframework named Document Object COntrastive learn-\ning (DoCo), which aims to extract fine-grained features\nfor LVLMs in text-rich scenarios. Different from previ-\nous image-level contrastive learning methods which dis-\ncriminate the whole image instance between visual and\ntextual inputs, DoCo discriminates the document objects\nbetween image and multimodal features within an image\nand across images, enabling the image encoder of LVLMs\nto learn more effective visual representations for text-rich\nscenes. Experiment results show that LVLMs equipped\nwith DoCo can achieve superior performance and mitigate\nthe gap between visual document understanding and con-\nventional vision-language tasks.\nAppendices\nA. Multimodal Feature Extraction\nWithin DoCo, we utilize LayoutLMv3 [12] to extract the\nmultimodal features of the document objects, which encom-\npass textual embeddings, visual embeddings and layout em-\nbeddings. This section provides a comprehensive descrip-\ntion of the multimodal feature extraction process.\nTextual embeddings: We pre-process the document im-\nages with an off-the-shelf OCR toolkit PaddleOCR [7] to\nobtain textual content and bounding boxes of the document\nobjects. The textual embeddings are subsequently extracted\nwith the word embedding matrix from LayoutLMv3 [12],\nculminating in a sequence with a length of T and a dimen-\nsion of dm. The maximum value of T is established at 512.\nVisual embeddings: Inspired by LayoutLMv3 [12], the\ndocument image is resized to H Ã—W, and subsequently di-\nvided into a sequence of P Ã— P patches. These patches are\nthen linearly projected to a dimension of dm and flattened\ninto a sequence with a length of I = HW/P 2 = 196.\nLayout embeddings: For the layout embeddings, we fol-\nlow [12] to involve 1D and 2D position embeddings to the\nT textual tokens andI image patches, where the 1D position\nrefers to the index of tokens and the 2D position denotes the\nbox coordinates of the corresponding object layouts.\nAs a result, these embeddings are forwarded to the multi-\nmodal encoder to aggregate the multimodal features eFm =\b\nfm\n1 , fm\n2 , ...,fm\nN+1\n\t\nâˆˆ R(N+1)Ã—dm . Note that each object\nis processed as a sequence of fm\ni , and all the N + 1objects\b\nfm\n1 , fm\n2 , ...,fm\nN+1\n\t\nare processed in batch by the encoder.\nB. Fine-tuning Datasets\nDuring the fine-tuning stage, LVLMs undergo optimization\nutilizing approximately 0.4 million text-rich datasets, which\ninclude TextVQA [30], DocVQA [21], ChartQA [20],\nOCRVQA [23], InfoVQA [22], KLC [31], WTQ [24], and\nTextCaps [29]. These datasets can be classified into three\ntask categories: document Image captioning, key informa-\ntion extraction and document visual question answering.\nDocument image captioning: Document image caption-\ning involves generating descriptive text for a given docu-\nment image, necessitating models to interpret and rational-\nize the text within these images to produce accurate cap-\ntions. This process specifically requires models to integrate\na novel modality of text present in the images, and to reason\nover both this text and the visual content within the image to\ngenerate comprehensive image descriptions. To enhance the\nperformance of the model on the task of document image\ncaptioning, we utilize the training split of TextCaps [29].\nKey information extraction:Key information extraction\nin document understanding, alternatively referred to as\nProperty Extraction, denotes the methodological process of\npinpointing and extracting salient or pertinent information\nfrom a specified document. This extracted information can\nspan a wide array of data types, encompassing elements\nsuch as names, dates, geographical locations, organizational\nentities, financial figures, among other specific details that\nare integral to the comprehensive understanding of the doc-\numentâ€™s content. In the present study, we leverage the train-\ning and validation splits of the KLC [31] dataset to enhance\nthe performance of our model in executing the key informa-\ntion extraction task.\nDocument visual question answering: Document vi-\nsual question answering bears a superficial resemblance\nto knowledge information extraction in terms of struc-\nture. However, upon closer examination, the differences\nbecome more pronounced. Visual question answering\nnecessitates the interpretation of an open-ended set of\nquestions and the ability to handle a variety of docu-\nment types, thereby requiring superior generalization ca-\npabilities. Moreover, the specific content under analy-\nsis necessitates a more profound understanding of visual\nelements, as the questions often pertain to figures and\ngraphics that accompany the formatted text. To enhance\nthe performance of our model, we utilize a variety of\nhighly recognized public question-answering benchmark\n9\ndatasets, which include TextVQA [30], DocVQA [21],\nChartQA [20], OCRVQA [23], InfographicVQA [22] and\nWTQ [24].\nC. More Interpretability on DoCo\nIn Fig. 7 and Fig. 8 of this appendix, we visualize more\nheat-maps derived from the image encoder and the gener-\nated tokens at each decoding phase between CLIP [25] and\nDoCo based on the Qwen-VL-Chat [2] framework.\nFig. 7 elucidates the interpretability within text-rich docu-\nment scenarios. It is observed that the attention heat-maps\nof DoCo adeptly capture pertinent information associated\nwith the correct response, whereas the maps of CLIP dis-\nplay a significant drift and weak semantic interrelation de-\nviated from the ground truth, which is evident in the sec-\nond and fourth instances of the figure. Moreover, align-\ning the multimodal features to the visual representations in\nthe document object level enhances comprehension of fine-\ngrained text details and effectively mitigates recognition er-\nrors, which is corroborated by the first, third and fifth in-\nstances of the figure.\nFig. 8 further illustrates additional results within text-rich\nnatural scenes, following a similar pattern. For instance,\nin the first case, the CLIP model generates incorrect tokens\nâ€œbe-a-ure-gardâ€ from steps 7 to 10, which bear no relevance\nto the question â€œtypeâ€. Furthermore, the attention maps of\nCLIP display a significant lack of focus and drift from step\n7, resulting in the absence of fine-grained features. By com-\nparison, the maps of DoCo tend to focus on the continuous\nfine-grained textual features, yielding satisfactory results.\nAdditionally, our DoCo leverages the context information\nto predict the subsequent results, as demonstrated by the\nimage features of â€œ2002â€ in step 11, which are fuzzy but\ncan be inferred to the correct tokens from the context â€œen\n2002 parâ€ below.\nIn summary, Fig. 7 and Fig. 8 compellingly demonstrate\nthat DoCo can assist the image encoder in capturing more\neffective visual cues in text-rich scenarios.\nD. More Qualitative Results\nFig. 9 presents an expanded set of qualitative outcomes de-\nrived from a variety of benchmark datasets between CLIP\nand DoCo based on Qwen-VL-Chat [2]. The illustrations\nunderscore the superior generalization capacity of DoCo,\nwhich assists the vision encoder of LVLMs in acquiring\nmore efficacious cues and enhances comprehension in text-\nrich scenes.\nWe also delineate the failure cases of our method in Fig. 10.\nIt is evident that our DoCo still struggles with document-\nrelated commonsense reasoning and mathematical calcula-\ntions, which furnishes invaluable insights for the enhance-\nment of document comprehension capabilities with LVLMs\nin this domain. Future research endeavors will investigate\nthese problems and attempt to attack them by further im-\nproving visual understanding performance.\nE. Broader Impact\nThe remarkable proficiencies of LVLMs hold vast potential\nfor facilitating more robust document analysis and compre-\nhension in text-rich environments, but the significance of\nfine-grained features remains largely unexplored within the\nLVLM community. Thanks to the document object discrim-\nination between visual and multimodal representations, our\nproposed DoCo tailored for the fine-grained feature collapse\nissue can yield more precise results in text-rich scenarios.\nThe acquisition of more efficient fine-grained visual repre-\nsentations opens up a plethora of potential applications and\nopportunities for visual document understanding tasks. We\nadvocate for researchers to develop LVLMs integrated with\nDoCo for text-rich tasks, as we anticipate this to be particu-\nlarly advantageous.\nReferences\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 1,\n2\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 1,\n2, 3, 5, 6, 7, 10\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877â€“1901, 2020. 1, 2\n[4] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender\nKonukoglu. Contrastive learning of global and local fea-\ntures for medical image segmentation with limited annota-\ntions. Advances in neural information processing systems ,\n33:12546â€“12558, 2020. 3\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022. 1, 2\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 4, 6\n[7] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei\nLiu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing\n10\nClick news storieslinkson to\nLike * news stories\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nQuestion: What input situation is \nrepresented by the biggest bar?\nStep 1 Step 2 Step 3 Step 4 Step 5 Step 6\nQuestion: Who was writing this letter \nto  Dr.richard carchman?\nMaria\nHE ETA\nulleSh eta\nM ARA SH AL\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nStep 1 Step 2 Step 3 Step 4\nStep 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8\ns . c . m ck arn s\nS . J . Penn\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nQuestion: Who is \"Assoc R&D \nToxicologist\"?\nStep 1 Step 2 Step 3 Step 4 Step 5 Step 6\nR .P . Her et ick\nR . P . Her wick\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nQuestion: Who sent this?\nStep 1 Step 2 Step 3 Step 4 Step 5 Step 6 Step 7 Step 8\nDr . w . w . Fal oon\nDr . W . J . Falcon\nQwen-VL-Chatâ€ â€ Qwen-VL-Chatâ€ \nQuestion: Who is it addressed to?\nFigure 7. Visualization of the heat-maps and generated tokens from CLIP (â€œâ€ â€) and DoCo (â€œâ€ â€ â€) in text-rich document scenarios.\n11\nFigure 8. Visualization of the heat-maps and generated tokens from CLIP (â€œâ€ â€) and DoCo (â€œâ€ â€ â€) in text-rich natural scenes.\n12\nFigure 9. More qualitative results between CLIP (â€œâ€ â€) and DoCo (â€œâ€ â€ â€) based on the Qwen-VL-Chat model in text-rich scenes.\nQuestion: What's the average of all the values in the green bars (round to one decimal)?Qwen-VL-Chatâ€ â€ : 30.33Ground Truth: 21.6\n Question: Which is the biggest word in the infographic?Qwen-VL-Chatâ€ â€ : STATEGround Truth: America\n Question: what time is on the watch?Qwen-VL-Chatâ€ â€ : 10:10Ground Truth: 12:35\n Question: how many total films have they already appeared in?Qwen-VL-Chatâ€ â€ : 14Ground Truth: 17\nFigure 10. Failure cases of DoCo on document-related commonsense reasoning and mathematical calculations.\nDang, et al. Pp-ocr: A practical ultra lightweight ocr system.\narXiv preprint arXiv:2009.09941, 2020. 6, 9\n[8] Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wen-\ngang Zhou, Houqiang Li, and Can Huang. Unidoc: A uni-\nversal large multimodal model for simultaneous text detec-\ntion, recognition, spotting and understanding. arXiv preprint\narXiv:2308.11592, 2023. 2, 3\n[9] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440â€“1448,\n2015. 4\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll Â´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961â€“2969, 2017. 4\n[11] Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang,\nDaehyun Nam, and Sungrae Park. Bros: A pre-trained lan-\nguage model focusing on text and layout for better key infor-\nmation extraction from documents. In Proceedings of the\nAAAI Conference on Artificial Intelligence , pages 10767â€“\n13\n10775, 2022. 2\n[12] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu\nWei. Layoutlmv3: Pre-training for document ai with unified\ntext and image masking. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, pages 4083â€“4091,\n2022. 2, 4, 6, 9\n[13] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan. Supervised contrastive learning. Advances\nin neural information processing systems, 33:18661â€“18673,\n2020. 3\n[14] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung\nPark, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun,\nDongyoon Han, and Seunghyun Park. Donut: Docu-\nment understanding transformer without ocr. arXiv preprint\narXiv:2111.15664, 7:15, 2021. 2\n[15] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu,\nFangyu Liu, Julian Martin Eisenschlos, Urvashi Khandel-\nwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.\nPix2struct: Screenshot parsing as pretraining for visual lan-\nguage understanding. In International Conference on Ma-\nchine Learning, pages 18893â€“18912. PMLR, 2023. 2\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv\npreprint arXiv:2301.12597, 2023. 1, 2\n[17] Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi\nPang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Col-\nlier, and Julian Martin Eisenschlos. Matcha: Enhancing\nvisual language pretraining with math reasoning and chart\nderendering. arXiv preprint arXiv:2212.09662, 2022. 2\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 6\n[19] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin\nHuang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan\nLi, Lianwen Jin, et al. On the hidden mystery of ocr in large\nmultimodal models. arXiv preprint arXiv:2305.07895, 2023.\n7\n[20] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question an-\nswering about charts with visual and logical reasoning.arXiv\npreprint arXiv:2203.10244, 2022. 6, 9, 10\n[21] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.\nDocvqa: A dataset for vqa on document images. In Proceed-\nings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200â€“2209, 2021. 6, 7, 9, 10\n[22] Minesh Mathew, Viraj Bagal, Rub `en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 1697â€“1706, 2022. 6, 9,\n10\n[23] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In 2019 international conference\non document analysis and recognition (ICDAR), pages 947â€“\n952. IEEE, 2019. 6, 9, 10\n[24] Panupong Pasupat and Percy Liang. Compositional se-\nmantic parsing on semi-structured tables. arXiv preprint\narXiv:1508.00305, 2015. 6, 9, 10\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748â€“8763. PMLR, 2021. 2, 3, 4, 6, 10\n[26] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie\nPavlick, Suzana Ili Â´c, Daniel Hesslow, Roman Castagn Â´e,\nAlexandra Sasha Luccioni, Franc Â¸ois Yvon, Matthias GallÂ´e,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022. 1,\n2\n[27] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278â€“25294, 2022. 6\n[28] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages\n2556â€“2565, 2018. 6\n[29] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In Computer Visionâ€“ECCV\n2020: 16th European Conference, Glasgow, UK, August 23â€“\n28, 2020, Proceedings, Part II 16, pages 742â€“758. Springer,\n2020. 6, 9\n[30] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. InProceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317â€“8326, 2019. 6, 9, 10\n[31] Tomasz StanisÅ‚awek, Filip Grali Â´nski, Anna Wr Â´oblewska,\nDawid LipiÂ´nski, Agnieszka Kaliska, Paulina Rosalska, Bar-\ntosz Topolski, and PrzemysÅ‚aw Biecek. Kleister: key in-\nformation extraction datasets involving long documents with\ncomplex layouts. In International Conference on Document\nAnalysis and Recognition , pages 564â€“579. Springer, 2021.\n6, 9\n[32] Bo Sun, Banghuai Li, Shengcai Cai, Ye Yuan, and Chi\nZhang. Fsce: Few-shot object detection via contrastive pro-\nposal encoding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 7352â€“\n7362, 2021. 3\n[33] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang\nLiu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit\nBansal. Unifying vision, text, and layout for universal doc-\nument processing. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n19254â€“19264, 2023. 2\n[34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth Â´ee Lacroix, Bap-\n14\ntiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: open and efficient foundation language models,\n2023. URL https://arxiv. org/abs/2302.13971. 1, 2\n[35] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang\nXu, Peize Sun, Zhenguo Li, and Ping Luo. Detco: Unsuper-\nvised contrastive learning for object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 8392â€“8401, 2021. 3\n[36] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,\nand Ming Zhou. Layoutlm: Pre-training of text and layout\nfor document image understanding. In Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 1192â€“1200, 2020. 2\n[37] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei,\nGuoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang,\nWanxiang Che, et al. Layoutlmv2: Multi-modal pre-training\nfor visually-rich document understanding. arXiv preprint\narXiv:2012.14740, 2020. 2\n[38] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mplug-docowl: Modularized multimodal\nlarge language model for document understanding. arXiv\npreprint arXiv:2307.02499, 2023. 2, 3, 7\n[39] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nGuohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,\net al. Ureader: Universal ocr-free visually-situated language\nunderstanding with multimodal large language model. arXiv\npreprint arXiv:2310.05126, 2023. 2, 3, 7\n[40] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 1, 2, 3, 5, 6, 7\n[41] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced\nvisual instruction tuning for text-rich image understanding.\narXiv preprint arXiv:2306.17107, 2023. 2, 6, 7\n[42] Xiangyun Zhao, Raviteja Vemulapalli, Philip Andrew Mans-\nfield, Boqing Gong, Bradley Green, Lior Shapira, and Ying\nWu. Contrastive learning for label efficient semantic seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 10623â€“10633, 2021.\n3\n[43] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2, 7\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7946881055831909
    },
    {
      "name": "Natural language processing",
      "score": 0.6159994006156921
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5152738094329834
    },
    {
      "name": "Visual language",
      "score": 0.5143693685531616
    },
    {
      "name": "Linguistics",
      "score": 0.31720542907714844
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ]
}