{
    "title": "On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior",
    "url": "https://openalex.org/W3033254023",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287312531",
            "name": "Wilcox, Ethan Gotlieb",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4290223613",
            "name": "Gauthier, Jon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302924757",
            "name": "Hu, Jennifer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1985713137",
            "name": "Qian Peng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2511155128",
            "name": "Levy, Roger",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W162788685",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W2891399254",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2977268464",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2750539606",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2112106114",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W3027353876",
        "https://openalex.org/W2118276816",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2289899728",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2119728020",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2038536973"
    ],
    "abstract": "Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context. However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading. Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora. We find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is (near-)linear. We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations, the better its psychometric predictive power. However, we find nontrivial differences across model architectures. For any given perplexity, deep Transformer models and n-gram models generally show superior psychometric predictive power over LSTM or structurally supervised neural models, especially for eye movement data. Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.",
    "full_text": "On the Predictive Power of Neural Language Models for Human Real-Time\nComprehension Behavior\nEthan G. Wilcox1, Jon Gauthier2, Jennifer Hu2, Peng Qian2, and Roger P. Levy2\n1Department of Linguistics, Harvard University\n2Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology\n{jennhu,pqian,rplevy}@mit.edu\njon@gauthiers.net, wilcoxeg@g.harvard.edu\nAbstract\nHuman reading behavior is tuned to the statistics of natural lan-\nguage: the time it takes human subjects to read a word can be\npredicted from estimates of the word’s probability in context.\nHowever, it remains an open question what computational ar-\nchitecture best characterizes the expectations deployed in real\ntime by humans that determine the behavioral signatures of\nreading. Here we test over two dozen models, independently\nmanipulating computational architecture and training dataset\nsize, on how well their next-word expectations predict human\nreading time behavior on naturalistic text corpora. Consistent\nwith previous work, we ﬁnd that across model architectures\nand training dataset sizes the relationship between word log-\nprobability and reading time is (near-)linear. We next evalu-\nate how features of these models determine their psychometric\npredictive power, or ability to predict human reading behav-\nior. In general, the better a model’s next-word expectations\n(as measured by the traditional language modeling perplexity\nobjective), the better its psychometric predictive power. How-\never, we ﬁnd nontrivial differences in psychometric predictive\npower across model architectures. For any given perplexity,\ndeep Transformer models and n-gram models generally show\nsuperior psychometric predictive power over LSTM or struc-\nturally supervised neural models, especially for eye movement\ndata. Finally, we compare models’ psychometric predictive\npower to the depth of their syntactic knowledge, as measured\nby a battery of syntactic generalization tests developed using\nmethods from controlled psycholinguistic experiments. Once\nperplexity is controlled for, we ﬁnd no signiﬁcant relationship\nbetween syntactic knowledge and predictive power. These re-\nsults suggest that, at least for the present state of natural lan-\nguage technology, different approaches may be required to best\nmodel human real-time language comprehension behavior in\nnaturalistic reading versus behavior for controlled linguistic\nmaterials designed for targeted probing of syntactic knowl-\nedge.\nKeywords: Language modeling, real-time language compre-\nhension, deep learning, eye-tracking, self-paced reading\nIntroduction\nA large body of evidence suggests that humans are\nexpectation-based language processors, insofar as real-time\nlanguage comprehension involves making predictions about\nupcoming material (Levy, 2008; Hale, 2001). One strong\npiece of evidence supporting this view comes from the do-\nmain of computational modeling, where next-word log prob-\nabilities from statistical language models (LMs) turn out to\ncorrelate well with online processing measures—that is, to\nhave good psychometric predictive power—including gaze\nduration in eye-tracking studies and self-paced reading times\nScripts and data can be found online athttps://github.com/\nwilcoxeg/neural-networks-read-times.\n(Smith & Levy, 2013), and the N400 measure in EEG studies\n(Frank, Otten, Galli, & Vigliocco, 2015). Crucially, as statis-\ntical LMs improve on the broad-coverage objective function\nof perplexity (i.e. as they get better at predicting the next word\ngiven its context), so too do they improve at predicting real-\ntime processing data (Goodkind & Bicknell, 2018).\nMany of the previous studies linking information-theoretic\nmeasures and human psychometric data were conducted us-\ning n-gram models, which track local word co-occurrences\nand are blind to information outside of the n-gram window.\nRecently, however, neural network models such as Long\nShort-Term Memory Recurrent Neural Networks (LSTM-\nRNNs; Elman, 1990; Hochreiter & Schmidhuber, 1997) and\nTransformers (Vaswani et al., 2017) have set new standards\nin natural language processing, achieving state-of-the-art per-\nplexity results. We present a broad evaluation of these mod-\nern neural network models as predictors of human reading\nbehavior, testing the inﬂuence of both model inductive bias\nand the scale of training data provided to the model.\nOne important unanswered question involves the role of\nsyntactic knowledge in the link between statistical models\nand real-time processing. Experimental evidence, such as\nstudies of garden-path effects, demonstrates that humans de-\nploy hierarchically structured representations to drive pre-\ndictions about upcoming material (Stowe, 1986; Staub &\nClifton, 2006). This suggests that language models with sim-\nilar syntactic capacity — represented implicitly or explicitly\n— may be the best candidates for predicting human pro-\ncessing data. However, results from computational modeling\npaint a complicated story: while Frank and Bod (2011) found\nthat models without explicit hierarchical structure are best at\npredicting human reading times of naturalistic text, a follow-\nup study conducted by Fossum and Levy (2012) argued that\nperplexity, not inductive bias or syntactic capacity, was the\nprimary factor in determining a the ability of NLP models\nof that time to predict human reading times. The more re-\ncent work of Goodkind and Bicknell (2018), Aurnhammer\nand Frank (2019), and Merkx and Frank (2020) conﬁrm the\ngeneral ﬁnding that perplexity is the primary determinant\nof model ﬁt to human comprehension measures, but also\nﬁnd differences among model architectures once perplexity\nis controlled for.\nHere we contribute to this emerging picture through a\nscaled-up and carefully controlled assessment of language\narXiv:2006.01912v1  [cs.CL]  2 Jun 2020\nmodels’ ability to predict measures of human reading behav-\nior. Following Hu, Gauthier, Qian, Wilcox, and Levy (2020),\nwe train a ﬂeet of neural-network language models varying\nboth in inductive bias (from sequential LSTMs to syntax-\naware recurrent models) and in the amount of data provided\nto them at training time. We evaluate models’ psychometric\npredictive power for human reading times on three online pro-\ncessing datasets: the Dundee eye-tracking corpus (Kennedy,\n2003), selections from the Brown corpus and the Natural\nStories self-paced reading time corpus (Futrell et al., 2017).\nAcross model architectures and training datasets, our results\nbroadly conﬁrm the strong linear relationship between sur-\nprisal (or negative log probability) and reading time originally\ndocumented by Smith and Levy (2008, 2013) and conﬁrmed\nby Goodkind and Bicknell (2018). Like previous studies, we\nalso ﬁnd a generally positive relationship between a model’s\nnext-word prediction accuracy and its ability to predict hu-\nman reading times, supporting the ﬁndings of Goodkind and\nBicknell (2018) on a broad set of neural network models. Be-\nyond the role of perplexity, we ﬁnd that deep Transformer\nmodels demonstrate the best psychometric predictive power,\nand n-gram models achieve greater psychometric predictive\npower than would be expected based on their perplexity.\nWe next address the issue of syntactic knowledge. Rather\nthan positing a binary distinction between “hierarchical” and\n“non-hierarchical” models, we draw on recent work in lan-\nguage model evaluation to quantify models’ syntactic knowl-\nedge at a ﬁner grain (Hu et al., 2020). We compare each\nmodels’ psychometric predictive power against this measure\nof syntactic knowledge. After controlling for a model’s next-\nword prediction accuracy, we ﬁnd that syntactic knowledge\ndoes not explain signiﬁcant variance in a model’s psychome-\ntric predictive power.\nMethods\nModels\nWe train a ﬂeet of language models, each providing an es-\ntimate of word probability in context. The function of each\nlanguage model is to predict the next token in a corpus xi\nconditioned on its preceding context xj<i, producing a prob-\nability distribution Pmodel(xi |xj<i). Our ﬂeet contains four\nmajor architectural variants:\n• LSTM-RNNs are recurrent neural networks with Long\nShort-Term Memory units (Hochreiter & Schmidhuber,\n1997). We employ the boilerplate PyTorch implementation\n(Paszke et al., 2017).\n• Recurrent Neural Network Grammars(RNNGs; Dyer,\nKuncoro, Ballesteros, & Smith, 2016) model the joint\nprobability of a sequence of words as well as its syn-\ntactic structure. RNNGs are supervised during training\nwith Penn Treebank-style constituency parses (Marcus,\nMarcinkiewicz, & Santorini, 1993).\n• Transformers are deep neural networks which stack lay-\ners of self-attention mechanisms above word embedding\nrepresentations, which have recently achieved state-of-the-\nart performance on language modeling and set a new stan-\ndard for pretrained sentence encoding in natural language\nprocessing. We train the GPT-2 Transformer architecture\n(Radford et al., 2019) from scratch on our own corpora.\n• n-gram: We train a 5-gram model with Kneser-Ney\nsmoothing, using the SRILM language modeling toolkit\n(Stolcke, 2002).\nFollowing Hu et al. (2020), we train each model on four\ncorpora of varying sizes drawn from the Brown Labora-\ntory for Linguistic Information Processing (BLLIP) corpus\n(Charniak et al., 2000). The corpora are sampled such that\nthe training set of each corpus is a subset of each larger cor-\npus. The four corpora are BLLIP-XS (40K sentences, 100K\ntokens); BLLIP-SM (200K sentences, 5M tokens); BLLIP-\nMD (600K sentences, 14M tokens); and BLLIP-LG (2M sen-\ntences, 42M tokens). We trained 1–3 random seeds of each\nmodel architecture and training corpus.\nWhile the majority of the models tested here make predic-\ntions at the word level, some of our Transformers constitute\na notable exception. These models instead make predictions\nat the sub-word level, using a byte-pair encoding (BPE; Sen-\nnrich, Haddow, & Birch, 2015), which decomposes common\nword substrings into independent tokens. Models using this\nencoding can thus represent sublexical co-occurrence infor-\nmation. For the purposes of this paper, one of the most im-\nportant possible effects of this sub-word representation may\nbe that it supports well-tuned word probability estimates even\nfor very rare or unknown words. We train Transformer mod-\nels using both this BPE representation and standard word-\nlevel representations on the corpora mentioned above.\nThese language models are trained to mini-\nmize the perplexity of a corpus: PPL (model) =\n(∏i Pmodel(wordi |wordsj<i))−1\nN . Lower perplexity val-\nues correspond to language models that make more accurate\nnext-word predictions.1 As perplexity is interpretable only\nin the context of a speciﬁc vocabulary (i.e., over a space of\npossible next words), perplexity measures are only compara-\nble given a ﬁxed reference vocabulary. However, if a model\ntrained on a larger vocabulary has a better perplexity than a\nmodel trained on a smaller vocabulary, we can conﬁdently\nsay it is a better predictive model. This is generally the trend\nwe ﬁnd: models trained on larger corpora achieve better\nperplexity measures, despite being forced to predict over\nlarger vocabularies. Nonetheless, most of our analyses in\nthis paper will be comparing models with the same reference\nvocabulary to avoid this issue.\nPsychometric predictive power\nFollowing previous work (Frank & Bod, 2011; Fossum &\nLevy, 2012; Goodkind & Bicknell, 2018), we assess a\n1For models with sub-word representations, we deﬁne the prob-\nability of a word as the joint probability of its constituent subwords,\nfollowing the chain rule.\nmodel’s psychometric predictive power (termed ”psychologi-\ncal accuracy” by Frank and Bod) by asking how well its word-\nby-word surprisal estimates of the model can explain vari-\nous psychometric measures of how subjects read individual\nwords, after controlling for other features known to inﬂuence\nreading behavior, such as the length and frequency of words.\nWe draw psychometric data from three datasets across\ntwo measurement modalities of real-time human language\ncomprehension: eye-tracking data from the Dundee corpus\n(Kennedy, 2003); self-paced reading data from selections\nfrom the Brown corpus of American English (as reported in\nSmith and Levy (2013)); and self-paced reading data (herein\nSPRT) from the Natural Stories corpus (Futrell et al., 2017).\nThe Natural Stories corpus was explicitly designed to include\nsyntactic constructions that are relatively rare in both spoken\nand written English, such as object-extracted relative clauses,\ntopicalization, and long-distance dependencies.\nFor each language model, we ﬁt regression models which\npredict these psychometric data averaged across experimen-\ntal subjects. (For the Dundee eye-tracking corpus, we pre-\ndict the average gaze duration by subject for each word.)\nOur regression models combine model-speciﬁc and model-\ninvariant features of words. The main predictor of interest\nis word surprisal, or the negative logarithm of word proba-\nbility: Smodel(xi) = −log2 Pmodel(xi |xj<i). For each word\nread by a human subject, we extract the context-speciﬁc sur-\nprisal of the word and the previous word (or the previous\n3 words for SPRT) from a language model. The previous\nword estimates are included due to known spillover effects\nin both measurement paradigms (Smith & Levy, 2013). We\ncombine these surprisal estimates with model-invariant and\ncontext-invariant features of the current and previous word\n(or previous 3 words for SPRT) as control predictors: its\nlength in characters, and its log-frequency (or log-unigram-\nprobability).2\nWe evaluate each regression model relative to a baseline\nmodel, which attempts to predict the same human psychome-\ntric data from just the control features. For each language\nmodel, we compute its psychometric predictive power by cal-\nculating the mean by-token difference in log-likelihood of the\nresponse variable between the two models, which we refer to\nas ∆LogLik. A positive ∆LogLik value indicates that a lan-\nguage model’s surprisal estimates lead to more accurate pre-\ndictions of human reading behavior over the baseline model.\nWe repeat the above analyses with both generalized addi-\ntive models (GAMs) and linear regression. 3 Qualitative re-\nsults were similar with both approaches; unless otherwise\n2Word frequencies were measured from the larger Wikitext-2\ncorpus (Merity, Xiong, Bradbury, & Socher, 2017).\n3The R command to run the eye-tracking model was:read-time\n∼ s(surp, bs = \"cr\", k = 20) + s(prev.surp, bs\n= \"cr\", k = 20) + te(freq, len, bs = \"cr\") +\nte(prev.freq, prev.len, bs = \"cr\") for the GAM model\nand psychometric ∼ surprisal + prev surp + prev2 surp\n+ prev3 surp + freq * len + prev freq * prev len +\nprev2 freq * prev2 len + prev3 len * prev3 freq for the\nlinear model.\nnoted we report the linear regression results in ﬁgures and\nstatistical tests.\nOur methods differ from Goodkind and Bicknell (2018) in\ntwo respects: First, instead of reporting the difference in joint\nlog-likelihood of the entire dataset, we report the mean dif-\nference in log-likelihood between the baseline model and the\npredictive model on each individual token. Because the three\ncorpora tested in this paper are very different in size and com-\nposition, the joint log-likelihood cannot be used to compare\npsychometric predictive power results across testing corpora.\nThe second key difference is that, whereas Goodkind and\nBicknell (2018) report ∆LogLik of the model on the train-\ning data, we report mean per-word ∆LogLik of the model on\nheld-out test data, averaged over 10-fold cross validation, al-\nlowing us to conduct analyses using GAM ﬁts while guarding\nagainst overﬁtting.\nSyntactic Generalization score\nIn order to assess the syntactic capabilities of each model,\nwe report its score on the set of 34 targeted syntactic tests\npresented in Hu et al. (2020), which follow paradigms devel-\noped in Marvin and Linzen (2018), Futrell, Wilcox, Morita,\nand Levy (2018), and other recent papers on controlled\npsycholinguistics-style testing for grammatical knowledge.\nEach test is designed to probe whether the neural model has\nlearned a particular aspect of English syntax by examining\nits behavior across minimally different sentence pairs. For\nexample, Marvin and Linzen (2018) assess whether a model\nhas learned subject–verb number agreement by evaluating the\nmodel’s behavior on a construction such as The keys to the\ncabinet are/is.... If the model has learned the proper grammat-\nical generalization regarding subject-verb number agreement,\nthen it should assign lower probability to the ungrammatical\ncontinuation is compared to the grammaticalare, conditioned\non the ﬁxed preﬁx The keys to the cabinet.\nEach individual syntactic test comprises between 20–30\ntest items, with each item used in multiple experimental con-\nditions (generally 4, occasionally 2). In order for models to\nget the test item correct, their predictions must satisfy a set of\ninequality criteria among surprisals of regions of the sentence\nin each experimental condition. For example, following the\nlogic described above, for subject–verb number agreement,\na model must succeed at both of: (i) when the head noun\nof the subject NP is singular, the singular verb is should be\nmore likely than the plural verb are; and (ii) when the head\nnoun of the subject NP is plural, the plural verbare should be\nmore likely than the singular verbis. This design ensures that\nmodels will be unable to get high scores by relying on simple\nheuristics, such as a broad preference for plural verbs. We\nreport models’ mean by-test accuracy as its Syntactic Gener-\nalization (SG) score, which ranges from 0 to 1 with chance\nbeing ∼0.25.\nbllip-lg\n5gram\nbllip-lg\ngpt2\nbllip-lg\nrnng\nbllip-lg\nlstm\nbllip-md\n5gram\nbllip-md\ngpt2\nbllip-md\nrnng\nbllip-md\nlstm\nbllip-sm\n5gram\nbllip-sm\ngpt2\nbllip-sm\nrnng\nbllip-sm\nlstm\nbllip-xs\n5gram\nbllip-xs\ngpt2\nbllip-xs\nrnng\nbllip-xs\nlstm\nbnc-browndundeenatural-stories\n0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20\n0\n50\n100\n0\n50\n100\n0\n50\n100\nSurprisal (bits)\nSlowdown due to surprisal (ms)\nTraining data bllip-lg bllip-md bllip-sm bllip-xs BPE FALSE TRUE\nFigure 1: The relationship between surprisal and reading time. Lines are regressions from ﬁtted GAM models, using only con-\ntext sensitive predictors (i.e. surprisal of the current and previous words) to derive estimates. Shaded regions are bootstrapped\n95% conﬁdence intervals. Density plots of model-assigned surprisal values are below each ﬁt. The GAM ﬁts for the GPT-2\nBPE models (gptbpe, rightmost column) pool surprisal estimates from models trained on all corpora.\nResults\nSurprisal vs. Reading Times\nFigure 1 shows the relationship between language model sur-\nprisals and human reading times for all models and corpora.\nLines are ﬁts from generalized additive models (trained using\nthe formula described in Footnote 3), with only context sensi-\ntive predictors (i.e. surprisal of the current word and previous\nwords) used to derive estimates. They show the contribution\nof surprisal on reading time separate from word length and\nword frequency. Although there is some variance based on\nmodel architecture and training corpus, overall we ﬁnd a lin-\near relationship holds for most of the models tested.\nPredictive Power vs. Perplexity\nThe relationship between psychometric predictive power and\nperplexity is shown in Figure 2. Error bars denote the stan-\ndard error of by-fold mean ∆LogLik per token, estimated by\n10-fold cross validation. If better language models are bet-\nter predictors of human processing time, we would expect a\nnegative correlation between ∆LogLik and perplexity, which\nis visually evident for all three testing corpora. On average,\nthe Brown testing corpus shows slightly higher ∆LogLik, but\nalso higher variance across the 10-fold split.\nIn order to assess the relationship between perplexity and\npsychometric predictive power, we ﬁt a mixed-effects regres-\nsion model to predict ∆LogLik from language model per-\nplexity within each training vocabulary, with random inter-\ncepts by test corpus and model architecture. We ﬁnd a sig-\nniﬁcant effect of perplexity on ∆LogLik within each training\nvocabulary (p < 0.01), except for in the BLLIP-LG training\ndata, where p = 0.07. We take these results to indicate that\nthe relationship found in Frank and Bod (2011); Fossum and\nLevy (2012), and Goodkind and Bicknell (2018) between a\nmodel’s psychometric predictive power and its test perplex-\nity holds for a range of contemporary state-of-the-art mod-\nels, and for perplexity scores in the 30-100 range. However,\nwhereas Goodkind and Bicknell (2018) ﬁnd a strongly linear\nrelationship between perplexity and ∆LogLik, our results are\na bit more complicated: While there is a monotonic relation-\nship between ∆LogLik and perplexity, this may look more or\nless linear depending on the model class. For example, fo-\ncusing on the n-gram models tested on the Dundee corpus,\nthe relationship appears strongly linear across the 100–250\nperplexity range. However, focusing on the neural models in\nthe 30–100 perplexity range, the relationship appears more\nexponential, with stronger ∆LogLik gains between models in\nthe lower perplexity range.\nWhile all three testing corpora show a relationship between\nperplexity and ∆LogLik, we also ﬁnd an effect of model class\nfor Brown and Dundee. Here, the n-gram models demon-\nstrate predictive power comparable to the neural models de-\nspite much poorer perplexity scores. This is especially evi-\ndent for the BLLIP-SM and XS models tested on the Dundee\ncorpus. While the n-gram models’ perplexity is 2×that of the\nneural models, they achieve higher average ∆LogLik. While\nsurprising, this result accords with the ﬁndings presented in\nGoodkind and Bicknell (2018), who ﬁnd their LSTM model\n// // // // // // // // // // // // // // // // // //// // // // // // // // // // // // // // // // // // // // // // // // // // // // //// // // // // // // // // // // // // // // // // // // // // // // // // // // // //// // // // // // // // // // //\nbnc-brown dundee natural-stories\n50 100 150 200 250 500 50 100 150 200 250 500 50 100 150 200 250 5000.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.000\n0.003\n0.006\n0.009\n0.012\n0.000\n0.005\n0.010\n0.015\n0.020\nTest Perplexity\nΔLogLik per token\nModel\n5gram\ngpt2\nlstm\nrnng\nTraining vocabulary\nbllip-lg\nbllip-md\nbllip-sm\nbllip-xs\ngptbpe\nTest Perplexity vs. Predictive Power\nFigure 2: Relationship between predictive power ( ∆LogLik) and model perplexity. Error bars are standard errors of by-fold\nmean ∆LogLik per token, using 10-fold cross validation. As model perplexity decreases, predictive power increases for all test\ncorpora.\nto underperform relative to their n-gram models.4\nPsychometric Predictive Power vs. Syntactic\nGeneralization\nIn this section, we investigate the relationship between a\nmodel’s syntactic generalization (SG) ability and its psycho-\nmetric predictive power. The SG score is a models’ average\naccuracy across 34 targeted syntactic tests, whose designs are\ninspired by classic psycholinguistic assessments of human\nsyntactic abilities. Figure 3 reproduces Figure 2 from Hu\net al. (2020), which shows the range of SG scores achieved\nby our models, plotted against each model’s test perplexity.\nHu et al. (2020) argue that among the range of architectures\nand training dataset sizes investigated, it is model class, rather\nthan training data size or test perplexity, is the most important\ndeterminant of a model’s syntactic generalization capabili-\nties. For example, looking at Figure 3, the best performing\nLSTM model (squares) achieves a lower SG score than the\nlowest performing RNNG models (diamonds). The exception\nis GPT-2: the GPT-2 model trained on the smallest dataset\nperforms on par with then-gram models; however, the GPT-2\nmodels trained on larger datasets with BPE encoding perform\neven better than the best performing RNNG models.\nWe use SG scores to quantify the degree to which a\nmodel has derived human-like syntactic knowledge of lan-\nguage from text. Figure 4 shows the relationship between\nmodels’ SG scores and their psychometric predictive power\nas ∆LogLik. We plot this as a residualized regression, test-\ning the relationship between syntactic generalization score\nand ∆LogLik after controlling for the effects of perplexity\n4The LSTM model in that paper’s Figure 1 is the only model that\nfalls outside the regression’s 95% conﬁdence interval.\non both variables. The x-axis depicts each model’s syntactic\ngeneralization score residualized with respect to its perplexity\n(computed within each training vocabulary), and the y-axis\nshows each model’s∆LogLik residualized with respect to its\nperplexity (again computed within each training vocabulary).\nThe plot thus demonstrates the relationship between the two\nvariables unexplained by the relationship between perplexity\nand ∆LogLik.\nMany models in this ﬁgure show a large amount of variance\nin residual ∆LogLik unexplained by SG score, even when\ntrained on the same dataset. For example, the range of scores\nachieved by the RNNG BLLIP-XS model overlap with 16/25,\nor about 64%, of the other models. We conﬁrm this result\nquantitatively: in a stepwise regression analysis, SG scores\ndo not signiﬁcantly improve prediction of ∆LogLik over and\nabove perplexity measures of models ( p > 0.26 for all three\ncorpora).\nDiscussion\nThis paper tested the relationship between language model\nsurprisal estimates and human reading behavior across a\nbroad class of state-of-the-art language models, trained on\nvarying amounts of language data. We conﬁrmed the gen-\nerally linear relationship between word-level surprisal and\nhuman reading time in each of our replications, and discov-\nered that within model architecture, the relationship between\na language model’s next-word prediction performance and its\npsychometric predictive power is mostly monotonic. How-\never, the inﬂuence of language model architecture was sub-\nstantial. Furthermore, the inﬂuence of model architecture\non psychometric predictive power is not the same as the in-\nﬂuence of model architecture on performance on controlled\nrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random randomrandom random random\n// // // // // // // // // // // // // // //// // // // // // // // // // // // // // //// // // // // // // // // // // // // // //// // // // // // // // // // // // // // //// // // // // // // // // // // // // // //// // // // // // // // // // // //0.00\n0.25\n0.50\n0.75\n1.00\n50 100150200250500\nT est Perplexity\nSG Score\nT est PPL vs. SG Score\nFigure 3: Range of SG scores\nachieved by models plotted\nagainst perplexity.\nbnc-brown dundee natural-stories\n-0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2 -0.2 -0.1 0.0 0.1 0.2\n-0.0025\n0.0000\n0.0025\n0.0050\nResidual Syntax Generalization Score\nResidual ΔLogLik\nModel\n5gram\ngpt2\nlstm\nrnng\nTraining vocabulary\nbllip-lg\nbllip-md\nbllip-sm\nbllip-xs\ngptbpe\nSyntactic Generalization vs. Predictive Power\nFigure 4: Lack of relationship between models’ predictive power ( ∆LogLik) and Syntactic\nGeneralization score, both residualized with respect to model perplexity.\ngrammatical tests: we found no clear relationship between\nthe two types of evaluation metrics, once perplexity is con-\ntrolled (Figure 4).\nOur results complement and add to those of Aurnhammer\nand Frank (2019) and Merkx and Frank (2020), who use\nsimilar methodology to assess the psychometric predictive\npower of Transformers and gated vs. simple RNNs. The rela-\ntively strong performance of our n-gram model accords with\nAurnhammer and Frank’s (2019) ﬁnding that simple RNNs,\nwhich are more sensitive to local relationships, perform as\nwell as LSTMs and other gated models. Together these re-\nsults demand a more thorough investigation into the relation-\nship between locality and predictive power. One point of\ncontrast is that Merkx and Frank (2020) ﬁnd no advantage\nfor Transformer models at predicting human reading times\nin eye-tracking data, although they do ﬁnd an advantage for\nself-paced reading. The difference may be due to the assess-\nment metric, testing dataset size, byte-pair encoding or model\nsize (theirs has 2 layers, ours 12). Further investigation is re-\nquired.\nInterpreting our results in light of the ﬁndings presented in\nHu et al. (2020), who assess the relationship between perplex-\nity and syntactic generalization abilities, our ﬁndings suggest\na dissociation between two aspects of cognitive modeling us-\ning language models. On one hand, syntactic generalization\nabilities are largely determined by model architecture, with\nstructurally supervised models and deep Transformers out-\nperforming recurrent neural networks andn-gram models. On\nthe other hand, model ability to predict human reading times\nis determined more by model ability to accurately predict the\nnext word across a range of contexts, not just in specialized\nsyntactic testing. For these tasks, model architecture seems to\nplay less of an absolute role, although GPT-2 models trained\non larger datasets and enhanced with BPE achieve the highest\nscores on all three testing corpora.\nThe ﬁndings presented in this paper suggest that different\nlanguage comprehension contexts—isolated-sentence read-\ning with controlled materials targeting speciﬁc grammati-\ncal contrasts, versus reading of more naturalistic materials—\nbring to the fore different types of human linguistic expecta-\ntions that are in many cases best captured by different con-\ntemporary NLP models. As new model architectures and\ntraining procedures continue to emerge, continued examina-\ntion of the relationship with psychometric data can help guide\nthe way towards increasingly human-like high-performance\ncomputational models of language.\nAcknowledgments\nThe authors would like to thank the anonymous reviewers for\ntheir feedback. J.G. is supported by an Open Philanthropy AI\nFellowship. J.H. is supported by the NIH under award num-\nber T32NS105587 and an NSF Graduate Research Fellow-\nship. R.P.L. gratefully acknowledges support from the MIT-\nIBM Watson AI Lab, a Google Faculty Research Award, and\na Newton Brain Science Award.\nReferences\nAurnhammer, C., & Frank, S. (2019). Comparing gated and\nsimple recurrent neural network architectures as models of\nhuman sentence processing. In Proceedings of the 41st an-\nnual conference of the cognitive science society(pp. 112–\n118).\nCharniak, E., Blaheta, D., Ge, N., Hall, K., Hale, J., & John-\nson, M. (2000). BLLIP 1987–89 WSJ corpus release 1.\nLinguistic Data Consortium, Philadelphia, 36.\nDyer, C., Kuncoro, A., Ballesteros, M., & Smith, N. A.\n(2016). Recurrent neural network grammars. In Proceed-\nings of the 2018 conference of the north american chapter\nof the association for computational linguistics: Human\nlanguage technologies, volume 1 (long papers).\nElman, J. L. (1990). Finding structure in time. Cognitive Sci-\nence, 14(2), 179-211. doi: 10.1207/s15516709cog1402 \\\n1\nFossum, V ., & Levy, R. (2012). Sequential vs. hierarchical\nsyntactic models of human incremental sentence process-\ning. In Proceedings of the 3rd workshop on cognitive mod-\neling and computational linguistics(pp. 61–69).\nFrank, S. L., & Bod, R. (2011). Insensitivity of the human\nsentence-processing system to hierarchical structure. Psy-\nchological science, 22(6), 829–834.\nFrank, S. L., Otten, L. J., Galli, G., & Vigliocco, G. (2015).\nThe ERP response to the amount of information conveyed\nby words in sentences. Brain and Language, 140, 1–11.\nFutrell, R., Gibson, E., Tily, H., Blank, I., Vishnevetsky, A.,\nPiantadosi, S. T., & Fedorenko, E. (2017). The natural\nstories corpus. arXiv preprint arXiv:1708.05763.\nFutrell, R., Wilcox, E., Morita, T., & Levy, R. (2018). RNNs\nas psycholinguistic subjects: Syntactic state and grammat-\nical dependency.\nGoodkind, A., & Bicknell, K. (2018). Predictive power\nof word surprisal for reading times is a linear function of\nlanguage model quality. In Proceedings of the 8th work-\nshop on cognitive modeling and computational linguistics\n(CMCL 2018)(pp. 10–18).\nHale, J. (2001). A probabilistic Earley parser as a psycholin-\nguistic model. In Proceedings of the second meeting of\nthe north american chapter of the association for computa-\ntional linguistics on language technologies(pp. 1–8).\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term\nmemory. Neural Computation, 9(8), 1735-1780. doi: 10\n.1162/neco.1997.9.8.1735\nHu, J., Gauthier, J., Qian, P., Wilcox, E., & Levy, R. P. (2020).\nA systematic assessment of syntactic generalization in neu-\nral language models. In Proceedings of the association for\ncomputational linguistics.\nKennedy, A. (2003). The Dundee Corpus [cd-rom]. Psychol-\nogy Department, University of Dundee.\nLevy, R. (2008). Expectation-based syntactic comprehen-\nsion. Cognition, 106(3), 1126–1177.\nMarcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993).\nBuilding a large annotated corpus of English: The Penn\nTreebank. Computational Linguistics, 19, 313–330.\nMarvin, R., & Linzen, T. (2018, October-November). Tar-\ngeted syntactic evaluation of language models. In Pro-\nceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing(pp. 1192–1202). Brus-\nsels, Belgium: Association for Computational Linguistics.\nRetrieved from https://www.aclweb.org/anthology/\nD18-1151\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2017).\nPointer sentinel mixture models. In Proceedings of ICLR.\nMerkx, D., & Frank, S. L. (2020). Comparing transform-\ners and rnns on predicting human sentence processing data.\narXiv preprint arXiv:2005.09471.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., De-\nVito, Z., . . . Lerer, A. (2017). Automatic differentiation in\nPyTorch. In Neural information processing systems autod-\niff workshop.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., &\nSutskever, I. (2019). Language models are unsupervised\nmultitask learners.\nSennrich, R., Haddow, B., & Birch, A. (2015). Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nSmith, N. J., & Levy, R. (2008). Optimal processing times\nin reading: a formal model and empirical investigation. In\nProceedings of the 30th annual meeting of the Cognitive\nScience Society(pp. 595–600). Washington, DC.\nSmith, N. J., & Levy, R. (2013). The effect of word\npredictability on reading time is logarithmic. Cognition,\n128(3), 302–319.\nStaub, A., & Clifton, C. (2006). Syntactic prediction in lan-\nguage comprehension: Evidence fromeither . . . or. Journal\nof Experimental Psychology: Learning, Memory, & Cogni-\ntion, 32(2), 425–436.\nStolcke, A. (2002). SRILM – an extensible language model-\ning toolkit. In Seventh international conference on spoken\nlanguage processing.\nStowe, L. A. (1986). Parsing wh-constructions: Evidence\nfor on-line gap location. Language & Cognitive Processes,\n1(3), 227–245.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is\nall you need. In I. Guyon et al. (Eds.), Advances in neural\ninformation processing systems 30(pp. 5998–6008). Cur-\nran Associates, Inc. Retrieved from http://papers.nips\n.cc/paper/7181-attention-is-all-you-need.pdf"
}