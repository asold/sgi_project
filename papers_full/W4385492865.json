{
  "title": "Art authentication with vision transformers",
  "url": "https://openalex.org/W4385492865",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092094396",
      "name": "Ludovica Schaerf",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019419833",
      "name": "Eric Postma",
      "affiliations": [
        "Tilburg University"
      ]
    },
    {
      "id": "https://openalex.org/A5074740710",
      "name": "Carina Popovici",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388056646",
    "https://openalex.org/W2953861230",
    "https://openalex.org/W1536826092",
    "https://openalex.org/W2108240927",
    "https://openalex.org/W2078195226",
    "https://openalex.org/W2964289558",
    "https://openalex.org/W1987309482",
    "https://openalex.org/W1592886334",
    "https://openalex.org/W1793977698",
    "https://openalex.org/W2253709148",
    "https://openalex.org/W4221102170",
    "https://openalex.org/W4303712366",
    "https://openalex.org/W4200067971",
    "https://openalex.org/W2884238566",
    "https://openalex.org/W6600319451",
    "https://openalex.org/W2112422763",
    "https://openalex.org/W2100502520",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W3210162493",
    "https://openalex.org/W2796359075",
    "https://openalex.org/W3135562100",
    "https://openalex.org/W2949608135",
    "https://openalex.org/W4313007769"
  ],
  "abstract": null,
  "full_text": "S.I.: VISUAL PATTERN RECOGNITIO NA N DE X T R A C T I O NF O RC U L T U R A L\nHERITAGE\nArt authentication with vision transformers\nLudovica Schaerf1 • Eric Postma2,3 • Carina Popovici1\nReceived: 17 February 2023 / Accepted: 5 July 2023 / Published online: 2 August 2023\n/C211The Author(s) 2023\nAbstract\nIn recent years, transformers, initially developed for language, have been successfully applied to visual tasks. Vision\ntransformers have been shown to push the state of the art in a wide range of tasks, including image classiﬁcation, object\ndetection, and semantic segmentation. While ample research has shown promising results in art attribution and art\nauthentication tasks using convolutional neural networks, this paper examines whether the superiority of vision trans-\nformers extends to art authentication, improving, thus, the reliability of computer-based authentication of artworks. Using a\ncarefully compiled dataset of authentic paintings by Vincent van Gogh and two contrast datasets, we compare the art\nauthentication performances of Swin transformers with those of EfﬁcientNet. Using a standard contrast set containing\nimitations and proxies (works by painters with styles closely related to van Gogh), we ﬁnd that EfﬁcientNet achieves the\nbest performance overall. With a contrast set that only consists of imitations, we ﬁnd the Swin transformer to be superior to\nEfﬁcientNet by achieving an authentication accuracy of over 85%. These results lead us to conclude that vision trans-\nformers represent a strong and promising contender in art authentication, particularly in enhancing the computer-based\nability to detect artistic imitations.\nKeywords Art authentication /C1 Vision transformers /C1 Swin transformers /C1 Deep learning\n1 Art attribution and authentication\nArt attribution and art authentication are two signiﬁcant\ntasks in the cultural-heritage domain. The former involves\nidentifying the creator of an artwork, while the latter aims\nto verify whether the artwork was indeed crafted by the\npresumed artist. These tasks are important because they\ndirectly impact the economic and cultural value of artworks\n[1]. Art attribution entails analyzing various aspects of an\nartwork, such as its style, materials, and subject, in order to\ndetermine the artist responsible for its creation. This can be\na challenging task, especially for older works of art where\ninformation may be scarce or multiple artists might have\nbeen involved. Art authentication involves comprehensive\nscientiﬁc analysis, including the examination of pigments,\ncanvas, paint application techniques, and the historical\ncontext [ 2].\n1.1 Computer-based art attribution\nand authentication\nWith the rise of digital technology, computer-based visual\nanalysis of artworks has provided a new tool to support art\nattribution and authentication. Computer-based methods\nfor art attribution and authentication date back to the turn\nof the millennium, with the ﬁrst ‘visual stylometry’ efforts\n[3]. These works are characterized by the development of\n& Eric Postma\ne.o.postma@tilburguniversity.edu\nLudovica Schaerf\nludovica@art-recognition.com\nCarina Popovici\ncarina@art-recognition.com\n1 Art Recognition AG, Soodmattenstrasse 4, 8134 Adliswil,\nSwitzerland\n2 Department of Cognitive Science and Artiﬁcial Intelligence,\nTilburg University, Tilburg, The Netherlands\n3 Jheronimus Academy of Data Science, Eindhoven University\nof Technology/Tilburg University, St. Janssingel 92,\n5211 DA ’s-Hertogenbosch, The Netherlands\n123\nNeural Computing and Applications (2024) 36:11849–11858\nhttps://doi.org/10.1007/s00521-023-08864-8(0123456789().,-volV)(0123456789().,- volV)\nad hoc feature extraction methods (including fractal anal-\nysis, wavelet coefﬁcients, and edge detection) to represent\nvisual artistic features such as brushstrokes, followed by a\nmachine learning model trained on such features to dis-\ntinguish the works of the artist from possibly similar works\nby other artists [ 4–8]. The excellent pattern-recognition\nabilities of convolutional neural networks (CNNs) have led\nto a new wave of studies showing impressive performances\non art-classiﬁcation tasks [ 9–11] and many other visual\ntasks [ 12–14]. These studies involve complex CNN archi-\ntectures that are trained on large digitized art collections,\ngenerally adding to the CNN a last dense (fully connected)\nlayer. The last layer feeds into a single-output neuron in\ncase of art authentication or into N output neurons for art\nattribution to one of N artists [ 15].\nIt should be acknowledged that computer-based art\nattribution and authentication are not without their limita-\ntions and challenges. The ﬁrst group of limitations stems\nfrom the digital nature of the images used in this technique.\nThese images might have deformations and loss of infor-\nmation because of factors such as image resolution, light-\ning conditions, camera type, and post-processing\ncompression rate. The second group of limitations pertains\nto connoisseurship. Previous works [ 16, 17] have discussed\nthe role of the machine as a new type of art expert\nresponsible for attributions and authentications. Bell and\nOffert [ 16] have highlighted important similarities between\nhuman and machine connoisseur approaches, such as\nknowledge of numerous works by the same artist and\nrelated works. However, there are noteworthy differences\nthat constitute limitations of the computer-based tech-\nniques. While the computer relies solely on optical infor-\nmation (images), the human connoisseur also considers\ncontextual information, including but not limited to his-\ntorical knowledge, provenance, and scientiﬁc results.\nWhile most of the early studies have primarily focused\non traditional machine learning for art-attribution\ntasks [ 5, 6, 18, 19], our paper delves into the more speciﬁc\ntask of art authentication, using Vincent van Gogh as a case\nstudy. Our paper aims to perform a comparative evaluation\nof vision transformers (ViTs) [ 20–22] and CNNs on the art-\nauthentication task and determine the level of performance\nthat can be attained on this challenging task.\n1.2 Selection of architectures\nAs we are interested in a comparison between previous\nstate-of-the-art CNN-based methods and ViTs, we have to\nselect representatives of both types of methods. To select a\nCNN architecture, we determine the best-performing\narchitecture on art-classiﬁcation tasks by relying on a\nsample of studies performed over the last 10 years.\nAlthough the selected studies have been performed with\ndifferent methods and datasets, and mostly focused on art\nattribution instead of art authentication, their performances\nprovide a clear sign of the best-performing architecture.\nTable 1 lists the performances and performance measures\nfor ﬁve representative studies over the last 10 years. The\nperformances fall within a limited range, 78 /C0 91%. The\nbest-performing study of Table 1 [11] made use of the\nResNet101 architecture [ 23].\nHence, we select ResNet101 as one of the CNN archi-\ntectures for our experiments. As will be motivated in\nSect. 3.3, we include another CNN architecture called\nEfﬁcientNet [ 24] in our selection. For the ViTs we will rely\non two variants of a state-of-the-art architecture called\nSwin transformer [ 21].\nThe outline of the rest of the paper is as follows. Sec-\ntion 2 reviews CNNs and ViTs, highlighting the architec-\ntures used in this study, ResNet101, EfﬁcientNet, and the\nSwin transformer. Section 3 details the experimental pro-\ncedure, and Sect. 4 presents the results. Section 5 ends the\npaper with a conclusion and discussion of future work.\n2 Convolutional neural networks and vision\ntransformers\nConvolutional neural networks gained considerable popu-\nlarity with the 2012 release of AlexNet, which largely\noutperformed all previous models at the ILSVRC Ima-\ngeNet Challenge 2012 [ 12, 25, 26]. This popularity was\nfurther solidiﬁed by a continuous stream of improved\narchitectures and layers, most notably InceptionV3, VGG,\nand ResNet, and current state-of-the-art models such as\nEfﬁcientNet [ 23, 24, 27, 28].\nResNet and EfﬁcientNet are two of the most successful\nCNNs. ResNet, as described by He et al. [ 23], is a (po-\ntentially) extremely deep CNN. In contrast to a standard\nCNN, where each stage learns a function F(x) based on\ninput x, ResNet stages learn the residual function FðxÞ¼\nHðxÞ/C0 x by using skip connections. The use of skip con-\nnections allows ResNets to excel due to their increased\ndepth. EfﬁcientNet represents a class of CNN models\nintroduced by Tan and Le [ 24]. These models are opti-\nmized by scaling the width, depth, and input resolution of\nCNNs with a ﬁxed ratio. EfﬁcientNets have demonstrated\nsuperior performance compared to ResNets on image\nclassiﬁcation tasks. In our experiments, we use the variants\nResNet-101 and EfﬁcientNetB5.\nVision transformers are relatively new deep learning\narchitectures that have gained considerable attention and\npopularity in the computer vision community [ 20]. They\nrepresent a departure from traditional CNNs by replacing\nthe typical convolutional layers with attention mecha-\nnisms [ 29]. In linguistic tasks, the introduction of an\n11850 Neural Computing and Applications (2024) 36:11849–11858\n123\nattention mechanism facilitated the encoding of long-range\ncontextual information, which led to exceptional results on\na wide range of tasks [ 30]. Recent breakthrough perfor-\nmances of GPT4 [ 31] and related large language models\nare due to the power of transformers. One of the main\nadvantages of ViTs is their ability to capture relatively\nlong-range dependencies within an image, which is\nessential for a wide range of computer vision tasks. This is\nachieved through the attention mechanism, which allows\nthe model to attend to any region of the image when\nmaking predictions, rather than being limited to a ﬁxed\nimage context, like CNNs. ViTs have achieved state-of-\nthe-art results on several image classiﬁcation benchmarks,\nincluding ImageNet, and have shown promising results on\nother tasks, such as object detection and semantic\nsegmentation.\nThe Swin transformer was recently proposed as a gen-\neric transformer-based backbone for computer\nvision [ 21, 22]. The basic architecture is hierarchical and\nemploys an efﬁcient self-attention mechanism using shift-\ning windows. Its hierarchical architecture allows for cap-\nturing multi-scale relations, and its shifting windows\nmitigate the growth of computational complexity with\nimage size. Figure 1 illustrates a four-stage Swin trans-\nformer, the so-called Swin-Tiny variant. The input com-\nprises an image of size H /C2 W /C2 3 which is partitioned\ninto patches of size W=4 /C2 H=4 /C2 3 (the rectangle labeled\n‘‘Patch Partition’’). Each patch is embedded into a ‘‘token’’\nof size H=4 /C2 W=4 /C2 C by means of a linear layer\n(‘‘Linear Embedding’’), where C is an arbitrary dimen-\nsionality parameter of the Swin architecture. The token is\nfed into the building block of the Swin transformer\n(‘‘SWIN Transformer Pair’’), the inner structure of which is\nillustrated in Fig. 2. The ﬁrst block consists of layer nor-\nmalization, multi-head attention, layer normalization, and\ntwo multilayer perceptrons. The multi-head (self-)attention\nis applied within non-overlapping M /C2 M windows of the\ninput token ( M ¼ 7). The curved arrows represent skip\nconnections. The second block is identical to the ﬁrst, but\napplies attention to shifted M /C2 M windows.\nTo create a hierarchical ‘‘pyramid-like’’ representation,\nin the second stage of Fig. 1 the ‘‘patch merging’’ con-\ncatenates all values of each non-overlapping 2 /C2 2 /C2 C\nregion into 1 /C2 1 /C2 4C values and uses a linear layer to\nmap these onto 2 C values. As a result, the\nH=4 /C2 W=4 /C2 C\noutput of the ﬁrst stage is transformed into a patch-merged\noutput of dimensions H=8 /C2 W=8 /C2 2C. The third stage\nperforms the same steps as the second stage, but applies\nthree Swin transformer pairs instead of one. Finally, in the\nfourth stage, patch merging is combined with a single Swin\ntransformer pair. In our experiments, the output of the\nTable 1 Overview of\nperformances in art\nclassiﬁcation over the last 10\nyears\nReferences Year Method Measure Performance\n[5] 2013 Wavelets Attribution accuracy 85 /C0 88%\n[9] 2015 CNN (AlexNet) Mean class accuracy 78 %\n[6] 2016 Geometric tight frame Attribution accuracy 87 /C0 89%\n[10] 2017 Multi-scale CNN Mean class accuracy 82 %\n[11] 2022 ResNet101 Mean class accuracy 91 %\nFig. 1 Schematic illustration of the ‘ ‘tiny’’ Swin transformer (Swin-Tiny). Based on [ 21]\nNeural Computing and Applications (2024) 36:11849–11858 11851\n123\nfourth Swin transformer pair is average pooled and sub-\nmitted to a binary classiﬁer.\nApart from the Swin-Tiny variant, three larger variants,\nwhich differ from Swin Tiny in the value of the dimen-\nsionality parameter C and in the number of Swin Pairs in\nthe third stage N, have been proposed. In our experiments,\nwe use the Swin-Tiny ( C ¼ 96, N ¼ 3) and Swin-base\n(C ¼ 128, N ¼ 9) variants.\n3 Experiments\nThis section speciﬁes the experiments by discussing the\nvan Gogh dataset Sect. 3.1, the data preparation and aug-\nmentation Sect. 3.2, the speciﬁc CNN and Swin architec-\ntures and their hyperparameter settings Sect. 3.3, and our\nevaluation procedure Sect. 3.4.\n3.1 Van Gogh dataset\nOur dataset for the authentication task was carefully col-\nlected and consists of 654 images of authentic paintings\n(authentic set) and 669 or 137 images of non-authentic\nones (depending on the type of contrast set). The resolu-\ntions of the images of artworks vary from one reproduction\nto another. In what follows, we outline the authentic set and\ntwo versions of the contrast set: the ‘‘standard contrast set’’\nand the ‘‘reﬁned contrast’’ set. As will be described in\nSect. 4, the development of the reﬁned contrast is moti-\nvated by the results on the standard contrast set, which\nreveal that art authentication requires a more constrained\nselection of artworks in the contrast set. The composition\nof each contrast set is described below.\n3.1.1 Authentic set\nWhen compiling our authentic set, we have used the\nstandard ’La Faille’ Catalogue Raisonne´ [32] as a refer-\nence, meaning that all authentic images used for training\nare recorded there. Moreover, we have removed from the\nauthentic set the images whose authenticity is questioned\nby contemporary experts. This approach enables us to\nmitigate the risk of accidentally introducing fake artworks\ninto the original dataset (label noise). The careful crafting\nof the authentic set distinguishes this work from previous\nones, which are usually trained on images downloaded\nfrom WikiArt [ 33] (a less reliable source as compared to\nthe established Catalogue Raisonne´).\n3.1.2 Contrast set\nAs art authentication involves a binary classiﬁcation task,\nwe carefully compile a second set that serves as a contrast\nto the authentic works. This secondary set consists of\nnegative examples, i.e., artworks that are not attributed to\nvan Gogh.\n3.1.3 Standard contrast set\nThe standard contrast set features 69 imitations: 10 copies\nby followers of van Gogh such as Vik Muniz, Blanche\nDerousse, and Jamini Roy; 40 imitations in van Gogh’s\nstyle; and 21 known forgeries, including 8 produced by the\nfamous forger Wacker [ 34, 35]. In addition, to achieve a\nbalance with the authentic set, the standard contrast set also\nincorporates 600 proxies which are paintings by contem-\nporary artists who utilized techniques and styles similar to\nthose of van Gogh—mainly Post-Impressionism, Cloison-\nnism, and Japonism. The main proxy artists are Paul\nCe´zanne (114 images), Henri de Toulouse-Lautrec (48\nimages), Maurice Prendergast (47 images), and Henri\nMatisse (47 images).\n3.1.4 Refined contrast set\nIncluding proxies in the standard contrast set introduces\npainting styles that differ greatly from those of van Gogh.\nHence, for the construction of our reﬁned contrast set, we\nremove all proxies and gather additional imitations from\nauction archives. We include 68 additional images that\nwere cataloged as being inspired by van Gogh: 50 images\nare described as After Vincent van Gogh , 14 are in Manner\nof Vincent van Gogh , 2 are Attributed to Vincent van Gogh ,\n1i s Circle Vincent van Gogh , and 1 is Follower Vincent\nvan Gogh . Table 2 shows the composition of the reﬁned\ncontrast set relative to the standard contrast set.\nFig. 2 Illustration of the inner structure of the Swin transformer pair.\nBased on [ 21]\n11852 Neural Computing and Applications (2024) 36:11849–11858\n123\n3.2 Data preparation\nThe dataset consists of sub-images of paintings, i.e., RGB\nimages normalized to a ﬁxed size of 256 /C2 256 pixels, and\nthe channel values normalized to the unit interval. The sub-\nimages are created by dividing the whole image into 2 p /C2\n2p equally sized units, with p depending on the resolution\nof the original image as follows: p ¼ 2, if the smaller side\nof an image is larger than 1024 pixels, and p ¼ 1, if the\nsmaller side is larger than 512 pixels and smaller than\n1024. For all images, regardless of the resolution, we also\ninclude the sub-image of the center-cropped square stem-\nming from the full image. Figure 3 exempliﬁes the gen-\neration of 16 squared, center-cropped patches from an\nauthentic van Gogh painting. This patching method allows\nthe models to extract very ﬁne-grained brushstroke-level\ninformation from the smaller patches, but also more com-\npositional and representational features from the full patch\nand the larger patches. Some of the examined architectures\nrequire an input size of 224 /C2 224 pixels. In that case the\noriginal 256 /C2 256 sub-images were downsampled using\nbicubic resampling.\nTo emphasize the importance of imitations over proxies,\nin the standard contrast set, we assign sample weights w\nim\nto the imitations. In preliminary experiments, we found that\nwim ¼ 10, showing that imitations weight ten times more\nthan proxies, yields the best results. This value will remain\nconsistent across the experiments conducted using the\nstandard contrast set described in this study. In the reﬁned\ncontrast set, we did not employ sample weighting, setting\nw\nim ¼ 1.\nWe evaluate each model in N ¼ 20 experiments. In each\nexperiment, we randomly assign the paintings including\ntheir constituent patches, to the training, validation, and\ntest partitions. These random assignments result in\nN training, validation, and test partitions. Each model is\ntrained and evaluated on exactly the same N partitions.\nThis ensures that each architecture is trained and evaluated\nin the same manner, which enables a fair comparative\nevaluation. Table 3 lists the compositions of the partitions\nin terms of the number of images for the authentic and\ncontrast sets. In each experiment, a randomly selected\nsubset of authentic images of approximately the same size\nas the size of contrast images is used for training.\nBecause we subdivide each image into patches, the\nactual number of patches in each partition is much larger.\nFor instance, for the experiments with the standard contrast\nset, the actual numbers vary slightly (because images differ\nin their number of patches): About ﬁfteen thousand patches\nin the training set and two thousand patches in the vali-\ndation and test partition each. We emphasize that all pat-\nches of each painting are always assigned to the same\npartition. As a consequence, the test set always consists of\npatches that were not part of the training or validation\npartitions.\nTable 2 Composition of the standard and reﬁned contrast sets. The\nproxies for the standard contrast set consist of artists that painted in\nthe same styles as van Gogh, whereas for the reﬁned contrast set the\nimitations are expanded and include imitations from auction records\nStandard contrast set Reﬁned contrast set\nType # images # images\nImitations 69 137\nProxies 600 0\nFig. 3 Left side: original image. Right side: 4 /C2 4 grid and center-\ncropped squared patches highlighted in bright regions. The image\nshows the Oliviers avec ciel jaune et soleil . Collection: Minneapolis\nInstitute of Art, Location: Minneapolis Institute of Art, oil on canvas.\nAvailability: public domain, CR number: F710\nNeural Computing and Applications (2024) 36:11849–11858 11853\n123\n3.3 Architectures and training procedure\nThe recent outstanding art-classiﬁcation results reported by\nDobbs and Ras [ 11], as discussed in Sect. 1.2, have led us\nto choose ResNet101, the 101-layer version of ResNet [ 23],\nas a representative CNN for our van Gogh authentication\ntask. Although ResNet101 represents the state of the art in\nart classiﬁcation, it may not be the most robust CNN\navailable. Therefore, to provide a more comprehensive\nevaluation, we include another CNN in our analysis that\nbetter represents the class of modern CNNs: EfﬁcientNet\n[24]. Speciﬁcally, we select EfﬁcientNetB5, as its com-\nplexity (measured by the number of parameters) roughly\nmatches that of the simplest Swin transformer. For our\nexperiments, we utilize two variations of Swin transform-\ners (Swin Tiny and Swin Base), with the detailed\ndescription of the Swin transformer architecture provided\nin Sect. 2.\nUsing the standard contrast set, the four architectures\nexamined are: EfﬁcientNetB5, ResNet101, Swin-Tiny, and\na larger version called Swin-Base. The latter is included to\ndetermine the potential beneﬁcial effect of this larger Swin\ntransformer variant. EfﬁcientNetB5 has 28 M parameters,\nResNet101 has 44.7M parameters, and Swin-Tiny and\nSwin-Base have 28 M and 88 M parameters, respectively.\nAll architectures are pretrained on ImageNet [ 25].\nResNet101, EfﬁcientNetB5, and Swin-Tiny are pre-trained\non the 1K version of ImageNet, whereas Swin-Base is pre-\ntrained on the 22K version of ImageNet.\nIn preliminary experiments we explored three variants\nof transfer learning: (i) freezing the base architecture and\ntraining a new top layer (the standard method of transfer\nlearning), (ii) initially freezing the base, training the new\ntop layer, and subsequently training the base and top with a\nsmall learning rate, and (iii) unfreezing all layers and\ntraining the entire architecture with a small learning rate. It\nturned out that variant (iii) gave the best results for all\narchitectures, which is in line with previous ﬁndings in art\nclassiﬁcation [ 15, 36]. Hence, in contrast to what is typical\nto transfer learning, we employed variant (iii), where the\ntop was deﬁned as a randomly initialized dense layer. For\nthe initialization, we use ‘‘He normal’’ initialization [ 37]\nthat ensures that the random weight values do not saturate\nthe receiving neurons’ activations. To this end, the w\nn\nvalues of the weights feeding into a neuron are drawn from\na (truncated) normal distribution with l ¼ 0 and\nr ¼\nﬃﬃ\nð\np\n2=wnÞ. For all architectures, training is performed\nwith binary cross-entropy as loss function, the Adam\noptimizer, batch size 32, learning rate 0.0001, early stop-\nping (patience ¼ 20 epochs and minimum delta ¼ 0:001),\nand imitation-sample weights w\nim ¼ 10.\nFor the experiments with the reﬁned contrast set, we\napply the same training procedure but do not use imitation-\nsample weights and restrict ourselves to EfﬁcientNetB5\nand Swin-Tiny. The motivation for focusing on these two\narchitectures is twofold: (i) both architectures perform best\nin the experiments with the standard contrast set, and (ii)\ncomparing the performances of these architectures is fair\ndue to their almost equal parameter complexity.\n3.4 Evaluation procedure\nFor each architecture, we performed N ¼ 20 experiments\nand report the average prediction accuracies for individual\npatches and for the entire paintings. The latter is deter-\nmined for each artwork by taking the mean of the predic-\ntions of its constituent patches, including the sub-image\nwith a center-cropped square stemming from the full\nimage. To further understand the model’s performance, we\npresent accuracies per class, distinguishing between the\nauthentic and the contrast classes. Additionally, within the\ncontrast set, we provide separate accuracies for proxies and\nimitations.\n4 Results\nIn this section, we present separately the results for the\nexperiments conducted with both the standard contrast set\nand the reﬁned contrast set.\n4.1 Results for the standard contrast set\nTable 4 reports the results obtained with the standard\ncontrast set. For each of the examined architectures (with\nthe pretraining variants mentioned in Sect. 3.3), it lists the\nmean accuracy for the patches and the entire paintings, as\nwell as the number of parameters for each architecture.\nFrom these results, we draw three observations. The\nmain observation is that EfﬁcientNetB5 yields the best art-\nauthentication performance, both on patches and on entire\nTable 3 Number of images in\neach of the three partitions for\nthe experiments with the\nstandard and reﬁned contrast set\nStandard contrast Reﬁned contrast\n# images Training Validation Test Training Validation Test\nAuthentic set 520 78 73 87 20 30\nContrast set 523 65 65 87 20 30\n11854 Neural Computing and Applications (2024) 36:11849–11858\n123\nimages. We reiterate that in terms of the number of\nparameters, EfﬁcientNetB5 has roughly the same com-\nplexity and initialization as Swin-Tiny (i.e., 28 M and\nImageNet 1K, respectively), which makes it a fair com-\nparison. The second observation is that both Swin archi-\ntectures yield a considerable improvement in performance\nregarding ResNet101, i.e., accuracies /C25 0:89 /C0 0:90\nroughly matching the performances listed in Table 1 in\nSect. 1.2. The third observation is that although the Swin-\nBase transformer performs marginally better than the\nSwin-Tiny transformer on patches, it does not result in a\nbetter performance on paintings. At ﬁrst sight, these results\nsuggest EfﬁcientNetB5 outperforms ResNet101 and the\nSwin transformers on art-authentication tasks. However, a\ncloser examination of the results for the constituents of the\nstandard contrast set leads to a different view.\nTable 5 lists the accuracies for the authentic and stan-\ndard contrast sets, as well as the two constituent types of\ncontrast artworks: imitations and proxies. The results show\nthe performances obtained by all architectures mainly\nreﬂect a successful separation of authentic paintings and\nartworks by proxies, given that both have accuracies of\nmore than 90 %. On the other side, the performance on the\nimitations is considerably lower, despite the use of sample\nweights. We acknowledge that the task of distinguishing\nimitations from originals is a much more complex and ﬁne-\ngrained one, than distinguishing proxies from originals.\nProxies are artworks created by known artists in their own\nstyle, albeit similar to the style of van Gogh, while the\nimitations (including copies and forgeries) contain only\nartworks that were created, explicitly or implicitly in the\nstyle of van Gogh, with a clear and close emulation of the\nartist. Thus, this last category contains artworks with a\nmuch higher degree of similarity to the authentic ones.\nClearly, art authentication requires a ﬁne distinction\nbetween imitations and authentic art. Hence, the poor\nperformance on the imitations motivated the development\nof the reﬁned contrast set. The results of our experiments\nwith the reﬁned contrast set are the subject of the next\nsection.\n4.2 Results for the refined contrast set\nAs mentioned in Sect. 3.3, we trained the two comparable\narchitectures on the van Gogh dataset by using a reﬁned\ncontrast set which only comprises imitations. We did not\nuse sample weights for these experiments. The obtained\nresults are presented in Tables 6 and 7.\nTable 6 shows the accuracies for paintings in the\nauthentic and reﬁned contrast sets. We observe that in this\ncase, a much better balance is achieved between the per-\nformances on the authentic and contrast artworks. This\napplies especially to Swin-Tiny, which outperforms Efﬁ-\ncientNetB5 and achieves the best overall performance. The\nmuch-improved performance on the imitations also sug-\ngests the relative improvement of the second dataset with\nrespect to the ﬁrst, as this second dataset tackles best the\ncore of art authentication: the separation between authentic\nworks and reproductions. Alongside this reasoning, the\nsuperior performance of Swin-Tiny on this second dataset\nsuggests a non-negligible improvement over state-of-the-\nart CNNs.\nTable 7 lists the mean accuracy, precision, and recall for\nEfﬁcientNetB5 and Swin-Tiny. The latter scores best on all\nthree metrics, showing that Swin-Tiny exhibits the best\npainting-based authentication performance.\nTable 8 provides insight into the degree of overlap of\npatch predictions made by both architectures. The confu-\nsion table shows the percentages of patches predicted\nTable 4 Overview of the results obtained with the standard contrast set. For each architecture, the accuracies averaged over N ¼ 20 experiments\nand standard deviations are listed for the patches and entire images\nArchitecture Number of parameters Patches accuracy (SD) Paintings accuracy (SD)\nEfﬁcientNetB5 30 M 0.912 (0.027) 0.925 (0.032)\nResNet101 45 M 0.861 (0.027) 0.868 (0.032)\nSwin-Tiny 28 M 0.893 (0.017) 0.895 (0.023)\nSwin-Base 88 M 0.898 (0.018) 0.894 (0.024)\nTable 5 Painting-based test\naccuracies for the authentic and\nstandard contrast sets, and the\ntwo types of contrast types:\nimitations and proxies\nArchitecture Accuracy authentic Accuracy contrast Accuracy imitations Accuracy proxies\nEfﬁcientNetB5 0.954 (0.029) 0.898 (0.047) 0.527 (0.32) 0.975 (0.035)\nResNet101 0.905 (0.062) 0.836 (0.062) 0.446 (0.15) 0.917 (0.060)\nSwin-tiny 0.912 (0.046) 0.880 (0.046) 0.526 (0.21) 0.954 (0.040)\nSwin-base 0.905 (0.047) 0.883 (0.057) 0.585 (0.28) 0.946 (0.045)\nNeural Computing and Applications (2024) 36:11849–11858 11855\n123\ncorrectly and incorrectly by both architectures. While they\nagree on the majority of correctly classiﬁed patches (79%),\ntheir disagreement is limited to smaller percentages (7.4%\nand 5.7%). Both architectures incorrectly classify a slightly\nlarger percentage (8.2%) of patches. The differences in\ncorrectly predicted artworks suggest that avenues com-\nbining the strengths of both models may yield even better\nperformance. In this sense, art authentication may beneﬁt a\nlittle from a hybrid CNN-ViT approach that combines the\nstrengths of both architectures.\nFigure 4 illustrates the differences between both archi-\ntectures in terms of the distributions of their patch pre-\ndictions. The histograms for EfﬁcientNetB5 and Swin Tiny\nare shown in the left and right columns, respectively. The\ntop row displays the incorrect patch predictions, and the\nbottom row the correct ones. The top left histogram shows\na relatively large number of occurrences of wrong predic-\ntions in the interval 0.5-0.7 for EfﬁcientNetB5, i.e., the ﬁrst\npeak right from the middle. These indicate false positives,\nrevealing a bias toward classifying patches as authentic.\nSuch a peak is not evident for Swin Tiny, although there\nare more ‘‘conﬁdent’’ false predictions at 0 and 1 (see the\ntop right histogram). Comparing the bottom two his-\ntograms, showing the correct predictions, it is clear that\nSwin Tiny (right histogram) has a much larger number of\nvery conﬁdent predictions (near 0 and 1), than Efﬁ-\ncientNetB5. These illustrations reveal the subtle ways in\nwhich both types of architectures (CNN and vision\ntransformer) differ in the realization of their predictions.\nTo what extent these differences are algorithm-speciﬁc is\nunclear and subject to further investigations.\n5 Conclusion and future work\nWe performed a comparative evaluation of CNNs and\nvision transformers. We found EfﬁcientNetB5 outperforms\nthe Swin-Tiny and Swin-Base transformers on the standard\ncontrast set, by favoring the classifying of proxies over the\nclassifying of imitations. In our example, this shows that\nEfﬁcientNetB5 is better able to distinguish between van\nGogh and his contemporaries than both Swin transformers.\nThe Swin-Tiny transformer was shown to be marginally\nsuperior to EfﬁcientNetB5 on a reﬁned contrast set (con-\ntaining imitations only) that better reﬂects the essence of\nart authentication. For the Swin-Tiny transformer, the\nchange in contrast set was associated with a jump in imi-\ntation–classiﬁcation accuracy from 0.53 for the standard\ncontrast set to 0.84 on the reﬁned contrast set.\nWhile further tests should be carried out to determine\nthe generalizability of these results to other artists’ data-\nsets, we also highlight how the deep learning approach to\nart authentication has an inherent superiority in terms of\ngeneralizability to all feature engineering approaches\nmentioned in Sect. 1.1, as they require little\nTable 6 Painting-based test accuracies for authentic and reﬁned contrast sets, by using the EfﬁcientNetB5 and Swin-Tiny architectures\nArchitecture Painting accuracy (SD) authentic Painting accuracy (SD) reﬁned contrast\nEfﬁcientNetB5 0.956 (0.033) 0.732 (0.11)\nSwin-Tiny 0.875 (0.060) 0.842 (0.074)\nTable 7 Painting-based accuracies, precision, and recall per type of artwork, with the reﬁned contrast set. Pre-training and number of parameters\nas indicated in Table 4. See text for details\nArchitecture Painting accuracy (SD) Painting precision (SD) Painting recall (SD)\nEfﬁcientNetB5 0.843 (0.051) 0.804 (0.053) 0.756 (0.052)\nSwin-Tiny 0.858 (0.035) 0.818 (0.041) 0.802 (0.045)\nTable 8 Confusion table showing the percentages of correct and incorrect patch predictions for Swin-Tiny and EfﬁcientNetB5 on the enhanced\ncontrast set\nSwin-Tiny correct (%) Swin-Tiny incorrect (%)\nEfﬁcientNetB5 Correct 79.0 5.7\nEfﬁcientNetB5 Incorrect 7.4 8.2\n11856 Neural Computing and Applications (2024) 36:11849–11858\n123\nhyperparameter tuning and do not rely on an isolated fea-\nture (i.e., brushstroke) which may not be visible in all\nartists.\nOur results lead us to conclude that visual backbones\nbased on vision transformers are at least as viable for art\nauthentication as CNNs and that their predictions largely\noverlap. In our future work, we will further explore how\nvision transformers realize their advantage and determine\nto what extent recently proposed improvements to the Swin\ntransformer, i.e., the cross-shaped window transformer\n[38], lead to further improvements on the task of art\nauthentication.\nFuture work should also address the limitations arising\nfrom the digital nature of the training images. We stress the\nimportance of developing methodologies that can achieve\ninvariance to different camera acquisitions, resolutions,\nand scales. Additionally, an interesting line of research\ncould explore incorporating contextual information into the\nmodels, potentially leveraging multi-modality and textual\nguidance.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nData availability statement The datasets generated during and/or\nanalyzed during the current study are not publicly available due to\nlicensing constraints, but are available from the corresponding author\non reasonable request.\nReferences\n1. Spencer RD (2004) The expert versus the object: judging fakes\nand false attributions in the visual arts. Oxford University Press,\nOxford\n2. Sloggett R (2019) Unmasking art forgery: scientiﬁc approaches\nIn: Hufnagel S, Chappell D (eds). The Palgrave handbook on art\ncrime. Springer, London, pp 381–406\nFig. 4 Histograms of the patch predictions for EfﬁcientNetB5 (ENB5, left) and Swin Tiny (SwinT, right). The top rows show the distribution of\nincorrect prediction values, the bottom row those of the correct ones. Note that the top and bottom rows have different vertical scales\nNeural Computing and Applications (2024) 36:11849–11858 11857\n123\n3. Postma EO, Herik HJvd (2000) Discovering the visual signature\nof painters. Future directions for intelligent systems and infor-\nmation sciences. The future of speech and image technologies,\nbrain computers, WWW, and Bioinformatics. Springer, Heidel-\nberg, pp 129–147\n4. Johnson CR, Hendriks E, Berezhnoy IJ, Brevdo E, Hughes SM,\nDaubechies I, Li J, Postma E, Wang JZ (2008) Image processing\nfor artist identiﬁcation. IEEE Signal Process Mag 25(4):37–48\n5. Qi H, Taeb A, Hughes SM (2013) Visual stylometry using\nbackground selection and wavelet-HMT-based Fisher informa-\ntion distances for attribution and dating of impressionist paint-\nings. Signal Process 93(3):541–553\n6. Liu H, Chan RH, Yao Y (2016) Geometric tight frame based\nstylometry for art authentication of van gogh paintings. Appl\nComput Harmon Anal 41(2):590–602\n7. Li J, Yao L, Hendriks E, Wang JZ (2011) Rhythmic brushstrokes\ndistinguish van gogh from his contemporaries: ﬁndings via\nautomated brushstroke extraction. IEEE Trans Pattern Anal Mach\nIntell 34(6):1159–1176\n8. Taylor RP, Micolich AP, Jonas D (1999) Fractal analysis of\nPollock’s drip paintings. Nature 399(6735):422–422\n9. van Noord N, Hendriks E, Postma E (2015) Toward discovery of\nthe artist’s style: learning to recognize artists by their artworks.\nIEEE Signal Process Mag 32(4):46–54\n10. van Noord N, Postma E (2017) Learning scale-variant and scale-\ninvariant features for deep image classiﬁcation. Pattern Recogn\n61:583–592\n11. Dobbs T, Ras Z (2022) On art authentication and the Rijksmu-\nseum challenge: a residual neural network approach. Expert Syst\nAppl 116933\n12. Goodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT\npress, Cambridge, MA\n13. Amelio A, Bonifazi G, Corradini E, Di Saverio S, Marchetti M,\nUrsino D, Virgili L (2022) Deﬁning a deep neural network\nensemble for identifying fabric colors. Appl Soft Comput\n130:109687\n14. Corradini E, Porcino G, Scopelliti A, Ursino D, Virgili L (2022)\nFine-tuning Salgan and Pathgan for extending saliency map and\ngaze path prediction from natural images to websites. Expert Syst\nAppl 191:116282. https://doi.org/10.1016/j.eswa.2021.116282\n15. Cetinic E, Lipic T, Grgic S (2018) Fine-tuning convolutional\nneural networks for ﬁne art classiﬁcation. Expert Syst Appl\n114:107–118\n16. Bell P, Offert F (2021) Reﬂections on connoisseurship and\ncomputer vision. J Art Historiography (24)\n17. Zhu Y, Ji Y, Zhang Y, Xu L, Zhou AL, Chan E (2019) Machine:\nthe new art connoisseur. arXiv preprint arXiv:1911.10091\n18. Lyu S, Rockmore D, Farid H (2004) A digital technique for art\nauthentication. In: Proceedings of the National Academy of the\nU.S.A. 101(49), pp 17006–17010\n19. Hughes JM, Graham DJ, Rockmore DN (2010) Quantiﬁcation of\nartistic style through sparse coding analysis in the drawings of\nPieter Bruegel the Elder. Proc Natl Acad Sci 107(4):1279–1283\n20. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,\nUnterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S,\nUszkoreit J, Houlsby N (2021) An image is worth 16x16 words:\ntransformers for image recognition at scale. In: 9th international\nconference on learning representations, ICLR 2021, Virtual\nEvent, Austria, May 3–7\n21. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B\n(2021) Swin transformer: hierarchical vision transformer using\nshifted windows. In: Proceedings of the IEEE/CVF international\nconference on computer vision (ICCV)\n22. Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang\nZ, Dong L, Wei F, Guo B (2022) Swin transformer v2: scaling up\ncapacity and resolution. In: CVPR 2022\n23. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for\nimage recognition. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (CVPR)\n24. Tan M, Le Q (2019) Efﬁcientnet: Rethinking model scaling for\nconvolutional neural networks. In: International conference on\nmachine learning, pp 6105–6114 . PMLR\n25. Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009)\nImagenet: a large-scale hierarchical image database. In: IEEE\nconference on computer vision and pattern recognition, 2009.\nCVPR 2009. IEEE. https://ieeexplore.ieee.org/abstract/document/\n5206848/\n26. Krizhevsky A, Sutskever I, Hinton GE (2017) Imagenet classi-\nﬁcation with deep convolutional neural networks. Commun ACM\n60(6):84–90\n27. Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2015)\nRethinking the inception architecture for computer vision\n28. Simonyan K, Zisserman A (2015) Very deep convolutional net-\nworks for large-scale image recognition\n29. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Lu, Polosukhin I (2017) Attention is all you need. In:\nGuyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vish-\nwanathan S, Garnett R (eds) Advances in neural information\nprocessing systems, vol 30, pp 6000–6010\n30. Lin T, Wang Y, Liu X, Qiu X (2022) A survey of transformers.\nAI Open 3:111–132\n31. OpenAI: GPT-4 Technical report (2023)\n32. de la Faille JB (1928) L’oeuvre de Vincent van Gogh: Catalogue\nRaisonne´. Van Oest, Paris\n33. David LO, Pedrini H, Dias Z, Rocha A (2021) Authentication of\nVincent van Gogh’s work. In: International conference on com-\nputer analysis of images and patterns, pp 371–380 . Springer\n34. Nelson MR (2011) Underneath the van Gogh F614. Chemmat-\nters, 15\n35. Feilchenfeldt W (1989) Van Gogh fakes: the Wacker affair, with\nan illustrated catalogue of the forgeries. Simiolus: Netherlands Q\nHist Art 19(4):289–316\n36. Gonthier N, Gousseau Y, Ladjal S (2021) An analysis of the\ntransfer learning of convolutional neural networks for artistic\nimages. In: Pattern recognition. ICPR international workshops\nand challenges: virtual event, January 10–15, 2021, Proceedings,\nPart III, pp 546–561. Springer\n37. He K, Zhang X, Ren S, Sun J (2015) Delving deep into rectiﬁers:\nsurpassing human-level performance on ImageNet classiﬁcation.\nhttps://doi.org/10.48550/ARXIV.1502.01852. arXiv:abs/1502.\n01852\n38. Dong X, Bao J, Chen D, Zhang W, Yu N, Yuan L, Chen D, Guo\nB (2022) CSwin transformer: a general vision transformer\nbackbone with cross-shaped windows. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (CVPR), pp 12124–12134\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n11858 Neural Computing and Applications (2024) 36:11849–11858\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7485350370407104
    },
    {
      "name": "Transformer",
      "score": 0.6954311728477478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5863667130470276
    },
    {
      "name": "Segmentation",
      "score": 0.528672456741333
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4814300239086151
    },
    {
      "name": "Painting",
      "score": 0.4617061913013458
    },
    {
      "name": "Authentication (law)",
      "score": 0.435769259929657
    },
    {
      "name": "Computer vision",
      "score": 0.4177993834018707
    },
    {
      "name": "Visual arts",
      "score": 0.1569468080997467
    },
    {
      "name": "Computer security",
      "score": 0.15105167031288147
    },
    {
      "name": "Art",
      "score": 0.14944031834602356
    },
    {
      "name": "Engineering",
      "score": 0.10523533821105957
    },
    {
      "name": "Electrical engineering",
      "score": 0.07385808229446411
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}