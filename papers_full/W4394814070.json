{
  "title": "Combining transformer global and local feature extraction for object detection",
  "url": "https://openalex.org/W4394814070",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5001175281",
      "name": "Tianping Li",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5100776807",
      "name": "Zhenyi Zhang",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5007139691",
      "name": "Mengdi Zhu",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5031347239",
      "name": "Zhaotong Cui",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5101487458",
      "name": "Dongmei Wei",
      "affiliations": [
        "Shandong Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320002812",
    "https://openalex.org/W2806252395",
    "https://openalex.org/W3196048515",
    "https://openalex.org/W2996367417",
    "https://openalex.org/W3191821575",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W4226512186",
    "https://openalex.org/W4251712421",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W3117704549",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W4322747009",
    "https://openalex.org/W3162418282",
    "https://openalex.org/W4319663728",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W4312785900",
    "https://openalex.org/W4214507171",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2975233290",
    "https://openalex.org/W2894878591",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W4307494192",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W3183430956",
    "https://openalex.org/W4312446817",
    "https://openalex.org/W2886904239",
    "https://openalex.org/W3035694605",
    "https://openalex.org/W3035473155",
    "https://openalex.org/W4361293393",
    "https://openalex.org/W2988302524",
    "https://openalex.org/W2996603555",
    "https://openalex.org/W2988452521",
    "https://openalex.org/W4385999145",
    "https://openalex.org/W1921523184",
    "https://openalex.org/W3177052299",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W3024979424",
    "https://openalex.org/W3113453806",
    "https://openalex.org/W4389937693",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4383113375",
    "https://openalex.org/W3199093552",
    "https://openalex.org/W3106250896"
  ],
  "abstract": "Abstract Convolutional neural network (CNN)-based object detectors perform excellently but lack global feature extraction and cannot establish global dependencies between object pixels. Although the Transformer is able to compensate for this, it does not incorporate the advantages of convolution, which results in insufficient information being obtained about the details of local features, as well as slow speed and large computational parameters. In addition, Feature Pyramid Network (FPN) lacks information interaction across layers, which can reduce the acquisition of feature context information. To solve the above problems, this paper proposes a CNN-based anchor-free object detector that combines transformer global and local feature extraction (GLFT) to enhance the extraction of semantic information from images. First, the segmented channel extraction feature attention (SCEFA) module was designed to improve the extraction of local multiscale channel features from the model and enhance the discrimination of pixels in the object region. Second, the aggregated feature hybrid transformer (AFHTrans) module combined with convolution is designed to enhance the extraction of global and local feature information from the model and to establish the dependency of the pixels of distant objects. This approach compensates for the shortcomings of the FPN by means of multilayer information aggregation transmission. Compared with a transformer, these methods have obvious advantages. Finally, the feature extraction head (FE-Head) was designed to extract full-text information based on the features of different tasks. An accuracy of 47.0% and 82.76% was achieved on the COCO2017 and PASCAL VOC2007 + 2012 datasets, respectively, and the experimental results validate the effectiveness of our method.",
  "full_text": "Complex & Intelligent Systems (2024) 10:4897–4920\nhttps://doi.org/10.1007/s40747-024-01409-z\nORIGINAL ARTICLE\nCombining transformer global and local feature extraction for object\ndetection\nTianping Li 1 · Zhenyi Zhang 1 · Mengdi Zhu 1 · Zhaotong Cui 1 · Dongmei Wei 1\nReceived: 13 December 2023 / Accepted: 9 March 2024 / Published online: 15 April 2024\n© The Author(s) 2024\nAbstract\nConvolutional neural network (CNN)-based object detectors perform excellently but lack global feature extraction and cannot\nestablish global dependencies between object pixels. Although the Transformer is able to compensate for this, it does not\nincorporate the advantages of convolution, which results in insufﬁcient information being obtained about the details of local\nfeatures, as well as slow speed and large computational parameters. In addition, Feature Pyramid Network (FPN) lacks\ninformation interaction across layers, which can reduce the acquisition of feature context information. To solve the above\nproblems, this paper proposes a CNN-based anchor-free object detector that combines transformer global and local feature\nextraction (GLFT) to enhance the extraction of semantic information from images. First, the segmented channel extraction\nfeature attention (SCEFA) module was designed to improve the extraction of local multiscale channel features from the model\nand enhance the discrimination of pixels in the object region. Second, the aggregated feature hybrid transformer (AFHTrans)\nmodule combined with convolution is designed to enhance the extraction of global and local feature information from the\nmodel and to establish the dependency of the pixels of distant objects. This approach compensates for the shortcomings of\nthe FPN by means of multilayer information aggregation transmission. Compared with a transformer, these methods have\nobvious advantages. Finally, the feature extraction head (FE-Head) was designed to extract full-text information based on the\nfeatures of different tasks. An accuracy of 47.0% and 82.76% was achieved on the COCO2017 and PASCAL VOC2007 +\n2012 datasets, respectively, and the experimental results validate the effectiveness of our method.\nKeywords Object detection · Attention mechanism · Transformer · Anchor-free · Detector head\nIntroduction\nObject detection [ 1] is a fundamental vision task in computer\nvision that focuses on categorizing objects for recognition\nand location localization. It has been widely used in the\nB Dongmei Wei\nweidongmei@sdnu.edu.cn\nTianping Li\nsdsdltp@sdnu.edu.cn\nZhenyi Zhang\nzzy0693@163.com\nMengdi Zhu\n1342785693@qq.com\nZhaotong Cui\nhaotong120@163.com\n1 School of Physics and Electronics, Shandong Normal\nUniversity, Jinan, Shandong, China\nﬁelds of automatic driving, security monitoring, and medi-\ncal imaging [ 2]. Recent research on high-performance object\ndetectors [ 3] has not stopped and still faces great chal-\nlenges. For example, the problem of optimizing the number of\nparameters, the speed, and the convergence rate while main-\ntaining high detection accuracy can be addressed. For this\nreason, much work has been done by researchers studying\nhigh-performance object detection algorithms.\nIn recent years, CNN-based [ 4] and Transformer-based\n[5] object detectors have become the current dominant archi-\ntectures, and models from both architectures have achieved\nstunning detection results. CNN-based object detectors have\nevolved from original two-stage detectors (e.g., R-CNN [ 6]\nand Cascade R-CNN [ 7]) to single-stage detectors (e.g.,\nYOLO [8] and RetinaNet [ 9]) and from anchor-based detec-\ntors (e.g., Faster R-CNN [ 10] and YOLOV4 [ 11]) to anchor-\nfree detectors (e.g., FCOS [ 12] and A TSS [13]). Among them,\nthe emergence of anchor-free detectors has led to a further\nincrease in the speed of detection. The above CNN-based\n123\n4898 Complex & Intelligent Systems (2024) 10:4897–4920\nobject detection algorithms not only have made important\nbreakthroughs in terms of efﬁciency and accuracy but also\nhave obvious advantages in extracting local and multiscale\nfeatures. However, the drawback of insufﬁcient ability to\nextract information globally is still prevalent, and the detec-\ntion performance still somewhat deviates from that of current\nobject detectors.\nSince its introduction into the ﬁeld, the transformer archi-\ntecture has become a research hotspot in the ﬁeld of computer\nvision [ 14]. Therefore, researchers started to introduce the\nTransformer to object detection and proposed a series of\nTransformer-based object detection algorithms. For the ﬁrst\ntime, DETR [ 15] used a transformer for image object detec-\ntion via a tandem splicing approach involving a CNN\nfollowed by a transformer, and the 2D features extracted\nby the CNN were subsequently input into the transformer,\nwhich enhanced the extraction of global features. The dis-\nadvantages of these methods include poor detection of small\nobjects, large computational parameters and slow conver-\ngence. Deformable DETR [ 16] fuses the sparse sampling\ncapability of variability convolution with the powerful rela-\ntional modelling capability of the transformer using a small\nset of sampling locations as a feature image element that\nhighlights where the key elements are located, resulting in\nenhanced training effects and small object detection. The dis-\nadvantage is that the complexity and number of parameters\nare large, a large amount of data is needed for training, and\nthe ability to detect small objects needs to be strengthened.\nTransformers [ 17] have received a great deal of academic\nattention due to their simple encoder-decoder architecture\nand remarkable detection results. The architecture simpliﬁes\nthe detection process and has a uniﬁed paradigm, and the abil-\nity to capture remote dependencies in captured objects and\nperform global feature extraction with robust image semantic\ninformation makes it easy to achieve highly accurate end-to-\nend object detection. However, there are also shortcomings\nin its weak ability to extract multiscale and local features.\nAlthough researchers have recently achieved good results\nin transformer optimization [ 18], this approach still suffers\nfrom the disadvantages of a large number of computational\nparameters, slow convergence, and a lack of ability to acquire\nlocal information (edges and texture) compared to CNN-\nbased detection models, and it is difﬁcult to strike a balance\nbetween the detection accuracy and the number of parame-\nters. This shows that although the computational process is\ngreatly simpliﬁed, the computational cost of the Transformer\nitself is still large, and the number of parameters is difﬁcult to\nreduce. For abbreviations and full names used in this paper,\nplease consult the glossary of terms shown in Table 1.\nThe Microsoft team’s Mobile-Former [ 19] parallelizes\nMobileNet [20] and Transformer using a bidirectional cross-\nbridging approach, which achieves bidirectional fusion of\nTable 1 Abbreviations and full names\nAbbreviations Indexes\nCNN Convolutional Neural Network\nFPN Feature Pyramid Network\nSCEFA Segmented Channel Extracted Feature\nAttention\nAFHTrans Aggregated Feature Hybrid Transformer\nFE-Head Feature Extraction Head\nGLFT Combining Transformer Global and Local\nFeature Extraction\nMHA Multi-Head Attention\nMSDA Multi-Scale Dilated Attention\nFFN Feedforward Neural Network\nFE-Head Feature Extraction Head\nMFCEM Multiscale Full Text Channel Extraction\nModule\nGT Global Text\nAGAP Adaptive Global Average Pooling\nAGMP Adaptive Max Average Pooling\nPE Position Encoding\nFE Feature Extractor\nlocal and global features and reduces the number of ran-\ndom tokens generated. The disadvantage is that the pres-\nence of multiple convolutions makes the patch unstable,\noften leading to high computational effort and performance\ndegradation. UniFormer [ 21] uses convolution and trans-\nformers to extract global and local features, effectively\nsolving redundancy and dependency problems in the model\nlearning process. TransXNet [ 22] builds a novel hybrid\nCNN-Transformer architecture to aggregate global informa-\ntion and local details with excellent detection performance.\nInﬂuenced by Mobile-Former, UniFormer and TransXNet, a\ncombination of a CNN and a transformer is considered for\nthe above problem to enhance the feature extraction capabil-\nity of local and global information. Moreover, the fast speed\nof the anchor-free end-to-end architecture and the absence\nof nonextremely large value suppression are utilized for the\ncomprehensive design of the detector. The current main-\nstream CNN-based anchor-free object detector is composed\nof three main parts: the backbone, neck, and head. The back-\nbone network here is ResNet, and no changes are usually\nmade to that structure. To better realize the above, this paper\nrethinks the structure of the Transformer. From this, it is\nfound that the encoder-decoder structure in the Transformer,\nalthough favorable for extracting multiscale features and\nimproving detection performance, leads to the problems of\nan excessive number of computational parameters and slow\nconvergence. Moreover, TSP-FCOS [ 23] achieves acceler-\nated training and an improved ability to extract multiscale\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4899\nfeatures by combining FCOSs and removing the structure\nof the transformer decoder; however, the detection perfor-\nmance needs to be further improved. To reduce the number\nof unnecessary calculations and parameter values as well as\nimprove the detection performance, the key components are\nanalysed in detail, and experiments are carried out based on\nthem. Therefore, the multi-head cross-attention part of the\ndecoder was removed. Additionally, the ability of the Trans-\nformer to extract localized and multiscale features that result\nfrom removing this part should be mitigated. Inspired by\nthe ability of EPSANet [ 24] to segment the spatial infor-\nmation of extracted feature maps at different scales and\nestablish long-term dependencies between local multiscale\nchannel attention mechanisms, the multiscale full-text chan-\nnel extraction module (MFCEM) is proposed to enhance the\nability to extract multiscale features. The highest layer in the\noutput part of the backbone network is rich in local semantic\ninformation, but the attention mechanism has difﬁculty bal-\nancing the number of channels and parameters at this layer,\nresulting in many detectors being unable to extract sufﬁ-\ncient feature information from this layer. Inspired by the fact\nthat the feature grouping and channel substitution operations\nof SANet [ 25] can optimize the interaction of parametric\nquantities and channel information, the segmented channel\nextracted feature attention (SCEFA) method combined with\nMFCEM is proposed to solve the above problem.\nTo further improve the encoder structure of the Trans-\nformer, inspired by the ability of the Dilateformer [ 26]t o\nefﬁciently aggregate multiscale semantic information from\ndifferent receptive ﬁelds and to reduce structural redundancy,\nmulti-scale dilation attention (MSDA) is introduced. More-\nover, the convolutional and depth-separated convolutional\nstructures of CNNs are added to enhance local information\nacquisition. Then, the core parts of the position encoding\n(PE), multi-head attention (MHA), and feed-forward neural\nnetwork (FFN) structures in the transformer are extracted,\nand the front and back parts are combined to improve the\ntransformer. FPNs [ 27] are mostly used in necking structures\nto address the challenges of multiscale detection but have\nlong been characterized by a signiﬁcant drawback: the inabil-\nity to adequately communicate and fuse information across\nlayers (e.g., Layer 1 and Layer 3) limits the scope of informa-\ntion fusion. For the cross-level information fusion approach,\ninspired by TopFormer’s [ 28] powerful hierarchical feature\nstructure for acquiring intensive prediction tasks, extensions\nhave been developed based on its theory. By combining the\nabove self-attention improved architecture for transformers,\nthe aggregate feature hybrid transformer (AFHTrans) archi-\ntecture incorporating convolution is designed to be added\nto the front end of the retained FPN. Effective exchange\nof information across hierarchical levels is realized by fus-\ning global multilevel information injected into rich semantic\nFig. 1 Comparison of the epochs and APs of our network with those of\nother networks on the COCO2017 dataset\ninformation at a high level. The above improvements fur-\nther enhance the global and local feature extraction of neck\ninformation, improve the multiscale detection performance\nof the model, and avoid causing an increase in the number\nof multiple parameters. Finally, to improve the information\ninjection capability, an efﬁcient injection module for detec-\ntion information is designed to realize the effective fusion of\nthe original information into high-level information.\nTOOD [ 29] proposes a task-aligned detection head that\nimproves detection performance by aligning classiﬁcation\nand localization tasks in a learning-based manner. However,\nthis detection head lacks the ability to perform feature extrac-\ntion on the fused full-text information. For this purpose, an\nanchor-free feature extraction head (FE-Head) is designed in\nwhich the fused global and local feature map information\nis further extracted and processed, optimizing the feature\nextraction capability of full-text information for different\ntasks at the decoupled head.\nBy combining the above improvements, in this paper, we\ndesign a CNN-based end-to-end anchor-free object detec-\ntor that combines the Global and Local Feature Extraction\n(GLFT) of Transformers. The model not only enhances\nthe global and local feature information extraction of the\nwhole model feature map but also realizes the cross-channel\ninformation fusion of feature maps at different levels. GLFT-\nNet outperforms other state-of-the-art detection networks\nin terms of the number and accuracy of computational\nparameters, achieving signiﬁcant advantages. Comparison\nwith other classical networks using the backbone network\nResNet101. As shown in Fig. 1, our network converges\nquickly, and its detection accuracy is better than that of other\nclassical networks.\nIn summary, the main contributions of this paper are as\nfollows:\n123\n4900 Complex & Intelligent Systems (2024) 10:4897–4920\n1. From a microscopic point of view, this approach can\nimprove the feature extraction of local semantic infor-\nmation and achieve a balance between the number of\nchannels and the number of parameters. To this end, this\npaper proposes the segmented channel extracted feature\nattention (SCEFA) module combined with the multi-\nscale full-text channel extraction module (MFCEM),\nwhich can be used to efﬁciently extract high-level local-\nized channel feature map information and establish a\nremote dependency on the number of long-distance chan-\nnels, effectively reducing the number of parameters. The\nSCEFA is ﬂexible and easy to use and can be used in\nmany computer vision network models.\n2. The ability of the Transformer to extract localized infor-\nmation is limited by the large number of parameters and\nslow convergence. Moreover, considering the FPN from\na macro perspective, it lacks the ability to interact and\nmerge information across hierarchical levels. To this end,\nthis paper proposes an aggregate feature hybrid trans-\nformer (AFHTrans) module combined with convolution\nand an efﬁcient injection module for detecting infor-\nmation, which not only improves the global and local\nfeature extraction capability of the model and the cross-\nfertilization of feature information across hierarchical\nlevels but also signiﬁcantly reduces the number of com-\nputational parameters.\n3. The feature extraction head (FE-Head) was designed to\naddress the lack of extraction of the full-text informa-\ntion of the feature map by the detection head. It not only\nhas the powerful ability to acquire full-text feature infor-\nmation but also balances the number of parameters and\naccuracy well. It can be applied to a variety of single-\nstage object detectors in a plug-and-play manner with\nsigniﬁcant results.\n4. By combining the SCEFA, AFHTrans, and FE-Head\nmethods, a novel GLFT object detection model incorpo-\nrating a transformer is designed to improve global and\nlocal feature extraction. Because of the smaller num-\nber of parameters and faster training speed compared to\nthose of the transformer detector, the proposed architec-\nture allows this paper to achieve superior performance in\nbenchmark model comparison tests, outperforming most\nstate-of-the-art object detectors.\nThe remainder of this paper is organized as follows. First,\nthe “ Related work ” of this paper is described in detail.\nThen, the “ Methods” proposed in this paper are described.\nThe experimental work is also described in the “ Experi-\nments” section of the paper. Finally, the “ Conclusion” section\nprovides a comprehensive summary of the work presented\nthroughout the text.\nTable 2 Work related to the “decoder–encoder” style of feature extrac-\ntion of contextual information and content characterization\nRelated work Content features\nFaster R-CNN [ 10] This work uses a convolutional layer\nnetwork to extract features and a Region\nProposal Network to efﬁciently extract\ncandidate region context information,\nwhich, combined with the shared feature\nproperty of convolution, results in a\nsigniﬁcant improvement in detection\nperformance. However, whether using\nVGGNet or ResNet, the extracted feature\nmaps are single-layer and the resolution\nis relatively small, which leads to poor\ndetection of multiscale and small objects,\ngenerating leakage and false detection\nMask R-CNN [ 30] This work extracts mask information from\nthe ﬁner spatial layout of an object by\nmeans of pixel-to-pixel correspondences\nin the feature map produced by deep\nconvolution\nCA-CNN [ 31] This work, to enrich the feature\nrepresentation and mine the underlying\ncontextual information between objects,\nintegrates the feature information into a\nconvolutional layer and maps it into a\nmultilevel feature map to extract the\ncontextual information, and ﬁnally fuses\nthe multilevel feature information into a\nsingle layer. The disadvantage is that\ncandidate regions and fusing feature\ninformation from different layers can\nlead to model redundancy and slow speed\nPFPNet [ 32] This work uses a multiscale semantic\nfusion module to feature-fuse high-level\nto shallow-level information so that there\nis both shallow-level object position\ninformation and high-level object\nsemantic information\nRelated work\nIn this section, previous related work is reviewed, such as\nfeature extraction of contextual information, transformers\ncombined with convolution, and decoupled heads.\nFeature extraction of contextual information\nFeature extraction of contextual information has been widely\nstudied by researchers as a fundamental step in many image\nvisualizations, especially in the ﬁeld of deep learning. In gen-\neral, studies can be divided into two categories. The ﬁrst is\nthe “decoder–encoder” style. In Table 2, different tasks and\nspeciﬁc content features of the “decoder–encoder” style of\nfeature extraction of contextual information are described in\ndetail.\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4901\nTable 3 Features extract contextual information related to the “back-\nbone” style of work as well as content features\nRelated work Content features\nSSD [ 33] This work achieves multiscale object detection\nby using a convolutional layer to extract\ndifferent feature maps after the backbone\nnetwork, thus making full use of the sensory\nﬁeld information of different-sized feature\nmaps and activating the semantic and\ncontextual information of the object.\nHowever, there is also the disadvantage of a\nlow detection rate for small objects\nRFBNet [ 34] This work proposes an Inception-like receptive\nﬁeld module, which mainly utilizes\nmultibranch convolutional layers to\nincorporate receptive ﬁeld structures into the\nbackbone network, effectively expanding the\nchannel receptive ﬁeld and enhancing the\nfeature extraction capability\nCBNet [ 35] This work presents novel backbone networks\nthat do not require retraining of weights. By\ncompositely connecting and integrating\nmultiple identical backbone networks with\nhigh-level and low-level features, the scope of\nfeature information fusion and sensory ﬁeld is\nexpanded, thus aiding the performance of\nhigh-performance detectors\nThe second category is the “backbone” style. As shown in\nTable 3, different works and speciﬁc content features of the\n“backbone” style of feature extraction of contextual infor-\nmation are detailed.\nFor the contextual information extracted by the features,\ntwo scenarios exist. The ﬁrst scenario. When the percep-\ntual ﬁeld is too small and the object is too large, leakage\nmay occur because the model does not sense the presence of\nthe object. The second scenario. When the sensing range is\ntoo large and the object size is small, the network extracts\nbackground and redundant information, which leads to false\ndetections because the model has difﬁculty observing tiny\nobjects. Table 4 lists the different works and speciﬁc con-\ntent features of the above two cases of feature extraction of\ncontextual information.\nOverall, there are two drawbacks to these two types of\nstyles and scenarios. First, the afﬁnity matrix for detecting\npixel information is compared with other pixel information\nand lacks self-pixel information. Second, several methods\nare designed with redundant design operations, which results\nin computationally intensive and ineffective establishment\nof long-range channel number dependencies for localized\ninformation. To avoid the above drawbacks and to fully\nextract the global and local feature information of the object,\nthree methods are proposed in this paper: the SCEFA for\nlocal information extraction on the high-level network, the\nTable 4 Related work on two cases of feature extraction of contextual\ninformation and content characterization\nRelated work Content features\nThe ﬁrst scenario\nCornerNet [ 36] This work proposes the concept of keypoints,\nwhich enables the model to better understand\nthe feature information of the object. Its\ndisadvantage is that when performing object\ndetection, it only focuses on the features of\nthe two corners of the object, the upper-left\nand lower-right corners, and ignores the\ncenter features of the object, which may lead\nto the generation of some false positive\nsamples. Especially when dealing with\nobjects that cover each other, many objects\ndo not have obvious corner features, which\nmakes them perform poorly in the detection\nof covered objects\nThe second scenario\nPANNet [37] This work utilizes bottom-up path\naugmentation and adaptive feature pooling to\naggregate features between different layers,\nthus ensuring feature completeness and\ndiversity. However, this also makes the\nnetwork structure complex and requires more\ncomputational resources for training and\ninference. Moreover, some difﬁculties may\nbe encountered when dealing with the small\nobject task, as the localization information of\nlow-resolution feature maps may be lost\nAFHTrans for global and local information extraction and\nestablishing the pixel relationship of the long-distance object,\nand the FE-Head for global feature information extraction on\nthe head.\nTransformer combined with convolution\nThe local perceptual ﬁeld and shared weights provided by the\nCNN can be used to effectively capture the local feature infor-\nmation in the image and can effectively reduce the number\nof computational parameters. The high-dimensional feature\nrepresentation of a CNN converts the information in an image\ninto high-dimensional semantic information for better fea-\nture extraction. Moreover, CNNs are translation invariant,\nenabling enhanced generalization of the model. However,\nCNNs lack the ability to extract global features and cannot\neffectively capture global features or establish long-range\npixel dependencies for objects. In addition, the Transformer\nhas powerful context-aware scaling and the ability to extract\nglobal feature information but still suffers from two draw-\nbacks: ﬁrst, it is challenging in terms of training and data, and\nsecond, it occupies a large number of computational param-\neters. To solve the above problems, many researchers have\nstarted to combine the advantages of CNNs and transformers\n123\n4902 Complex & Intelligent Systems (2024) 10:4897–4920\nTable 5 Combined related work on convolution and converters as well\nas content characterization\nRelated work Content features\nConformer [ 38] This work enhances the global and local\nfeature learning capability by combining\nCNN with Vision Transformer in a hybrid\nnetwork structure, which focuses on the\nfusion of global and local features through\nfeature coupling units\nCMT [ 39] This work uses a parallel design of MobileNet\nand Transformer, where deep convolution is\nused to compress the channels to extract\nlocal information and the converter\nestablishes dependencies between patches\nfor global features\nCoTNet [ 40] This work creatively integrates the static\ncontext information aggregation of\nconvolution with the dynamic context\ninformation aggregation of the self-attention\nmechanism in Transformer, which is able to\nimprove the performance of the network\nwithout signiﬁcantly increasing the\ncomputational effort\nViT-Det [41] This work parallelizes CNN and Transformer\nin the shallow part of the backbone,\ncombining the advantages of CNN and its\nvariants in extracting high-frequency\ninformation and Transformer in extracting\nlow-frequency information, which can\nenhance the ability to fuse the information\nand improve the accuracy of detection.\nHowever, there are the shortcomings of\nunnecessary structure and the inability to\ncombine FPN, which limit the scope of the\napplication\nSMT [ 42] This work performs image global and local\nfeature extraction by combining\nconvolutional networks with visual\ntransformers to efﬁciently capture\nlong-range dependencies\nCloFormer [ 43] This work rethinks the implementation of\nlocal awareness at Transformer, which\nenhances access to high-frequency local\ninformation through the relationship\nbetween the global shared weights of the\nprimitive convolutional operator and the\ncontext of a particular token in self-attention\nand have performed much work. As shown in Table 5,t h e\ndifferent works and speciﬁc content features performed by\nthe combination of the CNN and transformer are detailed.\nDrawing on the merits of the above work, the AFHTrans\nmodule proposed in this paper incorporates convolution and\ndeep convolution, MHA, the MSDA, and the FFN as core\ncomponents. Convolution and deep convolution are used in\nfront of the channel to extract local features; in the middle,\nMHA, MSDA, and FFN are used to expand the receptive\nﬁeld while extracting global information; and in the back,\ndetection information is efﬁciently injected into the mid-\ndle and high levels of the FPN to fuse the global and local\nfeature information. This module effectively combines the\nadvantages of CNNs and Transformers to enhance the global\nand local feature extraction capabilities, establishes the long-\nrange interdependence of object pixels, optimizes the number\nof computational parameters, and achieves comprehensive\nand accurate detection of object information. This module\neffectively combines the advantages of CNNs and Trans-\nformers to enhance the global and local feature extraction\ncapabilities, establishes the long-range interdependence of\nobject pixels, optimizes the number of computational param-\neters, and achieves comprehensive and accurate detection of\nobject information.\nDecoupled head\nDecoupled heads have recently become the standard structure\nfor mainstream object detection. There have been tremen-\ndous breakthroughs in the research on decoupled heads. As\nshown in Table 6, the different recent works performed on\ndecoupled heads and the content characteristics are detailed.\nThe above work illustrates the importance of decoupling\nand task characterization information processing in the head.\nTo this end, the FE-Head method using a decoupled structure\nis proposed to be able to comprehensively improve the acqui-\nsition of full-text information from the head feature map,\nenabling the decoupled head to fully utilize the extracted\nfeature information for classiﬁcation and localization tasks.\nMethods\nIn this study, we propose a GLFTNet-based detection model\nthat combines the SCEFA module, the AFHTrans module,\nand the FE-Head. First, the paper describes GLFTNet in\n“GLFTNet”; then, the SCEFA module, the AFHTrans mod-\nule, and the FE-Head are presented in “ Segmented channel\nextraction feature attention module ”, “ Feature grouping ”,\nand “ Segmentation channels for extracting multiscale fea-\ntures”, respectively.\nGLFTNet\nAs shown in Fig. 2, this paper brieﬂy discusses global and\nlocal feature extraction for object image detection (GLFT)\nin conjunction with the transformer.\nFirst, information about the features of the image\nis extracted using the backbone network ResNet50 or\nResNet101 to generate different layers of multiscale feature\nimage pyramids. Next, the high-level output portion of the\npyramid uses the SCEFA module to identify the importance\ninformation of the feature map. First, the SCEFA module is\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4903\nTable 6 Related work on decoupling headers and content characteriza-\ntion\nRelated work Content features\nIoU-Net [ 44] This work ﬁrst introduces an additional\nheader to predict the IOU as the\nlocalization conﬁdence and then\naggregates the localization and\nclassiﬁcation conﬁdence into a ﬁnal\nclassiﬁcation score. This approach\nreduces the problem of detecting\nmislocalization and improves the\nconﬁdence scores for good-quality\nbounding boxes, but makes the conﬁdence\nscores for rough bounding boxes lower\nFCOS [ 12] This work then extends the centroid\nbranching task of the original header to\nlead to regression branching for effective\nbounding box localization under the\ncentroid, regression, and classiﬁcation\ntasks\nDouble-Head [ 45] This work proposes to branch the R-CNN\nhead to handle the classiﬁcation and\nlocalization tasks separately. Although\ngood detection results can be achieved,\nthe shared parameters of the two tasks are\nreduced, and there are task conﬂicts\nTSD [ 46] This work found that the spatial deviation\nbetween the classiﬁcation and\nlocalization tasks affects the training\nprocess, so it proposed a task-aware\nspatial separation method to divide the\ntwo tasks into separate decoupled heads,\nwhich achieved good training results\nYOLOX [47] This work introduces decoupled detection\nheads to the YOLO family for the ﬁrst\ntime, enhancing the performance and\nconvergence speed of detection\nRA-YOLOX [48] This work proposes a lightweight\ndecoupling head, while the detection head\nemploys a structural reparameterization\nmethod to achieve signiﬁcant\nperformance improvement\nused to extend the channel dimension split grouping after seg-\nmentation and extraction, and the channel information and\nspatial information of the resulting multiscale channel fea-\nture maps are integrated into the group feature maps to obtain\nthe global and local channel information interaction attention\nin the local features to adaptively differentiate the chan-\nnels. The aggregated output feature maps are then subjected\nto channel feature conditioning operations. Finally, channel\nreplacement operations (channel shufﬂing) are used for inter-\ngroup communication to output information that enriches\nlocal features. Second, the output feature maps with the same\nnumber of channels after three layers of channel compres-\nsion are aggregated using the AFHTrans module, after which\nthe spatial information about the feature maps is extracted\nand the global and local pixel features are augmented by\nthe similarity of the object pixel points to establish object\npixel dependencies over long distances of the model and cap-\nture the semantic dependency information of the channels\nat different scales. Finally, all the processed information is\npooled into the second and third layers using a detection-\nefﬁcient information injection module, and the full-layer\ninformation is passed horizontally into the top-down FPN\nfor feature fusion. Finally, the fused information is passed\ninto the FE-Head for full-text feature information extraction\nin the classiﬁcation and regression branches.\nSegmented channel extraction feature attention\nmodule\nAccording to previous research [ 33, 49–52], the detection\nability of images of objects of different sizes can be effec-\ntively improved by extracting multiscale features. Features at\ndifferent scales are extracted using ResNet50 or ResNet101\nto generate multiscale feature image pyramids built at differ-\nent resolutions. Moreover, low-level high-resolution images\ncontain more spatial information than high-resolution images\nFE-Head\nFE-Head\nFE-Head\nFE-Head\nFE-Head6&()$\nBackbone\nAFHTrans\nInject\nInject\n6&()$ AFHTransSegmented channel extrac/g415on \nfeature a/g425en/g415on\nAggregate feature hybrid \ntr ansformer module\nc3\nc4\nc5\nP3\nP4\nP5\nP6\nP7\nFeatur e extrac/g415on headFE-Head\nFig. 2 Overview of the overall GLFT framework diagram\n123\n4904 Complex & Intelligent Systems (2024) 10:4897–4920\nand can be used for the detection of small objects. A struc-\nture that preserves the FPN is used at the back end of\nthe model to fuse the efﬁciently acquired global and local\nmultiscale information in a top-down manner, enriching\nand effectively utilizing the low-level spatial information.\nHigh-level low-resolution feature maps contain additional\nsemantic information and can be used for the detection of\nlarge objects.\nFor this reason, the SCEFA module is designed to extract\nhigh-level local features. As shown in Fig. 3, the SCEFA\nmodule contains four parts.\nFeature grouping\nTo reduce the number of computational parameters, fea-\ntures are extracted using segmentation channel dimension\nsplit grouping. The information from the high-level local\nfeature maps is ﬁrst split into multiple groups along the\nchannel dimension. Suppose the input is characterized by\nX ∈ R\nC×H×W (the C channel number is 2048, H is the\nheight, and W is the width), and the input X is split into\nZ groups along the channels:X /equal1[X1, X2, ...... , X Z ] ∈\nRC/Z×H×W .\nSegmentation channels for extracting multiscale\nfeatures\nEach group of features is subsequently segmented into two\npaths, and the two paths are subjected to ﬁne feature context\ninformation extraction via the proposed multiscale full-text\nchannel extraction module (MFCEM), which is the core of\nthe SCEFA module. The MFCEM of the SCEFA block, as\nshown in Fig. 4, uses group convolution of different kernels\nto extract high-level localized multiscale channel feature map\ninformation, which is capable of obtaining feature maps with\ndifferent receptive ﬁelds and different resolution levels. Each\nbranch of the group convolutional branch can learn multi-\nscale spatial information over a wide range of horizons. A\nglobal text (GT) module is added after each branch of the\ngroup convolutional branch of the weighted path, modelling\nthe global context while saving the number of parameters.\nThe GT module here (shown in the gray dashed box) con-\nsists of the attention head (shown in the blue dashed box)\nand the transform neck (shown in the yellow dashed box).\nThe attention head is a structure in which the output fea-\nture maps after channel normalization and Softmax [ 53]\nare cross-multiplied with the original input feature maps to\nmake effective judgments about the importance of the chan-\nnel weighting information. The transform neck is a structure\nthat combines an extraction module, layer normalization,\nand Relu [ 54] and is able to further extract channel feature\nmaps and effectively reduce the number of parameters in the\nMFCEM. The main purpose of the GT module is to effec-\ntively encode the relevant information in the feature map\ninto the full-text attention map, thus enabling MFCEM to\ncorrectly discriminate between features of different scales or\ndimensions needed, enhancing the ability of the data to adapt\nto multiscale features, and providing more global channel\ninformation. According to CANet, only channel information\ncoding is considered while ignoring location information,\nand only local information is considered to be captured,\nwhile global information over long distances is missing.\nTherefore, CANet [ 55] remedies the above deﬁciencies by\nembedding spatial attention into channel attention using two\none-dimensional global pooling structures. In our MFCEM\nmethod, global textual information is extracted by segment-\ning channels from the input feature map via multiple group\nconvolutions.\nIn detail, feature X\ny will be split into two paths along the\nchannel: X y1, X y2 ∈ RC/2Z×H×W . In MFCEM, ﬁrst, after\nthe convolution of two multibranch groups, the channels are\ndivided into C/4 segments, reducing the number of compu-\ntational parameters. The input feature map is compressed\nat different resolutions to obtain rich feature information,\nand cross-channel information interaction is realized while\nextracting multiscale spatial information. Then, the GT mod-\nule follows the structure of GCNet [ 56], the feature maps\ngenerated by the convolution of each branch group are con-\nverted into weights to discriminate the important features in\nthe attention head of the GT module, and the important fea-\nture map information is extracted after the transform neck,\nwhich further reduces the computational parameter cost. The\nMFCEM generates global and local channel information, and\nthe detailed process equations are as follows:\n/Phi1\nj /equal1Con vi (ku , gv),#\nβi /equal1σ(SN (/Phi11(X yni ))) × /Phi11(X yni ), i /equal11, 2, 3, 4,\nρi /equal1ET 2(δ(LN (ET 1(βi )))),\n(1)\nHere, X yni ∈ RC/8Z×H×W ,n /equal11, 2 and and /Phi1j ,\nj /equal11, 2 represent the process of group convolution of the\nweighted paths and group convolution of the initial paths\nin the ith stage, respectively, where the convolution kernel\nsize is k\nu and the convolution group size is gv. SN denotes\nchannel normalization, and βi represents the channel infor-\nmation attention map output from the attention head of the GT\nmodule. ET stands for the extracted module, and LN stands\nfor layer normalization. ρ\ni represents the channel informa-\ntion attention feature map of the GT module-transformed\nneck output. σ denotes the Softmax function, and δ denotes\nthe Relu activation function.\nFinally, the output channel feature map information of\neach branch is aggregated, and the obtained global multi-\nscale channel feature map is subsequently recalibrated with\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4905\nMFCEM\nMFCEM\nCChunk\nAGAP\nAGMP\nFc1 Relu Fc2\n*\nsigmoid\nGroup\nch annel \nshu ﬄe\nFeature \nFig. 3 Detailed structure of the SCEFA block\nC\nSo/g332Max\n×\nSo/g332Max\n×\nSo/g332Max\n×\nSo/g332Max\n×\nC\nSo/g332Max\n*\nWeighted ro ad  \ngroup co nvo lu /g415on\nChan nel \nnormaliza/g415on Excita/g415onLayerNorm\n&R eLU\nA/g425en/g415on head\ntransform neck\nGroup convolu/g415o n \nof the ini/g415a l path\nWeightGT \nFig. 4 Detailed structure of the MFCEM\nweights and corresponding probability values via softmax,\nwhich realizes the interaction between local and global\nchannel information. The ﬁnal output feature map is then gen-\nerated by multiplying the product of the recalibrated weight\ninformation with the feature maps fused by the group con-\nvolutional channels of the initial path. Overall, this approach\nenables the two paths to simultaneously extract multiscale\nspatial information for different channels, establish long-\ndistance channel dependencies, and effectively minimize the\nimpact of channels on the number of parameters. Finally, the\noverall process equation for the MFCEM is as follows:\nα\ni /equal1/Phi12\n(\nX yni\n)\n, i /equal11, 2, 3, 4,\nYn /equal1σ(Cat (ρ1, ... , ρ4)) ∗ Cat (α1, ... , α4), (2)\nHere, Yn ∈ RC/2Z×H×W , n /equal11, 2 represents the ﬁnal\noutput feature map, αi represents the output feature map of\nthe group convolution after the initial path of the ith stage,\nand Cat denotes a fully connected process.\nChannel feature adjustment\nAs shown in Fig. 3, ﬁrst, the fused output feature maps of\nthe connected channel dimension information extracted from\nthe segmented channels are input into adaptive global aver-\nage pooling (AGAP) and adaptive global maximum pooling\n(AGMP) to enable the postprocessing conditioning fusion of\nthe channel and spatial dimension information, respectively,\nto better adapt to the global and local channel information\nextracted from each group. Then, the regulated feature weight\ninformation is further extracted and processed to maximize\nthe regulated useful feature weight information by the fully\nconnected F1, Relu, fully connected F2, and sigmoid [ 57]\nfunctions. Finally, the feature weight information is multi-\nplied by the fused original output feature map to achieve\nfeature conditioning. The detailed ﬂow formula for this part\nis as follows:\nγ\nm /equal1Cat (Y 1, Y2),\nOm /equal1Sigmoid (F2(δ(F1(ϕ(γm ) + ω(γm ))))) ∗ γm , (3)\nHere, γm ∈ RC/Z×H×W ,m /equal11, 2, ... ,z .ϕ, ω represent\nAGAP and AGMP , respectively. Om ∈ RC/Z×H×W denotes\nthe ﬁnal output feature map through feature conditioning.\n123\n4906 Complex & Intelligent Systems (2024) 10:4897–4920\nI_Global\nO_Local\nC\nUpsample/None\nConv\nI_Local\nDownsampl e\nDownsampl e\nC\nMul/g415-Head\nA/g425en/g415on\nFeed-Forw ard \nNetworks\nP4\nP5\nP3\nMul/g415-Scale \nDila ted \nA/g425en/g415on\nQ1\nK1\nV1\nQ2\nK2\nV2\n(a) Aggregate feature hybrid transformer module (b) Eﬃcient Injec/g415on Module for \nDetec/g415on Informa/g415on\nQPE\nCon v\nCon vs\n++\n+\nFeed-Forward \nNetworks +\n+\nBasic  \nco nvo lu /g415on al \nsec/g415ons\nTransfo rmer sec/g415on\nFig. 5 Main structure of AFHTrans and the structure of the efﬁcient injection module for detecting information\nChannel shuffle\nLike in ShufﬂeNetV2, the SCEFA module follows the struc-\nture of SANet to improve the interaction of channel infor-\nmation between groups by putting the conditioned output\nfeatures through a channel shufﬂing operation for interfeature\ngroup communication. The original channels are reshaped\nwhile being redistributed and mixed to ensure that the num-\nber of input channels and the number of output channels are\nuniform.\nThe convolution-based aggregated feature hybrid\ntransformer module\nThis section details the combination of the aggregate fea-\nture hybrid transformer for convolution (AFHTrans) module\nand the efﬁcient injection of the detection information mod-\nule. Figure 5a shows the general structure of the AFHTrans\nmodule. It consists of a basic convolutional section (shown\nin the yellow dashed box) and a transformer section (shown\nin the orange dashed box). Based on the original structure\nof the Transformer, the core parts of its position encoding\n(PE), multi-head attention (MHA), and feedforward neural\nnetwork (FFN) are extracted. Inspired by the use of Dilate-\nformer [ 26], a combination of multi-scale dilated attention\n(MSDA) is introduced, which not only achieves a trade-off\nbetween computational cost and sensory wildness but also\nfurther optimizes global dependency modelling and effec-\ntive aggregation of semantic multiscale information. Then,\nthe basic convolutional section and the two attention layers\nare cascaded to construct a module that combines convolution\nand a transformer, which effectively extracts global feature\ninformation and local feature information. Thus, the AFH-\nTrans module not only extracts multiscale channel features\nbut also reduces global object pixel independence.\nThe feature extraction process is divided into two stages:\nlocal feature extraction by convolution and the establishment\nof remote interdependencies. By using the basic convolu-\ntional section, it is possible not only to fully extract the\n×8\nMul/g415-Head Self-A/g425en/g415on\nCon v\nCon vs\nMul/g415-Scale Dilated A/g425en/g415on\nR=1 R=2 R=3 R=4\nFea ture map\n×L\n+\nF\nFig. 6 Stereogram of the combined convolution and transformer module\nin AFHTrans\npixel space information in the local feature extraction stage\nbut also to save computations. To save the computational\nresources of the video memory, the feature layer inputs were\nthen downsampled (average pooling operation) to the top\nresolution, and the feature map information of the three lay-\ners was fused using a three-way fully connected approach.\nThe three-dimensional structure of the convolution and trans-\nformer combination is shown in Fig. 6. L represents the\nnumber of layers in the convolution and transformer lay-\ners. Assume that the input features after full connection are\nFR\n3C∗×H×W (C∗=256). The AFHTrans module is divided\ninto a multiscale dilated attention path (upper part in Fig. 5a)\nand a multihead attention path (lower part in Fig. 5a). In the\nmultihead attention path, a conv block ordinary convolution\nis used to extract local features and compress the channel. In\nmultiscale dilated attention paths, the use of a combination of\nConvs block ordinary convolution and depth-separated con-\nvolution [ 58] of residuals enables the extraction of localized\nfeatures while compressing the channel and extracting spatial\ndetail information. In this case, depth-separated convolution\nis used by the module to extract spatial information from the\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4907\nfeature map and to improve the interaction of feature infor-\nmation in each channel. The extraction performance of the\nconverter is improved by deep separable convolution, which\nalso reduces the number of computational parameters. The\ncomputational formula ( 4) is shown below.\nI\n1 /equal1Con v(F)\nI2 /equal1DwCon v(I1) + I1, (4)\nHere, I1, I2ϵ RC∗×H×W , Con v stands for ordinary con-\nvolution, and andD wCon v represents depth-separated con-\nvolution. In this case, the input feature maps are ﬁltered by\ndepth-by-depth convolution, and the input channels are inte-\ngrated by point-by-point convolution.\nIn the remote interdependency establishment phase, the\nglobal feature acquisition capability is not only effectively\nimproved by the transformer but also capable of capturing\nmultiscale contextual semantic dependencies. This part of the\nwhole process combines the standard layer and the residual\nstructure. In the multiscale dilated attention path, ﬁrst, the\ninput feature maps go through the standard layer to generate\nQ\n1, K1, and V1 to accelerate convergence. Afterwards, Q1,\nK1, and V1 are computed by the multi-scale dilated attention\nand then linearly mapped by the FFN, thus expanding the\nreceptive ﬁeld and improving the mutual independence of\nthe object pixels. In the multi head attention path, ﬁrst, the\nfeature maps of the inputs to the basic convolution section\nare passed through the standard layer to Q\n2 and K2, and\nV2 generate to accelerate convergence. Then, Q2 and K2\nare passed through PE to correct the feature map position\ninformation. After that, Q2, K2, and V2 are computed by the\nmulti-head attention mechanism and then linearly mapped\nby the FFN in the standard layer to enhance the extraction\nof global features and spatial information. Eventually, the\nfeature maps of the two parts are aggregated to fuse rich\nlocal and global feature information. The formula is shown\nbelow:\nMSDA /equal1Sof tmax\n(\nQ1ij K T\n1r√\ndk1\n)\nV1r ,1 ≤ i ≤ W ;1 ≤ j ≤ H,\nMHA /equal1Sof tmax\n(\nQ2 K T\n2√\ndk2\n)\nV2,\nT /equal1rFFN (rMHA(PE(I1),P E (I1), I1) +r M S D A(I2, I2, I2)),\n(5)\nHere, T RC∗×H×W denotes the output feature map after\nAFHTrans fusion, Q1ij denotes the query of position (i, j)\nin the original feature map, and rin K1r and V1r denote the\ndilation rate of the multi-scale dilated attention. rinrMHA,\nrMSDA, and rFFN denote the residual structure operation.\nPE stands for the position encoding process. By default, a\nstandard layer is added before each computational step in\nEq. ( 5) to accelerate model training.\nAs shown in Fig. 5b, in the Injection’s Detection Infor-\nmation Efﬁcient Injection Module, a simple combination of\nstructures is used to inject efﬁcient information into the mid-\ndle and high feature layers of the neck to fuse the feature maps\nto transmit global and local feature information. Following\nTopFormer’s information injection module, I_Local is the\ninput path for the local network feature layer, and I_Global\nis the input path for the AFHTrans module, where the fusion\nof mid-layer features requires the addition of an upsampling\noperation. Afterwards, the two feature information sources\nare fully fused by full connectivity, and later convolution\nis used to compress the channel and improve the feature\nextraction ability to generate the feature map output from\nthe middle- and high-level fusion information. Therefore,\nthe calculation formula is:\nOτ /equal1Con v\n(\nCat\n(\nILocal(X∗), Interpolate /None\n(\nIGlobal(T)\n)))\n, τ /equal11, 2,\n(6)\nHere, O τ ϵ RC∗×H×W represents the feature map of the\nAFHTrans module after fusion with the middle and upper\nlayers, Interpolate /None represents the middle layer upsam-\npling operation and the high layer no upsampling operation.\nFinally, the information injected by the AFHTrans module\nis transferred to the FPN for feature fusion and later pooled\ninto the feature extraction detection head for classiﬁcation\nand localization after full connectivity. The process is as fol-\nlows:\n/Delta1/equal1/Gamma1\ncls , reg (Cat (FPN(Oτ1,O τ2, ... , ϒ))), (7)\nHere, 7 RC∗×H×W represents the feature maps of the lay-\ners that have not been injected with information by the\nAFHTrans module, and FPN represents the feature pyramid\nnetwork fusion feature process. /Gamma1\ncls , reg denotes the process-\ning of the feature extraction detection header. 1 RC∗×H×W\nrepresents the ﬁnal output of the detector.\nFeature extraction head\nImproving the traditional parallel head structure can effec-\ntively compensate for the lack of processing features in the\nfront-end model and provide comprehensive supervision of\nthe ﬁnal feature information. After front-end model network\nprocessing, the acquisition of local and global features and\nlong-range object pixel dependencies of the model are effec-\ntively improved. However, for the back-end head task, there\nis a lack of extraction of global feature information from the\nfeature maps after the fusion and stitching of features from\neach layer of the FPN. Therefore, to enhance the extraction of\nglobal feature information and optimize the head structure,\nwe consider two aspects: on the one hand, improving the\ninformation interaction of the detection head. On the other\n123\n4908 Complex & Intelligent Systems (2024) 10:4897–4920\n×\nCon v_key\nreshape\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nW\nC*W*H\n80/4*W*H\n80/4*W*H\nA\nCon cat&Conv\nSigmoid\nCon cat&Conv\nSigmoid\nLayer informa/g415on  extrac/g415on \n× Element-wise cross mul/g415plica/g415on\nA Classi ﬁca/g415on as product of eleme nts\n  Regression is  localiza/g415on space oﬀset\nCon v_value\nSo/g332 Max\nreshape\n+\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nC*W*H\nFE\nFE\nH*W*C\n4*W*H80*W*H\nLo caliza/g415on\nsup ervision\nClass iﬁca /g415o n\nSupervision\nCon v ×N\n\u000bD\f)HDWXUH\u0003H[WUDFWLRQ\u0003KHDG\u000b)(\u0010+HDG\f \u000bE\f)HDWXUH\u0003H[WUDFWRU\n+ Element-wise add\nf\nE\ni\n1~\nnter\nNE 1~\nk\nN\ntas\nE\ntask\nU /B P\nFig. 7 General structure of the FE head and the structure of the feature extractor\nhand, the attention of the detecting head is improved to the\nfeature information.\nFor this purpose, the feature extraction head (FE-Head),\nwhich consists of a feature interaction extractor and a fea-\nture extractor (FE), is designed as shown in Fig. 7(a).\nFollowing the structure of TOOD [ 29], to enhance the\ninteraction of information by the detection head, a fea-\nture interaction extractor is used to learn task information\ninteraction features from multiple convolutional layers. As\nshown in orange in Fig. 7(a). This design not only facilitates\ninformation interaction but also provides multiscale charac-\nterization of the different sensory ﬁelds involved in the two\ntasks. Assume that E\nf ϵ RC\n′\n×H\n′\n×W\n′ (\nC\n′\nnumberofchannels,\nH\n′\nheight, W\n′\nwidth\n)\nis the output feature of the FPN. The\nfeature interaction extractor is computed as follows:\nEinter\nk /equal1δ\n(\nconvk\n(\n...δ\n(\nconvkj (E f\n))\n...\n)\n, ∀k, jϵ{1, 2, ... , N }, (8)\nHere, k and kj of convk and convkj represent the kth con-\nvolutional layer and the jth activation function of the kth\nconvolutional layer of N consecutive convolutional layer pro-\ncesses, respectively. Therefore, rich multiscale features can\nbe efﬁciently extracted from an FPN using a single branch.\nFeature maps rich in feature information are generated\nthrough information interactions. Due to the single-branch\nstructure, it becomes difﬁcult to fully obtain the global feature\ninformation of the corresponding task. This is also discussed\nin [45, 59]. For this purpose, the features after the information\ninteraction can be input into the two FEs to extract the features\nfor the corresponding tasks from the direction of attention.\nThe features required to correspond to different tasks are dif-\nferent. Therefore, as shown in Fig. 7b, a layer information\nextraction mechanism (shown in the yellow dashed box) is\nproposed to obtain information by subtasking task-speciﬁc\nfeatures dynamically computed at the layer level. The for-\nmula is as follows:\nE\ntask\nk /equal1ϕk + Einter\nk , ∀kϵ{1, 2, ... , N }, (9)\nHere, ϕk is the kth feature map of the layer information\nextraction ϕ ∈ RC′×H′×W ′\n. ϕ is able to obtain the depen-\ndencies between layers while extracting global features after\nlayer information interaction feature computation:\nϕ /equal1Cv(Sof tmax (Ck (Einter\nk ) × Einter\nk )), (10)\nHere, Ck represents the channel normalization, and Cv\nrepresents the process of going through convolution, the stan-\ndard layer, ReLU, and convolution. The feature map obtained\nafter computing the corresponding task can fully extract fea-\ntures and reduce the dimensionality of the adjustment process\nto reduce the number of parameters. The E\ntask is a tan-\ndem characterization of Etask\nk . Finally, the ﬁnal extracted\nfeatures for localization or classiﬁcation are obtained from\neach Etask :\nUtask /equal1conv2\n(\nSigmoid\n(\nconv1\n(\nEtask )))\n, (11)\nwhere Utask is a tandem feature of Utsak\nk and conv1 and\nconv2 are 1 × 1 convolutional layers for variable dimen-\nsions. Utask is converted to a dense classiﬁcation score\nP ∈ RH×W ×80 using the sigmoid function or to a bounding\nbox B ∈ RH×W ×4 by applying the distance-to-bbox method\nin [ 12, 13]. Finally, following the TOOD approach, the spa-\ntial distribution and task coordination learning are further\nadjusted for both the P and B tasks to obtain global fea-\nture information for the classiﬁcation and localization tasks,\nrespectively. Thus, high-resolution feature map information\nwith additional edge information is extracted to improve the\nclassiﬁcation and localization accuracy of the detector.\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4909\nExperiments\nDatasets\nOur approach is evaluated using two main datasets: the PAS-\nCAL VOC2007 + 2012 [ 60] and COCO2017 [ 61] datasets.\nPASCAL VOC2007 + 2012 is a dataset of a comprehensive\nclass of scenarios. The dataset contains 27,088 images in\n20 categories. Of these, 16,551 out of 27,088 images were\nused for training, and 4952 were used for validation. The\nCOCO2017 dataset is a dataset of a class of common every-\nday objects. A total of 163,957 images were included in 80\ncategories. Of these, out of 163,957 images, 118,287 were\nused for training, 5000 for validation, and 40,670 for test-\ning. The training and veriﬁcation operations have labels with\ncorresponding information.\nImplementation details\nGLFTNet uses ResNet50 or ResNet101 pretrained on the\nImageNet dataset as the backbone. The SCEFA module and\nthe AFHTrans module were combined to serve as the NECK,\nwhere the number of heads for multi-head attention and\nmulti-scale dilated attention were set to 8 and 4, respec-\ntively. FE-Head as a Model Head. The model was trained\nusing stochastic gradient descent (SGD) with momentum /equal1\n0.9 and weight decay /equal10.0001 and a strategy of setting\nthe learning rate in segments with gamma /equal10.1 and lin-\near warmup steps /equal1500. The initial learning rate was set to\n0.001.\nThe resolution of the images in the experiment is set to\n800 × 1333, and the data enhancement includes operations\nsuch as random ﬂipping and multiscale training. Training\nwas performed with the number of epochs set to 12 and the\nbatch size set to 4. Experiments were performed on a single\nNVIDIA Tesla V100 GPU device. Our model is optimized\nfor classiﬁcation and localization tasks using two loss func-\ntions. The ﬁrst classiﬁcation loss function is focal loss. The\nsecond bounding box regression loss function is the GIOU\n[62]. Thus, the total loss function is as follows:\nL /equal1w\ncls Lcls + wreg Lreg , (12)\nHere, Lcls denotes the classiﬁcation loss function, and\nLreg denotes the boundary regression loss function. wcls and\nwreg are weights set to 1.0 and 2.0, respectively.\nEvaluation metrics\nIn this paper, the intersection over union (IOU), average\nprecision (AP), mean average precision (mAP), frames per\nsecond transmission (FPS), parameters (number of parame-\nters), and number of ﬂoating point operations (FLOPs) are\nused as evaluation metrics for the model. IOU denotes the\ndegree of resynthesis of the predicted frame with the true\nframe, which is the ratio of the intersection and concatenation\nof the detection result and ground truth. This metric usually\nsets the threshold (IOU\nthreshold ) to determine the truthful-\nness of the prediction frame. That is, it is usually set to 0.5,\nwith a value greater than 0.5 considered a correct prediction\nand a value less than 0.5 considered an invalid detection. AP\nand mAP (here, the IOU is generally 0.5) are used to evalu-\nate the ability of the algorithm to correctly detect an object\nand are the most important evaluation metrics for detection\nalgorithms. Depending on the accuracy or inaccuracy of the\nprediction frames, the evaluation object algorithm includes\nfour samples: true positives (TP), false positives (FP), false\nnegatives (FN), and true negatives (TN). TP is the number of\npredicted frames with IOU > IOU\nthreshold , FP is the number\nof predicted frames with IOU ≤ IOUthreshold , and FN is the\nnumber of real frames that are not detected correctly. As a\nresult, Pprecision , Rrecal , and AP are calculated as follows:\nPprecision /equal1TP\nTP +FP ,\nRrecal /equal1TP\nTP +FN ,\nAP /equal1\n1∫\n0\nPprecision dR recal ,\n(13)\nAP denotes the area of the curve formed by Pprecision\nand Rrecal . It speciﬁcally refers to the accuracy of a particu-\nlar category among multiple categories. Based on the above\ndescription of AP , the speciﬁc formula for determining the\nmAP is as follows:\nmAP /equal1\n∑ C\nk/equal10 APk\nC , (14)\nThe mAP is the average of the AP values after sum-\nming multiple categories. C denotes the total number of\nmultiple categories, and k represents different categories. In\nparticular, the AP for the COCO dataset calculates the mul-\nticategory mean by default, i.e., the mAP . The FPS is an\nimportant metric for evaluating how fast an algorithm can\ndetect; it is the ratio of the number of frames to the elapsed\ntime. In addition, Params and FLOPs represent the compu-\ntational space complexity and computational complexity of\nthe model, respectively.\nAblation study\nIn this section, ablation experiments are performed to ver-\nify the validity of our method. The validity of structures\nsuch as the SCEFA, AFHTrans, and FE-Head was veriﬁed\nby ablation experiments on the PASCAL VOC2007 + 2012\nand COCO2017 datasets.\n123\n4910 Complex & Intelligent Systems (2024) 10:4897–4920\nTable 7 V ariation in the convolutional group size and convolutional\nkernel size in the experiments\nBackbone Kernel size Group size mAP\nResNet50 1, 3, 5, 7 1, 2, 4, 8 80.56\nResNet50 1, 3, 7, 9 1, 2, 2, 8 80.63\nResNet50 3, 5, 7, 9 1, 4, 8, 16 81.79\nResNet50 3, 5, 5, 7 1, 4, 4, 16 80.80\nThe bold symbols represent the best results of the convolution kernel\nand convolution group size experiments\nSCEFA module\nAccording to Fig. 3, the SCEFA module is able to fully extract\nthe channel information from the split segmentation. First,\nthe features are grouped for segmentation to decrease the\nnumber of computational parameters. Afterwards, accord-\ni n gt oF i g .4, MFCEM can effectively extract various global\npieces of information from each group of segmented channels\nto generate an information-rich feature map. In this case, the\ndata are ﬁrst divided into two paths to go through the group\nconvolution in the MFCEM to compress the channels to pro-\nduce spatially scaled feature maps. Here, ResNet50 is used\nas the backbone network. The appropriate convolution kernel\nand convolution group size for group convolution are deter-\nmined. The performance of the MFCEM in the network is\ntested on the PASCAL VOC2007 + 2012 dataset using differ-\nent convolutional kernel sizes and convolutional group sizes,\nas shown in Table 7, considering the number of channels and\nthe need for practical experimental comparisons, based on\nthe work of [ 24, 52]. Table 7 shows the experimental results.\nThe average accuracy obtained for the ﬁrst group with a con-\nvolution kernel size and convolution group size set to 1, 3, 5,\nand 7 and 1, 2, 4, and 8 is 80.56%. The next set of settings\nwere 1, 3, 7, 9, and 1, 2, 2, 8, yielding a result of 80.63%,\nan improvement of 0.07% over the previous set. The convo-\nlution kernel sizes for the following two groups are set to 3,\n5, 7, 9 and 3, 5, 5, and 7; the convolution group sizes are set\nto 1, 4, 8, and 16; and 1, 4, 4, and 16, respectively. The ﬁnal\naccuracies obtained are 81.79% and 80.80%, respectively. In\na comprehensive comparison, the optimal convolution kernel\nsizes 3, 5, 7, and 9 and convolution group sizes 1, 4, 8, and 16\nwere selected as the parameters for group convolution in the\nmodule. Therefore, the use of multiple convolution kernels\nand convolution groups of different sizes and a large range\nof numbers can improve the speed and accuracy of parallel\ntraining, and the appropriate adjustment of the size of the\nconvolution kernels and convolution groups can enhance the\nability to obtain feature map information. The experimental\nresults in the table effectively prove our conclusion.\nThe feature map is input to the GT module after group\nconvolution extraction of the weighted paths, which has an\nattention head to discriminate the important information of\neach branch and a transform neck to fully extract the fea-\nture information. As a result, the GT module can extract\nrich global textual information. Then, the channel attention\nweight values are generated by the Softmax function, which\nis effective at establishing long-range channel dependencies\nand calibrating weight information. To demonstrate the effect\nof the GT module and the Softmax comparison Sigmoid\nfunction on the experimental results, the above structures\nwere added. Table 8 shows. A total of 81.28% of the mAP\nis achieved using only the GT module in the ﬁrst row. The\nsecond step involves removing the GT module and using the\nSoftmax function to determine the probability correspond-\ning to the weights; this approach yields an mAP of 80.93%,\nwhich represents a decrease in the indicator, indicating that\nthe GT module is indispensable. Finally, to verify the effect\nof Softmax on the sigmoid function, the use of the sigmoid\nfunction alone in the third row improved the accuracy by\n0.33% over that of the Softmax function alone, conﬁrming\nthat the replacement of the sigmoid function indeed improved\nthe accuracy. However, by combining the GT module with the\nsigmoid and softmax functions, the mAP reached 81.58% and\n81.79%, respectively, with a difference of 0.21% between the\ntwo accuracies, and the combination of the GT module and\nthe softmax function resulted in an improvement of 0.51%\ncompared to the original use of the GT module alone. There-\nfore, the GT module and Softmax function can better extract\nimportant information about the weights so that the detection\naccuracy is improved because the overall models of these two\nparts are indispensable.\nThe two segmented channels are then fused dimensionally\nafter the global and local channel information is extracted\nby the MFCEM of the SCEFA module. This is followed\nby channel feature adjustments to better accommodate the\nfused global and local channel information. The feature\nmaps of the adjusted channels are then channel-swapped\nto perform intergroup communication to realize information\ninteraction. To verify the effects of group segmentation, chan-\nnel feature adjustment, and channel shufﬂing on the model\neffects and parameters, the above operations were performed\nsequentially. Table 9 shows. The ﬁrst row removes the group\nsegmentation process, and the mAP reaches 81.57%; here,\nthe parameter is 41.54 M. The last two rows sequentially\nremoved the channel feature adjustment and channel shuf-\nﬂing to obtain average accuracies of 81.45% and 81.40%,\nrespectively, and parameters of 35.17 M and 36.25 M were\nobtained. The highest accuracy was achieved under channel\nfeature adjustment and channel shufﬂing; therefore, these\ntwo processes provide effective extraction of feature infor-\nmation, but the number of parameters is large. After the latter\nuse of group segmentation, the parameters drop by 6.37 M\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4911\nTable 8 Ablation experiments\nvalidating the global text module\n(GT) and the Softmax\ncomparison sigmoid function\nBackbone GT Softmax Sigmoid mAP\nResNet50 √ 81.28\nResNet50 √ 80.93\nResNet50 √ 81.26\nResNet50 √√ 81.58\nResNet50 √√ 81.79\nTable 9 Ablation experiments\nwere performed in the SCEFA\nmodule to verify the effects of\ngroup segmentation, channel\nfeature adjustment, and channel\nshufﬂing\nBackbone GS CFA CS mAP Params (M)\nResNet50 √√ 81.57 41.54\nResNet50 √√ 81.45 35.17\nResNet50 √√ 81.40 36.25\nResNet50 √√ √ 81.79 36.41\nGS represents the group segmentation process, CF Arepresents channel feature adjustment, CS represents\nchannel shufﬂing\nTable 10 Experimental comparison of the SE-Net and the SCEFA\nMethod Backbone mAP FPS\nSE-Net ResNet50 81.25 13.75\nSCEFA (Ours) ResNet50 81.79 12.98\nSE-Net denotes the substitution for channel attention, SCEF Adenotes\nthe substitution for segmentation channel extraction feature attention\nand 5.29 M, respectively, and the number of parameters is\ngreatly reduced.\nFinally, by adding all three processes, the mAP fur-\nther increases to 81.79%, and the parameter quantization is\n36.41 M, which achieves a remarkable accuracy-parameter\nbalance. Overall, the group segmentation, channel feature\nadjustment, and channel shufﬂing processes in the SCEFA\nmodule enhance the spatial adjustment of feature information\nand the information interaction of feature maps and reduce\nthe number of parameters.\nFinally, the validity of the SCEFA module is fully demon-\nstrated. The SCEFA module and its structurally similar\nSE-Net module were subjected to comparative experiments,\nthe results of which are shown in Table 10. In the ﬁrst\nline, replacing the SCEFA module with the SE-Net mod-\nule achieves an mAP of 81.25% and a speed of 13.75 FPS.\nSimilarly, the use of the SCEFA module proves to be effec-\ntive by improving the accuracy by 0.54%, despite a 0.77\nFPS decrease in speed. Overall, the speed improved when\nusing the SE-Net module, but it still did not achieve the\nsame performance results as the SCEFA module. This further\ndemonstrated the importance of the SCEFA module for the\noverall model.\nTable 11 Comparison of modelling effects against basic convolutional\nsections\nMethod mAP\n+ Basic convolutional 81.79\n− Basic convolutional 81.44\nThe basic convolution section includes convolution and depth-separated\nconvolution. + Use the basic convolution section. − Does not use the\nbasic convolution section, but needs to add a dimensionality reduction\nconvolution to reduce the number of parameters\nAFHTrans module\nWith respect to the AFHTrans block, it is veriﬁed that the\ncombination of convolution and the transformer improves\nthe model performance. For the FPN, there is a lack of infor-\nmation interaction across layers, so aggregated connections\nare used. Moreover, after three layers of aggregation, the fea-\nture map is rich in information. Therefore, convolution and\ndual-attention fusion joining are used to enhance the ability\nto acquire both local channel spatial information and global\ninformation. Among them, embedding convolution in a trans-\nformer not only enhances spatial information extraction but\nalso improves the interaction between channel information.\nTo this end, ablation experiments are performed on the PAS-\nCAL VOC2007 + 2012 dataset to validate the effect of the\nembedded convolution on the modelling effect. As shown in\nTable 11, the mAP reaches 81.79% with the addition of the\nbasic convolutional section, decreases by 0.35% under no\naddition, and reaches 81.44%. As a result, whether the basic\nconvolution section is added to the AFHTrans module has a\nsigniﬁcant impact on the results, effectively verifying that the\ncombination of convolution and the transformer contributes\n123\n4912 Complex & Intelligent Systems (2024) 10:4897–4920\nTable 12 Ablation experiments were performed for the multi-head\nattention and multi-scale dilated attention components\nBackbone BC and FFN MHA MSDA mAP\nResNet50 √√ 81.41\nResNet50 √√ 81.58\nResNet50 √√ √ 81.79\nBC denotes the basic convolutional section, MHA denotes a multi-head\nattention section process, MSDA denotes a multi-scale dilated attention\nsection process, FFN denotes a feedforward neural network\nTable 13 V ariation in expansion rates for multi-scale dilated attention\nin the AFHTrans module\nBackbone Dilation rate mAP\nResNet50 2, 4, 4, 6 81.55\nResNet50 2, 4, 6, 6 81.62\nResNet50 2, 4, 6, 8 81.79\nResNet50 2, 4, 8, 8 81.71\nThe bold text represents the best results of the experiment at different\ndilation rates\nto the results. Therefore, convolution can acquire local fea-\ntures and spatial location information.\nUsing ResNet50 as the backbone network, the effects of\nthe multi-head attention and multi-scale dilated attention\ncomponents on the network gain were observed to verify\nthe effectiveness of both components in the model. Table\n12 shows the ablation experiments for both the multi-head\nattention and multi-scale dilated attention components under\nthe use of the basic convolutional component and the feed-\nforward neural network. The mAP reached 81.41% and\n81.58% when using multi-head attention and multi-scale\ndilated attention, respectively. The combination of the two\ncomponents amounted to 81.79%, a 0.38% improvement\nover the use of multi-head attention. It is well demonstrated\nthat the combination of multi-head attention and multi-\nscale dilated attention enhances the model’s gain effect and\nimproves the ability to acquire global features and establish\nobject pixel dependencies at long distances.\nIn the multiscale dilation attention section, further exper-\niments are conducted to verify the effect of dilation rate\nchanges on our model to validate the performance of the\nAFHTrans module in the model. The results of the experi-\nments are shown in Table 13. The ﬁrst and second rows of\nthe table with expansion rates of 2, 4, 4, and 6 and 2, 4, 6, and\n6 correspond to accuracies of 81.55% and 81.62%, respec-\ntively, while the third row with expansion rates of 2, 4, 6,\nand 8 achieves the highest average accuracy of 81.79%, and\nthe ﬁnal expansion rate of 2, 4, 8, and 8 has a decrease in\nTable 14 Experimental tests of the comparisons of different connec-\ntions\nBackbone Combination mAP\nResNet50 {1, 2} 79.97\nResNet50 {1, 3} 80.72\nResNet50 {2, 3} 81.79\nResNet50 {1, 2, 3} 81.11\nBromos 1, 2, and 3 of the combination represent connections corre-\nsponding to low, medium, and high levels, respectively\nTable 15 Effect of different positions in the AFHTrans module on the\neffectiveness of the experiment\nMethod mAP\nFront 81.79\nBehind 80.90\nThe front is at the front end of the FPN, and the behind is at the back\nend of the FPN\naccuracy of 0.08% compared to the previous row. Consid-\nering these results, as with the size law of the convolution\nkernel and convolution group above, more expansion events\nwith different sizes and large ranges can effectively expand\nthe sensory ﬁeld and capture feature information. Similarly,\nthe use of an appropriate dilation rate helps to improve the\neffectiveness of the model. Thus, our model uses the 2, 4, 6,\nand 8 best expansion rates in multiscale expansion attention.\nExperiments were conducted on the effectiveness of the\nmodel using different connections in the information injec-\ntion section of the AFHTrans module. Table 14 shows that\nthe accuracy is 79.97% when connecting the low and middle\nlevels, while the accuracy rises to 80.72% when connecting\nthe low and high levels. The best accuracy of 81.79% was\nachieved in connecting the middle and high levels; for this\nreason, this injection was subsequently used in our model.\nHowever, there was a 0.68% decrease when connecting all\nthree levels. From the table, it can be found that it is not the\ncase that the more connected the hierarchical layer is, the\ngreater the effect, but the model will improve accordingly\nwith the higher precision of the connected levels. Among\nother things, we believe that the speciﬁc cause is the interac-\ntion of the middle and upper levels with the FPN.\nTo investigate the effect of the location of the AFHTrans\nmodule in the FPN on the model, experiments were con-\nducted to place the module in different locations in the FPN\nto compare the model effects. Placing the module at the front\nend of the FPN yielded an accuracy of 81.79%, while at the\nback end, the accuracy decreased to 80.90%. As shown in\nTable 15, the module placement at the front end can thus\neffectively extract rich semantic information and facilitate\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4913\nTable 16 Comparison of the experimental effects on the DETR trans-\nformer and AFHTrans\nMethod Backbone mAP FLOPs(G) FPS\nDETR Transformer ResNet50 81.67 192 12.51\nAFHTrans(Ours) ResNet50 81.79 189 12.98\nThe DETR Transformer denotes the structure of the Transformer\nencoder and decoder using DETR, and the AFHTrans denotes the\nimproved structure using the Transformer\nfurther feature fusion in the FPN. Moreover, removing fused\ninformation from the back end results in useless and redun-\ndant information, leading to a decrease in the ability to obtain\nvalid features. Therefore, the AFHTrans module is placed at\nthe front end of the FPN.\nFinally, the transformer part of DETR is very similar to the\noriginal transformer. To test the effectiveness of AFHTrans\nin optimizing the cost and speed of transformer computa-\ntions. Thus, a comparison experiment was set up using the\nDETR Transformer (including the encoder and decoder parts\nof DETR), and the results of the experiment are shown in\nTable 16. The ﬁrst row of the table achieves an accuracy of\n81.67% with the use of the DETR transformer, with a com-\nputational cost of FLOPs and a speed of 193 G and 12.51\nFPS, respectively. In the second row of the table, the replace-\nment of AFHTrans results in a 0.12% increase in accuracy,\na 3G decrease in computational cost FLOPs, and a 0.47 FPS\nimprovement in speed. Thus, AFHTrans is effective at opti-\nmizing the computational cost and speed of the transformer.\nFE-Head\nThe FE-Head not only has task interaction capability but also\nhas the powerful ability to acquire global feature information.\nAmong them, the feature interaction extractor and the feature\nextractor are the main parts of our detection head. The feature\ninteraction extractor in TOOD has been proven effective, so\nan experimental study is not developed here. The feature\nextractor is analysed in the following ablation experiment.\nThe effects of Conv_key and Conv_value in the feature\nextractor as well as multiscale and random ﬂipping on the\nexperimental results are explored. As shown in Table 17,t h e\nﬁrst line has an mAP of 81.10% when only Conv_key is\nused and the parameter is 36.34 M. The second line has an\nmAP of 80.22% when using only Conv_value and a param-\neter of 36.26 M. In the third line, when both Conv_key and\nConv_value are used, the precision reaches 81.79%, and the\nparameter rises by only 0.15 M. As a result, it can be con-\nﬁrmed that the Conv_key and Conv_value boosting effects\nare signiﬁcant and reduce the number of parameters, which\nfacilitates the extraction of feature context information. The\nTable 18 Comparing the T-Head and FE-Head experimentally\nMethod Backbone mAP Params (M)\nFCOS-Head ResNet50 79.65 36.60\nT-Head ResNet50 82.28 36.32\nFE-Head (Ours) ResNet50 82.39 36.41\nFCOS-Head denotes the use of a multibranch task decoupling header, T-\nHead denotes the use of a task alignment header, and FE-Head denotes\nthe use of our detection headerfollowing two rows sequentially add random ﬂip and mul-\ntiscale data enhancement operations, yielding metrics with\n82.39% and 82.76% excellent accuracy, respectively. There-\nfore, data augmentation is used so that the richer the amount\nof data is, the more accurate our model will be. The latter\nexperiments were conducted using the randomized ﬂipped\ndata enhancement method.\nThe multibranch task decoupling head (FCOS-Head [ 12])\nof the FCOS can effectively suppress the generation of low-\nquality bounding boxes far from the target location without\nintroducing any hyperparameters. TOOD’s [ 29] Task Align-\nment Head (T-Head) improves coordination of classiﬁcation\nand localization tasks while enhancing task interactivity. It\ncan also apply the task alignment learning method to adjust\nthe distance between anchor points, thus realizing accurate\ndetection of objects. Among them, FCOS-Head, T-Head, and\nFE-Head are all anchor-free detection heads. To validate the\neffectiveness of the FE-Head, the FE-Head was experimen-\ntally compared to these two detected head structures. The\ncomparison results are shown in Table 18. In the ﬁrst row of\nthe table, the accuracy of FE-Head is 2.74% greater than that\nof FCOS-Head, and the number of parameters decreases by\n0.19 M. In the second row of the table, on the other hand, the\nprecision of FE-Head is 0.11% higher than that of T-Head,\nand the number of parameters is elevated by only 0.09 M.\nThe detection plots of FCOS-Head, T-Head and FE-Head at\na conﬁdence level of 0.5 are visualized in Fig. 8. The cars and\npeople in the ﬁrst and second columns of the ﬁrst and second\nrows using FCOS-Head and T-Head had varying degrees of\nmissed detections and low category accuracy, whereas the\ncars and people in the ﬁrst and second columns of the third\nrow using FE-Head could be accurately detected (shown as\ngray circles in Fig. 8). The ﬁgure shows that the detection of\nobjects is signiﬁcantly improved, and the detection accuracy\nis also greatly improved. The effectiveness of the FE-Head\ndetection head in extracting global contextual information\nfrom image objects is fully conﬁrmed. Although the number\nof parameters was slightly greater than that of T-Head, the\ndetection achieved satisfactory results.\n123\n4914 Complex & Intelligent Systems (2024) 10:4897–4920\nCombination of the SCEFA, AFHTrans module\nand FE-Head\nThe combination of the SCEFA module, the AFHTrans mod-\nule, and the FE-Head yielded excellent detection results\nwhen combined with GLFTNet. The entire AFHTrans mod-\nule section consists of the AFHTrans module and the FPN.\nAmong them, the AFHTrans module, which combines con-\nvolution and transformation, plays a key role in global and\nlocal feature extraction. To demonstrate the effect of the\nabove three modules, different experimental setups (shown\nin Tables 19 and 20) are tested to verify that the use of the\nSCEFA, AFHTrans, or FE-Head can improve the detection\nperformance. Table 19 shows that, compared to the Base-\nline, the SCEFA module improves the mAP by 1.38% and is\n0.23 FPS faster. The AFHTrans module increased the mAP\nby 1.42% and the speed by 2.90 FPS. With our FE-Head, the\nmAP increased by 1.45%, and the speed was 0.52 FPS faster.\nThe modules produced signiﬁcant improvements in accu-\nracy and speed, validating the effectiveness of the modules.\nWhen the three modules are combined, the detection per-\nformance reaches a high accuracy metric of 82.39%, with a\nslight decrease in speed of 0.68 FPS; thus, our model achieves\na balance of speed and accuracy. An improvement in accu-\nracy of 2.38% over that of the baseline case demonstrates that\nTable 17 Conducting FE\nexperiments to validate\nConv_key and Conv_value\nablation and exploring the impact\nof employing data augmentation\non the experimental results\nBackbone Ck Cv RF MS train mAP Params(M)\nResNet50 √ 81.10 36.34\nResNet50 √ 80.22 36.26\nResNet50 √√ 81.79 36.41\nResNet50 √√√ 82.39 36.41\nResNet50 √√√√ 82.76 36.41\nCk denotes Conv_key, Cv denotes Conv_value, RF denotes RandomFlip, MS train denotes multiscale training\nFig. 8 The top, middle, and\nbottom graphs show the results\nof the effect plots using\nFCOS-Head, T-Head, and\nFE-Head, respectively, at a\nconﬁdence level of 0.5\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4915\nTable 19 Ablation experiments\nwere performed on the PASCAL\nVOC2007 + 2012 dataset using\nthe SCEFA module, AFHTrans\nmodule, and FE-Head\nMethod Backbone SCEFA AFHTrans FE-Head mAP FPS\nBaseline ResNet50 80.01 13.23\nGLFT ResNet50 √ 81.39 13.46\nGLFT ResNet50 √ 81.43 16.13\nGLFT ResNet50 √ 81.46 13.75\nGLFT ResNet50 √√ √ 82.39 12.64\nThe baseline represents TOOD without using task-aligned heads, SCEFA represents the process of extracting\nfeatures after segmentation channels, AFHTrans represents the convolution-based aggregated feature mixing\ntransformer, and FE-Head represents the process of using feature extraction heads\nTable 20 Comparison of the results of adding the SCEFA module, the\nAFHTrans module, and the FE-Head to the COCO2017 validation set\nin turn\nMethod Backbone AP\nBaseline ResNet50 41.8\nBaseline + SCEFA ResNet50 42.8\nBaseline + SCEFA + AFHTrans ResNet50 43.3\nBaseline + SCEFA + AFHTrans + FE-Head ResNet50 43.5\nThe baseline represents a TOOD that does not use task alignment head-\ners, and + represents the process of adding modules sequentially\nour method effectively improves the detection performance\nof the detector.\nTo further validate the effect, the detection plots of the\nTOOD, SCEFA, and AFHTrans modules were visualized\nin detail on the PASCAL VOC2007 + 2012 validation set.\nAccording to the ﬁrst row of Fig. 9, the detection accuracy\nof the airplane has signiﬁcantly improved, and the detec-\ntion of medium-sized objects has signiﬁcantly improved. The\nsmall target of a person riding a horse (marked by a red\ncircle) in the second row was detected without any prob-\nlems, and the objects for detecting the small target were thus\nimproved. The people and chairs (marked by red circles and\nellipses) in the third row are correctly detected, allowing the\nimage to improve the problem of missed detection in complex\nscenes. The lamb in the fourth row (marked by a red circle)\nis correctly detected, further validating the adaptability of\nour model to detect objects at different scales. An overall\nimprovement in detection accuracy was achieved compared\nto that of the baseline. Thus, combining the three modules\nplays an active and effective role in our model, and our model\nis competitive in detection applications.\nIn addition, the SCEFA module, the AFHTrans module,\nand the FE-Head were added in turn to test the effectiveness\nof the modules even further on the COCO2017 validation set.\nA ss h o w ni nT a b l e20, the effect of adding the SCEFA module\non the extraction of multiscale local features of the channel\nincreases the accuracy by 1.0% over that of the baseline. The\nlater addition of the AFHTrans module for global and local\nfeature extraction by the network, as well as the establishment\nof object pixel dependencies over long distances, improved\nthe network by another 0.5%. Finally, the addition of FE-\nHead to the head for effective access to the collected feature\ncontext information yields a 0.2% boost. Based on the above\nresults, the validity of our modules is fully veriﬁed.\nComparison with other object detectors\non the COCO2017 dataset\nTo further validate the effectiveness of our model, com-\nparative experiments with other research algorithms were\nconducted on the COCO2017 dataset. The large-scale\nCOCO2017 dataset was used for training, which included\nthe SCEFA module (which improves the extraction of high-\nlevel local feature map information), the AFHTrans module\n(which improves the independence of global and local infor-\nmation extraction and establishes the dependency of pixels on\ndistant objects), and the FE-Head module (which performs\nthe extraction of global feature contextual information at the\nhead). The three main modules are combined to produce a\nnew GLFTNet detection model. As shown in Fig. 1, when\ncomparing GLFTNet with several classical networks, it can\nbe visualized that our model achieves a leading edge. When\ntraining our model using multiple scales (480–800), as shown\nin Table 21, the performance of our detection network is\ndetailed for each parameter. With the use of the backbone net-\nworks ResNet50 and ResNet101, GLFTNet achieves 44.3%\nand 47.0% AP on the COCO2017 test-dev dataset, respec-\ntively, outperforming the state-of-the-art detectors Sparse\nR-CNN [ 63], Anchor-DETR [ 64], and DAB-DETR [ 65].\nCompared to other detectors, our model achieves greater\nprogress in terms of improving AP metrics under different\nIOU sizes, effectively validating its effectiveness for improv-\ning global and local feature extraction. The parameters and\ncalculations are also reduced from those of the baseline net-\nwork, which is at the upper level among the other detection\nmethods.\n123\n4916 Complex & Intelligent Systems (2024) 10:4897–4920\nFig. 9 Effect map results of ablation experiments. a Baseline, b SCEFA + Baseline, c AFHTrans + Baseline, d FE-Head + Baseline, e GLFTNet\nTable 21 Comparison of GLFTNet with several state-of-the-art methods on the COCO2017 dataset\nMethod Backbone Reference Epoch MS train Params(M) FLOPs(G) AP AP 50 AP75 APS APM APL\nCNN-based object detectors\nFaster R-CNN [ 10] ResNet101 TPAMI\n17\n108 60 246 44.0 63.9 47.8 27.2 48.1 56.0\nRetinaNet [ 9] ResNet101 ICCV 17 36 √ 56 197 40.4 60.2 43.2 24.0 44.3 52.2\nCascade R-CNN\n[7]\nResNet101 CVPR 18 36 77 364 42.8 62.1 46.3 23.7 45.5 55.2\nFCOS [ 12] ResNet101 ICCV 19 24 √ 50 290 41.5 60.7 45.0 24.4 44.8 51.6\nAT S S [13] ResNet101 CVPR 20 24 √ 50 292 43.6 62.1 47.4 26.1 47.0 53.6\nSparse R-CNN\n[63]\nResNet101 CVPR 21 36 √ 125 256 43.5 62.1 47.2 26.1 46.3 59.7\nTOOD [ 29] ResNet101 ICCV 21 24 √ 51 267 46.7 64.6 50.7 28.9 49.6 57.0\nBased on the Transforner object detectors\nDETR [ 15] ResNet101 ECCV 20 500 √ 60 152 43.5 63.8 46.4 21.9 48.0 61.8\nDeformable-\nDETR\n[16]\nResNet50 ICLR 21 50 √ 40 173 43.8 62.6 47.7 26.4 47.1 58.0\nTSP-FCOS [ 23] ResNet101 ICCV 21 36 √ – 255 44.4 63.8 48.2 27.7 48.6 57.3\nAnchor-DETR\n[64]\nResNet101 AAAI 22 50 58 – 43.5 64.3 46.6 23.2 47.7 61.4\nDAB-DETR [65] ResNet101 ICLR 22 50 63 174 44.1 64.7 47.2 24.1 48.2 62.9\nGLFTNet(Ours) ResNet50 – 12 36 189 44.3 62.5 48.0 25.3 47.5 57.2\nGLFTNet(Ours) ResNet101 – 24 √ 55 263 47.0 65.3 51.1 28.6 50.2 58.7\nThe reference column represents the literature sources for comparing object detection algorithms, MS train denotes multiscale training, and the data comparison\nresults (single-model results) are provided by the corresponding ofﬁcial and experimental sources\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4917\nFig. 10 The top, middle, and bottom two ﬁgures show a comparison of the effect plots for TOOD, DAB-DETR, and GLFTNet (Ours), respectively\nOur method was compared with TOOD and DAB-DETR\nfor visualizing results on the COCO test set, as shown in\nFig. 10. In the ﬁrst column, TOOD detects only soccer balls,\nand DAB-DETR detects only people. Our method success-\nfully detects both people and soccer balls correctly (shown by\nthe yellow circle and ellipse), so our method is signiﬁcantly\neffective for small and overlapping object detection. Among\nother things, it beneﬁts from improvements in feature extrac-\ntion to enhance pixel discrimination in object regions. The\nbackpack behind the second column (shown by the yellow\nellipse) was not detected by either the TOOD or DAB-DETR\nmethods but was successfully detected by our method, again\nvalidating the effectiveness of our method. Compared to that\nof the TOOD method, the detection accuracy of our model\nis overall greater, thus far ahead of the effectiveness of the\noriginal detection algorithm.\nConclusion\nIn this paper, local and global information extraction of object\npixels are enhanced from micro- and macroperspectives,\nrespectively. The acquisition of image feature information\nis crucial to the performance of a model and is prone to mis-\ndetection and omission when detecting objects of different\nscales and occlusions in an image. Inspired by this problem,\nthis paper proposes the use of the SCEFA module to enhance\nlocal feature extraction of multiscale channel information\nin high-level feature maps from a microscopic perspective.\nAFHTrans is presented from a macro perspective. First,\nthis approach enhances the interaction between the feature\ninformation of each layer, maximizing the computational\ncost. Then, self-attention and convolution are combined to\nenhance global and local feature extraction. Finally, long-\ndistance dependencies between object pixels are established.\nIn terms of modelling, the big picture proposed FE-Head.\nThis approach not only further enhances the feature extrac-\n123\n4918 Complex & Intelligent Systems (2024) 10:4897–4920\ntion of the detected head but also improves the acquisition\nof full-text information. This method can accurately detect\nobjects under complex conditions, such as different scales of\nobjects, overlaps, and similar colours. Compared with other\nadvanced methods, our method has the best detection effect.\nHowever, the methodology of this paper is still deﬁcient. On\nthe one hand, there is still a need to improve the accuracy of\nthe detection of particularly small and over obscured objects.\nOn the other hand, our model only tested on the PASCAL\nVOC2007 + 2012 and COCO2017 datasets. Moreover, the\neffect of the model is difﬁcult to sustain in the face of a highly\nvariable dataset. Therefore, for the method proposed in this\npaper, there is a need to continuously optimize our model\nand expand the generalization ability of the model to achieve\nsuperior results for training on this experimental dataset and\nadditional datasets.\nAcknowledgements The authors gratefully acknowledge the ﬁnancial\nsupport from the National Natural Science Foundation of China (Grant\nNos. 61472220, 61572286).\nAuthor contributions Formal analysis, Zhenyi Zhang and Tianping Li;\nMethodology, Zhenyi Zhang and Tianping Li; Supervision, Dongmei\nWei; Writing original draft, Tianping Li and Zhenyi Zhang.\nFunding National Natural Science Foundation of China, Grant/Award\nNumbers: 61472220, 61572286.\nData availability Data related to the current study are available from\nthe corresponding author upon reasonable request.\nDeclarations\nConﬂict of interest The authors have no conﬂicts of interest in the pub-\nlication of this paper.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adaptation,\ndistribution and reproduction in any medium or format, as long as you\ngive appropriate credit to the original author(s) and the source, pro-\nvide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in\nthe article’s Creative Commons licence and your intended use is not\npermitted by statutory regulation or exceeds the permitted use, you will\nneed to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n1. Zou Z, Chen K, Shi Z et al (2023) Object Detection in 20 Years: A\nSurvey. Proc IEEE 111:257–276. https://doi.org/10.1109/JPROC.\n2023.3238524\n2. Pathak AR, Pandey M, Rautaray S (2018) Application of\nDeep Learning for Object Detection. Procedia Comput Sci\n132:1706–1717. https://doi.org/10.1016/j.procs.2018.05.144\n3. Arulprakash E, Aruldoss M (2022) A study on generic object detec-\ntion with emphasis on future research directions. J King Saud Univ\n- Comput Inf Sci 34:7347–7365. https://doi.org/10.1016/j.jksuci.\n2021.08.001\n4. Dhillon A, V erma GK (2020) Convolutional neural network: a\nreview of models, methodologies and applications to object detec-\ntion. Prog Artif Intell 9:85–112. https://doi.org/10.1007/s13748-\n019-00203-0\n5. V aidwan H, Seth N, Parihar AS, Singh K (2021) A study on\ntransformer-based Object Detection. In: 2021 International Con-\nference on Intelligent Technologies (CONIT). IEEE, Hubli, India,\npp 1–6\n6. Girshick R, Donahue J, Darrell T, Malik J Rich Feature Hierar-\nchies for Accurate Object Detection and Semantic Segmentation.\nIn: arXiv preprint arXiv:1311.2524\n7. Cai Z, V asconcelos N (2018) Cascade R-CNN: Delving Into High\nQuality Object Detection. In: 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. IEEE, Salt Lake City,\nUT, pp 6154–6162\n8. Redmon J, Divvala S, Girshick R, Farhadi A (2016) Y ou Only\nLook Once: Uniﬁed, Real-Time Object Detection. In: 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR).\nIEEE, Las V egas, NV , USA, pp 779–788\n9. Lin T-Y , Goyal P , Girshick R, et al Focal Loss for Dense Object\nDetection. In: arXiv preprint arXiv:1708.02002\n10. Ren S, He K, Girshick R (2015) Faster r-cnn: Towards real-time\nobject detection with region proposal networks. Advances in neu-\nral information processing systems. In: arXiv preprint arXiv:1506.\n01497\n11. Bochkovskiy A, Wang C-Y , Liao H-YM (2020) YOLOv4: Optimal\nSpeed and Accuracy of Object Detection. In: arXiv preprint arXiv:\n2004.10934\n12. Tian Z, Shen C, Chen H, He T (2019) FCOS: Fully Convolutional\nOne-Stage Object Detection. In: 2019 IEEE/CVF International\nConference on Computer Vision (ICCV). IEEE, Seoul, Korea\n(South), pp 9626–9635\n13. Zhang S, Chi C, Yao Y , et al (2020) Bridging the Gap Between\nAnchor-Based and Anchor-Free Detection via Adaptive Training\nSample Selection. In: 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, Seattle, W A, USA,\npp 9756–9765\n14. Liu Y , Zhang Y , Wang Y , et al (2023) A Survey of Visual Trans-\nformers. IEEE Trans Neural Netw Learn Syst 1–21. https://doi.org/\n10.1109/TNNLS.2022.3227717\n15. V edaldi A, Bischof H, Brox T, Frahm J-M (2020) End-to-End\nObject Detection with Transformers. In: Computer Vision – ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part I. Springer International Publishing, Cham.\n16. Zhu X, Su W, Lu L, et al (2021) Deformable detr: Deformable\ntransformers for end-to-end object detection. In: arXiv preprint\narXiv:2010.04159\n17. V aswani A, Shazeer N, Parmar N, et al Attention is All you Need.\nIn: arXiv preprint arXiv:1706.03762\n18. Ivanov A, Dryden N, Ben-Nun T, et al Data Movement Is All\nY ou Need: A Case Study on Optimizing Transformers. In: arXiv\npreprint arXiv:2007.00072\n19. Chen Y , Dai X, Chen D, et al (2022) Mobile-Former: Bridging\nMobileNet and Transformer. In: 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE, New\nOrleans, LA, USA, pp 5260–5269\n20. Harjoseputro Y , Y uda IgnP , Danukusumo KP (2020) MobileNets:\nEfﬁcient Convolutional Neural Network for Identiﬁcation of Pro-\ntected Birds. Int J Adv Sci Eng Inf Technol 10:2290. https://doi.\norg/10.18517/ijaseit.10.6.10948\n21. Li K, Wang Y , Gao P , et al (2022) Uniformer: Uniﬁed trans-\nformer for efﬁcient spatiotemporal representation learning. In:\narXiv preprint arXiv:2201.04676\n123\nComplex & Intelligent Systems (2024) 10:4897–4920 4919\n22. Lou M, Zhou H-Y , Yang S, Y u Y (2023) TransXNet: Learning Both\nGlobal and Local Dynamics with a Dual Dynamic Token Mixer for\nVisual Recognition. In: arXiv preprint arXiv:2310.19380\n23. Sun Z, Cao S, Yang Y , Kitani K (2021) Rethinking Transformer-\nbased Set Prediction for Object Detection. In: 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV). IEEE, Mon-\ntreal, QC, Canada, pp 3591–3600\n24. Zhang H, Zu K, Lu J, et al (2023) EPSANet: An Efﬁcient Pyramid\nSqueeze Attention Block on Convolutional Neural Network. In:\nWang L, Gall J, Chin T-J, et al (eds) Computer Vision – ACCV\n2022. Springer Nature Switzerland, Cham, pp 541–557\n25. Zhang Q-L, Yang Y -B (2021) SA-Net: Shufﬂe Attention for Deep\nConvolutional Neural Networks. In: ICASSP 2021 - 2021 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, Toronto, ON, Canada, pp 2235–2239\n26. Jiao J, Tang Y -M, Lin K-Y et al (2023) DilateFormer: Multi-Scale\nDilated Transformer for Visual Recognition. IEEE Trans Multimed\n25:8906–8919. https://doi.org/10.1109/TMM.2023.3243616\n27. Lin T-Y , Dollar P , Girshick R, et al (2017) Feature Pyramid\nNetworks for Object Detection. In: 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR). IEEE, Honolulu, HI,\npp 936–944\n28. Zhang W, Huang Z, Luo G, et al (2022) TopFormer: Token Pyra-\nmid Transformer for Mobile Semantic Segmentation. In: 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR). IEEE, New Orleans, LA, USA, pp 12073–12083\n29. Feng C, Zhong Y , Gao Y , et al (2021) TOOD: Task-aligned\nOne-stage Object Detection. In: 2021 IEEE/CVF International\nConference on Computer Vision (ICCV). IEEE, Montreal, QC,\nCanada, pp 3490–3499\n30. He K, Gkioxari G, Dollar P , Girshick R (2017) Mask R-CNN. In:\n2017 IEEE International Conference on Computer Vision (ICCV).\nIEEE, V enice, pp 2980–2988\n31. Gong Y , Xiao Z, Tan X et al (2020) Context-Aware Convolutional\nNeural Network for Object Detection in VHR Remote Sensing\nImagery. IEEE Trans Geosci Remote Sens 58:34–44. https://doi.\norg/10.1109/TGRS.2019.2930246\n32. Kim S-W, Kook H-K, Sun J-Y et al (2018) Parallel Feature Pyramid\nNetwork for Object Detection. In: Ferrari V , Hebert M, Sminchis-\nescu C, Weiss Y (eds) Computer Vision – ECCV 2018. Springer\nInternational Publishing, Cham, pp 239–256\n33. Liu W, Anguelov D, Erhan D, et al (2016) SSD: Single Shot Multi-\nBox Detector. pp 21–37\n34. Deng L, Yang M, Li T, et al (2019) RFBNet: Deep Multimodal\nNetworks with Residual Fusion Blocks for RGB-D Semantic Seg-\nmentation. In: arXiv preprint arXiv:1907.00135\n35. Liang T, Chu X, Liu Y et al (2022) CBNet: A Composite Backbone\nNetwork Architecture for Object Detection. IEEE Trans Image Pro-\ncess 31:6893–6906. https://doi.org/10.1109/TIP .2022.3216771\n36. Law H, Deng J CornerNet: Detecting Objects as Paired Keypoints.\nIn: arXiv preprint arXiv:1808.01244\n37. Liu S, Qi L, Qin H, et al Path Aggregation Network for Instance\nSegmentation. In: arXiv preprint arXiv:1803.01534\n38. Peng Z, Huang W, Gu S, et al (2021) Conformer: Local Features\nCoupling Global Representations for Visual Recognition. In: 2021\nIEEE/CVF International Conference on Computer Vision (ICCV).\nIEEE, Montreal, QC, Canada, pp 357–366\n39. Guo J, Han K, Wu H, et al (2022) CMT: Convolutional Neural Net-\nworks Meet Vision Transformers. In: 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR). IEEE, New\nOrleans, LA, USA, pp 12165–12175\n40. Li Y , Yao T, Pan Y , Mei T (2023) Contextual Transformer Net-\nworks for Visual Recognition. IEEE Trans Pattern Anal Mach Intell\n45:1489–1500. https://doi.org/10.1109/TPAMI.2022.3164083\n41. Li Y , Mao H, Girshick R, He K (2022) Exploring Plain Vision\nTransformer Backbones for Object Detection. In: Avidan S, Bros-\ntow G, Cissé M, et al (eds) Computer Vision – ECCV 2022.\nSpringer Nature Switzerland, Cham, pp 280–296\n42. Lin W, Wu Z, Chen J, et al Scale-Aware Modulation Meet Trans-\nformer. In: arXiv preprint arXiv:2307.08579\n43. Fan Q, Huang H, Guan J, He R (2023) Rethinking Local Perception\nin Lightweight Vision Transformer. In: arXiv preprint arXiv:2303.\n17803\n44. Jiang B, Luo R, Mao J et al (2018) Acquisition of Localization\nConﬁdence for Accurate Object Detection. In: Ferrari V , Hebert M,\nSminchisescu C, Weiss Y (eds) Computer Vision – ECCV 2018.\nSpringer International Publishing, Cham, pp 816–832\n45. Wu Y , Chen Y , Y uan L, et al (2020) Rethinking Classiﬁcation and\nLocalization for Object Detection. In: 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR). IEEE,\nSeattle, W A, USA, pp 10183–10192\n46. Song G, Liu Y , Wang X (2020) Revisiting the Sibling Head in\nObject Detector. In: 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). IEEE, Seattle, W A, USA,\npp 11560–11569\n47. Ge Z, Liu S, Wang F, et al (2021) YOLOX: Exceeding YOLO\nSeries in 2021. In: arXiv preprint arXiv:2107.08430\n48. Zhao Z, He C, Zhao G et al (2023) RA-YOLOX: Re-\nparameterization align decoupled head and novel label assignment\nscheme based on YOLOX. Pattern Recognit 140:109579. https://\ndoi.org/10.1016/j.patcog.2023.109579\n49. Qin J, Huang Y , Wen W (2020) Multi-scale feature fusion resid-\nual network for Single Image Super-Resolution. Neurocomputing\n379:334–342. https://doi.org/10.1016/j.neucom.2019.10.076\n50. Ma W, Wu Y , Cen F, Wang G (2020) MDFN: Multi-scale deep\nfeature learning network for object detection. Pattern Recognit\n100:107149. https://doi.org/10.1016/j.patcog.2019.107149\n51. Li Y , Chen Y , Wang N, Zhang Z-X (2019) Scale-Aware Trident\nNetworks for Object Detection. In: 2019 IEEE/CVF International\nConference on Computer Vision (ICCV). IEEE, Seoul, Korea\n(South), pp 6053–6062\n52. Li T, Wei Y , Liu M et al (2023) Reﬁned Division Features Based\non Transformer for Semantic Image Segmentation. Int J Intell Syst\n2023:1–15. https://doi.org/10.1155/2023/6358162\n53. Jang E, Gu S, Poole B. Categorical reparameterization with\ngumbel-softmax. In: arXiv preprint arXiv:1611.01144\n54. Xu B, Wang N, Chen T, et al. Empirical evaluation of rectiﬁed acti-\nvations in convolutional network. In: arXiv preprint arXiv:1505.\n00853\n55. Hou Q, Zhou D, Feng J (2021) Coordinate Attention for Efﬁcient\nMobile Network Design. In: 2021 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR). IEEE, Nashville,\nTN, USA, pp 13708–13717\n56. Cao Y , Xu J, Lin S, et al (2019) GCNet: Non-Local Net-\nworks Meet Squeeze-Excitation Networks and Beyond. In: 2019\nIEEE/CVF International Conference on Computer Vision Work-\nshop (ICCVW). IEEE, Seoul, Korea (South), pp 1971–1980\n57. Tanaka M (2020) Weighted sigmoid gate unit for an activation func-\ntion of deep neural network. Pattern Recognit Lett 135:354–359.\nhttps://doi.org/10.1016/j.patrec.2020.05.017\n58. Khan ZY , Niu Z (2021) CNN with depthwise separable convolu-\ntions and combined kernels for rating prediction. Expert Syst Appl\n170:114528. https://doi.org/10.1016/j.eswa.2020.114528\n59. Wei X, Zhang L, Zhang J et al (2024) Decoupled Sequential\nDetection Head for accurate acne detection. Knowl-Based Syst\n284:111305. https://doi.org/10.1016/j.knosys.2023.111305\n60. Everingham M, V an Gool L, Williams CKI et al (2010) The Pas-\ncal Visual Object Classes (VOC) Challenge. Int J Comput Vis\n88:303–338. https://doi.org/10.1007/s11263-009-0275-4\n123\n4920 Complex & Intelligent Systems (2024) 10:4897–4920\n61. Lin T-Y , Maire M, Belongie S et al (2014) Microsoft COCO: Com-\nmon Objects in Context. In: Fleet D, Pajdla T, Schiele B, Tuytelaars\nT (eds) Computer Vision – ECCV 2014. Springer International\nPublishing, Cham, pp 740–755\n62. Rezatoﬁghi H, Tsoi N, Gwak J, et al (2019) Generalized Inter-\nsection Over Union: A Metric and a Loss for Bounding Box\nRegression. In: 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR). IEEE, Long Beach, CA, USA,\npp 658–666\n63. Sun P , Zhang R, Jiang Y et al (2023) Sparse R-CNN: An End-to-End\nFramework for Object Detection. IEEE Trans Pattern Anal Mach\nIntell 45:15650–15664. https://doi.org/10.1109/TPAMI.2023.32\n92030\n64. Wang Y , Zhang X, Yang T, Sun J (2022) Anchor DETR: Query\nDesign for Transformer-Based Detector. Proc AAAI Conf Artif\nIntell 36:2567–2575. https://doi.org/10.1609/aaai.v36i3.20158\n65. Liu S, Li F, Zhang H, et al (2022) DAB-DETR: Dynamic Anchor\nBoxes are Better Queries for DETR. In: arXiv preprint arXiv:2201.\n12329\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7734630703926086
    },
    {
      "name": "Feature extraction",
      "score": 0.7376388311386108
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7131548523902893
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5894775390625
    },
    {
      "name": "Object detection",
      "score": 0.511134922504425
    },
    {
      "name": "Pixel",
      "score": 0.5073713660240173
    },
    {
      "name": "Transformer",
      "score": 0.48633795976638794
    },
    {
      "name": "Pascal (unit)",
      "score": 0.47683587670326233
    },
    {
      "name": "Computer vision",
      "score": 0.47576063871383667
    },
    {
      "name": "Detector",
      "score": 0.4493997097015381
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4465050995349884
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4115177392959595
    },
    {
      "name": "Engineering",
      "score": 0.11634978652000427
    },
    {
      "name": "Voltage",
      "score": 0.08409661054611206
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28006308",
      "name": "Shandong Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 13
}