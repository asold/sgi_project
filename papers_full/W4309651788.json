{
    "title": "Towards a foundation model for geospatial artificial intelligence (vision paper)",
    "url": "https://openalex.org/W4309651788",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2142693922",
            "name": "Gengchen Mai",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A3001269636",
            "name": "Chris Cundy",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2900957590",
            "name": "Kristy Choi",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2114691444",
            "name": "Yingjie Hu",
            "affiliations": [
                "University at Buffalo, State University of New York"
            ]
        },
        {
            "id": "https://openalex.org/A2145221253",
            "name": "Ni Lao",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A248646318",
            "name": "Stefano Ermon",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1974932599",
        "https://openalex.org/W2515960978",
        "https://openalex.org/W3159619744",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W3214340375",
        "https://openalex.org/W2104583100",
        "https://openalex.org/W3185276638",
        "https://openalex.org/W2979524927",
        "https://openalex.org/W3024097351",
        "https://openalex.org/W3104532573"
    ],
    "abstract": "Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet to see an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges for developing multimodal foundation models for GeoAI. We first show the advantages of this idea by testing the performance of existing Large pre-trained Language Models (LLMs) (e.g. GPT-2 and GPT-3) on two geospatial semantics tasks. Results indicate that these task-agnostic LLMs can outperform task-specific fully-supervised models on both tasks with 2--9% improvement in a few-shot learning setting. However, we also show the limitations of these existing foundation models given the multimodality nature of GeoAI, especially when dealing with geometries in conjunction with other modalities. So we discuss the possibility of a multimodal foundation model which can reason over various types of geospatial data through geospatial alignments. We conclude this paper by discussing the unique risks and challenges to develop such model for GeoAI.",
    "full_text": null
}