{
    "title": "Building Ensemble of Deep Networks: Convolutional Networks and Transformers",
    "url": "https://openalex.org/W4388430515",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2097021400",
            "name": "Loris Nanni",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A939433112",
            "name": "Andrea Loreggia",
            "affiliations": [
                "University of Brescia"
            ]
        },
        {
            "id": "https://openalex.org/A4209387654",
            "name": "Leonardo Barcellona",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A2089553447",
            "name": "Stefano Ghidoni",
            "affiliations": [
                "University of Padua"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4313415067",
        "https://openalex.org/W4364305464",
        "https://openalex.org/W4293253174",
        "https://openalex.org/W3092254810",
        "https://openalex.org/W4319988665",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2949449018",
        "https://openalex.org/W3165086995",
        "https://openalex.org/W3033157674",
        "https://openalex.org/W6796931752",
        "https://openalex.org/W2791568081",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2759912964",
        "https://openalex.org/W4296707792",
        "https://openalex.org/W3084521418",
        "https://openalex.org/W6681435938",
        "https://openalex.org/W3006943693",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4365135358",
        "https://openalex.org/W3010726750",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W4308104991",
        "https://openalex.org/W3212185757",
        "https://openalex.org/W4318566866",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6746052068",
        "https://openalex.org/W2984313420",
        "https://openalex.org/W3171728179",
        "https://openalex.org/W2758308612",
        "https://openalex.org/W4382313309",
        "https://openalex.org/W4375929559",
        "https://openalex.org/W3196347879",
        "https://openalex.org/W6795475546",
        "https://openalex.org/W6795676476",
        "https://openalex.org/W2984906359",
        "https://openalex.org/W4285124323",
        "https://openalex.org/W6874255383",
        "https://openalex.org/W2964054038",
        "https://openalex.org/W1530098540",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6767312599",
        "https://openalex.org/W6784591795",
        "https://openalex.org/W4404618747",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W3164284422",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1522301498"
    ],
    "abstract": "This paper presents a study on an automated system for image classification, which is based on the fusion of various deep learning methods. The study explores how to create an ensemble of different Convolutional Neural Network (CNN) models and transformer topologies that are fine-tuned on several datasets to leverage their diversity. The research question addressed in this work is whether different optimization algorithms can help in developing robust and efficient machine learning systems to be used in different domains for classification purposes. To do that, we introduce novel Adam variants. We employed these new approaches, coupled with several CNN topologies, for building an ensemble of classifiers that outperforms both other Adam-based methods and stochastic gradient descent. Additionally, the study combines the ensemble of CNNs with an ensemble of transformers based on different topologies, such as Deit, Vit, Swin, and Coat. To the best of our knowledge, this is the first work in which an in-depth study of a set of transformers and convolutional neural networks in a large set of small/medium-sized images is carried out. The experiments performed on several datasets demonstrate that the combination of such different models results in a substantial performance improvement in all tested problems. All resources are available at https://github.com/LorisNanni.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2022.DOI\nBuilding Ensemble of Deep Networks:\nConvolutional Networks and\nTransformers\nLORIS NANNI1, ANDREA LOREGGIA2, LEONARDO BARCELLONA1, STEFANO GHIDONI1\n1DEI, Department of Information Engineering, University of Padova, Italy (e-mail: loris.nanni@unipd.it)\n2DII, Department of Information Engineering, University of Brescia, Brescia, Italy (e-mail: andrea.loreggia@unibs.it)\nCorresponding author: Loris Nanni (e-mail: loris.nanni@unipd.it).\nABSTRACT This paper presents a study on an automated system for image classification, which is\nbased on the fusion of various deep learning methods. The study explores how to create an ensemble of\ndifferent Convolutional Neural Network (CNN) models and transformer topologies that are fine-tuned on\nseveral datasets to leverage their diversity. The research question addressed in this work is whether different\noptimization algorithms can help in developing robust and efficient machine learning systems to be used\nin different domains for classification purposes. To do that, we introduce new Adam variant approaches.\nWe employed these new approaches, coupled with several CNN topologies, for building an ensemble of\nclassifiers that outperform both other Adam-based methods and stochastic gradient descent. Additionally,\nthe study combines the ensemble of CNNs with an ensemble of transformers based on different topologies,\nsuch as Deit, Vit, Swin, and Coat. To the best of our knowledge, this is the first work in which an in-\ndepth study of a set of transformers and convolutional neural networks in a large set of small/medium-sized\nimages is carried out. The experiments conducted on several datasets demonstrate that the combination of\nsuch different models results in a substantial performance improvement in all tested problems. All resources\nare available at https://github.com/LorisNanni.\nINDEX TERMS Convolutional neural networks, transformers, optimization, ensemble.\nI. INTRODUCTION\nThe use of Convolutional Neural Networks (CNN) has revo-\nlutionized the field of image recognition, achieving impres-\nsive results in a wide range of applications [20]. Researchers\nhave explored various ways to improve the architecture of\nthese networks, including the combination of specialized\nlayers to form new topologies. However, as neural networks\nbecome deeper, they are susceptible to problems such as the\nvanishing gradient and difficulties with optimization. In ad-\ndition to enhancing the architecture of CNNs, finding robust\nand stable optimization algorithms is equally important to\nmaximize performance [16]. Traditional optimization meth-\nods such as Gradient Descent (GD) and Stochastic Gradient\nDescent (SGD) have been widely used, with Adam being the\nmore popular method due to its ability to improve the net-\nwork’s search for a solution. This has led to the development\nof many Adam-based variants. In this study, we propose new\nAdam-based variants and compare them to other optimiza-\ntion methods. We also explore the potential of transformers\nfor building ensembles and compare different transformer\ntopologies. Our experiments show that ensembles based on\nthe combination of different approaches outperform state-of-\nthe-art results in all tested problems. Despite the complexity\nof the proposed system, it requires minimal parameter tuning\nand works well in various problems without the need for\nspecific pre-processing or optimization for each dataset. All\nthe tested code and datasets are available for reproducibility.\nThe contributions of this article are emphasized as follows:\ni) We propose a set of new Adam-based variants useful\nduring the training phase of the models.\nii) We define ensembles of different CNN models, trained\nwith these new Adam-based variants, and different\ntransformer topologies. Compared with existing ma-\nchine learning methods, our ensembles provide compet-\nitive results in different domains.\niii) We provide an empirical evaluation of ensembles\ntrained with standard and new Adam-based variants.\nEvaluating the performance on several metrics and\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ndatasets, we demonstrate that adopting different opti-\nmization algorithms and different network models is\nbeneficial for ensembles.\niv) We introduce a testbed for evaluating Transformers\nensembles, leveraging publicly accessible datasets, and\nshowcasing a baseline ensemble. As a result, fellow re-\nsearchers can readily compare their ensembles with our\nreported baseline, under the condition that they employ\nthe same Transformers with identical hyperparameters.\nThe remainder of the paper is structured as follows:\nSection II reports related works on the topic analyzed in\nthis work. Section III introduces some basic notions about\ntransformers, CNNs, the Adam approaches proposed in this\nwork, as well as the datasets used for experiments. Section\nIV describes the experimental environments, the testing pro-\ntocols, and the performance indicators; moreover, we suggest\nand discuss a set of experiments to evaluate our ensembles.\nIn Section V , we discuss the value of our work providing\nmore insights about the usefulness of the proposed approach.\nSection VI concludes this work and provides some further\nperspective on this research.\nII. RELATED WORK\nA. TRANSFORMERS IN IMAGE CLASSIFICATION\nIn image classification, Convolutional Neural Networks\n(CNNs) have been state-of-the-art for many years, and mod-\nels such as ResNet [20] or EfficientNet [48] are still con-\nsidered baseline approaches in many works. However, the\nrecent introduction of transformers by Vaswani et al. [47]\nhas changed this trend. Transformer models have achieved\ndisruptive performance in the neural language processing\nfield, making it the leading approach [6], [12], [28]. The\nimprovement is attributed to the multi-head self-attention.\nAfter the encoding of words in tokens, the self-attention is\ncomputed as:\nAttention(Q, K, V) = Softmax\n\u0012QKT\n√Dk\n\u0013\nV, (1)\nwhere Q ∈ RNxDk is the query matrix, K ∈ RMxDk\nis key matrix and V ∈ RMxDv is the value matrix. Dk\nis the dimension of the keys. With N and M denoting the\nlengths of queries and keys. The multi-head self-attention\nuses H different heads computing the attention, as in Eq. 1,\nand concatenating the output.\nIn image classification, Dosovitskiy et al. [14] introduced\nthe Vision-Transformer (ViT) as the first pure transformer\nmodel to achieve results equivalent or superior to CNNs. ViT\nutilizes the encoder from Vaswani’s original work [47] and\nstacks a multilayer perceptron to classify features extracted\nby the encoder. In this approach, the input tokens are image\npatches encoded in a latent space. However, training the\nViT network is challenging due to its lack of inductive\nbiases compared to CNNs [14], [46]. Moreover, the network\nstruggles to aggregate local information. To address these\nchallenges, ViT requires a pre-training on a dataset with\nmillions of images, namely JFT-300M dataset.\nA solution to ViT data issues is proposed in [46]. Their\napproach involves distilling knowledge acquired from a con-\nvolutional neural network, combined with actual ground-\ntruth values, to enable transformers to learn the inductive bias\nfrom the distillation. Additionally, the authors introduced a\n“distillation token\" to enhance the distillation process and\nleading to better performance with simpler training. The\nmodel is named Data-efficient Transformer (DeiT).\nSwin [29], one of the top transformer models inspired by\nViT, utilizes a hierarchical window structure to address scale-\nvariation issues. The attention is computed within the local\nwindow, resulting in improved performance and execution\ntime. The partitioning is gradually shifted along the network\nhierarchy to maintain interaction among different windows.\nThe authors also demonstrated the effectiveness of Swin\nas a backbone in various vision tasks, including semantic\nsegmentation.\nTransformers have different characteristics compared to\nCNNs. According to [38], CNN models make decisions\nmainly based on texture, while ViT weights the shape more.\nTherefore, some authors have considered merging the two.\nAn example is CoAtNet [10], which merges depthwise con-\nvolution with self-attention by stacking them vertically. With\nthis approach, the model leverages CNNs inductive bias,\nTransformers capacity, and attention mechanism. CoAtNet\nmay be considered a hybrid approach since it adopts a con-\nvolutional structure while inserting self-attention from [47].\nB. ENSEMBLE OF IMAGE TRANSFORMERS\nVision transformers are gaining more attention for their abil-\nity to recognize long-range dependencies and their capacity\n[19], [27]. Despite the impressive results obtained by deep\nlearning models, their performance can be boosted through\nan ensemble of different models [26] or the same model with\ndifferent weights [31], [40]. Each model of the ensemble can\nlearn a representation that concentrates on particular aspects\nor representations of images, improving the final prediction,\nas demonstrated by the work of Savelli et al. [40] that\nimproved small lesion detection using an ensemble of CNNs\ntrained on different views of the same lesion. Kyathanahally\net al. [26] used an ensemble of six CNNs to improve plankton\nclassification by averaging the predictions of the components,\nfollowing the demonstration by d’Ascoli et al. [18] that an\naverage ensemble can reduce overfitting.\nDespite the impact that vision transformers have had on\nimage classification, only a few works have exploited their\nbenefits in an ensemble approach [25], [35], [41]. In [21]\nthe authors compared a vision transformer ensemble with a\nCNN ensemble for brain tumors. However, they found that\nthe ensemble of vision transformer models performed worse\nthan the convolutional ensemble. In [41] the authors proposed\na model that exploits both ViT and convolutions, named\nCvit, and created an ensemble of two models trained on the\nfrequency domain and time domain to classify movement\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfrom EMG signals. The authors compared the proposed\nsolution only with convolutional baselines and ViT. Recent\nwork from Kyathanahally et al. [25] showed astonishing\nresults in image classification among different datasets with\nan ensemble of three DeiT models. In [35] , authors achieve\nenhanced ensemble performance for polyp segmentation by\ncombining diverse transformer models (including HarDNet-\nMSEG, Polyp-PVT, and HSNet) through distinct training\ntechniques and introducing a novel mask averaging method,\nresulting in superior segmentation results across multiple\ndatasets. ETECADx [1] is an AI-based computer-aided di-\nagnosis (CAD) framework for early breast cancer detection,\nwhich combines convolutional neural networks and the self-\nattention mechanism of vision transformer encoder. Another\nstudy [2] introduces a masked face recognition system, com-\nbining two Convolutional Neural Network (CNN) models\nand two Transformer models, which achieve better accuracy\nwhen ensembled, outperforming other models in recognizing\nmasked faces.\nIn our work, we propose to combine different transformers\nand convolutional models, demonstrating their superiority\non various datasets. Each model contributes to the final\nprediction through the weighted sum rule. Thus, the vision\nTransformers and CNNs are trained separately and their\noutputs are merged together. Similarly to [2] and [25] ,\nwe find that Transformer models, exploiting the attention\nmechanism, boost the ensemble results when merged with\nCNNs.\nC. ADAM OPTIMIZERS\nA fundamental role in deep learning is played by optimiz-\ners. An optimizer in machine learning is a mathematical al-\ngorithm or technique that adjusts the internal parameters of\na model to minimize or maximize a specified objective func-\ntion, improving the model’s performance on a given task. One\nof the most used optimizers is Adam [23]. It is used for gradi-\nent-based optimization of stochastic objective functions. It is\na combination of two other optimization algorithms, namely,\nAdagrad [17] and RMSprop [23]. The key idea behind Adam\nis to adaptively estimate the first and second moments of the\ngradients in order to perform effective optimization.\nSome authors proposed variations of Adam, creating new\noptimizers. An example is DiffGrad [16], proposed in 2019.\nIt relies on the assumption that a decrease in the rate of gra-\ndient variations suggests the presence of a global minimum.\nThe objective is to generate substantial strides when the gra-\ndient is undergoing large changes while taking smaller steps\nwhen the gradient is changing more gradually. Overall, Dif-\nfGrad has shown promising results in experiments on vari-\nous image classification and object detection tasks, often out-\nperforming Adam and other popular optimization methods in\nterms of both speed and accuracy.\nIn [37], three more Adam variations were proposed,\nnamely DGrad, Cos and Exp. DGrad is a modified version\nof DiffGrad which considers the absolute difference between\nthe current and the moving average of the element-wise\nsquares of the parameter gradients. In this way, it is more\nrobust to fluctuations in the difference between the gradients.\nCos is a modification of DGrad that incorporates a learning\nrate that varies in a cyclical manner [43], leading to an im-\nprovement in classification accuracy and typically requiring\nfewer iterations.\nExp is a modification of DGrad that includes two simple\nelement-wise operations: product and exponential. The pur-\npose of Exp formulation is to mitigate the effect of large vari-\nations in the gradient but also to allow the function to con-\nverge for small values.\nBAS-ADAM [22] is an improved version of the Beetle\nAntennae search (BAS) algorithm, enhancing convergence\nbehavior and avoiding local-minima by adaptively adjusting\nstep-sizes using the ADAM update rule, resulting in faster\nconvergence and efficient optimization of non-convex func-\ntions compared to particle swarm optimization (PSO) and the\noriginal BAS algorithm.\nA recent Adam variation is AngularGrad [39] that exploits\nthe direction/angle of consecutive gradients to adjust the\nlearning rate. Thanks to angle direction, the optimization\nbecomes smoother while keeping a good trade-off between\nspeed and performance.\nIn this paper, three more Adam variations are proposed.\nTheir objective is to search the solution space differently and\nvary the prediction made by the models. Changing optimiza-\ntion creates models suitable for ensemble.\nIII. MATERIALS AND METHODS\nIn this section, we describe the different components of the\nproposed ensemble and we detail the new Adam variants\nproposed in this study.\nA. CONVOLUTIONAL NEURAL NETWORKS\nCNNs are a type of deep neural network that was specif-\nically designed for image classification, computer vision,\nand other applications, such as medical image analysis [9],\nface identification, and object recognition, among others.\nCNNs are designed to operate in a similar way to the human\nbrain by perceiving visual information [24]. Recently, the\ncombination of these models in ensembles has been proven\nto be beneficial in terms of performance (see for instance, [8],\n[36], [40]).\nConvolution involves sliding a small filter (also known as\na kernel) over the input data, which is typically a two-dimen-\nsional grid of pixels in the case of images. The element-wise\nmultiplication is performed between the values of the given\nfilter (learnable weights) and the corresponding values in the\ninput data. The resulting products are summed up to produce\na single value. This process is repeated by sliding the filter\nover the entire input, with a certain stride, to produce a new\noutput matrix called a feature map.\nThe convolution operation allows the neural network to de-\ntect patterns or features present in the input data. These pat-\nterns can be as simple as edges or corners in the case of im-\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nages. As the network learns through training, it adjusts the\nweights of the filters to capture increasingly complex and ab-\nstract features, such as textures, shapes, or object parts. Con-\nvolutional layers are typically stacked in CNN architectures,\nand each layer learns to recognize different levels of features.\nThis hierarchical feature extraction enables CNNs to under-\nstand the content of images in a way that is analogous to how\nthe human visual system processes information [7].\nIn our experiments, various models pre-trained on Ima-\ngeNet are tested and combined. The last layers of each model\nare modified to fit the number of classes of the target problem\nwithout freezing the weights of the previous layers. The mod-\nels evaluated include ResNet50 [20], which is about 8 times\ndeeper than VGGNet [20] and uses residual layers and global\naverage pooling layers instead of fully connected layers, and\nEfficientNetB0 [45], which is designed for mobile devices\nand uses a multi-objective network search that optimizes\naccuracy.\nB. VISION TRANSFORMERS\nIn our experiments, we utilized advanced transformer models\nfor image classification. Due to the significant amount of\ndata required for training transformers from scratch, we\nadopted models that are already trained on ImageNet and we\nperformed on them a fine-tuning. These pre-trained models\nwere obtained from the Timm library1, including DeiT-Base\nwith a patch dimension of 16, ViT-Base with a patch size of\n16, Swin-Base with a patch size of 4, and CoAtNet with a\ncontinuous log-coordinate relative position bias removed. It\nis worth noting that ViT implementation in Timm was pre-\ntrained on Imagenet-21k, whereas the others were not.\nFor each model, we adjusted the last layer to align the\noutput with the number of categories in each tested dataset.\nTo prevent overfitting, we retained a validation split with\na 0.25 split ratio from each training set and resized the\nimage dimension to 224 × 224 to match the required input\ndimension.\nC. TRAINING AND TEST PHASES\nDuring the training phase, each CNN is trained by adopting\nan optimization algorithm that is chosen at random among\nthe ones available. The training process includes 20 epochs\nwith a mini-batch size of 30 patterns and a learning rate of\n0.001. Data augmentation is applied by flipping and rescaling\nthe images, but only if the size of the training set is less than\n5000 images. If the size of the training set is greater than or\nequal to 5000, no data augmentation is performed.\nInstead, transformers are trained following the pipeline\nadopted for the DeiT model in [25], replicating the procedure\nfor all four models. Specifically, we trained the models using\nthe AdamW optimizer with cosine annealing [30], and set a\nlow learning rate of 10e − 4 coupled with a weight decay of\n0.03 to preserve the learned network. We set the batch size to\n32.\n1https://timm.fast.ai/ - Last access, June 28 2023\nTABLE 1: Description of the datasets used in this study.\nShort Name #Classes #Samples Image Size Protocol Ref\nWHOI 22 6600 greyscale 50:50 [44]\nZooScan 20 3771 greyscale 2CV [44]\nKaggle 38 14374 greyscale 5CV [44]\nZoolake 35 17943 RGB 85:15 [26]\nBG 3 300 RGB 5CV [13]\nLAR 4 1320 RGB 3CV [34]\nDeng 10 563 RGB 35:65 [11]\nVIR 15 1500 greyscale 10CV [37]\nTEM 14 1245 RGB 80:20 [33]\nDuring the test phase, the outputs of all the models are\ncombined to compute the overall prediction. Thus, each\nmodel of the ensemble contributes to the final prediction.\nThe output of the models is not automatically mapped into a\nprobability distribution. Thus, we applied a softmax function\nto the score of the last layer before creating the ensemble\nprediction. As a loss function, we used the standard cross-\nentropy, as opposed to [25] that used the weighted cross-\nentropy. If the minimum validation F1 score did not decrease\nafter five consecutive epochs, we decreased the learning rate\nto 10e − 5 and 10e − 6 to improve convergence. The best-\nperforming model with the highest F1-score in validation\nduring the training was saved. During training, data were\nrandomly flipped or rotated. To create the ensemble, we\nrepeated the training procedure ten times for each model and\neach dataset.\nIn Figure 1, we depict the training phase and the test phase.\nIn the literature, there are no testbeds for ensembles of\ntransformer models. In this work, we propose a testbed for\nensembles by using four different topologies in seven pub-\nlicly available datasets. Overall, the experiments are easily\nreplicable.\nD. DATASETS\nWe assess the proposed ensembles by adopting several image\nclassification benchmarks. Table 1 reports some information\nfor each dataset: a short name, the number of classes and\nsamples, the testing protocol, and the original reference. For\nthe testing protocols, we adopt the following abbreviations:\n• xCV indicates that an x-fold cross-validation has\nbeen adopted (e.g., 10CV means that a 10-fold cross-\nvalidation has been used);\n• X : Z indicates the fractions of the dataset used for\ntraining and testing respectively.\nThe adopted datasets contain information from very differ-\nent domains, in particular:\n• WHOI consists of images in tiff format, taken from\nWoods Hole Harbor water using Imaging FlowCytobot;\n• ZooScan is composed of grayscale images that were\nacquired from the Bay of Villefranche-sur-mer using\nthe Zooscan technology. The images were automatically\ncropped before classification to remove any artifacts\ncaused by manual segmentation.\n• Kaggle dataset is a subset of a larger dataset obtained\nusing the ISIIS technology in the Straits of Florida and\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1: Example of an ensemble. Each CNN model is trained using an optimization algorithm that is randomly chosen a\npriori. During the test phase, the predictions are combined to compute the outcome of the ensemble.\nwas used for the National Data Science Bowl 2015 com-\npetition.\n• Zoolake is a dataset of microscopic plankton taxa\nimages collected from the dual-magnification Scripps\nPlankton Camera in Lake Greifensee. The dataset was\nbuilt using images acquired from wild plankton from\n2018 to 2020.\n• BG (Breast Grading Carcinoma) dataset contains im-\nages of size 1280x960 pixels and is divided into three\nclasses representing grades 1-3 of invasive ductal carci-\nnoma of the breast.\n• LAR (Laryngeal dataset) contains patch images of size\n100 × 100 pixels, divided equally into four classes:\nIPCL (tissue with intrapapillary capillary loops), Le\n(tissue with leukoplakia), Hbv (tissue with hypertrophic\nvessels), and He (healthy tissue).\n• Deng contains images of pests commonly found on\nplants between Europe and Central Asia. It was created\nby collecting images from several online sources such\nas Insert Images, IPM images, Dave’s Garden, and\nMendeley Data.\n• VIR comprises a total of 1500 Transmission Electron\nMicroscopy images, each with a size of 41x41 pixels,\nof various virus types classified into fifteen distinct cat-\negories.\n• TEM (Transmission Electron Microscopy) contains an-\nnotated transmission electron microscopy images (size\nof 1376x1032 or 2048x2048 pixels, depending on with\nwhich electron microscope they were captured) of 14\nvirus classes along with extracted image patches cen-\ntered on virus particles.\nE. NEW OPTIMIZATION METHODS\nIn this section, we introduce three new optimization meth-\nods. To better understand the proposed optimizers, it is worth\nrecalling the Adam optimization algorithm and DGrad [37].\nThe update rule for Adam can be written as follows:\nmt = β1 × mt−1 + (1 − β1) × gt (2)\nvt = β2 × vt−1 + (1 − β2) × g2\nt (3)\nmˆt = mt/(1 − βt\n1) (4)\nvˆt = vt/(1 − βt\n2) (5)\nθt = θt−1 − α × mˆt\n(√vˆt + ϵ) (6)\nβ1 = 0.9; β2 = 0.999; α = 0.001\nwhere mt is the first moment (mean) of the gradients up to\ntime step t; vt is the second moment (uncentered variance)\nof the gradients up to time step t; gt is the gradient at time\nstep t; β1 and β2 are the decay rates for the first and sec-\nond moments, respectively; α is the learning rate; ϵ is a small\nconstant added for numerical stability; mˆt and vˆt are bias-\ncorrected estimates of the first and second moments, respec-\ntively; θt is the current set of parameters at time step t.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDGrad [37] is a variation of Adam inspired by DiffGrad\n[16]. It considers the absolute difference between the current\nand the moving average of the element-wise squares of the\ngradients (avt), see Eq. (7-9). In this way, it is more robust to\nfluctuations in the difference between the gradients. For up-\ndating the parameters, Eq. (10) is applied using the definition,\nfor the weighting factor, reported in Eq. (9).\n∆agt = |gt − avt| (7)\n∆cagt =\n\u0012 ∆agt\nmax (∆agt)\n\u0013\n(8)\nξt = Sig(4 · ∆cagt) (9)\nθt = θt−1 − ξt\nα × mˆt\n(√vˆt + ϵ), (10)\na: Hyperbolic optimizer (Hyp)\nThe first proposed optimizer is Hyperbolic (Hyp). It has a\nsimilar behavior of Exp [37], but does not mitigate the effects\nof large variations in gradient. In fact, while the function de-\nscribed in Exp gets low values for great gradient differences,\nthis approach simply calculates parameters as follows:\nlrt = −1\n(a∆agt + b) + c (11)\nWith a = 10 , b = 2 /3, and c = 3 /2 we obtain the plot\nreported in Figure 2. The calculation of the final weighting\nfactor of the learning rate is:\nξt = lrt\nmax(lrt) (12)\nthen, Eq.(12) is applied in Eq.(10).\nb: MinD optimizer\nThe second novel optimizer is called MinD and exploits two\nof the best optimizers in different ways. It calculates at ev-\nery iteration ξt1 using DGrad and ξt2 using Exp. Then, the\neffective ξt is chosen by applying:\nξt = min(ξt1, ξt2) (13)\nthen, Eq.(13) is applied in Eq.(10).\nc: Angular Injection optimizer (AI)\nAngular Injection (AI) optimizer is based on AngularGrad\n[39] and injection [15]. It generates a score to control the step\nsize based on the gradient angular information of previous\niterations. AI takes into account the information from the\nangle/direction of the gradient vector instead of just the\nmagnitude of it. To exploit the change of gradients during\noptimization steps an angular coefficient was introduced (Eq.\n(14)). Given the following definitions\nAt = tan−1 (gt − gt−1)\n(1 + gtgt−1) (14)\nAmin = min(At, At−1) (15)\nlrt = −1\n(aAmin + b) + c (16)\nWhere a = 10, b = 2/3, and c = 3/2.\nThe weighting factor is:\nξt = lrt\nmax(lrt) (17)\nthen, Eq.(17) is applied in Eq.(23).\nIn order to utilize the curvature information during op-\ntimization, the curvature information guided (weighted)\nsecond-order momentum is injected into first-order momen-\ntum:\ngs = g2\nt (18)\nk = 2 (19)\navgt = β1 × avgt−1 + (1 − β1)(gt − delta × gs)\nk (20)\ndelta = θt−2 − θt−1 (21)\navgsqt = β2 × avgsqt−1 + (1 − β2) × gs (22)\nstep = ξt × ( avgt\n√avgsqt + ϵ) (23)\nθt = θt−1 − step · α ×\np\n(1 − βt\n2)\n(1 − βt\n1) (24)\ndelta represents the difference in the short-term of the\nparameters, the weighting factor ξt in Eq. (23) is the one\ndefined in Eq. (17), β1 and β2 are initialized as in Adam.\nIn this approach:\n• in the first iteration, we use standard Adam;\n• in the second we fix ξt=1 in Eq. (23), we use avgt and\navgsqt calculated using gradients obtained by standard\nAdam in the first iteration (i.e. using eq. (2) and (3));\n• from the third iteration as in the method explained\nabove.\nF. WILCOXON SIGNED-RANK TEST\nThe Wilcoxon signed-rank test, introduced by Mann and\nWhitney in 1947 [32], is a statistical test designed for com-\nparing paired data samples obtained from individual evalua-\ntions. Unlike parametric tests, this non-parametric test does\nnot rely on assumptions about the underlying distribution of\nthe data, such as a normal distribution. Instead, the Wilcoxon\nsigned-rank test takes into account both the magnitudes and\nsigns of the differences between paired observations.\nThis test serves as a non-parametric counterpart to the\npaired Student’s t-test and is particularly useful when the\npopulation data does not follow a normal distribution. Its pri-\nmary purpose is to assess whether two related paired samples\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2: Plot of lrt for a = 10, b = 2/3, and c = 3/2. On y-axis, the lrt value. On the x-axis, the ∆agt value.\nare drawn from the same distribution. By analyzing the ranks\nassigned to the differences between the paired observations,\nthe Wilcoxon signed-rank test provides a reliable means of\nevaluating the null hypothesis.\nIV. EXPERIMENTAL ANALYSIS\nIn this section, we report the results of the experimental\nevaluation, considering different performance indicators for\ncomparing the approaches. Moreover, validation of the supe-\nriority (using p-value) of one method over the others is pro-\nvided by the Wilcoxon signed rank test. All the experiments\nwere taken on a Windows Server 2019, with an Intel Core\ni9-10920X CPU, 3.5 GHz, and 256 GB RAM, we employed\nan Nvidia Titan RTX 24 GB, 1350 MHz. They are developed\nin Matlab 2022a/PyTorch.\nSince CNNs require input images at a fixed size, we apply\n2 different strategies for resizing plankton images: sqr, in\nwhich the process of square resizing involves first padding\nthe image to achieve a square dimension and subsequently\nresizing it to match the CNN input size. Conversely, when\nemploying padding alone (pad), the image is directly adjusted\nto match the CNN input size, without the intermediate square\nresizing step. However, it’s important to note that the square\nresizing step becomes necessary only in specific scenarios\nwhere the original image dimensions exceed the dimensions\nof the CNN input size, prompting the image to undergo resiz-\ning. Padding is performed by adding white pixels to plankton\nimages. Since we propose ensembles, half of the nets (in each\nensemble) use sqr and the other half pad.\nIn all the experiments, CNNs and transformer networks\nwere trained using the standard Cross-Entropy loss, see Fig-\nure 3 for examples of loss convergence. In what follows, we\nadopt the following abbreviations:\n• A+B represents the fusion by sum rule. It combines net-\nwork set predictions by simply adding them up. Both\nmodels (A and B) are trained seven times and combined\nby the sum rule;\n• SGD corresponds to the stochastic gradient descent. The\noutcome is the fusion of fourteen stand-alone CNNs\nby the average rule. CNNs are trained with the given\noptimization method. In this way, the result obtained\nby SGD is fairly comparable with those obtained by\nHyp+MinD (ensemble both of size fourteen).\nFirst of all, we analyzed the behavior of the loss function\nduring the training phase adopting different optimization al-\ngorithms. As can be noticed in Figure 3, the charts clearly\nshow that Hyp and AI training are more homogeneous and\nconverge much faster.\nA. ENSEMBLES OF CONVOLUTIONAL NEURAL\nNETWORKS\nWe first run a set of experiments to get the performance of\nthe ensemble when using different Adam variants during the\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(a)\n (b)\n (c)\nFIGURE 3: Loss function adopting different methods on the Deng dataset for 20 epochs using ResNet50. (a) Adam, (b) Hyp,\nand (c) AI.\ntraining phase. This was also useful for a comparison with the\noriginal methods that are reported as baselines. The results\nare reported in Table 2 . In particular, we compare several\nAdam variants with the original Adam and SGD; for stand-\nalone Adam variant approaches, there are two values in each\ncell of the table:\n• Average accuracy among seven stand-alone CNNs\ntrained with the given optimization method;\n• Fusion by average rule among seven stand-alone CNNs\ntrained with the given optimization method.\nFrom the results in Table 2 it can be noticed that the\nperformance of the ensembles with the fusion by average\nrule is always better than the average accuracy of the stand-\nalone networks, providing another piece of evidence that the\nadoption of the ensemble is beneficial for the performance\nof the system. In Table 3 , we report the performance on\nthe plankton datasets of the most interesting Adam variants.\nThe Adam variants also reported in Table 3 outperform\nthose not reported by a p-value of 0.001. However, all the\nAdam variants reported in Table 3 have similar performance.\nMoreover,in Tables 2 and 3it can be noticed that Hyp+MinD\noutperforms both ensembles based on Adam and SGD with a\np-value of 0.0001. Hyp+MinD+AI outperforms Hyp+MinD\nwith a p-value of 0.005.\nB. ENSEMBLES OF TRANSFORMERS\nThe second set of experiments focuses on the employment of\ntransformers, this was useful to get the performance of the\nensembles and to compare them with the stand-alone origi-\nnal methods. We also tested transformer models trained with\nAdam variants, but we noticed no particular gains, therefore,\nfor the sake of space, we have not reported them.\nIn Tables 4 and 5 , we report the results obtained by the\ntransformers. The following methods are reported in the\ntable, where x represents the number of combined models:\n• D(x), sum rule among x Deit;\n• S(x), sum rule among x Swin;\n• V(x), sum rule among x Vit;\n• C(x), sum rule among x Coat;\n• CNNs is the ensemble of Hyp+MinD+AI coupled with\nboth ResNet50 and EffNetB0;\n• (D+V+S+C)(x), sum rule among x Deit, x Swin, x Vit\nand x Coat;\n• CNNs+(D+V+S+C)(x), sum rule among CNNs, x Deit,\nx Swin, x Vit, and x Coat, before the fusion, the scores\nof each topology are normalized by the number of\nnetworks in the given ensemble, so the weight of CNNs\nis equal to that of C, S, V , and T.\nInterestingly, there is no winner among stand-alone trans-\nformers and among the ensemble of transformers. However,\nit is interesting to notice once again that each set of 10\ntransformers outperforms its stand-alone transformer with a\np-value of 0.002 (e.g., D(10) outperforms D(1) with a p-value\nof 0.002). On the other side, combining different transformer\ntopologies does not seem useful as in the CNN case, for\ninstance: (D+V+S+C)(2) behaves similarly to V(10) (sets of\nsimilar size); (D+V+S+C)(10) outperforms (D+V+S+C)(2)\nwith a p-value of 0.002, however, this comparison is not fair\ngiven the different sizes of the two ensembles. Moreover,\n(D+V+S+C)(10) outperforms CNNs with a p-value of 0.002.\nC. ENSEMBLES OF CNNS AND TRANSFORMERS\nIn the last rows of Tables 4 and 5 , we report the results\nobtained by ensembles of CNNs and transformers. The small\ngain from ensembles of different topologies is probably due\nto the saturation of the models’ performance on datasets.\nIn Table 6, we compare the best-performing approaches,\nconsidering the error under the ROC curve as a performance\nindicator. The fusion between CNNs and transformers out-\nperforms both CNNs and transformers with a p-value of 0.01.\nWe tried to increase the size of the ensemble named\n\"CNNs\" by also using DenseNet201 and MobileNetV2\ntopologies. The results are reported in Tables 7 and 8. For\nthe sake of computational time, we have run this test only\non ZooLake and Deng datasets. It is interesting to notice\nthat while the use of these CNN topologies improves the\nperformance of the CNNs ensemble, the performance of the\nfusion of CNNs with the transformer ensemble remains sim-\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 2: Accuracy (in %) of ResNet50 and EffNetB0.\nModel Method BG LAR Deng VIR TEM\nResNet50\nAdam 86.57 89.67 92.15 96.29 74.02 83.37 76.85 86.40 85.47 92.00\nDiffGrad 89.00 91.67 93.01 95.91 81.15 90.55 80.08 89.47 84.11 91.84\nDGrad 89.29 92.67 91.07 94.85 81.36 90.61 78.13 90.00 84.98 91.68\nCos 88.38 92.67 92.19 95.38 79.28 89.28 81.40 88.00 89.05 94.79\nExp 92.00 94.67 94.37 96.67 92.12 94.48 82.07 88.53 89.37 95.42\nHyp 91.95 94.00 94.22 96.67 93.18 94.59 85.19 90.27 91.80 95.63\nMinD 92.00 94.33 93.58 97.20 91.30 94.20 85.37 90.60 90.35 94.16\nAI 92.81 94.67 95.53 96.89 93.38 94.48 83.58 89.33 93.12 95.84\nHyp+MinD 94.33 96.44 94.42 91.73 95.47\nHyp+MinD+AI 94.67 97.12 94.59 91.33 95.53\nSGD 94.33 95.76 94.20 90.04 95.05\nEffNetB0\nAdam 90.62 93.33 94.17 96.59 86.58 93.20 84.35 91.13 90.01 94.47\nDiffGrad 92.43 94.33 94.59 96.29 87.55 93.20 84.53 90.80 90.26 94.79\nDGrad 92.62 94.67 94.34 96.44 86.49 92.93 84.23 91.53 91.04 95.00\nCos 92.33 94.67 94.66 96.29 87.47 93.31 82.80 89.00 93.89 94.84\nExp 94.48 95.33 94.72 96.36 93.36 94.20 85.08 89.27 93.58 95.16\nHyp 94.10 94.33 93.80 95.91 93.27 93.59 87.39 91.27 93.77 95.79\nMinD 94.19 95.33 94.02 95.98 93.39 94.25 87.20 91.13 94.08 95.89\nAI 94.10 95.00 94.02 95.23 92.47 93.26 85.08 89.87 94.63 96.11\nHyp+MinD 94.67 95.83 93.76 91.47 96.11\nHyp+MinD+AI 95.00 96.14 93.70 91.40 96.37\nSGD 93.00 93.11 92.71 88.53 94.95\nTABLE 3: Accuracy (in %) obtained by ResNet50 and EfficientNetB0 on plankton datasets.\nMethod ResNet50 EffNetB0\nWHOI Kaggle ZooScan ZooLake WHOI Kaggle ZooScan ZooLake\nExp 94.97 94.00 88.15 96.9 95.66 93.93 88.47 96.45\nHyp 95.27 93.90 88.71 96.93 95.48 93.72 88.02 96.27\nMinD 95.27 93.92 88.10 96.49 95.76 94.04 88.87 96.56\nAI 95.60 94.12 88.07 96.64 95.33 93.56 87.67 96.60\nHyp+MinD 95.42 94.00 88.49 96.71 95.79 93.99 88.60 96.42\nHyp+MinD+AI 95.33 94.20 88.49 96.93 95.79 94.01 88.87 96.56\nSGD 95.36 92.99 87.04 96.45 94.66 91.45 85.15 94.46\nilar. This suggests a possible plateau of performance reached\nwith these approaches.\nD. COMPARISON WITH SOTA AND ELAPSED TIME\nLast, we compare our approach with state-of-the-art (SOTA)\nreported in the literature, our proposed ensemble obtains new\nSOTA in many tested datasets. The datasets used were tested\nin hundreds of articles, so to avoid a huge table we reported\nonly a few articles in which SOTAs were obtained in those\ndatasets. In addition, we only reported articles with the same\ntest protocol we used, while we found other articles where the\nresults were better but with different protocols, and therefore\nthe comparison is not correct. Table 9 reports these results,\nnotice that, for [42] we report the results obtained after 100\nepochs to be coherent with our test protocol.\nIt is clear that our approach is not suitable for problems\nwith strong computational constraints; we trained a large set\nof networks and achieved very good performance, however,\nwe did not set any hyperparameters to optimize performance\nin a dataset, so there is no risk of overfitting. Nevertheless,\nwith current GPUs, even ensembles of >10 networks can\nclassify dozens of images per second. In Table 10 , we report\nthe inference time. It is clear that inference time increases\nwith vision transformers, but considering that these are able\nto analyze hundreds of images in a second, this is a reason-\nable time for many applications.\nV. DISCUSSION ON ADAM VARIANTS\nIn this work, we have developed several optimization meth-\nods that address the problem of finding a good minimum\nin different ways, so it is useful to combine them in an\nensemble. The proposed approaches consider the absolute\ndifference between the current and the moving average of the\nsquares of the parameter gradients. In this way, it is more\nrobust to fluctuations of the difference between the gradients\nof different iterations with respect to Adam and DiffGrad.\nAs detailed in the literature (e.g. [15]), an ideal parameter\noptimization method should follow the rules depicted in\nFigure 4 , where Θ is the parameters tensor, delta is the\ndifference of the parameters between two training iterations\n(see Eq. 21), g are the gradients. In the flat region (S1), an\nideal optimizer should perform a large step in order to escape\nfrom the flat area. In the so-called “large gradient - small\ncurvature” area (S2), for faster convergence, it is important to\nperform large step size. In the “steep and narrow valley”(S3),\nthere is a minimum. In this area both gradients and delta are\nlarge, a small step size is required for finding the minimum\nand reducing the oscillations.\nTaking this into consideration, the three proposed optimiz-\ners have the following properties:\n• Hyp optimizer is based on DGrad/DiffGrad, so the idea\nis that gradient is changing more gradually near the\nminimum, see S3 area when approaching the minimum.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4: Accuracy (in %) and F1 obtained using transformers.\nModels BG LAR ZooLake Deng VIR TEM\nAcc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1\nD(1) 93.3 0.931 97.6 0.976 96.6 0.863 93.1 0.941 90.2 0.895 95.8 0.948\nS(1) 94.0 0.935 96.9 0.969 96.4 0.871 92.1 0.931 91.4 0.912 95.1 0.953\nV(1) 95.3 0.950 97.6 0.976 96.4 0.861 94.5 0.952 90.8 0.906 95.5 0.939\nC(1) 92.6 0.924 94.8 0.947 96.7 0.876 92.8 0.937 90.1 0.900 95.8 0.954\nD(10) 96.7 0.964 97.6 0.976 96.9 0.871 94.7 0.954 91.5 0.912 96.5 0.956\nS(10) 96.0 0.957 97.9 0.979 96.9 0.878 95.7 0.963 92.0 0.918 95.8 0.957\nV(10) 95.3 0.950 97.6 0.976 97.1 0.880 96.9 0.973 92.9 0.926 96.4 0.957\nC(10) 96.3 0.960 96.9 0.969 97.1 0.884 94.0 0.950 91.5 0.913 95.7 0.952\n(D+V)(5) 95.7 0.954 98.0 0.980 97.2 0.884 94.6 0.954 92.3 0.921 96.7 0.962\n(D+V)(10) 96.0 0.957 98.2 0.982 97.1 0.884 95.5 0.962 92.2 0.919 96.7 0.962\n(D+V+S)(3) 95.0 0.947 98.3 0.983 97.3 0.888 95.9 0.965 92.5 0.923 96.4 0.960\n(D+V+S+C)(2) 95.7 0.954 98.3 0.983 97.2 0.887 95.6 0.936 92.6 0.924 96.6 0.964\n(D+V+S+C)(10) 96.0 0.957 97.9 0.979 97.3 0.891 96.2 0.968 92.7 0.925 96.2 0.962\nCNNs 94.7 0.943 96.4 0.964 96.9 0.861 94.6 0.954 91.9 0.919 96.3 0.965\nCNNs + (D+V+S+C)(2) 95.0 0.946 98.3 0.983 97.2 0.888 95.9 0.965 93.0 0.929 96.7 0.964\nCNNs + (D+V+S+C)(10) 95.3 0.950 98.0 0.980 97.3 0.889 96.4 0.969 93.1 0.929 96.7 0.965\nTABLE 5: Accuracy (in %) and F1 obtained using transformers.\nModels WHOI Kaggle ZooScan\nAcc. F1 Acc. F1 Acc. F1\nD(1) 94.5 0.945 93.6 0.920 87.3 0.889\nS(1) 95.6 0.956 94.0 0.926 87.4 0.897\nV(1) 93.6 0.936 93.6 0.920 87.8 0.892\nC(1) 95.7 0.957 93.8 0.922 87.3 0.892\nD(10) 95.7 0.957 94.4 0.931 88.9 0.903\nS(10) 95.9 0.959 94.7 0.935 89.5 0.915\nV(10) 95.8 0.958 94.4 0.931 88.9 0.903\nC(10) 96.3 0.963 94.7 0.936 89.1 0.910\n(D+V)(5) 95.7 0.957 94.5 0.932 89.4 0.909\n(D+V)(10) 95.8 0.958 94.6 0.933 89.5 0.909\n(D+V+S)(3) 95.9 0.959 94.6 0.934 90.0 0.916\n(D+V+S+C)(2) 96.3 0.963 94.7 0.936 89.8 0.915\n(D+V+S+C)(10) 96.3 0.963 94.8 0.938 90.1 0.918\nCNNs 95.9 0.959 94.5 0.932 89.5 0.911\nCNNs + (D+V+S+C)(2) 96.4 0.964 94.8 0.937 90.0 0.917\nCNNs + (D+V+S+C)(10) 96.4 0.964 94.9 0.939 90.3 0.918\nTABLE 6: Error under the ROC curve (in %)\nEUC WHOI Kaggle ZooScan BG LAR ZooLake Deng VIR TEM\n(D+V+S+C)(10) 0.226 0.417 0.917 0.192 0.033 0.320 0.191 0.965 0.241\nCNNs 0.285 0.400 0.928 2.102 0.095 0.355 0.515 0.666 0.260\nCNNs+(D+V+S+C)(10) 0.210 0.367 0.796 0.227 0.030 0.272 0.180 0.532 0.228\nTABLE 7: Accuracy (in %) using more CNN topologies\nAccuracy ZooLake Deng\n(D+V+S+C)(10) 97.3 96.2\nCNNs 97.0 95.2\nCNNs+(D+V+S+C)(10) 97.4 96.4\nTABLE 8: Error under the ROC curve (in %), using more CNN topologies\nEUC ZooLake Deng\n(D+V+S+C)(10) 0.320 0.191\nCNNs 0.350 0.469\nCNNs+(D+V+S+C)(10) 0.272 0.178\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 9: Comparison vs. state-of-the-art methods (Accuracy in %)\nDataset WHOI ZooScan Kaggle ZooLake BG LAR Deng VIR TEM\nHere 96.4 90.3 94.9 97.3 95.3 98.0 96.4 93.1 96.7\n[26] 96.1 89.8 94.7 97.7\n[31] 95.8 88.8 94.2\n[5] 94.3 96.8 95.0\n[11] 85.5\n[4] 95.2\n[37] 95.0 97.0 95.6 92.5\n[13] 96.3\n[42] 96.4\n[3] 96.1\n[33] 93.1\nTABLE 10: Inference time (seconds) for a batch of 100 images using a NVidia Titan RTX\nModel Images Batch Inference\nResnet 50 100 0.22 s\nVit 100 0.43 s\nDeiT 100 0.43 s\nSwin 100 0.40 s\nCoAtNet 100 0.46 s\n𝑔is small, 𝑑𝑒𝑙𝑡𝑎is smallLarge step size required\n𝑔is large, 𝑑𝑒𝑙𝑡𝑎is smallLarge step size required\n𝑔is large, 𝑑𝑒𝑙𝑡𝑎is largeSmall step size required\nS2 S1\nS3\nFIGURE 4: A common situation in optimization that illus-\ntrates the significance of adaptive parameter updates in the\noptimization process [49].\nDGrad/DiffGrad are based on a sigmoid function that\nsquashes every value between 0.5 and 1. Instead, Hyp is\nbased on a function that squashes every value between 0\nand 1 (see Eq. (12)), it takes larger steps in the S2 area\nand lower steps near the minimum. The main cons of\nthis approach is that when gradient variations are close\nto zero the approach parameters are updated only of\na small value, so this approach could converge more\nslowly than DGrad/DiffGrad in the S1 area. However,\nunlike DGrad, we do not consider the difference be-\ntween the gradients of two iterations but between the\ncurrent iteration and the moving average of the element-\nwise squares of the gradients, so we have values less\nclose to zero even when the gradient changes very\nslowly. Moreover, the step size is sufficiently large due\nto the small value of Eq. (5) in the denominator of Eq.\n(10).\n• MinD is a minimum rule, it reduces the steps and\nsmoothes them more, minimizing the risk of skipping\na minimum. Once more, the cons of this approach could\nbe a slower convergence in the S1 area.\n• AI optimizer controls the step size based on the infor-\nmation from the angle/direction of the gradient vector\ninstead of just the magnitude of it. As shown in Eq. (15),\nwe take the minimum between two angular coefficients,\nin this way, the AI optimizer is more robust to shape\ncurvature changes. Moreover, we add a further step:\nthe curvature information is used as a weight to inject\nthe second-order momentum in the update rule, see\nEq. (20) and Eq. (23). In the S1 area, the step size is\nsufficiently large due to the small value of the gradients\nand accordingly of the denominator of Eq. (23), notice\nthat: delta is applied only in Eq. (20), i.e. the numerator\nof Eq. (23), and not in its denominator; delta <0 when\ngt > 0 and delta >0 when gt < 0. In the S2 area, |gt|\nis large and the value of Eq. (20) is also sufficiently\nlarge even if delta is small, delta increases the value\nof Eq. (20), therefore we have a larger step with respect\nto Adam or Hyp. In the S3 area, |gt| and delta are\nboth large, as stressed above delta <0 when gt > 0\nand delta >0 when gt < 0. In these cases, the injection\ntakes a larger step with respect to Hyp. In conclusion,\nAI performs better than Hyp/DGrad in the S1 and S2\nareas and worse in the S3 area. Due to this different\nbehavior, we believe that these methods are suitable to\nbe combined in an ensemble.\nVI. CONCLUSION\nIn this paper, we combine CNNs based on different topolo-\ngies and various Adam optimization methods for image\nclassification. New Adam algorithms for deep network opti-\nmization are proposed for training a set of CNNs. In addition,\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwe compare sets of CNNs with sets of transformers. The\nensembles were compared and evaluated using different eval-\nuation metrics. The best-performing ensemble, consisting of\nthe CNN ensemble and the transformer ensemble, was shown\nto have the best performance, compared to the literature, on\nseveral benchmarks. In the future, we plan to evaluate ad-hoc\nAdam’s variants on transformers. Future directions include\nrefining distillation techniques, optimizing ensemble design,\nexploring data augmentation strategies, and diversifying data\nsources to mitigate ensemble limitations.\nACKNOWLEDGMENTS\nWe would like to acknowledge the support that NVIDIA\nprovided us through the GPU Grant Program. We used a do-\nnated TitanX GPU to train deep networks used in this work.\nWe thank Giuliano Boscarin, Matteo Bassani and Lorenzo\nGiannini who worked on this project as partial fulfillment of\ntheir bachelor’s degree.\nDECLARATION OF INTERESTS\nThe authors declare that they have no known competing\nfinancial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nREFERENCES\n[1] A. M. Al-Hejri, R. M. Al-Tam, M. Fazea, A. H. Sable, S. Lee, and\nM. A. Al-Antari. Etecadx: Ensemble self-attention transformer encoder\nfor breast cancer diagnosis using full-field digital x-ray breast images.\nDiagnostics, 13(1):89, 2022.\n[2] M. R. Al-Sinan, A. F. Haneef, and H. Luqman. Ensemble learning using\ntransformers and convolutional networks for masked face recognition.\nIn 2022 16th International Conference on Signal-Image Technology &\nInternet-Based Systems (SITIS), pages 421–426. IEEE, 2022.\n[3] M. M. Ali, R. C. Joshi, M. K. Dutta, R. Burget, and A. Mezina. Deep\nlearning-based classification of viruses using transmission electron mi-\ncroscopy images. In 2022 45th International Conference on Telecommu-\nnications and Signal Processing (TSP), pages 174–178. IEEE, 2022.\n[4] E. Ayan, H. Erbay, and F. Varçın. Crop pest classification with a\ngenetic algorithm-based weighted ensemble of deep convolutional neural\nnetworks. Computers and Electronics in Agriculture, 179:105809, dec\n2020.\n[5] R. Bravin, L. Nanni, A. Loreggia, S. Brahnam, and M. Paci. Varied\nimage data augmentation methods for building ensemble. IEEE Access,\n11:8810–8823, 2023.\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models\nare few-shot learners. Advances in neural information processing systems,\n33:1877–1901, 2020.\n[7] S. A. Cadena, G. H. Denfield, E. Y . Walker, L. A. Gatys, A. S. Tolias,\nM. Bethge, and A. S. Ecker. Deep convolutional models improve predic-\ntions of macaque v1 responses to natural images. PLoS computational\nbiology, 15(4):e1006897, 2019.\n[8] C. Cornelio, M. Donini, A. Loreggia, M. S. Pini, and F. Rossi. V oting\nwith random classifiers (vorace): theoretical and experimental analysis.\nAutonomous Agents and Multi-Agent Systems, 35(2):22, 2021.\n[9] P. Coupé, B. Mansencal, M. Clément, R. Giraud, B. D. de Senneville, V .-T.\nTa, V . Lepetit, and J. V . Manjon. Assemblynet: A large ensemble of cnns\nfor 3d whole brain mri segmentation. NeuroImage, 219:117026, 2020.\n[10] Z. Dai, H. Liu, Q. V . Le, and M. Tan. Coatnet: Marrying convolution and\nattention for all data sizes. Advances in Neural Information Processing\nSystems, 34:3965–3977, 2021.\n[11] L. Deng, Y . Wang, Z. Han, and R. Yu. Research on insect pest image\ndetection and recognition based on bio-inspired methods. Biosystems\nEngineering, 169:139–148, 2018.\n[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[13] K. Dimitropoulos, P. Barmpoutis, C. Zioga, A. Kamas, K. Patsiaoura,\nand N. Grammalidis. Grading of invasive breast carcinoma through\ngrassmannian vlad encoding. PloS one, 12(9):e0185110, 2017.\n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[15] S. R. Dubey, S. H. S. Basha, S. K. Singh, and B. B. Chaudhuri. Adainject:\nInjection based adaptive gradient descent optimizers for convolutional\nneural networks, 2022.\n[16] S. R. Dubey, S. Chakraborty, S. K. Roy, S. Mukherjee, S. K. Singh, and\nB. B. Chaudhuri. diffgrad: an optimization method for convolutional\nneural networks. IEEE transactions on neural networks and learning\nsystems, 31(11):4500–4511, 2019.\n[17] J. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for\nonline learning and stochastic optimization. Journal of Machine Learning\nResearch, 12(61):2121–2159, 2011.\n[18] S. d’Ascoli, M. Refinetti, G. Biroli, and F. Krzakala. Double trouble in\ndouble descent: Bias and variance (s) in the lazy regime. In International\nConference on Machine Learning, pages 2280–2290. PMLR, 2020.\n[19] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu, et al. A survey on vision transformer. IEEE transactions on\npattern analysis and machine intelligence, 45(1):87–110, 2022.\n[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 770–778, 2016.\n[21] S. Hossain, A. Chakrabarty, T. R. Gadekallu, M. Alazab, and M. J. Piran.\nVision transformers, ensemble model, and transfer learning leveraging\nexplainable ai for brain tumor detection and classification. IEEE Journal\nof Biomedical and Health Informatics, 2023.\n[22] A. H. Khan, X. Cao, S. Li, V . N. Katsikis, and L. Liao. Bas-adam: An adam\nbased approach to improve the performance of beetle antennae search\noptimizer. IEEE/CAA Journal of Automatica Sinica, 7(2):461–471, 2020.\n[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980, 2014.\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification\nwith deep convolutional neural networks. Advances in neural information\nprocessing systems, 25, 2012.\n[25] S. Kyathanahally, T. Hardeman, M. Reyes, E. Merz, T. Bulas, P. Brun,\nF. Pomati, and M. Baity-Jesi. Ensembles of data-efficient vision transform-\ners as a new paradigm for automated classification in ecology. Scientific\nReports, 13(1):6243, 2023.\n[26] S. P. Kyathanahally, T. Hardeman, E. Merz, T. Bulas, M. Reyes, P. Isles,\nF. Pomati, and M. Baity-Jesi. Deep learning classification of lake zoo-\nplankton. Frontiers in microbiology, page 3226, 2021.\n[27] J. Li, J. Chen, Y . Tang, C. Wang, B. A. Landman, and S. K. Zhou.\nTransforming medical imaging with transformers? a comparative review\nof key properties, current progresses, and future perspectives. Medical\nimage analysis, page 102762, 2023.\n[28] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov. Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[29] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows.\nIn Proceedings of the IEEE/CVF international conference on computer\nvision, pages 10012–10022, 2021.\n[30] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam.\n2017.\n[31] A. Lumini, L. Nanni, and G. Maguolo. Deep learning for plankton and\ncoral classification. Applied Computing and Informatics, (ahead-of-print),\n2020.\n[32] H. Mann, D. Whitney, et al. On a test of whether one of two random\nvariables is stochastically larger than the other. Annals of Mathematical\nStatistics, 18(1):50–60, 1947.\n[33] D. J. Matuszewski and I.-M. Sintorn. Tem virus images: Benchmark\ndataset and deep learning classification. Computer Methods and Programs\nin Biomedicine, 209:106318, 2021.\n[34] S. Moccia, E. De Momi, M. Guarnaschelli, M. Savazzi, A. Laborai,\nL. Guastini, G. Peretti, and L. S. Mattos. Confident texture-based laryngeal\ntissue classification for early stage diagnosis support. Journal of Medical\nImaging, 4(3):034502–034502, 2017.\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[35] L. Nanni, C. Fantozzi, A. Loreggia, and A. Lumini. Ensembles of\nconvolutional neural networks and transformers for polyp segmentation.\nSensors, 23(10), 2023.\n[36] L. Nanni, A. Lumini, A. Loreggia, S. Brahnam, and D. Cuza. Deep\nensembles and data augmentation for semantic segmentation. In Diag-\nnostic Biomedical Signal and Image Processing Applications with Deep\nLearning Methods, pages 215–234. Elsevier, 2023.\n[37] L. Nanni, A. Manfè, G. Maguolo, A. Lumini, and S. Brahnam. High\nperforming ensemble of convolutional neural networks for insect pest\nimage detection. Ecological Informatics, 67:101515, 2022.\n[38] M. M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. Shahbaz Khan,\nand M.-H. Yang. Intriguing properties of vision transformers. Advances\nin Neural Information Processing Systems, 34:23296–23308, 2021.\n[39] S. K. Roy, M. E. Paoletti, J. M. Haut, S. R. Dubey, P. Kar, A. Plaza,\nand B. B. Chaudhuri. Angulargrad: A new optimization technique for\nangular convergence of convolutional neural networks. arXiv preprint\narXiv:2105.10190, 2021.\n[40] B. Savelli, A. Bria, M. Molinara, C. Marrocco, and F. Tortorella. A multi-\ncontext cnn ensemble for small lesion detection. Artificial Intelligence in\nMedicine, 103:101749, 2020.\n[41] S. Shen, X. Wang, F. Mao, L. Sun, and M. Gu. Movements classification\nthrough semg with convolutional vision transformer and stacking ensem-\nble learning. IEEE Sensors Journal, 22(13):13318–13325, 2022.\n[42] N. Sikder, M. A.-M. Khan, A. K. Bairagi, M. Masud, and A.-A. Nahid.\nHeterogeneous virus classification using a functional deep learning model\nbased on transmission electron microscopy images (pre-print).\n[43] L. N. Smith. Cyclical learning rates for training neural networks. In\n2017 IEEE winter conference on applications of computer vision (W ACV),\npages 464–472. IEEE, 2017.\n[44] H. M. Sosik and R. J. Olson. Automated taxonomic classification of\nphytoplankton sampled with imaging-in-flow cytometry. Limnology and\nOceanography: Methods, 5(6):204–216, 2007.\n[45] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolu-\ntional neural networks. In International conference on machine learning,\npages 6105–6114. PMLR, 2019.\n[46] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou.\nTraining data-efficient image transformers & distillation through attention.\nIn International conference on machine learning, pages 10347–10357.\nPMLR, 2021.\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017.\n[48] B. Yang, G. Bender, Q. V . Le, and J. Ngiam. Condconv: Conditionally\nparameterized convolutions for efficient inference. Advances in Neural\nInformation Processing Systems, 32, 2019.\n[49] J. Zhuang, T. Tang, Y . Ding, S. C. Tatikonda, N. Dvornek, X. Papademetris,\nand J. Duncan. Adabelief optimizer: Adapting stepsizes by the belief in\nobserved gradients. Advances in neural information processing systems,\n33:18795–18806, 2020.\nLORIS NANNI is an associate professor with\nthe Department of Information Engineering, Uni-\nversity of Padua. He carries out research at DEI\nUniversity of Padua in the fields of biometric sys-\ntems, pattern recognition, machine learning, image\ndatabases, and bioinformatics. He has extensively\nserved as a referee for international journals (the\nIEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, Pattern Recognition, Bioinfor-\nmatics, the BMC Bioinformatics, and the Pattern\nRecognition Letters) and projects. He is co-author of more than 300 research\npapers. He has an H-index of 57 and more than 11,000 citations (Google\nScholar).\nANDREA LOREGGIA is an Assistant Professor\nat the University of Brescia in the Department\nof Information Engineering. His research inter-\nests in artificial intelligence span from knowledge\nrepresentation to deep learning. His studies are\ndedicated to designing and providing tools for de-\nveloping intelligent agents capable of representing\nand reasoning with preference and ethical-moral\nprinciples. He is a member of the UN/CEFACT\ngroup of experts, he actively participates in the\ndissemination and sustainable development of technology. He received his\nMaster’s Degree cum laude from the University of Padova in 2012, and in\n2016 he received his Ph.D. degree in Computer Science.\nLEONARDO BARCELLONA is Ph.D. student\nin Artificial Intelligence with the Intelligent and\nAutonomous Systems Laboratory, University of\nPadova, Italy and Politecnico di Torino, Torino,\nItaly. He received the master’s degree in Com-\nputer Engineering from the University of Padua\nwith laude in 2021. His research interest include\nsemantic segmentation, few-shot learning, human\npose estimation, robotics and deep learning.\nSTEFANO GHIDONI is full professor of com-\nputer vision at the University of Padova, Italy.\nHis research interests focus on computer vision\nand deep learning for human-robot collaboration,\nmultimodal human perception, semantic segmen-\ntation, scene understanding and medical imaging.\nDuring the PhD, he worked in the field of hu-\nman perception for autonomous driving. Stefano\nGhidoni received his MSc degree in Telecommu-\nnication Engineering and his PhD in Information\nTechnologies from the University of Parma in 2004 and 2008, respectively.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3330442\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}