{
  "title": "Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining",
  "url": "https://openalex.org/W3095698432",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224602715",
      "name": "Lai, Cheng-I",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221356867",
      "name": "Chuang, Yung-Sung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221356874",
      "name": "Lee, Hung-Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221356872",
      "name": "Li, Shang-Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745591341",
      "name": "Glass, James",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2963288440",
    "https://openalex.org/W2889201969",
    "https://openalex.org/W2974831423",
    "https://openalex.org/W3099944122",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2806429264",
    "https://openalex.org/W3015267417",
    "https://openalex.org/W3080993257",
    "https://openalex.org/W3092630929",
    "https://openalex.org/W2894164357",
    "https://openalex.org/W2963446094",
    "https://openalex.org/W2981458636",
    "https://openalex.org/W3026408381",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2926827382",
    "https://openalex.org/W2928075308",
    "https://openalex.org/W2251599843",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W2964117975",
    "https://openalex.org/W3015412890",
    "https://openalex.org/W1936920915",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3025324327",
    "https://openalex.org/W2019116789",
    "https://openalex.org/W3049038774",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2979722627",
    "https://openalex.org/W2077302143"
  ],
  "abstract": "Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.",
  "full_text": "SEMI-SUPERVISED SPOKEN LANGUAGE UNDERSTANDING\nVIA SELF-SUPERVISED SPEECH AND LANGUAGE MODEL PRETRAINING\nCheng-I Lai⋆, Yung-Sung Chuang†, Hung-Yi Lee†, Shang-Wen Li‡, James Glass⋆\n⋆ MIT Computer Science and Artiﬁcial Intelligence Laboratory\n† College of Electrical Engineering and Computer Science, National Taiwan University‡ Amazon AI\nABSTRACT\nMuch recent work on Spoken Language Understanding (SLU) is\nlimited in at least one of three ways: models were trained on or-\nacle text input and neglected ASR errors, models were trained to\npredict only intents without the slot values, or models were trained\non a large amount of in-house data. In this paper, we propose a\nclean and general framework to learn semantics directly from speech\nwith semi-supervision from transcribed or untranscribed speech to\naddress these issues. Our framework is built upon pretrained end-to-\nend (E2E) ASR and self-supervised language models, such as BERT,\nand ﬁne-tuned on a limited amount of target SLU data. We study\ntwo semi-supervised settings for the ASR component: supervised\npretraining on transcribed speech, and unsupervised pretraining by\nreplacing the ASR encoder with self-supervised speech representa-\ntions, such as wav2vec. In parallel, we identify two essential crite-\nria for evaluating SLU models: environmental noise-robustness and\nE2E semantics evaluation. Experiments on ATIS show that our SLU\nframework with speech as input can perform on par with those using\noracle text as input in semantics understanding, even though envi-\nronmental noise is present and a limited amount of labeled semantics\ndata is available for training.\nIndex Terms— spoken language understanding, speech repre-\nsentation learning, semi-supervised learning, speech recognition.\n1. INTRODUCTION\nSpoken Language Understanding (SLU)1 is at the front-end of many\nmodern intelligent home devices, virtual assistants, and socialbots\n[1, 2]: given a spoken command, an SLU engine should extract rel-\nevant semantics2 from spoken commands for the appropriate down-\nstream tasks. Since SLU tasks such as the Airline Travel Information\nSystem (ATIS) [4], the ﬁeld has progressed from knowledge-based\n[5] to data-driven approaches, notably those based on neural net-\nworks. In the seminal paper on ATIS by Tur et al. [3], incorporating\nlinguistically motivated features for NLU and improving ASR ro-\nbustness were underscored as the research emphasis for the coming\nyears. Now, a decade later, we should ask ourselves again, how much\nhas the ﬁeld progressed, and what is left to be done?\nSelf-supervised language models (LMs), such as BERT [6], and\nend-to-end SLU [7, 8, 9] appear to have addressed the problems\nposed in [3]. As shown in Figure 1, we can examine past SLU work\nfrom the angle of how they constructed the input/output pairs. In\nThe fourth author contributed to the work before joining Amazon.\n1SLU typically consists of Automatic Speech Recognition (ASR) and\nNatural Language Understanding (NLU). ASR maps audio to text, and NLU\nmaps text to semantics. Here, we are interested in learning a mapping directly\nfrom raw audio to semantics.\n2Semantic acquisition is commonly framed as Intent Classiﬁcation (IC)\nand Slot Labeling/Filling (SL), see [1, 2, 3].\nintents, slotsOracle Text(A)\n(B)\n(C)\nlabeled Audio\nlabeled Audio\nNLU\nE2E SLU intents\nE2E SLU text, intents, slots\n(D) labeled Audio Ours text, intents, slots\nUnlabeled Audio\nFig. 1. Comparison of input/output pairs of our proposed framework\nwith past work, which are categorized as one of: (A) NLU, which\nassumes oracle text as input instead of speech, (B) predicting intent\nonly from speech, ignoring their slot values, and (C) predicting text,\nintent, and slots from speech. (D) Our work predicts text, intent, and\nslots from speech while taking advantage of unlabeled data.\n[10], Intent Classiﬁcation (IC) and Slot Labeling (SL) are jointly\npredicted on top of BERT, discarding the need of a Conditional Ran-\ndom Fields (CRF) [11]. However, these NLU works [10, 12, 13]\nusually ignore ASR or require an off-the-shelf ASR during testing.\nA line of E2E SLU work does take speech as input, yet it frames slots\nas intents and therefore their SLU models are really designed for IC\nonly [8, 9, 14, 15, 16]. Another line of E2E SLU work jointly pre-\ndicts text and IC/SL from speech, yet it either requires large amounts\nof in-house data, or restricts the pretraining scheme to ASR subword\nprediction [7, 17, 18, 19]. In contrast, we would desire a framework\nthat predicts text, intents, and slots from speech, while learning with\nlimited semantics labels by pretraining on unlabeled data.\nThe case for semi-supervised SLU.Neural networks beneﬁt\nfrom large quantities of labeled training data, and one can train end-\nto-end SLU models with them [2, 7, 8, 17]. However, curating\nlabeled IC/SL data is expensive, and often only a limited amount\nof labels are available. Semi-supervised learning could be a use-\nful scenario for training SLU models for various domains whereby\nmodel components are pretrained on large amounts of unlabeled data\nand then ﬁne-tuned with target semantic labels. While [9, 14, 17,\n18] have explored this pretraining then ﬁne-tuning scheme, they did\nnot take advantage of the generalization capacity of contextualized\nLMs, such as BERT, for learning semantics from speech. Notably,\nself-supervised speech representation learning [21, 22, 23, 24, 25]\nprovides a clean and general learning mechanism for downstream\nspeech tasks, yet the semantic transferrability of these representa-\ntions are unclear. Our focus is on designing a better learning frame-\nwork distinctly for semantic understandingunder limited semantic\nlabels, on top of ASR and BERT. We investigated two learning set-\ntings for the ASR component: (1) pretraining on transcribed speech\nwith ASR subword prediction, and (2) pretraining on untranscribed\nspeech data with contrastive losses [23, 24].\nThe key contributions of this paper are summarized as follows:\narXiv:2010.13826v1  [cs.CL]  26 Oct 2020\nASR\n(A) End-to-End: \nJoint ASR and BERT ﬁne-tuning\nBERTASR BERT\n(B) 2-Stage Baseline: \nCascade ASR to BERT\nTextText\nText\nIC/SL IC/SL\nText\nBERT\nTextText\n(C) SpeechBERT Baseline: Cascade ASR to \nBERT in Joint Speech/Text Embedding Space\nconcat\nEnd-to-End\nASR BERT\nIC/SL\n[MASK] [MASK]\nAudio MLM Fine-tune\n(D) Building Blocks: \nE2E ASR and BERT for Joint IC/SL\nSL lossIC lossASR loss\n[CLS]\nBertToken\nBPE\nText\nFig. 2. Our proposed semi-supervised learning framework with ASR and BERT for joint intent classiﬁcation (IC) and slot labeling (SL)\ndirectly from speech. (A) shows the end-to-end approach, in which E2E ASR and BERT are trained jointly by predicting text and IC/SL.(B)\nshows the 2-Stage baseline, where text and IC/SL are obtained successively. (C) shows the SpeechBERT baseline, where BERT is adapted\nto take audio as input by ﬁrst pretraining with Audio MLM loss and then ﬁne-tuning for IC/SL. A separate pretrained ASR is still needed for\n(B) and (C). (D) shows the ASR (θASR) and NLU ({θBERT ,θIC ,θSL}) building blocks used in (A)-(C). Note that θASR and θBERT have\ndifferent subword tokenizations: SentencePiece (BPE) [20] and BertToken. Dotted shapes are pretrained. Figure best viewed in colors.\n• We introduce a semi-supervised SLU framework for learning\nsemantics from speech to alleviate: (1) the need for a large\namount of in-house, homogenous data [2, 7, 8, 17], (2) the\nlimitation of only intent classiﬁcation [8, 9, 13] by predicting\ntext, slots and intents, and (3) any additional manipulation\non labels or loss, such as label projection [26], output seri-\nalization [7, 18, 19], ASR n-best hypothesis, or ASR-robust\ntraining losses [13, 27]. Figure 2 illustrates our approach.\n• We investigate two learning settings for our framework: su-\npervised pretraining and unsupervised pretraining (Figure 3),\nand evaluated our framework with a new metric, the slot edit\nF1 score, for end-to-end semantic evaluation. Our framework\nimproves upon previous work in Word Error Rate (WER) and\nIC/SL on ATIS, and even rivaled its NLU counterpart with\noracle text input [10]. In addition, it is trained with noise aug-\nmentation such that it is robust to real environmental noises.\n2. PROPOSED LEARNING FRAMEWORK\nWe now formulate the mapping from speech to text, intents, and\nslots. Consider a target SLU dataset D= {A,W,S,I}M\ni=1, con-\nsisting of M i.i.d. sequences, where A,W,S are the audio, word\nand slots sequences, and I is their corresponding intent label. Note\nthat W and S are of the same length, and I is a one hot vector. We\nare interested in ﬁnding the model θ∗\nSLU with loss,\nLSLU (θSLU ) =E(A,W,S,I)∼D\n[\nln P(W,S,I |A; θ)\n]\n(1)\nWe proceed to describe an end-to-end implementation ofθSLU\n3.\n2.1. End-to-End: Joint E2E ASR and BERT Fine-Tuning.\nAs illustrated in Figure 2, θSLU consists of a pretrained E2E ASR\nθASR and a pretrained deep contextualized LM θNLU , such as\nBERT, and is ﬁne-tuned jointly for W, S and I on D. The choice\nof E2E ASR over hybrid ASR here is because the errors from S\nand I can be back-propagated through A; following [10], we have\nS predicted via an additional CRF/linear layer on top of BERT,\nand I is predicted on top of the BERT output of the [CLS] token.\nThe additional model parameters for predicting SL and IC are θSL\nand θIC , respectively, and we have θNLU = {θBERT ,θIC ,θSL}.\nDuring end-to-end ﬁne-tuning, outputs from θASR and θBERT are\nconcatenated to predict S and I with loss LNLU , while W is pre-\ndicted with loss LASR. The main beneﬁt this formulation brings is\n3We abuse some notations by representing models by their model param-\neters, e.g. θASR for the ASR model and θBERT for BERT.\nthat now S and I do not solely depend on an ASR top-1 hypothesis\nW∗during training, and the end-to-end objective is thus,\nLSLU (θSLU ) =LASR(θSLU ) +LNLU (θSLU ). (2)\nThe ASR objective LASR is formulated to maximize sequence-level\nlog-likelihood, and LASR(θSLU ) =LASR(θASR). Before writing\ndown LNLU , we describe a masking operation because ASR and\nBERT typically employ different subword tokenization methods.\nDifferentiate Through Subword Tokenizations To concatenate\nθASR and θBERT outputs along the hidden dimension, we need to\nmake sure they have the same length along the token dimension.\nWe stored the ﬁrst indices where W are broken down into subword\ntokens into a matrix: Ma ∈I RNa×N for θASR and Mb ∈I RNb×N\nfor θBERT , where N is the number of tokens for W and S, Na\nis the number of ASR subword tokens, and Nb for BERT. Let Ha\nbe the θASR output matrix before softmax, and similarly Hb for\nθBERT . The concatenated matrix Hcat ∈I RN×(Fa+Fb) is given as\nHcat = concat(\n[\n(Ma)T Ha,(Mb)T Hb]\n,dim=1), where Fa and\nFb are hidden dimensions for θASR and θBERT . LNLU is then,\nLNLU = E\n[\nln P(S |Hcat; θSL) + lnP(I |Hcat; θIC )\n]\n, (3)\nwhere the sum of cross entropy losses for IC and SL are maximized,\nand θASR and θBERT are updated through Hcat. Ground truth W\nis used as input to θBERT instead of W∗due to teacher forcing.\n2.2. Inference\nAt test time, an input audio sequence a = a1:T and the sets of all\npossible word tokens W, slots S, and intents Iare given. We are\nthen interested in decoding for its target word sequencew∗= w1:N ,\nslots sequence s∗ = s1:N , and intent label i∗. Having obtained\nθ∗\nSLU , the decoding procedure for the end-to-end approach is,\nw∗= argmax\nwn∈W\nN∏\nn=1\np(wn |wn−1,a; θ∗\nASR), (4)\ni∗,s∗= argmax\ni∈I\np(i|w∗,a; θ∗\nSLU ),argmax\nsn∈S\nN∏\nn=1\np(sn |w∗,a; θ∗\nSLU )\n(5)\nThis two step decoding procedure, ﬁrstw∗then (i∗,s∗) is necessary\ngiven that no explicit serialization on W and S are imposed, as in\n[7, 18]. While decoding for (i∗,s∗), additional input a is given and\nwe have w∗instead of wn given the context from self-attention in\nBERT. Note that here and throughout the work, we only take the\ntop-1 hypothesis w∗(instead of top-N) to decode for (i∗,s∗).\n3. LEARNING WITH LESS SUPERVISION\nOur semi-supervised framework relies on pretrained ASR and NLU\ncomponents. Depending on the accessibility of the data, we ex-\nplored two level of supervision4. The ﬁrst setting is where an exter-\nnal transcribed corpus is available, and we utilized transfer learning\nfor initializing the ASR. The second setting is where external au-\ndio is available but not transcriptions, and in this case, the ASR is\ninitialized with self-supervised learning. In both settings, BERT is\npretrained with MLM and NSP as described in [6]. Figure 3 distin-\nguishes the two learning settings.\nwav2vec\n ASR BERT\nwav2vec ASR BERT\n(B.1) Unsupervised Pretrain: wav2vec and BERT\n(B.2) SLU Fine-tune: ASR and BERT\nASR BERT\nASR BERT\n(A.3) SLU Fine-tune: ASR and BERT\n(A.2) Supervised Pretrain on Librispeech: ASR\nASR BERT\n(A.1) Unsupervised Pretrain: BERT\nFig. 3. Two semi-supervised settings: (A) additional transcribed\nspeech is available. θASR is pretrained and ﬁne-tuned for ASR. (B)\nadditional audio is available but without transcription.θASR encoder\nis replaced with a pretrained wav2vec [24, 23] before ﬁne-tuning.\n3.1. Transfer Learning from a Pretrained ASR\nFollowing [9, 17, 18], θASR is pretrained on an external transcribed\nspeech corpus before ﬁne-tuning on the target SLU dataset.\n3.2. Unsupervised ASR Pretraining with wav2vec\nAccording to UNESCO, 43% of the languages in the world are\nendangered. Supervised pretraining is not possible for many lan-\nguages, as transcribing a language requires expert knowledge in\nphonetics, morphology, syntax, and so on. This partially motivates\nthe line of self-supervised learning work in speech, that powerful\nlearning representations require little ﬁne-tuning data. Returning to\nour topic, we asked, how does self-supervised learning help with\nlearning semantics?\nAmong many others, wav2vec 1.0 [24] and 2.0 [23] demon-\nstrated the effectiveness of self-supervised representations for ASR.\nThey are pretrained with contrastive losses [22], and differ mainly by\ntheir architectures. We replaced θASR encoder with these wav2vec\nfeatures, and appended the θASR decoder for ﬁne-tuning on SLU.\n4. EXPERIMENTS\nDatasets ATIS [4] contains 8hr of audio recordings of people mak-\ning ﬂight reservations with corresponding human transcripts. A total\nof 5.2k utterances with more than 600 speakers are present. Note that\nATIS is considerably smaller than those in-house SLU data used in\n4In either settings, the amount of IC/SL annotations remains the same.\nFig. 4. Our SpeechBERT [32] pretraining and ﬁne-tuning setup.\nartist\nnew\nPlay one song\n[MASK]\nfrom Jay Chou's album\n[MASK] [MASK]\nsong Jay Mojito\nmusic_item artist albummusic_item\nAudio MLM target\nFine-Tune target\nTranscription\nMasked Audio\nMojito\n[2, 7, 8, 17], justifying our limited semantics labels setup. Wave-\nforms are sampled at 16kHz. For the unlabeled semantics data,\nwe selected Librispeech 960hr (LS-960) [28] for pretraining. Be-\nsides the original ATIS, models are evaluated on its noisy copy (aug-\nmented with MS-SNSD [29]). We made sure the noisy train and test\nsplits in MS-SNSD do not overlap. Text normalization is applied on\nthe ATIS transcription with an open-source software 5. Utterances\nare ignored if they contain words with multiple possible slot tags.\nHyperparameters All speech is represented as sequences of 83-\ndimensional Mel-scale ﬁlter bank energies with F0, computed every\n10ms. Global mean normalization is applied. E2E ASR is imple-\nmented in ESPnet, where it has 12 Transformer encoder layers and 6\ndecoder layers. The choice of the Transformer is similar to [16]. E2E\nASR is optimized with hybrid CTC/attention losses [30] with label\nsmoothing. The decoding beam size is set to 5 throughout this work.\nWe do notuse an external LM during decoding. SpecAugment [31]\nis used as the default for data augmentation. A SentencePiece (BPE)\nvocabulary size is set to 1k. BERT is a bert-base-uncased from Hug-\ngingFace. Code will be made available6.\n4.1. E2E Evaluation with Slots EditF1 score.\nOur framework is evaluated with an end-to-end evaluation metric,\ntermed the slots editF1. Unlike slots F1 score, slots editF1 accounts\nfor instances where predicted sequences have different lengths as the\nground truth. It bears similarity with the E2E metric proposed in\n[7, 17]. To calculate the score, the predicted text and oracle text are\nﬁrst aligned. For each slot label v ∈V , where Vis the set of all\npossible slot labels except for the “O” tag, we calculate the insertion\n(false positive, FP), deletion (false negative, FN) and substition (FN\nand FP) of its slots value. Slots edit F1 is the harmonic mean of\nprecision and recall over all slots:\nslots edit F1 =\n∑\nv∈V2 ×TPv\n∑\nv∈V\n[\n(2 ×TPv) +FPv + FNv\n] (6)\n4.2. End-to-End 2-Stage Fine-tuning\nAn observation from the experiment was that ASR is much harder\nthan IC/SL. Therefore, we adjusted our end-to-end training to a two-\nstage ﬁne-tuning: pretrain ASR on LS-960, then ﬁne-tine ASR on\nATIS, and lastly jointly ﬁne-tune for ASR and IC/SL on ATIS.\n4.3. Baselines: AlternativeθSLU Formulations\nTwo variations for constructing θSLU are presented (refer to Fig-\nure 2). They will be the baselines to the end-to-end approach.\n2-Stage: Cascade ASR to BERTA natural complement to the E2E\napproach is to separately pretrain and ﬁne-tune ASR and BERT. In\nthis case, errors from S and I cannot be back-propagated to θASR.\nSpeechBERT : BERT in Speech-Text Embed SpaceAnother sen-\nsible way to construct θSLU is to somehow “adapt” BERT such that\nit can take audio as input and outputs IC/SL, while not compromising\nits original semantic learning capacity. SpeechBERT [32] was ini-\ntially proposed for spoken question answering, but we found the core\n5https://github.com/EFord36/normalise\n6Code: Semi-Supervsied-Spoken-Language-Understanding-PyTorch\nidea of training BERT with audio-text pairs ﬁtting as another base-\nline for our end-to-end approach. We modiﬁed the pretraining and\nﬁne-tuning setup described in [32] for SLU. Audio MLM (c.f MLM\nin BERT [6]) pretrains θBERT by mapping masked audio segments\nto text. This pretraining step gradually adapts the original BERT to a\nphonetic-semantic joint embedding space. Then, θNLU is ﬁne-tuned\nby mapping unmasked audio segmentsto IC/SL. Figure 4 illustrates\nthe audio-text and audio-IC/SL pairs for SpeechBERT. Unlike the\nend-to-end approach, θASR is kept frozen throughout SpeechBERT\npretraining and ﬁne-tuning.\n4.4. Main Results on Clean ATIS\nWe benchmarked our proposed framework with several prior works,\nand Table 1 presents their WER, slots edit F1 and intent F1 results.\nJointBERT [10] is our NLU baseline, where BERT is jointly ﬁne-\ntuned for IC/SL, and it gets around 95% slots edit F1 and over 98%\nIC F1. Since JointBERT has access to the oracle text, this is the up-\nper bound for our SLU models with speech as input. CLM-BERT\n[26] explored using in-house conversational LM for NLU. We repli-\ncated [18], where an LAS [33] directly predicts interleaving word\nand slots tokens (serialized output), and optimized with CTC over\nwords and slots. We also experimented with a Kaldi hybrid ASR.\nBoth our proposed end-to-end and baselines approaches sur-\npassed prior SLU work. We hypothesize the performance gain\noriginates from our choices of (1) adopting pretrained E2E ASR and\nBERT, (2) applying text-norm on target transcriptions for training\nthe ASR, and (3) end-to-end ﬁne-tuning text and IC/SL.\nTable 1. WER, slots edit and intent F1 on ATIS. ASR is pretrained\non Librispeech 960h (LS-960). Results indicate our semi-supervised\nframework is effective in data scarcity setting, exceeding prior work\nin WER and IC/SL while approaching the NLU upperbound.\nFrameworks Unlabeled ATIS clean test\nSemantics Data WER slots editF1 intentF1\nNLU with Oracle Text\nJointBERT [10] - 95.64 98.99\nProposed\nEnd-to-End w/ 2-Stage LS-960 2.18 95.88 97.26\n2-Stage Baseline LS-960 1.38 93.69 97.01\nSpeechBERT Baseline LS-960 1.4 92.36 97.4\nPrior Work\nASR-Robust Embed [13] WSJ 15.55 - 95.65\nKaldi Hybrid ASR+BERT LS-960 13.31 85.13 94.56\nASR+CLM-BERT [26] in-house 18.4. 93.8 7 97.1\nLAS+CTC [18] LS-460 8.32 86.85 -\n4.5. Environmental Noise Augmentation\nA common scenario where users utter their spoken commands to\nSLU engines is when environmental noises are present in the back-\nground. Nonetheless, common SLU benchmarking datasets like\nATIS, SNIPS [2], or FSC [9] are very clean. To quantify model\nrobustness under noisy settings, we augmented ATIS with envi-\nronmental noise from MS-SNSD. Table 2 reveals that those work\nwell on clean ATIS may break under realistic noises, and although\nour models are trained with SpecAugment, there is still a 4-27%\nperformance drop from clean test.\nWe followed the noise augmentation protocol in [29], where for\neach sample, ﬁve noise ﬁles are sampled and added to the clean ﬁle\nwith SNR levels of[0,10,20,30,40]dB, resulting in a ﬁve-fold aug-\nmentation. We observe that augmenting the training data with a di-\nverse set of environmental noises work well, and there is now min-\nimal model degradation. Our end-to-end approach reaches 95.46%\n7For [26], model predictions are evaluated only if its ASR hypothesis and\nhuman transcription have the same number of tokens.\nfor SL and 97.4% for IC, which is merely a 1-2% drop from clean\ntest, and almost a 40% improvement over hybrid ASR+BERT.\nTable 2. Noise augmentation effectively reduces model degradation.\nFrameworks ATIS noisy test\nWER slots editF1 intentF1\nKaldi Hybrid ASR+BERT 44.72 69.55 88.94\nProposed w/ Noise Aug.\nEnd-to-End w/ 2-Stage 3.6 95.46 97.40\n2-Stage Baseline 3.5 92.52 96.49\nSpeechBERT Baseline 3.6 88.7 96.15\nProposed w/o Noise Aug.\nEnd-to-End w/ 2-Stage 9.62 91.54 96.14\n2-Stage Baseline 8.98 90.09 95.74\nSpeechBERT Baseline 9.0 81.72 94.05\n4.6. Effectiveness of Unsupervised Pretraining with wav2vec\nTable 3 shows the results on different ASR pretraining strategies:\nunsupervised pretraining with wav2vec, transfer learning from ASR,\nand no pretraining at all. We extracted both the latent vector z and\ncontext vector c from wav2vec 1.0. To simplify the pipeline and in\ncontrast to [24], we pre-extracted the wav2vec features and did not\nﬁne-tune wav2vec with θSLU on ATIS. We also chose not to decode\nwith a LM to be consistent with prior SLU work. We ﬁrst observed\nthe high WER for latent vector z from wav2vec 1.0, indicating they\nare sub-optimal and merely better than training from scratch by a\nslight margin. Nonetheless, encouragingly, context vector c from\nwav2vec 1.0 gets 67% slots and 90% intent F1.\nTo improve the results, we added subsampling layers [33] on\ntop of the wav2vec features to downsample the sequence length with\nconvolution. The motivation here is c and z are comparably longer\nthan the normal ASR encoder outputs. With sub-sampling, c from\nwav2vec 1.0 now achieves 85.64% for SL and 95.67% for IC, a huge\nrelative improvement over training ASR from scratch, and closes the\ngap between unsupervised and supervised pretraining for SLU.\nTable 3. Effectiveness of different ASR pretraining strategies for\nour 2-Stage baseline. Results with wav2vec 2.0 [23] is omitted since\nthey are not much better. Setup is visaulized in Figure 3.\nFrameworks ATIS clean test\nWER slots editF1 intentF1\nProposed 2-Stage w/o ASR Pretraining\n2-Stage Baseline 58.7 29.22 82.08\nProposed 2-Stage w/ Transfer Learning from ASR\n2-Stage Baseline 1.38 93.69 97.01\nProposed 2-Stage w/ Unsupervised Pretraining\nwav2vec1.0 [24]z+ 2-Stage 54.2 35.04 83.68\nwav2vec1.0 [24]c+ 2-Stage 30.4 67.33 89.86\nwav2vec1.0 [24]c+ subsample + 2-Stage 13.2 85.64 95.67\n5. CONCLUSIONS AND FUTURE WORK\nThis work attempts to respond to a classic paper ”What is left to\nbe understood in ATIS? [3]”, and to the advancement put forward\nby contextualized LM and end-to-end SLU up against semantics un-\nderstanding. We showed for the ﬁrst time that an SLU model with\nspeech as input could perform on par with NLU models on ATIS,\nentering the 5% “corpus errors” range [3, 34]. However, we collec-\ntively believe that there are unsolved questions remaining, such as\nthe prospect of building a single framework for multi-lingual SLU\n[35], or the need for a more spontaneous SLU corpus that is not lim-\nited to short segments of spoken commands.\nAcknowledgments We thank Nanxin Chen, Erica Cooper, Alexander H. Liu,\nWei Fang, and Fan-Keng Sun for their comments on this work.\n6. REFERENCES\n[1] Dian Yu, Michelle Cohn, Yi Mang Yang, Chun-Yen Chen, Weiming\nWen, Jiaping Zhang, Mingyang Zhou, Kevin Jesse, Austin Chau, An-\ntara Bhowmick, et al., “Gunrock: A social bot for complex and engag-\ning long conversations,” arXiv preprint arXiv:1910.03042, 2019.\n[2] Alice Coucke, Alaa Saade, Adrien Ball, Th ´eodore Bluche, Alexan-\ndre Caulier, David Leroy, Cl ´ement Doumouro, Thibault Gisselbrecht,\nFrancesco Caltagirone, Thibaut Lavril, et al., “Snips voice platform:\nan embedded spoken language understanding system for private-by-\ndesign voice interfaces,” arXiv preprint arXiv:1805.10190, 2018.\n[3] Gokhan Tur, Dilek Hakkani-T ¨ur, and Larry Heck, “What is left to\nbe understood in atis?,” in 2010 IEEE Spoken Language Technology\nWorkshop. IEEE, 2010, pp. 19–24.\n[4] Charles T Hemphill, John J Godfrey, and George R Doddington, “The\natis spoken language systems pilot corpus,” in Speech and Natural\nLanguage: Proceedings of a Workshop Held at Hidden Valley, Pennsyl-\nvania, June 24-27, 1990, 1990.\n[5] Stephanie Seneff, “Tina: A natural language system for spoken lan-\nguage applications,” Computational linguistics, vol. 18, no. 1, pp. 61–\n86, 1992.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,\n“Bert: Pre-training of deep bidirectional transformers for language un-\nderstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[7] Parisa Haghani, Arun Narayanan, Michiel Bacchiani, Galen Chuang,\nNeeraj Gaur, Pedro Moreno, Rohit Prabhavalkar, Zhongdi Qu, and\nAustin Waters, “From audio to semantics: Approaches to end-to-end\nspoken language understanding,” in2018 IEEE Spoken Language Tech-\nnology Workshop (SLT). IEEE, 2018, pp. 720–726.\n[8] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Kumar,\nBaiyang Liu, and Yoshua Bengio, “Towards end-to-end spoken lan-\nguage understanding,” in 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp.\n5754–5758.\n[9] Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar,\nand Yoshua Bengio, “Speech model pre-training for end-to-end spoken\nlanguage understanding,” arXiv preprint arXiv:1904.03670, 2019.\n[10] Qian Chen, Zhu Zhuo, and Wen Wang, “Bert for joint intent classiﬁca-\ntion and slot ﬁlling,” arXiv preprint arXiv:1902.10909, 2019.\n[11] Jie Zhou and Wei Xu, “End-to-end learning of semantic role labeling\nusing recurrent neural networks,” in Proceedings of the 53rd Annual\nMeeting of the Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), 2015, pp. 1127–1137.\n[12] Su Zhu and Kai Yu, “Encoder-decoder with focus-mechanism for se-\nquence labelling based spoken language understanding,” in 2017 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2017, pp. 5675–5679.\n[13] Chao-Wei Huang and Yun-Nung Chen, “Learning asr-robust contex-\ntualized embeddings for spoken language understanding,” in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2020, pp. 8009–8013.\n[14] Pengwei Wang, Liangchen Wei, Yong Cao, Jinghui Xie, and Zaiqing\nNie, “Large-scale unsupervised pre-training for end-to-end spoken lan-\nguage understanding,” in ICASSP 2020-2020 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2020, pp. 7999–8003.\n[15] Won Ik Cho, Donghyun Kwak, Jiwon Yoon, and Nam Soo Kim,\n“Speech to text adaptation: Towards an efﬁcient cross-modal distilla-\ntion,” arXiv preprint arXiv:2005.08213, 2020.\n[16] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann,\n“End-to-end neural transformer based spoken language understanding,”\narXiv preprint arXiv:2008.10984, 2020.\n[17] Milind Rao, Anirudh Raju, Pranav Dheram, Bach Bui, and Ariya Ras-\ntrow, “Speech to semantics: Improve asr and nlu jointly via all-neural\ninterfaces,” arXiv preprint arXiv:2008.06173, 2020.\n[18] Natalia Tomashenko, Antoine Caubri `ere, Yannick Est`eve, Antoine Lau-\nrent, and Emmanuel Morin, “Recent advances in end-to-end spoken\nlanguage understanding,” in International Conference on Statistical\nLanguage and Speech Processing. Springer, 2019, pp. 44–55.\n[19] Sahar Ghannay, Antoine Caubriere, Yannick Esteve, Antoine Laurent,\nand Emmanuel Morin, “End-to-end named entity extraction from\nspeech,” arXiv preprint arXiv:1805.12045, 2018.\n[20] Taku Kudo and John Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neural text\nprocessing,” arXiv preprint arXiv:1808.06226, 2018.\n[21] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass, “An un-\nsupervised autoregressive model for speech representation learning,”\narXiv preprint arXiv:1904.03240, 2019.\n[22] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Represen-\ntation learning with contrastive predictive coding,” arXiv preprint\narXiv:1807.03748, 2018.\n[23] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael\nAuli, “wav2vec 2.0: A framework for self-supervised learning of\nspeech representations,” arXiv preprint arXiv:2006.11477, 2020.\n[24] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli,\n“wav2vec: Unsupervised pre-training for speech recognition,” arXiv\npreprint arXiv:1904.05862, 2019.\n[25] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi\nLee, “Mockingjay: Unsupervised speech representation learning with\ndeep bidirectional transformer encoders,” in ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2020, pp. 6419–6423.\n[26] Jin Cao, Jun Wang, Wael Hamza, Kelly Vanee, and Shang-Wen Li,\n“Style attuned pre-training and parameter efﬁcient ﬁne-tuning for spo-\nken language understanding,” arXiv preprint arXiv:2010.04355, 2020.\n[27] Chia-Hsuan Lee, Yun-Nung Chen, and Hung-Yi Lee, “Mitigating\nthe impact of speech recognition errors on spoken question answer-\ning by adversarial domain adaptation,” in ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2019, pp. 7300–7304.\n[28] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudan-\npur, “Librispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.\n[29] Chandan KA Reddy, Ebrahim Beyrami, Jamie Pool, Ross Cut-\nler, Sriram Srinivasan, and Johannes Gehrke, “A scalable noisy\nspeech dataset and online subjective test framework,” arXiv preprint\narXiv:1909.08050, 2019.\n[30] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and\nTomoki Hayashi, “Hybrid ctc/attention architecture for end-to-end\nspeech recognition,” IEEE Journal of Selected Topics in Signal Pro-\ncessing, vol. 11, no. 8, pp. 1240–1253, 2017.\n[31] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Bar-\nret Zoph, Ekin D Cubuk, and Quoc V Le, “Specaugment: A simple\ndata augmentation method for automatic speech recognition,” arXiv\npreprint arXiv:1904.08779, 2019.\n[32] Yung-Sung Chuang, Chi-Liang Liu, and Hung-Yi Lee, “Speechbert:\nCross-modal pre-trained language model for end-to-end spoken ques-\ntion answering,” arXiv preprint arXiv:1910.11559, 2019.\n[33] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals, “Listen,\nattend and spell: A neural network for large vocabulary conversational\nspeech recognition,” in2016 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–\n4964.\n[34] Fr ´ed´eric B ´echet and Christian Raymond, “Is atis too shallow to go\ndeeper for benchmarking spoken language understanding models?,”\n2018.\n[35] James Glass, Giovanni Flammia, David Goodine, Michael Phillips,\nJoseph Polifroni, Shinsuke Sakai, Stephanie Seneff, and Victor Zue,\n“Multilingual spoken-language understanding in the mit voyager sys-\ntem,” Speech communication, vol. 17, no. 1-2, pp. 1–18, 1995.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8479226231575012
    },
    {
      "name": "Oracle",
      "score": 0.7528457641601562
    },
    {
      "name": "Spoken language",
      "score": 0.6317110061645508
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6214283108711243
    },
    {
      "name": "Natural language processing",
      "score": 0.5958566069602966
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5857284069061279
    },
    {
      "name": "Encoder",
      "score": 0.582946240901947
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5461758375167847
    },
    {
      "name": "Language model",
      "score": 0.5292828679084778
    },
    {
      "name": "Speech recognition",
      "score": 0.522591233253479
    },
    {
      "name": "Software engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}