{
  "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
  "url": "https://openalex.org/W4389524151",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2574157058",
      "name": "Dheeraj Mekala",
      "affiliations": [
        "OpenAI (United States)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2048077336",
      "name": "Jason Wolfe",
      "affiliations": [
        "Microsoft (United States)",
        "OpenAI (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2194586962",
      "name": "Subhro Roy",
      "affiliations": [
        "OpenAI (United States)",
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951181836",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W3168491067",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W4206496297",
    "https://openalex.org/W2970745243",
    "https://openalex.org/W4298187912",
    "https://openalex.org/W4385573570",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2251199578",
    "https://openalex.org/W3105662186",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W2987553933",
    "https://openalex.org/W4385574313",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3099053576",
    "https://openalex.org/W4280633590",
    "https://openalex.org/W3199969499",
    "https://openalex.org/W3205950290",
    "https://openalex.org/W4229058120",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4288614645",
    "https://openalex.org/W3034588688",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. LLMs are generally trained on publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. We address this by fine-tuning a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can zero-shot parse ≈ 16% of utterances in the MTOP dataset.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5792–5799\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using\nLarge Language Models\nDheeraj Mekala* Jason Wolfe ♠ Subhro Roy\nUC San Diego OpenAI Microsoft Semantic Machines\ndmekala@ucsd.edu jasonwolfe@openai.com subhro.roy@microsoft.com\nAbstract\nWe explore the use of large language models\n(LLMs) for zero-shot semantic parsing. Seman-\ntic parsing involves mapping natural language\nutterances to task-specific meaning representa-\ntions. LLMs are generally trained on publicly\navailable text and code and cannot be expected\nto directly generalize to domain-specific pars-\ning tasks in a zero-shot setting. In this work, we\npropose ZEROTOP, a zero-shot task-oriented\nparsing method that decomposes semantic pars-\ning problem into a set of abstractive and extrac-\ntive question-answering (QA) problems. For\neach utterance, we prompt the LLM with ques-\ntions corresponding to its top-level intent and a\nset of slots and use the LLM generations to con-\nstruct the target meaning representation. We ob-\nserve that current LLMs fail to detect unanswer-\nable questions; and as a result, cannot handle\nquestions corresponding to missing slots. We\naddress this by fine-tuning a language model\non public QA datasets using synthetic nega-\ntive samples. Experimental results show that\nour QA-based decomposition paired with the\nfine-tuned LLM can zero-shot parse ≈16% of\nutterances in the MTOP dataset.\n1 Introduction\nLarge language models (LLMs) are trained on pub-\nlicly available text (Raffel et al., 2020; Sanh et al.,\n2021; Brown et al., 2020) and code (Chen et al.,\n2021) and have been shown to attain reasonable\nzero-shot generalization on a diverse set of NLP\ntasks (Wang et al., 2019). However, they are not\nexpected to generalize to domain-specific semantic\nparsing tasks in a similar way, where the inductive\nbias from pre-training is less helpful. In this work,\nwe propose ZEROTOP that decomposes the seman-\ntic parsing task into one of answering a series of\nextractive and abstractive questions, corresponding\n* Work done during an internship at Microsoft Semantic\nMachines.\n♠Work done while at Microsoft Semantic Machines.\nto its top-level intent and a set of relevant slots,\nand leverage the LLM’s ability to zero-shot answer\nreading comprehension questions.\nAs illustrated in Figure 1, we cast top-level in-\ntent classification as an abstractive QA task. To\naddress LLMs’ bias towards predicting labels com-\nmon in the pretraining data (Zhao et al., 2021),\nwe propose to generate an intent description in an\nunconstrained manner and infer the intent label\nmost similar to the generated description. We view\nslot value prediction as an extractive QA problem.\nMost utterances do not mention all the slots. It is\ntherefore essential for the model to abstain from\nprediction when corresponding slots are not men-\ntioned. Through our analyses, we observe that\nmost LLMs frequently hallucinate text for miss-\ning slots with high confidence, resulting in poor\nperformance. To address this, we fine-tune an LM\non a collection of public QA datasets augmented\nwith synthetic unanswerable samples. We call our\ntrained model Abstainer, as it is capable of identi-\nfying unanswerable questions and abstaining from\nprediction. We hierarchically prompt for nested\nslots using the Abstainer, and infer nested intents\nif their corresponding slots are detected. We empir-\nically show that this QA based decomposition of\nZEROTOP is an effective way to leverage LLMs\nfor domain specific semantic parsing, outperform-\ning several strong baselines in the zero shot setting.\n2 Related Work\nLLMs are increasingly used for semantic parsing in\nlow-data scenarios utilizing canonical representa-\ntions (Shin et al., 2021; Yang et al., 2022), and\nprompt-tuning (Schucher et al., 2022; Drozdov\net al., 2022). The closest work to ours is Zhao\net al. (2022) where they decompose parsing into\nQA tasks. However, they assume access to some an-\nnotated data whereas we focus on a strict zero-shot\nsetting where only the schema information is avail-\nable along with some natural language prompts for\n5792\nC1\nGET_CONTACT\nGET_SCHOOL…Cn…\nS1\nSn…\nPossible Nested intents\nSlotsWhat is the relation?\nWhere is the school located?\nAbstainer\nAbstainer\n“please send that message to my mom on WhatsApp”What did the user intend to do?Send Message\nRecipientWho is the recipient?my mom\nName_AppWhich app?WhatsApp\nOrdinalWhich one?N/A\nmom\nN/A\n[IN:SEND_MESSAGE [SL:RECIPIENT [IN:GET_CONTACT [SL:CONTACT_RELATED my ] [SL:TYPE_RELATION mom ] ] ] ]\nFigure 1: ZEROTOP hierarchically prompts LLMs for identifying intents, slots, and nested intents, and combines\nLLM generations to create meaning representation. First, we identify top-level intent. Next, we prompt for each slot.\nIf a slot can accommodate nested intents (in red), we prompt for nested slots and infer nested intents.\nschema entities. Our work is also related to ap-\nproaches towards zero-shot dialog state tracking\nusing LLMs (Gao et al., 2020; Lin et al., 2021a,b).\nSpecifically, Lin et al. (2021a) uses an Abstainer\nto handle missing slots. Our method differs in that,\nwe focus on semantic parsing where the Abstainer\nneeds to be applied multiple times along with intent\ndetection to create nested meaning representations.\n3 Z EROTOP: Zero-Shot Task-Oriented\nSemantic Parsing\nProblem Formulation We focus on task-\noriented parsing with hierarchical intent-slot\nschema. Let I = {I1,I2,..., In}and S =\n{S1,S2,..., Sm}be the set of all possible top-\nlevel intents and slots respectively. Each intent\nIj has a set of slots Sj = {Sj\n1,Sj\n2,..., Sj\nn}that\ncan be filled. Possible slots in an intent are rep-\nresented by the intent-to-slot mapping I2S: I→\nP(S), where P(·) is the powerset operator. Sim-\nilarly, the inverse slot-to-intent mapping is repre-\nsented by S2I: S→I . The input in our setting\nconsists of I2S and S2I, but no annotated data.\nZEROTOP requires users to provide a question per\nslot Q = {QS1,QS2,..., QSk }, that represents\ntheir purpose. In a real-life setting, this can be\nobtained from a domain developer.\nUnconstrained Generation for Zero-Shot Intent\nClassification We view zero-shot intent classi-\nfication as an abstractive QA problem. One intu-\nitive way is to prime the LLM with a QA prompt\nand then constrain the generation to search over\nonly valid intent labels (Shin et al., 2021). How-\never, LLMs are known to be biased towards text\nsequences (Zhao et al., 2021) more common in pre-\ntraining data. For example, in the MTOP dataset,\nthe T0-3B model predicts CREATE_CALL (make\nAlgorithm 1: ZEROTOP: Our proposed\nZero-shot semantic parsing method.\nInput: Set of intents I, Set of slots S, Slot questions\nQ, intent-to-slot mapping I2S, slot-to-intent\nmapping S2I, slot-to-candidate-nested-intent\nmapping S2NI, Intent-model MI, Abstainer Mabs,\nand Utterance u\nOutput: Predicted meaning representations MR\nintent = MI(u)\nslotValues = {}\nfor slot Si ∈I2S(intent) do\nslotValues[Si] = Mabs(u,QSi )\nfor candidate N.intent Ij ∈S2NI(Si) do\nfor slot Sj ∈I2S(Ij) do\nif Mabs(slotValues[Si],QSj ) is not\nNONE then\nUpdate slotValues[Si] with\nnested intent Ij,\nMabs(slotValues[Si],QSj )\nMR = Construct representation with intent,\nslotValues\nReturn MR\ncall) as intent for 92% of the data in the call do-\nmain. Therefore, we propose to first generate an\nintent description in an unconstrained fashion by\npriming the LM with the following prompt.\nAnswer the following question depending on the context.\ncontext: A user said, {utterance}.\nquestion: What did the user intend to do?\nanswer:\nThen, we choose the intent label that is most simi-\nlar to generated answer using RoBERTa sentence\nsimilarity (Reimers and Gurevych, 2019). Unlike\nZhao et al. (2022), our approach does not require\nenumerating choices in the prompt allowing us to\nhandle a large number of intents and slots as found\nin datasets like MTOP.\nLeveraging QA datasets for Slot Value Predic-\ntion Slot value prediction involves extracting\nphrases for a slot from the user utterance. We cast\nthis as an extractive QA problem. All slots might\n5793\nnot be mentioned in an input utterance. For exam-\nple, in the MTOP dataset, on average, only one-\nthird of possible slots are mentioned per utterance.\nThe QA model needs to abstain from prediction\nfor such missing slots. To analyze the abstaining\ncapability of pre-trained QA models, we consider a\nfew top-performing zero-shot LLMs T0-3B, GPT-\n3, and Codex with their corresponding prompts and\nexperiment on a 500 sample subset of unanswer-\nable questions from the SQuAD dataset (Rajpurkar\net al., 2018). We observe the accuracy of all mod-\nels to be < 5% and notice that they frequently\nhallucinate and generate answers for unanswerable\nquestions. In section 4, we also consider a log-\nlikelihood-based threshold for abstaining and show\nthat this threshold is difficult to tune using public\nQA datasets.\nTo address this challenge, we leverage multiple\npublicly available QA datasets1 to train Abstainer,\na QA model capable of abstaining from prediction.\nSpecifically, we generate synthetic unanswerable\ntraining samples by modifying existing QA data,\nand train a QA model jointly on existing datasets\nand synthetic unanswerable questions. For every\n(question, answer, context) triplet, we generate syn-\nthetic unanswerable questions by either (1) remov-\ning the sentence containing the answer span from\nthe context, or (2) randomly sampling a context\nthat doesn’t have the same question. After training\nthe Abstainer, we prompt it for each slot with its\ncorresponding question for slot value prediction, in\nthe following format:\nAnswer the following question depending on the context.\ncontext: A user said, {utterance}.\nquestion: {slot question}\nanswer:\nNested Intents To identify nested intents, we as-\nsume knowledge of candidate nested intents that\ncan be accommodated by each slot, represented by\nthe slot-to-candidate-nested-intent mapping S2NI:\nS→P (I). Our method assumes that depth of out-\nput representations is at most 4 i.e. nested intents\ncannot further have more nested intents. One intu-\nitive way is to prompt the LLM for nested intent\nwith the intent prediction prompt. However, our un-\nconstrained generation-based intent model would\npredict many false positive nested intents. We in-\nstead use Abstainer to prompt for their respective\nslots. If any slot value is identified, we consider its\ncorresponding intent via S2I to be present as well.\n1The QA datasets details are mentioned in Appendix A.1\nZEROTOP: Putting it all together The pseudo-\ncode of ZEROTOP is mentioned in Algorithm-1.\nZEROTOP employs a top-down, greedy prompting\nstrategy, where we first prompt for intent and then,\nits respective slots. First, we obtain the top-level in-\ntent using the intent model. Based on the predicted\nintent, we prime the Abstainer for corresponding\nslots using their respective questions as prompts.\nFor each identified slot value, we prompt the Ab-\nstainer for slots of candidate nested intents. We\nuse the same prompt format for this step with the\nidentified slot value now considered as the input\nutterance. Finally, we combine predicted intent,\nidentified slot values, and nested intents to create\nthe meaning representation.\n4 Experiments\nWe experiment on the English language subset of\nMTOP (Li et al., 2021) dataset. MTOP is a multi-\nlingual task-oriented semantic parsing dataset com-\nprising data from 6 languages and 11 domains. The\ntest set has 4386 samples with 113 distinct intents\nand 74 slots. On average, each intent has 3.6 slots\nand 33% of possible slots are filled per utterance.\nExperiment Settings. We evaluate on the zero-\nshot setting, therefore we have no training data.\nWe manually create questions for slotsQusing one\nexample per slot. For training Abstainer, we fine-\ntune T0-3B on the extractive and abstractive QA\ndatasets for 1 epoch with a constant learning rate\nof 10−4. We use complete meaning representation\nmatch accuracy as the performance metric. More\ndetails in Appendix A.2.\nBaselines. We compare ZEROTOP with con-\nstrained T0-3B, GPT-3and Codex as intent mod-\nels where they are primed with intent generation\nprompt and are constrained to search over valid\nintent labels. We also compare with calibrated\nconstrained T0-3B(Zhao et al., 2021) whose log-\nits are adjusted to counter LM biases. We con-\nsider an ablation of ZEROTOP where we assign in-\ntent labels based only on their similarity with user\nutterance using RoBERTa sentencetransformer.\nZEROTOP-Intent represents our proposed intent\nprediction method.\nWe compare with constrained T0-3B, GPT-3,\nand Codex as slot models as well, however, when\nprimed with a question corresponding to a slot, the\noutput is constrained to be either from the utter-\nance or from their corresponding phrases indicat-\ning that question cannot be answered. We com-\n5794\nIntent Model Accuracy(%)\nT0-3B constrained 34.02\nT0-3B constrained calibrated 36.64\nGPT-3 constrained 40.44\nCodex constrained 48.02\nRoBERTa sentence 47.14\nZEROTOP-Intent 49.58\nTable 1: Top-level intent classification results.\nIntent Model Slot Model Acc(%)\nGPT-3 constrained GPT-3 constrained 3.00*\nCodex constrained Codex constrained 5.40*\nT0-3B constrainedT0-3B constrained 2.42\nAbstainer 11.81\nRoBERTa sentenceT0-3B constrained 3.88\nAbstainer 12.90\nZEROTOP-Intent\nT0-3B constrained 4.10\nT0-3B constrained-MTQA 4.63\nT0-3B constrained-SEQZERO8.39\nAbstainer-MTQA 13.12\nAbstainer-SEQZERO 13.68\nAbstainer 15.89\nTable 2: Complete meaning representation match eval-\nuation. To limit API cost, we limit GPT-3 and Codex\nevaluation on a 500-example subset, and hence their\nresults are not directly comparable.\npare with two kinds of prompting for slot values.\nMTQA (Zhao et al., 2022) propose prompting an\nLLM to identify filled slots and then prime for\ntheir respective values. SEQZERO (Yang et al.,\n2022) introduce prompting each slot sequentially\nand using the previously identified slot value for\nprompting the next one. Abstainer is our finetuned\nT0-3B that abstains from prediction. We prompt\nfor each slot independently.\nResults and Discussion From zero-shot intent\nclassification results in Table 1, we observe\nthat ZEROTOP-Intent performs significantly bet-\nter than constrained T0-3B, GPT-3, and Codex.\nWe found that constrained T0-3B is biased to-\nwards certain labels. For example, it predicts\nCREATE_CALL (make call), SEND_MESSAGE\n(send message), CREATE_REMINDER (create re-\nminder), as intent for more than 90% of the data in\ncall, message, reminder domains respectively. Our\nproposed unconstrained formulation lets the model\nfreely express the intent and, computing similarity\nlater with the intent labels addresses this bias.\nAs shown in Table 2, the combination of\nZEROTOP-Intent and Abstainer demonstrates su-\nperior performance than alternative combinations.\nWe observe that T0-3B, GPT-3, and Codex fail to\nabstain frequently. The T0-3B model abstains only\nfor 38% of unanswerable slot questions whereas\nAbstainer does for 89%. As a result, we observe\na notable performance gain by plugging in Ab-\nstainer as the slot model for each intent model base-\nline. Moreover, we observe MTQA and SEQZERO\nprompting methods, which are originally proposed\nfor custom-finetuned & few-shot models, offer lit-\ntle assistance in zero-shot settings. For instance,\nwhen applying MTQA prompting to the T0-3B\nmodel, we observe marginal improvements. This\nindicates that the pre-trained T0-3B is unable to\naccurately identify fillable slots, demonstrating the\nnecessity of Abstainer. Similarly, while SEQZERO\nprompting improves the performance of T0-3B,\nits effectiveness remains significantly inferior to\nAbstainer. Finally, we see similar performance of\nMTQA and SEQZERO prompting with Abstainer.\nHowever, our approach of independently prompt-\ning for each slot outperforms them.\nAnnotation Effort Analysis We use 74 samples\ni.e. one per slot to design questions for slots. To\nanalyze annotation effort, we train an utterance-to-\nmeaning representation T5-3B (Raffel et al., 2020)\nparser using these 74 samples and compare it with\nour method. The match accuracy of T5-3B parser\non the MTOP dataset is 8.19% and of ZEROTOP\nis 15.89%, justifying our annotation effort.\nGreedy vs Beam search ZEROTOP follows a\ngreedy strategy where we hierarchically prompt for\ntop-level intent and for its corresponding slots. We\ncompare it with the beam search strategy with beam\nsize 3. Specifically, we consider 3 top-level intents\nand prompt for their corresponding slots, consider\ntop-3 slot values for every slot and finally compute\nthe best meaning representation based on their ag-\ngregated NLL scores. The NLL score of intent Im,\nits slots Sj ∈I2S(Im), and their corresponding\nslot values slotValues[Sj] is computed:\nαlog p(Im) + (1−α)\n∑\nSj\nlog p(slotValues[Sj]|Im)\nwhere αis tuned on a held-out validation set. Note\nthat p(slotValues|Im) is computed recursively for\nits nested intents. The complete match accuracy of\nthe greedy prompting strategy on MTOP dataset\nis 15.89% and of beam search strategy is 16.86%.\nThis demonstrates that beam search can improve\nperformance with validation data. Without valida-\ntion data and setting αto 0.5, performance drops\n5795\n1 2 3 4 5\nNLL Threshold\n5\n10\n15\n20\n25\n30\n35\n40F1 score\nF1 score vs NLL Threshold on MTOP-EN\nT0-3B\nGPT3\nCodex\nAbstainer\n(a) F1 score vs NLL threshold on MTOP dataset\n1 2 3 4 5\nNLL Threshold\n0\n5\n10\n15\n20\n25F1 score\nCodex F1 vs NLL Threshold on SQuAD\nAnswerable\nUnanswerable\nAll\n(b) F1 score vs NLL threshold on SQuAD dataset\nFigure 2: We consider negative log-likelihood (NLL) as\na confidence score and vary the threshold to abstain from\nprediction and plot F1 scores on the MTOP dataset. We\nshow that this NLL threshold is difficult to tune using\npublic QA datasets such as SQuAD as performance\non answerable and unanswerable subsets is mutually\nexclusive.\nto 12.36% i.e. 3% less than greedy. Therefore, we\nbelieve greedy prompting is a better choice.\nConfidence score-based Abstainer study We\ncan alternatively have LLMs abstain from predic-\ntion based on a confidence score based threshold.\nWe consider negative log likelihood (NLL) of the\npredicted slot value as the confidence score and\nabstain from prediction if it is greater than the\nthreshold. We experiment on slot value predic-\ntion task with T0-3B, Codex, and GPT3 as LLMs\nand plot macro F1 scores for multiple NLL thresh-\nolds on a randomly sampled subset of 500 samples\nfrom MTOP dataset in Figure 2(a). Specifically,\nwe consider the gold intent of each sample and\nprime LLM for extracting slot values for each slot\nof the gold intent. We consider F1 score as the met-\nric due to the label imbalance across possible slot\nvalues. We present the F1-score of the Abstainer\nfor reference. First, we observe that Abstainer is\nsignificantly better than T0-3B and GPT3 for all\nconfidence thresholds. Second, we notice that there\nis no threshold that consistently results in good per-\nformance for all LLMs, which implies that this has\nto be individually tuned for each LLM. Finally, we\nobserve Codex performs better than Abstainer for\nsome thresholds. As our problem setting includes\nno annotated data, we investigate whether we can\ninfer the optimal threshold for Codex using public\nQA datasets. Specifically, we consider 500 answer-\nable and 500 unanswerable QA pairs from SQuAD\ndataset and plot F1 scores with a range of confi-\ndence thresholds in Figure 2(b). We can observe\nthat the performance on answerable and unanswer-\nable subsets is mutually exclusive i.e. there is no\nthreshold where the performance on both answer-\nable and unanswerable subsets is high. The range\nof thresholds that result in the best performance\non the whole set (highlighted in green) does not\ntransfer to MTOP and is achieved at the cost of\nunanswerable set where the F1 score is less than\n5%. Given the difficulty in tuning threshold and\nthe API costs of Codex, we believe using Abstainer\nas the slot model to be a better choice.\n5 Conclusion\nIn this paper, we propose ZEROTOP that decom-\nposes semantic parsing into abstractive and extrac-\ntive QA tasks. ZEROTOP identifies top-level intent\nby generating in an unconstrained fashion and infer-\nring the intent label most similar to the generated\ndescription. We train Abstainer using public QA\ndatasets, that is capable of identifying unanswer-\nable questions and abstaining from prediction.\n6 Limitations\nZEROTOP assumes that the meaning representa-\ntions are of a limited depth i.e. nested intents can-\nnot further have more nested intents and this is\none of the limitations. Moreover, we also assume\nthat it is possible to write natural questions corre-\nsponding to slots. A slot for which a natural ques-\ntion cannot be expressed, the LLM can’t handle it\nwithout additional supervision. Finally, we believe\nthere is a huge scope for improvement in the perfor-\nmance of LLMs and ZEROTOP in domain-specific\ntasks such as zero-shot semantic parsing and on the\nMTOP dataset.\n7 Ethics Statement\nThis paper proposes a zero-shot semantic parsing\nmethod using large language models. The aim\nof the paper is to minimize the human effort in\n5796\nannotation by leveraging language models. The\noutput of our method is a meaning representation\nthat doesn’t contain any harmful content. Hence,\nwe do not anticipate any major ethical concerns.\n8 Acknowledgments\nWe thank the anonymous reviewers and our col-\nleagues from Microsoft Semantic Machines, espe-\ncially Hao Fang, Richard Shin, Adam Pauls, Matt\nGardner, and Jason Eisner for their helpful feed-\nback.\nReferences\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebas-\ntian Riedel, and Pontus Stenetorp. 2020. Beat the ai:\nInvestigating adversarial human annotation for read-\ning comprehension. Transactions of the Association\nfor Computational Linguistics, 8:662–678.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nPradeep Dasigi, Nelson F. Liu, Ana Marasovi´c, Noah A.\nSmith, and Matt Gardner. 2019. Quoref: A read-\ning comprehension dataset with questions requiring\ncoreferential reasoning. In EMNLP.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003.\nShuyang Gao, Sanchit Agarwal, Tagyoung Chung,\nDi Jin, and Dilek Hakkani-Tur. 2020. From machine\nreading comprehension to dialogue state tracking:\nbridging the gap. In ACL 2020 Workshop on NLP for\nConversational AI.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n643–653, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950–2962, Online. Association for Computa-\ntional Linguistics.\nKevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-\nner. 2019. Reasoning over paragraph effects in situa-\ntions. ArXiv, abs/1908.05852.\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan\nMoon, Zhenpeng Zhou, Paul A Crook, Zhiguang\nWang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al.\n2021a. Zero-shot dialogue state tracking via cross-\ntask transfer. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7890–7900.\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A\nCrook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,\nAndrea Madotto, Eunjoon Cho, and Rajen Subba.\n2021b. Leveraging slot descriptions for zero-shot\ncross-domain dialogue statetracking. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5640–5648.\nDheeraj Mekala, Chengyu Dong, and Jingbo Shang.\n2022a. LOPS: Learning order inspired pseudo-label\nselection for weakly supervised text classification.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 4894–4908, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nDheeraj Mekala, Varun Gangal, and Jingbo Shang.\n2021. Coarse2Fine: Fine-grained text classification\non coarsely-grained annotated data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 583–594, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nDheeraj Mekala and Jingbo Shang. 2020. Contextu-\nalized weak supervision for text classification. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 323–\n333.\nDheeraj Mekala, Tu Vu, Timo Schick, and Jingbo Shang.\n2022b. Leveraging qa datasets to improve generative\ndata augmentation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nDheeraj Mekala, Xinyang Zhang, and Jingbo Shang.\n2020. Meta: Metadata-empowered weak supervi-\nsion for text classification. In Proceedings of the\n5797\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nJoshua Robinson and David Wingate. 2023. Leveraging\nlarge language models for multiple choice question\nanswering. In The Eleventh International Conference\non Learning Representations.\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and\nKarthik Sankaranarayanan. 2018. DuoRC: Towards\nComplex Language Understanding with Paraphrased\nReading Comprehension. In Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nNathan Schucher, Siva Reddy, and Harm de Vries. 2022.\nThe power of prompt tuning for low-resource seman-\ntic parsing. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 148–156, Dublin,\nIreland. Association for Computational Linguistics.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\nand Claire Cardie. 2019. Dream: A challenge data\nset and models for dialogue-based reading compre-\nhension. Transactions of the Association for Compu-\ntational Linguistics, 7:217–231.\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\nClark. 2019. Quartz: An open-domain dataset of\nqualitative relationship questions. arXiv preprint\narXiv:1909.03553.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nWenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-\nrni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and\nWilliam Yang Wang. 2019. TWEETQA: A social\nmedia focused question answering dataset. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5020–\n5031, Florence, Italy. Association for Computational\nLinguistics.\nJingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing\nZhang, Bing Yin, and Diyi Yang. 2022. SEQZERO:\nFew-shot compositional semantic parsing with se-\nquential prompts and zero-shot models. In Findings\nof the Association for Computational Linguistics:\nNAACL 2022, pages 49–60, Seattle, United States.\nAssociation for Computational Linguistics.\nWenting Zhao, Konstantine Arkoudas, Weiqi Sun, and\nClaire Cardie. 2022. Compositional task-oriented\nparsing as abstractive question answering. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4418–4427, Seattle, United States. Association for\nComputational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n5798\nType Dataset # Samples\nExtractive\nAdversarial QA (Bartolo et al., 2020)36000\nQA-SRL (He et al., 2015) 8597\nDuoRC (Saha et al., 2018) 186089\nROPES (Lin et al., 2019) 14000\nSQuADv2 (Rajpurkar et al., 2018)150000\nQuoref (Dasigi et al., 2019) 24000\nAbstractive\nReCoRD (Wang et al., 2019) 121000\nDREAM (Sun et al., 2019) 10197\nQuaRTz (Tafjord et al., 2019) 3864\nTweet-QA (Xiong et al., 2019) 10692\nTable 3: Relevant statistics of the QA dataset used to\ntrain Abstainer.\nA Appendix\nA.1 QA Datasets for Training Abstainer\nPublicly available QA datasets have been\npreviously leveraged for generating synthetic\ndata (Mekala et al., 2022b) in weakly (Mekala and\nShang, 2020; Mekala et al., 2020) and minimally\nsupervised settings (Mekala et al., 2021, 2022a). In\nthis paper, we use multiple extractive and abstrac-\ntive QA datasets to generate synthetic unanswer-\nable samples and train Abstainer. The details about\ndatasets are mentioned in Table 3.\nA.2 Experimental Settings\nWe use the OpenAI API text-davinci-001\nfor GPT-3 and code-davinci-002 for Codex.\nThe Abstainer is fine-tuned on 411732 answerable\nand 435898 unanswerable samples. The batch size\nis 32 and each batch contains an equal number of\nanswerable and unanswerable samples. We used 8\n×NVIDIA Tesla V100 for our experiments.\nA.3 Frequently Asked Questions\nWhat is the scope of the presented ideas? We\nbelieve our idea can be easily extended to any se-\nmantic parsing tasks involving natural language\ninterfaces; we considered Task-oriented parsing\nas a first step because of its simpler representa-\ntion. Through this work, we wanted to highlight an\nimportant real-life task that LLMs such as GPT3,\nCodex, and T0 underperform. Therefore, we be-\nlieve this is useful and hope our work motivates\nmore researchers to focus on this shortcoming.\nWhy didn’t you extract all syntactic phrases of a\ncertain type in the tree for slot-value detection?\nSyntactic phrase-based extraction of slot values re-\nquires users to manually enter rules (a.k.a. labeling\nfunctions) for each slot. When the number of slots\nincreases (e.g. 74 in MTOP), it demands significant\nmanual effort from users, which our paper aims to\nreduce. Moreover, such rules would generally pre-\ndict many false positives/negatives, and classifying\nor identifying the appropriate ones accurately re-\nquires training data, which is not available in our\nzero-shot setting. Therefore, we compare against\nstrong LLMs that are known for their impressive\nzero-shot performance such as GPT3, Codex, T0.\nThe baseline LLMs such as GPT3 and Codex\nare not trained for semantic parsing. Wouldn’t\nthis make the performance improvement us-\ning ZEROTOP less significant? We compare\nagainst the T0 model which is fine-tuned on several\nQA datasets like our Abstainer model. We cannot\nfine-tune the GPT3 and Codex models on these\ndatasets separately. However, these instruction-\ntuned GPT3 and Codex are known to perform\nwell on several question-answering & reading com-\nprehension benchmarks (Robinson and Wingate,\n2023). Therefore, we consider them as competitive\nbaselines. In this paper, we show that these per-\nform worse on zero-shot task-oriented parsing even\nwhen it is converted into a QA task, for which they\nare known to perform well. The reason behind their\npoor performance is because (1.) they are biased to-\nward predicting labels common in the pre-training\ndata, and (2.) they frequently hallucinate text for\nunanswerable questions. Through our work, we\npresent this shortcoming, analyze the cause, and\npropose a method to fix it.\nWhy did you choose one prompt per slot and\nnot multiple? We can possibly consider multi-\nple prompts per slot and ensemble the predictions,\nwhich would intuitively boost the performance.\nHowever, multiple prompts per slot imply more\nannotations. Our motivation behind this work is to\nminimize the human annotations, thus we chose a\nsingle prompt per slot.\n5799",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.841275691986084
    },
    {
      "name": "Computer science",
      "score": 0.8105172514915466
    },
    {
      "name": "Natural language processing",
      "score": 0.7089508175849915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6594889163970947
    },
    {
      "name": "Task (project management)",
      "score": 0.6364228129386902
    },
    {
      "name": "Shot (pellet)",
      "score": 0.61360764503479
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.574726939201355
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5118321776390076
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5046271085739136
    },
    {
      "name": "Bottom-up parsing",
      "score": 0.4760803282260895
    },
    {
      "name": "Top-down parsing",
      "score": 0.45904654264450073
    },
    {
      "name": "Utterance",
      "score": 0.43079033493995667
    },
    {
      "name": "Linguistics",
      "score": 0.2685813903808594
    },
    {
      "name": "Programming language",
      "score": 0.18885371088981628
    },
    {
      "name": "Psychology",
      "score": 0.07434126734733582
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210161460",
      "name": "OpenAI (United States)",
      "country": "US"
    }
  ]
}