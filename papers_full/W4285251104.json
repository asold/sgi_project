{
    "title": "How does the pre-training objective affect what large language models learn about linguistic properties?",
    "url": "https://openalex.org/W4285251104",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4320554684",
            "name": "Ahmed Alajrami",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A93365683",
            "name": "Nikolaos Aletras",
            "affiliations": [
                "University of Sheffield"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3104453885",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2515741950",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3035027743",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W3170826848",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3152698349",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3159900299",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W3037458976",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3114268635",
        "https://openalex.org/W3197120431",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963430224",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3104235057"
    ],
    "abstract": "Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 131 - 147\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nHow does the pre-training objective affect what large language models\nlearn about linguistic properties?\nAhmed Alajrami and Nikolaos Aletras\nDepartment of Computer Science\nUniversity of Sheffield, UK\n{ajsalajrami1, n.aletras}@sheffield.ac.uk\nAbstract\nSeveral pre-training objectives, such as masked\nlanguage modeling (MLM), have been pro-\nposed to pre-train language models (e.g. BERT)\nwith the aim of learning better language repre-\nsentations. However, to the best of our knowl-\nedge, no previous work so far has investi-\ngated how different pre-training objectives af-\nfect what BERT learns about linguistics prop-\nerties. We hypothesize that linguistically mo-\ntivated objectives such as MLM should help\nBERT to acquire better linguistic knowledge\ncompared to other non-linguistically motivated\nobjectives that are not intuitive or hard for hu-\nmans to guess the association between the input\nand the label to be predicted. To this end, we\npre-train BERT with two linguistically moti-\nvated objectives and three non-linguistically\nmotivated ones. We then probe for linguistic\ncharacteristics encoded in the representation\nof the resulting models. We find strong evi-\ndence that there are only small differences in\nprobing performance between the representa-\ntions learned by the two different types of ob-\njectives. These surprising results question the\ndominant narrative of linguistically informed\npre-training.1\n1 Introduction\nThe most popular way to pre-train a transformer-\nbased (Vaswani et al., 2017) language model (LM),\ne.g. BERT (Devlin et al., 2019), is by optimizing a\nmasked language modeling (MLM) objective. The\nMLM task was inspired by the Cloze Task (Taylor,\n1953), where humans were asked to guess omitted\nwords in a sentence using its context, knowledge\nof syntax and other skills. The premise is that such\nan objective will guide a LM to encode linguistic\ninformation.\nApart from MLM, different types of objectives\nhave been recently proposed. Yang et al. (2019)\n1Code and models are available here: https://gith\nub.com/aajrami/acl2022-pre-training-obje\nctives-probing\nintroduced a pre-training objective based on token\norder permutations. Clark et al. (2020) proposed\na Replaced Token Detection pre-training task, that\nuses the output of a small MLM to corrupt the in-\nput by replacing some of the tokens. It then trains\na discriminative model to predict if a token has\nbeen replaced or not. Aroca-Ouellette and Rudzicz\n(2020) explored various sentence and token-level\nauxiliary pre-training tasks (e.g. sentence ordering,\nterm-frequency prediction), as better alternatives to\nthe next sentence prediction (NSP) auxiliary task\noriginally used to train BERT. Lan et al. (2020)\nintroduced the sentence-order prediction task that\nfocuses on the inter-sentence coherence, by predict-\ning if two contiguous sentences have been swapped\nor not. Iter et al. (2020) proposed another inter-\nsentence pre-training task, that helps LMs to en-\ncode discourse relationships between sentences us-\ning contrastive learning. Yamaguchi et al. (2021)\nshowed that a non-linguistically intuitive task (i.e.\nmasked first character prediction) can effectively\nbe used for pre-training.\nMeanwhile, several studies have explored how\nwell and to what extent LMs learn linguistic in-\nformation. This is usually examined using prob-\ning tasks, i.e. simple classification tasks that test\nthe LM’s encodings for a single linguistic fea-\nture such as grammatical information. It has been\nfound through probing that BERT encodes syn-\ntactic (Tenney et al., 2019; Liu et al., 2019; Mi-\naschi and Dell’Orletta, 2020; Hewitt and Manning,\n2019; Jawahar et al., 2019) and semantic informa-\ntion (Ettinger, 2020; Jawahar et al., 2019; Tenney\net al., 2019). However, Hall Maudslay and Cot-\nterell (2021) argue that BERT’s syntactic abilities\nmay have been overestimated.\nIn this paper, we hypothesize that linguistically\nmotivated objectives (e.g. MLM) should help\nBERT to acquire better linguistic knowledge com-\npared to using non-linguistically motivated objec-\ntives, i.e. tasks that are hard for humans to guess\n131\nthe association between the input and the label to\nbe predicted. To this end, we seek to answer the\nfollowing research question: How does the pre-\ntraining objective affect what LMs learn about the\nEnglish language?\nOur findings challenge the MLM status quo,\nshowing that pre-training with non-linguistically\ninformative objectives (§2) results in models with\ncomparable linguistic capabilities, as measured by\nstandard probing benchmarks (§3). These surpris-\ning results (§4) suggest that careful analysis of how\nLMs learn is critical to further improve language\nmodeling (§5).\n2 Pre-training Objectives\nWe experiment with five different pre-training ob-\njectives. Two of them are considered linguistically\nmotivated while the rest are not.\n2.1 Linguistically Motivated Objectives\nMasked Language Modeling (MLM): We use\nMLM as our first linguistically motivated pre-\ntraining objective. First introduced by Devlin et al.\n(2019), MLM randomly chooses 15% of the tokens\nfrom the input sentence and replaces 80% of them\nwith a [MASK] token, 10% with a random token,\nand 10% remain unchanged.\nManipulated Word Detection (S+R): We also\nexperiment with a simpler linguistically motivated\nobjective, where the model selects and replaces\n10% of input tokens with shuffled tokens from\nthe same input sequence. Concurrently, it selects\nand replaces another 10% of input tokens with ran-\ndom tokens from the vocabulary (Yamaguchi et al.,\n2021).\n2.2 Non-Linguistically Motivated Objectives\nWe assume that tasks that are hard for humans (such\nas a completely random prediction task) will make\nless likely the deeper layers of BERT (i.e. closer to\nthe output layer) to acquire meaningful information\nabout language. We also hypothesize that layers\ncloser to the input might learn word co-occurrence\ninformation (Sinha et al., 2021).\nMasked First Character Prediction (First Char):\nFor our first non-linguistically motivated pre-\ntraining objective, we use the masked first char-\nacter prediction introduced by Yamaguchi et al.\n(2021). In this task, the model predicts only the\nfirst character of the masked token (e.g. ‘[c]at’ and\n‘[c]omputer’ belong to the same class). The model\npredicts the first character as one of 29 classes, in-\ncluding the English alphabet and digit, punctuation\nmark, and other character indicators.\nMasked ASCII Codes Summation Predic-\ntion (ASCII): We also propose a new non-\nlinguistically motivated pre-training objective,\nwhere the model has to predict the summation of\nthe ASCII code values of the characters in a masked\ntoken. To make this harder and keep the number of\nclasses relatively small, we define a 5-way classi-\nfication task by taking the modulo 5 of the ASCII\nsummation: V = [P\ni ascii(chari)] %5. Guess-\ning the association between the input and such la-\nbel, is an almost impossible task for a human.\nMasked Random Token Classification (Ran-\ndom): Finally, we propose a completely random\nobjective where we mask 15% of the input tokens\nand we assign each masked token a class from 0 to\n4 randomly for a 5-way classification similar to the\nASCII task. We assume that a model pre-trained\nwith a random objective should not be able to learn\nanything meaningful about linguistic information.\n3 Probing Tasks\nProbing tasks (Adi et al., 2016; Conneau et al.,\n2018; Hupkes et al., 2018) are used to explore in\nwhat extent linguistic properties are captured by\nLMs. A model is normally trained, using the repre-\nsentations of a language model, to predict a specific\nlinguistic property. If it achieves high accuracy, it\nimplies that the LM encodes that linguistic prop-\nerty. In this work, we use nine standard probing\ntasks introduced by Conneau et al. (2018) to ex-\namine the representation output for each layer of\nthe different LMs we pre-train following Shen et al.\n(2020). These tasks probe for surface, syntactic\nand semantic information. The dataset for each\nprobing task contains 100k sentences for training,\n10k sentences for validation and another 10k sen-\ntences for testing.2 We train a multi-layer percep-\ntron (MLP) classifier for each probing task using\nthe recommended hyperparameters in the SentEval\ntoolkit (Conneau and Kiela, 2018).\nSurface information task: SentLen aims for\ncorrectly predicting the number of words in a sen-\ntence.\n2The datasets are all publicly available by Conneau and\nKiela (2018).\n132\nModel MNLI QNLI QQP RTE SST MRPC CoLA STS GLUE Avg.\nBASE - 40 Epochs Pre-training (Upper Bound)\nMLM + NSP 83.8 90.8 87.8 69.9 91.9 85.0 58.9 89.3 82.1 (0.4)\nBASE - 500k Steps Pre-training\nMLM 81.4 89.0 86.5 65.1 90.6 86.0 52.8 87.2 79.8 ± 0.3\nS+R 79.2 88.1 86.0 67.7 88.5 85.9 55.8 87.2 79.8 ± 0.3\nFirst Char 78.8 87.2 85.4 60.0 89.1 83.5 44.5 85.1 76.7 ± 0.4\nASCII 76.8 85.3 84.3 60.8 87.9 82.2 42.0 82.4 75.2 ± 0.3\nRandom 67.5 63.3 74.9 53.5 81.7 71.8 15.1 23.3 56.4 ± 0.4\nMEDIUM - 250k Steps Pre-training\nMLM 78.3 85.6 85.2 62.2 90.0 82.0 44.3 84.0 76.4 ± 0.4\nS+R 76.2 85.5 84.8 62.5 86.5 79.8 46.1 84.4 75.7 ± 0.1\nFirst Char 77.7 85.7 85.4 58.8 88.7 82.6 37.4 83.5 75.0 ± 0.3\nASCII 75.1 84.4 83.8 56.6 87.1 80.5 34.8 81.2 72.9 ± 0.4\nRandom 72.9 81.4 83.1 54.7 84.0 73.7 27.3 76.9 69.3 ± 0.5\nSMALL - 250k Steps Pre-training\nMLM 75.8 84.6 84.4 59.7 89.0 81.7 38.7 83.6 74.7 ± 0.4\nS+R 75.1 84.2 84.4 55.8 85.6 76.0 36.6 82.5 72.5 ± 0.2\nFirst Char 74.5 83.3 84.5 56.3 87.3 78.4 35.4 81.4 72.6 ± 0.4\nASCII 72.9 82.3 83.1 55.7 87.0 72.2 32.8 77.1 70.4 ± 0.2\nRandom 70.7 81.0 82.4 54.4 84.2 72.5 23.4 76.2 68.1 ± 0.6\nTable 1: Results on GLUE dev sets with standard deviations over five runs.Bold values denote the best performance\nacross each GLUE task and GLUE Avg. for each model setting.\nSyntactic information tasks: TreeDepth tests\nif the representations preserve information about\nthe hierarchical structure of a sentence, by predict-\ning the depth of its parse tree. TopConst predicts\nthe top constituents of the parse tree of a sentence.\nBShift tests if two adjacent words have been in-\nverted or not.\nSemantic information tasks: Tense aims to pre-\ndict if the main-clause verb is present or past. Sub-\njNum predicts if the subject of the main clause\nis singular or plural. ObjNum tests if the direct\nobject of the main clause is singular or plural. Se-\nmantic Odd Man Out (SOMO) tests if a noun or\nverb has been replaced with another noun or verb.\nCoordInv predicts if a sentence made of two coor-\ndinate clauses has been inverted or not.\n4 Experiments & Results\n4.1 Experimental Setup\nModels We pre-train BERT-BASE (Devlin et al.,\n2019) models by replacing MLM and the next sen-\ntence prediction (NSP) objectives, with one of the\nlinguistically or non-linguistically motivated pre-\ntraining objectives (§2). For completeness, we also\npre-train two smaller model architectures,MEDIUM\nand SMALL from (Turc et al., 2019) as in Yam-\naguchi et al. (2021). The MEDIUM model has\neight hidden layers and eight attention heads. The\nSMALL model has four hidden layers and eight at-\ntention heads. Both, MEDIUM and SMALL , models\nhave feed-forward layers of size 2048 and hidden\nlayers of size 512. More details on hyperprameters\ncan be found in Appendix A.\nPre-training Data All models are pre-trained on\nthe BookCorpus (Zhu et al., 2015) and English\nWikipedia from Hugging Face. 3 The text is tok-\nenized using Byte-Pair-Encoding (Sennrich et al.,\n2016), resulting to a total of 2.7 billion tokens.\nPre-training Details Due to limited computa-\ntional resources, each BASE model is pre-trained\nfor 500k steps, while each MEDIUM and SMALL\nmodel is pre-trained for 250k steps using 8\nNVIDIA Tesla V100 (SXM2 - 32GB). We use a\nbatch size of 32 for BASE , and 64 for MEDIUM\nand SMALL . We optimize the models using Adam\n(Kingma and Ba, 2014).\nFine-tuning Details We use the General Lan-\nguage Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2018) to fine-tune each model\nfor up to 20 epochs with early stopping. For each\nfine-tuning task, we use five different seeds and\n3https://github.com/huggingface/datas\nets\n133\nModel SentLen TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nBASE- Jawahar et al. (2019)\nMLM+NSP 96.2 41.3 84.1 87.0 90.0 88.1 82.2 65.2 78.7\nMLM+NSP(untrained) 92.5 29.8 55.2 50.1 63.8 67.4 63.7 50.6 50.3\nBASE- 500k Steps Pre-training\nMLM 96.0±0.2 41.5±0.6 76.9±0.2 86.5±0.1 88.5±0.7 87.4±1.2 83.8±0.2 61.7±0.5 65.5±0.3\nS+R 92.9 ±0.4 45.2±0.6 83.6±0.2 91.3±0.7 87.8±0.4 88.7±0.2 84.5±0.2 59.6±0.4 69.2±0.3\nFirst Char 93.7 ±2.4 43.4±1.2 81.1±0.3 85.0±0.4 86.0±0.3 88.9±0.1 86.4±0.1 56.5±0.4 66.5±0.8\nASCII 92.9 ±0.4 43.3±0.7 81.4±0.4 82.7±0.3 88.7±0.3 89.1±0.3 84.7±0.5 54.0±0.3 68.5±0.8\nRandom 95.0 ±0.6 39.6±0.6 71.4±1.0 68.9±0.4 72.1±0.5 74.3±0.2 70.3±0.1 50.4±0.3 63.3±0.3\nTable 2: Mean accuracy with standard deviation over three runs for the best performing layer on the probing tasks\nusing BASE models. Bold values denote the best performance across each probing task.\nreport the average. We report matched accuracy for\nMNLI task, Matthews correlation for CoLA task,\nSpearman correlation for STS-B task, accuracy for\nMRPC task, F1 scores for QQP task, and accu-\nracy for all other tasks. The WNLI task is omitted\nfollowing Aroca-Ouellette and Rudzicz (2020).\nBERT Representations In all of the probing\ntasks, we use the BERT representations of the\n[CLS] token at every layer as the input to the prob-\ning classifier.\n4.2 Fine-tuning Results\nTable 1 shows the results of fine-tuning the mod-\nels with all pre-training objectives on GLUE to\nmeasure their performance in downstream tasks.\nFor the BASE model configuration, we observe\nthat linguistically motivated objectives (e.g. MLM,\nS+R) achieve the best performance in downstream\ntasks. However, models pre-trained with non-\nlinguistically motivated objectives (e.g. First Char,\nASCII) still achieve competitive results. As ex-\npected, the model pre-trained using the Random\nobjective obtains the lowest performance with 56.4\nGLUE average score. However, its performance\nis still reasonable in many downstream tasks, sug-\ngesting that the model is able to learn some co-\noccurrence information from the input (Sinha et al.,\n2021; Yamaguchi et al., 2021). Similar behavior\ncan be observed for the other two model configura-\ntions, MEDIUM and SMALL .\n4.3 Probing Results\nTable 2 presents the results of the best performing\nlayer on the nine probing tasks using the representa-\ntions from the BERT-BASE models as inputs to the\nMLP classifier. Similar to the fine-tuning results,\nwe first observe that the predictive performance of\nmodels trained on representations learned using lin-\nguistically motivated objectives (e.g. MLM, S+R)\nachieve the best performance in six out of the nine\nprobing tasks. However, models trained on the rep-\nresentations learned using non-linguistically moti-\nvated objectives (e.g. First Char, ASCII) achieve\nvery competitive results.. For example, in the Top-\nConst probing task, the model pre-trained using\nMLM pre-training objective achieves the best per-\nformance of 83.6%, while the the model pre-trained\nusing ASCII pre-training objective achieves 81.4%.\nSimilar patterns can be observed from the prob-\ning results of the other two model configurations,\nMEDIUM and SMALL (see Tables 3 and 4 respec-\ntively). For instance, in the SentLen probing task in\ntable 3, the difference between the best performing\nMEDIUM model (S+R) and the worst performing\nMEDIUM model (ASCII) is only 3.6%. In the Ob-\njNum probing task in table 4, the SMALL model\npre-trained using a non-linguistically motivated pre-\ntraining objective (ASCII) achieves 84.4%, while\nthe SMALL models pre-trained using linguistically\nmotivated pre-training objectives, MLM and S+R,\nachieve 83.5% and 83.3% respectively.\nThe full results of the probing tasks including all\nlayers can be found in appendix B.\n5 Discussion\nTheoretically, LMs with non-linguistically moti-\nvated objectives would be expected to perform dras-\ntically worse than LMs pre-trained using MLM\nin both downstream tasks and linguistic capabil-\nities. However, our results show that both types\nof LMs have surprisingly close performance (af-\nter fine-tuning on downstream tasks) and linguistic\ncapabilities (after probing them) using the same\ntraining data, architecture and training scheme. We\nspeculate that the pre-training data, and the size of\n134\nModel SentLen TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nMEDIUM- 250k Steps Pre-training\nMLM 92.3±0.2 41.1 ±0.1 76.9 ±0.5 80.8 ±0.1 85.9 ±0.1 86.7 ±0.1 83.7 ±0.5 56.1±0.6 63.5 ±0.7\nS+R 94.0±0.5 42.6±0.2 83.0±0.5 84.6±0.3 85.7 ±0.2 87.9±0.4 81.9 ±0.5 55.8 ±0.3 66.5±1.2\nFirst Char 93.3±0.3 40.4 ±0.5 76.8 ±0.3 80.3 ±0.4 85.8 ±0.5 86.3 ±1.3 83.1 ±0.1 53.8 ±0.6 61.8 ±0.3\nASCII 90.4 ±0.5 40.5 ±0.6 79.6 ±0.2 80.0 ±0.8 87.8±0.5 85.3 ±0.3 83.9 ±0.1 52.7 ±0.4 64.7 ±0.1\nRandom 92.9±0.2 42.4 ±0.8 71.5 ±0.9 74.2 ±0.0 86.1 ±0.1 84.3 ±0.3 85.7±0.3 51.3 ±0.7 61.5 ±0.4\nTable 3: Mean accuracy with standard deviation over three runs for the best performing layer on the probing tasks\nusing MEDIUM models. Bold values denote the best performance across each probing task.\nModel SentLen TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nSMALL- 250k Steps Pre-training\nMLM 93.7±0.4 41.6 ±0.2 73.1 ±0.2 78.3 ±0.1 86.4 ±0.7 83.5 ±0.2 83.5 ±0.1 55.9±0.6 64.0±0.3\nS+R 94.7±0.8 43.3±1.0 76.8 ±0.6 82.1±0.1 86.5±0.2 85.6±0.3 83.3 ±0.5 54.9 ±0.4 63.9 ±0.1\nFirst Char 90.7±0.4 42.3 ±0.4 77.5±0.1 76.2 ±0.2 86.0 ±0.1 84.7 ±0.5 82.9 ±0.7 52.4 ±0.3 64.0±0.6\nASCII 89.9 ±0.3 41.3 ±0.4 74.6 ±0.4 74.6 ±0.1 85.7 ±0.4 84.0 ±0.3 84.4±0.2 52.3 ±0.4 62.5 ±0.1\nRandom 94.1±1.0 42.6 ±0.5 75.8 ±0.4 71.0 ±0.4 85.5 ±0.5 83.8 ±0.3 81.6 ±0.3 50.7 ±0.4 61.7 ±0.5\nTable 4: Mean accuracy with standard deviation over three runs for the best performing layer on the probing tasks\nusing SMALL models. Bold values denote the best performance across each probing task.\nthe models have more impact on the effectiveness\nof LMs than the pre-training objectives. Further-\nmore, the comparable performance of different ob-\njectives in probing suggests that LMs mainly learn\nword co-occurrence information from pre-training\n(Sinha et al., 2021; Yamaguchi et al., 2021) and\nthat the objectives may have a little effect to what\nactually learn about linguistic properties.\nRecent studies have explored the limitations of\nusing probing tasks to draw conclusions over a\nmodel’s linguistic knowledge with some also sug-\ngesting improvements or alternative probing meth-\nods (Hewitt and Liang, 2019; V oita and Titov, 2020;\nElazar et al., 2021; Maudslay and Cotterell, 2021).\nHowever, our results show no substantial differ-\nences in the performance across tasks that probe for\nsyntactic or semantic information between models\nthat have been pre-trained using linguistically mo-\ntivated objectives or non-linguistically motivated\nones.\n6 Conclusions\nIn this work, we compared the linguistic capabili-\nties of LMs. Surprisingly, our results show that pre-\ntraining with linguistically motivated objectives ob-\ntain comparable performance to non-linguistically\nmotivated objectives. This suggests that the data\nand the size of the model could be more influential\nthan the objectives themselves in language model-\ning. In future work, we plan to extend our experi-\nments into other languages and probing tasks.\nAcknowledgments\nWe would like to thank Katerina Margatina and\nGeorge Chrysostomou for their invaluable feed-\nback. We also thank the anonymous reviewers\nfor their constructive feedback. AA is supported\nby the Centre for Doctoral Training in Speech\nand Language Technologies (SLT) and their Ap-\nplications funded by UK Research and Innovation\ngrant EP/S023062/1. NA is supported by EPSRC\ngrant EP/V055712/1, part of the European Com-\nmission CHIST-ERA programme, call 2019 XAI:\nExplainable Machine Learning-based Artificial In-\ntelligence.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi,\nand Yoav Goldberg. 2016. Fine-grained analysis of\nsentence embeddings using auxiliary prediction tasks.\narXiv preprint arXiv:1608.04207.\nStéphane Aroca-Ouellette and Frank Rudzicz. 2020. On\nLosses for Modern Language Models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4970–4981, Online. Association for Computational\nLinguistics.\n135\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. arXiv preprint arXiv:1803.05449.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral expla-\nnation with amnesic counterfactuals. Transactions of\nthe Association for Computational Linguistics, 9:160–\n175.\nAllyson Ettinger. 2020. What BERT Is Not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics for\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nRowan Hall Maudslay and Ryan Cotterell. 2021. Do\nsyntactic probes probe syntax? experiments with\njabberwocky probing. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 124–131, Online. As-\nsociation for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. arXiv preprint\narXiv:1909.03368.\nJohn Hewitt and Christopher D Manning. 2019. A struc-\ntural probe for finding syntax in word representations.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4129–4138.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classifiers’ reveal\nhow recurrent and recursive neural networks process\nhierarchical structure. Journal of Artificial Intelli-\ngence Research, 61:907–926.\nDan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky.\n2020. Pretraining with contrastive sentence objec-\ntives improves discourse performance of language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4859–4870.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? In ACL 2019-57th Annual Meeting of the\nAssociation for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. arXiv preprint arXiv:1903.08855.\nRowan Hall Maudslay and Ryan Cotterell. 2021. Do\nsyntactic probes probe syntax? experiments with\njabberwocky probing. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 124–131.\nAlessio Miaschi and Felice Dell’Orletta. 2020. Con-\ntextual and non-contextual word embeddings: an in-\ndepth linguistic investigation. In Proceedings of the\n5th Workshop on Representation Learning for NLP,\npages 110–119, Online. Association for Computa-\ntional Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances\nin neural information processing systems, 32:8026–\n8037.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nSheng Shen, Alexei Baevski, Ari S Morcos,\nKurt Keutzer, Michael Auli, and Douwe Kiela.\n2020. Reservoir transformers. arXiv preprint\narXiv:2012.15045.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\n136\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quarterly,\n30(4):415–433.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2019. What do you learn from context? probing for\nsentence structure in contextualized word representa-\ntions. arXiv preprint arXiv:1905.06316.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita and Ivan Titov. 2020. Information-theoretic\nprobing with minimum description length. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 183–196.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nAtsuki Yamaguchi, George Chrysostomou, Katerina\nMargatina, and Nikolaos Aletras. 2021. Frustratingly\nsimple pretraining alternatives to masked language\nmodeling. arXiv preprint arXiv:2109.01819.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), pages 19–27.\n137\nAppendices\nA Hyperparameter Details\nWe implement the models using PyTorch (Paszke\net al., 2019) and the Transformers library (Wolf\net al., 2020). We use maximum 10 epochs for\nBASE and MEDIUM , and 15 epochs for SMALL . We\nalso use a learning rate of 1e-4 for MLM. 5e-5 for\nBASE First Char, S+R, and ASCII. 5e-6 for BASE\nRandom. 1e-4 for SMALL and MEDIUM First Char,\nASCII and Random. We also use weight decay of\n0.01, attention dropout of 0.1, 10000 warmup steps.\nWe also use 1e-8 Adam ϵ, 0.9 Adam β1 and 0.999\nAdam β2.\nB Results of each Probing Task\nTables 5 to 13 show the full results of each of the\nnine probing tasks for all model architectures and\nlayers.\n138\nSentLen\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 95.4 ± 0.2 92.9 ± 0.4 90.7 ± 0.8 91.5 ± 0.3 92.6 ± 0.5\n2 96.0 ± 0.2 92.9 ± 0.2 92.4 ± 0.4 91.7 ± 0.7 93.6 ± 0.3\n3 95.3 ± 0.2 91.6 ± 0.6 92.9 ± 0.5 92.4 ± 1.7 94.4 ± 0.4\n4 93.8 ± 1.2 92.2 ± 0.8 93.4 ± 1.3 92.9 ± 1.0 94.1 ± 0.6\n5 93.9 ± 0.4 92.1 ± 0.6 93.7 ± 2.4 92.4 ± 0.5 93.8 ± 0.6\n6 93.6 ± 0.5 92.4 ± 0.5 93.5 ± 1.7 92.1 ± 0.7 94.3 ± 0.4\n7 92.6 ± 0.5 92.1 ± 0.8 93.1 ± 0.9 90.7 ± 1.4 94.4 ± 0.6\n8 91.2 ± 0.5 91.7 ± 0.5 92.0 ± 1.6 89.9 ± 1.0 94.2 ± 1.0\n9 89.0 ± 0.3 91.8 ± 0.4 90.9 ± 0.7 88.5 ± 1.6 95.0 ± 0.6\n10 82.8 ± 0.7 91.1 ± 0.9 90.0 ± 0.9 86.7 ± 1.7 94.6 ± 0.1\n11 79.4 ± 0.7 91.0 ± 0.4 88.6 ± 0.1 87.8 ± 0.5 94.4 ± 0.2\n12 73.9 ± 0.3 90.1 ± 0.3 85.9 ± 0.1 86.4 ± 0.2 93.6 ± 0.4\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 91.8 ± 0.5 88.4 ± 1.1 87.1 ± 0.8 86.6 ± 0.8 90.0 ± 0.9\n2 92.3 ± 0.2 94.0 ± 0.5 93.3 ± 0.3 90.4 ± 0.5 92.3 ± 0.2\n3 92.1 ± 0.2 94.0 ± 0.7 92.0 ± 0.6 89.2 ± 0.5 92.9 ± 0.2\n4 91.7 ± 0.2 93.4 ± 0.7 91.4 ± 0.2 89.5 ± 0.5 92.2 ± 0.5\n5 90.6 ± 0.3 92.7 ± 0.7 91.0 ± 0.2 89.7 ± 0.4 91.2 ± 0.7\n6 89.3 ± 0.3 93.0 ± 0.6 90.1 ± 0.8 89.0 ± 0.5 88.7 ± 0.7\n7 85.6 ± 0.2 92.0 ± 0.9 89.3 ± 0.5 86.1 ± 0.9 88.4 ± 0.7\n8 70.5 ± 0.1 87.8 ± 1.4 84.9 ± 0.5 83.9 ± 0.5 83.2 ± 0.1\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 92.9 ± 0.3 90.3 ± 1.3 89.8 ± 1.1 89.9 ± 0.3 94.1 ± 1.0\n2 93.7 ± 0.4 93.8 ± 0.4 90.7 ± 0.4 88.7 ± 0.2 93.3 ± 1.1\n3 91.7 ± 0.2 94.7 ± 0.8 89.7 ± 0.2 86.8 ± 0.5 90.1 ± 1.3\n4 77.2 ± 0.3 93.0 ± 0.5 84.4 ± 0.5 85.5 ± 0.4 84.7 ± 0.3\nTable 5: Results of the Sentence Length (SentLen) probing task for each layer of the pre-trained models.\n139\nTreeDepth\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 40.0 ± 0.6 36.6 ± 0.6 35.7 ± 0.2 36.1 ± 0.5 33.5 ± 0.7\n2 41.2 ± 1.1 38.6 ± 0.9 37.7 ± 0.5 36.6 ± 0.3 35.9 ± 0.5\n3 41.5 ± 0.6 40.0 ± 0.8 38.9 ± 0.6 37.1 ± 0.4 36.2 ± 0.4\n4 40.3 ± 0.7 41.7 ± 0.6 39.4 ± 0.6 37.7 ± 0.9 36.9 ± 0.4\n5 40.3 ± 1.1 44.2 ± 0.5 39.3 ± 0.3 38.4 ± 1.2 36.7 ± 0.5\n6 40.9 ± 0.7 45.0 ± 0.3 40.6 ± 0.4 40.7 ± 0.5 36.5 ± 0.5\n7 40.8 ± 0.8 44.9 ± 0.8 42.1 ± 0.6 42.4 ± 0.6 37.0 ± 0.6\n8 40.0 ± 0.7 45.0 ± 0.7 43.4 ± 1.2 43.3 ± 0.7 39.0 ± 0.3\n9 38.8 ± 1.1 44.3 ± 0.7 43.2 ± 1.3 43.3 ± 0.7 39.2 ± 0.3\n10 37.4 ± 0.3 45.2 ± 0.6 43.4 ± 1.1 42.9 ± 0.5 39.3 ± 0.5\n11 38.7 ± 0.6 44.5 ± 0.4 42.9 ± 1.2 42.7 ± 0.5 39.6 ± 0.6\n12 38.3 ± 0.3 42.1 ± 0.7 41.5 ± 0.7 42.3 ± 0.4 37.9 ± 1.3\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 37.9 ± 0.2 37.8 ± 0.5 36.4 ± 0.3 37.4 ± 0.1 36.1 ± 0.5\n2 39.0 ± 0.5 39.0 ± 1.2 36.5 ± 0.4 38.0 ± 0.4 36.4 ± 0.6\n3 39.4 ± 0.2 40.4 ± 0.5 36.3 ± 0.2 37.7 ± 0.6 38.3 ± 0.6\n4 40.5 ± 0.5 40.3 ± 0.6 36.7 ± 0.3 38.3 ± 0.3 41.6 ± 0.6\n5 41.1 ± 0.1 41.8 ± 1.0 36.9 ± 0.6 39.1 ± 0.5 42.4 ± 0.8\n6 40.5 ± 0.2 42.6 ± 0.2 37.5 ± 0.7 40.5 ± 0.6 40.5 ± 1.1\n7 39.3 ± 0.2 42.5 ± 0.4 40.4 ± 0.5 39.1 ± 0.8 39.1 ± 0.5\n8 38.6 ± 0.9 38.5 ± 0.6 40.2 ± 0.2 40.5 ± 0.1 35.6 ± 0.1\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 37.8 ± 0.3 39.2 ± 0.2 39.1 ± 0.3 37.5 ± 0.2 38.0 ± 0.2\n2 40.1 ± 0.5 41.9 ± 0.6 40.6 ± 0.7 37.4 ± 0.2 41.6 ± 0.4\n3 39.9 ± 0.9 41.6 ± 0.4 41.2 ± 0.3 41.3 ± 0.4 42.6 ± 0.5\n4 41.6 ± 0.2 43.3 ± 1.0 42.3 ± 0.4 40.9 ± 0.6 39.2 ± 0.3\nTable 6: Results of the Tree Depth (TreeDepth) probing task for each layer of the pre-trained models.\n140\nTopConst\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 62.0 ± 0.3 70.2 ± 0.7 60.9 ± 0.4 66.7 ± 1.1 65.2 ± 0.2\n2 72.6 ± 0.4 73.7 ± 0.2 69.3 ± 0.2 67.7 ± 0.2 68.4 ± 0.6\n3 74.0 ± 0.5 79.6 ± 0.8 70.7 ± 0.5 69.2 ± 0.2 69.3 ± 0.1\n4 73.0 ± 0.5 81.4 ± 0.4 71.0 ± 0.1 70.8 ± 0.3 69.9 ± 0.4\n5 73.7 ± 0.5 83.6 ± 0.2 71.3 ± 0.3 70.6 ± 0.5 69.8 ± 1.1\n6 74.6 ± 0.6 83.1 ± 0.7 71.7 ± 0.5 75.4 ± 0.9 69.2 ± 0.6\n7 75.1 ± 0.7 82.4 ± 0.2 76.2 ± 0.5 78.4 ± 0.5 70.0 ± 1.1\n8 76.9 ± 0.2 81.6 ± 0.4 78.2 ± 0.3 78.5 ± 0.4 71.4 ± 1.0\n9 76.8 ± 0.4 81.7 ± 0.6 80.1 ± 0.3 80.4 ± 0.2 70.7 ± 0.6\n10 74.6 ± 0.6 80.6 ± 0.7 81.1 ± 0.3 81.4 ± 0.4 71.2 ± 1.1\n11 74.2 ± 0.1 79.6 ± 0.9 80.7 ± 0.4 81.3 ± 0.6 69.8 ± 0.6\n12 72.5 ± 0.2 76.5 ± 0.5 79.9 ± 0.2 81.0 ± 0.2 67.4 ± 0.4\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 64.9 ± 0.3 63.1 ± 1.7 67.6 ± 0.6 68.2 ± 0.6 55.3 ± 0.3\n2 72.1 ± 0.6 69.8 ± 0.6 68.7 ± 0.5 70.5 ± 1.2 61.9 ± 1.0\n3 72.1 ± 0.6 72.3 ± 0.8 68.3 ± 0.7 69.1 ± 1.0 66.0 ± 1.4\n4 72.6 ± 0.6 80.6 ± 0.3 69.1 ± 0.6 74.2 ± 0.6 69.8 ± 0.4\n5 74.8 ± 0.5 81.9 ± 0.6 69.8 ± 0.7 78.1 ± 0.7 71.5 ± 0.9\n6 75.2 ± 0.4 81.9 ± 0.5 73.2 ± 0.1 79.3 ± 0.6 69.7 ± 0.8\n7 76.9 ± 0.5 83.0 ± 0.5 75.7 ± 0.7 78.5 ± 0.5 70.7 ± 0.6\n8 72.6 ± 0.3 79.8 ± 0.3 76.8 ± 0.3 79.6 ± 0.2 62.9 ± 0.2\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 66.4 ± 0.2 69.2 ± 0.4 74.6 ± 0.3 66.3 ± 0.2 66.7 ± 1.4\n2 72.5 ± 0.4 73.2 ± 0.2 75.8 ± 0.3 66.0 ± 0.5 74.2 ± 0.3\n3 71.9 ± 0.3 73.8 ± 0.2 76.4 ± 0.6 72.6 ± 0.9 75.8 ± 0.4\n4 73.1 ± 0.2 76.8 ± 0.6 77.5 ± 0.1 74.6 ± 0.4 72.7 ± 0.1\nTable 7: Results of the Top Constituent (TopConst) probing task for each layer of the pre-trained models.\n141\nBShift\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0\n2 50.0 ± 0.1 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0\n3 56.6 ± 0.3 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0\n4 57.9 ± 0.2 74.1 ± 0.3 50.0 ± 0.0 53.4 ± 0.4 50.0 ± 0.0\n5 59.8 ± 0.1 80.7 ± 0.2 50.0 ± 0.0 50.8 ± 1.4 50.0 ± 0.0\n6 60.0 ± 0.7 83.3 ± 0.4 50.0 ± 0.0 69.6 ± 1.4 50.0 ± 0.0\n7 64.9 ± 0.8 85.6 ± 0.2 63.5 ± 0.6 73.7 ± 2.8 60.2 ± 1.7\n8 72.0 ± 1.3 88.1 ± 0.1 74.4 ± 0.8 78.5 ± 1.5 66.9 ± 0.2\n9 81.4 ± 0.7 89.5 ± 0.2 82.4 ± 0.7 81.7 ± 0.8 67.0 ± 0.3\n10 85.6 ± 0.2 90.2 ± 0.3 84.8 ± 0.3 81.7 ± 1.4 68.4 ± 0.2\n11 86.5 ± 0.1 91.2 ± 0.6 85.0 ± 0.4 82.7 ± 0.3 68.9 ± 0.4\n12 82.3 ± 0.3 91.3 ± 0.7 83.3 ± 0.2 82.4 ± 0.2 68.4 ± 0.1\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0\n2 49.8 ± 0.3 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0 50.0 ± 0.0\n3 49.6 ± 0.4 50.0 ± 0.0 50.0 ± 0.0 57.9 ± 0.5 65.6 ± 0.7\n4 56.2 ± 0.7 64.9 ± 0.3 50.0 ± 0.0 58.1 ± 0.7 70.5 ± 0.4\n5 64.9 ± 0.3 76.4 ± 0.4 50.9 ± 1.6 58.9 ± 0.8 74.2 ± 0.0\n6 69.6 ± 0.7 79.6 ± 0.1 73.5 ± 1.3 67.9 ± 1.3 72.5 ± 1.5\n7 80.8 ± 0.1 82.1 ± 0.3 79.9 ± 0.4 75.1 ± 2.7 73.7 ± 0.1\n8 77.9 ± 0.5 84.6 ± 0.3 80.3 ± 0.4 80.0 ± 0.8 70.3 ± 0.6\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 50.0 ± 0.1 50.0 ± 0.0 50.4 ± 0.2 53.2 ± 0.8 50.7 ± 0.4\n2 49.8 ± 0.2 61.9 ± 0.3 57.7 ± 0.1 60.2 ± 1.2 60.0 ± 0.6\n3 60.8 ± 0.7 74.4 ± 0.0 65.3 ± 0.2 72.1 ± 0.6 68.7 ± 0.7\n4 78.3 ± 0.1 82.1 ± 0.1 76.2 ± 0.2 74.6 ± 0.1 71.0 ± 0.4\nTable 8: Results of the Bigram Shift (BShift) probing task for each layer of the pre-trained models.\n142\nTense\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 79.5 ± 0.8 83.6 ± 0.1 81.3 ± 0.1 79.9 ± 0.8 67.9 ± 0.6\n2 84.0 ± 0.7 84.3 ± 1.0 82.0 ± 0.2 80.3 ± 0.6 68.9 ± 0.8\n3 83.3 ± 0.3 85.7 ± 0.7 82.7 ± 0.5 82.0 ± 0.8 69.1 ± 0.6\n4 83.7 ± 0.7 86.3 ± 0.7 83.9 ± 0.7 82.9 ± 0.4 69.0 ± 0.3\n5 85.0 ± 0.5 86.3 ± 0.7 84.3 ± 0.9 83.0 ± 0.4 68.8 ± 0.5\n6 86.2 ± 0.2 87.8 ± 0.4 84.3 ± 0.9 85.3 ± 0.1 68.9 ± 0.4\n7 87.0 ± 0.1 87.1 ± 0.8 84.7 ± 0.6 86.0 ± 0.5 69.1 ± 0.5\n8 86.4 ± 0.8 87.2 ± 0.4 86.0 ± 0.3 86.1 ± 0.5 70.9 ± 0.1\n9 85.8 ± 1.8 86.3 ± 0.0 85.9 ± 0.2 87.2 ± 0.2 71.4 ± 0.6\n10 86.5 ± 1.5 85.9 ± 0.6 85.7 ± 0.8 88.5 ± 0.2 72.1 ± 0.5\n11 88.5 ± 0.7 83.7 ± 0.8 86.0 ± 0.7 88.7 ± 0.3 72.1 ± 0.5\n12 83.9 ± 0.0 81.7 ± 1.7 85.9 ± 0.5 88.6 ± 0.4 71.0 ± 0.4\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 85.1 ± 0.5 82.2 ± 0.6 83.2 ± 0.4 81.4 ± 0.2 79.6 ± 0.9\n2 84.1 ± 0.5 84.0 ± 0.3 82.5 ± 0.3 82.4 ± 0.5 80.0 ± 0.8\n3 84.8 ± 0.4 85.4 ± 0.3 82.7 ± 0.1 82.0 ± 0.5 82.6 ± 0.8\n4 85.6 ± 0.6 85.5 ± 0.6 82.7 ± 0.4 83.4 ± 0.5 84.6 ± 0.7\n5 85.9 ± 0.4 85.0 ± 0.4 83.7 ± 0.4 84.1 ± 0.8 86.1 ± 0.1\n6 85.7 ± 0.8 85.7 ± 0.2 84.7 ± 0.7 85.4 ± 0.5 83.9 ± 1.5\n7 85.9 ± 0.1 84.6 ± 0.5 85.8 ± 0.5 85.3 ± 0.5 84.9 ± 0.4\n8 83.9 ± 0.5 82.8 ± 0.4 85.6 ± 0.5 87.8 ± 0.5 84.6 ± 0.5\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 86.3 ± 0.4 84.9 ± 0.2 84.7 ± 0.7 82.7 ± 0.6 84.4 ± 0.3\n2 86.2 ± 0.6 85.6 ± 0.5 84.7 ± 0.8 82.9 ± 0.2 85.2 ± 0.5\n3 86.4 ± 0.7 86.0 ± 0.2 84.7 ± 0.6 84.5 ± 0.8 85.5 ± 0.5\n4 85.2 ± 0.6 86.5 ± 0.2 86.0 ± 0.1 85.7 ± 0.4 84.9 ± 0.3\nTable 9: Results of the Tense (Tense) probing task for each layer of the pre-trained models.\n143\nSubjNum\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 75.1 ± 0.5 75.5 ± 0.3 75.7 ± 0.8 77.0 ± 0.1 69.5 ± 0.2\n2 81.6 ± 0.3 80.2 ± 0.3 78.3 ± 0.3 78.0 ± 0.6 71.7 ± 0.4\n3 82.3 ± 0.3 85.0 ± 0.1 79.1 ± 0.4 78.7 ± 0.5 72.4 ± 0.3\n4 81.8 ± 0.3 86.2 ± 0.5 79.1 ± 0.6 79.5 ± 0.1 72.1 ± 0.5\n5 83.0 ± 0.3 88.7 ± 0.2 80.3 ± 0.9 80.5 ± 0.2 72.8 ± 0.1\n6 85.0 ± 0.2 88.2 ± 0.3 82.2 ± 0.5 84.1 ± 0.4 72.7 ± 0.5\n7 84.9 ± 0.6 87.5 ± 0.5 84.3 ± 0.1 85.5 ± 0.4 73.4 ± 0.6\n8 86.0 ± 0.3 87.0 ± 0.9 85.5 ± 0.2 86.9 ± 0.9 73.9 ± 0.7\n9 87.2 ± 1.0 87.1 ± 0.3 87.9 ± 0.4 88.9 ± 0.6 73.7 ± 0.4\n10 87.4 ± 1.2 86.5 ± 0.5 88.9 ± 0.1 89.1 ± 0.3 74.3 ± 0.2\n11 86.2 ± 0.2 86.1 ± 0.4 88.1 ± 0.4 88.8 ± 0.3 74.1 ± 0.1\n12 82.3 ± 0.2 84.3 ± 0.4 86.3 ± 0.4 88.2 ± 0.4 74.2 ± 0.3\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 79.3 ± 0.7 77.3 ± 0.6 77.0 ± 0.3 77.2 ± 1.1 75.0 ± 1.1\n2 80.7 ± 0.2 80.0 ± 0.1 78.2 ± 0.6 80.4 ± 0.5 79.9 ± 0.5\n3 81.0 ± 0.4 83.0 ± 0.7 78.0 ± 0.5 79.6 ± 0.2 80.4 ± 0.5\n4 82.5 ± 0.5 86.9 ± 0.3 79.3 ± 0.6 81.0 ± 0.9 83.4 ± 0.4\n5 83.9 ± 0.3 87.9 ± 0.4 79.7 ± 0.4 82.5 ± 0.5 84.3 ± 0.3\n6 84.5 ± 0.2 87.5 ± 0.3 83.4 ± 0.3 84.4 ± 0.3 83.1 ± 1.0\n7 86.7 ± 0.1 87.3 ± 0.1 86.3 ± 1.3 85.1 ± 0.5 83.9 ± 0.2\n8 82.5 ± 0.2 85.3 ± 0.5 85.7 ± 0.2 85.3 ± 0.3 81.0 ± 0.1\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 78.0 ± 0.8 80.9 ± 0.3 81.2 ± 0.1 76.5 ± 0.4 79.3 ± 0.3\n2 82.2 ± 0.2 82.5 ± 0.3 82.1 ± 0.4 76.5 ± 0.5 82.4 ± 0.6\n3 83.5 ± 0.2 81.8 ± 1.1 82.6 ± 0.2 82.6 ± 0.3 83.8 ± 0.3\n4 83.3 ± 0.4 85.6 ± 0.3 84.7 ± 0.5 84.0 ± 0.3 81.9 ± 0.1\nTable 10: Results of the Subject Number (SubjNum) probing task for each layer of the pre-trained models.\n144\nObjNum\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 75.6 ± 0.3 73.6 ± 0.3 76.5 ± 0.4 77.5 ± 0.3 64.9 ± 0.6\n2 81.1 ± 0.1 77.0 ± 0.1 77.9 ± 0.9 77.7 ± 1.3 67.5 ± 0.4\n3 80.5 ± 1.0 79.7 ± 0.5 78.5 ± 0.5 79.7 ± 0.7 68.0 ± 0.3\n4 80.3 ± 0.8 81.9 ± 0.5 78.7 ± 3.0 78.6 ± 0.4 68.1 ± 0.1\n5 80.4 ± 1.0 84.4 ± 1.1 79.2 ± 2.9 78.8 ± 1.1 68.4 ± 0.4\n6 82.0 ± 0.1 84.5 ± 0.2 81.1 ± 1.3 82.2 ± 1.2 68.4 ± 0.6\n7 82.1 ± 0.4 84.4 ± 0.1 84.0 ± 0.7 83.3 ± 0.8 69.2 ± 0.2\n8 82.1 ± 1.0 84.0 ± 0.9 84.4 ± 0.8 84.3 ± 1.2 69.4 ± 0.2\n9 82.9 ± 0.3 84.1 ± 0.5 86.4 ± 0.1 84.5 ± 1.4 69.7 ± 0.1\n10 83.8 ± 0.2 82.9 ± 0.5 86.4 ± 0.2 84.7 ± 0.6 69.9 ± 0.2\n11 83.3 ± 0.3 83.8 ± 0.3 86.0 ± 0.3 84.5 ± 0.2 70.3 ± 0.1\n12 78.5 ± 0.3 81.1 ± 1.7 83.5 ± 0.2 84.7 ± 0.5 70.2 ± 0.3\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 80.1 ± 0.3 76.2 ± 0.4 76.2 ± 0.6 76.0 ± 0.3 75.2 ± 0.1\n2 80.1 ± 0.1 78.4 ± 0.2 77.8 ± 0.7 78.5 ± 0.6 76.4 ± 0.5\n3 80.6 ± 0.0 80.9 ± 0.1 77.2 ± 0.0 77.7 ± 0.8 78.7 ± 0.3\n4 80.7 ± 0.2 81.0 ± 0.4 78.1 ± 0.1 77.8 ± 1.0 84.6 ± 0.2\n5 82.5 ± 0.3 81.2 ± 0.6 78.7 ± 0.5 81.5 ± 0.2 85.7 ± 0.3\n6 82.9 ± 0.1 81.9 ± 0.5 81.1 ± 0.3 82.9 ± 0.4 84.2 ± 0.6\n7 83.7 ± 0.5 80.8 ± 0.3 83.1 ± 0.1 82.6 ± 0.2 83.8 ± 0.0\n8 80.2 ± 0.4 80.3 ± 0.5 81.8 ± 0.3 83.9 ± 0.1 82.2 ± 0.3\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 78.2 ± 0.9 81.4 ± 0.2 77.8 ± 0.4 77.7 ± 0.4 78.2 ± 0.3\n2 82.0 ± 0.2 82.4 ± 0.3 79.7 ± 0.2 78.5 ± 0.4 79.0 ± 0.4\n3 83.5 ± 0.1 82.5 ± 0.4 80.4 ± 0.2 84.4 ± 0.2 81.6 ± 0.3\n4 80.9 ± 0.2 83.3 ± 0.5 82.9 ± 0.7 83.8 ± 0.2 79.4 ± 0.1\nTable 11: Results of the Object Number (ObjNum) probing task for each layer of the pre-trained models.\n145\nSOMO\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 50.0 ± 0.2 50.0 ± 0.2 50.0 ± 0.2 50.0 ± 0.2 50.0 ± 0.2\n2 52.5 ± 0.7 51.6 ± 0.3 50.5 ± 1.1 50.0 ± 0.2 50.0 ± 0.2\n3 54.4 ± 1.3 50.0 ± 0.2 51.8 ± 0.9 50.0 ± 0.2 50.0 ± 0.2\n4 55.2 ± 0.5 53.7 ± 0.8 52.5 ± 0.5 50.7 ± 1.2 50.0 ± 0.2\n5 55.8 ± 0.0 55.4 ± 0.1 52.1 ± 0.8 50.0 ± 0.2 50.0 ± 0.2\n6 57.6 ± 0.7 56.1 ± 0.3 52.8 ± 0.2 50.0 ± 0.2 50.0 ± 0.2\n7 58.2 ± 1.0 56.8 ± 0.5 52.8 ± 1.1 50.0 ± 0.2 50.0 ± 0.2\n8 58.1 ± 0.6 56.9 ± 1.3 53.7 ± 0.7 50.0 ± 0.2 50.0 ± 0.2\n9 59.1 ± 0.4 57.9 ± 1.5 54.1 ± 1.0 53.2 ± 0.9 50.0 ± 0.2\n10 60.6 ± 0.5 58.5 ± 0.9 56.3 ± 0.7 53.4 ± 0.2 50.4 ± 0.3\n11 61.7 ± 0.5 58.9 ± 0.6 56.5 ± 0.4 53.9 ± 1.0 50.2 ± 0.3\n12 57.8 ± 0.4 59.6 ± 0.4 55.4 ± 1.0 54.0 ± 0.3 50.2 ± 0.5\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 51.6 ± 0.5 50.2 ± 0.3 50.0 ± 0.2 50.7 ± 0.8 50.0 ± 0.2\n2 52.3 ± 0.7 51.1 ± 0.1 50.0 ± 0.2 52.2 ± 0.4 50.0 ± 0.2\n3 53.2 ± 0.1 52.6 ± 0.4 50.0 ± 0.2 52.1 ± 0.3 50.0 ± 0.2\n4 53.1 ± 0.8 52.9 ± 0.7 50.0 ± 0.2 51.3 ± 0.3 50.8 ± 0.3\n5 53.5 ± 0.6 53.8 ± 0.6 50.0 ± 0.2 51.2 ± 0.4 51.0 ± 0.4\n6 54.6 ± 1.1 53.9 ± 0.7 51.5 ± 1.5 51.3 ± 0.2 50.0 ± 0.1\n7 56.1 ± 0.6 55.2 ± 0.6 53.2 ± 0.2 52.0 ± 0.2 51.3 ± 0.7\n8 54.1 ± 0.1 55.8 ± 0.3 53.8 ± 0.6 52.7 ± 0.4 50.6 ± 0.3\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 52.5 ± 0.2 52.2 ± 0.2 51.8 ± 0.2 51.3 ± 0.3 50.4 ± 0.3\n2 55.4 ± 0.2 54.3 ± 0.4 51.5 ± 0.2 51.1 ± 0.2 50.7 ± 0.4\n3 55.9 ± 0.6 54.8 ± 0.8 51.5 ± 0.2 52.2 ± 0.0 50.6 ± 0.2\n4 53.9 ± 0.7 54.9 ± 0.4 52.4 ± 0.3 52.3 ± 0.4 50.2 ± 0.5\nTable 12: Results of the Semantic Odd Man Out (SOMO) probing task for each layer of the pre-trained models.\n146\nCoordInv\nLayer BASE - 500k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 57.3 ± 1.1 56.5 ± 1.0 55.1 ± 1.6 50.0 ± 0.0 50.0 ± 0.0\n2 61.0 ± 0.5 59.7 ± 0.5 58.0 ± 0.6 50.0 ± 0.0 51.7 ± 3.0\n3 61.8 ± 0.8 63.5 ± 0.8 58.9 ± 0.3 57.2 ± 0.6 57.8 ± 0.3\n4 61.2 ± 0.5 64.8 ± 1.4 59.4 ± 0.6 59.6 ± 0.5 52.3 ± 4.0\n5 62.0 ± 0.6 67.6 ± 0.4 60.2 ± 0.7 59.1 ± 0.3 55.2 ± 4.5\n6 62.8 ± 0.4 69.2 ± 0.3 59.6 ± 0.7 59.8 ± 1.6 58.2 ± 0.4\n7 61.6 ± 0.6 68.0 ± 0.3 61.3 ± 0.9 61.5 ± 2.0 59.8 ± 0.2\n8 62.1 ± 0.4 67.4 ± 0.4 63.4 ± 0.7 62.9 ± 2.1 61.4 ± 0.2\n9 62.1 ± 1.0 66.9 ± 0.2 63.9 ± 0.9 66.0 ± 1.0 62.6 ± 1.0\n10 64.4 ± 0.5 67.8 ± 0.2 65.6 ± 0.6 67.6 ± 1.1 63.0 ± 0.2\n11 65.5 ± 0.3 67.7 ± 0.5 66.5 ± 0.8 68.4 ± 0.5 63.3 ± 0.3\n12 63.7 ± 1.3 65.4 ± 0.4 64.4 ± 0.9 68.5 ± 0.8 61.3 ± 0.7\nLayer MEDIUM - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 59.4 ± 0.2 57.7 ± 0.3 56.9 ± 0.8 56.7 ± 0.7 55.9 ± 1.6\n2 63.5 ± 0.7 60.7 ± 0.8 56.7 ± 0.4 60.4 ± 0.5 57.9 ± 0.2\n3 62.1 ± 0.0 63.6 ± 0.4 56.5 ± 0.1 59.3 ± 0.7 58.6 ± 0.2\n4 62.5 ± 0.2 65.6 ± 1.0 56.0 ± 0.7 60.0 ± 0.7 61.5 ± 0.4\n5 63.1 ± 0.3 66.2 ± 1.2 57.6 ± 1.2 60.2 ± 0.4 61.4 ± 0.5\n6 62.5 ± 0.3 65.7 ± 1.5 58.3 ± 0.4 60.2 ± 1.0 60.1 ± 0.7\n7 61.7 ± 0.6 66.5 ± 1.2 60.4 ± 0.9 60.1 ± 1.5 60.3 ± 0.7\n8 58.4 ± 0.5 63.8 ± 1.8 61.8 ± 0.3 64.7 ± 0.1 58.7 ± 0.4\nLayer SMALL - 250k Steps Pre-training\nMLM S +R First Char ASCII Random\n1 61.4 ± 0.1 60.1 ± 0.6 62.8 ± 0.1 59.7 ± 0.4 58.9 ± 0.5\n2 64.0 ± 0.3 62.2 ± 0.4 64.0 ± 0.6 59.1 ± 0.2 61.0 ± 0.8\n3 62.2 ± 0.3 62.7 ± 0.3 63.0 ± 0.4 61.4 ± 0.2 61.7 ± 0.5\n4 59.4 ± 0.5 63.9 ± 0.1 62.2 ± 0.3 62.5 ± 0.1 59.9 ± 0.2\nTable 13: Results of the Coordination Inversion (CoordInv) probing task for each layer of the pre-trained models.\n147"
}