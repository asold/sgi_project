{
  "title": "Lightweight Adaptive Mixture of Neural and N-gram Language Models",
  "url": "https://openalex.org/W2798540241",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5024680544",
      "name": "Anton Bakhtin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5053145694",
      "name": "Arthur Szlam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111465122",
      "name": "Marc’Aurelio Ranzato",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069316249",
      "name": "Édouard Grave",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2150884987",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2411934291",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2585519081",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2151315616",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W2952339051"
  ],
  "abstract": "It is often the case that the best performing language model is an ensemble of a neural language model with n-grams. In this work, we propose a method to improve how these two models are combined. By using a small network which predicts the mixture weight between the two models, we adapt their relative importance at each time step. Because the gating network is small, it trains quickly on small amounts of held out data, and does not add overhead at scoring time. Our experiments carried out on the One Billion Word benchmark show a significant improvement over the state of the art ensemble without retraining of the basic modules.",
  "full_text": "Lightweight Adaptive Mixture of Neural and N-gram Language Models\nAnton Bakhtin Arthur Szlam Marc’Aurelio Ranzato Edouard Grave\nFacebook AI Research, NY\nyolo@fb.com\nAbstract\nIt is often the case that the best performing lan-\nguage model is an ensemble of a neural lan-\nguage model with n-grams. In this work, we\npropose a method to improve how these two\nmodels are combined. By using a small net-\nwork which predicts the mixture weight be-\ntween the two models, we adapt their relative\nimportance at each time step. Because the gat-\ning network is small, it trains quickly on small\namounts of held out data, and does not add\noverhead at scoring time. Our experiments\ncarried out on the One Billion Word bench-\nmark show a signiﬁcant improvement over the\nstate of the art ensemble without retraining of\nthe basic modules.\n1 Introduction\nThe goal of statistical language modeling is to es-\ntimate the probability of sentences or sequences\nof words (Bahl et al., 1990). By the chain rule\nof probability theory, this is equivalent to esti-\nmation of the conditional probability of a word\ngiven all preceding words. This problem is key\nto natural language processing, with applications\nnot only in type-ahead systems, but also ma-\nchine translation (Brown et al., 1993) and au-\ntomatic speech recognition (Bahl et al., 1990).\nWhile earlier work on statistical language model-\ning focused on n-gram language models (Kneser\nand Ney, 1995; Chen and Goodman, 1999), re-\ncent advances are based on variants of neural lan-\nguage models (Bengio et al., 2003; Mikolov et al.,\n2010; Dauphin et al., 2016), which have yielded\nstate of the art performance on several large scale\nbenchmarks (Jozefowicz et al., 2016). Neural ap-\nproaches require less memory than n-grams and\nthey generalize better, but with a substantial in-\ncrease in computational complexity both at train-\ning and test time. Despite the superior perfor-\nmance of neural models, even better results in\n1 2 3 4 5\nTarget word frequency bucket\n0.50\n0.75\n1.00\n1.25\n1.50Ratio of perplexities\nPPL(lstm)/PPL(ensemble)\nPPL(lstm)/PPL(n-gram)\nFigure 1: As the frequency of the word-to-predict de-\ncreases (from left to right), the relative performance of\nneural models gets better compared to n-grams (orange\ncurve). Yet, ensembling the two models (with a ﬁxed\nscalar weight) is more effective on rarer words (blue\ncurve). Bins were built by sorting words by frequency\nand by dividing them into buckets with equal probabil-\nity mass.\nterms of perplexity can be achieved by ensembling\nneural models with n-grams (Mikolov et al., 2011;\nChelba et al., 2013; Jozefowicz et al., 2016). How-\never, Fig. 1, which shows results using a single\nconstant scalar to weigh the output distribution of\na neural model and an n-gram model, suggests that\nthe relative contributions of the two models are not\nsimple. For example, the neural model generalizes\nbetter than the n-gram on rarer words, yet on rarer\nwords the ensemble yields the largest gains.\nIn this work we will study more sophisticated\nmethods for combining the results of n-gram and\nneural language models than a ﬁxed scalar weight.\nWe will propose a simple gating network which\ntakes as input a handful of features based on fre-\nquency statistics to produce as output an input de-\npendent weight to be used in the ensemble, effec-\ntively turning the ensemble into an adaptive mix-\nture of experts model. We show that given already\ntrained neural and n-gram language models, the\ngating network can be trained quickly on a hand-\narXiv:1804.07705v2  [cs.CL]  26 Oct 2018\nful of examples. The gating network consistently\nyields better results than the ensemble which uses\na ﬁxed weight in the mixture, while adding a neg-\nligible computational cost. We evaluated our pro-\nposed approach on the One Billion Word bench-\nmark (Chelba et al., 2013), the biggest publicly\navailable benchmark for language modeling, and\non the Wall Street Journal corpus, demonstrating\nseizable gains on both datasets.\n2 Related work\nThe method we propose is a particular instance of\na mixture of experts(MoEs) (Jacobs et al., 1991),\nwhere experts are pre-trained and the gating is a\npossibly recurrent function of some handcrafted\nfeatures. The advantages are twofold. First, we do\nnot need to update the experts which are very large\nsystems and instead, we can learn very quickly to\nmodulate between them. Second, the gating net-\nwork is tiny as we do not need to represent the ac-\ntual input words, which further speeds up training\ntime and reduces sample complexity.\nShazeer et al. (2017) also proposed to use a\nMoEs for language modeling. The major technical\ndifference is that they employ MoEs in between\nlayers of a deep LSTM, while we do it at the out-\nput. While our focus is to design a lightweight\nsystem that optimally combines pre-trained mod-\nels, their focus is to train a single much higher ca-\npacity system, a much more engineering involved\nendeavor.\nIn (Kneser and Steinbiss, 1993) information\nabout ground truth performance of several n-gram\nmodels on previous 400 words is used to predict\nthe optimal interpolation weights at each position.\nHowever, this approach could only be applied if\nthe ground truth is known in advance and enough\ncontext is given.\nA few works explored using n-gram features\nwithin a neural language model. Mikolov et al.\n(2011) and Chelba et al. (2013) train a neural\nmodel jointly with a maximum entropy model tak-\ning as input n-gram features. (Neubig and Dyer,\n2016) proposes an approach more similar to ours,\nexcept that the gating network takes as input the\nhidden state of the neural model. This has two\ndrawbacks. First, the hidden state may already\nhave lost the discriminative information necessary\nfor the selection of the expert. Second, the gating\nnetwork operates on a much higher dimensional\ninput, and therefore, it requires more data to train.\nMoreover, we do not attempt at tuning the experts\nnor we care about how these were trained (Jean\net al., 2014; Grave et al., 2016), but we use them\nas black-boxes and only train the gating network\nwhich is a much simpler task.\n3 Basic Models\nThe goal of language modeling is to estimate the\nprobability P(wt|c) of a next word wt given its\ncontext sequence c = (wt−1,...,w 1); the context\nbeing empty if t= 1. In this section, we introduce\nthe models we use to instantiate our experts.\n3.1 N-gram Language Models\nN-gram models rely on the following Markov as-\nsumption: the next word depends only on the\nN −1 previous words: P (wt|c) = P(wt|cN−1),\nwhere cN−1 = (wt−1,...,w t−N+1); maximum\nlikelihood estimation then yields: P (wt|cN−1) =\nC(wt,cN−1)/C(cN−1), where C(•) stands for\nthe number of occurrences of the sequence in the\ntraining corpus.\nFor high order models, e.g,N = 5, only a small\nfraction of the n-grams appear in the training cor-\npus, a problem also referred to as data sparsity,\nwhich would yield0 probability for almost all sen-\ntences. To counteract this, several back-off tech-\nniques have been suggested, the most popular be-\ning deﬁned as:\nPNG\nN (wt) =\n{\npwt,cN−1 if not zero\nPNG\nN−1(wt)αcN−1 otherwise (1)\nwhere α and p are called back-off coefﬁcients\nand discounted probabilities, respectively. In this\nwork, we use Kneser-Ney formulation (Kneser\nand Ney, 1995) that yields state of the art results\namong n-gram models.\n3.2 Neural Language Model\nAnother approach to reduce sparsity is to encode\nthe context c as a ﬁxed length dense vector ht.\nTo do so each word w is mapped to an em-\nbedding vector v(w). The sequence of vectors\nv(w1),...,v (wt) is then fed to a neural network\nf to produce ht. A linear classiﬁer a is then ap-\nplied to ht to estimate the probability distribution\nover the next word:\nPNN (•|c) =a(f(v(w1),...,v (wt−1))) . (2)\nDifferent types of networks could be used as en-\ncoders, such as fully connected (Bengio et al.,\n. . . will be made at an event at the San Fran-\ncisco Museum of Science\nRobert Jew of the National Archives and\nRecords Administration\n. . . We need professional advice , said Senator\nGeorge H. Winner\n. . . he shows up armed to buy machine guns\nand siliences\nXbox 360 ( R ) video game and entertainment\nsystem from\nTable 1: Examples of contexts where the n-gram\nmodel signiﬁcantly outperforms the neural model. The\nword to be predicted is marked in bold font.\n2003), convolutional (Dauphin et al., 2016) or\nrecurrent (Mikolov et al., 2010; Chelba et al.,\n2013). In this work we use LSTMs (Hochreiter\nand Schmidhuber, 1997) which is nowadays one\nof the strongest performing methods.\n4 Mixture of experts\nDifferent experts have different strengths and\nweaknesses. The complementarity of neural and\nn-gram language models explains why ensembling\nworks so well. However, it is conceivable that\ndifferent contexts may need different weighting.\nMoEs address exactly this issue, enhancing the\nmodel with a gating network that weighs experts in\nan input dependent manner. Next, we ﬁrst analyze\nwhere n-grams outperform neural language mod-\nels, and then propose a simple gating mechanism\nto automatically select the most suitable expert.\n4.1 Analysis\nN-gram language models require a large mem-\nory which grows with the amount of training\ndata (Silva et al., 2016), but are fast at test time\nas they require only table lookups. They can eas-\nily memorize patterns but do not generalize well\nto rare events. On the other hand, neural language\nmodels are much more compact, generalize much\nbetter but require more computation. Jozefowicz\net al. (2016) found that the relative advantage of\nneural models increases as the frequency of tar-\nget word decreases. While we observe the same\nbehavior, we also notice that the relative improve-\nment of an ensemble over the neural model is big-\nger for rare words, as shown in Fig. 1.\nIn order to gain better understanding, we se-\nlected sentences where the n-gram model signif-\nicantly outperforms the neural model in the One\n#features\nback-off weights (α) 5\ndiscounted probabilities (p) 5\nlogarithm of position (log(t)) 1\nmax(PNG (•|c)), entropy(PNG (•|c)) 2\nmax(PNN (•|c)), entropy(PNN (•|c)) 2\nTable 2: Description of features used. Top block of\nfeatures are used for both SIMPLE and FULL. Lower\nblock is used only for FULL. The notation follows that\nin Eq. 1 and Eq. 2.\nBillion Word dataset, see some examples in Ta-\nble 4.1. In the vast majority of the cases, these\ncontexts contain long proper nouns, e.g., Sena-\ntor George H. Winner. As a quantitative evi-\ndence, we found that 23% of words, such that\nPNG −PNN > 0.5, are capitalized versus 13%\nin general distribution. In other cases, we found\nphrases that exactly match training examples. In\nboth cases, n-gram models are better equipped at\npredicting since these are essentially memoriza-\ntion tasks. This also explains why n-grams yield\nthe biggest gains on the rarest word bucket of\nFig. 1, despite being limited on rare words (be-\ncause of data sparsity). Overall, it seems that\nthe task of assessing whether the n-gram model\nis better than the neural model is fairly easily pre-\ndictable. Next, we propose a simple method to do\nso.\n4.2 Gating Network\nA mixture of experts can be written as:\nPens(wt|c) = λ(c)PNN (wt|c) +\n(1 −λ(c))PNG (wt|c), (3)\nwhere λ(c) is a scalar between 0 and 1 which is\nthe output of our gating network, and PNN and\nPNG are deﬁned by Eq. 1 and Eq. 2 respectively.\nIn this work, we propose to use as gating net-\nwork a small model that takes as input a handful\nof hand crafted features. We choose our features\nto convey our intuition that switching to an n-gram\nmodel should depend on both the frequency of the\nword as well as the entropy of the prediction. We\ntherefore use both the back-off Kneser-Ney coefﬁ-\ncients and discounted probabilities, as well as en-\ntropy of the distribution over the next word and its\nmode. In order to account for positional informa-\ntion in the sentence (as we also found that n-grams\nare worse at later positions in long sentences), we\nWSJ 1B\ntraining tokens 36M 768M\nunique words 20k 793k\nTable 3: Data sizes for the WSJ and 1B Word corpora.\nalso add the log of the word position. The full\nlist of features is given in Table 2. We have a to-\ntal of 15 features, denoted as FULL set, which we\nuse as input to the gating network. We denote a\nsubset of features that require nothing more than\nthe coefﬁcients from the existing n-gram model as\nSIMPLE. This allows us to investigate the relative\nimportance of the signal from the neural language\nmodel and to further reduce the computational bur-\nden.\nWe train the gating network with cross-entropy\nloss using Eq. 3 as predictive distribution, and\nwithout updating the expert models.\n5 Experiments\nWe performed language modeling at the word\nlevel using the One Billion Word bench-\nmark (Chelba et al., 2013) and the Wall Street\nJournal (WSJ) 1, see details in Table 3.\n5.1 Expert models\nWe used KenLM toolkit (Heaﬁeld, 2011) to train a\n5-gram model with modiﬁed Kneser-Ney smooth-\ning for both datasets.\nFor the One Billion Word dataset, we used the\nbest neural model reported by Jozefowicz et al.\n(2016), composed of two LSTM layers with 8092\nhidden units each, and projection layers with 1024\nunits. Due to the large vocabulary size, we trained\nusing sampled softmax (Jean et al., 2014). We\ntrained the model for four days on 8 GPU. For the\nsmaller WSJ we used a much simpler model with\na single LSTM layer with 500 units and trained\nit until convergence. We used dropout as a reg-\nularization technique for both models (Srivastava\net al., 2014).\n5.2 Gating model\nThe gating model was trained using only valida-\ntion data. For WSJ we used 14k sentences for\ntraining and the remaining 2k for early stopping.\nFor 1B Word, we used two separate held-out par-\ntitions to train and validate the gating models, each\n1Obtained from http://www.fit.vutbr.cz/\n˜imikolov/rnnlm/kaldi-wsj.tgz\nfeature set\nmodel SIMPLE FULL\nLIN 30.82 30.75\nMLP 30.35 30.30\nLSTM 30.05 29.85\nTable 4: Validation perplexity (lower is better) for dif-\nferent features and architectures of the gating model.\nEach experiment is an average over 10 runs.\nWSJ 1B\nn-gram LM 113.23 66.96\nneural LM 71.39 33.01\nensemble 67.44 29.80\nHIDDEN 67.01 ±0.05 29 .72 ±0.03\n+end-to-end 66.88 ±0.07 -\nSIMPLE 67.11 ±0.11 28.88 ±0.04\nFULL 66.75 ±0.03 28.70 ±0.04\nTable 5: Test perplexity on WSJ and 1B (lower is bet-\nter). For MoE models we report mean and standard\nerror over 10 runs. Results in bold show signiﬁcant\nimprovement over the best baseline (two tailed t-test,\np< 0.001).\nwith 6K sentences. In both cases we didn’t use any\ntest set to train nor tune the gating model.\nWe experimented with three architectures: a lin-\near model (LIN), a fully connected neural network\nwith two hidden layers with 32 units each (MLP),\nand an LSTM with 8 units on top of the previous\nMLP to test if temporal dependencies matter.\nThe largest gating network has only 2572 train-\nable parameters. All hyper-parameters were tuned\non 1B Word dataset and applied to WSJ as is. We\nnormalize feature on the training set to have zero-\nmean and unit-variance. All networks were trained\nuntil convergence with Adam optimizer (Kingma\nand Ba, 2014) with initial learning rate of6×10−3,\nwhich was halved every 5k steps.\n5.3 Results\nFirst, we evaluated the three gating architectures\non the validation set of the One Billion Word\ndataset, as shown in Table 4. While extra features\ndecrease the perplexity, the effect of the model ar-\nchitecture is much more impactful. On the follow-\ning, we only consider the LSTM architecture for\nthe gating model.\nNext, we evaluated the model on the test sets,\nsee Table 5. We compare the mixture of experts\nwith static ensembling, which is the current state\nof the art.\nTo compare our approach with the method pro-\nposed in Neubig and Dyer (2016), we trained a\ngating model using the last hidden layer of the\nneural language model as features (HIDDEN; 500\nfeatures for WSJ and 1024 for 1B Word). We tried\ntwo variants of the model: with and without gra-\ndient ﬂow through neural LM. The ﬁne tuning re-\nquires computing of the full softmax as well as its\ngradient. Attempt to do this with 1B Word neu-\nral model resulted in running out of GPU memory.\nAs described in 5.1 this model is trained with a\nsampled softmax that requires much less memory\nthan full softmax. Due to this reason we report re-\nsults on HIDDEN features with end-to-end train-\ning only for WSJ.\nOn both datasets the mixture model with FULL\nfeatures show a signiﬁcant improvement over\nthese baselines. Note that on 1B Word dataset\nSIMPLE features alone outperform HIDDEN fea-\ntures by a margin. We speculate that this is caused\nby the better ratio of the model size to the size\nof the dataset: SIMPLE features are derived from\nan n-gram model that scales better than a neural\nmodel.\n5.4 Conclusion\nWe proposed a very simple yet effective method\nto combine pretrained neural and n-gram language\nmodels. Instead of ensembling, we learn a per-\ntime step predictor of the optimal weight between\nthe two models. The gating network is small,\nfast to train and to run, because it takes as input\nhandcrafted features found by analyzing where n-\ngrams outperform neural models.\nReferences\nLalit R Bahl, Frederick Jelinek, and Robert L Mer-\ncer. 1990. A maximum likelihood approach to con-\ntinuous speech recognition. In Readings in speech\nrecognition, pages 308–319. Elsevier.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nPeter F Brown, Vincent J Della Pietra, Stephen A Della\nPietra, and Robert L Mercer. 1993. The mathemat-\nics of statistical machine translation: Parameter esti-\nmation. Computational linguistics, 19(2):263–311.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nStanley F Chen and Joshua Goodman. 1999. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language,\n13(4):359–394.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2016. Language modeling with\ngated convolutional networks. arXiv preprint\narXiv:1612.08083.\nEdouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2016. Efﬁcient\nsoftmax approximation for GPUs. arXiv preprint\narXiv:1609.04309.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197. Association for Computational Linguis-\ntics.\nSepp Hochreiter and Jurgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural computation, 3(1):79–87.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2014. On using very large tar-\nget vocabulary for neural machine translation. arXiv\npreprint arXiv:1412.2007.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nAcoustics, Speech, and Signal Processing, 1995.\nICASSP-95., 1995 International Conference on, vol-\nume 1, pages 181–184. IEEE.\nReinhard Kneser and V olker Steinbiss. 1993. On the\ndynamic adaptation of stochastic language models.\nIn Acoustics, Speech, and Signal Processing, 1993.\nICASSP-93., 1993 IEEE International Conference\non, volume 2, pages 586–589. IEEE.\nTom´aˇs Mikolov, Anoop Deoras, Daniel Povey, Luk ´aˇs\nBurget, and Jan ˇCernock`y. 2011. Strategies for\ntraining large scale neural network language mod-\nels. In Automatic Speech Recognition and Under-\nstanding (ASRU), 2011 IEEE Workshop on, pages\n196–201. IEEE.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nGraham Neubig and Chris Dyer. 2016. Generalizing\nand hybridizing count-based and neural language\nmodels. In Conference on Empirical Methods in\nNatural Language Processing.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nJoaquim F Silva, Carlos Goncalves, and Jose C Cunha.\n2016. A theoretical model for n-gram distribution\nin big data corpora. In Big Data (Big Data), 2016\nIEEE International Conference on, pages 134–141.\nIEEE.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The Journal of Machine Learning\nResearch, 15(1):1929–1958.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8070811033248901
    },
    {
      "name": "Computer science",
      "score": 0.7463414072990417
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7015609741210938
    },
    {
      "name": "Artificial neural network",
      "score": 0.6007351875305176
    },
    {
      "name": "Retraining",
      "score": 0.5998519062995911
    },
    {
      "name": "Word (group theory)",
      "score": 0.5633103847503662
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.515644907951355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4978973865509033
    },
    {
      "name": "Train",
      "score": 0.48532313108444214
    },
    {
      "name": "n-gram",
      "score": 0.4429454505443573
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4414406716823578
    },
    {
      "name": "Machine learning",
      "score": 0.40703094005584717
    },
    {
      "name": "Mathematics",
      "score": 0.10164347290992737
    },
    {
      "name": "Programming language",
      "score": 0.08334493637084961
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    }
  ]
}