{
  "title": "Collaborative three-stream transformers for video captioning",
  "url": "https://openalex.org/W4385617545",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100653158",
      "name": "Hao Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100337161",
      "name": "Libo Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A5047220188",
      "name": "Heng Fan",
      "affiliations": [
        "University of North Texas"
      ]
    },
    {
      "id": "https://openalex.org/A5022111330",
      "name": "Tiejian Luo",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6793119350",
    "https://openalex.org/W6684090549",
    "https://openalex.org/W2905145027",
    "https://openalex.org/W6802341993",
    "https://openalex.org/W2964532449",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1995820507",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6772485217",
    "https://openalex.org/W6765766786",
    "https://openalex.org/W3094751268",
    "https://openalex.org/W2142900973",
    "https://openalex.org/W2963504927",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6640295108",
    "https://openalex.org/W2902469875",
    "https://openalex.org/W6772247646",
    "https://openalex.org/W6785957266",
    "https://openalex.org/W3034815696",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W6654335291",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3035237998",
    "https://openalex.org/W6839062726",
    "https://openalex.org/W6741891468",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3009192917",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4214931354",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W6797128033",
    "https://openalex.org/W2556388456",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963811641",
    "https://openalex.org/W2962990649",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6789747622",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6694260854",
    "https://openalex.org/W6796432426",
    "https://openalex.org/W6631456553",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2139501017",
    "https://openalex.org/W2964241990",
    "https://openalex.org/W2962994439",
    "https://openalex.org/W6724944384",
    "https://openalex.org/W2963315828",
    "https://openalex.org/W6799914745",
    "https://openalex.org/W3035588244",
    "https://openalex.org/W6753623787",
    "https://openalex.org/W6803739019",
    "https://openalex.org/W2889418103",
    "https://openalex.org/W1586939924",
    "https://openalex.org/W6804041311",
    "https://openalex.org/W2963576560",
    "https://openalex.org/W3188999884",
    "https://openalex.org/W2897439619",
    "https://openalex.org/W6763633978",
    "https://openalex.org/W3035365026",
    "https://openalex.org/W3034221024",
    "https://openalex.org/W2968101724",
    "https://openalex.org/W2952132648",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W3006320872",
    "https://openalex.org/W2962861647",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W4382240304",
    "https://openalex.org/W2524365899",
    "https://openalex.org/W4312463400",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W4382464395",
    "https://openalex.org/W3205625102",
    "https://openalex.org/W3210165781",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W3217340782",
    "https://openalex.org/W4312560592"
  ],
  "abstract": null,
  "full_text": "1\nThe published version is available on\nhttps://doi.org/10.1016/j.cviu.2023.103799.\nCollaborative Three-Stream Transformers for Video Captioning\nHao Wanga, Libo Zhangb,∗∗, Heng Fanc, Tiejian Luoa\naSchool of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, 101408, China\nbInstitute of Software, Chinese Academy of Sciences, Beijing, 100190, China\ncDepartment of Computer Science and Engineering, University of North Texas, Denton 76203, Texas, United States of America\nABSTRACT\nAs the most critical components in a sentence, subject, predicate and object require special attention in\nthe video captioning task. To implement this idea, we design a novel framework, named COllaborative\nthree-Stream Transformers (COST), to model the three parts separately and complement each other\nfor better representation. Specifically, COST is formed by three branches of transformers to exploit\nthe visual-linguistic interactions of different granularities in spatial-temporal domain between videos\nand text, detected objects and text, and actions and text. Meanwhile, we propose a cross-granularity\nattention module to align the interactions modeled by the three branches of transformers, then the\nthree branches of transformers can support each other to exploit the most discriminative semantic\ninformation of different granularities for accurate predictions of captions. The whole model is trained\nin an end-to-end fashion. Extensive experiments conducted on three large-scale challenging datasets,\ni.e., YouCookII, ActivityNet Captions and MSVD, demonstrate that the proposed method performs\nfavorably against the state-of-the-art methods.\n© 2023 Elsevier Ltd. All rights reserved.\n1. Introduction\nVideo captioning aims to generate natural language descrip-\ntions of video content, which attracts much attention in re-\ncent years along with the rapidly increasing amount of videos\nrecorded in daily life. It can be adopted for a wide range of real-\nworld applications, such as blind people assistance, automatic\nvideos summarization and classification, and intelligent video\nsurveillance. However, as noted in former works (Xiong et al.,\n2018; Park et al., 2019; Lei et al., 2020), it is very challenging\nto generate natural paragraph descriptions due to the di fficul-\nties of having relevant, less redundant, and semantic coherent\nsentences.\nRecently, researchers attempt to use the transformer model to\nsolve the video captioning task (Vaswani et al., 2017; Dai et al.,\n2019; Iashin and Rahtu, 2020; Zhu and Yang, 2020; Tang et al.,\n2021), which relies on the self-attention mechanism to describe\nthe interactions between di fferent modalities of the input data,\nsuch as video, audio, and text. In practice, the aforementioned\n∗∗Corresponding author:\ne-mail: libo@iscas.ac.cn (Libo Zhang)\nmethods generally concatenate the features extracted from indi-\nvidual modalities, or use self-attention to model the interactions\nbetween extracted features. Although they advance the state-\nof-the-art of video captioning, it is still far from satisfactory\nin real applications due to the domain gap between di fferent\nmodalities. Thus, a question naturally arises, “ How to reduce\nthe domain gap and capture the interactions among visual and\nlinguistic modalities for video captioning?”\nBefore answering this question, let us see the basic grammar\nrules at first. As pointed out in Krishna et al. (2017a) and Zhou\net al. (2018a), a sentence is generally presented as the following\nform, e.g.,\nWomen wear Arabian skirts on a stage.\nwhere Subject, Object, and Predicate are the three most criti-\ncal elements, and indicate the objects, the actions of objects,\nand the interactions among di fferent objects, respectively. We\nbelieve that these components correspond to visual representa-\ntions with different granularities, and modeling mutil-modal in-\nteractions based on them can effectively reduce the domain gap\nand help model understand the content of video better. Thus\nmotivated, we propose a novel framework, called COllabora-\ntive three-Stream Transformers (COST), for video captioning.\narXiv:2309.09611v1  [cs.CV]  18 Sep 2023\n2\nConcate\nVideo Captioning Model\nVideo Captions\nSelf-Attention\nCross-Granularity\nAttention\nxS\nVideo Captions\nVideo Frames Video Frames\n(a) Prior Works (b) COST \n or\n2D/3D Feature Extraction\nText Input Text Input\nConcat\n2D/3D Feature Extraction\nMot\nFeature\nWord\nEmbed\nWord\nEmbed\nSelf-Attention Self-Attention\nConcat Concat\nApp\nFeature\nApp\nFeature\nReg\nFeature\nText\nFeature\nReg\nFeature\nText\nFeature\nMot\nFeature\nFig. 1. Comparison between prior works and the proposed COST. Previ-\nous works tended to concatenate the visual features, i.e., motion feature\n(Mot Feature), appearance feature (App Feature) and region feature (Reg\nFeature), as input to their video captioning model. Di fferently, we pro-\npose a three-branch transformer-based architecture to encode the visual-\nlinguistic interactions of di fferent granularities separately and design the\ncross-granularity module to complement the interactions with each other.\nDifferent from former methods (Zhou et al., 2018b; Lei et al.,\n2020; Dai et al., 2019; Wang et al., 2021) which directly con-\ncatenate visual features of different granularities and model the\ncorrelation between visual and linguistic modality directly (Fig.\n1(a)), COST models the visual-linguistic (Nan et al., 2021;\nFan and Yang, 2020; Seo et al., 2022; Wei et al., 2020; Luo\net al., 2020) interactions of di fferent granularities separately in\nspatial-temporal domain and then fuses the multi-modal fea-\nture in di fferent branches for better video content understand-\ning (Fig. 1(b)). As we argue that these previous works have\noverlooked the domain gap present in the features extracted\nfrom different pre-trained models, and this oversight renders it\nunreasonable to simply concatenate these features as a unified\nvideo representation. Note that, the dashed lines in Fig. 1(a)\nindicate that there exist some works extracting and combining\nregion features with the concatenated motion and appearance\nfeatures (Park et al., 2019) together or sending them parallel\n(Zhang et al., 2020) as the visual input to their models.\nSpecifically, the proposed COST consists of three trans-\nformer branches, including the Video-Text, Detection-Text, and\nAction-Text transformers. The Video-Text transformer is used\nto model the interactions between the global video appearances\nand linguistic texts, which makes the model perceive the gen-\neral content of the video. The Detection-Text transformer aims\nat accurately locating objects in individual video frames, which\nenforces the model to focus on the objects being aligned in\nthe visual and linguistic modalities, i.e., indicating the Subjects\nand Objects in caption sentences. The Action-Text transformer\nis designed to model the actions /relations of objects between\nthe visual and linguistic modalities, i.e., indicating the Predi-\ncate in caption sentences. Meanwhile, to align the interactions\nmodeled by the three branches of transformers, we introduce\na cross-granularity attention model in COST. In particular, the\nsimilarity between interactions from different branches is com-\nputed to represent the relevance among visual modalities and\nhelp inject the information from other interactions. Note that\nwe use the terminology ”granularity” to describe the di fferent\nlevels of detail or scales at which the same input data is pro-\ncessed. For example, our motion features and region features\n(detection features) are both extracted from images, yet they\nprovide distinct information. In contrast, ”modality” is a re-\nlated term that refers to the various types of inputs that a ma-\nchine learning system processes, such as text, image, and au-\ndio. Each modality requires specific processing techniques and\nalgorithms to extract valuable information and patterns e ffec-\ntively. In addition, we introduce additional training objectives\nfor Detection-Text and Action-Text streams separately to align\nthe semantics of embeddings to the underlying video informa-\ntion which is supposed to be conveyed at the time of feature\nextraction, instead of only setting cross-entropy loss between\ngenerated captions and ground-truth descriptions like most for-\nmer methods. As we believe that introducing appropriate guid-\nance can ensure the uniqueness of information in each stream\nand provide complementary information for each other. In this\nway, different branches of transformers support each other to\nexploit more discriminative semantic information in di fferent\nmodalities and granularities, and enforce the model to pay more\nattention on generating the accurate Subject, Object and Pred-\nicate predictions. The whole model is trained in an end-to-end\nfashion using Adam algorithm (Kingma and Ba, 2015).\nExtensive experiments are conducted on three publicly chal-\nlenging datasets, i.e., YouCookII (Zhou et al., 2018a), Ac-\ntivityNet Captions (Krishna et al., 2017a) and MSVD (Chen\nand Dolan, 2011), to demonstrate the superior performance of\nthe proposed method compared to the state-of-the-art methods\n(Zhou et al., 2018b; Dai et al., 2019; Park et al., 2019; Lei\net al., 2020; Wang et al., 2021; Seo et al., 2022; Lin et al.,\n2021). Specifically, our method achieves very competitive\nCIDEr scores with TSN (Wang et al., 2019) features as input,\ni.e., 45.54% and 24.77%, on the YouCookII val set and the Ac-\ntivityNet ae-test set, surpasses or approaches the state-of-the-\narts.\nThe main contributions of this paper are summarized as fol-\nlows:\n1. We propose a novel framework, i.e., COllaborative three-\nStream Transformers (COST), that leverages multiple\ntransformer branches to explore various components in a\nsentence for video captioning.\n2. We design a simple but e ffective cross-granularity atten-\ntion module to align the interactions modeled by di ffer-\nent transformer branches, which supports each other to ex-\nploit discriminative semantic cue from different granulari-\nties for more accurate predictions of captions.\n3. We specially introduce a new training objective for COST\nthat constrains the semantics of embeddings in Detection-\nText branch and Action-Text branch to supply for caption\ngeneration, further enhancing performance.\n4. Extensive experiments conducted on three challenging\ndatasets show that our method performs favorably against\nthe state-of-the-art methods.\n3\n2. Related Work\nVideo Captioning. Over the past few years, video caption-\ning has received increasing attention from both computer vision\nand natural language processing community (Xu et al., 2021),\nwhich aims to generate linguistic description for the video con-\ntent. Early works mainly focus on template-based approaches.\nFor example, Guadarrama et al. (2013) and Das et al. (2013)\nfirst detect the visual objects in a video with human-crafted\nfeatures and then use them to fill the pre-defined templates\nwith slots. However, such methods are restricted in generat-\ning semantically-rich sentences due to the high dependence on\nfixed templates and language rules.\nMotivated by the success of neural network in translation\ntask (Sutskever et al., 2014), the methods of taking caption-\ning task as translation task became popular (Venugopalan et al.,\n2015b; Yu et al., 2016; Pan et al., 2017; Zhang et al., 2018). The\nnature of these methods is performing sequence-to-sequence\nlearning in an encoder-decoder paradigm (Chen et al., 2019),\nwhere the convolutional neural networks (CNNs) (Zheng et al.,\n2020) and Long-Short Term Memory (LSTM) networks (Pei\net al., 2019; Park et al., 2019) are generally adopted to extract\ndiscriminative feature embeddings from input video and gen-\nerate accurate captions separately. To alleviate the heavy com-\nputational burden of applying 2D-CNN, especially 3D-CNN to\ndense frame inputs for visual feature extraction, the methods\nin this paradigm mostly operate on the pre-extracted features\nand the subsequent works mainly concentrate on the improve-\nments in feature extraction or utilization, including 1) multi-\nmodal feature extraction (Wang et al., 2018; Hao et al., 2018;\nHori et al., 2018; Xu et al., 2019; Zhang et al., 2020) and 2)\nfeature utilization optimization (Venugopalan et al., 2015b,a;\nYao et al., 2015; Li et al., 2017; Gao et al., 2020; Chen and\nJiang, 2019). In the former one, HACA (Wang et al., 2018)\nproposes a hierarchically LSTM-based network to learn and\nalign the attentive representations of both visual and audio fea-\ntures at different granularities. DS-RNN (Xu et al., 2019) pro-\nposes a dual-stream framework to model visual and semantic\nfeatures independently and decode the hidden states from both\nmodalities jointly. ORG-TRL (Zhang et al., 2020) not only\nproposes an object relational graph to connect each object in\nvideo and do relational reasoning by graph convolutional net-\nwork, but also designs a teacher-recommended learning method\nto utilize the external language model to improve the genera-\ntion of caption model by integrating the linguistic knowledge.\nIn the later one, LSTM-YT (Venugopalan et al., 2015b) pro-\nposes that mean pooling features across all frames is a ratio-\nnal representation for generating simple descriptions to short\nvideo clips. However, it totally ignores the order of frames in\noriginal video and discards the temporal information which is\ncrucial for video captioning. S2VT (Venugopalan et al., 2015a)\nfirstly introduces sequence-to-sequence model to video caption-\ning task and processes frames and generates words sequentially\nwhile preserving the temporal structure. In order to make the\nmodel concentrate on relevant features for specific word gener-\nation, temporal attention (Yao et al., 2015) is proposed to assign\nhigher weights to relevant features. Apart from the frame-level\nattention, MAM-RNN (Li et al., 2017) proposes that di fferent\nregions in the video frame contribute di fferently to the word\nprediction, and designs a two-layers structure with the first layer\nfocusing on the most salient regions in each frame and the sec-\nond one attending to the most correlated frames.\nRecently, inspired by the superior performance in learning\nlong-range relations through the attention mechanism com-\npared with RNNs(e.g., LSTM and GRU (Chung et al., 2014)),\nTransformer-based models have emerged in this field. Zhou\net al. (2018b) proposes an end-to-end trained transformer\nmodel, where the encoder is designed to extract semantic rep-\nresentations from the video, and the proposal decoder receives\nthe encoding output with different anchors to form video event\nproposals. Sun et al. (2019) designs the VideoBERT model to\nlearn bidirectional joint distributions over sequences of visual\nand linguistic tokens. Lei et al. (2020) develops the memory-\naugmented recurrent transformer, which uses a highly summa-\nrized memory state from the video clips and the sentence his-\ntory to facilitate better prediction of the next sentence. More re-\ncently, by employing the tubelet embedding scheme and factor-\nized encoder architecture from ViViT (Arnab et al., 2021) and\nelaborately designed bi-directional objective, MV-GPT (Seo\net al., 2022) can be applied to raw pixels instead pre-extracted\nvisual features and generates captions directly. SwinBERT (Lin\net al., 2021) is also an end-to-end transformer-based video cap-\ntioning model with video frame patches as input, which uses\nVidSwin (Liu et al., 2021b) as visual encoder to extract video\ntokens and promotes cross-modality representation by applying\nmasked text token prediction. They provide new insights for\nthe future development of this field.\nMulti-modal cross-attention mechanism. The interactions\nbetween different modalities are critical for the video caption-\ning task. Recent transformer based methods (Iashin and Rahtu,\n2020; Zhu and Yang, 2020; Tang et al., 2021) use the cross-\nattention module to learn correlations across di fferent modal-\nities. For example, Iashin and Rahtu (2020) concatenates the\nlearned embeddings from multiple modalities, e.g., video, au-\ndio and speech, for event description. Tang et al. (2021) uses\nframe-level dense captions as an auxiliary text input for better\nvideo and language associations, where the constrained atten-\ntion loss is used to force the model to automatically focus on the\nbest matched caption from a pool of misalignment caption can-\ndidates. Recently, Li et al. (2022) proposed a novel approach to\nstrengthen image and text embeddings through the incorpora-\ntion of action information through cross-attention manner, and\nachieves state-of-art performance on image-text retrieval task.\nDespite sharing some similarities with these approaches in\nusing self-attention to align visual-linguistic interactions, our\napproach is significantly di fferent. In specific, we design the\ncross-granularity attention module integrated in the collabora-\ntive three-stream transformers to align three types of visual-\nlinguistic interactions of di fferent granularities, as we believe\nthat they could support and complement each other, for exam-\nple, the model could more easily understand the current mo-\ntion scene after injecting the information from detection feature\nof ”basketball” and action feature of ”play” to video feature,\nleading to more discriminative semantic cues for better caption\ngeneration.\n4\nMulti-branch architectures. The idea of multi-branch has\nbeen widely applied in various tasks (Liu et al., 2021a; Zhu\nand Yang, 2020; Kim et al., 2020). SibNet (Liu et al., 2021a)\nproposes a two-branch architecture to encode the content and\nsemantic of videos separately and then the both branches are\ncombined and fed to decoder for video caption generation. Act-\nBert (Zhu and Yang, 2020) proposes the tangled transformer\nblock to encode three sources of information, i.e., global ac-\ntions, local regional objects, and linguistic descriptions to learn\nbetter video-text representation. MTTSNet (Kim et al., 2020)\ndefines three region features corresponding to part-of-speech\n(POS) relation and designs a triple-stream network to encode\nthese features separately and merges the processed embeddings\nto generate caption and predicts POS of each word for image\ncaption task.\nOur COST also adopts the multi-branch architecture, but it\nis different than the above methods from two aspects. First, the\ndesign motivations are di fferent. We use three-branch archi-\ntecture to encode the visual-linguistic interaction of di fferent\ngranularities to disentangle the complex video information by\nself-attention, and use the cross-granularity attention module to\ncomplement and enhance the embeddings extracted from differ-\nent branches, so as to improve the video analysis ability of the\nmodel to generate accurate description. Second, the contribu-\ntions of branch to task are di fferent. For example, ActBert and\nMTTSNet only set optimization objective for the whole model\ninstead of each branch, and SibNet designs specific loss for two\nbranches but only to improve the ability of the model to extract\nfeature from the video. Differing from them, we set training ob-\njective for each branch to align the semantics of pre-extracted\nvisual features to correspond to POS, and the embeddings in\neach branch are semantically complementary and directly par-\nticipate in caption generation after information fusion.\nMulti-modal pre-training models. Large-scale pre-training\nis another e ffective way to improve the accuracy of caption-\ning models. Specifically, the jointly trained video and language\nmodels (Sun et al., 2019; Zhu and Yang, 2020; Huang et al.,\n2020; Luo et al., 2020; Ging et al., 2020; Seo et al., 2022) on the\nlarge-scale datasets, such as YouTube-8M (Abu-El-Haija et al.,\n2016) and HowTo100M (Miech et al., 2019) with automatic\nspeech recognition1 transcripts, provide discriminative features\nfor downstream tasks, such as video captioning, action local-\nization and et al.. VideoBert (Sun et al., 2019) and ActBert\n(Zhu and Yang, 2020) collect large paired video sequences and\ntext descriptions with the help of off-the-shelf automatic speech\nrecognition (ASR) model, and construct the BERT-style objec-\ntive to train the video and text encoders simultaneously. Huang\net al. (2020) constructs a dense video captioning dataset, i.e.,\nVideo Timeline Tags (ViTT), and explores several multi-modal\nsequence-to-sequence pre-raining strategies using transformers\n(Vaswani et al., 2017). Luo et al. (2020) also applies trans-\nformers with two single-modal encoders to encode the video\nand text separately, a cross encoder to model the interactions\nbetween video and text representations, and a decoder to recon-\nstruct or generate text. Ging et al. (2020) develops the Coop-\n1https://developers.google.com/youtube/v3/docs/captions\nerative hierarchical Transformer (COOT) to model the interac-\ntions between di fferent levels of granularities and modalities,\nwhich achieves superior results on video-text retrieval task and\nprovides learned representations to improve the performance of\nvideo captioning model (Lei et al., 2020) significantly. Re-\ncently, Seo et al. (2022) proposes that recent visual-language\npre-training frameworks lack the ability to generate sentences\nand presents MV-GPT with novel bidirectional objective for\ngeneration task.\n3. Our Approach\nAs discussed above, we design the collaborative three-stream\ntransformers to model the interactions of objects, and ac-\ntions/relations of objects between di fferent modalities, which\nis formed by three branches of transformers, i.e., Video-Text,\nDetection-Text, and Action-Text transformers. Specifically, the\nvideo and text inputs are firstly encoded to extract the multi-\nmodal feature embeddings. After that, the embeddings are fed\ninto the three-stream transformers to exploit the interactions be-\ntween linguistic embedding and visual embeddings of different\ngranularities in spatial-temporal domain,i.e., global videos, ob-\nject regions, and actions. Meanwhile, the cross-granularity at-\ntention module is designed to align the interactions modeled by\nthe three branches of transformers. The overall architecture of\nthe proposed method is shown in Fig. 2.\n3.1. Multi-Modality Tokens\nThree kinds of tokens, i.e., visual tokens, linguistic tokens,\nand special tokens, are used to express the video and text inputs,\nwhich are described as follows.\nVisual tokens. For the visual tokens, we use three kinds of to-\nkens for different granularities in spatial-temporal domain, that\nis the video tokens, the detection tokens, and the action tokens.\n• Video tokens provide the global semantic information in\nthe video sequence. In contrast to Lei et al. (2020),\nwe only use the appearance features extracted by Tem-\nporal Segment Networks (TSN) (Wang et al., 2016) (de-\nnoted as TSN-APP in Fig. 2) as the video tokens, i.e.,\n{f v\n1 , f v\n2 ,··· , f v\nNv\n}, where f v\ni is the extracted feature of the i-\nth video clip, andNv is the number of video clips. Notably,\nwe can also leverage more powerful multi-modal feature\nextraction method COOT (Ging et al., 2020) to improve\nthe performance, which is pre-trained on the large-scale\nHowTo100M dataset (Miech et al., 2019).\n• Detection tokensare used to enforce the model to focus on\nthe Subjects or Objects in caption sentences. Similar to\nPark et al. (2019); Lu et al. (2019); Zhu and Yang (2020),\nwe use the Faster R-CNN method with 101-layer residual\nnet (ResNet-101) (He et al., 2016) as backbone to detect\nthe objects in each frame, which is pre-trained on the Vi-\nsual Genome dataset (Krishna et al., 2017b). After that,\nthe detection features in Faster R-CNN corresponding to\n5\nFC & Concat\nTSN-MOTWord Embedding\nOutput Captions\n(shifting right) Video Clips\nCross-Granularity\nAttention\nFeed Forward\nFeed Forward\nFFNaction\npos\nxS\nFC & Concat\nMasked\nMulti-Head\nAttention\nTSN-APP\nFeed Forward\nFeed Forward\nFFNword\npos\nxS\nFC & Concat\nFaster R-CNN\nFeed Forward\nFeed Forward\nFFNnoun\npos\nxS\nQ QKV K V\nMasked\nMulti-Head\nAttention\nMasked\nMulti-Head\nAttention\nH Y X\nY H X XH Y X Y H\nLN\nLN LN\nLN\nLN\nLN\nAction-Text Transformer Video-Text Transformer Detection-Text Transformer\nNax1024 Nvx1024 Ndx1024\n(Na+Nt)x768 (Nv+Nt)x768 (Nd+Nt)x768\nMasked\nMulti-Head\nAttention\nMasked\nMulti-Head\nAttention\nCross-Granularity\nAttention\nCross-Granularity\nAttention\nFig. 2. The network architecture of the proposed COST method, which is formed by three branches of transformers, i.e., the Action-Text, Video-Text\nand Detection-Text transformers. The cross-granularity attention module is designed to align the interactions modeled by the three di fferent branches of\ntransformers, and Y, H and X represent the interactions of these branches separately.\nthe objects with the highest confidence scores in K cate-\ngories2 are used to generate the detection tokens for each\nframe. We use {f d\n1 , f d\n2 ,··· , f d\nNd\n}to denote the set of detec-\ntion tokens, where f d\ni is the i-th detection feature, and Nd\nis the total number of detections in the video sequence.\n• Action tokensare designed to enforce the model to concen-\ntrate on the Predicates in caption sentences. Following Lei\net al. (2020), the optical flow features of video sequences\nare extracted by TSN (Wang et al., 2016) (denoted as TSN-\nMOT is Fig. 2) to generate the action tokens, which are\nused to describe the actions /relations of objects. The ac-\ntion tokens are denoted as {f a\n1 , f a\n2 ,··· , f a\nNa\n}, where f a\ni is\nthe motion feature of the i-th video clip, and Na is the total\nnumber of action tokens. It is noted that the Nv is always\nequal to Na in our method based on the pre-trained model\n2If the category number of the detected objects is less thanK in a frame, we\nselect the K detected objects with the highest confidence scores regardless the\nobject categories to generate the detection tokens.\nwe use for extracting appearance features and motion fea-\ntures.\nLinguistic tokens. We break down the captions of video se-\nquences into individual words and compute the correspond-\ning linguistic tokens using the GloVe model (Pennington et al.,\n2014). The linguistic tokens are denoted as {f t\n1, f t\n2,··· , f t\nNt\n},\nwhere f t\ni is the extracted features of the i-th word using the\nGloVe model, and Nt is the total number of words. In light of\nthe emergence of more powerful language representation mod-\nels, such as BERT (Devlin et al., 2019) and CLIP (Radford\net al., 2021), GloVe appears outdated as a model for vector rep-\nresentations for text. Nonetheless, we utilize GloVe features\nto represent captions for the sake of comparison fairness, as\nthe main methods under comparison, such as MART Lei et al.\n(2020) and PDVC Wang et al. (2021), rely on GloVe as their\ntext encoding model. Additionally, setting feature vectors to\nspecial characters by first random initialization with the same\nshape as features provided by GloVe, such as ”[EOS]”, and the\nsemantics of these characters can to be learned during the train-\ning process, rendering GloVe a suitable choice for the current\n6\ntask.\nSpecial tokens. Besides the aforementioned tokens, we also in-\ntroduce three kinds of special tokens in transformer, similar to\nBERT (Devlin et al., 2019). The first one is the granularity to-\nken [CLS], which is added at the beginning of visual features to\ndenote which granularity the following tokens belong to. The\nsecond one is the three kinds of separation token, i.e., [SEP],\n[BOS], and [EOS]. [SEP] is used at the end of the visual to-\nkens to separate them from the linguistic tokens, [BOS] is used\nto denote the beginning of linguistic tokens, and [EOS] is used\nto denote the ending of the linguistic tokens, respectively. And\nthe last one is the padding token [PAD] with two purposes: sup-\nplementing the visual token sequence or text token sequence to\nspecified length for training or inference in parallel; substitut-\ning for all linguistic tokens in inference phase so no caption\ninformation will be leaked to the model. In addition, we use\na fully-connected layer to encode the aforementioned tokens to\nthe same dimension. Thus, the inputs for the three-stream trans-\nformer are computed as\n\b[CLS(·)], f (·)\n1 , f (·)\n2 ,··· , f (·)\nN(·) ,[SEP],[PAD]×N1 ,\n[BOS], f t\n1,··· , f t\nNt ,[EOS],[PAD]×N2\n\t (1)\nwhere (·) ∈{v,d,a}indicates the video, detection, and action\ntokens, respectively. And the subscript of [PAD] means the\nnumber of it to supplement. Please note that the purpose of\nconcatenating visual and linguistic tokens is not to directly fuse\ntheir information. Rather, the interaction between the features\nof two modalities will be encoded through a subsequent masked\nmulti-head attention module. Thus, we choose concatenation as\nit helps to preserve the information of both modalities. We also\nuse the positional encoding strategy (Vaswani et al., 2017) in\nthe Video-Text, Detection-Text, and Action-Text transformers\nto describe the order information of caption sentences.\n3.2. Three-Stream Transformers\nAs shown in Fig. 2, we feed the aforementioned tokens into\nthe three-stream transformers. The Video-Text, Detection-Text,\nand Action-Text branches are formed by S basic blocks, and\neach block mainly consists of a self-attention module and a\ncross-granularity attention module. Both the self-attention and\ncross-granularity modules are followed by a feed forward layer.\nSelf-attention module. The self-attention module is designed\nto model the visual-linguistic alignments in each branch of\ntransformers, i.e., Video-Text, Detection-Text, and Action-Text.\nFollowing Vaswani et al. (2017), we compute the attention\nfunction between different tokens as follows.\nA(Q,K,V) = softmax\u0000QKT\n√\nd\n\u0001V (2)\nwhere Q, K and V are created by inputting tokens into three\ndifferent fully-connected layers in each branch. And their di-\nmensions are RN×d, where N and d are the number of tokens\nand the dimension of embeddings, respectively. We advocate\nh paralleled heads of scaled dot-product attentions to increase\nthe diversity. It is noted that we also add mask to the scaled\ndot-product before applying Softmax to prevent the model from\nseeing future words (Vaswani et al., 2017). We denote the\nwhole module as Masked Multi-Head Self-Attentionin Fig. 2\nQ Q K V K V \nM-MHA \nH Y X Y X H \nQ K V \nConcat \nM-MHA M-MHA \n(a) Concatenated (b) Parallel\nFig. 3. Two cross-granularity attention architectures to merge embed-\ndings from different branches. M-MHA denotes the Masked Multi-Head\nAttention. X, Y and H indicate the hidden states from Detection-Text\nbranch, Action-Text branch and Video-Text branch separately and both\nsub-figures demonstrate the process of information fusion from the other\ntwo branches to Action-Text branch.\nand the output is merged with input using a residual connection\nand layer norm.\nCross-granularity attention module. Besides the self-\nattention module, we use the cross-granularity attention module\nto align the interactions modeled by the three branches of trans-\nformers. Specifically, we complement and enhance the embed-\ndings in each branch by injecting the information from other\nbranches based on cross-attention between them. The branch\nthat is incorporated into information and the other two branches\nare dubbed as injected branch and supply branches for better\nunderstanding the following description. We propose two ar-\nchitectures as shown in Fig. 3 and compare their performance\nby experiments. It is important to note that we only illustrate\nthe implementation of the cross-granularity attention module\nwithin the Action-Text branch in Fig 3. Specifically, embed-\ndings from this branch are utilized as the query to fuse informa-\ntion from the other two branches, while the embeddings from\nthe other two branches similarly serve as queries within their\nrespective cross-granularity attention modules.\nConcatenated ArchitectureA simple and straightforward way\nto inject the related information from supply branches is to con-\ncatenate the embeddings from them and use the obtained result\nas key and value while regarding the embeddings from injected\nbranch as query in the multi-head attention layer. The whole\nprocess in Action-Text branch is shown in Fig. 3(a) and the\nother two branches perform the same computation with their\nown embeddings as query. The computation process is\nMY = softmax\u0000Y ⊙\u0000\u0002H; X\u0003T\u0001\u0001 (3)\nY′= FFN\u0000MY ⊙\u0000\u0002H; X\u0003\u0001\u0001 (4)\nwhere ⊙represents the dot product, \u0002·; ·\u0003 denotes the con-\ncatenation operation, FFN is the feed forward layer, X,Y and H\ndenote the hidden states from Detection-Text branch, Action-\nText branch and Video-Text branch separately. We estimate the\nmodality-wise normalization score of the interactions using a\nsoftmax layer. It should be noted that we add the mask here\nto prevent possible linguistic information leakage to the model.\nThe affinity matrix MY ∈R(Na+Nt)×(Nv+Nd+2Nt) and MY(i, j) de-\nnotes the normalized interaction score between the i-th entity\nin the Action-Text embeddings and the j-th entity in the con-\n7\ncatenated Video-Text and Detection-Text embeddings. Based\non the matrix, the feature embeddings of the Action-Text trans-\nformer can extract information from other branches. Feed for-\nward layer is used to further encode the extracted information\nwhich will be merged to Action-Text transformer.\nParallel ArchitectureAs shown in 3 (b), the other architec-\nture design of cross-granularity attention is to feed embedding\nfrom one of supply branches and that from the injected branch\nto multi-head attention layer parallel, and then fuse the result\nfrom the two attention layers. The computation process is\nMYH = softmax(Y ⊙HT) MYX = softmax(Y ⊙XT) (5)\nY′= FFN(MYH ⊙H + MYX ⊙X) (6)\nThen Y′ will be merged with Y using the residual connec-\ntion and layer norm. Notably, we evaluate our model equipped\nwith different cross-granularity architecture and analyse the re-\nsult in Ablation Studies. The results show that the our model\nwith the parallel cross-granularity structure performs better, and\nwe keep this structure in all comparisons with other methods.\nWe apply the cross-granularity attention in all blocks for each\nbranch of transformers. In this way, the visual entities of dif-\nferent granularities can enhance each other with more discrim-\ninative semantics for video captioning. It is noteworthy that\nwe can leverage the video-text features in history to obtain the\nlong-term sentence-level recurrence to generate the next sen-\ntences according to Lei et al. (2020), which can further enhance\nthe performance of our model.\n3.3. Optimization Objective\nAs we stated before, we introduce specified loss to each\nstream to guide the training of our COST method. This ap-\nproach helps in preserving the semantics of visual features at\ndifferent granularities and facilitate their complementary na-\nture. The loss is formed by three terms, i.e., Lv(·,·) for the\nVideo-Text transformer, Ld(·,·) for the Detection-Text trans-\nformer, and La(·,·) for the Action-Text transformer, i.e.,\nL=Lv(ℓv,[{f t\n1,··· , f t\nNt ,[PAD]×N2 }])\n+ λd ·Ld(ℓd,[{f d\n1 , f d\n2 ,··· , f d\nNd }])\n+ λa ·La(ℓa,[CLSa])\n(7)\nwhere λd and λa are the pre-set parameters used to balance these\nthree terms. Lv(·,·) is the cross-entropy loss used to penal-\nize the errors of the predicted captions by the linguistic tokens\nfrom Video-Text branch comparing to the ground-truth descrip-\ntions ℓv, which are the indexes of words in caption sentences.\nLd(·,·) is also the cross-entropy loss used to penalize the er-\nrors of the predicted categories of objects by the visual tokens\nfrom Detection-Text branch comparing to the pseudo category\nlabels generated by the Faster R-CNN detector 3. La(·,·) is de-\nsigned as the multi-label classification loss because there may\n3Notably, we do not use the annotated objects for model training, but use the\npseudo category labelℓd generated by the Faster R-CNN detector as the ground-\ntruth label to enforce the network to maintain the original encoded semantic\ninformation of detector. The reason is that the ground-truth of video captioning\ndoes not explicitly provide the categories of items in the scene and there is a\none-to-one correspondence between pseudo categories and extracted features.\nbe multiple actions in a video clip. Specifically, we first aggre-\ngate all action tokens as the granularity token [CLS a] and then\ncompute the confidence score using a fully-connected layer,\ni.e., FC([CLSa]). Following Zhang et al. (2021) and Sun et al.\n(2020), the loss function is computed as\nLa(ℓa,[CLSa]) = log \u00001 +\nX\ni∈Ωpos(ℓa)\ne−si \u0001 + log \u00001 +\nX\nj∈Ωneg(ℓa)\nesj \u0001\n(8)\nwhich expects the confidence scores si of the existing actions\nΩpos(ℓa) in video sequences are greater than the predefined\nthreshold 0 while the confidence scores sj of non-existing ac-\ntions Ωneg(ℓa) are less than 0. The ground-truth action labels\nare the most common verbs in caption sentences, which are re-\ntrieved by the o ff-the-shelf part-of-speech tagger method (Sun\net al., 2019).\n4. Experiments\n4.1. Datasets And Evaluation Metrics\nDatasets. We conduct several experiments on two challenging\ndatasets, i.e., YouCookII (Zhou et al., 2018a) and ActivityNet\nCaptions (Krishna et al., 2017a).\n1) YouCookII includes 2 ,000 long untrimmed videos de-\nscribing 89 cooking recipes, where each video contains one ref-\nerence paragraph and is further split into several event segments\nwith annotated sentences. 1 ,333 and 457 video sequences are\nused for training and validation, respectively.\n2) ActivityNet Captions is a large-scale dataset formed by\n10,009 videos for training and 4,917 videos for validation and\ntesting. Notably, since the testing set is not publicly available,\nfollowing Zhou et al. (2019), the original validation set is split\ninto the ae-val subset with 2,460 videos for validation and the\nae-test subset with 2,457 videos for testing.\n3) MSVD consists of 1970 video clips collected from\nYouTube and each clip is spanning over 10 to 25 seconds, with\n40 sentences as annotations. Following Chen and Jiang (2021);\nYe et al. (2022), we split the dataset into 1 ,200, 100, and 670\nvideo clips for training, validation and testing, respectively.\nData Preprocessing. For fair comparisons, we try to keep con-\nsistent with the previous works. For YouCookII and Activi-\ntyNet Captions datasets, we use the features extracted provided\nby Zhou et al. (2018b) as video features and motion features\nto represent videos. Specifically, firstly the video is down-\nsampled to extract frame every 0 .5s, then the video feature is\nextracted from the ’Flatten-673’ layer in ResNet-200 (He et al.,\n2016) with the dimension of 2048 and motion feature is ex-\ntracted from the ’global pool’ layer in BN-Inception (Io ffe and\nSzegedy, 2015) with the dimension of 1024. Both networks\nare pre-trained on ActivityNet dataset (Heilbron et al., 2015)\nfor the action recognition task. Following previous works (Ye\net al., 2022; Zhang et al., 2020), we utilize InceptionResNetV2\n(Szegedy et al., 2017) and C3D (Tran et al., 2015) to extract\n1536-dim video features and 2048-dim motion features, respec-\ntively. The two models are pretrained on ImageNet dataset\n(Russakovsky et al., 2015) and Sports 1M dataset Karpathy\n8\net al. (2014) separately. And we use 16-frame clips with 8-\nframe overlap as input to C3D. For all the three datasets, we uni-\nformly use Faster R-CNN (Ren et al., 2017) with ResNet-101\nas the backbone to extract the object representation as detection\nfeatures with the dimension of 2048. Considering the trade-\noff between accuracy and complexity, we give preference to 5\ndetection features with highest confidence and different classes\nper frame. As we predefine that at most 100 frames are sam-\npled from one video clips, i.e., Nv = Na = 102,Nd = 502 with\nthe two special tokens [CLS] and [SEP]. Meanwhile, we exploit\nthe first 20 words in the caption sentences and compute the 300-\ndim GloVe features, i.e., Nt = 22 with the two special tokens\n[BOS] and [EOS]. It is worth noting that, we selected 20 as\nthe truncation number firstly to be consistent with the previous\nmethod. Concurrently, we also conducted a statistical analysis\non the distribution of caption lengths across the three datasets.\nThe results indicate that over 90% of captions fall within this\nrange, and even more than 97% of YouCookII satisfies this cri-\nterion. Therefore, this choice is a reasonable decision. For the\nCOOT features, we concatenate the local clip-level (384-dim)\nand the global video-level (768-dim) features to describe the\nvideos. After the fully-connected layer, all the tokens are con-\nverted into the 768-dim features.\nEvaluation metrics. Similar to former works (Park et al.,\n2019; Lei et al., 2020; Zhu and Yang, 2020), we use several\nstandard metrics to evaluate our method, including BLEU@ n\n(B@n) (Papineni et al., 2002) for n-gram precision, METEOR\n(M) (Denkowski and Lavie, 2014) for n-gram with synonym\nmatching, CIDEr-D (C) (Vedantam et al., 2015) for consen-\nsus measurement, Rouge(R) (Lin, 2004) for longest subse-\nquence similarities and Repetition@4 (R@4) (Xiong et al.,\n2018; Park et al., 2019) for n repetition in the description. No-\ntably, two evaluation modes are considered, i.e., micro-level\nand paragraph-level. The micro-level evaluation reports the\naverage score on all video sequences separately; while the\nparagraph-level evaluation first concatenates the caption sen-\ntences of all video sequences and then computes the scores av-\neraged across all videos based on the ground-truth paragraph\ncaption sentences, which aims to preserve the story flow with\ncoherence and conciseness. Following existing works (Xiong\net al., 2018; Lei et al., 2020), we apply the paragraph-level eval-\nuation to YouCookII and ActivityNet Captions datasets as they\nannotates multiple clips in each videos. In both modes, CIDEr\nis used as the primary metric for ranking.\n4.2. Implementation Details\nOur COST algorithm is implemented using PyTorch. All\nthe experiments are conducted on a machine with 2 NVIDIA\nRTX-3090 GPUs. We train the model using the strategies sim-\nilar to BERT (Devlin et al., 2019). Specifically, we use Adam\n(Kingma and Ba, 2015) with an initial learning rate of 1 e −4,\nβ1=0.9, β2=0.999, L2 weight decay of 0 .01, and the learning\nrate warmup over the first 2 epochs. We train the model at most\n20 epochs with early-stop strategy based on CIDEr-D and the\nbatch size is set to 64. For each branch of transformers, we set\nthe dimension of the feature embeddings d = 768, the number\nof transformer blocks S = 2, and the number of attention heads\nh = 12. The loss weights λa and λd in equation 7 are set to 2.0\nand 0.02 which are verified by multiple experiments. It is noted\nthat text tokens are all initialized as [PAD] at the time of evalu-\nation and the description is generated word by word: one word\ntoken is generated at a time and substitutes for [PAD] in the cor-\nresponding position, and then sent to the model with previously\ngenerated tokens to predict the next one until [EOS] token is\ngenerated or the length of sentence reaches the maximum.\n4.3. Evaluation Results\nWe compare the proposed COST method with the state-\nof-the-art methods on the three challenging datasets, i.e.,\nYouCookII, ActivityNet Captions and MSVD. As shown in Ta-\nble 1, our method obtains the competitive results on both the\nYouCookIIval subset and the ActivityNet Captionsae-test sub-\nset. Without using the COOT features, our method approaches\nthe state-of-art work, i.e., VLTinT (Yamazaki et al., 2022) on\nthe YouCookII val subset. Beside using 3D-CNN network and\na human detector to extract global and local visual features,\nVLTinT utilizes Language-Image Pre-training (CLIP) (Radford\net al., 2021) to obtain additional linguistic features. And com-\npare our work with other works which don’t introduce addi-\ntional linguistic features, it can be observed that our method\nimproves near 10% CIDEr score compared to the second best\nmethod, i.e., MART (Lei et al., 2020). This is attributed to the\nfact that we use the multi-branch structure to process visual in-\nformation at different granularities and make them complemen-\ntary to each other through the proposed cross-granularity atten-\ntion module, which make it easier for the model to understand\nthe video content. Besides, our method exploits the local ap-\npearance information from the Detection-Text transformer for\nmore accuracy caption generation. We also give experiment re-\nsults in ablation part to verify their e ffects respectively. Using\nthe COOT features, the overall video captioning results are sig-\nnificantly improved, which demonstrates that the video features\nextracted by di fferent pre-trained models have a great impact\non the performance of the captioning model, and our COST\nmethod also performs favorably against other algorithms by im-\nproving over 3% CIDEr score. We observe that the similar trend\nappears in the ActivityNet Captions ae-test subset. It is worth\nnoting that PDVC (Wang et al., 2021) performs better on Activ-\nityNet but relatively poorly on YouCookII, probably due to the\ndifferent characteristics of the two datasets: each video of Ac-\ntivityNet lasts 120s with 3.65 temporally-localized sentences\non average while the duration of each video in YouCookII is\n320s with 7.7 annotated segments and associated sentences.\nHowever, COST has a balanced and satisfactory performance\non both datasets, which further proves the e ffectiveness of our\nmethod. As presented in Table 2, we also compare the proposed\nmethod to some LSTM based methods with input detection fea-\ntures on the ActivityNet Captionsae-val subset. Note that Table\n1 and 2 don’t list same number of compared methods is because\nthese LSTM-based methods had only evaluated on the ae-val\nsubset of Activity Captions and provided the corresponding re-\nsults. Compared to AdvInf (Park et al., 2019), which also in-\nputs multi-modal features (concatenation of image recognition,\naction recognition and object detection features) and designs\n9\nTable 1. Experimental Results on the YouCookII Val Subset and ActivityNet Captions Ae-Test Subset in the Paragraph-Level Evaluation\nmode. COOT Indicates That the Evaluated Methods Use the Feature Extracted by COOT (Ging et al., 2020) Pre-Trained on HowTo100M\n(Miech et al., 2019). * indicates that VLTinT utilizes extra linguistic features obtained through CLIP (Radford et al., 2021). We Report\nBLEU@4 (B@4), METEOR (M), CIDEr-D (C) and Repetition@4 (R@4).\nMethod COOT YouCookII (val) ActivityNet Captions (ae-test)\nB@4 M C R@4 ↓ B@4 M C R@4 ↓\nVanilla Transformer (Zhou et al., 2018b) ✗ 7.62 15.65 32.26 7.83 9.31 15.54 21.33 7.45\nTransformer-XL (Dai et al., 2019) ✗ 6.56 14.76 26.35 6.30 10.25 14.91 21.71 8.79\nTransformer-XLRG (Lei et al., 2020) ✗ 6.63 14.74 25.93 6.03 10.07 14.58 20.34 9.37\nMART (Lei et al., 2020) ✗ 8.00 15.90 35.74 4.39 9.78 15.57 22.16 5.44\nPDVC (Wang et al., 2021) ✗ 7.11 15.05 26.03 7.65 11.36 15.73 25.03 10.92\nVLTinT* (Yamazaki et al., 2022) ✓ 9.40 17.94 48.70 4.29 14.50 17.97 31.13 4.75\nCOST ✗ 9.47 17.67 45.54 4.04 11.14 15.91 24.77 5.86\nVanilla Transformer (Zhou et al., 2018b) ✓ 11.05 19.79 55.57 5.69 10.47 15.76 25.90 19.14\nTransformer-XL (Dai et al., 2019) ✓ - - - - 10.57 14.76 22.04 15.85\nMART (Lei et al., 2020) ✓ 11.30 19.85 57.24 6.69 10.85 15.99 28.19 6.64\nSART (Man et al., 2022) ✓ 11.43 19.91 57.66 8.58 11.35 16.21 28.35 7.18\nCOST ✓ 11.56 19.67 60.78 6.63 11.88 15.70 29.64 6.11\nTable 2. Comparison With the State-of-the-Art Methods on Activ-\nityNet Captions Ae-Val Subset in the Paragraph-Level Evaluation\nMode. Det. And Re. Indicate Whether the Model Uses Detec-\ntion Features And the Sentence-Level Recurrence. * indicates that\nVLTinT utilizes extra linguistic features obtained through CLIP\n(Radford et al., 2021).\nDet. Re. B@4 M C R@4 ↓\nLSTM based methods:\nMFT (Xiong et al., 2018) ✗ ✓ 10.29 14.73 19.12 17.71\nHSE (Zhang et al., 2018) ✗ ✓ 9.84 13.78 18.78 13.22\nLSTM based methods with detection feature:\nGVD (Zhou et al., 2019) ✓ ✗ 11.04 15.71 21.95 8.76\nGVDsup (Zhou et al., 2019) ✓ ✗ 11.30 16.41 22.94 7.04\nAdvInf (Park et al., 2019) ✓ ✓ 10.04 16.60 20.97 5.76\nTransformer based methods:\nVanilla Transformer (Zhou et al., 2018b)✗ ✗ 9.75 15.64 22.16 7.79\nTransformer-XL (Dai et al., 2019)✗ ✓ 10.39 15.09 21.67 8.54\nTransformer-XLRG (Lei et al., 2020)✗ ✓ 10.17 14.77 20.40 8.85\nMART (Lei et al., 2020) ✗ ✓ 10.33 15.68 23.42 5.18\nPDVC (Wang et al., 2021) ✗ ✗ 11.80 15.93 27.27 10.68\nSART (Man et al., 2022) ✗ ✓ 11.35 16.21 28.35 7.18\nVLTinT* (Yamazaki et al., 2022)✗ ✗ 14.93 18.16 33.07 4.87\nCOST ✓ ✓ 11.22 16.58 25.70 7.09\nthree discriminators to enhance the fluency and relevance in the\ngenerated captions, our method produces higher scores for both\nB@4 and CIDEr, demonstrating the superiority of the collab-\norative transformers to learn multi-modal representations over\nLSTM. Meanwhile, MART (Lei et al., 2020) without input de-\ntections performs inferior than our method in terms of B@4, M\nand C metrics, even though it utilizes complex memory module\nto store video-text features in history to generate more coherent\nand accurate captions. It indicates that detection features are\nimportant entity for video captioning task which is neglected\nby recent transform-based approaches, and aligning the inter-\nactions between these visual features of di fferent granularities\ncan contribute greatly to generating accurate captions. PDVC\n(Wang et al., 2021) adds multiple temporal convolutional layers\nto obtain feature sequences across multiple resolutions as input\nand performs better than our method in terms of B@4 and C\nmetrics, but the sentences generated by it have significant re-\ndundancy (with high R@4 metric) due to complex input. It is\nworth noting that both SART (Man et al., 2022) and VLTinT\nTable 3.Comparison With State-of-the-Art Methods on YouCookII\nVal Subset in the Micro-Level Evaluation Mode. * indicates the\nevaluated method had been pre-trained on large-scale datasets and\n†indicates the evaluate method using multi-modal features ex-\ntracted by COOT (Ging et al., 2020).\nMethod B@3 B@4 M R C\nMasked Trans. (Zhou et al., 2018b) 7.53 3.84 11.55 27.44 0.38\nS3D (Xie et al., 2017) 6.12 3.24 9.52 26.09 0.31\nVideoBERT* (Sun et al., 2019) 6.80 4.04 11.01 27.50 0.49\nVideoBERT+S3D* (Sun et al., 2019) 7.59 4.33 11.94 28.80 0.50\nActBERT* (Zhu and Yang, 2020) 8.66 5.41 14.30 30.56 0.65\nSwinBERT* (Lin et al., 2021) 13.80 9.00 15.60 37.30 1.09\nCOST 10.69 6.63 12.61 31.09 0.71\nCOST† 13.50 8.65 15.62 36.53 1.05\n(Yamazaki et al., 2022) both performs well on this subset while\nbased on different solutions: the former one proposes a scenario\nunderstanding module to enhance the corresponding ability of\ntheir model while the latter captures coherent semantics by in-\ntroducing more powerful linguistic features, which provides in-\nsights for future exploration in this dataset.\nAccording to Table 3, our method outperforms several BERT\nbased methods which pre-trained on large-scale datasets in\nterms of the micro-level evaluation. In particular, ActBERT\n(Zhu and Yang, 2020) pre-trains on HowTo100M (Miech et al.,\n2019) firstly and then relies on better visual modalities than our\nmethod (for example, its extraction network for action feature\npre-trained on Kinetics dataset (Kay et al., 2017)). However, it\ndirectly focuses on the learning the alignment between text and\nother visual modalities, which is difficult to exploit the discrim-\ninative semantic information. In contrast, our cross-granularity\nattention module learns from three visual-linguistic interactions\nof di fferent granularities using the three-stream transformers,\nproducing better CIDEr score. SwinBERT (Lin et al., 2021)\nis proposed as a pure Transformer-based end-to-end architec-\nture for video captioning, which leverages VidSwin (Liu et al.,\n2021b) pre-trained on Kinetics-600 to extract spatial-temporal\nvideo representations and then proposes Multi-modal Trans-\n10\nGT: blend the onions and garlicMART: add garlic onion and ginger to the potPDVC:add salt and pepper to the bowlVLTinT:blend the ingredients in the food processorOurs: addchopped onions and garlic to the food processorand blend\nadd,blend stop, put\nblenderhandring bowl\nhandtablefoodplatebowl\nhand\nbowlspoononionsfood\nadd, stir\nbowl pan\nwater\nshrimpfood\nGT: fry garlic and shrimp in the wokMART: add salt and pepper to the potPDVC:add oil to the pot VLTinT:add oil and the wok to the wokOurs: addthe shrimpto the wokand stir\nwindownose\nbottlewall\nhead\nheadbottlearm\nlightwallhair\nhand\nshirt\nman\nman\nhead\nhairshirt\nhead\nshirtwindowhandwallbottle\nnose\nhold, play\nGT: A man is standing in front of a window playing a violinMART: a woman is standing in front of a mirrorPDVC: a woman is holding a violin and a womanVLTinT:a woman is shown playing a violin in the background as well as a small group of people watching onOurs: a man is seen standing in front of a windows holdinga violin and begins to playwith a violin\nGT: The guy lets go of the handles and puts his hands to his sideMART: he finishes and walks awayPDVC:he is standing on a tableVLTinT:the man finishes and stands upOurs: the man stopsand putshis handsin the air\nFig. 4. Qualitative comparison with MART (Lei et al., 2020), PDVC (Wang et al., 2021) and VLTinT (Yamazaki et al., 2022) on YouCookII\nval and ActivityNet Captions ae-val split. The Subject, Predicate, and Object in a sentence are highlighted in the red, green and blue\nfonts, respectively. Best viewed in color.\nTable 4.Comparison With State-of-the-Art Methods on MSVDTest\nSubset in the Micro-Level Evaluation Mode.\nMethod B M R C\nOA-BTG (Zhang and Peng, 2019) 56.9 36.2 - 90.6\nMSGA (Chen and Jiang, 2019) 52.5 34.1 71.3 88.7\nORG-TRL (Zhang et al., 2020) 54.3 36.4 73.9 95.2\nJCRR (Hou et al., 2020) 57.0 36.8 - 96.8\nSGN (Ryu et al., 2021) 52.8 35.5 72.9 94.3\nMGRMP (Chen and Jiang, 2021) 55.8 36.9 74.5 98.5\nBFSD (Zhong et al., 2022) 51.2 35.7 72.9 96.7\nCOST 56.8 37.2 74.3 99.2\nformer Encoder takes them as input to generate a natural sen-\ntence, and achieves significant improvements compared to the\nprevious methods. In order to make a relatively fair compari-\nson as we had not pre-trained our model on large-scale datasets,\nsuch as HowTo100M or Kinectics, we use the visual features\nprovided by COOT (Ging et al., 2020) as input, whose video\nembedding network pre-trained on HowTo100M, and obtain re-\nsults comparable to SwinBERT, which demonstrates the poten-\ntial of our method.\nWe also compare our methods with the state-of-art meth-\nods on MSVD dataset, and the results are presented in Table\n4. As shown in Table 4, our COST could approach or surpass\nthe state-of-art method. Considering the significant di fferences\nbetween the characteristics of this dataset and the former two\ndatasets, for example, the duration of video and the umber of\nannotations corresponding to each video, which further proves\nthe robustness of our method.\nFurthermore, the qualitative results of MART (Lei et al.,\n2020), PDVC (Wang et al., 2021), VLTinT (Yamazaki et al.,\n2022) and our COST are shown in Fig. 4. From Fig. 4, it\nis evident that our COST approach outperforms the compared\nmethods in terms of generating accurate and concise captions,\nregardless of the complexity of the scenarios. This is attributed\nto two reasons. First, using the Detection-Text transformer,\nthe Objects in caption sentences can be learned explicitly (e.g.,\nshrimp, hand, and windows) or implicitly ( e.g., from bowl to\nfood processor, and from pan to wok). Second, the Action-Text\ntransformer in our method can perceive the key actions in cap-\ntion sentences such as add, blend, put and hold. In comparison,\nthe existing methods often fail to recognize these crucial sub-\njects, objects, and verbs. The results indicate that under the\nconstraint of our objective, these two branches of transformers\ncan enforce the network to learn the key elements and supply\nvisual-linguistic interactions with discriminative semantics for\ncaptions generation.\nLastly, as there exists many videos shot by moving cam-\neras in our evaluated datasets, for example, YouCookII contains\n2000 long untrimmed videos describing 89 cooking recipes,\nand many videos are shot with varying perspectives for provid-\ning good viewing experience, and we wonder whether the per-\nformance of our model decreases significantly under such mov-\ning camera situation. We have random choose several video\nclips shot by apparent moving cameras without cherry pick and\nevaluate our COST on it, and the results are shown on 5. It can\nbe observed that there is no significant decrease in evaluation\nmetrics, which further confirms the robustness of our method.\n11\n12 3\n45 6\n12 3\n45 6\n12 3\n45 6\n12 3\n45 6\nFig. 5. Visualization of attention score map of all action tokens to [CLSa] token in Action-Text transformer for given video clips with\nground-truth caption ”add butter and milk to the mashed potatoes and mix”. More crimson color indicates high attention score. We also\nshow the video segments corresponding to the top four attention values. The number in the lower left corner of each picture indicates the\ntime order. Best viewed in color.\nTable 5. Evaluation Results on Moving Scenes in YouCookII. * indicates\nthe subset picked from YouCookII shot with apparent camera movements.\nDataset B@4 M C R@4 ↓\nYouCookII* 9.21 17.43 43.61 5.62\nYouCookII 9.47 17.67 45.54 4.04\n4.4. Visualization of Self-Attention in Action-Text Transformer\nIn Fig. 5, the central heatmap shows the visualization of the\nattention scores in self-attention between [CLSa] and all action\ntokens in Action-Text transformer. We obtain the scores by av-\neraging attention scores from all heads of the multi-head self-\nattention module in our last transformer block. We choose this\nbranch because its granularity token [CLS a] is used as the in-\nput for action classification, and whether it can utilize action\ntokens effectively partially reflects the ability of our model to\nunderstand the context of video. The ground-truth caption of\nchosen video is ”add butter and milk to the mashed potatoes\nand mix” and the segments corresponding to top four attention\nvalues are presented, where each one consists of 6 figures as\nsetting in motion feature extraction network. According to the\nchronological order of the action, we can see that the four video\nsegments are: finishing adding butter, readying to pour milk,\npouring the milk and stirring the mixture, which proves that\nour Action-Text Transformer can capture the key actions effec-\ntively via self-attention although there exists significant redun-\ndancy in the action tokens (vast majority of this video’s content\nis mixing the mixture).\n4.5. Visualization of Cross-Granularity Attention in Video-Text\nTransformer\nTo better understand the cross-granularity attention module,\nwe use the heatmap to visualize the cross-granularity attention\nscores of the Video-Text transformer in Fig. 6. For the con-\nvenience of display, we concatenate the a ffinity matrix MHY\nand MHX to get MH. At epoch 1, all values in the heatmap\nare similar and the maximal value is only 0 .001. The corre-\nsponding caption results are noisy with the false predictions of\nnouns and verbs, e.g., pan instead of butter. It indicates that the\ncross-granularity attention is randomly initialized and interac-\ntions between different modalities are not learned to align well.\nAfter training for several epochs, a few entities dominate the\nheatmap with the maximal value of 0 .3 (see the bright verti-\ncal lines in the heatmap). It indicates that our cross-granularity\nattention module can successfully exploit the most relevant en-\ntities from other modalities to inject the information from other\n12\nepoch 1: add the pan epoch 4: add the potatoes and salt to the pot\nepoch 7: add butter and milk to the pot and stir epoch 10: add butter and milk to the potatoes and mix\nGT: add butter and milk to the mashed potatoes and mix\nFig. 6. Heatmap used to indicate the a ffinity matrix MH in the Video-Text transformer, where the row denotes the video tokens and the\ncolumn denotes the action and detection tokens. The false predictions of nouns and verbs are denoted in red font. For clarity, we only\nshow a few epochs in the training phase. Best viewed in color and zoom in.\nTable 6. Ablation on multi-modality features and three-stream ar-\nchitecture.\nCOST Variants YouCookII (val)\nB@4 M C R@4 ↓\nCOST-1 (v) 6.59 14.29 29.20 6.49\nCOST-1 (v+a) 7.72 15.45 33.98 4.60\nCOST-1 (v+a+d) 8.73 16.90 38.63 4.66\nCOST-2 (v+a) 7.79 15.74 34.99 5.82\nCOST-2 (v+d) 9.04 17.31 43.09 4.59\nCOST-3 (v+a+d) 9.47 17.67 45.54 4.04\nbranches of transformers. This way, the verb stir and the noun\npan can be corrected to mix and butter for more accurate cap-\ntion results.\n4.6. Ablation Studies\nTo study the influence of di fferent components in the pro-\nposed method, we conduct detailed ablation study on the\nYouCookII val subset. Notably, we use the appearance features\nextracted by TSN (Wang et al., 2016) in all COST- k variants,\nwhere k denotes the number of transformer branches retained\nin our COST method.\nEffectiveness of multi-modality features. To verify the ef-\nfectiveness of the multi-modality features, we construct three\nCOST-1 variants, i.e., COST-1 (v), COST-1 (v+a) and COST-1\n(v+a+d). In particular, we only use the Video-Text transformer,\nbut change the input features as the combinations of the GloVe\ntext features and the concatenated features from video (v), ac-\ntion (a) and detection (d). As shown in Table 6, the scores under\nall metrics are improved considerably by integrating the action\nor detection features. Moreover, the CIDEr score is boosted\nTable 7. Ablation on training objectives.\nCOST Variants YouCookII (val)\nB@4 M C R@4 ↓\nCOST w/o La 8.95 17.31 42.90 5.27\nCOST w/o Ld 9.10 16.58 41.00 5.56\nCOST w/o Ld and La 9.01 16.29 40.58 5.91\nCOST 9.47 17.67 45.54 4.04\nfrom 29.20% to 38 .63%, if we include all the three-modality\nfeatures. It indicates that multi-modality features definitely fa-\ncilitate to generate more accurate video captions. And it can be\nobserved that the incorporation of detection features into COST\nhas yielded notable performance improvements, highlighting\nthe potential for leveraging more detailed features to advance\nthe field of video captioning.\nEffectiveness of three-stream transformers. To demonstrate\nthe e ffectiveness of the three-stream transformers compared\nto the simple feature concatenation used in COST-1, we con-\nstruct three COST-k (k = 2,3) variants, shown in Table 6. It\ncan be seen that the accuracy can be improved by using the\nthree-stream transformers, i.e., the CIDEr score improved from\n38.63% to 45 .54%. Meanwhile, the accuracy is considerably\nimproved by using more branches of transformers. This is be-\ncause our cross-granularity attention module is able to capture\nthe most relevant semantic information from di fferent visual-\nlinguistic interactions. Compared to the COST-1 variants, our\nfull model can obtain a more discriminative feature representa-\ntions for video captioning.\nEffectiveness of training objective. To investigate the effec-\ntiveness and rationality of proposed training objective, we do\n13\nMART:  remove the fat from the water and place on a paper towel\nOurs:    add rice vinegar and rice to the seaweed and roll the seaweed\nMART:  add the dough to the center of the dough\nOurs:    put the mixture in the bowl and coat them well with the flour\nFig. 7. Generated Descriptions for Internet videos from the state-of-art MART (Lei et al., 2020) method and our COST method. The blue\nnumber in the lower left corner of each picture indicates the time order.\nTable 8. Ablation on cross-granularity attention module. concat\nand para indicate concatenated and parallel cross-granularity at-\ntention module separately. COST w/o cgam. denotes a COST vari-\nant without cross-granularity attention module.\nCOST Variants YouCookII (val)\nB@4 M C R@4 ↓\nCOST w/ concat 9.27 17.40 43.43 6.34\nCOST w/ para 9.47 17.67 45.54 4.04\nCOST w/o cgam. 7.55 15.11 30.32 8.03\nablation experiments to obtain corresponding results in Table 7.\nWe observe that removing either La or Ld has a negative im-\npact on the performance, especially when they are all removed,\ni.e., 40.58 vs. 45.54 for CIDEr. This demonstrates that using\nappropriate objective to make the semantics of visual-linguistic\ninteractions explicit can definitely contribute to accurate cap-\ntions.\nEffectiveness of cross-granularity attention module. To ver-\nify the importance of cross-granularity attention module to the\nsuccess of our method, we compare the performance of COST\nwith the two proposed architectures(concatenated and parallel\narchitectures) separately and the variant with cross-granularity\nattention module removed. As shown in Table 8, our method\nwith cross-granularity module using parallel structure outper-\nforms that with concatenated structure in all evaluation met-\nrics, it is probably because that there exists feature gap between\nthe embeddings from di fferent branches and the softmax op-\neration may hinders the information of one branch being sup-\npressed during fusion. And after removing the cross-granularity\nmodule, CIDEr drops significantly, i.e., 30.32 vs. 45.54, which\ndemonstrate the effectiveness of this module to align the visual-\nlinguistic interactions of different granularities for accurate cap-\ntions.\n5. Generalization\nIn order to verify the generalization and practicality of our\nmethod, we collected some videos outside the trained datasets\nand evaluate the quality of generated descriptions to them. Be-\ncause the video included in YouCookII dataset has relatively\nclear action intention and contains abundant clips recording\nthe interactions between human and surrounding objects, we\ncollected some cooking videos from the Internet and applied\nmodel trained on YouCookII to them. The way to extract\nfeatures from collected videos keeps consistent with that to\nYouCookII, and we adopt trained models on both short and\nlong video clips to test the performance comprehensively. We\nlist the captions generated by MART (Lei et al., 2020) and our\nmethod in Fig. 7. The first clip is one step of making fried\nmilk, which is a dessert of Cantonese cuisine, and lasts about\n20 seconds to coat the fried milk with breadcrumbs. It can\nbe seen that our method not only makes a correct judgment\non the action(coat), but also correctly recognizes the state of\nusing substances(both breadcrumbs and flour are powdery ob-\njects), which is much better than MART. The second clip is one\nstep of making rice ball and lasts about 1 minute to add rice,\ncooked beef and asparagus to seaweed and wrap them. Because\nthere are many added contents, our model makes wrong recog-\nnition (the beef is recognized as vinegar because of the similar\n14\ncolor) and generates redundant words (rice are added twice in\nthe clip). Nevertheless, the generated description is far more\naccurate in action and much natural than the description gener-\nated by MART. Through these two examples, it illustrates that\nalthough our model may make mistakes in generating object\nwords due to the interference of similar shape or color, it can\nmake accurate recognition on the actions of the subject and the\nstructure of generated sentence is very natural, which indicates\nthe effectiveness and practical value of our model.\n6. Limitation\nAlthough our model has demonstrated satisfactory perfor-\nmance on three benchmark datasets and exhibits strong gener-\nalization capabilities, there still exists some limitations. Firstly,\nin terms of evaluation efficiency and model parameters, we ob-\nserve certain disadvantages when compared to single-stream\nmethods like MART. Specially, under the same hardware con-\nditions, our model generates captions for approximately 2.47\nvideos per second, and the duration of each videos is about 5\nminutes, whereas MART achieves a higher processing rate of\n6.85 videos per second. Furthermore, our model has a larger\nparameter count of 35.6M compared to MART’s 25.5M. Ad-\nditionally, it is worth noting that our model requires additional\ndata processing steps for extracting region features, which re-\nquires additional pre-processing time.\n7. Conclusion\nIn this paper, we propose the collaborative three-stream\ntransformers to exploit the interactions of objects, and the ac-\ntions/relations of objects between different modalities of differ-\nent granularities in spatial-temporal domain. Meanwhile, with\nthe help of proposed training objective to specify the seman-\ntics of visual-linguistic interactions in each branch, the cross-\ngranularity attention module is designed to align the interac-\ntions modeled by the three branches of transformers, which\ncontributes to more accurate captions generation. Several ex-\nperiments conducted on the YouCookII, ActivityNet Captions\nand MSVD datasets demonstrate the e ffectiveness of our pro-\nposed method, and we also evaluate its generalization ability\nthrough testing on Internet videos. In the future, we intent to\nfurther improve the performance of our model with the help of\npre-training on large-scale datasets.\nAcknowledgments\nThis work was supported by the Key Research Program of\nFrontier Sciences, CAS, Grant No. ZDBS-LY-JSC038 for fund-\ning this work. Libo Zhang was also supported by Youth Inno-\nvation Promotion Association, CAS (2020111). Heng Fan and\nhis employer received no financial support for the research, au-\nthorship, and/or publication of this article.\nReferences\nAbu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,\nVijayanarasimhan, S., 2016. Youtube-8m: A large-scale video classification\nbenchmark. CoRR abs/1609.08675.\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., Schmid, C., 2021.\nVivit: A video vision transformer, in: ICCV, IEEE. pp. 6816–6826.\nChen, D.L., Dolan, W.B., 2011. Collecting highly parallel data for paraphrase\nevaluation, in: ACL, The Association for Computer Linguistics. pp. 190–\n200.\nChen, S., Jiang, Y ., 2019. Motion guided spatial attention for video captioning,\nin: AAAI, pp. 8191–8198.\nChen, S., Jiang, Y ., 2021. Motion guided region message passing for video\ncaptioning, in: ICCV, IEEE. pp. 1523–1532.\nChen, S., Yao, T., Jiang, Y ., 2019. Deep learning for video captioning: A\nreview, in: IJCAI, pp. 6283–6290.\nChung, J., G ¨ulc ¸ehre, C ¸ ., Cho, K., Bengio, Y ., 2014. Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling. CoRR\nabs/1412.3555.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J.G., Le, Q.V ., Salakhutdinov, R., 2019.\nTransformer-xl: Attentive language models beyond a fixed-length context,\nin: ACL, pp. 2978–2988.\nDas, P., Xu, C., Doell, R.F., Corso, J.J., 2013. A thousand frames in just a few\nwords: Lingual description of videos through latent topics and sparse object\nstitching, in: CVPR, pp. 2634–2641.\nDenkowski, M.J., Lavie, A., 2014. Meteor universal: Language specific trans-\nlation evaluation for any target language, in: WMT@ACL, pp. 376–380.\nDevlin, J., Chang, M., Lee, K., Toutanova, K., 2019. BERT: pre-training of\ndeep bidirectional transformers for language understanding, in: NAACL-\nHLT, pp. 4171–4186.\nFan, H., Yang, Y ., 2020. Person tube retrieval via language description, in:\nAAAI, AAAI Press. pp. 10754–10761.\nGao, L., Li, X., Song, J., Shen, H.T., 2020. Hierarchical lstms with adaptive\nattention for visual captioning. IEEE Trans. Pattern Anal. Mach. Intell. 42,\n1112–1131.\nGing, S., Zolfaghari, M., Pirsiavash, H., Brox, T., 2020. COOT: cooperative\nhierarchical transformer for video-text representation learning, in: NeurIPS.\nGuadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S.,\nMooney, R.J., Darrell, T., Saenko, K., 2013. Youtube2text: Recognizing\nand describing arbitrary activities using semantic hierarchies and zero-shot\nrecognition, in: ICCV, pp. 2712–2719.\nHao, W., Zhang, Z., Guan, H., 2018. Integrating both visual and audio cues for\nenhanced video caption, in: AAAI, pp. 6894–6901.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: CVPR, IEEE Computer Society. pp. 770–778.\nHeilbron, F.C., Escorcia, V ., Ghanem, B., Niebles, J.C., 2015. Activitynet: A\nlarge-scale video benchmark for human activity understanding, in: CVPR,\nIEEE Computer Society. pp. 961–970.\nHori, C., Hori, T., Wichern, G., Wang, J., Lee, T., Cherian, A., Marks, T.K.,\n2018. Multimodal attention for fusion of audio and spatiotemporal features\nfor video description, in: CVPRW, pp. 2528–2531.\nHou, J., Wu, X., Zhang, X., Qi, Y ., Jia, Y ., Luo, J., 2020. Joint commonsense\nand relation reasoning for image and video captioning, in: AAAI, AAAI\nPress. pp. 10973–10980.\nHuang, G., Pang, B., Zhu, Z., Rivera, C., Soricut, R., 2020. Multimodal pre-\ntraining for dense video captioning, in: AACL/IJCNLP, pp. 470–490.\nIashin, V ., Rahtu, E., 2020. Multi-modal dense video captioning, in: CVPRW,\npp. 4117–4126.\nIoffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift, in: ICML, JMLR.org. pp. 448–\n456.\nKarpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.,\n2014. Large-scale video classification with convolutional neural networks,\nin: CVPR, IEEE Computer Society. pp. 1725–1732.\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,\nS., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.,\n2017. The kinetics human action video dataset. CoRR abs /1705.06950.\nKim, D., Oh, T.H., Choi, J., Kweon, I.S., 2020. Dense relational image cap-\ntioning via multi-task triple-stream networks. CoRR abs/2010.03855.\nKingma, D.P., Ba, J., 2015. Adam: A method for stochastic optimization, in:\nICLR.\nKrishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C., 2017a. Dense-\ncaptioning events in videos, in: ICCV , pp. 706–715.\n15\nKrishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,\nKalantidis, Y ., Li, L., Shamma, D.A., Bernstein, M.S., Fei-Fei, L., 2017b.\nVisual genome: Connecting language and vision using crowdsourced dense\nimage annotations. IJCV 123, 32–73.\nLei, J., Wang, L., Shen, Y ., Yu, D., Berg, T.L., Bansal, M., 2020. MART:\nmemory-augmented recurrent transformer for coherent video paragraph cap-\ntioning, in: ACL, pp. 2603–2614.\nLi, J., Niu, L., Zhang, L., 2022. Action-aware embedding enhancement for\nimage-text retrieval, in: AAAI, AAAI Press. pp. 1323–1331.\nLi, X., Zhao, B., Lu, X., 2017. MAM-RNN: multi-level attention model based\nRNN for video captioning, in: IJCAI, ijcai.org. pp. 2208–2214.\nLin, C.Y ., 2004. Rouge: A package for automatic evaluation of summaries, in:\nText summarization branches out, pp. 74–81.\nLin, K., Li, L., Lin, C., Ahmed, F., Gan, Z., Liu, Z., Lu, Y ., Wang, L., 2021.\nSwinbert: End-to-end transformers with sparse attention for video caption-\ning. CoRR abs/2111.13196.\nLiu, S., Ren, Z., Yuan, J., 2021a. Sibnet: Sibling convolutional encoder for\nvideo captioning. IEEE Trans. Pattern Anal. Mach. Intell. 43, 3259–3272.\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., Hu, H., 2021b. Video\nswin transformer. CoRR abs/2106.13230.\nLu, J., Batra, D., Parikh, D., Lee, S., 2019. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks, in: NeurIPS,\npp. 13–23.\nLuo, H., Ji, L., Shi, B., Huang, H., Duan, N., Li, T., Chen, X., Zhou, M., 2020.\nUnivilm: A unified video and language pre-training model for multimodal\nunderstanding and generation. CoRR abs/2002.06353.\nMan, X., Ouyang, D., Li, X., Song, J., Shao, J., 2022. Scenario-aware recur-\nrent transformer for goal-directed video captioning. ACM Trans. Multim.\nComput. Commun. Appl. 18, 104:1–104:17.\nMiech, A., Zhukov, D., Alayrac, J., Tapaswi, M., Laptev, I., Sivic, J., 2019.\nHowto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips, in: ICCV , pp. 2630–2640.\nNan, G., Qiao, R., Xiao, Y ., Liu, J., Leng, S., Zhang, H., Lu, W., 2021. Interven-\ntional video grounding with dual contrastive learning, in: CVPR, Computer\nVision Foundation / IEEE. pp. 2765–2775.\nPan, Y ., Yao, T., Li, H., Mei, T., 2017. Video captioning with transferred se-\nmantic attributes, in: CVPR, pp. 984–992.\nPapineni, K., Roukos, S., Ward, T., Zhu, W., 2002. Bleu: a method for auto-\nmatic evaluation of machine translation, in: ACL, pp. 311–318.\nPark, J.S., Rohrbach, M., Darrell, T., Rohrbach, A., 2019. Adversarial inference\nfor multi-sentence video description, in: CVPR, pp. 6598–6608.\nPei, W., Zhang, J., Wang, X., Ke, L., Shen, X., Tai, Y ., 2019. Memory-attended\nrecurrent network for video captioning, in: CVPR, pp. 8347–8356.\nPennington, J., Socher, R., Manning, C.D., 2014. Glove: Global vectors for\nword representation, in: EMNLP, pp. 1532–1543.\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sas-\ntry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I., 2021.\nLearning transferable visual models from natural language supervision, in:\nICML, PMLR. pp. 8748–8763.\nRen, S., He, K., Girshick, R.B., Sun, J., 2017. Faster R-CNN: towards real-time\nobject detection with region proposal networks. TPAMI 39, 1137–1149.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C., Fei-Fei, L., 2015.\nImagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115,\n211–252.\nRyu, H., Kang, S., Kang, H., Yoo, C.D., 2021. Semantic grouping network for\nvideo captioning, in: AAAI, AAAI Press. pp. 2514–2522.\nSeo, P.H., Nagrani, A., Arnab, A., Schmid, C., 2022. End-to-end generative\npretraining for multimodal video captioning. CoRR abs/2201.08264.\nSun, C., Myers, A., V ondrick, C., Murphy, K., Schmid, C., 2019. Videobert:\nA joint model for video and language representation learning, in: ICCV , pp.\n7463–7472.\nSun, Y ., Cheng, C., Zhang, Y ., Zhang, C., Zheng, L., Wang, Z., Wei, Y ., 2020.\nCircle loss: A unified perspective of pair similarity optimization, in: CVPR,\npp. 6397–6406.\nSutskever, I., Vinyals, O., Le, Q.V ., 2014. Sequence to sequence learning with\nneural networks, in: NeurIPS, pp. 3104–3112.\nSzegedy, C., Io ffe, S., Vanhoucke, V ., Alemi, A.A., 2017. Inception-v4,\ninception-resnet and the impact of residual connections on learning, in:\nAAAI, AAAI Press. pp. 4278–4284.\nTang, Z., Lei, J., Bansal, M., 2021. Decembert: Learning from noisy instruc-\ntional videos via dense captions and entropy minimization, in: NAACL-\nHLT, pp. 2415–2426.\nTran, D., Bourdev, L.D., Fergus, R., Torresani, L., Paluri, M., 2015. Learning\nspatiotemporal features with 3d convolutional networks, in: ICCV, IEEE\nComputer Society. pp. 4489–4497.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I., 2017. Attention is all you need, in: NeurIPS, pp.\n5998–6008.\nVedantam, R., Zitnick, C.L., Parikh, D., 2015. Cider: Consensus-based image\ndescription evaluation, in: CVPR, pp. 4566–4575.\nVenugopalan, S., Rohrbach, M., Donahue, J., Mooney, R.J., Darrell, T., Saenko,\nK., 2015a. Sequence to sequence - video to text, in: ICCV, pp. 4534–4542.\nVenugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R.J., Saenko, K.,\n2015b. Translating videos to natural language using deep recurrent neural\nnetworks, in: HLT-NAACL, pp. 1494–1504.\nWang, L., Xiong, Y ., Wang, Z., Qiao, Y ., Lin, D., Tang, X., Gool, L.V ., 2016.\nTemporal segment networks: Towards good practices for deep action recog-\nnition, in: ECCV , pp. 20–36.\nWang, L., Xiong, Y ., Wang, Z., Qiao, Y ., Lin, D., Tang, X., Gool, L.V ., 2019.\nTemporal segment networks for action recognition in videos. IEEE Trans.\nPattern Anal. Mach. Intell. 41, 2740–2755.\nWang, T., Zhang, R., Lu, Z., Zheng, F., Cheng, R., Luo, P., 2021. End-to-end\ndense video captioning with parallel decoding, in: ICCV, IEEE. pp. 6827–\n6837.\nWang, X., Wang, Y ., Wang, W.Y ., 2018. Watch, listen, and describe: Glob-\nally and locally aligned cross-modal attentions for video captioning, in:\nNAACL-HLT, pp. 795–801.\nWei, X., Zhang, T., Li, Y ., Zhang, Y ., Wu, F., 2020. Multi-modality cross\nattention network for image and sentence matching, in: CVPR, pp. 10938–\n10947.\nXie, S., Sun, C., Huang, J., Tu, Z., Murphy, K., 2017. Rethinking spatiotempo-\nral feature learning for video understanding. CoRR abs/1712.04851.\nXiong, Y ., Dai, B., Lin, D., 2018. Move forward and tell: A progressive gener-\nator of video descriptions, in: ECCV, pp. 489–505.\nXu, N., Liu, A., Wong, Y ., Zhang, Y ., Nie, W., Su, Y ., Kankanhalli, M.S., 2019.\nDual-stream recurrent neural network for video captioning. IEEE Trans.\nCircuits Syst. Video Technol. 29, 2482–2493.\nXu, Y ., Liu, X., Cao, X., Huang, C., Liu, E., Qian, S., Liu, X., Wu, Y ., Dong,\nF., Qiu, C.W., et al., 2021. Artificial intelligence: A powerful paradigm for\nscientific research. The Innovation 2, 100179.\nYamazaki, K., V o, K., Truong, S., Raj, B., Le, N., 2022. Vltint: Visual-\nlinguistic transformer-in-transformer for coherent video paragraph caption-\ning. CoRR abs/2211.15103.\nYao, L., Torabi, A., Cho, K., Ballas, N., Pal, C.J., Larochelle, H., Courville,\nA.C., 2015. Describing videos by exploiting temporal structure, in: ICCV,\npp. 4507–4515.\nYe, H., Li, G., Qi, Y ., Wang, S., Huang, Q., Yang, M., 2022. Hierarchical\nmodular network for video captioning, in: CVPR, IEEE. pp. 17918–17927.\nYu, H., Wang, J., Huang, Z., Yang, Y ., Xu, W., 2016. Video paragraph caption-\ning using hierarchical recurrent neural networks, in: CVPR, pp. 4584–4593.\nZhang, B., Hu, H., Sha, F., 2018. Cross-modal and hierarchical modeling of\nvideo and text, in: ECCV, pp. 385–401.\nZhang, J., Peng, Y ., 2019. Object-aware aggregation with bidirectional tem-\nporal graph for video captioning, in: CVPR, Computer Vision Foundation /\nIEEE. pp. 8327–8336.\nZhang, N., Chen, X., Xie, X., Deng, S., Tan, C., Chen, M., Huang, F., Si, L.,\nChen, H., 2021. Document-level relation extraction as semantic segmenta-\ntion, in: IJCAI, pp. 3999–4006.\nZhang, Z., Shi, Y ., Yuan, C., Li, B., Wang, P., Hu, W., Zha, Z., 2020. Object\nrelational graph with teacher-recommended learning for video captioning,\nin: CVPR, pp. 13275–13285.\nZheng, Q., Wang, C., Tao, D., 2020. Syntax-aware action targeting for video\ncaptioning, in: CVPR, pp. 13093–13102.\nZhong, X., Li, Z., Chen, S., Jiang, K., Chen, C., Ye, M., 2022. Refined seman-\ntic enhancement towards frequency di ffusion for video captioning. CoRR\nabs/2211.15076.\nZhou, L., Kalantidis, Y ., Chen, X., Corso, J.J., Rohrbach, M., 2019. Grounded\nvideo description, in: CVPR, pp. 6578–6587.\nZhou, L., Xu, C., Corso, J.J., 2018a. Towards automatic learning of procedures\nfrom web instructional videos, in: AAAI, pp. 7590–7598.\nZhou, L., Zhou, Y ., Corso, J.J., Socher, R., Xiong, C., 2018b. End-to-end dense\nvideo captioning with masked transformer, in: CVPR, pp. 8739–8748.\nZhu, L., Yang, Y ., 2020. Actbert: Learning global-local video-text representa-\n16\ntions, in: CVPR, pp. 8743–8752.\nReferences\nAbu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B.,\nVijayanarasimhan, S., 2016. Youtube-8m: A large-scale video classification\nbenchmark. CoRR abs/1609.08675.\nArnab, A., Dehghani, M., Heigold, G., Sun, C., Lucic, M., Schmid, C., 2021.\nVivit: A video vision transformer, in: ICCV, IEEE. pp. 6816–6826.\nChen, D.L., Dolan, W.B., 2011. Collecting highly parallel data for paraphrase\nevaluation, in: ACL, The Association for Computer Linguistics. pp. 190–\n200.\nChen, S., Jiang, Y ., 2019. Motion guided spatial attention for video captioning,\nin: AAAI, pp. 8191–8198.\nChen, S., Jiang, Y ., 2021. Motion guided region message passing for video\ncaptioning, in: ICCV, IEEE. pp. 1523–1532.\nChen, S., Yao, T., Jiang, Y ., 2019. Deep learning for video captioning: A\nreview, in: IJCAI, pp. 6283–6290.\nChung, J., G ¨ulc ¸ehre, C ¸ ., Cho, K., Bengio, Y ., 2014. Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling. CoRR\nabs/1412.3555.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J.G., Le, Q.V ., Salakhutdinov, R., 2019.\nTransformer-xl: Attentive language models beyond a fixed-length context,\nin: ACL, pp. 2978–2988.\nDas, P., Xu, C., Doell, R.F., Corso, J.J., 2013. A thousand frames in just a few\nwords: Lingual description of videos through latent topics and sparse object\nstitching, in: CVPR, pp. 2634–2641.\nDenkowski, M.J., Lavie, A., 2014. Meteor universal: Language specific trans-\nlation evaluation for any target language, in: WMT@ACL, pp. 376–380.\nDevlin, J., Chang, M., Lee, K., Toutanova, K., 2019. BERT: pre-training of\ndeep bidirectional transformers for language understanding, in: NAACL-\nHLT, pp. 4171–4186.\nFan, H., Yang, Y ., 2020. Person tube retrieval via language description, in:\nAAAI, AAAI Press. pp. 10754–10761.\nGao, L., Li, X., Song, J., Shen, H.T., 2020. Hierarchical lstms with adaptive\nattention for visual captioning. IEEE Trans. Pattern Anal. Mach. Intell. 42,\n1112–1131.\nGing, S., Zolfaghari, M., Pirsiavash, H., Brox, T., 2020. COOT: cooperative\nhierarchical transformer for video-text representation learning, in: NeurIPS.\nGuadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S.,\nMooney, R.J., Darrell, T., Saenko, K., 2013. Youtube2text: Recognizing\nand describing arbitrary activities using semantic hierarchies and zero-shot\nrecognition, in: ICCV, pp. 2712–2719.\nHao, W., Zhang, Z., Guan, H., 2018. Integrating both visual and audio cues for\nenhanced video caption, in: AAAI, pp. 6894–6901.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: CVPR, IEEE Computer Society. pp. 770–778.\nHeilbron, F.C., Escorcia, V ., Ghanem, B., Niebles, J.C., 2015. Activitynet: A\nlarge-scale video benchmark for human activity understanding, in: CVPR,\nIEEE Computer Society. pp. 961–970.\nHori, C., Hori, T., Wichern, G., Wang, J., Lee, T., Cherian, A., Marks, T.K.,\n2018. Multimodal attention for fusion of audio and spatiotemporal features\nfor video description, in: CVPRW, pp. 2528–2531.\nHou, J., Wu, X., Zhang, X., Qi, Y ., Jia, Y ., Luo, J., 2020. Joint commonsense\nand relation reasoning for image and video captioning, in: AAAI, AAAI\nPress. pp. 10973–10980.\nHuang, G., Pang, B., Zhu, Z., Rivera, C., Soricut, R., 2020. Multimodal pre-\ntraining for dense video captioning, in: AACL/IJCNLP, pp. 470–490.\nIashin, V ., Rahtu, E., 2020. Multi-modal dense video captioning, in: CVPRW,\npp. 4117–4126.\nIoffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network\ntraining by reducing internal covariate shift, in: ICML, JMLR.org. pp. 448–\n456.\nKarpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.,\n2014. Large-scale video classification with convolutional neural networks,\nin: CVPR, IEEE Computer Society. pp. 1725–1732.\nKay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,\nS., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.,\n2017. The kinetics human action video dataset. CoRR abs /1705.06950.\nKim, D., Oh, T.H., Choi, J., Kweon, I.S., 2020. Dense relational image cap-\ntioning via multi-task triple-stream networks. CoRR abs/2010.03855.\nKingma, D.P., Ba, J., 2015. Adam: A method for stochastic optimization, in:\nICLR.\nKrishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C., 2017a. Dense-\ncaptioning events in videos, in: ICCV , pp. 706–715.\nKrishna, R., Zhu, Y ., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,\nKalantidis, Y ., Li, L., Shamma, D.A., Bernstein, M.S., Fei-Fei, L., 2017b.\nVisual genome: Connecting language and vision using crowdsourced dense\nimage annotations. IJCV 123, 32–73.\nLei, J., Wang, L., Shen, Y ., Yu, D., Berg, T.L., Bansal, M., 2020. MART:\nmemory-augmented recurrent transformer for coherent video paragraph cap-\ntioning, in: ACL, pp. 2603–2614.\nLi, J., Niu, L., Zhang, L., 2022. Action-aware embedding enhancement for\nimage-text retrieval, in: AAAI, AAAI Press. pp. 1323–1331.\nLi, X., Zhao, B., Lu, X., 2017. MAM-RNN: multi-level attention model based\nRNN for video captioning, in: IJCAI, ijcai.org. pp. 2208–2214.\nLin, C.Y ., 2004. Rouge: A package for automatic evaluation of summaries, in:\nText summarization branches out, pp. 74–81.\nLin, K., Li, L., Lin, C., Ahmed, F., Gan, Z., Liu, Z., Lu, Y ., Wang, L., 2021.\nSwinbert: End-to-end transformers with sparse attention for video caption-\ning. CoRR abs/2111.13196.\nLiu, S., Ren, Z., Yuan, J., 2021a. Sibnet: Sibling convolutional encoder for\nvideo captioning. IEEE Trans. Pattern Anal. Mach. Intell. 43, 3259–3272.\nLiu, Z., Ning, J., Cao, Y ., Wei, Y ., Zhang, Z., Lin, S., Hu, H., 2021b. Video\nswin transformer. CoRR abs/2106.13230.\nLu, J., Batra, D., Parikh, D., Lee, S., 2019. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks, in: NeurIPS,\npp. 13–23.\nLuo, H., Ji, L., Shi, B., Huang, H., Duan, N., Li, T., Chen, X., Zhou, M., 2020.\nUnivilm: A unified video and language pre-training model for multimodal\nunderstanding and generation. CoRR abs/2002.06353.\nMan, X., Ouyang, D., Li, X., Song, J., Shao, J., 2022. Scenario-aware recur-\nrent transformer for goal-directed video captioning. ACM Trans. Multim.\nComput. Commun. Appl. 18, 104:1–104:17.\nMiech, A., Zhukov, D., Alayrac, J., Tapaswi, M., Laptev, I., Sivic, J., 2019.\nHowto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips, in: ICCV , pp. 2630–2640.\nNan, G., Qiao, R., Xiao, Y ., Liu, J., Leng, S., Zhang, H., Lu, W., 2021. Interven-\ntional video grounding with dual contrastive learning, in: CVPR, Computer\nVision Foundation / IEEE. pp. 2765–2775.\nPan, Y ., Yao, T., Li, H., Mei, T., 2017. Video captioning with transferred se-\nmantic attributes, in: CVPR, pp. 984–992.\nPapineni, K., Roukos, S., Ward, T., Zhu, W., 2002. Bleu: a method for auto-\nmatic evaluation of machine translation, in: ACL, pp. 311–318.\nPark, J.S., Rohrbach, M., Darrell, T., Rohrbach, A., 2019. Adversarial inference\nfor multi-sentence video description, in: CVPR, pp. 6598–6608.\nPei, W., Zhang, J., Wang, X., Ke, L., Shen, X., Tai, Y ., 2019. Memory-attended\nrecurrent network for video captioning, in: CVPR, pp. 8347–8356.\nPennington, J., Socher, R., Manning, C.D., 2014. Glove: Global vectors for\nword representation, in: EMNLP, pp. 1532–1543.\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sas-\ntry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I., 2021.\nLearning transferable visual models from natural language supervision, in:\nICML, PMLR. pp. 8748–8763.\nRen, S., He, K., Girshick, R.B., Sun, J., 2017. Faster R-CNN: towards real-time\nobject detection with region proposal networks. TPAMI 39, 1137–1149.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C., Fei-Fei, L., 2015.\nImagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115,\n211–252.\nRyu, H., Kang, S., Kang, H., Yoo, C.D., 2021. Semantic grouping network for\nvideo captioning, in: AAAI, AAAI Press. pp. 2514–2522.\nSeo, P.H., Nagrani, A., Arnab, A., Schmid, C., 2022. End-to-end generative\npretraining for multimodal video captioning. CoRR abs/2201.08264.\nSun, C., Myers, A., V ondrick, C., Murphy, K., Schmid, C., 2019. Videobert:\nA joint model for video and language representation learning, in: ICCV , pp.\n7463–7472.\nSun, Y ., Cheng, C., Zhang, Y ., Zhang, C., Zheng, L., Wang, Z., Wei, Y ., 2020.\nCircle loss: A unified perspective of pair similarity optimization, in: CVPR,\npp. 6397–6406.\nSutskever, I., Vinyals, O., Le, Q.V ., 2014. Sequence to sequence learning with\nneural networks, in: NeurIPS, pp. 3104–3112.\nSzegedy, C., Io ffe, S., Vanhoucke, V ., Alemi, A.A., 2017. Inception-v4,\n17\ninception-resnet and the impact of residual connections on learning, in:\nAAAI, AAAI Press. pp. 4278–4284.\nTang, Z., Lei, J., Bansal, M., 2021. Decembert: Learning from noisy instruc-\ntional videos via dense captions and entropy minimization, in: NAACL-\nHLT, pp. 2415–2426.\nTran, D., Bourdev, L.D., Fergus, R., Torresani, L., Paluri, M., 2015. Learning\nspatiotemporal features with 3d convolutional networks, in: ICCV, IEEE\nComputer Society. pp. 4489–4497.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L., Polosukhin, I., 2017. Attention is all you need, in: NeurIPS, pp.\n5998–6008.\nVedantam, R., Zitnick, C.L., Parikh, D., 2015. Cider: Consensus-based image\ndescription evaluation, in: CVPR, pp. 4566–4575.\nVenugopalan, S., Rohrbach, M., Donahue, J., Mooney, R.J., Darrell, T., Saenko,\nK., 2015a. Sequence to sequence - video to text, in: ICCV, pp. 4534–4542.\nVenugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R.J., Saenko, K.,\n2015b. Translating videos to natural language using deep recurrent neural\nnetworks, in: HLT-NAACL, pp. 1494–1504.\nWang, L., Xiong, Y ., Wang, Z., Qiao, Y ., Lin, D., Tang, X., Gool, L.V ., 2016.\nTemporal segment networks: Towards good practices for deep action recog-\nnition, in: ECCV , pp. 20–36.\nWang, L., Xiong, Y ., Wang, Z., Qiao, Y ., Lin, D., Tang, X., Gool, L.V ., 2019.\nTemporal segment networks for action recognition in videos. IEEE Trans.\nPattern Anal. Mach. Intell. 41, 2740–2755.\nWang, T., Zhang, R., Lu, Z., Zheng, F., Cheng, R., Luo, P., 2021. End-to-end\ndense video captioning with parallel decoding, in: ICCV, IEEE. pp. 6827–\n6837.\nWang, X., Wang, Y ., Wang, W.Y ., 2018. Watch, listen, and describe: Glob-\nally and locally aligned cross-modal attentions for video captioning, in:\nNAACL-HLT, pp. 795–801.\nWei, X., Zhang, T., Li, Y ., Zhang, Y ., Wu, F., 2020. Multi-modality cross\nattention network for image and sentence matching, in: CVPR, pp. 10938–\n10947.\nXie, S., Sun, C., Huang, J., Tu, Z., Murphy, K., 2017. Rethinking spatiotempo-\nral feature learning for video understanding. CoRR abs/1712.04851.\nXiong, Y ., Dai, B., Lin, D., 2018. Move forward and tell: A progressive gener-\nator of video descriptions, in: ECCV, pp. 489–505.\nXu, N., Liu, A., Wong, Y ., Zhang, Y ., Nie, W., Su, Y ., Kankanhalli, M.S., 2019.\nDual-stream recurrent neural network for video captioning. IEEE Trans.\nCircuits Syst. Video Technol. 29, 2482–2493.\nXu, Y ., Liu, X., Cao, X., Huang, C., Liu, E., Qian, S., Liu, X., Wu, Y ., Dong,\nF., Qiu, C.W., et al., 2021. Artificial intelligence: A powerful paradigm for\nscientific research. The Innovation 2, 100179.\nYamazaki, K., V o, K., Truong, S., Raj, B., Le, N., 2022. Vltint: Visual-\nlinguistic transformer-in-transformer for coherent video paragraph caption-\ning. CoRR abs/2211.15103.\nYao, L., Torabi, A., Cho, K., Ballas, N., Pal, C.J., Larochelle, H., Courville,\nA.C., 2015. Describing videos by exploiting temporal structure, in: ICCV,\npp. 4507–4515.\nYe, H., Li, G., Qi, Y ., Wang, S., Huang, Q., Yang, M., 2022. Hierarchical\nmodular network for video captioning, in: CVPR, IEEE. pp. 17918–17927.\nYu, H., Wang, J., Huang, Z., Yang, Y ., Xu, W., 2016. Video paragraph caption-\ning using hierarchical recurrent neural networks, in: CVPR, pp. 4584–4593.\nZhang, B., Hu, H., Sha, F., 2018. Cross-modal and hierarchical modeling of\nvideo and text, in: ECCV, pp. 385–401.\nZhang, J., Peng, Y ., 2019. Object-aware aggregation with bidirectional tem-\nporal graph for video captioning, in: CVPR, Computer Vision Foundation /\nIEEE. pp. 8327–8336.\nZhang, N., Chen, X., Xie, X., Deng, S., Tan, C., Chen, M., Huang, F., Si, L.,\nChen, H., 2021. Document-level relation extraction as semantic segmenta-\ntion, in: IJCAI, pp. 3999–4006.\nZhang, Z., Shi, Y ., Yuan, C., Li, B., Wang, P., Hu, W., Zha, Z., 2020. Object\nrelational graph with teacher-recommended learning for video captioning,\nin: CVPR, pp. 13275–13285.\nZheng, Q., Wang, C., Tao, D., 2020. Syntax-aware action targeting for video\ncaptioning, in: CVPR, pp. 13093–13102.\nZhong, X., Li, Z., Chen, S., Jiang, K., Chen, C., Ye, M., 2022. Refined seman-\ntic enhancement towards frequency di ffusion for video captioning. CoRR\nabs/2211.15076.\nZhou, L., Kalantidis, Y ., Chen, X., Corso, J.J., Rohrbach, M., 2019. Grounded\nvideo description, in: CVPR, pp. 6578–6587.\nZhou, L., Xu, C., Corso, J.J., 2018a. Towards automatic learning of procedures\nfrom web instructional videos, in: AAAI, pp. 7590–7598.\nZhou, L., Zhou, Y ., Corso, J.J., Socher, R., Xiong, C., 2018b. End-to-end dense\nvideo captioning with masked transformer, in: CVPR, pp. 8739–8748.\nZhu, L., Yang, Y ., 2020. Actbert: Learning global-local video-text representa-\ntions, in: CVPR, pp. 8743–8752.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.807396411895752
    },
    {
      "name": "Computer science",
      "score": 0.8009413480758667
    },
    {
      "name": "Transformer",
      "score": 0.7573833465576172
    },
    {
      "name": "Exploit",
      "score": 0.735381007194519
    },
    {
      "name": "Discriminative model",
      "score": 0.6241809725761414
    },
    {
      "name": "Sentence",
      "score": 0.5997587442398071
    },
    {
      "name": "Granularity",
      "score": 0.541915774345398
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5195848941802979
    },
    {
      "name": "Natural language processing",
      "score": 0.4874565601348877
    },
    {
      "name": "Engineering",
      "score": 0.07472243905067444
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}