{
  "title": "A Pilot Study for BERT Language Modelling and Morphological Analysis for Ancient and Medieval Greek",
  "url": "https://openalex.org/W3212892226",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3024898845",
      "name": "Pranaydeep Singh",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A3213574146",
      "name": "Gorik Rutten",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A2141599409",
      "name": "Els Lefever",
      "affiliations": [
        "Ghent University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1964861265",
    "https://openalex.org/W2077419346",
    "https://openalex.org/W3023728302",
    "https://openalex.org/W3098749165",
    "https://openalex.org/W3124859010",
    "https://openalex.org/W2530151296",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2911323286",
    "https://openalex.org/W3102764015",
    "https://openalex.org/W2962935543",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W35086054",
    "https://openalex.org/W2951899185",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W3013246784",
    "https://openalex.org/W2904419184",
    "https://openalex.org/W2981821110"
  ],
  "abstract": "This paper presents a pilot study to automatic linguistic preprocessing of Ancient and Byzantine Greek, and morphological analysis more specifically. To this end, a novel subword-based BERT language model was trained on the basis of a varied corpus of Modern, Ancient and Post-classical Greek texts. Consequently, the obtained BERT embeddings were incorporated to train a fine-grained Part-of-Speech tagger for Ancient and Byzantine Greek. In addition, a corpus of Greek Epigrams was manually annotated and the resulting gold standard was used to evaluate the performance of the morphological analyser on Byzantine Greek. The experimental results show very good perplexity scores (4.9) for the BERT language model and state-of-the-art performance for the fine-grained Part-of-Speech tagger for in-domain data (treebanks containing a mixture of Classical and Medieval Greek), as well as for the newly created Byzantine Greek gold standard data set. The language models and associated code are made available for use at https://github.com/pranaydeeps/Ancient-Greek-BERT",
  "full_text": "Proceedings of LaTeCH-CLfL 2021, pages 128–137\nPunta Cana, Dominican Republic (Online), November 11, 2021.\n128\nA Pilot Study for BERT Language Modelling and Morphological Analysis\nfor Ancient and Medieval Greek\nPranaydeep Singh, Gorik Rutten and Els Lefever\nLT3, Language and Translation Technology Team\nDepartment of Translation, Interpreting and Communication – Ghent University\nGroot-Brittanniëlaan 45, 9000 Ghent, Belgium\nfirstname.lastname@ugent.be\nAbstract\nThis paper presents a pilot study to auto-\nmatic linguistic preprocessing of Ancient\nand Byzantine Greek, and morphological\nanalysis more speciﬁcally. To this end, a\nnovel subword-based BERT language model\nwas trained on the basis of a varied corpus\nof Modern, Ancient and Post-classical Greek\ntexts. Consequently, the obtained BERT\nembeddings were incorporated to train a\nﬁne-grained Part-of-Speech tagger for An-\ncient and Byzantine Greek. In addition, a\ncorpus of Greek Epigrams was manually\nannotated and the resulting gold standard\nwas used to evaluate the performance of the\nmorphological analyser on Byzantine Greek.\nThe experimental results show very good\nperplexity scores (4.9) for the BERT language\nmodel and state-of-the-art performance for\nthe ﬁne-grained Part-of-Speech tagger for in-\ndomain data (treebanks containing a mixture\nof Classical and Medieval Greek), as well as\nfor the newly created Byzantine Greek gold\nstandard data set. The language models and\nassociated code are made available for use\nat https://github.com/pranaydeeps/Ancient-\nGreek-BERT\n1 Introduction\nDuring the last decades, large collections of digital\ntexts have become available for Ancient Greek and\nLatin. As a result, classicists are becoming more\nand more interested to apply Natural Language\nProcessing (NLP) techniques to extract information\nfrom these texts in an automatic and structured way.\nAlthough there has been a boost in NLP research\nfor Greek and Latin thanks to the introduction of\nthe Classical Language Tool Kit1 and the develop-\nment of Dependency Treebanks, such as the The\nAncient Greek and Latin Dependency Treebank\n(AGLDT)2, there is still a lack of NLP tools that\n1http://cltk.org/\n2https://perseusdl.github.io/treebank_data/\nperform well on different historical varieties, such\nas for instance Byzantine Greek3.\nThe presented research is to be situated in\nthe overarching DBBE project4, where Byzantine\nepigrams are studied as nodes between textual\ntransmission and cultural and linguistic develop-\nments (Bernard and Demoen, 2019). This multi-\ndisciplinary project aims to reveal the connections\nbetween linguistic patterns and text-historical de-\nvelopments in a corpus of metrical paratexts in\nByzantine Greek manuscripts and will develop new\ndigital tools designed to perform linguistic anal-\nysis and to detect patterns and variations in this\nfragmented corpus of Byzantine text.\nThe linguistic analysis of Ancient (and Me-\ndieval) Greek has some challenges. The free word\norder is expected to create difﬁculties for automatic\nlinguistic preprocessing, but both Dik and Whal-\ning (2008) and Keersmaekers (2019) conclude that\nthe word order does not cause speciﬁc problems\nfor the task of Part-of-Speech (PoS) tagging. Con-\nsidering we are interested in Byzantine Greek in\nparticular, it is also important to mention there are\nmajor differences between Classical Greek and\nByzantine Greek, which can be observed on the\nphonetic, phonological, morphological, syntacti-\ncal and lexical level. In some cases, Byzantine\nGreek is reminiscent of Modern Greek, rather than\nAncient Greek. As a consequence, many of the\nsystems that are widely used for the analysis of An-\ncient Greek are expected to struggle with Byzantine\nGreek. The very popular morphological analysis\ntool Morpheus (cf. infra) in particular, is expected\nto not operate well for Medieval Greek.\nThe remainder of this paper is organized as fol-\nlows. In Section 2, we brieﬂy discuss some of the\nrelevant research related to linguistic preprocessing\nof Ancient Greek. In Section 3, we describe the\n3In this paper, Classical and Medieval Greek are used in-\nterchangeably with Ancient and Byzantine Greek respectively.\n4https://www.dbbe.ugent.be/\n129\ndata set used to train and evaluate the morphologi-\ncal analyser and provide details on the creation of\na novel Byzantine gold standard data set. Section\n4 gives an overview of the BERT language model\ncreation for Ancient and medieval Greek, whereas\nSection 5 describes the ﬁne-tuning of the resulting\nmodel on the task of morphological analysis. In\nSection 6, we end with concluding remarks as well\nas indications for future research.\n2 Related Research\nPrevious research has resulted in various tools and\nresources for the linguistic analysis of Ancient\nGreek.\nThe ﬁrst systems to perform NLP tasks on An-\ncient Greek relied on morphological analysis tools.\nIn the early 1970s, Packard (1973) already wrote\na program to analyze Greek words with a 95% ac-\ncuracy. Great strides forward were achieved in\nthe 1990s, when the morphological Analysis tool\nMorpheus was developed in the framework of the\nPERSEUS project (Kent, 1991). From this point\nonward, Morpheus has been the standard tool for\nAncient Greek morphological analysis. In his pa-\nper on Generating and Parsing Classical Greek,\nCrane (1991) describes how Morpheus operates by\ncombining information from three databases: one\ndatabase containing stems, a second one contain-\ning possible inﬂections and a third one containing\nirregular forms. Although this system is accurate\nin optimal circumstances, it has two distinct weak-\nnesses: (1) all possible morphological analyses\nare provided, but Morpheus does not decide which\nanalysis is correct in casu, and (2) the rule-based\nnature does not deal well with unknown words or\nnon-classical Greek forms. The system can deal\nwith some historical and dialectal variation to a cer-\ntain extent, but Byzantine Greek is not within the\nscope of Morpheus. A possible remedy consists in\nmanually adding words to Morpheus’ vocabulary\nas shown by Keersmaekers (2019). This approach,\nhowever, does not scale very well nor would it rem-\nedy the issue of the conjugation and declension\nof existing words changing. Nevertheless, we can\nobserve that in many of the approaches presented\nfor Ancient Greek, the morphological information\nprovided by Morpheus is fundamental in the analy-\nsis.\n2.1 Part-of-Speech Tagging\nPart-of-Speech tagging is the NLP task of auto-\nmatically assigning a morphological label to each\ntoken in a text, and traditionally refers to assigning\na coarse-grained grammatical category, viz. a part\nof speech (such as noun, adjective, verb) to every\ntoken. For the analysis of Ancient Greek, however,\nPart-of-Speech tagging usually refers to a more\nﬁne-grained, full morphological analysis, i.e., the\npart of speech and morphological features such as\ngender, person, number, mood or tense. Therefore,\nwe will also use the term Part-of-Speech tagging\nand (full) morphological analysis interchangeably\nin this paper.\nTreetagger (Schmid, 1994), a probabilistic tag-\nging method employing decision trees, was one of\nthe ﬁrst Part-of-Speech taggers used to analyse An-\ncient Greek. Dik and Whaling (2008) enhanced the\ntool with a lexicon from Morpheus. The efﬁcacy\nof Treetagger is proven by its continued use: even\nrecently the tagger has been used to achieve state-\nof-the-art results on the Diorisis Ancient Greek\nCorpus (Mcgillivray and Vatri, 2018).\nCelano, Crane and Majidi (2016) compared ﬁve\ndifferent PoS-taggers for Ancient Greek: Mate 5,\nHunpos6, RFTagger7, OpenNLP8 and NLTK uni-\ngram tagger9. They perform their comparison us-\ning a 10-fold cross-validation test on the Ancient\nGreek Dependency Treebank (AGDT). In this com-\nparison, Morpheus was applied to provide all pos-\nsible morphological analyses for each word, from\nwhich the correct one was manually chosen in or-\nder to construct the training (and test) data. Mate\nachieved the best results (88% accuracy), followed\nby the other taggers. This relatively low accuracy\ncan be attributed to the very ﬁne-grained nature of\nthe analyses. It is important to keep the standard,\nclassical nature of the texts in mind here when ob-\nserving the tagging results.\nKeersmaekers (2019) offers a second look at the\ndifferent available part-of-speech taggers. He com-\npares RFTagger, MarMoT10 and Mate. The results\nobtained from this study differ from the aforemen-\ntioned study by Celano et al. (2016). Mate was\noutperformed by both RFTagger and MarMoT on\n5https://www.ims.uni-stuttgart.de/en/research/resources/\ntools/matetools/\n6https://code.google.com/archive/p/hunpos/\n7https://www.cis.uni-muenchen.de/ schmid/tools/RFTagger/\n8https://opennlp.apache.org/\n9https://www.nltk.org/book/ch05.html\n10http://cistern.cis.lmu.de/marmot/\n130\na papyrus test corpus. With a reported accuracy of\n94.7%, RFTagger and MarMoT both exceed expec-\ntations. It is worth mentioning that the addition of\na lexicon from Morpheus increased the accuracy of\nRFTagger by 2.4%. Keersmaekers attributes these\ndivergent results to the difference between tag sets.\nThe relatively worse performance of Mate was at-\ntributed to the following factors: (1) the tagging\nmodel being unsuitable for Greek, (2) the smaller\namount of training data, and (3) the joint parsing\nmodel of Mate could be detrimental due to low\nparsing accuracy. Just like the Byzantine data that\nwe want to analyze for this project, the test data\nused for this comparison also diverges from classi-\ncal Ancient Greek, albeit in a different way. At the\none hand, the Greek found in the papyrus corpus is\ncloser to Ancient Greek on the morphological level.\nOn the other hand, different constructions can be\nexpected to be encountered on the syntactical level.\nAll in all, the Greek found in the papyrus data can\nmore accurately be analyzed by Morpheus, which\nwill constitute a fundamental difference in possible\nviable approaches.\nThe problem caused by the incompatibility of\nMorpheus with Byzantine is a major concern for\nour research. Since the ﬁnal goal is to develop a\npreprocessing pipeline that will work for the very\nvaried Byzantine DBBE corpus, which contains\na lot of out-of-vocabulary words, the use of Mor-\npheus is preferably avoided. As mentioned already,\nit would be possible to manually add words to par-\ntially alleviate this issue, but this is not a viable\nsolution due to the size of the DBBE.\nMore recently, researchers have started creating\nPart-of-Speech taggers not incorporating informa-\ntion from Morpheus. Helmut Schmid’s most re-\ncent creation, the RNNTagger, was of particular\ninterest due to an increased tagging accuracy on\nthe AGDT (Schmid, 2019). The reported results,\nviz. accuracy scores of 91%, are very impressive\nconsidering this system is no longer dependent on\nseparate output from Morpheus. Additionally, if\nRNNTagger would be trained on sufﬁcient Byzan-\ntine Greek data, it is likely to perform similarly,\nand would be the ﬁrst system to perform Part-of-\nSpeech tagging on Byzantine Greek with such high\naccuracy. Unfortunately, no such data is publicly\navailable as of yet.\nFinally, some other recent developments can also\nbe mentioned here. Stanford university’s Stanza\npackage (Qi et al. 2020) contains two different PoS-\ntaggers, each one trained on a different database,\nviz. the AGLDT and PROIEL treebanks. A shallow\nevaluation we performed for these taggers showed\nthat RNNTagger outperformed both of them.\n2.2 Language Models for Ancient Greek\nRecent state-of-the-art PoS-taggers (see for in-\nstance Heinzerling and Strube (2019)) often inte-\ngrate information from a BERT language model.\nBERT (Bidirectional Encoder Representations\nfrom Transformers) (Devlin et al., 2019) is based\non Transformers, a deep learning model where\nevery output node is linked to every input node,\nwhereas the weights between them are dynami-\ncally computed during training. As opposed to\ndirectional models, which read the input sentence\nfrom left-to-right (or right-to-left), the Transformer\nmodel is “bidirectional”, i.e. it reads the entire sen-\ntence at once. As a result, the model learns the\nrepresentation of a word based on both its left and\nright context.\nUsually, such a BERT neural language model is\npre-trained on a huge data set for a particular target\nlanguage, and then subsequently, the neural net-\nwork is used as the basis to ﬁne-tune for a speciﬁc\nNLP taks, such as PoS-tagging.\nTo the best of our knowledge, only Brennan\nNicholson has trained a BERT model for Ancient\nGreek11, which makes use of character-based em-\nbeddings. This model can, however, not easily be\nﬁne-tuned to perform tasks other than masked word\nprediction, since it’s a character-based model un-\nlike the standard sub-word model that we strive to\ntrain. Therefore, we present in this research a new\nsubword-based BERT model that can be ﬁne-tuned\nto perform various tasks like morphological analy-\nsis for Ancient and Medieval Greek. More details\non the creation of this language model can be found\nin Section 4.\n3 Training and Evaluation Data Sets for\nMorphological Analysis\nTo train (and evaluate) our novel morphological\nanalyser, we extracted the relevant information\nfrom three different treebanks (Section 3.1). To\nevaluate the resulting model on Byzantine Greek,\na novel gold standard data set was created (Sec-\ntion 3.2).\n11https://github.com/brennannicholson/ancient-greek-\nchar-bert\n131\n3.1 Training Data Morphological Analyser\nIn order to train and validate a morphological anal-\nyser for Ancient and Medieval Greek, we used three\nexisting treebanks, viz. the PROIEL treebank12, the\nAncient Greek Dependency treebank13 and the tree-\nbank created by Vanessa Gorman14.\nThe PROIEL treebank (Haug and Jøhndal,\n2008) consists of three different texts: (1) The\nGreek New Testament, (2) Herodotus’ Histories,\nand (3) Sphrantzes’ chronicles. As such, part of the\ntreebank is Byzantine Greek, while the other part\nstems from the classical period.\nThe Ancient Greek Dependency Tree-\nbank (Bamman and Crane, 2011) includes\nexclusively Classical Greek data. It currently\ncontains 557,922 tokens of various different\nauthors, genres and dialects, ranging from Homer’s\nEpics to various of Sophocles’ tragedies, different\ndialects and periods. The most recent text present\nin this database is Athenaeus’ Deipnosophists.\nGorman’s Treebank(Gorman, 2020) aims to\ncollect representative texts from Ancient Greek\nprose authors. This monumental work annotated\nby the author contains over 550,000 tokens of Clas-\nsical Greek Prose, and no Byzantine data.\nIn conclusion, the main part of the data used\nto train the morphological analyser is written in\nClassical Greek, but the presence of a reasonable\namount of Byzantine Greek should not be under-\nstated. This Byzantine fraction will be likely to\nhelp the model in analyses of Byzantine Greek.\nTo create the training data set for the develop-\nment of the ﬁne-grained PoS-model, we extracted\nall tokens and corresponding Part-of-Speech tags\nfrom all three treebanks, that had already been con-\nverted to a uniform format by Keersmaekers (2020).\nIt is important to mention, though, that the Part-of-\nSpeech tags present in the treebanks have been\nfurther reﬁned by Keersmaekers (2020), who for\ninstance makes a distinction between coordinate\nand subordinate conjunctions, which is not present\nin the original treebank annotations.\nAs mentioned before, the Part-of-Speech tags\nused in this research contain a full morphologi-\ncal analysis as well. The predicted labels consist\nof nine parts: coarse-grained part-of-speech cate-\ngory, person, number, tense, mood, voice, gender,\ncase, and degree. Elements that are not relevant\n12https://proiel.github.io/\n13https://perseusdl.github.io/treebankdata/\n14https://github.com/perseids-publications/gorman-trees\nfor a given token are represented by ’-’. The label\nv2spme— , for instance, stands for a verb - in the\nsecond person - singular - present tense - imperative\n- mediopassive. This ﬁne-grained morphological\ninformation leads to a high number of possible la-\nbels, resulting in 558 different class labels for the\nGorman Treebank, 483 for the PROIEL Treebank\nand 599 for AGDT. Needless to say that predict-\ning these ﬁne-grained morphological labels makes\nthe present machine learning task far more chal-\nlenging than the more traditional coarse-grained\nPart-of-Speech tagging task.\n3.2 Gold Standard Creation for Byzantine\nGreek\nIn order to create a gold standard for evaluating\nthe morphological analyser for medieval Greek, we\nmanually annotated part of the existing Database\nof Byzantine Book Epigrams (henceforth DBBE).\nThis database, made and maintained by Ghent Uni-\nversity, consists of Greek book epigrams (poems\nin and on books) up to the ﬁfteenth century. Two\nkinds of textual material are distinguished and de-\nﬁned as follows by the DBBE team:\nOccurrences: “Book epigrams exactly as they oc-\ncur in one speciﬁc manuscript. The data collected\nhere is largely the result of careful manuscript con-\nsultation, either in situ or based on (digital) re-\nproductions. The remainder is compiled from de-\nscriptive catalogues and other relevant publications.\nIndividual verses found in multiple occurrences\nare linked together by means of dedicated Verse\nvariants pages.”\nTypes: “Book epigrams independently of how\nexactly they occur in the manuscripts, often, yet\nnot always, regrouping several occurrences that\nhave an identical or at least very similar text. If\navailable, the text of a type is drawn from a critical\nedition. If not, it is a normalised version of a single\nrepresentative occurrence.” For the presented pilot\nstudy, we focused on the “Types” data. In future\nresearch, we will also annotate part of the more\nirregular “Occurrences” data.\nTo create the gold standard, we selected about\n9000 tokens (which amounts to 6,5% of the com-\nplete “Types” database). To speed up the manual\nvalidation process, we bootstrapped the PoS infor-\nmation from the output of the RNN-tagger (Schmid,\n2019), that performed reasonably well on this data.\n132\nAncient Greek BERTtrained on text from treebanks with random initialisation 18.0\n+ Initialization from bert-greek-uncased instead of random\nMixed Greek BERT 9.8\n+ Data from Perseus Digital Library and First1K Greek Database\nExpanded Ancient Greek BERT 4.9\nTable 1: Content and perplexity scores of the three ﬂavours of BERT language models trained for Ancient Greek.\nFour peculiarities of RNNTagger, however, came\nto light during the manual correction of the output:\n1. Incomplete pronoun PoS-tags: in many cases\n(62/96 in a manually analysed gold standard\nsample), RNNTagger did not correctly tag pro-\nnouns with the correct person. This is partic-\nularly strange as it managed to predict the\nlemma as the ﬁrst person singular pronoun in\nall of these cases, but did not indicate the ﬁrst\nperson in the provided Part-of-Speech tag.\n2. Consistency problems: the same word is anal-\nysed in different ways in different loci, even if\nonly one analysis is possible for that speciﬁc\nword.\n3. Lemmatization and Part-of-Speech tagging\nare not corresponding: even when words are\nanalysed as a singular nominative, meaning\nthat the analysed word would in all cases\ncorrespond with the lemma, the predicted\nlemma surprisingly suggests a different word\nfor about one in ﬁve tokens.\n4. Generally poor predicted lemmata: continu-\ning on the previous remark, it can be noted\nthat the predicted lemmata are generally im-\nprecise (roughly obtaining a 50% accuracy).\nEspecially substantives, adjectives and verbs\nsuffer from this problem, whereas pronouns\nseem less problematic. As this study focuses\non Part-of-Speech tagging only, this is not a\nmajor concern, but it might be relevant for\nfurther research.\nAfter the output from RNNTagger was manually\ncorrected, an analysis from our own Part-of-Speech\ntagger output (as discussed in chapter 5) allowed\nus to correct remaining errors in the gold standard\nlabeling.\nIn future research, we plan to further annotate\nthe complete DBBE data set, which will make the\nresulting gold standard also suited to ﬁne-tune a\nsystem for PoS-tagging using this Byzantine cor-\npus.\n4 Language Modeling with BERT\nA shallow linguistic analysis showed that the\nactual nature of the “Types” diverges from the ex-\npected Byzantine Greek. Many of the byzantine\nirregularities have been normalized, and the text\nof the Types contains a lot of classical structures.\nConsequently, this will cause many of the exist-\ning systems for Ancient Greek to work remarkably\nwell. Therefore, we decided to train our ﬁnal BERT\nlanguage model on a combination of Ancient Greek\nand Modern Greek data, as the Types are deﬁnitely\nmore similar to Ancient Greek than to Modern\nGreek.\n4.1 Ancient Greek Data Used to Train the\nModel\nBERT models are very data greedy. Consequently,\nwe did not only rely on the treebank databases to\ntrain the language model, but also used the (much\nlarger) full text databases available for Ancient\nGreek. The text databases used were the follow-\ning: (1) The (Ancient Greek part of the) Perseus\nDigital Library and (2) The First1KGreek Project\nDatabase.\nThe Perseus Digital Library currently contains\n13,407,448 words of Classical Greek texts. The\nFirst1KGreek collects one edition of every Greek\nwork composed between Homer (9th century BC)\nand 250 AD, that does not occur in the Perseus Digi-\ntal Library. The combination of these two databases\nresults in a relatively complete representation of\nClassical Greek.\n4.2 BERT Model for Ancient Greek\nIn this research, we opted to ﬁrst train a generic\nBERT language model for Ancient and Byzantine\nGreek, and then in a second step ﬁne-tune the\nmodel for the task of morphological analysis.\nA ﬁrst version of the BERT language model was\ntrained primarily on Ancient and Byzantine Greek\ndata, considering the afﬁnity of the “Types” and An-\n133\ncient Greek data. All available Greek texts present\nin the three treebanks were used to train the model.\nThe model was trained with following conditions:\n15% of the words are replaced with [MASK] to-\nkens, the maximum sequence length per sentence\nwas limited to 512 sub-words and we experimented\nwith two random initialisations. The model opti-\nmizes for the Masked Language Modelling objec-\ntive which is formulated as:\n−\nV∑\nw=1\nyo,wlog(po,w) (1)\nwhere V is the size of the vocabulary, yo,w is\na binary indicator, 0 or 1, for the word w being\nthe correct replacement for [MASK], while po,w is\nthe probability predicted by the Transformer of the\nword w being a good replacement.\nIn order to evaluate the various iterations of our\nlanguage model, we use perplexity as a metric,\nshowing how uncertain any given system is. More\nformally, Perplexity is the inverse probability of\nthe test set, normalized by the number of words as\nshown in the formula below:\nPP (W) = N\n√\n1\nP(w1, w2....., wn) (2)\nThe perplexity for this ﬁrst iteration of our\nBERT model trained on the available Ancient\nGreek corpora was 18.0, whereas perplexity of\na good language model is expected to be around\n10. However, the texts in question did not have a\nmassive vocabulary comparable to, for example,\nEnglish. Therefore, theoretically the perplexity\nshould be well under 10.\nIn order to improve (i.e. decrease) the perplexity\nof our model, we tried to leverage the existing\nresources for Modern Greek. We started from the\npublicly available BERT-Greek-uncased (Kout-\nsikakis et al., 2020) pre-trained model. This BERT\nmodel for Modern Greek was trained on various\nmodern Greek resources like the Greek Wikipedia.\nEven though this model initially did not perform\nwell on Ancient Greek (perplexity >20), the\npre-training can serve as a better starting point\nthan random initialisation, helping it understand\nthe Greek characters and some common parts of\nthe vocabulary. After training this model on the\nAncient Greek corpus using MLM, performance\nwas signiﬁcantly increased and the perplexity\ndropped to 9.8 on the held-out test set. From now\nFigure 1: The convergence of loss for the Expanded\nAncient Greek BERT model on the held out test set.\non, we will refer to this model as theMixed Greek\nBERT model.\nHowever, since we hypothesize that the theo-\nretical perplexity could be much lower than that\nof a standard BERT model, we wanted to further\nexplore the upper bound of an Ancient Greek\nBERT. We built a third and ﬁnal model starting\nfrom the pre-trained Modern Greek model, but also\nincluding the two large text collections described\nin Section 4.1, the Greek part of the Perseus\nDigital library and the First1KGreek database, in\naddition to the treebank data, overall increasing\nthe original training data by around 300 percent.\nWe also used special lower-casing for Greek, and\nde-accentuated the text as a pre-processing step.\nThis new model considerably outperforms all the\nprevious iterations and results in a perplexity of\n4.9 on a held-out test set. The validation loss\nconvergence as a function of time can be seen in\nFigure 1. This model is henceforth referred to\nas the Expanded Ancient Greek BERTmodel.\nThis pre-trained language model has been made\npublicly available, together with some sample code\nshowing how to use it15. Table 1 summarizes the\ncontent and perplexity scores of the three ﬂavours\nof BERT language models we trained for Ancient\nand Byzantine Greek.\n5 Fine-tuning of the BERT model for\nMorphological Analysis\nIn a last step, we incorporated the novel BERT\nlanguage model for Ancient and Byzantine Greek\ninto our morphological analysis system.\n15https://github.com/pranaydeeps/Ancient-Greek-BERT\n134\n5.1 Training of the Morphological Analyser\nAs explained in Section 3.1, we extracted all tokens\nand corresponding ﬁne-grained PoS-tags from the\nthree treebanks. We use the contextual token em-\nbeddings from the Expanded Ancient Greek BERT\nmodel described in Section 4.2, and stack them\nwith randomly initialized Character Embeddings\n(Lample et al., 2016), which are then processed by\na standard Bi-directional Long Short Term Mem-\nory (LSTM) encoder and a Conditional Random\nField (CRF) decoder, commonly used in sequential\ntagging tasks. The Expanded Ancient Greek BERT\nembeddings are the only frozen part of the model,\nwhile all the other fragments are trained. We use\nthe FLAIR framework (Akbik et al., 2019) for this\nset of experiments. While the Bi-LSTM CRF ar-\nchitecture is a staple of many successful sequential\ntaggers, we elect to stack our BERT embeddings\nwith character embeddings that have found exten-\nsive use in models for languages with high morpho-\nlogical diversity (Vylomova et al., 2017). We use\na hidden size of 256 for our LSTM, and initialize\nwith a starting learning rate of 0.1, which is linearly\ndecreased.\n5.2 Evaluation of the Morphological\nAnalyser\nFor the evaluation of the tagger it is important\nto keep in mind that we are analyzing Byzan-\ntine Greek with models trained mainly on Ancient\nGreek. The tagger’s accuracy will therefore in-\nevitably be lower. Second, inter-annotator agree-\nment on Greek PoS-tagging is most likely a lot\nlower than for other languages. In various cases,\ndifferent analyses can be considered correct by\nexperts. Participles, for example, form a middle\nground between verbs on the one hand and sub-\nstantives and adjectives on the other hand. In some\ncases, a participle that was repeatedly used in a\ncertain form is accepted as a noun or adjective by\nlinguists. This status, however, depends on con-\nvention. As such, it is very difﬁcult for a system\nto detect whether a participle is considered a noun,\nadjective of verb in many occasions. Even for hu-\nmans, this issue can be hard to resolve, arguments\ncan sometimes be made for different analyses. For\nthis pilot study, we only constructed a modest gold\nstandard of 9000 tokens. In future research, we\nintend to construct detailed annotation guidelines\nand perform inter-annotator experiments to further\nexpand the current gold standard, based on the in-\nsights gained during this pilot study.\n5.2.1 Validation on the in-domain data\nThe performance of our trained model obtains\nstate-of-the-art results on the very ﬁne-grained PoS\nscheme incorporating a full morphological analysis\nas applied in the treebanks. Table 2 shows the ac-\ncuracy scores for the different treebank validation\nsets. These validation sets have been randomly se-\nlected from the treebanks data, and have not been\nused for training the morphological analysis model.\nThe scores presented are the result of a four-fold\ncross-validation evaluation.\nTreebank Accuracy\nAGDT 88.64%\nPROIEL 92.87%\nGORMAN 91.85%\nTable 2: Accuracy of our morphological analysis tag-\nger on in-domain data, viz. validation sets of the three\ntreebanks used for training.\nWhen we compare this to reported scores for the\nvarious treebanks, the newly trained PoS-tagger ob-\ntains very competitive results. The study of Celano\net al. (2016) comparing ﬁve taggers for Ancient\nGreek reports the best score for Mate on the AGDT,\nincorporating information from Morpheus, with an\naccuracy of 88%. It is important to note, however,\nthat the scores are not directly comparable, as they\nare not obtained on exactly the same data partitions.\nWhen we only consider the coarse-grained PoS-\ntags, viz. the syntactic categories (e.g. noun, verb),\nthe results are comparable to state-of-the-art PoS-\ntaggers for other languages. Table 3 gives an\noverview of the accuracy scores when only taking\ninto account the coarse-grained PoS-tags.\nTreebank Accuracy\nAGDT 90.28%\nPROIEL 97.40%\nGORMAN 95.71%\nTable 3: Accuracy for the coarse-grained PoS-tags on\nin-domain data.\n5.2.2 Evaluation on the held-out test data\nThe gold standard that was created allows to eval-\nuate the performance of existing PoS-taggers on\nByzantine Greek data. For this pilot study, we\ncompare RNNTagger with our newly created PoS-\ntagger. As is illustrated by Table 4, our PoS-tagger\n135\nincorporating the new BERT language model ob-\ntains very promising results, with an accuracy of\n86.88% for the full morphological analysis. When\nwe only consider the coarse-grained PoS-tags, the\naccuracy increases to 92.97%.\nFine-grained PoS Tagger Accuracy\nRNNTagger 76.97%\nBERT model 86.88%\nTable 4: Accuracy of the two ﬁne-grained PoS-taggers\non the Byzantine gold standard data set.\nWe also performed a qualitative analyse of both\ntaggers’ output. The tags predicted by RNNTagger\nstructurally subtly differ from the tags in our gold\nstandard. This leads to reduced accuracy. Our train-\ning data, for instance, distinguishes between coor-\ndinating and subordinating conjunctions whereas\nRNNTagger does not make this distinction. On the\nother hand, in cases where several interpretations\nwere possible, RNNTagger’s interpretation is more\nlikely to be adopted as manual correction started\nfrom RNNTagger’s output.\nTo offer some insights into the frequency of the\ndetected errors, a small test sample containing 523\nﬁne-grained PoS-tags of RNNTagger and our own\nmorphological analyser (discussed in section 5)\nwas examined and compared in detail. RNNTagger\nmainly struggles with the following categories:\n• Providing full tags for personal pronouns: the\nperson is often omitted (in 8/8 cases in our\nsmall test sample).\n• Consistently analysing words with only one\npossible correct analysis (2 occurrences of this\nphenomenon in the small test sample).\n• Recognizing Byzantine names and vocatives\nin general (8 errors due to incorrectly analysed\nByzantine names).\n• Detecting ’·’ as a punctuation mark (the Greek\n’:’, not to be confused with ’.’). This is incor-\nrectly analysed as a part of the previous word\nin about 95% of cases.\nOn our gold standard, RNNTagger was 76.97%\naccurate. Considering the nature of the Byzantine\ntext and the slight mismatch between the annota-\ntions of the gold standard and RNNTagger, this\naccuracy is in line with the expectations. Pronouns\nnot being recognized as such, and the wrongly\ntagged proper names, caused a signiﬁcant decrease\nfor the accuracy of the tagger.\nFinally, our own model achieved an accuracy of\n86.88%. The tagger’s most notable errors observed\nwere the following:\n• Iota subscriptum (a iota in subscript) was of-\nten not recognized by the model, leading to\nincorrect analyses.\n• Parentheses and other symbols indicating un-\ncertainty in the text caused the model to not\nmake sensible predictions for the word in\nquestion. Even an apostrophe would cause\nwrong PoS-analyses.\n• The perfect tense was sometimes not recog-\nnized.\n• Proper names that are indeclinable are anal-\nysed as a noun without gender, case and num-\nber. The gold standard does, however, include\nthis more detailed information, causing all in-\ndeclinable names to be evaluated as incorrect.\n• As was the case with RNNTagger, ’·’ is never\nanalysed as a punctuation mark, but as part of\nthe previous word instead.\nIt is important to remark that comparing RNN-\nTagger with our newly developed morphological\nanalyser is a difﬁcult exercise. The output of both\ntaggers diverges in a different way from the gold\nstandard. In 26 cases in our small test sample of\n523 words, an error resulted from an analysis that\nwas in line with how the respective tagger was\ntrained, but was evaluated as incorrect because the\npredicted label was not corresponding with our\ngold standard labeling.\nTo conclude, our new morphological analysis\nmodel resolves most of the issues RNNTagger\nstruggled with: Byzantine names and vocatives\nwere almost always correctly analysed, full pro-\nnoun tags were provided, and results were consis-\ntent. Punctuation, however, appears to be challeng-\ning for the new model as well.\n6 Conclusions\nIn this pilot study, we ﬁrst present a new BERT-\nbased language model for Ancient and Byzan-\ntine Greek, the so-called Expanded Ancient Greek\nBERT model, obtaining very good perplexity re-\nsults. The model starts from a pre-trained Mod-\nern Greek language model, to which Ancient and\n136\nByzantine Greek data (Perseus Digital library,\nFirst1KGreek database and AGDT, Proiel and Gor-\nman treebank data) is added. The performance of\nthe language model was further improved by using\nspecial lowercasing and de-accentuation for Greek.\nSecond, we ﬁne-tuned a novel morphological\nanalysis model for Ancient and Byzantine Greek,\nthat does not rely on separate morphological input\nfrom Morpheus, as is the case for many existing\ntaggers. As a result, this model is more ﬂexible\nto analyse irregular forms in Ancient Greek and\nwill perform better at analysing Greek that does\nnot originate from the classical period. The model\nachieves state-of-the-art performance on a valida-\ntion set extracted from the PROIEL, AGDT and\nGorman treebank data sets. The model also obtains\nvery promising results on a newly created gold\nstandard consisting of Byzantine Greek epigrams.\nIn future research, we will build on the insights\nfrom this pilot study to improve the performance of\nthe ﬁne-grained Part-of-Speech tagger for Byzan-\ntine Greek. A ﬁrst line of research will consist of\nestablishing detailed annotation guidelines and con-\nducting inter-annotator experiments to further im-\nprove and expand the gold standard data set. Once\na considerable amount of the DBBE will be anno-\ntated, this data can be used to further ﬁne-tune the\nmorphological analyser for Byzantine Greek data.\nAnother venue for future research will consist of\nadding more diverse Greek data, e.g. Byzatine data\nfrom the DBBE, to update the language model, and\nevaluate whether this impacts the performance of\nthe tagger on Byzantine Greek data.\nFinally, we will also experiment with a cascaded\napproach predicting the various parts of the mor-\nphological analysis consecutively, starting with\nthe coarse-grained part-of-speech category (e.g.,\nnoun, verb). This allows to separate the “classical”\npart-of-speech from the ﬁne-grained morphological\nanalysis, providing more evidence for each part of\nthe analysis. This approach might, however, intro-\nduce error percolation from the various consecutive\nsteps.\nTo conclude, we are conﬁdent that the newly\ndeveloped BERT-based language model will be\na valuable contribution to NLP for Ancient and\nMedieval Greek, as it can be ﬁne-tuned for a variety\nof downstream tasks, such as linguistic analysis.\nAcknowledgements\nSpecial thanks go out to Dr. Alek Keersmaekers,\nwho provided us with uniform versions of the three\ntreebanks. We also thank Dr. Ilse De V os, Marthe\nNemegeer and Noor Vanhoe from the DBBE team\nfor their help with the creation of the gold standard\nevaluation set.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFlair: An easy-to-use framework for state-of-the-art\nnlp. In NAACL 2019, 2019 Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59.\nD. Bamman and G. Crane. 2011. The ancient greek and\nlatin dependency treebanks. pages 79–98. Springer.\nF. Bernard and K. Demoen. 2019. Book epigrams. a\ncompanion to byzantine poetry. volume 4, pages\n404–429. Brill.\nG. Celano, G. Crane, and S. Majidi. 2016. Part of\nspeech tagging for ancient greek. Open Linguistics,\n2:393–399.\nG. Crane. 1991. Generating and parsing classical greek.\nliterary and linguistic computing. Literary and Lin-\nguistic Computing, 6:243–245.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nH. Dik and R. Whaling. 2008. Bootstrapping classical\ngreek morphology. In Proceedings of Digital Hu-\nmanities 2008, pages 105–106, Oulu.\nV .B. Gorman. 2020. Dependency treebanks of ancient\ngreek prose. Journal of Open Humanities Data, 6.\nD. T. Haug and M. Jøhndal. 2008. Creating a paral-\nlel treebank of the old indo-european bible transla-\ntions. In Proceedings of the Second Workshop on\nLanguage Technology for Cultural Heritage Data\n(LaTeCH 2008), pages 27–34.\nBenjamin Heinzerling and Michael Strube. 2019. Se-\nquence tagging with contextual and non-contextual\nsubword representations: A multilingual evaluation.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 273–\n291, Florence, Italy. Association for Computational\nLinguistics.\n137\nA. Keersmaekers. 2019. Creating a richly annotated\ncorpus of papyrological greek: The possibilities of\nnatural language processing approaches to a highly\ninﬂected historical language. Digital Scholarship in\nthe Humanities, 35:67–82.\nA. Keersmaekers. 2020. A computational approach to\nthe greek papyri: Developing a corpus to study vari-\nation and change in the post-classical greek comple-\nmentation system.\nA. Kent. 1991. Encyclopedia of Library and Infor-\nmation Science: Volume 48 - Supplement 11: Au-\ntomated Archival Systems to University-Based Tech-\nnology Transfer and 2000: Explanation: Example,\nand Expectations. CRC Press.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. GREEK-\nBERT: the greeks visiting sesame street. In SETN\n2020: 11th Hellenic Conference on Artiﬁcial Intelli-\ngence, Athens, Greece, September 2-4, 2020 , pages\n110–117. ACM.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nB. Mcgillivray and A. Vatri. 2018. The diorisis ancient\ngreek corpus. Research Data Journal for the Hu-\nmanities and Social Sciences, 3:55–65.\nD.W. Packard. 1973. Computer-assisted morphologi-\ncal analysis of ancient greek. In COLING 1973 Vol-\nume 2: Computational And Mathematical Linguis-\ntics: Proceedings of the International Conference on\nComputational Linguistics.\nH. Schmid. 1994. Probabilistic part-of-speech tagging\nusing decision trees. In Proceedings of International\nConference on New Methods in Language Process-\ning, Manchester, UK.\nH. Schmid. 2019. Deep learning-based morphologi-\ncal taggers and lemmatizers for annotating historical\ntexts. In DATeCH2019: Proceedings of the 3rd In-\nternational Conference on Digital Access to Textual\nCultural Heritage, pages 133–137.\nEkaterina Vylomova, Trevor Cohn, Xuanli He, and\nGholamreza Haffari. 2017. Word representation\nmodels for morphologically rich languages in neu-\nral machine translation. In Proceedings of the First\nWorkshop on Subword and Character Level Models\nin NLP, pages 103–108, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.",
  "topic": "Byzantine architecture",
  "concepts": [
    {
      "name": "Byzantine architecture",
      "score": 0.7580274343490601
    },
    {
      "name": "Computer science",
      "score": 0.7352226376533508
    },
    {
      "name": "Ancient Greek",
      "score": 0.6774083375930786
    },
    {
      "name": "Greek language",
      "score": 0.6607664823532104
    },
    {
      "name": "Language model",
      "score": 0.6257901191711426
    },
    {
      "name": "Natural language processing",
      "score": 0.6201544404029846
    },
    {
      "name": "Preprocessor",
      "score": 0.5522637963294983
    },
    {
      "name": "Perplexity",
      "score": 0.5062193274497986
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44986724853515625
    },
    {
      "name": "Analyser",
      "score": 0.437508761882782
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4208652675151825
    },
    {
      "name": "Art",
      "score": 0.22021207213401794
    },
    {
      "name": "Classics",
      "score": 0.21307605504989624
    },
    {
      "name": "Programming language",
      "score": 0.1783428192138672
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32597200",
      "name": "Ghent University",
      "country": "BE"
    }
  ]
}