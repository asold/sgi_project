{
  "title": "Space-time Mixing Attention for Video Transformer",
  "url": "https://openalex.org/W3170860705",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227082103",
      "name": "Bulat, Adrian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287274040",
      "name": "Pérez-Rúa, Juan-Manuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225109995",
      "name": "Sudhakaran, Swathikiran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223661298",
      "name": "Martinez, Brais",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750763277",
      "name": "Tzimiropoulos, Georgios",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2016053056",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2963315828",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W3109304426",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2981385151",
    "https://openalex.org/W2770804203",
    "https://openalex.org/W3037916678",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2889614720",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2963844898",
    "https://openalex.org/W3035619757",
    "https://openalex.org/W2948048211",
    "https://openalex.org/W3025409017",
    "https://openalex.org/W2964242760",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3096833468",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2971680695",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3035104321",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2989728968",
    "https://openalex.org/W2549139847"
  ],
  "abstract": "This paper is on video recognition using Transformers. Very recent attempts in this area have demonstrated promising results in terms of recognition accuracy, yet they have been also shown to induce, in many cases, significant computational overheads due to the additional modelling of the temporal information. In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model. To achieve this, our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer's depth to obtain full temporal coverage of the video sequence. (b) It uses efficient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. We also show how to integrate 2 very lightweight mechanisms for global temporal-only attention which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model produces very high recognition accuracy on the most popular video recognition datasets while at the same time being significantly more efficient than other Video Transformer models. Code will be made available.",
  "full_text": "Space-time Mixing Attention for Video Transformer\nAdrian Bulat\nSamsung AI Cambridge\nadrian@adrianbulat.com\nJuan-Manuel Perez-Rua\nSamsung AI Cambridge\nj.perez-rua@samsung.com\nSwathikiran Sudhakaran\nSamsung AI Cambridge\nswathikir.s@samsung.com\nBrais Martinez\nSamsung AI Cambridge\nbrais.a@samsung.com\nGeorgios Tzimiropoulos\nSamsung AI Cambridge\nQueen Mary University of London\ng.tzimiropoulos@qmul.ac.uk\nAbstract\nThis paper is on video recognition using Transformers. Very recent attempts in\nthis area have demonstrated promising results in terms of recognition accuracy,\nyet they have been also shown to induce, in many cases, signiﬁcant computational\noverheads due to the additional modelling of the temporal information. In this\nwork, we propose a Video Transformer model the complexity of which scales\nlinearly with the number of frames in the video sequence and hence induces\nno overheadcompared to an image-based Transformer model. To achieve this,\nour model makes two approximations to the full space-time attention used in\nVideo Transformers: (a) It restricts time attention to a local temporal window\nand capitalizes on the Transformer’s depth to obtain full temporal coverage of the\nvideo sequence. (b) It uses efﬁcient space-time mixing to attend jointly spatial and\ntemporal locations without inducing any additional cost on top of a spatial-only\nattention model. We also show how to integrate 2 very lightweight mechanisms for\nglobal temporal-only attention which provide additional accuracy improvements at\nminimal computational cost. We demonstrate that our model produces very high\nrecognition accuracy on the most popular video recognition datasets while at the\nsame time being signiﬁcantly more efﬁcient than other Video Transformer models.\nCode will be made available.\n1 Introduction\nVideo recognition – in analogy to image recognition – refers to the problem of recognizing events\nof interest in video sequences such as human activities. Following the tremendous success of\nTransformers in sequential data, speciﬁcally in Natural Language Processing (NLP) [34, 5], Vision\nTransformers were very recently shown to outperform CNNs for image recognition too [43, 11, 30],\nsignaling a paradigm shift on how visual understanding models should be constructed. In light of\nthis, in this paper, we propose a Video Transformer model as an appealing and promising solution for\nimproving the accuracy of video recognition models.\nA direct, natural extension of Vision Transformers to the spatio-temporal domain is to perform the\nself-attention jointly across all Sspatial locations and T temporal locations. Full space-time attention\nthough has complexity O(T2S2) making such a model computationally heavy and, hence, impractical\neven when compared with the 3D-based convolutional models. As such, our aim is to exploit the\ntemporal information present in video streams while minimizing the computational burden within the\nTransformer framework for efﬁcient video recognition.\nPreprint. Under review.\narXiv:2106.05968v2  [cs.CV]  11 Jun 2021\n(a) Full space-time atten-\ntion: O(T2S2)\n(b) Spatial-only attention:\nO(TS 2)\n(c) TimeSformer [ 3]:\nO(T2S + TS 2)\n(d) Ours: O(TS 2)\nFigure 1: Different approaches to space-time self-attention for video recognition. In all cases, the key\nlocations that the query vector, located at the center of the grid in red, attends are shown in orange.\nUnlike prior work, our key vector is constructed by mixing information from tokens located at the\nsame spatial location within a local temporal window. Our method then performs self-attention with\nthese tokens. Note that our mechanism allows for an efﬁcient approximation of local space-time\nattention at no extra cost when compared to a spatial-only attention model.\nA baseline solution to this problem is to consider spatial-only attention followed by temporal\naveraging, which has complexity O(TS2). Similar attempts to reduce the cost of full space-time\nattention have been recently proposed in [3, 1]. These methods have demonstrated promising results\nin terms of video recognition accuracy, yet they have been also shown to induce, in most of the\ncases, signiﬁcant computational overheads compared to the baseline (spatial-only) method due to the\nadditional modelling of the temporal information.\nOur main contributionin this paper is a Video Transformer model that has complexity O(TS2)\nand, hence, is as efﬁcient as the baseline model, yet, as our results show, it outperforms re-\ncently/concurrently proposed work [ 3, 1] in terms of efﬁciency (i.e. accuracy/FLOP) by signiﬁcant\nmargins. To achieve this, our model makes two approximations to the full space-time attention used\nin Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on\nthe Transformer’s depth to obtain full temporal coverage of the video sequence. (b) It uses efﬁcient\nspace-time mixing to attend jointly spatial and temporal locations without inducing any additional\ncost on top of a spatial-only attention model. Fig. 1 shows the proposed approximation to space-time\nattention. We also show how to integrate two very lightweight mechanisms for global temporal-only\nattention, which provide additional accuracy improvements at minimal computational cost. We\ndemonstrate that our model is surprisingly effective in terms of capturing long-term dependencies and\nproducing very high recognition accuracy on the most popular video recognition datasets, including\nSomething-Something-v2 [15], Kinetics [4] and Epic Kitchens [ 7], while at the same time being\nsigniﬁcantly more efﬁcient than other Video Transformer models.\n2 Related work\nVideo recognition:Standard solutions are based on CNNs and can be broadly classiﬁed into two\ncategories: 2D- and 3D-based approaches. 2D-based approaches process each frame independently\nto extract frame-based features which are then aggregated temporally with some sort of temporal\nmodeling (e.g. temporal averaging) performed at the end of the network [ 37, 24, 25]. The works\nof [24, 25] use the “shift trick” [40] to have some temporal modeling at a layer level. 3D-based\napproaches [4, 14, 31] are considered the current state-of-the-art as they can typically learn stronger\ntemporal models via 3D convolutions. However, they also incur higher computational and memory\ncosts. To alleviate this, a large body of works attempt to improve their efﬁciency via spatial and/or\ntemporal factorization [33, 32, 13].\nCNN vs ViT:Historically, video recognition approaches tend to mimic the architectures used for\nimage classiﬁcation (e.g. from AlexNet [21] to [18] or from ResNet [16] and ResNeXt [42] to [14]).\nAfter revolutionizing NLP [34, 28], very recently, Transformer-based architectures showed promising\nresults on large scale image classiﬁcation too [11]. While self-attention and attention were previously\nused in conjunction with CNNs at a layer or block level [6, 44, 29], the Vision Transformer (ViT)\nof Dosovitskiy et al. [11] is the ﬁrst convolution-free, Transformer-based architecture that achieves\nstate-of-the-art results on ImageNet [9].\n2\nVideo Transformer:Recently/concurrently with our work, vision transformer architectures, derived\nfrom [11], were used for video recognition [3, 1], too. Because performing full space-time attention\nis computationally prohibitive (i.e. O(T2S2)), their main focus is on reducing this via temporal and\nspatial factorization. In TimeSformer [3], the authors propose applying spatial and temporal attention\nin an alternating manner reducing the complexity to O(T2S+ TS2). In a similar fashion, ViViT [1]\nexplores several avenues for space-time factorization. In addition, they also proposed to adapt the\npatch embedding process from [11] to 3D (i.e. video) data. Our work proposes a completely different\napproximation to full space-time attention that is also efﬁcient. To this end, we ﬁrstly restrict full\nspace-time attention to a local temporal window which is reminiscent of [ 2] but applied here to\nspace-time attention and video recognition. Secondly, we deﬁne a local joint space-time attention\nwhich we show that can be implemented efﬁciently via the “shift trick” [40].\n3 Method\nVideo Transformer:We are given a video clip X ∈RT×H×W×C (C = 3, S = HW). Following\nViT [11], each frame is divided into K×K non-overlapping patches which are then mapped into\nvisual tokens using a linear embedding layer E ∈R3K2×d. Since self-attention is permutation\ninvariant, in order to preserve the information regarding the location of each patch within space\nand time, we also learn two positional embeddings, one for space: ps ∈R1×S×d and one for time:\npt ∈RT×1×d. These are then added to the initial visual tokens. Finally, the token sequence is\nprocessed by LTransformer layers.\nThe visual token at layer l, spatial location sand temporal location tis denoted as:\nzl\ns,t ∈Rd, l = 0,...,L −1, s = 0,...,S −1, t = 0,...,T −1. (1)\nIn addition to the TS visual tokens extracted from the video, a special classiﬁcation token zl\ncls ∈Rd\nis prepended to the token sequence [ 10]. The l−th Transformer layer processes the visual tokens\nZl ∈R(TS +1)×d of the previous layer using a series of Multi-head Self-Attention (MSA), Layer\nNormalization (LN), and MLP (Rd →R4d →Rd) layers as follows:\nYl = MSA(LN(Zl−1)) +Zl−1, (2)\nZl+1 = MLP(LN(Yl)) +Yl. (3)\nThe main computation of a single full space-time Self-Attention (SA) head boils down to calculating:\nyl\ns,t =\nT−1∑\nt′=0\nS−1∑\ns′=0\nSoftmax{(ql\ns,t ·kl\ns′,t′ )/\n√\ndh}vl\ns′,t′ ,\n{s=0,...,S−1\nt=0,...,T−1\n}\n(4)\nwhere ql\ns,t,kl\ns,t,vl\ns,t ∈Rdh are the query, key, and value vectors computed fromzl\ns,t (after LN) using\nembedding matrices Wq,Wk,Wv ∈Rd×dh . Finally, the output of the hheads is concatenated and\nprojected using embedding matrix Wh ∈Rhdh×d.\nThe complexity of the full model is: O(3hTSddh) (qkv projections) +O(2hT2S2dh) (MSA for h\nattention heads) +O(TS(hdh)d) (multi-head projection) +O(4TSd2) (MLP). From these terms, our\ngoal is to reduce the cost O(2T2S2dh) (for a single attention head) of the full space-time attention\nwhich is the dominant term. For clarity, from now on, we will drop constant terms and dh to report\ncomplexity unless necessary. Hence, the complexity of the full space-time attention is O(T2S2).\nOur baselineis a model that performs a simple approximation to the full space-time attention by\napplying, at each Transformer layer, spatial-only attention:\nyl\ns,t =\nS−1∑\ns′=0\nSoftmax{(ql\ns,t ·kl\ns′,t)/\n√\ndh}vl\ns′,t,\n{s=0,...,S−1\nt=0,...,T−1\n}\n(5)\nthe complexity of which is O(TS2). Notably, the complexity of the proposed space-time mixing\nattention is also O(TS2). Following spatial-only attention, simple temporal averaging is performed\non the class tokens zfinal = 1\nT\n∑\nt\nzL−1\nt,cls to obtain a single feature that is fed to the linear classiﬁer.\n3\nRecent workby [3, 1] has focused on reducing the cost O(T2S2) of the full space-time attention of\nEq. 4. Bertasius et al. [3] proposed the factorised attention:\n˜yl\ns,t =\nT−1∑\nt′=0\nSoftmax{(ql\ns,t ·kl\ns,t′ )/\n√\ndh}vl\ns,t′ ,\nyl\ns,t =\nS−1∑\ns′=0\nSoftmax{˜ql\ns,t ·˜kl\ns′,t)/\n√\ndh}˜vl\ns′,t,\n{\ns= 0,...,S −1\nt= 0,...,T −1\n}\n, (6)\nwhere ˜ql\ns,t,˜kl\ns′,t˜vl\ns′,t are new query, key and value vectors calculated from ˜yl\ns,t\n1. The above model\nreduces complexity to O(T2S+ TS2). However, temporal attention is performed for a ﬁxed spatial\nlocation which is ineffective when there is camera or object motion and there is spatial misalignment\nbetween frames.\nThe work of [1] is concurrent to ours and proposes the following approximation: Ls Transformer\nlayers perform spatial-only attention as in Eq. 5 (each with complexity O(S2)). Following this,\nthere are Lt Transformer layers performing temporal-only attention on the class tokens zLs\nt . The\ncomplexity of the temporal-only attention is, in general, O(T2).\nOur modelaims to better approximate the full space-time self-attention (SA) of Eq. 4 while keeping\ncomplexity to O(TS2), i.e. inducing no further complexity to a spatial-only model.\nTo achieve this, we make a ﬁrst approximation to perform full space-time attention but restricted to a\nlocal temporal window [−tw,tw]:\nyl\ns,t =\nt+tw∑\nt′=t−tw\nS−1∑\ns′=0\nSoftmax{(ql\ns,t ·kl\ns′,t′ )/\n√\ndh}vl\ns′,t′ =\nt+tw∑\nt′=t−tw\nVl\nt′ al\nt′ ,\n{s=0,...,S−1\nt=0,...,T−1\n}\n(7)\nwhere Vl\nt′ = [vl\n0,t′ ; vl\n1,t′ ; ... ; vl\nS−1,t′ ] ∈Rdh×S and al\nt′ = [al\n0,t′ ,al\n1,t′ ,...,a l\nS−1,t′ ] ∈RS is the\nvector with the corresponding attention weights. Eq. 7 shows that, for a single Transformer layer,\nyl\ns,t is a spatio-temporal combination of the visual tokens in the local window [−tw,tw]. It follows\nthat, after kTransformer layers, yl+k\ns,t will be a spatio-temporal combination of the visual tokens in\nthe local window [−ktw,ktw] which in turn conveniently allows to perform spatio-temporal attention\nover the whole clip. For example, for tw = 1and k= 4, the local window becomes [−4,4] which\nspans the whole video clip for the typical case T = 8.\nThe complexity of the local self-attention of Eq. 7 is O((2tw + 1)TS2). To reduce this even further,\nwe make a second approximation on top of the ﬁrst one as follows: the attention between spatial\nlocations sand s′according to the model of Eq. 7 is:\nt+tw∑\nt′=t−tw\nSoftmax{(ql\ns,t ·kl\ns′,t′ )/\n√\ndh}vl\ns′,t′ , (8)\ni.e. it requires the calculation of 2tw + 1attentions, one per temporal location over [−tw,tw]. Instead,\nwe propose to calculate a single attention over [−tw,tw] which can be achieved by ql\ns,t attending\nkl\ns′,−tw:tw ≜ [kl\ns′,t−tw ; ... ; kl\ns′,t+tw ] ∈R(2tw+1)dh . Note that to match the dimensions of ql\ns,t\nand kl\ns′,−tw:tw a further projection of kl\ns′,−tw:tw to Rdh is normally required which has complexity\nO((2tw + 1)d2\nh) and hence compromises the goal of an efﬁcient implementation. To alleviate this, we\nuse the “shift trick” [40, 24] which allows to perform both zero-cost dimensionality reduction, space-\ntime mixing and attention (between ql\ns,t and kl\ns′,−tw:tw ) in O(dh). In particular, each t′∈[−tw,tw]\nis assigned dt′\nh channels from dh (i.e. ∑\nt′ dt′\nh = dh). Let kl\ns′,t′ (dt′\nh ) ∈Rdt′\nh denote the operator for\nindexing the dt′\nh channels from kl\ns′,t′ . Then, a new key vector is constructed as:\n˜kl\ns′,−tw:tw ≜ [kl\ns′,t−tw (dt−tw\nh ),..., kl\ns′,t+tw (dt+tw\nh )] ∈Rdh . (9)\n1More precisely, Eq. 6 holds for h = 1heads. For h >1, the different heads ˜yl,h\ns,t are concatenated and\nprojected to produce ˜yl\ns,t.\n4\nMatMul\nMatMul\nSoftMax\nQ K\nV\nS*T\nS\nT\nS*T\nScale\n(a) Full Spatio-temporal attention.\nMatMul\nMatMul\nScale\nSoftMax\nS\nQ K\nV\nii-t i+tw w\ni i+tw wi-t (b) Ours.\nFigure 2: Detailed self-attention computation graph for (a) full space-time attention and (b) the\nproposed space-time mixing approximation. Notice that in our case only S tokens participate instead\nof TS. The temporal information is aggregated by indexing channels from adjacent frames. Tokens of\nidentical colors share the same temporal index.\nFig. 2 shows how the key vector ˜kl\ns′,−tw:tw is constructed. In a similar way, we also construct a new\nvalue vector ˜vl\ns′,−tw:tw . Finally, the proposed approximation to the full space-time attention is given\nby:\nyls\ns,t =\nS−1∑\ns′=0\nSoftmax{(qls\ns,t ·˜kl\ns′,−tw:tw )/\n√\ndh}˜vl\ns′,−tw:tw ,\n{s=0,...,S−1\nt=0,...,T−1\n}\n. (10)\nThis has the complexity of a spatial-only attention ( O(TS2)) and hence it is more efﬁcient than\npreviously proposed video transformers [3, 1]. Our model also provides a better approximation to the\nfull space-time attention and as shown by our results it signiﬁcantly outperforms [3, 1].\nTemporal Attention aggregation:The ﬁnal set of the class tokens zL−1\nt,cls,0 ≤t≤L−1 are used\nto generate the predictions. To this end, we propose to consider the following options: (a) simple\ntemporal averaging zfinal = 1\nT\n∑\nt zL−1\nt,cls as in the case of our baseline. (b) An obvious limitation\nof temporal averaging is that the output is treated purely as an ensemble of per-frame features and,\nhence, completely ignores the temporal ordering between them. To address this, we propose to use\na lightweight Temporal Attention (TA) mechanism that will attend the T classiﬁcation tokens. In\nparticular, a zfinal token attends the sequence [zL−1\n0,cls,..., zL−1\nT−1,cls] using a temporal Transformer\nlayer and then fed as input to the classiﬁer. This is akin to the (concurrent) work of [ 1] with the\ndifference being that in our model we found that a single TA layer sufﬁces whereas [1] uses Lt. A\nconsequence of this is that the complexity of our layer is O(T) vs O(2(Lt −1)T2 + T) of [1].\nSummary token:As an alternative to TA, herein, we also propose a simple lightweight mechanism\nfor information exchange between different frames at intermediate layers of the network. Given\nthe set of tokens for each frame t, Zl−1\nt ∈R(S+1)×dh (constructed by concatenating all tokens\nzl−1\ns,t ,s = 0,...,S ), we compute a new set of Rtokens Zl\nr,t = φ(Zl−1\nt ) ∈RR×dh which summarize\nthe frame information and hence are named “Summary” tokens. These are, then, appended to the\nvisual tokens of all frames to calculate the keys and values so that the query vectors attend the\noriginal keys plus the Summary tokens. Herein, we explore the case that φ(.) performs simple spatial\naveraging zl\n0,t = 1\nS\n∑\ns zl\ns,t over the tokens of each frame (R= 1for this case). Note that, for R= 1,\nthe extra cost that the Summary token induces is O(TS).\nX-ViT:We call the Video Transformer based on the proposed (a) space-time mixing attention and (b)\nlightweight global temporal attention (or summary token) as X-ViT.\n5\n4 Results\n4.1 Experimental setup\nDatasets: We trained and evaluated the proposed models on the following datasets (all datasets are\npublicly available for research purposes):\nKinetics-400 and 600: The Kinetics [19] dataset consists of short clips (typically 10 sec long) sampled\nfrom YouTube and labeled using 400 and 600 classes, respectively. Due to the removal of certain\nvideos from YouTube, the versions of the datasets used in this paper consist of approximately 261k\nclips for Kinetics-400 and 457k for Kinetics-600. Note, that these numbers are lower than the original\ndatasets and thus might induce a negative performance bias when compared with prior works.\nSomething-Something-v2 (SSv2): The SSv2 [ 15] dataset consists of 220,487 short videos, with a\nlength between 2 and 6 seconds that picture humans performing pre-deﬁned basic actions with\neveryday objects. Since the objects and backgrounds in the videos are consistent across different\naction classes, this dataset tends to require stronger temporal modeling. Due to this, we conducted\nmost of our ablation studies on SSv2 to better analyze the importance of the proposed components.\nEpic Kitchens-100 (Epic-100): Epic-100 is an egocentric large scale action recognition dataset\nconsisting of more than 90,000 action segments that span across 100 hours of recordings in native\nenvironments, capturing daily activities [ 8]. The dataset is labeled using 97 verb and 300 noun\nclasses. The evaluation results are reported using the standard action recognition protocol: the\nnetwork predicts the “verb” and the “noun” using two heads. The predictions are then merged to\nconstruct an “action” which is used to calculate the accuracy.\nNetwork architecture:The backbone models closely follow the ViT architecture [11]. Most of the\nexperiments were performed using the ViT-B/16 variant (L= 12, h= 12, d= 768, K = 16), where\nLrepresents the number of transformer layers, hthe number of heads, dthe embedding dimension\nand Kthe patch size. We initialized our models from a pretrained ImageNet-21k [9] ViT model. The\nspatial positional encoding ps was initialized from the pretrained 2D model and the temporal one, pt,\nwith zeros so that it does not have a great impact on the tokens early on during training. The models\nwere trained on 8 V100 GPUs using PyTorch [26].\nTesting details:Unless otherwise stated, we used ViT-B/16 and T = 8frames. We mostly used\nTemporal Attention (TA) for temporal aggregation. We report accuracy results for 1 ×3 views\n(1 temporal clip and 3 spatial crops) departing from the common approach of using up to 10 ×3\nviews [24, 14]. The 1 ×3 views setting was also used in Bertasius et al. [3]. To measure the variation\nbetween runs, we trained one of the 8–frame models 5 times. The results varied by ±0.4%.\n4.2 Ablation studies\nThroughout this section we study the effect of varying certain design choices and different components\nof our method. Because SSv2 tends to require a more ﬁne-grained temporal modeling, unless\notherwise speciﬁed, all results reported, in this subsection, are on the SSv2.\nTable 1: Effect of local window size. To\nisolate its effect from that of temporal\naggregation, the models were trained\nusing temporal averaging. Note, that\n(Bo.) indicates that only features from\nthe boundaries of the local window were\nused, ignoring the intermediate ones.\nVariant Top-1 Top-5\ntw = 0 45.2 71.4\ntw = 1 62.5 87.8\ntw = 2 60.5 86.4\ntw = 2(Bo.) 60.4 86.2\nEffect of local window size:Table 1 shows the accuracy\nof our model by varying the local window size [−tw,tw]\nused in the proposed space-time mixing attention. Firstly,\nwe observe that the proposed model is signiﬁcantly superior\nto our baseline (tw = 0) which uses spatial-only attention.\nSecondly, a window of tw = 1produces the best results.\nThis shows that more gradual increase of the effective\nwindow size that is attended is more beneﬁcial compared\nto more aggressive ones, i.e. the case where tw = 2.\nA performance degradation for the case tw = 2 could\nbe attributed to boundary effects (handled by ﬁlling with\nzeros) which are aggravated as tw increases. Based on\nthese results, we chose to use tw = 1 for the models\nreported hereafter.\nEffect of SA position:We explored which layers should the proposed space-time mixing attention\noperation be applied to within the Transformer. Speciﬁcally, we explored the following variants:\n6\nTable 2: Effect of (a) proposed SA position, (b) temporal aggregation and number of Temporal\nAttention (TA) layers, (c) space-time mixing qkv vectors and (d) amount of mixed channels on SSv2.\n(a) Effect of applying the proposed SA to certain layers.\nTransform. layers Top-1 Top-5\n1st half 61.7 86.5\n2nd half 61.6 86.3\nHalf (odd. pos) 61.2 86.4\nAll 62.6 87.8\n(b) Effect of number of TA layers. 0 corresponds to\ntemporal averaging.\n#. TA layers Top-1 Top-5\n0 (temp. avg.) 62.4 87.8\n1 64.4 89.3\n2 64.5 89.3\n3 64.5 89.3\n(c) Effect of space-time mixing. x denotes the\ninput token before qkv projection. Query produces\nequivalent results with key and thus omitted.\nx key value Top-1 Top-5\n\u0017 \u0017 \u0017 56.6 83.5\n✓ \u0017 \u0017 63.1 88.8\n\u0017 ✓ \u0017 63.1 88.8\n\u0017 \u0017 ✓ 62.5 88.6\n\u0017 ✓ ✓ 64.4 89.3\n(d) Effect of amount of mixed channels. * uses\ntemp. avg. aggregation.\n0%* 0% 25% 50% 100%\n45.2 56.6 64.3 64.4 62.5\nTable 3: Comparison between TA and Summary token on SSv2 (left) and Kinetics-400 (right).\nSummary TA Top-1 Top-5\n\u0017 \u0017 62.4 87.8\n✓ \u0017 63.7 88.9\n✓ ✓ 63.4 88.9\n\u0017 ✓ 64.4 89.3\nSummary TA Top-1 Top-5\n\u0017 \u0017 77.8 93.7\n✓ \u0017 78.7 93.7\n✓ ✓ 78.0 93.2\n\u0017 ✓ 78.5 93.7\nApplying it to the ﬁrst L/2 layers, to the last L/2 layers, to every odd indexed layer and ﬁnally, to all\nlayers. As the results from Table 2a show, the exact layers within the network that self-attention is\napplied to do not matter; what matters is the number of layers it is applied to. We attribute this result\nto the increased temporal receptive ﬁeld and cross-frame interactions.\nEffect of temporal aggregation:Herein, we compare the two methods used for temporal aggrega-\ntion: simple temporal averaging [36] and the proposed Temporal Attention (TA) mechanism. Given\nthat our model already incorporates temporal information through the proposed space-time attention,\nwe also explored how many TA layers are needed. As shown in Table 2b replacing temporal averaging\nwith one TA layer improves the Top-1 accuracy from 62.5% to 64.4%. Increasing the number of\nlayers further yields no additional beneﬁts. We also report the accuracy of spatial-only attention plus\nTA aggregation. In the absence of the proposed space-time mixing attention, the TA layer alone is\nunable to compensate, scoring only 56.6% as shown in Table 2d. This highlights the need of having\nboth components in our ﬁnal model. For the next two ablation studies, we therefore used 1 TA layer.\nTable 5: Effect of number of tokens on\nSSv2.\nVariant Top-1 Top-5 FLOPs\n(×109)\nViT-B/32 60.5 87.4 95\nViT-L/32 61.8 88.3 327\nViT-B/16 64.4 89.3 425\nEffect of space-time mixingqkv vectors: Paramount to\nour work is the proposed space-time mixing attention of\nEq. 10 which is implemented by constructing ˜kl\ns′,−tw:tw\nand ˜vl\ns′,−tw:tw efﬁciently via channel indexing (see Eq. 9).\nSpace-time mixing though can be applied in several differ-\nent ways in the model. For completeness, herein, we study\nthe effect of space-time mixing to various combinations for\nthe key, value and to the input token prior toqkvprojection.\nAs shown in Table 2c, the combination corresponding to\nour model (i.e. space-time mixing applied to the key and value) signiﬁcantly outperforms all other\nvariants by up to 2%. This result is important as it conﬁrms that our model, derived from the pro-\nposed approximation to the local space-time attention, gives the best results when compared to other\nnon-well motivated variants.\nEffect of amount of space-time mixing:We deﬁne as ρdh the total number of channels imported\nfrom the adjacent frames in the local temporal window [−tw,tw] (i.e. ∑tw\nt′=−tw,t̸=0 dt′\nh = ρdh) when\n7\nTable 4: Comparison with state-of-the-art on the Kinetics-600 dataset. T×is the number of frames\nused by our method.\nMethod Top-1 Top-5 Views FLOPs ( ×109)\nAttentionNAS [38] 79.8 94.4 - 1,034\nLGD-3D R101 [27] 81.5 95.6 10 ×3 -\nSlowFast R101+NL [14] 81.8 95.1 10 ×3 3,480\nX3D-XL [13] 81.9 95.5 10 ×3 1,452\nTimeSformer-HR [3] 82.4 96.0 1 ×3 5,110\nViViT-L/16x2 [1] 82.5 95.6 4 ×3 17,352\nX-ViT (8×) (Ours) 82.5 95.4 1 ×3 425\nX-ViT (16×) (Ours) 84.5 96.3 1 ×3 850\nconstructing ˜kl\ns′,−tw:tw (see Section 3). Herein, we study the effect of ρon the model’s accuracy. As\nthe results from Table 2d show, the optimalρis between 25% and 50%. Increasing ρto 100% (i.e. all\nchannels are coming from adjacent frames) unsurprisingly degrades the performance as it excludes\nthe case t′= twhen performing the self-attention.\nEffect of Summary token:Herein, we compare Temporal Attention with Summary token on SSv2\nand Kinetics-400. We used both datasets for this case as they require different type of understanding:\nﬁne-grained temporal (SSv2) and spatial content (K400). From Table 3, we conclude that the\nSummary token compares favorable on Kinetics-400 but not on SSv2 showing that is more useful in\nterms of capturing spatial information. Since the improvement is small, we conclude that 1 TA layer\nis the best global attention-based mechanism for improving the accuracy of our method adding also\nnegligible computational cost.\nEffect of the number of input frames:Herein, we evaluate the impact of increasing the number of\ninput frames T from 8 to 16 and 32. We note that, for our method, this change results in a linear\nincrease in complexity. As the results from Table 7 show, increasing the number of frames from 8 to\n16 offers a 1.8% boost in Top-1 accuracy on SSv2. Moreover, increasing the number of frames to 32\nimproves the performance by a further 0.2%, offering diminishing returns. Similar behavior can be\nobserved on Kinetics and Epic-100 in Tables 6 and 8.\nEffect of number of tokens:Herein, we vary the number of input tokens by changing the patch\nsize K. As the results from Table 5 show, even when the number of tokens decreases signiﬁcantly\n(ViT-B/32) our approach is still able to produce models that achieve satisfactory accuracy. The beneﬁt\nof that is having a model which is signiﬁcantly more efﬁcient.\nEffect of the number of crops at test time.Throughout this work, at test time, we reported results\nusing 1 temporal and 3 spatial crops (i.e. 1 ×3). This is noticeable different from the current practice\nof using up to 10 ×3 crops [14, 1].\nTo showcase the behavior of our method, herein, we test the effect of increasing the number of\ncrops on Kinetics-400. As the results from Fig. 3 show, increasing the number of crops beyond two\ntemporal views (i.e. 2 ×3), yields no additional gains. Our ﬁndings align with the ones from the\nwork of Bertasius et al. [3] that observes the same properties for the transformer-based architectures.\nLatency and throughput considerations:While the channel shifting operation used by the proposed\nspace-time mixing attention is zero-FLOP, there is still a small cost associated with memory movement\noperations. In order to ascertain that the induced cost does not introduce noticeable performance\ndegradation, we benchmarked a Vit-B/16 (8×frames) model using spatial-only attention and the\nproposed one on 8 V100 GPUs and a batch size of 128. The spatial-only attention model has a\nthroughput of 312 frames/second while our model 304 frames/second.\n4.3 Comparison to state-of-the-art\nOur best model uses the proposed space-time mixing attention in all the Transformer layers and\nperforms temporal aggregation using a single lightweight temporal transformer layer as described\nin Section 3. Unless otherwise speciﬁed, we report the results using the 1 ×3 conﬁguration for the\nviews (1 temporal and 3 spatial) for all datasets.\n8\nFigure 3: Effect of the number of temporal crops at test time as measured on Kinetics 400 in terms of\nTop 1 accuracy. For each temporal crop, 3 spatial clips are sampled, for a total of tcrops ×3 clips.\nNotice that beyond tcrops = 2no additional accuracy gains are observed.\nTable 6: Comparison with state-of-the-art on the Kinetics-400. T×is the number of frames used by\nour method.\nMethod Top-1 Top-5 Views FLOPs ( ×109)\nbLVNet [12] 73.5 91.2 3 ×3 840\nSTM [17] 73.7 91.6 - -\nTEA [23] 76.1 92.5 10 ×3 2,100\nTSM R50 [24] 74.7 - 10 ×3 650\nI3D NL [39] 77.7 93.3 10 ×3 10,800\nCorrNet-101 [35] 79.2 - 10 ×3 6,700\nip-CSN-152 [33] 79.2 93.8 10 ×3 3,270\nLGD-3D R101 [27] 79.4 94.4 - -\nSlowFast 8×8 R101+NL [14] 78.7 93.5 10 ×3 3,480\nSlowFast 16×8 R101+NL [14] 79.8 93.9 10 ×3 7,020\nX3D-XXL [13] 80.4 94.6 10 ×3 5,823\nTimeSformer-L [3] 80.7 94.7 1 ×3 7,140\nViViT-L/16x2 [3] 80.6 94.7 4 ×3 17,352\nX-ViT (8×) (Ours) 78.5 93.7 1 ×3 425\nX-ViT (16×) (Ours) 80.2 94.7 1 ×3 850\nTable 7: Comparison with state-of-the-art on SSv2. T×is the number of frames used by our method.\n* - denotes models pretrained on Kinetics-600.\nMethod Top-1 Top-5 Views FLOPs ( ×109)\nTRN [45] 48.8 77.6 - -\nSlowFast+multigrid [41] 61.7 - 1 ×3 -\nTimeSformer-L [3] 62.4 - 1 ×3 7,140\nTSM R50 [24] 63.3 88.5 2 ×3 -\nSTM [17] 64.2 89.8 - -\nMSNet [22] 64.7 89.4 - -\nTEA [23] 65.1 - - -\nViViT-L/16x2 [3] 65.4 89.8 4 ×3 11,892\nX-ViT (8×) (Ours) 64.4 89.3 1 ×3 425\nX-ViT (16×) (Ours) 66.2 90.6 1 ×3 850\nX-ViT* (16×) (Ours) 67.2 90.8 1 ×3 850\nX-ViT (32×) (Ours) 66.4 90.7 1 ×3 1,270\n9\nOn Kinetics-400, we match the current state-of-the-art results while being signiﬁcantly faster than the\nnext two best recently/concurrently proposed methods that also use Transformer-based architectures:\n20×faster than ViVit [1] and 8×than TimeSformer-L [3]. Note that both models from [ 1, 3] and\nours were initialized from a ViT model pretrained on ImageNet-21k [ 9] and take as input frames\nat a resolution of 224 ×224px. Similarly, on Kinetics-600 we set a new state-of-the-art result. See\nTable 4.\nOn SSv2 we match and surpass the current state-of-the-art, especially in terms of Top-5 accuracy\n(ours: 90.8% vs ViViT: 89.8% [1]) using models that are 14×(16 frames) and 9×(32 frames) faster.\nFinally, we observe similar outcomes on Epic-100 where we set a new state-of-the-art, showing\nparticularly large improvements especially for “Verb” accuracy, while again being more efﬁcient.\n5 Conclusions\nTable 8: Comparison with state-of-the-art on Epic-\n100. T×is the #frames used by our method. Re-\nsults for other methods are taken from [1].\nMethod Action Verb Noun\nTSN [36] 33.2 60.2 46.0\nTRN [45] 35.3 65.9 45.4\nTBN [20] 36.7 66.0 47.2\nTSM [20] 38.3 67.9 49.0\nSlowFast [14] 38.5 65.6 50.0\nViViT-L/16x2 [1] 44.0 66.4 56.8\nX-ViT (8×) (Ours) 41.5 66.7 53.3\nX-ViT (16×) (Ours) 44.3 68.7 56.4\nWe presented a novel approximation to the full\nspace-time attention that is amenable to an ef-\nﬁcient implementation and applied it to video\nrecognition. Our approximation has the same\ncomputational cost as spatial-only attention yet\nthe resulting Video Transformer model was\nshown to be signiﬁcantly more efﬁcient than re-\ncently/concurrently proposed Video Transform-\ners [3, 1]. By no means this paper proposes\na complete solution to video recognition using\nVideo Transformers. Future efforts could in-\nclude combining our approaches with other ar-\nchitectures than the standard ViT, removing the\ndependency on pre-trained models and applying\nthe model to other video-related tasks like de-\ntection and segmentation. Finally, further research is required for deploying our models on low power\ndevices.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A\nvideo vision transformer. arXiv preprint arXiv:2103.15691, 2021.\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021.\n[4] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.\nIn proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308,\n2017.\n[5] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Mike Schuster, Zhifeng Chen, et al. The best of both worlds: Combining recent\nadvances in neural machine translation. arXiv preprint arXiv:1804.09849, 2018.\n[6] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. A2-nets: Double attention\nnetworks. arXiv preprint arXiv:1810.11579, 2018.\n[7] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos\nKazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision:\nThe epic-kitchens dataset. In ECCV, 2018.\n[8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma,\nDavide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision. arXiv\npreprint arXiv:2006.13256, 2020.\n10\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[12] Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More is less: Learning\nefﬁcient video representations by big-little network and depthwise temporal aggregation. arXiv preprint\narXiv:1912.00869, 2019.\n[13] Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203–213, 2020.\n[14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video\nrecognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n6202–6211, 2019.\n[15] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something\nsomething\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 5842–5850, 2017.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[17] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion\nencoding for action recognition. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 2000–2009, 2019.\n[18] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.\nLarge-scale video classiﬁcation with convolutional neural networks. InProceedings of the IEEE conference\non Computer Vision and Pattern Recognition, pages 1725–1732, 2014.\n[19] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv\npreprint arXiv:1705.06950, 2017.\n[20] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual\ntemporal binding for egocentric action recognition. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5492–5501, 2019.\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[22] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature\nlearning for video understanding. In European Conference on Computer Vision, pages 345–362. Springer,\n2020.\n[23] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and\naggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 909–918, 2020.\n[24] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video understanding. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083–7093, 2019.\n[25] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for\nvideo recognition. arXiv preprint arXiv:2005.06803, 2020.\n[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. arXiv preprint arXiv:1912.01703, 2019.\n[27] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representa-\ntion with local and global diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12056–12065, 2019.\n11\n[28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019.\n[29] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605, 2021.\n[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal\nfeatures with 3d convolutional networks. In Proceedings of the IEEE international conference on computer\nvision, pages 4489–4497, 2015.\n[32] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at\nspatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 6450–6459, 2018.\n[33] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classiﬁcation with channel-separated\nconvolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 5552–5561, 2019.\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[35] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation networks. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 352–361,\n2020.\n[36] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal\nsegment networks: Towards good practices for deep action recognition. In European conference on\ncomputer vision, pages 20–36. Springer, 2016.\n[37] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal\nsegment networks for action recognition in videos. IEEE transactions on pattern analysis and machine\nintelligence, 41(11):2740–2755, 2018.\n[38] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S Ryoo, Anelia Angelova,\nKris M Kitani, and Wei Hua. Attentionnas: Spatiotemporal attention cell search for video classiﬁcation. In\nEuropean Conference on Computer Vision, pages 449–465. Springer, 2020.\n[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[40] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir Gholaminejad,\nJoseph Gonzalez, and Kurt Keutzer. Shift: A zero ﬂop, zero parameter alternative to spatial convolutions.\nIn CVPR, 2018.\n[41] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Feichtenhofer, and Philipp Krahenbuhl. A multigrid\nmethod for efﬁciently training video models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 153–162, 2020.\n[42] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\n[43] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[44] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial\nnetworks. In International conference on machine learning, pages 7354–7363. PMLR, 2019.\n[45] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos.\nIn Proceedings of the European Conference on Computer Vision (ECCV), pages 803–818, 2018.\n12",
  "topic": "Mixing (physics)",
  "concepts": [
    {
      "name": "Mixing (physics)",
      "score": 0.6088070869445801
    },
    {
      "name": "Transformer",
      "score": 0.5800437927246094
    },
    {
      "name": "Spacetime",
      "score": 0.41969627141952515
    },
    {
      "name": "Computer science",
      "score": 0.3888011574745178
    },
    {
      "name": "Electrical engineering",
      "score": 0.21327954530715942
    },
    {
      "name": "Physics",
      "score": 0.18268495798110962
    },
    {
      "name": "Engineering",
      "score": 0.15422752499580383
    },
    {
      "name": "Voltage",
      "score": 0.0626274049282074
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}