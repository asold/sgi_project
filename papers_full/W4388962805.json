{
  "title": "BEND: Benchmarking DNA Language Models on biologically meaningful tasks",
  "url": "https://openalex.org/W4388962805",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Marin, Frederikke Isa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305114007",
      "name": "Teufel, Felix",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Horlacher, Marc",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221569499",
      "name": "Madsen, Dennis",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Pultz, Dennis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2562191932",
      "name": "Winther, Ole",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227104186",
      "name": "Boomsma, Wouter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2989608901",
    "https://openalex.org/W4308264370",
    "https://openalex.org/W2990435446",
    "https://openalex.org/W4293581915",
    "https://openalex.org/W2417483443",
    "https://openalex.org/W4288421316",
    "https://openalex.org/W4382603228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3209435229",
    "https://openalex.org/W4321593177",
    "https://openalex.org/W2147524392",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W3015796860",
    "https://openalex.org/W2198606573",
    "https://openalex.org/W4318269938",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W2059185913",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W2988126442",
    "https://openalex.org/W2953263404",
    "https://openalex.org/W2077318478",
    "https://openalex.org/W2991283206",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W4318763431",
    "https://openalex.org/W3198923619",
    "https://openalex.org/W4367597910",
    "https://openalex.org/W2479945688",
    "https://openalex.org/W2141572089",
    "https://openalex.org/W4285006295",
    "https://openalex.org/W2345512687",
    "https://openalex.org/W4316339774",
    "https://openalex.org/W4362660267",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W4281719345",
    "https://openalex.org/W3111061871",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2259938310",
    "https://openalex.org/W2096791516",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W2952935243",
    "https://openalex.org/W2906884244",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W2952239877",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3194982033",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W1019830208",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4367692216"
  ],
  "abstract": "The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.",
  "full_text": "Published as a conference paper at ICLR 2024\nBEND: B ENCHMARKING DNA L ANGUAGE MODELS\nON BIOLOGICALLY MEANINGFUL TASKS\nFrederikke I. Marin1,2∗, Felix Teufel3,4∗, Marc Horlacher5, Dennis Madsen3,\nDennis Pultz1, Ole Winther4,6, Wouter Boomsma2\n1Bioinformatics & Design, Enzyme Research, Novozymes A/S\n2Department of Computer Science, University of Copenhagen\n3Digital Science & Innovation, Novo Nordisk A/S\n4Department of Biology, University of Copenhagen\n5Computational Health Center, Helmholtz Center Munich\n6DTU Compute, Technical University of Denmark\nABSTRACT\nThe genome sequence contains the blueprint for governing cellular processes.\nWhile the availability of genomes has vastly increased over the last decades, ex-\nperimental annotation of the various functional, non-coding and regulatory ele-\nments encoded in the DNA sequence remains both expensive and challenging.\nThis has sparked interest in unsupervised language modeling of genomic DNA,\na paradigm that has seen great success for protein sequence data. Although\nvarious DNA language models have been proposed, evaluation tasks often dif-\nfer between individual works, and might not fully recapitulate the fundamen-\ntal challenges of genome annotation, including the length, scale and sparsity of\nthe data. In this study, we introduce BEND, a Benchmark for DNA language\nmodels, featuring a collection of realistic and biologically meaningful down-\nstream tasks defined on the human genome. We find that embeddings from cur-\nrent DNA LMs can approach performance of expert methods on some tasks, but\nonly capture limited information about long-range features. BEND is available at\nhttps://github.com/frederikkemarin/BEND.\n1 I NTRODUCTION\nWithin the last two decades, the cost of sequencing whole genomes has significantly decreased, hav-\ning led to an extraordinary wealth of genomic DNA sequences. This has improved our understand-\ning of genetic variation among human genomes and introduced genomes of hitherto understudied\nspecies. However, the generation of experimental data to annotate and understand these genomic\nsequences has not kept pace.\nAt the same time, Natural Language Processing (NLP) has demonstrated the power of large-scale\nmodels to capture signals in sequences by masking and reconstructing them in a self-supervised\nmanner. The success of masked language modeling (MLM) has extended to the biological domain\nRao et al. (2019); Bepler & Berger (2021); Madani et al. (2023); Rives et al. (2019), with protein\nlanguage models (pLMs) now being widely used for prediction tasks on protein sequences. The\navailability of unlabeled genomic sequences and, in many organisms, limited labeled data appear to\nmake language modeling a natural fit for DNA. DNA language models (LMs) have indeed started to\nemerge, but while the paradigms of NLP have been easy to transfer to proteins, the same may not be\ntrue for modeling genomes, as they present unique challenges: signals can have an extremely long\nlength range, high-signal regions are sparse, and even in those regions the density of signal is lower\ncompared to proteins.\nIn this paper, we present BEND, a Benchmark for DNA Language Models, a collection of realistic\nand biologically meaningful downstream tasks. BEND aims to provide a standardized set of tasks\nthat measure the ability of LMs to capture the intricacies of genomic data, and to help advance this\nnascent field. In summary, BEND contributes:\n∗Equal contribution\n1\narXiv:2311.12570v4  [q-bio.GN]  9 Apr 2024\nPublished as a conference paper at ICLR 2024\n• Seven curated tasks and datasets, probing understanding of different DNA functional elements\nover a variety of length scales.\n• Experiments covering DNA LMs from six different sources.To our knowledge, this represents\nfirst evaluation of all publicly available self-supervised DNA LMs suitable for the human genome\ntogether with appropriate baseline methods.\n• An adaptable benchmarking framework for preparing embeddings and training lightweight\nsupervised models.\n• Result: DNA LMs approach expert method performance on some tasks . However, no LM\nconsistently outperforms all others, and reasoning over very long contexts, as e.g. required for\nfinding enhancers, is still challenging.\n• Result: DNA LMs can learn distinct features in masked language modeling. Some LMs’\nembeddings primarily capture information about gene structure, while others focus on noncoding\nregions.\n2 B ACKGROUND\n2.1 E UKARYOTIC DNA ORGANIZATION AND TERMINOLOGY\nIn order to facilitate understanding how different prediction tasks relate to various aspects of the\ngenome, we briefly discuss the fundamental structure and function of eukaryotic genomic DNA\n(Figure 1). DNA is a linear polymer of four nucleotide bases, which are represented by the four\nletters A, C, G and T. It consists of two complementary strands that form a double helix by base\npairing the bases A, T, and C, G respectively.\nGenomic DNA is physically organized in a hierarchical manner. The DNA polymer is coiled around\nhistone proteins, which reduces its physical length and plays a role in regulation. A complex of 8\nhistone proteins together with coiled DNA is called a nucleosome. Nucleosomes further condense\nto form chromatin fibers, which occur in compact (closed) or loose (open) form. This controls\nthe accessibility of the DNA sequence to the transcriptional machinery, a process tightly regulated\nby chemical modifications of the histones (Bannister & Kouzarides, 2011). Chromatin can form\nloops, which allows regions distant in the sequence to be close in physical space. DNA appears in\nindependent modules called chromosomes, which are typically millions of base pairs (bp) in length.\nThe genome contains genes, segments that are transcribed to RNA molecules and potentially trans-\nlated to proteins. Protein-coding genes are structured as introns and exons. For expression, a gene is\nfirst transcribed to a pre-mRNA molecule, and introns are removed via splicing. This combines the\nexons to one contiguous sequence that encodes the protein. Flanking nucleotides in the RNA that do\nnot code for the protein are called untranslated regions (UTRs) and can have regulatory function. In\naddition, genes are associated with regulatory regions such as promoters, enhancers, silencers and\ninsulators that modulate their expression. Some elements, such as promoters, may lie in close prox-\nimity to the start of the gene, thetranscription start site(TSS). Others can appear multiple thousands\nbp away from the gene, but mediate their effect by physical proximity.\n2.2 L ANGUAGE MODELING FOR BIOMOLECULAR SEQUENCES : F ROM PROTEINS TO DNA\nOver the last years, language modeling has achieved breakthroughs in representation learning for\nprotein property and structure prediction, with transformer-based pLMs emerging as powerful foun-\ndation models, capable of learning long-range interactions fully unsupervised (Rives et al., 2019;\nElnaggar et al., 2022; Lin et al., 2023). The development of pLMs benefitted from the availability\nof standardized, representative benchmarks, such as TAPE (Rao et al., 2019) and PEER (Xu et al.,\n2022), as well as long-running protein machine learning tasks with an emphasis on fair benchmark-\ning to measure progress (Kryshtafovych et al., 2021; Zhou et al., 2019).\nWhile LMs have been extremely successful for modeling proteins, key differences between the two\ntypes of macromolecules hinder their widespread adoption for DNA. A typical protein consists of\n400-500 amino acids, which are represented as tokens from an alphabet of size 20. The analogy of\namino acid tokens with word tokens in NLP, as well as the fact that size of inputs to pLMs and NLP\nmodels are on the same order of magnitude, made methods developed for NLP directly transferable\nto protein data, with little to no methodological adaption required (Rao et al., 2020; Elnaggar et al.,\n2022). The alphabet of DNA is significantly smaller (4 tokens), while at the same time sequences,\n2\nPublished as a conference paper at ICLR 2024\nFigure 1: The organization of eukaryotic genomic DNA. Numbers are indicative examples for the\nhuman genome. Genes are structured as alternating introns (average: 5,400 bp) and exons (average:\n170 bp), and have a promoter regulatory element before their TSS. Enhancers can be thousands of bp\naway from the gene. DNA is wrapped around histone proteins and densely packed as a chromosome.\nsuch as those of genes, are considerably longer and have no naturally defined border, as e.g. the\nposition of the most distant relevant regulatory element is typically unknown. In contrast, protein\nsequences are naturally self-contained and, being the final gene product, have a significantly higher\ninformation density. Together, sparsity and long sequences pose unique challenges to DNA LMs.\n2.3 R ELATED WORKS\n2.3.1 DNA LANGUAGE MODELS\nThe first available DNA LM was DNABERT (Ji et al., 2021), a 12-layer BERT (Devlin et al., 2018)\nmodel trained on sequences of length 512 from the human genome. Sequences were tokenized\nas k-mers using a sliding window. DNABERT was evaluated by fine-tuning on tasks comprising\npromoter, transcription factor (TF) binding site and splice site (SS) prediction.\nA growing number of DNA LMs has been proposed since the release of DNABERT. These include\nthe Genomic Pretrained Network (GPN) (Benegas et al., 2023), FloraBERT (Levy et al., 2022), the\nNucleotide Transformer (NT) (Dalla-Torre et al., 2023), Species-aware LM (Gankin et al., 2023),\nGENA-LM (Fishman et al., 2023), DNABERT-2 (Zhou et al., 2023) and HyenaDNA (Poli et al.,\n2023). With the exception of HyenaDNA, models were trained using the MLM objective, but differ\nin their model architectures, tokenization strategies and training data.\nGPN uses dilated convolution layers rather than a transformer model. It showed strong performance\nfor zero-shot prediction of variant effects in the A. thalianagenome it was trained on. Qualitative\nresults showed that GPN captures information about gene structure and motifs of binding sites.\nNucleotide Transformer introduced the first large-scale transformer-based DNA LMs. All NT mod-\nels share the same architecture, but differ in their number of training genomes and model parameters.\nModels were trained on either the human reference genome, 3,202 different genetically diverse hu-\nman genomes or a selection of 850 genomes from a range of species. To increase the receptive field\nof the model, sequences were tokenized as 6-mers, allowing for processing sequences of up to 5,994\nbp in length. A second generation of multispecies models released later (NT-V2) extended the input\nlength to 12,282 bp. The NT models were evaluated on tasks comprising promoter, SS, histone\nmodification and enhancer prediction with a context length of up to 600 bp.\nGENA-LM (Fishman et al., 2023) proposed multiple medium-size LMs trained on human and multi-\nspecies genomes based on BERT and the BigBird (Zaheer et al., 2020) architecture for long se-\nquences. Byte-Pair Encoding (BPE) was used for tokenization to further increase the receptive field,\nenabling an input length of about 36,000 bp. Models were evaluated on tasks comprising promoter,\nSS, enhancer, chromatin profile and polyadenylation site prediction. While covering the same bio-\nlogical phenomena, tasks were defined differently than in NT. Similarly, DNABERT-2 (Zhou et al.,\n2023) replaced DNABERT’s k-mer tokenizer with BPE and pre-trained on multi-species genomes,\nwhile GROVER (Sanabria et al., 2023) adopted BPE for the human genome.\nPredating NT and GENA-LM, FloraBERT (Levy et al., 2022) proposed pre-training on 93 different\nplant genomes to enable transfer learning for predicting gene expression. However, FloraBERT was\n3\nPublished as a conference paper at ICLR 2024\ntrained exclusively on promoter-containing sequences. As this requires features to already be anno-\ntated in the genome, it can be considered a departure from the paradigm of fully self-supervised\nlearning. Similarly, Gankin et al. (2023) pre-trained on 3’ UTRs from 1,500 fungal genomes.\nSpecies information was made explicit by providing a species label with each sequence to the model.\nHyenaDNA (Nguyen et al., 2023) introduced a collection of autoregressive LMs, trained using the\nnext token prediction objective at single-nucleotide resolution on the human genome. The Hyena\nLM architecture (Poli et al., 2023) enabled scaling to input lengths of up to 1 million nucleotides.\nHyenaDNA models were evaluated by fine-tuning on NT’s supervised tasks and the Genomic Bench-\nmarks (Greˇsov´a et al., 2023) collection, outperforming NT on the majority of tasks.\nA number of DNA LMs were proposed without making trained models available. These comprise\nthe original BigBird (Zaheer et al., 2020), GeneBERT, which includes the prediction of ATAC-\nseq signals in the pre-training stage, MoDNA (An et al., 2022), with a motif prediction task as\nan additional objective, the BERT-based LOGO (Yang et al., 2021), and Revolution (Cheng et al.,\n2023), which adopts convolutions with circular padding.\n2.3.2 S UPERVISED LEARNING ON DNA\nDeveloping models on genomic DNA sequences for the prediction of properties and understand-\ning of transcriptional regulation has long been a central task of computational genomics research.\nThe availability of large-scale functional genomics data and advancements in deep learning tech-\nniques have brought progress in predicting various genomic features directly from DNA sequences.\nDeepBind (Alipanahi et al., 2015) and DeepSEA (Zhou & Troyanskaya, 2015) were two of the first\nmethods leveraging shallow CNNs for predicting TF binding and chromatin features, respectively.\nDeepCpG (Angermueller et al., 2017) predicts DNA methylation via a CNN/GRU architecture. Bas-\nset (Kelley et al., 2016) and ChromTransfer (Salvatore et al., 2023) model chromatin state in a cell\ntype specific manner by predicting the presence of DNase-I peaks. Using chromatin state as an aux-\niliary input, DeepChrome (Singh et al., 2016) predicts gene expression via multi-modal learning on\nDNA sequence and histone mark information.\nRecently, methods for predicting gene expression have leveraged information across thousands of\nfunctional genomic tracks by training in a large-scale, multi-task fashion. Basenji (Kelley et al.,\n2018) and Enformer (Avsec et al., 2021) demonstrated state-of-the-art performance for gene expres-\nsion prediction from DNA sequence alone, by integrating genomic information across up to 200\nkilobases and multi-task training across several genome-wide functional tasks, including DNase-I\nactivity and CAGE signal prediction. Similarly, Sei (Chen et al., 2022) models cis-regulatory TF\nbinding, chromatin accessibility and histone modification profiles across a large range of cell types.\n2.3.3 B ENCHMARK COLLECTIONS ON DNA\nGenomic Benchmarks (Gre ˇsov´a et al., 2023) features balanced classification tasks on DNA se-\nquences with median lengths ranging from 200 to 2,381 bp. The benchmark covers the classification\nof functional elements and the prediction of a sequence’s origin. The element classification tasks are\ndefined on human DNA, with one task covering D. melanogasteradditionally. For each task, only\nperformance of a baseline supervised neural network model was reported.\nDNABERT-2 introduced Genome Understanding Evaluation (GUE), a collection of classification\ntasks ranging from 70 to 1,000 bp. On the human genome, it includes classification of promoter, SS\nand TF binding sequences. It covers other species with a TF binding task on mouse, a histone mod-\nification task on yeast and a Covid variant classification task on viruses. DNABERT, DNABERT-2\nand NT were evaluated. No non-LM task-specific baselines are included in GUE.\nThe authors of NT provide a public leaderboard for their tasks, comprising promoter (hu-\nman/mouse), enhancer (human), SS (human/multispecies) and histone modification (yeast) pre-\ndiction with lengths ranging from 300 to 600 bp. NT is compared to DNABERT, DNABERT-2,\nHyenaDNA and Enformer. No task-specific baselines are included in the leaderboard.\n2.3.4 M OTIVATION OF BEND\nWhile existing DNA LMs have reported good performance on the tasks on which they were evalu-\nated, evaluation strategies to date have shown limited consistency across individual works, with GUE\nconstituting the most recent attempt at benchmarking on equal terms. Beyond comparability, it is\nimportant to ensure that benchmark tasks reflect the complexity and characteristics of real-world\ngenome analysis. In practice, genomes are vast, and functional regions are sparsely distributed\n4\nPublished as a conference paper at ICLR 2024\nTable 1: Overview of the tasks included in the benchmark. Nucleotide-wise tasks require the pre-\ndiction of a sequence of labels with the same length as the input. In sequence-wise tasks the whole\ninput sequence is to be classified. In binned tasks, multiple nucleotides share a label.\nTask Type\n(# labels) # Samples Length rangeEvaluation\n(# train/val/test)Metric Source\nGene finding Nucleotide-wiseMulticlass (9) 5,976 1,433 -14,000 bp4780/597/597MCC GENCODE(Frankish et al., 2021)\nEnhancer annotationBinned (128bp)Binary 285 100,096 bp 10-fold CV AUPRCFulco et al. (2019), Gasperini et al. (2019),Enformer (Avsec et al., 2021)\nChromatin accessibilitySequence-wiseMultilabel (125)2,005,617 512 bp 1,354,042/\n279,422/372,153AUROC ENCODE Project Consortium (2012)\nHistone modificationSequence-wiseMultilabel (18)612,081 512 bp 420,713/\n70,801/120,567AUROC ENCODE Project Consortium (2012)\nCpG methylationSequence-wiseMultilabel (7)959,039 512 bp 743,095/\n109,717/106,227AUROC ENCODE Project Consortium (2012)\nNoncoding varianteffects (expression)Sequence-wiseBinary 105,263 512 bp zero-shot AUROC DeepSEA(Zhou & Troyanskaya, 2015)Noncoding varianteffects (disease)Sequence-wiseBinary 295,495 512 bp zero-shot AUROC ClinVar(Landrum et al., 2020)\nthroughout the genome. While there are tasks on DNA that are inherently local, such as classifying\nfunctional regions (e.g. classifying TF binding sites), it needs to be recognized that such tasks do\nnot allow us to evaluate a model’s understanding of the genome over longer ranges.\nTherefore, focusing solely on tasks on short sequences, such as distinguishing promoter from non-\npromoter sequences, falls short of evaluating the extent to which a model’s representations capture\ncomplex features of genomic organization, preventing us from measuring benefits of modeling the\ngenome with larger context windows. For instance, reporting performance on predicting SSs, which\ncan be done on short sequences, does not allow us to evaluate how useful a model would be for gene\nfinding over longer ranges, a common task in genome annotation.\nTo provide a more comprehensive assessment, BEND proposes genomic tasks that rely less on prior\nknowledge of feature positions and require reasoning over potentially long contexts. The tasks\ncover a range of length scales, selected to be both biologically relevant and to cover a variety of\nDNA properties. The tasks explore representations at different resolutions, requiring modelling of\nDNA at single bp resolution as well as over longer stretches (Table 1). We establish our benchmark\non the human genome, as it offers ample experimental data for the derivation of tasks, has a complex\norganization, and was the focus of most published DNA LMs.\n3 T ASKS AND DATASETS\nWe introduce the collection of tasks included in BEND. For each task, we additionally provide a\ndatasheet (Gebru et al., 2018) in section A.1. All tasks are provided in bed format, listing the\ngenome coordinates of samples (A.2). This makes it convenient to include more flanking context\nwithout reprocessing the data, should future works find it useful to take more bp into account.\n3.1 G ENE FINDING\nDefinition Gene finding is a multiclass problem where each nucleotide is either part of an exon\n(EF/R), intron (IF/R), a donor (DF/R) or acceptor (AF/R) splice site or a noncoding region (NC ).\nThe F/R subscript denotes whether the gene is located on the forward or reverse strand.\nBiological relevance Annotating genes and identifying coding sequences is a key step in genome\nannotation and protein discovery. It requires a model to use local context to identify correct reading\nframes and codon structure, while using longer range signals to propagate the location of SS to dis-\ntant bp between SS, and correctly annotate them as lying in introns or exons. Introns vary in length\nfrom a few hundred to several thousand bp, requiring an LM to understand long-range dependencies.\nData GENCODE (Frankish et al., 2021) gene annotations were processed to construct sequences\nof nucleotide labels y ∈ {EF , DF , IF , AF , ER, DR, IR, AR, NC} for each gene. Detailed pro-\ncessing is laid out in A.1.1. Samples were partitioned at 80% identity following AUGUSTUS’\nrecommendations (Stanke & Waack, 2003). It should be noted that there is a large label imbalance\nas there is only one donor and acceptor site per intron segment.\nMetric We compute the multi-class Matthews correlation coefficient (MCC) (Gorodkin, 2004)\nover all bp. The MCC is used as it is robust to the inherently highly uneven label ratios of this task.\n5\nPublished as a conference paper at ICLR 2024\n3.2 E NHANCER ANNOTATION\nDefinition Enhancer annotation is the problem of finding enhancer regions for a given gene. We\ndefine enhancer annotation as a binary classification task. Given a sequence of gene-adjacent ge-\nnomic DNA that contains enhancers, a binary label indicating whether it is part of an enhancer needs\nto be predicted for each segment of 128bp.\nBiological relevance Enhancers are short, noncoding segments that contribute to regulating gene\nexpression. They can be located anywhere from a few thousand to a million bp away from their\ntarget gene and work by being brought into physical proximity to the gene’s promoter. Their anno-\ntation is a highly challenging task that requires detection of long-range interactions.\nData Experimentally validated enhancer-gene pairs were taken from CRISPR interference experi-\nments (Fulco et al. (2019); Gasperini et al. (2019) and paired with the main TSS of each gene from\nAvsec et al. (2021). We extracted a sequence of 100,096 bp centered on the TSS for each gene.\nEach 128bp were annotated with a binary label y ∈ {0, 1} indicating whether the bin is part of an\nenhancer, yielding a label sequence of length 782. Detailed processing is laid out in A.1.2. Samples\nwere partitioned based on chromosomes.\nMetric The AUPRC is computed over all labels. As the number of samples is too limited for\nmeasuring performance robustly on a single test split, we perform 10-fold cross-validation in order\nto evaluate performance over all samples.\n3.3 C HROMATIN ACCESSIBILITY PREDICTION\nDefinition Chromatin accessibility prediction is a multilabel task where sequences are classified\nas being in open or closed chromatin across a range of cell types.\nBiological relevance Dynamically modulating chromatin accessibility is a key mechanism for the\ncell type specific regulation of gene expression, as binding of the transcription machinery is highly\ndependent on the accessibility of DNA elements, including promoters, enhancers and TSS.\nData DNase I hypersensitive sites were obtained from ENCODE (ENCODE Project Consortium,\n2012) for 125 cell types. Following the preprocessing of Kelley et al. (2016), segments of length 512\nbp were labeled with binary vectors y ∈ {0, 1}125, with yi = 1if the chromatin is open for the i’th\ncell type. Detailed processing is laid out in A.1.3. Samples were partitioned based on chromosomes.\nMetric The AUROC is computed for each label and averaged.\n3.4 H ISTONE MODIFICATION PREDICTION\nDefinition Histone modification prediction is a multilabel task, where the histones which are part\nof the nucleosomes of a given DNA sequence are labeled with one or more histone marks.\nBiological relevance Histone proteins are key to the organisation of DNA into chromatin. Mod-\nifications of histones modulate chromatin structure and thus contribute to regulating chromatin ac-\ncessibility and gene expression. Histone modification prediction requires modeling local binding of\nTFs as well as long-range regulation, such as by distant enhancers.\nData Histone ChIP-seq data for 11 histone marks and 18 replicates in the K562 cell line was\nobtained from ENCODE. Detailed processing is laid out in A.1.4 and follows the methodology of\n3.3. Each sample is a sequence of length 512 bp with a label vectory ∈ {0, 1}18, such that yi = 1if\na histone bound to this sequence carries mark i. Samples were partitioned based on chromosomes.\nMetric The AUROC is computed for each label and averaged.\n3.5 C PG METHYLATION PREDICTION\nDefinition CpG methylation prediction is a multilabel classification task, where a given CpG site\nis either methylated or unmethylated in different cell lines.\nBiological relevance Methylation of cytosine nucleotides in CpG sites is a prominent form of\nepigenetic modification and plays a key role in the repression of gene expression.\nData Bisulfite sequencing data for 7 human cell lines was obtained from ENCODE. Detailed\nprocessing is laid out in A.1.5. Each sample is a sequence of length 512 bp centered on the CpG site\nwith a label vector y ∈ {0, 1}7, such that yi = 1 if the C is methylated. Samples were partitioned\nbased on chromosomes.\nMetric The AUROC is computed for each label and averaged.\n3.6 N ONCODING VARIANT EFFECTS (EXPRESSION AND DISEASE )\nDefinition Predicting variant effects is a binary problem, where single-bp mutations are classified\nas either having an effect or not. We treat classification as a zero-shot task, using the cosine distance\n6\nPublished as a conference paper at ICLR 2024\nTable 2: Overview of the LMs applicable to the human genome included in the benchmark.\nModel Seq length Trained on Architecture Source\nA WD-LSTM Infinitea Multispecies RNN This work\nDilated ResNet 10,000 Human Refd CNN This work\nNucleotide Transformer 5,994 Human Refd BERT Dalla-Torre et al. (2023)\nNucleotide Transformer 5,994 Multispecies BERT Dalla-Torre et al. (2023)\nNucleotide Transformer 5,994 1000 Genomes projectd BERT Dalla-Torre et al. (2023)\nNucleotide Transformer V212,282 Multispecies BERT Dalla-Torre et al. (2023)\nDNABERT 512 Human Refd BERT Ji et al. (2021)\nDNABERT-2 Infiniteb Multispecies BERT Zhou et al. (2023)\nGENA-LM 4,500 1000 Genomes projecte BERT Fishman et al. (2023)\nGENA-LM 36,000 1000 Genomes projecte BigBird Fishman et al. (2023)\nHyenaDNA 1,000,000 Human Refd Hyena Nguyen et al. (2023)\nHyenaDNA 1,000 Human Refd Hyena Nguyen et al. (2023)\nGROVER 8,160c Human Refd BERT Sanabria et al. (2023)\na As the LSTM compresses all preceding tokens into a single hidden state, it can technically process infinite sequences, even\nthough it was trained at finite lengths and might not have learnt to exploit such long contexts.\nb DNABERT-2 uses ALiBi (Press et al., 2022) to encode position, which can technically scale to any sequence length. In\npractice, the model was trained on finite lengths and the authors recommend embedding sequences below 10,000 bp.\nc No explicit length was reported in bp. The indicated number was derived by considering 510 BPE tokens of size 16.\nd Schneider et al. (2017),e McVean et al. (2012)\nin embedding space between a variant nucleotide and its reference nucleotide as the prediction score.\nBiological relevance Single-bp variants in noncoding regions can have functional consequences\nby altering gene expression levels or causing disease. This task probes the LM’s understanding of\nlocal context and potentially the structure of regulatory motifs. We focus on noncoding regions, as\ncoding variant effects can be predicted with high accuracy by modeling the mutation in the resulting\nprotein sequence (Frazer et al., 2021).\nData For expression variants, we adapt the DeepSEA dataset (Zhou & Troyanskaya, 2015). For\ndisease-associated variants, we process ClinVar (Landrum et al., 2020). We apply Ensembl VEP\n(McLaren et al., 2016) to categorize variants by genomic regions into consequence types. De-\ntailed processing is laid out in A.1.6 and A.1.7. Each variant is a genomic position with a mutation\nx ∈ {A, C, G, T} and a label y ∈ {0, 1}. The adjacent 512 bp serve as embedding context.\nMetric We compute the AUROC. Additionally, we report separate AUROCs for the variant con-\nsequence types to gain further insight into what genomic features are driving performance.\n4 M ODELING\nLanguage Models We benchmark available LMs suitable for the human genome (Table 2).\nCheckpoint selection criteria are laid out in A.6.2. Additionally, we train two simple baseline DNA\nLMs: An AWD-LSTM (Merity et al., 2017) model trained on three species, and a dilated CNN\nsimilar to GPN (Benegas et al., 2023), trained on the human genome. The model differs from GPN\nin the parameter count and the length of training sequences (A.6.1).\nDownstream model We train a lightweight supervised two-layer CNN model with 64 channels\non top of the LM embeddings for each task. LM weights are kept frozen and are not fine-tuned.\nFor LMs with reduced output length due to tokenization, embeddings are upsampled to the original\nsequence length (A.6.3). For sequence-level tasks, we apply average pooling after the last CNN\nlayer. For the enhancer annotation task, the number of channels was reduced to prevent overfitting.\nNo downstream model is trained for variant effect prediction, as the cosine distance of the LM\nembeddings directly serves as the zero-shot predictor.\nSupervised baselines For each task, we train two supervised models without pre-training. For\na direct comparison of raw and embedded DNA, we train the two-layer CNN on one-hot encoded\nsequences. For chromatin accessibility, histone modificaton and CpG methylation prediction, we\ntrain the Basset model (Kelley et al., 2016), which was specifically designed for modeling genome-\nwide functional genomics data. For gene finding and enhancer annotation, we train the ResNet\nCNN model on one-hot encoded sequences. For variant effect prediction, no supervised models\nare trained. For all tasks where Basset is not applicable, we report the performance of a previously\npublished task-specific expert method on the benchmark dataset to put LM performance into context.\n7\nPublished as a conference paper at ICLR 2024\nTable 3: Results on all tasks. The best performing DNA LM for each task is highlighted in bold.\nGenefinding EnhancerannotationChromatinaccessibilityHistonemodificationCpGMethylationVariant effects(expression)Variant effects(disease)\nExpert method 0.80AUGUSTUS\n0.07ENFORMER\n0.85BASSET\n0.74BASSET\n0.93BASSET\n0.70DEEPSEA 0.56DEEPSEA\nFully supervisedResNet 0.46 0.06 - - - - -CNN 0.00 0.03 0.75 0.76 0.84 - -\nPre-trained\nResNet-LM 0.36 0.02 0.82 0.77 0.87 0.55 0.55A WD-LSTM 0.05 0.03 0.69 0.74 0.81 0.53 0.45NT-H 0.41 0.05 0.74 0.76 0.88 0.55 0.48NT-MS 0.68 0.06 0.79 0.78 0.92 0.54 0.77NT-1000G 0.49 0.04 0.77 0.77 0.89 0.45 0.49NT-V2 0.64 0.05 0.80 0.76 0.91 0.48 0.48DNABERT 0.20 0.03 0.85 0.79 0.91 0.60 0.56DNABERT-2 0.43 0.03 0.81 0.78 0.90 0.49 0.51GENA-LM BERT0.52 0.03 0.76 0.78 0.91 0.49 0.55GENA-LM BigBird0.39 0.04 0.82 0.78 0.91 0.49 0.52HyenaDNA large0.35 0.03 0.84 0.76 0.91 0.51 0.45HyenaDNA tiny0.10 0.02 0.78 0.76 0.86 0.47 0.44GROVER 0.28 0.03 0.82 0.77 0.89 0.56 0.51\n5 R ESULTS\nGene finding DNA LMs show promising performance for gene finding (Table 3). The two-layer\nCNN baseline fails to learn, possibly due to its inherent limitation to local context. However, the\nsame CNN is able to achieve varying levels of performance when using LM embeddings, suggesting\nthat embeddings capture some long-range information. NT-MS and NT-V2 outperform all other\nmodels by a wide margin, but still do not approach the highly specialized AUGUSTUS (Stanke &\nWaack, 2003) gene finding model. This highlights that while more specialized downstream models\nare still needed to accurately predict gene structure, using pre-trained DNA LM embeddings presents\na promising avenue to attain good performance. Computing individual performance metrics across\nall classes (Table A8) reveals that although there is high variance in the performance across all\nclasses, some embeddings capture splice sites fairly considering their low frequency. HyenaDNA-\nlarge, although being the only LM whose context length fully covers the input length of the task,\nonly shows modest performance.\nEnhancer annotation All investigated models perform poorly on this task. Enhancer annotation\nis an extremely difficult task due to the length scale, sparsity of the signal, and small dataset, which\npose challenges for all investigated models. Although the supervised baseline has a large enough\nreceptive field to detect the long-range interaction, the size of the dataset is prohibitive for perfor-\nmance. The performance of Enformer (Avsec et al., 2021) (A.7) is comparable on this task, but it\nmust be noted that this is an unsupervised method that was not trained directly on enhancer data.\nRather, it infers their locations from learning to predict other genome annotations. Predicting gene-\nspecific enhancers from sequence alone without considering supporting experimental data as input\ntherefore remains a highly difficult problem. While this task already proves to be highly challeng-\ning for current models at the given length scales, we note that biology is even more complex, with\nenhancers potentially being millions of bp away.\nChromatin accessibility DNABERT shows the highest performance, on par with the specialized\nBasset model (0.85). All other LMs perform worse on this task, offering no advantage over Basset.\nHistone modification NT-MS and DNABERT show the highest performance (0.74), outperform-\ning Basset (0.72). This suggests that LM embeddings can improve performance for histone modifi-\ncation prediction, albeit at marginal levels.\nCpG methylation NT-MS performs best (0.92) on all included cell lines (Table A11), but is out-\nperformed by Basset (0.93). DNABERT, GENA-LM and HyenaDNA-large also perform competi-\ntively, indicating that embeddings capture information about CpG island methylation patterns.\nVariant effect prediction DNA LMs show some signal for unsupervised prediction of noncoding\nvariant effects. As the two datasets focus on different genomic regions, we only see limited con-\nsistency between the expression and disease variant tasks, with DNABERT and NT-MS performing\nbest respectively. While being worse than the supervised DeepSEA method, DNABERT matches\nDeepSEA’s unsupervised performance on the expression dataset (AUROC 0.6, A.7). On the disease\n8\nPublished as a conference paper at ICLR 2024\ndataset, multiple LMs approach DeepSEA’s Disease Impact Score, with NT-MS outperforming it.\nWhen dissecting performance by variant types, we find that the performance of NT-MS is driven\nby variants affecting splice sites and introns (Table A13). While splice sites can be considered\nnoncoding DNA, they are not the focus of DeepSEA, which models chromatin features, and shows\nstronger performance in UTRs and up- or downstream regions. Similar to the results on the expres-\nsion dataset, we find that DNABERT outperforms NT-MS in such regions, suggesting that the two\nLMs learned distinct sequence features during pre-training. As all other NT models show weaker\nperformance on variants affecting gene structure, this could be a consequence of the model’s large\nsize and multi-species pre-training. However, we do not see similarly strong performance in the\nmulti-species DNABERT-2 and NT-V2.\n6 D ISCUSSION\nWe find that currently available DNA LMs already show promising performance on some tasks over\nfully supervised baselines, but do not offer consistent improvements over all included tasks and can\nfall short of surpassing specialized existing prediction methods. Overall, we find that NT-MS is\na strong default LM, but is in some tasks inferior to the much smaller DNABERT. Interestingly,\nwhile both models trained using the MLM objective, we find that they learned distinct genomic\nfeatures during pre-training. With the pre-training data and the tokenization strategy being the key\narchitectural difference, these choices may deserve more attention in future DNA LMs.\nFor modeling functional genomics data, DNA LMs only show limited utility. In direct comparison\nto the Basset model trained on the same data, LM embeddings fail to yield consistent improvements\nin performance when only using a two-layer CNN.\nOn the gene finding task, we observe that NT-MS with a simple two-layer CNN shows promising\nperformance compared to the specialized AUGUSTUS, which was found to be the state of the art in\na recent benchmark (Scalzitti et al., 2020). This suggests that future more sophisticated LM-based\ngene finders might become a method of choice for this problem. The result also indicates that current\nDNA LMs are capable of modeling long-range dependencies to some extent.\nProbing LMs at even longer ranges in the enhancer annotation task reveals that long-range under-\nstanding still needs improvement for sparse problems with limited data. This highlights a key issue\nfacing DNA LMs: Not only is there a need for long-range modeling to improve our understanding\nof the genome, as demonstrated by Avsec et al. (2021), but it also raises a fundamental question\nas to whether current LM training objectives will lead to the incorporation of such distant, sparse\nsignals, or whether the local sequence context is all that is required for sequence reconstruction and\nsome level of supervision is needed. Since BEND is not inherently tied to an LM objective, our\nstandardized benchmark may also prove useful for evaluating eventual DNA representation models\nthat follow a different paradigm.\n7 L IMITATIONS AND OUTLOOK\nAs the curation of a comprehensive benchmark task collection requires experimental ground-truth\ndata to be available, and most published models are trained on human data, we focused BEND on\nthe human genome. BEND aims at comparing the effectiveness of different model architectures\nand training strategies for learning representations from genomic data, under the assumption that\nother, similarly structured genomes should behave comparably under self-supervision. However, an\nimportant question that remains unanswered is whether DNA LMs can aid with generalization across\ndifferent organisms. In the future, we hope to extend the benchmark to other, diverse organisms, so\nthat generalization power can be tested in a transfer-learning setting, i.e. by training a task on a\ngiven organism, and evaluating performance on another.\nIn BEND, we benchmarked to what extent embeddings capture features that can be leveraged by\ndownstream models for prediction. This approach is fully agnostic regarding the underlying LM’s\nmethodology and scales to models of any size. Other works proposed to fine-tune LMs on tasks\ndirectly. While this potentially conflates a representation’s content with the inductive bias of a model\narchitecture for a given task, fine-tuning may yield performance gains beyond the results observed\nin this work (Nguyen et al., 2023; Zhou et al., 2023). Another aspect to be investigated in the future\nis to dive deeper into how LMs learn features during pre-training, as done previously for protein\nLMs (Vig et al., 2021).\n9\nPublished as a conference paper at ICLR 2024\n8 A CKNOWLEDGEMENTS AND DISCLOSURE OF FUNDING\nThis work was funded in part by Innovation Fund Denmark (0153-00197B), the Novo Nordisk\nFoundation through the MLLS Center (Basic Machine Learning Research in Life Science,\nNNF20OC0062606), the Pioneer Centre for AI (DRNF grant number P1), and the Danish Data\nScience Academy, which is funded by the Novo Nordisk Foundation (NNF21SA0069429) and VIL-\nLUM FONDEN (40516).\nThis work was supported by the Helmholtz Association under the joint research school ”Munich\nSchool for Data Science (MUDS)”.\nWe would like to acknowledge and thank Ziga Avsec, David Kelley, as well as the rest of the authors\nbehind the Enformer model Avsec et al. (2021), for providing the set of transcription start sites used\nin the enhancer annotation task.\nREFERENCES\nBabak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the se-\nquence specificities of DNA- and RNA-binding proteins by deep learning. Nature Biotechnol-\nogy, 33(8):831–838, August 2015. ISSN 1087-0156, 1546-1696. doi: 10.1038/nbt.3300. URL\nhttps://www.nature.com/articles/nbt.3300.\nWeizhi An, Yuzhi Guo, Yatao Bian, Hehuan Ma, Jinyu Yang, Chunyuan Li, and Junzhou Huang.\nMoDNA: motif-oriented pre-training for DNA language model. In Proceedings of the 13th ACM\nInternational Conference on Bioinformatics, Computational Biology and Health Informatics, pp.\n1–5, Northbrook Illinois, August 2022. ACM. ISBN 978-1-4503-9386-7. doi: 10.1145/3535508.\n3545512. URL https://dl.acm.org/doi/10.1145/3535508.3545512.\nChristof Angermueller, Heather J. Lee, Wolf Reik, and Oliver Stegle. DeepCpG: accurate prediction\nof single-cell DNA methylation states using deep learning.Genome Biology, 18(1):67, December\n2017. ISSN 1474-760X. doi: 10.1186/s13059-017-1189-z. URL http://genomebiology.\nbiomedcentral.com/articles/10.1186/s13059-017-1189-z .\nAdam Auton, Gonc ¸alo R. Abecasis, David M. Altshuler, Richard M. Durbin, Gonc ¸alo R. Abeca-\nsis, David R. Bentley, Aravinda Chakravarti, Andrew G. Clark, Peter Donnelly, Evan E. Eichler,\nPaul Flicek, Stacey B. Gabriel, Richard A. Gibbs, Eric D. Green, Matthew E. Hurles, Bartha M.\nKnoppers, Jan O. Korbel, Eric S. Lander, Charles Lee, Hans Lehrach, Elaine R. Mardis, Ga-\nbor T. Marth, Gil A. McVean, Deborah A. Nickerson, Jeanette P. Schmidt, Stephen T. Sherry, Jun\nWang, Richard K. Wilson, Richard A. Gibbs, Eric Boerwinkle, Harsha Doddapaneni, Yi Han,\nViktoriya Korchina, Christie Kovar, Sandra Lee, Donna Muzny, Jeffrey G. Reid, Yiming Zhu, Jun\nWang, Yuqi Chang, Qiang Feng, Xiaodong Fang, Xiaosen Guo, Min Jian, Hui Jiang, Xin Jin,\nTianming Lan, Guoqing Li, Jingxiang Li, Yingrui Li, Shengmao Liu, Xiao Liu, Yao Lu, Xuedi\nMa, Meifang Tang, Bo Wang, Guangbiao Wang, Honglong Wu, Renhua Wu, Xun Xu, Ye Yin,\nDandan Zhang, Wenwei Zhang, Jiao Zhao, Meiru Zhao, Xiaole Zheng, Eric S. Lander, David M.\nAltshuler, Stacey B. Gabriel, Namrata Gupta, Neda Gharani, Lorraine H. Toji, Norman P. Gerry,\nAlissa M. Resch, Paul Flicek, Jonathan Barker, Laura Clarke, Laurent Gil, Sarah E. Hunt, Gavin\nKelman, Eugene Kulesha, Rasko Leinonen, William M. McLaren, Rajesh Radhakrishnan, Asier\nRoa, Dmitriy Smirnov, Richard E. Smith, Ian Streeter, Anja Thormann, Iliana Toneva, Brendan\nVaughan, Xiangqun Zheng-Bradley, David R. Bentley, Russell Grocock, Sean Humphray, Terena\nJames, Zoya Kingsbury, Hans Lehrach, Ralf Sudbrak, Marcus W. Albrecht, Vyacheslav S. Am-\nstislavskiy, Tatiana A. Borodina, Matthias Lienhard, Florian Mertes, Marc Sultan, Bernd Timmer-\nmann, Marie-Laure Yaspo, Elaine R. Mardis, Richard K. Wilson, Lucinda Fulton, Robert Fulton,\nStephen T. Sherry, Victor Ananiev, Zinaida Belaia, Dimitriy Beloslyudtsev, Nathan Bouk, Chao\nChen, Deanna Church, Robert Cohen, Charles Cook, John Garner, Timothy Hefferon, Mikhail\nKimelman, Chunlei Liu, John Lopez, Peter Meric, Chris O’Sullivan, Yuri Ostapchuk, Lon Phan,\nSergiy Ponomarov, Valerie Schneider, Eugene Shekhtman, Karl Sirotkin, Douglas Slotta, Hua\nZhang, Gil A. McVean, Richard M. Durbin, Senduran Balasubramaniam, John Burton, Petr\nDanecek, Thomas M. Keane, Anja Kolb-Kokocinski, Shane McCarthy, James Stalker, Michael\nQuail, Jeanette P. Schmidt, Christopher J. Davies, Jeremy Gollub, Teresa Webster, Brant Wong,\nYiping Zhan, Adam Auton, Christopher L. Campbell, Yu Kong, Anthony Marcketta, Richard A.\nGibbs, Fuli Yu, Lilian Antunes, Matthew Bainbridge, Donna Muzny, Aniko Sabo, Zhuoyi Huang,\n10\nPublished as a conference paper at ICLR 2024\nJun Wang, Lachlan J. M. Coin, Lin Fang, Xiaosen Guo, Xin Jin, Guoqing Li, Qibin Li, Yingrui\nLi, Zhenyu Li, Haoxiang Lin, Binghang Liu, Ruibang Luo, Haojing Shao, Yinlong Xie, Chen Ye,\nChang Yu, Fan Zhang, Hancheng Zheng, Hongmei Zhu, Can Alkan, Elif Dal, Fatma Kahveci, Ga-\nbor T. Marth, Erik P. Garrison, Deniz Kural, Wan-Ping Lee, Wen Fung Leong, Michael Stromberg,\nAlistair N. Ward, Jiantao Wu, Mengyao Zhang, Mark J. Daly, Mark A. DePristo, Robert E. Hand-\nsaker, David M. Altshuler, Eric Banks, Gaurav Bhatia, Guillermo del Angel, Stacey B. Gabriel,\nGiulio Genovese, Namrata Gupta, Heng Li, Seva Kashin, Eric S. Lander, Steven A. McCarroll,\nJames C. Nemesh, Ryan E. Poplin, Seungtai C. Yoon, Jayon Lihm, Vladimir Makarov, Andrew G.\nClark, Srikanth Gottipati, Alon Keinan, Juan L. Rodriguez-Flores, Jan O. Korbel, Tobias Rausch,\nMarkus H. Fritz, Adrian M. St ¨utz, Paul Flicek, Kathryn Beal, Laura Clarke, Avik Datta, Javier\nHerrero, William M. McLaren, Graham R. S. Ritchie, Richard E. Smith, Daniel Zerbino, Xi-\nangqun Zheng-Bradley, Pardis C. Sabeti, Ilya Shlyakhter, Stephen F. Schaffner, Joseph Vitti,\nDavid N. Cooper, Edward V . Ball, Peter D. Stenson, David R. Bentley, Bret Barnes, Markus\nBauer, R. Keira Cheetham, Anthony Cox, Michael Eberle, Sean Humphray, Scott Kahn, Lisa\nMurray, John Peden, Richard Shaw, Eimear E. Kenny, Mark A. Batzer, Miriam K. Konkel, Jeri-\nlyn A. Walker, Daniel G. MacArthur, Monkol Lek, Ralf Sudbrak, Vyacheslav S. Amstislavskiy,\nRalf Herwig, Elaine R. Mardis, Li Ding, Daniel C. Koboldt, David Larson, Kai Ye, Simon Gravel,\nThe 1000 Genomes Project Consortium, Corresponding authors, Steering committee, Produc-\ntion group, Baylor College of Medicine, BGI-Shenzhen, Broad Institute of MIT and Harvard,\nCoriell Institute for Medical Research, European Bioinformatics Institute European Molecular\nBiology Laboratory, Illumina, Max Planck Institute for Molecular Genetics, McDonnell Genome\nInstitute at Washington University, US National Institutes of Health, University of Oxford, Well-\ncome Trust Sanger Institute, Analysis group, Affymetrix, Albert Einstein College of Medicine,\nBilkent University, Boston College, Cold Spring Harbor Laboratory, Cornell University, Euro-\npean Molecular Biology Laboratory, Harvard University, Human Gene Mutation Database, Icahn\nSchool of Medicine at Mount Sinai, Louisiana State University, Massachusetts General Hospital,\nMcGill University, and NIH National Eye Institute. A global reference for human genetic vari-\nation. Nature, 526(7571):68–74, October 2015. ISSN 1476-4687. doi: 10.1038/nature15393.\nURL https://www.nature.com/articles/nature15393. Number: 7571 Publisher:\nNature Publishing Group.\nˇZiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska,\nKyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R. Kelley. Effective\ngene expression prediction from sequence by integrating long-range interactions. Nature Meth-\nods, 18(10):1196–1203, October 2021. ISSN 1548-7105. doi: 10.1038/s41592-021-01252-x.\nURL https://www.nature.com/articles/s41592-021-01252-x . Number: 10\nPublisher: Nature Publishing Group.\nAndrew J Bannister and Tony Kouzarides. Regulation of chromatin by histone modifications.\nCell Research, 21(3):381–395, March 2011. ISSN 1748-7838. doi: 10.1038/cr.2011.22. URL\nhttps://doi.org/10.1038/cr.2011.22.\nGonzalo Benegas, Sanjit Singh Batra, and Yun S. Song. DNA language models are powerful zero-\nshot predictors of genome-wide variant effects. bioRxiv, pp. 2022.08.22.504706, January 2023.\ndoi: 10.1101/2022.08.22.504706. URL http://biorxiv.org/content/early/2023/\n04/12/2022.08.22.504706.abstract.\nTristan Bepler and Bonnie Berger. Learning the protein language: Evolution, structure, and\nfunction. Cell Systems, 12(6):654–669.e3, June 2021. ISSN 24054712. doi: 10.1016/\nj.cels.2021.05.017. URL https://linkinghub.elsevier.com/retrieve/pii/\nS2405471221002039.\nKathleen M. Chen, Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou. A sequence-based global\nmap of regulatory activity for deciphering human genetics. Nature Genetics, 54(7):940–949,\nJuly 2022. ISSN 1061-4036, 1546-1718. doi: 10.1038/s41588-022-01102-2. URL https:\n//www.nature.com/articles/s41588-022-01102-2 .\nLei Cheng, Tong Yu, Tero Aittokallio, Jukka Corander, Ruslan Khalitov, and Zhirong Yang. Self-\nsupervised learning for DNA sequences with circular dilated convolutional networks. preprint,\nBioinformatics, February 2023. URL http://biorxiv.org/lookup/doi/10.1101/\n2023.01.30.526193.\n11\nPublished as a conference paper at ICLR 2024\nHugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk\nGrzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume\nRichard, Marcin Skwark, Karim Beguir, Marie Lopez, and Thomas Pierrot. The Nucleotide Trans-\nformer: Building and Evaluating Robust Foundation Models for Human Genomics. bioRxiv,\npp. 2023.01.11.523679, January 2023. doi: 10.1101/2023.01.11.523679. URL http://\nbiorxiv.org/content/early/2023/01/15/2023.01.11.523679.abstract.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. 2018. doi: 10.48550/ARXIV .1810.\n04805. URL https://arxiv.org/abs/1810.04805. Publisher: arXiv Version Number:\n2.\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones,\nTom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and\nBurkhard Rost. ProtTrans: Toward Understanding the Language of Life Through Self-Supervised\nLearning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):7112–7127,\nOctober 2022. ISSN 0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2021.3095381.\nURL https://ieeexplore.ieee.org/document/9477085/.\nENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome.\nNature, 489(7414):57–74, September 2012. ISSN 1476-4687. doi: 10.1038/nature11247.\nVeniamin Fishman, Yuri Kuratov, Maxim Petrov, Aleksei Shmelev, Denis Shepelin, Nikolay\nChekanov, Olga Kardymon, and Mikhail Burtsev. GENA-LM: A Family of Open-Source Foun-\ndational Models for Long DNA Sequences, June 2023. URL https://www.biorxiv.org/\ncontent/10.1101/2023.06.12.544594v1. Pages: 2023.06.12.544594 Section: New\nResults.\nAdam Frankish, Mark Diekhans, Irwin Jungreis, Julien Lagarde, Jane E Loveland, Jonathan M\nMudge, Cristina Sisu, James C Wright, Joel Armstrong, If Barnes, Andrew Berry, Alexandra\nBignell, Carles Boix, Silvia Carbonell Sala, Fiona Cunningham, Tom ´as Di Domenico, Sarah\nDonaldson, Ian T Fiddes, Carlos Garc ´ıa Gir ´on, Jose Manuel Gonzalez, Tiago Grego, Matthew\nHardy, Thibaut Hourlier, Kevin L Howe, Toby Hunt, Osagie G Izuogu, Rory Johnson, Fergal J\nMartin, Laura Mart´ınez, Shamika Mohanan, Paul Muir, Fabio C P Navarro, Anne Parker, Baikang\nPei, Fernando Pozo, Ferriol Calvet Riera, Magali Ruffier, Bianca M Schmitt, Eloise Stapleton,\nMarie-Marthe Suner, Irina Sycheva, Barbara Uszczynska-Ratajczak, Maxim Y Wolf, Jinuri Xu,\nYucheng T Yang, Andrew Yates, Daniel Zerbino, Yan Zhang, Jyoti S Choudhary, Mark Ger-\nstein, Roderic Guig ´o, Tim J P Hubbard, Manolis Kellis, Benedict Paten, Michael L Tress, and\nPaul Flicek. GENCODE 2021. Nucleic Acids Research, 49(D1):D916–D923, January 2021.\nISSN 0305-1048. doi: 10.1093/nar/gkaa1087. URL https://doi.org/10.1093/nar/\ngkaa1087.\nJonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin\nGal, and Debora S. Marks. Disease variant prediction with deep generative models of evo-\nlutionary data. Nature, 599(7883):91–95, November 2021. ISSN 1476-4687. doi: 10.1038/\ns41586-021-04043-8. URL https://doi.org/10.1038/s41586-021-04043-8 .\nCharles P. Fulco, Joseph Nasser, Thouis R. Jones, Glen Munson, Drew T. Bergman, Vidya Subra-\nmanian, Sharon R. Grossman, Rockwell Anyoha, Benjamin R. Doughty, Tejal A. Patwardhan,\nTung H. Nguyen, Michael Kane, Elizabeth M. Perez, Neva C. Durand, Caleb A. Lareau, Elena K.\nStamenova, Erez Lieberman Aiden, Eric S. Lander, and Jesse M. Engreitz. Activity-by-contact\nmodel of enhancer–promoter regulation from thousands of CRISPR perturbations. Nature Genet-\nics, 51(12):1664–1669, December 2019. ISSN 1546-1718. doi: 10.1038/s41588-019-0538-0.\nURL https://www.nature.com/articles/s41588-019-0538-0 . Number: 12\nPublisher: Nature Publishing Group.\nDennis Gankin, Alexander Karollus, Martin Grosshauser, Kristian Klemon, Johannes Hingerl, and\nJulien Gagneur. Species-aware DNA language modeling. bioRxiv, pp. 2023.01.26.525670, Jan-\nuary 2023. doi: 10.1101/2023.01.26.525670. URL http://biorxiv.org/content/\nearly/2023/01/27/2023.01.26.525670.abstract.\n12\nPublished as a conference paper at ICLR 2024\nMolly Gasperini, Andrew J. Hill, Jos ´e L. McFaline-Figueroa, Beth Martin, Seungsoo Kim,\nMelissa D. Zhang, Dana Jackson, Anh Leith, Jacob Schreiber, William S. Noble, Cole Trap-\nnell, Nadav Ahituv, and Jay Shendure. A Genome-wide Framework for Mapping Gene Regula-\ntion via Cellular Genetic Screens. Cell, 176(1):377–390.e19, January 2019. ISSN 0092-8674.\ndoi: 10.1016/j.cell.2018.11.029. URL https://www.sciencedirect.com/science/\narticle/pii/S009286741831554X.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum ´e, and Kate Crawford. Datasheets for Datasets. 2018. doi: 10.48550/ARXIV .1803.\n09010. URL https://arxiv.org/abs/1803.09010. Publisher: arXiv Version Number:\n8.\nJ. Gorodkin. Comparing two K-category assignments by a K-category correlation coefficient.\nComputational Biology and Chemistry, 28(5):367–374, December 2004. ISSN 1476-9271.\ndoi: 10.1016/j.compbiolchem.2004.09.006. URL https://www.sciencedirect.com/\nscience/article/pii/S1476927104000799.\nKatar´ına Greˇsov´a, Vlastimil Martinek, David ˇCech´ak, Petr ˇSimeˇcek, and Panagiotis Alexiou. Ge-\nnomic benchmarks: a collection of datasets for genomic sequence classification. BMC Ge-\nnomic Data, 24(1):25, May 2023. ISSN 2730-6844. doi: 10.1186/s12863-023-01123-8. URL\nhttps://doi.org/10.1186/s12863-023-01123-8 .\nBenjamin C. Hitz, Jin-Wook Lee, Otto Jolanki, Meenakshi S. Kagda, Keenan Graham, Paul Sud,\nIdan Gabdank, J. Seth Strattan, Cricket A. Sloan, Timothy Dreszer, Laurence D. Rowe, Nikhil R.\nPodduturi, Venkat S. Malladi, Esther T. Chan, Jean M. Davidson, Marcus Ho, Stuart Miyasato,\nMatt Simison, Forrest Tanaka, Yunhai Luo, Ian Whaling, Eurie L. Hong, Brian T. Lee, Richard\nSandstrom, Eric Rynes, Jemma Nelson, Andrew Nishida, Alyssa Ingersoll, Michael Buckley,\nMark Frerker, Daniel S Kim, Nathan Boley, Diane Trout, Alex Dobin, Sorena Rahmanian, Dana\nWyman, Gabriela Balderrama-Gutierrez, Fairlie Reese, Neva C. Durand, Olga Dudchenko, David\nWeisz, Suhas S. P. Rao, Alyssa Blackburn, Dimos Gkountaroulis, Mahdi Sadr, Moshe Olshansky,\nYossi Eliaz, Dat Nguyen, Ivan Bochkov, Muhammad Saad Shamim, Ragini Mahajan, Erez Aiden,\nTom Gingeras, Simon Heath, Martin Hirst, W. James Kent, Anshul Kundaje, Ali Mortazavi, Bar-\nbara Wold, and J. Michael Cherry. The ENCODE Uniform Analysis Pipelines. preprint, Bioinfor-\nmatics, April 2023. URL http://biorxiv.org/lookup/doi/10.1101/2023.04.\n04.535623.\nYanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. DNABERT: pre-trained Bidirectional\nEncoder Representations from Transformers model for DNA-language in genome. Bioinformat-\nics, 37(15):2112–2120, August 2021. ISSN 1367-4803. doi: 10.1093/bioinformatics/btab083.\nURL https://doi.org/10.1093/bioinformatics/btab083.\nMeenakshi S. Kagda, Bonita Lam, Casey Litton, Corinn Small, Cricket A. Sloan, Emma Spragins,\nForrest Tanaka, Ian Whaling, Idan Gabdank, Ingrid Youngworth, J. Seth Strattan, Jason Hilton,\nJennifer Jou, Jessica Au, Jin-Wook Lee, Kalina Andreeva, Keenan Graham, Khine Lin, Matt\nSimison, Otto Jolanki, Paul Sud, Pedro Assis, Philip Adenekan, Eric Douglas, Mingjie Li, Pedro\nAssis, Keenan Graham, Paul Sud, Stuart Miyasato, Weiwei Zhong, Yunhai Luo, Zachary Myers,\nJ. Michael Cherry, and Benjamin C. Hitz. Data navigation on the ENCODE portal. 2023. doi:\n10.48550/ARXIV .2305.00006. URLhttps://arxiv.org/abs/2305.00006. Publisher:\narXiv Version Number: 2.\nDavid R. Kelley, Jasper Snoek, and John L. Rinn. Basset: learning the regulatory code of the\naccessible genome with deep convolutional neural networks. Genome Research, 26(7):990–999,\nJuly 2016. ISSN 1549-5469. doi: 10.1101/gr.200535.115.\nDavid R. Kelley, Yakir A. Reshef, Maxwell Bileschi, David Belanger, Cory Y . McLean, and Jasper\nSnoek. Sequential regulatory activity prediction across chromosomes with convolutional neural\nnetworks. Genome Research, 28(5):739–750, May 2018. ISSN 1088-9051, 1549-5469. doi: 10.\n1101/gr.227819.117. URL http://genome.cshlp.org/lookup/doi/10.1101/gr.\n227819.117.\nAndriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, and John Moult. Crit-\nical assessment of methods of protein structure prediction (CASP)—Round XIV. Proteins:\n13\nPublished as a conference paper at ICLR 2024\nStructure, Function, and Bioinformatics, 89(12):1607–1617, 2021. ISSN 1097-0134. doi: 10.\n1002/prot.26237. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/\nprot.26237. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.26237.\nMelissa J Landrum, Shanmuga Chitipiralla, Garth R Brown, Chao Chen, Baoshan Gu, Jennifer Hart,\nDouglas Hoffman, Wonhee Jang, Kuljeet Kaur, Chunlei Liu, Vitaly Lyoshin, Zenith Maddipatla,\nRama Maiti, Joseph Mitchell, Nuala O’Leary, George R Riley, Wenyao Shi, George Zhou, Valerie\nSchneider, Donna Maglott, J Bradley Holmes, and Brandi L Kattman. ClinVar: improvements to\naccessing data. Nucleic Acids Research, 48(D1):D835–D844, January 2020. ISSN 0305-1048.\ndoi: 10.1093/nar/gkz972. URL https://doi.org/10.1093/nar/gkz972.\nRichard Leslie, Christopher J. O’Donnell, and Andrew D. Johnson. GRASP: analysis of geno-\ntype–phenotype results from 1390 genome-wide association studies and corresponding open\naccess database. Bioinformatics, 30(12):i185–i194, June 2014. ISSN 1367-4803. doi:\n10.1093/bioinformatics/btu273. URL https://doi.org/10.1093/bioinformatics/\nbtu273.\nBenjamin Levy, Zihao Xu, Liyang Zhao, Karl Kremling, Ross Altman, Phoebe Wong, and\nChris Tanner. FloraBERT: cross-species transfer learning withattention-based neural networks\nfor geneexpression prediction. preprint, In Review, August 2022. URL https://www.\nresearchsquare.com/article/rs-1927200/v1.\nZeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,\nRobert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom\nSercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic-level\nprotein structure with a language model. Science, 379(6637):1123–1130, March 2023. ISSN\n0036-8075, 1095-9203. doi: 10.1126/science.ade2574. URL https://www.science.org/\ndoi/10.1126/science.ade2574.\nYunhai Luo, Benjamin C. Hitz, Idan Gabdank, Jason A. Hilton, Meenakshi S. Kagda, Bonita Lam,\nZachary Myers, Paul Sud, Jennifer Jou, Khine Lin, Ulugbek K. Baymuradov, Keenan Graham,\nCasey Litton, Stuart R. Miyasato, J. Seth Strattan, Otto Jolanki, Jin-Wook Lee, Forrest Y . Tanaka,\nPhilip Adenekan, Emma O’Neill, and J. Michael Cherry. New developments on the Encyclopedia\nof DNA Elements (ENCODE) data portal. Nucleic Acids Research, 48(D1):D882–D889, January\n2020. ISSN 1362-4962. doi: 10.1093/nar/gkz1062.\nAli Madani, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P. Mohr, James M.\nHolton, Jose Luis Olmos, Caiming Xiong, Zachary Z. Sun, Richard Socher, James S.\nFraser, and Nikhil Naik. Large language models generate functional protein sequences\nacross diverse families. Nature Biotechnology, January 2023. ISSN 1087-0156, 1546-\n1696. doi: 10.1038/s41587-022-01618-2. URL https://www.nature.com/articles/\ns41587-022-01618-2 .\nWilliam McLaren, Laurent Gil, Sarah E. Hunt, Harpreet Singh Riat, Graham R. S. Ritchie, Anja\nThormann, Paul Flicek, and Fiona Cunningham. The Ensembl Variant Effect Predictor. Genome\nBiology, 17(1):122, June 2016. ISSN 1474-760X. doi: 10.1186/s13059-016-0974-4. URL\nhttps://doi.org/10.1186/s13059-016-0974-4 .\nGil A. McVean, David M. Altshuler (Co-Chair), Richard M. Durbin (Co-Chair), Gonc ¸alo R. Abeca-\nsis, David R. Bentley, Aravinda Chakravarti, Andrew G. Clark, Peter Donnelly, Evan E. Eichler,\nPaul Flicek, Stacey B. Gabriel, Richard A. Gibbs, Eric D. Green, Matthew E. Hurles, Bartha M.\nKnoppers, Jan O. Korbel, Eric S. Lander, Charles Lee, Hans Lehrach, Elaine R. Mardis, Gabor T.\nMarth, Gil A. McVean, Deborah A. Nickerson, Jeanette P. Schmidt, Stephen T. Sherry, Jun Wang,\nRichard K. Wilson, Richard A. Gibbs (Principal Investigator), Huyen Dinh, Christie Kovar, San-\ndra Lee, Lora Lewis, Donna Muzny, Jeff Reid, Min Wang, Jun Wang (Principal Investigator),\nXiaodong Fang, Xiaosen Guo, Min Jian, Hui Jiang, Xin Jin, Guoqing Li, Jingxiang Li, Yingrui\nLi, Zhuo Li, Xiao Liu, Yao Lu, Xuedi Ma, Zhe Su, Shuaishuai Tai, Meifang Tang, Bo Wang,\nGuangbiao Wang, Honglong Wu, Renhua Wu, Ye Yin, Wenwei Zhang, Jiao Zhao, Meiru Zhao,\nXiaole Zheng, Yan Zhou, Eric S. Lander (Principal Investigator), David M. Altshuler, Stacey B.\nGabriel (Co-Chair), Namrata Gupta, Paul Flicek (Principal Investigator), Laura Clarke, Rasko\nLeinonen, Richard E. Smith, Xiangqun Zheng-Bradley, David R. Bentley (Principal Investigator),\n14\nPublished as a conference paper at ICLR 2024\nRussell Grocock, Sean Humphray, Terena James, Zoya Kingsbury, Hans Lehrach (Principal In-\nvestigator), Ralf Sudbrak (Project Leader), Marcus W. Albrecht, Vyacheslav S. Amstislavskiy, Ta-\ntiana A. Borodina, Matthias Lienhard, Florian Mertes, Marc Sultan, Bernd Timmermann, Marie-\nLaure Yaspo, Stephen T. Sherry (Principal Investigator), Gil A. McVean (Principal Investigator),\nElaine R. Mardis (Co-Principal Investigator) (Co-Chair), Richard K. Wilson (Co-Principal In-\nvestigator), Lucinda Fulton, Robert Fulton, George M. Weinstock, Richard M. Durbin (Princi-\npal Investigator), Senduran Balasubramaniam, John Burton, Petr Danecek, Thomas M. Keane,\nAnja Kolb-Kokocinski, Shane McCarthy, James Stalker, Michael Quail, Jeanette P. Schmidt\n(Principal Investigator), Christopher J. Davies, Jeremy Gollub, Teresa Webster, Brant Wong,\nYiping Zhan, Adam Auton (Principal Investigator), Richard A. Gibbs (Principal Investigator),\nFuli Yu (Project Leader), Matthew Bainbridge, Danny Challis, Uday S. Evani, James Lu, Donna\nMuzny, Uma Nagaswamy, Jeff Reid, Aniko Sabo, Yi Wang, Jin Yu, Jun Wang (Principal Inves-\ntigator), Lachlan J. M. Coin, Lin Fang, Xiaosen Guo, Xin Jin, Guoqing Li, Qibin Li, Yingrui\nLi, Zhenyu Li, Haoxiang Lin, Binghang Liu, Ruibang Luo, Nan Qin, Haojing Shao, Bingqiang\nWang, Yinlong Xie, Chen Ye, Chang Yu, Fan Zhang, Hancheng Zheng, Hongmei Zhu, Gabor T.\nMarth (Principal Investigator), Erik P. Garrison, Deniz Kural, Wan-Ping Lee, Wen Fung Leong,\nAlistair N. Ward, Jiantao Wu, Mengyao Zhang, Charles Lee (Principal Investigator), Lauren Grif-\nfin, Chih-Heng Hsieh, Ryan E. Mills, Xinghua Shi, Marcin von Grotthuss, Chengsheng Zhang,\nMark J. Daly (Principal Investigator), Mark A. DePristo (Project Leader), David M. Altshuler,\nEric Banks, Gaurav Bhatia, Mauricio O. Carneiro, Guillermo del Angel, Stacey B. Gabriel, Giulio\nGenovese, Namrata Gupta, Robert E. Handsaker, Chris Hartl, Eric S. Lander, Steven A. Mc-\nCarroll, James C. Nemesh, Ryan E. Poplin, Stephen F. Schaffner, Khalid Shakir, Seungtai C.\nYoon (Principal Investigator), Jayon Lihm, Vladimir Makarov, Hanjun Jin (Principal Investiga-\ntor), Wook Kim, Ki Cheol Kim, Jan O. Korbel (Principal Investigator), Tobias Rausch, Paul\nFlicek (Principal Investigator), Kathryn Beal, Laura Clarke, Fiona Cunningham, Javier Herrero,\nWilliam M. McLaren, Graham R. S. Ritchie, Richard E. Smith, Xiangqun Zheng-Bradley, An-\ndrew G. Clark (Principal Investigator), Srikanth Gottipati, Alon Keinan, Juan L. Rodriguez-\nFlores, Pardis C. Sabeti (Principal Investigator), Sharon R. Grossman, Shervin Tabrizi, Ridhi\nTariyal, David N. Cooper (Principal Investigator), Edward V . Ball, Peter D. Stenson, David R.\nBentley (Principal Investigator), Bret Barnes, Markus Bauer, R. Keira Cheetham, Tony Cox,\nMichael Eberle, Sean Humphray, Scott Kahn, Lisa Murray, John Peden, Richard Shaw, Kai\nYe (Principal Investigator), Mark A. Batzer (Principal Investigator), Miriam K. Konkel, Jeri-\nlyn A. Walker, Daniel G. MacArthur (Principal Investigator), Monkol Lek, Sudbrak (Project\nLeader), Vyacheslav S. Amstislavskiy, Ralf Herwig, Mark D. Shriver (Principal Investigator),\nCarlos D. Bustamante (Principal Investigator), Jake K. Byrnes, Francisco M. De La Vega, Simon\nGravel, Eimear E. Kenny, Jeffrey M. Kidd, Phil Lacroute, Brian K. Maples, Andres Moreno-\nEstrada, Fouad Zakharia, Eran Halperin (Principal Investigator), Yael Baran, David W. Craig\n(Principal Investigator), Alexis Christoforides, Nils Homer, Tyler Izatt, Ahmet A. Kurdoglu, Shri-\npad A. Sinari, Kevin Squire, Stephen T. Sherry (Principal Investigator), Chunlin Xiao, Jonathan\nSebat (Principal Investigator), Vineet Bafna, Kenny Ye, Esteban G. Burchard (Principal Inves-\ntigator), Ryan D. Hernandez (Principal Investigator), Christopher R. Gignoux, David Haussler\n(Principal Investigator), Sol J. Katzman, W. James Kent, Bryan Howie, Andres Ruiz-Linares\n(Principal Investigator), The 1000 Genomes Project Consortium, Corresponding Author, Steering\ncommittee, Production group:, Baylor College of Medicine, BGI-Shenzhen, Broad Institute of\nMIT and Harvard, European Bioinformatics Institute, Illumina, Max Planck Institute for Molec-\nular Genetics, US National Institutes of Health, University of Oxford, Washington University\nin St Louis, Wellcome Trust Sanger Institute, Analysis group:, Affymetrix, Albert Einstein Col-\nlege of Medicine, Boston College, Brigham and Women’s Hospital, Cold Spring Harbor Labora-\ntory, Dankook University, European Molecular Biology Laboratory, Cornell University, Harvard\nUniversity, Human Gene Mutation Database, Leiden University Medical Center, Louisiana State\nUniversity, Massachusetts General Hospital, Pennsylvania State University, Stanford University,\nTel-Aviv University, Translational Genomics Research Institute, San Diego University of Califor-\nnia, San Francisco University of California, Santa Cruz University of California, University of\nChicago, University College London, and University of Geneva. An integrated map of genetic\nvariation from 1,092 human genomes. Nature, 491(7422):56–65, November 2012. ISSN 1476-\n4687. doi: 10.1038/nature11632. URL https://doi.org/10.1038/nature11632.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM\nLanguage Models. 2017. doi: 10.48550/ARXIV .1708.02182. URL https://arxiv.org/\n15\nPublished as a conference paper at ICLR 2024\nabs/1708.02182. Publisher: arXiv Version Number: 1.\nEric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow,\nAman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A.\nBaccus, and Chris R ´e. HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nu-\ncleotide Resolution. 2023. doi: 10.48550/ARXIV .2306.15794. URL https://arxiv.org/\nabs/2306.15794. Publisher: arXiv Version Number: 1.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R ´e. Hyena Hierarchy: Towards Larger Convo-\nlutional Language Models, April 2023. URL http://arxiv.org/abs/2302.10866.\narXiv:2302.10866 [cs].\nOfir Press, Noah A. Smith, and Mike Lewis. Train Short, Test Long: Attention with Linear Biases\nEnables Input Length Extrapolation, April 2022. URL http://arxiv.org/abs/2108.\n12409. arXiv:2108.12409 [cs].\nRoshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter\nAbbeel, and Yun S. Song. Evaluating Protein Transfer Learning with TAPE. Advances in neu-\nral information processing systems, 32:9689–9701, December 2019. ISSN 1049-5258. URL\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7774645/.\nRoshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. Transformer pro-\ntein language models are unsupervised structure learners. preprint, Synthetic Biology, December\n2020. URL http://biorxiv.org/lookup/doi/10.1101/2020.12.15.422761.\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,\nMyle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function\nemerge from scaling unsupervised learning to 250 million protein sequences. preprint, Synthetic\nBiology, April 2019. URL http://biorxiv.org/lookup/doi/10.1101/622803.\nMarco Salvatore, Marc Horlacher, Annalisa Marsico, Ole Winther, and Robin Andersson. Trans-\nfer learning identifies sequence determinants of cell-type specific regulatory element acces-\nsibility. NAR Genomics and Bioinformatics, 5(2):lqad026, March 2023. ISSN 2631-9268.\ndoi: 10.1093/nargab/lqad026. URL https://academic.oup.com/nargab/article/\ndoi/10.1093/nargab/lqad026/7092956.\nMelissa Sanabria, Jonas Hirsch, and Anna R. Poetsch. The human genome’s vocabulary as proposed\nby the DNA language model GROVER, September 2023. URL https://www.biorxiv.\norg/content/10.1101/2023.07.19.549677v2. Pages: 2023.07.19.549677 Section:\nNew Results.\nNicolas Scalzitti, Anne Jeannin-Girardon, Pierre Collet, Olivier Poch, and Julie D. Thomp-\nson. A benchmark study of ab initio gene prediction methods in diverse eukaryotic\norganisms. BMC Genomics , 21(1):293, December 2020. ISSN 1471-2164. doi:\n10.1186/s12864-020-6707-9. URL https://bmcgenomics.biomedcentral.com/\narticles/10.1186/s12864-020-6707-9 .\nValerie A. Schneider, Tina Graves-Lindsay, Kerstin Howe, Nathan Bouk, Hsiu-Chuan Chen, Paul A.\nKitts, Terence D. Murphy, Kim D. Pruitt, Franc ¸oise Thibaud-Nissen, Derek Albracht, Robert S.\nFulton, Milinn Kremitzki, Vincent Magrini, Chris Markovic, Sean McGrath, Karyn Meltz Stein-\nberg, Kate Auger, William Chow, Joanna Collins, Glenn Harden, Timothy Hubbard, Sarah Pelan,\nJared T. Simpson, Glen Threadgold, James Torrance, Jonathan M. Wood, Laura Clarke, Sergey\nKoren, Matthew Boitano, Paul Peluso, Heng Li, Chen-Shan Chin, Adam M. Phillippy, Richard\nDurbin, Richard K. Wilson, Paul Flicek, Evan E. Eichler, and Deanna M. Church. Evalua-\ntion of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of\nthe reference assembly. Genome Research, 27(5):849–864, May 2017. ISSN 1549-5469. doi:\n10.1101/gr.213611.116.\nRitambhara Singh, Jack Lanchantin, Gabriel Robins, and Yanjun Qi. DeepChrome: deep-\nlearning for predicting gene expression from histone modifications. Bioinformatics, 32(17):\ni639–i648, September 2016. ISSN 1367-4803, 1367-4811. doi: 10.1093/bioinformatics/\n16\nPublished as a conference paper at ICLR 2024\nbtw427. URL https://academic.oup.com/bioinformatics/article/32/17/\ni639/2450757.\nMario Stanke and Stephan Waack. Gene prediction with a hidden Markov model and a new intron\nsubmodel. Bioinformatics (Oxford, England), 19 Suppl 2:ii215–225, October 2003. ISSN 1367-\n4811. doi: 10.1093/bioinformatics/btg1080.\nFelix Teufel, Magn ´us Halld ´or G ´ıslason, Jos ´e Juan Almagro Armenteros, Alexander Rosenberg\nJohansen, Ole Winther, and Henrik Nielsen. GraphPart: homology partitioning for biologi-\ncal sequence analysis. NAR Genomics and Bioinformatics, 5(4):lqad088, October 2023. ISSN\n2631-9268. doi: 10.1093/nargab/lqad088. URL https://academic.oup.com/nargab/\narticle/doi/10.1093/nargab/lqad088/7318077.\nJesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, and Nazneen Fatema\nRajani. BERTology Meets Biology: Interpreting Attention in Protein Language Models, March\n2021. URL http://arxiv.org/abs/2006.15222. arXiv:2006.15222 [cs, q-bio].\nMinghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Ma Chang, Runcheng\nLiu, and Jian Tang. PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence\nUnderstanding. Advances in Neural Information Processing Systems, 35:35156–35173, De-\ncember 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/hash/e467582d42d9c13fa9603df16f31de6d-Abstract-Datasets_\nand_Benchmarks.html.\nMeng Yang, Haiping Huang, Lichao Huang, Nan Zhang, Jihong Wu, Huanming Yang, and Feng\nMu. LOGO, a contextualized pre-trained language model of human genome flexibly adapts to\nvarious downstream tasks by fine-tuning. preprint, In Review, August 2021. URL https:\n//www.researchsquare.com/article/rs-448927/v1.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Al-\nberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and\nAmr Ahmed. Big Bird: Transformers for Longer Sequences. In Advances in\nNeural Information Processing Systems , volume 33, pp. 17283–17297. Curran Asso-\nciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nc8512d142a2d849725f31a9a7a361ab9-Abstract.html.\nJian Zhou and Olga G Troyanskaya. Predicting effects of noncoding variants with deep learn-\ning–based sequence model. Nature Methods, 12(10):931–934, October 2015. ISSN 1548-7091,\n1548-7105. doi: 10.1038/nmeth.3547. URL https://www.nature.com/articles/\nnmeth.3547.\nNaihui Zhou, Yuxiang Jiang, Timothy R. Bergquist, Alexandra J. Lee, Balint Z. Kacsoh, Alex W.\nCrocker, Kimberley A. Lewis, George Georghiou, Huy N. Nguyen, Md Nafiz Hamid, Larry\nDavis, Tunca Dogan, V olkan Atalay, Ahmet S. Rifaioglu, Alperen Dalkıran, Rengul Cetin Atalay,\nChengxin Zhang, Rebecca L. Hurto, Peter L. Freddolino, Yang Zhang, Prajwal Bhat, Fran Supek,\nJos´e M. Fern ´andez, Branislava Gemovic, Vladimir R. Perovic, Radoslav S. Davidovi ´c, Neven\nSumonja, Nevena Veljkovic, Ehsaneddin Asgari, Mohammad R.K. Mofrad, Giuseppe Profiti,\nCastrense Savojardo, Pier Luigi Martelli, Rita Casadio, Florian Boecker, Heiko Schoof, Indika\nKahanda, Natalie Thurlby, Alice C. McHardy, Alexandre Renaux, Rabie Saidi, Julian Gough,\nAlex A. Freitas, Magdalena Antczak, Fabio Fabris, Mark N. Wass, Jie Hou, Jianlin Cheng,\nZheng Wang, Alfonso E. Romero, Alberto Paccanaro, Haixuan Yang, Tatyana Goldberg, Chen-\nguang Zhao, Liisa Holm, Petri T ¨or¨onen, Alan J. Medlar, Elaine Zosa, Itamar Borukhov, Ilya\nNovikov, Angela Wilkins, Olivier Lichtarge, Po-Han Chi, Wei-Cheng Tseng, Michal Linial, Pe-\nter W. Rose, Christophe Dessimoz, Vedrana Vidulin, Saso Dzeroski, Ian Sillitoe, Sayoni Das,\nJonathan Gill Lees, David T. Jones, Cen Wan, Domenico Cozzetto, Rui Fa, Mateo Torres, Alex\nWarwick Vesztrocy, Jose Manuel Rodriguez, Michael L. Tress, Marco Frasca, Marco Notaro, Giu-\nliano Grossi, Alessandro Petrini, Matteo Re, Giorgio Valentini, Marco Mesiti, Daniel B. Roche,\nJonas Reeb, David W. Ritchie, Sabeur Aridhi, Seyed Ziaeddin Alborzi, Marie-Dominique De-\nvignes, Da Chen Emily Koo, Richard Bonneau, Vladimir Gligorijevi ´c, Meet Barot, Hai Fang,\nStefano Toppo, Enrico Lavezzo, Marco Falda, Michele Berselli, Silvio C.E. Tosatto, Marco Car-\nraro, Damiano Piovesan, Hafeez Ur Rehman, Qizhong Mao, Shanshan Zhang, Slobodan Vucetic,\n17\nPublished as a conference paper at ICLR 2024\nGage S. Black, Dane Jo, Erica Suh, Jonathan B. Dayton, Dallas J. Larsen, Ashton R. Om-\ndahl, Liam J. McGuffin, Danielle A. Brackenridge, Patricia C. Babbitt, Jeffrey M. Yunes, Paolo\nFontana, Feng Zhang, Shanfeng Zhu, Ronghui You, Zihan Zhang, Suyang Dai, Shuwei Yao,\nWeidong Tian, Renzhi Cao, Caleb Chandler, Miguel Amezola, Devon Johnson, Jia-Ming Chang,\nWen-Hung Liao, Yi-Wei Liu, Stefano Pascarelli, Yotam Frank, Robert Hoehndorf, Maxat Kul-\nmanov, Imane Boudellioua, Gianfranco Politano, Stefano Di Carlo, Alfredo Benso, Kai Hakala,\nFilip Ginter, Farrokh Mehryary, Suwisa Kaewphan, Jari Bj ¨orne, Hans Moen, Martti E.E. Tolva-\nnen, Tapio Salakoski, Daisuke Kihara, Aashish Jain, Tomislav ˇSmuc, Adrian Altenhoff, Asa\nBen-Hur, Burkhard Rost, Steven E. Brenner, Christine A. Orengo, Constance J. Jeffery, Gio-\nvanni Bosco, Deborah A. Hogan, Maria J. Martin, Claire O’Donovan, Sean D. Mooney, Casey S.\nGreene, Predrag Radivojac, and Iddo Friedberg. The CAFA challenge reports improved protein\nfunction prediction and new functional annotations for hundreds of genes through experimen-\ntal screens. Genome Biology, 20(1):244, November 2019. ISSN 1474-760X. doi: 10.1186/\ns13059-019-1835-8. URL https://doi.org/10.1186/s13059-019-1835-8 .\nZhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. DNABERT-\n2: Efficient Foundation Model and Benchmark For Multi-Species Genome, June 2023. URL\nhttp://arxiv.org/abs/2306.15006. arXiv:2306.15006 [cs, q-bio].\n18\nPublished as a conference paper at ICLR 2024\nA A PPENDIX\nA.1 D ATASET DOCUMENTATION\nWe document datasets that were established in this work following the Datasheets for Datasets\nframework (Gebru et al., 2018), discussing Motivation, Composition, Collection process, Prepro-\ncessing and Uses, as appropriate. As no new experimental data was acquired within this study, and\ndiscussing the original experimental protocols would exceed the scope of a datasheet, we limit the\nCollection sections to listing all relevant sources, where experimental procedures are documented.\nWe omit Distribution and Maintenance as these are identical for each dataset.\nA.1.1 G ENE FINDING\n• Motivation The dataset was created to benchmark the performance of models on the gene finding\ntask. Given a DNA sequence, a model predicts the structure of the gene, classifying nucleotides\nas introns, exons, splice sites and noncoding regions.\n• Composition Instances are the coordinates of human genes including flanking con-\ntext, together with nucleotide-level labels. There are 9 different labels y ∈\n{EF , DF , IF , AF , ER, DR, IR, AR, NC} denoting exons, donor splice sites, introns, ac-\nceptor splice sites and noncoding nucleotides. F and R denote whether the gene lies on the\nforward or reverse strand. There are a total of 5,976 instances with instance lengths ranging from\n1,433 to 14,000 nucleotides (Figure A1). The dataset is a sample of instances, selected based\non the transcript support level of the genes. Label sequences are complete without missing data.\nA recommended data split is included. The dataset depends on the human reference genome\nGRCh38.\n• Collection All data was acquired from GENCODE release 44 (Frankish et al., 2021).\n• Preprocessing Label sequences were generated from gff files downloaded from GENCODE\n(Frankish et al., 2021). Only HA V ANA protein coding gene annotations that were tagged with\na transcript support level 1 or 2 from GENCODE as well as level 1 or 2 confidence, meaning\nthat the transcript is experimentally verified, were considered. For genes with alternative splicing,\nonly the transcript with the best level of experimental support was chosen. In cases where support\nwas equal, a random transcript was chosen. For each transcript, flanking context to include was\nsampled at random. Following AUGUSTUS’ recommendations 1 for training and testing gene\nfinding models, the data was split so that no pair of instances in different partitions shares more\nthan 80% sequence identity of the mature protein. GraphPart (Teufel et al., 2023) with Needleman-\nWunsch global sequence alignments was used for splitting at a 80% sequence identity into train\n(80% of the data), test and validation (10% each).\n• Uses The specific dataset was established in this study and not used before. Data from GENCODE\nhas seen widespread use.\nA.1.2 E NHANCER ANNOTATION\n• Motivation This dataset was created to benchmark the performance of models in annotating the\ncorrect enhancer segment. Given a DNA sequence starting at the transcription start site of a gene\nand encompassing the enhancer, each nucleotide is classified based on a binary task into enhancer\nor non-enhancer.\n• Composition Instances are coordinates in the human genome, covering 100,096 nucleotides each,\nassociated with binary label sequences of length 782. Instances are centered on the transcription\nstart site of a gene and extend in both directions symmetrically, containing the enhancer element\non one side (Figure A2). In the label sequence, each label applies to a binned segment of 128 bp.\nThe segment is labeled 1 if it contains a nucleotide lying in the enhancer element, and0 otherwise\n(Figure A3). Some genes have multiple enhancer elements. In these cases all enhancer elements\nare labelled in one sample.\n• Collection Enhancer locations for genes of interest are obtained from the CRISPR interfer-\nence (CRISPRi) experiments of Fulco et al. (2019) and Gasperini et al. (2019) (GEO acces-\nsion GSE120861) via Avsec et al. (2021). CRISPRi experiments perturb a candidate enhancer\n1https://vcru.wisc.edu/simonlab/bioinformatics/programs/augustus/docs/\ntutorial2015/training.html\n19\nPublished as a conference paper at ICLR 2024\nand record whether the perturbation resulted in a change in gene expression. These experiments\nthereby directly measure the connection of an enhancer element to a specific gene. Following\nAvsec et al., we consider enhancers that had an expression change as ”validated”. Enhancer-gene\npairs that were predicted by the activity-by-contact (ABC) method only were not considered ex-\nperimentally validated and excluded. For each gene, the predicted main transcription start site was\nobtained directly from Avsec et al. (2021).\n• Preprocessing All non-validated gene-enhancer pairs were discarded, as were all pairs with over\n50,048 bp between the enhancer element and the transcription start site. Samples were split\nchromosome-wise into 10 partitions for cross-validation (1: chr7, chr8, chr18; 2: chr10, chrX,\nchr13; 3: chr14, chr22, chr6; 4: chr20, chr3; 5: chr11, chr12; 6: chr19; 7: chr4, chr5; 8: chr15,\nchr21, chr2; 9: chr1, chr16; 10: chr17, chr9).\n• Uses The binned label sequences over 100,096 bp were established in this work. The same under-\nlying enhancer-gene pairs were amongst the ones used in Avsec et al. (2021).\nA.1.3 C HROMATIN ACCESSIBILITY PREDICTION\n• Motivation The data was created to benchmark the performance of models on the chromatin\naccessibility prediction task. Given a DNA sequence, a model predicts whether the sequence is in\nopen or closed chromatin in different cell types.\n• Composition Instances are coordinates in the human genome, covering 512 nucleotides each,\nassociated with a binary label vector y ∈ {0, 1}125, indicating whether the DNA is in open (1)\nor closed (0) chromatin in 125 cell types (Table A2). This state is determined experimentally\nby whether the window of 512 nucleotides contains a DNAse I hypersensitive site. There are\n2,005,617 instances.\n• Collection Data was obtained from ENCODE (ENCODE Project Consortium, 2012; Luo et al.,\n2020; Kagda et al., 2023; Hitz et al., 2023). We downloaded DNase I hypersensitivity peaks for\n125 cell types in bed format.\n• Preprocessing The preprocessing followed Kelley et al. (Kelley et al., 2016). Peaks were ex-\ntended from to 512 bp from their midpoint, and peaks overlapping by less than 200bp were merged\ngreedily. When peaks of two or more cell types were merged, the resulting sample was annotated\nwith multiple cell type labels. Samples were split chromosome-wise into test (chr1, chr8, chr9;\n372,153 samples), validation (chr2, chr4; 279,422 samples) and train (remaining chromsomes;\n1,354,042 samples).\n• Uses The specific dataset was established in this study and not used before. Data from ENCODE\nhas seen widespread use, and comparable datasets were originally created in (Kelley et al., 2016).\nA.1.4 H ISTONE MODIFICATION PREDICTION\n• Motivation This dataset benchmarks the ability of models to predict post-translational modifica-\ntions of Histone proteins. Given a DNA sequence, a model is tasked to predict which histone-\nmodifications are present in the underlying nucleosome.\n• Composition Instances are coordinates in the human genome, covering 512 nucleotides each,\nassociated with a binary label vector of size 18, indicating whether a given histone mark (Table A1)\nis present (1) or not (0).\n• Collection Data was obtained from ENCODE (ENCODE Project Consortium, 2012). Narrow\npeaks files of 18 Histone ChIP-seq experiments was gathered from ENCODE (ENCODE Project\nConsortium, 2012) in bed format.\n• Preprocessing Following Kelley et al. (2016), peaks were extended from to 512 bp from their\nmidpoint, with peaks overlapping by less than 200bp being merged greedily. When peaks of two\nor more ChIP-seq experiments were merged, the resulting sample was annotated with the label of\neach experiment. Note that some Histone marks were covered by multiple experiments. Samples\nwere split chromosome-wise into test (chr1, chr8, chr9; 120,567 samples), validation (chr2, chr4;\n70,801 samples) and train (remaining chromsomes; 420,713 samples).\n• Uses The specific dataset was established in this study and not used before. It is based on publicly\navailable Histone ChIP-seq dataset from the ENCODE project, has seen widespread use.\n20\nPublished as a conference paper at ICLR 2024\nA.1.5 C PG METHYLATION\n• Motivation This dataset benchmarks the ability of models to predict the methylation of CpG\nsites. Methylation is an epigenetic modification of DNA that can affect a sequence’s activity and\nrepress gene expression. The methylation of a C to form 5-methylcytosine in CpG sites is the\nmost prominent type of methylation.\n• Composition Instances are coordinates in the human genome, covering 512 nucleotides each,\nassociated with a binary label vector of size 19, indicating whether the CpG site at the center of\nthe segment is methylated (1) or not (0) in a given cell line (Table A3).\n• Collection We gathered 7 human cell line whole-genome shotgun bisulfite sequencing (WGBS)\nexperiments from ENCODE (ENCODE Project Consortium, 2012) and processed the “methyla-\ntion state at CpG” bed files. To select cell lines, experiments marked in ENCODE as “Extremely\nlow coverage” or “Insufficient coverage” were excluded.\n• Preprocessing We removed all CpG sites that lie on non-standard chromosomes and that have\na variant in the respective sample genome that does not match the reference genome. Following\nDeepCpG (Angermueller et al., 2017), we removed CpG sites that are covered by less than 4 reads.\nCpG sites that had at least 90% methylated reads were labeled as methylated, sites with less than\n10% methylated reads were labeled as unmethylated, remaining sites were discarded. We took\nthe common subset of CpG sites passing the filtering criteria in all 7 experiments. Sites that were\nnot measured in all experiments were discarded, obtaining 959,039 sites in total. CpG sites were\nextended with flanking context to yield 512bp windows centered on the CpG site. Samples were\nsplit by chromosomes (test: chr4, chr13, chr19, chr21 - 106,227 samples; validation: chr5, chr9,\nchr22 - 109,717 samples; remainder train - 743,095 samples).\n• Uses The specific dataset was established in this study and not used before. It is based on publicly\navailable WGBS data from the ENCODE project which has seen widespread use.\nA.1.6 N ONCODING VARIANT EFFECTS (EXPRESSION )\n• Motivation The dataset was created to benchmark the zero-shot noncoding variant effect pre-\ndiction performance of models. Given a reference nucleotide, and a mutated nucleotide, two\nembeddings are computed and their cosine distance is used as the predictor.\n• Composition Instances are single nucleotide polymorphisms (SNP), genetic coordinates with a\nreference nucleotide xref ∈ {A, C. G, T} and a variant xvar ∈ {A, C. G, T} together with a bi-\nnary label y ∈ {0, 1} indicating whether the SNP has an effect on gene expression (1) or is genetic\nbackground variation (0). We use the same SNPs included in DeepSEA (Zhou & Troyanskaya,\n2015). The discovery of such functional SNPs, so-called eQTLs (Expression quantitative trait\nloci) is done through large-scale genetics studies that link genetic variation to gene expression.\nThere are 98,221 background SNPs and 8,000 variants with effect in total. As this is a zero-shot\ntask, no split is required. The dataset depends on the human reference genome GRCh38. eQTLs\nwere collected from GRASP (Leslie et al., 2014) and background SNPs from the 1000 Genomes\nProject (McVean et al., 2012). The dataset is a subsample of SNPs present in these databases.\nWhile the 1000 Genomes Project aims at faithfully representing human genetic variation, it might\nstill suffer from ethnicity biases (Table A6). The GRASP database is biased towards eQTLs ob-\nserved in individuals with european ancestry (Table A7).\n• Collection Genomic coordinates for SNPs were taken from DeepSEA (Zhou & Troyanskaya,\n2015).\n• Preprocessing As the original genomic coordinates refer to the previous reference genome\nGRCh37, we used LiftOver to transfer the coordinates to the current reference GRCh38. Any\ncoordinates that could not be mapped were discarded. Variants where the original reference nu-\ncleotide does not match the nucleotide at the indicated position in GRCh38 were removed. We\nonly use SNPs included in fold 0. We applied Ensembl VEP (McLaren et al., 2016) to categorize\nvariants by consequence. VEP infers the consequence of a variant by comparing a variant’s po-\nsition to the reference genome annotation, determining what type of sequence region (Table A4)\nthe variant lies in. To obtain one consequence per variant, we use VEP’s --most severe flag,\nreturning the consequence with the potentially most severe effect on function. In DeepSEA, the\nadjacent 1,000 bp served as context for classification. As this exceeds the maximum context length\n21\nPublished as a conference paper at ICLR 2024\nof some of the benchmarked models, and chunking inputs is not a meaningful strategy when ad-\njacent bps serve only as context for an unsupervised embedding, we use 512 bp instead. As this is\na zero-shot task, no split is performed, with the full dataset serving as test set.\n• Uses The same SNPs on GRCh37 were originally used in DeepSEA for both unsupervised (zero-\nshot) and supervised variant effect prediction.\nA.1.7 N ONCODING VARIANT EFFECTS (DISEASE )\n• Motivation The dataset was created to benchmark the zero-shot noncoding variant effect pre-\ndiction performance of models. Given a reference nucleotide, and a mutated nucleotide, two\nembeddings are computed and their distance is used as the predictor.\n• Composition Instances are single nucleotide polymorphisms (SNP), genetic coordinates with a\nreference nucleotide xref ∈ {A, C. G, T} and a variant xvar ∈ {A, C. G, T} together with a\nbinary label y ∈ {0, 1} indicating whether the SNP is benign (0) or pathogenic (1). There are\n274,399 benign and 21,524 pathogenic SNPs in total. As this is a zero-shot task, no split is\nrequired. The dataset depends on the human reference genome GRCh38.\n• Collection SNPs annotated as (likely) benign or pathogenic were collected from ClinVar, using\nthe variant summary file from 2023-07-02 (Landrum et al., 2020). We collected all variants\nannotated as single nucleotide variant with a review status of at least one star.\n• Preprocessing To subset ClinVar for noncoding variants, we first discarded all variants that are\nannotated as being in a protein in ClinVar itself. To further remove variants whose molecular effect\nmight not be annotated in ClinVar, we compared each SNP to GENCODE release 43 (Frankish\net al., 2021). All SNPs that were found to be in a CDS, start codon or stop codon were considered\ncoding and removed. We omit SNPs in the mitochondrial genome (”chromosome M”) as they are\nincompatible with the DeepSEA literature baseline. Following Frazer et al. (2021), the annotations\n”Likely pathogenic”, ”Pathogenic” and ”Likely benign”, ”Benign” were combined to yield binary\nlabels. We applied Ensembl VEP (McLaren et al., 2016) to categorize variants by consequence.\nVEP infers the consequence of a variant by comparing a variant’s position to the reference genome\nannotation, determining what type of sequence region (Table A5) the variant lies in. To obtain one\nconsequence per variant, we use VEP’s --most severe flag, returning the consequence with\nthe potentially most severe effect on function. The adjacent 512 bp serve as context for embedding.\nAs this is a zero-shot task, no split is performed, with the full dataset serving as test set.\n• Uses The specific dataset was established in this study and not used before. Data from ClinVar\nhas seen widespread use for variant effect prediction.\nFigure A1: Length distribution of samples in the gene finding dataset.\n22\nPublished as a conference paper at ICLR 2024\nFigure A2: Distance to main TSS distribution of the enhancer elements in the enhancer annotation\ndataset.\nFigure A3: Length distribution of the enhancer elements in the dataset.\nTable A1: Detailed label composition of the histone modification multilabel dataset (n=625,229).\nENCODE modification Label ID # positive instances % positive\nH3K27me3 K562 0 41,506 6.64%\nH3K9ac K562 1 93,261 14.92%\nH3K9me3 K562 2 25,295 4.05%\nH3K4me1 K562 3 98,678 15.78%\nH3K9ac K562 4 35,382 5.66%\nH3K4me1 K562 5 92,587 14.81%\nH3K36me3 K562 6 71,400 11.42%\nH3K36me3 K562 7 69,975 11.19%\nH4K20me1 K562 8 38,312 6.13%\nH3K27me3 K562 9 133,535 21.36%\nH3K4me3 K562 10 21,717 3.47%\nH3K4me3 K562 11 19,706 3.15%\nH3K4me3 K562 12 29,394 4.70%\nH3K4me3 K562 13 40,934 6.55%\nH3K79me2 K562 14 67,714 10.83%\nH3K4me2 K562 15 59,069 9.45%\nH3K27ac K562 16 42,993 6.88%\nH2AFZ K562 17 107,810 17.24%\nTable A2: Detailed label composition of the chromatin accessibility multilabel dataset\n(n=2,062,128).\nENCODE cell line Label ID # positive instances % positive\n8988T 0 184,985 8.97%\n23\nPublished as a conference paper at ICLR 2024\nENCODE cell line Label ID # positive instances % positive\nAoSMC 1 158,918 7.71%\nChorion 2 171,737 8.33%\nCLL 3 89,723 4.35%\nFibrobl 4 394,288 19.12%\nFibroP 5 249,221 12.09%\nGliobla 6 158,628 7.69%\nGM12891 7 135,186 6.56%\nGM12892 8 149,741 7.26%\nGM18507 9 109,689 5.32%\nGM19238 10 142,111 6.89%\nGM19239 11 120,883 5.86%\nGM19240 12 174,077 8.44%\nH9ES 13 154,898 7.51%\nHeLa-S3 IFNa4h 14 109,698 5.32%\nHepatocytes 15 164,799 7.99%\nHPDE6-E6E7 16 132,643 6.43%\nHSMM emb 17 123,566 5.99%\nHTR8svn 18 122,358 5.93%\nHuh-7.5 19 172,276 8.35%\nHuh-7 20 142,675 6.92%\niPS 21 192,872 9.35%\nIshikawa Estradiol 22 131,324 6.37%\nIshikawa 4OHTAM 23 133,612 6.48%\nLNCaP androgen 24 138,434 6.71%\nMCF-7 Hypoxia 25 146,053 7.08%\nMedullo 26 218,010 10.57%\nMelano 27 276,645 13.42%\nMyometr 28 165,059 8.00%\nOsteobl 29 367,127 17.80%\nPanIsletD 30 198,709 9.64%\nPanIslets 31 172,141 8.35%\npHTE 32 262,572 12.73%\nProgFib 33 201,038 9.75%\nRWPE1 34 146,568 7.11%\nStellate 35 157,369 7.63%\nT-47D 36 140,932 6.83%\nCD4 Th0 37 195,611 9.49%\nUrothelia 38 136,076 6.60%\nUrothelia UT189 39 169,356 8.21%\nAG04449 40 163,835 7.94%\nAG04450 41 145,390 7.05%\nAG09309 42 198,670 9.63%\nAG09319 43 139,005 6.74%\nAG10803 44 168,529 8.17%\nAoAF 45 171,356 8.31%\nBE2 C 46 172,185 8.35%\nBJ 47 160,706 7.79%\nCaco-2 48 118,338 5.74%\nCD20+ 49 100,298 4.86%\nCD34+ 50 158,606 7.69%\nCMK 51 129,859 6.30%\nGM06990 52 88,680 4.30%\nGM12864 53 132,999 6.45%\nGM12865 54 139,644 6.77%\nH7-hESC 55 263,281 12.77%\nHAc 56 177,288 8.60%\nHAEpiC 57 201,958 9.79%\n24\nPublished as a conference paper at ICLR 2024\nENCODE cell line Label ID # positive instances % positive\nHA-h 58 197,746 9.59%\nHA-sp 59 188,882 9.16%\nHBMEC 60 197,261 9.57%\nHCF 61 171,925 8.34%\nHCFaa 62 182,168 8.83%\nHCM 63 190,478 9.24%\nHConF 64 150,615 7.30%\nHCPEpiC 65 207,114 10.04%\nHCT-116 66 110,464 5.36%\nHEEpiC 67 206,638 10.02%\nHFF 68 189,177 9.17%\nHFF-Myc 69 206,882 10.03%\nHGF 70 143,241 6.95%\nHIPEpiC 71 222,312 10.78%\nHL-60 72 158,336 7.68%\nHMF 73 176,498 8.56%\nHMVEC-dAd 74 120,737 5.85%\nHMVEC-dBl-Ad 75 159,641 7.74%\nHMVEC-dBl-Neo 76 164,741 7.99%\nHMVEC-dLy-Ad 77 124,355 6.03%\nHMVEC-dLy-Neo 78 149,601 7.25%\nHMVEC-dNeo 79 137,163 6.65%\nHMVEC-LBl 80 167,109 8.10%\nHMVEC-LLy 81 141,044 6.84%\nHNPCEpiC 82 209,477 10.16%\nHPAEC 83 119,805 5.81%\nHPAF 84 185,109 8.98%\nHPdLF 85 168,839 8.19%\nHPF 86 151,615 7.35%\nHRCEpiC 87 189,381 9.18%\nHRE 88 184,386 8.94%\nHRGEC 89 134,424 6.52%\nHRPEpiC 90 224,149 10.87%\nHVMF 91 167,746 8.13%\nJurkat 92 155,987 7.56%\nMonocytes-CD14+ 93 131,745 6.39%\nNB4 94 140,287 6.80%\nNH-A 95 188,983 9.16%\nNHDF-Ad 96 227,566 11.04%\nNHDF-neo 97 185,464 8.99%\nNHLF 98 203,663 9.88%\nNT2-D1 99 179,350 8.70%\nPANC-1 100 114,230 5.54%\nPrEC 101 164,299 7.97%\nRPTEC 102 166,607 8.08%\nSAEC 103 195,586 9.48%\nSKMC 104 203,116 9.85%\nSK-N-MC 105 142,957 6.93%\nSK-N-SH RA 106 86,739 4.21%\nTh2 107 86,210 4.18%\nWERI-Rb-1 108 188,325 9.13%\nWI-38 109 163,827 7.94%\nWI-38 4OHTAM 110 202,173 9.80%\nA549 111 161,511 7.83%\nGM12878 112 168,725 8.18%\nH1-hESC 113 241,281 11.70%\nHeLa-S3 114 183,717 8.91%\n25\nPublished as a conference paper at ICLR 2024\nENCODE cell line Label ID # positive instances % positive\nHepG2 115 180,213 8.74%\nHMEC 116 321,049 15.57%\nHSMM 117 291,793 14.15%\nHSMMtube 118 304,753 14.78%\nHUVEC 119 179,245 8.69%\nK562 120 190,083 9.22%\nLNCaP 121 291,954 14.16%\nMCF-7 122 188,759 9.15%\nNHEK 123 201,376 9.77%\nTh1 124 293,092 14.21%\n26\nPublished as a conference paper at ICLR 2024\nTable A3: Detailed label composition of the CpG methylation multilabel dataset.\nENCODE cell line Label ID % methylated\nSK-N-SH 0 83%\nGM23248 1 84%\nA549 2 83%\nHepG2 3 81%\nHUES64 4 91%\nGM23248 5 84%\nHeLa-S3 6 84%\nTable A4: VEP variant consequence categories in the expression variant effects dataset.\nConsequence Background eQTL % eQTL\nIntron variant 55,710 5,002 8.24%\nIntergenic variant 22,465 753 3.24%\nUpstream gene variant 5,760 579 9.13%\nDownstream gene variant 4,146 435 9.50%\nRegulatory region variant 3,762 248 6.18%\nNoncoding transcript exon variant 2,757 342 11.03%\n3’ UTR variant 1,599 426 21.03%\n5’ UTR variant 408 54 11.69%\nTF binding site variant 410 23 5.31%\nSplice region variant 99 30 23.26%\nsplice polypyrimidine tract variant 85 18 17.48%\nMissense variant 51 9 15.00%\nSplice donor region variant 27 4 12.90%\nSynonymous variant 20 4 16.67%\nSplice donor variant 13 2 13.33%\nSplice donor 5th base variant 6 7 53.85%\nSplice acceptor variant 5 0 0.00%\nmature miRNA variant 2 0 0.00%\nStop lost variant 1 1 50.00%\n27\nPublished as a conference paper at ICLR 2024\nTable A5: VEP variant consequence categories in the disease variant effects dataset.\nConsequence Benign Pathogenic % Pathogenic\nIntron variant 138,023 188 0.14%\nSplice region variant 40,040 320 0.79%\nsplice polypyrimidine tract variant 39,501 185 0.47%\nNoncoding transcript exon variant 23,651 70 0.30%\n3’ UTR variant 20,407 34 0.17%\n5’ UTR variant 6,933 63 0.90%\nUpstream gene variant 2,245 19 0.84%\nSplice donor region variant 1,744 312 15.18%\nSplice donor 5th base variant 507 553 52.17%\nDownstream gene variant 268 4 1.47%\n- 262 0 0.00%\nSplice acceptor variant 194 9,086 97.91%\nSplice donor variant 189 10,622 98.25%\nmature miRNA variant 39 1 2.50%\nIntergenic variant 19 1 5.00%\nRegulatory region variant 10 0 0.00%\nSynonymous variant 3 0 0.00%\nMissense variant 1 0 0.00%\nTF binding site variant 1 0 0.00%\nTable A6: Population statistics of the 1000 Genomes Project (Phases 1 and 3) The data is based on\nSupplementary Information Table 1 from Auton et al. (2015).\nPopulation Count\nGambian in Western Division, The Gambia - Mandinka 113\nMende in Sierra Leone 85\nEsan in Nigeria 99\nColombian in Medellin, Colombia 174\nPeruvian in Lima, Peru 85\nPunjabi in Lahore, Pakistan 96\nIberian populations in Spain 121\nToscani in Italy 205\nMexican Ancestry in Los Angeles, California 130\nSri Lankan Tamil in the UK 102\nIndian Telugu in the UK 102\nBritish in England and Scotland 180\nYoruba in Ibadan, Nigeria 196\nJapanese in Tokyo, Japan 193\nUtah residents (CEPH) with Northern and Western European ancestry 184\nHan Chinese in Beijing, China 200\nChinese Dai in Xishuangbanna, China 93\nLuhya in Webuye, Kenya 196\nGujarati Indians in Houston, TX 103\nAfrican Ancestry in Southwest US 122\nFinnish in Finland 192\nHan Chinese South 205\nKinh in Ho Chi Minh City, Vietnam 99\nBengali in Bangladesh 86\nPuerto Rican in Puerto Rico 159\nAfrican Caribbean in Barbados 96\n28\nPublished as a conference paper at ICLR 2024\nTable A7: Population statistics of the eQTLs in the GRASP 2.0.0.0 database. GRASP combines\nresults from 2,082 individual studies. The ancestry information (GWASancestryDescription)\nis recorded on a study-wide level.\nAncestry eQTLs\nEuropean 446,403\nMixed 128,301\nUnspecified 111,218\nEuropean/Unspecified 11,376\nAfrican 2067\nNative 205\n29\nPublished as a conference paper at ICLR 2024\nA.2 F ORMATTING\nBuilding upon established standards in genomics, we curate all tasks in the same format for ease of\nreuse. Typically, it is not necessary to store DNA sequencesX explicitly for each task, as many tasks\nwill refer to the same reference genome. Therefore, for each task, we list the genome coordinates\nfor each sample in a bed genome annotation file. Splits and labels Y are also stored in these files,\nunless they are too complex to be stored in text format and are provided in ahdf5 file that shares its\nindex with the bed file. The bed-based format also makes it convenient to include more flanking\ncontext of the segments to be predicted without reprocessing the data, should future works find it\nuseful to take more bp into account.\nCode to extract DNA sequences from the reference genome with the bed coordinates, dataload-\ners, models and config files is available on Github (https://anonymous.4open.science/r/BEND-\n8C42/README.md).\nA.3 L ICENSE\nAs far as applicable, our contributions are licensed as CC BY 4.0. As no new data was generated\nin this study, the respective use/redistribution agreements and any copyright claims on the under-\nlying data sources (GENCODE (Frankish et al., 2021), ENCODE (ENCODE Project Consortium,\n2012), GRASP (Leslie et al., 2014), 1000 Genomes Project (McVean et al., 2012), Gasperini et al.\n(2019) (GEO accession GSE120861), Fulco et al. (2019), Avsec et al. (2021)) apply to the provided\ndatasets. Therefore, citation of the original sources is required when using the data provided with\nBEND. Citations in BibTex format are listed in the BEND repository.\nA.4 D ISTRIBUTION\nAll data is available at https://sid.erda.dk/cgi-sid/ls.py?share_id=\naNQa0Oz2lY Code, configs and scripts to extract data and run all experiments are provided\nat https://github.com/frederikkemarin/BEND.\nA.5 S OCIETAL IMPACT\nPredictors building upon DNA LMs may prove useful in a wide range of biomedical research ap-\nplications. Moreover, given their promising performance for understanding the effects of variants,\nfuture LMs or derived predictors with even higher performance may eventually become relevant\nfor medical applications. If LM-based predictors are used in clinical diagnostics on humans, it is\nimportant to ensure that their performance is evaluated over different populations and potential sub-\npopulation biases are accounted for. Moreover, should genomes from human individuals that are\nnot publicly released be used for pre-training LMs, it is important to ensure that their consent is\nobtained.\nA.6 LM DETAILS\nA.6.1 LM S TRAINED IN THIS WORK\nA WD-LSTM We trained an autoregressive AWD-LSTM LM using truncated backpropagation\nthrough time with a backpropagation window of 100 bps. Starting points were sampled randomly\nin the genome, and sequences processed until encountering a chromosome end, upon which the\nhidden state was reset. The model was trained on the full genomes of H. sapiens, M. musculusand\nD. melanogaster with a batch size of 1,024 for 1 million steps. This represents a minimal multi-\nspecies scenario that was selected due to computational constraints. The model has 3 LSTM layers\nwith dimensions 64, 1,024 and 64. Sequences were tokenized on the nucleotide level, yielding an\nalphabet of size 4. The model was trained on a single NVIDIA RTX 6000 GPU on a local cluster\nfor 35 days.\nDilated ResNet LM We trained a dilated CNN with residual connections, which is the same ar-\nchitecture used by GPN (Benegas et al., 2023). Since this model has a large receptive field due to\nthe dilations, we decided to take advantage of this by increasing the length of the training sequences\nfrom 512 nucleotides in GPN to 10,000 nucleotides here. To trade off the computational require-\nments, we reduce the number of hidden channels in the model from 512 to 256. The model was\ntrained by randomly sampling training sequences from contigs of the human reference genome. The\nreverse complementary of the sampled sequences was used with a 50% chance. Two chromosomes\nwere held out for testing and validation respectively. The model was trained with a batch size of 512\nfor a total of 50k steps, using 4 NVIDIA A40 GPUs for 14 days.\n30\nPublished as a conference paper at ICLR 2024\nA.6.2 LM CHECKPOINT SELECTION\nWe aimed to cover all DNA LM works that are publicly available and that included the human\ngenome in their pre-training data. For works that introduce more than one pre-trained checkpoint\nfor their proposed LM architectures, we choose a limited number of representative checkpoints in\norder to make efficient use of available computational resources. Whenever possible, the selection\nis driven by results and recommendations presented in the original work.\n• DNABERT We use the checkpoint that tokenizes DNA as overlapping 6-mers. The orig-\ninal DNABERT paper (Ji et al., 2021) states that the 6-mer checkpoint showed the best\nperformance when fine-tuning on the included tasks.\n• Nucleotide Transformer We evaluate the checkpoints trained on the human reference\ngenome (500M parameters), the 1000 Genomes Project (2.5B parameters) and on the set\nof genomes from multiple species (2.5B parameters).\n• GENA-LM In order to include one representative checkpoint both for the BigBird and the\nBERT architectures, we use bert-large-t2t and bigbird-base-t2t.\n• HyenaDNA HyenaDNA provides multiple sizes of the same model architecture trained on\nthe same data. The checkpoints also differ in the length of the sequences they were trained\non. We use tiny-1k, the smallest checkpoint that was trained on 1,000bp sequences, and\nlarge-1m, the largest checkpoint trained on 1 million bp sequences.\n• Nucleotide Transformer V2 We evaluate the largest available model with 500M parame-\nters.\nA.6.3 U PSAMPLING OF EMBEDDINGS\nLMs that make use of k-mer or byte-pair encoding (BPE) tokenization strategies return less em-\nbedding vectors than their original input sequence length. For nucleotide-level prediction tasks, an\nembedding sequence of equal length to the nucleotide-wise label sequence is needed. In order to\nbenchmark all LMs equally, regardless of how they tokenize inputs, we upsample embeddings. For\nthe 6-mer tokenization employed by NT, we repeat each embedding vector 6 times. For BPE in\nGENA-LM, DNABERT-2 and GROVER, we repeat each token’s embedding by the length of the\ntoken’s sequence. DNABERT, which uses overlapping k-mers, returns a reduced number of em-\nbeddings due to the fact that at the left and right borders of the sequence there is no k/2 context\navailable to construct a k-mer embedding around the nucleotide. As DNABERT does not perform\nany padding to correct for this, these initial and terminal k-mer embeddings are missing. We repeat\nthe first and the last embedding to match the original input sequence length. For k=6, we repeat the\nfirst embedding two and the last embedding three times.\nA.7 T ASK DETAILS\nComputations for all tasks were performed on single GPUs of the types RTX 6000, RTX 8000,\nV100, A40 and A100 on local clusters, depending on availability.\nGene finding CNN models were trained using AdamW with a learning rate of 0.003 and a weight\ndecay of 0.01 for 100 epochs with a batch size of 64.\nAUGUSTUS performance was evaluated on the test set. For each input sequence, exactly one com-\nplete gene model was predicted. Since AUGUSTUS only returns the CDS borders as well as the\nstrand, the remaining labels where inferred from the the CDS locations to compare with the ground\ntruth labels. All nucleotides prior to the first CDS and subsequent to the last are labeled as intergenic.\nNucleotides between two CDS segments are labeled as introns. The first and last nucleotide of each\nintron is labeled as a donor and acceptor site respectively for genes predicted to be on the positive\nstrand, on the negative strand it is reversed (acceptor site is the first nucleotide and donor the last).\nAugustus was run with the following settings:\n--strand=both --UTR=off --AUGUSTUS_CONFIG_PATH=path\n--gff3=on --genemodel=exactlyone --species=human sequence.fasta\nHistone modification CNN models were trained using AdamW with a learning rate of 0.003 and\na weight decay of 0.01 for 100 epochs with a batch size of 256.\n31\nPublished as a conference paper at ICLR 2024\nCpG methylation CNN models were trained using AdamW with a learning rate of 0.003 and a\nweight decay of 0.01 for 100 epochs with a batch size of 256.\nEnhancer annotation CNN models with channel size 2 were trained using AdamW with a learn-\ning rate of 0.001 and a weight decay of 0.01 for 100 epochs with a batch size of 8. Due to the high\nlabel imbalance in the data, positive labels were up-weighted in the loss with a weight corresponding\nto the average fraction of positive to negative labels.\nEnformer performance was evaluated using the code provided in the Compute contribution scores\nsection of the Enformer notebook 2. For each sample of 100,086 bp, context was expanded bidi-\nrectionally and Enformer contribution scores were obtained. The scores were trimmed back to the\noriginal 100,086 bp and average pooled at 128bp, yielding a sequence of 782 bins for each sample.\nNoncoding variant effects There are two ways of extracting an embedding for a variant sequence:\nIt is possible to either take the mean embedding of the full context window, or extract the embedding\nat the position where the SNP is found. Within BEND, we opted for the latter approach, as we\nconsider it more universally applicable to e.g. autoregressive models where preceding embeddings\nin the context window cannot contain any information on the variant that comes later in the sequence.\nFor NT, this means taking the embedding of the 6-mer token containing the variant. For DNABERT-\n2 and GENA-LM, the embedding of the BPE token containing the variant is used. For DNABERT\nwith 6-mer tokenization, we use the embedding of the token that has the mutated residue as its 3rd\nnucleotide.\nAs in autoregressive models subsequent tokens cannot affect already computed embeddings, we\nonly used an unidirectional context of 512 preceding nucleotides for AWD-LSTM and HyenaDNA.\nFor the expression dataset, supervised DeepSEA performance was computed from the cross-\nvalidated predictions for split 0 available in the supplementary material of the original DeepSEA\npublication (Zhou & Troyanskaya, 2015). Unsupervised performance could not be recomputed and\nwas taken at 0.6 from DeepSEA’s Supplementary Figure 6 for the expression dataset. For the disease\ndataset, supervised DeepSEA performance was computed by submission to DeepSEA’s online ver-\nsion, using the Beluga model. The Disease Impact Score (DIS) output was used for benchmarking.\n2https://github.com/google-deepmind/deepmind-research/blob/master/\nenformer/enformer-usage.ipynb\n32\nPublished as a conference paper at ICLR 2024\nA.8 E XTENDED RESULTS\nTable A8: Gene finding recall and precision per label.\nCDSF(0) DonorF(1) IntronF(2) AcceptorF(3) CDSR(4) AcceptorR(5) IntronR(6) DonorR(7) Intergenic(8)Model Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision\nAUGUSTUS0.89 0.90 0.80 0.88 0.83 0.87 0.79 0.86 0.89 0.91 0.81 0.86 0.85 0.89 0.80 0.85 0.86 0.81\nResNet 0.79 0.84 0.7 0.81 0.43 0.59 0.63 0.78 0.84 0.83 0.740.83 0.68 0.52 0.61 0.76 0.59 0.62CNN 0.0 0.0 0.0 0.0 0.01 0.28 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.39A WD-LSTM0.0 0.33 0.0 0.0 0.26 0.28 0.0 0.0 0.0 0.22 0.0 0.0 0.02 0.38 0.0 0.0 0.81 0.4ResNet-LM0.59 0.62 0.0 0.0 0.52 0.41 0.0 0.0 0.51 0.76 0.0 0.0 0.56 0.46 0.0 0.0 0.43 0.55NT-H 0.67 0.59 0.0 0.0 0.59 0.49 0.0 0.0 0.6 0.7 0.0 0.22 0.65 0.54 0.0 0.0 0.49 0.66NT-MS 0.94 0.89 0.73 0.66 0.84 0.69 0.5 0.73 0.93 0.89 0.64 0.74 0.86 0.69 0.57 0.66 0.55 0.79NT-1000G0.78 0.79 0.03 0.28 0.7 0.59 0.01 0.64 0.76 0.84 0.14 0.62 0.74 0.63 0.06 0.43 0.57 0.7NT-V2 0.94 0.91 0.75 0.73 0.78 0.65 0.55 0.8 0.94 0.91 0.75 0.74 0.81 0.68 0.590.77 0.57 0.77DNABERT0.43 0.49 0.47 0.33 0.54 0.34 0.24 0.36 0.39 0.58 0.2 0.52 0.2 0.45 0.38 0.35 0.56 0.5DNABERT20.51 0.69 0.09 0.42 0.6 0.5 0.0 0.0 0.51 0.69 0.13 0.49 0.57 0.56 0.0 0.0 0.63 0.65GENA-LM BERT0.82 0.81 0.34 0.83 0.69 0.6 0.29 0.59 0.82 0.81 0.26 0.57 0.7 0.61 0.31 0.65 0.53 0.65GENA-LM BigBird0.41 0.53 0.13 0.35 0.59 0.49 0.13 0.33 0.39 0.56 0.11 0.33 0.75 0.51 0.04 0.41 0.43 0.66HyenaDNA tiny0.17 0.26 0.05 0.11 0.29 0.33 0.02 0.23 0.02 0.39 0.06 0.43 0.04 0.46 0.0 0.0 0.79 0.41HyenaDNA large0.23 0.4 0.04 0.18 0.6 0.45 0.06 0.23 0.36 0.4 0.23 0.38 0.62 0.52 0.0 0.08 0.48 0.62GROVER 0.31 0.53 0.12 0.29 0.55 0.39 0.01 0.14 0.45 0.48 0.26 0.36 0.47 0.48 0.06 0.25 0.48 0.55\nTable A9: Chromatin accessibility prediction performance per cell line.\nCell line\nBasset\nCNN\nA WD-LSTM\nResNet-LM\nNT-H\nNT-MS\nNT-1000G (2.5B)\nNT-V2\nDNABERT\nDNABERT-2\nGENA-LM BERT\nGENA-LM BigBird\nHyenaDNA tiny\nHyenaDNA large\nGROVER\n8988T 0.86 0.82 0.82 0.85 0.83 0.85 0.84 0.84 0.86 0.85 0.84 0.85 0.84 0.85 0.85\nAoSMC 0.89 0.75 0.68 0.84 0.75 0.80 0.78 0.81 0.87 0.82 0.78 0.83 0.79 0.85 0.83\nChorion 0.81 0.78 0.77 0.81 0.79 0.81 0.80 0.80 0.82 0.81 0.80 0.81 0.80 0.81\nCLL 0.87 0.81 0.80 0.86 0.82 0.84 0.83 0.85 0.88 0.86 0.83 0.86 0.84 0.86 0.86\nFibrobl 0.71 0.68 0.67 0.71 0.69 0.71 0.69 0.70 0.72 0.71 0.70 0.71 0.70 0.71 0.71\nFibroP 0.78 0.69 0.65 0.75 0.70 0.74 0.72 0.74 0.77 0.75 0.72 0.75 0.72 0.76 0.75\nGliobla 0.86 0.75 0.71 0.84 0.77 0.82 0.80 0.82 0.86 0.83 0.80 0.83 0.81 0.85 0.83\nGM12891 0.89 0.83 0.81 0.87 0.83 0.85 0.84 0.86 0.89 0.87 0.84 0.87 0.85 0.88 0.87\nGM12892 0.88 0.84 0.83 0.87 0.85 0.86 0.85 0.86 0.89 0.87 0.86 0.87 0.86 0.88 0.87\nGM18507 0.87 0.77 0.74 0.84 0.77 0.80 0.78 0.82 0.87 0.83 0.79 0.84 0.80 0.85 0.84\nGM19238 0.86 0.79 0.77 0.84 0.80 0.82 0.81 0.83 0.87 0.84 0.81 0.84 0.82 0.85 0.84\nGM19239 0.87 0.79 0.77 0.85 0.80 0.82 0.81 0.84 0.88 0.85 0.81 0.85 0.82 0.86 0.85\nGM19240 0.81 0.74 0.73 0.80 0.76 0.77 0.76 0.78 0.82 0.79 0.77 0.80 0.77 0.80 0.79\nH9ES 0.88 0.81 0.79 0.86 0.82 0.85 0.85 0.85 0.89 0.85 0.83 0.86 0.84 0.87 0.86\nHeLa-S3 IFNa4h 0.85 0.72 0.70 0.82 0.76 0.81 0.79 0.81 0.85 0.81 0.79 0.82 0.79 0.83 0.82\nHepatocytes 0.72 0.73 0.72 0.75 0.74 0.75 0.74 0.74 0.76 0.76 0.75 0.76 0.74 0.75 0.75\nHPDE6-E6E7 0.90 0.75 0.70 0.85 0.77 0.83 0.81 0.84 0.88 0.84 0.81 0.85 0.82 0.87 0.85\nHSMM emb 0.90 0.80 0.77 0.88 0.82 0.87 0.85 0.87 0.90 0.88 0.85 0.88 0.85 0.89 0.88\nHTR8svn 0.91 0.76 0.72 0.86 0.78 0.84 0.82 0.85 0.89 0.85 0.82 0.86 0.83 0.88 0.86\nHuh-7.5 0.81 0.76 0.75 0.81 0.78 0.80 0.79 0.80 0.83 0.81 0.79 0.81 0.79 0.82 0.81\nHuh-7 0.84 0.77 0.75 0.83 0.78 0.81 0.80 0.81 0.86 0.82 0.80 0.82 0.80 0.84 0.83\niPS 0.91 0.87 0.87 0.90 0.88 0.90 0.89 0.90 0.91 0.90 0.88 0.90 0.89 0.91 0.90\nIshikawa Estradiol 0.85 0.76 0.74 0.83 0.78 0.81 0.79 0.81 0.85 0.81 0.78 0.82 0.79 0.84 0.82\nIshikawa 4OHTAM 0.85 0.77 0.75 0.83 0.78 0.81 0.80 0.81 0.86 0.82 0.79 0.83 0.80 0.84 0.83\nLNCaP androgen 0.82 0.76 0.74 0.83 0.77 0.79 0.78 0.79 0.85 0.81 0.78 0.81 0.78 0.83 0.82\nMCF-7 Hypoxia 0.83 0.75 0.74 0.81 0.76 0.79 0.78 0.80 0.85 0.80 0.77 0.80 0.78 0.81 0.80\nMedullo 0.72 0.71 0.69 0.73 0.71 0.72 0.71 0.72 0.75 0.74 0.71 0.74 0.72 0.74 0.73\nMelano 0.71 0.65 0.63 0.70 0.66 0.68 0.67 0.68 0.71 0.70 0.67 0.69 0.67 0.70 0.69\nMyometr 0.84 0.74 0.68 0.82 0.75 0.80 0.78 0.80 0.84 0.81 0.77 0.81 0.79 0.83 0.81\nOsteobl 0.72 0.69 0.68 0.72 0.70 0.72 0.71 0.71 0.73 0.72 0.71 0.72 0.71 0.73 0.72\nPanIsletD 0.85 0.74 0.68 0.82 0.74 0.79 0.78 0.80 0.84 0.81 0.76 0.81 0.79 0.83 0.82\nPanIslets 0.79 0.75 0.74 0.80 0.77 0.80 0.78 0.79 0.81 0.80 0.79 0.80 0.78 0.80 0.80\npHTE 0.81 0.73 0.70 0.79 0.74 0.78 0.76 0.78 0.81 0.78 0.76 0.78 0.77 0.80 0.79\nProgFib 0.85 0.76 0.71 0.83 0.76 0.80 0.79 0.81 0.85 0.82 0.78 0.82 0.80 0.84 0.82\nRWPE1 0.90 0.74 0.68 0.85 0.76 0.83 0.81 0.83 0.88 0.84 0.80 0.84 0.82 0.87 0.85\nStellate 0.88 0.77 0.71 0.85 0.77 0.82 0.81 0.83 0.87 0.84 0.80 0.84 0.81 0.86 0.84\nT-47D 0.81 0.75 0.73 0.79 0.75 0.78 0.77 0.78 0.81 0.79 0.77 0.79 0.76 0.80 0.79\nCD4 Th0 0.79 0.76 0.75 0.80 0.77 0.79 0.78 0.79 0.80 0.80 0.78 0.79 0.78 0.80 0.79\nUrothelia 0.90 0.79 0.76 0.87 0.81 0.85 0.84 0.86 0.89 0.87 0.83 0.86 0.84 0.88 0.87\nUrothelia UT189 0.85 0.76 0.73 0.82 0.78 0.81 0.80 0.81 0.88 0.82 0.79 0.81 0.80 0.84 0.83\nAG04449 0.90 0.74 0.65 0.83 0.72 0.79 0.76 0.80 0.87 0.81 0.75 0.82 0.78 0.86 0.83\nAG04450 0.89 0.74 0.66 0.84 0.74 0.80 0.78 0.81 0.87 0.82 0.77 0.83 0.79 0.86 0.83\nAG09309 0.89 0.73 0.65 0.83 0.72 0.78 0.76 0.80 0.87 0.80 0.75 0.82 0.78 0.85 0.82\nAG09319 0.89 0.75 0.67 0.84 0.74 0.80 0.78 0.81 0.87 0.82 0.77 0.83 0.79 0.85 0.83\nAG10803 0.90 0.74 0.65 0.83 0.72 0.78 0.76 0.80 0.87 0.81 0.75 0.82 0.78 0.86 0.83\nAoAF 0.89 0.74 0.66 0.83 0.73 0.79 0.77 0.80 0.87 0.81 0.76 0.82 0.78 0.85 0.83\nBE2 C 0.80 0.73 0.69 0.80 0.72 0.76 0.75 0.77 0.83 0.78 0.73 0.79 0.75 0.81 0.79\nBJ 0.89 0.75 0.66 0.83 0.73 0.79 0.77 0.80 0.87 0.81 0.76 0.82 0.78 0.85 0.83\nCaco-2 0.91 0.91 0.90 0.92 0.91 0.91 0.91 0.91 0.93 0.92 0.90 0.92 0.91 0.92 0.93\nCD20+ 0.87 0.78 0.73 0.84 0.76 0.80 0.79 0.83 0.88 0.84 0.78 0.84 0.80 0.85 0.84\nCD34+ 0.87 0.75 0.70 0.83 0.73 0.78 0.77 0.82 0.87 0.81 0.76 0.83 0.78 0.84 0.83\nCMK 0.81 0.74 0.69 0.81 0.72 0.76 0.74 0.79 0.85 0.78 0.72 0.81 0.76 0.81 0.81\nGM06990 0.85 0.78 0.72 0.83 0.73 0.75 0.75 0.80 0.86 0.81 0.74 0.82 0.78 0.83 0.82\nGM12864 0.85 0.74 0.67 0.80 0.69 0.73 0.72 0.78 0.85 0.79 0.71 0.80 0.76 0.82 0.80\nGM12865 0.86 0.74 0.67 0.81 0.70 0.74 0.73 0.79 0.85 0.79 0.71 0.80 0.76 0.83 0.81\nH7-hESC 0.78 0.72 0.69 0.78 0.73 0.75 0.75 0.76 0.83 0.77 0.71 0.79 0.75 0.79 0.79\nHAc 0.88 0.74 0.66 0.83 0.74 0.79 0.78 0.81 0.87 0.81 0.76 0.83 0.78 0.86 0.83\nHAEpiC 0.86 0.72 0.63 0.81 0.70 0.76 0.74 0.78 0.86 0.79 0.73 0.80 0.76 0.84 0.81\n33\nPublished as a conference paper at ICLR 2024\nHA-h 0.88 0.74 0.67 0.83 0.74 0.79 0.78 0.81 0.86 0.81 0.77 0.83 0.79 0.85 0.83\nHA-sp 0.84 0.73 0.68 0.81 0.73 0.78 0.76 0.79 0.83 0.80 0.76 0.80 0.77 0.82 0.80\nHBMEC 0.89 0.73 0.64 0.83 0.72 0.80 0.78 0.82 0.87 0.82 0.77 0.83 0.79 0.86 0.83\nHCF 0.89 0.74 0.66 0.83 0.73 0.79 0.77 0.80 0.87 0.81 0.76 0.83 0.78 0.86 0.83\nHCFaa 0.89 0.72 0.64 0.83 0.72 0.80 0.77 0.81 0.87 0.81 0.76 0.82 0.78 0.85 0.82\nHCM 0.89 0.73 0.65 0.82 0.72 0.78 0.76 0.80 0.87 0.80 0.75 0.82 0.77 0.85 0.82\nHConF 0.89 0.74 0.67 0.84 0.74 0.81 0.78 0.81 0.87 0.82 0.78 0.83 0.79 0.86 0.83\nHCPEpiC 0.88 0.71 0.64 0.81 0.71 0.77 0.75 0.79 0.85 0.79 0.74 0.80 0.76 0.84 0.81\nHCT-116 0.89 0.73 0.68 0.85 0.75 0.84 0.81 0.85 0.88 0.84 0.80 0.85 0.81 0.87 0.85\nHEEpiC 0.90 0.71 0.62 0.82 0.71 0.78 0.76 0.80 0.87 0.80 0.74 0.81 0.78 0.85 0.81\nHFF 0.89 0.73 0.65 0.83 0.73 0.79 0.77 0.80 0.86 0.81 0.76 0.82 0.78 0.85 0.82\nHFF-Myc 0.86 0.71 0.64 0.81 0.71 0.77 0.75 0.78 0.85 0.79 0.74 0.80 0.76 0.83 0.81\nHGF 0.89 0.76 0.66 0.83 0.73 0.78 0.77 0.80 0.87 0.81 0.75 0.82 0.78 0.85 0.83\nHIPEpiC 0.88 0.72 0.64 0.81 0.71 0.78 0.76 0.79 0.86 0.80 0.74 0.81 0.77 0.84 0.81\nHL-60 0.81 0.70 0.64 0.77 0.67 0.69 0.68 0.73 0.83 0.76 0.66 0.75 0.72 0.80 0.77\nHMF 0.90 0.74 0.64 0.84 0.73 0.81 0.78 0.82 0.88 0.83 0.77 0.83 0.80 0.87 0.84\nHMVEC-dAd 0.89 0.76 0.70 0.85 0.76 0.81 0.79 0.83 0.88 0.83 0.79 0.85 0.80 0.87 0.84\nHMVEC-dBl-Ad 0.89 0.74 0.66 0.84 0.72 0.78 0.76 0.82 0.88 0.81 0.75 0.83 0.78 0.86 0.83\nHMVEC-dBl-Neo 0.88 0.73 0.66 0.82 0.72 0.78 0.76 0.81 0.86 0.80 0.75 0.82 0.77 0.85 0.82\nHMVEC-dLy-Ad 0.88 0.76 0.68 0.83 0.74 0.79 0.78 0.81 0.87 0.81 0.77 0.83 0.79 0.86 0.83\nHMVEC-dLy-Neo 0.89 0.75 0.67 0.84 0.73 0.79 0.77 0.82 0.87 0.81 0.76 0.83 0.79 0.86 0.83\nHMVEC-dNeo 0.89 0.76 0.69 0.84 0.75 0.80 0.78 0.82 0.88 0.82 0.77 0.84 0.79 0.86 0.84\nHMVEC-LBl 0.89 0.73 0.65 0.83 0.72 0.79 0.77 0.82 0.87 0.81 0.76 0.83 0.78 0.86 0.83\nHMVEC-LLy 0.87 0.75 0.68 0.82 0.74 0.78 0.77 0.80 0.86 0.80 0.76 0.82 0.78 0.85 0.82\nHNPCEpiC 0.89 0.72 0.64 0.83 0.72 0.79 0.77 0.81 0.87 0.81 0.76 0.82 0.78 0.85 0.83\nHPAEC 0.88 0.75 0.68 0.83 0.74 0.80 0.78 0.82 0.87 0.82 0.77 0.84 0.79 0.86 0.83\nHPAF 0.89 0.73 0.65 0.83 0.72 0.79 0.77 0.80 0.87 0.81 0.75 0.82 0.78 0.86 0.83\nHPdLF 0.89 0.75 0.66 0.83 0.73 0.79 0.77 0.80 0.86 0.81 0.76 0.82 0.78 0.85 0.83\nHPF 0.90 0.75 0.67 0.85 0.75 0.81 0.79 0.82 0.88 0.83 0.78 0.83 0.79 0.87 0.84\nHRCEpiC 0.85 0.72 0.65 0.82 0.72 0.78 0.76 0.79 0.85 0.79 0.75 0.81 0.78 0.84 0.81\nHRE 0.87 0.73 0.65 0.83 0.73 0.80 0.78 0.81 0.87 0.82 0.77 0.83 0.80 0.85 0.83\nHRGEC 0.88 0.73 0.66 0.83 0.73 0.79 0.77 0.82 0.86 0.81 0.76 0.83 0.78 0.85 0.83\nHRPEpiC 0.83 0.73 0.65 0.80 0.70 0.75 0.74 0.77 0.84 0.79 0.73 0.79 0.76 0.83 0.80\nHVMF 0.86 0.74 0.65 0.82 0.71 0.77 0.75 0.78 0.85 0.80 0.74 0.80 0.76 0.83 0.81\nJurkat 0.82 0.72 0.65 0.80 0.67 0.71 0.70 0.78 0.84 0.76 0.68 0.78 0.74 0.82 0.79\nMonocytes-CD14+ 0.86 0.74 0.67 0.82 0.71 0.75 0.74 0.80 0.88 0.81 0.72 0.81 0.77 0.84 0.82\nNB4 0.87 0.74 0.68 0.83 0.72 0.77 0.75 0.80 0.88 0.81 0.74 0.82 0.77 0.85 0.83\nNH-A 0.89 0.75 0.66 0.84 0.73 0.79 0.77 0.81 0.87 0.82 0.76 0.83 0.80 0.86 0.84\nNHDF-Ad 0.87 0.74 0.64 0.81 0.70 0.76 0.75 0.78 0.85 0.79 0.73 0.80 0.76 0.84 0.81\nNHDF-neo 0.87 0.76 0.65 0.82 0.71 0.77 0.76 0.79 0.86 0.80 0.74 0.81 0.77 0.84 0.82\nNHLF 0.89 0.74 0.65 0.83 0.73 0.79 0.77 0.81 0.87 0.81 0.76 0.83 0.79 0.86 0.83\nNT2-D1 0.82 0.74 0.71 0.81 0.76 0.79 0.78 0.80 0.85 0.80 0.76 0.82 0.78 0.82 0.81\nPANC-1 0.86 0.71 0.65 0.82 0.73 0.81 0.79 0.82 0.85 0.81 0.78 0.82 0.78 0.84 0.82\nPrEC 0.89 0.73 0.64 0.83 0.72 0.79 0.77 0.80 0.87 0.80 0.75 0.82 0.79 0.85 0.82\nRPTEC 0.84 0.71 0.65 0.81 0.72 0.78 0.75 0.79 0.84 0.79 0.74 0.80 0.77 0.83 0.80\nSAEC 0.90 0.71 0.62 0.82 0.71 0.79 0.77 0.80 0.87 0.80 0.75 0.81 0.78 0.85 0.82\nSKMC 0.88 0.73 0.65 0.83 0.73 0.78 0.76 0.79 0.87 0.81 0.75 0.81 0.78 0.85 0.82\nSK-N-MC 0.81 0.74 0.68 0.79 0.71 0.76 0.74 0.77 0.81 0.77 0.73 0.78 0.75 0.80 0.78\nSK-N-SH RA 0.87 0.85 0.81 0.89 0.83 0.85 0.85 0.86 0.90 0.87 0.83 0.88 0.85 0.89 0.88\nTh2 0.86 0.79 0.73 0.84 0.76 0.78 0.78 0.83 0.87 0.82 0.77 0.84 0.81 0.86 0.84\nWERI-Rb-1 0.75 0.75 0.65 0.81 0.70 0.70 0.72 0.77 0.86 0.79 0.67 0.79 0.75 0.82 0.80\nWI-38 0.89 0.73 0.64 0.83 0.72 0.79 0.77 0.81 0.87 0.81 0.76 0.82 0.78 0.85 0.83\nWI-38 4OHTAM 0.84 0.72 0.62 0.81 0.70 0.77 0.75 0.79 0.84 0.79 0.74 0.80 0.77 0.83 0.80\nA549 0.84 0.71 0.67 0.81 0.74 0.79 0.78 0.80 0.84 0.80 0.77 0.80 0.78 0.82 0.81\nGM12878 0.82 0.73 0.69 0.78 0.71 0.74 0.73 0.77 0.82 0.78 0.73 0.78 0.75 0.80 0.78\nH1-hESC 0.86 0.82 0.80 0.85 0.82 0.84 0.84 0.84 0.87 0.84 0.82 0.85 0.84 0.86 0.85\nHeLa-S3 0.82 0.70 0.66 0.79 0.71 0.76 0.75 0.77 0.82 0.78 0.74 0.78 0.76 0.80 0.78\nHepG2 0.85 0.79 0.78 0.84 0.80 0.83 0.82 0.83 0.86 0.84 0.81 0.84 0.82 0.85 0.84\nHMEC 0.80 0.71 0.68 0.77 0.72 0.76 0.74 0.76 0.79 0.76 0.73 0.77 0.75 0.78 0.77\nHSMM 0.84 0.72 0.65 0.80 0.71 0.75 0.74 0.77 0.83 0.78 0.73 0.79 0.76 0.81 0.80\nHSMMtube 0.83 0.74 0.69 0.80 0.73 0.77 0.76 0.78 0.84 0.79 0.74 0.79 0.77 0.82 0.80\nHUVEC 0.86 0.75 0.69 0.83 0.75 0.79 0.78 0.81 0.85 0.81 0.77 0.82 0.79 0.84 0.82\nK562 0.76 0.73 0.69 0.78 0.71 0.74 0.73 0.75 0.81 0.75 0.72 0.77 0.74 0.77 0.78\nLNCaP 0.74 0.71 0.67 0.75 0.68 0.70 0.69 0.71 0.77 0.73 0.68 0.73 0.70 0.76 0.74\nMCF-7 0.80 0.69 0.67 0.77 0.69 0.73 0.72 0.74 0.79 0.75 0.71 0.75 0.72 0.77 0.76\nNHEK 0.86 0.72 0.67 0.81 0.74 0.79 0.77 0.80 0.85 0.80 0.76 0.81 0.78 0.83 0.81\nTh1 0.77 0.75 0.74 0.78 0.76 0.77 0.76 0.77 0.78 0.78 0.76 0.78 0.77 0.78 0.78\nTable A10: Histone modification prediction performance per label.\nModifiation (label no.)\nBasset\nCNN\nA WD-LSTM\nResNet-LM\nNT-H\nNT-MS\nNT-1000G (2.5B)\nNT-V2\nDNABERT\nDNABERT-2\nGENA-LM BERT\nGENA-LM BigBird\nHyenaDNA tiny\nHyenaDNA large\nGROVER\nH3K27me3 K562 (0) 0.63 0.67 0.66 0.70 0.69 0.70 0.69 0.68 0.72 0.70 0.70 0.71 0.68 0.67 0.70\nH3K9ac K562 (1) 0.87 0.85 0.85 0.87 0.86 0.86 0.87 0.86 0.87 0.86 0.86 0.87 0.87 0.87 0.87\nH3K9me3 K562 (2) 0.74 0.77 0.75 0.84 0.83 0.83 0.83 0.78 0.83 0.84 0.86 0.86 0.79 0.80 0.82\nH3K4me1 K562 (3) 0.65 0.67 0.65 0.68 0.67 0.68 0.67 0.67 0.71 0.69 0.67 0.69 0.67 0.67 0.69\nH3K9ac K562 (4) 0.74 0.75 0.70 0.74 0.73 0.75 0.74 0.74 0.77 0.75 0.75 0.75 0.74 0.74 0.75\nH3K4me1 K562 (5) 0.80 0.80 0.80 0.81 0.81 0.81 0.81 0.80 0.82 0.81 0.80 0.81 0.81 0.81 0.81\nH3K36me3 K562 (6) 0.63 0.65 0.62 0.70 0.70 0.74 0.71 0.66 0.70 0.72 0.73 0.74 0.65 0.67 0.69\nH3K36me3 K562 (7) 0.75 0.77 0.75 0.78 0.77 0.79 0.77 0.77 0.78 0.78 0.78 0.79 0.76 0.77 0.77\nH4K20me1 K562 (8) 0.62 0.69 0.69 0.71 0.69 0.71 0.69 0.69 0.72 0.71 0.70 0.72 0.69 0.69 0.71\n34\nPublished as a conference paper at ICLR 2024\nH3K27me3 K562 (9) 0.74 0.74 0.75 0.80 0.79 0.80 0.80 0.79 0.80 0.79 0.80 0.80 0.78 0.77 0.80\nH3K4me3 K562 (10) 0.88 0.89 0.87 0.89 0.88 0.89 0.89 0.89 0.9 0.89 0.89 0.89 0.89 0.89 0.89\nH3K4me3 K562 (11) 0.89 0.89 0.87 0.89 0.89 0.89 0.89 0.89 0.9 0.89 0.89 0.89 0.89 0.89 0.89\nH3K4me3 K562 (12) 0.84 0.85 0.82 0.85 0.84 0.85 0.85 0.84 0.86 0.85 0.85 0.85 0.85 0.85 0.85\nH3K4me3 K562 (13) 0.76 0.77 0.72 0.77 0.75 0.77 0.76 0.76 0.80 0.77 0.77 0.78 0.76 0.75 0.77\nH3K79me2 K562 (14) 0.74 0.76 0.75 0.76 0.76 0.76 0.76 0.75 0.76 0.76 0.76 0.77 0.76 0.70 0.76\nH3K4me2 K562 (15) 0.70 0.72 0.67 0.71 0.70 0.72 0.71 0.71 0.75 0.73 0.71 0.72 0.70 0.70 0.72\nH3K27ac K562 (16) 0.70 0.72 0.67 0.71 0.70 0.72 0.70 0.71 0.76 0.73 0.71 0.72 0.71 0.71 0.72\nH2AFZ K562 (17) 0.70 0.71 0.67 0.72 0.71 0.73 0.71 0.71 0.75 0.73 0.73 0.73 0.70 0.70 0.72\nTable A11: CpG methylation prediction performance per cell line.\nModel SK-N-SH GM23248 A549 HepG2 HUES64 GM23248 HeLa-S3ENCFF567KCL ENCFF170XYJ ENCFF948WVD ENCFF690FNR ENCFF890GMD ENCFF840XVU ENCFF754RAW\nBasset 0.93 0.94 0.93 0.90 0.95 0.94 0.93CNN 0.84 0.84 0.84 0.82 0.93 0.84 0.83\nResNet-LM 0.86 0.87 0.86 0.85 0.94 0.87 0.86A WD-LSTM 0.80 0.80 0.80 0.78 0.89 0.80 0.79NT-H 0.87 0.87 0.87 0.85 0.94 0.87 0.87NT-MS 0.92 0.92 0.92 0.89 0.96 0.92 0.91NT-1000G (2.5B)0.88 0.88 0.88 0.86 0.94 0.88 0.87NT-V2 0.90 0.91 0.90 0.88 0.96 0.91 0.90DNABERT 0.91 0.91 0.91 0.88 0.96 0.91 0.90DNABERT-2 0.89 0.89 0.89 0.87 0.96 0.89 0.89GENA-LM BERT0.91 0.91 0.91 0.89 0.95 0.91 0.90GENA-LM BigBird0.90 0.91 0.90 0.88 0.95 0.91 0.90HyenaDNA tiny 0.85 0.85 0.85 0.83 0.92 0.85 0.84HyenaDNA large0.91 0.91 0.91 0.88 0.94 0.91 0.90GROVER 0.88 0.89 0.88 0.86 0.94 0.89 0.88\nTable A12: Variant effect prediction performance (AUROC) on the expression variant effect pre-\ndiction dataset, stratified by variant category. Categories that only have samples of one label were\nommitted as no AUC can be determined. For completeness, also AUROCs on categories with very\nlow sample numbers are reported, but should be interpreted with caution.\nModel\nIntron(n=60,072)\nIntergenic(n=23,218)\nUpstream gene(n=6,339)\nDownstream gene(n=4,581)\nRegulatory region(n=4,010)\nNoncoding transcriptexon (n=3,099)\n3’ UTR(n=2,025)\n5’ UTR(n=462)\nTF binding site(n=433)\nSplice region(n=129)\nSplice polypyrimidinetract (n=103)\nMissense(n=60)\nSplice donor region(n=31)\nSynonymous(n=24)\nSplice donor(n=15)\nSplice donor 5th base(n=13)\nStop lost(n=2)\nDeepSEA 0.70 0.69 0.71 0.71 0.71 0.68 0.64 0.72 0.64 0.55 0.62 0.72 0.46 0.54 0.35 0.83 1.00\nResNet-LM 0.55 0.54 0.56 0.55 0.52 0.51 0.54 0.44 0.46 0.44 0.54 0.49 0.69 0.70 0.50 0.501.00A WD-LSTM 0.53 0.54 0.52 0.55 0.56 0.53 0.51 0.51 0.51 0.48 0.51 0.44 0.31 0.28 0.38 0.37 0.00NT-H 0.55 0.54 0.54 0.55 0.52 0.51 0.49 0.43 0.44 0.51 0.33 0.57 0.36 0.71 0.50 0.671.00NT-MS 0.55 0.53 0.54 0.55 0.54 0.550.57 0.48 0.53 0.54 0.51 0.54 0.56 0.65 0.19 0.601.00NT-1000G-2.5B 0.44 0.43 0.43 0.44 0.48 0.46 0.48 0.44 0.47 0.42 0.40 0.44 0.39 0.54 0.27 0.211.00NT-1000G-500M0.49 0.48 0.49 0.47 0.50 0.53 0.50 0.45 0.51 0.51 0.48 0.40 0.66 0.29 0.46 0.33 0.00NT-V2-500M 0.48 0.47 0.46 0.48 0.50 0.48 0.50 0.41 0.510.58 0.54 0.34 0.51 0.680.77 0.40 0.00DNABERT 0.60 0.59 0.61 0.60 0.57 0.57 0.55 0.60 0.51 0.510.70 0.57 0.79 0.810.65 0.5 0.00DNABERT-2 0.49 0.49 0.47 0.49 0.53 0.48 0.480.52 0.570.49 0.47 0.55 0.78 0.59 0.35 0.521.00GENA-LM BERT0.49 0.49 0.50 0.50 0.54 0.51 0.51 0.49 0.55 0.51 0.47 0.53 0.27 0.29 0.58 0.60 0.00GENA-LM BigBird0.49 0.48 0.48 0.49 0.52 0.51 0.52 0.49 0.53 0.51 0.47 0.53 0.24 0.43 0.35 0.55 0.00HyenaDNA large0.51 0.52 0.50 0.52 0.53 0.51 0.50 0.49 0.48 0.47 0.54 0.49 0.37 0.26 0.19 0.33 0.00HyenaDNA medium (160k)0.48 0.49 0.47 0.50 0.52 0.49 0.49 0.46 0.46 0.49 0.53 0.51 0.34 0.23 0.19 0.33 0.00HyenaDNA medium (450k)0.50 0.51 0.49 0.52 0.54 0.50 0.50 0.47 0.45 0.48 0.53 0.54 0.39 0.26 0.19 0.38 0.00HyenaDNA small0.46 0.47 0.45 0.47 0.50 0.47 0.48 0.47 0.50 0.49 0.50 0.42 0.32 0.25 0.19 0.33 0.00HyenaDNA tiny 0.47 0.48 0.44 0.49 0.51 0.49 0.48 0.45 0.50 0.48 0.49 0.39 0.37 0.35 0.23 0.24 0.00GROVER 0.55 0.55 0.58 0.55 0.55 0.56 0.56 0.50 0.56 0.41 0.480.66 0.36 0.46 0.420.74 0.00\n35\nPublished as a conference paper at ICLR 2024\nTable A13: Variant effect prediction performance (AUROC) on the disease variant effect prediction\ndataset, stratified by variant category. Categories that only have samples of one label were ommitted\nas no AUC can be determined. For completeness, also AUCs on categories with very low sample\nnumbers are reported, but should be interpreted with caution.\nModel\nIntron(n=138,211)\nSplice region(n=40,360)\nSplice polypyrimidinetract (n=39,686)\nNoncoding transcriptexon (23,721)\n3’ UTR(n=20,441)\nSplice donor(n=10,811)\nSplice acceptor(n=9,280)\n5’ UTR(n=6,996)\nUpstream gene(n=2,264)\nSplice donor region(n=2,056)\nSplice donor 5th base(n=1,060)\nDownstream Gene(n=274)\nMature miRNA(n=40)\nIntergenic(n=20)\nDeepSEA 0.48 0.44 0.46 0.73 0.69 0.47 0.48 0.58 0.61 0.45 0.41 0.72 0.18 0.92\nResNet-LM 0.51 0.67 0.53 0.50 0.51 0.55 0.48 0.46 0.58 0.64 0.63 0.31 0.79 0.05A WD-LSTM 0.53 0.56 0.59 0.52 0.50 0.48 0.52 0.46 0.47 0.50 0.43 0.40 0.12 0.53NT-H 0.43 0.53 0.49 0.51 0.56 0.51 0.52 0.53 0.52 0.49 0.50 0.53 0.38 0.58NT-MS 0.62 0.70 0.65 0.57 0.55 0.74 0.61 0.57 0.56 0.76 0.76 0.44 0.82 0.63NT-1000G-2.5B 0.49 0.57 0.54 0.52 0.48 0.51 0.50 0.47 0.45 0.52 0.52 0.45 0.10 0.11NT-1000G-500M 0.46 0.53 0.50 0.49 0.40 0.51 0.49 0.47 0.41 0.47 0.49 0.36 0.03 0.63NT-V2-500M 0.50 0.52 0.49 0.50 0.36 0.49 0.53 0.52 0.46 0.50 0.43 0.54 0.33 0.21DNABERT 0.52 0.55 0.47 0.48 0.63 0.54 0.51 0.62 0.58 0.55 0.56 0.62 0.72 0.05DNABERT-2 0.48 0.46 0.53 0.54 0.49 0.50 0.52 0.45 0.51 0.45 0.52 0.51 0.92 0.95GENA-LM BERT 0.50 0.49 0.48 0.51 0.50 0.56 0.60 0.50 0.41 0.47 0.42 0.50 0.95 1.00GENA-LM BigBird0.48 0.48 0.46 0.51 0.52 0.54 0.60 0.46 0.45 0.48 0.43 0.60 1.00 0.89HyenaDNA large 0.53 0.52 0.59 0.52 0.48 0.44 0.52 0.48 0.48 0.48 0.42 0.40 0.00 0.63HyenaDNA medium 160k0.52 0.54 0.60 0.54 0.46 0.44 0.51 0.46 0.50 0.47 0.41 0.41 0.00 0.47HyenaDNA medium 450k0.53 0.53 0.58 0.51 0.46 0.45 0.50 0.51 0.50 0.47 0.42 0.35 0.00 0.37HyenaDNA small 0.53 0.54 0.59 0.53 0.47 0.44 0.49 0.46 0.53 0.48 0.41 0.44 0.10 0.42HyenaDNA tiny 0.53 0.55 0.59 0.53 0.47 0.44 0.52 0.48 0.50 0.48 0.42 0.44 0.08 0.37GROVER 0.50 0.46 0.42 0.48 0.54 0.49 0.53 0.55 0.52 0.49 0,45 0.42 0.21 0.21\nTable A14: Variant effect prediction performance on the disease variant effects prediction dataset\nwith more stringent filtering. Variants labeled as ”Likely” in ClinVar were omitted, yielding a\nreduced dataset (Benign n=100,623, Pathogenic n=8,188). Similarly to the results on the full\ndataset, NT-MS outperforms DeepSEA. Additionally, ResNet-LM and DNABERT show strong per-\nformance.\nModel AUC\nDeepSEA 0.57\nResNet-LM 0.61\nAWD-LSTM 0.45\nNT-H 0.52\nNT-MS 0.74\nNT-1000G-2.5B 0.49\nNT-1000G-500M 0.46\nNT-V2-500M 0.48\nDNABERT 0.62\nDNABERT2 0.50\nGENA-LM BERT 0.56\nGENA-LM BigBird 0.52\nHyenaDNA large 0.44\nHyenaDNA medium 160k 0.43\nHyenaDNA medium 450k 0.44\nHyenaDNA small 0.41\nHyenaDNA tiny 0.43\nGROVER 0.52\n36",
  "topic": "Annotation",
  "concepts": [
    {
      "name": "Annotation",
      "score": 0.6788159608840942
    },
    {
      "name": "Computer science",
      "score": 0.6784263849258423
    },
    {
      "name": "Blueprint",
      "score": 0.6129279732704163
    },
    {
      "name": "Benchmarking",
      "score": 0.5234229564666748
    },
    {
      "name": "DNA sequencing",
      "score": 0.5064128637313843
    },
    {
      "name": "Genome",
      "score": 0.503224790096283
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.4733336567878723
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46869754791259766
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4684078097343445
    },
    {
      "name": "Human genome",
      "score": 0.43367648124694824
    },
    {
      "name": "Genomics",
      "score": 0.41042372584342957
    },
    {
      "name": "Computational biology",
      "score": 0.404024213552475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34715837240219116
    },
    {
      "name": "DNA",
      "score": 0.30731499195098877
    },
    {
      "name": "Biology",
      "score": 0.22221797704696655
    },
    {
      "name": "Genetics",
      "score": 0.12849214673042297
    },
    {
      "name": "Gene",
      "score": 0.12280654907226562
    },
    {
      "name": "Engineering",
      "score": 0.08694732189178467
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106887733",
      "name": "Novozymes (Denmark)",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I189090001",
      "name": "Novo Nordisk (Denmark)",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I4210110242",
      "name": "Digital Science (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I96673099",
      "name": "Technical University of Denmark",
      "country": "DK"
    }
  ]
}