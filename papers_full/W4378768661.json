{
    "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
    "url": "https://openalex.org/W4378768661",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2355347475",
            "name": "Zhu, Xizhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2356137228",
            "name": "Chen Yuntao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1985400251",
            "name": "TIAN Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3212080755",
            "name": "Tao, Chenxin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3034916525",
            "name": "Su Weijie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2364803442",
            "name": "Yang Chenyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2152944217",
            "name": "Huang, Gao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1598248660",
            "name": "Li Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4295680286",
            "name": "Lu, Lewei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1832420164",
            "name": "Wang Xiao-gang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052563777",
            "name": "Qiao Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2091561640",
            "name": "Zhang, ZhaoXiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2345450207",
            "name": "Dai, Jifeng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3094075584",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W4319323461",
        "https://openalex.org/W4308716551",
        "https://openalex.org/W3185826690",
        "https://openalex.org/W3196337984",
        "https://openalex.org/W2964118262",
        "https://openalex.org/W4383108457",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3039636162",
        "https://openalex.org/W4360892158",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W3173930728",
        "https://openalex.org/W4361806741",
        "https://openalex.org/W4283460722",
        "https://openalex.org/W4224220194",
        "https://openalex.org/W3013028018",
        "https://openalex.org/W4220880510",
        "https://openalex.org/W4221154819",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4383097638",
        "https://openalex.org/W4386065365",
        "https://openalex.org/W4283218993",
        "https://openalex.org/W4366327559",
        "https://openalex.org/W4315706776"
    ],
    "abstract": "The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular \"ObtainDiamond\" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.",
    "full_text": "Ghost in the Minecraft: Generally Capable Agents for\nOpen-World Environments via Large Language\nModels with Text-based Knowledge and Memory\nXizhou Zhu1,2‚àó, Yuntao Chen3‚àó , Hao Tian2‚àó , Chenxin Tao1,2‚àó , Weijie Su2,4‚àó , Chenyu Yang1‚àó ,\nGao Huang1, Bin Li4, Lewei Lu2, Xiaogang Wang2,5, Yu Qiao6, Zhaoxiang Zhang7, Jifeng Dai1,6 \u0000\n1Tsinghua University 2SenseTime Research\n3Centre for Artificial Intelligence and Robotics, HKISI, CAS\n4University of Science and Technology of China\n5The Chinese University of Hong Kong 6Shanghai Artificial Intelligence Laboratory\n7Institute of Automation, Chinese Academy of Science (CASIA)\n{zhuxizhou,gaohuang,daijifeng}@tsinghua.edu.cn, chenyuntao08@gmail.com\ntianhao2@senseauto.com, {tcx20,yangcy19}@mails.tsinghua.edu.cn,\njackroos@mail.ustc.edu.cn, binli@ustc.edu.cn, luotto@sensetime.com\nxgwang@ee.cuhk.edu.hk, qiaoyu@pjlab.org.cn, zhaoxiang.zhang@ia.ac.cn\nAbstract\nThe captivating realm of Minecraft has attracted substantial research interest in\nrecent years, serving as a rich platform for developing intelligent agents capable of\nfunctioning in open-world environments. However, the current research landscape\npredominantly focuses on specific objectives, such as the popular \"ObtainDiamond\"\ntask, and has not yet shown effective generalization to a broader spectrum of\ntasks. Furthermore, the current leading success rate for the \"ObtainDiamond\"\ntask stands at around 20%, highlighting the limitations of Reinforcement Learning\n(RL) based controllers used in existing methods. To tackle these challenges, we\nintroduce Ghost in the Minecraft (GITM), a novel framework integrates Large\nLanguage Models (LLMs) with text-based knowledge and memory, aiming to\ncreate Generally Capable Agents (GCAs) in Minecraft. These agents, equipped\nwith the logic and common sense capabilities of LLMs, can skillfully navigate\ncomplex, sparse-reward environments with text-based interactions. We develop\na set of structured actions and leverage LLMs to generate action plans for the\nagents to execute. The resulting LLM-based agent markedly surpasses previous\nmethods, achieving a remarkable improvement of +47.5% in success rate on the\n\"ObtainDiamond\" task, demonstrating superior robustness compared to traditional\nRL-based controllers. Notably, our agent is the first to procure all items in the\nMinecraft Overworld technology tree, demonstrating its extensive capabilities.\nGITM does not need any GPU for training, but a single CPU node with 32 CPU\ncores is enough. This research shows the potential of LLMs in developing capable\nagents for handling long-horizon, complex tasks and adapting to uncertainties\nin open-world environments. See the project website at https://github.com/\nOpenGVLab/GITM.\n1 Introduction\n‚ÄúWhat if a cyber brain could possibly generate its own ghost, create a soul all by itself? And if it did,\njust what would be the importance of being human then?‚Äù\n‚Äî Ghost in the Shell (1995)\n‚àóEqual contribution. This work is done when Chenxin Tao and Weijie Su are interns at SenseTime Research.\n\u0000 Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.\narXiv:2305.17144v2  [cs.AI]  1 Jun 2023\nFigure 1: Our GITM unlocks the entire technology tree by obtaining all items in Minecraft\nOverworld. Each node represents an individual item in Minecraft. The directed edges between nodes\nrepresent prerequisite relationships for obtaining items. For better readability, we manually merge\nsome similar nodes, e.g., ‚Äúwooden_pickaxe‚Äù, ‚Äúwooden_axe‚Äù, ‚Äúwooden_hoe‚Äù, and ‚Äôwooden_shovel‚Äô\nare merged into one node, and ‚Äúwooden_pickaxe‚Äù is selected to represent the merged node. Existing\nMinecraft agents [ 2, 7, 25] only unlocked 78 / 262 = 30% items, while our GITM successfully\nunlocked all items.\ngoal\ngoal\nRL Agent\nkeyboard & mouse\nobservation\naction list\nfeedback observation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft / smelt\ndig_down\n‚Ä¶\nRL-based method\nOurs\ngoal LLM\nDecomposer\nLLM Planner structured actions\naction list\nfeedback observation\nkeyboard & mouse\nEnvironment\nexplore\nmine\ncraft / smelt\ndig_down\n‚Ä¶\nGhost In the Minecraft (GITM)\nsub-goal tree\nLLM Interface\nFigure 2: Comparison between RL-based method and our GITM. RL agents try to map an\ncomplex goal directly to a sequence of low-level control signals, while our GITM leverages LLM to\nbreak down the goals and map them to structured actions for final control signals.\nMinecraft, as the world‚Äôs best-selling game, boasts over 238 million copies sold and more than\n140 million peak monthly active users [27]. Within the game, hundreds of millions of players have\nexperienced a digital second life by surviving, exploring and creating, closely resembling the human\nworld in many aspects. Given its massive scale, vast success, and unrestricted freedom, Minecraft\nhas established itself as an unparalleled platform for researching autonomous and robust Generally\nCapable Agents (GCAs) [23] in open-world environments brimmed with long-horizon challenges,\nenvironmental disruptions, and uncertainties.\nMinecraft acts as a microcosm of the real world. Developing an automated agent that can mas-\nter all technical challenges in Minecraft is akin to creating an artificial intelligence capable of\nautonomously learning and mastering the entire real-world technology. However, existing re-\nsearches [ 2, 7, 25] remain narrowly scoped. Prior studies have predominantly focused on the\nspecific goal of ObtainDiamond [18]. Yet, in the process of obtaining diamonds, the number of\ntypes of items involved only accounts for <5% of the entire Minecraft world. ObtainDiamond only\nrequires specialized skills in a specific domain, while obtaining all items in Minecraft demonstrates a\nwide range of knowledge and capabilities, similar to mastering multidisciplinary fields in the real\nworld. As illustrated in Fig. 1, our work endeavors to obtain all items in Minecraft within a reasonable\ncomputation budget. This achievement stands as a significant milestone in the development of GCAs,\nillustrating the potential of intelligent agents to match human performance in terms of versatility and\nadaptability.\nAlthough reinforcement learning (RL) [16] is the most popular paradigm for approaching GCAs, it\nhas shown some staggering limitations in conquering Minecraft. RL-based agents typically require\na vast number of learning steps ( e.g., nearly 30 million steps to obtain diamonds as reported in\nDreamerV3 [7]) and exhibit poor scalability when learning new tasks(e.g., VPT [2] uses different\nagents for world exploration and diamond mining). As a consequence, adopting RL-based agents for\n2\ncompleting a wide range of tasks may require an prohibitively high number of training steps, making\nit impractical to obtain all items in Minecraft. This inefficiency and lack of adaptability have hindered\nthe development of generally capable agents in open-world environments.\nAs shown in Fig. 2, the biggest dilemma of previous RL-based agents is how to map an extremely\nlong-horizon and complex goal to a sequence of lowest-level keyboard/mouse operations. To address\nthis challenge, we propose our framework Ghost In the Minecraft (GITM) 2, which uses Large\nLanguage Model (LLM)-based agents as a new paradigm. Instead of direct mapping like RL agents,\nour LLM-based agents employ a hierarchical approach. It first breaks down the decompose goal\ninto sub-goals, then into structured actions, and finally into keyboard/mouse operations. Such\ndecomposition is similar to how humans solve complex problems in the real world, enabling mastery\nof Minecraft with efficiency orders of magnitude higher than that of RL. LLM can also leverage\ntext-based knowledge and memory to quickly acquire the ability to interact with the environment\nand accomplish goals, offering immense learning efficiency improvements, unlimited scalability and\nrepresenting a disruptive innovation compared with RL. Our GITM framework has the potential to\nrevolutionize the path to generally capable agents.\nSpecifically, the proposed LLM-based agent consists of an LLM Decomposer, an LLM Planner, and\nan LLM Interface, which are responsible for the decomposition of sub-goals, structured actions,\nand keyboard/mouse operations, respectively. Given a goal in Minecraft, LLM Decomposer first\ndecomposes it into a series of well-defined sub-goals according to the text-based knowledge collected\nfrom the Internet. Then, LLM Planner plans a sequence of structured actions for each sub-goal. The\nstructured actions are defined with clear semantics and corresponding feedback, enabling LLMs to\nunderstand surrounding environments and make decisions at the cognitive level. LLM Planner also\nrecords and summarizes successful action lists into a text-based memory to enhance future planning.\nFinally, LLM Interface execute the structured actions to interact with the environment by processing\nraw keyboard/mouse input and receiving raw observations.\nIn this paper, we demonstrate the feasibility of employing Large Language Models (LLMs) to develop\nGenerally Capable Agents (GCAs) within an open-world environment built from Minecraft. By\nexploiting the common sense and reasoning capabilities of LLMs for hierarchical goal decomposition,\nas well as utilizing text-based knowledge and memory, this paradigm shows the possibility of\nenabling agents to address a wide range of challenges within Minecraft and allowing them to\neffectively handle such open-world environment. Consequently, our agent has surpassed all previous\nmethods in achieving the ObtainDiamond goal (+47.5% success rate). Our agent also demonstrates\nsuperior learning efficiency compared to previous methods, reducing the number of environment\ninteraction steps by more than 10,000√ó. Specifically, VPT [2] needs to be trained for 6,480 GPU days,\nDreamerV3 [7] needs to be trained for 17 GPU days, while our GITM does not require any GPUs\nand can be trained in just 2 days using a single CPU node with 32 CPU cores. More importantly, by\nobtaining all items in Minecraft Overworld as a milestone, this work represents a crucial first step\ntowards achieving GCAs that can handle any task humans can accomplish in Minecraft.\n2 Related Work\nMinecraft agents are intelligent programs that can perform various tasks within Minecraft world.\nReinforcement learning has dominated this area for many years. Some initial attempts have tried\nto use hierarchical RL [ 14, 15, 22] or imitation learning [ 1] in MineRL competitions [ 6, 10, 17].\nRecently, with large-scale web data, VPT [2] builds a foundation model for Minecraft by learning\nfrom videos. Based on its success, many works [ 18] have also explored to finetune foundation\nmodel with human feedback. On the other hand, as Minecraft agents become increasingly proficient\nin handling simple tasks, the importance of multi-task learning becomes more prominent. Some\nprevious works have adopted knowledge distillation [24] and curriculum learning [11], while recent\nworks [3, 5] tried to construct a language-conditioned multi-task agent via feeding the goal description\nembedding into the model.\nRecently, researchers have come to aware the extraordinary general planning ability for LLMs [8].\nMany works [ 9, 25, 28] have leveraged LLMs for enhancing the high-level planning ability of\nminecraft agents. Inner Monologue [ 9] leveraged environment feedback to improve the planning\nability of LLM. DEPS [25] further extended this closed-loop interaction by introducing description,\n2The name is chosen to pay tribute to the science fiction movie \"Ghost in the Shell\".\n3\ngoal LLM\nPlanner\nstructured actions\naction list\nfeedback observation\nkeyboard & mouse\nenvironment\nexplore\nmine\ncraft / smelt\ndig_down\n‚Ä¶\ntext-based\nknowledge\ntext-based\nmemory\n(Object, Count, Material, Tool, Info)\ngoal format\nLLM\nDecomposer\nequip, explore, approach, mine, \nattack, dig_down, go_up, build,\ncraft, smelt, apply, place\nstructured action set\nsub-goal tree\nupdate update\nLLM Interface\nFigure 3: Overview of our GITM. Given a Minecraft goal, the LLM Decomposer divides the goal\ninto a sub-goal tree. The LLM Planner then plans an action sequence for each sub-goal. Finally,\nthe LLM Interface executes each action in the environment. Our LLM-based agents can be further\nenhanced by leveraging text-based knowledge and memory.\nexplainer and selector. Plan4MC [ 28] pre-defined basic skills and instructed LLM to extract the\nrelationship between skills to construct a skill graph.\nUnlike previous RL-based or RL with LLM methods, our LLM-native approach brings the minecraft\nagent to another level both in efficiency and robustness by leveraging high-level action abstraction\nand text-based knowledge and memory.\nLarge Language Models with Tools Extending the ability of LLMs by leveraging external tools\nhave drawn a lot of attentions recently. Several works [4, 13, 21] have explored to augment LLMs\nwith robot perception and control abilities. Code as Polices[ 13] tried to prompt LLM to generate\ncodes that can drive robots. PaLM-E [4] unified robot perception, instruction following, task planning\nand low-level control into a unified framework. Another line of works tries to build external plugins\naround LLMs to enhance its ability. Toolformer [19] tries to teach LLMs to choose and use a wide\nrange of tools like calculator and search engines and incorporate the results from tools into text\ngeneration. HuggingGPT [20] builds an agent for leveraging a combination of vision, language and\naudio models hosted on HuggingFace for completing user request. API Bank [12] proposes a syntheic\nbenchmark suite for evaluating the how good LLMs are for using external tools.\nCompared with these tool-augmented LLMs, our agents are tasks for much more complex goals in a\nhigh uncertain open-world.\n3 Method\nTraditional RL-based agents struggle to develop generally capable agents in Minecraft. The core issue\nis that they attempt to map extremely long-horizon and complex goals directly to the lowest-level\nkeyboard and mouse operations. To overcome this, we propose LLM-based agents in Fig. 2 that utilize\nhierarchical goal decomposition. LLM Decomposer, LLM Planner, and LLM Interface are introduced\nto progressively decompose the task goal into sub-goals, structured actions, and keyboard/mouse\noperations. Moreover, LLM-based agents can leverage text-based knowledge and memory to quickly\nacquire the skills needed to master Minecraft.\n3.1 LLM Decomposer\nRather than directly assigning the task goal to the agent and expecting a comprehensive and robust\naction plan, this work suggests the more practical strategy of decomposing the task goal into a\nseries of more achievable sub-goals. By addressing each constituent sub-goal, the task goal can\nbe progressively achieved. To this end, an LLM Decomposer is proposed. Goals are fed to the\ndecomposer and recursively decomposed into a sub-goal tree. Text-base knowledge provides the\nnecessary information for decomposition.\nGoal Format. Since we aim to obtain all items in Minecraft, all goals can be defined in the format of\n(Object, Count, Material, Tool, Info), (1)\nwhere ‚ÄúObject‚Äù denotes the target item, ‚ÄúCount‚Äù specifies the target quantity. ‚ÄúMaterial‚Äù and ‚ÄúTool‚Äù\nrefer to prerequisites needed to obtain the target item. ‚ÄúInfo‚Äù stores the text-based knowledge related\nto this goal. Given a specific goal, its sentence embedding extracted from a pre-trained LLM is used\n4\nAction Interface:\nùëíùë•ùëùùëôùëúùëüùëí(ùëúùëèùëóùëíùëêùë°)\nùëöùëñùëõùëí(ùëúùëèùëóùëíùëêùë°,ùë°ùëúùëúùëô)\nùëëùëñùëî_ùëëùëúùë§ùëõ(ùë¶_ùëôùëíùë£ùëíùëô,ùë°ùëúùëúùëô)\n...\nQuery Illustration:\nQuery contains a goal, feedback from the \nagent, and reference plan from the memory\nResponse Format:\n{\n‚Äúexplanation‚Äù: ‚Äúexplain action failure‚Äù,\n‚Äúthoughts‚Äù: ‚Äúthoughts on your plan‚Äù,\n‚Äúaction_list‚Äù: [ ‚Ä¶ ]\n}\nInteraction Guideline:\nYou will receive feedback messages. If \nyour action failed, please replan ‚Ä¶\nInstruction\nQuery\nGoal: obtain 1 diamond given iron pickaxe.\nExternal info: Diamond is usually\nfound at level 12 ‚Ä¶\nFeedback\nReference plan\nPlan\n{\n‚Äúexplanation‚Äù: ‚ÄúThe last action failed because diamond cannot be found on the ground.‚Äù,\n‚Äúthoughts‚Äù: ‚ÄúFirst I need to dig down to level 12 where diamond is usually found. \nThen I can explore diamond ore underground with branching strategy.\nOnce I find diamond ore, I can mine it with the tool iron pickaxe.‚Äù\n‚Äúaction_list‚Äù: [\n{‚Äúname‚Äù: ‚Äúdig_down‚Äù, ‚Äúargs‚Äù: {‚Äúy_level‚Äù: ‚Äú12‚Äù, ‚Äútool‚Äù: ‚Äúiron_pickaxe‚Äù}},\n{‚Äúname‚Äù: ‚Äúexplore‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù, ‚Äústrategy‚Äù: ‚Äúbranching‚Äù}},\n{‚Äúname‚Äù: ‚Äúmine‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù, ‚Äútool‚Äù: ‚Äúiron_pickaxe‚Äù}\n]\n}\nLarge \nLanguage \nModel\nFeedback\nThe action {‚Äúname‚Äù: ‚Äúmine‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù, ‚Äútool‚Äù: ‚Äúiron_pickaxe‚Äù} failed\nError message: the target object ‚Äúdiamond_ore‚Äù is too far\n- inventory: {‚Äòiron_pickaxe‚Äô: 1, ‚Ä¶}\n- environment: {‚Äòbiome‚Äô: forest, ‚Äòy_level‚Äô: 12}\nAgent\nMemory\nGoal object: diamond\nReference plan:\n[\n{‚Äúname‚Äù: ‚Äúdig_down‚Äù, ‚Äúargs‚Äù: {‚Äúy_level‚Äù: ‚Äú12‚Äù, ‚Äútool‚Äù: ‚Äúiron_pickaxe‚Äù}},\n{‚Äúname‚Äù: ‚Äúexplore‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù, ‚Äústrategy‚Äù: ‚Äúbranching‚Äù}},\n{‚Äúname‚Äù: ‚Äúapproach‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù}},\n{‚Äúname‚Äù: ‚Äúmine‚Äù, ‚Äúargs‚Äù: {‚Äúobject‚Äù: ‚Äúdiamond_ore‚Äù, ‚Äútool‚Äù: ‚Äúiron_pickaxe‚Äù}\n]\nFigure 4: Illustration of our planning process with the LLM Planner and the agent in the loop.\nGiven a specific goal, the planner generates plans with structured actions under the guidance of\ninstruction, user query, previous feedback, and reference plan from memory. The agent executes the\nactions and provides feedback for the following planning.\nto retrieve the most relevant text-based knowledge from an external knowledge base. Then, the LLM\nidentifies the required material, tools, and related information from the gathered knowledge. The\ncomplete instructions for the LLM are described in Appendix.\nRecursive Decomposition. This goal format enables recursive decomposition of each goal into a sub-\ngoal tree. Specifically, given a goal, all prerequisite items are listed as sub-goals, including materials,\ntools, and their corresponding quantities. Then, the recursive decomposition continues for each\nsub-goal until it has no prerequisites. After the decomposition is completed, the execution sequence\nof the sub-goals is planned through post-order traversal. Such goal decomposition significantly\nenhances the success rate of LLM planning, especially for goals necessitating long-horizon planning.\nText-based Knowledge. External knowledge is essential for automatic goal decomposition. We\nbuild an external knowledge base documented in text from the Minecraft Wiki on the Internet3 and\nthe item crafting/smelting recipes, providing an exhaustive source of knowledge about the Minecraft\nworld. For instance, if we need to craft a wooden pickaxe, the item crafting recipe will indicate that\nthe required materials are three planks and two sticks, and the necessary tool is a crafting table. It also\nprovides information about the distribution of raw materials. For example, diamonds are frequently\nfound in levels 10‚àº12 underground.\n3.2 LLM Planner\nLLMs excel at language understanding and reasoning but struggle with low-level control and mul-\ntimodal perception. To leverage LLMs‚Äô strengths while addressing their limitations, we develop\nstructured actions and feedback mechanisms as an abstract interface for them to manage agent-\nenvironment interaction. We propose an LLM-based Planner to achieve goals in Minecraft. Given a\ngoal, it generates structured actions to control agents, receives feedback, and revises plans accordingly.\nIt also has a text memory that aids planning by providing solutions for frequent goals.\nStructured Actions. The structured actions are designed with well-defined functions and clear\nsemantics, enabling LLMs to make decisions at the cognitive level. A structured action can be defined\nas follows:\n(Name, Arguments, Description), (2)\n3https://minecraft-archive.fandom.com/wiki/Minecraft_Wiki\n5\nTable 1: Examples of structured actions. A structured action contains name and arguments for\nexecution, as well as description to help LLMs understand and decide when to choose this action.\nName Arguments Description\nequip object Equip the object from the inventory: used to equip equipment, including tools, weapons, and armor.explore object, strategyMove around to find the object: used to find objects including block items and entities on the ground.approach object Move close to a visible object: used to approach the object you want to attack or mine.mine/attackobject, tool Attack / Mine the object with the tool: used to attack / mine the object within reach.dig_down/go_upylevel, tool Dig down / Go up with the tool: used to go down / up underground.build blueprint Build according to a blueprint: used to place corresponding objects on locations according to a preset blueprint.craft/smeltobject, tool, materialCraft / Smelt the object with the materials and tool: used to craft new object that is not in the inventory or is not enough.apply/placeobject, tool Apply / Place the tool on the object: used to apply tools or place blocks.\nThe name and arguments defines the action we want the agent to execute, while the action description\nprovides enough information for letting LLMs know when to choose the corresponding actions, as\nshown in Tab. 1.\nWe extract the set of structured actions by leveraging the powerful reasoning capability of LLMs.\nSpecifically, a pre-trained LLM is utilized to decompose the 3141 predefined tasks provided by\nMineDojo [5] into action sequences. Instructions for guiding LLMs on action decomposition are\nprovided in Appendix. Then, we extract the structured actions by selecting frequent actions and\nmerging actions with similar functionalities. See Appendix for the set of structured actions.\nFeedback Mechanism. Open-loop planning cannot guarantee success, especially in open-world\nenvironments, where agents might encounter unexpected events. Feedback is crucial to form an effec-\ntive closed loop. Without appropriate feedback, the LLM has no information about the consequences\nof actions and may repeat failed action plans. Feedback message is designed to present the agent‚Äôs\ncurrent state in the environment (i.e., inventory and environment), as well as the success and failure\ninformation for each executed actions, as shown in Fig. 4. By incorporating this feedback message,\nthe LLM can update its understanding of the environment, refine their strategies, and adapt their\nbehavior accordingly.\nPlanning. Once the abstract interface is prepared, a pre-trained LLM is queried to generate goal-\nspecific action sequence. This is achieved through carefully designed instructions and user queries,\nenabling the LLM to efficiently create and revise the plans. Fig. 4 illustrates the planning process.\nSee Appendix for the full description.\nInstruction specifies the guidelines that LLMs must follow when planning, including 1) Action\nInterface provides functional descriptions of the structured actions and their parameters; 2) Query\nIllustration clarifies the structure and meaning of user queries; 3) Response Format requires LLM to\nreturn responses in the format of {Explanation, Thought, Action List}, where ‚ÄúExplanation‚Äù requires\nLLMs to explain the reason for action failure, ‚ÄúThought‚Äù requires LLM to use natural language to\nplan before outputting action sequences as a chain-of-thought (CoT) mechanism [26], and ‚ÄúAction\nList‚Äù outputs a list of structured actions to be executed; 4) Interaction Guideline guides LLMs to\ncorrect failed actions based on the feedback message, thus enabling the LLM to revise the plan.\nUser Query provides the specific query to LLMs for a given goal, including 1) Goal represents the\nobjective by text as ‚ÄúObtain Count Item, given Material and Tool. Extra info: Info‚Äù according to\nEq. (1); 2) Feedback is the feedback information of the abstract interface; 3) Reference Plan provides\na common reference plan for the current goal retrieved from the text-base memory.\nText-based Memory is designed for LLM to maintain common reference plans for each encountered\nobjective as experiential knowledge. LLMs acquire the experience about controlling agents and\nresolving specific situations through game play and agent interaction. Instead of starting from scratch\nevery time, using prior experience allows LLMs to handle tasks more efficiently, a process similar to\nhuman skill improvement through practice.\nTo this end, we design a text-based memory mechanism for LLM to store and retrieve gained knowl-\nedge. Unlike the RL-based model, which stores knowledge in parameters, this textual knowledge is\nexplicit, logical, and closely aligned with human thought processes. This allows for direct application\nto a wide range of similar tasks, leading to more efficient learning and improved generalization.\nSpecifically, during each game episode, once the goal is achieved, the entirely executed action list\nwould be stored in memory. The LLM may achieve the same goal under various circumstances,\nresulting in a range of different plans. To identify a common reference plan suitable for general\nsituations, essential actions from multiple plans are summarized. This summarization process is\n6\nacacia_boatacacia_dooracacia_fenceacacia_fence_gateacacia_stairs\nbeefbirch_boatbirch_doorbirch_fencebirch_fence_gatebirch_stairs\nboatbonebowlchestchickencobblestonecobblestone_wallcooked_beefcooked_chickencooked_muttoncooked_porkchopcrafting_tabledark_oak_boatdark_oak_doordark_oak_fencedark_oak_fence_gatedark_oak_stairs\ndirtdouble_plant\nfencefence_gate\nfurnaceglassglass_bottleglass_pane\nladderleverlogmuttonoak_stairs\npaperplanksporkchopred_flower\nreedssandsandstonesaplingsignspruce_boatspruce_doorspruce_fencespruce_fence_gatespruce_stairs\nstickstonestone_axe\nstone_brick_stairsstone_buttonstone_hoestone_pickaxestone_pressure_platestone_shovelstone_stairsstone_sword\nstonebricksugartallgrasstrapdoorwheatwheat_seedswooden_axewooden_buttonwooden_doorwooden_hoewooden_pickaxewooden_pressure_platewooden_shovelwooden_slabwooden_swordyellow_flowerbone_mealarmor_stand\nbookbreadcoaliron_ingotiron_nuggetiron_oreiron_shovelitem_frame\nleatherrotten_flesh\nshieldspider_eyestone_slab\ntorchtrapped_chesttripwire_hook\nfireworksgunpowdersnowsnow_layer\nsnowballcarpetgrassheavy_weighted_pressure_plateiron_hoeiron_swordleather_boots\nleavespaintingshearsstringwoolcoal_blockleather_helmetbrown_mushroombrown_mushroom_block\nflintbedbuckethay_blockiron_axeiron_pickaxemilk_bucketsandstone_stairswater_bucketfermented_spider_eyeleather_leggings\ngravelflint_and_steelfishing_rodiron_bootsiron_trapdoorleather_chestplatemossy_cobblestone\nvinewaterlilybone_block\nbowchest_minecartfurnace_minecart\nhopperiron_helmet\nminecartdiamonddiamond_shovel\ndropperjukeboxnoteblockredstoneredstone_torch\nbannerfeatheriron_barsiron_door\nrailbrickclay_balllapis_lazuli\npistonemeraldcauldroniron_leggings\ntntdiamond_hoediamond_swordflower_potiron_chestplate\narrowcompasshopper_minecartgold_ingotgold_nuggetgold_oregolden_shoveliron_blockbrick_block\nclayhardened_clayslime_ball\ndispenserdiamond_axediamond_pickaxelava_bucketactivator_raildetector_rail\neggrepeatertnt_minecart\nbookshelfgolden_hoegolden_swordlight_weighted_pressure_plateredstone_blockred_mushroomred_mushroom_block\nbeetrootbeetroot_seedsdiamond_bootsdiamond_helmetgolden_axegolden_pickaxesticky_pistonink_sacdiamond_leggingsgolden_bootsmushroom_stew\nmapbeetroot_soup\nleadgolden_helmetrabbit_hidecooked_rabbit\nrabbitbrick_stairs\ncakeobsidiancactusdiamond_chestplate\nclockdeadbushwritable_booklapis_blockgolden_leggingsgolden_railbaked_potato\npotatodiamond_blockgolden_chestplateemerald_block\ncarrotpumpkinpumpkin_seedsjungle_boatjungle_doorjungle_fencejungle_fence_gatejungle_stairslit_pumpkincarrot_on_a_stick\nmelonmelon_blockmelon_seedsgolden_carrotgold_blockpumpkin_piered_sandstonestone_slab2red_sandstone_stairsspeckled_melonenchanting_table\nappleanvilenchanted_bookpoisonous_potatorabbit_foot\nslimegolden_applerabbit_stew\n0\n20\n40\n60\n80\n100Success Rate (%)\nOurs\nDEPS\nDreamerV3\nVPT\nFigure 5: Success rate for all items in the entire Minecraft Overworld Technology Tree. The x\naxis lists all item names. We overlay the results from our GITM and the best results from baselines.\nalso implemented using LLMs (see Appendix for details). When encountering similar goals, the\nLLM creates new plans based on the summarized reference plans retrieved from memory. Successful\naction sequences from these new plans are also added to memory for future summarization. As the\nLLM-based Planner accumulates summaries, it becomes increasingly effective.\n3.3 LLM Interface\nUnlike the existing RL-based agents that directly control keyboard and mouse, LLM-based agents\ninteract with the environment through structured actions and feedback messages. The LLM interface\nserves to implement structured actions as keyboard/mouse operations, and extract observations\nprovided by the environment into feedback messages.\nStructured actions can be implemented in various ways such as hand-written scripts or RL-learned\nmodels. While RL-learned models have been employed in Minecraft previously, they were either\nbroad in functionality but inefficient in practice, or too specific in functionality, limiting their\napplicability to general tasks and actions. Clarifying the capability boundary of RL-learned models is\nchallenging. Instead, in this work, we choose to implement structured actions using hand-written\nscripts. Since structured actions are well-defined and easy to implement, we can manually implement\nthem based on observations ( e.g., location, LiDAR, and voxel) and basic operations ( e.g., move,\njump, adjust camera angle, click left mouse button, and click right mouse button) provided by the\nMineDojo [5] environment. See Appendix for details.\nFeedback messages can be obtained directly from the environment. These include whether the\nstructured action execution succeeded or failed. If the execution fails, the reason for the failure is\nadditionally notified. It also includes the current state of the agent in the environment, including the\nitems in the inventory, the current biome and depth, etc. See Appendix for details.\n4 Experiments\nTask Definition and Metrics. We measure the ability of GITM through item collection tasks. We\nonly collect items could be found in the Overworld. We exclude items could only be obtained by\ntrading with villagers, opening treasure chest or find a special structure on the map, using a tool\nenchanted with Silk Touch. This give us a total of 262 tasks. For the assessment of our agent, we\nemploy ‚ÄúCoverage of the Overworld Technology Tree‚Äù and‚ÄúSuccess Rate‚Äù as evaluation metrics.\n7\n4.1 Main Result\nUnlocking the Entire Technology Tree by Obtaining All Items. Compared with existing Minecraft\nagents [2, 7, 25] which mostly focuses on solving the ObtainDiamond task and could only unlock\na limited part of the full technology tree (13/262 for Dreamerv3, 15/262 VPT, 69/262 for DEPS),\nour approach could collect all 262 items as shown in Fig. 1. There are two major blockers for\nexisting methods. For RL-based methods like VPT [2] and DreamerV3 [7], the goal item(diamond)\nis hard-coded into the model weights, which means there are no easy way to re-task the trained\nRL agents for collecting other items in the inference stage. Moreover, the low training efficiency\nhinders them from solving extremely long-horizon tasks (e.g., obtaining a ‚Äúenchanted_book‚Äù). For\nmethods like DEPS [25] that use an RL controller [3] and LLM planner still rely on pre-trained RL\nagents to execute specific subtasks (e.g. mining 1 ‚Äúcobblestone‚Äù) in the generated plan. So these\napproaches still suffer from the inability of RL-based methods alone to generalize to unseen tasks\n(e.g. obtaining ‚Äúlapis_lazuli‚Äù). In contrast, we extract a well-defined set of structured actions by\nusing LLMs to decompose over 3000 predefined MineDojo tasks. This provides broad, open-world\nMinecraft capability. Combined with LLM planning, it enables solving more complex tasks than\nObtainDiamond - which RL cannot achieve. Our knowledge bases also improve efficiency. To our\nknowledge, we present the first agent to unlock the entire Overworld technology tree - a level of\nopen-world skill RL-based methods have not demonstrated.\nSuccess Rate for the Entire Technology Tree.We show the success rate of our method for collecting\nall Overworld items in Fig. 5. Our methods could achieve 100% success rate for simple tasks like\ncollecting wooden tools. It achieves non-zero success rates for all items which indicates a strong\ncollecting capability. The successful rate for collecting different items change smoothly for our agent,\nwhich showcase the robustness of our method against the highly uncertain open world environment.\n4.2 Comparison with Other Minecraft Agents\nTable 2: Comparison of our GITM with pre-\nvious methods on ObtainDiamond challenge.\nMethod Success Rate (%)\nDreamerV3 - 50.0 3.0 0.01 0.01\nDEPS 90.0 80.0 73.3 10.0 0.6\nVPT 100.0 100.0 100.0 85.0 20.0\nOur GITM 100.0 100.0 100.0 95.0 67.5\n101 103 105 107 109\nStep\n0\n10\n20\n30\n40\n50\n60\n70Success Rate\nVPT\nDreamV3\nOurs\nFigure 6: Comparison of learning efficiency.\nWe compared our LLM-based method with three existing agents: VPT [ 2], DreamerV3 [7], and\nDEPS [25] on the well known ObtainDiamond challenge, i.e, obtaining a diamond from scratch in\nMinecraft. Previous methods set different time limits of a single episode of game play (20 minutes\nfor VPT, 30 minutes for Dreamerv3, and 10 minutes for DEPS). For fair comparison, we use the\nstrictest limit of previous methods: 10 minutes (12,000 steps at 20Hz control).\nSuccess Rate for Obtaining Diamond and Other Items. Since VPT and Dreamerv3 are not\ntargeted for collecting items other than diamond, we mainly compare our method with DEPS for\nitems not related to obtain diamonds. Overall, our GITM and VPT rank task difficulty similarly, but\nDEPS rankings severely fluctuate for tasks more complex than mining coal. Dreamerv3 also behaves\noddly by having an abnormally low success rate on tasks like obtaining a stone sword. As shown in\nFig. 2, most agents performs generally well for easy tasks relating to make wooden tools. VPT could\neven rival with our GITM for the success rate of obtaining iron axes. But for obtaining diamonds, our\nmethod wins over any other methods by 3.5 times on the succeess rate.\nThis giant improvement comes from the following two aspects: First, we employ the strong long-term\nplanning capability of LLMs to decompose the complex tasks into feasible sub-goals and tackle\nthem step by step. Second, our model can directly leverage external knowledge such as the suitable\nlocations for mining ores, while RL models need to explore themselves and may not acquire reliable\nknowledge.\n8\nTable 3: Ablation study. The milestone items from left to right are crafting table\n , wooden\npickaxe\n , stone pickaxe\n , iron pickaxe\n , and diamond\n . The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). ‚ÄúGoal Decomp.‚Äù and\n‚ÄúExternal Info.‚Äù indicates goal decomposition and external knowledge respectively.\nGoal\nDecomp. Feedback External\nInfo. Memory Success Rate (%)\n57.5 32.5 5.0 0.0 0.0\n‚úì 90.0 90.0 67.5 2.5 0.0\n‚úì ‚úì 97.5 95.0 77.5 20.0 5.0\n‚úì ‚úì ‚úì 100.0 100.0 100.0 57.5 35.0\n‚úì ‚úì ‚úì ‚úì 100.0 100.0 100.0 95.0 67.5\nLearning Efficiency. Besides measuring the success rate of each agents, we also compare the\nlearning efficiency of our model with other learnable models. Since DEPS uses a LLM-based planner\nwithout learning mechanism and a pre-trained RL-based controller, its performance could not improve\nwith more episodes played and is excluded from the comparison here.\nIt usually takes tens of millions of steps to train an RL agent by updating parameters before its success\nrate starts to converges to meaningful non-zero numbers. However, the success rate for RL-based\nagents increases rather slowly even after them starts to converge. On the contrary, the learning process\nof our LLM-based agent is considerably faster. As shown in Fig. 6, our method requires several orders\nless episodes than any other methods before doubling its initial success rate. Moreover, our method is\nextremely sample efficient as our success rate raises from 35% to 47.5% by learning from the first five\nthousand steps. By just playing each task several times and summarize successful experience into the\nmemory, the LLM-based agent can acquire explicit experiential knowledge and achieve significantly\nhigher success rate.\n4.3 Ablation Study\nWe conduct ablation experiments on the ObtainDiamond task. We set a time limit of 10 minutes of\ngame play (12000 steps at the control frequency of 20Hz). When leveraging goal decomposition, for\neach sub-goal, we set the maximum number of queries to LLM as 30, and exceeding the query limit\nwill be judged as a failure. For each setting, we run 40 games and calculate the success rate. Tab. 3\nrecords the success rates of achieving the final goal diamond as well as the milestones in this goal,\nincluding crafting table, wooden pickaxe, stone pickaxe, and iron pickaxe.\nGoal Decomposition. Without goal decomposition, the planner can only accomplish several short-\nterm tasks such as obtaining stone axes with rather low success rate of 5%, which indicates the\nnecessity of goal decomposition. Leveraging the powerful long-term planning capabilities of LLMs,\nthe goals are decomposed into sub-goals feasible and practical for the planner, so the success rate for\nobtaining stone axes advances from 5% to 67.5% by leveraging goal decomposition alone.\nFeedback Message. Feedback contains the agent‚Äôs state and the execution result of the actions, which\nhelps the planner to understand and make another attempt to correct the mistakes in the previous and\ndeal with special cases. This enables the planner to accomplish a broader range of goals with higher\nsuccess rate. As shown in the 3rd row of Tab. 3, our agent gain the ability to collect diamond by\ncombining feedback with goal decomposition.\nExternal Knowledge Base. External knowledge contains general rules, crafting recipes, and common\ntricks in Minecraft, such as the recipes for crafting iron ingot and iron pickaxe, the suitable location to\nfind diamond ore, and the efficient way to get cobblestone. Providing the planner with this information\ngreatly boosts the success rate of obtaining iron pickaxe and diamond, and the success rate of mining\ndiamond increase by 7 times by learning from the knowledge base that diamonds are more likely to\nappear in specific levels.\nText-based Memory. Leveraging the reference plan recorded in the memory, the planner can handle\nthe task it has encountered more efficiently. The success rates of obtaining iron pickaxe and diamond\nare 95.0% and 67.5%, surpassing the model without memory by 37.5% and 32.5%, respectively.\n9\n5 Conclusion\nWe introduce the GITM framework, which utilizes Large Language Models (LLMs) for hierarchical\ndecomposition of goals. GITM introduces LLM Decomposer, LLM Planner and LLM Interface\nto gradually decompose goals into sub-goals, structured actions and keyboard/mouse operations.\nThis work makes significant progress towards the ObtainDiamond goal, outperforming all previous\nmethods by a significant margin (+47.5% success rate). This proves the potential inefficiency and\npoorly scalability of Reinforcement Learning (RL) in Minecraft, breaking the traditional reliance\non RL. Moreover, by obtaining all items in Minecraft Overworld, this research marks a critical step\ntoward Generally Capable Agents (GCAs) that match human performance in Minecraft.\nAcknowledgments The work is partially supported by the National Natural Science Foundation of\nChina under grants No.U19B2044, No.61836011, No.62022048, and No.62276150. This work is also\npartially supported by the National Key R&D Program of China under grants NO.2022ZD0114900,\nand the Guoqiang Institute of Tsinghua University.\nReferences\n[1] A. Amiranashvili, N. Dorka, W. Burgard, V . Koltun, and T. Brox. Scaling imitation learning in minecraft.\narXiv preprint arXiv:2007.02701, 2020.\n[2] B. Baker, I. Akkaya, P. Zhokov, J. Huizinga, J. Tang, A. Ecoffet, B. Houghton, R. Sampedro, and J. Clune.\nVideo pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural\nInformation Processing Systems, 35:24639‚Äì24654, 2022.\n[3] S. Cai, Z. Wang, X. Ma, A. Liu, and Y . Liang. Open-world multi-task control through goal-aware\nrepresentation learning and adaptive horizon prediction. arXiv preprint arXiv:2301.10034, 2023.\n[4] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong,\nT. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\n[5] L. Fan, G. Wang, Y . Jiang, A. Mandlekar, Y . Yang, H. Zhu, A. Tang, D.-A. Huang, Y . Zhu, and A. Anand-\nkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. arXiv preprint\narXiv:2206.08853, 2022.\n[6] W. H. Guss, S. Milani, N. Topin, B. Houghton, S. Mohanty, A. Melnik, A. Harter, B. Buschmaas, B. Jaster,\nC. Berganski, et al. Towards robust and domain agnostic reinforcement learning competitions: Minerl\n2020. In NeurIPS 2020 Competition and Demonstration Track, pages 233‚Äì252. PMLR, 2021.\n[7] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world models. arXiv\npreprint arXiv:2301.04104, 2023.\n[8] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning, pages\n9118‚Äì9147. PMLR, 2022.\n[9] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y . Chebotar,\net al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\narXiv:2207.05608, 2022.\n[10] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, W. Yang, et al.\nMinerl diamond 2021 competition: Overview, results, and lessons learned. NeurIPS 2021 Competitions\nand Demonstrations Track, pages 13‚Äì28, 2022.\n[11] I. Kanitscheider, J. Huizinga, D. Farhi, W. H. Guss, B. Houghton, R. Sampedro, P. Zhokhov, B. Baker,\nA. Ecoffet, J. Tang, et al. Multi-task curriculum learning in a complex, visual, hard-exploration domain:\nMinecraft. arXiv preprint arXiv:2106.14876, 2021.\n[12] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y . Li. Api-bank: A benchmark for tool-augmented llms.\narXiv preprint arXiv:2304.08244, 2023.\n[13] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\n[14] Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu, and W. Yang. Juewu-mc: Playing minecraft with sample-efficient\nhierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021.\n10\n[15] H. Mao, C. Wang, X. Hao, Y . Mao, Y . Lu, C. Wu, J. Hao, D. Li, and P. Tang. Seihai: A sample-efficient\nhierarchical ai for the minerl competition. In Distributed Artificial Intelligence: Third International\nConference, DAI 2021, Shanghai, China, December 17‚Äì18, 2021, Proceedings 3, pages 38‚Äì51. Springer,\n2022.\n[16] Y . Matsuo, Y . LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama, E. Uchibe, and J. Morimoto. Deep\nlearning, reinforcement learning, and world models. Neural Networks, 2022.\n[17] S. Milani, N. Topin, B. Houghton, W. H. Guss, S. P. Mohanty, K. Nakata, O. Vinyals, and N. S. Kuno.\nRetrospective analysis of the 2019 minerl competition on sample efficient reinforcement learning. In\nNeurIPS 2019 Competition and Demonstration Track, pages 203‚Äì214. PMLR, 2020.\n[18] S. Milani, A. Kanervisto, K. Ramanauskas, S. Schulhoff, B. Houghton, S. Mohanty, B. Galbraith, K. Chen,\nY . Song, T. Zhou, et al. Towards solving fuzzy tasks with human feedback: A retrospective of the minerl\nbasalt 2022 competition. arXiv preprint arXiv:2303.13512, 2023.\n[19] T. Schick, J. Dwivedi-Yu, R. Dess√¨, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom.\nToolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[20] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[21] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\nProgprompt: Generating situated robot task plans using large language models. arXiv preprint\narXiv:2209.11302, 2022.\n[22] A. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V . Davydov, and A. I. Panov. Hierarchical deep\nq-network from imperfect demonstrations in minecraft. Cognitive Systems Research, 65:74‚Äì78, 2021.\n[23] O. E. L. Team, A. Stooke, A. Mahajan, C. Barros, C. Deck, J. Bauer, J. Sygnowski, M. Trebacz,\nM. Jaderberg, M. Mathieu, et al. Open-ended learning leads to generally capable agents. arXiv preprint\narXiv:2107.12808, 2021.\n[24] C. Tessler, S. Givony, T. Zahavy, D. Mankowitz, and S. Mannor. A deep hierarchical approach to lifelong\nlearning in minecraft. In Proceedings of the AAAI conference on artificial intelligence, 2017.\n[25] Z. Wang, S. Cai, A. Liu, X. Ma, and Y . Liang. Describe, explain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\n[26] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[27] Wikipedia contributors. Minecraft ‚Äî Wikipedia, the free encyclopedia. https://en.wikipedia.org/\nw/index.php?title=Minecraft&oldid=1155148900, 2023.\n[28] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nA Implementation Details\nA.1 LLM Decomposer\nWe use gpt-3.5-turbo from OpenAI API 4 for goal decomposition. The prompt is shown as\nfollows, which consists of two parts: instruction with the role of ‚ÄúSYSTEM‚Äù and query with\nthe role of ‚ÄúUSER‚Äù. The {object quantity}, {object name} and {related knowledge} are\ninjectable slots that will be replace with corresponding texts before fed into the LLM.\n4https://platform.openai.com/docs/api-reference\n11\nSYSTEM:\nYou are an assistant for the game Minecraft.\nI will give you some target object and some knowledge related to the object. Please write the\nobtaining of the object as a goal in the standard form.\nThe standard form of the goal is as follows:\n{\n\"object\": \"the name of the target object\",\n\"count\": \"the target quantity\",\n\"material\": \"the materials required for this goal, a dictionary in the form {material_name:\nmaterial_quantity}. If no material is required, set it to None\",\n\"tool\": \"the tool used for this goal. If multiple tools can be used for this goal, only write\nthe most basic one. If no tool is required, set it to None\",\n\"info\": \"the knowledge related to this goal\"\n}\nThe information I will give you:\nTarget object: the name and the quantity of the target object\nKnowledge: some knowledge related to the object.\nRequirements:\n1. You must generate the goal based on the provided knowledge instead of purely depending\non your own knowledge.\n2. The \"info\" should be as compact as possible, at most 3 sentences. The knowledge I give you\nmay be raw texts from Wiki documents. Please extract and summarize important information\ninstead of directly copying all the texts.\nGoal Example:\n{ \"object\": \"iron_ore\",\n\"count\": 1,\n\"material\": None,\n\"tool\": \"stone_pickaxe\",\n\"info\": \"iron ore is obtained by mining iron ore. iron ore is most found in level 53. iron ore\ncan only be mined with a stone pickaxe or better; using a wooden or gold pickaxe will yield\nnothing.\"\n}\n{\n\"object\": \"wooden_pickaxe\",\n\"count\": 1,\n\"material\": {\"planks\": 3, \"stick\": 2},\n\"tool\": \"crafting_table\",\n\"info\": \"wooden pickaxe can be crafted with 3 planks and 2 stick as the material and\ncrafting table as the tool.\"\n}\nUSER:\nTarget object: {object quantity} {object name}\nKnowledge: {related knowledge}\nThe recursive decomposition generates a sub-goal tree starting from the final goal object as the root\nnode: if a goal has some prerequisites (materials or tools), for each required material or tool, we add a\nchild node representing the goal of obtaining that material or tool, and then recursively decompose the\nchild node, until there is no more prerequisites. The related knowledge is from: 1) Crafting/smelting\nrecipes in MineDojo [5], written in the form ‚ÄúCrafting {quantity} {object}requires {material}\nas the material and {tool} as the tool‚Äù; 2) Wiki on the Internet 5. We extract the paragraphs with\nkeywords ‚Äúobtaining‚Äù, ‚Äúmining‚Äù, ‚Äúsources‚Äù, etc.\n5https://minecraft-archive.fandom.com/wiki/Minecraft_Wiki\n12\nA.2 LLM Interface\nInstruction for Extracting Structured Actions. To extract structured actions, we first ask LLM\nto generate a tree-structured action planning for each of the 3141 predefined tasks provided by\nMineDojo, and then converts each action step into a (verb, object, tool, material) tuple.\nDuring decomposition, it is essential to ensure actions are neither too broad nor too specific. We\nadjusted the depth of the action decomposition tree to achieve balance, and empirically set the depth\nas 2 to meet our requirements.\nSpecifically, we use gpt-3.5-turbo from OpenAI API to generate the structured actions. We add\nthe following instruction to the content of ‚ÄúSYSTEM‚Äù role to generate the tree-structured plan. We\nadd the goal description, e.g., \"find material and craft a iron pickaxe\", to the content of ‚ÄúUSER‚Äù role\nand then asks LLM to response according to the requirements.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you my goal in the game, please break it down as a tree-structure plan to achieve\nthis goal.\nThe requirements of the tree-structure plan are:\n1. The plan tree should be exactly of depth 2.\n2. Describe each step in one line.\n3. You should index the two levels like ‚Äô1.‚Äô, ‚Äô1.1.‚Äô, ‚Äô1.2.‚Äô, ‚Äô2.‚Äô, ‚Äô2.1.‚Äô, etc.\n4. The sub-goals at the bottom level should be basic actions so that I can easily execute them\nin the game.\nUSER:\nThe goal is to {goal description}. Generate the plan according to the requirements.\nAfter that, we extract the action tuple from each sentence of the leaf nodes. We use the following\ninstruction as the content of ‚ÄúSYSTEM‚Äù role to extract the tuple, and add the sentence to the content\nof ‚ÄúUSER‚Äù role.\nSYSTEM:\nYou serve as an assistant that helps me play Minecraft.\nI will give you a sentence. Please convert this sentence into one or several actions according\nto the following instructions.\nEach action should be a tuple of four items, written in the form (‚Äôverb‚Äô, ‚Äôobject‚Äô, ‚Äôtools‚Äô,\n‚Äômaterials‚Äô)\n‚Äôverb‚Äô is the verb of this action.\n‚Äôobject‚Äô refers to the target object of the action.\n‚Äôtools‚Äô specifies the tools required for the action.\n‚Äômaterial‚Äô specifies the materials required for the action.\nIf some of the items are not required, set them to be ‚ÄôNone‚Äô.\nUSER:\nThe sentence is {sentence}. Generate the action tuple according to the requirements.\nThen, we extract the structured actions by selecting frequent actions and merging actions with similar\nfunctionalities. The set of structured actions is {equip, explore, approach, mine/attack,\ndig_down, go_up, build, craft/smelt, apply}. Note that we disregard more detailed ac-\ntion decomposition for attack and build to remove overly detailed short-term actions and focus on\nlong-term task completion.\nAction Implementation. The observation of the action contains LiDAR rays with an interval of 5\ndegrees in the horizon and vertical direction for locating objects, and voxels with 10 units radius only\nfor navigation, inventory, life status, and agent location status (X-ray cheating is carefully avoided).\n13\nRGB is not used in our implementation, although it provides more information than LiDAR rays.\nFor example, the biome, and category of the dropping item can not be identified by LiDAR rays.\nSome objects may also be missed by LiDAR due to sparseness of LiDAR rays. We also set the\nbreaking speed to 100 and strength to 100, mainly following [ 7]. The detailed implementation of\neach structured action is as follows:\n‚Ä¢ equip: The equip action calls the environment API to equip the required object. The action\nsucceeds when the API returns success. The action fails when the object is not in inventory or the\nequip API returns failure.\n‚Ä¢ explore: The explore action traverses the world until object is visible. This action regards the\nworld as a chessboard, and each node on the chessboard is the center point of a 20√ó20 units area.\nTwo strategies are implemented depending on whether the agent is on the ground or not. When\nthe agent is on the ground, the BFS explore will be adopted. When the agent is under the ground,\nmainly for exploring ore, the DFS explore will be adopted. In the DFS exploration, the agent will\nbreak the blocks to form a mine road with width of 1 and height of 2. The action succeeds when\nthe object is visible. The action fails when the explore exceeds a preset steps of 10,000 but the\nrequired object is not found.\n‚Ä¢ approach: The approach action finds the nearest visible required object and walks towards the\nobject. We adopt A‚àó algorithm for finding path. The A‚àó algorithm can jump, translate and fall\nin four directions of north, south, east and west. We also allow the agent to jump while placing a\nblock under the agent for ascent. If the object is out of the voxel observation range,A‚àó algorithm is\niteratively applied to find the location nearest to the object. The action succeeds when the ‚Ñì‚àû norm\ndistance between the object and agent is less than 2. The action fails when there is no required\nobject visible or no path can be found to walk close to the object.\n‚Ä¢ mine/attack: The mine/attack action uses the keyboard attack API with the tools to attack the\nobject. Only visible object could be mined or attacked. The object of mine should be blocks, and\nthe agent will continue mining the block until it is broken. The object of attack should be entities,\nand the agent will iteratively approach and attack the entity until it is killed. After the block is\nbroken or the entity is killed, if there are items dropped by them, the agent will approach the items\nto collect them. The action succeeds when the block is broken or the entity is killed. The action\nfails when there is no visible object, no required tools is in inventory, or the visible object is out of\nattack range.\n‚Ä¢ dig_down: The dig_down action iteratively breaks the block underfoot with the tool until the\nrequired ylevel is reached. If the agent is on the ground, before digging down, current location is\nstored for going up action. After the action succeeds, the state of the agent is set to under ground.\nThe action succeeds when the required ylevel is reached. The action fails when it exceeds the reset\nmax steps 10,000 or no required tool is in inventory.\n‚Ä¢ go_up: The agent will first go back to the location stored by dig_down. Then, the go_up action\nputs dirt blocks underfoot to raise the agent. After the action is finished, the state of agent is set to\non the ground. The action succeeds when the pre-stored location is reached. The action fails when\nthe walk fails, exceeds the reset max steps 10,000 or there is no required tool in inventory.\n‚Ä¢ build: The build action places the required blocks according to a given blueprint from bottom\nto up. The action succeeds when all blocks have been placed. The action fails when there are no\nenough materials in inventory or it is invalid to place some blocks.\n‚Ä¢ craft/smelt: The action calls the environment API to craft/smelt the required object. The action\nsucceeds when the required object is obtained. The actions fails when there are no enough materials\nin inventory or the agent is unable to place the crafting table/furnace or the API fails.\n‚Ä¢ apply: The apply action calls the keyboard use API, and applies the specific tool to the object,e.g.,\napplying the bucket on water to obtain water bucket. The action succeeds when the API returns\nsuccess. The action fails when there is no visible object, no tool in inventory or the API fails.\nFeedback Message. After the execution of each action, we will get feedback from the structured\nactions. The feedback will refresh the agent‚Äôs state in Sec. A.3.2, including current inventory, biome,\nylevel and on/under the ground status. The feedback will also contain the success/fail message from\nthese action, as well as the inventory change during the action.\n14\nA.3 LLM Planner\nHere we present the prompt for planning with LLM. We also usegpt-3.5-turbo from OpenAI API\nas the LLM planner. The model accepts inputs in form of a chat, i.e., the prompt is a dialogue consist-\ning of several messages, each of which contains a role and the content. We set the Instruction\nwith the role ‚ÄúSYSTEM‚Äù at the beginning, and use the User Query with the role ‚ÄúUSER‚Äù to query\nthe LLM for response. The content of the Instruction and User Query are as follows.\nA.3.1 Instruction\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI will give you a goal in the game. Please think of a plan to achieve the goal, and then write a\nsequence of actions to realize the plan. The requirements and instructions are as follows:\n1. You can only use the following functions. Don‚Äôt make plans purely based on your\nexperience, think about how to use these functions.\nexplore(object, strategy)\nMove around to find the object with the strategy: used to find objects including block items\nand entities. This action is finished once the object is visible (maybe at the distance).\nAugments:\n- object: a string, the object to explore.\n- strategy: a string, the strategy for exploration.\napproach(object)\nMove close to a visible object: used to approach the object you want to attack or mine. It may\nfail if the target object is not accessible.\nAugments:\n- object: a string, the object to approach.\ncraft(object, materials, tool)\nCraft the object with the materials and tool: used for crafting new object that is not in the\ninventory or is not enough. The required materials must be in the inventory and will be\nconsumed, and the newly crafted objects will be added to the inventory. The tools like the\ncrafting table and furnace should be in the inventory and this action will directly use them.\nDon‚Äôt try to place or approach the crafting table or furnace, you will get failed since this\naction does not support using tools placed on the ground. You don‚Äôt need to collect the items\nafter crafting. If the quantity you require is more than a unit, this action will craft the objects\none unit by one unit. If the materials run out halfway through, this action will stop, and you\nwill only get part of the objects you want that have been crafted.\nAugments:\n- object: a dict, whose key is the name of the object and value is the object quantity.\n- materials: a dict, whose keys are the names of the materials and values are the quantities.\n- tool: a string, the tool used for crafting. Set to null if no tool is required.\nmine(object, tool)\nMine the object with the tool: can only mine the object within reach, cannot mine object from\na distance. If there are enough objects within reach, this action will mine as many as you\nspecify. The obtained objects will be added to the inventory.\nAugments:\n- object: a string, the object to mine.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\nattack(object, tool)\nAttack the object with the tool: used to attack the object within reach. This action will keep\ntrack of and attack the object until it is killed.\nAugments:\n- object: a string, the object to attack.\n- tool: a string, the tool used for mining. Set to null if no tool is required.\n15\nequip(object)\nEquip the object from the inventory: used to equip equipment, including tools, weapons, and\narmor. The object must be in the inventory and belong to the items for equipping.\nAugments:\n- object: a string, the object to equip.\ndigdown(object, tool)\nDig down to the y-level with the tool: the only action you can take if you want to go\nunderground for mining some ore.\nAugments:\n- object: an int, the y-level (absolute y coordinate) to dig to.\n- tool: a string, the tool used for digging. Set to null if no tool is required.\ngo_back_to_ground(tool)\nGo back to the ground from underground: the only action you can take for going back to the\nground if you are underground.\nAugments:\n- tool: a string, the tool used for digging. Set to null if no tool is required.\napply(object, tool)\nApply the tool on the object: used for fetching water, milk, lava with the tool bucket, pooling\nwater or lava to the object with the tool water bucket or lava bucket, shearing sheep with the\ntool shears, blocking attacks with the tool shield.\nAugments:\n- object: a string, the object to apply to.\n- tool: a string, the tool used to apply.\n2. You cannot define any new function. Note that the \"Generated structures\" world creation\noption is turned off.\n3. There is an inventory that stores all the objects I have. It is not an entity, but objects can be\nadded to it or retrieved from it anytime at anywhere without specific actions. The mined or\ncrafted objects will be added to this inventory, and the materials and tools to use are also from\nthis inventory. Objects in the inventory can be directly used. Don‚Äôt write the code to obtain\nthem. If you plan to use some object not in the inventory, you should first plan to obtain it.\nYou can view the inventory as one of my states, and it is written in form of a dictionary whose\nkeys are the name of the objects I have and the values are their quantities.\n4. You will get the following information about my current state:\n- inventory: a dict representing the inventory mentioned above, whose keys are the name of\nthe objects and the values are their quantities\n- environment: a string including my surrounding biome, the y-level of my current location,\nand whether I am on the ground or underground\nPay attention to this information. Choose the easiest way to achieve the goal conditioned on\nmy current state. Do not provide options, always make the final decision.\n5. You must describe your thoughts on the plan in natural language at the beginning. After\nthat, you should write all the actions together. The response should follow the format:\n{\n\"explanation\": \"explain why the last action failed, set to null for the first planning\",\n\"thoughts\": \"Your thoughts on the plan in natural languag\",\n\"action_list\": [\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\n}\nThe action_list can contain arbitrary number of actions. The args of each action should\n16\ncorrespond to the type mentioned in the Arguments part. Remember to add ‚Äú‚Äòdict‚Äú‚Äò at the\nbeginning and the end of the dict. Ensure that you response can be parsed by Python json.loads\n6. I will execute your code step by step and give you feedback. If some action fails, I will\nstop at that action and will not execute its following actions. The feedback will include error\nmessages about the failed action. At that time, you should replan and write the new code just\nstarting from that failed action.\nA.3.2 User Query\nUSER:\nMy current state:\n- inventory: {inventory}\n- environment: {environment}\nThe goal is to {goal}.\nHere is one plan to achieve similar goal for reference: {reference plan}.\nBegin your plan. Remember to follow the response format.\nor Action {successful action} succeeded, and {feedback message}. Continue your\nplan. Do not repeat successful action. Remember to follow the response format.\nor Action {failed action} failed, because {feedback message}. Revise your plan from\nthe failed action. Remember to follow the response format.\nA.4 Memory\nA.4.1 Learning Process\nWe maintain the text-based memory with a dictionary, whose keys are sub-goals and values are lists\nof successful action sequences for the corresponding sub-goals. The construction and update of the\nmemory are through the following learning process:\n‚Ä¢ When encountering a new sub-goal that is not in the memory, the LLM planner creates plans\nwithout reference. Once the sub-goal is achieved, the entirely executed action sequence would be\nstored into the memory.\n‚Ä¢ When encountering a sub-goal with memory, the first action sequence in the recording list for this\ngoal is retrieved as the reference plan, with which the LLM planner tries to achieve the goal. If it\nsucceeds, the new executed action sequence will be added to the last of the recording list.\n‚Ä¢ For each sub-goal, once the number of action sequences recorded in its list reaches N, we pop\nall the N sequences and use LLM to summarize them into a common plan solution suitable for\nvarious scenarios, which is then put first in the list. N is set to 5 in all our experiments.\nTo learn the memory for obtaining all items, starting from scratch each time would take a long time.\nIn addition, it is necessary to avoid spending the most of time on learning simple tasks and not\ninvesting enough in learning difficult tasks. To improve the learning efficiency, we suggest to study\nthe sub-goals individually one by one. We first use our LLM Decomposer to generate sub-goal trees\nfor all items, acquiring the set of all sub-goals involved. Then for each sub-goal, the LLM planner\nplays multiple times given its prerequisites including the required materials and tools. The learning\nprocess of the sub-goal is finished once we obtain N = 5successful action sequences and summarize\nthem into one common plan solution for reference.\nA.4.2 Implementation of Memory Summarization\nWe also use gpt-3.5-turbo from OpenAI API for memory summarization but in a different\ndialogue. We use the following prompt to instruct the summarization with the role ‚ÄúSYSTEM‚Äù. The\nslot {action description} is replaced with the same descriptions of interfaces of the structured\nactions as Sec. A.3.1. We list all the action sequences to be summarized in the query with the role\n‚ÄúUSER‚Äù, which is fed into the LLM for response.\n17\nSYSTEM:\nYou serve as an assistant that helps me play the game Minecraft.\nI am using a set of actions to achieve goals in the game Minecraft. I have recorded several\naction sequences successfully achieving a goal in a certain state. I will give you the goal, the\nstate, and the sequences later. Please summarize the multiple action sequences into a single\naction sequence as a universal reference to achieve the goal given that certain state. Here are\nthe instructions:\n1. Each action sequence is a sequence of the following actions:\n{action description}\n2. The action sequences before and after summarization are always conditioned on the given\nstate, i.e., the actions are taken in that certain state to achieve the goal. I will describe the state\nin the following form: State: - inventory: a dict whose keys are the name of the objects and\nthe values are their quantities. This inventory stores all the objects I have. - environment: a\ndict including my surrounding biome and whether I am on the ground or underground.\n3. The action sequence you summarize should be able to achieve the goal in general cases\nwithout specific modification. Every necessary action should be included, even though it does\nnot appear in some sequences because I manually skipped it in some lucky cases. The actions\nredundant or irrelevant to the goal should be filtered out. The corner cases, such as success by\nluck and dealing with contingencies, should not be summarized into the final sequence.\n4. You should describe your thoughts on summarization in natural language at the beginning.\nAfter that, give me the summarized action sequence as a list in JSON format. Your response\nshould follow this form:\nThoughts: \"Your thoughts and descriptions of your summarization\"\nSummarized action sequence:\n[\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"},\n{\"name\": \"action name\", \"args\": {\"arg name\": value}, \"expectation\": \"describe the\nexpected results of this action\"}\n]\nB Results of All Items\nWe provide the success rate of all items in the entire Minecraft Overworld Technology Tree in Tab. 4.\nWe have attached a video of obtaining a diamond in the supplementary materials.\nExperiment Setting. Considering the large number of items, including those difficult to be obtained,\nwe implemented an incremental testing strategy. This strategy is designed to keep the testing costs\nwithin a reasonable range, while also accounting for the rarity of certain items. We avoided a uniform\nincrease in the number of tests across all items to accommodate the hardest-to-obtain ones, which\nwould have resulted in prohibitive testing costs. Instead, we employed a incremental testing process.\nFor each item, we begin with 20 games. If the success count is less than or equal to 1, we increase\nto 50 games. If the success count remains less than or equal to 1, we further increase to 100, and\neventually 200 games. This testing continues until the success count finally exceeds 1, or we complete\n200 games. By following this efficient strategy, we ensure a cost-effective and reliable evaluation of\neach item, regardless of its availability. Moreover, because some items need long-term planning and\ncrafting chain, we do not set restrictions on the time limit or query limit.\nExploring Biome. Biomes can be a key factor that strongly influences the success rate. Some items,\nlike cactus, pumpkin, or melon, can only be found in specific biomes. The distribution of biomes\nhighly limits the success rate of some items.\n18\nTable 4: Success rate for all 262 items in the entire Minecraft Overworld Technology Tree.\nItem Name Success\nRate Item Name Success\nRate Item Name Success\nRate Item Name Success\nRate\nacacia boat 100.0 stone sword 100.0 gravel 80.0 beetroot seeds 40.0\nacacia door 100.0 stonebrick 100.0 iron boots 80.0 diamond boots 40.0\nacacia fence 100.0 sugar 100.0 iron trapdoor 80.0 diamond helmet 40.0\nacacia fence gate 100.0tallgrass 100.0 leather chestplate 80.0 golden axe 40.0\nacacia stairs 100.0 trapdoor 100.0 leather leggings 80.0 golden pickaxe 40.0\nbeef 100.0 wheat 100.0 bone block 75.0 red mushroom 40.0\nbirch boat 100.0 wheat seeds 100.0 bow 75.0 red mushroom block 40.0\nbirch door 100.0 wooden axe 100.0 chest minecart 75.0 diamond leggings 35.0\nbirch fence 100.0 wooden button 100.0 furnace minecart 75.0 golden boots 35.0\nbirch fence gate 100.0 wooden door 100.0 hopper 75.0 ink sac 35.0\nbirch stairs 100.0 wooden hoe 100.0 iron helmet 75.0 sticky piston 35.0\nboat 100.0 wooden pickaxe 100.0 minecart 75.0 beetroot soup 30.0\nbone 100.0 wooden pressure plate 100.0 mossy cobblestone 75.0 golden helmet 30.0\nbone meal 100.0 wooden shovel 100.0 vine 75.0 lead 30.0\nbowl 100.0 wooden slab 100.0 waterlily 75.0 map 30.0\nchest 100.0 wooden sword 100.0 banner 70.0 mushroom stew 30.0\nchicken 100.0 yellow flower 100.0 brick 70.0 brick stairs 25.0\ncobblestone 100.0 armor stand 95.0 clay ball 70.0 cactus 25.0\ncobblestone wall 100.0book 95.0 diamond 70.0 cake 25.0\ncooked beef 100.0 bread 95.0 diamond shovel 70.0 clock 25.0\ncooked chicken 100.0 coal 95.0 dropper 70.0 cooked rabbit 25.0\ncooked mutton 100.0 fireworks 95.0 feather 70.0 diamond chestplate 25.0\ncooked porkchop 100.0gunpowder 95.0 iron bars 70.0 obsidian 25.0\ncrafting table 100.0 iron ingot 95.0 iron door 70.0 rabbit 25.0\ndark oak boat 100.0 iron nugget 95.0 jukebox 70.0 rabbit hide 25.0\ndark oak door 100.0 iron ore 95.0 lapis lazuli 70.0 deadbush 20.0\ndark oak fence 100.0 iron shovel 95.0 noteblock 70.0 golden leggings 20.0\ndark oak fence gate 100.0item frame 95.0 piston 70.0 golden rail 20.0\ndark oak stairs 100.0 leather 95.0 rail 70.0 lapis block 20.0\ndirt 100.0 rotten flesh 95.0 redstone 70.0 writable book 20.0\ndouble plant 100.0 shield 95.0 redstone torch 70.0 baked potato 15.0\nfence 100.0 spider eye 95.0 cauldron 65.0 carrot 15.0\nfence gate 100.0 stone slab 95.0 diamond hoe 65.0 diamond block 15.0\nfurnace 100.0 torch 95.0 diamond sword 65.0 emerald block 15.0\nglass 100.0 trapped chest 95.0 emerald 65.0 golden chestplate 15.0\nglass bottle 100.0 tripwire hook 95.0 iron leggings 65.0 potato 15.0\nglass pane 100.0 carpet 90.0 tnt 65.0 pumpkin 15.0\nladder 100.0 coal block 90.0 arrow 60.0 pumpkin seeds 15.0\nlever 100.0 grass 90.0 compass 60.0 carrot on a stick 10.0\nlog 100.0 heavy weighted pressure plate 90.0flower pot 60.0 jungle boat 10.0\nmutton 100.0 iron hoe 90.0 iron chestplate 60.0 jungle door 10.0\noak stairs 100.0 iron sword 90.0 brick block 55.0 jungle fence 10.0\npaper 100.0 leather boots 90.0 clay 55.0 jungle fence gate 10.0\nplanks 100.0 leather helmet 90.0 dispenser 55.0 jungle stairs 10.0\nporkchop 100.0 leaves 90.0 gold ingot 55.0 lit pumpkin 10.0\nred flower 100.0 painting 90.0 gold nugget 55.0 melon 10.0\nreeds 100.0 shears 90.0 gold ore 55.0 melon block 10.0\nsand 100.0 snow 90.0 golden shovel 55.0 melon seeds 10.0\nsandstone 100.0 snow layer 90.0 hardened clay 55.0 gold block 8.0\nsapling 100.0 snowball 90.0 hopper minecart 55.0 golden carrot 8.0\nsign 100.0 string 90.0 iron block 55.0 pumpkin pie 8.0\nspruce boat 100.0 wool 90.0 slime ball 55.0 red sandstone 6.0\nspruce door 100.0 bed 85.0 activator rail 50.0 red sandstone stairs 6.0\nspruce fence 100.0 brown mushroom 85.0 detector rail 50.0 speckled melon 6.0\nspruce fence gate 100.0brown mushroom block 85.0 diamond axe 50.0 stone slab2 6.0\nspruce stairs 100.0 bucket 85.0 diamond pickaxe 50.0 anvil 4.0\nstick 100.0 flint 85.0 egg 50.0 apple 4.0\nstone 100.0 hay block 85.0 lava bucket 50.0 enchanting table 4.0\nstone axe 100.0 iron axe 85.0 repeater 50.0 enchanted book 3.0\nstone brick stairs 100.0iron pickaxe 85.0 tnt minecart 50.0 poisonous potato 2.0\nstone button 100.0 milk bucket 85.0 bookshelf 45.0 golden apple 1.0\nstone hoe 100.0 sandstone stairs 85.0 golden hoe 45.0 rabbit foot 1.0\nstone pickaxe 100.0 water bucket 85.0 golden sword 45.0 slime 1.0\nstone pressure plate 100.0fermented spider eye 80.0 light weighted pressure plate 45.0rabbit stew 0.5\nstone shovel 100.0 fishing rod 80.0 redstone block 45.0\nstone stairs 100.0 flint and steel 80.0 beetroot 40.0\nC Supplementary Ablations\nWe make a more detailed comparison between our GITM with RL-based methods in Tab. 5. The most\nstraightforward pipeline is to directly map the goal into keyboard/mouse operations. We gradually\nadd goal decomposition and structured action stages into the pipeline, and ablate the use of RL-based\nmodels or LLM.\n19\nTable 5: Ablation study. The milestone items from left to right are crafting table\n , wooden\npickaxe\n , stone pickaxe\n , iron pickaxe\n , and diamond\n . The success rate is calculated\nunder time limit of 12000 steps (total) and query limit of 30 (each sub-goal). ‚ÄúGoal Decomp.‚Äù\nindicates whether to use LLM Decomposer to decompose the goal into sub-goals. ‚ÄúGoal / Sub-Goal\nto Structured Actions / Keyboard & Mouse Mapping‚Äù indicates which method is used for the mapping\nfrom goal / sub-goals to structured actions / keyboard & mouse operations.\nGoal\nDecomp.\nStructured\nAction\nGoal / Sub-Goal to\nStructured Actions / Keyboard & Mouse\nMapping\nSuccess Rate (%)\n(a) Specialist RL Model (VPT) 100.0 100.0 100.0 85.0 20.0\n(b) Goal-conditioned RL Model (DEPS) 0.0 0.0 0.0 0.0 0.0\n(c) Our LLM Planner 0.0 0.0 0.0 0.0 0.0\n(d) ‚úì Goal-conditioned RL Model (DEPS) 90.0 80.0 30.0 0.0 0.0\n(e) ‚úì Our LLM Planner 0.0 0.0 0.0 0.0 0.0\n(f) ‚úì Our LLM Planner 57.5 32.5 5.0 0.0 0.0\n(g) ‚úì ‚úì Our LLM Planner 100.0 100.0 100.0 95.0 67.5\nImplementation Details. We can only find open-sourced RL models from VPT [2] and DEPS [25],\nso they are adopted for the ablation. VPT model is specifically trained for the ObtainDiamond\nchallenge, while DEPS model can use goal description as input to guide the model‚Äôs output. We refer\nto them as specialist RL model and goal-conditioned RL model, respectively. As for the use of LLM\nPlanner, we note that if structured action is not used, LLM Planner will be inevitably asked to output\nreasonable keyboard/mouse operations. However, LLM Planner does not have access to environment\nobservations, so it cannot directly output reasonable keyboard/mouse operations.\nDirect Mapping. See Tab. 5(a)(b)(c). It is hard to directly mapping the long-horizon goal into\nreasonable keyboard/mouse operations. While a specialist RL model (i.e., VPT) can deliver promising\nresults, it requires large amount of data and computational resources to train such a model [2] (720\nV100 GPUs for 9 days). Moreover, a different goal will require further training of the specialist RL\nmodel, limiting the versatility of this paradigm. The goal conditional RL model (i.e., DEPS) cannot\nachieve the goal, because the model [ 25] we have access to is not generalizable to all scenarios.\nIf only the final goal is given, it will ignore preconditions, such as not crafting the necessary iron\npickaxe when mining diamonds. LLM also fails to accomplish the goal. The primary reason is that it\ncan not handle environment observation and keyboard/mouse operations well.\nStructured Action. We design structured actions to interact with the environment, and provide\nan abstract interface. Tab. 5(f) shows that adding structured action significantly improves LLM‚Äôs\nperformance. This is because structured actions can deal with environment observations and key-\nboard/mouse operations more precisely, unleashing the reasoning potential of LLM. We are not aware\nof a RL model using structured actions currently. It is possible for structure actions to enhance the\nRL model as well, and we will explore it in the future work.\nGoal Decomposition. Decomposing the goal into sub-goals can simplify the whole task. Tab. 5(b)(d)\nand Tab. 5(f)(g) show its effectiveness for both goal-conditioned RL model and our method. By\nexploiting goal decomposition, it is possible for our method to accomplish long-term tasks with high\nsuccess rate.\nComparison between RL-based methods. We also note the paradigm shift from traditional RL-\nbased methods to our GITM leads to a great performance boost. Comparing Tab. 5(d)(g), where we\nonly change the goal-conditioned RL model to LLM with strutured actions, our method significantly\noutperforms the RL model.\nD ObtainDiamond\nWe demonstrate a case of the popular ObtainDiamond challenge in Fig. 7. During the process, the\nagent have to collect materials,i.e., log, stone and iron ore, as shown in Fig 7(a)(c)(e). Necessary tools,\ni.e., wooden pickaxe, stone pickaxe, furnace and iron pickaxe are also crafted in Fig 7(b)(d)(f)(h).\n20\n(a) mine log\n (b) craft wooden_pickaxe\n (c) mine stone\n(d) craft stone_pickaxe\n (e) mine iron_ore\n (f) craft furnace\n(g) smelt iron_ingot\n (h) craft iron_pickaxe\n (i) mine diamond\nFigure 7: A case of the popular ObtainDiamond challenge. Figure(e)(i) are enhanced in brightness\nfor better display.\nFinally the diamond is obtained in Fig 7(i). We have attached a video of obtaining a diamond in the\nsupplementary materials.\nE Applications\n(a) Shelter with Farmland\n (b) Iron Golem\n (c) Redstone Circuit\n (d) Nether Portal\nFigure 8: Demonstration of the applications. GITM can construct Shelter with Farmland and\nIron Golem for survival, Redstone Circuit for automation equipment, and Nether Portal for\nthe Nether world exploration.\nOur proposed GITM makes survival and the nether exploration possible in Minecraft which has\nnever been accomplished by existing agents. To achieve this, our agent builds four necessary items,\nincluding Shelter with Farmland, Iron Golem, Redstone Circuit, and Nether Portal,\nshown in Fig. 8. Shelter with Farmland is firstly built to keep the agent from being attacked\nby monsters at night and provide enough food. Iron Golem can automatically attack monsters to\nprotect the agent and the shelter. Redstone Circuit is the foundation of all automation equipment.\nNether Portal is the entrance to the Nether world.\n21"
}