{
    "title": "Relation-Aware Language-Graph Transformer for Question Answering",
    "url": "https://openalex.org/W4382318301",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100428885",
            "name": "Jinyoung Park",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5063579922",
            "name": "Hyeong Kyu Choi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5088645808",
            "name": "Juyeon Ko",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5114859505",
            "name": "Hyeonjin Park",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100377802",
            "name": "Ji‐Hoon Kim",
            "affiliations": [
                "Naver (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5031027762",
            "name": "Jisu Jeong",
            "affiliations": [
                "Naver (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5100390573",
            "name": "Kyungmin Kim",
            "affiliations": [
                "Naver (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A5100330223",
            "name": "Hyunwoo Kim",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4282944597",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3021649351",
        "https://openalex.org/W6736685754",
        "https://openalex.org/W3042563449",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2971986145",
        "https://openalex.org/W3094258447",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2803457824",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W2996268457",
        "https://openalex.org/W2624614404",
        "https://openalex.org/W4200629408",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W6801998677",
        "https://openalex.org/W3023160663",
        "https://openalex.org/W6758105487",
        "https://openalex.org/W3186681406",
        "https://openalex.org/W3213783001",
        "https://openalex.org/W3152801999",
        "https://openalex.org/W4226281578",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W2606780347",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3101850416",
        "https://openalex.org/W3097986428",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3164540570",
        "https://openalex.org/W3204650139",
        "https://openalex.org/W2911286998",
        "https://openalex.org/W2963907629",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963829073",
        "https://openalex.org/W2983995706",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W4225831764",
        "https://openalex.org/W2767891136",
        "https://openalex.org/W4287889747",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W4287717240"
    ],
    "abstract": "Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entities of different modalities. We validate the effectiveness of QAT on commonsense question answering datasets like CommonsenseQA and OpenBookQA, and on a medical question answering dataset, MedQA-USMLE. On all the datasets, our method achieves state-of-the-art performance. Our code is available at http://github.com/mlvlab/QAT.",
    "full_text": "Relation-Aware Language-Graph Transformer for Question Answering\nJinyoung Park1*, Hyeong Kyu Choi1*, Juyeon Ko1*, Hyeonjin Park2,\nJi-Hoon Kim2,3,4, Jisu Jeong2,3,4, Kyungmin Kim2,3,4, Hyunwoo J. Kim1†\n1Korea University\n2 NA VER\n3 NA VER Cloud\n4 NA VER AI Lab\n{lpmn678, imhgchoi, juyon98, hyunwoojkim}@korea.ac.kr\n{hyeonjin.park.ml, genesis.kim, jisu.jeong, kyungmin.kim.ml}@navercorp.com\nAbstract\nQuestion Answering (QA) is a task that entails reasoning over\nnatural language contexts, and many relevant works augment\nlanguage models (LMs) with graph neural networks (GNNs)\nto encode the Knowledge Graph (KG) information. However,\nmost existing GNN-based modules for QA do not take ad-\nvantage of rich relational information of KGs and depend\non limited information interaction between the LM and the\nKG. To address these issues, we propose Question Answer-\ning Transformer (QAT), which is designed to jointly reason\nover language and graphs with respect to entity relations in\na unified manner. Specifically, QAT constructs Meta-Path to-\nkens, which learn relation-centric embeddings based on di-\nverse structural and semantic relations. Then, our Relation-\nAware Self-Attention module comprehensively integrates dif-\nferent modalities via the Cross-Modal Relative Position Bias,\nwhich guides information exchange between relevant entites\nof different modalities. We validate the effectiveness of QAT\non commonsense question answering datasets like Common-\nsenseQA and OpenBookQA, and on a medical question an-\nswering dataset, MedQA-USMLE. On all the datasets, our\nmethod achieves state-of-the-art performance. Our code is\navailable at http://github.com/mlvlab/QAT.\n1 Introduction\nQuestion Answering (QA) is a challenging task that en-\ntails complex reasoning over natural language contexts. The\nlarge-scale pre-trained language models (LMs) have proven\nto be capable of processing such contexts, by encoding the\nimplicit knowledge in textual data. To deal with various\nQA tasks, both generic and domain-specific LMs (Liu et al.\n2019; Lee et al. 2020; Liu et al. 2021a) have been employed.\nBut the QA systems only with LMs often struggle with\nstructured reasoning due to the lack of explicit and struc-\ntured knowledge. Thus, many works (Mihaylov and Frank\n2018; Lin et al. 2019; Feng et al. 2020; Wang et al. 2022; Ya-\nsunaga et al. 2021; Zhang et al. 2022) in the literature have\nstudied to effectively incorporate Knowledge Graphs (KGs)\ninto the question-answering system.\n*These authors contributed equally.\n†is the corresponding author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nMost existing efforts for fusing LM and KG utilized a\nseparate graph neural network (GNN) module to process\nthe KG and integrated it with LM in the ‘final’ predic-\ntion step (Feng et al. 2020; Yasunaga et al. 2021; Wang\net al. 2022). However, recent works pinpointed the short-\ncomings of the shallow fusion, and have proposed meth-\nods that fuse LM and KG early in the intermediate layers.\nThey aimed to find more appropriate ways to exchange in-\nformation between the two modalities, via special tokens\nand nodes (Zhang et al. 2022) or cross-attention (Sun et al.\n2022). But the approaches have a modality-specific encoder\nGNN and the information exchange occurs only on fusion\nlayers resulting in modest information exchange. In addi-\ntion, the GNNs in previous works are not suited for fully\nexploiting KG data, which comprise triplets of two entities\nand their relations. To process the rich relational informa-\ntion, a more relation-centric KG processor is necessary, in\nplace of thenode-centric GNNs that learn node features with\nthe message-passing scheme.\nTo this end, we propose the Question Answering Trans-\nformer (QAT), specifically designed to jointly reason over\nthe LM and KG in a unified manner, without an explicit KG\nencoder, e.g., GNN. We encode relation-centric information\nof the KG in the form of Meta-Path tokens. Along with LM\noutputs, the Meta-Path tokens are input into our Relation-\nAware Self-Attention (RASA) module, whose cross-modal\ninformation integration is facilitated by our Cross-Modal\nRelative Position Bias. The diverse structural and seman-\ntic relations along the meta-paths between question-answer\nentities can be easily integrated with the LM context via\nMeta-Path Tokens. Also, the Meta-Path token sets tend to\nbe diverse, bringing enhanced expressivity in self-attention.\nMoreover, we present a method to exchange information in\na unified manner, guided by the Cross-Modal Relative Posi-\ntion Bias. This provides a more flexible means to fuse LM\nand KG, encouraging information exchange between rele-\nvant entities across different modalities.\nThen, our contributions are as follows:\n• We introduce the Meta-Path token, a novel embedding to\nencode KG information based on diverse relations along\nmeta-paths.\n• We present Cross-Modal Relative Position Bias, which\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13457\nenables more flexible information exchange encourag-\ning interactions between relevant entities across different\nmodalities: LM and KG.\n• We propose Question Answering Transformer, a relation-\naware language-graph transformer for question answer-\ning by joint reasoning over LM and KG.\n• We achieve state-of-the-art performances on common-\nsense question answering: CommonsenseQA and Open-\nBookQA; and on medical question answering: MedQA-\nUSMLE, thereby demonstrating our method’s joint rea-\nsoning capability.\n2 Related Works\nKnowledge Graph based QA. KGs are structured data\ncontaining relational information on concepts (e.g., Con-\nceptNet (Speer, Chin, and Havasi 2017), Freebase (Bol-\nlacker et al. 2008), and Wikidata (Vrande ˇci´c and Kr ¨otzsch\n2014)), and they have been widely explored in the QA do-\nmain. Recent studies on KG based QA process the KG in an\nend-to-end manner with graph neural networks (Feng et al.\n2020; Yasunaga et al. 2021; Wang et al. 2022) or language\nmodels (Wang et al. 2020; Bansal et al. 2022) for knowledge\nmodule. Most of them focus on adequately processing the\nKG first, while independently training the LM in parallel.\nThe two modalities are then fused in the final step to render\na QA prediction. But this shallow fusion does not facilitate\ninteractions between the two modalities, and several meth-\nods (Sun et al. 2022; Zhang et al. 2022) to fuse LM and KG\nin the earlier layers have recently been introduced as well.\nThe commonality of these works is that the KG nodes are\nprocessed with the GNN message passing scheme, which\nare then fused with LM embeddings. Contrary to this line of\nworks, our method manages the joint reasoning process by\nexplicitly learning the multi-hop relationships, and facilitat-\ning active cross-modal attention.\nMulti-relational Graph Encoder. GNNs have shown su-\nperior performance on various type of graphs in gen-\neral (Kipf and Welling 2017; Veliˇckovi´c et al. 2018). How-\never, additional operations may be required to encode the\nrich relational information of multi-relational graphs like\nthe KGs. The KG is a directed graph whose edges are\nmulti-relational, and several relevant works (Schlichtkrull\net al. 2018; Wang et al. 2019) have been introduced to\nprocess such graph types. Unlike homogeneous graph neu-\nral networks such as the GCN (Kipf and Welling 2017),\nexploiting rich semantic information is essential to en-\ncoding multi-relational graphs like KGs. For instance, R-\nGCN (Schlichtkrull et al. 2018) performs graph convo-\nlution for each relation to consider the different interac-\ntions between entities. HAN (Wang et al. 2019) adopted\nmeta-paths to enrich semantic information between nodes\nby defining path-based neighborhoods. Moreover, (Hwang\net al. 2020) has proposed meta-path prediction as a self-\nsupervised learning task to encourage GNNs to comprehend\nrelational information.\n3 Preliminaries\nQuestion Answering with Knowledge Graphs. In this\npaper, we consider the question answering task via joint rea-\nsoning over the LM and KG. The objective of the task is\nto understand the context of the given question by jointly\nreasoning on the language and structured relational data.\nSpecifically, we use masked LMs (e.g., RoBERTa (Liu et al.\n2019), SapBERT (Liu et al. 2021a)) and multi-relational\nKGs (e.g., ConceptNet).\nIn multiple-choice question answering (MCQA), word\nentities in question q and answer choice a ∈ C\nare concatenated to form an input sequence X =\u0002\n[cls], xq1 , . . . ,xq|q|, [sep], xa1 , . . . ,xa|a|\n\u0003\n. Then, a pre-\ntrained language model gLM computes the context tokens\nHLM as HLM = gLM(X). To enable joint reasoning with\nexternal knowledge, we leverage the KG containing struc-\ntured relational data. Following prior works (Lin et al. 2019;\nYasunaga et al. 2021; Wang et al. 2022), we extract a multi-\nrelational subgraph G = ( V, E) from the full KG for each\nquestion and answer choice pair. We denote V as a set of\nnodes (entities) and E ⊆ V × R × Vas the set of subgraph\nedges where R indicates the edge types between entities.\nAlso, we respectively denote Vq ⊆ Vand Va ⊆ Vas the\nset of nodes mentioned in the given question q and answer\nchoice a. Given the multi-relational subgraph, a graph en-\ncoder gKG encodes G as HKG = gKG(G). Given multimodal\ninformation HLM and HKG, module F can be applied to out-\nput a joint representation Za∈C = F(HLM, HKG). Here, F\ncan be as simple as feature concatenation or as complex as\nthe Transformer module (Vaswani et al. 2017).\nMeta-Paths. The meta-path (Sun et al. 2011) is defined as\na path on a graph G whose edges are multi-relational edges,\ni.e., v1\nr1\n− →v2\nr2\n− →. . .\nrl\n− →vl+1, where rl ∈ Rrefers to\nthe edge type of the lth edge within the meta-path. Then, we\nmay view the meta-path as a composite relationr = r1 ◦r2 ◦\n. . .◦ rl that defines the relationship between nodes v1 and\nvl+1. The meta-path can be naturally extended to the multi-\nhop relation encoding, providing a useful means to define\nthe relationship between an arbitrary entity pair from the KG\nsubgraph. For instance, a meta-path (jeans – related\nto →\nmerchants – at location → shopping mall) can be defined\nto describe the relationship between ‘jeans’ and ‘shopping\nmall’ for the question in Figure 3.\n4 Question Answering Transformer\nIn this section, we introduce Question Answering Trans-\nformer (QAT). QAT is a relation-aware language-graph\nTransformer for knowledge-based question answering,\nwhich performs joint reasoning over LM and KG. We take a\nnovel approach of encoding the meta-path between KG en-\ntities to jointly self-attend to KG and LM representations.\nThis is enabled by our Meta-Path tokens (Section 4.1) and\nour Relation-Aware Self-Attention (Section 4.2) module.\n4.1 Meta-Path Token Embeddings\nThe gist of question answering is to understand the rela-\ntionship between entities. Accordingly, QAT learns embed-\n13458\n[CLS]Q A[SEP]Input sequence \n<latexit sha1_base64=\"ekc4fn/4syz/yOj03IfOrtwNDIo=\">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0484K9oG2yGQ6rcG8mEyEUnXrD7jV3xL/QP/CO2MKahGdkOTMufecmXuvlwR+qhzntWBNTc/MzhXnSwuLS8sr5dW1ZhpnkosGj4NYtj2WisCPREP5KhDtRAoWeoFoeddHOt66ETL14+hMDRPRDdkg8vs+Z4qo807I1BVngd2+LFecqmOWPQncHFSQr3pcfkEHPcTgyBBCIIIiHIAhpecCLhwkxHUxIk4S8k1c4A4l0maUJSiDEXtN3wHtLnI2or32TI2a0ykBvZKUNrZIE1OeJKxPs008M86a/c17ZDz13Yb093KvkFiFK2L/0o0z/6vTtSj0cWBq8KmmxDC6Op67ZKYr+ub2l6oUOSTEadyjuCTMjXLcZ9toUlO77i0z8TeTqVm953luhnd9Sxqw+3Ock6C5U3X3qu7pbqV2mI+6iA1sYpvmuY8ajlFHg7wjPOIJz9aJlVm31v1nqlXINev4tqyHD3OVkxM=</latexit>\nX\nQ…\nWhat would be built on a foundation of rock?pillarsQuestion –Answer choice pair\nType Embedding _.      ____\nQuestion Answering Transformer (QAT)\nSubgraph edges and paths\nSubgraph      \n<latexit sha1_base64=\"3moBhHMRLrEtCazI8PdJAEkS3Cs=\">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0oTsr2Ae2RSbTaRuaF8lEKFW3/oBb/S3xD/QvvDOmoBbRCUnOnHvPmbn3OpHnJtKyXnPGzOzc/EJ+sbC0vLK6VlzfqCdhGnNR46EXxk2HJcJzA1GTrvREM4oF8x1PNJzhiYo3bkScuGFwKUeR6PisH7g9lzNJ1FXbZ3LAmWeeXhdLVtnSy5wGdgZKyFY1LL6gjS5CcKTwIRBAEvbAkNDTgg0LEXEdjImLCbk6LnCHAmlTyhKUwYgd0rdPu1bGBrRXnolWczrFozcmpYkd0oSUFxNWp5k6nmpnxf7mPdae6m4j+juZl0+sxIDYv3STzP/qVC0SPRzpGlyqKdKMqo5nLqnuirq5+aUqSQ4RcQp3KR4T5lo56bOpNYmuXfWW6fibzlSs2vMsN8W7uiUN2P45zmlQ3yvbB2X7Yr9UOc5GnccWtrFL8zxEBWeookbeAR7xhGfj3EiNW+P+M9XIZZpNfFvGwwdLNZMC</latexit>\nG\n…\nLanguage Tokens__ -----Modality Embedding __.     -- Meta-Path Tokens(Sec. 4.1)\n<latexit sha1_base64=\"1EtRmCJaOBAsntT2//xcT4qwvz0=\">AAAC2XicjVHLSsNAFD2Nr1pf9bFzEyyCq5KIqMuimy4UKtgHtFIm06kG8yKZiLV04U7c+gNu9YfEP9C/8M4YwQeiE5KcOfeeM3PvdSLPTaRlPeeMsfGJyan8dGFmdm5+obi41EjCNOaizkMvjFsOS4TnBqIuXemJVhQL5jueaDrn+yrevBBx4obBsRxE4sRnp4HbdzmTRHWLKx2fyTPOPLPa7UhxKYcHh6NusWSVLb3Mn8DOQAnZqoXFJ3TQQwiOFD4EAkjCHhgSetqwYSEi7gRD4mJCro4LjFAgbUpZgjIYsef0PaVdO2MD2ivPRKs5neLRG5PSxDppQsqLCavTTB1PtbNif/Meak91twH9nczLJ1bijNi/dB+Z/9WpWiT62NU1uFRTpBlVHc9cUt0VdXPzU1WSHCLiFO5RPCbMtfKjz6bWJLp21Vum4y86U7Fqz7PcFK/qljRg+/s4f4LGZtneLttHW6XKXjbqPFaxhg2a5w4qqKKGOnlf4R4PeDTaxrVxY9y+pxq5TLOML8u4ewM6lpep</latexit>\nH LM\n<latexit sha1_base64=\"kI8M5xvgKMpN42q5amyj85Ggvo0=\">AAAC2XicjVHLSsNAFD2Nr/qOj52bYBFclUREXRZdWHBTwbZCW8pkHDWYF8lErKULd+LWH3CrPyT+gf6Fd8YIahGdkOTMufecmXuvG/teKm37pWCMjI6NTxQnp6ZnZufmzYXFRhplCRd1HvlRcuyyVPheKOrSk744jhPBAtcXTfdiT8WblyJJvSg8kr1YdAJ2FnqnHmeSqK653A6YPOfMt6rdthRXsn+wP+iaJbts62UNAycHJeSrFpnPaOMEETgyBBAIIQn7YEjpacGBjZi4DvrEJYQ8HRcYYIq0GWUJymDEXtD3jHatnA1przxTreZ0ik9vQkoLa6SJKC8hrE6zdDzTzor9zbuvPdXdevR3c6+AWIlzYv/SfWb+V6dqkTjFjq7Bo5pizajqeO6S6a6om1tfqpLkEBOn8AnFE8JcKz/7bGlNqmtXvWU6/qozFav2PM/N8KZuSQN2fo5zGDQ2ys5W2TncLFV281EXsYJVrNM8t1FBFTXUyfsaD3jEk9Eyboxb4+4j1SjkmiV8W8b9Oynul6I=</latexit>\nH KG\nModality Embedding-v    -\n<latexit sha1_base64=\"bN4DhNIMMiSpTTrjYiPXRNMy5co=\">AAAC3HicjVHLSsNAFD2Nr1pfVRcu3ASL4KokIuqy6MaFQgX7gLaWZDqtoXmRTMRSunMnbv0Bt/o94h/oX3hnTEEtohOSOXPuPSf3zrVD14mFYbxmtKnpmdm57HxuYXFpeSW/ulaNgyRivMICN4jqthVz1/F5RTjC5fUw4pZnu7xm949lvHbNo9gJ/AsxCHnLs3q+03WYJYhq5zeaduB24oFHm87bTcFvxPD0bNTOF4yioZY+CcwUFJCucpB/QRMdBGBI4IHDhyDswkJMTwMmDITEtTAkLiLkqDjHCDnSJpTFKcMitk/fHp0aKevTWXrGSs3oLy69ESl1bJMmoLyIsPybruKJcpbsb95D5SlrG9Bup14esQJXxP6lG2f+Vyd7EejiUPXgUE+hYmR3LHVJ1K3IyvUvXQlyCImTuEPxiDBTyvE960oTq97l3Voq/qYyJSvPLM1N8C6rpAGbP8c5Caq7RXO/aJ7vFUpH6aiz2MQWdmieByjhBGVUVP2PeMKzdqndanfa/Weqlkk16/i2tIcPS2qZQQ==</latexit>\ne LM\n<latexit sha1_base64=\"oRHQn26VQytFpf129IFxMAtC4Q4=\">AAAC3HicjVHLSsNAFD2Nr1pfVRcu3ASL4KokIuqy6ELBTQX7gLaWZDqtoXmRTMRSunMnbv0Bt/o94h/oX3hnTEEtohOSOXPuPSf3zrVD14mFYbxmtKnpmdm57HxuYXFpeSW/ulaNgyRivMICN4jqthVz1/F5RTjC5fUw4pZnu7xm949lvHbNo9gJ/AsxCHnLs3q+03WYJYhq5zeaduB24oFHm87bTcFvxPDsZNTOF4yioZY+CcwUFJCucpB/QRMdBGBI4IHDhyDswkJMTwMmDITEtTAkLiLkqDjHCDnSJpTFKcMitk/fHp0aKevTWXrGSs3oLy69ESl1bJMmoLyIsPybruKJcpbsb95D5SlrG9Bup14esQJXxP6lG2f+Vyd7EejiUPXgUE+hYmR3LHVJ1K3IyvUvXQlyCImTuEPxiDBTyvE960oTq97l3Voq/qYyJSvPLM1N8C6rpAGbP8c5Caq7RXO/aJ7vFUpH6aiz2MQWdmieByjhFGVUVP2PeMKzdqndanfa/Weqlkk16/i2tIcPOsKZOg==</latexit>\ne KG\nRASA(Sec. 4.2)\n<latexit sha1_base64=\"p7chlwR42f7J0lbuIhWMd/9Cq1U=\">AAACzXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0484K9kEfyGQ6bUPzIpkIperWH3CrvyX+gf6Fd8YU1CI6IcmZc+85M/deJ/LcRFrWa86Ym19YXMovF1ZW19Y3iptb9SRMYy5qPPTCuOmwRHhuIGrSlZ5oRrFgvuOJhjM6U/HGjYgTNwyu5DgSXZ8NArfvciaJanV8JoeceWbruliyypZe5iywM1BCtqph8QUd9BCCI4UPgQCSsAeGhJ42bFiIiOtiQlxMyNVxgTsUSJtSlqAMRuyIvgPatTM2oL3yTLSa0ykevTEpTeyRJqS8mLA6zdTxVDsr9jfvifZUdxvT38m8fGIlhsT+pZtm/lenapHo40TX4FJNkWZUdTxzSXVX1M3NL1VJcoiIU7hH8Zgw18ppn02tSXTtqrdMx990pmLVnme5Kd7VLWnA9s9xzoL6Qdk+KtuXh6XKaTbqPHawi32a5zEqOEcVNfIO8IgnPBsXRmrcGvefqUYu02zj2zIePgB4VZMV</latexit>\nZ\nFFN\n<latexit sha1_base64=\"SU8RD1J7+wr0GeRDavnehGSosqA=\">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040Khgn2ALZKk0zqYF5mJUKtLf8Ct/pf4B/oX3hmnoBbRCUnOnHvOnbn3+mnIhXSc14I1NT0zO1ecLy0sLi2vlFfXmiLJs4A1giRMsrbvCRbymDUklyFrpxnzIj9kLf/6SMVbNywTPInP5TBl3cgbxLzPA08S1e5IHjFhn1yWK07V0cueBK4BFZhVT8ov6KCHBAFyRGCIIQmH8CDouYALBylxXYyIywhxHWe4R4m8OakYKTxir+k7oN2FYWPaq5xCuwM6JaQ3I6eNLfIkpMsIq9NsHc91ZsX+lnukc6q7Denvm1wRsRJXxP7lGyv/61O1SPRxoGvgVFOqGVVdYLLkuivq5vaXqiRlSIlTuEfxjHCgneM+29ojdO2qt56Ov2mlYtU+MNoc7+qWNGD35zgnQXOn6u5V3bPdSu3QjLqIDWxim+a5jxqOUUdDz/ERT3i2Ti1h3Vp3n1KrYDzr+Lashw9P2ZI7</latexit>\n⇥ L\n<latexit sha1_base64=\"2TcU+gzoOLdpB2c/IKloB19JZMs=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040oq2IePIpN02g7mRTIRSunWH3Cr3yX+gf6Fd8YpqEV0QpIz595zZu69XhKITDrOa8GamZ2bXygulpaWV1bXyusbzSzOU583/DiI07bHMh6IiDekkAFvJylnoRfwlnd3ouKte55mIo4u5DDhnZD1I9ETPpNEXd6ETA68nn11W644VUcvexq4BlRgVj0uv+AGXcTwkSMERwRJOABDRs81XDhIiOtgRFxKSOg4xxgl0uaUxSmDEXtH3z7trg0b0V55Zlrt0ykBvSkpbeyQJqa8lLA6zdbxXDsr9jfvkfZUdxvS3zNeIbESA2L/0k0y/6tTtUj0cKRrEFRTohlVnW9cct0VdXP7S1WSHBLiFO5SPCXsa+Wkz7bWZLp21Vum4286U7Fq75vcHO/qljRg9+c4p0Fzr+oeVN3z/Urt2Iy6iC1sY5fmeYgaTlFHg7xDPOIJz9aZJa2RNf5MtQpGs4lvy3r4AFfbkqM=</latexit>\nZ type\n<latexit sha1_base64=\"+15mMs8z4cGZABQp/W65+qw2cVw=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl040pasLVSRSbTaTs0L5KJUEq3/oBb/S7xD/QvvDOmoBbRCUnOnHvPmbn3erEvU+U4rwVrbn5hcam4XFpZXVvfKG9utdIoS7ho8siPkrbHUuHLUDSVVL5ox4lggeeLK294puNX9yJJZRReqlEsbgPWD2VPcqaIur4JmBp4PbtxV644Vccsexa4OaggX/Wo/IIbdBGBI0MAgRCKsA+GlJ4OXDiIibvFmLiEkDRxgQlKpM0oS1AGI3ZI3z7tOjkb0l57pkbN6RSf3oSUNvZIE1FeQlifZpt4Zpw1+5v32Hjqu43o7+VeAbEKA2L/0k0z/6vTtSj0cGJqkFRTbBhdHc9dMtMVfXP7S1WKHGLiNO5SPCHMjXLaZ9toUlO77i0z8TeTqVm953luhnd9Sxqw+3Ocs6B1UHWPqm7jsFI7zUddxA52sU/zPEYN56ijSd4BHvGEZ+vCUtbYmnymWoVcs41vy3r4AEJ7kpo=</latexit>\nQ type\n<latexit sha1_base64=\"EXRHRIVlDNl1M9CBGUOO3SQF/4g=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdelj40oq2Ie0RZJ02g7mxWQiSOnWH3Cr3yX+gf6Fd8YpqEV0QpIz595zZu69fhryTDrOa8GamZ2bXygulpaWV1bXyusbjSzJRcDqQRImouV7GQt5zOqSy5C1UsG8yA9Z0789U/HmHRMZT+IreZ+ybuQNYt7ngSeJuu5Enhz6ffvkplxxqo5e9jRwDajArFpSfkEHPSQIkCMCQwxJOISHjJ42XDhIietiRJwgxHWcYYwSaXPKYpThEXtL3wHt2oaNaa88M60O6JSQXkFKGzukSShPEFan2Tqea2fF/uY90p7qbvf0941XRKzEkNi/dJPM/+pULRJ9HOkaONWUakZVFxiXXHdF3dz+UpUkh5Q4hXsUF4QDrZz02daaTNeueuvp+JvOVKzaByY3x7u6JQ3Y/TnOadDYq7oHVfdyv3J8akZdxBa2sUvzPMQxzlFDnbwjPOIJz9aFJa2RNf5MtQpGs4lvy3r4ABx7koo=</latexit>\nA type\n<latexit sha1_base64=\"raulXzqK7CAAjNb/znDihlh52q4=\">AAACzHicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdSm6caUV7EPaIkk6bQfzYjIRpHTrD7jV7xL/QP/CO+MU1CI6IcmZc+85M/dePw15Jh3ntWDNzM7NLxQXS0vLK6tr5fWNRpbkImD1IAkT0fK9jIU8ZnXJZchaqWBe5Ies6d+eqnjzjomMJ/GVvE9ZN/IGMe/zwJNEXXciTw79vn1xU644VUcvexq4BlRgVi0pv6CDHhIEyBGBIYYkHMJDRk8bLhykxHUxIk4Q4jrOMEaJtDllMcrwiL2l74B2bcPGtFeemVYHdEpIryCljR3SJJQnCKvTbB3PtbNif/MeaU91t3v6+8YrIlZiSOxfuknmf3WqFok+jnQNnGpKNaOqC4xLrruibm5/qUqSQ0qcwj2KC8KBVk76bGtNpmtXvfV0/E1nKlbtA5Ob413dkgbs/hznNGjsVd2Dqnu5Xzk+MaMuYgvb2KV5HuIYZ6ihTt4RHvGEZ+vcktbIGn+mWgWj2cS3ZT18AD27kpg=</latexit>\nO type\nKGSubgraphretrieval\n<latexit sha1_base64=\"3g/w3ShA9GVgDlQy4ii1dal35pM=\">AAAC5XicjVHLSsNAFD2Nr1pfVZdugkVwISWtpXVZdOOySquCSplMxxqcPEgmQilu3bkTt/6AW/0V8Q/0L7wzpqiI6IQk5557z5m5c91IeolynJecNTY+MTmVny7MzM7NLxQXlw6SMI256PBQhvGRyxIhvUB0lKekOIpiwXxXikP3YkfnDy9FnHhh0FaDSJz6rB94Zx5niqhu0T7xmTrnTNrt7uXGZ7RPuCekYt1iySnXqtV6Y9P+CSplx6wSstUKi884QQ8hOFL4EAigCEswJPQcowIHEXGnGBIXE/JMXuAKBdKmVCWoghF7Qd8+RccZG1CsPROj5rSLpDcmpY010oRUFxPWu9kmnxpnzf7mPTSe+mwD+ruZl0+swjmxf+lGlf/V6V4UzrBlevCop8gwujueuaTmVvTJ7S9dKXKIiNO4R/mYMDfK0T3bRpOY3vXdMpN/NZWa1THPalO86VPSgEdTtH8HB9VypV6u7NVKze1s1HmsYBXrNM8GmthFCx3yvsYDHvFk9a0b69a6+yi1cplmGd+Wdf8OSa+b3w==</latexit>\nT v , R , \u0000Language Model MLP\nFigure 1: Overall architecture of QAT. The Question Answering Transformer (QAT) is a relation-aware language-graph trans-\nformer for knowledge-based question answering tasks. Given a question-answer choice pair, a relevant subgraph G is retrieved\nfrom the knowledge graph. Then, the Meta-Path token is encoded for each n-hop meta-path with type embeddings based on\nstructural and semantic relationships between nodes (Section 4.1). Then, the set of Meta-Path tokens HKG are concatenated\nwith the LM outputs HLM, to be fed into our Relation-Aware Self-Attention module. The Relation-Aware Self-Attention mod-\nule effectively aggregates multimodal information via the Cross-Modal Relative Position Bias (Section 4.2). After L attention\nblocks, our QAT renders Z for each answer choice, which is later passed through a prediction head to output a logit value.\ndings that represent the relationship between each node pair,\ndubbed Meta-Path tokens.\nWhen encoding the edge feature, the edge type r ∈ R\nis utilized in the form of a one-hot vector. The node types\nare also provided as input, but the node features are not\ndirectly employed when learning the embeddings. Follow-\ning Yasunaga et al., a node type among Tv = {Z, Q, A, O}\nis selected based on the node’s source. To elaborate, Z is a\nnode which is deliberately inserted to connect all question\nand answer entities for context aggregation (similar to the\ncls token in Transformers),Q and A each denote the entities\nfrom the question and answer choice, respectively. O is the\n“other” type of node that does not fall in the three categories,\nbut constitutes the extracted subgraph. We further input the\nfeature translation (Bordes et al. 2013) of the head ( h) and\ntail (t) nodes that comprise an edge h\nr\n− →t. We compute the\ntranslation δh,t as δh,t = ft − fh, where ft, fh are the tail\nand head node features, respectively. Then, the embedding\nfor an edge is computed as\nh(h,r,t)∈E = gθ1 ([ϕ(h), r, ϕ(t), δh,t]), (1)\nwhere gθ1 (·) is a MLP and ϕ(·) : V → Tv is a one-hot\nencoder that maps a node v ∈ Vto a node type in Tv.\nIn addition, we aim to diversify the embedding set by in-\ncorporating multi-hop paths. Specifically, we learn the em-\nbedding for each path ψ between a question node and an an-\nswer node; either one is the head or tail node. We consider\npaths of length n, and utilize a separate network for each n-\nhop path embeddings, where n-hop path ψ from h to t is\nfeaturized as [ϕ(h), r1, ϕ(v1), . . . , ϕ(vn−1), rn, ϕ(t), δh,t].\nThen, for instance, the Meta-Path token of a 2-hop path is\nhψ∈Ψ2 = gθ2 ([ϕ(h), r1, ϕ(v1), r2, ϕ(t), δh,t]), (2)\nwhere gθ2 denotes the 2-hop path encoder and v1 is an in-\ntermediate node. Ψ2 refers to the set of all 2-hop paths that\nconnects h and t. Since an edge can be interpreted as a 1-hop\npath, we may concisely define the Meta-Path token set as\nHKG =\nK[\nk=1\n{hψ|ψ ∈ Ψk}, (3)\nwhere we set K to 2 in our experiments.\nCompared to QAT, many previous works have employed\ngraph neural networks to encode the KG (Yasunaga et al.\n2021; Wang et al. 2022; Lin et al. 2019). Generally adopting\nthe message-passing mechanism, the neighboring nodes and\nedge features are aggregated by propagating a layer, and the\nnodes are finally pooled for prediction. Although the nodes\nconsequently incorporate relational information, the objec-\ntive of message-passing is to learn node features rather than\nrelations, characterizing GNNs to be innately node-centric.\nOn the other hand, our QAT explicitly encodes the relational\ninformation into Meta-Path tokens. By utilizing Meta-Path\ntokens for joint reasoning, the model takes advantage of\nthe relation-centric tokens which contain information on the\nstructural and semantic relations within the subgraph, see\nSection 4.3 for further discussions.\nDrop-MP. Inspired by (Rong et al. 2019), we selectively\napply a simple tactic of stochastically dropping meta-paths\nfor regularization. Considering that each meta-path corre-\nsponds to one Meta-Path token, we simply drop a certain\nportion of Meta-Path tokens during training.\n13459\nAttention Map Relative Position Bias\nWhere would you see a dinosaur bone? \n(A) human body (B) arm (C) tomb (D) pyramid (E) museum\nmuseum-bone\ndinosaur\nbone\nmuseum\nsee\n…\n…\nLanguage Tokens\nMeta-Path Tokens\n<latexit sha1_base64=\"a5CovvUDnotEsjWyH/24xANIk5Q=\">AAAC3XicjVHLSsNAFD3GV62vqhvBTbAIrkoioi5FFxbcVLBWsKVMplMdzItkIkqpO3fi1h9wq78j/oH+hXfGCGoRnZDkzLn3nJl7rxf7MlWO8zJkDY+Mjo0XJoqTU9Mzs6W5+aM0yhIu6jzyo+TYY6nwZSjqSipfHMeJYIHni4Z3vqvjjQuRpDIKD9VVLFoBOw1lV3KmiGqXFpsytJsBU2ec+Xa13VTiUvX29/rtUtmpOGbZg8DNQRn5qkWlZzTRQQSODAEEQijCPhhSek7gwkFMXAs94hJC0sQF+iiSNqMsQRmM2HP6ntLuJGdD2mvP1Kg5neLTm5DSxgppIspLCOvTbBPPjLNmf/PuGU99tyv6e7lXQKzCGbF/6T4z/6vTtSh0sWVqkFRTbBhdHc9dMtMVfXP7S1WKHGLiNO5QPCHMjfKzz7bRpKZ23Vtm4q8mU7N6z/PcDG/6ljRg9+c4B8HRWsXdqLgH6+XtnXzUBSxhGas0z01so4oa6uR9jQc84slqWzfWrXX3kWoN5ZoFfFvW/TsEOpkd</latexit> \n2 HKG\n<latexit sha1_base64=\"b4gQVMgq40wUnnGQqoNoETSdvQE=\">AAAC3XicjVHLSsNAFD3Gd31V3QhugkVwVRIRdSm66UKhgrWFtpTJdKqDeZFMRCm6cydu/QG3+jviH+hfeGeMoBbRCUnOnHvPmbn3erEvU+U4L0PW8Mjo2PjEZGFqemZ2rji/cJxGWcJFjUd+lDQ8lgpfhqKmpPJFI04ECzxf1L2zPR2vn4sklVF4pC5j0Q7YSSh7kjNFVKe41JKh3QqYOuXMtyudlhIXqr9/cNUplpyyY5Y9CNwclJCvalR8RgtdRODIEEAghCLsgyGlpwkXDmLi2ugTlxCSJi5whQJpM8oSlMGIPaPvCe2aORvSXnumRs3pFJ/ehJQ2VkkTUV5CWJ9mm3hmnDX7m3ffeOq7XdLfy70CYhVOif1L95n5X52uRaGHbVODpJpiw+jqeO6Sma7om9tfqlLkEBOncZfiCWFulJ99to0mNbXr3jITfzWZmtV7nudmeNO3pAG7P8c5CI7Xy+5m2T3cKO3s5qOewDJWsEbz3MIOKqiiRt7XeMAjnqyOdWPdWncfqdZQrlnEt2XdvwMU4pkk</latexit> \n2 HLM\nCross-modal Matching\ndinosaur bone museumsee\nmuseum-bone museum-dinosaur_bones\n…\n…\nUnmatched attention weights\nMatched tokens\nAdd learnable \nbias\n<latexit sha1_base64=\"LlsVES0p/fHNBz78QBVDTG35900=\">AAACyXicjVHLSsNAFD2Nr1pfVZdugkVwVRIRdVl0I7iwgn1AW2SSTmtsXiYTsRZX/oBb/THxD/QvvDNOQS2iE5KcOfeeM3PvdWLfS4VlveaMqemZ2bn8fGFhcWl5pbi6Vk+jLHF5zY38KGk6LOW+F/Ka8ITPm3HCWeD4vOEMjmS8ccOT1IvCczGMeSdg/dDreS4TRNXbpwHvs4tiySpbapmTwNagBL2qUfEFbXQRwUWGABwhBGEfDCk9LdiwEBPXwYi4hJCn4hz3KJA2oyxOGYzYAX37tGtpNqS99EyV2qVTfHoTUprYIk1EeQlheZqp4plyluxv3iPlKe82pL+jvQJiBS6J/Us3zvyvTtYi0MOBqsGjmmLFyOpc7ZKprsibm1+qEuQQEydxl+IJYVcpx302lSZVtcveMhV/U5mSlXtX52Z4l7ekAds/xzkJ6jtle69sn+2WKod61HlsYBPbNM99VHCMKmrkfYVHPOHZODGujVvj7jPVyGnNOr4t4+EDdpORgg==</latexit> \n⌦\n<latexit sha1_base64=\"tfMqUnzbCsi1UNL73B+bc4Trc+0=\">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwFZIoWndFN26ECvYBbSnJdFpD8yKZCLW69Afc6n+Jf6B/4Z0xBV0UnZDkzLnn3Jl7rxv7XipM872gLSwuLa8UV0tr6xubW+XtnWYaZQnjDRb5UdJ2nZT7XsgbwhM+b8cJdwLX5y13fCHjrTuepF4U3ohJzHuBMwq9occcQVS7GwV85PTtfrliGmfVI/uoqpuGqZYEtm0TsHKmgnzVo/IbuhggAkOGABwhBGEfDlJ6OrBgIiauhylxCSFPxTkeUSJvRipOCofYMX1HtOvkbEh7mTNVbkan+PQm5NRxQJ6IdAlheZqu4pnKLNl5uacqp7zbhP5unisgVuCW2L98M+V/fbIWgSGqqgaPaooVI6tjeZZMdUXeXP9RlaAMMXESDyieEGbKOeuzrjypql321lHxD6WUrNyzXJvhU96SBjyboj4fNG3DOjGs6+NK7TwfdRF72MchzfMUNVyijoaa4zNe8Kpdaal2rz18S7VC7tnFr6U9fQHHRpJu</latexit> \n!2\n<latexit sha1_base64=\"lw464iyojrxFxyOxoYliEmqNR9o=\">AAACy3icjVHLSsNAFD2Nr1pfVZdugkVwFZIoWndFN26ECvYBbSnJdFpD8yKZCLW69Afc6n+Jf6B/4Z0xBV0UnZDkzLnn3Jl7rxv7XipM872gLSwuLa8UV0tr6xubW+XtnWYaZQnjDRb5UdJ2nZT7XsgbwhM+b8cJdwLX5y13fCHjrTuepF4U3ohJzHuBMwq9occcQVS7GwV85PStfrliGmfVI/uoqpuGqZYEtm0TsHKmgnzVo/IbuhggAkOGABwhBGEfDlJ6OrBgIiauhylxCSFPxTkeUSJvRipOCofYMX1HtOvkbEh7mTNVbkan+PQm5NRxQJ6IdAlheZqu4pnKLNl5uacqp7zbhP5unisgVuCW2L98M+V/fbIWgSGqqgaPaooVI6tjeZZMdUXeXP9RlaAMMXESDyieEGbKOeuzrjypql321lHxD6WUrNyzXJvhU96SBjyboj4fNG3DOjGs6+NK7TwfdRF72MchzfMUNVyijoaa4zNe8Kpdaal2rz18S7VC7tnFr6U9fQHE5pJt</latexit> \n!1\n…\n…\nFigure 2: Illustration of Cross-Modal Relative Position Bias.\nEach Meta-Path token incorporates at least one head h and\ntail t node. The two nodes are each matched with an LM\ntoken, whose corresponding position is parameterized in our\nrelative position bias Ω. Then, Ω is added to the attention\nmap (Eq. (8)).\n4.2 Relation-Aware Self-Attention\nAfter retrieving the LM token embeddings HLM and Meta-\nPath tokens HKG (Eq. (3)), we now jointly reason over the\nconcatenated token set to compute a logit value for each\nquestion and answer choice pair.\nLanguage-Graph Joint Self-Attention. The multi-head\nself-attention module in the Transformer model is well\nknown for its learning capacity and flexibility. By concate-\nnating the two modalities and applying self-attention, each\ntoken aggregates features based on inter- and intra-modal\nrelations. We also add learnable modality embeddings, eLM\nand eKG, to the query and key tokens in order to encode the\nmodality source into feature vectors. Then, the self-attention\noutput ˆZ is computed by\nX = [HLM; HKG], ˜X = [HLM + eLM; HKG + eKG] (4)\nQ = ˜XWQ, K = ˜XWK, V = XWV (5)\nˆZ = Softmax(QK⊤/\n√\nd) V , (6)\nwhere WQ, WK, and WV refers to the linear projection\nweights, and d is the key embedding dimension. Note, we\ndescribed a single-headed case to avoid clutter.\nCross-Modal Relative Position Bias. Although the\nTransformer architecture alone is powerful in itself, provid-\ning guidance on learning how to aggregate information fur-\nther enhances performance (Liu et al. 2021b; Yang et al.\n2021; Wu et al. 2021). To encourage joint reasoning, we\nutilize a learnable relative position bias Ω that controls the\ncross-modal attention weights between relevant tokens.\nThe interrelation of a token pair from the two modalities\nis determined by whether the natural language representa-\ntion of each entity is similar to each other. We utilize the\npretrained GloVE (Pennington, Socher, and Manning 2014)\nword embeddings to vectorize each entity. In the case of\nKG, several node entities contain a series of words joined\nwith underbars (e.g., jewelry\nstore). For those entities, we\ntake the average of the discretized word’s GloVE embed-\nding as its representation vector (e.g., [GloVE(“jewelry”) +\nGloVE(“store”)] / 2). Then, for each relation token in HKG,\nwe take its pertinent head and tail entity to compute their\nsimilarity with LM entities using cosine similarity, respec-\ntively. The head and tail nodes of a Meta-Path token are\nmapped to an LM token in HLM by selecting the LM token\nwith the highest similarity. That is, each Meta-Path token\nembedding is matched with two LM tokens. Based on this\nmatching, our Cross-Modal Relative Position Bias (RPB)\nΩ ∈ R|X|×|X| is defined. With abuse of notation, i, j re-\nfer to the ith, jth token:\nLet\npij = 1\n\"\ni = argmax\ni′∈HKG\nf(i′\nv) · f(j), ∃v ∈ {h, t}\n#\n· 1 [j ∈ HLM] ,\nqij = 1\n\"\ni = argmax\ni′∈HLM\nf(i′) · f(jv), ∃v ∈ {h, t}\n#\n· 1 [j ∈ HKG] .\nThen,\nΩ[m, n] =\n\n\n\nω1 , if pmn = 1, qmn = 0,\nω2 , if pmn = 0, qmn = 1,\n0 , otherwise\n(7)\nwhere f(·) is the GloVE-based embedding,iv is the text rep-\nresentation of node v from token i, and 1[·] is the indicator\nfunction. ω1, ω2 are learnable scalar parameters, and m and\nn are token indices of X. Also note that Ω is separately de-\nfined for each attention head, see Figure 2.\nThen, our Relation-Aware Self-Attention (RASA) can be\nfinally written as\nˆZ = Softmax(QK⊤/\n√\nd + Ω) V , (8)\nwhich is followed by an FFN layer that aggregates multiple\nheads. To enlist our transformer layer computation :\nˆZ = RASA(LN(X)) + X\nZ = FFN(LN( ˆZ)) + ˆZ,\n(9)\nwhere LN is the layer norm, and RASA refers to our\nRelation-Aware Self-Attention module (Eq. (8)).\nFurthermore, to encourage a positive bias, we use a regu-\nlarizer term in the final loss as\nL = LCE − λ\nLX\nl=1\nHX\nh=1\nσ(ω(hl)) (10)\nwhere LCE is the cross entropy loss for question answering,\nand σ can be a nonlinear function (e.g., logarithm, tanh).\nω(hl) refer to the relative position bias terms from the hth\nattention head of layer l, and λ is a constant.\n4.3 Comparisons with Existing Methods\nIn this section, we discuss QAT with GNN-based methods\nand RN-based methods for question answering problems.\n13460\nGNN-based methods. GNNs learn node representations\nwith the message-passing scheme, which recursively aggre-\ngates the representations of neighbors as follows (Gilmer\net al. 2017; Kipf and Welling 2017; Yasunaga et al. 2021):\nh(l+1)\ni = UPDATE\n\n X\nvj∈Ni\nα(l+1)\ni,j m(l+1)\ni,j\n\n, (11)\nwhere mi,j is the message that node vj sends to vi, αi,j\ndenotes the attention weight, and UPDATE (·) indicates a\nfunction that updates the node embedding by considering\nmessages from neighborhoods.\nBased on the learned node features, a representation for a\ngraph G can be obtained with a set function fgnn (i.e., atten-\ntive pooling) as follows:\nGNN (G) = fgnn\n\u0000\b\nh′\n1, h′\n2, . . . ,h′\nn\n\t\u0001\n, (12)\nwhere h′\ni is the embedding of node vi from the final layer.\nLike Eq. (12), GNN-based models with message-passing\nperform reasoning at the node level; so, they are not read-\nily available for path-based encoding and interpretation. On\nthe other hand, QAT performs reasoning at the (meta-)path\nlevel, considering a graph as a set of relations. By tokenizing\nthe meta-path relations, its reasoning ability is enhanced. We\nverify this effect by comparing the method by which the KG\nis encoded, in Section 6.\nRN-based methods. The Relation Network (RN) (San-\ntoro et al. 2017) is a neural network architecture that rep-\nresents an object set O based on the relations between the\nobjects within the set. A general form of RN is\nRN (O) = f ({g (oi, oj) |oi, oj ∈ O}) , (13)\nwhere function g encodes the relation of the pair of objects\nin O, and f learns to represent the set of encoded relations.\nConsidering that knowledge-based QA problems also entail\nrepresenting the set of relations between concept entities, the\nQA problem can naturally be regarded an extension of RN.\nThat is, given a pair of question and answer choice, a QA\nproblem solver encodes the set of entity relations as written\nin (Feng et al. 2020) as\nRN (G) =f ({g (vi, r, vj) |vi ∈ Vq, vj ∈ Va, (vi, r, vj) ∈ E}) ,\n(14)\nwhere function g (e.g., MLP) outputs the relations between\nquestion and answer entities, and function f (e.g., pooling\nfunction) aggregates the relations. Vq and Va refer to the en-\ntities from the question and answer choice, respectively.\nOur QAT is a more flexible and powerful architecture that\ngeneralizes the relation network. Specifically, QAT can be\nformulated as follows:\nQAT\n\u0010\nG, HLM\n\u0011\n= f({g (vi, r1, . . . , rk, vj) |(vi, r1, . . . , rk, vj) ∈ Ψk,\nk = 1, . . . , K} ∪\nn\nhLM\ncls, hLM\n1 , . . . ,hLM\nN\no\n),\n(15)\nwhere Ψk indicates a set of meta-paths with length k. In\ncontrast to RN (Eq. (14)), our QAT captures the multi-hop\nrelations (i.e., meta-paths) beyond mere one-hop relations\n(i.e., edges), via Meta-Path token embeddings. Moreover,\nQAT extends RN to integrate both the LMs and KGs with\nour relation-aware self-attention.\n5 Experiments\n5.1 Datasets\nWe evaluate our method on three question-answering\ndatasets: CommonsenseQA (Talmor et al. 2019), Open-\nBookQA (Mihaylov et al. 2018), and MedQA-USMLE (Jin\net al. 2021). Dataset statistics are in the supplement.\nCommonsenseQA. This dataset contains questions that\nrequire commonsense reasoning. Since the official test set\nlabels are not publicly available, we mainly report perfor-\nmance on the in-house development (IHdev) and test (IHt-\nest) sets following (Lin et al. 2019). For CommonsenseQA,\nwe adopt ConceptNet (Speer, Chin, and Havasi 2017) as the\nstructured knowledge source.\nOpenBookQA. This dataset requires reasoning with el-\nementary science knowledge. Here, ConceptNet is again\nadopted as the knowledge graph, and we experiment on the\nofficial data split from (Mihaylov and Frank 2018).\nMedQA-USMLE. This dataset originates from the United\nStates Medical License Exam (USMLE) practice sets, re-\nquiring biomedical and clinical knowledge. Thus, we uti-\nlize a knowledge graph provided by (Jin et al. 2021). The\ngraph is constructed via integrating the Disease Database\nportion of the Unified Medical Language System (Boden-\nreider 2004) and the DrugBank (Wishart et al. 2018) knowl-\nedge graph. We use the same data split as (Jin et al. 2021).\n5.2 Baselines and Experimental Setup\nWe conduct experiments to compare our QAT with GNN-\nbased methods and RN-based methods. For a fair compari-\nson, all the baselines and our model use the same LMs in all\nexperiments. The details on the backbone LMs and technical\ndetails are in the supplement.\n5.3 Experimental Results\nCommonsenseQA. Here, we report QAT and its baseline\nperformances evaluated on CommonsenseQA in Table 1.\nOur QAT outperforms all the methods and achieved a new\nSOTA in-house test accuracy (IHtest-Acc.) of 75.4%. The\nproposed model provides a 6.7% performance boost com-\npared to the vanilla RoBERTa language model (RoBERTa-L\n(w/o KG)) that does not utilize the KG. Specifically, the in-\ncrease over GreaseLM and JointLK suggests that our QAT’s\nRelation-Aware Self-Attention improves joint reasoning ca-\npabilities over the LM and KG. Also, the performance gaps\ncompared to GSC, QA-GNNetc. indicates the superiority of\nour relation-centric Meta-Path tokens.\nOpenBookQA. In Table 2, we report QAT performance\non OpenBookQA. QAT with the RoBERTa-Large renders\n71.2% mean test accuracy, which is a significant improve-\nment over previous state-of-the-art by 0.9% and a boost of\n6.4% compared to the vanilla RoBERTa-large LM. Follow-\ning prior works (Yasunaga et al. 2021), we further experi-\nment using AristoRoBERTa as the pre-trained LM, utilizing\nadditional text data. By adopting AristoRoBERTa, we again\n13461\nMethods IHdev-Acc.(%) IHtest-Acc.(%)\nRoBERTa-L (w/o KG) 73.1 (±0.5) 68.7 (± 0.6)\nRGCN 72.7 (±0.2) 68.4 (± 0.7)\nGconAttn 72.6 (±0.4) 68.6 (± 1.0)\nKagNet 73.5 (±0.2) 69.0 (± 0.8)\nRN 74.6 (±0.9) 69.1 (± 0.2)\nMHGRN 74.5 (±0.1) 71.1 (± 0.8)\nQA-GNN 76.5 (±0.2) 73.4 (± 0.9)\nCoSe-CO 78.2 (±0.2) 72.9 (± 0.3)\nGreaseLM 78.5 (±0.5) 74.2 (± 0.4)\nJointLK 77.9 (±0.3) 74.4 (± 0.8)\nGSC 79.1 (±0.2) 74.5 (± 0.4)\nQAT (ours) 79.5 (±0.4) 75.4 (± 0.3)\nTable 1: Performance comparison on CommonsenseQA.\nMethods RoBERTa-Large AristoRoBERTa\nLMs (w/o KG) 64.8 (±2.4) 78.4 (± 1.6)\nRGCN 62.5 (±1.6) 74.6 (± 2.5)\nGconAttn 64.8 (±1.5) 71.8 (± 1.2)\nRN 65.2 (±1.2) 75.4 (± 1.4)\nMHGRN 66.9 (±1.2) 80.6 (±NA)\nQA-GNN 67.8 (±2.8) 82.8 (± 1.6)\nGreaseLM - 84.8 (±NA)\nJointLK 70.3 (±0.8) 84.9 (± 1.1)\nGSC 70.3 (±0.8) 86.7 (± 0.5)\nQAT (ours) 71.2 (±0.8) 86.9 (± 0.2)\nTable 2: Test accuracy comparison onOpenBookQA.\nachieved a state-of-the-art performance of 86.9% mean ac-\ncuracy. This indicates that QAT successfully reasons over\ntwo different modalities and is adaptable to diverse LMs.\nMedQA-USMLE. We additionally conduct an experi-\nment on MedQA-USMLE (Jin et al. 2021) to evaluate the\ndomain generality of our QAT in Table 3. The table shows\nthat QAT achieves the state-of-the-art performance of 39.3%\nand improves performance over various medical LMs. This\nresult indicates that our QAT is effective in reasoning in var-\nious domains beyond commonsense reasoning.\n6 Analysis\nIn this section, we analyze our QAT to answer the follow-\ning research questions: [Q1] Does each component in QAT\nboost performance? [Q2] Are relation-centric Meta-Path to-\nkens really better than node-centric embeddings? [Q3] How\ndoes Relation-Aware Self-Attention utilize the language-\ngraph relations when answering questions?\nAblation Studies. To answer the research question [Q1],\nwe conduct a series of ablation studies to verify the effect\nof each component of QAT. By omitting Drop-MP, all the\nMeta-Path tokens (MP tokens) are used in the training phase.\nMethods Accuracy\nChance 25.0\nPMI 31.1\nIR-ES 35.5\nIR-Custom 36.1\nClinicalBERT-Base 32.4\nBioRoBERTa-Base 36.1\nBioBERT-Base 34.1\nBioBERT-Large 36.7\nSapBERT-Base (w/o KG) 37.2\nQA-GNN 38.0\nGreaseLM 38.5\nQAT (ours) 39.3\nTable 3: Test accuracy comparison onMedQA-USMLE.\nMP tokens RPB Drop-MP IHtest-Acc.(%)\n✓ ✓ ✓ 75.4 (±0.3)\n✓ ✓ 75.3 (±0.7)\n✓ 75.0 (±0.5)\n73.8 (±0.4)\nTable 4: Ablation study on CommonsenseQA.\nDataset Node Node+GNN Meta-Path\nCSQA 73.8 (± 0.4) 73.9 (± 0.2) 75.4 (±0.3)\nOBQA 69.0 (± 1.6) 69.6 (± 0.9) 71.2 (±0.8)\nQuestion Types Node Node+GNN Meta-Path\nFull question set 77.5 77.9 79.8 (↑1.9)\nQuestion w/ negation 75.2 75.9 79.0 (↑3.1)\nQuestion w/ entities ≤ 7 76.2 76.2 79.9 (↑3.7)\nQuestion w/ entities > 7 79.1 79.5 79.7 (↑0.2)\nTable 5: Meta-Path token and Node token Comparison. The\ntest set performances with different KG tokens are com-\npared (above). The development set performance on differ-\nent types of questions are compared (below).\nAblating Cross-Modal RPB is equivalent to having no rela-\ntive postion bias, and by removing MP tokens, we substitute\nthe KG tokens with node features without additional pro-\ncessing. In Table 4, we present the ablation results where\neach component is removed one by one. By removing all our\ncomponents, performance decreased significantly by 1.6%,\nand the biggest degradation was observed when the MP to-\nken was ablated. This proves the effectiveness of encoding\nthe relation-centric information with Meta-Path tokens.\nMeta-Path tokens vs. Node tokens. In order to address\n[Q2], we elucidate the efficacy of Meta-Path tokens (Meta-\nPath) by comparing them with node-centric tokens. We\nextensively compare our relation-centric method with raw\n13462\nshopping mall\nlaundromat hospital\nthrift storeclothing store\nLM\nKG\nLM KG\nWhere would you buy jeans in a place with a \nlarge number of indoor merchants?\nFigure 3: What and How QAT Selects. The attention maps\nfor each question-answer pair is plotted. Cross-modal infor-\nmation is actively combined in the selected answer choice.\nnode-level tokens (Node) as well as the GNN-processed\nnode tokens (Node+GNN), whose GNN architecture is bor-\nrowed from (Yasunaga et al. 2021). Their performances are\ncompared on CommonsenseQA and OpenBookQA in Ta-\nble 5 (above). In all cases, we observed that the Meta-Path\ntoken consistently outperforms the node-centric tokens.\nTo better understand how each reacts to different ques-\ntion traits, we further experimented with diverse question\ntypes such as questions with negation, questions with fewer\nentities (≤ 7), and questions with more entities (>7). In\nTable 5 (below), our Meta-Path tokens show clear domi-\nnance over the GNN-processed node tokens. Especially, it\nperforms well in questions with negations by a significant\nmargin of 3.1%, indicating a superiority in processing log-\nically complex questions. Furthermore, outperformance in\nquestions with less entities (an increase of 3.7%) implies that\nour method better leverages the KG to reason over external\nconcepts selecting the correct answer.\nQualitative Analyses. We conduct qualitative analyses to\nprovide intuition on how QAT understands the relationship\nbetween two modalities, thereby answering [Q3]. We first\nobserve that cross-modal attention is crucial to distinguish-\ning a correct answer from the others.\nOur Relation-Aware Self-Attention (RASA) exhibits rela-\ntively stronger cross-modal attention, especially from KG to\nLM (right-upper quadrant), when a correct question-answer\npair is given. Figure 3 shows that the attention map with\nthe correct answer “shopping mall” is smooth and QAT in-\ntegrates features from both modalities: LM and KG. On the\nother hand, for wrong answers (e.g., laundromat, hospital,\netc.) the cross-modal attention from KG to LM (right-upper\nquadrant) has relatively smaller attention weights and the\nmodel mainly focuses on the LM tokens (left-upper quad-\nrant). These distinctive attention maps are obtained by our\nMeta-Path token generation. Note that QAT extracts a sub-\ngraph from KG for each question-answer pair and constructs\na new set of Meta-Path tokens. Even with one-word answer\nhouse \nentryway\nleft\nleave\nmight\nmay\nshoes entry way\nleft\nleave\nmight\nmay\nshoes way house\nwith RPB \nwithout RPB \nSam left his muddy shoes in the entryway. Where might he be?\n... (D) office building (E) house\n(D) \noffice \nbuilding\n(E) house\nmatched unmatched\nleft might\nhouse \nshoe\nhouse\nhouse \nentryway\nmight\nhouse \nshoe\nentryleft\nFigure 4: The Effect of Cross-Modal Relative Position Bias.\nThe arrows indicate the attention direction, e.g., ‘left–leave’\ngets the most attention from ‘left’ with RPB, and the ar-\nrow colors signify whether the two tokens are matched.\nBy removing our RPB, the attention mapping becomes un-\ninterpretable, and leads to wrong answer selection.\nchoices, Meta-Path token generation results in more diversi-\nfied KG token sets than node-level token generation.\nWe provide additional qualitative analysis to understand\nhow our Cross-Modal RPB further enhances cross-modal at-\ntention. Figure 4 visualizes the attention mappings between\nLM and KG tokens. To avoid clutter, we only show the\nstrongest attention between entities, and the arrows indicate\nthe attention direction. For example, ‘left–leave’ gets the\nmost attention from ‘left’, with RPB. The top blue tokens,\ne.g., ‘left’ and ‘shoes’, are LM tokens, and bottom green\ntokens, e.g., ‘left–leave’ and ‘house–entryway’, are Meta-\nPath tokens. Without our RPB (bottom row in Figure 4),\nwe see more gray arrows indicating that cross-modal atten-\ntions between unmatched (irrelevant) tokens are prevalent.\nIn opposition, the RPB augments the cross-modal attention\nby ω1 (LM → KG) and ω2 (KG → LM) for matched\n(relevant) tokens. Thus, matched tokens have higher atten-\ntion and more red arrows are observed ‘with RPB’ on the\ntop row. To be specific, (1) ‘house (LM)’ is selected by\n‘house–entryway (KG)’ and ‘house–shoe (KG)’, (2) ‘house–\nentryway (KG)’ is selected by ‘entry (LM)’, ‘way (LM)’,\nand ‘house (LM)’ as an entity with the strongest attention.\n7 Conclusion\nWe proposed Question Answering Transformer (QAT),\nwhich effectively combines LM and KG information via\nour Relation-Aware Self-Attention module equipped with\nCross-Modal Relational Position bias. The key to state-of-\nthe-art performance in various QA datasets such as Com-\nmonsenseQA, OpenbookQA, and MedQA-USMLE is our\nMeta-Path token, which learns relation-centric representa-\ntions for the KG subgraph of a question-answer pair.\n13463\nAcknowledgments\nThis work was partly supported by NA VER Corp. and MSIT\n(Ministry of Science and ICT), Korea, under the ICT Cre-\native Consilience program (IITP-2023-2020-0-01819) su-\npervised by the IITP.\nReferences\nBansal, R.; Aggarwal, M.; Bhatia, S.; Kaur, J. N.; and Krishna-\nmurthy, B. 2022. CoSe-Co: Text Conditioned Generative Com-\nmonSense Contextualizer. In NAACL.\nBodenreider, O. 2004. The Unified Medical Language System\n(UMLS): integrating biomedical terminology. Nucleic Acids Re-\nsearch, 32(suppl\n1): D267–D270.\nBollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J.\n2008. Freebase: a collaboratively created graph database for struc-\nturing human knowledge. In SIGMOD.\nBordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and\nYakhnenko, O. 2013. Translating embeddings for modeling multi-\nrelational data. In NeurIPS.\nFeng, Y .; Chen, X.; Lin, B. Y .; Wang, P.; Yan, J.; and Ren, X. 2020.\nScalable Multi-Hop Relational Reasoning for Knowledge-Aware\nQuestion Answering. In EMNLP.\nGilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and Dahl,\nG. E. 2017. Neural Message Passing for Quantum Chemistry. In\nICML.\nHwang, D.; Park, J.; Kwon, S.; Kim, K.; Ha, J.-W.; and Kim, H. J.\n2020. Self-supervised Auxiliary Learning with Meta-paths for Het-\nerogeneous Graphs. In NeurIPS.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What Disease does this Patient Have? A Large-\nscale Open Domain Question Answering Dataset from Medical Ex-\nams. Applied Sciences, 11(14): 6421.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Classification\nwith Graph Convolutional Networks. In ICLR.\nLee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C. H.; and Kang,\nJ. 2020. BioBERT: a pre-trained biomedical language represen-\ntation model for biomedical text mining. Bioinformatics, 36(4):\n1234–1240.\nLin, B. Y .; Chen, X.; Chen, J.; and Ren, X. 2019. KagNet:\nKnowledge-Aware Graph Networks for Commonsense Reasoning.\nIn EMNLP-IJCNLP.\nLiu, F.; Shareghi, E.; Meng, Z.; Basaldella, M.; and Collier, N.\n2021a. Self-Alignment Pretraining for Biomedical Entity Repre-\nsentations. In NAACL-HLT.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach.\narXiv:1907.11692.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.; and\nGuo, B. 2021b. Swin Transformer: Hierarchical Vision Trans-\nformer using Shifted Windows. In ICCV.\nMihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can a\nSuit of Armor Conduct Electricity? A New Dataset for Open Book\nQuestion Answering. In EMNLP.\nMihaylov, T.; and Frank, A. 2018. Knowledgeable Reader: En-\nhancing Cloze-Style Reading Comprehension with External Com-\nmonsense Knowledge. In ACL.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal Vectors for Word Representation. In EMNLP.\nRong, Y .; Huang, W.; Xu, T.; and Huang, J. 2019. DropEdge: To-\nwards Deep Graph Convolutional Networks on Node Classifica-\ntion. In ICLR.\nSantoro, A.; Raposo, D.; Barrett, D. G.; Malinowski, M.; Pascanu,\nR.; Battaglia, P.; and Lillicrap, T. 2017. A simple neural network\nmodule for relational reasoning. In NeurIPS.\nSchlichtkrull, M.; Kipf, T. N.; Bloem, P.; Berg, R. v. d.; Titov, I.;\nand Welling, M. 2018. Modeling Relational Data with Graph Con-\nvolutional Networks. In ESWC.\nSpeer, R.; Chin, J.; and Havasi, C. 2017. ConceptNet 5.5: An Open\nMultilingual Graph of General Knowledge. In AAAI.\nSun, Y .; Han, J.; Yan, X.; Yu, P. S.; and Wu, T. 2011. PathSim: Meta\nPath-Based Top-K Similarity Search in Heterogeneous Information\nNetworks. VLDB Endowment, 4(11): 992–1003.\nSun, Y .; Shi, Q.; Qi, L.; and Zhang, Y . 2022. JointLK: Joint Rea-\nsoning with Language Models and Knowledge Graphs for Com-\nmonsense Question Answering. In NAACL-HLT.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. Common-\nsenseQA: A Question Answering Challenge Targeting Common-\nsense Knowledge. In ACL.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nAll you Need. In NeurIPS.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Lio, P.; and\nBengio, Y . 2018. Graph Attention Networks. InICLR.\nVrandeˇci´c, D.; and Kr¨otzsch, M. 2014. Wikidata: a free collabora-\ntive knowledgebase. In CACM.\nWang, K.; Zhang, Y .; Yang, D.; Song, L.; and Qin, T. 2022. GNN\nis a Counter? Revisiting GNN for Question Answering. In ICLR.\nWang, P.; Peng, N.; Ilievski, F.; Szekely, P.; and Ren, X. 2020.\nConnecting the dots: A knowledgeable path generator for common-\nsense question answering. In EMNLP-Findings.\nWang, X.; Ji, H.; Shi, C.; Wang, B.; Ye, Y .; Cui, P.; and Yu, P. S.\n2019. Heterogeneous Graph Attention Network. In WWW.\nWishart, D. S.; Feunang, Y . D.; Guo, A. C.; Lo, E. J.; Marcu, A.;\nGrant, J. R.; Sajed, T.; Johnson, D.; Li, C.; Sayeeda, Z.; et al. 2018.\nDrugBank 5.0: a major update to the DrugBank database for 2018.\nNucleic Acids Research, 46(Database-Issue): D1074–D1082.\nWu, K.; Peng, H.; Chen, M.; Fu, J.; and Chao, H. 2021. Rethinking\nand Improving Relative Position Encoding for Vision Transformer.\nIn ICCV.\nYang, J.; Li, C.; Zhang, P.; Dai, X.; Xiao, B.; Yuan, L.; and Gao,\nJ. 2021. Focal Attention for Long-Range Interactions in Vision\nTransformers. In NeurIPS.\nYasunaga, M.; Ren, H.; Bosselut, A.; Liang, P.; and Leskovec, J.\n2021. QA-GNN: Reasoning with Language Models and Knowl-\nedge Graphs for Question Answering. In NAACL-HLT.\nZhang, X.; Bosselut, A.; Yasunaga, M.; Ren, H.; Liang, P.; Man-\nning, C. D.; and Leskovec, J. 2022. GreaseLM: Graph REASoning\nEnhanced Language Models for Question Answering. In ICLR.\n13464"
}