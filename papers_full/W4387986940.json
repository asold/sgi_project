{
    "title": "A Dual Frequency Transformer Network for Hyperspectral Image Classification",
    "url": "https://openalex.org/W4387986940",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5086584445",
            "name": "Xin Qiao",
            "affiliations": [
                "Memorial University of Newfoundland"
            ]
        },
        {
            "id": "https://openalex.org/A5090385470",
            "name": "Weimin Huang",
            "affiliations": [
                "Memorial University of Newfoundland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2603834682",
        "https://openalex.org/W3024007459",
        "https://openalex.org/W2036590761",
        "https://openalex.org/W4366493021",
        "https://openalex.org/W2901549327",
        "https://openalex.org/W2942454403",
        "https://openalex.org/W2500751094",
        "https://openalex.org/W2767805377",
        "https://openalex.org/W2743255627",
        "https://openalex.org/W2801400875",
        "https://openalex.org/W2171171329",
        "https://openalex.org/W2158400785",
        "https://openalex.org/W2136251662",
        "https://openalex.org/W1997718749",
        "https://openalex.org/W2009286595",
        "https://openalex.org/W2763731268",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W4225630686",
        "https://openalex.org/W2799390666",
        "https://openalex.org/W2314785379",
        "https://openalex.org/W2029316659",
        "https://openalex.org/W1521436688",
        "https://openalex.org/W2914331134",
        "https://openalex.org/W3093896170",
        "https://openalex.org/W4294310684",
        "https://openalex.org/W2764276316",
        "https://openalex.org/W2789643644",
        "https://openalex.org/W4387802805",
        "https://openalex.org/W3047443805",
        "https://openalex.org/W2609880332",
        "https://openalex.org/W2942170965",
        "https://openalex.org/W2989871747",
        "https://openalex.org/W2977002487",
        "https://openalex.org/W4379535383",
        "https://openalex.org/W2950266692",
        "https://openalex.org/W3043181422",
        "https://openalex.org/W3114720220",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4386919833",
        "https://openalex.org/W4386634608",
        "https://openalex.org/W4296339430",
        "https://openalex.org/W4294068665",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W4210794570",
        "https://openalex.org/W4285303509",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W4292691763",
        "https://openalex.org/W4313147550",
        "https://openalex.org/W4292825900",
        "https://openalex.org/W4319069095",
        "https://openalex.org/W3043183554",
        "https://openalex.org/W2988396473",
        "https://openalex.org/W6838160470",
        "https://openalex.org/W6810653034",
        "https://openalex.org/W6797578546",
        "https://openalex.org/W3170347305",
        "https://openalex.org/W6810394512",
        "https://openalex.org/W3205965083",
        "https://openalex.org/W3103695279",
        "https://openalex.org/W3104418202",
        "https://openalex.org/W4221148784",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2168353136"
    ],
    "abstract": "Hyperspectral images (HSIs) provide detailed spectral information of objects to be detected and play an important role in distinguishing targets with a similar appearance. However, the characteristics of high dimensionality and complexity impose significant challenges for realizing pixelwise classification. Although existing convolutional neural networks and transformer-based models have presented promising performance for HSIs classification, they mainly extract features from spectral&#x2013;spatial perspective and do not fully consider the information in the frequency domain. To address this issue, in this article, we reconsider feature extraction and HSIs classification from the frequency domain. Specifically, inspired by the observation that high-frequency information contains detailed features within a local receptive field, whereas low-frequency information provides global smooth variations, a frequency domain feature extraction (FDFE) block with dual branches is developed. In the FDFE block, a multihead neighborhood attention block and a global filter block are designed to capture high- and low-frequency features, respectively. Besides, a pixel embedding module is constructed. Based on these, a novel hierarchical dual frequency transformer network is developed. Extensive experiments are performed on three open public hyperspectral datasets to evaluate the performance of our developed method. The experimental results demonstrate that our method is efficient and robust for HSIs classification, achieving overall accuracies of 94.14&#x0025;, 86.92&#x0025;, and 96.72&#x0025; on the University of Pavia, University of Houston, and University of Trento datasets, respectively.al",
    "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 1\nA Dual Frequency Transformer Network for\nHyperspectral Image Classification\nXin Qiao, Student Member, IEEE, Weimin Huang, Senior Member, IEEE\nAbstract—Hyperspectral images (HSIs) provide detailed spec-\ntral information of objects to be detected and play an important\nrole in distinguishing targets with a similar appearance. However,\nthe characteristics of high dimensionality and complexity im-\npose significant challenges for realizing pixel-wise classification.\nAlthough existing convolutional neural networks (CNNs) and\ntransformer-based models have presented promising performance\nfor HSIs classification, they mainly extract features from spectral-\nspatial perspective and do not fully consider the information in\nthe frequency domain. To address this issue, in this paper, we\nreconsider feature extraction and HSIs classification from the\nfrequency domain. Specifically, inspired by the observation that\nhigh-frequency information contains detailed features within a\nlocal receptive field whereas low-frequency information provides\nglobal smooth variations, a frequency domain feature extraction\n(FDFE) block with dual branches is developed. In the FDFE\nblock, an multi-head neighborhood attention (MSNA) block\nand a global filter block are designed to capture high- and\nlow-frequency features, respectively. Besides, a pixel embedding\nmodule is constructed. Based on these, a novel hierarchical dual\nfrequency transformer network (DFTN) is developed. Extensive\nexperiments are performed on three open public hyperspectral\ndatasets to evaluate the performance of our developed method.\nThe experimental results demonstrate that our method is efficient\nand robust for HSIs classification, achieving overall accuracies of\n94.14%, 86.92%, and 96.72% on the University of Pavia, Univer-\nsity of Houston, and University of Trento datasets, respectively.\nIndex Terms—Hyperspectral images (HSIs) classification, pixel\nembedding, high- and low-frequency feature extraction, multi-\nhead neighborhood attention (MHNA), global filter.\nI. INTRODUCTION\nH\nYPERSPECTRAL remote sensing is a powerful technol-\nogy to capture detailed spectral information of an object\nacross a wide range of wavelengths, including visible and near-\ninfrared (NIR) bands [1]. Due to its ability to supply rich\nspectral-spatial information, hyperspectral remote sensing has\nbeen broadly applied in various Earth observation (EO) tasks,\nincluding but not limited to urban planning [2], vegetation\nmonitoring [3], oil spill detection [4], and mineral exploration\n[5]. The abundant information provided by hyperspectral im-\nages (HSIs) is useful for precise identification of various\nobjects within the observed scene [6].\nManuscript received 22 August 2023; revised 15 October 2023; accepted\n24 October 2023. The work of Weimin Huang was supported in part by the\nNatural Sciences and Engineering Research Council of Canada Discovery\nGrants under Grant NSERC RGPIN-2017-04508, and in part by Seed, Bridge\nand Multidisciplinary Fund from Memorial University of Newfoundland.\n(Corresponding author: Weimin Huang)\nXin Qiao and Weimin Huang are with the Faculty of Engineering and\nApplied Science, Memorial University of Newfoundland, St. John’s, NL, A1B\n3X5, Canada (e-mail: xqiao@mun.ca, weimin@mun.ca).\nHSIs classification is one of the most fundamental research\ndirections and involves the process of assigning a specific\ncategory label for each pixel in HSIs [7]. Due to the high\ndimensionality and complexity of HSIs as well as the presence\nof noise and distortions caused by atmospheric effects, the\nHSIs classification task is challenging and has attracted a lot\nof research interest over the last few decades [8], [9].\nIn earlier studies, conventional classifiers, such as K-nearest\nneighbor [10], [11], logistic regression [12], and support vector\nmachine [13], [14] have been employed for HSIs classification.\nFor instance, Li et al. [15] developed a semi-supervised\nmultinomial logistic regression model to classify HSIs and\ntheir proposed model exhibited good performance with limited\ntraining samples. Yu et al.[16] combined features at different\nscales and utilized a subspace-based support vector machine to\nobtain classification maps with improved accuracy. However,\nthese traditional classifiers tend to focus on shallow or low-\nlevel features and have limitations in extracting discriminative\nand informative features.\nUnlike conventional models which rely on handcrafted fea-\ntures, deep learning models can automatically and effectively\ncapture high-level semantic features through advanced neural\nnetwork architectures and optimization techniques [17]. There-\nfore, deep learning and other artificial intelligence techniques\nhave been extensively explored in the field of remote sens-\ning data analysis, including hyperspectral data [18]. Various\nmodels have been proposed in recent years [19], [20]. Chen\net al. [21] introduced the concept of deep learning into HSIs\nclassification for the first time. They designed stacked autoen-\ncoders to extract high-level features hierarchically, demonstrat-\ning the significant potential of deep learning-based methods\nfor achieving accurate classification. Later, Hu et al. [22]\napplied convolutional neural networks (CNNs) to classify\nHSIs from spectral domain, achieving better performance\ncompared with traditional methods. To exploit both spatial\nand spectral information, Roy et al. [23] designed a hybrid\nspectral CNN (HybridSN), which was comprised of a spectral-\nspatial 3D-CNN and a spatial 2D-CNN. HybridSN employed\nthe component of 2D-CNN to capture more abstract spatial\nrepresentations and reduce the complexity of the model com-\npared with the model involving only 3D-CNN. Experiments\ndemonstrated that HybridSN can obtain better performance\nfor HSIs classification by effectively extracting spectral-spatial\ninformation. In [24], Duan et al. developed a novel classifier\nto fuse dual spatial information. Their method introduced\na structural profile feature extraction method in the HSIs\nclassification community for the first time and enhanced the\ndiscriminative capability of the classifier. Moreover, in order\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 2\nto address the issue of spatial variants, they further proposed\na shadow removal framework based on multiexposure fusion\nfor HSIs classification [25]. In order to address the problem\nthat the classification accuracy decreases with the increase\nof network depth, Zhong et al. [26] presented a spectral-\nspatial residual network (SSRN) consisting of four repeated\nspectral and spatial residual blocks. The constructed spectral\nand spatial block utilized the residual connections to preserve\nraw features and enhance the classification accuracy. In [27],\nZhang et al.introduced a region-based CNN that incorporated\nsemantic context-aware representation. The proposed method\ncaptured contextual interactional features from diverse region-\nbased inputs and achieved promising results. Qiao et al.\n[28] adopted a novel 3D sharpened cosine similarity (3D-\nSCS) operation for HSIs classification and proved that 3D-\nSCS can extract discriminative features compared with 3D\nconvolutional operation. In addition to CNNs, various other\narchitectures, such as graph convolutional networks (GCN)\nand recurrent neural networks (RNN), have also been explored\nfor HSIs classification. Hong et al. [29] proposed a mini-\nbatch GCN (miniGCN) to fully model the relationship be-\ntween samples in HSIs. Hang et al. [30] considered spectral\nsignatures as a sequence and utilized RNNs to extract features.\nThey also integrated gated recurrent units to analyze redundant\ninformation in HSIs, boosting the classification performance.\nMost of the above mentioned methods treat every spectral\nband or spatial position with the same level of importance.\nNevertheless, considering that not all information is equally\nessential in complex HSIs data, the attention mechanism has\nbeen introduced to accelerate the development of HSIs classifi-\ncation [31]–[34]. Attention mechanism can dynamically adjust\nthe weights for each spectral band and spatial location, allow-\ning the model to capture more discriminative features while\nsuppressing irrelevant information, leading to better feature\nrepresentation and improved classification results. Haut et al.\n[35] incorporated visual attention mechanisms into a residual\nnetwork to identify the most distinctive features. Experimental\nresults illustrated that the proposed method provided the state-\nof-the-art performance compared to some selected reference\nmethods. Hang et al. [36] designed an attention-aided CNN\nthat is made up of a spectral attention subnetwork and a spatial\nattention subnetwork to explore the distinguishing abilities\nof different spectral bands and spatial locations, respectively.\nRoy et al.[37] suggested an attention-based adaptive spectral-\nspatial kernel-improved residual network (A2S2K-ResNet) to\nadaptively select 3D convolutional kernels for salient spectral-\nspatial information extraction, outperforming other methods\non several HSIs datasets.\nAmong various attention mechanisms, self-attention and\nself-attention based vision transformer architectures have\ngained widespread popularity due to their ability to establish\nglobal relationships without the limitation of the size of recep-\ntive field [38]. A lot of studies have presented transformer-\nbased models for HSIs classification [39]–[42]. Hong et al.\n[43] rethought HSIs from a sequence perspective and proposed\na SpectralFormer to capture spectral representative information\nfrom groupwise neighboring bands. Sun et al. [44] developed\na spectral-spatial feature tokenization transformer (SSFTT)\nmodel for spectral-spatial feature extraction and HSIs clas-\nsification. They first introduced a Gaussian weighted feature\ntokenizer operation to tokenize the extracted features. Then\nthese tokenized features are fed into the transformer encoder\nblocks to learn high-level semantic features. Experiments in-\ndicated their model displayed better performance and required\nless computation time when compared with other methods.\nYu et al. [45] established an image-based framework called\nmultilevel spectral spatial transformer network (MSTNet) for\nHSIs classification. In their method, the HSIs were processed\ninto sequences and learnable position embeddings were added\nto integrate spatial information. A pure transformer encoder\nwas applied to acquire feature representations, followed by a\ndecoder to generate classification results.\nIn addition to the aforementioned methods, there are also\nsome variants of vision transformers. For instance, researchers\nhave found that integrating locality into the transformers\ncan further enhance their classification performance [46]–\n[48]. In [49], Tu et al. proposed a Local Semantic Fea-\nture Aggregation-based Transformer (LSFAT) model, which\nemployed the Neighborhood Aggregation Embedding (NAE)\nand Attention (NAA) modules to learn multiscale features\nand reduce computational costs. To incorporate morpholog-\nical features, Roy et al. [50] developed a Morphological\nTransformer (MorphFormer), which consists of a spectral-\nspatial morphological convolution layer to learn morphological\nfeatures, followed by a self-attention module. The fusion of\nmorphological operation and self-attention mechanism results\nin superior classification performance.\nAlthough analyzing images from the frequency domain is\na common and efficient method for natural images, most of\nexisting transformer-based classifiers extract features from the\nspectral-spatial domain but neglect the exploration from the\nfrequency domain. This gap motivates us to analyze HSIs\nfrom the frequency domain. The model proposed in [51]\nwas designed to extract high- and low-frequency information.\nHowever, their method was developed based on the CNN\narchitecture and had limitations in capturing global infor-\nmation. To address this issue, we reconsider HSIs feature\nextraction and classification from the frequency domain and\npropose a Dual Frequency Transformer Network (DFTN) in\nthis paper. Different from most existing transformer-based\nmethods, the DFTN is designed by considering the charac-\nteristics in frequency domain. Based on the fact that high-\nfrequencies contain the fine-detailed features within the local\nreceptive field and low-frequencies depict smooth variations\nin the global structure [52], [53], a frequency domain feature\nextraction (FDFE) block is proposed. The designed FDFE\nblock consists of two branches to capture high- and low-\nfrequency features respectively. The approach to extracting\nhigh- and low-frequency features is different from [53].\nSpecifically, in the high-frequency feature extraction (HiFE)\nbranch, Reference [53] partitions the input feature map into\nnon-overlapping smaller regions and computes local attention\nscores for each region separately. In contrast, our method\ncalculates the local attention score of each token considering\nitself and its neighboring tokens within the window. As for the\nlow-frequency feature extraction (LoFE) branch, they employ\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 3\nPixel\nEmbedding\nFDFE\nBlock\nStage 1  \nGAP\nLinear\nClassification\nMap\nPixel\nEmbedding\nFDFE\nBlock\nStage 2  \nPixel\nEmbedding\nFDFE\nBlock\nStage 3  \nConv2D\nConv2D\nMulti-heads\nNeighborhood\nAttention\nGlobal Filter\nConcat\nInput Output\nHiFE\nLoFE FDFE\n1 × 1\nConvolutional \nBatch\nNormalization\nPixel Embedding\nHSI\nFig. 1. Overall architecture of the proposed dual frequency transformer network.\nan average pooling layer to reduce the complexity of self-\nattention while our strategy is to use Fast Fourier transform\nto reduce computational cost. In addition, a pixel embedding\nmodule is constructed to preprocess the input features. The\nmain contributions of this study are as follows:\n• The HSIs classification task is reconsidered from the\nfrequency domain perspective which is often neglected in\nprevious studies. A novel hierarchical transformer-based\nnetwork called DFTN is proposed to enhance the HSIs\nclassification performance. 1\n• A FDFE bock with dual branches is proposed to ex-\ntract frequency domain features. Moreover, the frequency\ndomain features are further decomposed into high- and\nlow-frequency features. An MHNA block is integrated\ninto the HiFE branch to focus on high-frequency infor-\nmation within the local region. A global filter block is\nimplemented in the LoFE branch to capture global low-\nfrequency information while reducing the computational\ncost.\n• The performance of proposed network is evaluated on\nthree publicly available HSI datasets and compared with\nsome CNNs and transformers based methods. The results\nof our experiments demonstrate that our method achieves\na higher classification accuracy than the reference meth-\nods.\nThe paper is structured as follows: Section II contains a\ndetailed description of the proposed method. The experimental\nsetup and evaluation results are illustrated in Section III.\nSection IV concludes the paper and provides suggestions for\nfuture work.\nII. PROPOSED METHOD\nFig. 1 depicts the overall architecture of the proposed\nDFTN, which is designed with a three-stage hierarchical\n1https://github.com/xinqiao123/DFTN\nstructure to extract frequency features at different levels. The\nkey components of each stage include the proposed pixel\nembedding module and frequency domain feature extraction\n(FDFE) block, which are described in details below.\nA. Pixel Embedding Module\nSuppose the original HSI is denoted as I ∈ RH×W×B,\nwhere H, W and B represent the height, width, and the\nnumber of spectral bands, respectively. The proposed method\nis a patch-based learning framework, and HSI patches X ∈\nRP×P×B, where P denotes the patch size, are extracted first\nto capture spectral-spatial information simultaneously. Patch\nblocks are constructed by extracting both the central pixel and\nits neighboring pixels within a certain patch size. The patch\nsizes are chosen as 9 × 9, 11 × 11, 13 × 13, and 15 × 15\nhere. The label of the central pixel is regarded as the label for\nthe entire patch block.\nPatch embedding is a commonly used and effective tech-\nnique in vision transformers for different computer vision\ntasks, such as object detection or segmentation. It divides\nan image into small patches and projects them to high-\ndimensional feature maps. However, this approach is not\nsuitable for our tasks since the size of the input patches\nof transformer blocks is small. To overcome this issue, a\nnovel pixel embedding module is proposed in this study to\nbe applicable for pixel-level operation.\nFor the original image I ∈ RH×W×B, the first step is to\nextract small patches with a size of P × P × B. Then, these\nextracted patches are input into the pixel embedding block,\nwhich contains a 1×1 convolutional layer with C convolutional\nfilters and generates output features with a size of P ×P ×C.\nSpecifically, the proposed pixel embedding module employs\na 1 × 1 2D convolutional layer with Ci channels, where\ni = (1, 2, 3) represents the i-th stage, to encode the input\nHSI patches. Then, the output of this convolutional layer\nis normalized using batch normalization (BN) to regulate\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 4\nConv2D\nQuery\nKey & Value\nMulti-head\nNeighborhood\nAttention\nConcat\nPWConv Block\nFFT\nFrequency\nDomain\nFeatures× Global\nFilter\niFFT\nConv2D\nConcat\nLoFE\nHiFE\nFig. 2. Structure of FDFE block.\nthe learning process and improve classification performance.\nMathematically, the pixel embedding process can be expressed\nas follows:\nˆX = BN(Conv2D(X)) (1)\nwhere Conv2D denotes the 2D convolutional operation applied\nto the input HSI patches X. The output is then normalized\nusing BN, which helps to improve the overall stability of the\ntraining process. Subsequently, the output, denoted as ˆX ∈\nRP×P×Ci, is used for subsequent frequency feature extraction\nand classification.\nB. Frequency Domain Feature Extraction Block\nThe output feature maps from the embedding module are\ncommonly fed into transformer blocks to establish long-range\ndependencies. Although most existing transformer blocks de-\nsigned for HSIs classification are effective in extracting fea-\ntures in spectral-spatial domain, they do not fully consider\nthe information in frequency domain. Besides, the spatial\ndimension of the output feature maps still contains lots of\nredundancy, which also limits the capacity of the model to\nextract useful information.\nIn this study, we consider the output feature maps from\na frequency perspective and decompose them into high- and\nlow-frequency components. Similar with natural images, high\nfrequency information corresponds to fine details within a\nlocal window, while low frequency information represents\nthe smooth variations in global structures. Based on this\nobservation, a frequency domain feature extraction (FDFE)\nblock is proposed. The FDFE block is the critical component\nof the proposed network and is made up of dual branches\nincluding a high-frequency feature extraction (HiFE) branch\nand a low-frequency feature extraction (LoFE) branch. Since\nthe FDFE block at every stage employs a similar method to\nprocess the input features, for convenience, we take C1 as an\nexample to introduce the proposed FDFE block in detail.\n1) High-Frequency Feature Extraction (HiFE):For an in-\nput ˆX with a shape of P × P × C1, in order to allocate same\nnumber of channels for both the HiFE and LoFE branches,\nˆX is first projected into two parts with an equal number of\nchannels. To implement this, a 1 × 1 2D convolutional layer\nwith C1/2 channels is used to reduce the number of channels\nand obtain the output of XH ∈ RP×P×D, where D = C1/2\ndenotes the number of channels for each pixel in the XH.\nAs illustrated in Fig. 2, XH is then passed through an multi-\nhead neighborhood attention (MSNA) block. Although multi-\nhead self-attention (MHSA) mechanism is a common choice\nin the transformer blocks, it is not suitable for our task since\nit captures global information, whereas the high-frequency\ninformation we aim to extract is generally located within a\nsmall window. As a result, in order to overcome this limitation,\nwe introduce the MHNA mechanism in this study. Different\nfrom MHSA which captures global information by calculating\nthe attention score of each token by using all tokens, MHNA\nis designed to extract detailed high-frequency information in\na local window by calculating the attention score of each\ntoken considering itself and its neighboring tokens within the\nwindow.\nSpecifically, XH is first reshaped into a 2-D shape of N×D,\nwhere N is the number of pixels in the feature map and is\nequal to P × P. Next, the query ( Q), key ( K) and value\n(V) are generated. Similar to MHSA, both Q, K, and V are\nsplit into h heads along the channel dimension to capture\ncontextual relationships from various representation subspaces.\nThe generated Q, K and V with h heads can be denoted as\nfollows\nQ = XHWQ = {Q1, Q2, ··· , Qn} (2a)\nK = XHWK = {K1, K2, ··· , Kn} (2b)\nV = XHWV = {V1, V2, ··· , Vn} (2c)\nwhere WQ, WK, and WV are the projection matrices with\na shape of D ×D. Qi, Ki, and Vi ∈ RN×(D/h) represent the\ni-th head of query, key, and value, respectively.\nNext, the output of the i-th head in the MHNA block is\ncalculated. For the µ-th token in the query Qi, the attention\nscore with its neighboring keys is computed first. In order\nto retain the positional information of different pixels in the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 5\nAlgorithm 1 Calculate the output of the i-the head in the\nMHNA block.\nInput: Qi, Ki, Vi ∈ RN×(D/h)\n1: Compute the attention score between the µ-th token in the\nquery and its neighboring keys, with relative positional\nbiases added,\nSk\ni,µ =\n\n\nQi,µKT\ni,µ,1 + Bi,µ,1\nQi,µKT\ni,µ,2 + Bi,µ,2\n...\nQi,µKT\ni,µ,k + Bi,µ,k\n\n\n;\n2: Divide Sk\ni,µ by the square root of the dimension and con-\nvert the scores to probabilities using the softmax function,\nAk\ni,µ = Softmax(Sk\ni,µ/\np\n(D/h));\n3: Acquire the weighted value, headi,µ = Ak\ni,µ·Vk\ni,µ, where\nVk\ni,µ = [Vi,µ,1, Vi,µ,2, ··· , Vi,µ,k]\n4: Repeat for every token in the query;\nOutput: headi ∈ RN×(D/h)\nsequence, the relative positional biases are added. Then, the\nattention scores are normalized with the square root of the\ndimension to stabilize gradients during the training process.\nSoftmax function is applied to translate the attention scores\ninto probabilities. Finally, the probabilities are multiplied by\nthe corresponding values within the neighborhood and these\ncalculations are repeated for every token in the query.\nIn summary, the steps to calculate the output of the i-th\nhead are outlined in Algorithm 1, in which k denotes the\nneighborhood size and Bi,µ,j (j = 1, 2, ··· , k) represents the\nj-th positional bias for the µ-th token.\nThe outputs derived from different heads are then concate-\nnated and projected to obtain the final output of MHNA. This\nprocess can be formulated as\nMHNA(Q, K, V) =Concat(head1, ··· , headh)WO (3)\nheadi = Softmax(QiKT\ni + Bip\nD/h\n)Vi (4)\nwhere WO ∈ RD×D represents the projection matrix.\nIn the transformer encoder blocks, a feed-forward network\n(FFN) is often utilized after the attention layer. Typically, it\nis comprised of two linear layers and a nonlinear activation\nfunction such as Gaussian error linear unit (GELU) between\nthem. In order to reserve the spatial structure in XH, which\ndenotes the input of FFN, the linear layer is replaced with a\npointwise convolutional layer. The FFN in the HiFE branch\ncan be expressed as\nFFN(XH) =W2 ∗ σ(W1 ∗ XH) (5)\nwhere W1 and W2 represent the parameters of the two point-\nwise convolutional layers and ∗ is the convolution operation.\nσ is the nonlinear activation function of GELU.\nMoreover, layer normalization (LN) is applied before\nMSNA and FFN. Residual connection is used after each block.\nThe output of the HiFE branch could be expressed as follows:\nX\n′\nH = MHNA(LN(XH)) +XH (6)\nFH = FFN(LN(X\n′\nH)) +X\n′\nH (7)\nwhere FH is the extracted high frequency features. High-\nfrequency components often correspond to local details and\nour MHNA block focuses on neighboring pixels. Therefore,\neven without Fourier Transform, MHNA block allows the\nHiFE branch to effectively capture high-frequency information\nassociated with fine-grained details in the input data.\n2) Low-Frequency Feature Extraction: The LoFE branch\naims to capture low-frequency features from a global size.\nRecent study [54] has demonstrated that the global atten-\ntion MHSA can extract low-frequency features. Nevertheless,\ndirectly applying MHSA in the LoFE branch leads to a\nconsiderable computational cost. In order to efficiently learn\nlow-frequency features, a global filter block is presented in the\nLoFE branch.\nSimilar to the HiFE branch, a 1 × 1 2D convolutional\nlayer is applied at the beginning of the LoFE branch to\nobtain the features XL ∈ RP×P×D with a reduced number\nof channels. Then, the low-frequency features are extracted\nthrough a global filter block, which is implemented according\nto the following three key steps.\nFirstly, the input features XL are translated into frequency\ndomain with discrete Fourier transform (DFT). Concretely,\ngiven the input features XL, we first conduct 2D DFT for\nthe spatial dimension. It should also be noted that in our\nimplementation, fast Fourier transform (FFT) is used to reduce\nthe computation complexity [55]. Moreover, consideringXL is\na real-valued input, the conjugate symmetric property of DFT\nis fully utilized. Namely, only half of the transformed values\nare preserved while containing the full information, whereas\nthe rest are discarded to reduce redundant computation. The\nprocess of converting XL into the frequency domain is shown\nas follows\nXL = Fr[XL] ∈ CP×P\n2 ×D (8)\nwhere Fr is the 2D FFT for a real-valued input. XL is the\ncomplex-valued features with a shape of P × P\n2 × D in the\nfrequency domain.\nSecondly, the frequency domain features XL are element-\nwise multiplied with a global filter. In order to extract rich\nlow-frequency features, a filter with a global receptive field is\nemployed, which is defined as\ngXL = Gr ⊙ XL (9)\nwhere ⊙ denotes the element-wise multiplication operation.\nGr ∈ CP×P\n2 ×D is the global filter and gXL is the filtered\nfrequency domain features.\nFinally, inverse FFT is employed to convert the filtered\nfeatures gXL back to the spatial domain. In summary, all the\nabove steps in the global filter block could be combined into\none equation and expressed as\nGF(XL) =F−1\nr [Gr ⊙ Fr[XL]] (10)\nwhere F−1\nr represents the inverse FFT.\nGiven the input XL ∈ RP×P×D, the designed global filter\nblock requires (P × P\n2 × D) parameters, whereas MHSA\ntakes (4D2) parameters to generate the query, key, value, and\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 6\n(a)\n (b)\nBackground Asphalt Meadows Gravel\nTrees Painted metal sheetsBare Soil Bitumen\nSelf Blocking Bricks Shadows\nFig. 3. Spatial distribution of disjointed University of Pavia\ndataset. (a) Disjointed training set. (b) Disjointed test set.\nfinal projected output. Besides, the number of channels of the\ninput feature XL, i.e., D is generally greater than its spatial\ndimension of height and width P. Therefore, compared with\nMHSA, the global filter block in the LoFE branch reduces\nthe number of parameters and improves the computational\nefficiency.\nLike the HiFE branch, the designed LoFE branch also\nincorporates LN, FFN, and residual connections. The captured\nlow-frequency features FL from the LoFE branch could be\ndefined as\nX\n′\nL = GF(LN(XL)) +XL (11)\nFL = PWConv(LN(X\n′\nL)) +X\n′\nL (12)\nThe final output of the FDFE block is obtained by con-\ncatenating both high-frequency features FH and low-frequency\nfeatures FL as follows\nF = Concat(FH, FL) (13)\nwhere F represents the output of the FDFE block.\nC. DFTN\nThe overall architecture of the proposed DFTN is illustrated\nin Fig. 1. DFTN follows a path-based classification method,\nand HSI patches each with a spatial size of 11 × 11 are\nextracted first to explore spectral-spatial features. Then, the\nHSI patches are fed into three sequential stages named stage\n1, 2, and 3 to capture low-, middle-, and high-level features in\nfrequency domain. The number of layers is a crucial hyperpa-\nrameter in the proposed method. Using less layers can decrease\nthe computational cost but may fail to capture high-level\nfeatures. On the other hand, increasing the number of layers\ncan enhance the ability in extracting high-level features at the\nexpense of higher computational cost. Considering the balance\nbetween computation cost and the level of extracted features,\nthe proposed network employs a three-layer architecture as\ndone in [41]. Each stage contains a pixel embedding module\nand a FDFE block. Residual connection is also applied before\nand after the FDFE block to enhance the training process. For\nthe concrete settings of the architecture, in our implementation,\nthe embedding dimension of {C1, C2, C3} in stages 1, 2, and\n3 is set as 64, 32, 16 respectively. In terms of the MHNA block\nin the HiFE branch, the neighborhood size k is defined as 3×3\nand the numbers of heads for three stages are {8, 4, 2}. After\nthese three stages, a feature map with a shape of 11×11×16\nwill be generated.\nThe last step of DFTN involves a global average pooling\n(GAP) layer and a fully connected (FC) layer. The GAP layer\nis employed to compress the spatial dimension of the produced\nfeature map and the FC layer with a certain number of classes\nis utilized to obtain the final classification results.\nIII. EXPERIMENTS\nTo evaluate the performance of the proposed method, ex-\ntensive experiments are conducted on three open public HSIs\ndatasets, including the University of Pavia (UP), University of\nHouston (UH), and University of Trento (UT). The selected\ndatasets are introduced concisely below.\nA. Dataset Description\n1) University of Pavia (UP):The UP dataset was collected\nby the Reflective Optics Systems Imaging Spectrometer (RO-\nSIS) sensor in 2001. After discarding the noise bands, 103\nspectral bands ranging from 430 to 860 nm are reserved from\nthe original 115 bands. The height and width of the spatial\ndimensions are 610 and 340 respectively. There are 9 land\ncover classes and the spatial resolution is 1.3 meters per pixel\n(MPP). Non-overlapping training and test data are split, and\ntheir corresponding spatial distributions are depicted in Fig. 3.\n2) University of Houston (UH): The UH dataset was ob-\ntained over the University of Houston campus through the\nCompact Airborne Spectrographic Imager (CSAI) sensor in\n2012 and was used for the 2013 Data Fusion Contest. It\ncontains 349×1905 pixels with a spatial resolution of 2.5 MPP.\nThe number of spectral bands is 144 and the wavelength covers\nfrom 364 to 1046 nm. It includes 15 manually labeled classes\nand all pixels with ground truth are divided into training and\ntest sets, as illustrated in Fig. 4.\n3) University of Trento (UT): The UT dataset was ac-\nquired by the Airborne Imaging Spectrometer for Applications\n(AISA) Eagle sensor over the campus of Trento University,\nItaly. There are 600×166 pixels with a spatial resolution of 1\nMMP and 63 spectral bands ranging from 402 to 989 nm are\nprovided. All labeled pixels fall into 6 different classes, such\nas Buildings, Roads, and Ground. Fig. 5 depicts the spatial\ndistribution of separated training and test samples.\nThe selected three datasets present various challenges for\nHSIs classification tasks. Firstly, they exhibit class imbalance.\nBesides, both UP and UH datasets have high dimensionality,\nexceeding one hundred spectral bands. The UH dataset further\ndisplays significant spatial variability over an extensive spatial\nextent.\nB. Experimental Setup\nIn this study, the High Performance Computing (HPC) re-\nsources provided by Compute Canada are utilized. Concretely,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 7\n(a)\n(b)\nBackground Grass-healthy Grass-stressed Grass-synthetic\nTree Soil Water Residential\nCommercial Road Highway Railway\nParking-lot1 Parking-lot2 Tennis-court Running-track\nFig. 4. Spatial distribution of disjointed University of Houston\ndataset. (a) Disjointed training set. (b) Disjointed test set.\n(a)\n(b)\nBackground Apples Buildings Ground\nWoods Vineyard Roads\nFig. 5. Spatial distribution of disjointed University of Trento\ndataset. (a) Disjointed training set. (b) Disjointed test set.\nTABLE I\nTRAINING DATA AND TEST DATA SAMPLES NUMBER AND\nRATIO .\nDatasets Training data Test data\nNumber Ratio Number Ratio\nUP 3921 8.93% 40002 91.07%\nUH 2832 18.84% 12197 81.16%\nUT 819 2.71% 29395 97.29%\nan Nvidia V100 GPU with 32GB memory is selected for\nour experiments. The deep learning framework Pytorch is\nemployed to develop different models.\nTable I displays the ratio of training and test data. For the\nsake of fair comparison, the patch size is set to 11 × 11 for\nall models in the following experiments. During the training\nprocess, Cross Entropy is employed to acquire the loss value\nbetween the predicted results and ground truth. In addition,\nTABLE II\nCLASSIFICATION RESULTS OF DFTN WITH DIFFERENT\nFREQUENCY FEATURES . THE BEST RESULTS ARE SHOWN IN\nBOLD .\nDataset Model Metrics\nOA AA Kappa\nUP\nDFTN-H 92.95 ± 3.19 93.75 ± 0.98 90.70 ± 4.08\nDFTN-L 91.06 ± 2.32 92.64 ± 1.10 88.20 ± 2.90\nDFTN 94.14 ± 0.34 93.43 ± 0.81 92.12 ± 0.43\nUH\nDFTN-H 86.55 ± 1.08 89.04 ± 0.89 85.46 ± 1.15\nDFTN-L 86.28 ± 0.81 87.93 ± 0.81 85.13 ± 0.87\nDFTN 86.92 ± 1.13 89.20 ± 0.86 85.85 ± 1.21\nUT\nDFTN-H 96.37 ± 0.15 94.65 ± 0.31 95.16 ± 0.20\nDFTN-L 95.94 ± 1.15 94.31 ± 1.10 94.60 ± 1.88\nDFTN 96.72 ± 0.48 94.85 ± 0.61 95.61 ± 0.64\nthe Adam optimizer is used to optimize the parameters in the\nnetwork. The epoch is set to 500 for all datasets and an early\nstop strategy is adopted to prevent the overfitting problem.\nSpecifically, the training process will stop if the loss on the\nvalidation data is not decreased for 50 epochs continuously.\nTo quantitatively assess the performance of our models, three\nwidely used criteria for HSIs classification including overall\naccuracy (OA), average accuracy (AA), and Kappa coefficient\nare adopted, and higher values of these metrics indicate better\nclassification performance. Moreover, in order to alleviate the\ninfluence of experimental randomness, all experiments are run\n5 times. Mean values and standard deviations are reported for\neach class and metric.\nData preprocessing is a critical step to acquire stable training\nand robust performance for deep neural networks. In this work,\ndata normalization techniques are also incorporated and the\nspectral dimension for each dataset is normalized into a range\nof [0, 1] using the Min-Max Scaler.\nC. Ablation Study\n1) Impact of High- and Low-frequency: In this section,\nthe importance of the HiFE and LoFE branches, namely,\nthe high- and low-frequency features will be investigated.\nTwo additional models called the DFTN-H and DFTN-L are\ndesigned first. As the name suggested, for DFTN-H, only\nthe high-frequency features extracted by the HiFE branch are\nkept and the LoFE branch is removed. Unlike DFTN-H, only\nthe global low-frequency information is reserved in DFTN-L.\nThe comparison between these two modified models and the\noriginal DFTN method is summarized in Table II. It is evident\nthat the DFTN model combining both high- and low-frequency\nfeatures achieves the best classification performance. To be\nspecific, for the UP dataset, the OA score of DFTN is 94.14%,\nwhich is higher than those of DFTN-H and DFTH-L by 1.19%\nand 3.08%, respectively. Similar results are also reported for\nthe Kappa metrics. In terms of the UH dataset, the combination\nof dual branches results in a growth about 0.5%. Meanwhile,\nwe notice that DFTN-H performs better than DFTN-L. The\nreasons behind this phenomenon may be related to the spatial\nresolution of the dataset. Higher resolution could provide\nmore detailed high-frequency information, leading to a better\nclassification performance.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 8\nTABLE III\nCLASSIFICATION RESULTS OF DFTN WITH DIFFERENT\nNUMBER OF FDFE BLOCKS . THE BEST RESULTS ARE\nSHOWN IN BOLD .\nDataset Number\nof FDFE Block\nMetrics\nOA AA Kappa\nUP 1 94.14 ± 0.34 93.43 ± 0.81 92.12 ± 0.43\n2 93.89 ± 1.16 93.23 ± 0.53 91.80 ± 1.58\nUH 1 86.92 ± 1.13 89.20 ± 0.86 85.85± 1.21\n2 86.46 ± 0.82 88.72 ± 0.74 85.36 ± 0.88\nUT 1 96.72 ± 0.48 94.85 ± 0.61 95.61 ± 0.64\n2 96.46 ± 0.66 94.75 ± 0.68 95.27 ± 0.87\nTABLE IV\nCOMPARISON BETWEEN PIXEL EMBEDDING AND GROUPED\nPIXEL EMBEDDING . THE BEST RESULTS ARE SHOWN IN\nBOLD .\nDataset Model Metrics\nOA AA Kappa\nUP Pixel Embedding 94.14±0.34 93.43 ±0.81 92.12 ±0.43\nGrouped Pixel Embedding 90.79 ± 3.86 92.82 ± 1.23 87.95 ± 4.88\nUH Pixel Embedding 86.92 ±1.13 89.20±0.86 85.85±1.21\nGrouped Pixel Embedding87.02 ± 0.41 89.17 ± 0.35 85.96 ± 0.44\nUT Pixel Embedding 96.72±0.48 94.85 ±0.61 95.61 ±0.64\nGrouped Pixel Embedding 96.02 ± 1.01 94.68 ± 0.69 94.71 ± 1.32\n2) Impact of Number of FDFE blocks: Furthermore, to\nevaluate the impact of the number of FDFE blocks on DFTN\nmodel, experiments using one and two blocks are conducted\nand the classification accuracies are presented in Table III. As\nthe table shows, the accuracies using one block are slightly\nhigher than those obtained with two blocks. These results may\nbe explained by the potential overfitting as the complexity of\nthe model increases.\n3) Impact of Pixel Embedding: To demonstrate the effec-\ntiveness of the pixel embedding module, comparative experi-\nments with grouped pixel embedding are conducted. Table IV\npresents the classification accuracies of pixel embedding and\ngrouped pixel embedding where the number of groups is equal\nto 2. The results show that pixel embedding generates better\nclassification performance on the UP and UT datasets while\nachieving comparable accuracies on the UH dataset. Both\ngrouped pixel embedding and pixel embedding apply a 1 × 1\nconvolutional layer to address the problem of the small spatial\nsize of input patches. The difference is whether group convo-\nlution is adopted or not. Grouped pixel embedding employs\ngrouped convolution to reduce the number of parameters but\nthe obtained embedded features are limited to the local spectral\ninformation. In contrast, the pixel embedding in our method\nuses the common convolution which considers the entire\ninput feature map to enrich the feature representations. The\nadditional parameters in the proposed method help the model\nunderstand the input data comprehensively. This observation\nhighlights the significance of global spectral features for HSIs\nclassification.\nD. Spectrum Analysis\nIn this section, the magnitudes of the frequency component\n[53], [56] for the output feature maps from HiFE and LoFE\nbranches are displayed to distinguish the extracted features, as\nHiFE\nLoFE\nFig. 6. Frequency magnitude of HiFE and LoFE branches from\nStage 3 on the UP dataset.\nHiFE\nLoFE\nFig. 7. Frequency magnitude of HiFE and LoFE branches from\nStage 3 on the UH dataset.\nshown in Figs. 6 - 8. To be more precise, the output feature\nmaps are first translated into the frequency domain using the\nFFT algorithm. Subsequently, the frequency domain features\nare reordered in order to move the low frequency component\nto the center. Thus, in the above mentioned figures, points that\nare closer to the center represent lower frequencies. Finally,\nthe last four output channels of the HiFE and LoFE branches\nof Stage 3 are illustrated. Through observing these figures, it\ncould be found that the frequency magnitude of HiFE branch\nexhibits a lighter color, suggesting that the output feature maps\nof the HiFE branch have higher frequency components. On the\nother hand, the output features of the LoFE branch show lower\nfrequency magnitudes which are mainly located in the center,\nas shown in Fig. 6.\nThese observations are consistent with our motivation that\nthe HiFE branch is used to extract detailed high-frequency\nvariations while the LoFE branch is designed to capture global\nlow-frequency features. Based on this, the efficiency of our\ndesigned dual-branch structure in capturing high- and low-\nfrequency features is further demonstrated.\nE. Comparison with Reference Methods\nNine reference models are selected and implemented for\ncomparison with the proposed DFTN, including five CNN-\nbased methods, i.e., 3D-CNN, SSRN [26], 3DOC-SSAN [51],\nSPRN [57], FSKNet [58], and four transformer-based models,\ni.e., SSTN [59], SSFTT [44], GAHT [41], morphFormer [50].\nFor the CNN-based models, 3D-CNN was the first network\nused to explore spectral-spatial features. SSRN incorporated\nthe residual connection in both the spectral and spatial blocks\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 9\nTABLE V\nCLASSIFICATION RESULTS OF DIFFERENT METHODS ON THE UP DATASET. THE BEST RESULTS ARE SHOWN IN BOLD .\nClass CNN-based Methods Transformer-based Methods\n3D-CNN SSRN 3DOC-SSAN FSKNet SPRN SSTN SSFTT GAHT morphFormer DFTN\n1 83.97±2.39 97.11±1.63 83.97±2.39 80.53 ±2.24 96.20 ±0.84 92.63±2.51 84.74 ±1.59 92.05 ±2.65 86.92 ±2.79 92.92 ±2.44\n2 91.73±0.86 71.22 ±0.63 91.73 ±0.86 89.52 ±2.62 87.03 ±5.36 81.88±2.49 94.57 ±1.44 96.06±1.49 90.54±7.91 95.97 ±1.02\n3 57.79±6.66 83.43 ±6.48 57.79 ±6.66 62.84 ±4.01 96.79±2.38 83.68±8.12 70.34 ±2.73 66.17 ±3.82 72.40 ±4.21 77.92 ±7.47\n4 98.17±0.70 83.78±5.53 98.17±0.70 97.70±1.01 78.39 ±4.35 91.59±2.91 97.42 ±0.96 94.65 ±0.71 93.87 ±0.61 92.55 ±1.40\n5 98.45±0.98 99.53±0.40 98.45±0.98 98.87 ±0.91 98.89 ±0.38 98.78±0.64 99.10 ±0.64 98.90 ±1.15 99.12 ±0.42 99.25 ±0.19\n6 53.08±2.93 99.93±0.11 53.08±2.93 64.96 ±10.34 99.99 ±0.02 95.25±5.50 88.77 ±2.11 82.83 ±5.10 88.22 ±6.64 90.71 ±2.31\n7 68.48±4.62 99.88±0.12 68.48±4.62 82.59 ±1.81 99.80 ±0.21 97.78±1.90 85.65 ±7.07 87.12 ±3.81 95.09 ±2.56 97.94 ±2.16\n8 96.70±0.99 98.34±1.20 96.70±0.99 96.53 ±0.97 97.28 ±0.69 96.15±0.99 97.21 ±0.79 97.25 ±0.48 97.47 ±0.57 98.21 ±0.72\n9 93.64±1.91 99.22±0.52 93.64±1.91 94.67 ±3.11 94.72 ±0.95 93.99±1.25 97.46 ±0.53 95.32 ±1.70 95.02 ±1.01 95.45 ±0.61\nOA 85.09±0.90 84.38 ±0.58 82.99 ±2.84 85.46 ±0.85 91.43 ±2.36 88.19±1.67 91.65 ±0.66 92.40 ±0.77 90.14 ±3.54 94.14±0.34\nAA 82.45±1.29 92.49 ±0.88 82.77 ±2.19 85.36 ±1.02 94.34±0.49 92.41±1.41 90.58 ±0.76 90.04 ±0.66 90.96 ±1.51 93.43 ±0.81\nKappa 79.80±1.21 80.06 ±0.73 77.00 ±3.73 80.51 ±1.18 88.75 ±2.99 84.62±2.12 88.81 ±0.86 89.73 ±1.04 86.96 ±4.44 92.12±0.43\nHiFE\nLoFE\nFig. 8. Frequency magnitude of HiFE and LoFE branches from\nStage 3 on the UT dataset.\nto reserve the original input feature information. 3DOC-SSAN\nwas a highly relevant reference method which also captured\nfrequency information. It should be noted that although both\n3DOC-SSAN and DFTN were designed from the perspective\nof frequency domain, they had a significant difference. The\ndifference was that 3DOC-SSAN employed 3D octave con-\nvolution to extract high- and low-frequency information, and\nwas constructed based on CNNs, whereas the DFTN model\nproposed in this study applied the attention mechanisms to\nextract frequency features and was developed based on the\ntransformer architecture. SPRN partitioned spectral bands into\nnon-overlapping subbands and employed a cascaded resid-\nual network to capture intrinsic features. FSKNet extracted\nspectral-spatial features through a hybrid model consisting\nof 3D-CNN and 2D-CNN modules. Besides, the selective\nkernel mechanism and deformable convolution were utilized\nto choose informative features dynamically. For the reference\nmethods using transformer as the backbone network, SSTN\nintegrated the spatial attention and spectral association module\nto address the issue of limited receptive field in CNNs.\nSSFTT designed a gaussian weighted feature tokenizer to\nperform feature transformation. GAHT followed a global-local\nfashion to extract spectral-spatial features and MorphFormer\nintegrated the spectral-spatial morphological information into\nthe transformer.\n1) Quantitative Comparison:The quantitative classification\noutcomes of different methods on the three datasets are listed\nin Tables V-VII. By observing these tables, it is easy to\nfind that our proposed DFTN exhibits the best classification\nperformance in terms of OA, AA, and Kappa.\nTable V displays the numerical results on the UP dataset. As\nshown in the table, among the CNN-based methods, FSKNet\nobtains better classification accuracies than the 3D-CNN and\nSSRN models. Besides, SPRN achieves the best classifica-\ntion performance with an OA of 91.43%, demonstrating its\nrobustness in feature extraction. The transformer-based meth-\nods exhibit better classification results than most CNN-based\nmodels, indicating the significance of incorporating global\ninformation. Compared with transformer-based methods, the\nDFTN achieves the highest OA score of 94.14%, exceeding the\nsecond best model GAHT by 1.74%. Similar results can also\nbe observed on the AA and Kappa values. The improvement\nfor AA scores obtained by the designed method is 3.39%\ncompared with GAHT and 2.47% against morphFormer. The\nKappa coefficient of DFTN is 92.12%, i.e., 2.39% higher\nthan GAHT method. These promising results demonstrate that\nthe developed DFTN is able to capture both high- and low-\nfrequency features at both local and global scales, enhancing\nits classification accuracy.\nThe classification results of aforementioned methods on the\nUH dataset are shown in Table VI and the constructed DFTN\nmodel obviously presents the strongest behavior. Specifically,\nDFTN attains the maximum values of 86.92%, 89.20%, and\n85.85% for the metrics of OA, AA, and Kappa coefficient. Be-\nsides, it outperforms 3DOC-SSAN by 6.44%, demonstrating\nthe advantages of the transformer-based model in capturing\nglobal low-frequency information over the 3DOctave, which\nis a kind of convolutional operation and limited in capturing\nlong-range dependencies efficiently. Furthermore, as shown\nin Table VII, the OA, AA, and Kappa scores of the DFTN\nmodel on the UT dataset are 96.72%, 94.85%, and 95.61%,\nrespectively. Compared with the second best transformer-based\nmodel SSFTT, it acquires increases of 1.16%, 3.89%, and\n1.55% for the three metrics. These inspiring classification\nresults illustrate the effectiveness of our proposed DFTN\nmethod for HSIs classification.\nIn addition, the patch size determines the spatial information\nemployed for classification. In this subsection, experiments\nwith different patch sizes are conducted and the classification\nOAs are listed in Table VIII. Both CNN and transformer based\nmodels utilize the same patches as input. It is noted a larger\npatch size does not consistently yield a higher accuracy. This\nphenomenon may be attributed to the fact that larger patch\nsizes could include more irrelevant pixels which belong to\ndifferent categories, interfering with the classification results.\nMoreover, it is observed that the proposed DFTN acquires\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 10\n(a) False color map\n (b) 3D-CNN\n (c) SSRN\n (d) 3DOC-SSAN\n (e) FSKNet\n (f) SPRN\n(g) SSTN\n (h) SSFTT\n (i) GAHT\n (j) morphFormer\n (k) DFTN\n (l) Ground Truth\nFig. 9. Classification maps generated by different models on the UP dataset.\nTABLE VI\nCLASSIFICATION RESULTS OF DIFFERENT METHODS ON THE UH DATASET. THE BEST RESULTS ARE SHOWN IN BOLD .\nClass CNN-based Methods Transformer-based Methods\n3D-CNN SSRN 3DOC-SSAN FSKNet SPRN SSTN SSFTT GAHT morphFormer DFTN\n1 81.35±0.88 82.30 ±0.40 82.51 ±0.46 82.94 ±0.22 77.51 ±0.70 83.34±3.24 82.60 ±0.40 78.27 ±0.75 86.27 ±6.82 86.50±5.82\n2 78.83±1.39 85.13±0.04 83.95±0.58 84.04 ±0.86 84.72 ±0.30 80.45±6.07 84.47 ±0.62 80.34 ±1.56 84.55 ±0.37 84.45 ±0.62\n3 63.33±3.49 96.83 ±2.31 88.87 ±6.81 79.21 ±8.72 99.09 ±0.24 98.57±1.14 87.60 ±7.68 58.26 ±4.81 96.08 ±2.84 99.13±0.37\n4 97.61±2.32 92.99 ±0.33 90.66 ±1.50 95.59 ±2.43 86.72 ±1.24 86.16±3.99 96.69±3.41 88.77±1.07 91.74 ±1.13 88.83 ±1.88\n5 95.66±0.75 100.0±0.00 99.34±0.65 98.45 ±1.10 100.0±0.00 99.02±0.80 99.89 ±0.07 99.83 ±0.15 97.73 ±0.51 99.96 ±0.05\n6 84.90±5.24 95.94 ±0.28 89.23 ±5.52 93.15 ±1.56 95.66 ±0.28 95.80±0.00 87.41 ±4.92 82.94 ±3.41 98.04±1.90 96.22±0.84\n7 83.47±1.10 85.19 ±2.19 84.98 ±4.06 79.93 ±3.14 71.66 ±1.81 81.81±6.56 83.68 ±2.76 82.78 ±2.56 88.19±2.77 81.85±4.15\n8 57.87±2.84 66.13 ±5.87 67.56 ±3.44 63.63 ±1.33 73.69 ±3.54 75.44±2.41 70.75 ±3.19 70.92 ±2.25 72.06 ±1.56 76.56±4.16\n9 73.98±3.10 76.60 ±2.15 79.06 ±1.41 79.40 ±3.97 91.75±0.91 79.26±12.99 84.76 ±1.66 86.16 ±1.10 82.13 ±1.99 84.12 ±1.31\n10 48.46±2.29 59.23 ±5.59 48.03 ±3.01 48.03 ±4.74 60.10 ±6.02 56.54±6.69 51.24 ±3.50 59.03 ±5.93 54.40 ±3.79 67.07±2.13\n11 55.03±4.45 81.73 ±3.11 69.75 ±3.41 67.93 ±1.28 98.27±3.45 88.71±9.50 77.86 ±2.52 78.03 ±0.94 74.19 ±5.61 85.48 ±7.51\n12 75.45±1.67 92.37 ±5.8 87.34 ±3.95 79.48 ±6.18 97.77±1.83 91.39±7.53 86.92 ±2.04 88.63 ±1.64 91.34 ±3.14 97.21 ±1.58\n13 92.14±0.57 90.39±3.14 83.93 ±2.32 88.84 ±2.04 74.11 ±3.23 70.39±4.75 89.96 ±1.43 89.54 ±2.91 83.93 ±3.24 90.74 ±2.19\n14 76.92±2.02 100.0±0.00 91.42±6.15 90.69 ±6.94 100.0±0.00 99.11±1.21 97.65 ±4.10 98.38 ±1.38 91.74 ±3.08 100.0±0.00\n15 56.03±8.00 99.24 ±0.61 85.54 ±5.74 86.85 ±4.26 100.0±0.00 96.41±5.24 88.16 ±9.62 94.63 ±3.68 97.80 ±1.58 99.83 ±0.16\nOA 74.20±0.49 84.18 ±1.12 80.48 ±0.84 79.09 ±0.85 85.68 ±0.86 83.69±2.44 82.99 ±1.02 81.43 ±0.48 83.90 ±0.95 86.92±1.13\nAA 74.73±0.60 86.94 ±0.93 82.14 ±1.26 81.21 ±1.18 87.40 ±0.71 85.49±1.79 84.64 ±1.08 82.43 ±0.63 86.01 ±0.60 89.20±0.86\nKappa 72.11±0.53 82.96 ±1.21 78.90 ±0.91 77.40 ±0.93 84.46 ±0.92 82.33±2.61 81.61 ±1.10 79.93 ±0.52 82.58 ±1.01 85.85±1.21\nTABLE VII\nCLASSIFICATION RESULTS OF DIFFERENT METHODS ON THE UT DATASET. THE BEST RESULTS ARE SHOWN IN BOLD .\nClass CNN-based Methods Transformer-based Methods\n3D-CNN SSRN 3DOC-SSAN FSKNet SPRN SSTN SSFTT GAHT morphFormer DFTN\n1 97.93±0.77 99.31±0.29 97.58±1.01 98.79 ±0.42 96.98 ±0.26 94.8±1.69 98.54 ±0.44 92.18 ±2.18 97.18 ±0.32 96.57 ±0.88\n2 80.51±2.29 94.87 ±3.03 89.97 ±4.69 90.22 ±2.54 99.65 ±0.29 95.0±3.74 87.38 ±3.35 95.21 ±2.36 94.25 ±2.40 96.74±1.73\n3 67.65±3.50 98.45±1.49 62.73±5.91 63.26 ±33.74 97.11 ±0.52 97.17±1.25 77.22 ±5.24 96.79 ±2.26 95.08 ±2.38 98.13 ±1.08\n4 98.86±0.64 99.76 ±0.21 98.80 ±0.62 95.64 ±2.47 100.0±0.00 99.93±0.14 95.47 ±1.83 96.90 ±1.87 96.72 ±2.70 99.88 ±0.13\n5 98.97±0.95 98.94 ±1.22 99.75 ±0.32 98.91 ±0.57 89.43 ±1.81 92.38±8.11 99.81±0.15 99.36±0.48 99.77 ±0.06 99.37 ±0.93\n6 63.05±4.54 68.73 ±5.93 63.68 ±8.32 72.90 ±6.41 83.24 ±3.19 84.14±3.04 87.37±5.13 62.87 ±4.70 69.32 ±6.14 78.44 ±3.56\nOA 92.93±0.75 95.71 ±0.53 94.03 ±0.63 93.92 ±1.15 94.08 ±0.82 94.46±3.29 95.56 ±0.57 93.44 ±0.79 94.75 ±0.91 96.72±0.48\nAA 84.49±1.05 93.34 ±0.92 85.42 ±0.81 86.62 ±6.23 94.40 ±0.63 93.90±2.14 90.96 ±1.17 90.55 ±1.08 92.05 ±0.66 94.85±0.61\nKappa 90.52±1.00 94.26 ±0.71 92.00 ±0.83 91.87 ±1.53 92.20 ±1.05 92.73±4.18 94.06 ±0.76 91.25 ±1.05 92.99 ±1.20 95.61±0.64\nsuperior classification performance in most patch sizes, further\ndemonstrating its effectiveness.\n2) Qualitative Comparison: The visual classification maps\nof the above mentioned methods on the three datasets are\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 11\n(a) False Color map\n (b) 3D-CNN\n(c) SSRN\n (d) 3DOC-SSAN\n(e) FSKNet\n (f) SPRN\n(g) SSTN\n (h) SSFTT\n(i) GAHT\n (j) morphFormer\n(k) DFTN\n (l) Ground Truth\nFig. 10. Classification maps generated by different models on the UH dataset.\nTABLE VIII\nOVERALL ACCURACIES OF DIFFERENT METHODS WITH DIFFERENT PATCH SIZES . THE BEST RESULTS ARE SHOWN IN\nBOLD .\nDataset Patch size CNN-based methods Transformer-based methods\n3D-CNN SSRN 3DOC-SSAN FSKNet SPRN SSTN SSFTT GAHT morphFormer DFTN\nUP\n9 85.18 ± 0.83 85.80 ± 1.89 85.73 ± 1.39 85.94 ± 1.10 92.41 ± 1.15 87.16 ± 5.72 89.13 ± 2.05 92.77 ± 0.86 85.44 ± 1.38 94.84 ± 1.37\n11 85.09 ± 0.90 84.38 ± 0.58 82.99 ± 2.84 85.46 ± 0.85 91.43 ± 2.36 88.19 ± 1.67 91.65 ± 0.66 92.40 ± 0.77 90.14 ± 3.54 94.14 ± 0.34\n13 84.15 ± 2.11 84.40 ± 1.03 82.19 ± 0.79 86.28 ± 1.47 90.15 ± 2.56 88.07 ± 5.94 91.05 ± 1.22 90.48 ± 0.62 91.45 ± 1.05 92.37 ± 0.56\n15 82.20 ± 1.41 84.71 ± 1.37 82.54 ± 1.78 87.03 ± 1.32 90.25 ± 2.91 88.03 ± 3.50 90.88 ± 0.98 88.52 ± 1.27 87.67 ± 0.36 93.53 ± 0.75\nUH\n9 75.49 ± 0.80 84.26 ± 1.20 81.09 ± 1.09 77.54 ± 0.78 86.77 ± 0.88 84.20 ± 0.96 82.16 ± 0.66 81.94 ± 0.50 82.56 ± 0.41 86.04 ± 0.81\n11 74.20 ± 0.49 84.18 ± 1.12 80.48 ±0.84 79.09 ± 0.85 85.68 ± 0.86 83.69 ± 2.44 82.99 ± 1.02 81.43 ± 0.48 83.90 ± 0.95 86.92 ± 1.13\n13 70.82 ± 1.15 84.47 ± 1.13 79.61 ± 2.23 80.21 ± 0.48 84.09 ± 1.00 85.25 ± 1.25 83.43 ± 1.43 78.95 ± 0.75 83.60 ± 1.42 85.98 ± 0.77\n15 69.93 ± 1.56 84.86 ± 1.03 78.24 ± 1.79 81.08 ± 1.06 83.79 ± 0.81 82.26 ± 2.35 82.36 ± 1.58 77.31 ± 1.13 82.53 ± 0.52 85.59 ± 1.24\nUT\n9 93.98 ± 0.28 95.26 ± 3.11 95.03 ± 1.10 88.58 ± 5.55 95.33 ± 1.08 97.44 ± 0.23 94.49 ± 0.76 93.81 ± 1.09 95.50 ± 0.89 97.56 ± 0.63\n11 92.93 ± 0.75 95.71 ± 0.53 94.03 ± 0.63 93.92 ± 1.15 94.08 ± 0.82 94.46 ± 3.29 95.56 ± 0.57 93.44 ± 0.79 94.75 ± 0.91 96.72 ± 0.48\n13 92.36 ± 0.56 95.81 ± 0.88 92.80 ± 1.27 95.37 ± 0.90 92.47 ± 0.39 90.94 ± 4.29 95.25 ± 0.63 92.04 ± 0.94 94.02 ± 0.56 95.15 ± 0.53\n15 91.95 ± 1.34 94.24 ± 3.55 92.34 ± 1.19 95.86 ± 1.23 91.91 ± 0.56 89.71 ± 1.75 94.46 ± 1.20 90.13 ± 0.70 92.95 ± 0.83 92.25 ± 1.97\ndisplayed in Figs. 9 - 11 to provide a qualitative evaluation\nof these methods. The areas where the proposed method\nexhibits better classification results than other methods are\nhighlighted in the black box. Through observing these figures,\nit is apparent that the devised DFTN model can correctly\npredict most of the samples. The whole classification maps\ngenerated by our DFTN method have fewer noises in most\nboundaries compared with those produced by the reference\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 12\n(a) False color map\n (b) 3D-CNN\n (c) SSRN\n(d) 3DOC-SSAN\n (e) FSKNet\n (f) SPRN\n(g) SSTN\n (h) SSFTT\n (i) GAHT\n(j) morphFormer\n (k) DFTN\n (l) Ground Truth\nFig. 11. Classification maps generated by different models on the UT datasets.\nmethods. This observation demonstrates the superior classifi-\ncation performance of the developed method.\nFurthermore, the distributions of the extracted features by\nthe DFTN method are visualized via t-Distributed Stochastic\nNeighbor Embedding (t-SNE) [60], as shown in Fig. 12. It is\nevident that features associated with the same class are closely\nclustered, while those from different categories are separated\nto a great extent. These findings indicate that our suggested\nmethod can effectively capture intrinsic features from different\ncategories. To sum up, these qualitative visualizations further\nprove the superiority of our developed DFTN method.\n3) Computational Cost Analysis: In order to demonstrate\nthe computational efficiency of DFTN, a comprehensive com-\nplexity analysis is performed. The number of parameters and\ninference time are presented in Table IX. It should be noted\nthat the inference time is calculated on the 12th Gen Intel(R)\nCore(TM) i7-12700F with 16GB of RAM and RTX 3070\nGPU. This table exhibits that although SSTN has the least\nnumber of parameters, its classification accuracy is lower than\nthat of DFTN. The proposed DFTN model has the second least\nnumber of parameters while achieving the highest accuracy\nwithin a reasonable inference time.\nF . Impact of Number of Traning Samples\nThe above experiments with divided training and test\ndatasets illustrate the promising performance the developed\nDFTN. However, in practical applications, the number of\nlabeled training samples is usually insufficient, which poses\na significant challenge to implement accurate HSIs classifi-\ncation. Therefore, the robustness of a model with a limited\nnumber of training data is a crucial evaluation criterion.\nMoreover, in order to increase the challenge of the dataset,\nwe randomly select subsets of 1%, 3%, 5%, 7%, and 9% from\nthe original training data to create new training samples with\nless labeled samples, and the test datasets remain unchanged.\nThe classification results of our proposed method and other\nreference methods on these sub-training datasets are displayed\nin Fig. 13. The results indicate that when facing the challenge\nof limited training samples, the OA score shows a decrease\nwith varying degrees. Also, it improves as the number of train-\ning samples increases. In the context of challenging scenarios\nwith limited training samples, the designed DFTN method,\ndepicted by the gray curve, demonstrates superiority over\ncomparative approaches. Although it is slightly lower than\n3DOC-SSAN and GAHT on the UP dataset, it achieves the\nhighest classification accuracies on the UH and UT datasets,\nhighlighting its robustness in challenging scenarios.\nIV. C ONCLUSION\nIn this paper, the transformer-based models for HSIs classifi-\ncation are reconsidered from the frequency domain perspective\nand a novel dual frequency transformer network (DFTN) is\ndeveloped. The designed DFTN model is tested on three\nreal-world HSIs and the experimental results demonstrate\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 13\n(a) UP\n (b) UH\n (c) UT\nFig. 12. T-SNE visualization on UP, UH, and UT datasets of the proposed method.\n3 5 7 9\nTraining Size (%)        \n50\n55\n60\n65\n70\n75\n80\n85\n90OA (%)\n3D-CNN\nSSRN\n3DOC-SSAN\nSSTN\nSSFTT\nGAHT\nmorphFormer\nDFTN\n(a) UP\n3 5 7 9\nTraining Size (%)        \n10\n20\n30\n40\n50\n60\n70\n80OA (%)\n3D-CNN\nSSRN\n3DOC-SSAN\nSSTN\nSSFTT\nGAHT\nmorphFormer\nDFTN (b) UH\n3 5 7 9\nTraining Size (%)        \n50\n60\n70\n80\n90OA (%)\n3D-CNN\nSSRN\n3DOC-SSAN\nSSTN\nSSFTT\nGAHT\nmorphFormer\nDFTN (c) UT\nFig. 13. Overall accuracies of different methods with various sizes of training data.\nTABLE IX\nCOMPUTATIONAL COST OF DIFFERENT METHODS ON THE SELECTED DATASETS .\nDataset Computational Cost CNN-based methods Transformer-based methods\n3D-CNN SSRN 3DOC-SSAN FSKNet SPRN SSTN SSFTT GAHT morphFormer DFTN\nUP Number of Parameters 246,719 216,537 444,953 153,117 179,017 15,623 484,417 1,064,713 148,905 32,265\nInference Time (s) 6.38 10.57 26.62 6.32 6.58 6.18 5.86 8.53 11.68 6.98\nUH Number of Parameters 519,410 278,127 562,869 153,891 182,351 17,734 673,735 1,067,407 154,735 34,991\nInference Time (s) 2.37 4.16 11.15 1.91 1.99 1.95 1.95 2.85 3.76 2.44\nUT Number of Parameters 123,236 155,022 368,478 152,730 176,070 13,612 299,902 1,059,910 105,190 29,654\nInference Time (s) 2.48 4.44 11.75 2.23 2.64 2.54 2.20 4.84 6.64 3.58\nthe promising potential of our method. In the first place,\nthe suggested model exhibits the state-of-the-art classifica-\ntion performance and outperforms other reference methods\non the three HSIs datasets, achieving the overall accuracies\nof 94.14%, 86.92%, and 96.72% on the UP, UH, and UT\ndatasets, respectively. Second, the experiments with fewer\nnumber of training samples demonstrate the robustness of our\nproposed method. Thus, it could also be concluded that in the\nchallenging scenarios where training data are limited, DFTN\ncan still obtain satisfactory classification results. Thirdly, the\nfrequency domain representations are decomposed into high-\nand low-frequency features and the visual spectrum analysis\nfor the magnitude of the frequency component illustrates that\nour designed FDFE block with a dual-branch structure can\neffectively extract high- and low-frequency representations.\nFurthermore, the ablation studies with different frequency\nfeatures prove that the combination of both high- and low-\nfrequency features can enhance the classification results. For\ninstance, the integration of high- and low-frequency features\nresults in an increment about 3% for the UP dataset. All these\nfindings highlight the superiority of our developed method for\nHSIs classification.\nIn this work, for simplicity, we assume that high- and low-\nfrequency information have the same importance. Therefore,\nthe HiFE and LoFE branches are allocated with the same\nnumber of channels in our designed model. Nevertheless,\nHSIs data obtained under different scenes may have different\nrequirements for the high and low frequencies. As a result, how\nto split high and low frequency channels in a more reasonable\nway is our next research direction.\nREFERENCES\n[1] P. Ghamisi, J. Plaza, Y . Chen, J. Li, and A. J. Plaza, “Advanced spectral\nclassifiers for hyperspectral images: A review,” IEEE Geosci. Remote\nSens. Mag., vol. 5, no. 1, pp. 8–32, Mar. 2017.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 14\n[2] L. Mou, X. Lu, X. Li, and X. X. Zhu, “Nonlocal graph convolutional\nnetworks for hyperspectral image classification,” IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 12, pp. 8246–8257, Dec. 2020.\n[3] J. J. Mitchell, N. F. Glenn, M. O. Anderson, R. C. Hruska, A. Halford,\nC. Baun, and N. Nydegger, “Unmanned aerial vehicle (UA V) hyper-\nspectral remote sensing for dryland vegetation monitoring,” in Proc.\nHyperspectral Image and Signal Processing, 2012, pp. 1–10.\n[4] P. Duan, X. Kang, P. Ghamisi, and S. Li, “Hyperspectral remote sensing\nbenchmark database for oil spill detection with an isolation forest-guided\nunsupervised detector,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp.\n1–11, Apr. 2023.\n[5] R. Gloaguen, P. Ghamisi, S. Lorenz, M. Kirsch, R. Zimmermann,\nR. Booysen, L. Andreani, R. Jackisch, E. Hermann, L. Tusa et al., “The\nneed for multi-source, multi-scale hyperspectral imaging to boost non-\ninvasive mineral exploration,” in Proc. IEEE Int. Geosci. Remote Sens.\nSymp., 2018, pp. 7430–7433.\n[6] S. Li, W. Song, L. Fang, Y . Chen, P. Ghamisi, and J. A. Benediktsson,\n“Deep learning for hyperspectral image classification: An overview,”\nIEEE Trans. Geosci. Remote Sens., vol. 57, no. 9, pp. 6690–6709, Sep.\n2019.\n[7] Y . Chen, H. Jiang, C. Li, X. Jia, and P. Ghamisi, “Deep feature extraction\nand classification of hyperspectral images based on convolutional neural\nnetworks,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 10, pp. 6232–\n6251, Oct. 2016.\n[8] L. He, J. Li, C. Liu, and S. Li, “Recent advances on spectral–spatial\nhyperspectral image classification: An overview and new guidelines,”\nIEEE Trans. Geosci. Remote Sens., vol. 56, no. 3, pp. 1579–1597, Mar.\n2018.\n[9] Y . Gu, J. Chanussot, X. Jia, and J. A. Benediktsson, “Multiple kernel\nlearning for hyperspectral image classification: A review,” IEEE Trans.\nGeosci. Remote Sens., vol. 55, no. 11, pp. 6547–6565, Nov. 2017.\n[10] Y . Guo, S. Han, Y . Li, C. Zhang, and Y . Bai, “K-nearest neighbor\ncombined with guided filter for hyperspectral image classification,”Proc.\nComput. Sci, vol. 129, pp. 159–165, Apr. 2018.\n[11] L. Ma, M. M. Crawford, and J. Tian, “Local manifold learning-based\nk-nearest-neighbor for hyperspectral image classification,” IEEE Trans.\nGeosci. Remote Sens., vol. 48, no. 11, pp. 4099–4109, Nov. 2010.\n[12] J. Li, J. M. Bioucas-Dias, and A. Plaza, “Spectral–spatial hyperspectral\nimage segmentation using subspace multinomial logistic regression and\nmarkov random fields,”IEEE Trans. Geosci. Remote Sens., vol. 50, no. 3,\npp. 809–823, Mar. 2012.\n[13] F. Melgani and L. Bruzzone, “Classification of hyperspectral remote\nsensing images with support vector machines,” IEEE Trans. Geosci.\nRemote Sens., vol. 42, no. 8, pp. 1778–1790, Aug. 2004.\n[14] L. Gao, J. Li, M. Khodadadzadeh, A. Plaza, B. Zhang, Z. He, and\nH. Yan, “Subspace-based support vector machines for hyperspectral\nimage classification,” IEEE Geosci. Remote Sens. Lett., vol. 12, no. 2,\npp. 349–353, Feb. 2015.\n[15] J. Li, J. M. Bioucas-Dias, and A. Plaza, “Semisupervised hyperspectral\nimage classification using soft sparse multinomial logistic regression,”\nIEEE Geosci. Remote Sens. Lett., vol. 10, no. 2, pp. 318–322, Mar. 2013.\n[16] H. Yu, L. Gao, W. Liao, B. Zhang, A. Pi ˇzurica, and W. Philips,\n“Multiscale superpixel-level subspace-based support vector machines for\nhyperspectral image classification,” IEEE Geosci. Remote Sens. Lett.,\nvol. 14, no. 11, pp. 2142–2146, Nov. 2017.\n[17] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nat., vol. 521,\nno. 7553, pp. 436–444, May 2015.\n[18] L. Zhang and L. Zhang, “Artificial intelligence for remote sensing data\nanalysis: A review of challenges and opportunities,”IEEE Trans. Geosci.\nRemote Sens., vol. 10, no. 2, pp. 270–294, Jun. 2022.\n[19] X. Yang, Y . Ye, X. Li, R. Y . Lau, X. Zhang, and X. Huang, “Hyper-\nspectral image classification with deep learning models,” IEEE Trans.\nGeosci. Remote Sens., vol. 56, no. 9, pp. 5408–5423, Sep. 2018.\n[20] W. Zhao and S. Du, “Spectral–spatial feature extraction for hyper-\nspectral image classification: A dimension reduction and deep learning\napproach,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 8, pp. 4544–\n4554, Aug. 2016.\n[21] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep learning-based\nclassification of hyperspectral data,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 7, no. 6, pp. 2094–2107, Jun. 2014.\n[22] W. Hu, Y . Huang, L. Wei, F. Zhang, and H. Li, “Deep convolutional\nneural networks for hyperspectral image classification,” J. Sensors, vol.\n2015, pp. 1–12, Jan. 2015.\n[23] S. K. Roy, G. Krishna, S. R. Dubey, and B. B. Chaudhuri, “Hybridsn:\nExploring 3-d–2-d cnn feature hierarchy for hyperspectral image classi-\nfication,” IEEE Geosci. Remote Sens. Lett., vol. 17, no. 2, pp. 277–281,\nFeb. 2020.\n[24] P. Duan, P. Ghamisi, X. Kang, B. Rasti, S. Li, and R. Gloaguen, “Fusion\nof dual spatial information for hyperspectral image classification,” IEEE\nTrans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7726–7738, Nov. 2021.\n[25] P. Duan, S. Hu, X. Kang, and S. Li, “Shadow removal of hyperspectral\nremote sensing images with multiexposure fusion,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–11, Sep. 2022.\n[26] Z. Zhong, J. Li, Z. Luo, and M. Chapman, “Spectral–spatial residual\nnetwork for hyperspectral image classification: A 3-d deep learning\nframework,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 2, pp. 847–\n858, Feb. 2018.\n[27] M. Zhang, W. Li, and Q. Du, “Diverse region-based cnn for hyperspec-\ntral image classification,” IEEE Trans. Image Process., vol. 27, no. 6,\npp. 2623–2634, Jun. 2018.\n[28] X. Qiao, S. K. Roy, H. Wu, and W. Huang, “Hyperspectral image\nclassification based on 3d sharpened cosine similarity operation,” in\nProc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), 2023, pp. 7669–\n7672.\n[29] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph\nconvolutional networks for hyperspectral image classification,” IEEE\nTrans. Geosci. Remote Sens., vol. 59, no. 7, pp. 5966–5978, Jul. 2020.\n[30] R. Hang, Q. Liu, D. Hong, and P. Ghamisi, “Cascaded recurrent neural\nnetworks for hyperspectral image classification,” IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 8, pp. 5384–5394, Aug. 2019.\n[31] X. Mei, E. Pan, Y . Ma, X. Dai, J. Huang, F. Fan, Q. Du, H. Zheng,\nand J. Ma, “Spectral-spatial attention networks for hyperspectral image\nclassification,” Remote Sens., vol. 11, no. 8, p. 963, Apr. 2019.\n[32] H. Sun, X. Zheng, X. Lu, and S. Wu, “Spectral–spatial attention network\nfor hyperspectral image classification,” IEEE Trans. Geosci. Remote\nSens., vol. 58, no. 5, pp. 3232–3245, May 2020.\n[33] L. Mou and X. X. Zhu, “Learning to pay attention on spectral domain: A\nspectral attention module-based convolutional network for hyperspectral\nimage classification,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 1,\npp. 110–122, Jan. 2020.\n[34] X. Qiao, S. K. Roy, and W. Huang, “Rotation is all you need: Cross\ndimensional residual interaction for hyperspectral image classification,”\nIEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 16, pp. 5387–\n5404, Jun. 2023.\n[35] J. M. Haut, M. E. Paoletti, J. Plaza, A. Plaza, and J. Li, “Visual attention-\ndriven hyperspectral image classification,” IEEE Trans. Geosci. Remote\nSens., vol. 57, no. 10, pp. 8065–8080, Oct. 2019.\n[36] R. Hang, Z. Li, Q. Liu, P. Ghamisi, and S. S. Bhattacharyya, “Hy-\nperspectral image classification with attention-aided cnns,” IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 3, pp. 2281–2293, Mar. 2021.\n[37] S. K. Roy, S. Manna, T. Song, and L. Bruzzone, “Attention-based adap-\ntive spectral–spatial kernel resnet for hyperspectral image classification,”\nIEEE Trans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7831–7843, Sep.\n2021.\n[38] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. Adv.\nNeural Inf. Process. Syst., Dec. 2017, pp. 5998–6008.\n[39] X. Qiao and W. Huang, “Spectral-spatial-frequency transformer network\nfor hyperspectral image classification,” in Proc. IEEE Sens. Appl. Symp.,\n2023, pp. 1–6.\n[40] X. Qiao, S. K. Roy, and W. Huang, “Multi-scale neighborhood attention\ntransformer with optimized spatial pattern for hyperspectral image\nclassification,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp. 1–15,\nSep. 2023.\n[41] S. Mei, C. Song, M. Ma, and F. Xu, “Hyperspectral image classifica-\ntion using group-aware hierarchical transformer,” IEEE Trans. Geosci.\nRemote Sens., vol. 60, pp. 1–14, Oct. 2022.\n[42] Y . Peng, Y . Zhang, B. Tu, Q. Li, and W. Li, “Spatial–spectral transformer\nwith cross-attention for hyperspectral image classification,” IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1–15, Sep. 2022.\n[43] D. Hong, Z. Han, J. Yao, L. Gao, B. Zhang, A. Plaza, and J. Chanus-\nsot, “Spectralformer: Rethinking hyperspectral image classification with\ntransformers,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–15,\nFeb. 2022.\n[44] L. Sun, G. Zhao, Y . Zheng, and Z. Wu, “Spectral–spatial feature\ntokenization transformer for hyperspectral image classification,” IEEE\nTrans. Geosci. Remote Sens., vol. 60, pp. 1–14, Mar. 2022.\n[45] H. Yu, Z. Xu, K. Zheng, D. Hong, H. Yang, and M. Song, “Mstnet: A\nmultilevel spectral–spatial transformer network for hyperspectral image\nclassification,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–13,\nJul. 2022.\n[46] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al., “A survey on vision transformer,” IEEE Trans.\nPattern Anal. Mach. Intell., vol. 45, no. 1, pp. 87–110, Jan. 2023.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND REMOTE SENSING 15\n[47] J. Zou, W. He, and H. Zhang, “Lessformer: Local-enhanced spectral-\nspatial transformer for hyperspectral image classification,” IEEE Trans.\nGeosci. Remote Sens., vol. 60, pp. 1–16, Aug. 2022.\n[48] F. Wang, J. Li, Q. Yuan, and L. Zhang, “Local–global feature-aware\ntransformer based residual network for hyperspectral image denoising,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–19, Dec. 2022.\n[49] B. Tu, X. Liao, Q. Li, Y . Peng, and A. Plaza, “Local semantic feature\naggregation-based transformer for hyperspectral image classification,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–15, Sep. 2022.\n[50] S. K. Roy, A. Deria, C. Shah, J. M. Haut, Q. Du, and A. Plaza,\n“Spectral–spatial morphological attention transformer for hyperspectral\nimage classification,” IEEE Trans. Geosci. Remote Sens., vol. 61, pp.\n1–15, Feb. 2023.\n[51] X. Tang, F. Meng, X. Zhang, Y .-M. Cheung, J. Ma, F. Liu, and L. Jiao,\n“Hyperspectral image classification based on 3-d octave convolution\nwith spatial–spectral attention network,” IEEE Trans. Geosci. Remote\nSens., vol. 59, no. 3, pp. 2430–2447, Mar. 2021.\n[52] Y . Chen, H. Fan, B. Xu, Z. Yan, Y . Kalantidis, M. Rohrbach, S. Yan, and\nJ. Feng, “Drop an octave: Reducing spatial redundancy in convolutional\nneural networks with octave convolution,” in Proc. IEEE Int. Conf.\nComput. Vis., Oct. 2019, pp. 3435–3444.\n[53] Z. Pan, J. Cai, and B. Zhuang, “Fast vision transformers with hilo\nattention,” Adv. Neural Inf. Process. Syst., vol. 35, pp. 14 541–14 554,\n2022.\n[54] N. Park and S. Kim, “How do vision transformers work?” in Proc. Int.\nConf. Learn. Representations, 2022, pp. 1–14.\n[55] E. O. Brigham, The Fast Fourier Transform and its Applications .\nHoboken, NJ, USA: Prentice-Hall, 1988.\n[56] Y . Rao, W. Zhao, Z. Zhu, J. Lu, and J. Zhou, “Global filter networks for\nimage classification,” in Proc. Adv. Neural Inf. Process. Syst., vol. 34,\n2021, pp. 980–993.\n[57] X. Zhang, S. Shang, X. Tang, J. Feng, and L. Jiao, “Spectral partitioning\nresidual network with spatial attention mechanism for hyperspectral\nimage classification,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp.\n1–14, Jan. 2022.\n[58] G. Li and C. Zhang, “Faster hyperspectral image classification based\non selective kernel mechanism using deep convolutional networks,”\narXiv:2202.06458, 2022.\n[59] Z. Zhong, Y . Li, L. Ma, J. Li, and W. Zheng, “Spectral–spatial\ntransformer network for hyperspectral image classification: A factorized\narchitecture search framework,” IEEE Trans. Geosci. Remote Sens.,\nvol. 60, pp. 1–15, Jan. 2022.\n[60] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” J.\nMach. Learn. Res., vol. 9, pp. 2579–2605, Nov. 2008.\nXin Qiao (Graduate Student Member, IEEE) re-\nceived the B.Eng. degree in navigation from the\nDalian Maritime University, Dalian, China, in 2018,\nand the M.Eng. degree in naval architecture and\nocean engineering from the Zhejiang University,\nZhejiang, China, in 2021. He is currently work-\ning toward a Ph.D. degree in electrical engineering\nwith the Memorial University of Newfoundland, St.\nJohn’s, NL, Canada. His research interests include\nhyperspectral image classification.\nWeimin Huang (Senior Member, IEEE) received\nthe B.S., M.S., and Ph.D. degrees in radio physics\nfrom Wuhan University, Wuhan, China, in 1995,\n1997, and 2001, respectively, and the M.Eng. degree\nin electrical engineering from the Memorial Univer-\nsity of Newfoundland, St. John’s, NL, Canada, in\n2004.\nFrom 2008 to 2010, he was a Design Engineer\nwith Rutter Technologies, St. John’s. Since 2010, he\nhas been with the Faculty of Engineering and Ap-\nplied Science, Memorial University of Newfound-\nland, where he is currently a Professor. He has authored more than 260\nresearch articles. His research interests include the mapping of oceanic surface\nparameters via high-frequency ground wave radar, X-band marine radar,\nsynthetic aperture radar, and global navigation satellite systems.\nDr. Huang has been a Technical Program Committee Member. He served\nas the Technical Program Co-Chair for the IEEE Newfoundland Electrical\nand Computer Engineering Conference in 2012 and 2013. He is an Editor\nof the book Ocean Remote Sensing Technologies: High Frequency, Marine\nand GNSS-Based Radar. He is also an Area Editor for IEEE CANADIAN\nJOURNAL OF ELECTRICAL AND COMPUTER ENGINEERING, an Asso-\nciate Editor for IEEE GEOSCIENCE AND REMOTE SENSING LETTERS,\nIEEE JOURNAL OF OCEANIC ENGINEERING, Remote Sensing, Frontiers\nin Marine Science, and he has been a Guest Editor for IEEE JOURNAL\nOF SELECTED TOPICS IN APPLIED EARTH OBSERV ATIONS AND\nREMOTE SENSING and five other journals. He serves as a Reviewer for more\nthan 130 international journals and a Reviewer for many IEEE international\nconferences, such as RadarCon, International Conference on Communications,\nIEEE Global Communications Conference, IEEE International Geoscience\nand Remote Sensing Symposium, and Oceans. He was a Member and Co-\nChair of the Electrical and Computer Engineering Evaluation Group for\nNatural Sciences and Engineering Research Council of Canada (NSERC)\nDiscovery Grants from 2018-2021. He was the recipient of Postdoctoral\nFellowship from the Memorial University of Newfoundland; the Discovery\nAccelerator Supplements Award from NSERC in 2017; and the IEEE Geo-\nscience and Remote Sensing Society 2019 Letters Prize Paper Award as well\nas some other teaching and research awards.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3328115\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}