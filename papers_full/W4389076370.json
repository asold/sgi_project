{
    "title": "A Multi-Task Vision Transformer for Segmentation and Monocular Depth Estimation for Autonomous Vehicles",
    "url": "https://openalex.org/W4389076370",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2225435301",
            "name": "Durga Prasad Bavirisetti",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5093357687",
            "name": "Herman Ryen Martinsen",
            "affiliations": [
                "Capgemini (Norway)",
                "Fredrikstad Energi (Norway)"
            ]
        },
        {
            "id": "https://openalex.org/A4302274458",
            "name": "Gabriel Hanssen Kiss",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1863035111",
            "name": "Frank Lindseth",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285228542",
        "https://openalex.org/W2889985731",
        "https://openalex.org/W4312815172",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W1905829557",
        "https://openalex.org/W6786385858",
        "https://openalex.org/W2300779272",
        "https://openalex.org/W3177698162",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W4285109022",
        "https://openalex.org/W2892614179",
        "https://openalex.org/W6811016991",
        "https://openalex.org/W4312652696",
        "https://openalex.org/W6811234694",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6810242703",
        "https://openalex.org/W6810661541",
        "https://openalex.org/W4386076132",
        "https://openalex.org/W6843033949",
        "https://openalex.org/W6802719855",
        "https://openalex.org/W2793404847",
        "https://openalex.org/W4386187806",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W7064113314",
        "https://openalex.org/W4380318404",
        "https://openalex.org/W2343077198",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W6763070779",
        "https://openalex.org/W2562137921",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4293812175",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2996945478",
        "https://openalex.org/W4308450151",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W4313166855",
        "https://openalex.org/W6810073510",
        "https://openalex.org/W2962762260",
        "https://openalex.org/W6788620109",
        "https://openalex.org/W4226213006",
        "https://openalex.org/W4226191818",
        "https://openalex.org/W2951234442",
        "https://openalex.org/W4225949693",
        "https://openalex.org/W4386723894",
        "https://openalex.org/W3203597819",
        "https://openalex.org/W3099319035",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4221153029",
        "https://openalex.org/W3213165621",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3173727695"
    ],
    "abstract": "In this paper, we investigate the use of Vision Transformers for processing and understanding visual data in an autonomous driving setting. Specifically, we explore the use of Vision Transformers for semantic segmentation and monocular depth estimation using only a single image as input. We present state-of-the-art Vision Transformers for these tasks and combine them into a multitask model. Through multiple experiments on four different street image datasets, we demonstrate that the multitask approach significantly reduces inference time while maintaining high accuracy for both tasks. Additionally, we show that changing the size of the Transformer-based backbone can be used as a trade-off between inference speed and accuracy. Furthermore, we investigate the use of synthetic data for pre-training and show that it effectively increases the accuracy of the model when real-world data is limited.",
    "full_text": "Received XX Month, XXXX; revised XX Month, XXXX; accepted XX Month, XXXX; Date of publication XX Month, XXXX; date of\ncurrent version XX Month, XXXX.\nDigital Object Identifier 10.1109/OJITS.2022.1234567\nA Multi-task Vision Transformer for\nSegmentation and Monocular Depth\nEstimation for Autonomous Vehicles\nDurga Prasad Bavirisetti∗, Herman Ryen Martinsen∗, Gabriel Hanssen Kiss∗, Frank\nLindseth∗\n1Department of Computer Science, Norwegian University of Science and Technology, Gløshaugen, Trondheim, 7034, Trøndelag, Norway.\nCORRESPONDING AUTHOR: (e-mail: durga.bavirisetti@ntnu.no).\nABSTRACT In this paper, we investigate the use of Vision Transformers for processing and understanding\nvisual data in an autonomous driving setting. Specifically, we explore the use of Vision Transformers for\nsemantic segmentation and monocular depth estimation using only a single image as input. We present\nstate-of-the-art Vision Transformers for these tasks and combine them into a multitask model. Through\nmultiple experiments on four different street image datasets, we demonstrate that the multitask approach\nsignificantly reduces inference time while maintaining high accuracy for both tasks. Additionally, we show\nthat changing the size of the Transformer-based backbone can be used as a trade-off between inference\nspeed and accuracy. Furthermore, we investigate the use of synthetic data for pre-training and show that\nit effectively increases the accuracy of the model when real-world data is limited.\nINDEX TERMS vision transformer; monocular depth prediction; autonomous vehicles; segmentation; multi-\ntask.\nI. Introduction\nThe research and development of autonomous vehicles [1],\n[12] has gained significant attention in recent years, with\nbig companies such as Tesla, Waymo, and GM investing\nin new technology. Autonomous vehicles offer numerous\nbenefits, such as reduced road accidents [36], improved\ntraffic efficiency [14], easier accessibility for disabled and\nelderly people, and lower greenhouse gas emissions [17].\nHowever, reliability is crucial to ensure the safety of human\nlives.\nAutonomous vehicles sense their surroundings using a\nvariety of sensors such as cameras, radar, and LiDAR. While\ncameras are more affordable and compact, LiDAR sensors\nare larger and more expensive, thus making it desirable to\nreplace them with cameras.\nThe rapid advancement in the field of machine learning,\ndue to the availability of large data and powerful computing\nresources, has made deep learning a practical solution for\nreal-world problems. The field of computer vision, which ap-\nplies deep learning to visual data, is crucial for autonomous\ndriving as it enables the vehicle to gather information about\nits surroundings using cameras. Object detection, segmen-\ntation, and depth estimation are commonly used techniques\nfor autonomous driving.\nIn 2017, Vaswani et al. [35] introduced the Transformer,\na deep-learning architecture for natural language processing\ntasks, which achieved remarkable results. Inspired by its\nsuccess, Dosovitskiy et al. [7] applied the architecture to\nvision tasks in 2020, leading to the creation of the Vision\nTransformer, which achieved new state-of-the-art results and\ncaused a stir in the deep learning community. Since then,\nseveral new Vision Transformer architectures have been\nproposed.\nThis paper explores the use of Vision Transformers for\ndense prediction tasks in autonomous driving. It focuses\non the tasks of monocular depth estimation and semantic\nsegmentation and designs a multitask model that can perform\nboth simultaneously. The effectiveness of the model will\nbe studied through training and evaluation of multiple au-\ntonomous driving datasets. To achieve this goal the following\nresearch question (RQ)s are studied:\n• RQ1. Does a multitask Vision Transformer perform\nbetter than models trained for individual tasks?\n• RQ2: Can synthetic data enhance model performance\nwhen real-world data is scarce?\n• RQ3: How accurate are the depth predictions from the\nmodel?\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME , 1\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nThe research approach adopted in this paper is experi-\nmental, which involves exploring the relationship between\na cause and its effect. The experiments will focus on ex-\namining the impact of factors such as training methods and\nbackbone choice on performance. The performance of the\nmodels will be evaluated using both quantitative and qual-\nitative analysis. Quantitative analysis involves computing\naccuracy metrics, while qualitative analysis involves visually\ninspecting individual predictions for insights and anomalies.\nThe main contributions of the proposed method are as\nfollows:\n• Most of the multitask learning methods [25], [30]\nuse convolutional neural networks (CNNs) for joint\nsegmentation and depth estimation. In addition, they\nare multi-stage methods. Unlike them, our method is\na simple single-stage method that takes an image as\ninput and performs the joint segmentation and depth\nestimation tasks in a single forward pass.\n• For this purpose, we designed a hybrid encoding and\ndecoding framework based on Vision transformer vari-\nants SegFormer [40] and GLPDepth [18].\n• We chose the best model for each task (segmentation\nand depth estimation) to design a multitask model\nbased on a thorough assessment of their advantages and\ndrawbacks. Our selection process involved a meticulous\nanalysis of the benefits, drawbacks, and feasibility of\nimplementing these models.\n• The inference time of our multitask model is less than\nthat of the individual task models, even though it is\nperforming two tasks at a time.\n• Our model performed well during evaluation on unseen\ndata (NAP Lab dataset), despite being trained on other\ndatasets. This highlights the generalization capability of\nour model.\n• The quantitative results of our multitask model are\ncomparable to those of individual task models, demon-\nstrating the capability of our model to replace the need\nfor two different models to perform two distinct tasks.\n• We show that changing the size of the Transformer-\nbased backbone can be used as a trade-off between\ninference speed and accuracy.\n• We have also investigated the use of synthetic data for\npre-training and show that it effectively increases the\naccuracy of the model when real-world data is limited.\nThe remainder of the paper has the following organization:\nSection II, Background and Related Work explains the\nVision Transformers and gives an overview of state-of-the-\nart models. Section III, Methodology, presents the selection\nof the semantic segmentation and depth estimation models,\nthe functioning of chosen SegFormer and GLPDepth models,\nand the proposed multitask model. Section IV, Experiments\nand Results, details the data preparation process, metrics,\nhardware, and training setup. In addition, it also presents\nthe qualitative and quantitative outcomes of the experiments\nconducted. Section V, Discussion, examines the results and\nanswers the research questions. Section VI, Conclusion and\nFuture Future Directions, summarizes the paper, its key\nfindings, and suggests possible directions for future research.\nII. Background and Related Work\nIn this section, segmentation and depth estimation with their\nevaluation metrics, transformers, vision transformers, and\nthe relevant state-of-the-art Vision Transformer models for\nsemantic segmentation and monocular depth estimation will\nbe presented.\nSegmentation is the task of categorizing image pixels\ninto different labels. There are three types of segmentation:\nsemantic, instance, and panoptic. In semantic segmentation,\npixels are assigned labels based on what object/structure\nthey are part of (e.g. car, sky, building), without separating\ninstances of the same category. In instance segmentation,\neach instance of a category is given a separate label, usually\nonly for categories of interest, not the entire image. Panoptic\nsegmentation combines semantic and instance segmentation.\nThis paper focuses on exploring the semantic segmentation\ntask.\nDepth estimation is the task of predicting depth in a scene\nusing images. The aim is to determine the distance from the\ncamera to each pixel. There are two main methods for this\ntask: stereo depth estimation and monocular depth estima-\ntion. The traditional method, stereo depth estimation, uses\ntwo cameras with a known distance between them to find\nthe disparity between matching pixels and calculate depth.\nMonocular depth estimation, using deep neural networks,\nis a more recent approach that predicts the depth of each\npixel using a single image. Monocular depth estimation can\nbe done using either supervised learning with ground truth\ndepth maps or self-supervised learning without ground truth.\nThis study focuses on the supervised approach for monocular\ndepth estimation.\nThe Transformer [35] is a deep learning architecture that\nwas introduced in the 2017 paper ”Attention is All You\nNeed.” It was specifically designed for processing sequen-\ntial data, such as natural language, and has since become\na cornerstone in the field of natural language processing\n(NLP). Unlike traditional recurrent neural networks (RNNs),\nthe Transformer uses self-attention mechanisms to capture\ndependencies between elements in a sequence. The architec-\nture of the Transformer consists of multiple identical layers,\neach composed of two sub-layers: multi-head self-attention\nand a feed-forward network. The multi-head self-attention\nmechanism allows the network to weigh the importance of\ndifferent elements in the sequence. This is achieved by com-\nputing attention scores for each element in the sequence with\nrespect to every other element. The feed-forward network\nprocesses the information from the attention mechanism and\nis composed of two linear transformations followed by a non-\nlinear activation function. The Transformer’s self-attention\nmechanism and feed-forward network work in tandem to\n2 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nprocess the input sequence and produce an output sequence.\nThe output of the final layer is used to make predictions,\nsuch as for language modeling or machine translation. The\nTransformer’s use of self-attention mechanisms and parallel\nprocessing of the input sequence has proven to be highly\neffective, leading to its widespread use in various NLP tasks.\nThe Vision Transformer (ViT) [7] is a variant of the Trans-\nformer architecture that was introduced to tackle computer\nvision tasks. Unlike traditional convolutional neural networks\n(CNNs) that use convolutional layers to process image data,\nthe ViT processes image data as sequences of feature vectors\nextracted from patches of the image. This allows the ViT to\ntake advantage of the Transformer’s ability to handle se-\nquential data and process long-range dependencies between\npatches in an image. The network architecture of the ViT\nis similar to that of the Transformer, consisting of multiple\nidentical layers, each composed of two sub-layers: multi-\nhead self-attention and a feed-forward network. The multi-\nhead self-attention mechanism allows the network to weigh\nthe importance of different patches in the image and capture\nthe relationships between them. The feed-forward network\nprocesses the information from the attention mechanism\nand is composed of two linear transformations followed by\na non-linear activation function. The ViT architecture has\nbeen successfully applied to a variety of computer vision\ntasks, including image classification, object detection, and\nsegmentation. Its ability to process image data as sequences\nof feature vectors allows it to effectively capture long-range\ndependencies between patches in an image, resulting in\nimproved performance compared to traditional CNNs.\nThe following subsections discuss previous studies that\nemployed Vision Transformers for semantic segmentation\nand monocular depth estimation. In selecting relevant works,\nthree key criteria were taken into account: 1) high accuracy\non autonomous driving datasets, 2) efficient, near real-time\ninference, and 3) significant contribution to the field.\nA. Relevant state-of-the-art Vision Transformer models\nfor semantic segmentation\nThe Segmentation Transformer (SETR) [44] was one of the\nfirst attempts to apply Transformers to semantic segmen-\ntation. Prior to SETR, the common approach was to use\na CNN backbone, which suffered from a limited receptive\nfield and was unable to capture long-range dependencies in\nimages. SETR solved this issue by introducing a Transformer\nencoder with a global receptive field and combining it with\na lightweight convolutional decoder. The resulting model\nachieved a mean Intersection over Union (mIoU) score of\n82.15 on the Cityscapes dataset.\nIn May 2021, Xie et al. introduced SegFormer [40], a\nsimple and efficient encoder-decoder design for semantic\nsegmentation using Transformers. SegFormer’s Transformer\nencoder is inspired by ViT [7] and has multiple modifications\nto enhance performance for semantic segmentation. It has\na hierarchical structure, outputs multi-scale feature maps,\nand utilizes the efficient self-attention calculation method\nfrom PVT [37], which improves processing speed for high-\nresolution images. The decoder is lightweight and consists\nof only MLP layers. There are 6 different model sizes, with\nthe smallest suitable for real-time applications and the largest\nachieving high accuracy on the Cityscapes and ADE20K\ndatasets with a mIoU of 84.0 on the Cityscapes dataset.\nThe model also performs well on corrupted cityscapes data,\nindicating its robustness and potential suitability for safety-\ncritical tasks such as autonomous driving.\nTraditionally, semantic and instance segmentation were\napproached as two distinct tasks, where semantic segmen-\ntation was viewed as a per-pixel classification task, and\ninstance segmentation as a mask classification task. However,\nCheng et al. proposed that the tasks could be combined\nand solved by a single model - the MaskFormer [4]. The\narchitecture employs mask classification to produce semantic\nsegmentation predictions, incorporating a Transformer de-\ncoder that predicts binary masks with respective class labels.\nThe MaskFormer was tested with various backbones, but\nSwin Transformer [27] provided the best results. However,\nthe results were limited to the Cityscapes dataset using a\nResNet backbone and achieved a mIoU of 81.4.\nIn December 2021, Cheng et al. [3] improved upon the\nMaskFormer architecture with their new Mask2Former. They\nmade several improvements to increase performance and\nsimplify training, such as implementing masked attention\nin the Transformer decoder to limit attention calculations\nto local features, using multi-scale high-resolution features\nto detect small objects, and making small changes to im-\nprove performance and save training memory. Mask2Former\nachieved a mIoU of 84.5 on the Cityscapes dataset.\nIn December 2021, Jain et al. introduced SeMask [16], a\nsolution to address the issue of the encoder struggling to cap-\nture the semantic context of the image in Vision Transform-\ners for semantic segmentation. The architecture implements\na semantic layer that follows the Transformer layer at every\nstage of the encoder, consisting of multiple SeMask blocks\nthat apply semantic attention operation to feature maps. The\nSeMask block can be integrated with any hierarchical Vision\nTransformer and when used with the Swin Transformer [27]\nand SegFormer [40], the best-performing model achieved a\nmIoU of 84.98 on the Cityscapes dataset.\nYan et al. introduced the Lawin Transformer [42], a\nTransformer-based architecture for semantic segmentation,\nin January 2022. They address the issue of current Vision\nTransformers not producing contextual information at multi-\nple scales, which affects performance and efficiency. The\nLawin Transformer introduces a novel decoder, the large\nwindow attention spatial pyramid pooling (LawinASPP)\n[42], which queries a larger area of the feature map to\nproduce multi-scale contextual information. The decoder\ncan be combined with any hierarchical Vision Transformer\nencoder and when combined with the SegFormer or Swin\nTransformer encoder, the Lawin Transformer shows im-\nVOLUME , 3\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nproved accuracy and lower computational cost compared to\nthe original SegFormer or achieves a mIoU of 84.4 on the\nCityscapes dataset.\nB. Relevant state-of-the-art Vision Transformer models\nfor monocular depth estimation\nAdaBins [10] is a monocular depth estimation architecture\nintroduced in November 2020 by Bhat et al. It uses a CNN\nencoder-decoder design with a Transformer-based building\nblock named AdaBins to achieve good results. Depth es-\ntimation is treated as a classification task where the depth\nrange is split into adaptive bins of varying size, determined\nusing a mini-ViT architecture. The final prediction uses\na linear combination of the bin centers to avoid sharp\ndepth discontinuities. The AdaBins architecture achieved an\nabsolute relative error of 0.058 on the KITTI dataset.\nIn March 2021, Ranftl et al. proposed the Dense Prediction\nTransformer (DPT) [31], a novel architecture for the tasks\nof monocular depth estimation and semantic segmentation.\nDPT adapts a traditional encoder-decoder design, replacing\nthe CNN backbone with the ViT architecture [7]. The ViT\nbackbone provides a global receptive field and preserves\nthe initial feature map resolution to capture finer details.\nDPT is pre-trained on a massive monocular depth estimation\ndataset and fine-tuned on the KITTI dataset to achieve an\nabsolute relative error of 0.062. When trained for semantic\nsegmentation, the model achieves a mIoU of 49.02.\nKim et al. introduced GLPDepth [18], a Transformer-\nbased architecture and training strategy for monocular depth\nestimation that considers both the global and local context\nof the image. It uses SegFormer encoder [40] to capture\nglobal dependencies and a lightweight decoder with skip\nconnections to integrate local information. A new depth-\nspecific augmentation technique is also introduced to im-\nprove performance, achieving an absolute relative error of\n0.057 on the KITTI dataset.\nThe DepthFormer architecture [20] for monocular depth\nestimation was proposed in March 2022 by Li et al. The\nauthors stressed the importance of capturing both global and\nlocal information from an image and achieved this by using\ntwo separate encoder branches - one using the Swin Trans-\nformer [27] to model long-range correlations and the other\nusing convolutions for local information. The information\nfrom both branches is combined using a novel Hierarchical\nAggregation and Heterogeneous Interaction (HAHI) module.\nThe model achieved an absolute relative error of 0.052 on\nthe KITTI dataset, which is a state-of-the-art result.\nIn April 2022, Li et al. introduced BinsFormer [21],\na new architecture for monocular depth estimation. It is\nbased on AdaBins with multiple modifications, including\na Transformer decoder to enhance adaptive bin generation\nand a multi-scale design. During training, an auxiliary scene\nclassification task is used to improve the model performance.\nBinsFormer achieves state-of-the-art performance with an\nabsolute relative error of 0.052 on the KITTI dataset.\nC. Relevant state-of-the-art on Multi-task Learning\nMulti-task Learning involves training shared parameters for\nmultiple tasks to improve efficiency and accuracy by mining\nlatent information. Prominent models in this field include\nMask R-CNN [13], which combines Faster R-CNN [32] for\ninstance segmentation and object detection. Eigen et al. [8]\naddress depth prediction, surface normal estimation, and\nsemantic labeling, while MultiNet [34] handles classification,\ndetection, and semantic segmentation in a single model. In\ntheir study [43], the authors introduced a model that employs\na multi-task learning approach to concurrently predict both\nthe steering angle and speed command. YOLOP [39] uses\nCSPDarknet as its base and specializes in object detection,\ndrivable area segmentation, and lane detection. Standley et\nal. [33] and Christopher et al. [26] have improved multi-\ntask training by grouping related tasks together rather than\ntraining all tasks simultaneously. In the research conducted\nby Liang and their colleagues [23] they devised a multi-\ntask model capable of handling object detection, semantic\nsegmentation, and drivable area segmentation concurrently.\nIn a separate investigation by the same team [22], they\nintroduced the VE-Prompt framework. VE-Prompt utilizes\nvisual exemplars to offer task-specific visual cues, effectively\nguiding the model in learning various tasks including object\ndetection, semantic segmentation, instance segmentation, and\nlane line prediction. Furthermore, in a different study out-\nlined in [41], Xu and their collaborators presented a multitask\nlearning framework based on Transformers. This framework\nenables the simultaneous execution of segmentation, depth\nestimation, and saliency detection tasks within a single\nmodel.\nIn their study [30], Mousavian et al. presented an approach\nfor tackling both semantic segmentation and depth estimation\ntasks using deep convolutional neural networks (CNNs).\nTheir investigation centered on the feasibility of training\ndistinct components of the model for each task, followed\nby fine-tuning the entire integrated model to simultaneously\nenhance performance in both tasks through a single loss\nfunction. Additionally, the researchers enhanced their deep\nCNN by integrating a fully connected conditional random\nfield (CRF) to boost the overall model’s performance. The\nmodel’s evaluation encompassed both the training and eval-\nuation phases, conducted on the NYUDepth V2 dataset.\nIn another paper called ”Collaborative Deconvolutional\nNeural Network (C-DCNN)” [25], the authors introduced\nan approach that jointly addresses semantic segmentation\nand depth estimation as pixel-wise labeling problems. The\ntraining process involves three stages: firstly, pretraining\ntwo separate hierarchical supervised DCNNs for each task;\nsecondly, integrating these networks through a pointwise\nbilinear operation to allow simultaneous learning of both\ntasks and finetuning; and finally, applying a fully con-\nnected Conditional Random Field (CRF) to further improve\nsemantic segmentation performance using predicted depth\n4 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nand semantic labels. This model was also evaluated on the\nNYUDepth V2 dataset.\nIn both of the methods referred to in citations [25],\n[30], CNN-based architectures were employed. In contrast,\nour approach introduces a hybrid encoding and decoding\nframework based on Vision transformer variants SegFormer\n[40] and GLPDepth [18]. Unlike the methods mentioned\nin [25], [30], our proposed method does not require separate\npretraining, finetuning, and CRF steps. Our method is a\nstraightforward single-stage approach that takes an image\nas input and simultaneously performs the joint segmentation\nand depth estimation tasks in a single forward pass. While\nthese previous methods reported their performance solely on\nthe NYUDepth V2 dataset, our work presents results across\nthree publicly available datasets: Cityscapes [5], KITTI-360\n[24], and Apollo Synthetic Datasets [15], as well as on\nour custom NAPLab dataset. We validated the multi-task\nlearning capabilities of our vision transformer model on our\nproprietary dataset, demonstrating strong performance during\nevaluation on previously unseen NAP Lab data, despite being\ntrained on a different dataset. Additionally, we investigated\nthe utility of synthetic data for pre-training and observed its\neffective impact on increasing model accuracy when real-\nworld data is limited.\nIII. Methodology\nThis section presents the selection of models for the exper-\niments in the paper and provides a detailed description of\nthe chosen models and the proposed Multitask Model. This\ninformation is important for understanding the results and\nvalidity of the conclusions.\nA. Selection of Models\nThe objective of this paper is to create a multitask Vision\nTransformer for segmentation and depth estimation by in-\ntegrating two existing models. This section examines each\nmodel and provides a justification for its selection or non-\nselection. The accuracy and efficiency of the models were\ntaken into account during the decision-making process.\n1) Selection of Semantic Segmentation Model\nThe Segmentation Transformer (SETR) was an early suc-\ncessful attempt to use Transformers for semantic segmenta-\ntion and influenced other Transformer-based models. How-\never, it is no longer considered the best option due to\nadvancements in computer vision and improved models with\nhigher accuracy and efficiency.\nMaskFormer and Mask2Former are intriguing models for\nthis study due to their ability to carry out multiple seg-\nmentation tasks, and the innovative approach of using mask\nclassification to solve semantic segmentation. Mask2Former,\nwith its multiple improvements over MaskFormer, is consid-\nered the most promising option. However, its heavy decoder\nhinders its ability to perform real-time segmentation, leading\nto its non-selection for the experiments.\nSeMask is a semantic segmentation model that improves\nperformance by incorporating a small, task-specific layer\ninto the Transformer encoder. Although efficient, the mod-\nifications are specific to semantic segmentation and may\nnot transfer to other dense prediction tasks like monocular\ndepth estimation. Therefore, SeMask will not be used in the\nexperiments aimed at designing a multitask model with a\nshared encoder.\nThe Lawin Transformer was planned to be used in the\nexperiments for semantic segmentation as it promised to\nincrease accuracy and lower computational cost through its\nefficient decoder design. However, the results from integra-\ntion with the official SegFormer code did not match the\nresults reported in the paper. Hence, it was decided not to\nfurther explore the Lawin Transformer in this study.\nThe SegFormer architecture features an efficient self-\nattention calculation and a lightweight all-MLP decoder,\nmaking it suitable for real-time applications. Additionally,\nit offers 6 different backbone sizes, allowing for a balanced\ntrade-off between speed and accuracy. Furthermore, its ro-\nbustness on corrupted data makes it a strong candidate for\nsafety-critical tasks like autonomous driving. These advan-\ntages make SegFormer the chosen model for segmentation\nin this study.\n2) SegFormer\n[hbt!] The SegFormer is a semantic segmentation model\nthat uses an encoder-decoder design. The encoder is a\nhierarchical Vision Transformer inspired by ViT [7] with\nmodifications for improved performance, while the decoder\nconsists of MLP layers only. An overview of the architecture\ncan be found in Figure 1.\nFIGURE 1. SegFormer Architecture (Image source: Xie et al. [40]).\n1) Transformer Encoder:The SegFormer encoder takes\nan input image of size H × W × 3 and splits it\ninto patches of size 4 × 4. These patches are then\nprocessed through four Transformer blocks to create\nhierarchical feature maps. The Transformer blocks are\nimproved versions of those used in ViT, with a reduced\ncomplexity of the multi-head self-attention operation\nVOLUME , 5\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nusing the sequence reduction process proposed by\nWang et al [37] with a reduction ratioR. The positional\nencoding used in ViT is replaced by a 3 × 3 convolu-\ntion and an MLP, providing the model with positional\ninformation and better performance for images with\nvarying resolutions during inference. At the end of\neach Transformer block, an overlapped patch merging\nprocess is applied to reduce the spatial resolution and\nproduce hierarchical feature maps. The final feature\nmaps have a resolution of H\n2i+1 × W\n2i+1 × Ci, where\ni refers to the current Transformer block. The feature\nmaps are then sent to the decoder to generate the pre-\ndicted segmentation mask. The SegFormer architecture\nhas a total of 6 Transformer encoders, B0 to B5, with\ndifferent sizes and the same design. B0 is the smallest\nmodel and suited for real-time applications, while B5\nis the largest model with the highest accuracy.\n2) MLP Decoder: The Segformer decoder is a simple\nand lightweight design, consisting only of MLP layers.\nDespite its simplicity, the decoder still yields high-\nquality predictions, thanks to the large effective re-\nceptive field provided by the hierarchical Transformer\nencoder which helps capture the global context of the\nimage. The decoder follows these 4 steps to make a\nprediction:\na) Each feature map Fi is transformed using an\nMLP layer, converting the individual channel\ndimension Ci to a common channel dimension\nC.\nb) The feature maps are upsampled to H\n4 × W\n4 and\nconcatenated, resulting in a feature set F with a\nchannel dimension of 4C.\nc) F is then fused using another MLP layer, reduc-\ning the channel dimension from 4C to C.\nd) The final segmentation mask M is generated by\npassing the fused features through another MLP\nlayer, resulting in a resolution of H\n4 × W\n4 ×Ncls,\nwhere Ncls is the number of classes.\n3) Selection of Depth Estimation Model\nIn this study, the focus is on investigating the application of\nVision Transformers for dense prediction tasks. The depth\nestimation task is approached as a classification problem with\nAdaBins, which divides the depth range into adaptive bins.\nAlthough AdaBins achieved impressive results with a CNN\nbackbone and limited use of Transformer architecture, it was\nnot used in the experiments as the objective is to examine\nmodels with greater utilization of Transformers.\nThe DepthFormer model was not used in the experiments\neither, despite having two branches (Transformer-based and\nCNN-based) for gathering both global and local information\nfrom the image and producing remarkable results on the\nKITTI dataset. The lack of availability of official code at\nthe time of experimentation made incorporating the model\ndifficult and time-consuming.\nBinsFormer, one of the best models for monocular depth\nestimation on the KITTI dataset, could not be utilized in\nthe experiments due to the unavailability of the paper and\nofficial code at the time of experimentation.\nThe GLPDepth was chosen as the monocular depth esti-\nmation model for this study due to its lightweight decoder\nthat effectively merges global and local contexts to generate\naccurate depth predictions. Its lightweight design also makes\nit suitable for real-time use and its compatibility with Seg-\nFormer, the selected segmentation method, allows for easy\nintegration into a multitask model. The compatibility with\nSegFormer was the main reason for choosing GLPDepth over\nother methods for monocular depth estimation.\n4) GLPDepth\n[hbt!] This section explains the functioning of GLPDepth,\nthe selected depth estimation model. It merges the hierarchi-\ncal Vision Transformer encoder used in SegFormer with a\nunique lightweight decoder for monocular depth estimation.\nThe model’s performance is further improved by the intro-\nduction of Vertical Cut-Depth, a depth-specific augmentation\ntechnique. A visual representation of the architecture is\nprovided in Figure 2.\nFIGURE 2. The architecture of GLPDepth (Image source: Kim et al. [18].)\n1) Transformer Encoder: The GLPDepth architecture\nincorporates the SegFormer encoder by Xie et al. [40]\nto generate hierarchical feature maps from the input\nimage for the decoder. The original implementation\nuses the SegFormer B4 backbone for depth prediction,\nbut the experiments will test smaller backbone sizes to\nevaluate their impact on performance.\n2) Lightweight Decoder: The GLPDepth decoder takes\nthe final feature map F4\nE from the encoder and pro-\ncesses it through a channel reduction and bilinear\nupsampling step, resulting in a feature map of size\n1\n16 H × 1\n16 W × NC. The feature map then passes\nthrough multiple consecutive layers that include Se-\nlective Feature Fusion (SFF) modules and bilinear\nupsampling. The SFF module combines global and\nlocal features from the current decoder feature map\nand the encoder feature map, creating rich feature\nmaps. After multiple SFF layers and upsampling, the\n6 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\ndecoder feature map is transformed to H × W × NC.\nTo produce the final depth prediction, the feature map\nis sent through two convolutional layers and a sigmoid\nfunction, producing a depth map that is multiplied\nby the maximum depth value of the dataset to obtain\nthe predicted distance in meters. The SFF module is\ndepicted in Figure 3.\n3) Vertical CutDepth:The GLPDepth model introduces\na new augmentation method named Vertical CutDepth,\nwhich is specifically designed for depth estimation\nand inspired by CutDepth. The method improves the\ndiversity of the dataset by placing a random crop of the\nground truth depth map into the RGB image. Unlike\nCutDepth, Vertical CutDepth does not crop the depth\nmap vertically to better preserve the important vertical\ninformation. This method is specific to depth estima-\ntion and its effects on other dense prediction tasks are\nunknown. However, it was not used in the experiments\nfor this study as the goal was to design an architecture\nfor both depth estimation and segmentation.\nFIGURE 3. The GLPDepth Selective Feature Fusion (SFF) module (Image\nsource: Kim et al. [18].)\nIn short, the proposed method uses SegFormer for se-\nmantic segmentation and GLPDepth for monocular depth\nestimation. Both models use the same Transformer backbone\nand have lightweight decoders suited for real-time use. The\narchitecture of each model is explained already in sections\n2 and 4, and their combination into a multitask model is\ndetailed in the next section.\n5) Proposed Multitask Model\nThe multitask model performs monocular depth estimation\nand semantic segmentation, using SegFormer and GLP-\nDepth. The model can generate predictions for both tasks\nwith a single forward pass, resulting in improved accuracy\nand efficiency.\n1) Architecture: The multitask architecture leverages the\nSegFormer encoder, to obtain hierarchical features\nfrom the input image. These feature maps are then\ndirected to two individual decoders, one for semantic\nsegmentation and the other for monocular depth esti-\nmation. These decoders generate the ultimate predic-\ntions for each task. A visualization of the proposed\narchitecture can be seen in Figure 4, with further\nelaboration on the segmentation and depth decoders\nfound in 2 and 4 sections, respectively.\n2) Loss Function:The model is trained to perform both\nsemantic segmentation and monocular depth estima-\ntion simultaneously by employing two individual loss\nfunctions, specific to each task. The cross-entropy\nloss, a commonly used method, is utilized for the\nsegmentation task.\nLCE = −\nnX\ni\nyi log ( ˆyi) (1)\nFor the segmentation task, the cross entropy loss uses\nthe ground truth label yi and the softmax probability\nfor the i-th class ˆyi to calculate the loss. For the\ndepth estimation task, the scale-invariant log scale loss,\npreviously used in GLPDepth, is employed.\nLSI = 1\nn\nnX\ni\nd2\ni − 1\n2n2\n nX\ni\nd2\ni\n!\n(2)\nThe scale-invariant log scale loss calculates the differ-\nence between the log of the ground truth depth value\nyi and the log of the predicted depth value ˆyi for each\npixel (di = logyi−log ˆyi). During the training process,\nthe two task-specific loss functions are combined into\na single loss function to calculate the total loss.\nL = LCE + LSI (3)\nIV. Experiments and Results\nThis section provides an overview of the software, hardware,\ntraining configurations used for the proposed method, and\nother methods used for comparison. It covers the various\nexperiments conducted and the datasets used in the study.\nThe section also presents an analysis of the experimental\nresults.\nA. Datasets\nIn our study, we conducted experiments using four datasets:\nCityscapes [5], KITTI-360 [24], Appolo Synthetic [15] and\nNAPLab dataset (Our own data). The specifics of each\ndataset, such as the selection of training and testing data,\nthe number of classes, and the class types for both RGB and\ndepth images of each dataset are discussed in the subsequent\nsubsections. It is to be noted that due to the limited annotated\nframes in the NAPLab dataset, the models cannot be trained\nor fine-tuned on it. However, we used this data to evaluate the\nperformance of the models trained on the above-mentioned\ndata sets.\n1) Cityscapes\nThe Cityscapes dataset [5] includes 5,000 frames with fine\nannotations. Out of these, one can use 2,975 frames for\ntraining, 500 for validation, and 1,525 for testing. However,\nthe annotations for the test set are not available for use\nVOLUME , 7\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nFIGURE 4. The proposed multitask architecture.\nduring experimentation, leaving 3,475 frames for training\nand validation. Each frame comes with RGB images captured\nby the left front-facing camera, annotations for semantic\nsegmentation, pre-computed disparity maps, and camera pa-\nrameters (both intrinsic and extrinsic). Blurred RGB images\nare also included for visualization purposes.\nThe Cityscapes dataset contains high-resolution images\n(2048x1024), but it is not possible to perform real-time\ninference with the selected models using these images. To\nmake real-time inference feasible, the images can be resized\nto a smaller resolution (1024x512), which is half of the\noriginal size.\nTo use disparity maps for training, they must be converted\nto depth maps through some calculations. The first step is to\ncalculate the disparity values from the raw images using an\nequation:\nd = float(p) − 1.0\n256.0 (4)\nwhere the calculation of disparity values d from raw pixel\nvalues p is done. The depth values are calculated with\nsome intrinsic and extrinsic camera parameters using another\nequation as:\nD = B × fx\nd (5)\nwhere the disparity value (d), baseline (B), and focal length\n(fx) are used to calculate the depth value (D) in meters\nthrough the specified equation. The resulting depth maps\nrange from 0 to approximately 470 meters, however, it\nwas noticed that the high depth values were noisy and\ninaccurate, as shown in Figure 5a. Despite testing the\ndepth maps for training, the evaluation metrics failed to\nconverge as expected. As a result, it was decided to clip\nthe depth maps at 100 meters, meaning values greater than\n100 meters would be set to 100. This decision was made as\nmost of the significant objects in the image fall within this\nrange, as seen in the clipped depth map in Figure 5b. The\nCityscapes dataset is labeled with 30 semantic classes, but\nonly 19 of them are typically used for training and validation\nbecause the other classes are rare. These 19 commonly used\nclasses with their corresponding colors are listed in Table\n1. To train using the segmentation data, images with values\ncorresponding to the train IDs of the classes are generated\nusing the official Cityscapes scripts.\nTABLE 1. Cityscapes classes and color palette\nTrain ID Name Color\n0 Road 000000\n1 Sidewalk 000000\n2 Building 000000\n3 Wall 000000\n4 Fence 000000\n5 Pole 000000\n6 Traffic Light 000000\n7 Traffic Sign 000000\n8 Vegetation 000000\n9 Terrain 000000\n10 Sky 000000\n11 Person 000000\n12 Rider 000000\n13 Car 000000\n14 Truck 000000\n15 Bus 000000\n16 Train 000000\n17 Motorcycle 000000\n18 Bicycle 000000\n8 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFIGURE 5. Cityscapes depth map before and after clipping at 100m\n2) KITTI-360\nThe KITTI-360 dataset [24] includes raw perspective images,\n2D semantic labels, raw LiDAR scans, vehicle poses, and in-\ntrinsic and extrinsic camera parameters. The dataset includes\n61,280 annotated images that are divided into a train set and\na validation set. The train set has 49,004 frames, while the\nvalidation set has 12,276 frames.\nThe KITTI-360 dataset has a resolution of 1408x376,\nwhich is low enough to allow real-time inference but the\nimage height of 376 is not divisible by 32, a requirement of\nthe chosen models. To resolve this, 24 pixels are cropped\nfrom the top of the image resulting in a resolution of\n1408x352. During validation, a different crop will be used\nas proposed by Garg et al. [11].\nThe raw LiDAR point clouds must be transformed into 2D\ndepth maps for training purposes. The transformation is done\nusing the official KITTI-360 scripts and involves applying a\ntransformation matrix to all the points to convert the LiDAR\ncoordinate frame to the camera coordinate frame, which is\ncalculated using an equation.\nTL→k = T0→k × TL→0 (6)\nhere, TL→0 transformation matrix represents the process\nof transforming 3D points from the LiDAR coordinate frame\nto the left perspective camera. The T0→k matrix, which\ntransforms the left perspective camera to other cameras, is\nset to the identity matrix as only the left perspective camera\nwill be used for the experiments. The projected points in the\ncamera coordinate frame are then transformed into 2D depth\nmaps through the use of intrinsic camera parameters. These\nmaps have a depth range of 0 to 80 meters.\nThe semantic classes for KITTI-360 align with those\nutilized for Cityscapes, as listed in Table 1. As a result,\nKITTI-360 will also use the same 19 classes for training and\nevaluation. Images with train IDs for the semantic classes\nmust be generated to train the models.\n3) Apollo Synthetic Dataset\nThe dataset [15] consists of 273,000 frames in total. To\nimprove the balance between classes and scenes, a subset\nof the dataset was selected for use in the experiments.\nThe dataset, originally consisting of 273,000 frames, was\nreduced to 45,235 frames after only half of the available\nhighway sequences from the dataset were used due to data\nimbalance, and excluding 125 frames due to missing depth\nannotations. To create a balanced dataset, the frames were\nsplit into a training set and a validation set based on the\nvirtual scene level, with the Road Loop with Intersections\nscene used for validation and the remaining scenes for\ntraining. The final training set has 40,195 frames and the\nvalidation set has 5,040 frames.\nThe images in the Apollo Synthetic Dataset have a resolu-\ntion of 1920x1080, but to perform real-time inferences, the\nresolution needs to be reduced. A common approach is to\ndecrease the resolution to 960x540, which is half the original\nsize. However, due to the design of the chosen models, the\nimage size must be divisible by 32, so the final resolution\nselected is 960x544.\nThe depth values are decoded from the depth images using\nthe following equation:\nD =\n\u0012\nR + G\n255.0\n\u0013\n× 655.36 (7)\nwhere, the depth value in meters, D, is calculated by using\nthe normalized float values of the red and green channels,\nR and G, of a pixel in the image. This calculation produces\ndepth values ranging from 0 to 655.35 meters with a preci-\nsion of 1 cm. Despite the high accuracy of the depth maps,\nthe depth range is clipped to 200 meters, as is done with the\nCityscapes and KITTI-360 datasets. This larger max depth\nvalue is made possible due to the accuracy of the depth maps\nin the Apollo Synthetic Dataset.\nThe Apollo Synthetic Dataset has 24 classes for semantic\nsegmentation that were modified to match the classes in\nCityscapes, resulting in 14 classes that will be used in the\nexperiments. These classes are shown in Table 2 and have a\ncolor palette similar to Cityscapes.\n4) NAPLab\nThe NAPLab dataset comprises 10 frames that feature se-\nmantic segmentation annotations exclusively. The resolution\nVOLUME , 9\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nTABLE 2. Apollo Synthetic Dataset classes and color palette\nTrain ID Class Name Original Classes Color\n0 Road Road 000000\n1 Sidewalk Sidewalk 000000\n2 Building Building 000000\n3 Pole Pole, Street Light 000000\n4 Traffic Light Traffic Light 000000\n5 Traffic Sign Traffic Sign 000000\n6 Vegetation Vegetation 000000\n7 Terrain Terrain 000000\n8 Person Pedestrian 000000\n9 Car Coupe, SUV , Hatchback, Van 000000\n10 Truck Truck, Pickup Truck 000000\n11 Bus Bus 000000\n12 Motorcycle Motorcyclist 000000\n13 Bicycle Cyclist 000000\nof the images is 1920 × 1080, however, to facilitate real-\ntime inference, they have been resized to 960 × 544. The\nsemantic segmentation class definitions in the dataset match\nthose of Cityscapes (Table 1).\nB. Metrics\nEvaluation of segmentation methods can be performed using\nvarious metrics. Pixel accuracy, which is calculated as the\npercentage of image pixels labeled correctly, is a simple\nmethod but can provide misleading results if the class\ndistribution in the image is unbalanced. A more reliable\nevaluation metric is the Intersection over Union (IoU) or\nJaccard index.\nThe recent research in monocular depth estimation uses\nmetrics introduced by Eigen et al [9]. This overview cov-\ners the commonly used evaluation metrics for comparing\npredicted and actual values. The absolute error shows the\ndifference between the two but does not take into account\nthe size of the actual value, leading to potential issues when\ncomparing values of varying magnitudes. To remedy this, the\nabsolute relative error divides the absolute error by the actual\nvalue to incorporate the magnitude. The absolute relative\nerror, squared relative error, and root mean squared error\nare often used to evaluate model performance. Threshold\naccuracy measures the percentage of image pixels that have\na maximum ratio between predicted and actual values below\na specified threshold, with common threshold values being\n1.25, 1.252, and 1.253.\nC. Hardware\nThe experiments in this paper required significant computa-\ntional resources and were carried out using two solutions:\nthe IDUN cluster and a virtual machine provided by NTNU.\nThe IDUN cluster at NTNU is a large computing cluster\nfor research purposes, equipped with A100, V100, and\nP100 GPUs, which were utilized for training the models.\nTraining multiple GPUs was necessary due to the high mem-\nory consumption of the models. The virtual machine from\nNAPLab with an A10 virtual GPU with 23 GB RAM and a\ngraphical user interface was used for ease of development,\nbenchmarking, inference, and visualization of results.\nD. Training setup\nThe multitask model is trained on three datasets selected\nspecifically for this purpose. The number of training epochs\nvaries depending on the size of each dataset: 300 epochs for\nCityscapes, 30 epochs for the Apollo Synthetic Dataset, and\n20 epochs for KITTI-360. During training, the Adam opti-\nmizer [19] is utilized with a learning rate of 1.0×10−4 and a\nbatch size of 12. Augmentations are performed using the Al-\nbumentations library [2], including techniques such as Hori-\nzontalFlip, RandomBrightnessContrast, RandomGamma and\nHueSaturationValue. The model’s initial weights are from\nImageNet [6], pre-trained for optimal performance.\nE. Results\nThe results of the experiments conducted for this study are\npresented in this section. The following experiments were\ncarried out to answer the RQs posed in section I:\n1) Multitask Training Comparison - The multitask train-\ning method was compared with individual task training\nfor segmentation and depth to examine its impact on\nmodel performance.\n2) Backbone Size Analysis - The multitask model was\ntrained with different backbone sizes to evaluate how\nthe choice of backbone affects performance.\n3) Synthetic Data Pre-training - The multitask model\nwas pre-trained on a synthetic dataset to improve its\naccuracy.\n4) Depth Prediction Validation - The accuracy of the\npredicted distances to objects in images was evaluated.\n5) Evaluation on NAPLab Data - The multitask model\ntrained on Cityscapes was tested on NAPLab data.\n1) Multitask Training Comparison\nThe aim of the experiment was to determine the effect of\nmultitask training on performance. The approach involved\ntraining the model for both semantic segmentation and\nmonocular depth estimation simultaneously. Three models\nwere trained: one for segmentation only, one for depth\n10 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nestimation only, and one for both tasks. The models were\nevaluated on the Cityscapes, KITTI-360, and Apollo Syn-\nthetic datasets using the SegFormer B2 backbone for all\nmodels.\n1) Training\nThe models were trained based on the instructions in\nsection ??. The training process of the models was\nmonitored using graphs for three datasets: Cityscapes,\nKITTI-360, and Apollo Synthetic Dataset which can\nbe found in Figures 6, 7, and 8 respectively. In all the\ndatasets, the segmentation loss began to increase after\na few training epochs, which is a commonly observed\nsign of overfitting. Despite this, the mean Intersection\nover Union (mIoU) continued to improve. On the other\nhand, the depth loss showed a smoother convergence\npattern compared to the segmentation loss throughout\nthe training process. Comparing the convergence of the\nindividual task models and the multitask model, it can\nbe seen that both models generally converge similarly\nand no significant differences are apparent from the\ntraining graphs.\n2) Qualitative analysis\nThe qualitative results for the models applied to\nCityscapes, KITTI-360, and Apollo Synthetic Dataset\nare presented in Figures 9, 10, and 11, respectively.\nThe multitask model produced similar results to the\nindividual task models, and only the predictions of\nthe multitask model are shown. The model performed\nwell in generating convincing predictions across all\ndatasets. However, the model faced challenges in seg-\nmenting the sky region in the Apollo Synthetic Dataset\nas it was not labeled in the ground truth annotations.\nThe model also struggled to generate accurate depth\npredictions for the top part of the KITTI-360 images as\nthe ground truth depth maps only covered the bottom\npart of the images.\n3) Quantitative analysis\nThe quantitative results are displayed in Table 3. The\nindividual task model performed similarly or better\nthan the multitask model in terms of absolute rela-\ntive error for depth, with only small differences. For\nsegmentation, the individual task model showed the\nbest performance on Cityscapes while the multitask\nmodel performed better on the other two datasets. The\ninference speed was comparable across all datasets,\nas the images had a similar number of pixels to\nprocess. Despite performing an additional task, the\nmultitask model had a similar inference speed to the\nsegmentation model.\n2) Backbone Size Analysis\nThe purpose of the experiment is to examine the impact\nof the size of the Transformer backbone on the model\nperformance. Three sizes of the SegFormer backbone, B0,\nB2, and B4, are used in combination with the multitask\nmodel and trained and evaluated on Cityscapes, KITTI-360,\nand Apollo Synthetic Dataset.\n1) Training\nThe experiment tested three different sizes of the\nSegFormer backbone (B0, B2, and B4) with the mul-\ntitask model on Cityscapes, KITTI-360, and Apollo\nSynthetic Dataset. The training graphs for each dataset\ncan be seen in Figure 12, 13, and 14. The segmentation\nloss for all backbone sizes started increasing after a\nfew training epochs, which is a sign of overfitting.\nHowever, the problem was less prominent for the\nB0 backbone, which had a slower convergence rate.\nThe depth loss converged better and improved for a\nlonger period of time. In terms of accuracy, the B4\nbackbone performed the best, B2 the second best, and\nB0 performed the worst.\n2) Qualitative analysis\nThe results of the segmentation and depth tasks are\nshown in Figures 15 and 16 respectively. The two\nlargest models in the experiment perform better in\nsemantic segmentation, showing better distinction be-\ntween the road and sidewalk and accurately seg-\nmenting small objects and thin structures. In-depth\nestimation, the largest models produce clearer depth\nmaps and detect small objects and thin structures more\neffectively. The results are shown for the Cityscapes,\nKITTI-360, and Apollo Synthetic Datasets and com-\npare the original images with the results from Model\nB0, B2, and B4.\n3) Quantitative analysis\nAs depicted in Table 4, the model featuring the biggest\nbackbone typically achieves the best accuracy for both\ntasks, with the exception being depth estimation on\nthe Apollo Synthetic Dataset, where Model B2 out-\nperforms Model B4. In terms of computational speed,\nModel B0 stands out with its impressive 56-61 FPS\nperformance, which is well within the realm of real-\ntime processing. Model B2 also operates close to real-\ntime with a frame rate of 18-19 FPS, while the more\nrobust Model B4 operates at 11-12 FPS.\n3) Synthetic Data Pre-training\nThis section explores the impact of pre-training on a large\nsynthetic dataset. Specifically, we pre-trained a multitask\nmodel that uses a SegFormer B2 backbone on the Apollo\nSynthetic Dataset and then fine-tuned it on Cityscapes. We\ncompared the model’s performance to that of one trained\nexclusively on Cityscapes.\n1) Training\nIn accordance with section ??, the models were\ntrained, and Figure 17 shows their training graphs. Pre-\nVOLUME , 11\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nTABLE 3. Quantitative comparison of Multitask Training\nCityscapes\nModel FPS AbsRel SqRel RMSE RMSElog δ <1.25 δ <1.252 δ <1.253 mIoU mAcc aAcc\nDepth only 30 0.096 1.213 6.289 0.162 0.904 0.977 0.991 - - -\nSeg only 21 - - - - - - - 76.53 83.98 95.87\nMultitask 18 0.096 1.266 6.317 0.162 0.905 0.977 0.992 75.93 83.54 95.80\nKITTI-360\nDepth only 32 0.083 0.372 2.905 0.147 0.912 0.972 0.989 - - -\nSeg only 21 - - - - - - - 65.07 73.33 93.39\nMultitask 19 0.087 0.386 2.938 0.150 0.907 0.972 0.989 66.52 74.23 93.37\nApollo Synthetic Dataset\nDepth only 30 0.173 6.993 18.550 0.275 0.792 0.902 0.954 - - -\nSeg only 21 - - - - - - - 77.98 85.23 94.55\nMultitask 18 0.181 6.047 17.190 0.277 0.783 0.897 0.954 78.09 84.59 94.88\nFIGURE 6. Comparison of Multitask Training on Cityscapes dataset. The graphs illustrate the validation loss and evaluation metrics during training,\nwhere orange represents multitask, blue represents depth, and green represents segmentation.\nTABLE 4. Backbone Size Analysis: quantitative results\nCityscapes\nBackbone FPS AbsRel SqRel RMSE RMSElog δ <1.25 δ <1.252 δ <1.253 mIoU mAcc aAcc\nB0 57 0.109 1.435 6.999 0.177 0.876 0.969 0.990 69.78 78.34 94.85\nB2 18 0.096 1.266 6.317 0.162 0.905 0.977 0.992 75.93 83.54 95.80\nB4 12 0.093 1.263 6.251 0.160 0.909 0.978 0.992 76.94 84.83 95.94\nKITTI-360\nB0 61 0.097 0.440 3.172 0.163 0.889 0.967 0.987 60.44 69.27 92.69\nB2 19 0.087 0.386 2.938 0.150 0.907 0.972 0.989 66.52 74.23 93.37\nB4 12 0.084 0.377 2.925 0.149 0.909 0.972 0.989 68.08 75.75 93.58\nApollo Synthetic Dataset\nB0 56 0.208 7.096 17.750 0.298 0.761 0.881 0.945 68.76 77.16 92.93\nB2 18 0.181 6.047 17.190 0.277 0.783 0.897 0.954 78.09 84.59 94.88\nB4 11 0.190 6.960 18.390 0.278 0.775 0.896 0.955 79.37 85.80 94.69\ntraining on the synthetic dataset improved both the\nloss and absolute relative error for depth estimation,\nbut there was no observed improvement for semantic\nsegmentation. Unfortunately, the model pre-trained on\n12 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFIGURE 7. Comparison of Multitask Training on KITTI-360 dataset. The graphs illustrate the validation loss and evaluation metrics during training,\nwhere orange represents multitask, blue represents depth, and green represents segmentation.\nFIGURE 8. Comparison of Multitask Training on Apollo Synthetic Dataset. The graphs illustrate the validation loss and evaluation metrics during\ntraining, where orange represents multitask, blue represents depth, and green represents segmentation.\nsynthetic data had to be stopped after 150 epochs, but\nthere is potential for better performance with a longer\ntraining period. However, given that the last phase of\ntraining typically sees only slight improvements, the\ndifference in performance would likely be negligible.\n2) Results\nTable 5 presents the quantitative results, showing that\nthe model pre-trained on synthetic data outperforms\nthe original model in all-depth evaluation metrics.\nHowever, there is no observed improvement in se-\nmantic segmentation with pre-training on the synthetic\ndataset. The models with and without synthetic dataset\npre-training had no significant differences in their\npredictions, so there are no qualitative results provided\nfor this experiment.\n4) Depth Prediction Validation\nThis section presents the validation of depth prediction,\nwhich assesses the model’s capability to estimate the dis-\ntance between objects in an image. Six images, three from\nCityscapes and three from KITTI-360 are used, as shown in\nVOLUME , 13\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nFIGURE 9. Qualitative analysis of Multitask Training on Cityscapes dataset. The results shown were generated using the B2 multitask model and are\npresented in three rows. The first row displays the original image, the second row shows the predicted segmentation mask, and the third row displays\nthe predicted depth map.\nFIGURE 10. Qualitative analysis of Multitask Training on KITTI-360 Dataset. The results shown were generated using the B2 multitask model and are\npresented in three rows. The first row displays the original image, the second row shows the predicted segmentation mask, and the third row displays\nthe predicted depth map.\nTABLE 5. Quantitative results of pre-training on synthetic data.\nCityscapes\nPre-trained AbsRel SqRel RMSE RMSElog δ <1.25 δ <1.252 δ <1.253 mIoU mAcc aAcc\nNo 0.096 1.266 6.317 0.162 0.905 0.977 0.992 75.93 83.54 95.80\nYes 0.094 1.185 6.227 0.159 0.908 0.978 0.992 75.48 83.64 95.83\nFigure 18. The multitask model produces a depth prediction\nfor each image, and the predicted depth of an object, marked\nwith a red circle, is compared to the ground truth depth using\nthe average of multiple depth values in the same region.\nTwo different versions of the multitask model are used, one\ntrained on Cityscapes and one on KITTI-360, enabling an\nassessment of how accurately the model performs on the\ntraining dataset as well as on new and unseen data.\nTable 6 displays the results of the depth validation. The\nresults demonstrate that the model’s predicted depth is the\nmost precise for the dataset it was trained on, with a\nprediction error of less than a meter. However, when the\nmodel is tested on an unfamiliar dataset, the prediction\nerror is significant, and it seems that the Cityscapes model\noverestimates the distance on the KITTI-360 dataset, while\nthe KITTI-360 model underestimates the distance on the\nCityscapes dataset.\n5) Evaluation on NAPLab Data\nIn this section, the performance of the multitask model on the\nNAPLab dataset is evaluated. Due to the limited annotated\nframes in the dataset, the model cannot be trained or fine-\ntuned on it. Instead, an evaluation is carried out using a\nmodel trained on the Cityscapes dataset. Experimental setup\nin section 2 is followed, where three different model sizes\n(B0, B2, and B4) are tested.\n14 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFIGURE 11. Qualitative analysis of Multitask Training on Apollo Synthetic Dataset. The results shown were generated using the B2 multitask model and\nare presented in three rows. The first row displays the original image, the second row shows the predicted segmentation mask, and the third row\ndisplays the predicted depth map.\nFIGURE 12. Comparison of different Backbone sizes on Cityscapes dataset. The graphs depict the validation loss and evaluation metrics while the\nmodels were being trained. The orange color represents the B0 backbone, blue represents the B2 backbone, and green represents the B4 backbone.\nTable 7 displays the quantitative results, as no ground\ntruth depth data is available, only evaluation metrics for\nsemantic segmentation are presented. The highest mIoU\nscore is achieved by the largest model, while the smallest\nmodel achieves the lowest score, similar to the results of\nsection 2. Figure 19 provides qualitative results where the\npredicted segmentation masks look similar to the ground\ntruth masks.\nV. Discussion\nThe section examines the implications of the experimental\noutcomes on the suitability of the model for use in au-\ntonomous driving. Additionally, it considers the limitations\nof this study and endeavors to address the research questions\nin section I.\nVOLUME , 15\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nFIGURE 13. Comparison of different Backbone sizes on KITTI-360 dataset. The graphs depict the validation loss and evaluation metrics while the\nmodels are being trained. The orange color represents the B0 backbone, blue represents the B2 backbone, and green represents the B4 backbone.\nFIGURE 14. Comparison of different Backbone sizes on Apollo Synthetic Dataset. The graphs depict the validation loss and evaluation metrics while\nthe models are being trained. The orange color represents the B0 backbone, blue represents the B2 backbone, and green represents the B4 backbone.\n1) The potential applications in the field of autonomous\ndriving\nReal-time performance is an important requirement for deep\nlearning architectures in autonomous vehicles, with a frame\nrate of approximately 30 FPS considered real-time. In section\n2, different backbone sizes were tested and the B0 model\nachieved a frame rate of about 60 FPS, while the B2\nmodel achieved roughly 20 FPS. However, experiments were\nconducted on powerful GPUs that may not be available in an\nautonomous vehicle, and additional time is required for data\nloading, post-processing, and decision-making. TensorRT\ncan optimize the model for fast inference and may help\nincrease inference speed. In addition to inference speed,\naccuracy is also important in autonomous driving, and the\nlargest models (B2 and B4) outperformed B0 in section 2. To\nincrease performance, the resolution of input images can be\nincreased to capture finer details, but this may decrease the\nframe rate. Therefore, different image sizes should be tested\nto find a balance between accuracy and inference speed.\n16 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFIGURE 15. The results of the comparison of different Backbone sizes for the segmentation task can be seen in the first, second and third columns of\nthe images, which represent Cityscapes, KITTI-360, and Apollo Synthetic Dataset, respectively. The first row shows the original image, while the\nsecond, third, and fourth rows show the results obtained using the B0, B2, and B4 backbones, respectively. The images have been cropped for better\nvisual clarity.\nTABLE 6. The results of the depth validation are presented, where the\nestimated distance to individual objects in images from Cityscapes and\nKITTI-360 datasets are evaluated, as depicted in Figure 18. Two models are\nused for evaluation, one trained on Cityscapes and the other trained on\nKITTI-360. The distance values are expressed in meters.\nCityscapes\nTrained on Image Ground truth Prediction Error\nCityscapes\n1 9.26 9.50 0.24\n2 12.31 11.67 0.64\n3 35.09 36.40 1.31\nKITTI-360\n1 9.26 6.47 2.79\n2 12.31 6.64 5.67\n3 35.09 18.45 16.64\nKITTI-360\nKITTI-360\n1 4.83 4.31 0.52\n2 17.13 16.57 0.57\n3 34.14 36.75 2.61\nCityscapes\n1 4.83 9.31 4.48\n2 17.13 26.90 9.76\n3 34.14 67.89 33.76\nA. The limitations of the study\nIn this section, the limitations of the study are discussed, as\nwell as some possible solutions to address them.\nTABLE 7. Quantitative results on NAPLab data\nBackbone FPS mIoU mAcc aAcc\nB0 56 50.49 66.49 92.94\nB2 18 55.36 74.12 94.15\nB4 11 59.97 73.73 94.36\n1) Early Overfitting\nSections 1 and 2 showed that the segmentation loss in the\nmodel increased early in the training process, indicating\noverfitting, but the model’s accuracy continued to improve.\nThis unexpected outcome may be due to the cross-entropy\nloss used for segmentation, which is calculated differently\nfrom accuracy. The lack of augmentation techniques used\nduring training could be another reason for overfitting, and\nalternative techniques that work for both depth estimation\nand semantic segmentation could be explored. The hyperpa-\nrameters were not extensively tested during training, and the\nlearning rate was identified as a crucial parameter that could\nbe optimized through the use of a learning rate scheduler or\nmanual decrease of the learning rate after the initial training\nepochs. The use of individual learning rates for each task-\nspecific decoder could also help achieve better accuracy for\nboth depth and segmentation.\nVOLUME , 17\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nFIGURE 16. This experiment evaluates the qualitative results of the depth estimation task, which are shown through images cropped for better\nvisualization. The images are from the Cityscapes, KITTI-360, and Apollo Synthetic Dataset, with the original image in the first row, and the results from\nB0, B2, and B4 models in the subsequent rows.\nFIGURE 17. The graphs depict the validation loss and evaluation metrics of training with and without pre-training on synthetic data. The orange line\nrepresents training without pre-training, while the blue line represents training with pre-training.\n2) Few Available Datasets\nThe study focuses on using a multitask model for monoc-\nular depth estimation and semantic segmentation. Finding\ndatasets annotated for both tasks was challenging as most\npublicly available datasets focus on one task. Three datasets\n18 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nFIGURE 18. The validation of depth involves verifying the predicted distance to an object in each image (see Table 6 also for better understanding.).\nObjects selected for validation are highlighted with a red circle, with the Cityscapes dataset presented in the first row and the KITTI-360 dataset in the\nsecond row.\nFIGURE 19. The qualitative results of NAPLab data, where pedestrians and license plates are blurred for anonymity purposes. The first row displays the\noriginal image, followed by the ground truth segmentation mask in the second row, the predicted segmentation mask in the third row, and the predicted\ndepth map in the fourth row.\nwere chosen, all of which contain annotations for both tasks\nbut have their own issues. Cityscapes is small and its depth\nmaps are not as accurate as LiDAR depth maps, while\nKITTI-360 has a much larger dataset but sparse depth maps.\nThe lack of annotated real-world data led to the use of\nsynthetic data for the evaluation of the multitask model.\nThe chosen dataset, Apollo Synthetic Dataset, has a large\nnumber of annotated frames but cannot substitute real-world\ndata completely due to domain shift. Semi-supervised and\nself-supervised methods were not explored in the study, but\nthey could be interesting to look into as they require less\nannotated data. However, annotated data for both tasks would\nstill be necessary to evaluate the model’s performance.\nFor more theoretical background, additional experiments,\nand in-depth explanation of the proposed study, please refer\nto our work [28].\nVI. Conclusion and Future Directions\nThe study explored the use of Vision Transformers for dense\nprediction tasks in autonomous driving. The goal was to\ndevelop a multitask model that can perform monocular depth\nestimation and semantic segmentation simultaneously while\nbeing able to operate in real-time. A literature review was\nconducted to identify state-of-the-art Vision Transformers,\nand two models, SegFormer and GLP-Depth, were selected\nfor the proposed multitask architecture. The effectiveness\nof the proposed model was evaluated with three different\ndatasets and compared to individual task models. The results\ndemonstrated that the multitask model achieves comparable\naccuracy to the individual task models and significantly\nlowers the total inference time for both tasks. The choice of\nbackbone was found to be able to control the ratio between\ninference speed and accuracy. An experiment was conducted\nto investigate the impact of pre-training the model on a large\nVOLUME , 19\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nsynthetic dataset, which improved depth estimation accuracy\nsignificantly while maintaining segmentation accuracy. How-\never, the model struggled on new and unseen datasets when\nestimating depth accurately.\nThe potential research ideas in Vision Transformers can\nbe explored to further improve our work. One idea is\nto explore semi-supervised or self-supervised learning ap-\nproaches, which require less annotated data during training\nand can save time and money. Another idea is to investigate\nthe use of mixing multiple depth estimation datasets during\ntraining to improve the model’s performance across multiple\ndatasets without additional fine-tuning. This approach could\nforce the model to rely on more general features and not\nfeatures specific to a single dataset.\nAnother interesting direction to explore is multi-sensor\nfusion. As the technology continues to evolve, multi-sensor\nfusion [29], [38] is likely to play a central role in the\nadvancement of automated driving systems. Extensive hy-\nperparameter analysis for the enhanced performance of the\nproposed method can also be conducted. These ideas could\nbe interesting to investigate further in future research.\n20 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nDurga Prasad Bavirisetti is a researcher and\nalgorithm developer specializing in machine learn-\ning, deep learning, and computer vision. He earned\na Ph.D. in Signal and Image Processing from VIT\nin 2016 and completed postdoctoral research at\nShanghai Jiao Tong University. Bavirisetti also\nworked as an algorithm expert at the innovation\ncenter of the Alibaba Group of companies and was\na visiting researcher at UBC Okanagan and the\nUniversity of Warsaw. He is currently a researcher\nat NTNU, Norway, working on machine vision for\nautonomous driving and medical image computing.\nHerman Ryen Martinsen is a software engi-\nneer with experience at Capgemini and internships\nat Uninett AS and SINTEF Ocean. He holds a\nMaster’s degree in Data Technology, specializing\nin Databases and Search, and a Bachelor’s de-\ngree in Engineering, majoring in Data, from the\nNorwegian University of Science and Technology\n(NTNU).\nGabriel Hanssen Kiss is an Associate Profes-\nsor at the Computer Science Department, NTNU,\nTrondheim and senior engineer at the Operating\nRoom of the Future, St Olavs University Hospital.\nHe has a computer science engineering diploma\nfrom the Technical University of Cluj-Napoca,\nRomania, and a PhD in engineering from K.U.\nLeuven, Belgium. His main area of expertise\nis related to vision computing, medical image\nprocessing and visualization, digital twins, and\nultrasound technology. Special interests include\nextended reality, volumetric data visualization, image registration, and fusion\nfor ultrasound-related applications.\nFrank Lindsethis a professor at the Norwegian\nUniversity of Science and Technology (NTNU)\nin the Department of Computer Technology and\nInformatics of Faculty of Information Technol-\nogy and Electrical Engineering. He was also a\nSenior Research Scientist at SINTEF Medical\nTechnology, where he works in the field of Im-\nage Guided Interventions / Surgical Navigation.\nLindseth’s academic qualifications include a PhD\nin Computer Science from NTNU, an MSc in\nMathematical Science from NTNU, and a BSc in\nEngineering from BIH in Bergen, Norway. His areas of expertise include\nImage Guided Surgery, Navigation and Tracking technology, Medical image\nprocessing and analysis, Medical visualization, computer graphics, and\naugmented reality. He is also skilled in Human-Computer Interaction, GUI\nDesign, Open Source Software development, and project management.\nREFERENCES\n[1] Johannes Betz, Hongrui Zheng, Alexander Liniger, Ugo Rosolia,\nPhillip Karle, Madhur Behl, Venkat Krovi, and Rahul Mangharam.\nAutonomous vehicles on the edge: A survey on autonomous vehicle\nracing. IEEE Open Journal of Intelligent Transportation Systems ,\n3:458–488, 2022.\n[2] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex\nParinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations:\nFast and flexible image augmentations. Information, 11(2):125, feb\n2020.\n[3] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kir-\nillov, and Rohit Girdhar. Masked-attention mask transformer for\nuniversal image segmentation, 2021.\n[4] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmentation, 2021.\n[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,\nMarkus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,\nand Bernt Schiele. The cityscapes dataset for semantic urban scene\nunderstanding, 2016.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei. 2009 ieee conference on computer vision and pattern recognition.\n2009.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale, 2020.\n[8] David Eigen and Rob Fergus. Predicting depth, surface normals and\nsemantic labels with a common multi-scale convolutional architecture.\nIn Proceedings of the IEEE international conference on computer\nvision, pages 2650–2658, 2015.\n[9] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map predic-\ntion from a single image using a multi-scale deep network, 2014.\n[10] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins:\ndepth estimation using adaptive bins. arXiv e-prints , pages arXiv–\n2011, 2020.\n[11] Ravi Garg, Vijay Kumar BG, Gustavo Carneiro, and Ian Reid. Unsu-\npervised cnn for single view depth estimation: Geometry to the rescue,\n2016.\n[12] Ashkan Gholamhosseinian and Jochen Seitz. Vehicle classification\nin intelligent transport systems: An overview, methods and software\nperspective. IEEE Open Journal of Intelligent Transportation Systems,\n2:173–194, 2021.\n[13] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Girshick.\nMask r-cnn. In Proceedings of the IEEE international conference\non computer vision , pages 2961–2969, 2017.\n[14] Archie J Huang and Shaurya Agarwal. Physics-informed deep learning\nfor traffic state estimation: illustrations with lwr and ctm models. IEEE\nOpen Journal of Intelligent Transportation Systems , 3:503–518, 2022.\n[15] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan\nGeng, and Ruigang Yang. The ApolloScape open dataset for au-\ntonomous driving and its application. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 42(10):2702–2719, oct 2020.\n[16] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li,\nSteven Walton, and Humphrey Shi. Semask: Semantically masked\ntransformers for semantic segmentation, 2021.\n[17] Guillaume Jornod, Andreas Pfadler, Sofia Carreira, Ahmad El Assaad,\nand Thomas K¨urner. Fuel efficient high-density platooning using future\nconditions prediction. IEEE open journal of intelligent transportation\nsystems, 3:786–798, 2022.\n[18] Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Se-\nhwan Chun, and Junmo Kim. Global-local path networks for monoc-\nular depth estimation with vertical cutdepth, 2022.\n[19] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization, 2014.\n[20] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depth-\nformer: Exploiting long-range correlation and local information for\naccurate monocular depth estimation, 2022.\n[21] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Bins-\nformer: Revisiting adaptive bins for monocular depth estimation, 2022.\n[22] Xiwen Liang, Minzhe Niu, Jianhua Han, Hang Xu, Chunjing Xu, and\nXiaodan Liang. Visual exemplar driven task-prompting for unified\nperception in autonomous driving. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 9611–\n9621, 2023.\n[23] Xiwen Liang, Yangxin Wu, Jianhua Han, Hang Xu, Chunjing Xu,\nand Xiaodan Liang. Effective adaptation in multi-task co-training\nfor unified autonomous driving. Advances in Neural Information\nProcessing Systems, 35:19645–19658, 2022.\n[24] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel dataset\nand benchmarks for urban scene understanding in 2d and 3d, 2021.\n[25] Jing Liu, Yuhang Wang, Yong Li, Jun Fu, Jiangyun Li, and Hanqing\nLu. Collaborative deconvolutional neural networks for joint depth\nestimation and semantic segmentation. IEEE transactions on neural\nnetworks and learning systems , 29(11):5655–5666, 2018.\n[26] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin\nYang, and Jie Tang. Gpt understands, too. AI Open, 2023.\nVOLUME , 21\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\n[27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. Swin transformer: Hierarchical vision\ntransformer using shifted windows, 2021.\n[28] Herman Ryen Martinsen. Autonomous driving: Vision transformers\nfor dense prediction tasks. Master’s thesis, NTNU, 2022.\n[29] Zonglin Meng, Xin Xia, Runsheng Xu, Wei Liu, and Jiaqi Ma. Hydro-\n3d: Hybrid object detection and tracking for cooperative perception\nusing 3d lidar. IEEE Transactions on Intelligent Vehicles , 2023.\n[30] Arsalan Mousavian, Hamed Pirsiavash, and Jana Ko ˇseck´a. Joint\nsemantic segmentation and depth estimation with deep convolutional\nnetworks. In 2016 Fourth International Conference on 3D Vision\n(3DV), pages 611–619. IEEE, 2016.\n[31] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision\ntransformers for dense prediction, 2021.\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-\ncnn: Towards real-time object detection with region proposal networks.\nAdvances in neural information processing systems , 28, 2015.\n[33] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra\nMalik, and Silvio Savarese. Which tasks should be learned together\nin multi-task learning? In International Conference on Machine\nLearning, pages 9120–9132. PMLR, 2020.\n[34] Marvin Teichmann, Michael Weber, Marius Zoellner, Roberto Cipolla,\nand Raquel Urtasun. Multinet: Real-time joint semantic reasoning for\nautonomous driving. In 2018 IEEE intelligent vehicles symposium\n(IV), pages 1013–1020. IEEE, 2018.\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention\nis all you need, 2017.\n[36] Koen Vellenga, H Joe Steinhauer, Alexander Karlsson, G¨oran Falkman,\nAsli Rhodin, and Ashok Chaitanya Koppisetty. Driver intention\nrecognition: State-of-the-art review. IEEE Open Journal of Intelligent\nTransportation Systems, 2022.\n[37] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\nLiang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer:\nA versatile backbone for dense prediction without convolutions, 2021.\n[38] Zhangjing Wang, Yu Wu, and Qingqing Niu. Multi-sensor fusion in\nautomated driving: A survey. Ieee Access, 8:2847–2868, 2019.\n[39] Dong Wu, Man-Wen Liao, Wei-Tian Zhang, Xing-Gang Wang, Xiang\nBai, Wen-Qing Cheng, and Wen-Yu Liu. Yolop: You only look\nonce for panoptic driving perception. Machine Intelligence Research,\n19(6):550–562, 2022.\n[40] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M.\nAlvarez, and Ping Luo. Segformer: Simple and efficient design for\nsemantic segmentation with transformers, 2021.\n[41] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam Lim, and\nAntonio Torralba. Mtformer: Multi-task learning via transformer and\ncross-task reasoning. In European Conference on Computer Vision ,\npages 304–321. Springer, 2022.\n[42] Haotian Yan, Chuang Zhang, and Ming Wu. Lawin transformer:\nImproving semantic segmentation transformer with multi-scale rep-\nresentations via large window attention, 2022.\n[43] Zhengyuan Yang, Yixuan Zhang, Jerry Yu, Junjie Cai, and Jiebo Luo.\nEnd-to-end multi-modal multi-task vehicle control for self-driving cars\nwith visual perceptions. In 2018 24th international conference on\npattern recognition (ICPR) , pages 2289–2294. IEEE, 2018.\n[44] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo,\nYabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr,\nand Li Zhang. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers, 2020.\n22 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Intelligent Transportation Systems. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJITS.2023.3335648\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}