{
  "title": "PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking",
  "url": "https://openalex.org/W4396913708",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2132492283",
      "name": "YUZHANG XIE",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2150587268",
      "name": "Jiaying Lu",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2110628446",
      "name": "Joyce Ho",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A1995592236",
      "name": "Fadi Nahab",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2096945033",
      "name": "Xiao Hu",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2277406968",
      "name": "Carl Yang",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2923757114",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2122402213",
    "https://openalex.org/W4378782608",
    "https://openalex.org/W2404369708",
    "https://openalex.org/W138612281",
    "https://openalex.org/W2252174377",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W2118100588",
    "https://openalex.org/W1577541759",
    "https://openalex.org/W2129038679",
    "https://openalex.org/W3120118518",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W4224309032",
    "https://openalex.org/W4365597191",
    "https://openalex.org/W2809398771",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2102443632",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W3032553678",
    "https://openalex.org/W4367669997",
    "https://openalex.org/W2795959543",
    "https://openalex.org/W4385456320",
    "https://openalex.org/W6600655691",
    "https://openalex.org/W3035021780",
    "https://openalex.org/W4385574140"
  ],
  "abstract": "Linking (aligning) biomedical concepts across diverse data sources enables various integrative analyses, but it is challenging due to the discrepancies in concept naming conventions. Various strategies have been developed to overcome this challenge, such as those based on string-matching rules, manually crafted thesauri, and machine learning models. However, these methods are constrained by limited prior biomedical knowledge and can hardly generalize beyond the limited amounts of rules, thesauri, or training samples. Recently, large language models (LLMs) have exhibited impressive results in diverse biomedical NLP tasks due to their unprecedentedly rich prior knowledge and strong zero-shot prediction abilities. However, LLMs suffer from issues including high costs, limited context length, and unreliable predictions. In this research, we propose PromptLink, a novel biomedical concept linking framework that leverages LLMs. It first employs a biomedical-specialized pre-trained language model to generate candidate concepts that can fit in the LLM context windows. Then it utilizes an LLM to link concepts through two-stage prompts, where the first-stage prompt aims to elicit the biomedical prior knowledge from the LLM for the concept linking task and the second-stage prompt enforces the LLM to reflect on its own predictions to further enhance their reliability. Empirical results on the concept linking task between two EHR datasets and an external biomedical KG demonstrate the effectiveness of PromptLink. Furthermore, PromptLink is a generic framework without reliance on additional prior knowledge, context, or training data, making it well-suited for concept linking across various types of data sources. The source code of this study is available at https://github.com/constantjxyz/PromptLink.",
  "full_text": "PromptLink: Leveraging Large Language Models for\nCross-Source Biomedical Concept Linking\nYuzhang Xie\nEmory University, USA\nyuzhang.xie@emory.edu\nJiaying Lu\nEmory University, USA\njiaying.lu@emory.edu\nJoyce Ho\nEmory University, USA\njoyce.c.ho@emory.edu\nFadi Nahab\nEmory University, USA\nfnahab@emory.edu\nXiao Hu\nEmory University, USA\nxiao.hu@emory.edu\nCarl Yang\nEmory University, USA\nj.carlyang@emory.edu\nAbstract\nLinking (aligning) biomedical concepts across diverse data sources\nenables various integrative analyses, but it is challenging due to the\ndiscrepancies in concept naming conventions. Various strategies\nhave been developed to overcome this challenge, such as those\nbased on string-matching rules, manually crafted thesauri, and ma-\nchine learning models. However, these methods are constrained\nby limited prior biomedical knowledge and can hardly generalize\nbeyond the limited amounts of rules, thesauri, or training samples.\nRecently, large language models (LLMs) have exhibited impressive\nresults in diverse biomedical NLP tasks due to their unprecedent-\nedly rich prior knowledge and strong zero-shot prediction abilities.\nHowever, LLMs suffer from issues including high costs, limited\ncontext length, and unreliable predictions. In this research, we pro-\npose PromptLink, a novel biomedical concept linking framework\nthat leverages LLMs. It first employs a biomedical-specialized pre-\ntrained language model to generate candidate concepts that can\nfit in the LLM context windows. Then it utilizes an LLM to link\nconcepts through two-stage prompts, where the first-stage prompt\naims to elicit the biomedical prior knowledge from the LLM for\nthe concept linking task and the second-stage prompt enforces\nthe LLM to reflect on its own predictions to further enhance their\nreliability. Empirical results on the concept linking task between\ntwo EHR datasets and an external biomedical KG demonstrate the\neffectiveness of PromptLink. Furthermore, PromptLink is a generic\nframework without reliance on additional prior knowledge, con-\ntext, or training data, making it well-suited for concept linking\nacross various types of data sources. The source code of this study\nis available at https://github.com/constantjxyz/PromptLink.\nCCS Concepts\n‚Ä¢ Applied computing ‚ÜíHealth care information systems ; ‚Ä¢\nInformation systems ‚ÜíRetrieval models and ranking ;\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657904\nKeywords\nBiomedical Concept Linking, Few-Shot Prompting, Large Language\nModels for Resource-Constrained Field, Retrieve & Re-Rank\nACM Reference Format:\nYuzhang Xie, Jiaying Lu, Joyce Ho, Fadi Nahab, Xiao Hu, and Carl Yang.\n2024. PromptLink: Leveraging Large Language Models for Cross-Source\nBiomedical Concept Linking. In Proceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval\n(SIGIR ‚Äô24), July 14‚Äì18, 2024, Washington, DC, USA. ACM, New York, NY,\nUSA, 6 pages. https://doi.org/10.1145/3626772.3657904\n1 Introduction\nBiomedical concept linking studies the intricate task of linking\nclosely related concepts across different data sources by leveraging\ntheir semantic meanings and underlying biomedical knowledge, as\nexemplified in Figure 1 [29]. This linking process is crucial for en-\nabling integrative analyses, as biomedical concepts obtained from\ndiverse sources offer multifaceted views of biomedical knowledge\nand data [19, 32]. For example, the electronic health record (EHR),\nwhich is regarded as a valuable asset for comprehensive patient\nhealth analysis, contains various digital medical information in-\ncluding tabular data, clinical notes, and other types of patient data\n[1, 33, 39]. Similarly, the knowledge graph (KG), playing an impor-\ntant role in biomedical research, provides structured knowledge,\nsuch as definitions of concepts and their interrelationships [ 21].\nHowever, the cross-source biomedical linking task is challenging\ndue to discrepancies in the biomedical naming conventions used in\ndifferent systems [15]. For example, a KG may mention a disease\nas ‚ÄúEllis-Van Creveld syndrome‚Äù, while an EHR may refer to the\nsame disease as ‚ÄúChondroectodermal dysplasia‚Äù. This inconsistency\npresents a strong barrier to cohesive data analysis.\nThe challenge of biomedical concept linking has motivated the\ndevelopment of various methods. Conventional methods focus\non setting string-matching rules [7, 13] and leveraging constructed\nthesauri [3, 9, 27]. However, their reliance on fixed rules and crafted\nthesauri limits coverage and generalizability in real-world scenar-\nios [30]. Addressing these limitations, machine learning-based\nmethods have been widely explored, avoiding the manual design of\nrules or thesauri. These methods essentially transform biomedical\nconcepts from raw text into embeddings (latent vector representa-\ntions), which are then used to compute similarity scores via distance\nfunctions (e.g. cosine similarity) or learning-based scoring functions\n(e.g. bilinear attention [14]). Various models have been used to ob-\ntain biomedical concept embeddings, includingpre-trained language\narXiv:2405.07500v1  [cs.IR]  13 May 2024\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA. Yuzhang Xie et al\nFigure 1: A toy example of biomedical concept linking. Left:\nconcepts in the EHR. Right: concepts in the biomedical KG.\nmodels (PLMs) [34] that capture fine-grained semantic relations\nthrough extensive training on biomedical corpora [ 2, 16, 17, 38],\nand graph neural networks (GNNs) [42] that capture both semantics\nand relations of biomedical concepts [4, 10, 18]. Despite the notable\nachievements of these ML-based linking methods, they are data-\nhungry and require significant supervision signals when adapted\ninto novel downstream applications. They face challenges due to\nthe costly data annotation and model training processes.\nRecently, large language models (LLMs) have exhibited impres-\nsive performances in various NLP tasks, due to their unprecedent-\nedly rich prior knowledge and language capabilities [ 31, 35, 43],\nenabling various applications in a zero-shot learning setting [19].\nTherefore, LLMs provide a promising solution for linking related\nconcepts across different systems. Meanwhile, LLMs also face chal-\nlenges including the design of effective and cost-efficient prompts\nwithin the context length limits [40], and the NIL prediction capa-\nbility of reliably rejecting all candidates when correct concepts are\nabsent, instead of returning relatively close but incorrect ones [23].\nIn this paper, we proposePromptLink, leveraging LLMs for the\ncross-source biomedical concept linking task. Considering LLMs‚Äô\nhigh cost and context length constraints, we first employ a pre-\ntrained SAPBERT language model to generate biomedical-aware\nconcept embeddings and retrieve top candidates based on the co-\nsine similarities of these embeddings. We then design a novel two-\nstage prompting mechanism for the GPT-4 model to derive reliable\nlinking predictions. The first stage efficiently filters out irrelevant\ncandidates, thereby minimizing the response token numbers re-\nquired in the subsequent stage. The second stage generates the\nfinal linking results and incorporates a self-verification prompt to\naddress the NIL prediction challenge, effectively rejecting all can-\ndidates when none are relevant. In the experiments, PromptLink\ndemonstrates exceptional performance, surpassing various existing\nconcept linking methods by over 5% in two scenarios of biomedical\nconcept linking between EHR and external biomedical KG, which\ncould be attributed to LLM‚Äôs intrinsic strong biomedical knowledge.\nMoreover, PromptLink works as a zero-shot framework due to the\nutilization of pre-trained language models, eliminating the need for\na training process. It is also a versatile framework that performs\nwell even when only concept names, without concept context or\ntopological structure, are provided. Given its various advantages,\nPromptLink boasts strong generalization capabilities, making it\nsuitable for a wide range of biomedical research and applications.\n2 Biomedical Concept Linking\n2.1 Problem Definition\nThe biomedical concept linking task aims to link biomedical con-\ncepts across sources/systems based on semantic meanings and\nbiomedical knowledge. It solely relies on concept names and can\nthus cover much broader real-world applications. This task differs\nfrom existing tasks such as entity linking [30], entity alignment [19],\nand ontology matching [11], which depend on extra contextual or\ntopological information. In this study, we link the concepts in EHR\nto corresponding concepts in a biomedical KG. We define an EHR\ndatabase D, a biomedical KG G, and the linking task as follows:\nDefinition 1 (EHR). An EHR database Dis a relational database\nD= (ùëÉ,ùê¥,ùëâ ), with ùëÉ being patient identifiers, ùê¥patient attributes,\nùëâ ‚ààùëÉ√óùê¥the values of these attributes. Additionally,ùëÄrepresents\nmulti-token biomedical concepts associated with patient attributes.\nDefinition 2 (Biomedical KG). A biomedical KG is a multi-relation\ngraph G= (ùê∂,ùëÖ,ùëÖùëá ), where ùê∂ are concepts, ùëÖare relation names,\nand ùëÖùëá ‚ààùê∂√óùëÖ√óùê∂ are the relational triples among them.\nDefinition 3 (Biomedical Concept Linking). Link identified biomed-\nical concepts from an EHR Dto a biomedical KG Gbased on\nsemantic meanings and biomedical knowledge, forming linkages\nùêøùêæ = {(ùëöùëñ,ùëêùëó)|ùëöùëñ ‚ààùëÄD,ùëêùëó ‚ààùê∂G‚à™NIL}. If a concept ùëöfrom ùê∑is\nnot in ùê∫, link it to a special ‚ÄúNIL‚Äù entity, indicating it is unlinkable.\n2.2 PromptLink\nWe propose PromptLink, a novel LLM-based solution for cross-\nsource biomedical concept linking, as illustrated in Figure 2. Ad-\ndressing LLMs‚Äô high cost and limited input text length, we first\nemploy a biomedical-specialized pre-trained language model to\ngenerate concept embeddings and retrieve top candidates via co-\nsine similarities. Subsequently, we employ a two-stage prompting\nmechanism with GPT-4 to generate the final linking predictions.\nConcept representation and candidate generation. After pre-\nprocessing text by lowercasing and removing punctuation, we use\na pre-trained LM (specifically SapBERT [17]), to create embeddings\nùíâ ‚ààR1√óùëë for EHR concepts ùëöand KG concepts ùëê, represented as\nùíâùíé = ùëÉùêøùëÄ(ùëö), ùíâùíÑ = ùëÉùêøùëÄ(ùëê).\nFor concepts that span multiple tokens, the token-level embeddings\nare averaged to create the concept embedding. This model helps to\nproject the semantic meanings and prior biomedical knowledge into\nthe embedding space. For candidate generation, we compute cosine\nsimilarity ùëÜ ‚àà[0,1]between pairs of EHR concept embedding ùíâùíé\nand KG concept embedding ùíâùíÑ , represented as\nùëÜ = ùëêùëúùë†(ùíâùíé,ùíâùíÑ ).\nGiven each input query EHR conceptùëöùëñ, We select the top-ùêæ(ùêæ=10)\nKG concepts [ùëê1,ùëê2,...,ùëê ùêæ]with the highest similarities as candi-\ndates for further GPT-based linking prediction.\nLinking prediction using two-stage prompts. The next step of\nour framework is generating linking predictions of query ùëöùëñ over\nthe top-ùêæ candidate [ùëê1,ùëê2,...,ùëê ùêæ]using GPT-4 model, leveraging\nits text comprehension ability, logical reasoning ability, and prior\nPromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA.\nFigure 2: Overview of our proposed PromptLink framework.\nbiomedical knowledge [5, 31]. In this step, we design a novel two-\nstage prompt for our task, as can be seen in Figure 2. Combining the\ntwo prompts utilizes their strengths and mitigates weaknesses. The\nfirst stage focuses on concept pairs to filter out unrelated candidates.\nThe second stage evaluates all candidates in a broader context to\nidentify the closest match or reject all unmatch candidates.\nIn the first stage, the LLM is prompted to check if a concept\npair (ùëöùëñ,ùëêùëó)should be linked. By defining the response structure,\nthe LLM can return answers in specified formats. To improve the\nprompt response quality, we adopt the self-consistency [36] prompt-\ning strategy that repeatedly prompts the same question to the LLM\nmultiple times. Specifically, we prompt each concept pair (ùëöùëñ,ùëêùëó)\nfor ùëõ= 5 times, thus obtaining the belief score ùêµùëñ,ùëó ‚àà[0,1]by:\nùêµùëñ,ùëó = number of ‚Äúyes‚Äù responses\nùëõ .\nConsidering the belief scores across different candidates, we derive a\ncomprehensive filter strategy to exclude irrelevant candidates, using\nparameter ùúè (set as 0.8 √óùëõ). This approach ensures that irrelevant\ncandidates are not considered in the next stage, optimizing both\nefficiency and effectiveness. The approach is described as follows:\n‚Ä¢If ùëöùëéùë•(ùêµùëñ,1,¬∑¬∑¬∑ ,ùêµùëñ,ùêæ)‚â• ùúè, this indicates some candidates closely\nalign with the query concept. In such cases, candidates with belief\nscores of zero will be filtered out as they are deemed irrelevant\nto the query concept and there are closely aligning alternatives.\nThis filtering strategy effectively removes many irrelevant candi-\ndates, thereby optimizing both efficiency and effectiveness for\nthe subsequent stage.\n‚Ä¢Otherwise, the range of different candidates‚Äô belief scores is not\nwide enough to justify filtering. Thus, all ùêæ candidates will be\nsubjected to double-checking by the second-stage prompt.\nIn the second stage, the LLM evaluates the ùêæ1 candidates re-\ntained from the first stage‚Äôs filtering process[ùëê‚Ä≤\n1,ùëê‚Ä≤\n2,...,ùëê ‚Ä≤\nùêæ1\n], where\nùêæ1 ‚â§ùêæ , using a compositional prompt that consists of two con-\nsecutive questions to perform complex reasoning. Specifically, the\nLLM is asked to (1) label the relationship between the query con-\ncept and all candidate concepts as ‚Äúexact match‚Äù, ‚Äúrelated to‚Äù, or\n‚Äúdifferent from‚Äù; (2) use self-verification prompts to either identify\nthe closest candidate or dismiss all candidates if none are close,\nthus the final concept linking result of this prompt is ùêæ2 (usually\nùêæ2 = 1) item from [ùëê‚Ä≤\n1,...,ùëê ‚Ä≤\nùêæ1\n]‚à™[ NIL]. In this stage, we also use\nthe self-consistency strategy that prompts one question for the\nsame ùëõtimes. Subsequently, we calculate the occurrence frequency\nùëìùëñ,ùëó ‚àà[0,1]for answers in [ùëê‚Ä≤\n1,ùëê‚Ä≤\n2,..., NIL]and retrieve the final\nlinking result for query EHR concept ùëöùëñ as follows:\n‚Ä¢If ùëìùëñ, NIL > 0.5, this indicates a high probability that none of the\ncandidates are appropriate. Thus, ‚ÄúNIL‚Äù is chosen as the final\nlinking prediction.\n‚Ä¢Otherwise, the candidate ùëêùëó with the highest frequency ùëìùëñ,ùëó is\ndecided as the final linking result. If two candidates tie for the\nhighest frequency, the one ùëêùëó with higher embedding similarity\nùëÜùëñ,ùëó to the query concept ùëöùëñ is chosen.\n3 Experiments & Discussions\n3.1 Implementation Details\nDatasets. In our experiments, we curate two biomedical concept\nlinking benchmark datasets: MIID (MIMIC-III-iBKH-Disease) and\nCISE (CRADLE-iBKH-Side-Effect). MIID comprises 1,493 diagnosis\nconcepts from MIMIC-III [12], which is an EHR dataset including\nover 53,423 hospital patient records, and 18,697 disease concepts\nfrom iBKH [32], which is a KG dataset with 2,384,501 entities. To\nconstruct MIID, we first remove exact matches between MIMIC-III\ndiagnosis concepts and iBKH disease concepts. Then, we link the\nremaining MIMIC concepts to iBKH using ICD-9 [ 8] and UMLS\nCUI [28] codes. We use the linked concept pairs as ground-truth la-\nbels only for evaluation purposes. CISE contains 1,500 CRADLE [39]\ndiagnosis concepts and 4,251 iBKH drug side-effect concepts, con-\nstructed by using CUI [28] and SNOMED CT [6] codes. Ground-\ntruth matched pairs are also only used for evaluation purposes.\nExperimental Settings. Following the definition in Sec. 2.1 and\nrecognizing the scarcity of supervision in the biomedical domain,\nwe mainly focus on the biomedical concept linking under the zero-\nshot setting. Additionally, our biomedical concept linking task\nsolely relies on concept names for broad real-world application\ncoverage. Given this characteristic of our data input, graph-based\nlinking methods, such as selfKG [ 18], are not applicable as they\nneed topological information to establish concept alignment. Simi-\nlarly, thesauri-based methods, such as MetaMap [3], are unsuitable\nas they only establish links between EHR concepts and KG concepts\nexisting in the pre-defined vocabulary. Therefore, the following\nbaseline methods are compared:\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA. Yuzhang Xie et al\n‚Ä¢Conventional methods: Cosine Distance , Jaccard Distance ,\nLevenshtein Distance [24], Jaro-Winkler Distance [37], BM25\n[25]. These methods measure the concept pairs‚Äô string similarity\nand relevance and then obtain the linking prediction result.\n‚Ä¢Machine learning-based methods: Pre-trained language mod-\nels are used to generate concept embedding and linking predic-\ntion results (according to embedding cosine similarity). Specif-\nically, we select representative PLMs including BioBERT [16],\nBioGPT [20], BioClinicalBERT [2], BioDistilBERT [26], KRISS-\nBERT [41], ada002 [22], and SAPBERT [17].\n3.2 Concept Linking Experiment Results\nTable 1: Comparison of the zero-shot accuracy for different\nmethods on MIID and CISE.\nMethod Acc-MIID Acc-CISE\nCosine Distance 0.2981 0.2907\nJaccard Distance 0.2123 0.3280\nLevenshtein Distance 0.1995 0.3033\nJaro-Winkler Distance 0.3141 0.3693\nBM25 0.4722 0.3993\nBioBERT 0.3423 0.5280\nBioClinicalBERT 0.3007 0.5007\nBioGPT 0.3530 0.5093\nBioDistilBERT 0.4240 0.5293\nKRISSBERT 0.5265 0.5787\nada002 0.5968 0.6773\nSAPBERT 0.7213 0.8167\nPromptLink 0.7756 0.8880\nTable 1 shows the accuracy of our proposed PromptLink along\nwith baseline methods, when every method links a query EHR\nconcept ùëöùëñ with their predicted top-1 KG concept ùëêùëó. As can be\nseen, PromptLink outperforms competing approaches across both\ndatasets in terms of zero-shot accuracy, underscoring the supe-\nriority of our LLM-based concept linking methodology. Among\nthe compared methods, SAPBERT, a SOTA biomedical entity link-\ning method, achieves the second-highest performance. Moreover,\nconventional methods based on string similarity lag behind ma-\nchine learning techniques, which leverage embeddings from pre-\ntrained language models to effectively match conceptually similar\nbut lexically distinct entities like ‚ÄúEllis-Van Creveld syndrome‚Äù and\n‚ÄúChondroectodermal dysplasia‚Äù.\n3.3 Ablation Studies\nTable 2: Ablation results with different prompting methods\nused by PromptLink on the MIID dataset.\nPrompting Methods Acc Token Cost\nBefore Prompting 0.7213 N/A\nFirst-stage Prompt 0.7595 995,836 ($36.59)\nSecond-stage Prompt 0.7634 1,681,987 ($88.69)\nTwo-stage Prompts 0.7756 1,594,996 ($66.25)\nPrompt Effectiveness and Efficiency. We conduct ablation stud-\nies to reveal the effectiveness and cost-efficiency of the prompt used\nin our approach, as shown in Table 2. This comparison uses the same\ninput data and 10 linking candidates across various prompts. In the\ntable, the ‚ÄúBefore prompting‚Äù denotes the performance of using only\nembedding similarity obtained from the pre-trained LM, while other\nmethods use LLM to predict linking results based on LM-generated\ncandidates. From Table 2, the ‚ÄúBefore Prompting‚Äù method achieves\nthe worst accuracy, demonstrating that linking performance could\nbe improved by using LLM. Notably, PromptLinkwith both two-\nstage prompts achieves the best accuracy with the second-highest\ncost (‚àº1.7M total tokens, costing approximately $66.25), indicating\nthat the combined effect of the prompts substantially enhances accu-\nracy, with the costs being moderated by the first stage‚Äôs proficiency\nin eliminating unrelated candidates.\nNIL Prediction. Another ablation study examines PromptLink‚Äôs\nNIL prediction ability. In our built MIID and CISE datasets, each\nquery EHR concept ùëöùëñ is designed to have a ground-truth linking\nKG concept ùëêùëó. To reflect the real-world unlinkable scenario, we ex-\ntend our MIID dataset into ‚ÄúMIID-NIL‚Äù which contains a proportion\n(25%) of unlikable EHR conceptùëöùëñ. In Figure 3, the overall accuracy\nof PromptLink in the MIID-NIL dataset is 0.8145. Specifically for the\nunlikable concepts, PromptLink outputs the expected ‚ÄúNIL‚Äù with\n0.9290 accuracy, which validates the NIL prediction ability of our\nproposed method. Existing methods highly rely on the hard-coded\nthreshold. For example, we could threshold SAPBERT‚Äôs generated\nembeddings‚Äô cosine similarity, then output the KG concept with\nthe highest similarity above the threshold or ‚ÄúNIL‚Äù when none are\nabove. However, this straightforward idea, requiring a manually\nset threshold, is less effective than PromptLink. As shown in Figure\n3, SAPBERT achieves lower accuracy (maximum value 0.7920) no\nmatter what the threshold value is, which corresponds to our as-\nsumptions. When the threshold value is low, SAPBERT generates\nmany wrong predictions to unlinkable query concepts; otherwise,\nSAPBERT continues to output ‚ÄúNIL‚Äù for many linkable concepts.\nFigure 3: Accuracy on\nMIID-NIL: Traditional ML-\nbased methods outputting\nmatching scores have varying\nNIL prediction performance\nbased on the selected thresh-\nold, while PromptLink does\nnot need a threshold yet\nconsistently performs better.\n3.4 Case Studies\nIn case studies on linking EHR concepts to MIID‚Äôs KG disease con-\ncepts, three scenarios are presented: (1) concepts assessed by both\nground-truth labels and a clinician; (2) concepts evaluated by a\nclinician due to missing ground-truth labels; (3) irrelevant concepts\njudged by a clinician. The linking results of PromptLink and SAP-\nBERT are presented in Table 3. Overall, PromptLink could link\nbiomedical concepts more accurately and appropriately. For casse\nI-V, PromptLink‚Äôs linking results are justified by the ground-truth\nPromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA.\nlabel and clinician. Specifically, for cases I and II, PromptLink accu-\nrately links the EHR concepts to conceptually similar but lexically\ndistinct KG concepts, while SAPBERT links to lexically similar but\nconceptually different KG concepts. This difference showcases the\neffective use of LLM‚Äôs biomedical knowledge. SAPBERT also shows\ninaccuracies in cases III-IV, and provides a broader prediction in\ncase V, whereas PromptLink‚Äôs predictions are more accurate and\nspecific. For cases VI-IX, where linking ground truth labels are\nlacking, PromptLink‚Äôs predictions also align more accurately with\nEHR concepts than SAPBERT‚Äôs, according to a clinician‚Äôs review.\nIn cases VI and VII, PromptLink closely matches the EHR concepts,\nwhile SAPBERT‚Äôs predictions are overly specific. In cases VIII and\nIX, PromptLink correctly and automatically identifies no match-\ning KG disease concepts, while SAPBERT fails to resolve that NIL\nprediction challenge unless manual thresholds are set and adjusted.\nTable 3: Analyzed cases.\nID EHR Concept PromptLink‚Äôs Prediction SAPBERT‚Äôs Prediction\nI Chondroectodermal dysplasia Ellis-van Creveld syndrome\nCranioectodermal dysplasia\nII Dermatophytosis of hand Tinea manuum\n Hand dermatosis\nIII Late syphilis, unspecified Tertiary syphilis\n Secondary syphilis\nIV Hypopotassemia Hypokalemia\n Hypocupremia nos\nV Epidemic vertigo Vestibular neuronitis\n Vertigo\nVI Postprocedural fever Postoperative complications\nPostcardiotomy syndrome\nVII Acquired cardiac septal defect Heart septal defect\nAtrial heart septal defect\nVIII Height of bed NIL\n Binge eating disorder\nIX Level one NIL\n Glaucoma 1 open angle\nNote: ‚Äú\n ‚Äù indicates this prediction is justified by the clinician. ‚Äú\n ‚Äù indicates this prediction is\njustified by the ground-truth label.\n4 Conclusion\nIn this study, we introduce PromptLink, a novel framework lever-\naging LLMs and multi-stage prompts for effective biomedical con-\ncept linking. Compared with previous concept linking methods,\nPromptLink achieves better linking accuracy, attributed to LLM‚Äôs\nintrinsic strong biomedical knowledge. PromptLink further em-\nploys multi-stage prompts to maintain cost-efficiency and handle\nthe NIL prediction problem. Moreover, PromptLink functions as\na zero-shot framework, requiring no training and demonstrating\nstrong flexibility and generalizability across biomedical systems.\nPromising future work can focus on further enhancing the prompt\neffectiveness, reducing costs, and minimizing manual efforts, aim-\ning to extend PromptLink‚Äôs application to broader systems.\nAcknowledgements\nResearch reported in this publication was partially supported by\nthe National Institute Of Diabetes And Digestive And Kidney Dis-\neases of the National Institutes of Health under Award Number\nK25DK135913. JH was supported by the National Science Founda-\ntion under Award Number IIS-2145411.\nReferences\n[1] Noura S Abul-Husn and Eimear E Kenny. 2019. Personalized medicine and the\npower of electronic health records. Cell 177, 1 (2019), 58‚Äì69.\n[2] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan\nNaumann, WA Redmond, and Matthew BA McDermott. 2019. Publicly Available\nClinical BERT Embeddings. NAACL HLT 2019 (2019), 72.\n[3] Alan R Aronson and Fran√ßois-Michel Lang. 2010. An overview of MetaMap:\nhistorical perspective and recent advances. Journal of the American Medical\nInformatics Association 17, 3 (2010), 229‚Äì236.\n[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Ok-\nsana Yakhnenko. 2013. Translating embeddings for modeling multi-relational\ndata. Advances in neural information processing systems 26 (2013).\n[5] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023).\n[6] Kevin Donnelly et al. 2006. SNOMED-CT: The advanced terminology and coding\nsystem for eHealth. Studies in health technology and informatics 121 (2006), 279.\n[7] Jennifer D‚ÄôSouza and Vincent Ng. 2015. Sieve-based entity linking for the biomed-\nical domain. In Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 2: Short Papers) . 297‚Äì302.\n[8] Centers for Disease Control. 2007. International Classification of Diseases-9-CM.\nAvailable at http://www.cdc.gov/nchs/icd.htm. Accessed Feb, 2024.\n[9] Carol Friedman, Hongfang Liu, Lyudmila Shagina, Stephen Johnson, and George\nHripcsak. 2001. Evaluating the UMLS as a source of lexical knowledge for medical\nlanguage processing.. In Proceedings of the AMIA Symposium . American Medical\nInformatics Association, 189.\n[10] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD international conference on\nKnowledge discovery and data mining . 855‚Äì864.\n[11] Ernesto Jim√©nez-Ruiz and Bernardo Cuenca Grau. 2011. Logmap: Logic-based\nand scalable ontology matching. In 10th International Semantic Web Conference .\n273‚Äì288.\n[12] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and\nRoger G Mark. 2016. MIMIC-III, a freely accessible critical care database.Scientific\ndata 3, 1 (2016), 1‚Äì9.\n[13] Ning Kang, Bharat Singh, Zubair Afzal, Erik M van Mulligen, and Jan A Kors. 2013.\nUsing rule-based natural language processing to improve disease normalization\nin biomedical text. Journal of the American Medical Informatics Association 20, 5\n(2013), 876‚Äì881.\n[14] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. Bilinear attention\nnetworks. Advances in neural information processing systems 31 (2018).\n[15] Isaac S Kohane, Bruce J Aronow, Paul Avillach, Brett K Beaulieu-Jones, Riccardo\nBellazzi, Robert L Bradford, Gabriel A Brat, Mario Cannataro, James J Cimino,\nNoelia Garc√≠a-Barrio, et al. 2021. What every reader should know about studies\nusing electronic health record data but may be afraid to ask. Journal of medical\nInternet research 23, 3 (2021), e22219.\n[16] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,\nChan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining. Bioinformatics 36, 4 (2020),\n1234‚Äì1240.\n[17] Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier.\n2021. Self-alignment pretraining for biomedical entity representations. In Pro-\nceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies . 4228‚Äì4238.\n[18] Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov, Yuxiao\nDong, and Jie Tang. 2022. Selfkg: Self-supervised entity alignment in knowledge\ngraphs. In Proceedings of the ACM Web Conference 2022 . 860‚Äì870.\n[19] Jiaying Lu, Jiaming Shen, Bo Xiong, Wenjing Ma, Steffen Staab, and Carl Yang.\n2023. Hiprompt: Few-shot biomedical knowledge fusion via hierarchy-oriented\nprompting. In 46th International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval - Short Paper .\n[20] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and\nTie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for biomedical\ntext generation and mining. Briefings in bioinformatics 23, 6 (2022), bbac409.\n[21] Fenglong Ma, Jing Gao, Qiuling Suo, Quanzeng You, Jing Zhou, and Aidong\nZhang. 2018. Risk prediction on electronic health records with prior medical\nknowledge. In Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining . 1910‚Äì1919.\n[22] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry\nTworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al .\n2022. Text and code embeddings by contrastive pre-training. arXiv preprint\narXiv:2201.10005 (2022).\n[23] Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi,\nSameer Singh, and Noah A Smith. 2019. Knowledge Enhanced Contextual Word\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA. Yuzhang Xie et al\nRepresentations. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) . 43‚Äì54.\n[24] Eric Sven Ristad and Peter N Yianilos. 1998. Learning string-edit distance. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 20, 5 (1998), 522‚Äì532.\n[25] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends ¬Æ in Information Retrieval\n3, 4 (2009), 333‚Äì389.\n[26] Omid Rohanian, Mohammadmahdi Nouriborji, Samaneh Kouchaki, and David A\nClifton. 2023. On the effectiveness of compact biomedical transformers. Bioinfor-\nmatics 39, 3 (2023), btad103.\n[27] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan\nSohn, Karin C Kipper-Schuler, and Christopher G Chute. 2010. Mayo clinical\nText Analysis and Knowledge Extraction System (cTAKES): architecture, compo-\nnent evaluation and applications. Journal of the American Medical Informatics\nAssociation 17, 5 (2010), 507‚Äì513.\n[28] Peri L Schuyler, William T Hole, Mark S Tuttle, and David D Sherertz. 1993.\nThe UMLS Metathesaurus: representing different views of biomedical concepts.\nBulletin of the Medical Library Association 81, 2 (1993), 217.\n[29] √ñzge Sevgili, Artem Shelmanov, Mikhail Arkhipov, Alexander Panchenko, and\nChris Biemann. 2022. Neural entity linking: A survey of models based on deep\nlearning. Semantic Web 13, 3 (2022), 527‚Äì570.\n[30] Jiyun Shi, Zhimeng Yuan, Wenxuan Guo, Chen Ma, Jiehao Chen, and Meihui\nZhang. 2023. Knowledge-graph-enabled biomedical entity linking: a survey.\nWorld Wide Web (2023), 1‚Äì30.\n[31] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won\nChung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.\n2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023),\n172‚Äì180.\n[32] Chang Su, Yu Hou, Manqi Zhou, Suraj Rajendran, Jacqueline RMA Maasch, Zehra\nAbedi, Haotan Zhang, Zilong Bai, Anthony Cuturrufo, Winston Guo, et al. 2023.\nBiomedical discovery through the integrative biomedical knowledge hub (iBKH).\nIscience 26, 4 (2023).\n[33] Wencheng Sun, Zhiping Cai, Yangyang Li, Fang Liu, Shengqun Fang, and Guoyan\nWang. 2018. Data processing and text mining technologies on electronic medical\nrecords: a review. Journal of healthcare engineering 2018 (2018).\n[34] Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li,\nand Jie Fu. 2023. Pre-trained language models in biomedical domain: A systematic\nsurvey. Comput. Surveys 56, 3 (2023), 1‚Äì52.\n[35] Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023. Exploring the in-context\nlearning ability of large language model for biomedical concept linking. arXiv\npreprint arXiv:2307.01137 (2023).\n[36] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-Consistency Improves\nChain of Thought Reasoning in Language Models. In The Eleventh International\nConference on Learning Representations .\n[37] William E Winkler. 1990. String comparator metrics and enhanced decision rules\nin the Fellegi-Sunter model of record linkage. (1990).\n[38] Dongfang Xu, Zeyu Zhang, and Steven Bethard. 2020. A generate-and-rank frame-\nwork with semantic type regularization for biomedical concept normalization.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. 8452‚Äì8464.\n[39] Ran Xu, Yue Yu, Chao Zhang, Mohammed K Ali, Joyce C Ho, and Carl Yang. 2022.\nCounterfactual and factual reasoning over hypergraphs for interpretable clinical\npredictions on ehr. In Machine Learning for Health . PMLR, 259‚Äì278.\n[40] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023. How\nlanguage model hallucinations can snowball. arXiv preprint arXiv:2305.13534\n(2023).\n[41] Sheng Zhang, Hao Cheng, Shikhar Vashishth, Cliff Wong, Jinfeng Xiao, Xiaodong\nLiu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2022. Knowledge-Rich\nSelf-Supervision for Biomedical Entity Linking. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 . 868‚Äì880.\n[42] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,\nLifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks:\nA review of methods and applications. AI open 1 (2020), 57‚Äì81.\n[43] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,\nHarris Chan, and Jimmy Ba. 2023. Large Language Models Are Human-Level\nPrompt Engineers.(2023). ProQuest Number: INFORMATION TO ALL USERS\n30490868 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7317796349525452
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.3434073328971863
    },
    {
      "name": "Natural language processing",
      "score": 0.3327397108078003
    }
  ]
}