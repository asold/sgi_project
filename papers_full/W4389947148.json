{
  "title": "Pre-Trained Transformer-Based Models for Text Classification Using Low-Resourced Ewe Language",
  "url": "https://openalex.org/W4389947148",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5017252813",
      "name": "Victor Kwaku Agbesi",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100687323",
      "name": "Wenyu Chen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5010089486",
      "name": "Sophyani Banaamwini Yussif",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5012779371",
      "name": "Md Altab Hossin",
      "affiliations": [
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A5023088189",
      "name": "Chiagoziem C. Ukwuoma",
      "affiliations": [
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A5015553254",
      "name": "Noble Arden Kuadey",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5046597434",
      "name": "Collinson Colin M. Agbesi",
      "affiliations": [
        "Koforidua Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5002227806",
      "name": "Nagwan Abdel Samee",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A5051158911",
      "name": "Mona Jamjoom",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A5022092645",
      "name": "Mugahed A. Al–antari",
      "affiliations": [
        "Sejong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292212771",
    "https://openalex.org/W4283771777",
    "https://openalex.org/W3017277899",
    "https://openalex.org/W4253720782",
    "https://openalex.org/W4310202185",
    "https://openalex.org/W4382721535",
    "https://openalex.org/W4205586967",
    "https://openalex.org/W4367627723",
    "https://openalex.org/W4361828872",
    "https://openalex.org/W6811297763",
    "https://openalex.org/W2078169166",
    "https://openalex.org/W4385363939",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292474994",
    "https://openalex.org/W6810137975",
    "https://openalex.org/W4226112897",
    "https://openalex.org/W3155743827",
    "https://openalex.org/W4379882224",
    "https://openalex.org/W3126191299",
    "https://openalex.org/W2885141472",
    "https://openalex.org/W4291908500",
    "https://openalex.org/W3089144988",
    "https://openalex.org/W6782604223",
    "https://openalex.org/W3088171479",
    "https://openalex.org/W6803943978",
    "https://openalex.org/W2974335209",
    "https://openalex.org/W4385454542",
    "https://openalex.org/W3169056399",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W3198331626",
    "https://openalex.org/W3124640783",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4244952642",
    "https://openalex.org/W1912982817",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226418765",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4220707319",
    "https://openalex.org/W3214828763",
    "https://openalex.org/W3083272626"
  ],
  "abstract": "Despite a few attempts to automatically crawl Ewe text from online news portals and magazines, the African Ewe language remains underdeveloped despite its rich morphology and complex \"unique\" structure. This is due to the poor quality, unbalanced, and religious-based nature of the crawled Ewe texts, thus making it challenging to preprocess and perform any NLP task with current transformer-based language models. In this study, we present a well-preprocessed Ewe dataset for low-resource text classification to the research community. Additionally, we have developed an Ewe-based word embedding to leverage the low-resource semantic representation. Finally, we have fine-tuned seven transformer-based models, namely BERT-based (cased and uncased), DistilBERT-based (cased and uncased), RoBERTa, DistilRoBERTa, and DeBERTa, using the preprocessed Ewe dataset that we have proposed. Extensive experiments indicate that the fine-tuned BERT-base-cased model outperforms all baseline models with an accuracy of 0.972, precision of 0.969, recall of 0.970, loss score of 0.021, and an F1-score of 0.970. This performance demonstrates the model’s ability to comprehend the low-resourced Ewe semantic representation compared to all other models, thus setting the fine-tuned BERT-based model as the benchmark for the proposed Ewe dataset.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7435085773468018
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6887401342391968
    },
    {
      "name": "Computer science",
      "score": 0.671958863735199
    },
    {
      "name": "Language model",
      "score": 0.5959261655807495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5552951097488403
    },
    {
      "name": "Natural language processing",
      "score": 0.5524661540985107
    },
    {
      "name": "Word embedding",
      "score": 0.5306013226509094
    },
    {
      "name": "F1 score",
      "score": 0.49182629585266113
    },
    {
      "name": "Embedding",
      "score": 0.47313275933265686
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46746328473091125
    },
    {
      "name": "Geography",
      "score": 0.11464554071426392
    },
    {
      "name": "Engineering",
      "score": 0.09678897261619568
    },
    {
      "name": "Cartography",
      "score": 0.08184394240379333
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}