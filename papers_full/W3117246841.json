{
  "title": "CoCoLM: COmplex COmmonsense Enhanced Language Model",
  "url": "https://openalex.org/W3117246841",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5036253401",
      "name": "Changlong Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100445028",
      "name": "Hongming Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020880385",
      "name": "Yangqiu Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067034635",
      "name": "Wilfred Ng",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2972784023",
    "https://openalex.org/W3039578880",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3036882087",
    "https://openalex.org/W95183648",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3015339775",
    "https://openalex.org/W2946545670",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2798602728",
    "https://openalex.org/W2971694752",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3115729981",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2982426914",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2798865369",
    "https://openalex.org/W3034918576",
    "https://openalex.org/W3023293091",
    "https://openalex.org/W3210120707"
  ],
  "abstract": "Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contains rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with the complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address this problem, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Different from existing fine-tuning approaches, we do not focus on a specific task and propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graphs ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense knowledge among eventualities. Experiments on multiple downstream commonsense tasks that requires the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.",
  "full_text": "COCOLM : CO MPLEX COMMONSENSE ENHANCED\nLANGUAGE MODEL\nChanglong Yu∗, Hongming Zhang∗, Yangqiu Song, Wilfred Ng\nDepartment of Computer Science and Engineering\nThe Hong Kong University of Science and Technology\n{cyuaq, hzhangal, yqsong, wilfred}@cse.ust.hk\nJanuary 1, 2021\nABSTRACT\nLarge-scale pre-trained language models have demonstrated strong knowledge representation ability.\nHowever, recent studies suggest that even though these giant models contains rich simple com-\nmonsense knowledge (e.g., bird can ﬂy and ﬁsh can swim.), they often struggle with the complex\ncommonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying\nthe relationship between “Jim yells at Bob” and “Bob is upset”). To address this problem, in this\npaper, we propose to help pre-trained language models better incorporate complex commonsense\nknowledge. Different from existing ﬁne-tuning approaches, we do not focus on a speciﬁc task and\npropose a general language model named CoCoLM. Through the careful training over a large-scale\neventuality knowledge graphs ASER, we successfully teach pre-trained language models (i.e., BERT\nand RoBERTa) rich complex commonsense knowledge among eventualities. Experiments on multiple\ndownstream commonsense tasks that requires the correct understanding of eventualities demonstrate\nthe effectiveness of CoCoLM.\n1 Introduction\nRecently, large-scale pre-trained language representation models (LMs) [1, 2] have demonstrated the strong ability to\ndiscover useful linguistic properties of syntax and remember an impressive amounts of knowledge with self-supervised\ntraining over a large unlabeled corpus [3]. On top of that, with the help of the ﬁne-tuning step, LMs can learn how\nto use the memorized knowledge for different tasks, and thus achieve outstanding performance on many downstream\nnatural language processing (NLP) tasks.\nQuery Answer\nBirds can [MASK]. ﬂy\nCars are used for [MASK]. transport\nJim yells at Bob, [MASK] Jim is upset. but\nJim yells at Bob, [MASK] Bob is upset. but\nTable 1: Exploring knowledge contained in pre-trained language models following LAMA [3]. Exploiting queries and prediction\nreturned by BERT-large are presented. Semantically plausible and implausible prediction are indicated with blue and red colors.\nAs discussed in [4], while language models has already captured rich knowledge, they often only perform well when\nthe semantic unit is a single token while poorly when the semantic unit is more complex (e.g., a multi-token named\nentity which is a noun phrase or an eventuality which is a verb-centric phrase). For example, as shown in Table 1, if we\nfollow LAMA [3] to analyze the knowledge contained in BERT-large [1] with a token prediction task, we can ﬁnd out\nthat BERT can understand that bird can ﬂy, and car is used for the transportation, but it fails to understand the relation\n∗ Equal Contribution.\narXiv:2012.15643v1  [cs.CL]  31 Dec 2020\nA PREPRINT - JANUARY 1, 2021\nAfter\nCause\nConjection\nLarge Free Corpus\n Pre-training\nCoCoLM\nComplex Commonsense\nPre-training\nTask-speciﬁc\nFine-tuning\nEvent tasks\n[CLS] E1 [SEP] E2\nTransformer\nFigure 1: The overall framework of CoCoLM. On top of base pre-trained language models, complex commonsense knowledge from\nthe eventuality sequences is injected by ﬁne-tuning masked language models and auxiliary discourse tasks.\nbetween “Jim yells at Bob” and relevant eventualities. An important reason behind this is that current language models\nheavily rely on the token-level masked language model (MLM) as the loss function, which can effectively represent and\nmemorize the token-level co-occurrence information but struggle at perceiving multi-token concepts. For example, to\ncorrectly understand the relation between “Jim yells at Bob” and “Bob is upset”, the model needs perceive these two\neventualities as independent semantic units while current LMs often failed to do so. Even though BERT introduced an\nextra next sentence prediction task to capture more complex semantics, its effect is not very signiﬁcant because different\nfrom tokens, entities, or eventualities, the semantics of a sentence is often the combination of multiple semantic units\nrather than a single one [5].\nTo address this problem and equip LMs with complex and accurate human knowledge, [4] proposed to regularize the\nlanguage model with key-value based entity representations from external knowledge graphs. While such approach has\nbeen proved effective merging accurate structured knowledge into the language models, it still has two limitations: (1) It\nis restricted to a ﬁxed set of named entities, and cannot be easily generated to more complex semantic components (i.e.,\neventualities) because there are enormous eventualities; (2) The key-value structure can only encode one-hop knowledge\nand thus the language models still struggle to understand complex knowledge that involves high-order inference. As a\nresult, it can only be used to model entity-based factual knowledge rather than more complex commonsense knowledge.\nIn this paper, to effectively inject complex commonsense knowledge about eventualities into pre-trained language\nrepresentation models, we propose a knowledge injection framework CoCo LM. Through carefully controlled walk\nover a large-scale eventuality knowledge graph, ASER [6], we manage to collect rich knowledge about the discourse\nrelations among eventualities (e.g., “being hungry” can cause “eat food” and “being hungry” often happens at the\nsame time as “being tired”). After that, we design a special masking strategy to help language representation models\nto view each eventuality as a whole. By doing so, we successfully inject rich eventuality knowledge into pre-trained\nlanguage representation models. As ASER is also automatically extracted from unlabeled corpus, the proposed pipeline\nis cheap and scalable. Experiments on multiple eventuality-relevant natural language understanding tasks show that\nthe proposed solution can help improve the performance of all four language models (i.e., BERT-base, BERT-large,\nRoBERTa-base, and RoBERTa-large). Extensive analysis are conducted to show the effect and contribution of all\ncomponents in CoCoLM. The main contributions of this paper are as follows:\n• We propose CoCoLM, a new contextualized language model enhanced by complex commonsense knowledge from\nhigh-order discourse relations. CoCo LM is trained to predict the whole eventuality among the sequences using a\nlarge-scale eventuality KG.\n• We introduce two auxiliary discourse tasks to help incorporate discourse-related knowledge into pre-trained language\nmodels, which complement the special eventuality masking strategy.\n• CoCoLM achieves stronger performance than the baseline LMs on multiple datasets that require the understanding of\ncomplex commonsense knowledge about eventualities.\nOur code and pre-trained models are publicly available at https://github.com/HKUST-KnowComp/CoCoLM.\n2\nA PREPRINT - JANUARY 1, 2021\nType Sequences\nTemporal The police said. Succession The father of boy be charged murder yesterday. Co-occurrence\nEleven people were arrested yesterday in raid.\nCasual They speak. Condition They have a interest. Reason they come there.\nOthers The police said. Conjunction He was taken to house. Contrast Ofﬁcer Schwarz held him.\nTable 2: Sampled eventuality sequences with different relations from ASER. Discourse relations are highlighted with pink.\n2 Methods\nThe overall framework of CoCoLM is presented in Figure 1. Given a pre-trained language model, we inject complex\ncommonsense knowledge about eventualities into the pre-trained model. Speciﬁcally, we ﬁrst generate eventuality\nsequences based on carefully controlled walks over existing eventuality knowledge graphs, and then use the sequences\nas the context to help LMs handle eventualities. Besides the original masked language model loss, we also introduce\nthe eventuality-based masking loss function and several auxiliary tasks to assist the training. As this new language\nmodel training is not task speciﬁc, the resulting language model can be easily applied to any downstream tasks via\nanother task-speciﬁc ﬁne-tuning. Details about all steps are as follows.\n2.1 Eventuality Sequence Generation\nAs aforementioned, we leverage ASER, which uses eventualities as nodes and the discourse relations as edges, as\nthe eventuality knowledge resource. ASER contains rich eventuality knowledge such as “being hungry” and “being\ntired” often happen together and people often “make a call” before they go. Interestingly, beyond the single edges,\nhigher-order connections over ASER can also reﬂect insightful eventuality knowledge. For example, “sleep” and “go”\nare not likely to happen at the same time because “sleep” can be caused by “being tired” and there exist a contrast\nconnection between “being tired” and “go”. To include the higher-order knowledge into the model, we propose to take\nthe whole graph into consideration rather than just the edges. At the same time, considering the large-scale of ASER, it\nis infeasible to input the whole graph into a graph model. Motivated by DeepWalk [7], we randomly sample paths to\nsimulate the overall graph structure.\nGiven the initial knowledge graph G= (E, R), where Eis the eventuality set and Ris the relation set, we conduct\nthe weighted random walk based on the edge weights over Gto sample eventuality paths. We denote each path as\n(E0, r0, E1, r1, ..., rl−1, El), where E means an eventuality, r a discourse edge connecting two eventualities, and l the\nnumbers of eventualities along the sequence. To convert the sampled sentence into a token list, we keep all words in\neach event as a sentence and use representative connectives for each discourse relation to connect them. As ASER is\nautomatically extracted from raw documents, it may contains noise. To minimize the inﬂuence of the noise and improve\nthe quality of sampled paths, we required the selected paths to fulﬁll the following requirements:\n1. To ﬁlter out rare eventualities, the frequency of starting eventualities has to be larger than ﬁve.\n2. Other than the relations that have the transitive property (e.g., Precedence, Result), each selected path should not\ncontain successive edges with repeated relations.\n3. To make sampled sequences more informative, we manually improve the sampling probability of selecting sub-\nsequence patterns like “Ei Condition Ej Reason Ek”. Since it has been proven that if -then rules [8] and if -then-\nbecause rules [9] are crucial for reasoning.\nImplementation details and careful analysis are presented in Section 4 and several examples are shown in the Table 2.\n2.2 Eventuality-Level Mask\nMasking strategy plays a crucial role for the training of language representation models. Besides the random token-level\nmasking strategy, many other masking strategies have been explored by previous literature such as the-whole-word\nmasking [1, 10], named entity masking [11] or text span masking [12]. Similarly, to effectively help the model view each\neventuality as an independent semantic unit, we propose the following two masking strategies: (1) Whole Eventuality\nMasking: Similar to the whole word masking or entity masking strategies, the whole eventuality masking aims to\nreduce the prior biases of eventuality tokens. For example, given an eventuality sequence “I feel sleepy because I drink\na cup of [MASK].”, BERT would easily predict “coffee” or “tea” because of the prior knowledge of “cup of” inside the\neventuality. Instead of that, masking the whole “I drink a cup of coffee” would encourage the prediction to treat each\n3\nA PREPRINT - JANUARY 1, 2021\nEventuality \nMasking \nI\nTransformer Encoder\nsleep becauseI I am hungry I have lunch\nsleep [MASK]I [MASK] I am hungry [MASK] I have lunchI am tired\nI am tired and so\nsleep becauseI I am hungry I have lunch[MASK] [MASK] [MASK] and so\n[CLS]\n[CLS]\n[CLS]\nTransformer Encoder\nConnective\nMasking \nX5 X6 X7X1\nam tiredTrue\nX4 X8 X12\n(Reason) (Conjunction) (Result)because and so\nDiscourse\nRelation Prediction  \nCo-Occurrence\nPrediction \nX1\nTrue\n[SEP] I think [SEP]\n[SEP] I think [SEP]\n[SEP] I think [SEP]\nCo-Occurrence\nPrediction \nFigure 2: Illustration of CoCoLM-(Complex commonsense pre-training stage). Given an eventuality sequence, it is either masked by\nthe whole eventuality masking (in blue) or discourse connective masking strategy (in pink). Besides the regular masked language\nmodel, the discourse relation labels are jointly predicted for masked connective tokens (onx4, x8 and x12). Co-occurrence prediction\n(on x1) is conducted for both masking strategies.\neventuality as an independent semantic unit and focus on the relations between them. For each sampled sequence, we\nrandomly mask at most one eventuality to fulﬁll the masking budget, which is typically 25% of the sequence token\nlength. (2) Discourse Connective Masking: Besides masking the eventualities, to effectively encode the discourse\ninformation, we also tried masking the discourse connectives.\nExamples of the two masking strategies are shown in Figure 2. It is worth mentioning that for each sequence,\nwe only randomly select one type of masking strategy to guarantee that enough information is kept in the left\ntokens for the prediction. The formal masking strategy is deﬁned as follows. Given a tokenized sampled sequence\nX = (x1, x2, ..., xn), after randomly passing several tokens, we pass it to a transformer encoder [13] and denote the\nresulting vector representations as x1, x2, ...xn. The training loss Lmlm can thus be deﬁned as:\nLmlm = − 1\n|M|\n∑\ni∈M\nlogP(xi|xi), (1)\nwhere M means the set of masked tokens following the aforementioned masking strategies.\n2.3 Auxiliary Tasks\nA limitation of the MLM loss is that the prediction is over the entire vocabulary, and as a result the model could not\neffectively learn the connection between eventualities and connective words. To remedy this and force the model to\nlearn the discourse knowledge, we propose to add an additional classiﬁcation layer after the last layer of transformer\nencoder and it feeds the output vector xi of connective token xi into a softmax layer over the set of discourse relation\nlabels as follows.\nP(li|xi) = softmax(xiW + b), (2)\nLrel = −\n∑\ni∈MR\nlog P(li = ˜li|xi), (3)\nwhere MR is the index set of masked discourse connective tokens (e.g.,because, and, so) in Figure 2, li is the predicted\ndiscourse relation label, and ˜li the label provided by ASER. W and b are trainable parameters.\nBesides the aforementioned discourse relations, ASER also provides the Co-Occurrence relations between eventualities,\nwhich means that two eventualities appear in the same sentence, but there is no explicit discourse relations between\nthem. Even though compared with discourse relations, the Co-Occurrence relations are less informative, we still think\nit reﬂects rich knowledge about eventualities. Motivated by this, we propose another auxiliary task to help the model to\nlearn such knowledge. Specially, given an eventuality sequenceS = (E0, r0, E1, r1, ..., rl−1, El) and an eventuality\nEc, we format the input2 as “[CLS] S [SEP] Ec [SEP]”. We set 50% of the time Ec to be the positive co-occurred\n2The starting and separation tokens are based on the base model. For example, if we use BERT as the base model, we add “[CLS]”\nand “[SEP]”, and if we use RoBERTa, we add “<s>” and “</s>”. All notations in the rest of this paragraph are based on BERT.\n4\nA PREPRINT - JANUARY 1, 2021\nFigure 3: The distribution of lengths along with relation edges for generated eventuality sequences.\neventuality with one of eventualities in the sequence while 50% of the time Ec is randomly sampled negative in ASER.\nSimilar to the next sentence prediction in the original BERT, on top of the vector representation of token [CLS], i.e.,\nxcls, we add another classiﬁcation layer to predict whether the Co-Occurrence relations appears or not. The training\nobjective Loccur for binary classiﬁcation is similar to Lrel:\nLoccur = −log P(li = ˜li, |xcls), (4)\nwhere ˜li is the true co-occurrence label (positive or negative) for the sequence.\nMerging all three losses together, we can then deﬁne the overall loss function Las:\nL= Lmlm + Lrel + Loccur. (5)\n3 Implementation Details\nIn this work, we use the released ASER-core version 3 extracted from multi-domain multi-resource corpora, which\ncontains over 27.6 million eventualities and 8.8 million relations. We follow the heuristic rules in the Sec. 2.1 to\nsample eventuality sequences for pre-training. Overall we generated 4,901,511 eventuality sequences, ranging from\none to ﬁve hops and the one-hop sequence means the direct (ﬁrst-order) edge in the ASER. We also discard some\nedges with uninformative relation types such as Co-Occurrence except those used for auxiliary tasks and down-sample\neventuality nodes with extremely high frequency such as I see. The sequence distribution over different lengths is\nshown in Figure 3.\nWe select BERT-base, BERT-large [1], RoBERTa-base, and RoBERTa-large [2] as the base language model. All models\nare implemented based with the Huggingface library [ 14]. For the continual pre-training phrase, we use the Adam\noptimizer for 10 epochs with batch size 128, learning rate 1e-5 and weight decay 0.01. Considering the relative longer\nspan of masked eventualities, we enlarge the masking proportion from 15% to 25%.\n4 Experiments\nIn this section, we conduct experiments on three widely used downstream tasks that require the correct understanding\nof complex commonsense knowledge about events:\n1. ROCStories [16] is widely used for story comprehension tasks such as Story Cloze Test. It contains 98,162\nﬁve-sentence coherent stories as the unlabeled training dataset, 1,872 four-sentence story contexts along with two\ncandidate ending sentences in the development and test datasets. We follow the dataset split for the story ending\nprediction task in [17].\n3https://github.com/HKUST-KnowComp/ASER\n5\nA PREPRINT - JANUARY 1, 2021\nDataset Type # Train # Dev # Test Example\nROCStories Narrative 1,771 100 1,871 Context: The Mills next door had a new car. The car was stolen during\nthe weekend. They came to my house and asked me if I knew anything.\nI told them I didn’t, but for some reason they suspected me.\nPositive Ending: They called the police to come to my house.\nNegative Ending: They liked me a lot after that.\nMATRES Temporal 230* 25 20 Fidel Castro {e1: invited} John Paul to {e2: come} for a reason. Label:\nBEFORE\nCOPA Causal 400 100 500 Premise: I knocked on my neighbor’s door.\nPositive Hypothesis: My neighbor invited me in.\nNegative Hypothesis: My neighbor left his house.\nTable 3: The statistics and examples for all commonsense evaluation datasets. The tasks are binary or multiple classiﬁcation problems.\nNote the dataset of MATRES is split at the article level as in the previous work [15].\n2. MATRES [18] is a pairwise event temporal ordering prediction dataset, where each event pair in one document is\nannotated with a temporal relation (Before, After, Equal, Vague). It contains 13,577 event pairs extracted from 256\nEnglish documents.\n3. COPA [19] is a binary-choice commonsense causal reasoning task, which requires models to predict which the\ncandidate hypothesis is the plausible effect/cause of the given premise. We follow the training/dev/test split in\nSuperGLUE [20].\nStatistics and examples of the three selected datasets are presented in Table 3. We implemented the experiments with\nHuggingface [14] and run experiments on eight Nvidia V100 32GB GPUs. For all experiments, we set the learning rate\nto be 1e-5, and maximize the sequence length and batch size such that they can ﬁt into GPU memory.\nFor the ROCStories task, as mentioned in [21], there is strong bias about the human-created negative endings such that\nthe model can distinguish the positive and negative endings without seeing the ﬁrst four events. Even though [21] tried\nto ﬁlter the annotations, the bias still cannot be fully relieved. As a result, to clearly show the effect of adding complex\nknowledge about events into the LM, besides the most widely used supervised setting, we also report the performance\nof a debiased setting, where the model randomly selects events from other stories as the negative ending during the\ntraining. The debiased setting is indicated with “D”. Following previous works, we report accuracy for the ROCStories,\nMATRES and COPA tasks. For MATRES, we also report the metric of F1 by considering the task as general relation\nextraction and treating the label of vague as no relation[22]. All models are trained until converge on the training set\nand the best model on the dev set is selected to be evaluated4.\n5 Experimental Results\nThe experimental results are presented in Table 4, from which we can see that CoCoLM consistently outperform all the\nbaselines on all three different commonsense tasks, especially on the de-biased setting of ROCStories. Besides that we\ncan make the following observations:\n1. For the ROCStories dataset, Compared with the original supervised setting, the de-biased setting is more\nchallenging for all models, which helps verify our assumptions that previous models beneﬁt from the bias.\n2. The improvement of our model is more signiﬁcant on ROCStories and COPA rather than MATRES, which\nis mainly because events in MATRES are associated with the context and many temporal relations have to\nbe inferred with the local context rather than memorized by the language model. As a comparison, Both\nthe ROCStories and COPA do not have any extra context, and thus require the pre-trained LM to know the\nessential knowledge to solve those problems.\nIn the rest of this section, we conduct extensive experiments and case studies to demonstrate the contribution of different\ncomponents. For the experiment efﬁciency, in all analysis experiments, we use BERT-large as the base language model\nand ROCStories as the evaluation dataset.\n4The only exception is the supervised setting of ROCStories. As only the development and testing set are provided with the\nhuman-created negative endings, following previous works, we use development as the training set and directly test the converged\nmodel on the testing set.\n6\nA PREPRINT - JANUARY 1, 2021\nROCStories MATRES COPA\nModel Accuracy Accuracy (D) Accuracy F1 Accuracy\nBERT-base 52.9 52.9 71.5 77.2 69.8\nCoCoLM (BERT-base) 84.2 65.2 72.8 77.8 73.8\nBERT-large 88.9 69.1 73.5 78.9 70.6\nCoCoLM (BERT-large) 91.9 71.2 73.9 79.2 75.8\nRoBERTa-base 93.3 73.2 74.0 79.2 85.4\nCoCoLM (RoBERTa-base) 94.1 75.2 74.2 79.8 86.2\nRoBERTa-large 97.4 88.1 75.2 81.0 91.6\nCoCoLM (RoBERTa-large) 97.7 89.2 75.3 81.6 91.8\nTable 4: All three commonsense task evaluation results. The best performance is highlighted in bold. Note our proposed CoCoLM\nwith RoBERTa-large achieves the best scores over all the tasks.\n5.1 Ablation Study\nFrom the results in Table 5, we can see that all components contribute to the ﬁnal success of our model, especially\nthe Co-Occurrence relation. This result proves our previous assumption that even though compared other discourse\nrelations (e.g., Before and Cause), the Co-occurrence relations has relatively weaker semantic, it still can help models\nto better understand events due to its large scale.\nMethod Accuracy ∆ Accuracy (D) ∆ (D)\nCoCoLM (BERT-large) 91.9 - 71.2 -\n- Co-Occurrence Loss 91.3 -0.6 70.4 -0.8\n- Eventuality Mask 91.1 -0.8 70.2 -1.0\n- Relation Loss 90.5 -1.4 69.6 -1.6\n- Co-Occurrence Relation 90.3 -1.6 69.3 -1.9\nTable 5: Ablation study on ROCStories test set by removing different model components.\n5.2 Effect of Different Event Knowledge Resources\nBesides ASER [6], another important event knowledge resource is ATOMIC [8], which is a crowdsourced commonsense\nknowledge graph that contains nine human-deﬁned relations between daily events. As ATOMIC is a bipolar graph and\nthus we could not ﬁnd a multi-hop path, we did not select it as the event knowledge resource in our ﬁnal model. In\nthis experiment, we report the performance of using ATOMIC as the event knowledge resource. As ATOMIC does not\ncontain the multi-hop knowledge, we also add ASER (single-hop), which only uses the single-hop knowledge in ASER,\nas another baseline approach.\nThe overall results are shown in Table 6, from which we can see that there is still a notable gap between ATOMIC and\nour ﬁnal model. At the same time, we can also see that ATOMIC can outperform the single-hop version of ASER.\nBased on these two observations, we can make the following conclusions: (1) Compared with ASER, ATOMIC is\ncleaner because it is created with human annotations; (2) Multi-hop knowledge is crucial for LMs to understand events.\nWe leave how to combine ATOMIC and ASER to get more high-quality multi-hop event knowledge as our future work.\nMethod Accuracy ∆ Accuracy (D) ∆ (D)\nASER (Multi-hop) 91.9 - 71.2 -\nATOMIC (Single-hop) 88.2 -3.7 68.2 - 3.0\nASER (Single-hop) 85.4 -6.5 67.5 -3.7\nTable 6: Effect of different event knowledge resources.\n7\nA PREPRINT - JANUARY 1, 2021\nDataset Example BERT [MASK] CoCoLM [MASK]\nROCStories Context: Ed made beef jerky for a living. He ran the business\nout of his garage. One day he woke up and noticed his garage\njarred open. He looked inside and noticed everything in disarray\nPositive Ending: Ed was delighted to see this.\nNegative Ending: Ed was shocked called the police for an inves-\ntigation.\nContext + [MASK] +\nEnding:\nP: when, then, while\nN: and, but, so\nContext + [MASK] +\nEnding:\nP: so, hence, therefore\nN: or, and, though\nMATRES The last surviving member of the team which ﬁrst conquered\nEverest in 1953 has {e1: died} in a Derbyshire nursing home.\nGeorge Lowe, 89, {e2: died} in Ripley on Wednesday after a\nlong-term illness, with his wife Mary by his side.\nS1, +[MASK] + S2:\nand, sir, Dr\nS1, +[MASK] + S2:\nthen, afterwards, till\nCOPA Premise: The girl found a bug in her cereal.\nPositive Hypothesis: She lost her appetite.\nNegative Hypothesis: She poured milk in the bowl.\nPre+ [MASK] + Hypo:\nP: then, but, and\nN: then, next, so\nPre + [MASK] + Hypo:\nP: so, therefore, thus.\nN: but, instead, and.\nTable 7: Case study from three evaluation datasets. Connectives in blue are predicted by the BERT-large model and ones in pink are\npredicted by CoCoLM(BERT-large). “P” and “N” are used to represent the positive and negative candidates, respectively.\n5.3 Case Study\nWe present the case study in Table 7 and use a probing approach to further investigate the reason for our success.\nSimilar to [3], we put a “[MASK]” token between two events and try to ask the model to predict the connective. Take\nthe case from COPA dataset as an example, connectives predicted by CoCoLM clearly show the effect relation between\ntwo events. However predictions from the baseline models reveal weaker (temporal, conjunction) or wrong (contrast)\nrelations. Similar observations could be drawn from another two datasets. These observations show that compared with\nthe original LM, CoCoLM manages to memorize richer event knowledge.\n6 Related Work\nUnderstanding Events.It is important to represent and learn the commonsense knowledge for deeply understanding\nthe causality and correlation between events. Recently various kinds of tasks requiring multiple dimensional event\nknowledge are proposed such as story ending prediction [ 16], event temporal ordering prediction [ 23], and event\ncasual reasoning [19]. Prior studies have incorporated external commonsense knowledge from ConceptNet [24] and\nATOMIC [8] for solving event representation [25], story generation tasks [26], event KG completion [27]. However,\ntheir event-level knowledge is sparse and incomplete due to the human-annotated acquisition, which thus limits the\nmodel capacity, especially when injecting into pre-trained LMs. [6] builds a large-scale eventuality knowledge graph,\nASER, by specifying eventuality relations mined from discourse connectives. It explicitly provides structural high-order\ndiscourse information between events spanning from temporal, casual to co-occurred relations, which has been proven\nto be transferable to human-deﬁned commonsense [28]. In this work, we aim at making full use of multi-dimensional\nhigh-order event knowledge in the ASER to help pre-trained LMs understand events.\nInjecting Knowledge into LMs.Though [3] shows that pre-trained LMs store factual relational knowledge without ﬁne-\ntuning, still, LMs can not handle knowledge-intensive tasks such as open-domain question answering or commonsense\nreasoning. Previous works explore different ways to inject various knowledge into pre-trained LMs for downstream\ntask performance improvement. They mainly differ from the knowledge resources, masking strategies, and training\nobjectives. From the perspective of knowledge resources, entity-centric knowledge graphs are infused into LMs in the\nform of linked entities [29, 30, 31] or triplets [32, 33, 34]. Besides that, linguistic knowledge (e.g.,synonym/hypernym\nrelations [35], word-supersense knowledge [36], dependency parsing [34], and constituent parsing [ 37]) also plays\na critical role to improve the pre-trained LMs. Last but not least, domain-speciﬁc knowledge is also customized to\nimprove relevant tasks such as mined sentiment word [ 38], event temporal patterns [ 39], and numerical reasoning\ndata [40]. In this work, we aim at injecting complex commonsense into pre-trained LMs with two signiﬁcant difference\nagainst previous works: (1) we use the event rather than tokens as the semantic unit, and propose to use an event-based\nmasking strategy as well as two auxiliary tasks to help LMs understand events; (2) We ﬁrst leverage the random walk\nprocess on a large-scale knowledge graph to include multi-hop knowledge.\n8\nA PREPRINT - JANUARY 1, 2021\n7 Conclusion\nIn this work, we aim at helping pre-trained language models understand complex commonsense about events. Speciﬁ-\ncally, we ﬁrst conduct the random walk over a large-scale eventuality-based knowledge graph to collect multi-hop event\nknowledge and then inject the knowledge into the pre-trained LMs with an event-based mask strategy as well as two\nauxiliary tasks. Experiments on three downstream tasks as well as extensive analysis demonstrate the effectiveness of\nthe proposed model. As our approach is a general solution, we believe that it can also be helpful for other tasks that\nrequire complex commonsense about events. Both the code and pre-trained model CoCoLM are released.\nReferences\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of NAACL, pages 4171–4186, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics.\n[2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[3] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander H. Miller. Language models as knowledge bases? In Proceedings of EMNLP-IJCNLP 2019, pages\n2463–2473, 2019.\n[4] Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. Facts as experts: Adaptable and interpretable\nneural memory over symbolic knowledge. CoRR, abs/2007.00849, 2020.\n[5] Ray Jackendoff. Semantic structures, volume 18. MIT press, 1992.\n[6] Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. Aser: A large-scale eventuality\nknowledge graph. In Proceedings of The Web Conference 2020, pages 201–211, 2020.\n[7] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In The\n20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York,\nNY, USA - August 24 - 27, 2014, pages 701–710, 2014.\n[8] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan\nRoof, Noah A Smith, and Yejin Choi. Atomic: An atlas of machine commonsense for if-then reasoning. In\nProceedings of the AAAI, volume 33, pages 3027–3035, 2019.\n[9] Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, and Tom Mitchell.\nConversational neuro-symbolic commonsense reasoning. arXiv preprint arXiv:2006.10022, 2020.\n[10] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-training with\nwhole word masking for chinese bert. arXiv preprint arXiv:1906.08101, 2019.\n[11] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\nand Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223,\n2019.\n[12] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving\npre-training by representing and predicting spans. TACL, 8:64–77, 2020.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\n[14] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine\nJernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander\nRush. Transformers: State-of-the-art natural language processing. In Proceedings of EMNLP, pages 38–45,\nOnline, 2020. Association for Computational Linguistics.\n[15] Miguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani, Yogarshi Vyas, Jie Ma, Parminder Bhatia,\nKathleen McKeown, and Yaser Al-Onaizan. Severing the edge between before and after: Neural architectures for\ntemporal ordering of events. In Proceedings of the EMNLP 2020, 2020.\n[16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense\nstories. In Proceedings of the NAACL, pages 839–849, San Diego, California, June 2016.\n9\nA PREPRINT - JANUARY 1, 2021\n[17] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending prediction by transferable bert. In Proceedings of IJCAI,\npages 1800–1806. AAAI Press, 2019.\n[18] Qiang Ning, Hao Wu, and Dan Roth. A multi-axis annotation scheme for event temporal relations. In Proceedings\nof the ACL, pages 1318–1328, Melbourne, Australia, July 2018.\n[19] Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible\nalternatives: An evaluation of commonsense causal reasoning. In Proceedings of SemEval 2012, pages 394–398,\nMontréal, Canada, 7-8 June 2012.\n[20] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In\nAdvances in Neural Information Processing Systems, pages 3266–3280, 2019.\n[21] Rishi Sharma, James Allen, Omid Bakhshandeh, and Nasrin Mostafazadeh. Tackling the story ending biases in\nthe story cloze test. In Proceedings of ACL 2018, pages 752–757, 2018.\n[22] Qiang Ning, Sanjay Subramanian, and Dan Roth. An improved neural baseline for temporal relation extraction.\nIn Proceedings of the EMNLP-IJCNLP, pages 6203–6209, Hong Kong, China, November 2019. Association for\nComputational Linguistics.\n[23] Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. Joint reasoning for temporal and causal relations. In Proceedings\nof ACL, pages 2278–2288, Melbourne, Australia, July 2018.\n[24] Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general\nknowledge. pages 4444–4451, 2017.\n[25] Xiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, and Junwen Duan. Event representation learning enhanced with\nexternal commonsense knowledge. In Proceedings of EMNLP-IJCNLP, pages 4896–4905, 2019.\n[26] Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. A knowledge-enhanced pretraining model\nfor commonsense story generation. TACL, 8:93–108, 2020.\n[27] Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. Comet:\nCommonsense transformers for automatic knowledge graph construction. In Proceedings of ACL, pages 4762–\n4779, 2019.\n[28] Hongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. Transomcs: From linguistic graphs to\ncommonsense knowledge. In Proceedings of IJCAI, 2020.\n[29] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language\nrepresentation with informative entities. In Proceedings of ACL, pages 1441–1451, 2019.\n[30] Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith.\nKnowledge enhanced contextual word representations. In Proceedings of the EMNLP-IJCNLP, pages 43–54,\n2019.\n[31] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained encyclopedia: Weakly\nsupervised knowledge-pretrained language model. In International Conference on Learning Representations,\n2020.\n[32] Liang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv preprint\narXiv:1909.03193, 2019.\n[33] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-BERT: enabling\nlanguage representation with knowledge graph. In AAAI February 7-12, 2020, pages 2901–2908, 2020.\n[34] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, et al.\nK-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808, 2020.\n[35] Anne Lauscher, Ivan Vuli´c, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavaš. Specializing unsupervised\npretraining models for word-level semantic similarity. In Proceedings of COLING, pages 1371–1383, Barcelona,\nSpain (Online), December 2020.\n[36] Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and\nYoav Shoham. SenseBERT: Driving some sense into BERT. In Proceedings of ACL, pages 4656–4667, Online,\nJuly 2020. Association for Computational Linguistics.\n[37] Junru Zhou, Zhuosheng Zhang, and Hai Zhao. Limit-bert: Linguistic informed multi-task bert. arXiv preprint\narXiv:1910.14296, 2019.\n10\nA PREPRINT - JANUARY 1, 2021\n[38] Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, and Feng Wu. SKEP: Sentiment\nknowledge enhanced pre-training for sentiment analysis. In Proceedings of ACL, pages 4067–4076, Online, July\n2020. Association for Computational Linguistics.\n[39] Ben Zhou, Qiang Ning, Daniel Khashabi, and D. Roth. Temporal common sense acquisition with minimal\nsupervision. In Proceedings of ACL, 2020.\n[40] Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models. In\nProceedings of ACL, 2020.\n11",
  "topic": "Commonsense knowledge",
  "concepts": [
    {
      "name": "Commonsense knowledge",
      "score": 0.8546769022941589
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.7400285005569458
    },
    {
      "name": "Computer science",
      "score": 0.7316650748252869
    },
    {
      "name": "Artificial intelligence",
      "score": 0.558208703994751
    },
    {
      "name": "Natural language processing",
      "score": 0.5534899830818176
    },
    {
      "name": "Language model",
      "score": 0.5312058329582214
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.44267672300338745
    },
    {
      "name": "Task (project management)",
      "score": 0.42395448684692383
    },
    {
      "name": "Representation (politics)",
      "score": 0.41235336661338806
    },
    {
      "name": "Focus (optics)",
      "score": 0.41168704628944397
    },
    {
      "name": "Cognitive science",
      "score": 0.36568892002105713
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.24337804317474365
    },
    {
      "name": "Epistemology",
      "score": 0.17090696096420288
    },
    {
      "name": "Psychology",
      "score": 0.12276917695999146
    },
    {
      "name": "Philosophy",
      "score": 0.09426289796829224
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}