{
  "title": "Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs",
  "url": "https://openalex.org/W4375870255",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4288366184",
      "name": "Wadhwa, Somin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287429648",
      "name": "DeYoung, Jay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4292356362",
      "name": "Nye, Benjamin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287430662",
      "name": "Amir, Silvio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224585139",
      "name": "Wallace, Byron C.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3105063288",
    "https://openalex.org/W2341078838",
    "https://openalex.org/W3158862932",
    "https://openalex.org/W2963029083",
    "https://openalex.org/W4366549247",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W3004071925",
    "https://openalex.org/W3037234229",
    "https://openalex.org/W4407827074",
    "https://openalex.org/W3210323829",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2885925344",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963021258",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2058316166",
    "https://openalex.org/W3214342214",
    "https://openalex.org/W2997876626",
    "https://openalex.org/W3027335960",
    "https://openalex.org/W3214646361",
    "https://openalex.org/W2147994374",
    "https://openalex.org/W2963997908",
    "https://openalex.org/W2963404965",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W2020959315",
    "https://openalex.org/W3155073135",
    "https://openalex.org/W2808142148",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3104950207",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2146668368",
    "https://openalex.org/W2974004142"
  ],
  "abstract": "Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\\sim$20 point absolute F1 score) gains over the previous SOTA. We perform ablations and error analyses to assess aspects that contribute to model performance, and to highlight potential directions for further improvements. We apply our model to a collection of published RCTs through mid-2022, and release a searchable database of structured findings: http://ico-relations.ebm-nlp.com",
  "full_text": "Proceedings of Machine Learning Research 219:1–21, 2023 Machine Learning for Healthcare\nJointly Extracting Interventions, Outcomes, and Findings\nfrom RCT Reports with LLMs\nSomin Wadhwa wadhwa.s@northeastern.edu\nKhoury College of Computer Sciences\nNortheastern University\nBoston, MA, USA\nJay DeYoung deyoung.j@northeastern.edu\nKhoury College of Computer Sciences\nNortheastern University\nBoston, MA, USA\nBenjamin Nye bnye@coloradocollege.edu\nMathematics & Computer Science\nColorado College\nColorado Springs, CO, USA\nSilvio Amir s.amir@northeastern.edu\nKhoury College of Computer Sciences\nNortheastern University\nBoston, MA, USA\nByron C. Wallace b.wallace@northeastern.edu\nKhoury College of Computer Sciences\nNortheastern University\nBoston, MA, USA\nAbstract\nResults from Randomized Controlled Trials (RCTs) establish the comparative effective-\nness of interventions, and are in turn critical inputs for evidence-based care. However,\nresults from RCTs are presented in (often unstructured) natural language articles describ-\ning the design, execution, and outcomes of trials; clinicians must manually extract findings\npertaining to interventions and outcomes of interest from such articles. This onerous man-\nual process has motivated work on (semi-)automating extraction of structured evidence\nfrom trial reports. In this work we propose and evaluate a text-to-text model built on\ninstruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Out-\ncomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated\nresults reported. Manual (expert) and automated evaluations indicate that framing evi-\ndence extraction as a conditional generation task and fine-tuning LLMs for this purpose\nrealizes considerable (∼20 point absolute F1 score) gains over the previous SOTA. We per-\nform ablations and error analyses to assess aspects that contribute to model performance,\nand to highlight potential directions for further improvements. We apply our model to\na collection of published RCTs through mid-2022, and release a searchable database of\nstructured findings: http://ico-relations.ebm-nlp.com.\n© 2023 S. Wadhwa, J. DeYoung, B. Nye, S. Amir & B.C. Wallace.\narXiv:2305.03642v3  [cs.CL]  18 Jul 2023\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nPatients receiving aspirin \nexperienced headaches with \ncomparable duration but \nsigniﬁcantly lower reported \npain compared to those \nreceiving placebo.\nFine-tuned LLM\n<I> aspirin <C> placebo <O> duration of \nheadache <F> no signiﬁcant diﬀerence  \n<I> aspirin <C> placebo <O> pain \n<F> signiﬁcantly reduced  \nInput\nStructured evidence (linearized)\nModel Output\nRCT article text\nFigure 1: We fine-tune a Large Language Model (LLM) to map from free-text descriptions\nof clinical trials to structured representations of findings.\n1. Introduction\nRobust medical evidence concerning the comparative effectiveness of treatments is pri-\nmarily disseminated in published free-text articles that report outcomes from randomized\ncontrolled trials (RCTs). Such trial results are critical inputs for practicing Evidence-based\nmedicine (EBM; Sackett 1997), which seeks to inform patient care using the totality of\nrelevant findings. Trial results are also potentially important for augmenting clinical pre-\ndictions (Naik et al., 2022), and for calibrating trust in treatment suggestions offered by AI\nsupport systems (Yang et al., 2023), which ought to agree with the established evidence.\nA challenge to making use of all available evidence is that findings from trials are dis-\nseminated via unstructured published articles. Researchers and healthcare providers must\ntrawl through these to extract findings relevant to their clinical question(s). This problem\nhas been exacerbated by the rapid production of new evidence: A now outdated estimate\nsuggests that 75 trial reports are published every single day (Bastian et al., 2010); more\nrecent estimates put this number at ∼140 trial reports per day (Marshall et al., 2020).\nTo allow practitioners to draw upon newly published evidence as it accumulates, we need\ntools that make navigating findings more efficient. This has motivated work on Natural\nLanguage Processing (NLP) methods to semi-automate aspects of data extraction from\nclinical trial reports (Kang et al. 2021; Kiritchenko et al. 2010; Wallace et al. 2016; Nye\net al. 2022, inter alia ). In this work we capitalize on and extend recent advances in NLP,\nspecifically instruction-tuned LLM capabilities (Chung et al., 2022), to perform end-to-\nend structured evidence extraction from free-text (Figure 1). We achieve state-of-the-art\n(SOTA) performance on this challenging task: The model we introduce yields a ∼20 point\nabsolute gain in F1 score over the prior SOTA approach. We ablate model components\nto assess their contributions. We also release model weights, and a database of structured\nfindings inferred by our model over a comprehensive dataset of articles describing RCTs.\nGeneralizable Insights about Machine Learning in the Context of Healthcare\nWith respect to healthcare, this work makes significant progress on the important practical\nproblem of structured evidence extraction from published articles describing RCTs. The\noutputs of this system may aid evidence synthesis, and might also serve as inputs to other\n2\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nmachine learning models in healthcare which could benefit from conditioning on robust\nevidence. Beyond this, the need for data extraction from free-text (e.g., clinical notes) is\nwidespread in healthcare: Improved extraction methods have the potential to ultimately\nallow clinicians to focus on providing patient care instead of navigating unstructured data.\nIn terms of machine learning, we introduce and evaluate a method for training LLMs to\nperform a complex instance of relation extraction, a long-standing problem in ML (Ireson\net al., 2005). To our knowledge, this is one of the first efforts to evaluate LLMs for medical\nrelation extraction; we find that they outperform existing systems for this task by a large\nmargin. As an additional contribution which may be of interest to the broader machine\nlearning community, our ablations indicate that including evidence spans in extraction\ntargets is an an important design decision—this complements recent developments inducing\nLLMs to provide free-text “rationales” for their outputs (Wei et al., 2022), and may have\nimplications for those working with LLMs for relation extraction going forward.\n2. Related Work\nIn this work we develop and evaluate methods using LLMs to extract results from clinical\ntrial reports. Information and Relation Extraction (RE), generally, are well established\nsub-fields within NLP (Cowie and Lehnert, 1996), and we do not attempt to provide a\ngeneral survey here. Instead, we contextualize our work by reviewing closely related efforts\nthat focus on: (i) Information extraction from biomedical/clinical texts (Section 2.1); (ii)\nModels for jointly identifying entities and inferring relations between them (Section 2.2);\nand (iii) Recent approaches that treat RE as a text-to-text problem, a strategy that we\nadopt here (Section 2.3).\n2.1. Information Extraction from Biomedical Literature and Clinical Text\nA line of prior work in NLP attempts to extract relevant Populations, Interventions, Com-\nparators and Outcomes (PICO elements) from clinical texts (Kim et al., 2011). Nye et al.\n(2018) collected a corpus of 5,000 annotated RCT abstracts and introduced novel NLP\ntasks aiding evidence-based medicine. Lee and Sun (2019) highlighted important aspects\nof PICO human-annotations to refine datasets by adopting a relaxed agreement schemes\nfor human annotations of PICO. Jin and Szolovits (2018) introduced baselines in detecting\nPICO elements at the sentence level using LSTMs. Schmidt et al. (2020) proposed framing\nPICO extraction as a question-answering task and subsequently using transformer models,\nincluding SciBERT (Beltagy et al., 2019) — a masked language model pretrained on large-\nscale scientific data. These efforts either pre-dated Transformers, or used small encoder\nbackbones, i.e., BERT (Devlin et al., 2018), rather than the generative models we use here.\nElsewhere, Lehman et al. (2019) introduced the evidence inference dataset which en-\ntailed inferring which medical treatments work with respect to a given ICO-set of interest.\nUsing this dataset as a starting point, Nye et al. (2022) considered the end-to-end task of\nextracting PICO elements and inferring results (as opposed to performing inference for a\ngiven ICO triplet). They proposed an extractive entity extraction-linking-inference (ELI)\nsequential approach for this challenging task, and showed that it yielded results superior to\nstandard joint architectures for relation extraction (Wadden et al., 2019). We improve upon\n3\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nthese earlier efforts by introducing an end-to-end generative model for the task of medical\nevidence inference.\n2.2. Jointly Extracting Entities and their Relations\nEarly work in RE used pipeline approaches comprising separate models to, first, extract\nentities from a span of text, and then infer relations between those entities (if any). More\nrecently, researchers have introduced joint extraction models since they tend to reduce error\npropagation and can capitalize on the connections between entities and their relations (Wang\nand Lu, 2020). Traditionally, such joint extraction methods principally worked by predicting\n“BILOU” tags (Beginning, Inside, Last, Outside, and Unit) for tokens in the input (Bekoulis\net al., 2018b,a; Miwa and Bansal, 2016; Zheng et al., 2017; Verga et al., 2018). Span-based\napproaches extend these methods by constructing spans of tokens and then labeling these\nwith respect to specific entity types, which enables processing of overlapping entities (Eberts\nand Ulges, 2019; Wadden et al., 2019) .\n2.3. Generative Relation Extraction\nMost earlier methods for identifying entities and extracting relations in free text trained\nmodels with a joint objective (Eberts and Ulges, 2021; Wang and Lu, 2020). The recent rise\nin (very) large language models (LLMs) (Brown et al., 2020; Chung et al., 2022) has moti-\nvated research into using these models for structured prediction tasks such as named entity\nrecognition and RE (Nayak and Ng, 2019; Paolini et al., 2021; Huguet Cabot and Navigli,\n2021). This usually entails linearizing—that is, encoding into strings—the structured in-\nformation and then tasking models with generating linearized target relations conditioned\non corresponding inputs.\nBuilding on these efforts, we propose to train and evaluate models to conditionally\ngenerate ICO spans, findings regarding the reported comparative effectiveness of the cor-\nresponding intervention compared to the comparator for the outcome in question, and\nsupporting textual evidence. Specifically, we fine-tune an LLM to generate sets of linearized\noutputs (tuples) containing all the entities, relations, and supporting evidence from a given\ninput RCT abstract (Figure 3).\n3. Methods\n3.1. End-to-End Evidence Inference\nThe task of clinical evidence inference comprises two sub-tasks: (i) Extraction of sets of\nrelevant medical elements, i.e. ICO triplets; and (ii) Inference regarding the effect of the\nprimary intervention on the outcome (i.e., significant increase, significant decrease, no sig-\nnificant effect), given the available evidence. These two subtasks can be seen as specialized\ninstances of entity tagging and relation extraction, respectively. Recent work on clinical\nevidence inference has adopted a sequential (pipeline) approach in which ICO extraction\nis treated as a sequence tagging step, and then a separate inference module processes the\ntagged entities (Nye et al., 2022). This specialized approach outperformed model variants\nthat attempted to jointly perform the task. However, prior methods for joint extraction\nand inference pre-dated the modern LLMs which are the current dominant paradigm in\n4\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nAbstract\n…\n…\nLarge Language Model\nCOMP\nCOMP\nIG on average lost more weight \n(p=0.027), reduced BMI (p=0.04), and \nreduced in DRS (p=0.011) compared \nto NIG at week 16. At the 12-week \nfollow-up period, those in IG plus \ndeposit subgroup had twice the odds \n(OR=2.2, p=0.042) and those in the \nstandard IG had three times the odds \nof achieving weight loss goals than \nNIG; those in the IG plus deposit \ngroup reduced DRS by 0.4 (p=0.045).\nExtract Link Infer\nOUTINT\nCOMP\nOUT  Evidence\nINT\nINT Increased\nOUT Compared to\nCOMP\nStructured Evidence\nINT\nOUT\nEvidence Span\nInference \nLabel\nFigure 2: We propose instructional fine-tuning a large language model (top) using stan-\ndard supervision to elicit evidence within generated ICO tuples. This approach\nyields substantial improvements over existing joint extraction approaches (bot-\ntom) where the entire task is decomposed into different independent phases.\nNLP. Here we adopt such models, and treat the task of end-to-end evidence inference as a\nconditional language generation task (Figure 2).\nOur targets are linearized strings comprising multiple tuples, each containing the ele-\nments (Intervention, Comparator, Outcome, Evidence, Inference label), extracted directly\nfrom an input abstract describing a RCT. Formally, given a RCT abstract C, we model the\nprobability of generating a linearized string y of length T containing N tuples (separated\nby special tokens in the linearized forms), conditioned on C:\npLM(y|C) =\nTY\nt=1\np(yt|C, y<t)\nThis is the standard (conditional) language modeling objective, and we optimize for per\ntoken cross-entropy loss. During training, we “teacher force”, i.e., condition production of\ntarget token yt on the reference sequence y<t and C. At test time, the model iteratively\nconditions on its own outputs (we use greedy decoding).\nThe number of tuples associated with inputs is variable; language model flexibly models\nthis by allowing the model to produce a specialEOS token after enumerating all tuples. Note,\nhowever, that the model is unconstrained, and so can—and sometimes does, as we discus-\nsion in Section 4.2—produce invalid outputs (i.e., which do not conform to the linearized\nstructured we assume).\nFigure 3 provides an illustrative example where the abstract comprises two unique ref-\nerence tuples:\n(zinc sulfate capsules, placebo, warts, warts resolved in 68% of the\npatients in treatment group and 64% of the patients in placebo group, no\nsignificant difference)\n(zinc sulfate capsules, placebo, recurrence of warts, three patients in\ntreatment group and six patients in placebo group had a recurrence of warts\n(p=.19), no significant difference)\n5\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nBACKGROUND: Cutaneous warts are caused by a small group of speciﬁc types \nof human papillomaviruses. Cryotherapy is a highly eﬀective treatment for \npatients with viral warts; however, it is a painful method and usually requires \nseveral treatment sessions. Zinc is a trace element with many proven eﬀects on \nthe immune system.\nOBJECTIVE: Our aim was to assess the eﬃcacy and safety of oral zinc sulfate in \nthe treatment and recurrence rate of common warts.\nMETHODS: Eighty-three patients with common warts participated in this double-\nblind, randomized, placebo-controlled trial. In both groups, three sessions of \nliquid nitrogen cryotherapy were performed for up to 2 months with 3-week \nintervals. The treatment group (n = 45) received oral zinc sulfate capsules in a \ndose of 10 mg/kg per day up to 600 mg day. The control group (n = 38) was \nprovided with placebo of similar appearance. Treatment continued for 2 months \nand the follow-up period lasted up to 6 months.\nRESULTS: Warts completely resolved in 26 patients in the treatment group \n(68.4%) and 23 patients in the placebo group (63.9%; p = .68). The remaining \nthree in the placebo group did not report complete resolutions but substantive \nimprovements in condition. Three patients (7.9%) in the treatment group and six \npatients (16.6%) in the placebo group has a recurrence of the warts (p = .19).\nCONCLUSION: According to our study, the addition of zinc to cryotherapy was \nnot beneﬁcial in the treatment of patients with common warts nor did it prevent \nrecurrences.\nOral zin sulfate capsules in a dose of 10 \nmg/kg per day up to 600 mg day\nRecurrence\nCutaneous warts\nIntervention\nOutcome 1\nOutcome 2\nWarts completely resolved in 26 patients in the \ntreatment group (68.4%) and 23 patients in the \nplacebo group (63.9%; p = .68)\nComparator\nPlacebo\nEvidence\nEﬀect No signiﬁcant diﬀerence\nThree patients (7.9%) in the treatment group and six \npatients (16.6%) in the placebo group has a \nrecurrence of the warts (p = .19).\nEvidence\nEﬀect No signiﬁcant diﬀerence\nFigure 3: An illustration of the full evidence inference task. An end-to-end model is ex-\npected to extract all ICOs for which results were reported (highlighted here in\npink, green, and orange) in an abstract describing an RCT, and infer a label\n(significant increase, significant decrease, no significant difference ) based on the\nrelevant evidence snippets which are also to be output (underlined here).\n3.2. Data\nWe derived the data we use for training from the Evidence Inference dataset (Lehman et al.,\n2019; DeYoung et al., 2020). This comprises articles describing RCTs annotated by medical\ndoctors.1 An instance in this dataset comprises an abstract annotated with five elements:\nAn ICO triplet, alabel that indicates the directionality of a reported effect of the intervention\nfor the given outcome relative to the comparator (i.e., categorizing that the intervention\nyielded statistically significant increase, decrease, no effect with respect to the outcome),\nand an evidence snippet. The latter is an excerpt from the abstract providing support for a\nparticular label. This may be viewed as an explanation or “rationale”. Together, these five\nelements form our targets. Table 1 provides basic data statistics for our training, validation,\nand test sets.\nEvaluation Data To get an accurate assessment of model performance, Nye et al. (2022)\nalso collected exhaustive manual annotations from medical experts for 160 RCT abstracts.\nOwing to the inherent noise in distantly-supervised training lables, we observed that human\nannotators often identify substantially more tuples per abstract — 4 .97 tuples per abstract\nin the validation set, and 4 .01 in the test set, as opposed to 2 .76 in the (non-exhaustive)\ntraining set (Table 1). We provide more detailed examples of this phenomenon in our error\nanalysis in Section 4.2.\n1. Although the full dataset contains full-text RCT reports, here use use an abstract-only subset.\n6\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nTrain Dev Test\nAbstracts 1,964 (1.00) 46 (1.00) 89 (1.00)\nTotal ICO Tuples 5,430 (2.76) 229 (4.97) 357 (4.01)\nUnique ICO Triplets 4,951 (2.52) 224 (4.86) 351 (3.94)\nTable 1: Dataset statistics. We report the number of abstracts and the number of relations\nper abstract (denoted parenthetically). Development and test set statistics differ\nfrom their source (Nye et al., 2022) as we omit documents with no annotated\nrelations.\nFull Inference End to End Precision Recall F-1\nBRAN (Verga et al., 2018) 0.05 0.41 0.08\nDyGIE++ (Wadden et al., 2019) 0.24 0.13 0.17\nELI (Nye et al., 2022) 0.33 0.31 0.32\n(end-to-end generation of ICO triplets with labels and supporting evidence)\nBART (Lewis et al., 2020) 0.38 0.33 0.35\nT5-base (Raffel et al., 2020) 0.56 0.35 0.43\nFlan-T5-base (Chung et al., 2022) 0.69 0.43 0.53\nFlan-T5-large 0.75 0.48 0.59\nFlan-T5-large (without evidence span extraction) 0.49 0.36 0.41\nTable 2: End-to-end relation extraction results, compare to Nye et al. (2022) Table 2a\n3.3. Experimental Setup\nWe performed all of our experiments on a single NVIDIA Quadro RTX 8000 GPU. We used\nthe Huggingface library (v4.26.1; Wolf et al. 2020) and publicly available checkpoints. 2 of\nthe language models we used in our experiments Our best performing model was trained\nfor 8 epochs with a learning rate of 1 e − 6, batch size of 2 (for both training and evalu-\nation), with a maximum input length of 1024, and maximum output length of 512. For\nhyperparameter tuning, we only varied the learning rate, and max epochs. The remaining\nhyperparameters were left to their default values. We used the Adam optimizer without\ngradient accumulation or gradient checkpointing.\n4. Results\nWe perform both an end-to-end evaluation (Table 2) and ablate performance over ICO-\ntriplet extractions only (Table 3), maintaining comparability to existing work (Nye et al.,\n2022). Section 4.1 contains details of our manual evaluation, and Section 4.2 a detailed\nerror analysis of model performance.\n2. https://huggingface.co/docs/transformers/model_doc/flan-t5\n7\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nICO-Triplet Extraction Precision Recall F-1\nDyGIE++ (Wadden et al., 2019) 0.45 0.47 0.46\nELI (Nye et al., 2022) 0.46 0.69 0.55\n(end-to-end generation of ICO-triplets)\nT5-base (Raffel et al., 2020) 0.68 0.62 0.65\nFlan-T5-base (Chung et al., 2022) 0.78 0.68 0.73\nFlan-T5-large 0.85 0.74 0.79\nTable 3: ICO-Triplet Ablation, compare to Nye et al. (2022) Table 2b (entity extraction)\n4.1. Evaluation\nOpen-ended free text generation poses challenges to the evaluation of model outputs. Past\nwork in the area, especially prior to LLMs, tended to perform a “strict” evaluation (Taill´ e\net al., 2020) requiring exact matches of entities and their corresponding relations to reference\ntargets. This was appropriate because the models were effectively annotating input tokens,\nand references are assumed to be extractive. By contrast, because they are abstractive,\nLLMs can produce a variety of outputs that convey the desired semantic content—i.e.,\naligned with the reference target—without matching words exactly.\nThis motivates manual evaluation of RE outputs. Specifically, we recruited three medical\ndoctors (domain experts) via the Upwork platform.3 We asked these experts to individually\nevaluate each reference (to measure precision) and generated tuple (to measure recall) from\nour exhaustive test set. For each reference tuple we asked experts to indicate: (1) Whether\nthe reference ICO triplet appears in the set of generated tuples for that given abstract; and\n(2) Whether the target tuple as a whole could be derived from the set of generated tuples for\nthat given abstract. Similarly, for each generated tuple we asked annotators to indicate: (1)\nWhether the ICO triplet appears in the abstract; and (2) Whether the tuple as a whole is\ncorrect (i.e., if it also gets the relevant supporting evidence and reported directionality). We\nprovide examples of each category in the Appendix A. Human evaluators achieved strong\nannotation agreement; Fleiss kappa, κ = 0.77. All three evaluators chose the same relevance\nlabel ∼92.4% of the time. We derived final (consensus) labels by simple majority vote.\n4.2. Error Analysis\nWe now describe, and provide examples of, some of the recurring error types from our\nbest performing model (Flan-T5-large) on the validation data, and a set of abstracts from\napproximately 660,000 RCTs from the Trialstreamer database. 4\nIncorrectly structured outputs The model sometimes generated incorrectly formatted\noutputs which cannot be evaluated because they do not conform to the expected structure.\n(Recall that the model is not explicitly constrained to yield outputs that follow the desired\nlinearization scheme.) These include generations where: (1) there are missing elements in\nthe (partial) ICO triplets; (2) outputs have an invalid syntactic structure (and are thus\n3. https://upwork.com. We paid these experts $30/hour to evaluate generated tuples.\n4. https://trialstreamer.ieai.robotreviewer.net/\n8\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nunparseable by any downstream tools); (3) some elements are duplicated; (4) the output\ncontains irrelevant or unrelated tokens. The following is an example of one such instance:\nGenerated: [none, score, no, none, score was not significantly different\nbetween the two groups., no significant difference]\nHere the instance has an incorrect number of tuple elements (6 instead of 5), multiple ele-\nments are invalid, and while it does produce a valid label (“no significant difference”), there\nare no primary intervention and outcome spans associated with the label. This behavior\noccurs in only a small fraction ( ∼0.53%) of the RCT abstracts from Trialstreamer we ran\nthrough our model.\nOpposite inference labels for same ICOsApproximately 12.3% of generated tuples\nhad ICO-triplet matches in the reference set (i.e., the ICO triplet was correctly extracted),\nbut the inferred label regarding the reported findings concerning these was incorrect (e.g.,\nsignificant increase instead of significant decrease). On inspection we found that such tuples\nbelonged to two categories: (1) The primary intervention and comparator were swapped\n(leading to a flipped, albeit still correct, inference label with the same extracted evidence\nspan); (2) Minor differences in generated outcomes which resulted in a change in the label.\nThe following is an example of the latter from our development set (PMID: 24227660: 5)\nAbstract snippet: Canagliflozin increased urinary glucose excretion in a\ndose-dependent manner and produced statistically significant reductions in\nbody weight compared with placebo (least squares mean percent changes from\nbaseline of -2.2%, -2.9%, -2.7%, and -1.3% with canagliflozin 50, 100, and\n300 mg and placebo; P < 0.05 for all comparisons). Overall adverse event\n(AE) rates were similar across groups. Canagliflozin was associated with\nhigher rates of genital mycotic infections in women, which were generally\nmild and led to few study discontinuations. Osmotic diuresis-related AE\nrates were low and similar across groups.\nReference: [canagliflozin, body weight, placebo, Canagliflozin increased\nurinary glucose excretion in a dose-dependent manner and produced\nstatistically significant reductions in body weight compared with placebo.,\ncanagliflozin [LABEL] significantly decreased [OUT] body weight [COMP]\nplacebo]\nGenerated: [canagliflozin, body weight reduction, placebo, Canagliflozin\nincreased urinary glucose excretion in a dose-dependent manner and produced\nstatistically significant reductions in body weight compared with placebo.,\ncanagliflozin [LABEL] significantly increased [OUT] body weight reduction\n[COMP] placebo]\nAn increase in body weight reduction is functionally the same as a decrease in body weight,\nand this explains the label flip.\n5. https://pubmed.ncbi.nlm.nih.gov/24227660/\n9\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nCombining multiple tuples On average, our best performing model generates 3.49\nICO tuples per instance, as opposed to 4.01 per instance in the reference test set (Table\n1). This difference appears to be due to the model combining multiple interventions and/or\noutcomes into one in cases where the inference label is preserved, in turn reducing the\nnumber of generated tuples. Consider the following example 6 from our dev set where this\nbehavior can be observed (PMID: 27981024 7):\nReference: [memory game with fruit, banana intake, no fruit game, evidence,\nsignificant increase], [memory game with fruit, mandarin intake, no fruit\ngame, evidence, significant increase]\nGenerated: [fruit version of memory game, intake of mandarins and bananas,\nno fruit game, evidence, significant increase]\nHere we can observe that the generated tuple has combined banana and mandarin intake,\nyielding a single output instead of the two in the reference.\nCorrectly generated but without any corresponding referenceThis type of “er-\nror” is limited to non-exhaustive reference sets, and occurs when there is no corresponding\nreference tuple for a correctly generated ICO output (because the reference set is non-\nexhaustive). While this is rare, instances featuring this type of error highlight the utility\nof the retrieved evidence snippets, which can be used to verify the output. The following\nis one such example taken from an abstract that was not exhaustively annotated (PMID:\n282110208):\nAbstract snippet:9 High-risk patients undergoing brain surgery were randomly\nassigned to a usual care group (control group) or a GDFR group. In the\nGDFR group, (1) fluid maintenance was restricted to 3 ml/kg/h of a\ncrystalloid solution and (2) colloid boluses were allowed only in case of\nhypotension associated with a low cardiac index and a high stroke volume\nvariation. The primary outcome variable was ICU length of stay, and\nsecondary outcomes were lactates at the end of surgery, postoperative\ncomplications, hospital length of stay, mortality at day 30, and costs.\nICU length of stay was shorter (3 days [1-5] vs. 6 days [3-11], p =\n0.001) and ICU costs were lower in the GDFR group. The total number of\ncomplications (46 vs. 99, p = 0.043) and the proportion of patients who\ndeveloped one or more complications (19.2 vs. 34.7%, p = 0.034) were\nsmaller in the GDFR group. Hospital length of stay and costs, as well as\nmortality at 30 day, were not significantly reduced.\nReference: [goal-directed fluid restriction (GDFR) strategy, median\nhospital length of stay, usual care, Hospital length of stay and costs, as\nwell as mortality at 30-day, were not significantly reduced, no significant\ndifference]\n6. Example simplified for brevity.\n7. https://pubmed.ncbi.nlm.nih.gov/27981024/\n8. https://pubmed.ncbi.nlm.nih.gov/28211020/\n9. Shortened for brevity.\n10\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nGenerated: [goal-directed fluid restriction (GDFR), ICU length of stay,\nusual care group (control group), ICU length of stay was shorter (3 days\n[1{5] vs. 6 days [3{11], p = 0.001) and ICU costs were lower in the GDFR\ngroup., significantly decreased]\nHere, on inspecting only the ICO-triplet and the inference label, one might assume that the\ngenerated tuple is incorrect with respect to the reference (due to a flipped label). However,\nexamining the extracted evidence in support of the label and the full abstract reveals that\nthe study does indeed report median length of hospital stay and ICU-length of stay as\nseparate outcomes with different (opposite) labels.\n5. A Prototype for Browsing Structured Evidence\nTo further demonstrate the (potential) utility of structured evidence extraction over the\npublished evidence base, we make available a demonstration web application.10 This permits\nfree-text search, which retrieves relevant structured evidence extracted from papers (we also\nlink back to the original PubMed articles).\nWe processed all Randomized Control Trials indexed by Trialstreamer (Marshall et al.,\n2020) as of June 2022, yielding 657,698 total studies and a total of 1,204,027 extracted\nrelations. Relation extraction required 584 GPU (32GB NVIDIA V100) hours. Of the\n770,356 unique Trialstreamer documents, approximately 50k instances were missing a full\nabstract. When processed via FLAN, 74k (about 10%) had an unparseable output; lacking\n(or possessing an extra) a syntatic element (e.g. missing a bracket or having an extra\none, or other terminator symbol). Another 5k had an output with an incorrect number of\nfields. 82 had a malformed label. When parsing misclassified RCTs (erroneously included\nin Trialstreamer), the model would hallucinate ICOs and findings not present in the data.\nThe prototype implements a BM25 search (Robertson et al., 1994) backed by SQLite\n(Hipp, 2020), allowing for search over multiple fields.11 The website allows for downloading\nsearch results (by search or by list of PMIDs/PMCIDs); our hope is that this may be of\ninterest to researchers. We will make the entire raw database of inferred relations available\nupon publication.\n6. Discussion\nWe have introduced and evaluated a state-of-the-art approach to end-to-end structured\nevidence extraction from natural language articles describing the conduct and results of\nclinical trials. Specifically, we treat this problem as a conditional generation task and\nfine-tune Flan-T5 (Chung et al., 2022)—a modestly sized instruction-tuned sequence-to-\nsequence model—to consume unstructured texts and yield structured tuples composed of\ninterventions, comparators, outcomes and the results reported regarding these. The latter\ncomprises a discrete prediction encoding the direction of the reported finding, and a snippet\nof evidence supporting this determination. Ablations indicate the importance of jointly\nextracting evidence spans to support the inference task; this may have implications for\nwork on relation extraction via conditional generative models more broadly.\n10. Hosted at http://ico-relations.ebm-nlp.com.\n11. We experimented with embedding based methods but were ultimately disappointed with results\n11\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nA\nB\nC\nD\nE\nF\nFigure 4: A screenshot of our prototype search interface over structured evidence. (A) User\ninputs a search query and select the fields (B) to be searched over via an SQL\nsearch (C; Hipp 2020, e.g., entire abstract, only ICOs). Search results can either\nbe downloaded as a structured CSV (D) or the user can browse through individual\nresults (E). We retrieve up to 100 documents per search query with 10 documents\nper page (F). The interface allows the users to read expanded abstracts, view\nstructured findings (shown above), and expand structured markup for a tabular\nview of findings.\n12\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nStructured evidence extraction is an important task for realizing the promise of evidence-\nbased medicine (EBM; Sackett 1997), which aspires to inform treatment decisions on the\nbasis of all available relevant evidence. The vast (unstructured) evidence base and rapid\naccumulation of new findings render practicing EBM challenging. The proposed approach\nto evidence extraction achieves substantially better performance than the prior state-of-\nthe-art (Nye et al., 2022), and this brings us closer to being able to synthesize all evidence\nrelevant to a given query, in real-time.\nTo illustrate the potential utility of this model, we have also made available a proto-\ntype interface that permits search directly over structured evidence tuples automatically\nextracted from a comprehensive database of randomized controlled trial reports. Our hope\nis that this demonstrates the precision of model outputs, and suggests how such extracted\nevidence might help researchers and healthcare providers navigate the evidence base more\nefficiently than is currently possible. We also anticipate that the resultant database (com-\nprising tuples from all RCTs in humans) may be a useful resource for researchers in machine\nlearning for healthcare broadly, as one might draw upon such trial results to inform and/or\njustify ML predictions (Yang et al., 2023; Naik et al., 2022).\nLimitations This work has several important limitations. First, while we have reported\npromising empirical results, the model we have trained here still makes errors (e.g., provides\ninexhaustive extractions from an inputs; see Section 4.2). Any downstream use of the\nstructured evidence outputs need to take this into account.\nA methodological limitation is that we did not investigate the capabilities of even larger\nLLMs like GPT-3.5/4 Brown et al. (2020) for this task. One could, in principle, use Ope-\nnAI’s API to fine-tune such models for this task, and given their size it is likely that this\nwould yield (probably moderately) improved results. We opted not to pursue this primarily\nbecause we prefer to use open-source models, to ensure scientific transparency and so that\nwe can release model weights. Furthermore, the main contribution here is the framing of\nthe task as a language modeling problem; the particular choice of underlying LLM is a\nsecondary consideration.\nFinally, while we think structured evidence in the format that we have extracted—\nproviding explicit sets of interventions, comparators, outcomes and evidence concerning\nthese—will provide meaningful downstream utility for those interested in navigating and\nmaking sense of the published evidence base, it is currently an intermediate output. The\nactual utility of this sort of model for downstream tasks which ultimately might affect care\nwill require conducting further research.\nAcknowledgments\nThis work was supported by the National Institutes of Health (NIH) under award R01LM012086,\nand by the National Science Foundation (NSF) award 1750978.\nReferences\nHilda Bastian, Paul Glasziou, and Iain Chalmers. Seventy-five trials and eleven systematic\nreviews a day: how will we ever keep up? PLoS medicine, 7(9):e1000326, 2010.\n13\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. Joint entity\nrecognition and relation extraction as a multi-head selection problem. Expert Systems\nwith Applications, 114:34–45, dec 2018a. doi: 10.1016/j.eswa.2018.07.032. URL https:\n//doi.org/10.1016%2Fj.eswa.2018.07.032.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. Adversarial\ntraining for multi-context joint entity and relation extraction. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pages 2830–2836,\nBrussels, Belgium, October-November 2018b. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1307. URL https://aclanthology.org/D18-1307.\nIz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for\nscientific text. In Proceedings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–3620, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL\nhttps://aclanthology.org/D19-1371.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunx-\nuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixi-\nang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H.\nChi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\nScaling instruction-finetuned language models, 2022.\nJim Cowie and Wendy Lehnert. Information extraction. Communications of the ACM , 39\n(1):80–91, 1996.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\nJay DeYoung, Eric Lehman, Benjamin Nye, Iain Marshall, and Byron C. Wallace. Evi-\ndence inference 2.0: More data, better models. In Proceedings of the 19th SIGBioMed\nWorkshop on Biomedical Language Processing , pages 123–132, Online, July 2020. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.bionlp-1.13. URL\nhttps://aclanthology.org/2020.bionlp-1.13.\n14\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nMarkus Eberts and Adrian Ulges. Span-based joint entity and relation extraction with\ntransformer pre-training. CoRR, abs/1909.07755, 2019. URL http://arxiv.org/abs/\n1909.07755.\nMarkus Eberts and Adrian Ulges. An end-to-end model for entity-level relation extraction\nusing multi-instance learning. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume , pages 3650–\n3660, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.eacl-main.319. URL https://aclanthology.org/2021.eacl-main.319.\nRichard D Hipp. SQLite, 2020. URL https://www.sqlite.org/index.html.\nPere-Llu´ ıs Huguet Cabot and Roberto Navigli. REBEL: Relation extraction by end-to-\nend language generation. In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 2370–2381, Punta Cana, Dominican Republic, November 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.204. URL\nhttps://aclanthology.org/2021.findings-emnlp.204.\nNeil Ireson, Fabio Ciravegna, Mary Elaine Califf, Dayne Freitag, Nicholas Kushmerick, and\nAlberto Lavelli. Evaluating machine learning for information extraction. In Proceedings\nof the 22nd international conference on Machine learning , pages 345–352, 2005.\nDi Jin and Peter Szolovits. PICO element detection in medical text via long short-term\nmemory neural networks. In Proceedings of the BioNLP 2018 workshop , pages 67–75,\nMelbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.\n18653/v1/W18-2308. URL https://aclanthology.org/W18-2308.\nTian Kang, Ali Turfah, Jaehyun Kim, Adler Perotte, and Chunhua Weng. A neuro-symbolic\nmethod for understanding free-text medical evidence. Journal of the American Medical\nInformatics Association, 28(8):1703–1711, 2021.\nSu Nam Kim, David Martinez, Lawrence Cavedon, and Lars Yencken. Automatic classifica-\ntion of sentences to support evidence based medicine. In BMC bioinformatics, volume 12,\npages 1–10. BioMed Central, 2011.\nSvetlana Kiritchenko, Berry de Bruijn, Simona Carini, Joel Martin, and Ida Sim. ExaCT:\nautomatic extraction of clinical trial characteristics from journal publications. BMC\nmedical informatics and decision making , 10(1):56, 2010.\nGrace E. Lee and Aixin Sun. A study on agreement in pico span annotations. InProceedings\nof the 42nd International ACM SIGIR Conference on Research and Development in In-\nformation Retrieval, SIGIR’19, page 1149–1152, New York, NY, USA, 2019. Association\nfor Computing Machinery. ISBN 9781450361729. doi: 10.1145/3331184.3331352. URL\nhttps://doi.org/10.1145/3331184.3331352.\nEric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. Inferring which med-\nical treatments work from reports of clinical trials. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) , pages 3705–3717,\n15\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n10.18653/v1/N19-1371. URL https://aclanthology.org/N19-1371.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics,\npages 7871–7880, Online, July 2020. Association for Computational Linguistics. doi: 10.\n18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703.\nIain J Marshall, Benjamin Nye, Jo¨ el Kuiper, Anna Noel-Storr, Rachel Marshall, Rory\nMaclean, Frank Soboczenski, Ani Nenkova, James Thomas, and Byron C Wallace. Tri-\nalstreamer: A living, automatically updated database of clinical trial reports. Journal of\nthe American Medical Informatics Association , 27(12):1903–1912, 2020.\nMakoto Miwa and Mohit Bansal. End-to-end relation extraction using LSTMs on sequences\nand tree structures. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pages 1105–1116, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1105.\nURL https://aclanthology.org/P16-1105.\nAakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Lu Wang, and Tom Hope.\nLiterature-augmented clinical outcome prediction. In Findings of the Association for\nComputational Linguistics: NAACL 2022 , pages 438–453, Seattle, United States, July\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.\n33. URL https://aclanthology.org/2022.findings-naacl.33.\nTapas Nayak and Hwee Tou Ng. Effective modeling of encoder-decoder architecture for\njoint entity and relation extraction, 2019.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain Marshall, Ani Nenkova, and\nByron Wallace. A corpus with multi-level annotations of patients, interventions and\noutcomes to support language processing for medical literature. In Proceedings of the\n56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 197–207, Melbourne, Australia, July 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/P18-1019. URL https://aclanthology.org/P18-1019.\nBenjamin E. Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J. Marshall, and By-\nron C. Wallace. Understanding clinical trial reports: Extracting medical entities and\ntheir relations, 2022.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, RISHITA\nANUBHAI, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. Structured\nprediction as translation between augmented natural languages. In International Con-\nference on Learning Representations, 2021. URL https://openreview.net/forum?id=\nUS-TP-xnXI.\n16\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer, 2020.\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike\nGatford. Okapi at trec-3. In Text Retrieval Conference, 1994.\nDavid L Sackett. Evidence-based medicine. In Seminars in perinatology, volume 21, pages\n3–5. Elsevier, 1997.\nLena Schmidt, Julie Weeds, and Julian P. T. Higgins. Data mining in clinical trial text:\nTransformers for classification and question answering tasks. In International Conference\non Health Informatics , 2020.\nBruno Taill´ e, Vincent Guigue, Geoffrey Scoutheeten, and Patrick Gallinari. Let’s Stop\nIncorrect Comparisons in End-to-end Relation Extraction! In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 3689–\n3701, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/\nv1/2020.emnlp-main.301. URL https://aclanthology.org/2020.emnlp-main.301.\nPatrick Verga, Emma Strubell, and Andrew McCallum. Simultaneously self-attending\nto all mentions for full-abstract biological relation extraction. In Proceedings of the\n2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 872–884,\nNew Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-1080. URL https://aclanthology.org/N18-1080.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. Entity, relation, and\nevent extraction with contextualized span representations. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\n5784–5789, Hong Kong, China, November 2019. Association for Computational Linguis-\ntics. doi: 10.18653/v1/D19-1585. URL https://aclanthology.org/D19-1585.\nByron C Wallace, Jo¨ el Kuiper, Aakash Sharma, Mingxi Zhu, and Iain J Marshall. Extracting\npico sentences from clinical trial reports using supervised distant supervision.The Journal\nof Machine Learning Research, 17(1):4572–4596, 2016.\nJue Wang and Wei Lu. Two are better than one: Joint entity and relation extraction with\ntable-sequence encoders. In Proceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages 1706–1721, Online, November 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.133. URL\nhttps://aclanthology.org/2020.emnlp-main.133.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv\npreprint arXiv:2201.11903, 2022.\n17\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, An-\nthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison,\nSam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.\nTransformers: State-of-the-art natural language processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing: System Demon-\nstrations, pages 38–45, Online, October 2020. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.\nemnlp-demos.6.\nQian Yang, Yuexing Hao, Kexin Quan, Stephen Yang, Yiran Zhao, Volodymyr Kuleshov,\nand Fei Wang. Harnessing biomedical literature to calibrate clinicians’ trust in ai decision\nsupport systems. In Conference on Human Factors in Computing Systems (CHI) , 2023.\nSuncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. Joint\nextraction of entities and relations based on a novel tagging scheme. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1227–1236, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1113. URL https://aclanthology.org/P17-1113.\n18\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nAppendix A: Examples\nHere we provide some full length output examples generated directly from our best per-\nforming models. Abstracts have been shortened for brevity to include key parts.\nPMID 1457358\nAbstract Micronuclei reflect DNA damage in exfoliated cells and may thus provide\na marker of early-stage carcinogenesis. Pre-treatment blood levels of cotinine, beta-\ncarotene, retinol and vitamins C and E were similar in the placebo group (n = 61)\nand the treatment group (n = 53). Plasma beta-carotene levels increased 13-fold in\nthe treatment group during intervention. Initial micronuclei counts (per 3,000 cells)\nwere higher in the treatment group than in the placebo group (5.0 vs 4.0, P ¡ 0.05).\nDuring intervention, the treatment group showed a 47% decrease, whereas the placebo\ngroup showed a non-significant decrease (16%). After adjustment for the initial levels,\nthe treatment group had 27% lower micronuclei counts than the placebo group at the\nend of the trial (95% CI: 9-41%). These results indicate that beta-carotene may reduce\nlung cancer risk in man by preventing DNA damage in early-stage carcinogenesis.\nReference [beta-carotene supplementation (20 mg d-1), Initial micronuclei counts\n(per 3,000 cells), placebo, Initial micro nuclei counts (per 3,000 cells) were higher\nin the treatment group than in the placebo group (5.0 vs 4.0, P; 0.05)., [INT] beta-\ncarotene supplementation (20 mg d-1) [LABEL] significantly increased [OUT] Initial\nmicronuclei counts (per 3,000 cells) [COMP] placebo]\nGenerated [14 weeks of beta-carotene supplementation (20 mg d-1), micronuclei\ncounts, placebo, Initial micronuclei counts (per 3,000 cells) were higher in the treat-\nment group than in the placebo group (5.0 vs 4.0, P 0.05)., [INT] 14 weeks of beta-\ncarotene supplementation (20 mg d-1) [LABEL] significantly increased [OUT] mi-\ncronuclei counts [COMP] placebo]\nPMID 29295869\nAbstract Since sCD163 is shed to serum by inflammatory signals including lipopolysac-\ncharides (LPS, endotoxin), we investigated sCD163 and correlations with lipid metabolism\nfollowing LPS exposure. Eight healthy male subjects were investigated on two sepa-\nrate occasions: (i) following an LPS exposure and (ii) following saline exposure. Each\nstudy day consisted of a four-hour non-insulin-stimulated period followed by a two-hour\nhyperinsulinemic euglycemic clamp period. A 3H-palmitate tracer was used to calcu-\nlate the rate of appearance (Rapalmitate). Blood samples were consecutively obtained\nthroughout each study day. Abdominal subcutaneous adipose tissue was obtained for\nwestern blotting. We observed a significant two-fold increase in plasma sCD163 levels\nfollowing LPS exposure (P ¡ 0.001), and sCD163 concentrations correlated positively\nwith the plasma concentration of free fatty acids, Rapalmitate, lipid oxidation rates\nand phosphorylation of the hormone-sensitive lipase at serine 660 in adipose tissue (P\n19\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\n< 0.05, all). Furthermore, sCD163 concentrations correlated positively with plasma\nconcentrations of cortisol, glucagon, tumour necrosis factor (TNF)- α, interleukin (IL)-\n6 and IL-10 (P < 0.05, all). We observed a strong correlation between sCD163 and\nstimulation of lipolysis and fat oxidation following LPS exposure. These findings sup-\nport preexisting theory that inflammation and macrophage activation play a significant\nrole in lipid metabolic adaptions under conditions such as obesity, DM2 and NAFLD.\nReference [LPS exposure, macrophage activation, Saline axposure, We observed a\nsignificant two-fold increase in plasma sCD163 levels following LPS exposure (P <\n0.001), and sCD163 concentrations correlated positively with the plasma concentra-\ntion of free fatty acids, Rapalmitate, lipid oxidation rates and phosphorylation of the\nhormone-sensitive lipase at serine 660 in adipose tissue (P < 0.05, all). Further-\nmore, sCD163 concentrations correlated positively with plasma concentrations of cor-\ntisol, glucagon, tumour necrosis factor (TNF)- α, interleukin (IL)-6 and IL-10 (P <\n0.05, all). , [INT] LPS exposure [LABEL] significantly increased [OUT] macrophage\nactivation [COMP] Saline axposure]\nGenerated [lipopolysaccharides (LPS) exposure, plasma sCD163 levels, saline expo-\nsure, We observed a significant two-fold increase in plasma sCD163 levels following LPS\nexposure (P < 0.001), and sCD163 concentrations correlated positively with the plasma\nconcentration of free fatty acids, Rapalmitate, lipid oxidation rates and phosphorylation\nof the hormone-sensitive lipase at serine 660 in adipose tissue (P < 0.05, all)., [INT]\nlipopolysaccharides (LPS) exposure [LABEL] significantly increased [OUT] plasma\nsCD163 levels [COMP] saline exposure]\nPMID 26258157\nAbstract We conducted a double-blind, randomized trial of 134 outpatients with\npolymerase chain reaction-confirmed influenza to assess the effects of oseltamivir ini-\ntiated 48–119 hours after illness onset. Oseltamivir treatment did not reduce illness\nduration, severity, or duration of virus detection. However, the power of this study was\nlimited due to lower than expected enrollment.\nReference 1 [Oseltamivir, Severity of illness, Control, Oseltamivir treatment did not\nreduce illness duration, severity, or duration of virus detection., [INT] Oseltamivir\n[LABEL] no significant difference [OUT] Severity of illness [COMP] Control]\nReference 2 [Oseltamivir, Duration of virus detection, Control, Oseltamivir treat-\nment did not reduce illness duration, severity, or duration of virus detection., [INT]\nOseltamivir [LABEL] no significant difference [OUT] Duration of virus detection\n[COMP] Control]\nGenerated [Oseltamivir, Illness duration, severity, or duration of virus detection,\nControl, Oseltamivir treatment did not reduce illness duration, severity, or duration of\nvirus detection., [INT] Oseltamivir [LABEL] no significant difference [OUT] Illness\nduration, severity, or duration of virus detection [COMP] Control]\n20\nJointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs\nPMID 26283840\nAbstract Most commonly practiced surgical ”lay open” technique to treat fistula-in-\nano (a common anorectal pathology) has high rate of recurrence and anal incontinence.\nAlternatively, a nonsurgical cost efficient treatment with Ksharasutra (cotton Seton\ncoated with Ayurvedic medicines) has minimal complications. In our study, we have\ntried to compare these two techniques. A prospective randomized control study was\ndesigned involving patients referred to the Department of General Surgery in RG Kar\nMedical College, Kolkata, India, from January 2010 to September 2011. Among 50\npatients, 26 were in Ksharasutra and 24 were in fistulotomy group. 86% patients were\nmale and 54% of the patients were in the fourth decade. About 74% fistulas are inter-\nsphincteric and 26% were of trans-sphincteric variety. Severe postoperative pain was\nmore (7.7% vs. 25%) in fistulotomy group, while wound discharge was more associated\nwith Ksharasutra group (15.3% vs. 8.3%). Wound scarring, bleeding, and infection\nrate were similar in both groups. Ksharasutra group took more time to heal (mean:\n53 vs. 35.7 days, P = 0.002) despite reduced disruption to their routine work (2.7 vs.\n15.5 days work off, P < 0.001). Interestingly, pain experienced was less in Ksharasutra\ngroup, there was no open wound in contrast to fistulotomy and it was significantly cost\neffective (Rupees 166 vs 464).\nReference [Ksharasutra (cotton Seton coated with Ayurvedic medicines), number\nof days “off-work”, fistulotomy, Ksharasutra group took more time to heal (mean:\n53 vs 35.7 days, P = 0.002) despite reduced disruption to their routine work (2.7\nvs. 15.5 days work off, P < 0.001)., [INT] Ksharasutra (cotton Seton coated with\nAyurvedic medicines) [LABEL] significantly decreased [OUT] number of days “off-\nwork” [COMP] fistulotomy]\nGenerated 1 [Ksharasutra (cotton Seton coated with Ayurvedic medicines), Severe\npostoperative pain, fistulotomy, Severe postoperative pain was more (7.7% vs. 25%) in\nfistulotomy group, while wound discharge was more associated with Ksharasutra group\n(15.3% vs. 8.3%)., [INT] Ksharasutra (cotton Seton coated with Ayurvedic medicines)\n[LABEL] significantly decreased [OUT] Severe postoperative pain [COMP] fistulo-\ntomy]\nGenerated 2 [Ksharasutra (cotton Seton coated with Ayurvedic medicines), Wound\nscarring, bleeding, and infection rate, fistulotomy, Wound scarring, bleeding, and infec-\ntion rate were similar in both groups., [INT] Ksharasutra (cotton Seton coated with\nAyurvedic medicines) [LABEL] no significant difference [OUT] Wound scarring,\nbleeding, and infection rate [COMP] fistulotomy]\n21",
  "topic": "Psychological intervention",
  "concepts": [
    {
      "name": "Psychological intervention",
      "score": 0.7202432155609131
    },
    {
      "name": "Randomized controlled trial",
      "score": 0.6176770329475403
    },
    {
      "name": "Computer science",
      "score": 0.6163737773895264
    },
    {
      "name": "Framing (construction)",
      "score": 0.5097889304161072
    },
    {
      "name": "Data extraction",
      "score": 0.41513872146606445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34685149788856506
    },
    {
      "name": "Natural language processing",
      "score": 0.33522483706474304
    },
    {
      "name": "MEDLINE",
      "score": 0.3008711338043213
    },
    {
      "name": "Psychology",
      "score": 0.2681625485420227
    },
    {
      "name": "Medicine",
      "score": 0.24938994646072388
    },
    {
      "name": "Engineering",
      "score": 0.07706698775291443
    },
    {
      "name": "Pathology",
      "score": 0.0756603479385376
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I189774192",
      "name": "Colorado College",
      "country": "US"
    }
  ],
  "cited_by": 9
}