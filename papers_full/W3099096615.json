{
  "title": "Query-Key Normalization for Transformers",
  "url": "https://openalex.org/W3099096615",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2202399126",
      "name": "Alex Henry",
      "affiliations": [
        "CytRx (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2624433160",
      "name": "Prudhvi Raj Dachapally",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3091819255",
      "name": "Shubham Shantaram Pawar",
      "affiliations": [
        "CytRx (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2133749130",
      "name": "Yuxuan Chen",
      "affiliations": [
        "CytRx (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2594833348",
    "https://openalex.org/W2963086938",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2981040094",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W3035618147",
    "https://openalex.org/W2953830716",
    "https://openalex.org/W3021357296",
    "https://openalex.org/W2970903692",
    "https://openalex.org/W2997753998",
    "https://openalex.org/W2905927205",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3104273515",
    "https://openalex.org/W2964085268",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W3016151632",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3093960091",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W222053410",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W3026674654",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2798761464",
    "https://openalex.org/W2971120622"
  ],
  "abstract": "Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer’s normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply l2-normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT’15.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4246–4253\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4246\nQuery-Key Normalization for Transformers\nAlex Henry, Prudhvi Raj Dachapally, Shubham Pawar, Yuxuan Chen\nCyndx Technologies\n{alex.henry,prudhvi.dachapally,shubham.pawar,ethan.chen}@cyndx.com\nAbstract\nLow-resource language translation is a chal-\nlenging but socially valuable NLP task. Build-\ning on recent work adapting the Transformer’s\nnormalization to this setting, we propose\nQKN ORM , a normalization technique that\nmodiﬁes the attention mechanism to make the\nsoftmax function less prone to arbitrary satu-\nration without sacriﬁcing expressivity. Specif-\nically, we apply ℓ2 normalization along the\nhead dimension of each query and key ma-\ntrix prior to multiplying them and then scale\nup by a learnable parameter instead of di-\nviding by the square root of the embedding\ndimension. We show improvements averag-\ning 0.928 BLEU over state-of-the-art bilin-\ngual benchmarks for 5 low-resource transla-\ntion pairs from the TED Talks corpus and\nIWSLT’15.1\n1 Introduction\nThe Transformer (Vaswani et al., 2017) remains\nthe architecture of choice for machine translation.\nSince its introduction, various architectural and\nfunctional modiﬁcations have been made to im-\nprove its performance on NMT datasets (Ahmed\net al., 2017; Zhang et al., 2018; Wang et al., 2019;\nDai et al., 2019; Zhao et al., 2019). Translating\nlow-resource languages presents special challenges.\nRecent strategies for adapting Transformers to this\nsocially valuable task include exploiting transfer\nlearning with many-to-many multilingual models\n(Aharoni et al., 2019), reducing model depth (van\nBiljon et al., 2020), and adding a regularization\npenalty for diverging from the predictions of a\nmonolingual language model pretrained on the tar-\nget language (Baziotis et al., 2020). This paper\nbuilds on recent work on layer normalization for\n1Code to reproduce our experiments is available athttps:\n//github.com/CyndxAI/QKNorm\nlow-resource language pairs, introducing a normal-\nization technique that tries to keep the input to\nsoftmax attention within an appropriate range.\nLayer normalization. For Transformers and\nother NLP models, layer normalization (Ba et al.,\n2016) yields signiﬁcantly better performance than\nbatch normalization (Ioffe and Szegedy, 2015), in\npart because NLP models tend to exhibit greater\nvariance in batch statistics during training, for ex-\nample compared to computer vision (Shen et al.,\n2020). Layer normalization boosts performance in\ndeeper networks chieﬂy by controlling their gradi-\nents (Xu et al., 2019). It re-scales and re-centers\nactivation distributions (though re-centering may\nbe unnecessary, see Zhang and Sennrich 2019).\nThe type of normalization used and the placement\nof that normalization within the Transformer are\nboth crucial to Transformer performance (Nguyen\nand Salazar, 2019).\nSoftmax attention. Given a matrix X embed-\nding a sequence of tokens, attention transforms\neach embedding into a mixture of itself and other\nelements of the sequence according to the impor-\ntance of their connections for the modeling task at\nhand. In the case of multihead self-attention, the\nvectors of X are projected linearly into Query, Key\nand Value matrices. The operation\nsoftmax(QKT ) (1)\ndeﬁnes a distribution for each token over all the oth-\ners in its sequence that sums to 1. Multiplying by\nV then yields a new matrix where the embedding\nof each token is a weighted average of the vectors\nin V .\nRichter and Wattenhofer (2020) propose replac-\ning the softmax function in attention because it con-\nstrains attention’s output to the convex hull spanned\nby the vectors in V , limiting model ﬂexibility. For\n4247\nthe softmax over the vocabulary in next word pre-\ndiction, Demeter et al. (2020) ﬁnd that the norms\nof word embeddings drown out their angular dis-\nplacements, with the consequence that words with\nsmaller norms are systematically less likely to be\npredicted.\nIn this work, we replace the dot product inside\nof softmax attention with cosine similarity scaled\nup by a learnable parameter. This technique yields\nimproved performance in low-resource bilingual\ntranslation, which we conjecture is because it binds\nQKT to a narrower range in a way that makes\nit easier to learn more diffuse attention patterns\nwherever these prove valuable.\n2 Background\nNguyen and Salazar (2019) achieve state-of-the-art\nbilingual performance on 5 low-resource transla-\ntion pairs from the TED Talks (Qi et al., 2018)\nand IWSLT’15 (Cettolo et al., 2015) corpora. This\nwork builds directly on theirs, applying our tech-\nnique to the same 5 benchmarks. Their model\ncombines three normalization techniques that we\ndescribe below: FIXNORM (Nguyen and Chiang,\n2018), PRENORM (Klein et al., 2017; Domhan,\n2018; Vaswani et al., 2018; Chen et al., 2018), and\nSCALE NORM , which they introduce as a replace-\nment for layer normalization. They report that each\ntechnique contributes about 0.3 BLEU for an aver-\nage improvement of 1.1 BLEU across the test sets\nfor their 5 language pairs.\nFIXNORM sets word embeddings to unit length,\nwhich aids rare word translation (Nguyen and Chi-\nang, 2018). PRENORM simply changes the location\nof layer normalization within the Transformer ar-\nchitecture, applying it to the input to each sublayer\ninstead of after the residual connection. Moving\nlayer normalization ahead of the residual connec-\ntion enhances stability because the residual path\nis allowed to stay an identity map, instead of con-\ntributing terms to the gradient that could cause it\nto explode or vanish (Wang et al., 2019; Nguyen\nand Salazar, 2019). Interestingly, Nguyen and\nSalazar (2019) ﬁnd PRENORM to be superior in\nlow-resource but not high-resource translation set-\ntings.\nLastly, SCALE NORM replaces layer normaliza-\ntion with ℓ2 normalization along the embedding\ndimension, multiplied by a learnable scalar param-\neter initialized with 1√\nd (where d is the embedding\ndimension; the same term is used in scaled dot\nproduct attention (Vaswani et al., 2017)).\nIn other words, SCALE NORM applies ℓ2 normal-\nization along the embedding dimension of Q, K\nand V , and it does so before the input to multihead\nattention gets split into heads.\nBuilding on their work, we combine FIXNORM ,\nPRENORM , and vanilla layer normalization\n(LAYER NORM ) with a new technique we call\nquery-key normalization (QKN ORM ), surpassing\ntheir model’s performance on each of the same 5\ntranslation pairs by an average of 0.928 test BLEU.\nQKN ORM applies ℓ2 normalization to Q and\nK only, and it does so along the head dimension\n(which is the same dimension as the embedding di-\nmension, but after multihead attention has split its\ninput into separate heads). Q and K thus become\nˆQ and ˆK, where the ith row vector of ˆQ (the ith\nembedding in the sequence) is given by:\nˆqi = qi\n||qi|| (2)\nThe effect is to make each element of QKT the co-\nsine similarity of the corresponding pair of contex-\ntual token representations instead of their dot prod-\nuct. This is similar to Luo et al. (2018), who pro-\npose replacing the dot product in fully-connected\nnetworks between layer weights and previous layer\noutputs with cosine similarity.\nLike SCALE NORM , we also multiply by a learn-\nable parameter that we initialize according to a rule\nof thumb we describe below. Unlike SCALE NORM ,\nQKN ORM complements LAYER NORM rather than\nreplacing it.\n3 Dot Products and the Softmax\nFunction\nSoftmax attends only to the differences between\nvalues. For example,\nsoftmax([760, 752, 750])\n= softmax([12, 4, 2])\n= [0 .99962, 0.00034, 0.00005].\nSince the dot product is unbounded, differences\nbetween elements that may be insigniﬁcantly small\non a relative basis can silence all other signals in\nthe attention weights applied to V . We conjecture\nthat this limits the complexity of the patterns that\nattention heads can learn.\nThe impact is more obvious in less sophisticated\nTransformer implementations (perhaps in part be-\ncause subsequent advances have mitigated the same\n4248\nFigure 1: Scaled Dot Product Attention. Self-attention heatmaps for 4 heads from one encoder layer displaying\nmore “concentrated” attention, consistent with the conjecture that unnormalized dot products inQKT saturate the\nsoftmax and limit the attention patterns that can be learned.\nFigure 2: Query-Key Normalized Attention. Self-attention heatmaps of the same 4 heads in Figure 1. QKN ORM\nenables more diffuse attention patterns.\nissue in different ways). Figures 1 and 2 show a\nheatmap comparison of encoder weights trained\nusing the code for The Annotated Transformer 2,\nthe ﬁrst with scaled dot product attention and the\nsecond with QKN ORM .\nThe models containing these encoders were\ntrained for 10 epochs on IWSLT 2016 de→en\n(Cettolo et al., 2016) using the Annotated Trans-\nformer implementation, with the baseline model\nscoring 19.4 BLEU and the QKN ORM model scor-\ning 24.33 BLEU on the test set, computed with the\nSacreBLEU Python package (Post, 2018).\nThough this heatmap comparison is obviously\nnot systematic, we think the visual at least pro-\nvides a plausible intuition for the incremental gain\nthis technique achieves, with scaled dot product\nattention exhibiting the kind of “winner-take-all”\nbehavior we would expect from a softmax near\nsaturation.\nIn comparison to dot products, cosine similari-\nties are bounded by [−1, 1] which creates the oppo-\nsite problem as input to softmax – the differences\n2https://nlp.seas.harvard.edu/2018/04/\n03/attention.html\nbetween values are too small for softmax to let\nthe model effectively ignore connections between\nwords it should not attend to. Instead of dividing by√\nd as in scaled dot product attention we scale up\nusing a learnable parameter that we initialize with\na value that depends on the length of the sequences\nin the training data (and hence on the number of\nelements in QKT ):\ng0 = log2(L2 −L) (3)\nwhere L is the 97.5th percentile sequence length\nacross all training data sequences for source and\ntarget.\nThe attention operation thus changes from\nsoftmax(QKT\n√\nd\n)V (4)\nto\nsoftmax(g ∗ˆQ ˆKT )V (5)\nwhere ˆQ and ˆK are Q and K with ℓ2-normalization\napplied along their head dimensions and g is a\nlearnable scalar parameter initialized with g0 as\ncomputed in (3).\n4249\nExamplesSource + Target TokensNumber of ParametersTraining Time (in hours)Development BLEUGPU L\ngl→en 10k 0.37M 31,051,880 6 23.45 T4 79\nsk→en 61k 2.32M 48,356,907 11 31.34 T4 75\nen→vi 133k 5.99M 48,431,538 19 28.77 T4 72\nen→he 212k 7.88M 48,401,538 38 31.16 T4 72\nar→en 214k 8.09M 48,499,512 26 37.94 P100 75\nTable 1: Summary of data and model training information. Number of examples and number of tokens taken\ndirectly from Nguyen and Salazar (2019). L is the 97.5th percentile sequence length across all training data\nsequences.\nen→vi ar→en en→he gl→en sk→en\nNguyen and Salazar (2019)32.79 36.09 28.28 22.01 32.58\nQKNORM+ LAYERNORM 33.24 36.75 28.96 24.21 33.23\nTable 2: Comparison of test BLEU (Papineni et al., 2002), scored using the Moses toolkit scripts provided in the\nrepo for Nguyen and Salazar (2019). p <0.01 using bootstrap resampling (Koehn, 2004). Both architectures use\nPRENORM and FIXNORM . The Nguyen and Salazar (2019) architecture uses SCALE NORM where we instead use\nvanilla layer normalization (Ba et al., 2016), and scaled dot product attention where we use QKN ORM .\n4 Experiments and Results\nWe follow the implementation in the repository\nfor Nguyen and Salazar (2019), both in replicating\ntheir performance and as a starting point for our\nversion (and also for computing BLEU as reported\nin Table 2).3 We train on the same 5 low-resource\ntranslation pairs as Nguyen and Salazar (2019):\n4 from the TED Talks corpus (Qi et al., 2018) 4 –\nArabic, Slovak, and Galician translated to English,\nand English translated to Hebrew – and 1 from the\nIWSLT’15 corpus (Cettolo et al., 2015), English\nto Vietnamese. The repository for Nguyen and\nSalazar (2019) provides the tokenized text they\nused for English to Vietnamese.\nTokenization and BLEU. Apart from BPE (Sen-\nnrich et al., 2016), their repository does not include\nthe code they used for tokenization, so for the other\n4 language pairs we used the tokenization script\nfrom the repository for Qi et al. (2018).5\nThe repository for Nguyen and Salazar\n(2019) includes two Moses6 scripts for\nscoring BLEU, multi-bleu.perl and\nmulti-bleu-detok.perl. We can’t use\nmulti-bleu.perl for the 4 TED Talks pairs\nwithout being able to replicate their tokenization\nbecause scores from that script are not comparable\n3https://github.com/tnq177/\nTransformers_without_tears\n4http://phontron.com/data/ted_talks.\ntar.gz\n5https://github.com/neulab/\nword-embeddings-for-nmt/blob/master/\nted_reader.py\n6https://github.com/moses-smt/\nmosesdecoder\nwhen there are differences in tokenization, unlike\nmulti-bleu-detok.perl (Post, 2018). We\nuse multi-bleu.perl to score en→vi (since\nwe have their preprocessed text for this pair) and\nmulti-bleu-detok.perl to score the 4\nTED Talks pairs.\nFor additional conﬁrmation, we also\nscore all models using SacreBLEU (Post,\n2018) after detokenizing with NLTK’s\nTreebankWordDetokenizer (Bird and\nLoper, 2004). These scores are reported in Table 3.\nAll the detokenized BLEU scores from Table\n2 are basically unchanged in Table 3, with the\nexception of en→vi. The best scores for the\nbaseline model we could get on en→vi were 32.48\nfor Moses multi-bleu.perl and 32.41 for\nSacreBLEU, though in Table 2 we report the\nmulti-bleu.perl score from Nguyen and\nSalazar (2019), 32.79. Our model’s score for the\nsame pair comes in 0.06 BLEU lower as well.\nFollowing the Nguyen and Salazar (2019) repos-\nitory, we perform BPE using fastBPE7. We also\nuse the same Moses code for bootstrap resampling\n(Koehn, 2004).\nModel hyperparameters. Although PRENORM\nhas been shown to make warmup less important\nfor Transformers using scaled dot product attention\n(Nguyen and Salazar, 2019; Xiong et al., 2020), we\nobtained our best results using 8,000 steps of linear\nwarmup. How much linear warmup matters for\nQKN ORM and why it matters are both subjects for\nfurther investigation. We used the same validation-\n7https://github.com/glample/fastBPE\n4250\nen→vi ar→en en→he gl→en sk→en\nNguyen and Salazar (2019)32.41 36.09 28.28 22.01 32.58\nQKNORM+ LAYERNORM 33.18 36.75 28.96 24.21 33.22\nTable 3: Comparison of test BLEU (Papineni et al., 2002), scored using SACRE BLEU (Post, 2018).\nbased decay scheme as Nguyen and Salazar (2019)\nand allowed models to train until they had reached\nthe minimum learning rate. For all other model\nhyperparameters and preprocessing settings we fol-\nlowed Nguyen and Salazar (2019) and the code in\nthe lead author’s GitHub repository. As in their\nrepository, we calculate test BLEU on the trans-\nlation from the epoch with the highest validation\nBLEU.\nResults. Incorporating QKN ORM and using\nlayer normalization instead of SCALE NORM\nboosted performance by an average of 0.928 BLEU\nacross the test sets for the 5 translation pairs. On\nIWSLT’15en→vi, our SacreBLEU test score of\n33.18 is only 0.09 BLEU lower than Provilkov et al.\n(2020), who use BPE-dropout to increase BLEU\n1.49 over the same model with vanilla BPE.\n5 Conclusion\nIn this paper, we introduced a normalization tech-\nnique that modiﬁes the attention mechanism in\nTransformers and demonstrated its utility for low-\nresource bilingual translation by building it into\nan existing Transformer implementation with state-\nof-the-art performance on 5 low-resource language\npairs. QKN ORM improves performance for each of\nthe 5 pairs, with an average test BLEU increase of\n0.928. We pointed to possible explanations for its\neffectiveness but identifying exactly where it helps\nand why requires further research. First, we plan to\ncombine our approach with the fairseq Transformer\nimplementation (Ott et al., 2019) and apply it to the\nFLORES dataset (Guzm´an et al., 2019), investigat-\ning the effect of QKN ORM on the optimal depth,\nnumber of attention heads, and warmup schedule\nfor low-resource translation, in combination with\nrecent advances like BPE-dropout (Provilkov et al.,\n2020). Next, we plan to look at high-resource set-\ntings to see whether the beneﬁts of query-key nor-\nmalization dissipate with access to more training\ndata. Lastly, we intend to study how QKN ORM\nimpacts what attention heads actually learn, adapt-\ning methods from BERT attention studies such as\nClark et al. (2019).\nAcknowledgments\nThe authors would like to thank the reviewers for\ntheir valuable and insightful comments, and Toan\nQ. Nguyen for helpful clariﬁcations and sugges-\ntions along the way.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer network for\nmachine translation.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nChristos Baziotis, Barry Haddow, and Alexandra Birch.\n2020. Language model prior for low-resource neu-\nral machine translation.\nElan van Biljon, Arnu Pretorius, and Julia Kreutzer.\n2020. On optimal transformer depth for low-\nresource language translation.\nSteven Bird and Edward Loper. 2004. NLTK: The nat-\nural language toolkit. In Proceedings of the ACL In-\nteractive Poster and Demonstration Sessions, pages\n214–217, Barcelona, Spain. Association for Compu-\ntational Linguistics.\nM. Cettolo, Niehues Jan, St ¨uker Sebastian, L. Ben-\ntivogli, R. Cattoni, and M. Federico. 2016. The iwslt\n2016 evaluation campaign.\nM. Cettolo, J. Niehues, S. St¨uker, L. Bentivogli, R. Cat-\ntoni, and M. Federico. 2015. The iwslt 2015 evalua-\ntion campaign.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\nAshish Vaswani, Jakob Uszkoreit, Lukasz Kaiser,\nZhifeng Chen, Yonghui Wu, and Macduff Hughes.\n2018. The best of both worlds: Combining recent\nadvances in neural machine translation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 76–86, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\n4251\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nDavid Demeter, Gregory Kimmel, and Doug Downey.\n2020. Stolen probability: A structural weakness\nof neural language models. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 2191–2197, Online. As-\nsociation for Computational Linguistics.\nTobias Domhan. 2018. How much attention do you\nneed? a granular analysis of neural machine trans-\nlation architectures. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1799–\n1808, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nFrancisco Guzm ´an, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation: Nepali–English and Sinhala–\nEnglish. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6098–6111, Hong Kong, China. Association for\nComputational Linguistics.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by\nreducing internal covariate shift. volume 37 of Pro-\nceedings of Machine Learning Research, pages 448–\n456, Lille, France. PMLR.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nChunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang,\nand Rui Ren. 2018. Cosine Normalization: Using\nCosine Similarity Instead of Dot Product in Neural\nNetworks: 27th International Conference on Artiﬁ-\ncial Neural Networks, Rhodes, Greece, October 4-7,\n2018, Proceedings, Part I, pages 382–391.\nToan Nguyen and David Chiang. 2018. Improving lex-\nical choice in neural machine translation. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), pages 334–343, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nToan Q. Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. In Proc. Workshop on Spoken Lan-\nguage Translation. To appear.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-\nmanabhan, and Graham Neubig. 2018. When and\nwhy are pre-trained word embeddings useful for neu-\nral machine translation? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 529–535, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nOliver Richter and Roger Wattenhofer. 2020. Normal-\nized attention without probability cage.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\n4252\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Ma-\nhoney, and Kurt Keutzer. 2020. Powernorm: Re-\nthinking batch normalization in transformers. In\nICML.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszko-\nreit. 2018. Tensor2Tensor for neural machine trans-\nlation. In Proceedings of the 13th Conference of the\nAssociation for Machine Translation in the Ameri-\ncas (Volume 1: Research Papers) , pages 193–199,\nBoston, MA. Association for Machine Translation\nin the Americas.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang\nZhao, and Junyang Lin. 2019. Understanding\nand improving layer normalization. In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. dAlch´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 4381–\n4391. Curran Associates, Inc.\nBiao Zhang and Rico Sennrich. 2019. Root mean\nsquare layer normalization. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. dAlch ´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 32 , pages 12381–\n12392. Curran Associates, Inc.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel-\nerating neural transformer via an average attention\nnetwork. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguis-\ntics, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nGuangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xu-\nancheng Ren, Qi Su, and Xu Sun. 2019. Explicit\nsparse transformer: Concentrated attention through\nexplicit selection.\nNumber of HeadsTest BLEU\n2 32.40\n4 33.16\n8 33.24\n16 32.42\n32 32.30\nTable 4: IWSLT’15 en→vi test BLEU for QKN ORM\nvarying the number of attention heads.\nPercentile Test BLEU\n75th 32.58\n90th 32.89\n92.5th 32.64\n95th 33.13\n97.5th 33.24\n99th 32.64\nMaximum Word Count33.10\nTable 5: IWSLT’15 en→vi test BLEU for QKN ORM\nvarying the training set word count percentile used to\ninitialize the learnable scaling factor g.\nAppendix\nA Varying the Number of Heads\nIn Table 4, we show the performance of QKN ORM\non the en→vi test set varying the number of heads.\nEven when the number of heads is 32 (with head\ndimension 16), the performance remains stable.\nB Equation 3\nIntuitively, longer sequences require more scaling\nto make it at least possible for the maximum values\nin QKT to softmax to 1. We arrived at Equation\n3 empirically by applying softmax to similarity\nmatrices of word vectors scaled up with various\nheuristics. Like\n√\nd in scaled dot product attention\n(Vaswani et al., 2017), Equation 3 is a rule of thumb\nbut it initializes a learnable parameter.\nWe determined the best value ofL in Equation 3\nby running theen→vi translation task with different\npercentile values. Table 5 shows the results from\nthose experiments.\nC Ablation Experiments\nTable 6 shares test performance on en→vi when\nwe ablate speciﬁc components of QKN ORM . The\nbiggest performance drop in these experiments\ncomes from omitting g, the learnable scaling factor.\nThis is unsurprising because if we don’t scale up\n4253\nExperiment Test BLEU\nWithoutg 24.53\nWithout LAYERNORM 31.56\nWithout FIXNORM 32.63\nWithout FIXNORM or PRENORM 32.20\nℓ2-normalizingV along withQandK 32.34\nTable 6: Ablation Experiments.\nˆQ ˆKT its values are all within [−1, 1] and softmax\nis a function of the differences between values.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7419547438621521
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.7341383099555969
    },
    {
      "name": "Machine translation",
      "score": 0.7031431198120117
    },
    {
      "name": "Softmax function",
      "score": 0.6865787506103516
    },
    {
      "name": "Transformer",
      "score": 0.5966452360153198
    },
    {
      "name": "Natural language processing",
      "score": 0.5530458688735962
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5408786535263062
    },
    {
      "name": "Embedding",
      "score": 0.45156338810920715
    },
    {
      "name": "Deep learning",
      "score": 0.10938113927841187
    },
    {
      "name": "Voltage",
      "score": 0.08520755171775818
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}