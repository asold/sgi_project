{
  "title": "Graph Kernel Attention Transformers",
  "url": "https://openalex.org/W3185731487",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5031842812",
      "name": "Krzysztof Choromański",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5101799775",
      "name": "Han Lin",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5060553638",
      "name": "Haoxian Chen",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A5083828420",
      "name": "Jack Parker-Holder",
      "affiliations": [
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2565330852",
    "https://openalex.org/W3100561866",
    "https://openalex.org/W2964311892",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W2962940432",
    "https://openalex.org/W2081301924",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W3105136071",
    "https://openalex.org/W2604302777",
    "https://openalex.org/W2753798143",
    "https://openalex.org/W2944401074",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3206190083",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W3117502566",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2101491865",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2963984147",
    "https://openalex.org/W2099438806",
    "https://openalex.org/W2407879741",
    "https://openalex.org/W2788919350",
    "https://openalex.org/W2771422378",
    "https://openalex.org/W2995983896",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2092750499",
    "https://openalex.org/W2144000913",
    "https://openalex.org/W3110699758",
    "https://openalex.org/W2806331055",
    "https://openalex.org/W2558748708",
    "https://openalex.org/W2034618876",
    "https://openalex.org/W2963241951",
    "https://openalex.org/W2765711500",
    "https://openalex.org/W2811124557",
    "https://openalex.org/W2163788280",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W2008857988"
  ],
  "abstract": "We introduce a new class of graph neural networks (GNNs), by combining several concepts that were so far studied independently - graph kernels, attention-based networks with structural priors and more recently, efficient Transformers architectures applying small memory footprint implicit attention methods via low rank decomposition techniques. The goal of the paper is twofold. Proposed by us Graph Kernel Attention Transformers (or GKATs) are much more expressive than SOTA GNNs as capable of modeling longer-range dependencies within a single layer. Consequently, they can use more shallow architecture design. Furthermore, GKAT attention layers scale linearly rather than quadratically in the number of nodes of the input graphs, even when those graphs are dense, requiring less compute than their regular graph attention counterparts. They achieve it by applying new classes of graph kernels admitting random feature map decomposition via random walks on graphs. As a byproduct of the introduced techniques, we obtain a new class of learnable graph sketches, called graphots, compactly encoding topological graph properties as well as nodes' features. We conducted exhaustive empirical comparison of our method with nine different GNN classes on tasks ranging from motif detection through social network classification to bioinformatics challenges, showing consistent gains coming from GKATs.",
  "full_text": "From block-Toeplitz matrices to differential equations on graphs: towards a\ngeneral theory for scalable masked Transformers\nKrzysztof Choromanski * 1 2 Han Lin * 2 Haoxian Chen * 2 Tianyi Zhang 2 Arijit Sehanobish 3\nValerii Likhosherstov4 Jack Parker-Holder5 Tamas Sarlos6 Adrian Weller4 7 Thomas Weingarten8\nAbstract\nIn this paper we provide, to the best of our knowl-\nedge, the ﬁrst comprehensive approach for in-\ncorporating various masking mechanisms into\nTransformers architectures in a scalable way. We\nshow that recent results on linear causal attention\n(Choromanski et al., 2021) and log-linear RPE-\nattention (Luo et al., 2021) are special cases of this\ngeneral mechanism. However by casting the prob-\nlem as a topological (graph-based) modulation of\nunmasked attention, we obtain several results un-\nknown before, including efﬁcient d-dimensional\nRPE-masking and graph-kernel masking. We\nleverage many mathematical techniques ranging\nfrom spectral analysis through dynamic program-\nming and random walks to new algorithms for\nsolving Markov processes on graphs. We provide\na corresponding empirical evaluation.\n1. Introduction & Related Work\nTransformers (Vaswani et al., 2017; Brown et al., 2020; De-\nvlin et al., 2019) have revolutionized machine learning by\nreintroducing an attention mechanism explicitly modeling\ncomplicated relationships between elementary ingredients\nof the ML models’ inputs, e.g. words for text data, or\npatches/pixels for the image data (Han et al., 2020; Doso-\nvitskiy et al., 2021). Crucially, attention quantiﬁes these\nrelationships via dynamic weights that depend on the input\ndata. This architectural solution is the strength and at the\nsame time the weakness of Transformer models. An atten-\ntion matrix scales quadratically in the length of the input\nsequence, making corresponding computations prohibitively\nexpensive for longer inputs.\n*Equal contribution 1Google Brain Robotics 2Columbia\nUniversity 3Independent Researcher 4University of Cambridge\n5University of Oxford 6Google Research 7The Alan Turing In-\nstitute 8Google. Correspondence to: Krzysztof Choromanski\n<kchoro@google.com>.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-\nright 2022 by the author(s).\nSeveral solutions were proposed to address this limitation.\nLocal attention (Vaswani et al., 2021; Parmar et al., 2019)\nexplicitly narrows down the attention context to a ﬁxed-size\nwindow, effectively zeroing out most attention weights. In\napplications where long-range attention is crucial (e.g. pro-\ntein modeling), other techniques were introduced. These\ninclude: (1) pooling mechanisms compressing sequences to\nshorter-ones agglomerating multiple-tokens signal (Avsec\net al., 2021; Dai et al., 2020), (2) hashing/clustering methods\nsparsifying attention by giving up attention modeling for to-\nkens from different learnable hash-buckets/clusters (Kitaev\net al., 2020; Roy et al., 2021), (3) low-rank/kernel-based\nmethods decomposing the attention matrix (Choromanski\net al., 2020; 2021; Katharopoulos et al., 2020; Peng et al.,\n2021; Xiong et al., 2021) and other (Qin et al., 2022).\nMasking is a powerful mechanism altering the attention\nmatrix by incorporating structural inductive bias. Flagship\nexamples include (1) causal attention, applied in generative\nTransformers (Yang et al., 2019), where the arrow of time\ninduces token-ordering with tokens not attending to their\nsuccessors in the sequences, (2) relative positional encoding\n(RPE, Shaw et al., 2018) reducing interactions between dis-\ntant tokens (but via a much more general mechanism than\nlocal attention) and (3) graph attention incorporating topo-\nlogical signal from the graph (Ying et al., 2021b; Velickovic\net al., 2018). RPE-mechanisms were shown to signiﬁcantly\nimprove speech models (Pham et al., 2020; Zhou et al.,\n2019) and masks obtained from shortest-path length matri-\nces were recently demonstrated to close the gap between the\nbest customized graph neural networks models and Trans-\nformers (Ying et al., 2021a). Straightforward application\nof the masking mechanism requires materialization of the\nattention matrix and consequently - impractical quadratic\ntime complexity for long input sequences (or large graphs).\nIn this paper we aim to answer the following question: Un-\nder which conditions can masking be incorporated into at-\ntention mechanisms in a scalable way, i.e. in sub-quadratic\ntime complexity in the number of input tokens?\nSo far this question was answered only partially. Causality\nwas incorporated in linear time into linear low-rank atten-\ntion via the so-called preﬁx sum mechanism by Choroman-\narXiv:2107.07999v8  [cs.LG]  28 Mar 2023\nTowards a general theory for scalable masked Transformers\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n0\n1\n2\n3\n4\n5\n6\n7\nL1-dist:  f(0) f(5)… …\nFigure 1.RPEs & Beyond: The (i,j)-entry of the regular RPE-mask is a (learnable) function f of the distance i−jbetween the ith and\njth token in the input sequence that can be interpreted as a 1d-grid, thus it has the so-calledToeplitz structure (ﬁrst graph and colored-matrix\nin the ﬁgure). The proposed d-dimensional RPE acts on the d-dimensional grid input with the length of the shortest path d(i,j) between\nnode iand jin the gird replacing expression i−jin the corresponding mask (d= 2 includes image input and d= 3, video input, see:\nsecond graph/matrix and third graph/matrix respectively). The corresponding mask is no longer Toeplitz, but isd-level block-Toeplitz.\nInterestingly, all these matrix classes support fast matrix-vector multiplication (via Fast Fourier Transform) and thus, based on our ﬁrst\nresult, corresponding masked low-rank attention can be performed in sub-quadratic time (see Sec. 3.2).\nski et al. (2021). The same was proven recently for the\nspecial class of stochastic RPEs (Liutkus et al., 2021). Even\nmore recently, a log-linear algorithm (applying Fast Fourier\nTransform) for incorporating general RPEs into low-rank\nattention was given by Luo et al. (2021). All these results\nleverage low-rank attention since so far that was the only\nknown scalable mechanism which can approximate in par-\nticular regular dense softmax attention. Hence, the starting\npoint of our analysis is also a low-rank attention model.\nOur contributions in this paper are as follows:\n1. We answer the above question in Sec. 3 by providing\na surprisingly simple characterization of the efﬁcient\nmasking mechanisms: as long as the masking-matrix\n(element-wise multiplied with the regular attention ma-\ntrix) supports sub-quadratic matrix-vector multiplica-\ntion, the corresponding mechanism can be incorpo-\nrated into low-rank attention in sub-quadratic time .\nInterestingly, as we explain later, this result includes\nall mentioned partial results as special cases.\n2. We present multiple consequences, leading in particu-\nlar to novel scalable d-dimensional RPE mechanisms\nthat can be applied in image and video processing (see\nFig. 1 and Sec. 3.2), efﬁcient implementations for the\nlow-rank attention with padding and packing mech-\nanisms (in common practical use for regular Trans-\nformers) (Sec. 3), and new scalable graph-based atten-\ntion masking applying shortest-path signal and graph-\ndiffusion kernels (Sec. 3.3, Sec. 4).\n3. Using our developed theory, we introduce a new\nmasked-attention ML-model called graph kernel at-\ntention Transformer (GKAT, Sec. 4.2), and conduct\ncomprehensive comparisons against nine other SOTA\ngraph neural networks (GNNs) in Sec. 5.\nWe cast the masking problem as a topological (graph-based)\nmodulation of unmasked attention, and leverage many math-\nematical techniques ranging from spectral analysis through\ndynamic programming on trees and random walks to new\nalgorithms for solving Markov processes on graphs. The\nproofs of all theoretical results are given in the Appendix.\n2. Preliminaries\nWe introduce notation used throughout the paper.\nDenote by L the number of input tokens. The attention\nused in a regular Transformer linearly projects their repre-\nsentations into three learnable matrices Q,K ∈RL×dQK ,\nV ∈RL×d called queries, keys and values respectively.\nDeﬁnition 2.1 (general masked attention). General masked\nsoftmax attention is of the following form, where N ∈\nRL×L is the logits-mask, and A ∈RL×L is the so-called\nmasked attention matrix (MAM):\nAttSM(Q,K,V,N) = D−1AV,\nA = exp(N + QK⊤/\n√\ndQK), D = diag(A1L).\n(1)\nHere exp(·) is applied element-wise, 1L is the all-ones vec-\ntor of length L, and diag(·) is a diagonal matrix with the\ninput vector as the diagonal. The time complexity of com-\nputing (1) is O(L2d). The above is a special instantiation\nof the general masked kernel attention which is deﬁned as:\nAttK(Q,K,V,M) = D−1AV,\nA = M ⊙K(Q,K), D = diag(A1L), (2)\nwhere ⊙denotes the element-wise (Hadamard) matrix prod-\nuct, K : Rd×Rd →R is some kernel function andK(Q,K)\nis a kernel matrix deﬁned as: K(Q,K)i,j = K(q⊤\ni ,k⊤\nj ) for\nthe ithrow qiof Q and the jth rowkj of K respectively. We\nTowards a general theory for scalable masked Transformers\ncall A′= K(Q,K) the unmasked attention matrix (UAM).\nThe softmax attention can be obtained from the kernel one\nby taking: K(x,y)\ndef\n= exp( x⊤y√\ndQK\n) (the so-called softmax\nkernel) and M\ndef\n= exp( N) (element-wise exponentiation).\nLow-rank attention methods provide (approximate) atten-\ntion computation in time linear in the length Lof the in-\nput sequence if no masking is applied (i.e. M is all-ones)\nand kernel K admits (at least in expectation) a dot-product\ndecomposition, i.e. K(x,y) = E[φ(x)⊤φ(y)] for some\n(usually randomized) mapping: φ : RdQK → Rm (and\nsome m >0). Such a decomposition (in fact more than\none!) exists in particular for the softmax kernel used in most\napplications of regular Transformers. We call φ(u) a (ran-\ndom) feature map (RFM) for u ∈Rd. For Q′,K′∈RL×m\nwith rows given asφ(q⊤\ni )⊤and φ(k⊤\ni )⊤respectively, RFM-\nbased kernel linearization leads directly to the efﬁcient un-\nmasked attention mechanism of the form:\nˆAttK(Q,K,V) = ˆD−1(Q′((K′)⊤V)),\nˆD = diag(Q′((K′)⊤1L)).\n(3)\nHere ˆAttK stands for the approximate attention and brackets\nindicate the order of computations. It is easy to see that such\na mechanism is characterized by time complexity O(Lmd)\nas opposed to O(L2d) for regular attention. If m ≪L,\ncomputational gains are obtained.\n3. Fast matrix-vector product is all you need\nOur ﬁrst result, a natural extension of the theoretical analysis\nby Luo et al. (2021), shows that as long as maskM ∈RL×L\nsupports sub-quadratic matrix-vector multiplication, it can\nbe incorporated into low-rank attention in sub-quadratic\ntime. This is explained in Lemma 3.1.\nAlgorithm 1 General Efﬁcient Low-Rank Masked Attention\nInput: Query/key matrices: Q,K ∈ RL×dQK , value\nmatrix V ∈ RL×d, mask M ∈ RL×L, procedure\nFastMultM : RL → RL calculating Mx (or its ap-\nproximation) for the input x ∈RL, kernel feature map:\nφ: RdQK →Rm. vec(·) denotes vectorization.\nOutput: Masked low-rank attention embeddings using φ.\n1. Compute matrices V1 ∈RL×(md), V2 ∈RL×m with\nrows deﬁned as: V1\ni: = vec( φ(k⊤\ni )vi), V2\ni: = φ(k⊤\ni )⊤,\nwhere ki/vi stands for the ith row of K/V.\n2. Take ˜D1 = [FastMultM(V1\n:1),..., FastMultM(V1\n:md)] ∈\nRL×md, ˜D2 = [FastMultM(V2\n:1),..., FastMultM(V2\n:m)] ∈\nRL×m for V1/2\n:i denoting ith column of V1/2.\n3. Output the embedding ri of the ith tokens as:\nri = φ(q⊤\ni )⊤devec( ˜D1\ni:)\nφ(q⊤\ni )⊤( ˜D2\ni:)⊤ , where qi is the ith row of Q and\ndevec(·) devectorizes its input back to Rm×d.\nLemma 3.1 (Tractable Mask Lemma). Assume that mask\nM ∈ RL×L from Deﬁnition 2.1 supports matrix-vector\nmultiplication in time TM(L). Then the general masked\nkernel attention algorithm with maskM can be implemented\nin time O((TM(L) + L)md).\nThe algorithm is given in the algorithmic box 1. We analyze\nit in the Appendix, but the intuition is that, in the unmasked\nlow-rank setting, attention embeddings could be obtained\nfrom the action of φ(qi) on the ﬁxed (token-independent)\nmatrix of shape Rm×d summarizing all the tokens, whereas\nin the masked case the matrix depends on each token, but\ncan be obtained from mask-vector products.\nCausal attention: Note that the preﬁx-sum algorithm\nfrom (Choromanski et al., 2021) is a special instantiation\nof Algorithm 1. Indeed, causality is encoded by the lower-\ntriangular mask M such that: Mi,j = 1 for j ≤i and\nMi,j = 0 otherwise. Every product Mx is trivially a vector\nof preﬁx sums: x1 + ...+ xi for i= 1,...,L and thus can\nbe computed in time O(L).\nPacking & Padding: Both masking mechanisms are stan-\ndard Transformers’ techniques used to optimize attention\ncomputation on TPUs. The former packs multiple sequences\nin one super-sequence. Mask is used here to prevent cross-\nsequence attention. The latter adds fake tokens at the end of\nthe legitimate input sequence (used if input’s length varies).\nMask is used here to prevent attention to fake tokens. Both\nmasks M trivially support linear matrix-vector multiplica-\ntion (see: Fig. 2) and thus both packing and padding can be\nincorporated into low-rank attention in time linear in L.\npadded\nFigure 2.Left: Padding with the super-sequence consisting of four\nsequences and its corresponding mask M. Right: Packing with\nthree fake padded tokens and its corresponding mask M. For\nboth masks, colored entries are equal to one and non-colored are\nequal to zero. Both masks trivially support linear matrix-vector\nmultiplication.\n3.1. Mask M as graph topology encoder\nFrom now on we will think about mask M ∈RL×L as a\nweighted adjacency matrix Adj(G) of some weighted graph\nG = ( V,E,W ) with nodes/vertex-set V of size L, edge\nset E and edge-weight function W : E →R. Lemma\n3.1 combined with this observation leads to several far-\nreaching conclusions regarding efﬁcient incorporation of\nvarious masking mechanisms to Transformers, to which we\ndevote the remaining part of our theoretical analysis.\nTowards a general theory for scalable masked Transformers\n3.2. D-dimensional Relative Positional Encodings\nWe need the following deﬁnition:\nDeﬁnition 3.2 (block-Toeplitz matrices). We say that a ma-\ntrix M ∈RL×L is Toeplitz (or 1-level block Toeplitz) if\nthere exists some ξ : Z →R such that Mi,j = ξ(i−j).\nWe say that M ∈RL×L is d-level block-Toeplitz for d≥2\nif M = (Bi,j) consists of block-matrices Bi,j taken from\nsome set {A1,..., Ar}of (d−1)-level block-Toeplitz ma-\ntrices and if each block Bi,j is replaced with the index kof\nits corresponding matrix Ak, a Toeplitz matrix is obtained.\nConsider the unweighted (i.e. all-one edge-weights) 1d-grid\ngraph Gbase (see: left graph in Fig. 1) and a complete graph\n(i.e. with all possible edges) Gobtained from it by deﬁning\neach weight as Wi,j = f(distGbase (i,j)) for some (learn-\nable) function f : N →R and where distGbase (i,j) is the\nlength of the shortest path between iand jin Gbase. If we\ndeﬁne M = Adj(G) then we get the regular RPE mecha-\nnism with the 1d-graph interpreted as the input sequence.\nNote that M deﬁned in such a way is Toeplitz and thus\nsupports O(Llog(L)) matrix-vector multiplication via Fast\nFourier Transform (FFT). Thus low-rank RPE-masked at-\ntention can be conducted in O(Ldmlog(L)) time. This was\nthe observation of Luo et al. (2021). What if we replace\nthe 1d-grid with the d-dimensional grid and deﬁne M in the\nanalogous way? The idea is to maintain the initial structure\nof the topologically more complicated input, e.g. 2d-grid\nfor images (with nodes as patches or even individual pixels)\nor 3d-grid for videos (with 2d-slices as different frames).\nThere is a particularly elegant answer to this question:\nLemma 3.3 (d-dimensional RPEs). Consider the gener-\nalized RPE-mechanism for the d-dimensional grid input\ndeﬁned above. Then there exists an ordering of input nodes\nsuch that M is a d-level block-Toeplitz matrix (see: Fig. 1).\nSince d-level block-Toeplitz matrices support O(Llog(L))\nmatrix-vector multiplication via FFT for any ﬁxed constant\nd(see Lee, 1986), Lemma 3.3 immediately leads to efﬁcient\ncorresponding masked attention computation.\n3.3. More general graph-masks using shortest-paths\nSo far Gbase was assumed to have a grid structure. What\nif we replace it with an arbitrary weighted graph? The\nfollowing natural question arises: Which condition does\nGbase and mapping f : R →R need to satisfy for the\nmask M\ndef\n= [ f(distGbase (i,j))]i,j=1,...,L to support sub-\nquadratic matrix-vector multiplication ?\nWe call such a pair (Gbase,f) tractable. From what we\nhave said so far, we conclude that:\nCorollary 3.4. If Gbase is an unweighted grid (of any di-\nmensionality) then (Gbase,f) is tractable ∀f : R →R.\nIn several bioinformatics applications, e.g. molecular as-\nsembly trees (Artemova et al., 2011), the underlying input’s\ntopology is a forest (e.g. a tree). We prove the following:\nLemma 3.5. If Gbase is a forest and f(z) = exp( τ(z))\nfor afﬁne mapping τ, then (Gbase,f) is tractable and the\nrelated mask supports linear matrix-vector multiplication.\nSketch of the proof: The efﬁcient algorithm for comput-\ning w = Mx in this case is an application of the dy-\nnamic programming method for rooted trees. The algorithm\nﬁrst computes for each node i the following expression:\nsi = ∑\nj∈Ti exp(τ(dist(i,j)))xj, where Ti stands for the\nsubtree rooted in i(in the bottom-up fashion from leaves to\nthe ﬁxed root). This is followed by the computation of the\nfollowing expression: wi = ∑\nj∈T exp(τ(dist(i,j)))xj\nfor every node in the order from the root to the leaves (lever-\naging already computed si). Details are given in the Ap-\npendix and computations are illustrated in Fig. 3.\nWe ﬁnd a comprehensive description of tractable (Gbase,f)\nan exciting analytic and combinatorial open problem.\n…\n …\nroot root\nFigure 3.Illustration of sketch of the proof of Lemma 3.5. The\ndirections of arrows show computation-ﬂow. In phase I, si-terms\nare calculated in bottom-up fashion (from leaves to the root). The\nvalue of si involving paths in i-rooted subtrees (red path with\ndiscarded directions) is updated based on svk -terms involving\npaths in subtrees Tvk . To complete calculations, in phase II paths\nto nodes outside of the i-rooted tree are considered (purple path\nwith directions discarded). Their contribution is calculated from\nthe already computed wp(i) for the parent p(i) of node iand si.\n3.4. Low-rank masking\nNote that in all previously considered cases, mask M is\nin general full-rank. However in several applications M\ncan be assumed to have (at least in expectation) a low-rank\ndecomposition, i.e.: M = E[M1M2] for some (random)\nM1 ∈RL×r, M2 ∈Rr×L and some 0 < r≪L. A ﬂag-\nship example is the stochastic RPE mechanism presented in\n(Liutkus et al., 2021) corresponding to (logits-added) dot-\nproduct mask N translating to the softmax-kernel values\nmask M. The latter one can be low-rank decomposed using\nany random feature map based softmax-kernel linearization\nmechanism, e.g. from (Choromanski et al., 2021). In such\na case, matrix-vector product v = Mx can be computed\n(approximately) as: ˜v = (M1(M2x)) in time O(Lr), lead-\ning to overall time complexity O(Lmrd) of the attention\nmodule using mask M.\nTowards a general theory for scalable masked Transformers\n4. Masking with graph kernels\nMasks deﬁned by shortest paths were shown to provide ef-\nfective inductive bias for graph data (see Ying et al., 2021b),\nyet they cannot be interpreted as applying any valid kernel\nfunction on graph nodes and are very sensitive to small\nchanges in the graph. A prominent class of kernel-functions\nK : V ×V →R deﬁned on pairs of graphs nodes is the\nfamily of graph-diffusion or heat kernels (GDKs). Thus it is\nnatural to identify masks M for input graph data Gwith the\ngraph diffusion kernel matrices KK = [K( i,j)]i,j=1,...,L.\nGDK is deﬁned, for a hyperparameter λ> 0 and Xi denot-\ning the ith power of matrix X, as:\nKK = exp(−λT)\ndef\n=\n∞∑\ni=0\n(−λ)iTi\ni! , (4)\nwhere either: T = L for the Laplacian matrix L =\nD −Adj(G) and D = diag([deg(i)]L\ni=1); or T = LD−1\n(normalized Laplacian case) or T = −Adj(G).\nGDK is related to the diffusion process (Kondor & Lafferty,\n2002) which describes in particular heat propagation. In a\nvacuum, the solution of the partial differential heat equation\nis the Gaussian-kernel, and in graphs it leads to GDK. Nodes\nbetter connected with each other (graph diffusion kernel\nquantiﬁes it via the number of different-length walks with\nlonger walks exponentially-deprioritized) give rise to larger\nkernel values. Finally, t = 1\nλ can be interpreted as time\nwhen the solution is taken. GDK imprints topological signal\nof the propagation medium via left heat-signature. As t→\n∞the kernel “ﬂattens” and the topological signal is lost.\nDirect computation of the GDK matrixKK is of O(L3) time\ncomplexity, thus prohibitively expensive even for sparse\ninput graphs. However, a key observation is that Lemma 3.1\nteaches us that for efﬁcient masked low-rank attention we\nonly need to compute efﬁciently the action exp(−λT)x of\nKK on a given x ∈RL. This leads to our next result.\nTheorem 4.1 (scalable Laplacian-GDK masking) . Let a\nmask M be deﬁned as M = exp( −λA), for A = L\nor A = LD−1, where L is the Laplacian of the input\ngraph, and D = diag([deg(i)]L\ni=1). Then low-rank masked\nattention can be computed in time ˜O((|E|+ L) log(2 +\n∥A∥F)md), where ˜Ohides polylog(L) factors, |E|is the\nnumber of graph edges and ∥·∥F is the Frobenius norm.\nThe theorem is a consequence of Lemma 3.1 and Theorem\n1.2 from (Orecchia et al., 2012). We see that if|E|= o(L2),\nthe masked attention mechanism is sub-quadratic in L.\nLow-rank attention & Markov processes with random\ninitial distributions: As noted by Orecchia et al. (2012),\nthe heat kernel matrix for T = LD−1 can be interpreted as\nthe probability transition matrix of the discrete-time random\nwalk where ﬁrst the number of steps i is sampled from\na Poisson distribution with mean λ, and then i steps of\nthe natural random walk are performed on G. Looking at\nAlgorithm 1, we conclude that here the low-rank structure of\nattention enables us to incorporate the GDK mask by solving\nthat process in mdinitial (randomized) distributions over\nnodes (randomization coming from mapping φ) rather than\nin all Lone-hot initial distributions (that would correspond\nto the reconstruction of the entire transition matrix).\nRemark: The literature on efﬁciently computing the ac-\ntions of matrix-exponentials (which is our main focus in this\nsection) is very rich (Al-Mohy & Higham, 2011), partially\nbecause of straightforward applications in the theory of dif-\nferential equations (Li et al., 2021). In principle, each of\nthese methods can be used by our algorithm.\n4.1. Hypercubes with graph-diffusion kernels\nIf the underlying graph is a hypercube, then GDK with\nT = L has a closed-form formula. The following is true\n(Kondor & Lafferty, 2002): KGDK(i,j) ∝(tanhλ)dist(i,j)\nfor the hyperbolic tangent tanh. Thus, as in Sec. 3.2, the\ncorresponding mask M is block-Toeplitz and hypercube-\ninduced GDK-masking can be incorporated into low-rank\nattention in O(Lmdlog(L)) time.\n4.2. Low-rank masking strikes back for GDKs\nWe now propose a proxy of the GDK with T = Adj(G)\nsuch that the corresponding kernel matrix admits (in expec-\ntation) low-rank decomposition as in Sec. 3.4. Thus, based\non the theory we developed, the corresponding mask M\ncan be efﬁciently incorporated into low-rank attention with\nno need to call efﬁcient solvers for the actions of matrix\nexponentials. We call our graph kernel the Random Walks\nGraph-Nodes Kernel or RWGNK.\nIntuitively, the value of the RWGNK for two nodes is given\nas a dot-product of two frequency vectors that record vis-\nits in graph nodes of random walks beginning in the two\nnodes of interest. More formally, for the hyperparameters\nλ,α ≥0,and two random walks ω(k), ω(l) with stopping\nprobability 0 ≤p≤1 (or of a ﬁxed length) starting at kand\nlrespectively, the RWGNK is given as:\nKλ,α\np (k,l) = Eω(k)[fω(k),λ\nk ]\n∥Eω(k)[fω(k),λ\nk ]∥α\n2\n(\nEω(l)[fω(l),λ\nl ]\n∥Eω(l)[fω(l),λ\nl ]∥α\n2\n)⊤\n.\n(5)\nThe (row) frequency vector fω(h),λ\nh for h ∈V is given as\nfω(h),λ\nh (i)\ndef\n= ∑\ne∈Lω(h)(i) λe, where Lω(h)(i) is the set of\nlengths of those preﬁx sub-walks of a given random walk\nω(h) that end at i(where the preﬁx sub-walk of the walk\n(j1,.j2,...,j t) is any walk of the form (j1,...,j r) for some\nr ≤t or an empty walk). Note that Eq. 5 leads to the\ndesired representation of Kλ,α\np as Kλ,α\np (k,l) = Ψ(k)Ψ(l)⊤,\nTowards a general theory for scalable masked Transformers\nFigure 4.From left to right: unweighted graph G, its adjacency matrix Adj(G), its GDK matrix exp(Adj(G)) and RWGNK-matrix with\nwalk length of 3 and α= 1. Colored cells measure the relationship among pairs of nodes (darker is stronger). The last two matrices can\nbe thought of as continuous smoothings of Adj(G).\nwhere Ψ(h) is the renormalized expected frequency vector.\nIn practice, expectations are replaced by Monte Carlo sam-\nplings over a few random walks, and vectors Ψ(h) are not\nstored explicitly but in the form of weighted lookup tables.\nFigure 4 compares RWGNK-induced mask with the regular\nGDK-mask and the adjacency matrix mask. We call a Trans-\nformer applying low-rank masked attention via RWGNK, a\nGraph Kernel Attention Transformer (or GKAT).\nNext we explore the connection of RWGNKs with GDKs.\nWe denote by dmax,dmin the maximum and minimum de-\ngree of a vertex in G respectively.\nTheorem 4.2 (RWGNKs count discounted numbers of\nwalks). The following is true for the kernel matrix\nKλ,α\np (G) = [K λ,α\np (k,l)]k,l∈V(G) of the RWGNK kernel\nwith 0 ≤λ≤1, α= 0 and 0 <p< 1 for a graph G with\nvertex set V(G) of size N (element-wise matrix inequality):\nΓ\n( ρ\ndmax\nAdj(G)\n)\n≤Kλ,α\np (G) ≤Γ\n( ρ\ndmin\nAdj(G)\n)\n,\n(6)\nwhere ρ = (1 −p)λ and Γ(A) = ∑∞\ni=0(i+ 1)Ai. Us-\ning the fact that Adji(G) encodes the number of walks\nof length i between pairs of vertices in G, we conclude\nthat Kλ,0\np (k,l) = ∑∞\ni=0 ci\nk,lrk,l(i), where: rk,l(i) is the\nnumber of walks of length i between nodes: k and l\nand\ni√i+1(1−p)λ\ndmax\n≤ ck,l ≤\ni√i+1(1−p)λ\ndmin\n. Note that val-\nues of GDK with parameter λ satisfy: GDKλ(k,l) =∑∞\ni=0 ˜ci(k,l)rk,l(i), where: ˜c(k,l) = λ\ni√\ni! . In practice, it\nsufﬁces to have random walks of ﬁxed length (instead of tak-\ning p> 0) (see: Sec 5). Furthermore, by taking α> 0 (e.g.\nα= 1) we can guarantee that kernel values are bounded.\n5. Experiments\nWe focus on the GKAT architecture introduced in Sec. 4.2\nas a prominent instantiation of the general mechanism pre-\nsented in this paper and experiments with 2-level block-\nToeplitz masking mechanisms introduced in Sec. 3.2 for\nvision Transformers.\nRegarding GKAT, we conducted exhaustive evaluations on\ntasks ranging from purely combinatorial to bioinformatics,\nand benchmarked 10 different methods. All these experi-\nments were run on a single Tesla P100 GPU with 16GB\nmemory. Experiments with vision Transformers were con-\nducted on the ImageNet dataset.\n5.1. Combinatorial Classiﬁcation\nIn this section we focus on the problem of detecting local\npatterns in graphs. A model takes a graph G as an input\nand decides whether it contains some graph from the given\nfamily of graphs Has a subgraph (not necessarily induced)\nor is H-free. This benchmark tests the abilities of different\nmethods to solve purely combinatorial tasks.\nFigure 5.Five motifs (patterns) used in the ﬁrst class of combina-\ntorial classiﬁcation experiments. For each pattern H, an algorithm\nis trained to distinguish between graphs G containing H and those\nthat are H-free. A naive brute-force algorithm for conducting this\nhas time complexityΩ(Nh), where his the number of nodes of the\nmotif, prohibitively expensive for all these motifs (since h≥9).\n5.1.1. E RD ˝OS-R ´ENYI RANDOM GRAPH WITH MOTIFS\nData Generation: Following the procedure from (Nikolent-\nzos & Vazirgiannis, 2020), we used ﬁve binary classiﬁcation\ndatasets consisting of random Erd˝os-R´enyi (ER) graphs con-\nnected with motifs (positive example) or other smaller ER\ngraphs with the same average degree as a motif (negative\nexample), see Fig. 5 (details in the Appendix, Sec. A.2).\nFor each dataset we constructed S = 2048 positive and S\nnegative examples.\nTested Algorithms & Parameter Setting: We tested our\nGKAT, graph convolution networks (GCNs, Kipf & Welling,\n2017), spectral graph convolution networks (SGCs, Def-\nferrard et al., 2016) and graph attention networks (GATs,\nVelickovic et al., 2018). A feature vector in each vertex\nwas of length l = 5 and contained top ordered l degrees\nof its neighbors (if there were fewer than lneighbors, we\npadded zeroes). A dataset for each motif was randomly split\ninto 75%/25% training/validation set. We chose: the num-\nber of epochs E = 500, batch size B = 128, used Adam\nTowards a general theory for scalable masked Transformers\nFigure 6.Model accuracy comparison of all four methods: GKAT, GAT, GCN and SGC on the motif-detection task. All architectures are\n2-layer. GKAT outperforms other algorithms on all the tasks. See also Appendix:Sec. A.4 for the tabular version with 100K-size graphs.\noptimizer with learning rate η = 0.001 and early-stopped\ntraining if neither the validation loss nor validation accuracy\nimproved for c= 80 continuous epochs.\nWe applied 2-layer architectures. For GCNs and SGCs,\nwe used h = 32 nodes in the hidden layer. For SGC, we\nfurthermore bound each hidden layer with 2 polynomial\nlocalized ﬁlters. For GAT and GKAT, we used2 attention\nheads, with h = 9 nodes in the hidden layer to make all\nmodels of comparable sizes. In GKAT we used random\nwalks of length τ = 3. The results are presented in Fig. 6.\nGKAT outperforms all other methods for all the motifs.\n5.1.2. G LOBAL GRAPH PROPERTIES & DEEP VS DENSE\nNext we took as Han inﬁnite family of motifs rather than\njust a single motif. The algorithm needs to decide whether\na graph contains an induced cycle of length >T for a given\nconstant T. Thus the motif itself became a global property\nthat cannot be detected by exploring just a close neighbor-\nhood of a node. In this experiment we focused also on\nthe “depth versus density” trade-off. Shallow neural net-\nworks with dense attention are capable of modeling deeper\nnetworks relying on sparse layers, yet the price is extra com-\nputational cost per layer. We test here whether architectures\nthat apply RWGNK kernels leveraging efﬁcient decompos-\nable long-range attention from Sec. 4.2 can also replace\ndeeper counterparts or if they lose their expressiveness.\nFigure 7.Comparison of the two-layer GKAT with different vari-\nants of GCNs, GATs and SGCs, varying by the number of hidden\nlayers. Shallow GKAT architecture has the expressiveness of\ndeeper version of its counterparts and in fact outperforms many of\nthem (e.g. graph convolution networks.)\nDataset Generation: We created S = 2048 random bi-\nnary trees, each having 50 nodes, with 75%/25% for train-\ning/validation. For each tree, we constructed a positive\nexample, by connecting two nodes with the farthest dis-\ntance from each other (a negative example was obtained by\nconnecting two random vertices, but not farthest from each\nother). Note that a positive example constructed in such a\nway has shortest induced cycle of length P + 1, where P is\nthe diameter of the tree.\nTested Algorithms & Parameter Setting: We used the\nsame algorithms as before and run detailed ablation studies\non the depth of the GKAT competitors, by comparing two-\nlayer GKAT with GATs, GCNs and SGCs of up to six layers.\nFor a fair comparison, we used models with a comparable\nnumber of parameters. For the two-layer GKAT, we applied\n8 heads in the ﬁrst layer, and 1 head in the second layer.\nThe dimension of each head was d = 4 . The last layer\nwas fully-connected with output dimensionality o= 2 for\nbinary classiﬁcation. We applied random walk length of\nτ = 6 . For GCN, GAT and SGC, we tested number of\nlayers ranging from 2 to 6. We controlled the number of\nnodes in the hidden layer(s) for GCN, GAT and SGC, and\nthe number of attention heads in each head for GAT so that\ntheir total number of trainable parameters was comparable\nwith that of our two-layer GKAT. All other parameters were\nchosen as in Sec. 5.1.1. More details on parameter settings\nand additional ablation tests over random walk length of\nGKAT are given in Table 5 and Fig. 9 in the Appendix (Sec.\nA.2). Our main results are presented in Fig. 7.\nWe see that a shallow two-layer GKAT beats all GCN-\nvariants (also deeper ones) as well as GATs and SGCs with\n<4 layers by a wide margin. A two-layer GKAT is asymp-\ntotically equivalent to the four-layer GAT and SGC, yet as\nwe show in Sec. 5.3, is faster to train and run inference on.\n5.2. Bioinformatics & Social Networks experiments\nDatasets: We tested GKAT for graph classiﬁcation tasks on\n9 standard and publicly available bioinformatics and social\nnetworks datasets (Kersting et al., 2016) using a carefully\ndesigned model selection and assessment framework for a\nfair comparison (Errica et al., 2020). The former include:\nD&D (Dobson & Doig, 2003), PROTEINS (Borgwardt et al.,\n2005), NCI1 (Wale et al., 2008) and ENZYMES (Schom-\nburg et al., 2004), and the latter: IMDB-BINARY , IMDB-\nMULTI, REDDIT-BINARY , REDDIT-5K and COLLAB\n(Yanardag & Vishwanathan, 2015), see also Sec. A.3.1.\nTested Algorithms: We compared GKAT with top GNN\nmethods used previously for that data: DCGNN (Zhang\nTowards a general theory for scalable masked Transformers\net al., 2018), DiffPool (Ying et al., 2018), ECC (Simonovsky\n& Komodakis, 2017), GraphSAGE (Hamilton et al., 2017)\nand RWNN (Nikolentzos & Vazirgiannis, 2020), which are\nselected based on their popularity and architectural differ-\nences. For bioinformatics datasets, but ENZYMES, we used\nthe Molecular Fingerprinting (MF, Ralaivola et al., 2005;\nLuzhnica et al., 2019) as a baseline. This ﬁrst applies global\nsum pooling and then a single-layer MLP with ReLU ac-\ntivations. For social datasets and ENZYMES, we applied\nthe DeepMultisets (DM) method (Zaheer et al., 2017) as a\nbaseline. We did not add the numbers for the GIN method\n(Xu et al., 2018) since we could not reproduce its reported\nresults for the models of size similar to GKAT.\nTable 1.Performance of different algorithms on the bioinformatics\ndata. For each dataset, we highlighted/underlined the best/second\nbest method. GKAT is the best on three out of four tasks.\nD&D NCI1 Proteins Enzymes\nBaseline 78.4 ±4.5%69.8±2.2% 75.8±3.7% 65.2±6.4%\nDGCNN 76.6±4.3% 76.4±1.7%72.9±3.5% 38.9±5.7%\nDiffPool 75.0±3.5%76.9±1.9%73.7±3.5% 59.5±5.6%\nECC 72.6±4.1% 76.2±1.4% 72.3±3.4% 29.5±8.2%\nGraphSAGE72.9±2.0% 76.0±1.8% 73.0±4.5% 58.2±6.0%\nRWNN 77.6±4.7% 71.4±1.8% 74.3±3.3% 56.7±5.2%\nGKAT 78.6±3.4% 75.2±2.4%75.8 ±3.8% 69.7 ±6.0%\nTable 2.Performance of different algorithms on the social network\ndata. GKAT is among two top methods for four out of ﬁve tasks.\nIMDB-B IMDB-M REDDIT-B REDDIT-5K COLLAB\nBaseline 70.8±5.0%49.1 ±3.5%82.2±3.0% 52.2±1.5% 70.2±1.5%\nDGCNN 69.2±5.0% 45.6±3.4% 87.8±2.5% 49.2±1.2% 71.2±1.9%DiffPool 68.4±3.3% 45.6±3.4% 89.1±1.6% 53.8±1.4%68.9±2.0%ECC 67.7±2.8% 43.5±3.1% OOM OOM OOMGraphSAGE68.8±4.5% 47.6±3.5% 84.3±1.9% 50.0±1.3%73.9±1.7%RWNN 70.8±4.8%47.8±3.8%90.4±1.9%51.7±1.5% 71.7±2.1%\nGKAT 71.4±2.6%47.5±4.5% 89.3±2.3%55.3±1.6%73.1±2.0%\nGKAT Setting: We used a two-layer GKAT followed by\nthe baseline layers: we ﬁrst applied an attention layer with\nkheads (a hyperparameter to be tuned), and then another\none with one head to aggregate topological information\non graphs. Next, we applied either the MF method or the\nDM method to further process the aggregated information.\nThe random walk length τ in each GKAT layer satisﬁed\nτ ≤4 and depended on the evaluated datasets. The average\ngraph diameter shown in Table 6 in the Appendix helps to\ncalibrate walk length. We chose it to balance the pros of\nusing a shallow architecture and the cons of information loss\nfrom dense layer compression. GKAT increased the number\nof the baseline’s parameters by a negligible fraction.\nTraining Details: We used a 10-fold CV for model as-\nsessment, and an inner holdout with 90%/10% train-\ning/validation split for model selection following the same\nsettings (Errica et al., 2020). We then trained the whole\ntraining-fold three times, randomly holding out 10% of data\nfor early stopping after model selection in each fold. The\naverage score for these runs was reported in Table 1 & 2.\nThe results from Table 1 and Table 2 show that GKAT is\nthe best on three out of four bioinformatics datasets and is\namong two best methods on four out of ﬁve social network\ndatasets. It is the only GNN method that consistently out-\nperforms baseline on all but one bioinformatics dataset (bio-\ndata beneﬁts more than others from efﬁcient longer-range\nattention modeling as showed by Choromanski et al. (2021)).\nIn the Appendix (Sec. A.5) we provide additional com-\nparisons of GKAT with GAT on citation networks, where\nGKAT outperforms GAT on two out of three tested datasets.\n5.3. Space & Time Complexity Gains of GKAT\nWe measured speed and memory improvements coming\nfrom GKAT as compared to GAT as well as accuracy loss\nin comparison to Transformer using GKAT masking, but\nexplicitly computed (GKAT-0), see Table 3. We decided to\nreport relative rather than absolute numbers since the former\nare transferable across different computational setups. The\naccuracy gaps of the corresponding GKAT-0 and GKAT\nmodels (obtained after the same # of epochs) are marginal,\nyet GKAT yields consistent speed and memory gains as\ncompared to GAT per attention layer, particularly substantial\nfor very large graphs as those from Citeseer and Pubmed.\nTable 3.Speed & Space Complexity gains provided by GKAT.\nFirst row: memory compression (lower better). Second & third\nrow: speedup in training and inference respectively per one at-\ntention layer as compared to GAT. Last row: accuracy loss as\ncompared to GKAT-0 applying brute-force RWGNK masking. We\nused four datasets from Sec. 5.1.1, a dataset from Sec. 5.1.2 (Tree)\nand two citation network datasets (see: Sec. A.5): Citeseer and\nPubmed with graphs of much larger sizes and on which GKAT\nalso outperforms GAT. We applied r = 256 random features to\nlinearize softmax kernel for features in nodes for citation network\ndatasets, r= 16/8 for datasets from Sec. 5.1.1/ 5.1.2.\nCavem. Circle Grid Ladder Tree Citeseer Pubmed\nGKAT / GKAT-0 memory0.54 0.53 0.55 0.52 0.95 0.18 0.07\ntrain speedup vs GAT1.40x 1.41x 1.42x 1.40x 1.10x 5.10x 9.50x\ninf speedup vs GAT1.46x 1.49x 1.49x 1.47x 1.12x 5.21x 9.54x\nGKAT-0 - GKAT (accur.)0.07% 0.09% 0.08% 0.07% 0.06% 0.05% 0.06%\nIn Table 4 we show that GKAT is also faster that its counter-\nparts (GCN, GAT, SGC) in terms of wall clock time needed\nto reach particular accuracy levels, by comparing accuracy\nlevels reached by different models in a given wall clock time\nbudget (time GKAT needs to complete ﬁrst 100 epochs).\nTable 4.Running time of training different networks on datasets\nfrom Sec. 5.1.1 and Sec. 5.1.2. For GCN, GAT and SGC, we\nreported the accuracy with 2 layers. For GKAT, we used a 2-layer\narchitecture and reported the accuracy with a ﬁxed walk length of\n6 for Induced Cycle Detection, and of 3 for motifs from Sec. 5.1.1.\nInduced Cycle Caveman Circle Grid Ladder Circle Ladder\nGCN 63.2% 62 .1% 71 .4% 59.3% 66.7% 87 .4%GAT 77.0% 69 ,1% 80.6% 73.8% 75.9% 93 .7%SGC 56.6% 55 .4% 64 .7% 58.2% 59.1% 66 .5%\nGKAT 83.6% 85.1% 83.3% 77.1% 82.4% 94.6%\nTowards a general theory for scalable masked Transformers\n5.4. 2-level block-Toeplitz masks for vision data\nIn this section (see: Fig. 8), we present additional results\nin the vision domain, showing large, +2.5-3.4 percentage\npoint, gains in accuracy arising from applying the 2-level\nblock Toeplitz masking introduced in the paper (see: Sec.\n3.2) on top of the regular vision Performer. As explained\nbefore, the price we pay for these gains is only a log(L)\nmultiplicative factor in time complexity.\nFigure 8.Comparison of regular Performers using x2, ELU + 1,\nand ReLU kernels, with their counterparts applying 2-level block\nToeplitz masking from our paper on the ImageNet classiﬁcation\ntask (hidden size = 768, 12 layers & heads, MLP dim = 3072).\n6. Additional results & some open questions\nIn this section, we present additional theoretical results\nregarding the theory of the scalable efﬁcient masked Trans-\nformers that we have developed. We focus on masking\nmechanisms for the tree-graph inputs. We also discuss some\nopen problems for future work.\nIn Section 3.3 we showed that for the speciﬁc classes of\nfunctions f (exponentials of afﬁne mappings of the shortest-\ndistance paths) and arbitrary weighted trees, pairs(Gbase,f)\nare tractable. Here we will provide additional related results,\nbut for arbitrary functions f. Our ﬁrst result is as follows:\nLemma 6.1. If T = Gbase is an unweighted tree and f is\nan arbitrary function then the corresponding mask matrix\nM = M(Gbase,f) supports matrix-vector multiplication\nin time O(L·log2(L)). Thus (Gbase,f) is tractable.\nWe now show that if the diameter diam(T) of the tree T is\nof the order of magnitude o(log2(L)), where L= |V(T)|,\na more efﬁcient algorithm can be used.\nLemma 6.2. If T = Gbase is an unweighted tree and f is\nan arbitrary function then the corresponding mask matrix\nM = M(Gbase,f) supports matrix-vector multiplication\nin time O(L·diam(Gbase)).\nCorollary 6.3. From the above, we obtain an O(Llog(L))\nalgorithm for computing the action of M on x if Gbase is\na tree with: (a) a node of degree ≥2 that has same-length\npaths to all the leaves and (b) all other nodes of degree ≥3\n(e.g. complete binary tree).\nCorollary 6.4. The algorithm from the proof of Lemma\n6.2 (see:Appendix) can be used to improve algorithm from\nLemma 6.1 (that works for graphs with arbitrary diame-\nters) if the input T to that algorithm satisﬁes: diam(T) =\no(log2(|V(T)|)). Note that computing the diameter of any\ntree Tcan be done in timeO(|V(T)|) by running two depth-\nﬁrst-search procedures: the ﬁrst one from an arbitrary nodes\nvof T and the second one from the node farthest from vin\nT (obtained via the ﬁrst depth-ﬁrst-search procedure).\nWe leave the Reader with an interesting open problem:\nCan we improve Lemma 6.1 to obtain O(Llog(L)) running\ntime, i.e. replace the log2(L) factor with a log(L) factor?\nNote that if the unweighted tree is a single path, the an-\nswer to the above question is: Yes. Indeed, this is precisely\nthe 1D-RPE setting that we have discussed before. Fur-\nthermore, since in that setting the problem reduced to the\nmultiplication with Toeplitz matrices, inherently relying on\nthe FFT, the log(L) factor in all likelihood cannot be im-\nproved (unless FFT can be replaced with a faster algorithm\nor multiplication with Toeplitz matrices is conducted ap-\nproximately). Still, we do not know whether for general\nunweighted trees (or even nontrivial tree-extensions of the\npath) we can reach the O(Llog(L)) running time.\nIt might be also interesting to analyze how those of our\npresented methods that work for tree input data can be ex-\ntended to non-tree graphs, but with low treewidth (Cygan\net al., 2015), that can be thought of as relaxations of trees.\n7. Conclusion\nWe presented a holistic approach to incorporating masking\ninto scalable low-rank Transformers. We provided general\ntheoretical results which include earlier results as special\ncases. We conducted comprehensive empirical evaluations\nof the new instantiations of the mechanism for graph data.\nWe focused in the paper not only on scalable variants, but\nhave introduced several new masking meethods that can\nbe used on their own, even in regular Transformers. These\ninclude in particular d-level block-Toeplitz masking mecha-\nnisms with applications in vision and video processing, that\nwe believe might lead to new vision-Transformers archi-\ntectures. We show that topological masking is a powerful\ninductive bias and that corresponding “topological Trans-\nformers” turn out to be effective in various domains such as\nbioinformatics and vision.\n8. Acknowledgements\nAW acknowledges support from a Turing AI Fellowship\nunder grant EP/V025279/1, The Alan Turing Institute, and\nthe Leverhulme Trust via CFI.\nTowards a general theory for scalable masked Transformers\nReferences\nAl-Mohy, A. H. and Higham, N. J. Computing the action of\nthe matrix exponential, with an application to exponential\nintegrators. SIAM J. Sci. Comput., 33(2):488–511, 2011.\ndoi: 10.1137/100788860. URL https://doi.org/\n10.1137/100788860.\nArtemova, S., Grudinin, S., and Redon, S. Fast construction\nof assembly trees for molecular graphs.J. Comput. Chem.,\n32(8):1589–1598, 2011. doi: 10.1002/jcc.21738. URL\nhttps://doi.org/10.1002/jcc.21738.\nAvsec, ˇZ., Agarwal, V ., Visentin, D., Ledsam, J. R., Grabska-\nBarwinska, A., Taylor, K. R., Assael, Y ., Jumper, J.,\nKohli, P., and Kelley, D. R. Effective gene expression\nprediction from sequence by integrating long-range inter-\nactions. bioRxiv, 2021. doi: 10.1101/2021.04.07.438649.\nURL https://www.biorxiv.org/content/\nearly/2021/04/08/2021.04.07.438649.\nBorgwardt, K. M., Ong, C. S., Sch¨onauer, S., Vishwanathan,\nS. V . N., Smola, A. J., and Kriegel, H.-P. Protein func-\ntion prediction via graph kernels. Bioinformatics, 21\n(suppl 1):i47–i56, 06 2005. ISSN 1367-4803. doi:\n10.1093/bioinformatics/bti1007. URL https://doi.\norg/10.1093/bioinformatics/bti1007.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,\nJ., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances\nin Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X.,\nGane, A., Sarlos, T., Hawkins, P., Davis, J., Belanger, D.,\nColwell, L., and Weller, A. Masked language modeling\nfor proteins via linearly scalable long-context transform-\ners. arXiv preprint arXiv:2006.03555, 2020.\nChoromanski, K. M., Likhosherstov, V ., Dohan, D., Song,\nX., Gane, A., Sarl ´os, T., Hawkins, P., Davis, J. Q.,\nMohiuddin, A., Kaiser, L., Belanger, D. B., Colwell,\nL. J., and Weller, A. Rethinking attention with per-\nformers. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net, 2021. URL https:\n//openreview.net/forum?id=Ua6zuk0WRH.\nCygan, M., Fomin, F. V ., Kowalik, L., Lokshtanov,\nD., Marx, D., Pilipczuk, M., Pilipczuk, M., and\nSaurabh, S. Parameterized Algorithms . Springer,\n2015. ISBN 978-3-319-21274-6. doi: 10.1007/\n978-3-319-21275-3. URL https://doi.org/10.\n1007/978-3-319-21275-3 .\nDai, Z., Lai, G., Yang, Y ., and Le, Q. Funnel-transformer:\nFiltering out sequential redundancy for efﬁcient language\nprocessing. In Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M., and Lin, H. (eds.), Advances in Neural In-\nformation Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual, 2020.\nDaniely, A., Frostig, R., Gupta, V ., and Singer, Y .\nRandom features for compositional kernels. CoRR,\nabs/1703.07872, 2017. URL http://arxiv.org/\nabs/1703.07872.\nDefferrard, M., Bresson, X., and Vandergheynst, P. Con-\nvolutional neural networks on graphs with fast localized\nspectral ﬁltering. CoRR, abs/1606.09375, 2016. URL\nhttp://arxiv.org/abs/1606.09375.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. In Burstein, J., Doran, C., and\nSolorio, T. (eds.), Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pp. 4171–4186.\nAssociation for Computational Linguistics, 2019. doi:\n10.18653/v1/n19-1423. URL https://doi.org/\n10.18653/v1/n19-1423.\nDobson, P. D. and Doig, A. J. Distinguishing en-\nzyme structures from non-enzymes without alignments.\nJournal of molecular biology , 330(4):771—783, July\n2003. ISSN 0022-2836. doi: 10.1016/s0022-2836(03)\n00628-4. URL https://doi.org/10.1016/\ns0022-2836(03)00628-4.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,\nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,\nN. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In 9th International Con-\nference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021 . OpenReview.net,\n2021. URL https://openreview.net/forum?\nid=YicbFdNTTy.\nErrica, F., Podda, M., , Bacciu, D., and Micheli, A. A fair\ncomparison of graph neural networks for graph classiﬁca-\ntion. ICLR 2020, 2020.\nTowards a general theory for scalable masked Transformers\nHamilton, W., Ying, Z., and Leskovec, J. Inductive\nrepresentation learning on large graphs. In Guyon, I.,\nLuxburg, U. V ., Bengio, S., Wallach, H., Fergus, R., Vish-\nwanathan, S., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems, volume 30. Curran As-\nsociates, Inc., 2017. URL https://proceedings.\nneurips.cc/paper/2017/file/\n5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.\npdf.\nHan, K., Wang, Y ., Chen, H., Chen, X., Guo, J., Liu, Z.,\nTang, Y ., Xiao, A., Xu, C., Xu, Y ., Yang, Z., Zhang,\nY ., and Tao, D. A survey on visual transformer. CoRR,\nabs/2012.12556, 2020. URL https://arxiv.org/\nabs/2012.12556.\nKatharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.\nTransformers are rnns: Fast autoregressive transformers\nwith linear attention. In Proceedings of the 37th Interna-\ntional Conference on Machine Learning, ICML 2020, 13-\n18 July 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pp. 5156–5165. PMLR,\n2020. URL http://proceedings.mlr.press/\nv119/katharopoulos20a.html.\nKersting, K., Kriege, N. M., Morris, C., Mutzel,\nP., and Neumann, M. Benchmark data sets for\ngraph kernels, 2016. URL https://ls11-www.cs.tu-\ndortmund.de/staff/morris/graphkerneldatasets, 795,\n2016.\nKipf, T. N. and Welling, M. Semi-supervised classiﬁcation\nwith graph convolutional networks. In 5th International\nConference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track\nProceedings. OpenReview.net, 2017. URL https://\nopenreview.net/forum?id=SJU4ayYgl.\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer:\nThe efﬁcient transformer. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020 . OpenReview.net,\n2020. URL https://openreview.net/forum?\nid=rkgNKkHtvB.\nKondor, R. and Lafferty, J. D. Diffusion kernels on graphs\nand other discrete input spaces. In Sammut, C. and Hoff-\nmann, A. G. (eds.), Machine Learning, Proceedings of\nthe Nineteenth International Conference (ICML 2002),\nUniversity of New South Wales, Sydney, Australia, July\n8-12, 2002, pp. 315–322. Morgan Kaufmann, 2002.\nLee, D. Fast multiplication of a recursive block toeplitz\nmatrix by a vector and its application. J. Complex.,\n2(4):295–305, 1986. doi: 10.1016/0885-064X(86)\n90007-5. URL https://doi.org/10.1016/\n0885-064X(86)90007-5.\nLi, D., Zhang, X., and Liu, R. Exponential integrators for\nlarge-scale stiff Riccati differential equations. J. Com-\nput. Appl. Math., 389:113360, 2021. doi: 10.1016/j.cam.\n2020.113360. URL https://doi.org/10.1016/\nj.cam.2020.113360.\nLiutkus, A., C´ıfka, O., Wu, S., Simsekli, U., Yang, Y ., and\nRichard, G. Relative positional encoding for transform-\ners with linear complexity. In Meila, M. and Zhang,\nT. (eds.), Proceedings of the 38th International Con-\nference on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of Ma-\nchine Learning Research, pp. 7067–7079. PMLR, 2021.\nURL http://proceedings.mlr.press/v139/\nliutkus21a.html.\nLuo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S.,\nKe, G., Wang, L., and Liu, T. Stable, fast and accu-\nrate: Kernelized attention with relative positional en-\ncoding. CoRR, abs/2106.12566, 2021. URL https:\n//arxiv.org/abs/2106.12566.\nLuzhnica, E., Day, B., and Li `o, P. On graph classiﬁca-\ntion networks, datasets and baselines. arXiv preprint\narXiv:1905.04682, 2019.\nNikolentzos, G. and Vazirgiannis, M. Random walk\ngraph neural networks. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M. F., and Lin, H. (eds.),\nAdvances in Neural Information Processing Systems ,\nvolume 33, pp. 16211–16222. Curran Associates,\nInc., 2020. URL https://proceedings.\nneurips.cc/paper/2020/file/\nba95d78a7c942571185308775a97a3a0-Paper.\npdf.\nOrecchia, L., Sachdeva, S., and Vishnoi, N. K. Approximat-\ning the exponential, the lanczos method and an ˜o(m)-time\nspectral algorithm for balanced separator. In Karloff, H. J.\nand Pitassi, T. (eds.), Proceedings of the 44th Symposium\non Theory of Computing Conference, STOC 2012, New\nYork, NY, USA, May 19 - 22, 2012, pp. 1141–1160. ACM,\n2012. doi: 10.1145/2213977.2214080. URL https:\n//doi.org/10.1145/2213977.2214080.\nParmar, N., Ramachandran, P., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in vi-\nsion models. In Wallach, H. M., Larochelle, H., Beygelz-\nimer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett, R.\n(eds.), Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pp. 68–80, 2019.\nPeng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith,\nN. A., and Kong, L. Random feature attention. In\nTowards a general theory for scalable masked Transformers\n9th International Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net, 2021. URLhttps://openreview.\nnet/forum?id=QtTKTdVrFBB.\nPham, N., Ha, T., Nguyen, T., Nguyen, T., Salesky, E.,\nSt¨uker, S., Niehues, J., and Waibel, A. Relative positional\nencoding for speech recognition and direct translation. In\nMeng, H., Xu, B., and Zheng, T. F. (eds.), Interspeech\n2020, 21st Annual Conference of the International Speech\nCommunication Association, Virtual Event, Shanghai,\nChina, 25-29 October 2020, pp. 31–35. ISCA, 2020. doi:\n10.21437/Interspeech.2020-2526. URL https://doi.\norg/10.21437/Interspeech.2020-2526.\nQin, Z., Sun, W., Deng, H., Li, D., Wei, Y ., Lv, B., Yan, J.,\nKong, L., and Zhong, Y . cosformer: Rethinking softmax\nin attention. CoRR, abs/2202.08791, 2022. URL https:\n//arxiv.org/abs/2202.08791.\nRalaivola, L., Swamidass, S. J., Saigo, H., and Baldi, P.\nGraph kernels for chemical informatics. Neural networks,\n18(8):1093–1110, 2005.\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. Efﬁ-\ncient content-based sparse attention with routing trans-\nformers. Trans. Assoc. Comput. Linguistics , 9:53–\n68, 2021. URL https://transacl.org/ojs/\nindex.php/tacl/article/view/2405.\nSchomburg, I., Chang, A., Ebeling, C., Gremse, M., Heldt,\nC., Huhn, G., and Schomburg, D. BRENDA, the en-\nzyme database: updates and major new developments.\nNucleic Acids Research, 32(Database issue):D431–3, Jan-\nuary 2004.\nSen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B.,\nand Eliassi-Rad, T. Collective classiﬁcation in network\ndata, 2008.\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with\nrelative position representations. In Walker, M. A., Ji,\nH., and Stent, A. (eds.), Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 2 (Short Papers), pp. 464–468.\nAssociation for Computational Linguistics, 2018. doi:\n10.18653/v1/n18-2074. URL https://doi.org/\n10.18653/v1/n18-2074.\nSimonovsky, M. and Komodakis, N. Dynamic edge-\nconditioned ﬁlters in convolutional neural networks on\ngraphs. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pp. 3693–3702,\n2017.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pp. 5998–6008, 2017.\nVaswani, A., Ramachandran, P., Srinivas, A., Parmar,\nN., Hechtman, B. A., and Shlens, J. Scaling local\nself-attention for parameter efﬁcient visual backbones.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2021, virtual, June 19-25, 2021 ,\npp. 12894–12904. Computer Vision Foundation / IEEE,\n2021. URL https://openaccess.thecvf.\ncom/content/CVPR2021/html/Vaswani_\nScaling_Local_Self-Attention_for_\nParameter_Efficient_Visual_Backbones_\nCVPR_2021_paper.html.\nVelickovic, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y . Graph attention networks. In6th\nInternational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May 3,\n2018, Conference Track Proceedings. OpenReview.net,\n2018. URL https://openreview.net/forum?\nid=rJXMpikCZ.\nWale, N., Watson, I. A., and Karypis, G. Comparison\nof descriptor spaces for chemical compound retrieval\nand classiﬁcation. Knowl. Inf. Syst. , 14(3):347–375,\n2008. doi: 10.1007/s10115-007-0103-5. URL https:\n//doi.org/10.1007/s10115-007-0103-5 .\nXiong, Y ., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li,\nY ., and Singh, V . Nystr¨omformer: A nystr ¨om-based al-\ngorithm for approximating self-attention. In Thirty-Fifth\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of\nArtiﬁcial Intelligence, IAAI 2021, The Eleventh Sympo-\nsium on Educational Advances in Artiﬁcial Intelligence,\nEAAI 2021, Virtual Event, February 2-9, 2021, pp. 14138–\n14148. AAAI Press, 2021. URL https://ojs.aaai.\norg/index.php/AAAI/article/view/17664.\nXu, K., Hu, W., Leskovec, J., and Jegelka, S. How\npowerful are graph neural networks? arXiv preprint\narXiv:1810.00826, 2018.\nYanardag, P. and Vishwanathan, S. Deep graph kernels. In\nProceedings of the 21th ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining, KDD\n’15, pp. 1365–1374, New York, NY , USA, 2015. Associa-\ntion for Computing Machinery. ISBN 9781450336642.\ndoi: 10.1145/2783258.2783417. URL https://doi.\norg/10.1145/2783258.2783417.\nTowards a general theory for scalable masked Transformers\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J. G., Salakhutdinov,\nR., and Le, Q. V . Xlnet: Generalized autoregressive pre-\ntraining for language understanding. In Wallach, H. M.,\nLarochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox,\nE. B., and Garnett, R. (eds.), Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, pp.\n5754–5764, 2019.\nYing, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen,\nY ., and Liu, T. Do transformers really perform bad for\ngraph representation? CoRR, abs/2106.05234, 2021a.\nURL https://arxiv.org/abs/2106.05234.\nYing, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen,\nY ., and Liu, T. Do transformers really perform bad for\ngraph representation? CoRR, abs/2106.05234, 2021b.\nURL https://arxiv.org/abs/2106.05234.\nYing, Z., You, J., Morris, C., Ren, X., Hamilton, W.,\nand Leskovec, J. Hierarchical graph representation\nlearning with differentiable pooling. In Bengio, S.,\nWallach, H., Larochelle, H., Grauman, K., Cesa-\nBianchi, N., and Garnett, R. (eds.), Advances in Neural\nInformation Processing Systems, volume 31. Curran As-\nsociates, Inc., 2018. URL https://proceedings.\nneurips.cc/paper/2018/file/\ne77dbaf6759253c7c6d0efc5690369c7-Paper.\npdf.\nZaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,\nSalakhutdinov, R. R., and Smola, A. J. Deep sets.\nIn Guyon, I., Luxburg, U. V ., Bengio, S., Wallach,\nH., Fergus, R., Vishwanathan, S., and Garnett,\nR. (eds.), Advances in Neural Information Pro-\ncessing Systems , volume 30. Curran Associates,\nInc., 2017. URL https://proceedings.\nneurips.cc/paper/2017/file/\nf22e4747da1aa27e363d86d40ff442fe-Paper.\npdf.\nZhang, M., Cui, Z., Neumann, M., and Chen, Y . An end-\nto-end deep learning architecture for graph classiﬁca-\ntion, 2018. URL https://aaai.org/ocs/index.\nphp/AAAI/AAAI18/paper/view/17146.\nZhou, P., Fan, R., Chen, W., and Jia, J. Improving gen-\neralization of transformer for speech recognition with\nparallel schedule sampling and relative positional em-\nbedding. CoRR, abs/1911.00203, 2019. URL http:\n//arxiv.org/abs/1911.00203.\nTowards a general theory for scalable masked Transformers\nA. Appendix\nA.1. Several Pointers\nWe include pointers to this part of the code that does not include sensitive/proprietary information. The core GKAT framework\n(with the additional analysis of new graph sketches that GKAT leads to, called graphots, mixing regular feature vectors\nin nodes with the topological features) is here: https://github.com/HL-hanlin/GKAT. We used (deterministic and random)\nfeature map mechanisms corresponding to the features deﬁned in graph nodes from this repository: https://github.com/google-\nresearch/google-research/tree/master/performer.\nA.2. Combinatorial Classiﬁcation Experiments: Additional Details\nThe data for the motif detection task from Section 5.1.1 was generated as follows:\n• Firstly we created ﬁve simple motifs as shown in Fig. 5. Note that each motif has ≥9 vertices so a brute-force\ncombinatorial algorithm for motif-detection would take time Ω(N9), prohibitively expensive even for small graphs G.\n• We then generated for each motif Ssmall Erd˝os-R´enyi graphs with the same number of nodes as that motif and the\nsame average degree.\n• For each motif, we also generated Slarger Erd˝os-R´enyi random graphs, each of 100 vertices, again of the same average\ndegree.\n• We obtained positive/negative samples by connecting each larger Erd˝os-R´enyi random graph with the motif/previously\ngenerated smaller Erd˝os-R´enyi random graph (with certain edge probability).\nIn Table 5 we present additional details regarding architectures used in the experiments from Section 5.1.2, in particular the\nnumber of parameters and heads / polynomial ﬁlters used in different layers. Ablation tests over GKAT random walk length\nfor Section 5.1.2 are presented in Fig. 9.\nFigure 9.Ablation tests over random walk length of GKAT in Sec. 5.1.2.\nA.3. GNNs for Bioinformatics Tasks & Social Networks Data: Additional Details\nA.3.1. D ATASETS DESCRIPTIONS\nDetailed proﬁles of the datasets used in the experiments from Sec. 5.2 are given in Table 6.\nFor each dataset, we chose graphs with the number of nodes close to the average number of nodes shown in Table 6.\nExamples of bioinformatics-graphs from these datasets are given in Fig. 10. Examples of social network graphs from these\ndatasets are given in Fig. 11.\nA.3.2. H YPERPARAMETER SELECTION\nIn this section, we present details regarding hyperparameter selection in Section 5.2 (see: Table 7).\nTowards a general theory for scalable masked Transformers\nTable 5.Additional details regarding architectures used in Section 5.1.2. For GKAT, we applied8 heads in the ﬁrst layer, and 1 head in\nthe second layer, with 4 hidden units in each attention head. The total number of trainable parameters was 242. For GAT, we tested the\nnumber of layers from 2 to 6, changing the number of attention heads in each layer, but with the same number of hidden units in each\nattention head. For GCN, we modiﬁed the number of hidden units in each layer. For SGC, we modiﬁed the number of polynomial ﬁlters\nand the number of hidden units in each layer. The number of attention heads in GAT, as well as the number of hidden units in each layer\nin GCN and SGC were chosen to make their total number of trainable parameters comparable with the corresponding number of GKAT.\n#Heads Dim. Head #Parameters\nGKAT 2 layers [8,1] 4 242\nGAT\n2 layers [8,1] 4 242\n3 layers [4,2,1] 4 242\n4 layers [4,2,1,1] 4 266\n5 layers [2,2,2,1,1] 4 258\n6 layers [3,2,1,1,1,1] 4 270\nDim. Layer #Parameters\nGCN\n2 layers [14,14] 268\n3 layers [10,10,10] 262\n4 layers [8,8,8,8] 250\n5 layers [10,8,6,6,6] 260\n6 layers [10,6,6,6,6,6] 268\n#Polynomial Filters Dim. Layer #Parameters\nSGC\n2 layers [4,2] [10 ,8] 236\n3 layers [4,2,2] [8 ,6,6] 234\n4 layers [8,2,2,2] [8 ,5,4,4] 247\n5 layers [8,2,2,2,2] [6 ,5,4,4,4] 245\n6 layers [8,2,2,2,2,2] [6 ,4,4,4,4,3] 249\nFigure 10.Representative plots for bioinformatics datasets. For each bioinformatics dataset, we chose the graph with number of nodes\nmost similar to the average number of nodes shown in Table 6.\nThe tunable parameters included: general parameters like batch size, learning rate, dropout ratio, global pooling methods,\nregularization rate, data normalization methods, as well as parameters speciﬁc to our GKAT layers, which included: number\nof GKAT layers, number of attention heads and dimension of each head in a GKAT layer. We also tuned other options:\nwhether to add a fully-connected layer after data normalization, but before GKAT layers, and dimension of fully-connected\nlayers (both preceding and coming after GKAT layers). Due to the large amount of tunable parameters, we decided to ﬁrst\nconduct a rough search for each parameter using only one random CV fold, select one/several parameter combination(s)\nwith best performance, and then reused on all other folds.\nFor all other methods, we reported the best scores conducted via an extensive hyperparameters grid search (Errica et al.,\n2020). For GKAT, we ﬁxed the number of epochs to E = 1000, early stopping patience as 500 epochs, the criterion\nfor early stopping as validation accuracy, global pooling method as summation, and used Adam optimizer. Then we\nperformed hyperparameter tuning for: batch size B ∈ {32,128}, learning rate η ∈ {0.01,0.001,0.0001}, dropout\nratio ∈{0.0,0.1,0.2,0.4}, L2-regularization rate ∈{0.001,0.005}, dimension of attention head in the ﬁrst GKAT layer\nh∈{4,8,16,32}, number of attention heads in the ﬁrst GKAT layer ∈{1,4,8,12}, number of nodes in the MLP layer\n∈{32,64,128}, GKAT random walk length τ ∈{1,2,3,4}, whether to use a fully-connected layer before the ﬁrst GKAT\nlayer, and whether to apply batch normalization to pre-prosess data before feeding the model with it.\nFor some of the datasets (e.g. D&D), we selected the best hyperparameter set optimized over one random CV fold, and used\nit across all cross-validation outer folds.\nTowards a general theory for scalable masked Transformers\nTable 6.Bioinformatics and Social Dataset descriptions. #NODES, #EDGES and Diameter columns contain values averaged over all\ngraphs in a given dataset.\n#Graphs #Classes #Nodes #Edges Diameter #Features\nBIOINF.\nD&D 1178 2 284 .32 715 .66 19 .90 89\nENZYMES 600 6 32 .63 64 .14 10 .86 3\nNCI1 4110 2 29 .87 32 .30 13 .26 37\nPROTEINS 1113 2 39 .06 72 .82 11 .48 3\nSOCIAL\nCOLLAB 5000 3 74 .49 2457 .78 1 .86 1\nIMDB-BINARY 1000 2 19 .77 96 .53 1 .86 1\nIMDB-MULTI 1500 3 13 .00 65 .94 1 .47 1\nREDDIT-BINARY 2000 2 429 .63 497 .75 9 .72 1\nREDDIT-5K 4999 5 508 .82 594 .87 11 .96 1\nFigure 11.Representative plots for social datasets. For each social dataset, we chose the graph with number of nodes most similar to the\naverage number of nodes shown in Table 6.\nA.4. Space & Time Complexity Gains of GKAT: Additional Experiments\nAdditionally, we have conducted experiments on much larger Erd ˝os-R´enyi graphs with motifs, see: Section 5.1.1. The\naverage number of nodes of each ER graph was 100K. Tested architectures had the same characteristics as in Section 5.1.1.\nThe results (ﬁnal model accuracy) are presented in Table 8 for datasets: Caveman, Circle, Grid, Ladder and Circular-Ladder\nrespectively.\nA.5. Experiments with Citation Networks Datasets\nA.5.1. D ATASETS DESCRIPTIONS\nDatasets: To directly compare GKAT with GAT, we also tested both algorithms on three publicly available citation networks datasets:\nCora, Citeseer and Pubmed ((Sen et al., 2008)) with the same data splits as in (Velickovic et al., 2018). Datasets descriptions are given in\nTable 9.\nA.5.2. C OMPARISON WITH GAT\nExperiment Settings: We used the same model architecture and parameters as in GAT for our GKAT to make the\ncomparison as accurate as possible. The only difference is that we replaced the adjacency matrix masking in GAT by the\nnormalized dot-product based similarity matrix generated from random walks, as described in Section 4.2. Both models\nused two-layer attention, with 8 attention heads in the ﬁrst layer, and 1 head in the second layer. We used 8 hidden units\nin the ﬁrst layer, and the number of output units in the second layer was the same as number of classes. Each layer was\nfollowed by an exponential linear unit (ELU) activation. We applied L2-regularization with λ = 0.0005, dropout with\np= 0.6 for inputs and normalized attention coefﬁcients in both layers for all three datasets.\nResults: The results are shown in Table 10. Our GKAT algorithm achieved lower accuracy on Cora dataset, but higher on\nthe remaining two.\nDynamic Generator of Random Walks: We also tried the so-called dynamic-GKAT. The dynamic variant generated\nrandom walks from scratch in each training epoch, thus requiring additional compute. However one advantage of the\ndynamic version is that we could assign different transition probabilities for adjacent nodes (rather than sampling next\npoint of the walk uniformly at random). The transition probability matrix can be a masked attention matrix and we only\nneed its actions on the L-dimensional vectors to compute probability vectors in the visited nodes. Since the GKAT-masked\nTowards a general theory for scalable masked Transformers\nTable 7.Hyperparameter settings for the bioinformatics and social network datasets from Section 5.2.\nBS #Heads d Head dFC Lenrw Drop L2 add FC Norm\nCHEM.\nD&D 32 8 16 128 2 − 0.005 No BN\nNCI1 32 8 32 64 4 0 .1 0.001 Yes BN\nPROTEINS 32 4 8 32 3 − 0.001 Yes BN128 8 32 128\nENZYMES 32 4 16 32 3 0 .1 0 .001 Yes BN8 32 64\nSOCIAL\nCOLLAB 32 12 4 128 2 − 0.005 No No\nIMDB-BINARY 32 8 4 64 1 − 0.005 No No\n12 8 128 BN\nIMDB-MULTI 32 8 4 64 1 − 0.005 No No\n12 8 128 BN\nREDDIT-BINARY 32 4 4 128 2 − 0.005 No BN\nREDDIT-5K 32 8 8 64 2 − 0.005 No BN\nTable 8.Running time of training different networks on datasets from Sec 5.1.1 but with much larger number of nodes (∼100K). For\nGCN, GAT and SGC, we reported the accuracy with 2 layers. For GKAT, we used a 2-layer architecture and reported the accuracy with a\nﬁxed walk length of 3 for motifs from Sec. 5.1.1.\nCaveman Circle Grid Ladder Circle Ladder\nGCN 88.3% 82 .7% 80 .4% 80 .6% 91 .4%\nGAT 75.0% 81 .0% 69 .8% 77 .0% 89 .2%\nSGC 70.0% 80 .4% 72 .3% 76 .1% 82 .4%\nGKAT 89.3% 83.2% 80.7% 81.5% 92.3%\nTable 9.Citation Networks Datasets Descriptions.\nCora Citeseer Pubmed\n#Nodes 2708 3327 19717\n#Edges 5419 4732 44338\n#Features 1433 3703 500\n#Classes 7 6 3\n#Training Nodes 140 120 60\n#Validation Nodes 500 500 500\n#Test Nodes 1000 1000 1000\nTable 10.Comparison of GAT and GKAT on citation networks datasets. For Cora and Citeseer, we reported the results for GAT from\n(Velickovic et al., 2018). For GAT and Pubmed dataset, we reported the results averaged over 15 runs with the same parameter settings as\nin Cora and Citeseer. GKAT was run 15 times over multiple random walk lengths up to 7, and the best was reported.\nCora Citeseer Pubmed\nGAT 83.0±0.7% 72.5 ±0.7% 77 .2 ±0.6%\nGKAT 82.1 ±0.7% 73.0±0.7% 78.0 ±0.7%\nattention can be interpreted as a kernelizable attention of a product-kernel (the product of the kernel between feature vectors\nin nodes and the nodes in the graph) and each factor-kernel admits on expectation a linearization, the product-kernel also\ndoes it via the mechanism of the Cartesian-product random features (see: (Daniely et al., 2017)). Thus this matrix admits on\nexpectation a lower-rank decomposition and thus based on our analysis from Sec. 3.4, the actions of that matrix on the input\nTowards a general theory for scalable masked Transformers\nvectors can be efﬁciently computed.\nAn intuition behind that particular variant is that we assign higher transition probabilities for neighbors with higher attention\ncoefﬁcients. The dynamic variant enabled us to improve accuracy of GKAT on Citeseer to 73.3% (with reduced 0.6%\nstandard deviation).\nA.5.3. A BLATION TESTS ON RANDOM WALK LENGTH FOR GKAT\nFigure 12 compares the effect of random walk path length of GKAT algorithms on training for Cora, Citeseer and Pubmed\ndatasets. We run GKAT with multiple random walk lengths up to 7. The results show that a small path length no longer than\n4 is enough for GKAT and dynamic-GKAT, which supports our claim that short walks are sufﬁcient for GKAT.\nFigure 12.Ablation tests over random walk path lengths of GKAT and dynamic-GKAT on Cora, Citeseer and Pubmed datasets. The\nerrorbar represents 1 standard deviation. Test-accuracies for the optimized GAT were shown as horizontal red dotted lines in each subplot.\nBest test accuracies of GKAT algorithms were highlighted with red squares. The dynamic-GKAT was tested only on datasets, where\naccuracy advantage of the regular optimized GKAT over optimized GAT was≤0.5%.\nA.6. Proof of Lemma 3.1 and Algorithm 1\nProof. Note that the ithtoken representation ri obtained from the general masked kernel attention is of the following form:\nri =\nφ(q⊤\ni )⊤∑L\nj=1 Mi,jφ(k⊤\nj )vj\nφ(q⊤\ni )⊤∑L\nj=1 Mi,jφ(k⊤\nj )\n, (7)\nwhere qi,kj,vj stand for the ithquery and jthkey/value row vectors respectively. As in (Luo et al., 2021), we deﬁne the\nfollowing two sequences of matrices and vectors of shapes Rm×d and R1×m respectively:\nD1 =\n\n\nL∑\nj=1\nMi,jφ(k⊤\nj )vj\n\n\nL\ni=1\n, D2 =\n\n\nL∑\nj=1\nMi,jφ(k⊤\nj )⊤\n\n\nL\ni=1\n. (8)\nIf we deﬁne ˜D1 and ˜D2 as the vectorized variants of D1 and D2, where each element of the sequence is (row) vectorized\n(note that the elements of D2 are already row-vectorized) and the resulting vectors are stacked into matrices, then:\n˜D1 = MV1, ˜D2 = MV2, (9)\nwith the ithrows of V1 and V2 given as: V1\ni = vec(φ(ki)⊤vi), V2\ni = φ(k⊤\ni )⊤.\nWe conclude that the computation of D1 and D2 takes time TN(L)mdand consequently all representations ri can be\ncomputed in time O((TN(L) + L)md). That completes the proof of Lemma 3.1.\nNote that Algorithm 1 follows immediately from the above proof. The algorithm consists of two phases. In the ﬁrst\nphase, sequences D1 and D2 are computed (in the form of ˜D1 and ˜D2). In the second phase, they are used to get\nnew token-representations (different tokens apply different elements of D1 and D2 for the computation of their new\nrepresentations).\nTowards a general theory for scalable masked Transformers\nA.7. Proof of Lemma 3.3\nProof. We deﬁne the so-called grid-ordering on the nodes of the d-dimensional grid recursively. For d= 1 we take the\nnatural ordering on the line. For the d-dimensional grid with d >1, we take the (d−1)-dimensional slices: S1,S2,...\nordered according to their dth-dimension index. In each slice we recursively order all the tokens We then combine the\norderings of all the slices to get the ordering on the d-dimensional grid.\nWe claim that the mask corresponding to the d-dimensional grid with the grid-ordering of tokens is d-level block-Toeplitz.\nWe will proceed by an induction on d. For d= 1 the corresponding mask is of the form: M = [f(i−j)]i,j=1,...,L for some\nlearnable function f and a natural ordering on the line. Therefore M is constant on each diagonal thus it is Toeplitz (e.g.\n1-level block-Toeplitz). Now let us assume that d >1 and the result holds for d−1. We take the grid-ordering for the\nd-dimensional grid and the partitioning of the tokens given by the (d−1)-dimensional slices S1,S2,... ordered according\nto their dth-dimension index. This partitioning induces the block-partitioning of the mask M (using grid-ordering) on\nthe d-dimensional grid. Now note that the shortest-path distance between two nodes v1 and v2 from slices Si1 and Si2\nrespectively can be computed as: dist(v1,v′\n2) + |i1 −i2|, where v′\n2 is the projection of v2 into slice Si1 . This observation\ncombined with the inductive assumption implies that deﬁned above block-partitioning of M produces matrices Bi,j from\nDeﬁnition 3.2 and that completes the proof.\nA.8. Proof of Lemma 3.5\nProof. Without loss of generality we can assume that Gbase is a tree. Assume that τ is of the form: τ(z) = az+ bfor some\na,b ∈R. Take some x ∈RL. Let us root the tree T in one of its vertices that we denote as v0. Denote by v1,...,v k for\nsome k≥0 its neighbors. For every node iwe deﬁne si as follows:\nsi =\n∑\nj∈Ti\nexp(τ(dist(i,j)))xj, (10)\nwhere Ti denotes a subtree of T rooted in i. Our ﬁrst observation is that all si for i= 1,...,L can be computed in O(L)\ntime. To see this, note that:\nsv0 = exp(τ(dist(v0,v0)))x0 +\n∑\nl=1,...,k\n∑\nj∈Tvl\nexp(τ(dist(v0,j)))xj =\nebx0 +\n∑\nl=1,...,k\n∑\nj∈Tvl\nea·dist(v0,j)+bxj = ebx0 +\n∑\nl=1,...,k\n∑\nj∈Tvl\nea·(dist(vl,j)+W(v0,vl))+bxj =\nebx0 +\n∑\nl=1,...,k\neW(v0,vl) ∑\nj∈Tvl\nea·dist(vl,j)+bxj = ebx0 +\n∑\nl=1,...,k\neW(v0,vl)svl\n(11)\nThus we see that computing sv0 requires computing each svl, followed by additional addition/multiplication operations\nthat take time O(deg(v0)). We conclude that we can recursively compute all si in time O(L). Let us note that the entry of\nw = Mx corresponding to node iis of the form:\nwi =\n∑\nj∈T\nexp(τ(dist(i,j)))xj, (12)\nTherefore ultimately we aim to compute all wi for i= 1,...,L in time O(L). We observe that:\nwv0 = sv0 (13)\nNow take node i̸= v0. Denote by p(i) the predecessor of iin T. We have:\nwi =\n∑\nj∈Ti\nexp(τ(dist(i,j)))xj +\n∑\nj/∈Ti\nexp(τ(dist(i,j)))xj =\nsi +\n∑\nj/∈Ti\nea·dist(i,j)+bxj = si +\n∑\nj/∈Ti\nea(W(i,p(i))+dist(p(i),j))+bxj =\nsi + eaW(i,p(i)) ∑\nj/∈Ti\neadist(p(i),j)+bxj = si + eaW(i,p(i))ti,\n(14)\nTowards a general theory for scalable masked Transformers\nwhere ti = ∑\nj/∈Ti eadist(p(i),j)+bxj. Now note that:\nwp(i) = ti + eW(i,p(i))si (15)\nand thus: ti = wp(i) −eW(i,p(i))si. Plugging in the formula for ti into Equation 14, we get:\nwi = eW(i,p(i))wp(i) + (1 −e2W(i,p(i)))si (16)\nWe conclude that having computed all si for i= 1,...,L in time O(L), we can compute all wi for i= 1,...,L in time O(L)\nby ordering vertices in their increasing distance from the root v0, setting up wv0 = sv0 and applying Equation 16.\nA.9. Proof of Theorem 4.1\nProof. We need the following deﬁnition.\nDeﬁnition A.1. A matrix A is Symmetric and Diagonally Dominant (SDD) if Ai,j = Aj,i for all i,j and Ai,i ≥∑\nj̸=i|Ai,j|.\nThe results is a straightforward consequence of Theorem 1.2 from (Orecchia et al., 2012) and Lemma 3.1. For Reader’s\nconvenience we restate that theorem here:\nTheorem A.2 (SDD Matrix Exponential Computation). Given an L×LSDD matrix A, a vector x and a parameter δ≤1,\nthere is an algorithm that computes a vector u such that ∥exp(−A)x −u∥≤ δ∥x∥in time ˜O((|E|+ L) log(2 +∥A∥)),\nHere tilde hides poly(log(L)) and poly(log( 1\nδ)) factors.\nIt sufﬁces to notice that both Laplacian matrix and its renormalized version are SDD. Furthermore, by Lemma 3.1, fast\n(approximate) computation of exp(−λA)x for any x ∈RL and A as in Theorem 4.1 leads to fast computation of the\nlow-ranked attention with mask M = exp(−λA), as explained in Algorithm 1.\nA.10. Proof of Theorem 4.2\nProof. Note ﬁrst that since ω(k) and ω(l) are chosen independently, we have:\nKλ,0\np (k,l) = Eω(k)[fω(k),λ\nk ] ·(Eω(l)[fω(l),λ\nl ])⊤= Eω(k),ω(l)[fω(k),λ\nk (fω(l),λ\nl )⊤] (17)\nDenote: X = fω(k),λ\nk (fω(l),λ\nl )⊤. The key observation is that X can be rewritten as:\nX =\n∑\nu∈V(G)\n∑\n(j1=k,...,ja+1=u)=pref(ω(k)),\n(j′\n1=l,...,j′\nb+1=u)=pref(ω(l))\nλaλb =\n∑\n(j1=k,...,ja+b+1=l)∈Ω(k,l)\nλa+b (18)\nwhere Ω(k,l) is the multi-set of walks from kto lthat are built from some preﬁx of ω(k) concatenated with some preﬁx of\nω(l). Therefore we can write X as:\nX =\n∑\nr∈R(k,l)\nlen(r)∑\ni=0\nλlen(r)1[E(r,i)], (19)\nwhere R(k,l) is the set of walks from kto l, len(r) stands for the length (number of edges) of walk rand E(r,i) is an event\nthat ﬁrst iedges of the walk r(counting from k) form the preﬁx sub-walk of ω(k) and the remaining ones form the preﬁx\nsub-walk of ω(l). Therefore we have:\nKλ,0\np (k,l) = Eω(k),ω(l)\n\n ∑\nr∈R(k,l)\nlen(r)∑\ni=0\nλlen(r)1[E(r,i)]\n\n=\n∑\nr∈R(k,l)\nlen(r)∑\ni=0\nλlen(r)Pω(k),ω(l)[E(r,i)] =\n∑\nr∈R(k,l)\nlen(r)∑\ni=0\nλlen(r)\ni−1∏\nj=0\n1 −p\ndeg(rj)\nlen(r)−i−1∏\nt=0\n1 −p\ndeg(rlen(r)−1−t),\n(20)\nTowards a general theory for scalable masked Transformers\nwhere ry stands for the yth vertex of the walk rstarting from kand deg(v) denotes the degree of a vertex v.\nTherefore we obtain:\n∑\nr∈R(k,l)\nlen(r)∑\ni=0\n((1 −p)λ\ndmax\n)len(r)\n≤Kλ,0\np (k,l) ≤\n∑\nr∈R(k,l)\nlen(r)∑\ni=0\n((1 −p)λ\ndmin\n)len(r)\n(21)\nWe conclude that:\n∞∑\ni=0\nrk,l(i)\n((1 −p)λ\ndmax\n)i\n(i+ 1) ≤Kλ,0\np (k,l) ≤\n∞∑\ni=0\nrk,l(i)\n((1 −p)λ\ndmin\n)i\n(i+ 1) (22)\nTo complete the proof, it sufﬁces to notice that matrix Adji(G) encodes the number of walks of length ibetween pairs of\nvertices in G.\nA.11. Extensions of the results from Section 3.3\nWe will provide here the proofs of the results presented in Section 6. We ﬁrst introduce additional concepts wee will leverage\nin the proofs.\nDeﬁnition A.3 (balanced separators). Take some function w : V(G) →R≥0 and some α >0. We say that a subset\nS⊆ V(G) is the α-balanced separator with respect to w, if the set of vertices Cof every connected component of the\nsubgraph graph G|V(G)\\Sof G, induced by V(G)\\S, satisﬁes: w(C) ≤α·w(V(G)), where w(X)\ndef\n= ∑\nx∈Xw(x).\nLemma A.4. If Gis a tree then for an arbitrary function w : V(G) →R≥0 the 1\n2 -balanced separator consisting of two\nadjacent vertices can be found in time O(L).\nProof. The proof is given in the proof of Lemma 7.19 in (Cygan et al., 2015) and relies on the standard tree-search.\nA.11.1. T HE PROOF OF LEMMA 6.1\nProof. Take some vector x ∈RL. The goal is to compute Mx in time O(Llog2(L)). For the node iin a tree T, denote:\nsi =\n∑\nj∈T\nf(dist(i,j))xj (23)\nThus we want to compute all si in time O(Llog2(L)). If |V(T)|≤ 2 then all the calculations can be trivially done in O(1)\ntime, so we will assume now that |V(T)|> 2. Take the 1\n2 -balanced separator {a,b}in T (with respect to the standard\nmeasure that counts the number of vertices) that exists and can be found in time O(L) by Lemma A.4. Denote by Ta the\nset of those trees in T|V(T)\\{a,b}that are are incident to ain T and by Tb the set of those trees in T|V(T)\\{a,b}that are are\nincident to bin T. Note that one of these sets might be potentially empty. Let us assume, without loss of generality that Ta\nis not empty. Denote by Va the union of the set of all the vertices of all the elements of Ta and by Vb the corresponding\nset for Tb. If 1\n10 ≤|Va|≤ 9\n10 |V(T)|, take: T1 to be the subtree of T induced by Va ∪{a}and T2 to be the subtree\nof T induced by Vb ∪{a,b}. Otherwise take this c ∈{a,b}such that |Vc|> 9\n10 |V(T)|. Denote: Tc = {T1,...,T m}.\nNote that m >0 (Tc is not empty). By the deﬁnition of the balanced separator, we have: |V(Ti)| ≤1\n2 |V(T)|for\ni= 1,...,m . On the other hand: |V(T1)|+ ...+ |V(Tm)|≥ 9\n10 |V(T)|. Denote by i∗the smallest i∈{1,...,m }such that\n|V(T1)|+ ...+ |V(Ti∗\n)|≥ 9\n10 |V(T)|. Note that i∗>1. We have:\n2\n5|V(T)|= 9\n10|V(T)|− 1\n2|V(T)|≤| V(T1)|+ ...+ |V(Ti∗\n)|−|V(Ti∗\n)|\n= |V(T1)|+ ...+ |V(Ti∗−1)|≤ 9\n10|V(T)|\n(24)\nDenote by T1 a subtree of Tinduced by V(T1)∪...∪V(Ti∗−1)∪{c}and by T2 a subtree of Tinduced by V(T)\\(V(T1)∪\n...∪V(Ti∗−1)). Note that in both cases we obtain two trees:T1 and T2 sharing a single vertex and such that:V(T1)∪V(T2) =\nV(T). Furthermore, we have:\n|V(T1)|= f|V(T)|+ c1,|V(T2)|= (1 −f)|V(T)|+ c2, (25)\nTowards a general theory for scalable masked Transformers\nfor c1,c2 ∈{0,1}and 2\n5 ≤f ≤ 9\n10 . Denote: {v}= V(T1) ∩V(T2).\nDenote: y1\ni = ∑\nj∈Z1\ni\nxj and y2\ni = ∑\nj∈Z2\ni\nxj for i= 1,..., |V(T)|, where Zk\ni for k∈{1,2}stands for the set of vertices\nin Tk with distance ifrom v. Note that all yk\ni can be trivially computed in time O(|V(T)|).\nTo compute all si for i= 1,..., |V(T)|, we ﬁrst compute recursively the following expressions:\nsk\ni =\n∑\nj∈Tk\nf(dist(i,j))xj (26)\nfor i∈V(Tk) and k ∈{1,2}. In order to compute expressions si, in addition to expressions sk\ni, we need to include the\ncross-term contributions (for pairs of vertices where one is fromT1 and the other fromT2). Note that this can be trivially done\nin time O(V(T)) as long as we have computed the following two vectors: Hy1 and Hy2, where yk = (yk\n1 ,...,y k\n|V(T)|)⊤\nfor k∈{1,2}and H is the Hankel matrix with the ﬁrst row of the form:(f(2),f(3),...,f (|V(T)|+1)) and the last column\nof the form: (f(|V(T)|+ 1),...,f (|V(T)|+ |V(T)|))⊤. This can be done in time O(|V(T)|log(|V(T|))) with Fast\nFourier Transform. We conclude that our algorithm needs two recursive calls for subproblems of sizes which are constant\nfractions of |V(T)|and given in Eq. 25, as well as additional computations conducted in time O(|V(T)|log(|V(T|))).\nThat leads to the total time complexity O(|V(T)|log2(|V(T|))) which completes the proof.\nA.11.2. T HE PROOF OF LEMMA 6.2\nProof. Let us root T in a ﬁxed vertex v0. We denote by Ti the subtree of T rooted in i. For every node i, we maintain an\narray gi of length diam(T) + 1, where: gi[l] = ∑\nj∈Ti:dist(i,j)=lxj. Computing gi for vertices iwhich are the leaves of the\ntree T rooted in v0 can be trivially done in time O(1) per vertex. Now assume that iis not a leaf and denote by: q1,...,q k\n(for some k> 0) its children. Note that: gi can be computed as follows:\ngi[l] =\n{∑k\np=1 gqp[l−1], if l≥1\nxi, if l= 0\nWe also deﬁne an array hi for every node ias follows:\nhi[l] =\n∑\nj∈T:dist(i,j)=l\nxj (27)\nFor a given array z, denote by circ(z) its circulant-shift given as: circ(z)[l] = z[l−1] for l> 0 and circ(z)[0] = 0. Note\nthat: hv0 = gv0 . Furthermore, for i̸= v0, we can compute hi from gi and hp(i), where p(i) stands for the parent of iin T\n(rooted in v0), as follows:\nhi = gi + circ(hp(i) −circ(gi)), (28)\nwhere addition and subtraction are dimension-wise. Thus having computed all gi, we can compute all hi by proceeding from\nthe root v0 in the order induced by the distance from the root. We conclude that calculating hi for all vertices itakes time\nO(L·diam(T)). Therefore, as in the case of our proof for the unweighted tree with f given as the exponential mapping of\nthe afﬁne transform, effectively we perform in two stages - bottom-up to compute gi-arrays and from the root to the leaves\nto compute arrays hi (with the use of already computed arrays gi).\nDenote: w = Mx. Note that:\nwi =\ndiam(T)∑\nl=0\nf(l)hi(l) (29)\nThus computing all wi can be also conducted in time O(L·diam(T)) and that completes the proof.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6354422569274902
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5745931267738342
    },
    {
      "name": "Modular decomposition",
      "score": 0.5048592686653137
    },
    {
      "name": "Graph",
      "score": 0.4559914767742157
    },
    {
      "name": "Transformer",
      "score": 0.4530482292175293
    },
    {
      "name": "Algorithm",
      "score": 0.3567783832550049
    },
    {
      "name": "Line graph",
      "score": 0.22948160767555237
    },
    {
      "name": "Pathwidth",
      "score": 0.16271987557411194
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}