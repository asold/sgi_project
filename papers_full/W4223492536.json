{
  "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model",
  "url": "https://openalex.org/W4223492536",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2119207893",
      "name": "Hongyi Yuan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2032612703",
      "name": "Zheng Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343603905",
      "name": "Ruyi Gan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2124456965",
      "name": "jiaxing Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2272077919",
      "name": "Yutao Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022078535",
      "name": "Sheng Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205598244",
    "https://openalex.org/W2621196449",
    "https://openalex.org/W3212804087",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3037063616",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W3202031169",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1573258796",
    "https://openalex.org/W2163107094",
    "https://openalex.org/W3105801792",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2131546905",
    "https://openalex.org/W4220732108",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W3170822064",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3090073303",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2953126493",
    "https://openalex.org/W3175432034",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3096403953",
    "https://openalex.org/W2990031975",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3166358520",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W3034229721",
    "https://openalex.org/W4226470037",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3164540570",
    "https://openalex.org/W2950161719",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4221158733",
    "https://openalex.org/W3081260973",
    "https://openalex.org/W2047477415",
    "https://openalex.org/W2793567354",
    "https://openalex.org/W2950281195",
    "https://openalex.org/W2158743996",
    "https://openalex.org/W3198080531",
    "https://openalex.org/W2509884321",
    "https://openalex.org/W3101223450",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W3170083118",
    "https://openalex.org/W4293227627",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3175225269",
    "https://openalex.org/W3007759824"
  ],
  "abstract": "Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (NLG) tasks are of critical importance, while understudied. Approaching natural language understanding (NLU) tasks as NLG achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model BioBART that adapts BART to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. BioBART pretrained on PubMed abstracts has enhanced performance compared to BART and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for BioBART and find that sentence permutation has negative effects on downstream tasks.",
  "full_text": "Proceedings of the BioNLP 2022 workshop, Dublin, Ireland, pages 97–109\nMay 26, 2022. ©2022 Association for Computational Linguistics\n97\nBioBART: Pretraining and Evaluation of\nA Biomedical Generative Language Model\nHongyi Yuan1 ∗ Zheng Yuan1 ∗ Ruyi Gan2 Jiaxing Zhang2 Yutao Xie2 Sheng Yu1 †\n1Tsinghua University 2International Digital Economy Academy\n{yuanhy20,yuanz17}@mails.tsinghua.edu.cn\n{gaunruyi,zhangjiaxing,xieyutao}@idea.edu.cn\nsyu@tsinghua.edu.cn\nAbstract\nPretrained language models have served as im-\nportant backbones for natural language process-\ning. Recently, in-domain pretraining has been\nshown to benefit various domain-specific down-\nstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical\nimportance, while understudied. Approaching\nnatural language understanding (NLU) tasks\nas NLG achieves satisfying performance in\nthe general domain through constrained lan-\nguage generation or language prompting.We\nemphasize the lack of in-domain generative\nlanguage models and the unsystematic gener-\native downstream benchmarks in the biomedi-\ncal domain, hindering the development of the\nresearch community. In this work, we intro-\nduce the generative language model BioBART\nthat adapts BART to the biomedical domain.\nWe collate various biomedical language gen-\neration tasks including dialogue, summariza-\ntion, entity linking, and named entity recogni-\ntion. BioBART pretrained on PubMed abstracts\nhas enhanced performance compared to BART\nand set strong baselines on several tasks. Fur-\nthermore, we conduct ablation studies on the\npretraining tasks for BioBART and find that\nsentence permutation has negative effects on\ndownstream tasks.\n1 Introduction\nSince the advent of ELMo (Peters et al., 2018) and\nBERT (Devlin et al., 2019), the new pretrain-then-\nfinetune paradigm has brought great performance\nimprovement and dominated the methodology re-\nsearch of the natural language processing (NLP)\nfield. Previous research has illustrated that pre-\ntraining language models on the domain-specific\ncorpora can improve the model performance on\ndomain-specific tasks further (Gururangan et al.,\n2020). With the large-scale publicly accessible\n∗ Contributed equally.\n† Corresponded author.\ncorpora from PubMed, researchers have already\nproposed biomedical domain pretrained language\nmodels such as BioBERT (Lee et al., 2020) and\nPubMedBERT (Gu et al., 2022) to aid the later\nresearch.\nNatural language generation (NLG) tasks such\nas dialogue system (Chao et al., 2017) and ques-\ntion answering (Jin et al., 2022) are of critical im-\nportance for the biomedical artificial intelligence\nresearch, and there is also a trend to approach nat-\nural language understanding as NLG tasks in the\ngeneral domain (Sun et al., 2021; Yan et al., 2021).\nFor example, an entity retrieval task can be solved\nby constrained natural language generation (Cao\net al., 2021). However, there exist two gaps in\nthe research of the biomedical NLG. On the one\nhand, the architectures of the biomedical pretrained\nlanguage models are almost all encoder-only trans-\nformers. Such architecture is incapable of generat-\ning natural languages auto-regressively. A decoder\nis necessary for language generation (Liu and La-\npata, 2019). On the other hand, there are very\nfew in-domain generative language models for bio-\nmedicine (Phan et al., 2021). Models pretrained\non biomedical corpora may further enhance the\nperformance of current biomedical NLG methods.\nTo bridge the gaps mentioned above, we propose\na biomedical auto-regressive generative language\nmodel, BioBART, pretrained on the biomedical\ncorpora. In our work, we adopt BART (Bidirec-\ntional and Auto-Regressive Transformers), a gen-\nerative pretrained language model which achieves\nstate-of-the-art results on different NLG tasks in\nthe general domain (Lewis et al., 2020a). We con-\ntinuously pretrain BART on PubMed abstracts to\nachieve biomedical domain adaption only using the\ntext-infilling task. We also collate and evaluate Bio-\nBART on the existing biomedical NLG tasks. The\nin-domain BioBART outperforms BART model\nand sets strong baselines for several NLG tasks.\nThe main contributions of our work are summa-\n98\nrized as follows1:\n1. In aid of the research concerning the biomedi-\ncal NLG tasks, we collate existing biomedical\nNLG tasks along with corresponding data and\nexperimental settings. The archived biomedi-\ncal tasks will be released.\n2. We further analyze the influence of the\npretraining task of sentence permutation in\nBART, and we find it brings degradation on\nthe biomedical NLG tasks.\n3. We evaluate our BioBART models on various\nNLG tasks and demonstrate the superb perfor-\nmance over BART. We will release the codes\nand weights to help reproduce our results.\n2 Related Work\n2.1 Auto-regressive Language Model\nMost of the prestigious language models such\nas BERT, RoBERTa (Liu et al., 2019) are auto-\nencoding transformers. The encoder-only archi-\ntecture prevents the direct implementation of the\nseq2seq language generation. Several generative\nauto-regressive language models are proposed to\nmitigate the problem. The serial GPT models\n(Radford and Narasimhan, 2018; Radford et al.,\n2019; Brown et al., 2020) adopt the decoder-only\ntransformer architecture which is a left-to-right lan-\nguage model. They pretrain the models by auto-\nregressively predicting the upcoming word of sen-\ntences. UniLM1 (Dong et al., 2019) and UniLM2\n(Bao et al., 2020) implement attention masks to\nthe transformer encoder to achieve unidirectional\nlanguage modeling. They pretrain their models\nwith a mixture of masked language modeling and\nauto-regressive language generation. T5 (Raffel\net al., 2020) and BART (Lewis et al., 2020a) ap-\nply the full transformer architecture, the encoder is\nused for input sequence encoding and the decoder\nis used for language generation. T5 and BART are\nboth pretrained by denoising the corrupted corpora.\nSuch models achieve many state-of-the-art results\non various NLG tasks and some NLU tasks.\n2.2 Biomedical Domain Pretraining\nExisting work has shown that pretraining the lan-\nguage models on the domain-specific corpora can\n1Our codes and pretrained checkpoints can be found at\nhttps://github.com/GanjinZero/BioBART.\nbring better model transferability on the corre-\nsponding downstream tasks (Gururangan et al.,\n2020). There are endeavors to adapt language\nmodels to the specific domain. BioBERT (Lee\net al., 2020) pretrained BERT model using biomed-\nical corpora from PubMed abstracts and PubMed\nCentral (PMC) full-text articles. BlueBERT (Peng\net al., 2020) and clinicalBERT (Alsentzer et al.,\n2019) add electronic medical record (EMR) cor-\npora from MIMIC-III (Johnson et al., 2016) to\nthe pretraining data. Instead of continuous train-\ning from the general BERT checkpoint, SciBERT\n(Beltagy et al., 2019) and PubMedBERT (Gu et al.,\n2022) are trained from scratch using scientific pa-\npers from Semantic Scholar (Ammar et al., 2018)\nand PubMed articles respectively. (Shin et al.,\n2020) releases BioMegatron, a larger-size BERT-\nstyle language model pretrained on PubMed ab-\nstracts, PMC and MIMIC-III. The aforementioned\nwork all use the model architecture of BERT. Other\nresearchers are exploring different language mod-\nels.\nBioELMo (Jin et al., 2019) is pretrained on\nbiomedical corpora based on stacked bidirectional\nLSTM language model ELMo (Peters et al., 2018).\nBioELECTRA (Kanakarajan et al., 2021) applies\nan adversarial training scheme consisting of a dis-\ncriminator and a generator. They use PubMed ab-\nstracts and PMC articles as in-domain pretraining\ncorpora. BioMed-RoBERTa (Gururangan et al.,\n2020) is initialized from RoBERTa (Liu et al.,\n2019), with additional training on the scientific pa-\npers from Semantic Scholar. Bio-lm (Lewis et al.,\n2020b) is pretrained on data from PubMed, PMC,\nand MIMIC-III based on the RoBERTa model. Ke-\nBioLM (Yuan et al., 2021) uses Entity as Experts\n(Févry et al., 2020) model to inject biomedical en-\ntity knowledge into the language model, starting\nfrom the weights of PubMedBERT. Coder (Yuan\net al., 2022b) and SapBERT (Liu et al., 2021) take\nadvantage of the synonyms resource from biomed-\nical knowledge base UMLS (Bodenreider, 2004)\nand enhance the model with entity knowledge by\ncontrastive pretraining.\nDue to the nature of model architecture, encoder-\nonly language models have limited performance on\nthe NLG tasks, such as summarization and question\nanswering. In recent research, SciFive (Phan et al.,\n2021) is proposed for biomedical NLP tasks. Sci-\nFive is pretrained on PubMed abstracts and PMC\narticles based on T5 architecture. While T5 is avail-\n99\nable for NLG tasks, SciFive is focused on evaluat-\ning NLU tasks. Compared to SciFive, we choose\nto use BART as our model backbone and evalu-\nate more on NLG tasks to leverage the power of\ndecoders.\n2.3 Biomedical Natural Language Generation\nIn the biomedical domain, most of the NLP tasks\nare natural language understanding (NLU) tasks.\nThere are well-archived benchmarks for the evalua-\ntion of biomedical NLU, such as BLUE (Gu et al.,\n2022) and CBLUE (Zhang et al., 2021). NLG tasks\nare relatively less studied. (Ju et al., 2020) collects\nthe patients and doctors’ dialogues and forms a\nbenchmark for Covid-19 related dialogue system.\n(Ben Abacha et al., 2021) is an annual biomedical\nNLP competition containing NLG tasks such as\nmedical question (or answer) summarization and\nfigure captions.\nMoreover, with the success of GPT-3, there is a\nnovel trend that unifies all the NLP tasks as NLG\ntasks (McCann et al., 2018; Brown et al., 2020).\nThe traditional NLU tasks can be approached by\nconstrained language generation. Much attention\nis paid on the NLG methods recently. In the\nbiomedical domain, entities are of primary concern.\nGENRE (Cao et al., 2021), Yuan et al. (2022a) and\nBARTNER (Yan et al., 2021) reach the new state-\nof-the-art by auto-regressive language model on\nentity linking and named entity recognition tasks.\nSuch methods can be adapted to the biomedical\ndomain.\n3 Biomedical Domain Pretraining\nBART is a sequence-to-sequence model with a\nbi-directional encoder and a left-to-right auto-\nregressive decoder. The model architecture is con-\nsistent with the Transformers (Vaswani et al., 2017)\nexcept for changing the ReLU activation functions\nto GeLUs (Hendrycks and Gimpel, 2016). BART\nis pretrained by denoising the corrupted input doc-\numents. The work ablates five different types of\ncorruption noise: text masking, text deletion, text\ninfilling, sentence permutation, and document ro-\ntation. As a result, the pretraining documents are\ncorrupted in two ways: 1) Text Infilling: For each\ndocument, a number of token spans are sampled,\nand each sample span is replaced with a single\nmask token. 2) Sentence Permutation: A docu-\nment is split into sentences and sentences are shuf-\nfled in random orders. The pretraining objective\nis to minimize the negative log-likelihood of the\noriginal documents.\nPrior work has shown that continuous-pretrained\nmodels can get competitive results compared with\nthose trained from scratch (Gu et al., 2022). In\nour work, we continuously pretrain BART on the\nbiomedical domain corpora. We revisit the methods\nto corrupt input texts. BART keeps the sentence\npermutation noise because of the significant perfor-\nmance gain on the summarization task, although\nthis noise may lead to slight degradation on other\ntasks. We run further ablation studies on various\nbiomedical NLG tasks. We show that the model\npretrained without sentence permutation has better\nperformance. Further details are listed in Section\n5.5. Therefore we only implement the text infilling\ntask to corrupt input texts for pretraining BioBART.\n4 Generative Downstream Task\nIn this section, we introduce the generative down-\nstream tasks in the biomedical domain. We will\nconduct experiments on these tasks to illustrate the\nperformance of the domain-specific BioBART.\n4.1 Dialogue System\nA medical dialogue system aims to imitate the hu-\nman doctor to communicate with human patients in\na natural way. Based on the BART-style model, the\npatients’ primitive descriptions and dialogue histo-\nries are used as inputs to the model, then the model\nauto-regressively generates the replies as outputs.\nThe task is trained and evaluated in a sequence-to-\nsequence fashion.\n4.2 Abstractive Summarization\nSummarization is a classical NLP task. It is\nimportant for healthcare to concisely summarize\nknowledge-rich biomedical documents. Tech-\nnically, there are abstractive and extractive ap-\nproaches to generate better summaries. With the\nhelp of large pretrained language models, abstrac-\ntive summarization methods outperform extractive\nmethods in summary diversity and conciseness\n(Zhang et al., 2020a; Dou et al., 2021). The ab-\nstractive summarization is naturally an NLG task.\nWe follow the BART (Lewis et al., 2020a) work\nand evaluate our BioBART on the biomedical sum-\nmarization tasks in the same fashion. The input\ndocuments are encoded by the model encoder and\nthe summaries are generated by the decoder auto-\nregressively.\n100\n4.3 Entity Linking\nEntity linking is a task that maps entity mentions in\ntexts to its standard entity concepts. Traditional en-\ntity linking methods use language models to encode\nentity concepts from knowledge bases(e.g. UMLS)\nand mentions into the same dense space and disam-\nbiguate mentions by vector similarity. The large\nmemory footprint requirements and difficult model\ntraining hinder the development of such methods.\nCao et al. (2021) proposes GENRE which uses\ngenerative language models to disambiguate en-\ntity mentions by auto-regressively generating the\nstandard concept names conditioned on the inputs.\n(Yuan et al., 2022a) achieves state-of-the-art entity\nlinking performance on various biomedical entity\nlinking datasets by generative methods. We include\nthis leading-edge method to show the superior per-\nformance of BioBART.\n4.4 Named Entity Recognition\nNamed entity recognition (NER) is a critical task\nin the biomedical NLP community which extracts\nbiomedical-related entities from texts. Nested and\ndiscontinuous entities widely exist in biomedical\npapers and EMR due to the multi-granularity se-\nmantic meanings and complex syntax structures\n(Yuan et al., 2020). Well-used sequential labelling\nframework in NER (Lample et al., 2016) is not\ndirectly fitted for nested and discontinuous NER\n(Finkel and Manning, 2009). Yan et al. (2021)\npropose BARTNER to model nested and discontin-\nuous NER into seq2seq task by inputting sentences\nand outputting entities with their entity types one\nby one. The generative approach of BARTNER\nachieves state-of-the-art performance on nested and\ndiscontinuous NER datasets, and we will use it to\nevaluate our proposed BioBART can further en-\nhance the performance.\n5 Experiments\n5.1 Pretraining\nPretraining Corpora There are two main\nsources of biomedical corpora: PubMed abstracts,\nPMC articles. In the prior work (Gu et al., 2022),\ntraining on both corpora surprisingly leads to a\nslight degradation in performance compared to\nsolely training on PubMed abstracts. Therefore, we\nonly use PubMed abstracts as the pretraining cor-\npora. The corpora contain about 41 GB of biomed-\nical research paper abstracts on PubMed.\nPretraining Setup We continuously pretrain\nboth large and base versions of BART for 120k\nsteps with a batch size of 2560. We use the same\nvocabulary as BART to tokenize the texts. Al-\nthough the input length limitation of BART is 1024,\nthe tokenized PubMed abstracts rarely exceed 512.\nTherefore, for the sake of training efficiency, we\ntruncate all the input texts to 512 maximum length.\nWe mask 30% of the input tokens and the masked\nspan length is determined by sampling from a Pois-\nson distribution (λ = 3) as used in BART. We use\na learning rate scheduler of 0.02 warm-up ratio\nand linear decay. The learning rate is set to 1e-4.\nWe train the base version of BioBART on 2 DGX\nwith 16 40GB A100 GPUs for about 100 hours and\nthe large version of BioBART on the same devices\nfor 168 hours with the help of the open-resource\nframework DeepSpeed (Rajbhandari et al., 2020).\n5.2 Dataset for Downstream Task\n5.2.1 Dialogue System\nCovidDialog (Ju et al., 2020) Concerning the\nwidespread Coronavirus disease 2019 (COVID-19)\npandemic, the CovidDialog dataset is proposed to\nfacilitate the development of dialogue system pro-\nviding COVID-related consultations to people. The\ndataset is collected from online healthcare forums.\nIt contains 603 consultations about COVID-19 and\nother related pneumonia, having 1232 utterances in\ntotal. Each consultation starts with a description re-\nlated to patients’ medical conditions, then followed\nthe conversation between a doctor and a patient.\n5.2.2 Abstractive Summarization\niCliniq, HealthCareMagic Both datasets are\nextracted from MedDialog (Zeng et al., 2020)\ndataset, collected from the online healthcare plat-\nform. iCliniq contains 31,062 samples and Health-\nCareMagic contains 226,405 samples. Each sam-\nple is comprised of a summary and corresponding\ndialogues between a patient and a doctor. Health-\nCareMagic’s summaries are more abstractive and\nare written in a formal style, unlike iCliniq’s\npatient-written summaries. We follow the previous\nwork (Mrini et al., 2021) for training, developing,\nand testing data separations of both datasets.\nMeQSum (Ben Abacha and Demner-Fushman,\n2019) The dataset is created for better medical ques-\ntion summarization because the original patients’\nquestions are verbose, causing difficulty for the\nquestion-answering system. The dataset contains\n101\nTask Dataset Train Dev Test Dataset Train Dev Test Metric\nDialogue CovidDialog 490 63 61 Rouge,BERTscore,\nBLEU\nSummarization\nMeQSum 500 - 500 MEDIQA-ANS 38,166 174 552\nRouge, BERTscoreiCliniq 24,851 3,105 3,108 MEDIQA-QS 1,000 50 100\nHealthCareMagic 181,122 22,641 22,642 MEDIQA-MAS 1,104 50 80\nEntity Linking\nMedMentions 122,241 40,884 40,157 NCBI 5,784 787 960\nRecall@1,@5BC5CDR 9,285 9,515 9,654 COMETA 13,489 2,176 4,350\nAskAPatients 16,826 1,663 1,712\nNER ShARe13 5,146 669 5,333 ShARe14 10,380 771 7,922 Entity-level F1 scoreCADEC 4,430 898 990 GENIA 50,509 - 5,506\nTable 1: The statistics of the datasets for biomedical generative tasks. The counts for NER are entity counts.\nCovid19-Dialogue\nModel Rouge-1 Rouge-2 Rouge-L BLEU BERTscore\nBART BASE 27.24 12.31 25.66 10.36 0.852\nBioBART BASE 28.14 12.77 26.32 11.40 0.849\nBART LARGE 29.02 12.08 26.93 10.96 0.852\nBioBART LARGE 28.81 13.79 26.96 12.05 0.850\nState-of-the-art - - - 7.60 -\nSource - - - (Zhou et al., 2021) -\nTable 2: The main results on Dialogue System task.\n1000 patients’ health questions selected from a col-\nlection distributed by the U.S. National Library of\nMedicine (Kilicoglu et al., 2018). Each question is\nannotated with a question summarization by medi-\ncal experts.\nMEDIQA-ANS (Savery et al., 2020) When feel-\ning discomfort, people may turn to the internet for\nthe answers to their medical questions. The raw\nsearching result may be obscure for even medical\nexperts. The dataset is proposed to emphasize the\nneed for a medical answer summarization system\nin aid of better understanding biomedical materials.\nIt consists of 156 health questions, corresponding\nanswers to these questions, and expert-created sum-\nmaries (both abstractive and extractive) of these\nanswers. Following the paper, we use BioASQ\n(Tsatsaronis et al., 2015) to construct training data,\nMedInfo (Abacha et al., 2019) for validation, and\nthe whole MEDIQA-ANS dataset for testing.\nMEDIQA-QS, MEDIQA-MAS Both datasets\nare derived from the MEDIQA 2021 Tasks\n(Ben Abacha et al., 2021). MEDIQA-QS dataset\naims to incentivize the development of new sum-\nmarization approaches that address specifically the\nchallenges of long and complex health questions.\nThe dataset provides the validation and test sets,\nand MeQSum dataset is used as the training set.\nMEDIQA-MAS aims to prompt research that si-\nmultaneously aggregates and summarize the differ-\nent relevant answers to a medical question. This\ndataset provides the validation and test sets, and\nMEDIQA-ANS dataset comprises the training set.\n5.2.3 Entity Linking\nMedMentions (Mohan and Li, 2019) MedMen-\ntions is a large-scale biomedical entity recognition\ndataset. The commonly used St21pv subset con-\ntains 4,392 PubMed abstracts, and over 350,000\nmentions are linked to concepts of 21 selected se-\nmantic types in UMLS (Bodenreider, 2004).\nBC5CDR (Li et al., 2016) BC5CDR is a bench-\nmark for biomedical entity linking. 1500 PubMed\narticle abstracts are annotated with 4409 chemicals,\n5818 diseases entities, and 3116 chemical-disease\ninteractions. MeSH ontology, a subset of UMLS\nis used to annotate entities. We follow most recent\nwork (Angell et al., 2021; Varma et al., 2021) for\ndata pre-processing.\nNCBI (Do˘gan et al., 2014) The dataset is built\nfrom 793 PubMed abstracts. It consists of 6892\nannotated disease mentions of 790 unique disease\nconcepts. The annotators label all the mentions to\nconcepts in MEDIC ontology (Davis et al., 2012).\nMEDIC is a medical dictionary that merges the\ndiseases concepts, synonyms, and definitions in\nMeSH and OMIM and is composed of 9700 unique\ndiseases. We follow BioSyn (Sung et al., 2020) to\nprocess data and construct dataset splits.\nCOMETA (Basaldella et al., 2020) COMETA\nis derived from the online publicly available and\n102\niCliniq HealthCareMagic MEDIQA-QS\nModel Rouge-1/2/L BERTscore Rouge-1/2/L BERTscore Rouge-1/2/L BERTscore\nBART BASE 61.43 /48.68/59.71 0.941 46.81/26.19/44.34 0.918 28.82/10.99/26.99 0.896\nBioBART BASE 61.07/48.47/59.42 0.941 46.67/26.03/44.11 0.918 30.12/11.28/27.44 0.898\nBART LARGE 59.87/47.01/58.12 0.938 47.24/26.54/44.68 0.919 29.97/10.64/28.41 0.901\nBioBART LARGE 60.32/47.98/58.69 0.940 46.54/26.14/44.23 0.919 31.97/12.39/29.70 0.903\nState-of-the-art 62.3/48.7/58.5 - 46.9 /24.8/43.2 - 35.14/16.08/31.31 -\nSource (Mrini et al., 2021) (Mrini et al., 2021) (Ben Abacha et al., 2021)\nMEDIQA-MAS MEDIQA-ANS(Pages) MeQSum\nModel Rouge-1/2/L BERTscore Rouge-1/2/L BERTscore Rouge-1/2/L BERTscore\nBART BASE 31.63 /9.98/27.85 0.859 19.10/6.77/16.90 0.851 52.93/35.79/50.46 0.927\nBioBART BASE 32.90/11.28/29.26 0.861 18.97/7.46/16.77 0.850 53.75/36.50/51.27 0.929\nBART LARGE 29.32/9.00/26.14 0.857 21.52/9.31 /19.15 0.853 53.68/36.80/51.05 0.928\nBioBART LARGE 30.60/10.37/27.04 0.861 21.58/9.34/19.18 0.857 55.61 /38.11/53.15 0.933\nState-of-the-art 32.15/ 16.21/19.10 - 23.07/ 5.41/15.35 - 54.5 /37.9/50.2 -\nSource (Ben Abacha et al., 2021) (Laskar et al., 2021) (Mrini et al., 2021)\nTable 3: The main results on Summarization tasks.\nMedMentions BC5CDR NCBI COMETA AAP\nModel Recall@1/@5 Recall@1/@5 Recall@1/@5 Recall@1/@5 Recall@1/@5\nBART BASE 69.77/84.59 91.56/94.89 88.54/95.31 78.34/87.40 86.37/94.29\nBioBART BASE 71.15/ 86.22 93.01/95.59 89.27/95.31 79.63/88.64 87.51/94.92\nBART LARGE 71.49/84.95 92.48/95.26 90.21 /95.52 80.70/88.65 88.79/96.59\nBioBART LARGE 71.78 /85.42 93.26/95.74 89.90/95.63 81.77 /88.87 89.40 /95.76\nState-of-the-art 74.6/ - 91.9/ - 92.4/ - 80.1/ - 89.0 / -\nSource (Varma et al., 2021) (Varma et al., 2021) (Lai et al., 2021) (Lai et al., 2021) (Liu et al., 2021)\nTable 4: The main results on Entity Linking tasks.\nShARe13 ShARe14 CADEC GENIA\nModel F1 F1 F1 F1\nBART BASE 76.63 77.87 68.37 78.06\nBioBART BASE 78.78 79.17 68.39 78.43\nBART LARGE 79.69 80.34 70.64 78.93\nBioBART LARGE 80.75 80.41 70.53 79.93\nState-of-the-art 82.52 81.75 73.21 81.39\nSource (Li et al., 2021)\nTable 5: The main result on NER tasks.\nanonymous health discussion on Reddit. It consists\nof 20k English biomedical entity mentions expert-\nannotated with concepts from SNOMED CT. We\nuse the “stratified (general)” split and follow the\ntraining and evaluation procedures of SapBert (Liu\net al., 2021) and ResCNN (Lai et al., 2021).\nAskAPatient (Limsopatham and Collier, 2016)\nIt contains 8,662 phrases from social media. Each\nphrase can be mapped to one of the 1,036 medical\nconcepts from SNOMED-CT and AMT (the Aus-\ntralian Medicines Terminology). The samples in\nAskAPatient do not include contextual information.\nWe follow Sung et al. (2020) and Limsopatham and\nCollier (2016) for data pre-processing and apply\nthe 10-fold evaluation protocol.\n5.2.4 Named Entity Recognition\nShARe13, ShARe14, CADEC These three\ndatasets annotate discontinuous adverse drug\nevents entities. The main difference is the anno-\ntated data of ShARe tasks (Pradhan et al., 2013;\nMowery et al., 2014) comes from MIMIC-II, and\nCADEC (Karimi et al., 2015) comes from social\nmedia. There is only one entity type for these\ndatasets. We follow Yan et al. (2021) for dataset\npreprocess.\nGENIA (Kim et al., 2003) GENIA annotates\n2000 MEDLINE abstracts with biological entities.\nEntities can be nested with others. We follow (Lin\net al., 2019) to combine fine-grained entity types\ninto 5 coarse-grained entity types and to construct\ndataset splits.\nAll the aforementioned datasets are in English.\nThe statistical overview of the aforementioned\ndatasets is listed in Table 1.\n5.3 Fine-tuning details\nDialogue We use BioBART as the dialogue sys-\ntem model. The dialogue history is fed into the en-\ncoder and the decoder generates the response auto-\nregressively. We apply the negative log-likelihood\nfunction as the training objective with respect to\n103\nthe reference dialogue response. We fine-tune the\nmodel with learning rate 5e-5 for the base version\nand 1e-5 for the large version for 20 epochs. We\nrun evaluations on the validation set at the end of\neach epoch and use the checkpoint with the best\nvalidation performance for testing. During infer-\nence, we use beam search of size 5 to sample re-\nsponses from the model’s outputs. We use Rouge-\n1/2/L (Lin, 2004), BLEU (Papineni et al., 2002)\nand BERTscore (Zhang et al., 2020b) as our evalu-\nation metrics. RoBERTa-large (Liu et al., 2019) is\nused as scorer in BERTscore.\nSummarization Similarly, for summarization,\nthe encoder takes the documents as input, and the\ndecoder generates the corresponding summariza-\ntions. We minimize the log-likelihood objective\nto fine-tune the model and apply beam search for\ninference. Across different summarization datasets,\nthe beam size is set to 5 and we use no length\npenalty. We fine-tune the model with learning rate\n5e-5 for the base version and 1e-5 for the large\nversion for 6 epochs. We run evaluations on the\nvalidation set at the end of each epoch and use the\ncheckpoint with the best validation performance\nfor testing. We apply the commonly used Rouge-\n1/2/L and BERTscore for evaluation metrics. The\nlarge version of RoBERTa is used as the scorer in\nBERTscore.\nEntity Linking We follow the method and ex-\nperimental settings in Yuan et al. (2022a) to imple-\nment the generative model for biomedical entity\nlinking tasks. Knowledge-base guided pre-training\nin Yuan et al. (2022a) has not been applied. The\ndocuments with the positions of mentions marked\nare fed into the encoder and the decoder outputs\nthe corresponding synonyms in the knowledge base\ndirectly. We use the top1 and top5 recall (Recall@1\nand Recall@5) as the evaluation metrics.\nNER We use BARTNER (Yan et al., 2021) as\nour model. The target type for BARTNER is word\n(i.e. output first BPE of each word in entities). We\nuse the parameters selected by Yan et al. (2021) for\nall pretrained models and fine-tune for 30 epochs.\nEntity-level F1 is used as the metric.\n5.4 Main Result\nIn this section, we present the base and large ver-\nsion of BioBART on various generation tasks. We\ncompare our in-domain BioBART with BART to\nillustrate the effectiveness of domain adaption. We\nalso compare with the existing state-of-the-art re-\nsults on each dataset to shed light on the superior\nperformance of BioBART. The experimental re-\nsults are shown in Table 2-5. The best and the\nsecond-best scores are highlighted with bold num-\nbers and underlines respectively.\nDialogue We evaluate biomedical dialogue re-\nsponse generation on CovidDialog. For both base\nand large version, BioBART shows improvement\non the automatic metric Rouge. The large Bio-\nBART outperforms BART by 1.71 on Rouge-2 and\n0.03 on Rouge-L . Our evaluations surpasses the\ncurrent state-of-the-art on BLEU score by 4.45.\nSummarization We present broad experimen-\ntal results on biomedical summarization datasets.\nFrom Table 3, BioBART has competitive or even\nsuperior performance on the task. Except for\niCliniq and HealthCareMagic, we see consistent\nimprovement on different datasets for both sizes of\nBioBART. For MeQSum, BioBART large exceeds\nBART large for 1.93/1.31/2.1 on Rouge-1/2/L and\neven outperforms the current state-of-the-art. The\npossible reason that biomedical in-domain pretrain-\ning fails on iCliniq and HealthCareMagic is that\nboth datasets are built upon a clinical corpus. There\nstill exists a domain-shifting problem for BioBART\npretrained on biomedical scientific articles from\nPubMed.\nOn dialogue and summarization tasks, there are\nminor changes in BERTscore for different models.\nThis is possible because the metric is calculated\nby other pretranined language models. The im-\nplemented RoBERTa may suffer from biomedical\ndomain-shifting and cannot quantify the model per-\nformance accurately.\nEntity Linking The results on biomedical en-\ntity linking tasks are shown in Table 4. For all\nthe tasks, models finetuned based on BioBART\nhave better performance. On AAP, BC5CDR, and\nCOMETA, our results outperform the current dis-\ncriminative state-of-the-art methods by 0.4, 1.67,\nand 1.36 points of Recall@1 respectively.\nNER The performance improvement of Bio-\nBART on ShARe13, ShARe14, and GENIA is sig-\nnificant, while the increase on CADEC is mediocre.\nFor the large models, BioBART improves entity-\nlevel F1 scores for 1.06 and 1 on ShARe13 and\nGENIA datasets. There are promising results for\ngenerative biomedical NER methods, while the gap\n104\nCovidDialogue MeQSum MEDIQA-MAS\nRouge-2/L BLEU Rouge-2/L BERTscore Rouge-2/L BERTscore\nBART BASE 12.31/25.66 10.36 35.79/50.46 0.927 9.98/ 27.85 0.859\nw/ TI & SP 10.90/25.46 10.46 34.93/49.28 0.926 9.04/27.43 0.859\nw/ TI 11.81/ 25.79 12.79 37.14 /51.71 0.929 10.66 /27.65 0.862\nMedMentions COMETA ShARe13 CADEC\nRecall@1 Recall@5 Recall@1 Recall@5 F1 F1\nBART BASE 69.77 84.59 78.34 87.40 76.63 68.37\nw/ TI & SP 70.76 85.69 79.31 87.93 77.01 67.98\nw/ TI 70.82 85.68 79.43 88.28 78.03 69.22\nTable 6: Ablation study for pretraining tasks. TI is short for Text Infilling and SP is short for Sentence Permutation.\nContext(Document) BART Response(Summary) BioBART Response(Summary)\nDescription: I have type 1 diabetes that I try\nhard to control, but i struggle with. Are my\nchances high for poor outcomes with the virus?\nI’m in such a panic about it.\nIn brief: diabetes. Your risk of complications\nwith covid-19 is very low. If you are well con-\ntrolled your risk should not be elevated. If your\ndiabetes is uncontrolled you should not have a\nparticularly high risk.\nDiabetes and Covid-19. If you are a type 1\ndiabetic, you are at high risk for poor outcomes\nwith the virus. Your risk is very high. You\nshould be aware of your risk factors and monitor\nyour glycemic control closely.\nSUBJECT: treatment for dry mouth caused by\nnecessary medicine MESSAGE: My provider\ncan’t help (I asked.) I am intolerant of all the\n\"sugar alcohols\" such as maltilol, ...\nWhat are the treatments for dry mouth caused\nby sugar alcohols?\nWhat are the treatments for dry mouth?\nAre there any new treatments for orthostatic\ntremor? ... Deep brain stimulation is some-\ntimes done when drugs cannot control a severe,\ndisabling tremor. Sometimes essential tremors\nor tremors due to Parkinson disease or another\ndisorder require such treatment. ... There is\nno cure for most forms of tremor, but there are\ntreatments to help manage symptoms. ...\nThere is no cure for tremor, but there are treat-\nments to help manage the symptoms.\nThere is no cure for tremor, but there are treat-\nments to help manage the symptoms. The most\ncommon ... help relieve symptoms. Deep brain\nstimulation is sometimes done when drugs can-\nnot control a severe, disabling tremor. Some-\ntimes essential tremors or tremors due to Parkin-\nson disease or another disorder require such\ntreatment.\nTable 7: Example dialogue and summaries from the fine-tuned BART and BioBART model excepts of showing the\nreferences. The key information and differences are highlighted with colors.\nwith the current state-of-the-art NER method (Li\net al., 2021) is still salient.\n5.5 Ablation Study on Pretraining Task\nIn this section, we test on pretraining with or with-\nout the sentence permutation task. We pretrain\nBART base following the same pretraining settings\nexcept for reducing the training step to 40k for effi-\nciency. We fine-tuned the pretrained models on the\ndownstream tasks. The ablation results are shown\nin Table 6.\nFrom the result, it is illustrated that the model\npretrained on isolated text infilling task performs\nthe best. The sentence permutation task down-\ngrades the model’s performance even for generative\nsummarization and dialogue system tasks.\n5.6 Generated example\nHere we demonstrate BioBART’s performance\nqualitatively. In Table 7, we present three gen-\nerative examples on CovidDialog, MeQSum, and\nMEDIQA-ANS respectively. In the first example,\nwe can see that BART generates an erroneous in-\nstruction of the influence of diabetes. BioBART\ninjected with domain knowledge can correctly give\nthe response. In the second, BART misunderstands\nthe document where sugar alcohol is not the cause\nof dry mouth. BioBART generates an accurate\nand concise summary. In the final example, the\nMEDIQA-ANS document is rather long and BART\nfails to extract complete information (colored in\nred). From the examples, we can conclude that\nBioBART has improvements on biomedical com-\nmon sense and documents understanding.\n6 Conclusions\nIn this work, we pretrain the biomedical domain\ngenerative language model BioBART. We also\ncollect various publicly available benchmarks for\nbiomedical generative tasks to prompt future re-\nsearch. Our experimental results show that con-\ntinuous pretraining on PubMed abstracts helps the\nmodel with domain adaption. BioBART shows\n105\ngreat improvements on different benchmarks and\nachieves competitive or superior results over the\ncurrent state-of-the-art methods. We also release\nour pretraining and fine-tuning codes to facilitate\nfuture research for reproducibility.\nWe will explore pretraining generative language\nmodels 1) on in-domain vocabularies and from\nscratch, 2) and with clinical corpora such as EMRs\nin MIMIC-III (Johnson et al., 2016) or PMC-\nPatients (Zhao et al., 2022) in the future studies.\nAcknowledgements\nWe appreciate three anonymous reviewers for help-\nful comments. This work was supported by the Na-\ntional Natural Science Foundation of China (Grant\nNo. 12171270), and the Natural Science Founda-\ntion of Beijing Municipality (Grant No. Z190024).\nReferences\nAsma Ben Abacha, Yassine Mrabet, Mark E. Sharp,\nTravis R. Goodwin, Sonya E. Shooshan, and Dina\nDemner-Fushman. 2019. Bridging the gap between\nconsumers’ medication questions and trusted an-\nswers. Studies in health technology and informatics,\n264:25–29.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop, pages\n72–78.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu A. Ha, Rodney Michael Kinney, Sebastian\nKohlmeier, Kyle Lo, Tyler C. Murray, Hsu-Han Ooi,\nMatthew E. Peters, Joanna L. Power, Sam Skjonsberg,\nLucy Lu Wang, Christopher Wilhelm, Zheng Yuan,\nMadeleine van Zuylen, and Oren Etzioni. 2018. Con-\nstruction of the literature graph in semantic scholar.\nIn NAACL.\nRico Angell, Nicholas Monath, Sunil Mohan, Nishant\nYadav, and Andrew McCallum. 2021. Clustering-\nbased inference for biomedical entity linking. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2598–2608, Online. Association for Computa-\ntional Linguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. In International Conference on Ma-\nchine Learning, pages 642–652. PMLR.\nMarco Basaldella, Fangyu Liu, Ehsan Shareghi, and\nNigel Collier. 2020. COMETA: A corpus for med-\nical entity linking in the social media. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n3122–3137, Online. Association for Computational\nLinguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nEMNLP.\nAsma Ben Abacha and Dina Demner-Fushman. 2019.\nOn the summarization of consumer health questions.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28th - August 2.\nAsma Ben Abacha, Yassine Mrabet, Yuhao Zhang, Chai-\ntanya Shivade, Curtis Langlotz, and Dina Demner-\nFushman. 2021. Overview of the MEDIQA 2021\nshared task on summarization in the medical domain.\nIn Proceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 74–85, Online. Associa-\ntion for Computational Linguistics.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical ter-\nminology. Nucleic acids research , 32 Database\nissue:D267–70.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Representa-\ntions.\nHsiao-Tuan Chao, Lucy Liu, and Hugo J Bellen. 2017.\nBuilding dialogues between clinical and biomedical\nresearch through cross-species collaborations. In\nSeminars in cell & developmental biology, volume 70,\npages 49–57. Elsevier.\nAllan Peter Davis, Thomas C Wiegers, Michael C\nRosenstein, and Carolyn J Mattingly. 2012. Medic: a\npractical disease vocabulary used at the comparative\ntoxicogenomics database. Database, 2012.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\n106\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2021. GSum: A gen-\neral framework for guided neural abstractive summa-\nrization. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4830–4842, Online. Association for\nComputational Linguistics.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4937–4951, Online. Association\nfor Computational Linguistics.\nJenny Rose Finkel and Christopher D Manning. 2009.\nNested named entity recognition. In Proceedings of\nthe 2009 conference on empirical methods in natural\nlanguage processing, pages 141–150.\nYuxian Gu, Robert Tinn, Hao Cheng, Michael R. Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2022. Domain-\nspecific language model pretraining for biomedical\nnatural language processing. ACM Transactions on\nComputing for Healthcare (HEALTH), 3:1 – 23.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89.\nQiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu,\nHuaiyuan Ying, Chuanqi Tan, Mosha Chen, Song-\nfang Huang, Xiaozhong Liu, and Sheng Yu. 2022.\nBiomedical question answering: A survey of ap-\nproaches and challenges. ACM Comput. Surv., 55(2).\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen,\nLi wei H. Lehman, Mengling Feng, Moham-\nmad Mahdi Ghassemi, Benjamin Moody, Peter\nSzolovits, Leo Anthony Celi, and Roger G. Mark.\n2016. Mimic-iii, a freely accessible critical care\ndatabase. Scientific Data, 3.\nZeqian Ju, Subrato Chakravorty, Xuehai He, Shu Chen,\nXingyi Yang, and Pengtao Xie. 2020. Coviddi-\nalog: Medical dialogue datasets about covid-19.\nhttps://github.com/UCSD-AI4H/COVID-Dialogue.\nKamal Raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021. Bioelec-\ntra:pretrained biomedical text encoder using discrim-\ninators. In BIONLP.\nSarvnaz Karimi, Alejandro Metke-Jimenez, Madonna\nKemp, and Chen Wang. 2015. Cadec: A corpus of ad-\nverse drug event annotations. Journal of biomedical\ninformatics, 55:73–81.\nHalil Kilicoglu, Asma Ben Abacha, Yassine Mrabet,\nSonya E. Shooshan, Laritza M. Rodriguez, Kate Mas-\nterton, and Dina Demner-Fushman. 2018. Seman-\ntic annotation of consumer health questions. BMC\nBioinformatics, 19.\nJ-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi\nTsujii. 2003. Genia corpus—a semantically anno-\ntated corpus for bio-textmining. Bioinformatics,\n19(suppl_1):i180–i182.\nTuan Lai, Heng Ji, and ChengXiang Zhai. 2021. BERT\nmight be overkill: A tiny but effective biomedical\nentity linker based on residual convolutional neural\nnetworks. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 1631–\n1639, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nMd Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy Xiangji Huang. 2021. Domain adaptation\nwith pre-trained transformers for query focused\nabstractive text summarization. arXiv preprint\narXiv:2112.11670.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36:1234 – 1240.\n107\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020b. Pretrained language models for biomed-\nical and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157, Online. Association for Computational Lin-\nguistics.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nJingye Li, Hao Fei, Jiang Liu, Shengqiong Wu, Meishan\nZhang, Chong Teng, Donghong Ji, and Fei Li. 2021.\nUnified named entity recognition as word-word rela-\ntion classification. arXiv preprint arXiv:2112.10070.\nNut Limsopatham and Nigel Collier. 2016. Normalis-\ning medical concepts in social media texts by learn-\ning semantic representation. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1014–1023, Berlin, Germany. Association for Com-\nputational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nHongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2019.\nSequence-to-nuggets: Nested entity mention detec-\ntion via anchor-region networks. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5182–5192, Florence,\nItaly. Association for Computational Linguistics.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021. Self-alignment\npretraining for biomedical entity representations. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4228–4238.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language\ndecathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nSunil Mohan and Donghui Li. 2019. Medmentions: A\nlarge biomedical corpus annotated with {umls} con-\ncepts. In Automated Knowledge Base Construction\n(AKBC).\nDanielle L Mowery, Sumithra Velupillai, Brett R South,\nLee Christensen, David Martinez, Liadh Kelly, Lor-\nraine Goeuriot, Noemie Elhadad, Sameer Pradhan,\nGuergana Savova, et al. 2014. Task 2: Share/clef\nehealth evaluation lab 2014. In Proceedings of CLEF\n2014.\nKhalil Mrini, Franck Dernoncourt, Seunghyun Yoon,\nTrung Bui, Walter Chang, Emilia Farcas, and Ndapa\nNakashole. 2021. A gradually soft multi-task and\ndata-augmented approach to medical question under-\nstanding. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1505–1515, Online. Association for Computa-\ntional Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nYifan Peng, Qingyu Chen, and Zhiyong Lu. 2020. An\nempirical study of multi-task learning on bert for\nbiomedical text mining. In BIONLP.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nLong Phan, James T. Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. ArXiv,\nabs/2106.03598.\nSameer Pradhan, Noemie Elhadad, Brett R South, David\nMartinez, Lee M Christensen, Amy V ogel, Hanna\nSuominen, Wendy W Chapman, and Guergana K\nSavova. 2013. Task 1: Share/clef ehealth evaluation\nlab 2013. In CLEF (Working Notes), pages 212–31.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\n108\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In Pro-\nceedings of the International Conference for High\nPerformance Computing, Networking, Storage and\nAnalysis, SC ’20. IEEE Press.\nMax E. Savery, Asma Ben Abacha, Soumya Gayen, and\nDina Demner-Fushman. 2020. Question-driven sum-\nmarization of answers to consumer health questions.\nScientific Data, 7.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. BioMegatron: Larger\nbiomedical domain language model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4700–4706, Online. Association for Computational\nLinguistics.\nTianxiang Sun, Xiangyang Liu, Xipeng Qiu, and Xuan-\njing Huang. 2021. Paradigm shift in natural language\nprocessing. arXiv preprint arXiv:2109.12575.\nMujeen Sung, Hwisang Jeon, Jinhyuk Lee, and Jaewoo\nKang. 2020. Biomedical entity representations with\nsynonym marginalization. In ACL.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R. Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, Yannis Almirantis, John Pavlopoulos, Nico-\nlas Baskiotis, Patrick Gallinari, Thierry Artières,\nAxel-Cyrille Ngonga Ngomo, Norman Heino, Éric\nGaussier, Liliana Barrio-Alvers, Michael Schroeder,\nIon Androutsopoulos, and Georgios Paliouras. 2015.\nAn overview of the bioasq large-scale biomedical se-\nmantic indexing and question answering competition.\nBMC Bioinformatics, 16.\nMaya Varma, Laurel Orr, Sen Wu, Megan Leszczyn-\nski, Xiao Ling, and Christopher Ré. 2021. Cross-\ndomain data integration for named entity disambigua-\ntion in biomedical text. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2021 ,\npages 4566–4575, Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nHang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng\nZhang, and Xipeng Qiu. 2021. A unified generative\nframework for various NER subtasks. InProceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5808–5822, Online.\nAssociation for Computational Linguistics.\nHongyi Yuan, Zheng Yuan, and Sheng Yu. 2022a. Gen-\nerative biomedical entity disambiguation via knowl-\nedge base-guided pre-training and synonyms-aware\nfine-tuning. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,\nand Fei Huang. 2021. Improving biomedical\npretrained language models with knowledge. In\nBIONLP.\nZheng Yuan, Yuanhao Liu, Qiuyang Yin, Boyao Li, Xi-\naobin Feng, Guoming Zhang, and Sheng Yu. 2020.\nUnsupervised multi-granular chinese word segmenta-\ntion and term discovery via graph partition. Journal\nof Biomedical Informatics, 110:103542.\nZheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao Li, Fei\nWang, and Sheng Yu. 2022b. Coder: Knowledge-\ninfused cross-lingual medical term embedding for\nterm normalization. Journal of Biomedical Informat-\nics, page 103983.\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang,\nSicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi\nZeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang,\nPenghui Zhu, Shu Chen, and Pengtao Xie. 2020.\nMedDialog: Large-scale medical dialogue datasets.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9241–9250, Online. Association for Computa-\ntional Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020a. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nNingyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang,\nLei Li, Xin Shang, Kangping Yin, Chuanqi Tan, Jian\nXu, Fei Huang, et al. 2021. Cblue: A chinese biomed-\nical language understanding evaluation benchmark.\narXiv preprint arXiv:2106.08087.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nZhengyun Zhao, Qiao Jin, and Sheng Yu. 2022. Pmc-\npatients: A large-scale dataset of patient notes and\nrelations extracted from case reports in pubmed cen-\ntral. arXiv preprint arXiv:2202.13876.\n109\nMeng Zhou, Zechen Li, Bowen Tan, Guangtao Zeng,\nWenmian Yang, Xuehai He, Zeqian Ju, Subrato\nChakravorty, Shu Chen, Xingyi Yang, Yichen Zhang,\nQingyang Wu, Zhou Yu, Kun Xu, Eric Xing, and\nPengtao Xie. 2021. On the generation of medical\ndialogs for COVID-19. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 886–896, Online. Association\nfor Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.848961353302002
    },
    {
      "name": "Automatic summarization",
      "score": 0.7794241905212402
    },
    {
      "name": "Generative grammar",
      "score": 0.7588828206062317
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7305405735969543
    },
    {
      "name": "Natural language generation",
      "score": 0.7176121473312378
    },
    {
      "name": "Sentence",
      "score": 0.6664168238639832
    },
    {
      "name": "Natural language processing",
      "score": 0.6631886959075928
    },
    {
      "name": "Language model",
      "score": 0.640129804611206
    },
    {
      "name": "Natural language understanding",
      "score": 0.6058500409126282
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5963062047958374
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5379047989845276
    },
    {
      "name": "Natural language",
      "score": 0.481205016374588
    },
    {
      "name": "Programming language",
      "score": 0.08636236190795898
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}