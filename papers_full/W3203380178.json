{
  "title": "Probing Language Models for Understanding of Temporal Expressions",
  "url": "https://openalex.org/W3203380178",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3127475380",
      "name": "Shivin Thukral",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2558124489",
      "name": "Kunal Kukreja",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A3201884932",
      "name": "Christian Kavouras",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3099246072",
    "https://openalex.org/W2147923392",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W1956885672",
    "https://openalex.org/W3092457365",
    "https://openalex.org/W2140244223",
    "https://openalex.org/W2888945678",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W30314283",
    "https://openalex.org/W2964263366",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2251325107",
    "https://openalex.org/W3104036557",
    "https://openalex.org/W2904990800",
    "https://openalex.org/W2892202918",
    "https://openalex.org/W2979649142",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2098844768",
    "https://openalex.org/W2971236147",
    "https://openalex.org/W2251220668",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W1990886313",
    "https://openalex.org/W2963846996"
  ],
  "abstract": "We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.",
  "full_text": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 396–406\nOnline, November 11, 2021. ©2021 Association for Computational Linguistics\n396\nProbing Language Models for Understanding of Temporal Expressions\nShivin Thukral and Kunal Kukreja and Christian Kavouras\nDepartment of Linguistics\nUniversity of Washington\n{shivin7, kkukreja, cdkavour}@uw.edu\nAbstract\nWe present three Natural Language Inference\n(NLI) challenge sets that can evaluate NLI\nmodels on their understanding of temporal ex-\npressions. More speciﬁcally, we probe these\nmodels for three temporal properties: (a) the\norder between points in time, (b) the duration\nbetween two points in time, (c) the relation be-\ntween the magnitude of times speciﬁed in dif-\nferent units. We ﬁnd that although large lan-\nguage models ﬁne-tuned on MNLI have some\nbasic perception of the order between points\nin time, at large, these models do not have\na thorough understanding of the relation be-\ntween temporal expressions.\n1 Introduction\nWhile contextualized embeddings obtained from\nrecent transformer-based models such as BERT\n(Devlin et al., 2019) have proven to contain a lot\nof semantic and syntactic information about the\ntokens they encode, recent studies have shown that\nthere are still gaps in their understanding (Rogers\net al., 2020). On the semantic side, for instance,\nBERT struggles with representations of numbers\n(Wallace et al., 2019) and cannot reason based on\nits world knowledge (Rogers et al., 2020). Work in\nNLI has also developed challenge sets showing that\nthe reported performance of these language models\non various tasks can be exaggerated (McCoy et al.,\n2019), and they rely on lexical cues in the dataset\ninstead of actual language comprehension.\nOur work explores the grasp of such models on\nthe relation between temporal expressions. Tempo-\nral expressions, or time expressions, in text are a\nsequence of tokens that denote time, such as a point\nin time (6 May 1980, Monday, 12 PM) or duration\n(7 minutes, 5 years, 2 months). More speciﬁcally,\nwe try to determine whether these models capture\nthe ordering and duration relationships between dif-\nferent points in time. We also analyze if these mod-\nels can reason about durations speciﬁed in different\nunits. Recognition of temporal expressions has\nhad applications in timeline construction (Do et al.,\n2012; Leeuwenberg and Moens, 2018) and clinical\nanalysis (Bethard et al., 2015) previously, and can\nbe beneﬁcial for dialogue assistants in scheduling\nreminders and meetings, which shapes our moti-\nvation behind conducting such an analysis. We\nevaluate these models on the above temporal prop-\nerties by presenting three NLI challenge sets.\nOur experiments demonstrate that language mod-\nels such as RoBERTa (Liu et al., 2019) and De-\nBERTa (He et al., 2021) ﬁne-tuned on existing\nlarge NLI datasets are unable to completely reason\nabout the ordering and duration between temporal\nexpressions. We further analyze the examples and\nﬁnd that while these models recognize whether a\npoint in time lies within an interval, they cannot\ncapture other relations between time instances and\ndurations1.\n2 Related Work\nMuch work has been done on the extraction of\nevents, temporal expressions (Chen et al., 2019;\nDing et al., 2019), and the temporal relations be-\ntween the two. TimeBank (Pustejovsky et al.,\n2003b) was one of the ﬁrst annotated corpora for\nthis task. It utilized the TimeML (Pustejovsky et al.,\n2003a) standard for annotation. TempEval-1 (Ver-\nhagen et al., 2007), TempEval-2 (Verhagen et al.,\n2010), and TempEval-3 (UzZaman et al., 2013) are\nshared tasks created for evaluating models on var-\nious temporal properties, and most methods used\nwere traditional rule-based (Strötgen and Gertz,\n2010; Ning et al., 2018) or grammar-based (Lee\net al., 2014) solutions.\nVarious corpora have been developed that test\nfor different temporal properties. Vashishtha et al.\n(2019) map events to their ﬁne-grained duration,\nand event pairs to their relative timelines. Naik et al.\n1Code and data available on GitHub\n397\n(2019) create additional annotations in the exist-\ning TimeBank-Dense corpus (Cassidy et al., 2014)\nfor discourse-level temporal ordering. Ning et al.\n(2020) create a reading comprehension dataset that\ntests for temporal ordering. Zhou et al. (2019) test\nfor various temporal commonsense properties us-\ning a multiple-choice question-answering dataset.\nVashishtha et al. (2020) recast existing temporal\ndatasets into NLI format to test for temporal order-\ning and duration.\nOur goal is to create similar datasets to probe for\na semantic understanding of temporal expressions\nin pre-trained language models. We create these\ndatasets in an NLI format and use them to evalu-\nate NLI models trained on MNLI (Williams et al.,\n2018), which is a generic NLI dataset. We choose\nMNLI because it is large and diverse. The dataset\ncontains time terms in 36% of the development in-\nstances, including examples containing temporal\nexpressions like months and days of the week. We\ninvestigate whether these examples are sufﬁcient\nfor a general perception of temporal expressions.\nTo our knowledge, there has been little work\ninvestigating the implicit understanding of time\nexpressions in pre-trained large language models.\nThe most similar work to ours is Vashishtha et al.\n(2020). They produce ﬁve NLI datasets recast from\nexisting temporal reasoning corpora and test NLI\nmodels for event duration (how long an event lasts)\nand event ordering (how events are temporally ar-\nranged). However, there are some key differences:\n• Our focus is to investigate the temporal proper-\nties of ordering and duration for explicit time\nexpressions, and not for events in a sentence.\n• We analyze whether language models can rea-\nson about more ﬁne-grained duration (e.g.,\nwhether an event takes exactly 5 hours) where\nas they analyze reasoning about more coarse-\ngrained duration (e.g., whether an event takes\nplace in the order of hours or days).\n• We also investigate whether language mod-\nels can ﬁgure out commonplace conversions\namong adjacent units of time.\n• We introduce numerous variations in our data\ncreation process about how the time expres-\nsions are inserted and draw conclusions from\nhow these variations affect performance.\nList Type List Range\nhour (12 hr) 12 AM, ..., 11 PM\nhour (24 hr) 00:00, 01:00, ..., 23:00\nweekday Sunday, ..., Saturday\nmonth-day 1st, 2nd, ..., 28th\nmonth (full name) January, ..., December\nmonth (abbreviated) Jan, Feb, ..., Dec\nyear 1900, 1901, ..., 2000\nTable 1: Different lists of temporal expressions\n3 Dataset Creation\nWe construct three NLI datasets that aim to test\ndifferent relations between temporal expressions.\nThe datasets use templates from a manually cu-\nrated list of 71 events, labeled with their temporal\noccurrence (when the event is likely to occur) and\ntemporal duration (how long the event is expected\nto last) values. For instance:\nTemplate: I went to Paris\nOccurrence: day, month, year\n• I went to Paris on Monday.\n• I went to Paris in March.\n• I went to Paris in 2010.\nDuration: hours, days\n• I visited Paris from 10 AM to 9 PM.\n• I visited Paris from Mon to Wed.\nEach temporal unit corresponds to some list(s)\nspanning different magnitudes of time (Table 1),\nwhich are used during NLI pair creation.\n3.1 Set I: Temp-Order\nWe create this NLI challenge set to test whether\nlanguage models recognize the relationship of or-\ndering between two distinct temporal expressions.\nWe frame this in the NLI format by having the\npremise mention an event occur at a particular time\ninstance, while the hypothesis mentions the same\nevent but occurring at a different time instance:\nPremise : They got married in March.\nHypothesis : They got married before July.\nLabel : Entailment\nWe start constructing a basic NLI pair by choos-\ning a sentence template from the list of events.\nBased on the temporal occurrence label of the\nevent, one of the lists from Table 1 is chosen, and\ntwo time instances are sampled from that list with\n398\nPremise Hypothesis Label\na) He left his job at 12 PM. He left his job before 5 PM. E\nb) At 12 PM, he left his job. Before 5 PM, he left his job. E\nc) He will leave his job at 12 PM. He will leave his job before 5 PM. E\nd) He left his job after 12 PM. He left his job after 9 AM. E\ne) He left his job after 12 PM. He left his job before 5 PM. N\nf) He left his job after 12 PM. He left his job before 9 AM. C\ng) He left his job at 12 PM. He left his job before 17:00. E\nh) He left his job in February. He left his job after Apr. C\ni) He left his job in October 2011. He left his job after Jan 2011. E\nj) He left his job on 21st Sep 2013. He left his job before 23rd Sep 2012. C\nTable 2: Variations in NLI pairs for ordering of temporal expressions (E →entailment, C →contradiction, N →\nneutral). a) is the basic construction; b), c) is with the variation in event template; d), e), f) are when premise uses\na relative preposition to allow the event to happen in a time interval;g), h) are examples of choosing time instances\nfrom two different lists; i), j) are generation of more speciﬁc dates using months and month-days with years.\nreplacement. For the premise, the ﬁrst time in-\nstance is attached so that the event happens pre-\ncisely at this time instance. For the hypothesis, we\nrandomly choose a relative ordering between ‘be-\nfore’and ‘after’and attach it to the template event\nand the second instance. Since the premise claims\nthat the event occurs at an exact point in time while\nthe hypothesis claims that the event happens in a\nspeciﬁc time interval, the premise time instance\neither lies inside the hypothesis time interval or\nit does not, generating the labels of entailment or\ncontradiction correspondingly.\nDuring label generation, we have assumed that\nboth time instances lie in the same cycle (e.g., two\nweekdays lie in the same week). However, for cases\nwhere the two time instances are close across con-\nsecutive cycles, the automated label generated this\nway might be considered conventionally wrong:\nPremise: The concert starts at 2 AM.\nHypothesis: The concert starts before 11 PM.\nLabel: Entailment\nTo reduce the number of such edge cases in the\ndataset, we do not allow sampling of time instances\nthat are more than half the length of the list far\napart (e.g., for within a day, the distance between\ntwo hours will be at most 12).\nWe also introduce some variations in the sen-\ntence generation process to analyze the sensitivity\nof the models. Firstly, we tweak the event template\nby changing its position in the sentence (Table 2\nb) and by switching it to future tense (Table 2 c).\nSecondly, we allow the premise event to also oc-\ncur over an interval of time rather than a point in\ntime (Table 2 d, e, f ). To generate labels for these\ncases, the criteria we follow is that the pair is anen-\ntailment if the premise time interval is completely\nincluded in the hypothesis time interval (temporal\ninclusion), a contradiction if there is no overlap be-\ntween the two (temporal precedence), and neutral\notherwise. Moreover, we allow the premise and\nhypothesis to sample points in time from different\nlists when possible (Table 2 g, h). We also gener-\nate more speciﬁc dates by combining months and\nmonth-days with years (Table 2 i, j) to see if the\nlanguage models are still able to reason about the\ndifference in their ordering.\nWe construct separate train and test datasets, us-\ning 53 templates for the train split and 18 templates\nfor the test split. We have 11 different ways of\nchoosing the two time instances: seven ways of\nchoosing both from the same list (Table 1) and four\nways of choosing from different lists (Table 2 g-j).\nWe choose the two time instances for each template\nbased on its temporal occurrence label and run it\nfor ﬁve iterations, which results in a train dataset\nof 16,980 instances and test dataset of 6,140 in-\nstances, with the distribution of labels being: 40%\ncontradiction, 35% entailment, 25% neutral.\n3.2 Set II: Temp-Duration\nThe motivation behind this dataset is to test whether\nlanguage models can reason about ﬁne-grained tem-\nporal durations. We frame this in an NLI format\nby having the premise mention an event occurring\nbetween two points in time, while the hypothe-\nsis mentions the same event having occurred for a\ngiven duration:\nPremise : The war lasted from 1939 to 1945.\n399\nPremise Hypothesis Label\na) The meeting lasted from 12 PM to 5 PM. The meeting lasted for 5 hours. E\nb) The meeting lasted from 12 PM to 5 PM. The meeting lasted for 50 hours. C\nc) The meeting lasted from 12 PM to 5 PM. The meeting lasted for less than 5 hours. C\nd) The meeting lasted from 12 PM to 5 PM. The meeting lasted for less than 6 hours. E\ne) The meeting began at 12 PM and lasted\nuntil 5 PM.\nThe meeting lasted for 5 hours. E\nf) The meeting lasted from 9 PM to 3 AM. The meeting lasted for 6 hours. E\ng) The meeting lasted from 12 PM to 17:00. The meeting lasted for 5 hours. E\nh) The spring quarter lasts from Mar to June. The spring quarter lasts for 3 months. E\ni) The war lasted from July 1914 to Nov\n1918.\nThe war lasted for 4 years 4 months. E\nj) The war lasted from July 1914 to Nov\n1918.\nThe war lasted for 52 months. E\nTable 3: Variations in NLI pairs for duration calculation (E →entailment, C →contradiction). a) - d) are a few\nexamples from the 6 basic pairs; e) is with a changed premise structure; f) is when the hypothesis time instance\ncrosses over to the next cycle; g), h) are examples of choosing time instances from two different lists; i), j) are\ngeneration of speciﬁc dates using months and years in two different formats.\nHypothesis : The war lasted for 6 years.\nLabel : Entailment\nWe begin forming a basic NLI pair by choosing\na sentence template. Based on the event’stemporal\nduration label, a list from Table 1 is selected, and\ntwo time instances are randomly sampled without\nreplacement. The smaller instance is mentioned in\nthe premise as the event start time and the other in-\nstance as the event end time. We construct multiple\nhypotheses for the same premise. First, we calcu-\nlate the gold duration (GOLD) by ﬁnding the dif-\nference between the two instances, assuming both\nthe instances are part of the same cycle. Then, the\nhypothesis mentions the event to have occurred in\ntwo different settings (equal to, less than) for three\ndifferent durations (GOLD, GOLD+1, GOLD*10),\ngenerating a total of six hypotheses (Table 3 a-d\nare a few examples). We do this to test whether\nthe NLI models can reason for the claimed du-\nration’s validity only when they are very distant\n(GOLD*10) or also very close (GOLD+1) to the\ngold duration. Generation of true labels for the\npairs is automated, producing an entailment or con-\ntradiction depending on whether the gold duration\nfalls in the duration range speciﬁed by the hypothe-\nsis.\nWe again introduce two variations in the dataset\ncreation process. First, we change the wording of\nthe premise sentence (Table 3 e). Secondly, while\nsampling the time instances, we force the ending\ninstance to be picked such that it falls before the\nstarting instance in the list, which implies that the\nevent crossed over to the next cycle. In such cases,\nthe calculation of the gold duration is slightly dif-\nferent (Table 3 f ). We perform this next cycle\ncalculation for all lists in Table 1 except month-\ndays (because gold calculation without specifying\nthe exact month is ambiguous) and years (because\nthe list is acyclic). We also allow a similar blend\nof temporal expressions, like in Temp-Order set,\ncombining the two hours (Table 3 g) and months\n(Table 3 h) lists. We construct speciﬁc dates by\nincluding months and years and allow the duration\nto be mentioned in a year-month format (Table 3 i)\nor a months-only format (Table 3 j).\nWe create separate train and test datasets using\nthe same split of 53 and 18 templates as before.\nFor each template, time instances are chosen based\non their temporal duration label, along with the\nvariations as mentioned above applied (Table 3e-j).\nRunning each template for ﬁve iterations produces\na train dataset of 13,500 instances and test dataset\nof 3,540 instances, with the label distribution: 50%\nentailment and 50% contradiction.\n3.3 Set III: Cross-Unit Duration\nThe motivation behind the creation of the Cross-\nUnit Duration set is to test whether language mod-\nels understand the conversion relationship between\nmagnitudes speciﬁed in different units of time; for\ninstance, if models are able to interpret that 5 hours\nare less than 350 minutes but more than 250. More-\nover, we investigate if these models are better at\n400\nPremise Hypothesis Label\na) The store will close in 2 hours. The store will close before 40 minutes. C\nb) In 2 hours, the store will close. The store will close after 84 minutes. E\nc) The store will close in 2 days. After 34 hours, the store will close. E\nd) After 4 days, the store will close. The store will close before 38 hours. C\ne) The store will close before 4 days. Before 174 hours, the store will close. E\nf) The store will close before 6 hours. The store will close after 77 minutes. N\ng) After 3 hours, the store will close. The store will close after 409 minutes. N\nTable 4: Variations in NLI pairs for cross-unit duration comparison (E →entailment, C →contradiction, N →\nneutral). a) is the basic pair; b), c) are variations of basic pair with template position changed;d) - g) are variations\nin which the premise event occurs over a range of time.\ncertain kinds of conversions. We frame this task\nin an NLI format in a similar manner to the Temp-\nOrder set. In the premise, we mention a future\nevent that will occur after a given duration (T1),\nwhile in the hypothesis we mention the same future\nevent to occur before or after a different duration\n(T2). Apart from varying magnitudes, T1 and T2\nare also speciﬁed in different but adjacent units\nof time. More speciﬁcally, T1 is speciﬁed in the\nhigher adjacent unit of time, i.e., if T2 is speci-\nﬁed in minutes, then T1 will be speciﬁed in hours.\nSince the premise mentions an event occurring at a\nfuture point in time while the hypothesis mention\nan event occurring over a time interval bounded on\njust one side, the premise event either lies in the\ninterval or not, leading to the labels entailment and\ncontradiction respectively. We tried multiple varia-\ntions similar to Temp-Order set, like changing the\nposition of the template in the premise/hypothesis\n(Table 4 b, c) and making the event in the premise\nalso occur over a future interval (Table 4 d-g). The\nlabeling procedure of the second variation is again\nsimilar to Temp-Order set.\nTo create the challenge set, we ﬁrst pick a tem-\nplate and look at the list of its temporal occurrence\nvalues. We then iterate over all adjacent values in\nthis list, e.g., seconds-minutes, hours-days, months-\nyears. For each pair, we iterate over a manually\ncreated list of magnitudes for the higher unit of\ntime (T1) for the premise. We then pick a magni-\ntude in the smaller unit of time (T2) which is either\nhigher or lower than T1. T2’s value is generated\nrandomly, but a difference range parameter con-\ntrols its absolute difference with T1’s value. For\neach ﬁxed template, ﬁxed duration unit pair, and\nﬁxed magnitude of T1, we generate twelve differ-\nent premise-hypothesis pairs, four in which the\npremise occurs at a future point in time, and eight\nin which it occurs over a future interval of time.\nUsing a difference range parameter of 5, we cre-\nate a training set of 42,240 rows and a test set of\n15,840 rows. Due to the challenge set creation pro-\ncedure, the resultant dataset is naturally balanced\nwith the same number of samples for each of the\nthree labels.\n4 Experimental Setup\nWe evaluate three different NLI models on each\nof our challenge sets. The ﬁrst model is a pre-\ntrained RoBERTa-large model ﬁne-tuned on the\nMNLI corpus, which reports 90.8% accuracy on\nthe MNLI-matched task. The second model is Mi-\ncrosoft’s DeBERTa-large model ﬁne-tuned on the\nMNLI corpus, which reports 91.9% accuracy on\nthe MNLI-matched task. Both these models are\ntrained on all three labels: entailment, contradic-\ntion, and neutral.\nThe third model comes from Vashishtha et al.\n(2020), which is a RoBERTa-large model ﬁne-\ntuned on their temporal NLI datasets. For Temp-\nOrder set, we evaluate their model trained on the\nUDS-NLI (order) corpus as it explored ordering re-\nlations between events, and we wanted to analyze\nif any of that knowledge transfers over for deter-\nmining the ordering between temporal expressions.\nSimilarly, for Temp-Duration and Cross-Unit Du-\nration sets, we evaluate their model trained on the\nUDS-NLI (duration) corpus, which explored more\ncoarse-grained duration of events. In contrast to\nthe models ﬁne-tuned on MNLI, these models are\nonly trained on binary classiﬁcation - producing\n‘entailed’for the entailment label and ‘not-entailed’\nfor the contradiction and neutral labels. We have a\nseparate majority baseline corresponding to these\nmodels.\nWe report performances of all datasets under\n401\nModel Method Accuracy F1 Score\nMajority Ternary Classiﬁcation 40.29 23.14\nBinary Classiﬁcation 65.19 51.46\nRoBERTa (MNLI)\nDirect Evaluation 52.75 45.36\nHypothesis Only Training 40.29 ±0 23.14 ±0\nTrain and Evaluate 99.81 ±0.03 99.81 ±0.03\nDeBERTa (MNLI)\nDirect Evaluation 51.57 44.29\nHypothesis Only Training 40.29 ±0 23.14 ±0\nTrain and Evaluate 99.76 ±0.04 99.76 ±0.04\nUDS-NLI (order) Direct Evaluation 56.36 57.20\nTable 5: Evaluating Temp-Order set on NLI models\nthree different settings:\n1. Direct Evaluation: Evaluating the pre-\ntrained NLI models directly on the test splits\nof our challenge sets.\n2. Train and Evaluate: Fine-tuning the NLI\nmodels with the train splits and reporting per-\nformances on the test splits. We report this\nto recognize the complexity of the synthetic\ndatasets, and the ceiling performances that\nvarious NLI models can achieve on them.\n3. Hypothesis Only Training: Fine-tuning the\nNLI models in a hypothesis-only setting (Po-\nliak et al., 2018) with the train splits, and re-\nporting performances on the test splits. We\nreport this as a control for the results achieved\nin the Train and Evaluate setting.\nWe do not train the models ﬁne-tuned on UDS-\nNLI corpora, and only report their performance\nunder the Direct Evaluation setting, as the archi-\ntecture of those models is similar to that of the pre-\ntrained RoBERTa-MNLI model and we hypothe-\nsize that this may lead to similar results on training.\nMore details on the training process are mentioned\nin Appendix A.\n5 Results & Discussions\nWe present the results of all three challenge sets\nseparately.\n5.1 Set I: Temp-Order\nResults of Temp-Order set are summarised in Ta-\nble 5. When evaluating on the RoBERTa and De-\nBERTa models, there is an improvement of about\n10% over the majority baseline. On analyzing the\neffect of different variations mentioned in Table\n2, we ﬁnd that changing the template position or\nits tense does not produce any signiﬁcant differ-\nence in performance. However, we ﬁnd that pairs\nwhere the premise event occurred at a ﬁxed time\ninstance (2 a) have an average of 75% accuracy,\nwhile the pairs where the premise event occurred\nover a time interval (2 d-f ) have an average accu-\nracy of 28%. This implies that models trained on\nMNLI have some basic understanding of temporal\nordering, especially in determining whether a ﬁxed\ntime instance is present in another time interval.\nHowever, it gets difﬁcult to reason about the order-\ning between two time intervals, where discerning\nthe label is also not as straightforward.\nWe further analyze the accuracies for different\nmethods of choosing the two time instances, and\nthe results for DeBERTa are summed up in Figure\n1. For pairs where the premise event takes place at\na ﬁxed point in time, most methods in which both\ntime instances were sampled from the same list give\nover 73% accuracy. Among the methods in which\ntime instances are sampled across multiple lists,\ndates in which months are combined with years\ngive an average accuracy of 74%, but this drops\nto 57% when month-days are added, signifying\nthat the comparison of speciﬁc dates becomes too\ncomplicated for the NLI model. The model also\nhas a hard time mapping hours between 12 and 24-\nhour format, giving only 59% accuracy. For pairs\nwhere the premise takes place over a time interval,\nthe accuracies of all methods are below 35%.\nWe also evaluate the RoBERTa model trained\non UDS-NLI (order) corpus on our challenge set.\nHowever, the average performance was only 56%,\neven below the majority baseline for the corpus,\nnot indicating any signiﬁcant transfer of knowledge\nfrom their task of event-based temporal ordering.\nThe hypothesis-only baseline for both the\nRoBERTa/DeBERTa models is exactly the major-\n402\nFigure 1: Comparison of accuracies across different ways of choosing temporal expressions when running Temp-\nOrder set on DeBERTa ﬁne-tuned on MNLI. Label ‘DATE DMY’ implies generating dates using all month-day,\nmonth and year (Table 2j), while ‘DATE MY’ uses onlymonth and year (Table 2i). The added descriptions signify\nwhere the month instances are drawn from, where ‘month (full)’ implies that both months come from the month\n(full name) list, ‘month (abv)’ implies that they come from month (abbreviated) list, and ‘month (both)’ implies\nthat one month instance comes from each of the two lists. Labels ‘MONTH (both)’ and ‘HOUR (both)’ signify\nsimilar methods of choosing from multiple lists (Table 2 h and g correspondingly).\nity baseline, which is not surprising as the true\nlabel cannot be determined without knowing the\npremise time instance. Further ﬁne-tuning the pre-\ntrained MNLI models on our Temp-Order set leads\nto an almost perfect accuracy of 99%. This is de-\nspite having separate templates for the train and test\nsplit, and having time instances randomly sampled\nfrom different lists. Since our method of gener-\nating labels was automated and depended on the\nvalues of the time instances, we infer that the nu-\nmerous parameters of a large language model were\nable to learn this label generation process from the\nartiﬁcial NLI data.\n5.2 Set II: Temp-Duration\nResults for Temp-Duration set are summarized in\nTable 6. Both the RoBERTa and DeBERTa models\nﬁne-tuned on MNLI produce poor accuracies of\naround 55%, just over the majority baseline. While\nanalyzing the DeBERTa predictions, we found that\nthe model produced entailment for 83% of the data\npoints, implying that it is not able to adequately\ndetermine durations. The different variations (Ta-\nble 3) or methods of sampling time instances also\ndid not have any signiﬁcant effect. We investigated\nthe performances of the six types of hypotheses\nand found that among the hypothesis types with\nthe contradiction gold label, ‘equal to GOLD*10’\nproduced 0.68 F1-score, compared to ‘equal to\nGOLD+1’and ‘less than GOLD’, which produced\n0.15 and 0.07 F1-scores respectively. This might\nindicate that while the NLI model has difﬁculty\nﬁguring out when the claimed duration is incor-\nrect, it still does better off when it is very distant\n(GOLD*10) from the gold duration compared to\nwhen it is very close (GOLD+1). We also ﬁnd\nthat for the ‘equal to GOLD’hypothesis, pairs of\ninstances far apart in a list tend to be misclassi-\nﬁed more than pairs of instances that are closer\nto each other. This implies that determination of\nexact duration gets difﬁcult for the NLI model as\nthe distance between instances increases.\nWe also evaluate the model trained on UDS-NLI\n(duration) corpus on our set, and the results are\nslightly above the majority baseline. While the\npredictions by this model were not as skewed, we\ncould not ﬁnd any signiﬁcant impact of the vari-\nations or the different methods of sampling time\ninstances, not indicating any possible knowledge\ntransfer from their problem of determining coarse-\ngrained event duration.\nOn ﬁne-tuning our challenge set under the\nhypothesis-only setting, both MNLI models sur-\nprisingly produce at least 15% gains in accuracy\nover the majority baseline. We investigate and ﬁnd\nthat the models use lexical cues from the different\nhypothesis types, producing entailment for all ‘less\nthan’hypotheses. For the ‘equal’hypotheses, they\npredict contradiction when the claimed duration\nis a large value (more likely to be GOLD*10) and\nentailment when it is smaller (more likely to be\nGOLD). However, under the standard NLI training\n403\nModel Method Accuracy F1 Score\nMajority1 Binary Classiﬁcation 50.00 33.33\nRoBERTa (MNLI)\nDirect Evaluation 54.32 46.64\nHypothesis Only Training 68.82 ±0.22 66.84 ±0.51\nTrain and Evaluate 91.86 ±6.42 91.85 ±6.42\nDeBERTa (MNLI)\nDirect Evaluation 56.67 51.53\nHypothesis Only Training 64.15 ±0.27 59.64 ±0.41\nTrain and Evaluate 73.72 ±3.92 73.28 ±4.50\nUDS-NLI (duration) Direct Evaluation 58.44 57.66\nTable 6: Evaluating Temp-Duration set on NLI models\nModel Method Accuracy F1 Score\nMajority Ternary Classiﬁcation 33.33 16.67\nBinary Classiﬁcation 66.67 53.33\nRoBERTa (MNLI)\nDirect Evaluation 35.47 28.71\nHypothesis Only Training 49.38 ±0.71 39.51 ±0.52\nTrain and Evaluate 99.97 ±0.02 99.97 ±0.02\nDeBERTa (MNLI)\nDirect Evaluation 45.02 38.60\nHypothesis Only Training 49.58 ±0.79 41.29 ±0.56\nTrain and Evaluate 99.94 ±0.03 99.94 ±0.03\nUDS-NLI (duration) Direct Evaluation 52.61 53.54\nTable 7: Evaluating Cross-Unit Duration set on NLI models\nscenario, these cues are not the only factor behind\nlearning, as the RoBERTa MNLI model produces\n91.86% average accuracy, which is a gain of 20%\nover the hypothesis-only setting. Among the var-\nious methods of sampling time instances, years\nperforms the worst, producing only 66% accuracy,\npossibly because the lengths of duration can be as\nlarge as 100 years. Finally, the hypothesis types\n‘equal to GOLD*10’and ‘less than GOLD*10’pro-\nduce 99% accuracy, while ‘equal to GOLD’and\n‘less than GOLD’ report below 90%, conﬁrming\nour speculation that it is easier for the models to\nreason about the validity of the claimed duration\nwhen it is distant from the gold duration.\n5.3 Set III: Cross-Unit Duration Set\nAs shown in Table 7, all the models produce\npoor performances on direct evaluation, just near\nthe majority baseline. DeBERTa ﬁne-tuned on\nMNLI manages to perform better when compared\nto RoBERTa ﬁne-tuned on MNLI by around 10%\non overall accuracy. Hence, we can conclude that\nDeBERTa has a slightly better understanding of\ncross-unit duration comparison when compared to\nRoBERTa.\n1Same majority baseline because no neutral labels.\nSimilar to Temp-Orderset, all models performed\nbetter compared to their respective majority base-\nlines when the premise event occurs at a future\npoint in time rather than over a time interval. More\nspeciﬁcally, we see an improvement in accuracy\nof around 18% (29.65 to 47.12) for RoBERTa and\naround 10% (41.77 to 51.52) for DeBERTa when\nwe switch from the premise occurring over a time\ninterval to a point in time.\nWe analyzed the results on adjacent units to rec-\nognize if there are speciﬁc pairs for which the mod-\nels are better able to ﬁgure out the conversion re-\nlationship. We did not ﬁnd any signiﬁcant pair\nfor RoBERTa or DeBERTa models, but we ﬁnd\nthat the UDS-NLI (duration) model does better on\nbigger unit pairs of duration, i.e., it performs the\nbest on conversion between month-years (56.03%\nF1), then day-months (55.32% F1), then hours-\ndays (51.02% F1), and then minutes-hours (16.74%\nF1). This suggests a better transfer of knowledge\nfor bigger time units from the UDS-NLI (duration)\nmodel to our challenge set.\nSimilar to Temp-Duration set, on ﬁne-tuning un-\nder the hypothesis-only setting, both MNLI models\nproduce around 16-17% gains in accuracy over the\nmajority baseline using lexical cues present in the\n404\nhypotheses due to the challenge set creation pro-\ncess. For the hypotheses that contain ‘before’, the\nmodels tend to predict entailment if the duration\n(T2) is large, and contradiction if it is small. Sim-\nilarly, for the hypotheses that contain ‘after’, the\nmodels mostly predict contradiction if the dura-\ntion is large, and entailment if it is small. How-\never, on standard training, the accuracy goes up\nfrom around 50% to near perfect 99%, showing\nthat these cues are not the only reason behind the\nmodel’s performance and that it actually learns the\nrelationship between the premise and hypothesis.\nWe believe a valuable addition to this challenge\nset would be introducing more varied phrasing of\nprepositions. That is, using synonymous ways of\ndenoting a temporal event occurring before, after,\nor strictly at a point in time. In particular, phrasing\nlike ‘after the next 60 minutes’ or ‘sometime after\n60 minutes pass’ could be examples of more spe-\nciﬁc ways to represent that an event occurred ‘after\n60 minutes’ - a phrase which we acknowledge may\nread to mean ‘in exactly 60 minutes’, as opposed\nto some time after.\n6 Conclusion\nWe create three challenge sets that test different\nkinds of relationships between temporal expres-\nsions. We evaluate these challenge sets on popular\nNLI models like RoBERTa and DeBERTa trained\non MNLI, and ﬁnd that while they can reason about\nsimple cases of ordering between time instances,\nthey fail when presented with more complicated\ncases or when temporal reasoning requires deter-\nmining ﬁne-grained duration. Since our challenge\nsets were synthetically created, training on them\nhelped the NLI models to ﬁgure out the label gen-\neration process, and they produced near-perfect\naccuracy for the Temp-Order and the Cross-Unit\nDuration sets. A direction for future research could\nbe evaluating and comparing models, trained on\nother NLI datasets containing temporal expressions,\non our challenge sets. Another direction could be\nto collect naturally occurring sentences that con-\ntain temporal expressions from large corpora and\nrecast them into NLI format for similar testing of\nunderstanding of temporal expressions.\nReferences\nSteven Bethard, Leon Derczynski, Guergana Savova,\nJames Pustejovsky, and Marc Verhagen. 2015.\nSemEval-2015 task 6: Clinical TempEval. In Pro-\nceedings of the 9th International Workshop on Se-\nmantic Evaluation (SemEval 2015), pages 806–814,\nDenver, Colorado. Association for Computational\nLinguistics.\nTaylor Cassidy, Bill McDowell, Nathanael Chambers,\nand Steven Bethard. 2014. An annotation frame-\nwork for dense event ordering. InProceedings of the\n52nd Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 501–506, Baltimore, Maryland. Association\nfor Computational Linguistics.\nSanxing Chen, Guoxin Wang, and Börje Karlsson.\n2019. Exploring word representations on time ex-\npression recognition. Technical report, Microsoft\nResearch Asia.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWentao Ding, Guanji Gao, Linfeng Shi, and Yuzhong\nQu. 2019. A pattern-based approach to recognizing\ntime expressions. Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, 33(01):6335–6342.\nQuang Do, Wei Lu, and Dan Roth. 2012. Joint infer-\nence for event timeline construction. In Proceedings\nof the 2012 Joint Conference on Empirical Methods\nin Natural Language Processing and Computational\nNatural Language Learning, pages 677–687, Jeju Is-\nland, Korea. Association for Computational Linguis-\ntics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention.\nKenton Lee, Yoav Artzi, Jesse Dodge, and Luke Zettle-\nmoyer. 2014. Context-dependent semantic parsing\nfor time expressions. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1437–\n1447, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nArtuur Leeuwenberg and Marie-Francine Moens. 2018.\nTemporal information extraction by predicting rel-\native time-lines. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1237–1246, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\n405\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAakanksha Naik, Luke Breitfeller, and Carolyn Rose.\n2019. TDDiscourse: A dataset for discourse-level\ntemporal ordering of events. In Proceedings of the\n20th Annual SIGdial Meeting on Discourse and Dia-\nlogue, pages 239–249, Stockholm, Sweden. Associ-\nation for Computational Linguistics.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. Torque: A reading\ncomprehension dataset of temporal ordering ques-\ntions.\nQiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, and\nDan Roth. 2018. CogCompTime: A tool for under-\nstanding time in natural language. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions, pages 72–77, Brussels, Belgium. Association\nfor Computational Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language in-\nference. In Proceedings of the Seventh Joint Con-\nference on Lexical and Computational Semantics ,\npages 180–191, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nJames Pustejovsky, José M Castano, Robert Ingria,\nRoser Sauri, Robert J Gaizauskas, Andrea Set-\nzer, Graham Katz, and Dragomir R Radev. 2003a.\nTimeml: Robust speciﬁcation of event and temporal\nexpressions in text. New directions in question an-\nswering, 3:28–34.\nJames Pustejovsky, Patrick Hanks, Roser Saurí,\nAndrew See, Rob Gaizauskas, Andrea Setzer,\nDragomir Radev, Beth Sundheim, David Day, Lisa\nFerro, and Marcia Lazo. 2003b. The timebank cor-\npus. Proceedings of Corpus Linguistics, page 40.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nJannik Strötgen and Michael Gertz. 2010. HeidelTime:\nHigh quality rule-based extraction and normaliza-\ntion of temporal expressions. In Proceedings of the\n5th International Workshop on Semantic Evaluation,\npages 321–324, Uppsala, Sweden. Association for\nComputational Linguistics.\nNaushad UzZaman, Hector Llorens, Leon Derczyn-\nski, James Allen, Marc Verhagen, and James Puste-\njovsky. 2013. SemEval-2013 task 1: TempEval-3:\nEvaluating time expressions, events, and temporal\nrelations. In Second Joint Conference on Lexical\nand Computational Semantics (*SEM), Volume 2:\nProceedings of the Seventh International Workshop\non Semantic Evaluation (SemEval 2013) , pages 1–\n9, Atlanta, Georgia, USA. Association for Computa-\ntional Linguistics.\nSiddharth Vashishtha, Adam Poliak, Yash Kumar Lal,\nBenjamin Van Durme, and Aaron Steven White.\n2020. Temporal reasoning in natural language infer-\nence. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4070–4078,\nOnline. Association for Computational Linguistics.\nSiddharth Vashishtha, Benjamin Van Durme, and\nAaron Steven White. 2019. Fine-grained temporal\nrelation extraction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2906–2919, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMarc Verhagen, Robert Gaizauskas, Frank Schilder,\nMark Hepple, Graham Katz, and James Pustejovsky.\n2007. SemEval-2007 task 15: TempEval tempo-\nral relation identiﬁcation. In Proceedings of the\nFourth International Workshop on Semantic Evalua-\ntions (SemEval-2007), pages 75–80, Prague, Czech\nRepublic. Association for Computational Linguis-\ntics.\nMarc Verhagen, Roser Saurí, Tommaso Caselli, and\nJames Pustejovsky. 2010. SemEval-2010 task 13:\nTempEval-2. In Proceedings of the 5th Interna-\ntional Workshop on Semantic Evaluation, pages 57–\n62, Uppsala, Sweden. Association for Computa-\ntional Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5307–\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing.\n406\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan\nRoth. 2019. “going on a vacation” takes longer\nthan “going for a walk”: A study of temporal com-\nmonsense understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3363–3369, Hong Kong,\nChina. Association for Computational Linguistics.\nA Training Details\nWe use three different NLI models for\nour experiments. For the RoBERTa and\nDeBERTa models ﬁne-tuned on MNLI,\nwe use the roberta-large-mnli and\nmicrosoft/deberta-large-mnli models\nrespectively, available under thetransformers\nlibrary from HuggingFace (Wolf et al., 2020). For\nthe UDS-NLI models, we directly use the saved\nRoBERTa-large models for UDS-NLI (duration)\nand UDS-NLI (order) made publicly available by\nVashishtha et al. (2020).\nFor training, we use an Adam optimizer, with\na learning rate of 2 ∗10−5 and 0.1 weight decay.\nWe use a batch size of 16 for training and 128 for\ntesting. We train for 10 epochs, using early stop-\nping with a patience of 2. We run each experiment\nfor three random seeds (3, 5, 7), and use them to\ncalculate the mean and standard deviation for the\naccuracy and F1 (weighted) score metrics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7130479216575623
    },
    {
      "name": "Relation (database)",
      "score": 0.6701891422271729
    },
    {
      "name": "Duration (music)",
      "score": 0.5516914129257202
    },
    {
      "name": "Inference",
      "score": 0.5171827077865601
    },
    {
      "name": "Perception",
      "score": 0.5108723044395447
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48179858922958374
    },
    {
      "name": "Natural language processing",
      "score": 0.4700806438922882
    },
    {
      "name": "Temporal database",
      "score": 0.4459958076477051
    },
    {
      "name": "Natural language",
      "score": 0.4294513463973999
    },
    {
      "name": "Data mining",
      "score": 0.14577049016952515
    },
    {
      "name": "Psychology",
      "score": 0.09227645397186279
    },
    {
      "name": "Physics",
      "score": 0.07283273339271545
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ]
}