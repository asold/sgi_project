{
  "title": "When Language Model Meets Private Library",
  "url": "https://openalex.org/W4385574207",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4258956186",
      "name": "Daoguang Zan",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2096510293",
      "name": "Bei Chen",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2117288342",
      "name": "Zeqi Lin",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2166141094",
      "name": "Bei Guan",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Software",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A799087688",
      "name": "Wang Yongji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4212328323",
      "name": "Jian-Guang Lou",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4280629343",
    "https://openalex.org/W4206251287",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4306153253",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4309986599",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W4285149002",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W4285483825",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W4308538981",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W2912223780"
  ],
  "abstract": "With the rapid development of pre-training techniques, a number of language models have been pre-trained on large-scale code corpora and perform well in code generation. In this paper, we investigate how to equip pre-trained language models with the ability of code generation for private libraries. In practice, it is common for programmers to write code using private libraries. However, this is a challenge for language models since they have never seen private APIs during training. Motivated by the fact that private libraries usually come with elaborate API documentation, we propose a novel framework with two modules: the APIRetriever finds useful APIs, and then the APICoder generates code using these APIs. For APIRetriever, we present a dense retrieval system and also design a friendly interaction to involve uses. For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information. Both modules are trained with data from public libraries and can be generalized to private ones. Furthermore, we craft three benchmarks for private libraries, named TorchDataEval, MonkeyEval, and BeatNumEval. Experimental results demonstrate the impressive performance of our framework.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 277‚Äì288\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nWhen Language Model Meets Private Library\nDaoguang Zan1,2‚àó, Bei Chen3, Zeqi Lin3, Bei Guan2,4, Yongji Wang2,4,5, Jian-Guang Lou3\n1Cooperative Innovation Center, Institute of Software, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences; 3Microsoft Research Asia\n4Integrative Innovation Center, Institute of Software, Chinese Academy of Sciences\n5State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences\n{daoguang@, guanbei@, ywang@itechs.}iscas.ac.cn\n{beichen, zeqi.lin, jlou}@microsoft.com\nAbstract\nWith the rapid development of pre-training\ntechniques, a number of language models have\nbeen pre-trained on large-scale code corpora\nand perform well in code generation. In this\npaper, we investigate how to equip pre-trained\nlanguage models with the ability of code gener-\nation for private libraries. In practice, it is com-\nmon for programmers to write code using pri-\nvate libraries. However, this is a challenge for\nlanguage models since they have never seen pri-\nvate APIs during training. Motivated by the fact\nthat private libraries usually come with elabo-\nrate API documentation, we propose a novel\nframework with two modules: the APIRetriever\nfinds useful APIs, and then the APICoder gen-\nerates code using these APIs. For APIRetriever,\nwe present a dense retrieval system and also de-\nsign a friendly interaction to involve uses. For\nAPICoder, we can directly use off-the-shelf lan-\nguage models, or continually pre-train the base\nmodel on a code corpus containing API infor-\nmation. Both modules are trained with data\nfrom public libraries and can be generalized\nto private ones. Furthermore, we craft three\nbenchmarks for private libraries, named Torch-\nDataEval, MonkeyEval, and BeatNumEval. Ex-\nperimental results demonstrate the impressive\nperformance of our framework.1.\n1 Introduction\nCode generation, automatically generating code\nsnippets based on user descriptions, is one of the\nlong-standing challenges in the software engineer-\ning and artificial intelligence communities. With\nthe rapid development of pre-training techniques,\na number of language models are pre-trained on\nlarge-scale code corpora and able to generate de-\ncent code snippets, for example, Codex (Chen et al.,\n2021), AlphaCode (Li et al., 2022), CODE GEN (Ni-\njkamp et al., 2022), and InCoder (Fried et al., 2022).\n‚àóWork done as an intern at Microsoft Research Asia.\n1Our work is available at https://github.com/\nmicrosoft/PyCodeGPT/tree/main/apicoder.\nimportpandasaspddefselect_rows_from_column(df, col_name, values):# How do I select rows from a DataFramedfbased on column values?# Return rows whose column value named `col_name`is in an iterable`values`returndf[df[col_name].isin(values)]importmonkeyasmkdefselect_rows_from_column(kf, col_name, values):# How do I select rows from a KnowledgeFramekfbased on column values?# Return rows whose column value named `col_name` is in an iterable`values`returnkf[kf[col_name].incontain(values)]\nMonkeyEval\nPandasEval\nCodex12BCODEGEN-Mono350Mpass@kk=1 k=10k=100k=1k=10k=100PandasEval18.88%43.05%64.37%14.24%30.71%46.04%MonkeyEval1.47%3.53%7.31%0.95%4.90%8.89%\nFigure 1: A practical example of converting PandasEval\n(public) to MonkeyEval (private). The changed parts are\nhighlighted in yellow. The performance of Codex 12B\nand CODE GEN-MONO 350M is shown at the bottom.\nThey bring fresh energy to code generation and im-\nprove coding efficiency (Vaithilingam et al., 2022).\nAlthough making remarkable progress, these mod-\nels may be biased towards generating code that\nis similar to the training distribution (Chen et al.,\n2021). What if one wants to generate code beyond\nthe training distribution? A real-world scenario for\nprogrammers is to write code using a private library,\nwhich is very common in practice. For example,\nfor security and functionality reasons, companies\noften build private libraries for internal use only.\nPrivate libraries provide a number of private APIs\nthat have not been seen by the language models and\nare also not publicly available on any code hosting\nplatform like GitHub. Therefore, it is worth explor-\ning whether and how pre-trained language models\ncan generate code using private libraries.\nIt is challenging for existing language models to\ngenerate code that uses a private library directly. A\npractical evidence is shown in Figure 1. We built a\npseudo private library named Monkey based on a\n277\npublic one named Pandas. PandasEval (Zan et al.,\n2022) is a benchmark consisting of 101 Pandas\nprogramming problems. We convert all Pandas-\nrelated keywords in PandasEval into the new ver-\nsion and construct MonkeyEval (details in Section\n4). As seen in Figure 1, Codex12B and CODE GEN-\nMONO 350M show a significant drop in perfor-\nmance on the private MonkeyEval compared to\ntheir performance on the public PandasEval. For ex-\nample, Codex 12B drops from 18.88% to 1.47% on\npass@1, showing the inadequacy of the language\nmodels in code generation for private libraries.\nTo meet the challenge, we propose a framework\nto equip pre-trained language models with the abil-\nity to generate code that uses private libraries. As\nis known, private libraries usually come with elabo-\nrate API documentation, which motivates our main\nidea to mimic the process of a programmer learning\nto write code using private libraries. This process\nis also known as API practices in the software en-\ngineering field (Snodgrass and Winnie, 2019): first\nlearning the private API documentation and then\ninvoking the APIs to implement the needed func-\ntionalities. Analogically, there are two modules in\nour framework: an APIRetriever first retrieves the\nuseful APIs based on the programming problem\nand the API documentation, and then an APICoder\nuses these APIs to generate code. For APIRetriever,\nwe train a dense retriever and also design a friendly\ninteraction to involve users in the loop optionally.\nFor APICoder, we can directly use existing lan-\nguage models of code generation, such as CODE -\nGEN, to invoke the private APIs; furthermore, to\nbetter teach a language model how to invoke APIs,\nwe also continually pre-train the base model on a\ncode corpus containing API information from pub-\nlic libraries, and obtain our reinforced model called\nCODE GENAPI . Since we only have access to the\ndata of public libraries during training, we expect\nthat APIRetriever and APICoder can be general-\nized to private libraries via learning.\nTo evaluate the code generation for private li-\nbraries, we craft three benchmarks, named Torch-\nDataEval, MonkeyEval, and BeatNumEval. Torch-\nDataEval includes 50 programming problems using\nthe TorchData library. The last two are adapted\nfrom PandasEval and NumpyEval (Zan et al.,\n2022), respectively, each consisting of 101 pro-\ngramming problems. Extensive experiments on the\nthree benchmarks have revealed that our framework\neffectively improves the performance of pre-trained\nPrivate APIDocumentationProper APIsTarget CodeContextAPICoder\nAPIRetriever\nùê± ùê≤\nùíú\nFigure 2: The overview of our proposed framework.\nlanguage models on code generation for private li-\nbraries. We also provide a thorough analysis to\nfacilitate progress in this direction.\n2 Framework\nFirst, we would like to define the task of code gen-\neration formally. Given context, the task aims to\ngenerate target code. In Figure 1, context and tar-\nget code are shown in white and grey backgrounds,\nrespectively. Context consists of a comment, which\nis a natural language description of the program-\nming problem, and a code snippet including import\nstatements, function header, etc. Target code solves\nthe programming problem in context. We denote\nthe context by x. Code generation model Mout-\nputs target code y based on x. For the task of code\ngeneration for private library, the context x con-\ntains the instruction for using a private library, such\nas an import statement. The target code y contains\nthe calls of the corresponding private library APIs.\nAs mentioned in Section 1, private libraries are\nusually equipped with elaborate API documenta-\ntion. As a technical reference manual outlining\nhow to use the library, API documentation typi-\ncally includes a quick start guide, tutorials, and an\ninstruction for each API (e.g., API name, signature,\ndescription, parameters, and examples). To take ad-\nvantage of the API documentation, we propose to\nmimic the generic process of a programmer coding\nwith private APIs, and design a framework to gen-\nerate code that can invoke private APIs. The frame-\nwork consists of APIRetriever and APICoder with\nthe overview shown in Figure 2. Given the context,\nAPIRetriever MR aims to retrieve possible used\nAPIs from the API documentation; and APICoder\nMC is dedicated to generating code using the re-\ntrieved APIs. The process can be formalized as\nA= MR(x) and y = MC(A; x), where Arepre-\nsents the set of information of all proper APIs, and\neach a ‚ààA is the information of an API. In our\nimplementation, we design the API information to\ninclude the API name, signature and description.\nNote that we only use the first sentence of the API\n278\nContinual Pre-training\nA Code Block in Files using theLibrarynumpy\n# [start]# transpose(a, axes=None): Reverse or permute the axes of an array; returns the modified array.#squeeze(axis=None):Squeeze1dimensionalaxisobjectsintoscalars.# reshape(a, newshape, order='C'): Gives a new shape to an array without changing its data.# [end]importnumpyasnpdefchange_shape_and_trans(a):\"\"\"How can I use reshape to divide it into 4 chucks, such that it looks like this:I would like to reshape a to (2, 4, 2, 4) and then transpose it by (0, 2, 1, 3) to c\"\"\"c= a.reshape(2,4,2,4).transpose(0,2,1,3) \nPublic API DocumentationExtract APIs\n[1].reshape(a, newshape, order='C'): Gives a new shape to an array without changing its data.[2].transpose(a, axes=None): Reverse or permute the axes of an array; returns the modified array.\nGenerate API Info\nTheFormat ofOneAPIInfois:API Name(APISignature):APIDescription\nBase ModelCODEGENAPI\nConcatenate\nA New Code Block with API InformationSet\nCODEGEN-Mono\nHow can I use reshape to divide it into 4 chucks, such that it looks like this:I would like to reshape a to (2, 4, 2, 4) and then transpose it by (0, 2, 1, 3) to c\nNatural LanguageDescription\nAPIRetriever\nNL DespBERTTokenizer\nNL DespEmbeddingCross Entropy\nSelecteachAPIInfo\nShuffletheAPIsNoiseAPIsimportnumpyasnpdefchange_shape_and_trans(a):\"\"\"How can I use reshape to divide it into 4 chucks, such that it looks like this:I would like to reshape a to (2, 4, 2, 4) and then transpose it by (0, 2, 1, 3) to c\"\"\"c= a.reshape(2,4,2,4).transpose(0,2,1,3) API InfoBERTsimilarityscoreAPIInfoEmbedding\nTokenizer\nDenseRetriever\nFigure 3: The training process of APIRetriever and CODE GENAPI.\ndescription since it is sufficient to summarize.\n3 Methodology\nWe have introduced our framework that provides\npre-trained models a fantastic way to deal with\nprivate libraries. In this section, we present the\ndata collection, followed by the detailed design of\nour APIRetriever and APICoder.\n3.1 Data Collection\nWe collect API information and code files of pub-\nlic libraries due to the fact that we can only ac-\ncess data from public libraries. Then we train the\nmodels based on the public data with the expecta-\ntion that the model can be generalized to private\nlibraries. For API information, we consider the\n31 most popular public libraries in Python (e.g.,\nPandas, NumPy, and scikit-learn) according to the\npopularity ranking on StackOverFlow2. For each\nof the libraries, we crawled its API documentation\nand extracted detailed information about each API,\nincluding the API name, signature, description, pa-\nrameters, usage examples, and so on. Please refer\nto Appendix A for the details of the 31 public li-\nbraries. For code files, we first collected a 330GB\ncorpus from GitHub containing 60.6M python files\nand then extracted those files that involved one or\nmore API calls from the 31 public libraries. After\na bunch of pre-processing strategies, for example,\nde-duplicating, cleaning, and formatting, we finally\nobtained 4.54M python files, denoted by D.\n3.2 APIRetriever\nAPIRetriever aims to find the proper APIs based\non the description of a programming problem. We\n2https://stackoverflow.com/tags?tab=popular\nregard it as a dense retrieval task (Qu et al., 2020;\nXiong et al., 2020; Santhanam et al., 2021; For-\nmal et al., 2022) and design a simple dual-encoder\nmodel (Karpukhin et al., 2020) to retrieve the possi-\nble used APIs for each programming problem. To\nfurther boost the retrieval performance, a friendly\ninteraction approach is designed to involve users.\nTraining. To train APIRetriever, we need a large\namount of pairwise data, natural language de-\nscription and API information. We first segment\neach python file d ‚àà Dinto K code blocks\n(d1, d2, ¬∑¬∑¬∑, dK) using the pip-tools, i.e., redbaron,\nautopep8, and docformatter, where each code block\nis a relatively well-rounded code fragment, such as\na function or a class. For each code block di, we\nextract all API names and obtain the correspond-\ning API signatures and descriptions by searching\nour collected 31 API documentations 3. The in-\nformation of an API includes its name, signature\nand description, denoted by a ‚ààA. Each a and\nthe natural language description p extracted from\nthe same code block di are regarded as a positive\ntraining sample. For the negative training sample,\nwe randomly sample an API ÀÜa that is unrelated\nto di from the same library. In total, we obtained\n40.3M (p, a, ÀÜa1, ÀÜa2, . . . ) sets as training samples.\nAs in Figure 3, the left part shows the training\nprocess of APIRetriever. Our APIRetriever is a\ndual-encoder model. The two dense encoder, Ep(.)\nand Ea(.), map p and a to z-dimensional vectors,\nrespectively. Then, we use the dot product of their\nvectors to calculate the similarity score formalized\nas Ep(p)‚ä§Ea(a), where Ep(.) and Ea(.) are im-\nplemented by two independent BERT (Devlin et al.,\n3If an API name matches more than one candidate API,\nwe randomly pick one.\n279\n[1]: flatmap: Applies a function over each item from...[2]: cycle: Cycles the specified input in perpetuity by default, or for the specified number of times.[3]: mux:Yieldsoneelementatatimefromeachof...[4]: header: YieldselementsfromthesourceDataPipe...[5]: concat: ConcatenatesmultipleIterableDataPipes...[6]:Noneoftheabove.[7]:Notsure.\nfromtorchdata.datapipes.iterimportIterableWrapperdatapipe= IterableWrapper([1,2,3])# How to augumentthe datapipeby repeating it six times.new_datapipe=\nWhich APIs would you like to use?\nProgrammingProblem\nYour Choices:[2]\nChioces: ([choice]: API Name: API Description)\nFigure 4: Friendly interaction interface for users.\n2019) with base-uncased version. We use BERT\ninstead of CodeBERT (Feng et al., 2020) as most\ntokens in p and a are natural language rather than\nprogramming language.\nInference. After the training phase, we can use\nAPIRetriever to retrieve private APIs for each pro-\ngramming problem description. In detail, we apply\nEa to all the APIs and index them by FAISS (John-\nson et al., 2019) offline. Given a new programming\nproblem description p at run-time, we only need to\nproduce its embedding vp = Ep(p) and recall the\ntop-k APIs with the embeddings closest to vp.\nHuman Interaction with APIRetriever. In or-\nder to further increase the accuracy of API retrieval,\nwe provide a friendly interaction interface to allow\nhumans in the loop with APIRetriever, as shown in\nFigure 4. In the interaction interface, we give the\nprogramming problem and the top-5 APIs retrieved\nby APIRetriever, and let users choose one or more\nAPIs that may be used in the target code. Note\nthat we only provide API names and descriptions\nto users, as we find in our empirical experiments\nthat providing API signatures has a negative effect\non making the correct choice.\n3.3 APICoder\nAPIRetriever finds useful APIs for a program-\nming problem, and then APICoder aims to gen-\nerate code that solves the problem with these APIs.\nWe make use of the most straightforward way for\nAPICoder: prompting API information set Ain\nfront of the context x. Formally, the APICoder\ncan be written as y = MC(Concat(A, x)), where\nConcat(A, x) means to concatenate the API infor-\nmation set and the context. Examples can be found\nin Figure 3. Each API information is in the form\nof ‚Äúname(signature):description‚Äù. This is to\nmimic programmers learning the APIs properly\nbefore writing code using them.\nTechnically speaking, the off-the-shelf code gen-\neration models, such as CodeT5, CodeGPT, Code-\nClippy, CodeParrot, CODE GEN, and Codex, can\nbe applied directly to land APICoder. Although\nthese base models can achieve gains in correctly\ninvoking APIs, they have not learned how to use\nthem as an explicit training task. To better use the\nAPIs, we devised a fantastic idea of continually\npre-training the base models using code files with\nAPI information inserted.\nIn practice, we use CODE GEN-MONO 350M\n(Nijkamp et al., 2022) as our base model, based\non which we continually pre-train and obtain our\nreinforced model called CODE GENAPI . CODE -\nGEN is a GPT-based model skilled at generat-\ning code. We choose it because it is by far the\nmost popular and publicly available model. As for\nthe training corpus, we use the collected python\nfiles Dmentioned in Section 3.1. Firstly, as done\nfor APIRetriever, each file d ‚àà Dis split into\nK code blocks (d1, d2, ¬∑¬∑¬∑, dK). For each code\nblock di, we obtain the set of API information\nAi. Then, the K code blocks and sets of API in-\nformation are cross-merged to output a new file\nÀÜd = (A1, d1, A2, d2, ¬∑¬∑¬∑, AK, dK). This mimics\nthe process of API information as a prompt for\neach block. Then, we continually pre-train the base\nmodel on the new code files, teaching the model to\nwrite code based on the prompted APIs. In addi-\ntion, as shown in Figure 3, to make APICoder more\nrobust, we shuffle the APIs in each set Ai and also\nadd noise APIs, since APIRetriever does not know\nthe order of APIs in the target code and often finds\nincorrect APIs.\nDuring the training phase of CODE GENAPI, un-\nlike the previous settings that force all files to have\nthe same priority, we design a resampling strategy\nto enable high-quality python files to appear more\nfrequently and vice versa. The strategy considers\nthe star number of the repository, the unit test func-\ntion rate of the code file, and the API rate of the\ncode file. More details can be found in Appendix B.\n4 Benchmark Construction\nPrivate libraries are commonly used in practice, but\nfew attempts have been made to evaluate the per-\nformance of generating code invoking private APIs.\n280\nTo fill this gap, we craft three benchmarks, called\nTorchDataEval, MonkeyEval, and BeatNumEval.\nEach programming problem consists of context,\ntarget code, and the corresponding test cases.\nTo create a realistic benchmark for evaluating\ncode generation for private library, we use Torch-\nData, a Python library released just recently4. We\ncarefully learnt the official API documentation of\nTorchData and made sure we were proficient in\nall APIs. Then, we manually created 50 program-\nming problems based on the API usage examples\nin the documentation. Two volunteers with exten-\nsive experience in Python were invited to check the\ncorrectness of each problem. We control the diffi-\nculty of the programming problems by the number\nof APIs in the target code. The percentage of pro-\ngramming problems containing 1 API, 2 APIs, and\nmore APIs is set to 6:3:1.\nWe also construct two benchmarks using pseudo\nprivate libraries, named MonkeyEval and BeatNu-\nmEval, each containing 101 programming prob-\nlems. They are modified from PandasEval and\nNumpyEval, which were proposed for the public\nlibraries Pandas and Numpy (Zan et al., 2022). In\ndetail, we manually modified all library-related\nkeywords in PandasEval and NumpyEval. For\nexample, as shown in Figure 1, pandas is con-\nverted to monkey, dataframe is converted to\nknowledgeframe, and the API name isin is con-\nverted to iscontain. For more details on keyword\nconversion, please refer to Appendix C. To craft\nthe API documentation for Monkey and BeatNum,\nwe manually paraphrased the descriptions of all the\nnew APIs to ensure that the pre-trained language\nmodels have never seen them.\n5 Experiments\nIn this section, we conduct experiments to illustrate\nthe superiority of our proposed framework.\n5.1 Experimental Setup\nAPI Information. As shown in the second col-\numn (APIs) in Table 1, there are four settings for\nprompting API information before the context:\n‚Ä¢ No API: there is nothing to be prompted;\n‚Ä¢ Perfect: the information of golden APIs in\nthe target code is prompted;\n4Our base model, CODE GEN, is pre-trained with GitHub\ndata before 2021-10. TorchData was released after this time\npoint and no code files using it are available on GitHub so far;\nhence we can consider it as a private library.\n‚Ä¢ Top-N: the information of top N APIs re-\ntrieved by APIRetriever is prompted, where\nN ‚àà{1, 2, 3, 5};\n‚Ä¢ Human: the information of the APIs chosen\nby users is prompted. In our experiments, we\ninvited three volunteers who are programmers\nfamiliar with Python but without any back-\nground in our three benchmarks. As in Figure\n4, they interacted with the APIRetriever and\nprovided their choices for all programming\nproblems. The final APIs are determined by\nvoting on their choices.\nBaselines. Our contributions can be reviewed in\nterms of both APIRetriever and APICoder. For\nAPIRetriever, all models in the No API setting are\nour baseline, while we propose the Perfect, Top-\nN, and Human settings. For APICoder, the main\nbaseline is our base model, CODE GEN-MONO\n350M (Nijkamp et al., 2022), in the same API in-\nformation setting. We use CODE GEN for short in\nthe following. In addition, we include advanced\npre-trained code generation models that are com-\nparable in parameter size: CodeT5 (Wang et al.,\n2021), CodeGPT (Lu et al., 2021), CodeClippy 5\nand CodeParrot6. Codex 12B (Chen et al., 2021) is\nalso used to show the performance of giant models.\nEvaluation Metrics. Followed by Chen et al.\n(2021), we regard pass@k as our metric. For each\nprogramming problem, we sample n ‚â•k code\nsnippets, and then count the number of correct ones\nc, where passing all test cases is considered as cor-\nrect. If n‚àíc < k, then pass@k equals 1; otherwise,\nequals 1 ‚àí‚àèn\ni=n‚àíc+1(1 ‚àík\ni ). In our experiments,\nk is set to one of [1, 10, 100] and n is set to 200.\nImplementation Details. We implement our ap-\nproach based on PyTorch (Paszke et al., 2019) and\nHuggingface‚Äôs transformers (Wolf et al., 2019). We\nuse a dense retrieval toolkit7 to train APIRetriever\nby setting the batch size to 10 per device, the learn-\ning rate to 1e-5, the ratio of positive vs. negative\nsamples to 1:8, and the vector dimensions z of p\nand a to 768. The model uses cross-entropy as the\nloss function and Adam (Kingma and Ba, 2014)\nas the parameters optimizer. It is trained for 100K\nsteps on a cluster of 8 NVIDIA V100 GPUs with\n32GB memory. The training time is about 3 days.\n5https://github.com/CodedotAl/gpt-code-clippy\n6https://huggingface.co/lvwerra/codeparrot\n7https://github.com/luyug/Dense\n281\nAPIs TorchDataEval MonkeyEval BeatNumEvalAPICoder pass@1 pass@10 pass@100 pass@1 pass@10 pass@100 pass@1 pass@10 pass@100\nCodeT5 220M Top-2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nCodeGPT 124M Top-2 0.67 2.78 7.72 0.82 0.99 1.73 0.52 1.88 4.70\nCodeClippy 125M Top-2 0.04 0.39 2.75 0.10 0.76 1.86 0.03 0.33 2.11\nNo API4.04 7.11 13.26 0.54 2.04 7.38 2.67 7.66 18.86\nPerfect4.86+0.82 8.88+1.77 17.25+3.99 2.39+1.85 3.33+1.29 9.99+2.61 5.01+2.34 11.30+3.64 26.36+7.5\nTop-1 4.02-0.02 8.35+1.24 18.17+4.91 2.54+2.00 3.43+1.39 11.39+4.01 4.32+1.65 9.39+1.73 19.91+1.05\nTop-2 4.64+0.60 8.96+1.85 17.48+4.22 1.52+0.98 2.96+0.92 9.32+1.94 2.77+0.10 8.57+0.91 19.74+0.88\nTop-3 4.00-0.04 7.51+0.40 15.13+1.87 1.32+0.78 3.16+1.12 10.59+3.21 1.69-0.98 9.01+1.35 19.90+1.04\nTop-5 4.22+0.18 7.51+0.40 15.43+2.17 0.99+0.45 2.78+0.74 11.76+4.38 1.74-0.93 8.11+0.45 17.54-1.32\nCodeParrot\n110M\nHuman4.01-0.03 7.60+0.49 14.47+1.21 2.44+1.90 3.62+1.58 9.83+2.45 5.23+2.56 11.78+4.12 22.81+3.95\nNo API6.72 15.71 22.00 0.95 4.90 8.89 5.15 11.96 18.79\nPerfect9.84+3.12 22.62+6.91 34.00+12.00 2.14+1.19 6.41+1.51 11.86+2.97 9.47+4.32 17.05+5.09 28.67+9.88\nTop-1 8.72+2.00 19.22+3.51 27.97+5.97 2.22+1.27 7.20+2.30 12.85+3.96 7.52+2.37 15.25+3.29 24.71+5.92\nTop-2 7.52+0.80 16.36+0.65 26.00+4.00 2.46+1.51 6.35+1.45 9.89+1.00 6.65+1.50 13.68+1.72 22.74+3.95\nTop-3 7.92+1.20 18.65+2.94 28.00+6.00 2.02+1.07 5.26+0.36 8.89+0.00 6.26+1.11 16.12+4.16 24.72+5.93\nTop-5 6.08-0.64 17.48+1.77 25.95+3.95 1.58+0.63 5.45+0.55 9.88+0.99 6.34+1.19 15.05+3.09 21.76+2.97\nCODEGEN\n350M\nHuman8.08+1.36 19.85+4.14 31.95+9.95 2.14+1.19 6.14+1.24 11.86+2.97 9.47+4.32 17.12+5.06 28.67+9.88\nNo API7.19 16.93 23.97 1.19 4.68 7.91 4.44 8.24 13.83\nPerfect20.23+13.04 33.37+16.44 41.97+18.00 4.59+3.40 9.14+4.46 13.85+5.94 9.62+5.18 16.51+8.27 22.75+8.92\nTop-1 12.89+5.70 24.26+7.33 31.97+8.00 2.89+1.70 8.28+3.60 12.86+4.94 6.61+2.17 12.62+4.38 17.80+3.97\nTop-2 10.41+3.22 23.50+6.57 31.98+8.01 3.41+2.22 8.33+3.65 11.87+8.90 5.90+1.46 11.79+3.55 15.83+2.00\nTop-3 10.49+3.30 25.45+8.52 35.98+12.01 3.17+1.98 7.51+2.83 10.88+2.97 5.11+0.67 11.40+3.16 15.82+1.99\nTop-5 10.34+3.15 23.04+6.11 27.99+4.02 1.94+0.75 4.75+0.07 7.91+0.00 5.07+0.63 9.64+1.40 13.84+0.01\nCODEGENAPI\n350M\nHuman15.57+8.38 27.76+10.83 33.97+10.00 3.76+2.57 8.32+3.64 12.86+4.95 9.39+4.95 16.40+8.16 23.74+9.91\nNo API7.16 14.46 23.75 1.47 3.53 7.31 6.95 17.54 25.57\nPerfect25.03+17.87 51.26+36.80 56.75+33.00 3.58+2.11 7.48+3.95 12.61+5.30 8.59+1.64 23.75+6.21 36.99+11.42Codex\n12B Top-2 17.98+10.82 32.75+18.29 41.51+17.76 1.92+0.45 5.91+2.38 11.08+3.77 9.54+2.59 21.77+4.23 32.45+6.88\nTable 1: Pass@k(%) results on the three benchmarks. The blue background means no API as extra prompt; the\nyellow background means perfect APIs as extra prompt; the write background means top-1, 2, 3, or 5 APIs retrieved\nby APIRetriever as extra prompt; and the purple background means the APIs chosen by human from top- 5 of\nAPIRetriever as extra prompt. Numbers in red and green indicate the absolute changes over no API setting.\nFor pre-training CODE GENAPI , we set the code\nblock size to 1, 024, the batch size to 4, the learn-\ning rate to 5e-4, the gradient accumulation steps to\n4, the weight decay to 0.1, and the warm up steps\nto 1, 000. Noise APIs are added at a rate of 0.05.\nIt is trained for 100K steps about 1.6 days on 8\n32GB NVIDIA V100 GPUs. In all of our training\nphases, we use mixed precision FP16 to speed up.\nWhen generating code snippets using pre-trained\nmodels, we conduct various temperatures ranging\nfrom 0.1 to 1.0 with the interval of 0.1. All re-\nsults are reported with the best values across these\nhyper-parameters.\n5.2 Main Results\nTable 1 summarizes the performance of our frame-\nwork and all baselines on TorchDataEval, Mon-\nkeyEval, and BeatNumEval. Based on numerous\nexperimental results, we derived plausible observa-\ntions and valuable insights to answer the following\nresearch questions.\n‚ÄúIs API information useful for private library\noriented code generation?‚Äù As we can see in\nTable 1, all models without prompting any APIs\n(the No API setting) achieve relatively poor per-\nformance on all benchmarks. Especially, Codex\n12B, a powerful code generation model with large\nparameters, can only achieve similar performance\nto CODE GEN and CODE GENAPI 350M in the No\nAPI setting. This indicates that even with gigantic\nmodels, the task of code generation with private\nlibraries is extremely challenging. Encouragingly,\nwith prompted API information (the Perfect, Top-\nN, Human settings), both the off-the-shelf models\n(e.g., CodeParrot, CODE GEN, and Codex) and our\ncontinually pre-trained CodeGenAPI achieve con-\nsistent performance gains compared to those in the\nNo API setting. Moreover, the more powerful the\nmodel itself in code generation (i.e., Codex 12B >\nCODE GEN 350M > CodeParrot 110M), the more\nbenefits that API information can bring. For exam-\nple, on TorchDataEval in the Perfect setting, Codex\n12B brings pass@10 an absolute improvement of\n36.89%, while CodeParrot 110M only improves\n1.77%. This observation also suggests that prompt-\ning API information can unleash the potential of\ngigantic models towards invoking private APIs. All\nthe above results prove the usefulness of API infor-\nmation for code generation for private libraries.\n‚ÄúIs the APIRetriever effective in finding useful\n282\nRecall Rate\n40%\n50%\n60%\n70%\n80%\n90%\nTop-1 Top-2 Top-3 Top-5 Top-10\nPandasEval NumpyEval TorchDataEval\nBeatNumEval MonkeyEval\nFigure 5: The recall rates of retrieved APIs.\nAPI information?‚Äù All models in the Top-N setting\noutperform the same models in the No API setting,\nsuggesting that APIRetriever is able to find useful\nAPIs. For a certain model, we observe that the\nTop-1/Top-2 settings usually perform better than\nthe Top-3/Top-5 settings due to the fact that the\nlatter introduces more noise APIs to the APICoder.\nIn addition, involving humans (the Human setting)\nin the selection of APIs can further improve perfor-\nmance, suggesting the effectiveness of the human\ninteraction we designed. Note that the Top-N and\nHuman settings are occasionally superior to the\nPerfect setting, which is reasonable because the\nnoise APIs exist when training the model.\n‚ÄúIs the APICoder effective in invoking private\nAPIs?‚Äù As shown in Table 1, off-the-shelf mod-\nels like CODE GEN are capable of handling private\nlibrary invocations. To seek more extraordinary\nperformance, we continually pre-train CODE GEN\nand obtain a new model CODE GENAPI . We can\nobserve that CODE GENAPI consistently outper-\nforms its base model CODE GEN on TorchDataEval\nand MonkeyEval, which proves the effectiveness\nof CODE GENAPI . However, on BeatNumEval,\nCODE GENAPI is inferior to CODE GEN. After\ncareful troubleshooting, we reveal that the process\nof continual pre-training aims to essentially learn\nhow to invoke the correct APIs with maximum like-\nlihood, while the key obstacle to using BeatNum\nmodified from Numpy lies in the numerical calcu-\nlation like ‚Äòa[:,None]+b*2‚Äô, instead of invoking\nthe correct APIs. Therefore, CODE GENAPI fails\nto yield benefits for BeatNumEval. Overall, API-\nCoder has the capability to invoke private APIs.\n5.3 Closer Analysis\nWe have demonstrated the effectiveness of our\nframework. In this subsection, we provide sev-\neral closer analyses to inspire future work in this\nAccuracy of APIs\n0%\n20%\n40%\n60%\n80%\nTorchDataEval MonkeyEval BeatNumEval\nHuman Top-1 Top-2 Top-3\nFigure 6: Accuracy of retrieved APIs.\nAPI number\nAccuracy\n0%\n10%\n20%\n30%\n40%\n50%\n1 2 >=3\nTorchDataEval/CODEGEN TorchDataEval/CODEGENAPI \nMonkeyEval/CODEGEN MonkeyEval/CODEGENAPI\nBeatNumEval/CODEGEN BeatNumEval/CODEGENAPI\nFigure 7: Accuracy of CODE GENAPI and CODE GEN\nwith respect to the number of APIs. The problem is\nsolved if one of 200 samples passes all test cases.\ndirection.\nQuality of Retrieved APIs. Retrieving the cor-\nrect APIs as prompts can enhance the code genera-\ntion performance for private libraries, so we would\nlike to evaluate the effectiveness of APIRetriever.\nFigure 5 shows the recall rate of APIRetriever on\nfive benchmarks. We can see that the recall rates\nof top-5 are already high, demonstrating that it is\nreasonable to provide 5 API candidates for users to\nchoose from. Furthermore, as shown in Figure 6,\nwe analyze the accuracy of APIs chosen by users.\nWe observe that it dramatically exceeds the accu-\nracy of top 1, 2 or 3 APIs retrieved by APIRetriever.\nThis suggests that it is feasible to involve humans\nin the retrieval of APIs.\nDifferent Difficulty. We would like to explore\nthe performance of CODE GENAPI on varying diffi-\nculty problems. So we calculate its accuracy across\nvarious numbers of APIs in target code y. Each\nbenchmark is divided into 3 parts, according to the\nnumber of APIs. Figure 7 shows that CODE GE-\nNAPI outperforms CODE GEN by a large margin on\nthe problems containing only one API. The trend\nstill holds as the number of APIs increases. It\ndemonstrates CODE GENAPI can boost the perfor-\nmance of generating code snippets using private\nlibrary on varying difficulty.\n283\nAPICoder TorchDataEval MonkeyEval BeatNumEval\npass@1 pass@10 pass@100 pass@1 pass@10 pass@100 pass@1 pass@10 pass@100\nCODEGENAPI 10.41 23.50 31.98 3.41 8.33 11.87 5.90 11.79 15.83\n-w/ noise rate0% 9.41 22.88 31.08 2.69 8.03 11.18 5.77 11.01 14.52\n-w/ noise rate10% 9.19 22.87 30.98 3.04 7.67 11.10 4.99 10.80 15.18\n-w/ noise rate20% 8.92 23.04 30.57 2.00 7.39 10.64 4.48 10.97 13.41\n-w/o resampling 8.65 21.00 29.71 2.47 7.96 10.13 5.21 8.68 14.75\nTable 2: Ablation studies for CODE GENAPI in the Top-2 setting (top 2 APIs provided by APIRetriever are\nprompted). The default setting of CODE GENAPI is to use the resampling strategy and a noise rate of 5%.\nNoise Rate. A well-chosen noise rate can im-\nprove the robustness of CODE GENAPI against a\nvariety of APIs. If we set the noise rate too large,\nit may change the original distribution of the code\ncorpus, while too small will lose the capability to\ndeal with noise APIs. The default noise rate is 5%,\nand we also try 0%, 10%, and 20%. As shown in\nTable 2, both too large and too small noise rates\ncan degrade the performance.\nResampling Strategy. Making high-quality\npython files high-priority, and vice versa, is in\nline with our intuition. To demonstrate it, we\nremove the resampling strategy as mentioned in\nSection 3.3. As shown in Table 2, we observe\na steady decline in performance on the three\nbenchmarks. Such an observation demonstrates\nthe effectiveness of the sampling strategy.\nCODE GENAPI for Public Library. Technically\nspeaking, CODE GENAPI also can be employed\nfor generating code for public libraries. So, we do\nexperiments on PandasEval and NumpyEval and\nshow the results in Table 3. We find that the per-\nformance improvement of CODE GENAPI over the\nbase model on public libraries is not as significant\nas on private libraries. One major reason is that\nthe models have seen the public libraries during\npre-training, so prompting API information yields\nlimited benefit. We can see CODE GENAPI ex-\ncels over CODE GEN when prompting perfect APIs.\nBut when prompting top-2 APIs, the advantages of\nCODE GENAPI are not exhibited. This means that\nCODE GENAPI can also work on third-party public\nlibraries, but it depends heavily on the performance\nof APIRetriever.\n6 Related Work\n6.1 Code Generation\nThanks to the recent development of pre-training\ntechniques, a lot of pre-trained language mod-\nels have been proposed for code-related tasks.\nAPICoder APIs PandasEval\npass@1 pass@10 pass@100\nCODEGEN\nNo API14.24 30.71 46.04\nPerfect 11.21 33.59 48.47\nTop-2 9.54 29.02 40.56\nCODEGENAPI\nNo API13.58 34.95 46.51\nPerfect 19.96 42.36 53.43\nTop-2 11.25 28.61 39.48\nNumpyEval\nCODEGEN\nNo API19.31 40.89 60.58\nPerfect 21.41 41.08 56.38\nTop-2 18.30 35.12 48.46\nCODEGENAPI\nNo API16.55 29.48 42.52\nPerfect 24.83 41.47 54.41\nTop-2 12.67 27.32 35.62\nTable 3: Results of CODE GEN and CODE GENAPI on\nPandasEval and NumpyEval.\nFor example, CuBERT (Kanade et al., 2020),\nCodeBERT (Feng et al., 2020), GraphCode-\nBERT (Guo et al., 2020), CodeT5 (Wang et al.,\n2021), CodeGPT (Lu et al., 2021), PLBART (Ah-\nmad et al., 2021), PyCodeGPT (Zan et al., 2022),\nCODE GEN (Nijkamp et al., 2022), Codex (Chen\net al., 2021), AlphaCode (Li et al., 2022), and In-\nCoder (Fried et al., 2022). Almost all of them focus\non standalone code, while JigSaw (Jain et al., 2021)\nand CERT (Zan et al., 2022) are presented for gen-\nerating code using public libraries. In this paper,\nwe aim to generate code invoking private APIs,\nwhich is a common scenario in practice. It is more\nchallenging because pre-trained language models\nhave never seen any information about private li-\nbraries. As for benchmarks, HumanEval (Chen\net al., 2021), APPs (Hendrycks et al., 2021),\nP3 (Schuster et al., 2021), MBPP (Austin et al.,\n2021), BIG-bench (Srivastava et al., 2022), and\nCodeContests (Li et al., 2022) were proposed to\nevaluate the performance of generating standalone\ncode. GSM8K-Python (Cobbe et al., 2021) and\nMathQA-Python (Austin et al., 2021) were en-\ngaged in evaluating the capability of solving math-\nematical problems. PandasEval and NumpyE-\nval (Zan et al., 2022) were released to evaluate the\ncode generation for public library. We propose\n284\nthree benchmarks, called TorchDataEval, Mon-\nkeyEval, and BeatNumEval, aiming to evaluate the\nperformance of code generation for private library.\n6.2 Retrieval-Based Generation\nIn the natural language field, retrieval-based gen-\neration is a hot topic. A lot of works (Izacard\nand Grave, 2020; Karpukhin et al., 2020; Qu et al.,\n2020; Xiong et al., 2020; Santhanam et al., 2021;\nFormal et al., 2022) have emerged under this topic.\nTherefore, we refer to the above methods and de-\nsign our APIRetriever for private API retrieval. In\nthe programming language field, there are also sev-\neral attempts to use retrieval techniques, such as\nDEEPAPI (Gu et al., 2016), REDCODER (Parvez\net al., 2021), ReACC (Lu et al., 2022), and Doc-\nCoder (Zhou et al., 2022). Our work is funda-\nmentally different from them. They all aim to re-\ntrieve public code snippets or other resources on\nGitHub/StackOverFlow based on the user query,\nwhile our goal is to retrieve APIs from the API\ndocumentation of private library based on code\ncomments. Besides, we design retrieval because\nwe focus on private APIs, which have not been seen\nby the pre-trained generative language models.\n7 Conclusion\nIn this paper, we propose a novel framework for\ncode generation for private library. There are two\nmodules: for a specific programming problem,\nAPIRetriever first finds out the useful private APIs\nfrom API documentation, and then APICoder lever-\nages these APIs to generate the code. We craft three\nbenchmarks, including TorchDataEval, MonkeyE-\nval, and BeatNumEval, for better evaluating private\nlibrary oriented code generation. The experimen-\ntal results and thorough analysis demonstrate the\nreasonableness and effectiveness of our framework.\nIn future work, we would like to explore how to\nmake better use of API documentation for code\ngeneration and improve the approach for real use\nwhen programming with private libraries.\nLimitations\nWhile our proposed approach exhibits many advan-\ntages, it also has a few limitations. (1) As stated\nin Section 5.2, our approach that prompts APIs\nfor programming problem relies heavily upon the\ncode generation capacity of the language model\nitself. The more powerful the model itself, the\nmore benefits the prompting APIs bring. Likewise,\nwe also find that if a model itself shows very poor\nperformance, prompting APIs will not bring any\nbenefit to it or even bring negative effects. (2) As\nthe first navigator to explore code generation with\nprivate library, we have built three private libraries,\nbut they all include a relatively small number of\nAPIs (<200). With these APIs, our APIRetriever\ncan exhibit decent performance. But we surmise\nthat it may become more challenging for APIRe-\ntriever as the number of APIs increases. (3) It is\nextremely challenging to find a real private library\nand craft a benchmark like TorchDataEval. To eval-\nuate our idea quickly and cost-effectively, besides\nTorchDataEval, we also crafted two pseudo private\nlibraries that are modified from the existing public\nones as mentioned in Section 4. Although we have\ndone our best to preserve the two pseudo private\nlibraries in line with the real private library, it may\nstill pose some threats to the fair evaluation of code\ngeneration for private library. (4) We can see from\nTable 1 that most models with the Top-N setting\nfall behind the same model with the Perfect setting.\nSuch observation demonstrates that APIRetriever\nwe designed has a big room for improvement. (5)\nOur experiments show that our framework can en-\nhance the quality of private library oriented code\ngeneration on Python. Limitations may exist when\nwe generalize it to other programming languages\nsuch as Java, C, and C++ since the characteristics\nof libraries for different programming languages\nare slightly different.\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In North Amer-\nican Chapter of the Association for Computational\nLinguistics, pages 2655‚Äì2668.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\n285\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171‚Äì4186.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. CodeBERT: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1536‚Äì1547.\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and St√©phane Clinchant. 2022. From dis-\ntillation to hard negative sampling: Making sparse\nneural ir models more effective. arXiv preprint\narXiv:2205.04733.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. InCoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and\nSunghun Kim. 2016. Deep api learning. In ACM\nSIGSOFT International Symposium on Foundations\nof Software Engineering, pages 631‚Äì642.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, et al. 2020. GraphCodeBERT: Pre-training\ncode representations with data flow. In International\nConference on Learning Representations.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, et al. 2021.\nMeasuring coding challenge competence with apps.\nIn Neural Information Processing Systems Datasets\nand Benchmarks Track.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nNaman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan\nNatarajan, Suresh Parthasarathy, Sriram Rajamani,\nand Rahul Sharma. 2021. Jigsaw: Large language\nmodels meet program synthesis. arXiv preprint\narXiv:2112.02969.\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535‚Äì547.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2020. Learning and evaluating con-\ntextual embedding of source code. In International\nConference on Machine Learning, pages 5110‚Äì5121.\nVladimir Karpukhin, Barlas OÀòguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, R√©mi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814.\nShuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won\nHwang, and Alexey Svyatkovskiy. 2022. ReACC:\nA retrieval-augmented code completion framework.\nIn Association for Computational Linguistics, pages\n6227‚Äì6240.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin Clement,\net al. 2021. CodeXGLUE: A machine learning bench-\nmark dataset for code understanding and generation.\narXiv preprint arXiv:2102.04664.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, et al. 2022. A conversational\nparadigm for program synthesis. arXiv preprint\narXiv:2203.13474.\nMd Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,\nBaishakhi Ray, and Kai-Wei Chang. 2021. Retrieval\naugmented code generation and summarization. In\nFindings of EMNLP, pages 2719‚Äì2734.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, et al. 2019. PyTorch: An imperative style,\nhigh-performance deep learning library. Neural In-\nformation Processing Systems.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2020. RocketQA: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. arXiv preprint\narXiv:2010.08191.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2021. Col-\nbertv2: Effective and efficient retrieval via\nlightweight late interaction. arXiv preprint\narXiv:2112.01488.\nTal Schuster, Ashwin Kalyan, Oleksandr Polozov, and\nAdam Tauman Kalai. 2021. Programming puzzles.\narXiv preprint arXiv:2106.05784.\nEric Snodgrass and Soon Winnie. 2019. Api practices\nand paradigms: Exploring the protocological parame-\nters of apis as key facilitators of sociotechnical forms\nof exchange. First Monday, 24(2).\n286\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\net al. 2022. Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615.\nPriyan Vaithilingam, Tianyi Zhang, and Elena L Glass-\nman. 2022. Expectation vs. experience: Evaluat-\ning the usability of code generation tools powered\nby large language models. In CHI Conference on\nHuman Factors in Computing Systems Extended Ab-\nstracts, pages 1‚Äì7.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Empirical Methods in Natural\nLanguage Processing, pages 8696‚Äì8708.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface‚Äôs transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\narXiv preprint arXiv:2007.00808.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,\nMinsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,\nand Jian-Guang Lou. 2022. CERT: Continual pre-\ntraining on sketches for library-oriented code genera-\ntion. In The 2022 International Joint Conference on\nArtificial Intelligence, pages 3653‚Äì3660.\nShuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang,\nand Graham Neubig. 2022. DocCoder: Generating\ncode by retrieving and reading docs. arXiv preprint\narXiv:2207.05987.\nA Collection of API Documentation\nWe aim to use data from public libraries for train-\ning and generalize the models to private libraries.\nThus, we crawled the API documentation of the 31\nmost popular public libraries in Python. Table 4\nsummarizes the number of APIs we extracted for\neach library.\nB Resampling Strategy\nThe resampling strategy allows high-quality python\nfiles to be more frequently sampled, and vice versa.\nSo the resampling weight (w) of each python file is\ndefined in the following aspects: the star number of\nthe corresponding repository (Nstar), the unit test\nfunction rate (Rut) that is the number of unit test\nfunctions divided by the number of all functions,\nthe number of API name (Napi) in the file, and the\nnumber of APIs (Mapi) considering one API name\nmay match multiple APIs. Formally, the strategy\ncan be formulated as follows:\nwstar = 1.0 + log(Nstar + 1).clip(5\n0) √ó0.2,\nwut = (0.5 + (1‚àíRut)).clip(1\n0),\nwapi = 5.0 ‚àílog(Mapi\nNapi\n).clip(5\n0) √ó0.2,\nw = wstar √ówut √ówapi,\n(1)\nwhere clip(y\nx) limits the value to [x, y].\nC Keywords Conversion from Public\nLibrary to Private Library.\nAs mentioned in Section 4, we convert the public\nlibrary benchmarks (PandasEval and NumpyEval)\nto the private library benchmarks (MonkeyEval and\nBeatNumEval) by manually modifying all public\nlibrary-related keywords. In Table 5, we list all the\nkeywords before and after the conversion.\n287\nPandas NumPy sklearn PyTorchTensorFlowDjangoseleniumMatplotlib Flask SciPy Seaborn\n7,094 12,085 53,166 124,902 32,116 24,375 4,842 439,913 31,867 153,359 161,477\nNLTK BeatifulSouppygame PIL jieba Gensim spaCy transformersfairseq SQLAlchemyScrapy\n206,816 22,519 70,396 127,212 26,620 37,331 239,945 652,913 158,721 54,765 3,537\nAllenNLP datasets tokenizersMXNet imageio pytest MetPy ansible requests\n276,088 136,843 195 142,070 175,878 1,047 27,429 40,839 39,333\nTable 4: The number of APIs in the 31 public libraries we crawled.\nPandasEval-MonkeyEval\nisnull mean pandas dataframe df isin pd\nifnull average monkey knowledgeframe kf incontain mk\ntolist apply to_numeric dropna append tail copy\nconvert_list employ to_num sipna adding last_tail clone\ninnull astype select_dtypes iterrows min max map\nisnone totype choose_dtypes traversal get_min get_max mapping\nlast shift merge value_counts rename_axisreset_index sample\nfinal_item shifting unioner counts_value_num renaming_axis reseting_index sample_by_num\nconcat to_dict cumsum sort_index to_string drop_duplicatesduplicated\nconcating convert_dict cumulative_sum sorting_index convert_string remove_duplicates duplicated_values\nround format to_pydatetime div ceil assign intersection\nvalue_round formating convert_pydatetime division ceiling allocate interst\ndrop Series ravel any fillna all Pandas\nsip Collections flat_underlying whatever fillnone total_all Monkey\nreindex get std rename sum unique to_datetime\nreindexing getting standard renaming total_sum distinctive convert_datetime\napplymap sort_values DataFrame groupby nlargest replace len\nconduct_map sort_the_values KnowledgeFrame grouper nbiggest replacing length\nhead series isna\nheader_num collections ifna\nNumpyEval-BeatNumEval\nto_numpy ndarray array transpose numpy Numpy np\nto_beatnum ndnumset numset switching_places beatnum Beatnum bn\ncolumn_stackconcatenate slice sum imag abs real\nstack_col connect piece total_count imaginary absolute reality\nfill_diagonalall fromstring in1d mean where std\npad_diagonal total come_from_str intersection1dim average filter_condition standard_op\nadd histogram fromarrays reshape filled stack cumsum\nadd_concat hist_operation come_from_arrays change_shape_to masked_fill pile_operation cumulative_sum\nastype arange setxor1d compressed argmin argmax\nconvert_type arr_range seting_exclusive_or_one_dim remove_masked_data get_argmin_value get_argmax\nvstack squeeze hstack asarray repeat vectorize split\nvertical_stack sqz horizontal_stack asnumset duplicate vectorisation sep_split\ndiff unique unravel_index flatten norm delete ones\ndifference uniq convert_index_or_arr convert_into_one_dim normlizattion remove_operation create_ones\nappend any logical_and bincount isnan argpartition ravel\napd any_condition logic_and_element_wise binoccurrence ifnan perform_partition asview\narray_split inv insert searchsorted min max full\nsplit_array inverse stick find_sorted get_min get_max full_value_func\nTable 5: The keywords of converting PandasEval to MonkeyEval, and NumpyEval to BeatNumEval. The grey\nbackground means the original keywords, and the white background means the converted ones.\n288",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8338973522186279
    },
    {
      "name": "Code (set theory)",
      "score": 0.7333930730819702
    },
    {
      "name": "Documentation",
      "score": 0.7050446271896362
    },
    {
      "name": "Code generation",
      "score": 0.550365686416626
    },
    {
      "name": "Language model",
      "score": 0.5384199023246765
    },
    {
      "name": "Programming language",
      "score": 0.4585459232330322
    },
    {
      "name": "Craft",
      "score": 0.45445820689201355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44347262382507324
    },
    {
      "name": "Natural language processing",
      "score": 0.36308610439300537
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.17323288321495056
    },
    {
      "name": "Key (lock)",
      "score": 0.16889601945877075
    },
    {
      "name": "Operating system",
      "score": 0.08355611562728882
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}