{
    "title": "Understanding Large Language Model Based Metrics for Text Summarization",
    "url": "https://openalex.org/W4392669734",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2241518958",
            "name": "Abhishek Pradhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5116741222",
            "name": "Ketan Todi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4392669714",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W4385849424",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W3035252911"
    ],
    "abstract": "This paper compares the two most widely used techniques for evaluating generative tasks with large language models (LLMs): prompt-based evaluation and log-likelihood evaluation as part of the Eval4NLP shared task. We focus on the summarization task and evaluate both small and large LLM models. We also study the impact of LLAMA and LLAMA 2 on summarization, using the same set of prompts and techniques. We used the Eval4NLP dataset for our comparison. This study provides evidence of the advantages of prompt-based evaluation techniques over log-likelihood based techniques, especially for large models and models with better reasoning power.",
    "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 149–155\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nUnderstanding Large Language Model Based Metrics for Text\nSummarization\nAbhishek Pradhan∗#\nabhishek.pradhan2008@gmail.com\nKetan Kumar Todi∗+\ntodiketan@hotmail.com\nAbstract\nThis paper compares the two most widely used\ntechniques for evaluating generative tasks with\nlarge language models (LLMs): prompt-based\nevaluation and log-likelihood evaluation as part\nof the Eval4NLP shared task. We focus on the\nsummarization task and evaluate both small and\nlarge LLM models. We also study the impact\nof LLAMA and LLAMA 2 on summarization,\nusing the same set of prompts and techniques.\nWe used the Eval4NLP dataset for our com-\nparison. This study provides evidence of the\nadvantages of prompt-based evaluation tech-\nniques over log-likelihood based techniques,\nespecially for large models and models with\nbetter reasoning power.\n1 Introduction\nTransformer-based language models have revolu-\ntionized the field of natural language processing\n(NLP), particularly in the area of language genera-\ntion. However, the improved language generation\ncapabilities of these models have also exposed the\nlimitations of traditional lexical evaluation metrics,\nsuch as perplexity, BLEU (Papineni et al., 2002),\nand ROUGE (Lin, 2004). These metrics are often\nunable to accurately assess the quality of generated\ntext, especially when it is creative or informative.\nIn response, researchers have developed a wide\nrange of new automatic evaluation models, such as\nBLEURT (Sellam et al., 2020), BERTScore (Zhang\net al., 2020), and BARTScore (Yuan et al., 2021).\nThese models typically rely on a combination of\nlexical and semantic features to assess the quality\nof generated text, and some of them also take into\naccount the golden reference annotation.\nRecent large language models (LLMs) like\nPaLM (pal, 2022), GPT-3.5, and GPT-4 (OpenAI,\n2023) have taken language generation capabilities\nto a new level, making it difficult to distinguish be-\ntween machine-generated and human-written text.\nThis has led to the use of LLMs for a variety of\nmore complex tasks, such as summarizing entire\nresearch papers, even when the ground truth is not\nknown. The increased complexity of these tasks\nhas spurred interest in using LLMs themselves for\nmodel evaluation.\nPrompt-based and log-likelihood-based evalua-\ntion are two widely used approaches for automatic\nevaluation of large language models (LLMs). How-\never, it is unclear which approach works better\nwith different model sizes, as previous studies have\nused these approaches on mutually exclusive sets\nof models.\nIn this paper, we evaluate multiple LLM mod-\nels of different sizes using both prompt-based and\nlog-likelihood-based evaluation on the Eval4NLP\ndataset (Leiter et al., 2023) as part of the Eval4NLP\nshared task (Leiter et al., 2023). We experiment\nwith three models from the LLaMA (Touvron\net al., 2023a) and LLaMA2 (Touvron et al., 2023b)\nfamily, which are allowed in the Eval4NLP 2023\nshared task.\nOur results show that prompt-based evaluation\ngenerally outperforms log-likelihood-based eval-\nuation for all model sizes. This is likely because\nprompt-based evaluation is more directly aligned\nwith the tasks that LLMs are typically used for,\nsuch as generating text, translating languages, and\nanswering questions.\nOur findings suggest that prompt-based evalua-\ntion is a more reliable and informative approach\nfor evaluating LLMs of all sizes.\n2 Dataset and Task Description\nThe summarization track of the Eval4NLP task\ninvolved predicting an overall score for a model-\ngenerated summary of a source text. The compe-\ntition required participants to use only a limited\nset of models without fine-tuning, meaning that the\n*These authors contributed equally to this work.\n+Work done while author was at Google.\n#Work done while author was at Incivus.\n149\nFigure 1: Prompt Design for Prompt Based Evaluation\n#of examples\nTrain 320\nDev 1280\nTest 825\nTable 1: Dataset Statistics\nproposed approaches needed to determine differ-\nent prompting strategies to improve model perfor-\nmance.\nThe dataset statistics are shown in Table 1\n3 Related Work\nEarly work on NLG evaluation includes BLEURT,\nBERTScore and BARTScore to name a few.\nBLEURT and BERTScore both rely upon golden\nreference text to score the model generated text.\nBoth these models propose finetuning the BERT\nmodel to predict a similarity score between the\nreference output and the model generated output.\nBARTScore leverages the natural language gen-\neration capability of BART model and proposes\nvarious different approaches of automated scoring\nsome of which can be used even without knowing\nthe reference output.\nSimilar to BARTScore, GPTScore (Fu et al.,\n2023) use the log-likelihood of the model generated\noutput given the source text as a way of scoring the\nquality of the generated text. It carried out exten-\nsive experiments using different model sizes and\ndifferent model types on a variety of different NLG\nevaluation tasks.\nG-Eval (Liu et al., 2023) takes it a step further.\nIt proposes to leverage the language generation ca-\npabilities of LLM to directly predict an evaluation\nscore. As part of the prompt G-Eval provides the\nmodel with the metric definition and the model\ndefined evaluation steps for each metric.\n4 Experiments\nThe competition allowed only variants of the 13B\nLLaMA and LLaMA2 models, as well as quan-\ntized versions of LLaMA or LLaMA2 models with\n60B+ parameters. Our main aim was to com-\npare prompt-based evaluation and log-likelihood-\nbased evaluation techniques across different model\nsizes. Therefore, we decided to work with the\nNousHermes-13B (Teknium, 2023) and Platypus-\n70B (Lee et al., 2023a) models. However, since\nthese two models belong to different LLaMA fam-\nilies, we also included the results obtained using\nthe Ocra-13B model (Lee et al., 2023b), which is\nbased on LLaMA2, for a fair comparison.\nWe experimented with two different approaches\nas follows:\n4.1 Prompt-based evaluation\nPrompt-based evaluation involved providing the\nmodel with a prompt that contains an instruction\nto evaluate the summary and provide a score along\nwith the original text, and the summary of the text\n(Liu et al., 2023).\nTwo types of prompt-based evaluation tech-\nniques were used to assess the quality of the sum-\nmary of the provided text: 1) a single prompt for a\nfinal score and 2) four different prompts to evalu-\nate coherence, consistency, fluency, and relevance.\nThe scores from the four prompts were averaged\nto produce the final score for the technique. The\nintuition behind this approach was to reduce the\n150\nFigure 2: Prompt Design for Log-Likelihood Based Evaluation\ncomplexity of the task and make the model focus\non individual aspects, before we average it out.\nThe prompts used for the two settings are shown\nin Appendix B and Appendix C respectively. For\nthe second setting of calculating four scores on four\ndifferent aspects we modified the task description\nand evaluations steps in the same way as G-Eval\n(Liu et al., 2023). The prompt design for Prompt\nBased Evaluation is shown in Figure 1.\nFor both the prompt settings mentioned in the\nabove paragraph we used sampling to sample 10\noutput scores for each input example, and then\naveraged it out to generate a single prediction score.\n4.2 Log-Likelihood-based evaluation\nLog-Likelihood-based evaluation involved provid-\ning the model with a prompt that contains an in-\nstruction to generate the summary along with the\noriginal text, and the summary of the text. The final\nscore is calculated by multiplying log-likelihood\nof the tokens of the summary. This method helps\nto evaluate the likelihood of LLM generating the\ngiven summary. If summary is good according\nto the evaluating LLM, the summary gets a high\nlog-likelihood. This method was used in both\nGPTScore (Fu et al., 2023) and BARTScore (Yuan\net al., 2021).\nWe adopted a similar strategy as above for like-\nlihood based approaches as well, i.e. a prompt\nto generate a single likelihood score and four dif-\nferent prompts to obtain four different likelihood\nvalues, which are then averaged out. The prompt\ndesign for log-likelihood based evaluation is shown\nin Figure 2. In addition we experimented with two\ndifferent sets of prompt\n• the first set of the prompts is similar to the one\nwe used for prompt-based evaluation. The\nassociated prompt has been shown in Ap-\npendix D.\nNous-\nHermes\nOcra Platypus\nSingle\nLikelihood 0.314 0.292 0.292\nPrompt-based 0.192 0.310 0.398\nAverage\nLikelihood (Our\nPrompts)\n0.317 0.297 0.298\nLikelihood (Origi-\nnal Prompts)\n0.320 0.295 0.296\nPrompt-based 0.296 0.376 0.463\nTable 2: Performance on Dev Set\n• the second set of the prompts are the ones\nproposed in GPTScore.\n5 Results\nComparing the likelihood based scores for the\nPlatypus-13B model across the single scoring and\nthe 2 different prompts sets for average scoring\nfrom Table 2 we can see that the co-relation val-\nues remains the same. Same is the case for the\nother two models as well. This shows that the\nprompts are not too relevant for likelihood based\napproaches.\nThe likelihood performance of LLaMA2 based\nmodels is consistently worse than those of LLaMA\nbased models across all settings. The performance\nof Ocra-13B model is similar to the NousHermes-\n13B model in case of likelihood based approach.\nBut considering that prompt based scores are re-\nversed for the two, it seems LLaMA2 based mod-\nels are generally worse than LLaMA based models\nin the case of likelihood. We believe that one of\nthe reasons for this could be that LLaMA2 based\nmodel’s generation distribution might be different.\ni.e. it might consider most of the summaries to be\naverage in nature resulting in low likelihood. Fur-\n151\nther analysis and experiments with other instruc-\ntion tuned model might be required to understand\nif other LLaMA2 based models also have similar\nresults.\nFor the prompt based evaluations we can see\nthat using a single prompt to get a score led to per-\nformance degradation across all the three models.\nThis shows that the use of a complex prompt makes\nthe reasoning process difficult for the model.\nThe performance of LLaMA2 based Ocra-13B\nmodel is much better than the LLaMA based\nNousHermes model. The performance different\nbetween the two models is vastly different. The\ntwo reasons for this could be (a) Ocra is a LLaMA2\nbased model or (b) different instrucion tuning data\nused for the two models. We believe the first to\nbe true as it is eviden from the huggingface leader-\nboards, where LLaMA2 based models are consis-\ntentl ranked higher than LLaMA based models.\nLastly the quantized Platypus-70B model sur-\npasses the performance of Ocra-13b model in the\nscoring based approach showing that bigger mod-\nels tend to improve performance, even if it has been\nquantized down to 4-bits.\nWe tested the best models across both the set-\ntings i.e. the likelihood and the prompt based\napproach on test dataset. All the submission\nwere made under the team name of Beginners.\nNousHermes-13b model achieved the best results\nusing the likelihood based approach with a score\n0.38 on test data. A single prompt was used as\nshown in Appendix D with the submission ID\n20138. The Platypus-70B model achieved the best\nscore score in the prompt based approach. It got a\nscore of 0.44 on test data by averaging the scores\nobtained using four different prompts for four dif-\nferent aspects (consistency, fluency, relevance, co-\nherence) with submission ID 20254.\n6 Conclusion\nPrompt-based evaluation technique outperforms\nlog-likelihood-based evaluation technique in text\nsummarization evaluation. However, evaluating\nsingle summaries is challenging, as there are many\ndifferent aspects to consider, and some aspects may\nbe more important than others. Averaging scores\nfrom different aspects improves performance, sug-\ngesting that there are other evaluation aspects that\nwe did not consider. LLaMA2 based models seem\nbetter at reasoning and making decisions, even\nwith low likelihood scores. Therefore, combin-\ning Prompt-based evaluation with LLaMA2 based\nmodels may further improve text summarization\nevaluation results.\nLimitations\nThis experiment used smaller open-source models\n(13B or quantized 70B), but the inference hardware\nrequirements for most of the models used in this\npaper are still high. For example, both the 13B\nand quantized 70B models took 24 hours to run on\ntwo 48GB A6000 GPU machines for the prompt\nscoring based approach, making it expensive and\ntime-consuming to iterate through different ideas.\nReferences\n2022. Palm: Scaling language modeling with pathways.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nAriel N. Lee, Cole J. Hunter, and Nataniel Ruiz. 2023a.\nPlatypus: Quick, cheap, and powerful refinement of\nllms.\nAriel N. Lee, Cole J. Hunter, Nataniel Ruiz, Bleys\nGoodson, Wing Lian, Guan Wang, Eugene Pent-\nland, Austin Cook, Chanvichet V ong, and \"Teknium\".\n2023b. Openorcaplatypus: Llama2-13b model\ninstruct-tuned on filtered openorcav1 gpt-4 dataset\nand merged with divergent stem and logic dataset\nmodel. https://huggingface.co/Open-Orca/\nOpenOrca-Platypus2-13B.\nChristoph Leiter, Juri Opitz, Daniel Deutsch, Yang Gao,\nRotem Dror, and Steffen Eger. 2023. The eval4nlp\n2023 shared task on prompting large language models\nas explainable metrics. In Proceedings of the 4th\nWorkshop on Evaluation and Comparison for NLP\nsystems.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nOpenAI. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\n152\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation.\nTeknium. 2023. Noushermes13b. https:\n//huggingface.co/NousResearch/\nNous-Hermes-13b.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with bert.\nA System Settings\nAll the experiments were run of A6000 40GB\nGPUs. We used pytorch-2.0.1 and transform-\ners=4.32.0 and nvidia-cuda-11.7.\n153\nB Single Scoring Prompt\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a\nresponse that appropriately completes the request.\n### Instruction:\nYou will be given a news article.\nYour task is to rate the generated summary with a score of 1-5.\nTo rate the summary evaluate it on 4 different aspects Coherent, Consistent, Fluent and relevant.\nPlease make sure you read and understand the definitions carefully. Please keep this document open while\nreviewing, and refer to it as needed.\nCoherence - the collective quality of all sentences. The summary should be well-structured and\nwell-organized. The summary should not just be a heap of related information, but should build from\nsentence to a coherent body of information about a topic.\nConsistency - the factual alignment between the summary and the news article. A factually consistent\nsummary contains only statements that are entailed by the news article. Annotators were also asked to\npenalize summaries that contained hallucinated facts.\nFluency - the quality of the summary in terms of grammar, spelling, punctuation, word choice, and\nsentence structure.\nRelevance - selection of important content from the news article. The summary should include only\nimportant information from the news article. Annotators were instructed to penalize summaries which\ncontained redundancies and excess information.\n### Input:\nNews Article: source_text\nSummary: summary\nEvaluation Form (scores ONLY):\n### Response: Score\nC Scoring Prompt\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a\nresponse that appropriately completes the request.\n### Instruction:\nYou will be given one summary written for a news article.\nYour task is to rate the summary on one metric.\nPlease make sure you read and understand these instructions carefully. Please keep this document open\nwhile reviewing, and refer to it as needed.\nEvaluation Criteria:\nCoherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality\nquestion of structure and coherence whereby \"the summary should be well-structured and well-organized.\nThe summary should not just be a heap of related information, but should build from sentence to a\ncoherent body of information about a topic.\"\n154\nEvaluation Steps:\n1. Read the news article Text carefully and identify the main topic and key points.\n2. Read the Summary and compare it to the news article Text. Check if the Summary covers the main topic\nand key points of the news article Text, and if it presents them in a clear and logical order.\n3. Assign a score for coherence on a scale of 1 to 5 (score can be decimal or integer), where 1 is the\nlowest and 5 is the highest based on the Evaluation Criteria.\n### Input:\nnews article Text: source_text\nSummary: summary\nEvaluation Form (scores ONLY):\n### Response:Coherence:\nD Likelihood Prompt\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a\nresponse that appropriately completes the request.\n### Instruction:\nYou will be given a news article.\nYour task is to write a summary for the article which is Coherent, Consistent, Fluent and relevant.\nPlease make sure you read and understand the definitions carefully. Please keep this document open while\nreviewing, and refer to it as needed.\nCoherence - the collective quality of all sentences. The summary should be well-structured and\nwell-organized. The summary should not just be a heap of related information, but should build from\nsentence to a coherent body of information about a topic.\nConsistency - the factual alignment between the summary and the news article. A factually consistent\nsummary contains only statements that are entailed by the news article. Annotators were also asked to\npenalize summaries that contained hallucinated facts.\nFluency - the quality of the summary in terms of grammar, spelling, punctuation, word choice, and\nsentence structure.\nRelevance - selection of important content from the news article. The summary should include only\nimportant information from the news article. Annotators were instructed to penalize summaries which\ncontained redundancies and excess information.\n### Input:\nNews Article: source_text\n### Response: summary\n155"
}