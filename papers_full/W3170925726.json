{
    "title": "On the Transformer Growth for Progressive BERT Training",
    "url": "https://openalex.org/W3170925726",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2798684433",
            "name": "Xiaotao Gu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100063962",
            "name": "Liyuan Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099812159",
            "name": "Hongkun Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1958446717",
            "name": "Jing Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095964268",
            "name": "Chen Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103606203",
            "name": "Jiawei Han",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3102816807",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2950452665",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W4288265053",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W2577229169",
        "https://openalex.org/W3102892879",
        "https://openalex.org/W3035104321",
        "https://openalex.org/W4301181331",
        "https://openalex.org/W2913732044",
        "https://openalex.org/W2994689640",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2756381707",
        "https://openalex.org/W4294643831",
        "https://openalex.org/W3034465644",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2945667196",
        "https://openalex.org/W2962676330",
        "https://openalex.org/W3033188311",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W3080717140",
        "https://openalex.org/W2963628712",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W2964088127",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2556046966",
        "https://openalex.org/W2962760235"
    ],
    "abstract": "Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, Jiawei Han. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5174–5180\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5174\nOn the Transformer Growth for Progressive BERT Training\nXiaotao Gu␆␄∗ Liyuan Liu␆ Hongkun Yu␄ Jing Li␄ Chen Chen␄ Jiawei Han␆\n␄ Google Research {hongkuny,jingli,chendouble}@google.com\n␆ University of Illinois at Urbana-Champaign {xiaotao2,ll2,hanj}@illinois.edu\nAbstract\nDue to the excessive cost of large-scale lan-\nguage model pre-training, considerable efforts\nhave been made to train BERT progressively—\nstart from an inferior but low-cost model and\ngradually grow the model to increase the\ncomputational complexity. Our objective is\nto advance the understanding of Transformer\ngrowth and discover principles that guide pro-\ngressive training. First, we ﬁnd that similar\nto network architecture search, Transformer\ngrowth also favors compound scaling. Speciﬁ-\ncally, while existing methods only conduct net-\nwork growth in a single dimension, we observe\nthat it is beneﬁcial to use compound growth op-\nerators and balance multiple dimensions (e.g.,\ndepth, width, and input length of the model).\nMoreover, we explore alternative growth oper-\nators in each dimension via controlled compar-\nison to give operator selection practical guid-\nance. In light of our analyses, the proposed\nmethod CompoundGrow speeds up BERT pre-\ntraining by 73.6% and 82.2% for the base\nand large models respectively, while achieving\ncomparable performances1.\n1 Introduction\nThanks to the rapid increase of computing power,\nlarge-scale pre-training has been breaking the glass\nceiling for natural language processing tasks (Liu\net al., 2018; Peters et al., 2018; Devlin et al., 2019;\nLiu et al., 2019; Brown et al., 2020). However, with\ngreat power comes great challenges: the required\nexcessive computational consumption signiﬁcantly\nimpedes the efﬁcient iteration of both research ex-\nploration and industrial application. To lower the\ntraining cost, many attempts have been made to\nconduct progressive training, which starts from\ntraining an inferior but low-cost model, and gradu-\nally increases its resource consumption (Gong et al.,\n∗Work done while interning at Google. Corresponding\nAuthor: Hongkun Yu and Xiaotao Gu.\n1Code will be released at: https://github.com/google-\nresearch/google-research/tree/master/grow_bert\n2019; Devlin et al., 2019). As elaborated in Sec-\ntion 5, two components are typically needed for de-\nsigning such progressive training algorithms—the\ngrowth scheduler and the growth operator (Dong\net al., 2020). The former controls when to conduct\nnetwork growth, and the latter controls how to per-\nform network growth. Here, our objectives are to\nbetter understand growth operators with a focus\non Transformer models (Vaswani et al., 2017; Liu\net al., 2020b), and speciﬁcally, to help design better\nprogressive algorithms for BERT pre-training (De-\nvlin et al., 2019). Speciﬁcally, we recognize the\nimportance of using compound growth operators\nin our study, which balance different model dimen-\nsions (e.g., number of layers, the hidden size, and\nthe input sequence length).\nRegarding previous efforts made on Transformer\ngrowth, they mainly focus on one single model di-\nmension: either the length (Devlin et al., 2019) or\nthe depth (Gong et al., 2019). In this work, how-\never, we ﬁnd that compound effectplays a vital role\nin growing a model to different capacities, just like\nits importance in deciding network architectures\nunder speciﬁc budgets (Tan and Le, 2019). Here,\nwe show that growing a Transformer from both\ndimensions leads to better performance with less\ntraining cost, which veriﬁes our intuition and shows\nthe potential of using compound growth operators\nin progressive BERT training.\nFurther, we explore the potential choices of\ngrowth operators on each dimension. We conduct\ncontrolled experiments and comprehensive analy-\nses to compare various available solutions. These\nanalyses further guide the design of effective com-\npound growth operators. Speciﬁcally, we observe\nthat, on the length dimension, embedding pooling\nis more effective than directly truncating sentences.\nOn the width dimension, parameter sharing outper-\nforms low-rank approximation.\nGuided by our analyses, we propose Compound-\nGrow by combining the most effective growth oper-\n5175\nMNLI\n0.82\n0.84\nS1/EM\n0.80\n0.84\nS1/F1\n0.87\n0.90\nS2/EM\n0.72\n0.77\n+100K +200K +300K\n100K small model\nS2/F1\n+100K +200K +300K\n300K small model\n+100K +200K +300K\n500K small model\n+100K +200K +300K\n700K small model\n0.75\n0.80\nlength depth compound (length+depth)\nFigure 1: Comparison of single-dimensional operators and the compound operator with comparable cost. Y-axis\nindicates ﬁnetuning performances, including MNLI-match valid accuracy (MNLI), SQuaD v1.1 exact match score\nand F1 (S1/EM, S1/F1), and SQuaD v2.0 exact match score and F1 (S2/EM, S2/F1). X-axis stands for different\ntraining steps of the full model (12-layer BERT-base model with 512-token training data) in the last stage. Different\ncolumns represent different training steps for the small (low-cost) model. The three compared methods start from\ndifferent small models: depth stands for a 3-layer model; length stands for training with 128-token training data;\ncompound stands for a 6-layer model with 256-token training data.\nAlgorithm 1: Progressive Training.\nf: network; opt: optimizer; D: dataset.\ngt: the growth operator at stage t.\nT: the total number of growing stages.\nf0 ←opt(f0, x, y)\nfor t ∈[1, T] do\nft ←gt(ft−1) for x, y∈D do\nft ←opt(ft, x, y)\nreturn Final network fT\nator on each dimension. Experiments on standard\nbenchmarks show that, without sacriﬁcing ﬁnal\nperformance, the ﬁnal model speeds up the overall\npre-training by 73.6% and 82.2% on BERT-base\nand BERT-large models respectively.\n2 Progressive Compound Growth\nProgressive Training. Algorithm 1 presents a\ngeneric setup for progressive training. In each train-\ning stage t, the corresponding growth operator gt\ngrows the model f. Then, f is updated by the op-\ntimizer opt before entering the next training step.\nCorrespondingly, our goal is to maximize the ﬁnal\nmodel performance after all training stages, which\ncan be formulated as minimizing the empirical loss\nLover dataset D:\nmin\ngt∈G\nL(fT ) s.t. ft = opt (gt(ft−1), D) (1)\nCompound Effect. Existing progressive training\nmethods only focus on one model dimension. For\nexample, Gong et al. (2019) conduct Transformer\ngrowth by gradually increasing the network depth.\nDevlin et al. (2019) use shorter input sequence\nlength at early stages. However, as studies in net-\nwork architecture search have revealed (Tan and\nLe, 2019), growth operators that balance different\nmodel dimensions can achieve better performance\nthan single-dimensional operators under the same\nbudget. Note that our objective (Equation 1) is\nclose to the objective of EfﬁcientNet (Tan and Le,\n2019), which aims to ﬁnd the optimal network ar-\nchitecture by maximizing the model accuracy for a\ngiven resource budget:\nmax\nd,w,r\nAccuracy(N(d, w, r))\ns.t. Resource_cost(N) ≤target_budget,\nwhere N(d, w, r) is a CNN network, d, w, r are\ncoefﬁcients to scale its depth, width, and resolu-\ntion. In this work, we ﬁnd that such a compound\neffect also plays a vital role in progressive BERT\ntraining. Intuitively, growing the network from\nmore than one dimension creates larger potential\nto get better performance with less resource. Re-\nstricting the growth operator from handling all di-\nmensions would lead to inferior performance, as\n5176\nming∈G L(fT ) ≥ming∈G∪G+ L(fT ). The opti-\nmal value of the objective function (Equation 1) is\nbounded by the feasible set of the growth operator.\nEmpirical Veriﬁcation. For empirical veriﬁcation,\nwe compare existing single-dimensional growth op-\nerators in model depth and length with the corre-\nsponding compound operator that balances both\ndimensions. For all three compared growth op-\nerators, their conﬁgurations are adjusted to make\nsure they have the same model after growth, and\ntheir low-cost models have empirically compara-\nble training costs. As to the training, we ﬁrst train\nthe low-cost model for 100/300/500/700K steps,\nand then grow the model to a standard BERT-base\nmodel for another 300K steps training. For mod-\nels trained with different steps/growth operators,\nwe compare their performance after ﬁnetuning on\nMNLI, SQuaD v1.1, and SQuaD v2.0 respectively.\nAs Figure 1 shows, across different settings\n(columns) and metrics (rows), the compound oper-\nator consistently outperforms or at least achieves\ncomparable results with single-dimensional oper-\nators. The observation meets our intuition: to\nachieve same speedup, the compound method can\ndistribute the reduction on training cost to different\ndimensions, and achieve better performance.\n3 Explore Possible Growth Operators\nAfter verifying the importance of compound grow-\ning, we conduct more analysis to provide guidance\nfor growth operator design.\n3.1 Length Dimension\nData Truncation ﬁrst limits the maximum length\nof input sequences by truncating the training sen-\ntences to a shorter length, and then train the model\non full-length data. Note that shorter input se-\nquences usually come with less masked tokens to\npredict in each sentence. For instance, Devlin et al.\n(2019) ﬁrst use sentences of at most 128 tokens\n(with 20 masked tokens) before training on data of\n512 tokens (with 76 masked tokens). The major\nissue of this data truncation operator is the incom-\nplete update of position embeddings. The model\nneeds to learn embeddings for the extra positions\nfrom scratch at the last stage.\nEmbedding Pooling. Inspired by the idea of\nmultigrid training in the vision domain (Wu et al.,\n2020), we train the model with “low-resolution text”\nthrough embedding pooling over unmasked tokens.\nCompared with data truncation, this method leaves\nthe training data intact and can update all position\nembeddings. Speciﬁcally, since the output length\nof self-attention modules is decided by the length of\nquery vectors, we only conduct pooling on query\nvectors in the ﬁrst self-attention layer and keep\nkey/value vectors intact.\nAs shown in the ﬁrst group of Table 1, data\ntruncation (sequence length=256) and mean pool-\ning (k=2) has similar performance on MNLI and\nSQuAD v1.1, while mean pooling outperforms data\ntruncation on SQuAD v2.0.\n3.2 Width Dimension\nOn the width dimension, we focus our study on\nthe feedforward network module (FFN). Similar\nto gradually increasing the network depth, one can\nalso gradually increase the network width for Trans-\nformer growth. Speciﬁcally, the FFN module can\nbe formed as f(xW1)W2, where f(·) is the activa-\ntion function, W1 ∈RD×H and W2 ∈RH×D are\nparameters, D and H are the embedding size and\nthe hidden size respectively.\nMatrix Factorization. A straightforward method\nis to approach the original weight matrix Wi ∈\nRm×n by the product of two small matrices Wi1 ∈\nRm×h and Wi2 ∈Rh×n in the early training stage.\nIn the late stage of training, we would recover Wi\nas Wi1 ×Wi2 and unleash the full potential.\nParameter Sharing. Instead of decomposing orig-\ninal weight matrices with low-rank approximation,\nwe try to employ parameter sharing by spliting the\nmatrix into multiple blocks and sharing parameters\nacross different blocks. Formally, for input x,\nf(xW1)W2=f(x[W′\n1,...,W′\n1])\n\n\nW′\n2/k\n...\nW′\n2/k\n\n=f(xW′\n1)W′\n2.\n(2)\nSpeciﬁcally, in the early training stage, we replace\nW1 and W2 with smaller matrices W′\n1 ∈RD×H\nk\nand W′\n2 ∈R\nH\nk ×D. Then, at the growth step, we\nvertically duplicate (share) W′\n1 for k times along\nthe dimension with size H/k as the new W1. W2\nis generated similarly. Similar to matrix factoriza-\ntion, this setting also preserves the output after the\ngrowth. Random noise is added to W1 and W2 by\nthe dropout layers in FFN, so that the shared small\nmatrices will have different outputs and gradients\nin later training steps (Chen et al., 2015).\nAs the second group of Table 1 shows, parameter\nsharing has signiﬁcant superiority over matrix fac-\n5177\nTable 1: Empirical comparison among growth operators. For each operator, a low-cost model is ﬁrst trained for\n700K steps, then grown to the original BERT model for another 300K steps training.\nBERTbase BERTlarge\nMNLI SQuAD v1.1 SQuAD v2.0MNLI SQuAD v1.1 SQuAD v2.0\nAcc. EM F1 EM F1 Acc. EM F1 EM F1\nData Truncation 83.72 82.72 90.00 76.06 79.18 85.80 85.51 92.18 79.56 82.57\nEmbed Pooling 84.04 82.96 90.16 76.83 79.88 85.88 85.07 91.95 80.86 83.69\nFFN Factorization83.53 82.21 89.45 75.27 78.11 85.96 85.66 92.10 79.35 82.38\nFFN Share Param.83.92 83.02 89.91 75.83 78.56 86.28 85.60 92.02 80.92 83.85\ntorization with comparable budgets (k=4 for param-\neter sharing and h=0.2D for matrix factorization).\n3.3 Depth Dimension\nTransformer growth in the depth dimension has\nbeen thoroughly discussed in literature (Gong et al.,\n2019; Li et al., 2020). Our observation in this di-\nmension is consistent with their conclusions. In\nexperiments we also compare compound growth\nwith the standard progressive stacking method.\nDiscussion. From the perspective of implementa-\ntion, compound growth introduces little additional\nengineering effort compared with progressive stack-\ning. Speciﬁcally, the growth step of progressive\nstacking basically copies the parameters of the\nsmall model to corresponding layers of the full\nmodel. The growth on the width dimension is\na similar parameter copying process for the fully\nconnected layers, while the growth on the length\ndimension removes the embedding pooling layer\nwithout changing any model parameters.\n4 Experiment\nExperiment Setups. We train the original BERT\nmodels following the same settings in (Devlin et al.,\n2019) with 256 batch size and 512-token data. All\ncompared models will ﬁnally grow to the original\nmodel, and keep the total number of training steps\nto 1M. We evaluate the ﬁnal model on the GLUE\nbenchmark (Wang et al., 2018) including 9 sub-\ntasks, and the two versions of SQuAD (Rajpurkar\net al., 2018) datasets for question answering. More\ndetailed experiment settings can be found in the\nappendix for reproduction.\nCompared Methods. Previous studies have rarely\nfocused on progressive Transformer growth for\nBERT training, and progressive Transformer stack-\ning (Gong et al., 2019) is the only directly compara-\nble method to the best of our knowledge. We apply\ntheir method on the ofﬁcial BERT model with the\nsame training setting, learning rate schedule and\nhardware as our method. We set the training sched-\nule as 300K steps with 1⁄4 number of layers, 400K\nsteps with 1⁄2 number of layers, and 300K steps with\nthe full model.\nOur Method. For CompoundGrow, we apply treat-\nments on three dimensions for the low-cost model:\n(1) mean embedding pooling with size 2 on the\nlength dimension; (2) parameter sharing with k = 2\non FFN modules on the width dimension; (3) stack-\ning on the depth dimension. Following the same\nsetting as compared methods, we try to equally dis-\ntribute the 1M training steps. We train the model\nwith all treatments with 1⁄4 number of layers and\n1⁄2 number of layers for 200K steps respectively,\nand then stack it to full layers with treatments on\nthe width and length dimensions for another 300K\nsteps. At the last stage, we train the full model for\n300K steps, just like the compared method.\nResults. Table 2 shows the speedup of different\nmodels. We estimate the inference FLOPs for\ncompared models and get their real training time\nfrom the Tensorﬂow proﬁler 2. On the BERT-base\nmodel, stacking and CompoundGrow speeds up\npre-training by 68.7% and 107.1% respectively in\nFLOPs, 64.9% and 73.6% respectively on wall-\ntime. On the BERT-large model, stacking andCom-\npoundGrow speeds up pre-training by 70.7% and\n111.4% respectively in FLOPs, 69.7% and 82.2%\nrespectively on walltime. Though CompoundGrow\nis signiﬁcantly faster, on development sets of MNLI\nand SQuaD, the compared methods do not have\nsigniﬁcantly different ﬁnetuning performance from\nthe original BERT models.\nTable 3 shows the test performance on the GLUE\nbenchmark. Both compared methods achieve at\nleast the same performance as the original BERT\nmodel. While CompoundGrow saves more training\ntime, it achieves the same performance with stack-\ning on the large model. On the base model, stacking\nis better in terms of average GLUE score, mainly\n2https://www.tensorﬂow.org/guide/proﬁler\n5178\nTable 2: The pre-training speedup and ﬁnetuning performance on dev sets of MNLI and SQuaD. M/MM stands\nfor matched/mismatched accuracy for MNLI. EM/F1 represents exact match score and F1 score for SQuaD. The\nFLOPs are estimated for forward pass operations, while the walltime is real training time proﬁled by the Tensor-\nFlow proﬁler from a distributed multi-host setting.\nspeedup speedup MNLI Acc.SQuAD v1.1SQuAD v2.0(FLOPs)(walltime) M MM EM F1 EM F1\nBERTBASE – – 84.4 84.4 83.3 90.2 77.4 80.4StackBASE +68.7% +64.9% 84.5 84.9 83.5 90.5 77.1 80.3CompoundBASE +107.1% +73.6% 84.7 84.7 83.8 90.3 77.0 80.0\nBERTLARGE – – 86.3 86.4 86.2 92.7 81.0 84.3StackLARGE +70.7% +69.7% 86.9 87.3 86.3 92.6 81.7 84.7CompoundLARGE +111.4% +82.2% 87.3 86.8 85.8 92.4 82.4 85.3\nTable 3: The test performance on the GLUE benchmark with metrics described in the original paper (Wang et al.,\n2018), the higher the better. Compound stands for the proposed method with speedup shown in Table 2.\nCoLA SST-2 MRPC SST-B QQP MNLI-m/mm QNLI RTE WNLI GLUE\nBERTBASE 52.1 93.5 88.9/84.8 87.1/85.8 71.2/89.2 84.6/83.4 90.5 66.4 65.1 78.3\nStackBASE 57.3 92.8 89.4/85.6 85.4/84.1 71.0/89.1 84.7/83.5 91.4 69.9 63.7 79.1\nCompoundBASE 50.1 92.6 89.1/85.2 85.4/83.9 70.9/88.9 84.6/83.6 91.3 70.1 65.1 78.3\nBERTLARGE 60.5 94.9 89.3/85.4 87.6/86.5 72.1/89.3 86.7/85.9 92.7 70.1 65.1 80.5\nStackLARGE 62.2 94.3 89.9/85.9 86.0/85.0 71.2/88.9 86.9/86.3 93.0 75.2 65.1 81.1\nCompoundLARGE 61.2 94.2 90.2/86.7 86.4/85.7 71.4/89.2 87.2/86.1 93.6 73.3 65.8 81.1\ndue to its advantage on the CoLA dataset. Such\nan unusual gap on CoLA might be caused by its\nrelatively small volume and corresponding random\nvariance (Dodge et al., 2020). On the larger and\nmore robust MNLI dataset, the compared methods\nachieve almost the same score.\n5 Related Work\nProgressive training was originally proposed to im-\nprove training stability, which starts from an efﬁ-\ncient and small model and gradually increase the\nmodel capacity (Simonyan and Zisserman, 2014).\nRecent study leverages this paradigm to accel-\nerate model training. For example, multi-level\nresidual network (Chang et al., 2018) explores\nthe possibility of augmenting network depth in\na dynamic system of view and transforms each\nlayer into two subsequent layers. AutoGrow (Wen\net al., 2020) attempts to automate the discover\nof proper depth to achieve near-optimal perfor-\nmance on different datasets. LipGrow (Dong et al.,\n2020) proposes a learning algorithm with an au-\ntomatic growing scheduler for convolution nets.\nAt the same time, many studies have been con-\nducted on the model growing operators. Network\nMorphism (Wei et al., 2016, 2017) manages to\ngrow a layer to multiple layers with the represented\nfunction intact. Net2net (Chen et al., 2015) is a\nsuccessful application to transfer knowledge to a\nwider network with function-preserving initializa-\ntion. Similar ideas can be discovered in many net-\nwork architectures, including progressive growing\nof GAN (Karras et al., 2017) and Adaptive Compu-\ntation Time (Graves, 2016; Jernite et al., 2016).\nAs large-scale pre-training keeps advancing the\nstate-of-the-art (Devlin et al., 2019; Radford, 2018),\ntheir overwhelming computational consumption be-\ncomes the major burden towards further developing\nmore powerful models (Brown et al., 2020). Prelim-\ninary application of progressive training has been\nmade on Transformer pre-training. (Devlin et al.,\n2019) designs two-stage training with a reduced\nsequence length for the ﬁrst 90% of updates. (Gong\net al., 2019) stack shallow model trained weights to\ninitialize a deeper model, which grows the BERT-\nbase model on the depth dimension and achieves\n25% shorter training time.\n6 Conclusion\nIn this work we empirically verify the importance\nof balancing different dimensions in Transformer\ngrowth and propose compound growth operators,\nwhich integrates operators for more than one di-\nmension. Moreover, we conduct controlled exper-\niments on various design choices of growth oper-\nators, which provides a practical guidance to al-\ngorithm design. Our ﬁnal model speeds up the\ntraining of the BERT-base and BERT-large mod-\nels by 73.6% and 82.2% in walltime respectively\nwhile achieving comparable performance.\n5179\nReferences\nT. Brown, B. Mann, Nick Ryder, Melanie Sub-\nbiah, J. Kaplan, P. Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, G. Krüger, Tom\nHenighan, R. Child, Aditya Ramesh, D. Ziegler, Jef-\nfrey Wu, Clemens Winter, Christopher Hesse, Mark\nChen, E. Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, J. Clark, Christopher Berner, Sam Mc-\nCandlish, A. Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In NIPS.\nBo Chang, Lili Meng, Eldad Haber, Frederick Tung,\nand David Begert. 2018. Multi-level residual net-\nworks from dynamical systems view. In ICLR.\nChen Chen, Xianzhi Du, Le Hou, Jaeyoun Kim, Peng-\nchong Jin, Jing Li, Yeqing Li, Abdullah Rashwan,\nand Hongkun Yu. 2020. Tensorﬂow ofﬁcial model\ngarden.\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens.\n2015. Net2net: Accelerating learning via knowl-\nedge transfer. In ICLR.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. arXiv\npreprint arXiv:2006.03236.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo\nShang. 2020. Towards adaptive residual network\ntraining: A neural-ode perspective. In ICML.\nLinyuan Gong, D. He, Zhuohan Li, T. Qin, Liwei\nWang, and T. Liu. 2019. Efﬁcient training of bert\nby progressively stacking. In ICML.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nYacine Jernite, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2016. Variable computation\nin recurrent neural networks. arXiv preprint\narXiv:1611.06188.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko\nLehtinen. 2017. Progressive growing of gans for im-\nproved quality, stability, and variation. In ICLR.\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du,\nTong Xiao, Huizhen Wang, and Jingbo Zhu. 2020.\nShallow-to-deep training for neural machine transla-\ntion. In EMNLP 2020.\nLiyuan Liu, Haoming Jiang, Pengcheng He, W. Chen,\nXiaodong Liu, Jianfeng Gao, and J. Han. 2020a. On\nthe variance of the adaptive learning rate and beyond.\nIn ICLR.\nLiyuan Liu, X. Liu, Jianfeng Gao, Weizhu Chen, and\nJ. Han. 2020b. Understanding the difﬁculty of train-\ning transformers. In EMNLP.\nLiyuan Liu, Jingbo Shang, Xiang Ren,\nFrank Fangzheng Xu, Huan Gui, Jian Peng,\nand Jiawei Han. 2018. Empower sequence labeling\nwith task-aware neural language model. In AAAI.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT.\nA. Radford. 2018. Improving language understanding\nby generative pre-training.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In ACL.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. In ICLR.\nMingxing Tan and Quoc Le. 2019. Efﬁcientnet: Re-\nthinking model scaling for convolutional neural net-\nworks. In ICML.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, L. Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need.\nIn NIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In ICLR.\nTao Wei, Changhu Wang, and Chang Wen Chen. 2017.\nModularized morphing of neural networks. arXiv\npreprint arXiv:1701.03281.\nTao Wei, Changhu Wang, Yong Rui, and Chang Wen\nChen. 2016. Network morphism. In ICML.\nWei Wen, Feng Yan, Yiran Chen, and Hai Li. 2020. Au-\ntogrow: Automatic layer growing in deep convolu-\ntional networks. In KDD.\nChao-Yuan Wu, Ross Girshick, Kaiming He, Christoph\nFeichtenhofer, and Philipp Krahenbuhl. 2020. A\nmultigrid method for efﬁciently training video mod-\nels. In CVPR.\n5180\n120 130 140 150 160 170\nAverage Time Per Step (ms)\n84.0\n84.5\n85.0SQuaDv2.0 F1 (%)\ncompound\nstacking\nFigure 2: Compare the speed-performance trade-off\nof stacking and CompoundGrow on BERTlarge. The\nthree data points in each curve is generated with\n300K/500K/700K low-cost training steps, respectively.\nA Experiment Details\nAll our models are implemented based on the Ten-\nsorFlow implementation3 of BERT (Chen et al.,\n2020) and trained on TPU v3 with 64 chips. We\nkeep the original WordPieceTokenizer and original\nposition embeddings (instead of relative position\nencoding used in (Dai et al., 2020)). Following\n(Devlin et al., 2019), we use the English Wikipedia\ncorpus and the BookCorpus for pre-training. For\neach ﬁnetuning task, we search hyperparameters\nfrom following candidates: batch size=16/32/64,\nlearning rate=3e-4/1e-4/5e-5/3e-5.\nOptimization. The original BERT models use the\nAdamW (Loshchilov and Hutter, 2019) optimizer\nwith learning rate decay from 0.0001 to 0 and 10K\nsteps of warmup (Liu et al., 2020a). At the start of\neach progressive training stage, the learning rate is\nreset to 0.0001 and keeps decaying as the original\nschedule.\nBaseline Implementation. We apply the com-\npared stacking method (Gong et al., 2019) on the\nofﬁcial BERT model with the same training setting,\nlearning rate schedule and hardware as our method,\nand achieves better performance than the reported\nnumbers in the original paper. To further unleash\nthe potential of the compared method, we adjust\ntheir original training schedule to 300K steps with\n1⁄4 number of layers, 400K steps with 1⁄2 number of\nlayers, and 300K steps with the full model. The\nnew training schedule is much faster than the re-\nported one (speedup from the reported +25% to\n+64.9%) and still gives better ﬁnal performance\nthan the original paper. This is the fastest stacking\nmodel we can get without performance drop.\n3https://github.com/tensorflow/models/\nblob/master/official/nlp/modeling/\nmodels/bert_pretrainer.py\nB Further Comparison Between\nCompoundGrow and Stacking\nTo have a deeper understanding of the compared\nmethods, we study their speed-performance trade-\noff by adjusting the training schedule. Speciﬁcally,\neach time we reduce 200K low-cost training steps\nfor both models, and compare their validation F1\nscore on SQuaDv2.0. As Figure 2 shows, Com-\npoundGrow has clear performance advantage when\ngiven comparable training budgets, which further\nveriﬁes our hypothesis."
}