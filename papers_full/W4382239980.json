{
  "title": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs",
  "url": "https://openalex.org/W4382239980",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2162085460",
      "name": "Harshit Joshi",
      "affiliations": [
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A4382259778",
      "name": "José Cambronero Sanchez",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A310804771",
      "name": "Sumit Gulwani",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1996469908",
      "name": "Vu Le",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2174407597",
      "name": "Gust Verbruggen",
      "affiliations": [
        "Microsoft (Belgium)"
      ]
    },
    {
      "id": "https://openalex.org/A2230408538",
      "name": "Ivan Radiček",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2174407597",
      "name": "Gust Verbruggen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230408538",
      "name": "Ivan Radiček",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6679740464",
    "https://openalex.org/W4288055372",
    "https://openalex.org/W6601206041",
    "https://openalex.org/W3166979522",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4221156404",
    "https://openalex.org/W2151497118",
    "https://openalex.org/W2909375883",
    "https://openalex.org/W3004804983",
    "https://openalex.org/W2769780037",
    "https://openalex.org/W6802905623",
    "https://openalex.org/W2605202003",
    "https://openalex.org/W3173687593",
    "https://openalex.org/W6734897383",
    "https://openalex.org/W3084918652",
    "https://openalex.org/W6654366121",
    "https://openalex.org/W6761586423",
    "https://openalex.org/W2729177083",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W2911684737",
    "https://openalex.org/W6666693235",
    "https://openalex.org/W6648982606",
    "https://openalex.org/W2889849786",
    "https://openalex.org/W1986204096",
    "https://openalex.org/W3027453785",
    "https://openalex.org/W3169504754",
    "https://openalex.org/W6668463283",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W4281611258",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4307887045",
    "https://openalex.org/W4244452926",
    "https://openalex.org/W4281944434",
    "https://openalex.org/W2063387237",
    "https://openalex.org/W2474318526",
    "https://openalex.org/W2933254221",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4283794478",
    "https://openalex.org/W4306294724",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3208407575",
    "https://openalex.org/W2998011150",
    "https://openalex.org/W4226305955",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W4244051999",
    "https://openalex.org/W4232704105",
    "https://openalex.org/W2133068784",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages.",
  "full_text": "Repair Is Nearly Generation: Multilingual Program Repair with LLMs\nHarshit Joshi1, Jos´e Cambronero Sanchez2*, Sumit Gulwani2*,\nVu Le2*, Ivan Radiˇcek3*, Gust Verbruggen4*\n1 Microsoft, India\n2 Microsoft, USA\n3 Microsoft, Croatia\n4 Microsoft, Belgium\n{t-hjoshi, jcambronero, sumitg, levu, ivradice, gverbruggen}@microsoft.com\nAbstract\nMost programmers make mistakes when writing code. Some\nof these mistakes are small and require few edits to the original\nprogram – a class of errors recently termed last mile mistakes.\nThese errors break the flow for experienced developers and can\nstump novice programmers. Existing automated repair tech-\nniques targeting this class of errors are language-specific and\ndo not easily carry over to new languages. Transferring sym-\nbolic approaches requires substantial engineering and neural\napproaches require data and retraining. We introduce RING , a\nmultilingual repair engine powered by a large language model\ntrained on code (LLMC) such as Codex. Such a multilingual\nengine enables a flipped model for programming assistance,\none where the programmer writes code and the AI assistance\nsuggests fixes, compared to traditional code suggestion tech-\nnology. Taking inspiration from the way programmers man-\nually fix bugs, we show that a prompt-based strategy that\nconceptualizes repair as localization, transformation, and can-\ndidate ranking, can successfully repair programs in multiple\nlanguages with minimal effort. We present the first results for\nsuch a multilingual repair engine by evaluating on 6 different\nlanguages and comparing performance to language-specific\nrepair engines. We show that RING can outperform language-\nspecific repair engines for three of these languages.\nIntroduction\nThe number of people writing code across different languages\nhas steadily grown (Bureau of Labor Statistics 2022) and\nranges from novices to experts. Regardless of their experi-\nence level, programmers can make mistakes when writing\ncode. Program errors can range from those that are easy\nto spot and fix, to those that require substantial application\nknowledge and may be very subtle logical bugs. Even simple\nmistakes, such as syntax errors that require a relatively small\nedit and may be apparent to a programming expert, can be\nfrustrating for novice programmers. Moreover they can slow\ndown the workflow of more experienced programmers (Wex-\nelblat 1976; Murphy et al. 2008; Altadmri and Brown 2015;\nDrosos, Guo, and Parnin 2017).\nOne way to help programmers who encounter these small\nmistakes is by using automated program repair (APR). These\n*Listed in alphabetical order\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmethods take a faulty program and a specification of cor-\nrectness as input, and return as output a fixed version of the\nprogram that conforms to the specification.\nRecent work (Bavishi et al. 2022) has introduced the term\nlast-mile repairsto broadly describe the class of repairs where\nthe original program is a small edit distance away from the\ncorrect program. In this definition, program correctness can\nbe checked without substantial additional context—a parser\nand a type checker suffice. A quick search on most program-\nming help forums reveals a large number of questions for\nsuch errors. For example, as of August 2022, there are over\n15K posts on StackOverflow tagged with Python and Syntax-\nError.\nExisting work has explored performing these kind of\nrepairs automatically. Symbolic systems, such as Grm-\ntools (Diekmann and Tratt 2020), typically build on error-\nrecovery mechanisms in parsers to enumerate local edits that\ncan resolve errors raised during parsing. Symbolic systems\ntypically restrict the search space to avoid state explosions\nand they cannot easily encode properties such as the likeli-\nhood of particular repair candidates being correct or not.\nMore recently, neural approaches have been successfully\napplied to repairing syntax and diagnostics errors. For exam-\nple, Dr. Repair (Yasunaga and Liang 2020), BIFI (Yasunaga\nand Liang 2021), and TFix (Berabi et al. 2021) use trans-\nformer architectures to produce repairs for C compilation\nerrors, Python syntax errors, and JavaScript linter diagnos-\ntics, respectively. Some systems, such as LaMirage (Bavishi\net al. 2022), have also combined symbolic and neural com-\nponents to successfully repair broken programs in low-code\nlanguages such as Excel and Power Fx.\nUnfortunately, all these systems share a key drawback: they\nrequire substantial engineering (symbolic) or additional data\nand training (neural) to adapt to new languages. In this pa-\nper, we propose a single repair engine, that leverages a large\nlanguage model trained on code (LLMC) to perform multi-\nlingual repair. We select Codex by OpenAI as the LLMC.\nOur system, RING , shows that repair is nearly generation\nand exploits Codex’s few-shot learning capabilities (Bareiß\net al. 2022; Drori et al. 2022) to perform multilingual program\nrepair. To do this effectively, we break down program repair\ninto the same three phases as symbolic automated program\nrepair systems: fault localization, code transformation, and\ncandidate ranking (Goues, Pradel, and Roychoudhury 2019;\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n5131\nLiu et al. 2021; Bavishi et al. 2022). We show how each stage\ncan be addressed with minimal effort by emulating what a\ndeveloper would do and using this intuition to design prompts\nfor an LLMC.\nWe evaluate RING on six languages: Excel, Power Fx,\nPython, JavaScript, C and PowerShell. Our results show that\nRING repairs significantly more programs than a language-\nspecific repair engine for three languages and shows com-\npetitive results for another two languages. We evaluate the\neffectiveness of our design choices for each of the three stages\nof repair. Additionally, we identify possible directions for\nimprovement based on our results, such as language-specific\nranking and iterative querying with Codex.\nJointly, these results provide the first evidence that an\nLLMC can enable multilingual repair with the same or better\nperformance than methods designed for a single language. In\ncontrast to other AI-assisted code editing features, such as\ncode completion, this advance opens up the possibility of a\nflipped interaction model where the user writes code and the\nAI assistant performs the fixing.\nIn summary, we make the following contributions:\n• We present an LLMC-based approach to multilingual\nrepair that enables a flipped interaction model for AI-\nassisted programming in which the user writes code and\nthe assistant suggests fixes for last-mile mistakes.\n• We implement our approach in the RING system, which\nemploys compiler (or diagnostic) messages, smart few-\nshot selection, and ranking of repair candidates to perform\nrepair across varying languages.\n• We perform an extensive evaluation across six different\nlanguages, showing that multilingual repair with LLMCs\nis viable and can compete with or outperform language-\nspecific repair engines.\n• We introduce PowerShell commands as a new application\nfor last-mile repair and collect a benchmark set of 200\nPowerShell commands from StackOverflow, which we\nalso release for future research1.\nRelated Work\nAutomated Program Repair Finding and fixing bugs is\nchallenging and tedious, even for language experts (Zhong\nand Su 2015). The software engineering community has built\nAutomated Program Repair (APR) tools (Arcuri 2008) to\nreduce the time and costs associated with debugging. The\npremise of APR has since grown into a substantial research\ndomain across different languages, classes of bugs, and use\ncases (Gazzola, Micucci, and Mariani 2019).\nEarly approaches for APR were symbolic and attempted to\nfix programs automatically by enumerating repair candidates\nfrom templates (Debroy and Wong 2010), crafting heuris-\ntics (Qi et al. 2014), and using program synthesis (Nguyen\net al. 2013). Although these systems can provide strong guar-\nantees for the generated code, they are strongly tied to their\ndomain language. Moreover, symbolic systems are restrictive\nin their scope, failing to repair programs that the correspond-\ning language compiler cannot process to at least some extent.\n1https://github.com/microsoft/prose-benchmarks/\nOn the other hand, building on the recent advances in natural\nlanguage processing, neural methods have shown promise in\nlearning program repairs. Researchers have studied automati-\ncally correcting programs in different settings, including in-\ntroductory programming assignments (Pu et al. 2016; Parihar\net al. 2017; Ahmed et al. 2018). For example, DeepFix (Gupta\net al. 2017) and SampleFix (Hajipour, Bhattacharyya, and\nFritz 2020) use sequence-based deep learning models to fix\nbroken C code written by students. However, these neural\nmodels are not as powerful as LLMCs like Codex.\nSynFix (Ahmed, Ledesma, and Devanbu 2021), Dr. Re-\npair (Yasunaga and Liang 2020), and TFix (Berabi et al. 2021)\nleverage compiler diagnostics for Java, C, and JavaScript, re-\nspectively, but require a substantial amount of training and\ndata, failing to generalize across languages. Although (Bav-\nishi et al. 2022) tries to bridge the gap between neural and\nsymbolic approaches, their approach requires language spe-\ncialization (symbolic parser) and large-scale data (neural lo-\ncalizer and ranker). In contrast, RING uses a powerful LLMC,\nCodex, capable of generating multilingual code while guiding\nrepair through readily available prompt-design strategies.\nLarge Language Models The advent of Large Language\nModels (LLM) trained on code and natural language shows\npromise for code understanding and generation results. Au-\ntoregressive models (Shannon 1948; Radford et al. 2018),\nsuch as Codex (Chen et al. 2021), are trained to predict the\nnext token, given the past token context, over enormous cor-\npora. However, training LLMs is technically challenging and\nexpensive. They require a large dataset for each fine-tuning\ntask and parallel training on multiple GPUs (Bommasani et al.\n2021) to scale. An exciting aspect of LLMs is the zero-shot\nand few-shot learning paradigm for adapting to tasks on-\nthe-fly (Brown et al. 2020; Chowdhury, Zhuang, and Wang\n2022).\nPrenner and Robbes (2021) evaluate Codex’s ability to\nfix 80 bugs in Python and Java using such in-context learn-\ning abilities. They manually provide buggy lines for each\nprogram and use fixed few shot examples in the prompt. In\ncontrast, our paper discusses various strategies to build the\nprompt, and performs a more extensive study with larger\ndatasets over more languages.\nApproach\nFigure 1 shows the architecture of RING . We divide the task\nof fixing bugs into three stages: fault localization, program\ntransformation and candidate ranking. Each stage is based\non the intuition for how developers might approach such a\nstage manually. In the following subsections, we show how\nto address each stage using an LLMC.\nWe illustrate our approach using a running example –\nshown in Figure 2 – drawn from the BIFI (Yasunaga and\nLiang 2021) dataset. The user has incorrectly used tuple no-\ntation in the function signature (highlighted in pink). This\nsyntax for unpacking tuples in function signatures was sup-\nported in Python 2. In Python 3, it raises a syntax error2 with\nvery little detail on the underlying issue. This example high-\n2https://peps.python.org/pep-3113/\n5132\nfunction Driver(opts) { \n  Driver.__super__.constructor.apply(\n ...\n  this.messages = []; \n}\nBuggy Javascript Code\nRING \n(LLMC Repair)\nProcessed Compiler Error\nMessage: Unused variable.\nError in line: 1, span starts 47.\nExample\nSelection\n#### Fix ES Lint Errors in Javascript  \n### Buggy Javascript \nfunction(req, res, next) { \n      ...\n    }\nError: (1) Unused variable. \nError in line: 1, span starts 34.\n### Fixed Javascript \nfunction(req, res) { \n      ...\n    }\n### Buggy Javascript\n<Buggy Javascript Code>\nError: Unused variable. Error\n in line: 1, span starts 47\n### Fixed Javascript\nGenerated Prompt\nfunction Driver(opts) { \n Driver.__super__.constructor.apply(\n...\n this.messages = []; \n}\nFixed Javascript Completion Pass@1\nLLMC \n(OpenAI Codex)\nLog Prob Ranking\nPrompt Generation\nFault Localization\nCode Transformation\nCandidate Ranking\nThree Pillars of APR\nDeleted Code\nFigure 1: RING , powered by a Large Language Model trained on Code (LLMC), performs multi-lingual program repair. RING\nobtains fault localization information from error messages and leverages LLMC’s few shot capabilities for code transformation\nthrough example selection, forming the prompt. Finally, a simple, yet effective, technique is used for ranking repair candidates.\n1 def boundary_difference_power(graph,\n(orig image, sigma, spacing) ):\n2 orig_image = scipy.asarray(orig_image)\n3 def boundary_term_division(i):\n4 i = 1. /(i + 1)\n5 i = scipy.power(i, sigma)\n6 i[i <= 0] = sys.float_info.min\n7 return i\n8 __skeleton_difference(graph,\n9 orig_image,\n10 boundary_term_division)\nFigure 2: A real Python 3 syntax error from the BIFI dataset.\nThe highlighted code uses tuple parameter unpacking syntax,\nwhich was valid in Python 2 but removed from Python 3. All\nlistings are simplified for presentation clarity and brevity.\nlights that errors can also be introduced as languages evolve.\nRING fixes this mistake without additional user intervention.\nFault Localization through Language Tooling\nAs a first step towards debugging, a programmer typically\nlocates the cause of the bug. For most modern languages,\nlocating syntactic mistakes and some semantic errors, such as\ntype errors, is aided by tools like the compiler, static analyz-\ners, or linters. Following this intuition, we include a prepro-\ncessed error message produced by the compiler or other static\nanalyzers. We normalize this message to enforce consistency\nacross languages. Figure 3 shows this prompt variant for our\nrunning example, where the highlighting corresponds to our\nprepared syntax error message. For languages where the error\nmessaging may not be precise, particularly with regards to\nthe error location reported, we found that a simple abstrac-\ntion that removes the reported error location but preserves\nthe error text worked well – we discuss how to create such\nan abstracted message in our discussion section.\n1 ### Buggy Python\n2 def boundary_difference_power(graph,\n3 (orig_image, sigma, spacing)):\n4 ...\n5 Error: (1) invalid syntax. Error in\n6 line: 2 span starts 4 and ends 32.\nFigure 3: To aid fault localization, we include a detailed\ncompiler error message with line/column span information.\nWe prepare uniform messages across languages by extracting\ndetails from the corresponding language compiler/analyzer.\nCode Transformation through Few-shot Learning\nOnce a developer has identified the location of a mistake, they\nmust now apply an appropriate transformation—a sequence\nof edits—to the original source code at this location. Most de-\nvelopers accumulate experience in the type of transformations\nneeded to resolve particular errors over time. Additionally,\nwhen novices encounter an unfamiliar mistake, they often\nsearch for examples of similar buggy/correct pairs that can\ninform their own transformation.\nIt has been shown that LLMs are capable of few-shot\nlearning—the ability to learn from a few examples of the\nintended task—by adding related examples of the task to\nthe prompt (Brown et al. 2020; Poesia et al. 2022). Given\nexamples of transformations that repair programs, we exploit\nthis capability in RING to address the code transformation\nstage. The main challenge is selecting relevant examples that\nare related to the mistake made by the developer.\nFollowing the intuition that programs with similar mistakes\nhave similar fixes, we select examples from a collection of\nbuggy-fixed pairs based on error message similarity. We call\nthis collection of buggy-fixed pairs the example bank.\nTo capture differences in language tooling, we implement\ntwo methods for selecting programs from our example bank.\nThe key difference between these two methods is how they\ncompute a similarity metric over error diagnostics.\nThe first variant, error vector selection, assumes that fine-\n5133\ngrained error reporting is available. For example, the Excel\nparser returns a detailed report with many different diagnos-\ntic counters. We count the occurrence of each error category\nreported by the tool and construct a vector out of these fre-\nquencies – we refer to this as an error vector. We then select\nprograms from the example bank by minimizing the L2 dis-\ntance between error vectors.\nThe second variant,message embedding selection, assumes\nthat high-level errors are accompanied by detailed descrip-\ntions in natural language. For example, the Python parser\noften returns the same error (like SyntaxError) for different\nmistakes and instead exposes additional information through\nthe associated natural language error message. We use this\ndescription by embedding the compiler messages with a pre-\ntrained CodeBert (Feng et al. 2020) model and comparing\nembeddings based on cosine similarity.\nFigure 4 shows a simplified few-shot prompt with an ex-\nample, chosen using message embedding, which exhibits the\nsame error (and required fix) as our buggy program. With\nthis prompt, RING ’s top candidate is the right repair.\n1 ### Buggy Python\n2 def initial solution(self, start,\n3 (max shares, desired weight) ):\n4 ...\n5 Error: (1) invalid syntax. Error in line\n: 3, span starts 35 and ends: 36.\n6 ### Fixed Python\n7 def initial solution(self, start,\n8 max shares, desired weight ):\n9 ...\nFigure 4: Our smart selection of few-shots retrieves rele-\nvant buggy-fix examples from an example bank. Shots are\nretrieved based on a similarity metric over error diagnos-\ntics. The shot selected (pink background) displays the same\ninvalid signature-level tuple parameter unpacking (dark red\nbackground, bold) as our target program. The fixed portion of\nthe shot (green background, bold) removes the parentheses.\nCandidate Ranking\nLLMs achieve variation in their output by iteratively sam-\npling each token from promising candidates. The extent to\nwhich less likely tokens can be selected is controlled by a\nparameter called temperature. We can thus generate multiple\ncandidates by controlling the temperature during generation.\nThe final step in RING is to rank the candidates obtained\nby querying Codex using the prompt described in the prior\ntwo stages. We use a relatively simple (but effective) ranking\nstrategy to order the candidate programs: averaging the log-\nprobabilities of tokens selected during the decoding process\nand sort the candidates in descending order of their averages.\nDuring development, we found that generating various\ncandidates with higher temperatures – encouraging diverse\ncandidates – and ranking them yields better performance than\nusing lower temperatures such as zero.\nLanguage-Specific Datasets\nWe evaluate RING on six different languages, ranging from\nlow-code formula languages to popular scripting languages.\nWe describe the dataset, language-specific baseline(s) and\nevaluation metric for each language.\nExcel We use a recently released dataset of 200 Excel repair\ntasks collected from Excel help forums (Bavishi et al. 2022).\nEach task consists of an Excel formula with syntax errors,\nsome semantic errors (such as wrong function call arity)\nand a ground truth repair. We also collect a set of 73 tasks\nwhere the Excel formula contains at least one type error and\nannotated each such formula with a ground truth repair. The\nfinal collection consists of 273 Excel repair tasks.\nA successful repair exactly matches the ground truth after\nnormalizing tokens like spaces, capitalizing all the identifiers\nand cell references. We compare RING to the neurosymbolic\nrepair engine LaMirage (Bavishi et al. 2022).\nPower Fx Like Excel, we use the recently released 200\nPower Fx repair tasks accompanying LaMirage. These tasks\nconsist of syntactic and basic semantic errors, and are col-\nlected from help forums and anonymized product telemetry.\nWe use the same evaluation criteria as in Excel and com-\npare to the neurosymbolic repair engine LaMirage.\nPython We evaluate RING on a random sample of 200 syn-\ntactically invalid Python code snippets from the dataset used\nby the SOTA syntax repair tool for Python: BIFI (Yasunaga\nand Liang 2021). These code snippets were collected from\nGitHub repositories.\nThese snippets do not have a ground truth repair. Hence,\nwe employ the same evaluation metric described in the BIFI\npaper. A repair is successful if the produced program is (1)\nparsed successfully by the Python 3 parser and (2) has a\nLevenshtein (Levenshtein et al. 1966) token edit distance\nless than 5 from the buggy program. The python tokens are\ngenerated by the Pygments3 lexer.\nWe compare to BIFI, a transformer-based repair system\nthat iteratively trains a code breaker that learns to generate\nrealistic errors and a code fixer that repairs such errors.\nJavaScript We evaluate RING on a random sample of 200\nJavaScript (JS) code snippets drawn from the dataset re-\nleased with TFix (Berabi et al. 2021). Each snippet has at\nleast one error or warning reported by the popular linter\nESLint (T\n´omasd´ottir, Aniche, and Van Deursen 2018). In\naddition to syntax errors, ESLint also reports stylistic issues.\nThe dataset released by TFix contains a ground truth repair\ncode snippet for each buggy snippet. Both buggy and ground\ntruth code snippets were mined by the TFix authors from\nGitHub commits. The originally released dataset contains\nonly the part of each code snippet relevant to the error and\nrepair. However, these parts are an arbitrary window around\nthe original fault location. We found that providing these\narbitrary windows to Codex resulted in spurious edits, as the\nsnippets had syntax errors that were just an artifact of the\nwindowing. To mitigate this, we extracted the whole function\n(or whole file, if not in a function) that encompassed the\n3https://pygments.org/\n5134\noriginally buggy and the repaired code snippets. We refer to\nthese as extended code snippets.\nWe compare our performance to TFix, a fine-tuned T5 (Raf-\nfel et al. 2020) model for JS repair. A repair is successful if it\nmatches the ground truth associated with the buggy program.\nWe run TFix on both the original window snippets and on\nour extended code snippets.\nC We evaluate RING on a random sample of 200 C code\nsnippets drawn from the dataset released with DeepFix\n(Gupta et al. 2017). These programs correspond to real user\nprograms written by students in an introductory programming\nclass and raise at least one compilation error.\nWe compare to Dr. Repair, a neural repair system that uses\ngraph attention to combine information from the buggy code\nsnippet and the associated compiler message (Yasunaga and\nLiang 2020). We use their success criterion: a repair must\nnot raise any error messages when compiled using gcc -w\n-std=c99 -pedantic. Following BIFI, a repair must be\nless than 5 token edits away from the original buggy program.\nPowerShell We introduce the novel task of repairing syn-\ntax errors in PowerShell commands. To create benchmarks,\nwe searched StackOverflow (StackOverflow) for the word\n“error” in threads tagged with powershell. This resulted\nin 14,954 threads. We extracted code blocks with least one\nspace from the question and the accepted answer. We keep\npairs from question and answer where the question code is\ninvalid and answer code is valid. We judged validity using\nthe PowerShell command Get-Command -syntax.\nFinally, we manually annotated these candidate tasks from\nthe associated StackOverflow post, confirming each pair was\nreflective of the original issue and did not have extra changes.\nWhen there were changes, we manually simplified the pair\nor corrected minor issues like new line characters. We kept a\nfinal set of 200 task pairs.\nThere is no existing language-specific engine to compare\nwith, as we introduce this task. A repair is successful if it\nexactly matches the associated answer code block.\nCommon Baseline We also use zero-shot Codex as a base-\nline for all languages. We use the following prompt:\nFix bugs in the below code:\n### Buggy <language>:\n<buggy program>\n### Fixed <language>:\nwhere <language> is replaced with the appropriate lan-\nguage name for the benchmark task. For all the experiments,\nwe used ### as stop token and top p= 1.0.\nResults and Analysis\nWe first ask: (RQ1) how viable is RING ’s Codex-powered\napproach for repair across multiple languages? Next, we\ninvestigate the extent to which Codex can address each of\nour conceptual stages. For localization, (RQ2) to what extent\ncan RING perform error localization across languages? For\ncode transformation, (RQ3) to what extent does our smart\nselection of few-shots improve performance? Finally, for\ncandidate ranking, (RQ4) to what extent can RING rely on\nCodex’s token log probabilities to rank candidates?\nRQ1. Viability of Multilingual Repair\nTable 1 shows the performance for RING , language-specific\nrepair engines, and a Codex-based zero-shot baseline, across\neach of our languages. We present the best performing con-\nfiguration for each language using top@k performance met-\nrics (Inala et al. 2022; Poesia et al. 2022; Bavishi et al. 2022),\nwhere we consider the topk candidates produced by a system\nand count the task as solved if any candidate satisfies our\ncorrectness criteria.\nSmart selection is done via leave-one-out. For languages\nwith ground truth, all other tasks are the example bank for\ndrawing shots. Since the C and Python datasets do not have\nground truth pair, we sample an additional 400 programs\nfrom their corresponding datasets. We run the best RING\nconfiguration (without smart selection) on these 400 pro-\ngrams and pick those that do not raise any diagnostics error.\nThese buggy/correct pairs form the example bank in C and\nPython.\nRING outperforms the state-of-the-art repair engines in\ntop@1 for Excel, Python, and C. For Power Fx, we find\nthat RING ’s top@3 rate is comparable to the top@1 rate for\nLaMirage. Furthermore, there is a substantial improvement\nin RING ’s top@3 compared to top@1.\nIn Javascript, we find that TFix applied to the original code\nsnippets obtains a top@1 rate of 0.59 (approximately 7 points\nhigher than that of RING ). However, applying TFix to the\nextended code snippets results in a much lower top@1 rate of\n0.09. This performance degradation can be attributed to the\nsubstantially longer sequences of the extended code snippets\ncompared to the original code snippets, an average of 208\nand 74 T5 tokens, respectively.\nIn PowerShell (PS), we observe thatRING ’s performance is\nsubstantially lower compared to other languages. We hypoth-\nesize that this may be a reflection of the (presumed) relative\nscarcity of PS commands in Codex’s training data. Manual\ninspection of failures also revealed that RING performs fewer\nedits than required to match the ground truth.\nGiven this evidence, we conclude that RING ’s Codex-\npowered approach can perform multilingual repair. We can\ncontrast this to the substantial effort required to build a\nlanguage-specific repair engine. TFix, BIFI, and Dr. Repair\nwere trained on 108K, 3M and 1.5M JavaScript, Python,\nand C code snippets, respectively. LaMirage trained error\nlocalizers and rankers based on pointer networks, as well as\nimplemented multiple language-specific rules.\nPrograms that were not fixed by either RING or the\nlanguage-specific engines shared some properties. In par-\nticular, some of these could be addressed with a combina-\ntion of iteratively querying Codex and explicit lightweight\nconstraints that enforce language-specific knowledge. For\nexample, we found a Python program that has two issues:\nan invalid use of the reserved keyword async and a missing\nparenthesis. For the keyword issue, we could query Codex\nwith the buggy program up to the invalid keyword usage,\nvalidate that the following token predicted is not a reserved\nkeyword, and then query Codex again with the modified\nbuggy code fragment. This is similar to constrained decoding\nused in Synchromesh (Poesia et al. 2022).\n5135\nLanguage Approach Top@1 Top@3 Top@50 * Metric Avg. Tokens\nExcel\nRING (Abstracted Message, Error Vector) 0.82 0.89 0.92\nExact Match 26 ±14LaMirage (Bavishi et al. 2022) 0.71 0.76 -\nCodex (Chen et al. 2021) 0.60 0.77 0.88\nPower Fx\nRING (Compiler Message, Message Embedding) 0.71 0.85 0.87\nExact Match 29 ±19LaMirage (Bavishi et al. 2022) 0.85 0.88 -\nCodex (Chen et al. 2021) 0.47 0.68 0.84\nJavascript\nRING (Compiler Message, Error Vector) 0.46 0.59 0.64\nExact Match 163 ±106TFix (extended code snippets) (Berabi et al. 2021) 0.09 - -\nTFix (original dataset) (Berabi et al. 2021) 0.59 - -\nCodex (Chen et al. 2021) 0.19 0.28 0.39\nPython\nRING (Compiler Message, Message Embedding) 0.94 0.97 0.97 Passes Parser\nEdit Distance < 5 104 ±150BIFI (Yasunaga and Liang 2021) 0.92 0.95 0.96\nCodex (Chen et al. 2021) 0.87 0.94 0.98\nC\nRING (Compiler Message, Message Embedding) 0.63 0.69 0.70 Passes Parser\nEdit Distance < 5 223 ±72Dr Repair (Yasunaga and Liang 2020) 0.55 - -\nCodex (Chen et al. 2021) 0.40 0.56 0.61\nPowershell RING (Compiler Message, Message Embedding) 0.18 0.25 0.28 Exact Match 24 ±30Codex (Chen et al. 2021) 0.10 0.15 0.18\nTable 1: Comparison of RING with language-specific approaches and a zero-shot baseline that uses Codex. Bold denotes best\nperformance for each language. *For Powershell we compute Top@20, due to rate limiting restrictions. All RING experiments\nare at 0.7 temperature. RING can outperform language-specific repair engines in Excel, Python, and C. In Javascript, RING is\ncapable of generating the right repair but ranking needs to improve. In Powershell, with no existing baseline,RING performs\nsubstantially worse – likely reflective of the lack of Powershell code in Codex’s training data. We ran all Codex-related queries\non August 9th 2022 using Open AI’s public API for “davinci-code-002”, with the exception of Powershell experiments which we\nran on March 7th 2023.\nRQ2. Error Localization\nEven if RING cannot fix a program at top@1, locating the\nerror can help users. We carry out the following experiment\nfor the four languages which have the ground truth. We con-\nsider programs that are not repaired at top@1 by RING and\nthose that are not repaired at top@1 by the language-specific\nbaseline. For each such program, we take the top candidate\nproduced by each system and compare the edit locations to\nthe ground truth edit locations. If the candidate edit loca-\ntions are all within a range of ±k tokens of the ground truth\nlocations, we mark this as a correct localization.\nFigure 5 summarizes our results. We observe that RING\ncorrectly locates a larger fraction of required edits compared\nto the language-specific baselines. This holds true across the\nfour languages with ground truth repairs. RING ’s localization\nsuccess varies by language but can reach as high as over\na quarter of unrepaired programs (for Power Fx, given a\ntolerance of one token). For such programs, where RING\ncan localize the error but does not perform the correct edit,\ndrawing shots from a larger example bank may help.\nNext, we explored a key contributing factor to overall re-\npair success (and localization in particular): program length.\nWe found that for most languages, the buggy programs that\nRING can repair tend to be shorter than the buggy programs\nit fails to repair. Figure 6 shows the cumulative fraction of\n0 ±1 ±2 ±3 ±4\n0\n0.1\n0.2\n0.3\nLocation Range (±k tokens)\nCorrect Localization\nfrac.\nRING\nExcel\nPower Fx\nJavaScript\nPowerShell\nBaselines\nExcel\nPower Fx\nJavaScript\nFigure 5: We consider separately the programs not repaired\nat top@1 by RING and language-specific baselines. We com-\npute an approximate error localization metric, which marks\nas correctly localized any edit that is within k tokens of\nthe groundtruth edit location. When RING fails to repair a\nprogram it correctly localizes a larger fraction of programs\ncompared to the language-specific baselines.\nbuggy programs by their length, grouped based on their out-\ncome (top@1). In both JavaScript and Python, the programs\nsuccessfully repaired by RING tend to be shorter than those\nwhere it fails. Interestingly, this relationship does not seem\n5136\n0 100 200 300 400 500\n0\n0.2\n0.4\n0.6\n0.8\n1\nNumber of tokens\nCumulativ\ne Frac.\nTrue\nFalse\nExact Matches@1\n(a) Javascript\n0 500 1 ,000 1 ,500 2 ,000 2 ,500\n0\n0.2\n0.4\n0.6\n0.8\n1\nNumber of tokens\nCumulativ\ne Frac.\nTrue\nFalse\nPasses Critic@1\n(b) Python\nFigure 6: Cumulative fraction of programs by number of\ntokens in the original buggy program, grouped by whether\nRING can repair at top@1. Successful repairs tend to be\nassociated with shorter buggy programs.\nto hold as strongly for Excel. We attribute this behaviour to\noverall shorter programs lengths and the restrictive Excel\ngrammar.\nRQ3. Code Transformation\nTable 2 shows the top@1 rate with our smart selection of\nfew-shots for the prompt, compared to a strategy that uses\npre-defined fixed examples. The pre-defined strategy allows\nus to curate high-quality repair examples for common er-\nrors, but these may not be relevant for all programs. Prior\nwork (Prenner and Robbes 2021) explored the use of fixed\nfew-shot examples for APR with Codex.\nOur results show that smart selection improves perfor-\nmance in all languages. This performance improvement\ncomes from examples in the prompt that reflect similar er-\nrors (and expected edits) to the target program. We use error\nvector selection for Excel and JavaScript, which have better\nand more granular error categorization, and message embed-\nding selection for other languages. We observe that Power\nFx shows the smallest performance improvement. Manual\ninspection revealed that Power Fx compiler messages tend\nto be imprecise, and using them to select examples can in-\ntroduce some noise into the prompt. An example that we\nencountered were cases where the compiler suggested there\nwas an extraneous token in the input program that did not\nactually appear in it.\nRQ4. Candidate Ranking\nRING ranks candidate repairs based on the average of per-\ntoken log probabilities produced by Codex. The effective-\nness of this strategy for our use case depends on the extent\n−0.4 −0.2 0\n0\n5\n10\n15\n20\navg. log probs\nKernel\nDensity\nTrue\nFalse\n(a) Excel\n-0.06 -0.03 0\n0\n20\n40\n60\navg. log probs\nKernel\nDensity\nTrue\nFalse\n(b) C\n−0.2 −0.1 0\n0\n5\n10\n15\n20\navg. log probs\nKernel\nDensity\nTrue\nFalse\n(c) Power Fx\n−0.3 −0.2 −0.1 0\n0\n5\n10\n15\navg. log probs\nKernel\nDensity\nTrue\nFalse\n(d) Powershell\nFigure 7: (Gaussian) Kernel density plots for average token\nlog probabilities across languages, based on their top@1\nsuccess status. Clearer separation of distributions tends to\nbe associated with better performance (e.g., Excel, C). In\nPowershell, where RING struggles, the relationship between\ndistribution peaks is inverted relative to other languages.\nLanguage Fixed Shots Smart Shots Fractional Change\nExcel 0.76 0.82 0.08\nPower Fx 0.70 0.71 0.01\nJavascript 0.43 0.46 0.07\nPython 0.91 0.94 0.03\nC 0.50 0.58 0.16\nPowershell 0.15 0.18 0.20\nTable 2: Top@1 for few-shots selected using our smart selec-\ntion strategy, compared to pre-defined fixed examples. Smart\nselection improves performance for all languages. For Power\nFx, we see the smallest improvement, which we attribute to\nimprecise compiler diagnostics.\nto which Codex is calibrated properly for program repair\n(Bella et al. 2010; Nixon et al. 2019; Dormann 2020). Fig-\nure 7 compares average log probabilities in Excel, Power Fx,\nPowerShell, and C. We show (Gaussian) kernel density plots\nacross languages based on the top@1 outcome. For languages\nlike Excel and C, where RING outperforms language-specific\nrepair engines, there is a clearer difference in distributions.\nIn Power Fx, where RING can repair programs but does not\noutperform the language-specific engine, this distribution\ndifference is less clear. In PowerShell, where RING fails to\nrepair a substantial fraction of programs, the relationship\nbetween the peaks of the distributions is inverted relative to\nother languages.\n5137\n-0.15 -0.1 -0.05 0\n0\n20\n40\n60\nToken average log probability\nKernel\nDensity\nPowerShell Excel\nJavaScript Python\nC Power Fx\nFigure 8: Per-language (Gaussian) kernel density plots of\nsuccessful top@1 scores (average token log probabilities).\nWe find that less popular languages, like PowerShell, Excel,\nand Power Fx, have lower average scores – likely reflective\nof their relatively small fraction of Codex’s training data.\nAdditionally, we find that even for programs with a top@1\nsuccess outcome, there are differences in average token log\nprobability across languages, as shown in Figure 8. For\nless popular languages (PowerShell or Excel), the distri-\nbution peaks are further left than more popular languages\n(JavaScript). This likely reflects the underlying language dis-\ntribution in Codex’s training data.\nBased on our observation of the gap between top@1 and\ntop@5, paired with these calibration insights, we believe that\na language-specific ranking model (Inala et al. 2022) may\nprovide a substantial payoff in multilingual repair.\nDiscussion\nWe now provide discussion on the design principles involved\nin building a good example bank for few-shot selection and\nthe tasks required to adapt RING to a new language.\nDesigning the Example Bank\nWhile curating the example bank, it is essential to have dif-\nferent types of errors to facilitate retrieval of similar mis-\ntakes/fixes for a new buggy program. There are several ways\nto collect such examples, including scraping public forums,\nusing telemetry data, and bootstrapping examples through the\nlanguage knowledge of an expert. We have found that scrap-\ning public forums is a good way to start, paired with expert\ncuration of corner cases. Buggy-fixed pairs can be collected\nincrementally, adding more diverse examples from different\nsources later. Telemetry data also provides a natural source\nfor examples, but depending on the platform/organization\ncan require anonymization that might impact retrieval.\nOur evaluation employs a strict leave-one-out strategy to\nbuild an example bank from benchmark programs. In prac-\ntice, this will be a very restrictive example bank that can\npotentially limit the number of successful repairs.\nWhile the example bank sizes used during our evaluation\ndo not present a performance concern, as example banks in\nproduction grow, retrieval time may become more signifi-\ncant. To address such challenges, RING could take advan-\ntage of off-the-shelf fast indexing/retrieval systems, such as\nFAISS (Johnson, Douze, and J´egou 2019) or ANNOY (Spo-\ntify 2022).\nAdapting RING for New Languages\nWe now detail the steps required to apply RING to a new\nlanguage. The first task is to build the associated example\nbank, using the principles discussed above. Next, we need to\nevaluate the language tooling available for error diagnostics.\nIn particular, there are two key decisions: determining what\nkind of error-based few-shot selection to make and if the error\nmessage needs to be abstracted prior to use for localization\nwith RING . We discuss each of these concerns in turn.\nChoosing between Error Vector and Message Embedding\nDifferent languages have different underlying language tools,\nsuch as compilers and linters. If the underlying language\ntools provide detailed error reports with granular error cat-\negories, counting the categories can help us extract precise\nerror information. We recommend using error vector selec-\ntion for such languages. Unfortunately, not all languages\nprovide fine-grained error categories but instead expose ad-\nditional information through an associated natural language\nerror message. For such languages, which provide more in-\nformation through natural language, we recommend using\nmessage embedding selection.\nCreating abstracted error message When incorporating\nthe error message in the localization portion of the prompt\nand in message-embedding-based few shot selection, some\nlanguages may benefit from abstracting the error message to\nremove extra (and possibly imprecise) information. In our ex-\nperiments, we found that providing an error message without\nexact location information can help in low-code languages\nlike Excel. If the language tool provides data structures with\nerror description, location, and error category, we only use\nthe description. Languages with natural language error mes-\nsages typically follow a template that we can use to extract\nthe portions of the message we want to preserve to create the\nabstracted message.\nFor example, in C, the error message for a missing semi-\ncolon (;) at the end of a statement is shown below:\nIn function ’main’:\n16:6: error: expected ’;’ before ’printf’\nprintf(\\\"%d\\\",catalan(h));\nˆ\nWe split the error message using regular expression\n“\\d+:\\d+: error:”, which captures the text 16:6\nerror: and leaves us with the following abstracted error\nmessage: expected ’;’ before ’printf’.\nConclusion\nWe present RING , a multilingual repair engine powered by\nCodex. We show various prompt-based strategies designed\nto convey developer-like information to address the three\nstages of automate program repair: error localization, code\ntransformation, and candidate ranking. We evaluate RING on\nsix languages, including a benchmark for the novel task of\nrepairing Powershell programs. We show RING can perform\nwell in multiple languages, even outperforming language-\nspecific engines in some, with little engineering effort.\n5138\nAcknowledgements\nWe thank Peter Lee for the CoPilot-flipped-model analogy.\nWe thank Julia Liuson for inspiring and facilitating use of\nCodex-as-a-component in our neuro-symbolic workflows.\nWe also thank the authors of baseline systems used in our\nresearch – their sharing of models and data made this work\npossible. We would also like to thank Abishai Ebenezer for\nhelping us curate the PowerShell evaluation benchmarks.\nReferences\nAhmed, T.; Ledesma, N. R.; and Devanbu, P. 2021. SYN-\nFIX: Automatically Fixing Syntax Errors using Compiler\nDiagnostics. arXiv preprint arXiv:2104.14671.\nAhmed, U. Z.; Kumar, P.; Karkare, A.; Kar, P.; and Gulwani,\nS. 2018. Compilation error repair: for the student programs,\nfrom the student programs. In Proceedings of the 40th In-\nternational Conference on Software Engineering: Software\nEngineering Education and Training, 78–87.\nAltadmri, A.; and Brown, N. C. 2015. 37 million compila-\ntions: Investigating novice programming mistakes in large-\nscale student data. In Proceedings of the 46th ACM technical\nsymposium on computer science education, 522–527.\nArcuri, A. 2008. On the automation of fixing software bugs.\nIn Companion of the 30th international conference on Soft-\nware engineering, 1003–1006.\nBareiß, P.; Souza, B.; d’Amorim, M.; and Pradel, M. 2022.\nCode Generation Tools (Almost) for Free? A Study of Few-\nShot, Pre-Trained Language Models on Code. arXiv preprint\narXiv:2206.01335.\nBavishi, R.; Joshi, H.; Cambronero, J.; Fariha, A.; Gulwani,\nS.; Le, V .; Radiˇcek, I.; and Tiwari, A. 2022. Neurosym-\nbolic Repair for Low-Code Formula Languages. Proc. ACM\nProgram. Lang., 6(OOPSLA2).\nBella, A.; Ferri, C.; Hern ´andez-Orallo, J.; and Ram ´ırez-\nQuintana, M. J. 2010. Calibration of machine learning mod-\nels. In Handbook of Research on Machine Learning Appli-\ncations and Trends: Algorithms, Methods, and Techniques,\n128–146. IGI Global.\nBerabi, B.; He, J.; Raychev, V .; and Vechev, M. 2021. Tfix:\nLearning to fix coding errors with a text-to-text transformer.\nIn International Conference on Machine Learning, 780–791.\nPMLR.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.; Arora,\nS.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut, A.;\nBrunskill, E.; et al. 2021. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877–\n1901.\nBureau of Labor Statistics, U. 2022. Software develop-\ners, Quality Assurance Analysts, and testers : Occupational\noutlook handbook. https://www.bls.gov/ooh/computer-and-\ninformation-technology/software-developers.htm. Accessed:\n2022-07-30.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Ponde, H.; Kaplan,\nJ.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman, G.; Ray,\nA.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf, H.; Sastry, G.;\nMishkin, P.; Chan, B.; Gray, S.; Ryder, N.; Pavlov, M.; Power,\nA.; Kaiser, L.; Bavarian, M.; Winter, C.; Tillet, P.; Such, F. P.;\nCummings, D. W.; Plappert, M.; Chantzis, F.; Barnes, E.;\nHerbert-V oss, A.; Guss, W. H.; Nichol, A.; Babuschkin, I.;\nBalaji, S. A.; Jain, S.; Carr, A.; Leike, J.; Achiam, J.; Misra,\nV .; Morikawa, E.; Radford, A.; Knight, M. M.; Brundage, M.;\nMurati, M.; Mayer, K.; Welinder, P.; McGrew, B.; Amodei,\nD.; McCandlish, S.; Sutskever, I.; and Zaremba, W. 2021.\nEvaluating Large Language Models Trained on Code. ArXiv,\nabs/2107.03374.\nChowdhury, J. R.; Zhuang, Y .; and Wang, S. 2022. Novelty\nControlled Paraphrase Generation with Retrieval Augmented\nConditional Prompt Tuning. Proceedings of the AAAI Con-\nference on Artificial Intelligence, 36(10): 10535–10544.\nDebroy, V .; and Wong, W. E. 2010. Using Mutation to Au-\ntomatically Suggest Fixes for Faulty Programs. 2010 Third\nInternational Conference on Software Testing, Verification\nand Validation, 65–74.\nDiekmann, L.; and Tratt, L. 2020. Don’t Panic! Better, Fewer,\nSyntax Errors for LR Parsers. In 34th European Conference\non Object-Oriented Programming, ECOOP 2020, volume\n166 of LIPIcs, 6:1–6:32.\nDormann, C. F. 2020. Calibration of probability predictions\nfrom machine-learning and statistical models.Global ecology\nand biogeography, 29(4): 760–765.\nDrori, I.; Zhang, S.; Shuttleworth, R.; Tang, L.; Lu, A.; Ke,\nE.; Liu, K.; Chen, L.; Tran, S.; Cheng, N.; et al. 2022. A\nneural network solves, explains, and generates university\nmath problems by program synthesis and few-shot learning\nat human level. Proceedings of the National Academy of\nSciences, 119(32): e2123433119.\nDrosos, I.; Guo, P. J.; and Parnin, C. 2017. HappyFace:\nIdentifying and predicting frustrating obstacles for learning\nprogramming at scale. In 2017 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCC), 171–\n179. IEEE.\nFeng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.;\nShou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. 2020. Codebert:\nA pre-trained model for programming and natural languages.\narXiv preprint arXiv:2002.08155.\nGazzola, L.; Micucci, D.; and Mariani, L. 2019. Automatic\nSoftware Repair: A Survey. IEEE Transactions on Software\nEngineering, 45: 34–67.\nGoues, C. L.; Pradel, M.; and Roychoudhury, A. 2019. Auto-\nmated program repair. Communications of the ACM, 62(12):\n56–65.\nGupta, R.; Pal, S.; Kanade, A.; and Shevade, S. K. 2017.\nDeepFix: Fixing Common C Language Errors by Deep Learn-\ning. In AAAI.\nHajipour, H.; Bhattacharyya, A.; and Fritz, M. 2020. Sam-\npleFix: Learning to Correct Programs by Efficient Sampling\nof Diverse Fixes. In NeurIPS 2020 Workshop on Computer-\nAssisted Programming.\n5139\nInala, J. P.; Wang, C.; Yang, M.; Codas, A.; Encarnaci´on, M.;\nLahiri, S. K.; Musuvathi, M.; and Gao, J. 2022. Fault-Aware\nNeural Code Rankers. arXiv preprint arXiv:2206.03865.\nJohnson, J.; Douze, M.; and J ´egou, H. 2019. Billion-scale\nsimilarity search with gpus. IEEE Transactions on Big Data,\n7(3): 535–547.\nLevenshtein, V . I.; et al. 1966. Binary codes capable of cor-\nrecting deletions, insertions, and reversals. In Soviet physics\ndoklady, volume 10, 707–710. Soviet Union.\nLiu, K.; Li, L.; Koyuncu, A.; Kim, D.; Liu, Z.; Klein, J.; and\nBissyand´e, T. F. 2021. A critical review on the evaluation of\nautomated program repair systems. Journal of Systems and\nSoftware, 171: 110817.\nMurphy, L.; Lewandowski, G.; McCauley, R.; Simon, B.;\nThomas, L.; and Zander, C. 2008. Debugging: the good,\nthe bad, and the quirky–a qualitative analysis of novices’\nstrategies. ACM SIGCSE Bulletin, 40(1): 163–167.\nNguyen, H. D. T.; Qi, D.; Roychoudhury, A.; and Chandra,\nS. 2013. SemFix: Program repair via semantic analysis.\nInternational Conference on Software Engineering, 772–781.\nNixon, J.; Dusenberry, M. W.; Zhang, L.; Jerfel, G.; and Tran,\nD. 2019. Measuring Calibration in Deep Learning. In CVPR\nWorkshops, volume 2.\nParihar, S.; Dadachanji, Z.; Singh, P. K.; Das, R.; Karkare, A.;\nand Bhattacharya, A. 2017. Automatic grading and feedback\nusing program repair for introductory programming courses.\nIn Proceedings of the 2017 ACM Conference on Innovation\nand Technology in Computer Science Education, 92–97.\nPoesia, G.; Polozov, A.; Le, V .; Tiwari, A.; Soares, G.; Meek,\nC.; and Gulwani, S. 2022. Synchromesh: Reliable Code Gen-\neration from Pre-trained Language Models. In International\nConference on Learning Representations.\nPrenner, J. A.; and Robbes, R. 2021. Automatic Program\nRepair with OpenAI’s Codex: Evaluating QuixBugs. arXiv\npreprint arXiv:2111.03922.\nPu, Y .; Narasimhan, K.; Solar-Lezama, A.; and Barzilay, R.\n2016. sk p: a neural program corrector for MOOCs. In\nCompanion Proceedings of the 2016 ACM SIGPLAN Inter-\nnational Conference on Systems, Programming, Languages\nand Applications: Software for Humanity, 39–40.\nQi, Y .; Mao, X.; Lei, Y .; Dai, Z.; and Wang, C. 2014. The\nstrength of random search on automated program repair. In\nProceedings of the 36th International Conference on Software\nEngineering, 254–265.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by generative\npre-training. https://paperswithcode.com/paper/improving-\nlanguage-understanding-by. Accessed: 2022-08-05.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the Limits of Transfer Learning with a Unified Text-to-\nText Transformer. Journal of Machine Learning Research,\n21(140): 1–67.\nShannon, C. E. 1948. A mathematical theory of communica-\ntion. The Bell system technical journal, 27(3): 379–423.\nSpotify. 2022. ANNOY library. https://github.com/spotify/\nannoy. Accessed: 2022-08-01.\nStackOverflow. 2022. StackOverflow Website. https://\nstackoverflow.com/.\nT´omasd´ottir, K. F.; Aniche, M.; and Van Deursen, A. 2018.\nThe adoption of javascript linters in practice: A case study on\neslint. IEEE Transactions on Software Engineering, 46(8):\n863–891.\nWexelblat, R. L. 1976. Maxims for malfeasant designers, or\nhow to design languages to make programming as difficult as\npossible. In Proceedings of the 2nd international conference\non Software engineering, 331–336.\nYasunaga, M.; and Liang, P. 2020. Graph-based, self-\nsupervised program repair from diagnostic feedback. In In-\nternational Conference on Machine Learning, 10799–10808.\nPMLR.\nYasunaga, M.; and Liang, P. 2021. Break-it-fix-it: Unsuper-\nvised learning for program repair. In International Confer-\nence on Machine Learning, 11941–11952. PMLR.\nZhong, H.; and Su, Z. 2015. An Empirical Study on Real Bug\nFixes. 2015 IEEE/ACM 37th IEEE International Conference\non Software Engineering, 1: 913–923.\n5140",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8197484016418457
    },
    {
      "name": "Programmer",
      "score": 0.5871651768684387
    },
    {
      "name": "Programming language",
      "score": 0.5715272426605225
    },
    {
      "name": "Code (set theory)",
      "score": 0.49401718378067017
    },
    {
      "name": "Class (philosophy)",
      "score": 0.48817378282546997
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.44708117842674255
    },
    {
      "name": "Language model",
      "score": 0.4240800440311432
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35477161407470703
    },
    {
      "name": "Natural language processing",
      "score": 0.3508007526397705
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}