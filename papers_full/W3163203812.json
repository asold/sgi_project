{
  "title": "Rethinking the Design Principles of Robust Vision Transformer",
  "url": "https://openalex.org/W3163203812",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2170242036",
      "name": "Xiao-Feng Mao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3006645534",
      "name": "Gege Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135878666",
      "name": "Yuefeng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096283650",
      "name": "Xiaodan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788542886",
      "name": "Shaokai Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102192046",
      "name": "Yuan He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097401155",
      "name": "Hui Xue",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3037492894",
    "https://openalex.org/W2947707615",
    "https://openalex.org/W2994088087",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2961301154",
    "https://openalex.org/W3016265891",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W3025573667",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3093019879",
    "https://openalex.org/W3107291594",
    "https://openalex.org/W3035467354",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3120460133",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2887603965",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2951655307",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2945767825",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3009751875",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2640329709",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3037742886",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3044036242",
    "https://openalex.org/W2402213890",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3142085127",
    "https://openalex.org/W3081624383",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2910426746"
  ],
  "abstract": "Recent advances on Vision Transformers (ViT) have shown that\nself-attention-based networks, which take advantage of long-range dependencies\nmodeling ability, surpassed traditional convolution neural networks (CNNs) in\nmost vision tasks. To further expand the applicability for computer vision,\nmany improved variants are proposed to re-design the Transformer architecture\nby considering the superiority of CNNs, i.e., locality, translation invariance,\nfor better performance. However, these methods only consider the standard\naccuracy or computation cost of the model. In this paper, we rethink the design\nprinciples of ViTs based on the robustness. We found some design components\ngreatly harm the robustness and generalization ability of ViTs while some\nothers are beneficial. By combining the robust design components, we propose\nRobust Vision Transformer (RVT). RVT is a new vision transformer, which has\nsuperior performance and strong robustness. We further propose two new\nplug-and-play techniques called position-aware attention rescaling and\npatch-wise augmentation to train our RVT. The experimental results on ImageNet\nand six robustness benchmarks show the advanced robustness and generalization\nability of RVT compared with previous Transformers and state-of-the-art CNNs.\nOur RVT-S* also achieves Top-1 rank on multiple robustness leaderboards\nincluding ImageNet-C and ImageNet-Sketch. The code will be available at\nhttps://github.com/vtddggg/Robust-Vision-Transformer.",
  "full_text": "Towards Robust Vision Transformer\nXiaofeng Mao1 Gege Qi1 Yuefeng Chen1 Xiaodan Li1 Ranjie Duan2 Shaokai Ye3\nYuan He1 Hui Xue1\n1Alibaba Group 2Swinburne University of Technology 3EPFL\n{mxf164419,qigege.qgg,yuefeng.chenyf,fiona.lxd}@alibaba-inc.com\nAbstract\nRecent advances on Vision Transformer (ViT) and its im-\nproved variants have shown that self-attention-based net-\nworks surpass traditional Convolutional Neural Networks\n(CNNs) in most vision tasks. However, existing ViTs fo-\ncus on the standard accuracy and computation cost, lack-\ning the investigation of the intrinsic influence on model ro-\nbustness and generalization. In this work, we conduct sys-\ntematic evaluation on components of ViTs in terms of their\nimpact on robustness to adversarial examples, common cor-\nruptions and distribution shifts. We find some components\ncan be harmful to robustness. By leveraging robust compo-\nnents as building blocks of ViTs, we propose Robust Vision\nTransformer (RVT), which is a new vision transformer and\nhas superior performance with strong robustness. Inspired\nby the findings during the evaluation, we further propose\ntwo new plug-and-play techniques called position-aware at-\ntention scaling and patch-wise augmentation to augment\nour RVT, which we abbreviate as RVT ‚àó. The experimen-\ntal results of RVT on ImageNet and six robustness bench-\nmarks demonstrate its advanced robustness and general-\nization ability compared with previous ViTs and state-of-\nthe-art CNNs. Furthermore, RVT-S ‚àó achieves Top-1 rank\non multiple robustness leaderboards including ImageNet-C,\nImageNet-Sketch and ImageNet-R.\n1. Introduction\nFollowing the popularity of transformers in Natural Lan-\nguage Processing (NLP) applications, e.g., BERT [8] and\nGPT [30], there has sparked particular interest in inves-\ntigating whether transformer can be a primary backbone\nfor computer vision applications previously dominated by\nConvolutional Neural Networks (CNNs). Recently, Vi-\nsion Transformer (ViT) [10] successfully applies a pure\ntransformer for classification which achieves an impres-\nsive speed-accuracy trade-off by capturing long-range de-\npendencies via self-attention. Base on this seminal work,\nnumerous variants have been proposed to improve ViTs\n0 5 10 15 20\nFLOPS(G)\n70\n72\n74\n76\n78\n80\n82\n84\n86Standard Accuracy(%)\nDeiT\nConViT\nSwin\nPVT\nPiT\nRVT\n0 5 10 15 20\nFLOPS(G)\n10\n20\n30\n40\n50\n60Robust Accuracy(%) DeiT\nConViT\nSwin\nPVT\nPiT\nRVT\nFigure 1. Comparison between RVT and the baseline transform-\ners. The robust accuracy in figure is recorded under FGSM [11]\nadversary.\nfrom different perspectives containing training data effi-\nciency [40], self-attention mechanism [25], introducing\nconvolution [23,45,50] or pooling layers [20,43], etc. How-\never, these works only focus on the standard accuracy and\ncomputation cost, lacking the investigation of the intrinsic\ninfluence on model robustness and generalization.\nIn this work, we take initiatives to explore a ViT model\nwith strong robustness. To this end, we first give an em-\npirical assessment of existing ViT models in Figure 1. Sur-\nprisingly, although all ViT variants reproduce the standard\naccuracy claimed in the paper, some of their modifications\nmay bring devastating damages on the model robustness. A\nvivid example is PVT [43], which achieves a high standard\naccuracy but suffered with large drop of robust accuracy.\nWe show that PVT-Small obtains only 26.6% robust accu-\nracy, which is 14.1% lower than original DeiT-S in Figure 1.\nTo demystify the trade-offs between accuracy and ro-\nbustness, we analyze ViT models with different patch em-\nbedding, position embedding, transformer blocks and clas-\nsification head whose impact on the robustness that has\nnever been thoroughly studied. Based on the valuable find-\nings revealed by exploratory experiments, we propose a\nRobust Vision Transformer (RVT), which has significant\nimprovement on robustness, but also exceeds most other\ntransformers in accuracy. In addition, we propose two new\nplug-and-play techniques to further boost the RVT. The\nfirst is Position-Aware Attention Scaling (PAAS), which\nplays the role of position encoding in RVT. PAAS im-\nproves the self-attention mechanism by filtering out redun-\ndant and noisy position correlation and activating only ma-\njor attention with strong correlation, which leads to the en-\nhancement of model robustness. The second is a simple\nand general patch-wise augmentation method for patch se-\nquences which adds rich affinity and diversity to training\ndata. Patch-wise augmentation also contributes to the model\ngeneralization by reducing the risk of over-fitting. With the\nabove proposed methods, we can build an augmented Ro-\nbust Vision Transformer‚àó (RVT‚àó). Contributions of this pa-\nper are three-fold:\n‚Ä¢ We give a systematic robustness analysis of ViTs and\nreveal harmful components. Inspired by it, we reform\nrobust components as building blocks as a new trans-\nformer, named Robust Vision Transformer (RVT).\n‚Ä¢ To further improve the RVT, we propose two new\nplug-and-play techniques called position-aware atten-\ntion scaling and patch-wise augmentation. Both of\nthem can be applied to other ViT models and yield sig-\nnificant enhancement on robustness and standard accu-\nracy.\n‚Ä¢ Experimental results on ImageNet and six robustness\nbenchmarks show that RVT exhibits best trade-offs\nbetween standard accuracy and robustness compared\nwith previous ViTs and CNNs. Specifically, RVT-S ‚àó\nachieves Top-1 rank on ImageNet-C, ImageNet-Sketch\nand ImageNet-R.\n2. Related Work\nRobustness Benchmarks. The rigorous benchmarks are\nimportant for evaluating and understanding the robustness\nof deep models. Early works focus on the model safety\nunder the adversarial examples with constrained perturba-\ntions [11, 38]. In real-world applications, the phenomenon\nof image corruption or out-of-distribution is more com-\nmonly appeared. Driven by this, ImageNet-C [17] bench-\nmarks the model against image corruption which simulates\ndistortions from real-world sources. ImageNet-R [16] and\nImageNet-Sketch [42] collect the online images consisting\nof naturally occurring distribution changes such as image\nstyle, to measure the generalization ability to new distri-\nbutions at test time. In this paper, we adopt all the above\nbenchmarks as the fair-minded evaluation metrics.\nRobustness Study for CNNs. The robustness research\nof CNNs has experienced explosive development in recent\nyears. Numerous works conduct thorough study on the ro-\nbustness of CNNs and aim to strengthen it in different ways,\ne.g., stronger data augmentation [16, 18, 33], carefully de-\nsigned [36,44] or searched [9,13] network architecture, im-\nproved training strategy [22, 26, 47], quantization [24] and\npruning [49] of the weights, better pooling [41, 53] or acti-\nvation functions [46], etc. Although the methods mentioned\nabove perform well on CNNs, there is no evidence that they\nalso keep the effectiveness on ViTs. A targeted research for\nimproving the robustness of ViTs is still blank.\nRobustness Study for ViTs. Until now, there are sev-\neral works attempting at studying the robustness of ViTs.\nEarly works focus on the adversarial robustness of ViTs.\nThey find that ViTs are more adversarially robust than\nCNNs [34] and the transferability of adversarial examples\nbetween CNNs and ViTs is remarkably low [27]. Follow up\nworks [2, 29] extend the robustness study on ViTs to much\ncommon image corruption and distribution shift, and indi-\ncate ViTs are more robust learners. Although some findings\nare consistent with above works, in this paper, we do not\nmake simple comparison of robustness between ViTs and\nCNNs, but take a step further by analyzing the detailed ro-\nbust components in ViT and its variants. Based on the anal-\nysis, we design a robust vision transformer and introduce\ntwo novel techniques to further reduce the fragility of ViT\nmodels.\n3. Robustness Analysis of Designed Compo-\nnents\nWe give the robustness analysis of four main components\nin ViTs: patch embedding, position embedding, transformer\nblocks and classification head. DeiT-Ti [40] is used as the\nbase model. All the robustness benchmarks mentioned in\nsection 2 are considered comprehensively. There is a pos-\nitive correlation between these benchmarks in most cases.\nDue to the limitation of space, we show the robust accuracy\nunder FGSM [11] adversary in the main body and other re-\nsults in Appendix A.\n3.1. Patch Embedding\nF1: Low-level feature of patches helps for the ro-\nbustness. ViTs [10] tokenize an image by splitting it into\npatches with size of 16 √ó16 or 32 √ó32. Such simple tok-\nenization makes the models hard to capture low-level struc-\ntures such as edges and corners. To extract low-level fea-\ntures of patches, CeiT [50], LeViT [12] and TNT [14] use\na convolutional stem instead of the original linear layer,\nT2T-ViT [51] leverages self-attention to model dependen-\ncies among neighboring pixels. However, these methods\nmerely focus on the standard accuracy. To answer how\nis the robustness affected by leveraging low-level features\nMHSA*\nFFN*(L-2)√óRVT Blocks\n‚Ä¶\n‚Ä¶\nClassifierBird\nRVT Blocks2√óPooling@2√ó2\nGlobal averagepooling\nRemovingCLS token\nRVT*ViT\nConv. stem for patch emb.Patch-wiseaugmentationEmbed to tokens*\nMulti-stageblocks\nToken featurepooling\n ‚Ä¶Linear\nEmbed to tokens\nQKV‚®Ç‚®Äsoftmax‚®ÇMHSALinearùüèùíÖ\nFFNLinear1\nViT Block\nImage Aug.+Patching\nPooling@2√ó2\n‚Ä¶Reshape2√ó2 convReshape‚Ä¶\nImage Aug. + Patching\nFlatten\nLinear Projection\n‚Ä¶\n[CLS]\nMHSA\nFFNL√óViT Blocks\n‚Ä¶\n‚Ä¶\nEmbed to tokens\nClassifierBird\nFactorised blocks\n‚Ä¶\n‚Ä¶Convolutional Stem\nPatch Augmentation\nEmbed to tokens*\nRVT Block\nQ K V‚®Ç‚®Äsoftmax‚®ÇMHSA*LinearWùíëùíÖ\nFFN*Linear13√ó3 convLinear2\n‚Ä¶\nLinear2PAASUsing a suitable head number\nConvolutional FFN\nFigure 2. Overall architecture of the proposed Robust Vision Transformer (RVT).\nof patches, we compare the original linear projection with\ntwo new convolution and tokens-to-tokens embedders, pro-\nposed by CeiT and T2T-ViT respectively. As shown in Ta-\nble 2, low-level patch embedding has a positive effect on the\nmodel robustness and standard accuracy as more detailed\nvisual features are exploited. Among them tokens-to-tokens\nembedder is the best, but it has quadratic complexity with\nthe expansion of image size. We adopt the convolutional\nembedder with less computation cost.\npositional embedding Acc Robust Acc\n(i) none 68.3 15.8\n(ii) learned absolute position 72.2 22.3\n(iii) sin-cos absolute position 72.0 21.9\n(iv) learned relative position [35] 71.8 22.3\n(v) input-conditioned position [3] 72.4 21.5\nTable 1. Effect of different positional embeddings. We use Deit-\nTi as the base model.\n3.2. Position Embedding\nF2: Position encoding is critical for learning shape-\nbias based semantic features which are robust to tex-\nture changes. Besides, existing position encoding meth-\nods have no big impact on the robustness. We first\nexplore the necessity of position embeddings. Previous\nwork [3] shows ViT trained without position embeddings\nhas 4% drop of standard accuracy. In this work, we find\nthis gap even can be larger on robustness. In Appendix\nA, we find with no position encoding, ViT fails to recog-\nnize shape-bias objects, which leads to 8% accuracy drop\non ImageNet-Sketch. Concerning the ways of positional\nencoding, learned absolute, sin-cos absolute, learned rela-\ntive [35], input-conditioned [3] position representations are\ncompared. In Table 1, the result suggests that most posi-\ntion encoding methods have no big impact on the robust-\nness, and a minority even have a negative effect. Especially,\nCPE [3] encodes position embeddings conditioned on in-\nputs. Such a conditional position representation makes it\nchanged easily with the input, and causes the poor robust-\nness. The fragility of position embeddings also motivates\nus to design a more robust position encoding method.\nTable 2. Ablations on other ViT components, where ‚úìindicates\nthe use of the corresponding component.\nPatch Emb. Local Conv. CLS Acc Rob.\nLinear Conv. T2T SA FFN Acc\n‚úì ‚úì 72.2 22.3\n‚úì ‚úì 73.6 23.2\n‚úì ‚úì 74.9 25.4\n‚úì ‚úì ‚úì 69.1 21.0\n‚úì ‚úì 73.9 31.9\n‚úì ‚úì 72.4 28.4\n3.3. Transformer Blocks\nF3: An elaborate multi-stage design is required for\nconstructing robust vision transformers. Modern CNNs\nalways start with a feature of large spatial sizes and a small\nchannel size and gradually increase the channel size while\ndecreasing the spatial size. The different sizes of feature\nmaps constitute the multi-stage convolution blocks. As\nshown by previous works [4], such a design contributes to\nthe expressiveness and generalization performance of the\nnetwork. PVT [43], PiT [20] and Swin [25] employ this\ndesign principle into ViTs. To measure the robustness vari-\nance with changing of stage distribution, we slightly modify\nthe DeiT-Ti architecture to get five variants (V2-V6) in Ta-\nble 3. We keep the overall number of transformer blocks\nconsistent to 12 and replace some of them with smaller or\nlarger spatial resolution. Detailed architecture is shown in\nAppendix A. By comparing with DeiT-Ti, we find all five\nvariants improve the standard accuracy, benefit from the ex-\ntraction of hierarchical image features. In terms of robust-\nness, transformer blocks with different spatial sizes show\ndifferent effects. An experimental conclusion is that the\nmodel will get worse on robustness when it contains more\ntransformer blocks with large spatial resolution. On the\ncontrary, reducing the spatial resolution gradually at later\ntransformer blocks contributes to the modest enhancement\nof robustness. Besides, we also observe that having more\nblocks with larger input spatial size will increase the num-\nber of FLOPs and memory consumption. To achieve the\nbest trade-off on speed and performance, we think V2 is the\nmost compromising choice in this paper.\nF4: Robustness can be benefited from the complete-\nness and compactness among attention heads, by choos-\ning an appropriate head number. ConViT [6], Swin [25]\nand LeViT [12] both use more self-attention heads and\nsmaller dimensions of keys and queries to achieve better\nperformance at a controllable FLOPs. To study how does\nthe number of heads affect the robustness, we train DeiT-\nTi with different head numbers. Once the number of heads\nincreases, we meanwhile reduce the head dimensions to en-\nsure the overall feature dimensions are unchanged. Simi-\nlar with generally understanding in NLP [28], we find the\ncompleteness and compactness among attention heads are\nimportant for ViTs. As shown in the Table 4, the robustness\nand standard accuracy still gain great improvement with the\nhead increasing till to 8. We think that an appropriate num-\nber of heads supplies various aspects of attentive informa-\ntion on the input. Such complete and non-redundant atten-\ntive information also introduces more fine-grained represen-\ntations which are prone to be neglect by model with less\nheads, thus increases the robustness.\nvariants [S1, S2, S3, S4] FLOPs Mem Acc Robust Acc\nV1 [0, 0, 12, 0] 1.3 1.1 72.2 22.3\nV2 [0, 0, 10, 2] 1.2 1.1 74.8 24.3\nV3 [0, 2, 10, 0] 1.5 1.7 73.8 22.0\nV4 [0, 2, 8, 2] 1.4 1.7 76.4 22.3\nV5 [2, 2, 8, 0] 3.4 6.0 73.4 17.0\nV6 [2, 2, 6, 2] 3.4 6.0 76.4 17.5\nTable 3. Effect of stage distribution. We ablate the number of\nblocks in stages S1, S2, S3, S4 of DeiT-Ti, where S1 is the stage\nwith the largest 56 √ó 56 input spatial dimension, and gradually\nreduced to half of the original in later stages. The GPU memory\nconsumption is tested on input with batch size of 64.\nF5: The locality constraints of self-attention layer\nmay do harm for the robustness. Vanilla self-attention\ncalculates the pair-wise attention of all sequence elements.\nBut for image classification, local region needs to be paid\nHeads 1 2 4 6 8 12\nAcc 69.0 71.7 73.1 73.4 73.9 73.5\nRob. Acc 17.6 21.4 22.8 24.6 25.2 24.7\nTable 4. The performance variance with the number of heads.\nDeiT-Ti with head number of 1, 2, 4, 6, 8 and 12 are trained for\ncomparison.\nmore attention than remoter regions. Swin [25] limits the\nself-attention computation to non-overlapping local win-\ndows on the input. This hard coded locality of self-attention\nenjoys great computational efficiency and has linear com-\nplexity with respect to image size. Although Swin can also\nget competitive accuracy, in this work we find such local\nwindow self-attention is harmful to the model robustness.\nThe result in Table 2 shows after modifying self-attention\nto the local version, the robust accuracy is getting worse.\nWe think this phenomenon may be partly caused by the de-\nstruction of long-range dependencies modeling in ViTs.\nF6: Feed-forward networks (FFN) can be extended\nto convolutional FFN by encoding multiple tokens in lo-\ncal regions. Such information exchange of local tokens\nin FFN makes ViTs more robust. LocalViT [23] and\nCeiT [50] introduce connectivity of local regions into ViTs\nby adding a depth-wise convolution in feed-forward net-\nworks (FFN). Our experiment in Table 2 verifies that the\nconvolutional FFN greatly improves both the standard ac-\ncuracy and robustness. We think the reason lies in two as-\npects. First, compared with locally self-attention, convo-\nlutional FFN will not damage the long-term dependencies\nmodeling ability of ViTs. The merit of ViTs can be inher-\nited. Second, original FFN only encodes single token rep-\nresentation, while convolutional FFN encodes both the cur-\nrent token and its neighbors. Such information exchange\nwithin a local region makes ViTs more robust.\n3.4. Classification Head\nF7: Is the classification token ( CLS) important for\nViTs? The answer is not, and replacing CLS with global\naverage pooling on output tokens even improves the ro-\nbustness. CNNs adopt a global average pooling layer\nbefore the classifier to integrate visual features at different\nspatial locations. This practice also inherently takes advan-\ntage of the translation invariance of the image. However,\nViTs use an additional classification token (CLS) to perform\nclassification, are not translation-invariant. To get over this\nshortcoming, CPVT [3] and LeViT [12] remove theCLS to-\nken and replace it by average pooling along with the last\nlayer sequential output of the Transformer. We compare\nmodels trained with and without CLS token in Table 2. The\nresult shows the adversarial robustness can be greatly im-\nproved by removing CLS token. Also we find removing CLS\ntoken has slight help for the standard accuracy, which can\nbe benefited from the desired translation-invariance.\n3.5. Combination of Robust Components\nIn the above, we separately analyze the effect of each de-\nsigned component in the ViTs. To make use of these find-\nings, we combine the selected useful components, listed in\nfollows: 1) Extract low-level feature of patches using a con-\nvolutional stem; 2) Adopt the multi-stage design of ViTs\nand avoid blocks with larger spatial resolution; 3) Choose\na suitable number of heads; 4) Use convolution in FFN; 5)\nReplace CLS token with token feature pooling. As we find\nthe effects of the above modifications are superimposed, we\nadopt all of these robust components into ViTs, the resultant\nmodel is called Robust Vision Transformer (RVT). RVT has\nachieved the new state-of-the-art robustness compared to\nother ViT variants. To further improve the performance,\nwe propose two novel techniques, position-aware attention\nscaling and patch-wise data augmentation, to train our RVT.\nBoth of them are also applicable to other ViT models.\n4. Position-Aware Attention Scaling\nIn this section, we introduce our proposed position en-\ncoding mechanism called Position-Aware Attention Scaling\n(PAAS), which modifies the rescaling operation in the dot\nproduct attention to a more generalized version. To start\nwith, we illustrate the scaled dot-product attention in trans-\nformer firstly. And then the modification of PAAS will be\nexplained.\nScaled Dot-product Attention. Scaled dot-product at-\ntention is a key component in Multi-Head Self Attention\nlayer (MHSA) of Transformer. MHSA first generates set of\nqueries Q ‚àà RN√ód, keys K ‚àà RN√ód, values V ‚àà RN√ód\nwith the corresponding projection. Then the query vector\nq ‚àà Rd is matched against the each key vector in K. The\noutput is the weighted sum of a set of N value vectors v\nbased on the matching score. This process is called scaled\ndot-product attention:\nAttention(Q, K, V) =Softmax(QKT /\n‚àö\nd)V (1)\nFor preventing extremly small gradients and stabilizing\nthe training process, each element in QKT multiplies by a\nconstant 1‚àö\nd to be rescaled into a standard range.\nPosition-Aware Attention Scaling. In this work, a more\neffective position-aware attention scaling method is pro-\nposed. To make the original rescaling process of dot-\nproduct attention position-aware, we define a learnable po-\nsition importance matrix Wp ‚àà RN√óN, which presents the\nimportance of each pair of q-k. The oringinal scaled dot-\nproduct attention is modified as follows:\nAttention(Q, K, V) =Softmax(QKT ‚äô(Wp/\n‚àö\nd))V (2)\nself-attention map before PAAS\nClean exampleAdv. example\nVisualization of learned scaling factor (block 1-12)\nself-attention map after PAAS\nFigure 3. Top: visualization of self-attention before and after the\nposition-aware attention scaling. Bottom: visualization of learned\nscaling factor by our PAAS.\nwhere ‚äô is the element-wise product. As Wp is input\nindependent and only determined by the position of each q,\nk in the sequence, our position-aware attention scaling can\nalso serve as a position representation. Thus, we replace the\ntraditional position embedding with our PAAS in RVT. Af-\nter that the overall self-attention can be decoupled into two\nparts: the QKT term presents the content-based attention,\nand Wp/\n‚àö\nd term acts as the position-based attention. This\nuntied design offers more expressiveness by removing the\nmixed and noisy correlations [21].\nRobustness of PAAS. As mentioned in section 3.2, most\nexisting position embeddings have no contribution to the\nmodel robustness, and some of them even do a negative\neffect. Differently, our proposed PAAS can improve the\nmodel robustness effectively. This superior property relies\non the position importance matrix Wp, which acts as a soft\nattention mask on each position pair of q-k. As shown in\nFigure 3, we visualize the attention map of 3th query patch\nin 3th transformer block. Without PAAS, an adversarial in-\nput can make some unrelated regions activated and produce\na noisy self-attention map. To filter out these noises, PAAS\nsuppresses the redundant positions irrelevant for classifica-\ntion in self-attention map, by a learned small multiplier in\nWp. Finally only the regions important for classification are\nactivated. We experimentally validate that PAAS can pro-\nvide certain defense power against some white-box adver-\nsaries, e.g., FGSM [11]. Not limited to adversarial attack, it\nalso helps to the corruption and out-of-distribution general-\nization. Details can be referred to section 6.3.\n5. Patch-Wise Augmentation\nImage augmentation is a strategy especially important\nfor ViTs since a biggest shortcoming of ViTs is the worse\ngeneralization ability when trained on relatively small-size\ndatasets, while this shortcoming can be remedied by suffi-\ncient data augmentation [40]. On the other hand, a rich data\naugmentation also helps with robustness and generalization,\nwhich has been verified in previous works [18]. For improv-\ning the diversity of the augmented training data, we propose\nthe patch-wise data augmentation strategy for ViTs, which\nimposes diverse augmentation on each input image patches\nat training time. Our motivation comes from the difference\nof ViTs and CNNs that ViTs not only extract intra-patch\nfeatures but also concern the inter-patch relations. We think\nthe traditional augmentation which randomly transforms the\nwhole image could provide enough intra-patch augmenta-\ntion. However, it lacks the diversity on inter-patch aug-\nmentation, as all of patches have the same transformation\nat one time. To impose more inter-patch diversity, we retain\nthe original image-level augmentation, and then add the fol-\nlowing patch-level augmentation on each image patch. For\nsimplicity, only three basic image transformations are con-\nsidered for patch-level augmentation: random resized crop,\nrandom horizontal flip and random gaussian noise.\nRobustness of Patch-Wise Augmentation. Same with\nthe augmentations like MixUp [52], AugMix [18], Ran-\ndAugment [5], patch-wise augmentation also benefit the\nmodel robustness. It effects on the phases after conven-\ntional image-level augmentations, and provides the mean-\ningful augmentation on patch sequence input. Different\nfrom RandAugment, which adopts augmentations conflict-\ning with ImageNet-C, we only use simple image transform\nfor patch-wise augmentation. It confirms that the most part\nof robustness improvement is derived from the strategy it-\nself but not the used augmentation. A significant advantage\nof patch-wise augmentation is that it can be in common use\nacross different ViT models and bring more than 1% and\n5% improvement on standard and robust accuracy. Details\ncan be referred to section 6.3.\n6. Experiments\n6.1. Experimental Settings\nImplementation Details. All of our experiments are\nperformed on the NVIDIA 2080Ti GPUs. We implement\nRVT in three sizes named by RVT-Ti, RVT-S, RVT-B re-\nspectively. All of them adopt the best settings investigated\nin section 2. For RVT ‚àó, we add PAAS on multiple trans-\nformer blocks. The patch-wise augmentation uses the com-\nbination of base augmentation introduced in section 6.4.\nOther training hyperparameters are same with DeiT [40].\nEvaluation Benchmarks. We adopt the ImageNet-\n1K [7] dataset for training and standard performance eval-\nuation. No other large-scale dataset is needed for pre-\ntraining. For robustness evaluation, we test our RVT in\nthree aspect: 1) for adversarial robustness, we test the adver-\nsarial examples generated by white-box attack algorithms\nFGSM [11] and PGD [26] on ImageNet-1K validation set.\nImageNet-A [19] is used for evaluating the model under\nnatural adversarial example. 2) for common corruption ro-\nbustness, we adopt ImageNet-C [17] which consists of 15\ntypes of algorithmically generated corruptions with five lev-\nels of severity. 3) for out-of-distribution robustness, we\nevaluate on ImageNet-R [16] and ImageNet-Sketch [42].\nThey contain images with naturally occurring distribution\nchanges. The difference is that ImageNet-Sketch only con-\ntains sketch images, which can be used for testing the classi-\nfication ability when texture or color information is missing.\n6.2. Standard Performance Evaluation\nFor standard performance evaluation, we compare our\nmethod with state-of-the-art classification methods includ-\ning Transformer-based models and representative CNN-\nbased models in Table 5. Compared to CNNs-based models,\nRVT has surpassed most of CNN architectures with fewer\nparameters and FLOPs. RVT-Ti ‚àó achieves 79.2% Top-1\naccuracy on ImageNet-1K validation set, which is com-\npetitive with currently popular ResNet and RegNet series,\nbut only has 1.3G FLOPs and 10.9M parameters (around\n60% smaller than CNNs). With the same computation\ncost, RVT-S‚àó obtains 81.9% test accuracy, 2.9% higher than\nResNet-50. This result is closed to EfficientNet-B4, how-\never EfficientNet-B4 requires larger 380 √ó380 input size\nand has much lower throughput.\nCompared to Transformer-based models, our RVT also\nachieves the comparable standard accuracy. We find just\ncombining the robust components can make RVT-Ti get\n78.4% Top-1 accuracy and surpass the existing state-of-the-\nart on ViTs with tiny version. By adopting our newly pro-\nposed position-aware attention scaling and patch-wise data\naugmentation, RVT-Ti‚àó can further improve 0.8% on RVT-\nTi with little additional computation cost. For other scales\nof the model, RVT-S‚àó and RVT-B‚àó also achieve a good pro-\nmotion compared with DeiT-S and DeiT-B. Although the\nimprovement becomes smaller with the increase of model\ncapacity, we think the advance of our model is still obvious\nas it strengthen the model ability in various views such as\nrobustness and out-of-domain generalization.\n6.3. Robustness Evaluation\nWe employ a series of benchmarks to evaluate the model\nrobustness on different aspects. Among them, ImageNet-C\n(IN-C) calculates the mean corruption error (mCE) as met-\nric. The smaller mCE means the more robust of the model\nTable 5. The performance of RVT and several SOTA CNNs and Transformers on ImageNet and six robustness benchmarks. RVT ‚àó\nrepresents the RVT model but trained with our proposed PAAS and patch-wise augmentation. Except for different architectures, we also\ncompare some methods such as AugMix, which aims at improving the model robustness based on ResNet-50.\nGroup Model FLOPs Params ImageNet Robustness Benchmarks\n(G) (M) Top-1 Top-5 FGSM PGD IN-C (‚Üì) IN-A IN-R IN-SK\nCNNs\nResNet-50 [15] 4.1 25.6 76.1 86.0 12.2 0.9 76.7 0.0 36.1 24.1\nResNet-50‚àó[15] 4.1 25.6 79.0 94.4 36.3 12.5 65.5 5.9 42.5 31.5\nInception v3 [37] 5.7 27.2 77.4 93.4 22.5 3.1 80.6 10.0 38.9 27.6\nRegNetY-4GF [31] 4.0 20.6 79.2 94.7 15.4 2.4 68.7 8.9 38.8 25.9\nEfficientNet-B4 [39]4.4 19.3 83.0 96.3 44.6 18.5 71.1 26.3 47.1 34.1\nResNeXt50-32x4d [48]4.3 25.0 79.8 94.6 34.7 13.5 64.7 10.7 41.5 29.3\nDeepAugment [16] 4.1 25.6 75.8 92.7 27.1 9.5 53.6 3.9 46.7 32.6\nANT [33] 4.1 25.6 76.1 93.0 17.8 3.1 63.0 1.1 39.0 26.3\nAugMix [18] 4.1 25.6 77.5 93.7 20.2 3.8 65.3 3.8 41.0 28.5\nAnti-Aliased CNN [53]4.2 25.6 79.3 94.6 32.9 13.5 68.1 8.2 41.1 29.6\nDebiased CNN [22] 4.1 25.6 76.9 93.4 20.4 5.5 67.5 3.5 40.8 28.4\nTransformers\nDeiT-Ti [40] 1.3 5.7 72.2 91.1 22.3 6.2 71.1 7.3 32.6 20.2\nConViT-Ti [6] 1.4 5.7 73.3 91.8 24.7 7.5 68.4 8.9 35.2 22.4\nPiT-Ti [20] 0.7 4.9 72.9 91.3 20.4 5.1 69.1 6.2 34.6 21.6\nPVT-Tiny [43] 1.9 13.2 75.0 92.5 10.0 0.5 79.6 7.9 33.9 21.5\nRVT-Ti 1.3 8.6 78.4 94.2 34.8 11.7 58.2 13.3 43.7 30.0\nRVT-Ti‚àó 1.3 10.9 79.2 94.7 42.7 18.9 57.0 14.4 43.9 30.4\nDeiT-S [40] 4.6 22.1 79.9 95.0 40.7 16.7 54.6 18.9 42.2 29.4\nConViT-S [6] 5.4 27.8 81.5 95.8 41.0 17.2 49.8 24.5 45.4 33.1\nSwin-T [25] 4.5 28.3 81.2 95.5 33.7 7.3 62.0 21.6 41.3 29.1\nPVT-Small [43] 3.8 24.5 79.9 95.0 26.6 3.1 66.9 18.0 40.1 27.2\nPiT-S [20] 2.9 23.5 80.9 95.3 41.0 16.5 52.5 21.7 43.6 30.8\nTNT-S [14] 5.2 23.8 81.5 95.7 33.2 4.2 53.1 24.7 43.8 31.6\nT2T-ViTt-14 [51] 6.1 21.5 81.7 95.9 40.9 11.4 53.2 23.9 45.0 32.5\nRVT-S 4.7 22.1 81.7 95.7 51.3 26.2 50.1 24.1 46.9 35.0\nRVT-S‚àó 4.7 23.3 81.9 95.8 51.8 28.2 49.4 25.7 47.7 34.7\nDeiT-B [40] 17.6 86.6 82.0 95.7 46.4 21.3 48.5 27.4 44.9 32.4\nConViT-B [6] 17.7 86.5 82.4 96.0 45.4 20.8 46.9 29.0 48.4 35.7\nSwin-B [25] 15.4 87.8 83.4 96.4 49.2 21.3 54.4 35.8 46.6 32.4\nPVT-Large [43] 9.8 61.4 81.7 95.9 33.1 7.3 59.8 26.6 42.7 30.2\nPiT-B [20] 12.5 73.8 82.4 95.7 49.3 23.7 48.2 33.9 43.7 32.3\nT2T-ViTt-24 [51] 15.0 64.1 82.6 96.1 46.7 17.5 48.0 28.9 47.9 35.4\nRVT-B 17.7 86.2 82.5 96.0 52.3 27.4 47.3 27.7 48.2 35.8\nRVT-B‚àó 17.7 91.8 82.7 96.5 53.0 29.9 46.8 28.5 48.7 36.0\nunder corruptions. All other benchmarks use Top-1 accu-\nracy on test data if no special illustration. The results are\nreported in Table 5.\nAdversarial Robustness. For evaluating the adver-\nsarial robustness, we adopt single-step attack algorithm\nFGSM [11] and multi-step attack algorithm PGD [26] with\nsteps t = 5, step size Œ± = 0.5. Both attackers perturb the\ninput image with max magnitude œµ = 1. Table 5 suggests\nthat the adversarial robustness has a strong correlation with\nthe design of model architecture. With similar model scale\nand FLOPs, most Transformer-based models have higher\nrobust accuracy than CNNs under adversarial attacks. This\nconclusion is also consistent with [34]. Some modifications\non ViTs or CNNs will also weaken or strengthen the ad-\nversarial robustness. For example, Swin-T [25] introduces\nwindow self-attention for reducing the computation cost\nbut damaging the adversarial robustness, and EfficientNet-\nB4 [39] uses smooth activation functions which is helpful\nwith adversarial robustness.\nWe summarize the robust design experiences of ViTs\nin this work. The resultant RVT model achieves superior\nperformance on both FGSM and PGD attackers. In detail,\nRVT-Ti and RVT-S get over 10% improvement on FGSM,\ncompared with the previous ViT variants. This advance is\nfurther expanded by our PAAS and patch-wise augmenta-\ntion. Adversarial robustness seems unrelated with standard\nperformance. Although models like Swin-T, TNT-S get\nhigher standard accuracy than DeiT-S, their adversarially\nrobust accuracy is well below the baseline. However, our\nRVT model can achieve the best trade-off between standard\nperformance and adversarial robustness.\nCommon Corruption Robustness. To metric the model\ndegradation on common image corruptions, we present the\nmCE on ImageNet-C (IN-C) in Table 5. We also list some\nmethods from ImageNet-C Leaderboard, which are built\nbased on ResNet-50. Our RVT-S ‚àó gets 49.4 mCE, which\nhas 4.2 improvement on top-1 method DeepAugment [16]\nin the leaderboard, and bulids the new state-of-the-art. The\nresult also indicates that Transformer-based models have a\nnatural advantage in dealing with image corruptions. At-\ntributed to its ability of long-range dependencies modeling,\nViTs are easier to learn the shape-bias features. Note that\nin this work we are not considering RandAugment. As a\ntraining augmentation of ViTs, RandAugment adopts con-\nflicted augmentation with ImageNet-C and may cause the\nunfairness of the comparison proposed by [1].\nOut-of-distribution Robustness. We test the gener-\nalization ability of RVT on out-of-distribution data by\nreporting the top@1 accuracy on ImageNet-R (IN-R)\nand ImageNet-Sketch (IN-SK) in Table 5. Our RVT and\nRVT‚àó also beat other ViT models on out-of-distribution\ngeneralization. As the superiority of Transformer-based\nmodels on capturing shape-bias features mentioned above,\nour RVT-S also surpasses most CNN and ViT models\nand get 35.0% and 46.9% test accuracy on ImageNet-\nSketch and ImageNet-R, buliding the new state-of-the-art.\nLayers Pos. Acc Rob.\nEmb. Acc\n0-1 Ori. 78.2 34.1\nOurs 78.4 34.3\n0-5 Ori. 78.4 34.6\nOurs 78.6 35.2\n0-10 Ori. 78.4 34.8\nOurs 78.6 35.3\nTable 6. Comparison of sin-\ngle and multiple block PAAS.\nOri. stands for the learned ab-\nsolute position embedding in\noriginal ViTs.\nAugmentations Acc Rob. AccRC GN HF\n‚úì 78.9 41.5\n‚úì 79.0 42.0\n‚úì 79.1 41.3\n‚úì ‚úì 78.8 41.3\n‚úì ‚úì 79.0 41.9\n‚úì ‚úì ‚úì 79.2 41.7\nTable 7. Ablation experiments on\npatch-wise augmentation. RC, GN,\nHF represent random resized crop,\nrandom gaussian noiseand random\nhorizontal flip respectively.\n6.4. Ablation Studies\nwe conduct ablation studies on the proposed compo-\nnents of PAAS and patch-wise augmentation in this sec-\ntion. Other modifications of RVT are not involved since\nthey have been analyzed in section 2. All of our ablation\nexperiments are based on the RVT-Ti model on ImageNet.\nSingle layer PAAS vs. Multiple layer PAAS.We evalu-\nate whether using PAAS on multiple transformer blocks can\nbenefit the performance or robustness. The result is sug-\ngested in Table 6. Learned absolute position embedding in\noriginal ViT model is adopted for comparison. With more\ntransformer blocks using PAAS, the standard and robust ac-\ncuracy gain greater enhancement. After applying PAAS on\n5 blocks, the benefit of PAAS gets saturated. There will be\nthe same trend if we replace PAAS with the original posi-\ntion embedding. But the original position embedding is not\nperformed as good as our PAAS on both standard and robust\naccuracy.\nDifferent types of basic augmentation. Due to the lim-\nited training resources, we only test three basic image aug-\nmentations: random resized crop , random horizontal flip\nand random gaussian noise . For random resized crop, we\ncrop the patch according to the scale sampled from [0.85,\n1.0], then resize it to original size with aspect ratio un-\nchanged. We set the mean and standard deviation as 0 and\n0.01 for random gaussian noise. For each transformation,\nwe set the applying probability p = 0 .1. Other hyper-\nparameters are consistent with the implementation in Ko-\nrnia [32]. As shown in Table 7, we can see both three aug-\nmentations are beneficial of standard and robust accuracy.\nAmong them, random gaussian noise is the better choice as\nit helps for more robustness improvement.\nCombination of basic augmentations. We further eval-\nuate the combination of basic patch-wise augmentations.\nFor traditional image augmentation, combining multiple ba-\nsic transformation [5] can largely improve the standard ac-\ncuracy. Differently, as shown in Table 7, the benefit is\nmarginal for combining basic patch-wise augmentations,\nbut combination of three is still better than using only sin-\ngle augmentation. In this paper, we adopt the combination\nof all basic augmentations.\nEffect on other ViT architectures. For showing the ef-\nfectiveness of our proposed position-aware attention scaling\nand patch-wise augmentation, we apply them to train other\nViT models. DeiT-Ti, ConViT-Ti and PiT-Ti are adopted\nas the base model. The experimental results are shown in\nTable 8, with combining the proposed techniques into these\nbase models, all the augmented models achieve significant\nimprovement. Specifically, all the improved models yield\nmore than 1% and 5% promotion on standard and robust\naccuracy on average.\nVanilla Acc Rob. Acc Improved Acc Rob. Accmodels models\nDeiT-Ti 72.2 22.3 DeiT-Ti‚àó 74.4 29.9\nConViT-Ti 73.3 24.7 ConViT-Ti‚àó 74.4 30.7\nPiT-Ti 72.9 20.4 PiT-Ti‚àó 74.3 27.7\nTable 8. Effect of our proposed PAAS and patch-wise augmenta-\ntion on other ViT architectures.\n7. Conclusion\nWe systematically study the robustness of key compo-\nnents in ViTs, and propose Robust Vision Transformer\n(RVT) by alternating the modifications which would dam-\nage the robustness. Furthermore, we have devised a novel\npatch-wise augmentation which adds rich affinity and di-\nversity to training data. Considering the lack of spa-\ntial information correlation in scaled dot-product atten-\ntion, we present position-aware attention scaling (PAAS)\nmethod to further boost the RVT. Experiments show that our\nRVT achieves outstanding performance consistently on Im-\nageNet and six robustness benchmarks. Under the exhaus-\ntive trade-offs between FLOPs, standard and robust accu-\nracy, extensive experiment results validate the significance\nof our RVT-Ti and RVT-S.\nReferences\n[1] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are\ntransformers more robust than cnns? Advances in Neural\nInformation Processing Systems, 34, 2021. 8\n[2] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner,\nDaliang Li, Thomas Unterthiner, and Andreas Veit. Under-\nstanding robustness of transformers for image classification.\narXiv preprint arXiv:2103.14586, 2021. 2\n[3] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 3, 4, 12\n[4] Nadav Cohen and Amnon Shashua. Inductive bias of deep\nconvolutional networks through pooling geometry. In Pro-\nceedings of the International Conference on Learning Rep-\nresentations, 2017. 3\n[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. InProceedings of the Computer\nVision and Pattern Recognition Workshops, pages 702‚Äì703,\n2020. 6, 8\n[6] St ¬¥ephane d‚ÄôAscoli, Hugo Touvron, Matthew Leavitt, Ari\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases.\narXiv preprint arXiv:2103.10697, 2021. 4, 7\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the Computer Vision and Pattern\nRecognition, pages 248‚Äì255. Ieee, 2009. 6\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 2019. 1\n[9] Minjing Dong, Yanxi Li, Yunhe Wang, and Chang Xu.\nAdversarially robust neural architectures. arXiv preprint\narXiv:2009.00902, 2020. 2\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In Proceedings of\nthe International Conference on Learning Representations ,\n2021. 1, 2\n[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples. InProceed-\nings of the International Conference on Learning Represen-\ntations, 2015. 1, 2, 5, 6, 7\n[12] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre\nStock, Armand Joulin, Herv ¬¥e J ¬¥egou, and Matthijs Douze.\nLevit: a vision transformer in convnet‚Äôs clothing for faster\ninference. arXiv preprint arXiv:2104.01136, 2021. 2, 4\n[13] Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua\nLin. When nas meets robustness: In search of robust archi-\ntectures against adversarial attacks. In Proceedings of the\nComputer Vision and Pattern Recognition , pages 631‚Äì640,\n2020. 2\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer.arXiv preprint\narXiv:2103.00112, 2021. 2, 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. InProceedings\nof the Computer Vision and Pattern Recognition, pages 770‚Äì\n778, 2016. 7\n[16] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of robust-\nness: A critical analysis of out-of-distribution generalization.\narXiv preprint arXiv:2006.16241, 2020. 2, 6, 7\n[17] Dan Hendrycks and Thomas Dietterich. Benchmarking neu-\nral network robustness to common corruptions and pertur-\nbations. In Proceedings of the International Conference on\nLearning Representations, 2019. 2, 6\n[18] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph,\nJustin Gilmer, and Balaji Lakshminarayanan. Augmix: A\nsimple data processing method to improve robustness and\nuncertainty. In Proceedings of the International Conference\non Learning Representation, 2020. 2, 6, 7\n[19] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-\nhardt, and Dawn Song. Natural adversarial examples. arXiv\npreprint arXiv:1907.07174, 2019. 6\n[20] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking\nspatial dimensions of vision transformers. arXiv preprint\narXiv:2103.16302, 2021. 1, 3, 7\n[21] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the po-\nsitional encoding in language pre-training. arXiv preprint\narXiv:2006.15595, 2020. 5\n[22] Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng\nTang, Wei Shen, Alan Yuille, and Cihang Xie. Shape-texture\ndebiased neural network training. In Proceedings of the In-\nternational Conference on Learning Representations , 2021.\n2, 7\n[23] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021. 1, 4\n[24] Ji Lin, Chuang Gan, and Song Han. Defensive quantization:\nWhen efficiency meets robustness. In Proceedings of the In-\nternational Conference on Learning Representations , 2019.\n2\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 1, 3, 4, 7\n[26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Rtowards deep learning\nmodels resistant to adversarial attacks. In Proceedings of\nthe International Conference on Learning Representations ,\n2018. 2, 6, 7\n[27] Kaleel Mahmood, Rigel Mahmood, and Marten Van Dijk.\nOn the robustness of vision transformers to adversarial ex-\namples. arXiv preprint arXiv:2104.02610, 2021. 2\n[28] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? Advances in Neural Informa-\ntion Processing Systems, 2019. 4\n[29] Sayak Paul and Pin-Yu Chen. Vision transformers are robust\nlearners. arXiv preprint arXiv:2105.07581, 2021. 2\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 1\n[31] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ¬¥ar. Designing network design\nspaces. In Proceedings of the Computer Vision and Pattern\nRecognition, pages 10428‚Äì10436, 2020. 7\n[32] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee,\nand Gary Bradski. Kornia: an open source differentiable\ncomputer vision library for pytorch. In Proceedings of\nthe Winter Conference on Applications of Computer Vision ,\npages 3674‚Äì3683, 2020. 8\n[33] Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Ju-\nlian Bitterwolf, Oliver Bringmann, Matthias Bethge, and\nWieland Brendel. A simple way to make neural networks\nrobust against diverse image corruptions. In Proceedings of\nthe European Conference on Computer Vision, pages 53‚Äì69.\nSpringer, 2020. 2, 7\n[34] Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and\nCho-Jui Hsieh. On the adversarial robustness of visual trans-\nformers. arXiv preprint arXiv:2103.15670, 2021. 2, 7\n[35] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. In Proceed-\nings of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\n2018. 3, 12\n[36] Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu\nChen, and Yupeng Gao. Is robustness the cost of accuracy?‚Äì\na comprehensive study on the robustness of 18 deep image\nclassification models. In Proceedings of the European Con-\nference on Computer Vision, pages 631‚Äì648, 2018. 2\n[37] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the Computer\nVision and Pattern Recognition, pages 2818‚Äì2826, 2016. 7\n[38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan\nBruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. In-\ntriguing properties of neural networks. In Proceedings of\nthe International Conference on Learning Representations ,\n2014. 2\n[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In Proceedings\nof the International Conference on Machine Learning, pages\n6105‚Äì6114. PMLR, 2019. 7\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efficient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 6, 7\n[41] Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin,\nNicolas Le Roux, and Ross Goroshin. An effective anti-\naliasing approach for residual networks. arXiv preprint\narXiv:2011.10675, 2020. 2\n[42] Haohan Wang, Songwei Ge, Eric P Xing, and Zachary C\nLipton. Learning robust global representations by penaliz-\ning local predictive power. Advances in Neural Information\nProcessing Systems, 2019. 2, 6\n[43] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 1, 3, 7\n[44] Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quan-\nquan Gu. Do wider neural networks really help adversarial\nrobustness? arXiv preprint arXiv:2010.01279, 2020. 2\n[45] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021. 1\n[46] Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and\nQuoc V Le. Smooth adversarial training. arXiv preprint\narXiv:2006.14536, 2020. 2\n[47] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\nLe. Self-training with noisy student improves imagenet clas-\nsification. In Proceedings of the Computer Vision and Pat-\ntern Recognition, pages 10687‚Äì10698, 2020. 2\n[48] Saining Xie, Ross Girshick, Piotr Doll ¬¥ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the Computer Vision and\nPattern Recognition, pages 1492‚Äì1500, 2017. 7\n[49] Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik\nLambrechts, Huan Zhang, Aojun Zhou, Kaisheng Ma,\nYanzhi Wang, and Xue Lin. Adversarial robustness vs.\nmodel compression, or both? In Proceedings of the Inter-\nnational Conference on Computer Vision , pages 111‚Äì120,\n2019. 2\n[50] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei\nYu, and Wei Wu. Incorporating convolution designs into vi-\nsual transformers. arXiv preprint arXiv:2103.11816, 2021.\n1, 2, 4\n[51] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 2, 7\n[52] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In Proceedings of the International Conference on\nLearning Representations, 2018. 6\n[53] Richard Zhang. Making convolutional networks shift-\ninvariant again. In Proceedings of International Conference\non Machine Learning, pages 7324‚Äì7334. PMLR, 2019. 2, 7\nAppendix\nA. Additional Results of Robustness Analysis\non Designed Components\nHere we will show the remaining results of robustness\nanalysis in section 3. As the each component has been dis-\ncussed detailly, we only give a summary of the results in\nappendix. We report the additional results of robustness\nanalysis in Table 10, 9, 12, 11, 13 and 14 respectively,\nwhere each table presents the results of one or some com-\nponents. The detailed architecture of models used in ro-\nbustness analysis on stage distribution is shown in Table 15.\nAlthough each robustness benchmark is consistent on the\noverall trend, we still find some special cases. For example,\nin Table 12, the V6 version of stage distribution poorly per-\nforms on adversarial robustness, but achieves best results\non IN-A and IN-R datasets, showing the superior general-\nization power. Another case is the token-to-token embed-\nder in Table 11. Compared with original linear embedder,\ntoken-to-token embedder obtains better results on IN-C, IN-\nA, IN-R and IN-SK datasets. However, under PGD attacker,\nit only gets the robust accuracy of 4.7%. The above phe-\nnomenon also indicates that using only several robustness\nbenchmarks is biased and cannot get a comprehensive as-\nsessment result. Therefore, we advocate that the works\nabout model robustness in future should consider multi-\nple benchmarks. For validating the generality of the pro-\nposed techniques, we show the robustness evaluation results\nwhen trained on other ViT architectures and larger datasets\n(ImageNet-22k) in Table 13 and 14.\nHeads 1 2 4 6 8 12\nAcc 69.0 71.7 73.1 73.4 73.9 73.5\nFGSM 17.6 21.4 22.8 24.6 25.2 24.7\nPGD 4.3 6.1 7.1 7.7 8.2 8.0\nIN-C (‚Üì) 79.5 72.9 69.0 68.5 67.7 68.2\nIN-A 5.1 6.9 8.2 8.3 8.9 8.4\nIN-R 28.1 32.9 33.9 34.1 34.2 33.7\nIN-SK 15.9 20.4 21.4 21.6 22.0 21.1\nTable 9. Additional results of robustness analysis on different head\nnumber.\nB. Feature Visualization\nIn general understanding, intra-class compactness and\ninter-class separability are crucial indicators to measure the\neffectiveness of a model to produce discriminative and ro-\nbust features. We use t-Distributed Stochastic Neighbor\nEmbedding (t-SNE) to visualize the feature sets extracted\nby ResNet50, DeiT-Ti, Swin-T, PVT-Ti and our RVT re-\nspectively. The features are produced on validation set\nof ImageNet and ImageNet-C. We randomly selected 10\nResNet50Deit-Ti Swin-T PVT-Ti Ours\nImageNetImageNet-C (GN)ImageNet-C (MB)\nFigure 4. t-SNE visualization of features produced by different\nmodels.\nResNet50 DeiT-S RVT-S\nFigure 5. Feature visualization of ResNet50, DeiT-S and our pro-\nposed RVT-S trained on ImageNet. Red boxes highlight the fea-\nture maps with high similarity.\nResNet50 RVT-Small\nFigure 6. Loss landscape of ResNet50 and RVT-S.\nclasses for better visualization. As shown in Figure 4, fea-\ntures extracted by our RVT is the closest to the intra-class\ncompactness and inter-class separability. It‚Äôs confirmed\nfrom the side that our RVT does have the stronger robust-\nness and classification performance.\nWe also visualize the feature maps of ResNet50, DeiT-S\nand our proposed RVT-S in Figure 5. Visualized features are\nextracted on the 5th layer of the models. The result shows\nResNet50 and DeiT-S contain a large part of redundant fea-\ntures, highlighted by red boxes. While our RVT-S reduces\nthe redundancy and ensures the diversity of features, reflect-\ning the stronger generalization ability.\nC. Loss Landscape Visualization\nLoss landscape geometry has a dramatic effect on gen-\neralization and trainability of the model. We visualize the\nloss surfaces of ResNet50 and our RVT-S in Figure 6. RVT-\nS has a flatter loss surfaces, which means the stability under\ninput changes.\nPosition Embeddings Acc FGSM PGD IN-C (‚Üì) IN-A IN-R IN-SK\nnone 68.3 15.8 3.6 82.4 5.2 24.3 12.0\nlearned absolute Pos. 72.2 22.3 6.2 71.1 7.3 32.6 20.3\nsin-cos absolute Pos. 72.0 21.9 5.9 71.9 7.0 31.4 20.2\nlearned relative Pos. [35] 71.8 22.3 6.1 71.6 7.6 32.5 18.6\ninput-conditioned Pos. [3] 72.4 21.5 5.3 72.5 6.8 31.0 18.0\nTable 10. Additional results of robustness analysis on different position encoding methods.\nPatch Emb. Local Conv. CLS PGD IN-C (‚Üì) IN-A IN-R IN-SKLinear Conv. T2T SA FFN\n‚úì ‚úì 6.2 71.1 7.3 32.6 20.3\n‚úì ‚úì 6.8 69.2 8.3 33.6 21.1\n‚úì ‚úì 4.7 69.6 10.1 36.7 23.8\n‚úì ‚úì ‚úì 9.0 76.9 4.8 28.7 16.6\n‚úì ‚úì ‚úì 12.7 65.0 8.4 39.0 31.9\n‚úì 12.0 70.0 7.4 32.5 20.2\nTable 11. Additional results of robustness analysis on different patch embeddings, locality of attention, convolutional FFN and the replace-\nment of CLS token.\nVar. [S1, S2, S3, S4] Acc FGSM PGD IN-C (‚Üì) IN-A IN-R IN-SK\nV1 [0, 0, 12, 0] 72.2 22.3 6.2 71.1 7.3 32.6 20.3\nV2 [0, 0, 10, 2] 74.8 24.3 6.8 66.9 8.8 35.5 21.9\nV3 [0, 2, 10, 0] 73.8 22.0 5.1 76.4 8.2 33.6 21.1\nV4 [0, 2, 8, 2] 76.4 22.3 4.5 71.5 10.3 36.8 23.9\nV5 [2, 2, 8, 0] 73.4 17.0 2.3 76.8 9.0 33.2 20.7\nV6 [2, 2, 6, 2] 76.4 17.5 1.9 71.6 11.2 36.8 23.1\nTable 12. Additional results of robustness analysis on stage distribution.\nModels Acc FGSM PGD IN-C (‚Üì) IN-A IN-R IN-SK\nDeiT-Ti 72.2 22.3 6.2 71.1 7.3 32.6 20.2\nDeiT-Ti‚àó 74.4 29.9 9.1 67.9 8.1 34.9 23.1\nConViT-Ti 73.3 24.7 7.5 68.4 8.9 35.2 22.4\nConViT-Ti‚àó 74.4 30.7 9.6 65.6 9.4 37.0 25.2\nPiT-Ti 72.9 20.4 5.1 69.1 6.2 34.6 21.6\nPiT-Ti‚àó 74.3 27.7 7.9 66.7 7.1 36.6 24.0\nDeiT-S 79.9 40.7 16.7 54.6 18.9 42.2 29.4\nDeiT-S‚àó 80.6 42.3 18.8 53.1 20.5 43.5 31.3\nConViT-S 81.5 41.0 17.2 49.8 24.5 45.4 33.1\nConViT-S‚àó 81.8 42.3 18.7 49.1 25.6 46.1 34.2\nPiT-S 80.9 41.0 16.5 52.5 21.7 43.6 30.8\nPiT-S‚àó 81.4 42.2 18.3 51.4 23.3 44.6 32.3\nTable 13. Additional results of position-aware attention scaling and patch-wise augmentation on other ViT architectures.\nModels Acc FGSM PGD IN-C (‚Üì) IN-A IN-R IN-SK\nDeiT-B 83.20 47.21 24.89 45.50 38.01 52.37 39.54\nRVT-B 83.57 53.67 30.45 44.26 41.00 49.67 35.01\nRVT-B‚àó 83.80 55.40 33.86 42.99 42.27 52.63 38.43\nTable 14. RVT pre-trained on ImageNet-22K and finetuned on ImageNet-1K.\nOutput Size Layer Name DeiT-Ti (V1) V4 V5\nStage 1 H\n4 √ó W\n4\nPatch Embedding C1 = 192 C1 = 96 C1 = 48\nTransformer\nEncoder - -\nÔ£Æ\nÔ£∞\nH1 = 48\nN1 = 1\nC1 = 48\nÔ£π\nÔ£ª √ó 2\nStage 2 H\n8 √ó W\n8\nPooling Layer - - k = 2√ó 2\nTransformer\nEncoder -\nÔ£Æ\nÔ£∞\nH2 = 48\nN2 = 2\nC2 = 96\nÔ£π\nÔ£ª √ó 2\nÔ£Æ\nÔ£∞\nH2 = 48\nN2 = 2\nC2 = 96\nÔ£π\nÔ£ª √ó 2\nStage 3 H\n16 √ó W\n16\nPooling Layer - k = 2√ó 2 k = 2√ó 2\nTransformer\nEncoder\nÔ£Æ\nÔ£∞\nH2 = 64\nN2 = 3\nC2 = 192\nÔ£π\nÔ£ª √ó 12\nÔ£Æ\nÔ£∞\nH3 = 64\nN3 = 3\nC3 = 192\nÔ£π\nÔ£ª √ó 8\nÔ£Æ\nÔ£∞\nH3 = 64\nN3 = 3\nC3 = 192\nÔ£π\nÔ£ª √ó 6\nStage 4 H\n32 √ó W\n32\nPooling Layer - k = 2√ó 2 k = 2√ó 2\nTransformer\nEncoder -\nÔ£Æ\nÔ£∞\nH3 = 64\nN3 = 6\nC3 = 384\nÔ£π\nÔ£ª √ó 2\nÔ£Æ\nÔ£∞\nH4 = 64\nN4 = 6\nC4 = 384\nÔ£π\nÔ£ª √ó 2\nTable 15. Detailed architecture of models used in robustness analysis on stage distribution. C, H and N represent the total feature\ndimension, feature dimension of each head and head number respectively. Only V4 and V5 are listed as examples. The other versions of\nthe model can be generalized by V4 and V5.",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.7166121006011963
    },
    {
      "name": "Computer science",
      "score": 0.6917131543159485
    },
    {
      "name": "Transformer",
      "score": 0.5842931866645813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5295836925506592
    },
    {
      "name": "Computation",
      "score": 0.41513803601264954
    },
    {
      "name": "Computer engineering",
      "score": 0.4111970067024231
    },
    {
      "name": "Machine learning",
      "score": 0.3869199752807617
    },
    {
      "name": "Algorithm",
      "score": 0.16136124730110168
    },
    {
      "name": "Engineering",
      "score": 0.13941225409507751
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}