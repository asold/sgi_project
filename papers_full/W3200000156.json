{
  "title": "Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-Supervised Learning",
  "url": "https://openalex.org/W3200000156",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5069169167",
      "name": "Ilwi Yun",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5102861073",
      "name": "Hyuk-Jae Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5066755055",
      "name": "Chae Eun Rhee",
      "affiliations": [
        "Inha University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6703405610",
    "https://openalex.org/W2798665861",
    "https://openalex.org/W2300779272",
    "https://openalex.org/W2520707372",
    "https://openalex.org/W2806446538",
    "https://openalex.org/W2935920407",
    "https://openalex.org/W3034556105",
    "https://openalex.org/W2796059865",
    "https://openalex.org/W6741449649",
    "https://openalex.org/W3174725336",
    "https://openalex.org/W3109126057",
    "https://openalex.org/W6796365952",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2941202065",
    "https://openalex.org/W2953288645",
    "https://openalex.org/W3034728336",
    "https://openalex.org/W2983498482",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2969450841",
    "https://openalex.org/W3092407498",
    "https://openalex.org/W2965084509",
    "https://openalex.org/W2973699682",
    "https://openalex.org/W2883505290",
    "https://openalex.org/W2963825193",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3174969937",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W2586114507",
    "https://openalex.org/W3089993621",
    "https://openalex.org/W2886168685",
    "https://openalex.org/W2982336692",
    "https://openalex.org/W3175201472",
    "https://openalex.org/W2738549586",
    "https://openalex.org/W2963760790",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964339842",
    "https://openalex.org/W2982574419",
    "https://openalex.org/W3009662750",
    "https://openalex.org/W2982102242",
    "https://openalex.org/W2900510798",
    "https://openalex.org/W2557465155",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W2171740948",
    "https://openalex.org/W3081167590",
    "https://openalex.org/W2985775862",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2950559486",
    "https://openalex.org/W2951234442",
    "https://openalex.org/W3109728105",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2115579991"
  ],
  "abstract": "Due to difficulties in acquiring ground truth depth of equirectangular (360) images, the quality and quantity of equirectangular depth data today is insufficient to represent the various scenes in the world. Therefore, 360 depth estimation studies, which relied solely on supervised learning, are destined to produce unsatisfactory results. Although self-supervised learning methods focusing on equirectangular images (EIs) are introduced, they often have incorrect or non-unique solutions, causing unstable performance. In this paper, we propose 360 monocular depth estimation methods which improve on the areas that limited previous studies. First, we introduce a self-supervised 360 depth learning method that only utilizes gravity-aligned videos, which has the potential to eliminate the needs for depth data during the training procedure. Second, we propose a joint learning scheme realized by combining supervised and self-supervised learning. The weakness of each learning is compensated, thus leading to more accurate depth estimation. Third, we propose a non-local fusion block, which can further retain the global information encoded by vision transformer when reconstructing the depths. With the proposed methods, we successfully apply the transformer to 360 depth estimations, to the best of our knowledge, which has not been tried before. On several benchmarks, our approach achieves significant improvements over previous works and establishes a state of the art.",
  "full_text": "Improving 360‚ó¶Monocular Depth Estimation\nvia Non-local Dense Prediction Transformer\nand Joint Supervised and Self-Supervised Learning\nIlwi Yun1 , Hyuk-Jae Lee1, Chae Eun Rhee2\n1 Seoul National University, Korea\n2 Inha University, Korea\nyuniw@capp.snu.ac.kr, hjlee@capp.snu.ac.kr, chae.rhee@inha.ac.kr\nAbstract\nDue to difÔ¨Åculties in acquiring ground truth depth of equirect-\nangular (360\u000e) images, the quality and quantity of equirectan-\ngular depth data today is insufÔ¨Åcient to represent the various\nscenes in the world. Therefore, 360 \u000e depth estimation stud-\nies, which relied solely on supervised learning, are destined\nto produce unsatisfactory results. Although self-supervised\nlearning methods focusing on equirectangular images (EIs)\nare introduced, they often have incorrect or non-unique so-\nlutions, causing unstable performance. In this paper, we pro-\npose 360 \u000e monocular depth estimation methods which im-\nprove on the areas that limited previous studies. First, we\nintroduce a self-supervised 360 \u000e depth learning method that\nonly utilizes gravity-aligned videos, which has the potential\nto eliminate the needs for depth data during the training pro-\ncedure. Second, we propose a joint learning scheme realized\nby combining supervised and self-supervised learning. The\nweakness of each learning is compensated, thus leading to\nmore accurate depth estimation. Third, we propose a non-\nlocal fusion block, which can further retain the global infor-\nmation encoded by vision transformer when reconstructing\nthe depths. With the proposed methods, we successfully ap-\nply the transformer to 360 \u000e depth estimations, to the best of\nour knowledge, which has not been tried before. On several\nbenchmarks, our approach achieves signiÔ¨Åcant improvements\nover previous works and establishes a state of the art.\nIntroduction\nRecently, research interest in processing equirectangular\n(360‚ó¶) images has increased as virtual reality enters the\nlimelight. Equirectangular images (EIs) have advantages\nover traditional rectilinear images (RIs) in that they enable\na 360¬∞ Ô¨Åeld of view. This beneÔ¨Åt, however, complicates the\nacquisition of ground truth depths. Aside from the technical\ndifÔ¨Åculties associated with 360¬∞ depth scanners, one practi-\ncal difÔ¨Åculty is that sensors would be visible from the 360¬∞\nRGB cameras, leading to partially obscured images (Matzen\net al. 2017; Zioulis et al. 2018). Moreover, to acquire diverse\nand realistic synthesized data, numerous things should be set\nexquisitely, which often requires professional designers and\ntools (Zheng et al. 2020). Due to such problems, the qual-\nity and quantity of equirectangular depth data today is in-\nsufÔ¨Åcient to represent fully the various scenes in the world.\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The effect of joint learning. Unlike the network\ntrained via supervised learning only (middle row), the net-\nwork trained via joint learning is able to distinguish win-\ndows from the walls (bottom row)\nTherefore, learning 360 ‚ó¶depths in a supervised manner is\ndestined to produce unsatisfactory results because the per-\nformance of supervised learning is highly dependent on the\ndataset. To overcome the lack of data, learning 360‚ó¶depths\nin a self-supervised manner has been attempted. However,\nprevious methods require either calibrated stereo EI pairs\n(Payen de La Garanderie, Atapour Abarghouei, and Breckon\n2018; Zioulis et al. 2019; Wang et al. 2020b) or conver-\nsion to cubemap projection (Wang et al. 2018a), both of\nwhich have limitations with regard to further improvements.\nMoreover, self-supervised learning often delivers incorrect\nor non-unique solutions (e.g., light reÔ¨Çected object), which\ncause unstable performance.\nIn this paper, we propose 360 ‚ó¶monocular depth estima-\ntion methods which improve on the areas that limited previ-\nous studies. First, we propose a self-supervised method for\nthe learning of depth that only utilizes gravity-aligned video\nsequences, which has the potential to eliminate the needs\nof depth data during the training procedure. Similar to prior\nwork (Zioulis et al. 2019), we utilize the relationships be-\ntween consecutive scenes but improve it through consistency\nbetween depths. Second, we propose a joint learning scheme\nrealized by combining supervised and self-supervised learn-\ning. Despite the limitations of each learning scheme, all pre-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3224\nvious works on the 360 ‚ó¶depth estimation, to the best of\nour knowledge, have relied solely on either supervised or\nself-supervised learning. We show that the joint learning im-\nproves the unstable performance of self-supervised learn-\ning as well as the incorrect prediction of supervised learn-\ning caused by data scarcity, as visualized in Figure 1. Third,\nwe propose a non-local fusion block which improves on the\nareas missed by vision transformers for dense prediction.\nThrough non-local operations, global information encoded\nby a transformer can be further retained when reconstruct-\ning the depths. Under a challenging environment for a trans-\nformer (i.e., lack of a large-scale dataset), we were able to\ntrain the vision transformer successfully using the features\nlearned from depth of RIs. To the best of our knowledge,\nthis is the Ô¨Årst work applying transformers successfully to\n360‚ó¶depth estimation. Our approaches achieve signiÔ¨Åcant\nimprovements over previous works on several benchmarks,\nthus establishing a state of the art.\nBackground and Related Work\nEI Geometry\nAlthough EIs appear to be two dimensional (2D) images,\nEIs and RIs are different in many ways. EIs are generated\nby Ô¨Çattening the rays projected on a three dimensional (3D)\nsphere, whereas RIs are generated by directly projecting rays\non a 2D plane. Therefore, EIs are 3D images despite their 2D\nstructure. The spherical coordinates (\u0012;\u001e;\u001a) are often used\ninstead of the pixel coordinates (x;y) for this reason, and\nrelationship between them is illustrated in Figure 2. Each\nvalue of \u0012 ‚àà(0¬∞;360¬∞) and \u001e ‚àà(0¬∞;180¬∞) represents the\nlatitude and longitude of an EI, and \u001adenotes the radius of\nthe sphere. Further, the spherical coordinates can be con-\nverted to Cartesian coordinates (Xc;Yc;Zc) by Eq.1\n8\n<\n:\nXc = \u001a¬∑sin(\u001e) ¬∑cos(\u0012)\nYc = \u001a¬∑sin(\u001e) ¬∑sin(\u0012)\nZc = \u001a¬∑cos(\u001e)\n(1)\n(a) Sphere\n (b) Equirectangular\nFigure 2: Equirectangular geometry\nMeanwhile, rotations on EIs are deÔ¨Åned as the yaw, pitch\nand roll. Due to undesirable visual changes (Sun, Sun, and\nChen 2021; Davidson, Alvi, and Henriques 2020), a gravity-\naligned structure (i.e., with the roll and pitch set to 0 ‚ó¶)\nis generally assumed in equirectangular depth benchmarks\n(Armeni et al. 2017; Chang et al. 2017; Zioulis et al. 2018;\nZheng et al. 2020) and in recent studies (Pintore et al. 2021;\nSun, Sun, and Chen 2021). If captured images/videos are\nnot gravity-aligned, they can be calibrated afterwards (Xian\net al. 2019; Davidson, Alvi, and Henriques 2020).\nSupervised 360\u000e Depth Estimation\nOmnidepth (Zioulis et al. 2018) presents a 3D60 dataset\n(Matterport3D, Stanford3D and SunCG) by re-rendering\nprevious 360 ‚ó¶data [e.g., Matterport (Chang et al. 2017),\nStanford (Armeni et al. 2017)], which are now commonly\nused for training 360 ‚ó¶depths. Bifuse (Wang et al. 2020a)\njointly uses cubemap projected images with EIs to improve\nthe performance. SliceNet (Pintore et al. 2021) splits the\ninputs and recovers them through long short-term mem-\nory (Xingjian et al. 2015) to retain the global information.\nHoHoNet (Sun, Sun, and Chen 2021) improves the perfor-\nmance and computational efÔ¨Åciency by focusing on the per-\ncolumn information of the gravity-aligned EIs. Recently,\nmulti-task learning among the depth, layout and semantics\nwas attempted to improve performance outcomes. (Jin et al.\n2020) regularizes the depth considering the layout, while\n(Zeng, Karaoglu, and Gevers 2020) train the layout, seman-\ntic and depth simultaneously.\nSelf-Supervised 360\u000e Depth Estimation\nSelf-supervised depth learning has been widely attempted\nfor RIs based on the following intuition: The closer the ob-\nject is to the camera, the greater the change in the object‚Äôs\nposition when the camera moves (Garg et al. 2016; Godard,\nMac Aodha, and Brostow 2017; Godard et al. 2019; Gor-\ndon et al. 2019). However, that intuition is not applied to EIs\ndue to the different geometry, as shown in Figure 3. When\nthe camera moves forward (denoted by the red arrows), the\nrelative movements of objects in the scenes of RIs are repre-\nsented by the dotted arrows in Figure 3 (a). The movement\nof objects only depends on the camera movements (direc-\ntion) and corresponding depths (magnitude). The movement\nof objects in EIs, however, is also affected by the positions\nof the objects in EIs. As the camera moves forward, the ob-\njects in front of the camera become closer, while those of\nopposite side become further away, as shown in Figure 3 (b).\nBecause more variables control the objects of EIs, learning\n360‚ó¶depths using a self-supervision becomes more difÔ¨Åcult.\n(a) Rectilinear\n (b) Equirectangular\nFigure 3: Difference in movements of a scene\nIn addition, self-supervised depth learning often has in-\ncorrect or non-unique solutions in some cases. Light re-\nÔ¨Çected objects, which are not predictable using only the\ndepth and camera motion, is one such example. These ob-\njects cause the network to output wrong depth values be-\ncause the light reÔ¨Çection is controlled by light sources, not\ndepths (Refer to Technical Appendix for more examples).\nAlthough several attempts have been made to remove those\n3225\nkinds of intractable objects during the training process (Go-\ndard et al. 2019; Gordon et al. 2019; Tan et al. 2021), there\nremain objects controlled by numerous variables and, not\nmerely by depth and camera motions, which makes self-\nsupervised learning a challenge.\n360SD-Net (Wang et al. 2020b) and SvSyn (Zioulis et al.\n2019) use stereo EI pairs as input for EI depth training\ndata to simplify the relationship between the depth, image\nand camera motions. However, 360 ‚ó¶Ô¨Åeld of view makes\nit difÔ¨Åcult to acquire stereo EIs using two 360 ‚ó¶cameras\ngiven that each camera is captured by others, which limits\nthe use of this method. EBS (Payen de La Garanderie, At-\napour Abarghouei, and Breckon 2018) proposes the method\nthat uses relatively abundant RI stereo pairs. They distort\nRIs considering the EI geometry, and use them as training\ndata. However, distorted data has a restricted Ô¨Åeld of view\n(<90¬∞), which cannot replace the EIs fundamentally. (Wang\net al. 2018a) transforms EIs into cubemap-projected images\nto alleviate the difference in the geometry. However, cube\nmap projection not only leads to discontinuity between each\nof the cubemap faces which results in large errors, but also\nrequires additional computations (Cheng et al. 2018; Wang\net al. 2020a).\nVision Transformer\nRecently, the vision transformer (ViT) network architecture\nwas proposed for image classiÔ¨Åcation (Dosovitskiy et al.\n2020). In this architecture, the transformer (Vaswani et al.\n2017), which has been widely used in natural language pro-\ncessing, is adopted instead of convolution block. Images are\nsplit into multiple Ô¨Çattened patches and are encoded by a\ntransformer. ViT achieves results that are comparable to or\neven better than those of a convolutional neural network\n(CNN) on image classiÔ¨Åcation tasks. Furthermore, it was\nrecently demonstrated that ViT yields notable performance\nimprovements on various vision tasks. The dense prediction\ntransformer (DPT) successfully applies ViT to segmentation\nand depth estimation tasks by upsampling the encoded fea-\ntures via convolution-based reassemble and fusion blocks\n(FB) (Ranftl, Bochkovskiy, and Koltun 2021). Reassemble\nblocks reassemble the encoded features into 3D featuresRs,\nwhereas fusion blocks upsample the Rs into fused features\nFs. Unlike a CNN, however, the transformer lacks induc-\ntive bias, necessitating large-scale dataset. Under an envi-\nronment with an insufÔ¨Åcient dataset, the performance of the\ntransformer becomes worse than that of a CNN (Dosovitskiy\net al. 2020). Therefore, several attempts have been made to\nalleviate data dependencies through multiple known tech-\nniques, such as knowledge distillation (Touvron et al. 2021).\nProposed Method\nOverall Architecture\nThe overall structure of the proposed training procedure,\nas illustrated in Figure 4, is composed of two Ô¨Çows: self-\nsupervised learning (black arrows) and supervised learn-\ning (brown arrows). For self-supervised learning Ô¨Çow, non-\nlocal DPT estimates the depth (D;D ‚Ä≤) for each consecu-\ntive video scene (V;V ‚Ä≤), while PoseNet predicts the cam-\nera motion (\u000ev;\u000er ) between them. Then, each scene and\ndepth at different viewpoints are reconstructed through sam-\npling function (fs). Image consistency (LI), depth consis-\ntency (LD) and pose consistency (LP) losses are imposed by\ncomparing reconstructed samples with corresponding video\nscenes and the estimated depths. For supervised learning\nÔ¨Çow, estimated depth (D I) for image input (I ) is com-\npared with the ground truth (D g\nI). Traditional pixel-wise\nloss (Lpix) and gradient loss (Lgrad) are imposed. Overall,\nthe objective function is constructed via Eq.2. Here, \u0015I and\n\u0015D are hyper-parameters balancing the supervised and self-\nsupervised losses. Each element of Eq.2 will be described in\nthe following sections.\nLtotal = \u0015I ¬∑LI + \u0015D ¬∑LD + LP + Lpix + Lgrad (2)\nSelf-Supervised Losses\nIn this subsection, we introduce the self-supervised learn-\ning Ô¨Çow illustrated in Figure 4. First, we formulate the\nrelationships among the depth, camera motion and gravity-\naligned 360‚ó¶video sequences. Then, the image consistency,\ndepth consistency and pose consistency losses are explained.\nRelationships between consecutive scenes in videoCon-\nsecutive video scenes V ‚ààR3√óH√óW and V‚Ä≤‚ààR3√óH√óW\ncan be expressed as spherical coordinate (\u0012;\u001e;\u001a) and\n(\u0012‚Ä≤;\u001e‚Ä≤;\u001a‚Ä≤), respectively. Here, each of the\u001aand \u001a‚Ä≤values can\nbe considered as estimated depths, which are denoted corre-\nspondingly as D ‚ààR1√óH√óW and D‚Ä≤‚ààR1√óH√óW. When\nthe video proceeds from V to V‚Ä≤(i.e., V ‚Üí V‚Ä≤), the trans-\nlation and rotation of the camera between scenes are deÔ¨Åned\nas \u000ev ‚ààR3√ó1√ó1 and \u000er ‚ààR3√ó1√ó1. Because we assume\nthat the videos are gravity-aligned, \u000ery ‚ààR1√ó1√ó1;\u000erz ‚àà\nR1√ó1√ó1 can be set as a constant (i.e.,0). Therefore, the cam-\nera motion is simpliÔ¨Åed to four variables: \u000evx;\u000evy;\u000evz and\n\u000erx. Under this environment, Eq.3 is formulated, which de-\nnotes the movements of the 3D scene point between V and\nV‚Ä≤in Cartesian coordinates according to the camera motion\nand depth .\n8\n<\n:\n\u001a¬∑cos(\u0012‚àí\u000erx) ¬∑sin(\u001e) ‚àí\u000evx = \u001a‚Ä≤¬∑cos(\u0012‚Ä≤) ¬∑sin(\u001e‚Ä≤)\n\u001a¬∑sin(\u001e) ¬∑sin(\u0012‚àí\u000erx) ‚àí\u000evy = \u001a‚Ä≤¬∑sin(\u001e‚Ä≤) ¬∑sin(\u0012‚Ä≤)\n\u001a¬∑cos(\u001e) ‚àí\u000evz = \u001a‚Ä≤¬∑cos(\u001e‚Ä≤)\n(3)\nBy solving Eq.3, a closed-form expression of Eq.4 is ob-\ntained, representing the relationship between V (\u0012;\u001e;\u001a) and\nV‚Ä≤(\u0012‚Ä≤;\u001e‚Ä≤;\u001a‚Ä≤) for the depth and camera motion. Eq.4 can be\nexpressed simply using fs in Eq.5.\n8\n>>>>\n>\n>\n<\n>>>>\n>\n>\n:\n\u0012‚Ä≤= tan‚àí1( \u001a¬∑sin(\u0012‚àí\u000erx) ¬∑sin(\u001e) ‚àí\u000evy\n\u001a¬∑cos(\u0012‚àí\u000erx) ¬∑sin(\u001e) ‚àí\u000evx\n)\n\u001e‚Ä≤= tan‚àí1(\u001a¬∑sin(\u0012‚àí\u000erx) ¬∑sin(\u001e) ‚àí\u000evy\nsin(\u0012‚Ä≤) ¬∑(\u001a¬∑cos(\u001e) ‚àí\u000evz) )\n\u001a‚Ä≤= cos(\u001e) ¬∑\u001a‚àí\u000evz\ncos(\u001e‚Ä≤)\n(4)\nV‚Ä≤= fs(V;D;\u000ev;\u000er ) (5)\n3226\nFigure 4: Overall architecture\nImage consistency lossFor consecutive scenes V ‚Üí V‚Ä≤,\nscenes at a different view point V‚Ä≤\nsyn can be synthesized\nfrom V with Eq.6 assuming the depth and camera mo-\ntions are well estimated. In the same vein, Vsyn can also\nbe synthesized considering the reversely ordered sequences\nV‚Ä≤‚Üí V. If the depth and camera motions are appropri-\nately estimated, each synthesized frame should be identical\nto each corresponding scene in the video (i.e., V‚Ä≤= V‚Ä≤\nsyn\nand V = Vsyn). Therefore, by regularizing the networks to\nsynthesize images equal to scenes in the video, the networ\nis indirectly trained to predict proper depth and camera mo-\ntions. Therefore, the image consistency loss is constructed\nas Eq.7, similar to the previous works (Zioulis et al. 2019).\nHere, SM indicates structural similarity (Wang et al. 2004),\n\u000brepresents the weight parameters, nindicates the number\nof pixels in an image.\nV‚Ä≤\nsyn = fs(V;D;\u000ev;\u000er )\nVsyn = fs(V‚Ä≤;D‚Ä≤;-\u000ev;-\u000er) (6)\nLI = 1\nn ¬∑\nnX\nk=1\n[\u000b¬∑(|V‚Ä≤‚àíV‚Ä≤\nsyn|+ |V‚àíVsyn|)\n+ (1‚àí\u000b) ¬∑(|1‚àíSM(V;Vsyn)|+ |1‚àíSM(V‚Ä≤;V ‚Ä≤\nsyn)|)]\n(7)\nDepth consistency loss Previous self-supervised learning\nstudies focusing on EIs only considered image consistency\n(Zioulis et al. 2019; Wang et al. 2020b). We argue that depth\nconsistency can also be used for regularization, which can\nfurther strengthen the training. Here, we introduce the depth\nconsistency loss for EIs, inspired by (Godard, Mac Aodha,\nand Brostow 2017). By regarding the depth as an image, the\ndepths of different viewpoints D‚Ä≤\nsyn and Dsyn can be syn-\nthesized using Eq.8. If the depth and camera motion are ac-\ncurately estimated, D‚Ä≤\nsyn and Dsyn become equal to D‚Ä≤and\nD, respectively. Therefore, similar to the image consistency\nloss, LD can be constructed with Eq.9. The estimated depths\nshould be consistent across the scenes to minimize the loss,\nwhich causes the network to check the images in more de-\ntail.\nD‚Ä≤\nsyn = fs(D;D;\u000ev;\u000er )\nDsyn = fs(D‚Ä≤;D‚Ä≤;-\u000ev;-\u000er) (8)\nLD = 1\nn ¬∑\nnX\nk=1\n(|D‚Ä≤‚àíD‚Ä≤\nsyn|+ |D‚àíDsyn|) (9)\nPose consistency lossIf PoseNet P is properly trained, es-\ntimated camera motions for scenes in reverse order (i.e.,\nV ‚Üí V‚Ä≤and V‚Ä≤‚Üí V) should also have the opposite di-\nrection. Therefore, the pose consistency loss LP can be es-\ntablished as Eq.10.\nLP = 1\nn ¬∑\n4X\nk=1\n|P(V;V ‚Ä≤) ‚àíP(V‚Ä≤;V )| (10)\nSupervised Losses for Joint Learning\nAlthough self-supervised learning on videos has the poten-\ntial to offer more accurate depth estimation, it has draw-\nbacks that should be resolved (i.e., non-optimal solutions).\nSupervised learning can alleviate those drawbacks because\nit has supervisions in the areas (e.g., light reÔ¨Çection) which\ncause problems in self-supervised learning. Conversely, self-\nsupervised learning can diversify the features learned from\nsupervised learning through various video sequences, which\nmakes the network perform well at the unexposed data.\nFrom this observation, we propose to use the supervised\nand self-supervised losses jointly. However, the scale of\nthe depth value differs according to the depth-acquisition\nmethod used or the environment (Eigen, Puhrsch, and Fer-\ngus 2014; Chen et al. 2016; Wang et al. 2019; Ranftl et al.\n2019). Therefore, scale and shift ambiguities regarding the\ndepth must be resolved in advance to use both losses to-\ngether. If this is not done, they conÔ¨Çict harshly and pro-\nduce even worse performances. For this reason, we initially\nalign the scale and shift of the depth via Eq.11 utilizing\nschemes proposed by (Ranftl et al. 2019). Considering the\nper-column characteristics of the gravity-aligned EIs (Sun,\nSun, and Chen 2021), the depth is aligned in a column-\nwise manner. Here, Dg\nI ‚ààR1√óH√óW indicates the ground\ntruth depth, DA\nI ‚ààR1√óH√óW is the aligned depth, where\n3227\ns;t ‚ààR1√ó1√óW represents the per-column scale and shift\nparameters.\ns;t = argmin\ns;t\n(s¬∑DI + t‚àíDg\nI)\nDA\nI = s¬∑DI + t\n(11)\nThen, we apply traditional pixel-wise loss expressed as\nEq.12 and the gradient loss (Li and Snavely 2018) of Eq.13,\nwhile the gradient loss is calculated on four different scales.\nThe gradient loss induces the network to place emphasis on\nthe edges of the estimated depths.\nLpix = 1\nn ¬∑\nnX\nk=1\n|(DA\nI ‚àíDg\nI)| (12)\nLgrad = 1\nn ¬∑\nnX\nk=1\n|‚àáx(DA\nI ‚àíDg\nI) +‚àáy(DA\nI ‚àíDg\nI)|(13)\nNon-local Dense Prediction Transformer\nNon-local fusion blockViT has advantages over a CNN\nin that it can see the input images globally. For dense pre-\ndictions, however, these advantages might be weakened.\nWhen upsampling encoded features via convolution-based\nFB (Ranftl, Bochkovskiy, and Koltun 2021), the receptive\nÔ¨Åeld is bounded to the convolution kernel size. Because EI\ncontains a geometric structure (e.g., a wall) which should\nbe seen in a global manner (Pintore et al. 2021), losing the\nglobal outlook may result in non-accurate depth estimations.\nFor these reasons, we propose the use of a non-local fusion\nblock (NLFB), which performs non-local operations (Wang\net al. 2018b) on each feature going into the FB, as shown\nin Figure 5. Here, we deÔ¨Åne Fs ‚ààRC√óHs√óWs as the fused\nfeatures at scale s and Ns ‚ààRC√óHs√óWs as the non-local\nfused features. For the fused feature at the ith index Fs\ni , the\nnon-local fused feature Ns\ni is calculated with Eq.14, where\nWs\n\u0012;\u001e;g ‚ààRC=2√ó1√ó1 and Ws\nz ‚ààRC√ó1√ó1 are the weight\nmatrix to be learned and j denotes the array of all possible\nindexes. The features of each index i are reconstructed by\nseeing all other indexes j, which makes the network to con-\ntinue seeing the features from a global perspective. There-\nfore, non-local DPT, which uses a NLFB instead of a FB,\nyields more accurate dense predictions.\nf(Fs\ni ;Fs\nj) =e(Ws\n\u0012Fi)TWs\n\u001eFj\nC(Fs) =\nX\n‚àÄj\nf(Fs\ni ;Fs\nj)\nNs\ni = Fs\ni + Ws\nz\nC(Fs)\nX\n‚àÄj\nf(Fs\ni ;Fs\nj)Ws\ngFs\nj\n(14)\nFine-tuning on EI depthDue to lack of a inductive bias\n(e.g., locality assumption), a large-scale dataset is needed\nto train a well-performing transformer (Dosovitskiy et al.\n2020). Considering the small amount of EI depth data, this\ndisadvantage is especially critical in EI depth estimation\nstudies. To overcome this hardship, we utilize a pre-trained\nmodel which is learned from depth of RIs based on the fol-\nlowing observation: Depth estimations on RIs and EIs work\nùë≠ùíè ‚Üí ùëµùíè\nùëπùíè\n‚Ä¶ ‚Ä¶\nFusion\nFusion\nùëπùíè‚àíùüè\nFigure 5: Non-local fusion block\nsimilarly from the relative point of view. Here, we assume\nthat two objects A and B are captured by RI and EI cam-\neras, respectively. If it is perceived thatAis closer than Bin\nRIs, it also should be the case in EIs. This implies that fea-\ntures learned from depth of RIs are also useful for EI depths.\nTherefore, we initialize the weights of our network using the\ntransformer trained with RI depths and Ô¨Åne-tune the whole\nnetwork using EI depths with some additional settings.\nAlthough estimated depths are aligned via Eq.11 during\nthe training procedure, we observe that scale mismatches in\ndepth between two learning Ô¨Çows dominate the total loss.\nBefore aligning depths via Eq.11, we thus robustly adjust\nthe scales of ground truth and the estimated depth of super-\nvised learning Ô¨Çows based on the scales of the depth learned\nfrom self-supervised losses. Further, we also pre-train the\npose network to minimize the negative effects of incorrect\npose estimations in the early phase of training. In this way,\nwe successfully transfer the various features learned from\nlarge-scale RI depths to the equirectangular geometry. More\ndetails are described in Technical Appendix.\nExperiments\nExperiments are composed of four parts. First, we brieÔ¨Çy\nexplain the experiment environment in Section . Then, we\nevaluate our methods under the various settings in Sections\nand . Finally, the effect of each proposed schemes is ana-\nlyzed through an ablation study in Section .\nExperimental Setup\nEvaluation details For fair and reproducible experiments,\nwe compare our results with studies that provide a pretrained\nmodel in the open-source community. Omnidepth (Zioulis\net al. 2018), SvSyn (Zioulis et al. 2019), Bifuse (Wang et al.\n2020a) and HoHoNet (Sun, Sun, and Chen 2021), which are\nproven useful in numerous works, are compared. Stanford\n(Armeni et al. 2017) and 3D60 (Stanford3D, Matterport3D,\nSunCG) (Zioulis et al. 2018) datasets are used for evalua-\ntion. Matterport (Chang et al. 2017) dataset is not used since\nit requires additional data pre-processing which may provide\ndifferent results according to how it is processed. The fol-\nlowing standard depth evaluation metrics are used to com-\npare methods: absolute relative error (Abs.rel), squared rela-\ntive error (Sq.rel), root mean square error (RMS), root mean\nsquare log error (RMSlog) and relative accuracy measures\n(\u000e). Lower is better for Abs.rel, Sq.rel, RMS and RMSlog,\nwhereas higher is better for \u000e. Similar to (Ranftl et al. 2019;\nRanftl, Bochkovskiy, and Koltun 2021), we align the pre-\ndicted depths using Eq.11 for all methods before measuring\n3228\nDataset Method AbsRel Sq rel RMS RMSlog \u000e < 1.25 \u000e < 1.252 \u000e < 1.253\nStanford3D\nOmnidepth 0.1009 0.0522 0.3835 0.1434 0.9114 0.9855 0.9958\nSvSyn 0.1003 0.0492 0.3614 0.1478 0.9096 0.9822 0.9949\nBifuse 0.1214 0.1019 0.5396 0.1862 0.8568 0.9599 0.9880\nHoHoNet 0.0901 0.0593 0.4132 0.1511 0.9047 0.9762 0.9933\nOurs w/ FB 0.0669 0.0249 0.2805 0.1012 0.9652 0.9944 0.9983\nOurs w/ NLFB 0.0649 0.0240 0.2776 0.993 0.9665 0.9948 0.9983\nMatterport3D\nOmnidepth 0.1136 0.0671 0.4438 0.1591 0.8795 0.9795 0.9950\nSvSyn 0.1063 0.0599 0.4062 0.1569 0.8984 0.9773 0.9934\nBifuse 0.1330 0.1359 0.6277 0.2079 0.8381 0.9444 0.9815\nHoHoNet 0.0671 0.0417 0.3416 0.1270 0.9415 0.9838 0.9942\nOurs w/ FB 0.0729 0.0302 0.3089 0.1079 0.9574 0.9935 0.9980\nOurs w/ NLFB 0.0700 0.0287 0.3032 0.1051 0.9599 0.9938 0.9982\nSunCG\nOmnidepth 0.1450 0.1052 0.5684 0.1884 0.8105 0.9761 0.9941\nSvSyn 0.1867 0.1715 0.6935 0.2380 0.7222 0.9427 0.9840\nBifuse 0.2203 0.2693 0.8869 0.2864 0.6719 0.8846 0.9660\nHoHoNet 0.0827 0.0633 0.3863 0.1508 0.9266 0.9765 0.9908\nOurs w/ FB 0.0740 0.0338 0.3475 0.1073 0.9584 0.9949 0.9986\nOurs w/ NLFB 0.0715 0.0321 0.3401 0.1042 0.9625 0.9950 0.9986\nTable 1: Quantitative comparison on 3D60 dataset using the pre-trained baselines provided by each author. Numbers in bold\nindicate the best results.\nInputOmnidepthBifuseHoHoNetOursG.T\n SvSyn\n(a) Stanford3D\n (b) Matterport3D\n (c) Stanford\nFigure 6: Qualitative comparison on Stanford3D, Matterport3D and Stanford dataset. Here, Ours indicates the model using\nNLFB module. Additional results are included in Technical and Multimedia Appendix.\nthe errors. For more details about the implementation, train-\ning, evaluation environment and additional experiments, re-\nfer to the the Technical, Code and Multimedia Appendix.\nDiscussions on evaluationBecause training neural network\nis affected by numerous variables, it often becomes sensitive\neven to the small changes in hyper-parameters. Therefore,\nunifying the training environment of the all previous works\nmay not result in the fair comparison. Actually, the training\nsetup of each previous study differs signiÔ¨Åcantly (Zioulis\net al. 2018, 2019; Wang et al. 2020a; Sun, Sun, and Chen\n3229\nDataset Method AbsRel Sq rel RMS RMSlog \u000e < 1.25 \u000e < 1.252 \u000e < 1.253\nStanford\nOmnidepth 0.1930 0.0042 0.0143 0.2691 0.7663 0.9140 0.9635\nSvsyn 0.1844 0.0039 0.0137 0.2596 0.7806 0.9220 0.9676\nBifuse 0.1017 0.0019 0.0086 0.1783 0.9082 0.9722 0.9879\nHoHoNet 0.0801 0.0016 0.0074 0.1577 0.9355 0.9803 0.9902\nOurs w/ NLFB 0.0666 0.0015 0.0066 0.1461 0.9531 0.9836 0.9910\nTable 2: Quantitative comparison on Stanford dataset using the pre-trained baselines provided by each author. Numbers in bold\nindicate the best results.\nDataset Method AbsRel Sq rel RMS RMSlog \u000e < 1.25 \u000e < 1.252 \u000e < 1.253\nStanford3D\nBifuse 0.0421 0.0160 0.2199 0.0842 0.9752 0.9948 0.9983\nHoHoNet 0.0541 0.0237 0.2566 0.1030 0.9573 0.9915 0.9968\nOurs w/ NLFB 0.0344 0.0116 0.1921 0.0709 0.9843 0.9965 0.9987\nMatterport3D\nBifuse 0.0455 0.0186 0.2368 0.0859 0.9744 0.9943 0.9981\nHoHoNet 0.0612 0.0319 0.2950 0.1142 0.9523 0.9887 0.9960\nOurs w/ NLFB 0.0364 0.0125 0.1963 0.0700 0.9852 0.9966 0.9988\nSunCG\nBifuse 0.0323 0.0141 0.2067 0.0739 0.9811 0.9954 0.9985\nHoHoNet 0.0518 0.0291 0.2789 0.1082 0.9587 0.9898 0.9967\nOurs w/ NLFB 0.0233 0.0076 0.1574 0.0534 0.9915 0.9979 0.9992\nStanford\nBifuse 0.1237 0.0026 0.0114 0.2067 0.8684 0.9560 0.9823\nHoHoNet 0.1306 0.0028 0.0114 0.2138 0.8510 0.9511 0.9804\nOurs w/ NLFB 0.0992 0.0018 0.0080 0.1717 0.9147 0.9764 0.9865\nTable 3: Further quantitative comparison using the baselines re-trained under the same training environment. Numbers in bold\nindicate the best results.\n2021), which makes it difÔ¨Åcult to compare only the superi-\nority of each method. For more fair and clear comparison,\ntherefore, our methods are evaluated under the two settings.\nFirst, our method is compared with the pre-trained baselines\nprovided by each author (Zioulis et al. 2018, 2019; Wang\net al. 2020a; Sun, Sun, and Chen 2021) in the open-source\ncommunity at Section . Because the performance of them is\nguaranteed by each author, this evaluation is fair and repro-\nducible. Then, we compare our method with the baselines\nre-trained under the same training environment in Section ,\nwhich further clariÔ¨Åes the superiority of each method.\nComparison with the Pre-trained Baselines\nIn this subsection, the pre-trained baselines provided by each\nauthor are used for evaluation. Our model is trained using\n3D60 (Zioulis et al. 2018) and Stanford (Armeni et al. 2017)\ndatasets, which are also used as training data in previous\nworks.\nQuantitative resultsTable 1 shows the quantitative depth\nprediction results on 3D60 testset. Ours w/ FB represents\nthe model using normal fusion blocks (Ranftl, Bochkovskiy,\nand Koltun 2021), while Ours w/ NLFB is the model us-\ning the proposed non-local fusion blocks. Except for the\nAbs.rel metric on Matterport3D, our method achieves sig-\nniÔ¨Åcant improvements over previous works. We observe that\npre-trained Bifuse produces biased results on some speciÔ¨Åc\ntest splits, which results in worse quantitative results than\nothers. Meanwhile, Ours w/ NLFB provides better results\nthan Ours w/ FB for all cases, which demonstrates the effec-\ntiveness of NLFB. Table 2 shows the quantitative results on\nStanford testset. For all metrics, Ours w/ NLFB provides the\nbest results.\nQualitative results Figure 6 shows the qualitative depth\nestimation results. Omnidepth provides good results for\nsmall objects. Overall, however, unstable depth results\nare observed (e.g., wall). HoHoNet produces stable depth\nresults, but lacks detail. Small objects are not appropriately\npredicted, which is also reported as a weakness in their pa-\npers (Sun, Sun, and Chen 2021). Meanwhile, it is observed\nthat Bifuse provides Ô¨Åne qualitative results compared to\nother previous works. However, the results of Ours (/w\nNLFB) are much more accurate than Bifuse, which are\neven better than the ground truth for some cases. Holes\nand inaccurate depths are observed among the ground truth\ndepths, whereas Ours provides stable and accurate depth\nresults.\nComparison with the Re-trained Baselines\nIn this subsection, we compare our method with the base-\nlines re-trained under the same training environment to fur-\nther clarify the superiority of our approaches. Bifuse and\nHoHoNet, which are the most recent studies and sharing\nsimilar training environment, are used for evaluation. Fol-\nlowing the training environment of Bifuse and HoHoNet,\neach method including ours is re-trained using 3D60 dataset\nwith 512√ó1024 resolutions. Table 3 shows the quantitative\nresults on the re-trained models. Compared to the results\nin Section , some improvements are observed except at the\nStanford testset, which is expected considering that they are\ntrained with 3D60 dataset only with higher resolutions. As\nsimilar to the results in Section , however, our approach pro-\nvides the best results for all metrics at all testset. Considering\nthe results of Tables 1,2 and 3 altogether, it is clear that our\n3230\nID LI LD Lpix + Lgrad NLFB AbsRel Sq rel RMS RMSlog \u000e < 1.25 \u000e < 1.252 \u000e < 1.253\n1 X 0.1056 0.0120 0.0646 0.1611 0.9121 0.9792 0.9926\n2 X X 0.1048 0.0116 0.0631 0.1593 0.9151 0.9804 0.9928\n3 X 0.0867 0.0086 0.0520 0.1403 0.9451 0.9855 0.9941\n4 X X 0.0840 0.0079 0.0498 0.1360 0.9477 0.9870 0.9947\n5 X X X 0.0802 0.0071 0.0476 0.1302 0.9540 0.9887 0.9952\n6 X X X X 0.0781 0.0071 0.0470 0.1294 0.9537 0.9885 0.9953\nTable 4: Ablation study on Structure3D dataset. Lp is used for all methods. Numbers in bold indicate the best results.\n(a) Input\n (b) Ground truth\n(c) Lpix + Lgrad\n (d) LI + LD\n(e) Lpix + Lgrad + LI\n (f) Lpix + Lgrad + LI + LD\nFigure 7: Studies on loss functions\napproach provides better results than others.\nAblation Study\nIn this section, we analyze the effect of each component\nof the proposed scheme through an ablation study on the\nStructure3D dataset (Zheng et al. 2020). Structure3D, a re-\ncently proposed dataset, is not used in the training proce-\ndure. Therefore, it is suitable to demonstrate our arguments\non the proposed loss functions (e.g., perform well at unex-\nposed data). Table 4 shows the quantitative results when\nproposed schemes are added gradually. Lp is used in all\ncases, though it is omitted in Table 4 for better visualiza-\ntion. When only self-supervised losses are applied (ID 1 and\n2), the results are not good as expected. However, when self-\nsupervised losses are used with supervised losses (ID 4 and\n5), the performance increases dramatically compared to the\ncases when only supervised losses are applied (ID 3). In this\ncase, LD plays an important role when both losses are used\ntogether (ID 4 and 5). This results show that self-supervised\nlearning actually improves the depth estimation results when\ncombined with supervised learning. Also, it is observed that\nNLFB improves the performance further (ID 6).\nFigure 7 shows the qualitative result of the schemes in\nTable 4. The model trained only with supervised losses in\nFigure 7 (c) produces unsatisfactory results for some areas.\nThe black arrows indicate where the model predicts a dec-\noration in front of the curtain as a defect. Therefore, these\nparts are predicted as holes, which are often found in ground\n(a) Input\n (b) Ground truth\n(c) Fusion block\n (d) Non-local fusion block\nFigure 8: Effect of non-local fusion block\ntruth depths. This indicates that the model is highly affected\nby inaccurate ground truth depths. The blue arrows indicate\ncases where the model fails to distinguish a sofa from the\nÔ¨Çoor, which occurs because the model was not exposed to\nsuch cases during the training procedure. On the other hand,\nthe model trained with self-supervised losses in Figure 7 (d)\nwas able to recognize the black object at the curtain appro-\npriately, and distinguish the sofa from the Ô¨Çoor. However,\nit produces unstable depths overall. When LI is used with\nsupervised losses as shown in Figure 7(e), the problems in\nFigure 7 (c) are mitigated according to the results shown in\nFigure 7 (d). When LD is applied in addition, as shown in\nFigure 7 (f), the model was able to distinguish the objects\nproperly. Figure 8 shows the effect of a non-local fusion\nblock in more detail. The fusion block fails to recognize the\nwall as a single object, and therefore, reconstructs undesir-\nable depths (red arrows). On the other hand, the non-local\nfusion block reconstructs the depth well. This demonstrates\nthat NLFB makes the network to continue to see the features\nwith a wider view when reconstructing the depths.\nConclusion\nIn this paper, we introduce a self-supervised learning\nscheme, a joint objective function, and a non-local fusion\nblock, in an effort to address the problems found in stud-\nies of EI depth estimations. Through the proposed scheme,\nsigniÔ¨Åcant improvements over prior works are achieved, and\nthe beneÔ¨Åts of each proposed method are also analyzed. We\nbelieve that each contribution not only affects the EI depth\nestimation research but also provides insight for those in-\nvolved in studies of other vision tasks.\n3231\nAcknowledgments\nThis work was supported by Samsung Research Fund-\ning Center of Samsung Electronics under Project Number\nSRFC-IT1702-54.\nReferences\nArmeni, I.; Sax, S.; Zamir, A. R.; and Savarese, S. 2017.\nJoint 2d-3d-semantic data for indoor scene understanding.\narXiv preprint arXiv:1702.01105.\nChang, A.; Dai, A.; Funkhouser, T.; Halber, M.; Niessner,\nM.; Savva, M.; Song, S.; Zeng, A.; and Zhang, Y . 2017. Mat-\nterport3d: Learning from rgb-d data in indoor environments.\narXiv preprint arXiv:1709.06158.\nChen, W.; Fu, Z.; Yang, D.; and Deng, J. 2016. Single-image\ndepth perception in the wild. Advances in neural informa-\ntion processing systems, 29: 730‚Äì738.\nCheng, H.-T.; Chao, C.-H.; Dong, J.-D.; Wen, H.-K.; Liu, T.-\nL.; and Sun, M. 2018. Cube padding for weakly-supervised\nsaliency prediction in 360 videos. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 1420‚Äì1429.\nDavidson, B.; Alvi, M. S.; and Henriques, J. F. 2020. 360ÀÜ‚ó¶\nCamera Alignment via Segmentation. In Computer Vision‚Äì\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23‚Äì28, 2020, Proceedings, Part XXVIII 16, 579‚Äì595.\nSpringer.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEigen, D.; Puhrsch, C.; and Fergus, R. 2014. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. arXiv preprint arXiv:1406.2283.\nGarg, R.; Bg, V . K.; Carneiro, G.; and Reid, I. 2016. Un-\nsupervised cnn for single view depth estimation: Geometry\nto the rescue. In European conference on computer vision,\n740‚Äì756. Springer.\nGodard, C.; Mac Aodha, O.; and Brostow, G. J. 2017. Unsu-\npervised monocular depth estimation with left-right consis-\ntency. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 270‚Äì279.\nGodard, C.; Mac Aodha, O.; Firman, M.; and Brostow, G. J.\n2019. Digging into self-supervised monocular depth estima-\ntion. In Proceedings of the IEEE international conference on\ncomputer vision, 3828‚Äì3838.\nGordon, A.; Li, H.; Jonschkowski, R.; and Angelova, A.\n2019. Depth from videos in the wild: Unsupervised monoc-\nular depth learning from unknown cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 8977‚Äì8986.\nJin, L.; Xu, Y .; Zheng, J.; Zhang, J.; Tang, R.; Xu, S.; Yu,\nJ.; and Gao, S. 2020. Geometric Structure Based and Regu-\nlarized Depth Estimation From 360 Indoor Imagery. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 889‚Äì898.\nLi, Z.; and Snavely, N. 2018. Megadepth: Learning single-\nview depth prediction from internet photos. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2041‚Äì2050.\nMatzen, K.; Cohen, M. F.; Evans, B.; Kopf, J.; and Szeliski,\nR. 2017. Low-cost 360 stereo photography and video cap-\nture. ACM Transactions on Graphics (TOG), 36(4): 1‚Äì12.\nPayen de La Garanderie, G.; Atapour Abarghouei, A.; and\nBreckon, T. P. 2018. Eliminating the blind spot: Adapting\n3d object detection and monocular depth estimation to 360\npanoramic imagery. In Proceedings of the European Con-\nference on Computer Vision (ECCV), 789‚Äì807.\nPintore, G.; Agus, M.; Almansa, E.; Schneider, J.; and Gob-\nbetti, E. 2021. SliceNet: deep dense depth estimation from\na single indoor panorama using a slice-based representation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 11536‚Äì11545.\nRanftl, R.; Bochkovskiy, A.; and Koltun, V . 2021. Vi-\nsion transformers for dense prediction. arXiv preprint\narXiv:2103.13413.\nRanftl, R.; Lasinger, K.; Hafner, D.; Schindler, K.; and\nKoltun, V . 2019. Towards robust monocular depth esti-\nmation: Mixing datasets for zero-shot cross-dataset transfer.\narXiv preprint arXiv:1907.01341.\nSun, C.; Sun, M.; and Chen, H.-T. 2021. Hohonet: 360 in-\ndoor holistic understanding with latent horizontal features.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2573‚Äì2582.\nTan, J.; Lin, W.; Chang, A. X.; and Savva, M. 2021. Mir-\nror3D: Depth ReÔ¨Ånement for Mirror Surfaces. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 15990‚Äì15999.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J√©gou, H. 2021. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347‚Äì10357. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nWang, C.; Lucey, S.; Perazzi, F.; and Wang, O. 2019. Web\nstereo video supervision for depth prediction from dynamic\nscenes. In 2019 International Conference on 3D Vision\n(3DV), 348‚Äì357. IEEE.\nWang, F.-E.; Hu, H.-N.; Cheng, H.-T.; Lin, J.-T.; Yang, S.-\nT.; Shih, M.-L.; Chu, H.-K.; and Sun, M. 2018a. Self-\nsupervised Learning of Depth and Camera Motion from 360\nVideos. In Asian Conference on Computer Vision, 53‚Äì68.\nSpringer.\nWang, F.-E.; Yeh, Y .-H.; Sun, M.; Chiu, W.-C.; and Tsai,\nY .-H. 2020a. BiFuse: Monocular 360 Depth Estimation via\nBi-Projection Fusion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 462‚Äì\n471.\nWang, N.-H.; Solarte, B.; Tsai, Y .-H.; Chiu, W.-C.; and Sun,\nM. 2020b. 360sd-net: 360 stereo depth estimation with\n3232\nlearnable cost volume. In 2020 IEEE International Confer-\nence on Robotics and Automation (ICRA), 582‚Äì588. IEEE.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018b. Non-\nlocal neural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 7794‚Äì\n7803.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image process-\ning, 13(4): 600‚Äì612.\nXian, W.; Li, Z.; Fisher, M.; Eisenmann, J.; Shechtman, E.;\nand Snavely, N. 2019. UprightNet: geometry-aware camera\norientation estimation from single images. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 9974‚Äì9983.\nXingjian, S.; Chen, Z.; Wang, H.; Yeung, D.-Y .; Wong, W.-\nK.; and Woo, W.-c. 2015. Convolutional LSTM network:\nA machine learning approach for precipitation nowcasting.\nIn Advances in neural information processing systems, 802‚Äì\n810.\nZeng, W.; Karaoglu, S.; and Gevers, T. 2020. Joint 3d lay-\nout and depth prediction from a single indoor panorama im-\nage. In European Conference on Computer Vision, 666‚Äì682.\nSpringer.\nZheng, J.; Zhang, J.; Li, J.; Tang, R.; Gao, S.; and Zhou, Z.\n2020. Structured3d: A large photo-realistic dataset for struc-\ntured 3d modeling. In Computer Vision‚ÄìECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23‚Äì28, 2020,\nProceedings, Part IX 16, 519‚Äì535. Springer.\nZioulis, N.; Karakottas, A.; Zarpalas, D.; Alvarez, F.; and\nDaras, P. 2019. Spherical view synthesis for self-supervised\n360¬∞ depth estimation. In 2019 International Conference on\n3D Vision (3DV), 690‚Äì699. IEEE.\nZioulis, N.; Karakottas, A.; Zarpalas, D.; and Daras, P. 2018.\nOmnidepth: Dense depth estimation for indoors spherical\npanoramas. In Proceedings of the European Conference on\nComputer Vision (ECCV), 448‚Äì465.\n3233",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7450095415115356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7075628042221069
    },
    {
      "name": "Monocular",
      "score": 0.6333982944488525
    },
    {
      "name": "Supervised learning",
      "score": 0.5954449772834778
    },
    {
      "name": "Ground truth",
      "score": 0.588179886341095
    },
    {
      "name": "Transformer",
      "score": 0.5831387042999268
    },
    {
      "name": "Machine learning",
      "score": 0.47125864028930664
    },
    {
      "name": "Semi-supervised learning",
      "score": 0.4594498574733734
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3795667886734009
    },
    {
      "name": "Computer vision",
      "score": 0.3525891900062561
    },
    {
      "name": "Artificial neural network",
      "score": 0.2555420994758606
    },
    {
      "name": "Engineering",
      "score": 0.07390296459197998
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}