{
    "title": "BERTective: Language Models and Contextual Information for Deception Detection",
    "url": "https://openalex.org/W3156843431",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2890445908",
            "name": "Tommaso Fornaciari",
            "affiliations": [
                "Bocconi University"
            ]
        },
        {
            "id": "https://openalex.org/A2124569955",
            "name": "Federico Bianchi",
            "affiliations": [
                "Bocconi University"
            ]
        },
        {
            "id": "https://openalex.org/A314690538",
            "name": "Massimo Poesio",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A310222905",
            "name": "Dirk Hovy",
            "affiliations": [
                "Bocconi University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2091034860",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2899204128",
        "https://openalex.org/W2103333826",
        "https://openalex.org/W4293355737",
        "https://openalex.org/W2996507500",
        "https://openalex.org/W2111216449",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2137226992",
        "https://openalex.org/W2489406233",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W3013286495",
        "https://openalex.org/W4299349867",
        "https://openalex.org/W2954482098",
        "https://openalex.org/W2284721260",
        "https://openalex.org/W2251993863",
        "https://openalex.org/W2789851222",
        "https://openalex.org/W34149036",
        "https://openalex.org/W2163833659",
        "https://openalex.org/W2949957935",
        "https://openalex.org/W3034545571",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2914393943",
        "https://openalex.org/W2804621770",
        "https://openalex.org/W2040849715",
        "https://openalex.org/W3101004475"
    ],
    "abstract": "Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts' preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT's representations.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2699–2708\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n2699\nBERTective:\nLanguage Models and Contextual Information for Deception Detection\nTommaso Fornaciari\nBocconi University\nfornaciari@unibocconi.it\nMassimo Poesio\nQueen Mary University of London\nm.poesio@qmul.ac.uk\nFederico Bianchi\nBocconi University\nf.bianchi@unibocconi.it\nDirk Hovy\nBocconi University\ndirk.hovy@unibocconi.it\nAbstract\nSpotting a lie is challenging but has an enor-\nmous potential impact on security as well as\nprivate and public safety. Several NLP meth-\nods have been proposed to classify texts as\ntruthful or deceptive. In most cases, however,\nthe target texts’ preceding context is not con-\nsidered. This is a severe limitation, as any com-\nmunication takes place in context, not in a vac-\nuum, and context can help to detect deception.\nWe study a corpus of Italian dialogues contain-\ning deceptive statements and implement deep\nneural models that incorporate various linguis-\ntic contexts. We establish a new state-of-the-\nart identifying deception and ﬁnd that not all\ncontext is equally useful to the task. Only the\ntexts closest to the target, if from the same\nspeaker (rather than questions by an interlocu-\ntor), boost performance. We also ﬁnd that\nthe semantic information in language models\nsuch as BERT contributes to the performance.\nHowever, BERT alone does not capture the im-\nplicit knowledge of deception cues: its contri-\nbution is conditional on the concurrent use of\nattention to learn cues from BERT’s represen-\ntations.\n1 Introduction\n“The sky is bright green” is easily identiﬁed as false\nstatement under normal circumstances. However,\nfollowing “Look at this surreal painting,” the assess-\nment changes. Spotting falsehoods and deception\nis useful in many personal, economic, legal, and\npolitical situations – but it is also extremely compli-\ncated. However, the reliability of communication\nis the basis of the social contract, with implications\non personal, economic, legal, and political levels.\nThere has been a growing interest in automatic de-\nception detection from academia and industry in\nrecent years (see section 9).\nOne of the main research lines tries to increase\nthe collection of deception cues in terms of number\nand variety. For example, several successful studies\nshow how to exploit multi-modal signals, jointly an-\nalyzing verbal, video, and audio data (P´erez-Rosas\net al., 2015). For the same reason, several early\nstudies tried to identify deception cues through\nmanual feature annotation, like irony or ambigu-\nity (Fitzpatrick and Bachenko, 2012). While these\napproaches offer a broad and interpretable descrip-\ntion of the phenomenon, their main limitation lies\nin data collection and preprocessing difﬁculty.\nSurprisingly, so far, little attention has been\npaid to expanding the targets’ linguistic context,\nwhich is the easiest source of additional cues and\ndata. Even in dialogues, which by deﬁnition are\nexchanges between different speakers/writers, the\nmain focus is typically on the target text. None\nconsider the preceding statements, be they issued\nby the same speaker of an interlocutor.\nWe hypothesize that linguistic context can be\nuseful for text classiﬁcation. Based on a data set of\ndialogues in Italian Courts, we train models that in-\ncorporate knowledge both from the target sentence\nand different conﬁgurations of the previous ones.\nWe use Hierarchical Transformers and neural mod-\nels based on BERT for text-pair representations and\ncompare with the previous state-of-the-art methods\nand other non-contextual neural models, including\nBERT for single text representation.\nWe distinguish different kinds of context, de-\npending on the window size and the speaker’s iden-\ntity (same one as of the target sentence or different).\nWe ﬁnd that context carries useful information for\ndeception detection, but only if it is narrow and\nproduced by the same author of the target text.\nWe also ﬁnd that BERT’s semantic knowledge\nhelps the classiﬁcation, but only when it is com-\nbined with neural architectures suitable to discover\nstylistic patterns beyond the texts’ content that are\npotentially associated with deception.\nTo our knowledge, this is the ﬁrst study that tests\n2700\nthese methods on data collected from real, high-\nstakes conditions for the subjects and not from a\nlaboratory or game environment.\nContributions The contributions of this paper\nare as follows:\n• We evaluate ways to incorporate contextual\ninformation for detecting deception on real-\nlife data.\n• We signiﬁcantly outperform the previous state-\nof-the-art results.\n• We show that language models are useful for\nthe task, but they need the support of meth-\nods dedicated to detect deception’s stylomet-\nric features.\n2 Dataset\nWe use the DECOUR dataset (Fornaciari and Poe-\nsio, 2012), which includes courtroom data tran-\nscripts of 35 hearings for criminal proceedings held\nin Italian courts. This provides a unique source of\nreal deception data. The corpus is in Italian. It\nconsists of dialogues between an interviewee and\nsome interviewers (such as the judge, the prosecu-\ntor, the lawyer). Each dialogue contains a sequence\nof utterances of the different speakers. These ut-\nterances are called turns. By deﬁnition, adjacent\nturns come from different speakers. Each turn con-\ntains one or more utterances. Each utterance by the\ninterviewee is labeled as True, False or Uncertain.\nThe utterances of the other speakers are not labeled.\nTable 1 shows some corpus and labels’ statistics.\nRole Turns Utterances tokens\nInterviewee 2094 3015 42K\nInterviewers 2373 3124 87K\n4467 6139 129K\nLabels: True Uncertain False Tot.\nNumber: 1202 868 945 3015\nTable 1: DECOUR ’s statistics\nThe authors anonymized the data and released\nthem here.\n3 Experimental conditions\nFornaciari and Poesio (2013) use binary classiﬁca-\ntion (false utterances versus the true and uncertain\nones, aggregated together into one class of non-\nfalse utterances, see section 2, Table 1). To avoid\noverﬁtting training and testing on utterances from\nthe same hearing, they use leave-one-out cross-\nvalidation, where each fold constitutes one hearing.\nIn these settings, in each fold one hearing is used\nas test set, one as development, and the others as\ntraining set. For the sake of comparison, we fol-\nlowed the same approach. We ran ﬁve epochs of\ntraining for each fold, selecting the model with the\nbest F-score in the development set.\nWe also identify seven kinds of different contexts\nthat should help the classiﬁcation task, together\nwith the target utterance. They are as follows:\n1 previous utterance- 1prev. We consider the\nﬁrst utterance preceding the target, regardless\nof the speaker who issued the statement.\n2 previous utterances- 2prev. Same as above,\nbut here we collect the ﬁrst two sentences\nbefore the target.\n3 previous utterances- 3prev. In this case, we\ncollect the three previous utterances, again\nregardless of the speaker.\nSpeaker’s previous utterance- s-utt. In this con-\ndition, we consider the utterance preceding\nthe target only if the speaker is the same inter-\nviewee. If another speaker issues the previous\nutterance, it is not collected, and the target\nutterance remains without context.\nSpeaker’s previous utterances- s-utts. Similarly\nto the previous condition, we only collect the\ninterviewee’s utterances, but if the target utter-\nance is preceded by more than one utterance\n(within the same turn), they are all collected.\nIn other words, we collect all the turn’s utter-\nances until the target one.\nSpeaker’s previous utterances + turn- s-utturn.\nIn these conditions, we consider all the possi-\nble speaker’s utterances and the previous turn,\nwhich belongs to another speaker. If there are\nno previous speaker’s utterances, we only col-\nlect the previous turn. This would make the\ninstance equal to those created according to\nthe last condition.\nPrevious turn- turn. We collect the whole previ-\nous turn, regardless of the possible previous\nspeaker’s utterances. This is the only condi-\ntion where, by deﬁnition, the context is not\nproduced by the interviewee him/herself.\n2701\n4 Metrics and baselines\nWe evaluate the model on four metrics: accuracy,\nprecision, recall and, F-measure. While accuracy is\na standard metric, its informative power is limited\nwhen the data set is imbalanced, and the class of\ninterest is the minority class, like in this case. In\nfact, the majority class’s performance conceals the\nreal performance on the minority one. Even so, it\ncan be a problematic baseline to beat, as the simple\nheuristic of always predicting the majority class\ncan result in high accuracy. In DECOUR , non-false\nutterances are the majority class with 68.66% of\nthe instances. Therefore, this is the accuracy we\nwould obtain always predicting the majority class.\nWe use this majority-class prediction as a baseline.\nFor the models’ overall evaluation, we rely on the\nF-measure, which reﬂects the real proﬁciency of\nthe models balancing the correct predictions in the\ntwo classes.\nBesides the majority class prediction, which\nreaches an F-measure of 40.71, we also compare\nour models with the previous state-of-the-art. We\nuse the highest performance in F-measure from For-\nnaciari and Poesio (2013). In that experiment, they\njointly used Bag-Of-Words - BOW features and\nthe lexical features provided by the LIWC (Pen-\nnebaker et al., 2001) and applied an SVM classiﬁer\n(Drucker et al., 1997). The accuracy of that model\nis 70.18% and the F-measure 62.98 (table 2).\n5 Methods\nWe perform the classiﬁcation with several neural\nmodels. For all the models that do not rely on\nthe BERT contextual embeddings (Devlin et al.,\n2018), we used the pre-trained Fast Text embed-\ndings (Joulin et al., 2016) as initialization weights,\nand we ﬁne-tuned them during the training process.\nWe did not ﬁne-tune the contextual BERT embed-\ndings for reasons of computational load. However,\nthe high number of the models’ parameters required\na low learning rate, which we manually adjusted to\n1.e − 4, and a small batch size, which was 8. The\ndrop-out probability was 0.1.\n5.1 Neural baselines\nWe add two neural baselines: a Multi-Layer Percep-\ntron (MLP) and a Convolutional Neural Network\n(CNN).\nThe MLP did not beat the SVM’s performance.\nThe CNN’s F-measure was better than that of the\nSVM, but not signiﬁcantly. Also, the CNN proved\nto be less effective than the attention-based models\nthat did not exploit contextual information (table 2).\nTherefore we did not feed the MLP and the CNN\nwith contextual information and kept them as ad-\nditional neural baselines. However, to obtain their\nbest performance possible, we carried out a com-\nprehensive hyper-parameters search. For the MLP,\nwe found the best results with trainable FastText\nembeddings followed by two hidden layers. For\nthe CNN, we used 3 Convolutional-MaxPooling\nlayers with 32, 64, and 128 channels, respectively,\nand windows’ sizes of 2, 4, and 6.\n5.2 Transformers-based models\nBased on the success of the Transformer architec-\nture in NLP (Vaswani et al., 2017), we used them\nto create two kinds of models, hierarchical and non-\nhierarchical. We adopted a non-hierarchical struc-\nture to analyze the target sentence alone, and we\nimplemented Hierarchical Transformers to encode\nthe target sentence and the contextual information\njointly.\nIn the Hierarchical model, the input is not a sin-\ngle utterance but a series of utterances. We pad\nthe maximum number of sentences to 5. This limit\nallows us to collect the whole text from about the\n98% of the turns in DECOUR . However, as we\nwill see in sections 6 and 8, considering a broader\ncontext would not have been useful.\nNot considering the batch, the Hierarchical\nTransformers take as input a 3D tensor of Doc-\numents by Words by Embeddings. Each Words\nby Embeddings matrix is passed to a multi-layer,\nmulti-head Transformer that provides a represen-\ntation of each utterance, returning as output a ten-\nsor of the same shape of the input. A following\nfully-connected layer reduces the embeddings’ di-\nmension. The documents’ representations are then\nconcatenated into a 2D tensor and passed to an-\nother multi-layer, multi-head Transformer, which\nprovides the overall document representation. An-\nother fully connected layer is used to reduce the\ntensor’s last dimension, which is then reshaped to a\nrow vector. This vector is fed into the last fully con-\nnected layer that provides the prediction. Figure 1\nshows such an architecture\nWith the Hierarchical Transformer, we run the\nexperiments for the seven contexts described in\nsection 3. Again, we tuned our hyper-parameters.\nIn the hierarchical models, we used six layers and\nsix heads Transformers for the encoders both at\nutterance and at documents level. For the non-\nhierarchical model, two layers and two heads were\n2702\nFigure 1: Hierarchical Transformers structure.\nsufﬁcient to obtain the best development set results.\n5.3 BERT-based models\nFinally, we perform the classiﬁcation using BERT\nbase (Devlin et al., 2018) for Italian. 1 We set up\nthree kinds of models:\nBERT + dense layerThis is the simplest network,\nand we use it for predictions on the target\nutterance alone. We feed the BERT mean\npooled output into a fully connected layer that\nperforms the prediction.\nBERT + TransformersThis is a more expressive\nnetwork, where the BERT output is passed to\na multi-layer, multi-head Transformer. The\nTransformer’s representation is then passed to\na fully connected layer that outputs the predic-\ntion. We adopted Transformers with six layers\nand six heads, like the Hierarchical Transform-\ners models. Similarly to the BERT + Dense\nmodel, we only feed this network with the\ntarget sentence.\ntext-pair BERT + TransformersThe last net-\nwork is structurally equal to the previous one,\nbut in this case, we use BERT in its text-pair\nmodality. Wet set the target sentence’s size to\n100 words and for the contexts to 400. The\ncontext is the concatenation of the selected\ntexts, padded or truncated at the head. We\nwould lose only the part of the text farthest\nfrom the target sentence in case of truncation.\nHowever, the corpus mostly contains brief\nstatements: padding to 100 and 400 guaran-\ntees a minimum data loss. With this model,\nwe test the seven contexts described above.\n6 Results\nThe results are drawn in table 2.\n1https://huggingface.co/dbmdz/\nbert-base-italian-cased\nThe ﬁrst group of experiments contains the base-\nlines from the literature and simple neural net-\nworks. The second and the third group show the\nTransformers-based and the BERT-based models,\nrespectively. We report Accuracy, Precision, Re-\ncall, and the F-measure. As a benchmark for the\nsigniﬁcance test, we use the literature baseline from\nFornaciari and Poesio (2013) The asterisks repre-\nsent the signiﬁcance levels, computed via bootstrap\nsampling for p ≤ .05 and p ≤ .01. Following\nSøgaard et al. (2014), who recommend avoiding\ntoo small sample sizes, we set our sample at 50%\nof the corpus.\n6.1 Overview\nThe results show that the SVM’s performance is a\nstrong baseline. Only a few models beat its accu-\nracy, and none signiﬁcantly. The same holds for\nprecision. The recall is the metric where most neu-\nral models outperform SVM (signiﬁcantly in ﬁve\ncases), even though the price they pay is a lower\nprecision of the predictions. As a result, only four\nmodels of the 16 Transformer- and BERT-based\nones show an F-Measure signiﬁcantly better than\nSVM, corresponding to a signiﬁcant improvement\nin the recall and better accuracy, albeit not signiﬁ-\ncant. Also, a couple of deep neural models perform\npoorly. We will discuss them in the next sections.\n6.2 Non-contextualized models\nTwo of the best models consider only the target\nsentence: the non-hierarchical Transformer and\nthe one using BERT for single text, followed by\nthe Transformers architecture. Despite our effort\nin the hyper-parameters exploration, including the\nuse of a very low learning rate and regularization\nmethods such as drop-out, we could not prevent\nthat model from strong, early overﬁtting at a low\nlevel of performance. It seems that a single fully\nconnected layer is unable to manage the complexity\nof this task, as we will discuss in section 8.\n2703\nModel Condition Accuracy Precision Recall F-Measure\nMajority class 68.66% 34.33% 50.00% 40.71%\nSVM (Fornaciari and Poesio, 2013) 70.18% 64.42% 62.41% 62.98%\nMLP no context 67.16% 61.75% 61.65% 61.70%\nCNN no context 69.75% 64.98% 65.15% 65.06%\nTransformers. no context 70.98% 66.41% 66.64 ** 66.52% *\nHierarchical Transformers. 1 prev 68.72% 64.06% 64.51% 64.25%\nHierarchical Transformers. 2 prev 67.56% 63.04% 63.70% 63.29%\nHierarchical Transformers. 3 prev 68.13% 63.52% 64.08% 63.75%\nHierarchical Transformers. s-utt 68.36% 64.22% 65.20% 64.54%\nHierarchical Transformers. s-utts 68.36% 63.98% 64.74% 64.26%\nHierarchical Transformers. s-uttturn 68.39% 63.82% 64.39% 64.05%\nHierarchical Transformers. turn 67.16% 53.53% 50.95% 46.17%\nBERT + Dense layer no context 69.09% 63.37% 51.78% 45.60%\nBERT + Transformers no context 70.41% 66.23% 67.10% ** 66.57% *\ntext-pair BERT + Transformers 1prev 68.66% 64.63% 65.70% * 64.97%\ntext-pair BERT + Transformers 2prev 66.14% 62.77% 64.18% 62.98%\ntext-pair BERT + Transformers 3prev 64.91% 61.38% 62.60% 61.55%\ntext-pair BERT + Transformers s-utt 71.34% 66.97% 67.46% ** 67.19% *\ntext-pair BERT + Transformers s-utts 71.61% 66.84% 66.44% * 66.63% *\ntext-pair BERT + Transformers s-uttturn 66.50% 62.17% 62.98% 62.42%\ntext-pair BERT + Transformers turn 68.76% 64.39% 65.14% 64.67%\nTable 2: Baselines, Hierarchical Transformers and text-pair BERT + Transformers models’ performance in the\ndifferent conditions (see section 3). In bold the signiﬁcant results against SVM, with ∗∗ : p ≤ 0.01; ∗ : p ≤ 0.05\n6.3 Contextualized models\nThe contextualized models show similar trends\nwithin the Transformer- and the BERT- based mod-\nels. They are more evident and result in higher\nperformance in the BERT models but are visible in\nthe Hierarchical Transformers as well.\nNone of the Hierarchical Transformers shows an\nF-measure better than that of the non-hierarchical\nTransformer model, and they are better than the\nSVM baseline, but not signiﬁcantly. We also see\nthat the performance slowly degrades when the\ncontext is expanded from one to three utterances,\nregardless of the speaker of those utterances (green\nhistogram in table 2). The same consideration\nholds for the subject’s previous utterance, all their\nprevious utterances, these utterances plus the pre-\nvious turn, or the previous turn alone. In this last\ncase, the fall of performance is remarkable. The\nmodel struggles to recognize the false utterances,\nand the recall is around 50%.\nThe BERT-based models conﬁrm the loss of per-\nformance with context from 1 to 3 utterances, re-\ngardless of the speaker. In this case, the F-measure\nslope in the three conditions is even more pro-\nnounced than in the case of the Hierarchical Trans-\nformers.\nThe best results come from the two models,\nwhich rely on the contexts where only the inter-\nviewee’s utterances are considered. These models\nare signiﬁcantly better than SVM in terms of F-\nmeasure, and they have the highest performance\neven in terms of precision and accuracy. The best\nmodel is even signiﬁcantly better than the one that\nuses convolutions, both for F1 and for recall, with\np < .05.\nIn the conditions where another speaker’s previ-\nous turn is included in the models, the performance\nworsens, similarly to the Hierarchical Transformers\nmodels tested in the same conditions.\n7 The language of deception\nWe adopt two methods to depict the deceptive lan-\nguage: 1) we compute the Information Gain (IG)\nof word n-grams (Forman, 2003), and 2) we apply\nthe Sampling and Occlusion (SOC) algorithm (Jin\net al., 2019).\nInformation Gain measures the entropy of (se-\n2704\nquences of) terms between the different classes.\nThe more imbalanced the presence of such terms\nfor one label class at the other’s expense, the higher\nthe IG value. Table 3 shows the tri-grams with the\nhighest IG values, divided according to the class of\nwhich they are indicative, i.e., where they are more\nfrequently found. While we computed the IG score\nfrom uni-grams to penta-grams, we show only\ntri-grams that, for illustration, represent the best\ntrade-off between meaningful and frequent chunks\nof text.\nThese n-grams show that deceptive statements\nabound with negations: mostly of not remembering,\nbut also not knowing and not having done. In con-\ntrast, truthful statements tend to be more assertive\nand focused on concrete details of time and cir-\ncumstances. The IG signal’s strength also suggests\nthat sincere expressions are much more varied than\ndeceptive ones, which are repeated more often and\nseem to be particularly stereotyped.\nEven though the patterns detected by the neural\nmodels are not necessarily interpretable in terms of\nhuman common sense, we also use SOC to high-\nlight the words that the models ﬁnd to be the most\ninﬂuential for their output.\nSOC gives a post-hoc explanation of the weight\nof speciﬁc words in a sentence for the classiﬁcation\ntask by considering the prediction difference after\nreplacing each word with a MASK token (Jin et al.,\n2019). Since the outcomes depend on the context\nwords, but Jin et al. (2019) are interested in the\nsingle words’ relevance, they do not use the whole\ncontext but sample words from it. In this way, they\nreduce the context’s weight, emphasizing that of\nthe word itself.\nFigure 2 shows two examples of correctly clas-\nsiﬁed sentences, one deceptive and one truthful.\nThe model interprets the red words as indicative\nof deception, the blue ones of truthfulness. They\nare coherent with the intuition provided by the IG.\nHowever, they cannot be interpreted as representa-\ntive of our most complex models’ inner functioning,\nas SOC relies on a standard BERT-based classiﬁer.\n8 Discussion\nOur results show that the Transformers-based mod-\nels, in the hierarchical and non-hierarchical form,\nobtain good results in the classiﬁcation task. Even\nthe non-hierarchical model is signiﬁcantly better\nthan the previous state-of-the-art.\nHowever, the BERT-based models are those that\nshow the best and the worst results. The worst ones\ncome from the BERT for single-text and a simple\ndense output layer. On the other hand, when the\nfully connected layer is substituted by multi-layer,\nmulti-head Transformers, while the BERT output is\nthe same, the performance improves substantially\n(non-contextual models, red histograms in table 2).\nWe also ran experiments with text-pair BERT +\nDense layer. We do not report the details since they\ndo not add to the results: performance is low, while\ntext-pair BERT with Transformers gives the best\noutcomes (blue histograms).\nThese results suggest that:\n1. BERT does not embody the knowledge nec-\nessary for detecting deception. The input rep-\nresentations of a single fully connected layer\nare not expressive enough to cope with the\ntask’s complexity. This makes sense: BERT\nis not trained on texts and on a task (to predict\nthe masked words) to train it to recognize de-\nception. The cues of deception are essentially\nstylometric (section 7) and need a dedicated\nneural architecture to learn them. This is just\nthe case of the Transformers that we associate\nwith BERT. Thanks to their positional em-\nbeddings, they can identify the texts’ relevant\nparts, which the task requires. This aspect\nalso explains the SVM’s performance based\non n-grams and CNNs. Its convolutional lay-\ners essentially explore patterns in the n-gram\nembeddings.\n2. When it is combined with architectures that\ndetect deception cues, such as the Transform-\ners, BERT’s knowledge becomes an added\nvalue that allows the models to reach the best\nperformance. Therefore, the key to success\nis to combine the power of transfer learning\nmodels that bring a robust semantic knowl-\nedge base and attention mechanisms to ex-\nplore sequences, detecting patterns more com-\nplex than those identiﬁed by simple, fully con-\nnected layers.\n3. On the other hand, when the contextual knowl-\nedge in BERT embeddings is missing, we see\nan over-estimation of the stylometric features\ncoming from the context. For example, in\nthe Hierarchical Transformers case, the mod-\nels rely only on the texts’ information, which\nprevents the hierarchical models from outper-\nforming the non-hierarchical ones. Therefore,\n2705\nTrue tri-gram Translation IG*100 False tri-gram Translation IG*100\nin quel periodo at that time 3.245 non ricordo . I don’t remember. 21.858\nnon ho capito I don’t understand 2.884 non lo so I don’t know 10.831\n`e vero che it is true that 2.884 non l’ ho I didn’t 09.257\nmi sembra che it seems to me that 2.884 non mi ricordo I didn’t remember 08.674\ntant’ `e vero so much so that 2.523 non posso dire I cannot say 07.789\nin carcere , in prison, 2.162 il mio amico my friend. 07.627\nc’ `e la there is the 2.162 io l’ ho I did. 06.843\ne niente , ultimately, 2.162 lo ricordo . ...remember it. 06.677\nho capito . I understand. 2.162 mi ricordo proprio I just remember 06.674\ndi s`ı. (I think) so. 2.162 l’ ho allontanato I pushed him away 06.674\nTable 3: Information Gain (rescaled by 100 to avoid tiny values) oftri-grams indicative of truth (left) and deception\n(right)\nFigure 2: Output of the SOC algorithm. The red terms predict deception, the blue ones predict truthfulness.\nwe speculate that BERT’s contextual knowl-\nedge works as a regularizer, which provides\nthe Transformer with previously weighted in-\nputs, according to the sentences’ meaning.\nOur results concerning BERT’s usefulness with\ncontext are different from those obtained by Peskov\net al. (2020), who work on Diplomacy board-game\ndeception data. Their study associated BERT to\nLSTM-based contextual models, and they did not\nﬁnd a BERT contribution in their model’s perfor-\nmance. They tried to ﬁne-tune it, and they hypoth-\nesized that the lack of performance improvement\nwas motivated by the “relatively small size” of the\ntraining data. This hypothesis could be correct, but\nour outcome allows us to formulate another hypoth-\nesis. Their data set concerns an online game, where\nthe range of topics in the dialogues is presumably\nrestricted and speciﬁc. This limitation would not\nallow BERT’s broad knowledge to give a concrete\ncontribution. In contrast, the data set we use comes\nfrom real life. The number of possible topics in\nCourt is the widest. Under such conditions, it is\nreasonable that BERT’s semantic information can\nplay a much more relevant role: this gives a dif-\nferent intuition about the kind of use-cases where\nBERT can be useful.\nRegarding the use of contexts to improve decep-\ntion detection, it turns out that they can be useful,\nbut they need to be carefully handled. In fact, not\n2706\nany context helps. It is not advisable to generically\n“collect something” before the target text. To select\nthe previous sentence(s), regardless of the speaker,\nmeans to incorporate noise that is more harmful\nthan helpful for the task.\nOur best models are those that only consider\nthe utterances of the speaker him/herself. More-\nover, even in that case, the context’s contribution\nimproves according to its proximity to the target\nsentence. The overall performance model that only\nuses the speaker’s ﬁrst previous utterance is slightly\nbetter than that of the models considers all of them.\nThis evidence is made even stronger by the ob-\nservation that, in most cases, there is no previous\nspeaker’s utterance, as he/she responds with a sin-\ngle utterance to a statement or question of an in-\nterlocutor. To be precise, only 921 utterances of\n3015 are preceded by another utterance by the same\nsubject. So in more than two-thirds of the cases,\nthe target utterance has no context from the same\nspeaker and has to be considered standing alone,\nsimilarly to non-contextualized models. In other\nwords, meaningful context is often absent but can\ncontribute remarkably to reach better performance,\nwhich suggests that context is crucial for the task.\nIn other words, the fact that the additional infor-\nmation, even if present in less than one-third of the\ncases, is enough to outperform the other models\nand to reach the best results suggests that this is the\nway to obtain the best help from the context when\npresent.\nThe loss of performance when the contexts in-\nclude the previous turn is also coherent with the\nresults with the contexts based on a given num-\nber of previous utterances: incorporating the state-\nments/questions of the other persons does not help\ndetect deception. If any, the right cues for detecting\ndeception are in the target sentence itself or just\nnearby.\nAlso, the contextual information’s usefulness is\nconditioned by using the right models. BERT and\nthe trainable Transformers need to be used together.\nThe attention mechanism that follows BERT is\nthe trainable part of the network and detects the\nstylometric patterns of deception. However, we\nspeculate that the BERT’s contextual word repre-\nsentations act as a regularizer, which reduces the\nprobability that the information from outside the\ntarget sentence, carried by non-contextual embed-\ndings, is overestimated.\n9 Related work\nThe ﬁrst computational linguistics study on decep-\ntion detection was Newman et al. (2003). They\nasked subjects to write truthful and deceptive es-\nsays and evaluated them using the Linguistic En-\nquiry and Word Count (LIWC), a lexicon that as-\nsigns texts several linguistic and psychological\nscores. LIWC is a popular tool in deception de-\ntection, also used in Fornaciari and Poesio (2013),\nwhich we compare to.\nThere are two main research lines: one relies\non artiﬁcially produced data, often using crowd-\nsourcing services, and the other focuses on data\nsets from real-life situations. The common bot-\ntleneck for data set creation is the availability of\nground truth, i.e., knowing the truth behind a sub-\nject’s statements. For this reason, many studies rely\non data collected in laboratory conditions (Ott et al.,\n2011). While these studies allow us to gain intu-\nitions about the deceptive language features, there\nare no real or relevant consequences for the liars.\nTheir validity concerning high-stakes conditions\nis therefore unclear. Artiﬁcially created texts are\nlikely not interchangeable with those from natural\nconditions (Fornaciari et al., 2020).\nThe notion of deception itself is used in a broad\nsense and includes studies that focus on a different\nkind of deception. A popular area, for example,\nconcerns the detection of fake news (Oshikawa\net al., 2018; Girgis et al., 2018) The ﬁeld is expand-\ning to include models that does not detect deceit\nstrictly speaking, but trolls in social media (Adda-\nwood et al., 2019).\nP´erez-Rosas et al. (2015) is more similar to our\nstudy. They collected videos from public court tri-\nals and built a multi-modal model that relies on\nverbal (unigrams and bigrams) and non-verbal fea-\ntures (Decision Trees (DT) and Random Forest\n(RF)). Krishnamurthy et al. (2018) used the same\ndata set, but with neural models to represent video,\naudio, and textual features. In particular, they ex-\ntracted verbal features relying on pre-trained word\nembeddings and Convolutional Neural Networks.\nThey reached an accuracy of 96.14%. These stud-\nies are particularly interesting for the type of data\nset and the multi-modal approach. However, nei-\nther take the linguistic context of the statements\ninto consideration.\nLevitan et al. (2018) used the data set of Levitan\net al. (2015), where 170 pairs of subjects play a\n“lying game”. This study addresses deception in di-\n2707\nalogues. I.e., the texts are structured as a sequence\nof turns, each containing one or more statements\nof a single participant. For the analysis, the authors\nselected several easily interpretable linguistic fea-\ntures, allowing the authors to draw a description of\nthe deceptive language and feed a Random Forest\nclassiﬁer. This considers both single and multiple\nturns, ﬁnding that the last ones allowed to reach\nthe best performance in their data set (F1-score of\n72.33%). However, this is a laboratory experiment\nthat is not a high-stakes scenario for the partici-\npants: this limits the possibilities of comparison\nwith our study.\nFrom a methodological point of view, our study\nis similar to that by Peskov et al. (2020). They col-\nlect data from an online negotiation game, where\nthe participants’ success depends on their ability to\nlie. They use state-of-the-art neural models, which\nalso consider contextual information. However,\nsubjects are not in a high-stakes condition in their\nstudy, so their ﬁndings are not directly comparable\nto our use case.\n10 Conclusion\nIn this paper, we explore the performance of lan-\nguage models in detecting lies using a unique data\nset that contains sentences that come from real hear-\nings created by Fornaciari and Poesio (2013) and\nanonymized for a research setting. We show that\ncontext is key to creating models that can detect de-\nception and that BERT with some added attention\nlayers can effectively beat different baselines.\nHowever, there is no evidence that the decep-\ntion cues derive from dialogic interaction, as the\nmost useful contributions come from the speaker\nhim/herself. To examine in depth this aspect is a\nline for future research.\n11 Ethical statement and limitations\nApplying predictive models in a legal and law en-\nforcement context can be problematic, especially of\nthe historical training data is biased towards certain\ngroups (Angwin et al., 2016).\nTherefore, we do not propose general-purpose\nmodels for deception detection. They only refer\nto the context of hearings in court, and they can\nbe applied, at best, to similarly ruled events, for\nexample texts coming from police interrogations.\nHowever, as statistical models, they do incorporate\nlinguistic biases that are possibly present in the\ntraining data (Shah et al., 2020). This should be\nconsidered for a fair and respectful interpretation\nof the results.\nIt is also important to point out that the model\npredictions have no absolute certainty but are intrin-\nsically probabilistic. As such, they are only meant\nto support investigations and to inform a judge’s\ndecisions. They cannot be a substitute for expert\nevaluations or for a due legal process.\nReferences\nAseel Addawood, Adam Badawy, Kristina Lerman,\nand Emilio Ferrara. 2019. Linguistic cues to decep-\ntion: Identifying political trolls on social media. In\nProceedings of the International AAAI Conference\non Web and Social Media, volume 13, pages 15–25.\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren\nKirchner. 2016. Machine bias. ProPublica, May,\n23.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nHarris Drucker, Christopher JC Burges, Linda Kauf-\nman, Alex J Smola, and Vladimir Vapnik. 1997.\nSupport vector regression machines. In Advances in\nneural information processing systems , pages 155–\n161.\nEileen Fitzpatrick and Joan Bachenko. 2012. Building\na data collection for deception research. In Proceed-\nings of the workshop on computational approaches\nto deception detection, pages 31–38.\nGeorge Forman. 2003. An extensive empirical study\nof feature selection metrics for text classiﬁcation.\nJournal of machine learning research, 3(Mar):1289–\n1305.\nTommaso Fornaciari, Leticia Cagnina, Paolo Rosso,\nand Massimo Poesio. 2020. Fake opinion detection:\nhow similar are crowdsourced datasets to real data?\nLanguage Resources and Evaluation, pages 1–40.\nTommaso Fornaciari and Massimo Poesio. 2012. De-\nCour: a corpus of DEceptive statements in Italian\nCOURts. In Proceedings of the Eigth International\nConference on Language Resources and Evaluation\n(LREC’12), Istanbul, Turkey. European Language\nResources Association (ELRA).\nTommaso Fornaciari and Massimo Poesio. 2013. Auto-\nmatic deception detection in italian court cases. Ar-\ntiﬁcial intelligence and law, 21(3):303–340.\nSherry Girgis, Eslam Amer, and Mahmoud Gadallah.\n2018. Deep learning algorithms for detecting fake\nnews in online text. In 2018 13th International Con-\nference on Computer Engineering and Systems (IC-\nCES), pages 93–97. IEEE.\n2708\nXisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue,\nand Xiang Ren. 2019. Towards hierarchical impor-\ntance attribution: Explaining compositional seman-\ntics for neural sequence models. arXiv preprint\narXiv:1911.06194.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for efﬁcient text\nclassiﬁcation. arXiv preprint arXiv:1607.01759.\nGangeshwar Krishnamurthy, Navonil Majumder, Sou-\njanya Poria, and Erik Cambria. 2018. A deep learn-\ning approach for multimodal deception detection.\narXiv preprint arXiv:1803.00344.\nSarah I Levitan, Guzhen An, Mandi Wang, Gideon\nMendels, Julia Hirschberg, Michelle Levine, and\nAndrew Rosenberg. 2015. Cross-cultural produc-\ntion and detection of deception from speech. In Pro-\nceedings of the 2015 ACM on Workshop on Multi-\nmodal Deception Detection, pages 1–8.\nSarah Ita Levitan, Angel Maredia, and Julia Hirschberg.\n2018. Linguistic cues to deception and perceived\ndeception in interview dialogues. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1941–1950.\nMatthew L Newman, James W Pennebaker, Diane S\nBerry, and Jane M Richards. 2003. Lying words:\nPredicting deception from linguistic styles. Person-\nality and social psychology bulletin, 29(5):665–675.\nRay Oshikawa, Jing Qian, and William Yang Wang.\n2018. A survey on natural language process-\ning for fake news detection. arXiv preprint\narXiv:1811.00770.\nMyle Ott, Yejin Choi, Claire Cardie, and Jeffrey T\nHancock. 2011. Finding deceptive opinion spam\nby any stretch of the imagination. arXiv preprint\narXiv:1107.4557.\nJames W Pennebaker, Martha E Francis, and Roger J\nBooth. 2001. Linguistic inquiry and word count:\nLiwc 2001. Mahway: Lawrence Erlbaum Asso-\nciates, 71(2001):2001.\nVer´onica P ´erez-Rosas, Mohamed Abouelenien, Rada\nMihalcea, and Mihai Burzo. 2015. Deception de-\ntection using real-life trial data. In Proceedings of\nthe 2015 ACM on International Conference on Mul-\ntimodal Interaction, pages 59–66.\nDenis Peskov, Benny Cheng, Ahmed Elgohary, Joe\nBarrow, Cristian Danescu-Niculescu-Mizil, and Jor-\ndan Boyd-Graber. 2020. It takes two to lie: One to\nlie and one to listen. In Association for Computa-\ntional Linguistics.\nDeven Santosh Shah, H. Andrew Schwartz, and Dirk\nHovy. 2020. Predictive biases in natural language\nprocessing models: A conceptual framework and\noverview. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5248–5264, Online. Association for Computa-\ntional Linguistics.\nAnders Søgaard, Anders Johannsen, Barbara Plank,\nDirk Hovy, and H ´ector Mart ´ınez Alonso. 2014.\nWhat’s in a p-value in nlp? In Proceedings of the\neighteenth conference on computational natural lan-\nguage learning, pages 1–10.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008."
}