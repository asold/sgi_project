{
  "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
  "url": "https://openalex.org/W4389520244",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098551404",
      "name": "Yang Deng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2438895182",
      "name": "Lizi Liao",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A255392272",
      "name": "Liang Chen",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2098605877",
      "name": "Hongru Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2168422861",
      "name": "Wenqiang Lei",
      "affiliations": [
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2730258684",
      "name": "Tat-Seng Chua",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3204850704",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3200895474",
    "https://openalex.org/W3154988854",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W2949769095",
    "https://openalex.org/W4308198680",
    "https://openalex.org/W2997219446",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4375959107",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3198952405",
    "https://openalex.org/W4389524319",
    "https://openalex.org/W4285239949",
    "https://openalex.org/W4312048869",
    "https://openalex.org/W4385572203",
    "https://openalex.org/W2950299257",
    "https://openalex.org/W4284701294",
    "https://openalex.org/W2976965103",
    "https://openalex.org/W2952607215",
    "https://openalex.org/W4366559955",
    "https://openalex.org/W4385572598",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3174986053",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4321485075",
    "https://openalex.org/W3088273075",
    "https://openalex.org/W4284707525",
    "https://openalex.org/W4377372130",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4386566609",
    "https://openalex.org/W3132316480",
    "https://openalex.org/W4385573361",
    "https://openalex.org/W2963170138",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4294810635",
    "https://openalex.org/W3175709814",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W4377111802",
    "https://openalex.org/W3174069185",
    "https://openalex.org/W4321524373",
    "https://openalex.org/W4221167095",
    "https://openalex.org/W4376122848",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W4385763941",
    "https://openalex.org/W4385571659",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4389009562",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W4375958072",
    "https://openalex.org/W2988937804"
  ],
  "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, they still possess limitations, such as failing to ask clarifying questions to ambiguous queries or refuse users' unreasonable requests, both of which are considered as key aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three key aspects of proactive dialogues: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10602–10621\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPrompting and Evaluating Large Language Models for Proactive\nDialogues: Clarification, Target-guided, and Non-collaboration\nYang Deng1, Lizi Liao2, Liang Chen3, Hongru Wang3, Wenqiang Lei4, Tat-Seng Chua1\n1National University of Singapore 2Singapore Management University\n3The Chinese University of Hong Kong 4Sichuan University\n{ydeng,dcscts}@nus.edu.sg, lzliao@smu.edu.sg\n{lchen,hrwang}@se.cuhk.edu.hk, wenqianglei@gmail.com\nAbstract\nConversational systems based on Large Lan-\nguage Models (LLMs), such as ChatGPT, show\nexceptional proficiency in context understand-\ning and response generation. However, they\nstill possess limitations, such as failing to ask\nclarifying questions to ambiguous queries or\nrefuse users’ unreasonable requests, both of\nwhich are considered as key aspects of a con-\nversational agent’s proactivity. This raises the\nquestion of whether LLM-based conversational\nsystems are equipped to handle proactive dia-\nlogue problems. In this work, we conduct a\ncomprehensive analysis of LLM-based conver-\nsational systems, specifically focusing on three\nkey aspects of proactive dialogues: clarifica-\ntion, target-guided, and non-collaborative di-\nalogues. To trigger the proactivity of LLMs,\nwe propose the Proactive Chain-of-Thought\nprompting scheme, which augments LLMs\nwith the goal planning capability over descrip-\ntive reasoning chains. Empirical findings are\ndiscussed to promote future studies on LLM-\nbased proactive dialogue systems.\n1 Introduction\nConversational systems are envisioned to provide\nsocial support or functional service to human\nusers via natural language interactions. Most\nresearch typically centers around a system’s re-\nsponse capabilities, such as understanding the\ndialogue context (Wu et al., 2020; Chen et al.,\n2022; Deng et al., 2022b) and generating appro-\npriate responses (Zhang et al., 2020b; Roller et al.,\n2021). The popularity of conversational systems\nhas grown unprecedentedly with the advent of Chat-\nGPT, which showcases exceptional capabilities\nof context understanding and response generation\nwith large language models (LLMs). Recent stud-\nies observe that, compared with current fine-tuned\nstate-of-the-art (SOTA) methods, ChatGPT can still\nachieve competitive performance under zero-shot\nsetting on different dialogue problems, such as the\nknowledge-grounded dialogues (Bang et al., 2023),\ntask-oriented dialogues (Zhang et al., 2023), and\nemotion-aware dialogues (Zhao et al., 2023).\nDespite the strength of ChatGPT, there are still\nseveral limitations1, such as failing to ask clarifica-\ntion questions to ambiguous user queries or refuse\nproblematic user requests. These kinds of capabili-\nties are typically regarded as the proactivity of the\nconversational system (Deng et al., 2023b), where\nthe system can create or control the conversation\nto achieve the conversational goals by taking ini-\ntiative and anticipating impacts on themselves or\nthe human users. Thus, it raises the question: Are\nthese LLM-based conversational systems equipped\nto manage proactive dialogue problems?\nIn this work, we conduct the first comprehen-\nsive analysis of LLM-based conversational systems\non three common aspects of proactive dialogues,\nincluding 1) clarification in information-seeking\ndialogues (Guo et al., 2021; Deng et al., 2022a)\nwhere the system is required to proactively ask clar-\nification questions when encountering ambiguity\nin user queries; 2) target-guided open-domain dia-\nlogues (Tang et al., 2019; Wu et al., 2019) where\nthe system is required to proactively lead the con-\nversation towards the designated target; and 3)\nnon-collaborative task-oriented dialogues (Li et al.,\n2020; Zhou et al., 2020; Deng et al., 2023a) where\nthe system and the user do not share the same con-\nversational goal while the system aims to strategi-\ncally reach a consensus with the user.\nMotivated by the emergent capabilities of LLMs\n(Wei et al., 2022a,b) on reasoning over texts, some\nrecent studies investigate in-context learning or\nchain-of-thought prompting schemes on planning\n(Huang et al., 2022) or taking actions (Yao et al.,\n2022) in interactive environments. Similarly, strat-\negy learning and goal planning attach great impor-\ntance in proactive dialogue systems. In order to\nenhance the proactivity of LLM-based conversa-\n1as stated in https://openai.com/blog/chatgpt/.\n10602\nAct: [“Directly Answer”, “Ask a \nClarification Question”] \nPrompt: Given the task background \nand the conversation history, first \nanalyse whether the question at the \ncurrent turn is ambiguous to answer, \nand then select appropriate actions to \ngenerate the response: \nResponse: There are two books that \nAngie's mother found. It is unclear \nwhich book is referred to. Therefore, \nthe question is ambiguous. Ask a \nclarification question: Which book \nare you referring to?\nAct: [“Directly Answer”, “Ask a Clarification \nQuestion”] \nPrompt: Given the task background and the \nconversation history, please use appropriate \nactions to generate the response: \nResponse: Ask a clarification question: \nCould you provide more information?\nPrompt: Given the task background and the \nconversation history, please generate the \nresponse: \nResponse: Green\nTask Background: The grounded document is “Angie …. She made a drawing of her \nmother. Her mother found a large red book. Then they went to the Mystery section. \nAngie sat in a blue chair. She drew a picture of her brother. Her mother found the \nbook. It was a green book. …” \nConversation History: [“User”:  “What did she draw?”, “System”: “Her mother”, “User”: \n“What did her mother find?”, “System”: “The book”, “User”: “What color was it?”]\n(1) Clarification Dialogues: Abg-CoQA\n(1a) Standard\n(1b) Proactive\n(1c) Proactive CoT\nStrategy: [“Build rapport”, “Show dominance”, \n“Negative sentiment”, …] \nAct: [“Ask a question”, “Insist on an offer”, \n“Proposing a counter price”, …] \nPrompt: Given the task background and the \nconversation history, first analyse the current \nnegotiation progress and consider an appropriate \nnegotiation goal, and then select appropriate \nstrategies and actions to generate the response: \nResponse: The seller firmly rejects the price. \nIt would be better to propose a higher price \nclosed to 8700. The Strategy is Negative \nsentiment. The act is Proposing a counter price. \nI really don’t need it but I  \ncan pay you 8200 if you let it go.\nStrategy: [“Build rapport”, “Show dominance”, …] \nAct: [“Ask a question”, “Insist on an offer”, …] \nPrompt: Given the task background and the \nconversation history, please use appropriate \nstrategies and actions to generate the response: \nResponse: The strategy is Communicate politely. \nThe act is Proposing a counter price. I understand. \nHow about we meet halfway at  \n10250? What do you think?\nPrompt: Given the task background and the \nconversation history, please generate the response: \nResponse: How about we meet in the  \nmiddle at 10000?\nTask Background: The item description is “Selling my 2006 Toyota 4 Runner with only 106k original miles. \nThe truck is in great condition with no mechanical flaws whatsoever and a clean accident history. …, don't \nwaste your time and my time. I'm a busy guy.” The selling price is 14500. The target price is 8700.  \nConversation History: [\"Buyer\": \"Man thats a nice trak.\", \"Seller\": \"Yes its really nice.\", \"Buyer\": \"How \nabout you give it to me for 8000?\", \"Seller\": \"That is way too little.\"]\n(2) Non-collaborative Dialogues: CraigslistBargain\n(2a) Standard\n(2b) Proactive\n(2c) Proactive CoT\nSL-ratio: 0.78\nSL-ratio: 0.74 SL-ratio: 1.09\nFigure 1: Examples of three kinds of prompting schemes for proactive dialogues. In the example of non-collaborative\ndialogue, the system plays the role of \"Buyer\", and the sale-to-list (SL) ratio shows the effectiveness of negotiation,\nwhich is calculated by (listed price −bargain price)/(listed price −buyer target price). The higher ratio means the\ncurrent bargain price is closer to the target.\ntional systems, we design the proactive chain-of-\nthought prompting (ProCoT) scheme. As shown in\nFigure 1, with standard prompting, LLM-based sys-\ntems directly provide a randomly-guessed answer\nto the ambiguous user question (1a), or generate a\ngeneral bargain response without any negotiation\nstrategy (2a). When providing the system with\noptions to take different dialogue acts (proactive\nprompting), the generated responses are unaware of\nthe conversational goal, such as generating under-\nspecified clarification questions (1b) and conserva-\ntive negotiation responses (2b). To this end, Pro-\nCoT first instructs the system to generate descrip-\ntive thoughts about intermediate steps of reasoning\nand planning for reaching the conversational goal,\nand then make the decision of the next action to\ntake. Finally, the system generates an appropriate\nresponse based on the decided action (1c & 2c).\nWe conduct extensive experiments with two\nLLM-based conversational systems, including\nChatGPT and an open-sourced model, Vicuna (Chi-\nang et al., 2023). With the aforementioned three\ntypes of prompting schemes, we compare these\nLLM-based conversational systems with fine-tuned\nSOTA dialogue models. The main contributions of\nthis work can be summarized as follows:\n• This work presents the first comprehensive eval-\nuation on the proactivity of LLM-based dialogue\nsystems, including the handling of clarification,\ntarget-guided, and non-collaborative dialogues.\n• We design the proactive chain-of-thought prompt-\ning scheme to endow LLM-based dialogue sys-\ntems with the capability of planning and taking\nthe initiative towards the conversational goal.\n• Specifically, the main findings of the evaluation\nof LLM-based dialogue systems include: 1) They\nbarely ask clarification questions when encoun-\ntering ambiguous queries, and ProCoT largely\novercomes this issue, though the performance\nis still unsatisfactory in domain-specific applica-\ntions (§4.1). 2) They are proficient at performing\ntopic shifting towards the designated target, but\ntend to make aggressive topic transition. ProCoT\nfurther improves this capability by planning a\nsmoother transition (§4.2). 3) They fail to make\nstrategic decision and tend to compromise with\nthe opponent. The key challenge is how to effec-\ntively optimize the strategy learning (§4.3).\n2 Related Works\nProactive Dialogues. Recent years have wit-\nnessed many advanced designs on developing\nproactive dialogue systems (Liao et al., 2023) for\nvarious applications. For example, target-guided\ndialogues aim to proactively lead the conversa-\ntion to either a designated target topic (Tang et al.,\n2019) or a pre-defined knowledge entity (Wu et al.,\n2019). Existing studies typically adopt keyword\ntransition (Qin et al., 2020; Zhong et al., 2021)\nor knowledge graph reasoning (Yang et al., 2022;\nLei et al., 2022) techniques to proactively plan\nthe topic thread towards the target. Besides, in\ninformation-seeking dialogues, proactive dialogue\nsystems can ask clarification questions for clari-\nfying the ambiguity of the query or question in\nconversational search (Aliannejadi et al., 2021) and\n10603\nquestion answering (Guo et al., 2021; Deng et al.,\n2022a). In addition, under the non-collaborative\nsetting, the system and the user have competing\ngoals towards the task completion but the system\naims to proactively reach an agreement favorable\nto itself (Zhou et al., 2020), such as negotiating a\nproduct price (He et al., 2018) or persuading users\nto make a donation (Wang et al., 2019).\nLarge Language Models for Dialogues. Previ-\nous dialogue systems, such as DialoGPT (Zhang\net al., 2020b), Meena (Adiwardana et al., 2020),\nBlenderBot (Roller et al., 2021), LaMDA (Thop-\npilan et al., 2022), typically fine-tune pre-trained\nlanguage models on public dialogue data. Inspired\nby the success of ChatGPT, recent practices build\ndialogue systems through conducting supervised\nfine-tuning on open-source large language models,\nsuch as LLaMA (Touvron et al., 2023), with either\nconstructed instruction-following examples (e.g.,\nAlpaca (Taori et al., 2023)) or distilled conversation\ndata (e.g., Vicuna (Chiang et al., 2023)) from Chat-\nGPT. As all these LLM-based dialogue systems are\ntrained to follow the user’s instruction, it remains\na question on whether these systems can take the\ninitiative for handling proactive dialogues.\nPrompting in Dialogue Systems. To induce\nknowledge from LLMs, various prompting meth-\nods are designed for zero-shot or few-shot learn-\ning in dialogue applications, such as task-oriented\ndialogues (Lee et al., 2021; Mi et al., 2022),\nknowledge-grounded dialogues (Shuster et al.,\n2022; Liu et al., 2022; Wang et al., 2023c), and\nopen-domain dialogues (Chen et al., 2023b; Lee\net al., 2023; Wang et al., 2023a). For example,\nChen et al. (2023c) propose to prompt LLMs for\ncontrollable response generation in emotional sup-\nport and persuasion dialogues, conditioned on the\nground-truth dialogue strategies. In this work, we\naim at prompting LLMs to proactively interact with\nthe users.\n3 Prompting LLMs to be Proactive\nAs presented in Figure 1, we describe the prompt-\ning schemes, including the standard, proactive, and\nproactive chain-of-thought (ProCoT) prompting.\nStandard Prompting. In order to instruct LLMs\nto perform specific dialogue tasks, the typical\nprompting scheme can be formulated as\np(r|D, C). (1)\nGiven the task background Dand the conversa-\ntion history C, instruct the LLM to generate the\nresponse r. In specific, the task background can be\nthe grounded document in clarification dialogues\nor the target description in target-guided dialogues.\nProactive Prompting. Proactive prompting aims\nto provide alternative options for LLMs to decide\nwhat kinds of actions should be taken in the re-\nsponse, instead of simply responding to the instruc-\ntion. It can be formulated as:\np(a, r|D, C, A). (2)\nGiven the task background D, the conversation\nhistory C, and a set of possible dialogue acts A,\ninstruct the LLM to select the most appropriate di-\nalogue act a ∈A and then generate the response r.\nFor example, the dialogue act can be Ask a Clarifi-\ncation Question or Directly Answer the Question in\nclarification dialogues, different negotiation strate-\ngies in non-collaborative dialogues, or different\nconversation topics in target-guided dialogues.\nProactive Chain-of-Thought Prompting. In or-\nder to endow LLMs with the capability of planning\nand taking the initiative towards the ultimate goal,\nwe develop the proactive chain-of-thought prompt-\ning scheme—ProCoT. It involves the analysis of\nthe next action to take by performing dynamic rea-\nsoning and planning for reaching the conversational\ngoal. ProCoT can be formulated as:\np(t, a, r|D, C, A), (3)\nwhere t is the thought description for the decision-\nmaking process of the next action. For example, in\nclarification dialogues, t can be the ambiguity anal-\nysis of the current user question as in Figure 1(1c).\nWhile in non-collaborative dialogues, t can be the\ngoal completion analysis of the current negotiation\nprogress as in Figure 1(2c).\n4 Evaluation\nWe evaluate the proactivity of LLM-based conver-\nsational systems from three perspectives, includ-\ning the capability of asking clarification questions\n(§4.1), guiding the conversation towards the des-\nignated target ( §4.2), and strategically handling\nconflicting goals (§4.3).\n4.1 Clarification Dialogues\nClarification in information-seeking dialogues (Za-\nmani et al., 2022) refers to the process of seeking\n10604\nAbg-CoQA PACIFIC\nCNP CQG CNP CQG\nMethod Shot Prompt F1 BLEU-1 Help. F1 ROUGE-2 Help.\nBaseline - - 22.1 36.5 30.0 79.0 69.2 38.2SOTA - - 23.6 38.2 56.086.9 90.7 80.1\nVicuna-13B\n0 Standard - 11.3 0.0 - 1.2 0.01 Standard - 11.4 0.0 - 2.5 0.00 Proactive 4.1 13.2 0.0 2.3 2.3 0.01 Proactive 12.1 13.2 4.5 0.0 3.3 0.00 ProCoT 1.4 21.3 9.1 9.7 3.8 10.51 ProCoT18.3 23.7 22.7 27.0 41.3 33.1\nChatGPT\n0 Standard - 12.1 0.0 - 2.2 0.01 Standard - 12.3 0.0 - 2.0 0.00 Proactive 22.0 13.7 17.6 19.4 2.9 0.01 Proactive 20.423.4 23.5 17.7 14.0 12.50 ProCoT 23.8 21.6 32.428.0 21.5 26.71 ProCoT27.9 18.4 45.927.7 16.2 35.8\nTable 1: Experimental results on Abg-CoQA and PA-\nCIFIC datasets, whose baseline and SOTA results are\nadopting from Guo et al. (2021) and Deng et al. (2022a).\nBold and underlined results denote the best performance\nfor each LLM and the fine-tuned methods, respectively.\nfurther information or details to better understand\nthe topic or question at hand. In this context, clar-\nification is an important part of the dialogue as it\nhelps to ensure that the information being shared is\naccurate and complete.\n4.1.1 Problem Definition\nFollowing previous studies (Aliannejadi et al.,\n2021; Guo et al., 2021; Deng et al., 2022a), the\nproblem of asking clarification questions can be\ndecomposed into two subtasks: 1) Clarification\nNeed Prediction (CNP) to identify the necessity\nof clarification in the current turn, and 2) Clarifi-\ncation Question Generation (CQG) to produce an\nappropriate clarifying question if needed. Given\nthe grounded document Dand the dialogue con-\ntext C= {q1, a1, ..., qt−1, at−1, qt}, the dialogue\nsystem aims to first predict the binary ambiguity\nlabel y on whether the current question qt needs\nto be clarified. If so, a corresponding clarification\nquestion should be generated as the response at for\nclarifying the ambiguity.\n4.1.2 Experimental Setups\nDatasets. We evaluate the capability of asking\nclarification questions in LLM-based dialogue sys-\ntems on two types of datasets: 1) Abg-CoQA (Guo\net al., 2021) in general domain, and 2) PA-\nCIFIC (Deng et al., 2022a) in finance domain. De-\ntails on these datasets can be found in Appendix A.\nEvaluation Metrics. Following previous stud-\nies (Guo et al., 2021; Deng et al., 2022a), we use\nthe F1 score for the evaluation of CNP, and BLEU-\n1 and ROUGE-2 (F1) for the evaluation of CQG. In\naddition, since the automatic lexical matching met-\nrics may fail to actually estimate the clarification ca-\npability of the generated clarifying questions (Guo\net al., 2021), we also adopt human evaluation to\nscore whether the generated question is helpful for\nclarifying the existing ambiguity (Help.).\nUsage of LLMs. To facilitate reproducibility,\nwe adopt a static version of ChatGPT, i.e.,\ngpt-3.5-turbo-0301, and set the temperature to\n0 for generating the deterministic outputs with the\nsame inputs. In addition, we adopt an open-source\nLLM, i.e., Vicuna-13B-delta-v1.1 , for the eval-\nuation. The maximum number of new tokens is set\nto 128 for the generation.\nPrompting Schemes. We evaluate the three\nprompting schemes introduced in Section 3, includ-\ning standard, proactive, and ProCoT prompting. In\naddition, we report their results under both zero-\nshot and few-shot settings. Due to the limitation\nof the maximum sequence length in Vicuna (2,048\ntokens), we only apply one-shot in-context learning\nfor comparisons. The complete prompts adopted\nfor evaluation is presented in Appendix C.\n4.1.3 Experimental Results\nTable 1 summarizes the evaluation results on Abg-\nCoQA and PACIFIC datasets. There are several\nnotable observations as follows:\nLLM-based conversational systems fail to ask\nclarification questions. Under standard prompt-\ning, both Vicuna and ChatGPT fail to ask clari-\nfication questions when encountering ambiguous\nqueries, according to the human evaluation on the\nhelpfulness (Help.) of the generated responses for\nclarifying ambiguity. Even with one-shot demon-\nstration, in-context learning (ICL) still cannot pro-\nvide them with such ability. Under proactive\nprompting, given the option of clarification, Vi-\ncuna’s ability to accurately take this action is still\nquite limited, with the F1 scores close to 0. In\ncontrast, ChatGPT becomes capable of asking clar-\nification questions on Abg-CoQA, as evidenced by\nthe improvement on both F1 and Help. scores.\nProCoT effectively endows LLM-based conver-\nsational systems with the capability of asking\nclarification questions. Zero-shot ProCoT is not\nworking in Vicuna, but one-shot ICL can largely\nimprove the performance. As for Abg-CoQA, Chat-\nGPT with zero-shot ProCoT achieves competitive\n10605\nAbg-CoQA PACIFIC\nWrong Aspect 21% 30%\nUnder-spec. Clari. 16% 23%\nOver-spec. Clari. 15% 5%\nGeneration Error 48% 42%\nTable 2: Statistics of error analysis.\nperformance with SOTA fine-tuned methods on\nthe CNP task (F1), but the generated clarification\nquestions are still unsatisfactory (Help.). One-shot\nICL further improves the performance of ChatGPT\nwith ProCoT to a great extent. The case study in\nAppendix D.1 shows that ProCoT also improves\nthe explanability of asking clarification questions.\nAs for domain-specific problem, there is still\na noticeable gap from the fine-tuned methods.\nAlthough ProCoT has already largely enhanced the\ncapability of asking clarification questions, the per-\nformance of LLMs on the domain-specific task,\ni.e., PACIFIC (Finance), is still far behind the fine-\ntuned methods. In fact, with fine-tuning on domain-\nspecific data, the SOTA method can achieve a re-\nmarkable performance on PACIFIC,i.e., 86.9 (F1)\nfor CNP and 80.1 (Help.) for CQG, indicating the\nimportance of domain knowledge.\n4.1.4 Error Analysis\nIn order to find out the reason why LLM-based\ndialogue systems with ProCoT prompting fall short\nof handling domain-specific clarification dialogues,\nwe randomly sample 100 error cases in clarification\nquestion generation from each dataset for analy-\nsis (all cases are generated by ChatGPT with one-\nshot ProCoT). We categorize these failure cases\ninto four groups, including Wrong Aspect, Under-\nspecified Clarification , Over-specified Clarifica-\ntion, and Generation Error. The details and ex-\namples can be found in the Appendix B. The statis-\ntics of error analysis is presented in Table 2. It\ncan be observed that the proportion of failure cases\nattribute to the wrong aspect and under-specified\nclarification in PACIFIC (Finance) is higher than\nthat in Abg-CoQA (General). This indicates that\nChatGPT may lack of certain domain knowl-\nedge required for asking precise and specific\nclarification questions.\n4.2 Target-guided Dialogues\nInstead of making consistent responses to the user-\noriented topics, the dialogue system for target-\nguided dialogues is required to proactively lead\nthe conversation topics towards a designated tar-\nget (Tang et al., 2019). According to different appli-\ncations, the target can be topical keywords (Zhong\net al., 2021), knowledge entities (Wu et al., 2019),\nor items to be recommended (Deng et al., 2023c).\n4.2.1 Problem Definition\nGiven a target Dthat is only presented to the agent\nbut unknown to the user, the dialogue starts from\nan arbitrary initial topic, and the system needs to\nproduce multiple turns of responses {un}to lead\nthe conversation towards the target in the end. The\nproduced responses should satisfy (i) transition\nsmoothness, natural and appropriate content under\nthe given dialogue context, and (ii) target achieve-\nment, driving the conversation towards the desig-\nnated target. The problem is typically decomposed\ninto two subtasks (Tang et al., 2019; Zhong et al.,\n2021; Yang et al., 2022): next topic selection and\ntransition response generation.\n4.2.2 Experimental Setups\nDatasets. We first conduct turn-level evaluation\nof the target-guided capability on a next-turn target-\noriented dataset OTTers(Sevegnani et al., 2021),\nwhich requires the dialogue system to proactively\nbridge the current conversation topic to approach\nthe target. Furthermore, we adopt TGConv (Yang\net al., 2022) to testify the ability to guide the\nmulti-turn conversation to the target topic as the\ndialogue-level evaluation. Details can be found in\nAppendix A.\nAutomatic Evaluation Metrics. Following pre-\nvious studies (Sevegnani et al., 2021; Yang et al.,\n2022), we adopt the hits@k (k ∈[1, 3]) for evaluat-\ning next topic prediction. Three text generation\nmetrics, including BLEU, ROUGE-L, and ME-\nTEOR scores, are used for the evaluation of re-\nsponse generation on the OTTers dataset.\nAs for the dialogue-level evaluation on the TG-\nConv dataset, we follow existing studies (Yang\net al., 2022; Wang et al., 2023b) to simulate multi-\nturn conversations via self-play (Tang et al., 2019),\nwhere the simulated user is unaware of the tar-\nget topic. Three aspects are evaluated: 1) Succ.\nis the success rate of generating the target word\nwithin 8 turns of conversations; 2) Turns is the\naverage turns of all dialogues that successfully\nreach the target word; and 3) Coh. is the contex-\ntual semantic similarity between the last utterance\nand the generated response, which is measured by\nMiniLM (Wang et al., 2020).\n10606\nResponse Generation Next Topic Prediction\nMethod Shot Prompt BLEU METEOR R-L hits@1 hits@3\nGPT2 - - 11.58 10.26 17.67 4.39 15.79DKRN - - 12.86 11.90 21.52 4.91 17.72CKC - - 13.34 11.65 24.77 6.87 21.89TopKG - - 15.35 13.4127.167.78 22.06\nVicuna-13B\n0 Standard 10.01 13.27 16.00 12.01 19.031 Standard 10.63 14.81 17.53 12.10 16.130 Proactive 1.41 18.45 15.45 9.41 19.891 Proactive13.87 20.96 21.3612.90 22.310 ProCoT 5.27 16.59 15.96 11.56 18.011 ProCoT 13.38 19.70 20.6215.05 20.70\nChatGPT\n0 Standard 11.34 20.6218.2613.44 27.691 Standard 14.41 19.29 17.73 15.86 26.340 Proactive 14.0921.0615.56 7.53 22.581 Proactive14.74 19.59 16.29 8.60 21.230 ProCoT 10.20 19.57 15.97 12.63 23.921 ProCoT 9.63 19.82 17.1917.74 29.57\nTable 3: Turn-level evaluation results on Next Topic\nPrediction and Transition Response Generation.\nHuman Evaluation Metrics. We also conduct\nthe same human evaluation as Yang et al. (2022),\nincluding two dialogue-level metrics with the fol-\nlowing instructions provided for annotators:\n• Global-Coherence (G-Coh.): Whether the entire\ndialogue is logically and topically coherent.\n• Effectiveness (Effect.): How efficiently the target\nis achieved.\nA total of 100 dialogues are generated through sim-\nulation for each method. Three annotators assign\nratings to the generated dialogues on a scale of [0,\n1, 2], where higher scores indicate better quality.\nBaselines. We report the results of several fine-\ntuned baselines for target-guided dialogues, includ-\ning GPT-2 (Radford et al., 2019), DKRN (Qin et al.,\n2020), CKC (Zhong et al., 2021), TopKG (Yang\net al., 2022), and COLOR (Wang et al., 2023b).\n4.2.3 Turn-level Evaluation\nTable 3 shows the turn-level evaluation results on\nOTTers. There are several notable observations:\nLLM-based dialogue systems are proficient at\nperforming topic shifting towards the desig-\nnated target. According to the performance of\nLLMs with standard prompting, we observe that:\n1) As for the next-topic prediction (hits@k), thanks\nto the extensive knowledge across various topics,\nzero-shot LLMs can achieve competitive (Vicuna)\nor even better (ChatGPT) performance than the\nfine-tuned methods. 2) As for the transition re-\nsponse generation, automatic evaluation metrics\n(BLEU, METEOR, R-L)2 suggest that zero-shot\n2Note that the automatic evaluation of response generation\nis less reliable (Sevegnani et al., 2021), as the same topic\nmodels perform closely to fine-tuned methods in\nterms of lexical similarity with the reference re-\nsponse. 3) One-shot ICL casts no positive impact\non the performance and may even lead to worse\nresults in next-topic prediction. This indicates that\nit is difficult for LLMs to enhance the topic shifting\ncapability from limited demonstrations.\nOnly ProCoT prompting with one-shot demon-\nstrations can improve the topic shifting capabil-\nity. Without demonstrations, proactive and Pro-\nCoT prompts perform even worse than standard\nprompts, since LLMs may confuse about what\nkinds of topics are desired. For example, we ob-\nserve a typical mistake that LLMs tend to analyse\nthe next topics using questions, such as \"What kind\nof food do you like? \", leading to a narrow topic\nfor the next turn. With one-shot demonstrations,\nChatGPT with proactive prompts continues to un-\nderperform compared to standard prompts when\nit comes to accurately predicting suitable topics\ntowards the target. However, it is worth noting\nthat only ProCoT prompts consistently show an\nimprovement in the performance of all LLMs for\nnext topic prediction.\n4.2.4 Dialogue-level Evaluation\nTable 4 shows the dialogue-level evaluation results\non TGConv. We draw the following conclusions:\nLLM-based dialogue systems tend to make ag-\ngressive topic transition. The results demon-\nstrate the effectiveness of LLMs in steering the\nconversation towards the designated target, with\nChatGPT exhibiting nearly perfect success rates\n(Succ.). Compared with baselines, LLMs also ex-\ncel in generating more coherent responses that align\nwith the dialogue context (Coh.), showcasing their\nimpressive abilities in context understanding and\nresponse generation. Furthermore, the analysis re-\nveals that ChatGPT basically achieves the target\ntopics within just three turns, suggesting its ten-\ndency to generate responses that aggressively in-\nvolve the desired topic. Similar observations can\nbe made with Vicuna using standard prompting.\nProCoT prompting enables a smoother topic\ntransition of target-guided dialogues. Under\nproactive prompting, the response coherency is im-\nproved by the topic planning. However, the suc-\ncess rate is negatively affected, which attributes to\ncan be described in different ways rather than the reference\nresponse. We mainly discuss the topic shifting capability in\nterms of the performance on next topic prediction.\n10607\nEasy Target Hard Target\nMethod Shot Prompt Succ.(%) Turns Coh. Succ.(%) Turns Coh.\nGPT2 - - 22.3 2.86 0.23 17.3 2.94 0.21\nDKRN - - 38.6 4.24 0.33 21.7 7.19 0.31\nCKC - - 41.9 4.08 0.35 24.8 6.88 0.33\nTopKG - - 48.9 3.95 0.31 27.3 4.96 0.33\nCOLOR - - 66.3 - 0.36 30.1 - 0.35\nVicuna-13B\n0 Standard 63.0 2.630.43 62.5 2.450.39\n1 Standard 62.7 2.83 0.45 65.0 2.90 0.43\n0 Proactive 37.8 2.71 0.48 35.6 2.560.55\n1 Proactive 48.3 2.71 0.50 34.6 2.95 0.51\n0 ProCoT 65.2 4.22 0.49 54.9 4.17 0.45\n1 ProCoT 72.3 3.550.52 59.8 3.81 0.48\nChatGPT\n0 Standard 97.5 2.26 0.38 96.3 2.30 0.41\n1 Standard 96.3 2.42 0.42 93.5 2.280.38\n0 Proactive 85.9 3.200.47 83.0 2.83 0.43\n1 Proactive 90.7 2.86 0.36 86.2 2.94 0.31\n0 ProCoT 96.3 2.47 0.41 92.0 2.29 0.34\n1 ProCoT 95.9 2.63 0.45 92.1 2.47 0.39\nTable 4: Dialogue-level evaluation results on target-\nguided dialogues.\nEasy Target Hard Target\nMethod Prompt G-Coh. Effect. G-Coh. Effect.\nTopKG - 1.42 1.24 1.21 1.10\nVicuna-13BStandard 1.37 1.60 1.20 1.49Proactive 1.51 1.27 1.26 1.23ProCoT1.57 1.70 1.35 1.59\nChatGPTStandard 0.971.92 0.84 1.89Proactive1.24 1.77 1.12 1.68ProCoT 1.20 1.901.14 1.85\nTable 5: Human evaluation on target-guided dialogues.\nAll reported methods are under the one-shot setting.\nits drawback of next topic prediction discussed in\nSection 4.2.3. Under ProCoT prompting, Vicuna\neffectively guide the conversation towards the des-\nignated target with a smoother (higher Coh.) and\nmore engaging (higher Turns) conversation than\nusing standard prompting. However, it still remains\nchallenging for ChatGPT to perform a smooth topic\ntransition. Case studies in Appendix D.2 provide\nintuitive examples for illustrating these observa-\ntions.\n4.2.5 Human Evaluation\nTable 5 presents the human evaluation results on\nTGConv. Compared with TopKG, LLMs demon-\nstrate remarkable efficiency in achieving the des-\nignated target (Effect.). However, the global co-\nherence (G-Coh.) of the generated dialogues by\nChatGPT is quite low, which may harm the user en-\ngagement and experience during the conversation.\nThus, the proficiency of controllable generation in\nLLMs is a double-edged sword for target-guided\ndialogues. The key challenge of LLMs is how to\nguarantee the topical smoothness and coherence\nof the generated transition responses.\nNego. Strategy Dial. Act Resp. Gen.\nMethod Shot Prompt F1 AUC F1 AUC BLEU BERTScore\nFeHED - - 17.6 55.8 20.6 76.9 23.7 27.0HED+RNN - - 23.2 65.3 33.0 83.1 22.5 22.8HED+TFM - - 26.3 68.2 32.5 85.6 24.4 27.7DIALOGRAPH- - 26.1 68.1 33.4 85.624.7 28.1\nVicuna-13B\n0 Standard - - - - 1.7 -14.01 Standard - - - - 1.9 -2.80 Proactive20.6 51.1 4.2 50.3 2.3 -7.01 Proactive 15.2 50.0 6.7 50.8 2.6 -0.90 ProCoT 19.0 49.7 3.6 50.3 2.6 -6.21 ProCoT 17.8 48.97.7 52.5 2.6 -0.9\nChatGPT\n0 Standard - - - - 2.3 -4.31 Standard - - - - 3.1 0.70 Proactive 12.8 51.3 13.3 56.34.2 1.31 Proactive 13.7 50.9 12.0 54.9 3.92.90 ProCoT 10.8 50.4 10.1 54.2 3.7 -0.91 ProCoT15.1 55.5 16.3 58.23.9 1.6\nTable 6: Evaluation results on Negotiation Strategy Pre-\ndiction, Dialogue Act Prediction, and Response Genera-\ntion.\n4.3 Non-collaborative Dialogues\nUnlike collaborative task-oriented dialogue set-\ntings (Zhang et al., 2020c), where the user and the\nsystem work together to reach a common goal (e.g.,\nbooking hotels), in non-collaborative dialogues, the\nuser and the system have a conflict of interest but\naim to strategically communicate to reach an agree-\nment (e.g., negotiation) (Zhan et al., 2022). The\nsystem is required to leverage a series of proactive\nstrategies to reach an agreement favorable to itself,\ninstead of passively following the user’s intents.\n4.3.1 Problem Definition\nGiven the dialogue history C= {u1, ..., ut−1}and\nthe dialogue background D, the goal is to generate\na response ut with appropriate dialogue strategy at\nthat can lead to a consensus between the system and\nuser. A set of dialogue strategies Ais pre-defined\nfor prediction. Based on different applications, the\ndialogue strategy can be coarse dialogue act labels\nor fine-grained strategy labels. The dialogue back-\nground includes the system’s goal and the related\ngrounded information, such as item descriptions\nin bargain negotiation (He et al., 2018) and user\nprofile in persuasion dialogues (Wang et al., 2019).\n4.3.2 Experimental Setups\nDatasets. We use the CraigslistBargain dataset\n(He et al., 2018) for evaluating the capability of\nstrategically handling non-collaboration in LLM-\nbased dialogue systems. The dataset was created\nunder the bargain negotiation setting where the\nbuyer and the seller are negotiating the price of an\nitem on sale. Details can be found in Appendix A.\n10608\nMetric Standard Proactive ProCoT Gold\nPersuasive 1.24 1.28 1.43 1.54\nCoherent 1.56 1.66 1.74 1.69\nNatural 1.94 1.82 1.89 1.97\nWin Rates\n- vs. Standard - 0.22 0.24 0.42\n- vs. Proactive 0.25 - 0.31 0.45\n- vs. ProCoT 0.20 0.18 - 0.34\n- vs. Gold 0.19 0.09 0.23 -\nSale-to-List Ratio 0.48 0.43 0.54 0.64\nTable 7: Human evaluation on non-collaborative dia-\nlogues. All reported methods are based on ChatGPT\nunder the one-shot setting. Gold denotes that we in-\nstruct the LLMs to generate responses conditioned on\nthe reference dialogue acts and negotiation strategies.\nAutomatic Evaluation Metrics. Following the\nprevious study (Joshi et al., 2021), we conduct a\ncomprehensive evaluation over three subtasks, in-\ncluding negotiation strategy prediction, dialogue\nact prediction, and response generation. We report\nthe F1 and ROC AUC scores for strategy predic-\ntion and dialogue act prediction, where the former\none is a multi-label prediction problem. For the\nresponse generation, we adopt BLEU score and\nBERTScore (Zhang et al., 2020a) for evaluation.\nHuman Evaluation Metrics. Following Joshi\net al. (2021), we also conduct human evaluation on\n100 randomly sampled dialogues with both subjec-\ntive and objective human judgement. As for the\nsubjective judgement, annotators are asked to score\n[0,1,2] on how persuasive, coherent, and natural\nthe generated response is.\nWe further pair the generated responses from\neach prompting scheme, including Standard, Proac-\ntive, ProCoT, and Ground-truth (GT), with the\ncorresponding responses from each of the other\nprompting scheme to compute the overall win rates\nbetween each pair.\nAs for the objective human judgement, we adopt\nthe sale-to-list ratio (SL%) (Joshi et al., 2021; Dutt\net al., 2021) as an indicator for explicitly measur-\ning the negotiation inclination in the generated re-\nsponse:\nSL% =bargain price −buyer target price\nlisted price −buyer target price , (4)\nwhere the bargain price is the price that the seller\nwould like to sell the item at the current turn. The\nlower the SL%, the more compromise the seller\nhave made.\nTo sum up, the instructions provided for annota-\ntors are as follows:\n• Persuasive: Whether the seller is persuasive in\nbargaining the price.\n• Coherent: Whether the seller’s responses are on\ntopic and in line with the conversation history.\n• Natural: Whether the seller is human-like.\n• Bargain Price: What is the current bargain price\nfrom the seller’s side.\n• Win: Assume you are the seller. Which dialogue\nsystem you would like to use for bargain the price\nwith the buyer (Win/Tie/Lose).\nUsage of LLMs & Prompting Schemes. The\nadopted LLMs are the same, but the maximum\nnumber of new tokens is set to be 256, as there are\nmore information needed to be generated, includ-\ning negotiation strategies and dialogue acts.\nBaselines. We compare several fine-tuned SOTA\nbaselines for negotiation dialogues, including Fe-\nHED (Zhou et al., 2020), HED+RNN/TFM, and\nDIALO GRAPH (Joshi et al., 2021).\n4.3.3 Experimental Results\nTable 6 and Table 7 present the results with auto-\nmatic and human evaluation metrics, respectively.\nThere are several notable findings as follows:\nLLM-based dialogue systems fail to predict ap-\npropriate negotiation strategies and dialogue\nacts. Table 6 shows that failures on strategy learn-\ning further result in a poor performance of re-\nsponse generation. Specifically, ChatGPT gener-\nally performs better than Vicuna in strategy learn-\ning. Although both proactive and ProCoT prompt-\ning schemes can slightly improve the final perfor-\nmance of response generation, there is still a large\ngap from fine-tuned methods according to auto-\nmatic evaluation metrics.\nThe key challenge of LLMs in handling non-\ncollaborative dialogues is how to effectively op-\ntimize the strategy planning. Table 7 shows\nthat the generated responses conditioned on ref-\nerence strategies are more favorable (Win Rates).\nIn specific, ChatGPT guarantees a high score on the\nhuman-like response generation (Natural). With\nthe ProCoT, the generated responses are more\ncoherent to the conversation history ( Coherent),\nwhich can also be observed from the case study in\n10609\ninformagreeintroreject\ncounter-pricevague-price\ninquiryunknown\ninsistacceptoffer\ndisagreeinit-price\nquit\ninform\nagree\nintro\nreject\ncounter-price\nvague-price\ninquiry\nunknown\ninsist\naccept\noffer\ndisagree\ninit-price\nquit\nZero-shot Standard\ninformagreeintroreject\ncounter-pricevague-price\ninquiryunknown\ninsistacceptoffer\ndisagreeinit-price\nquit\ninform\nagree\nintro\nreject\ncounter-price\nvague-price\ninquiry\nunknown\ninsist\naccept\noffer\ndisagree\ninit-price\nquit\nZero-shot Proactive\ninformagreeintroreject\ncounter-pricevague-price\ninquiryunknown\ninsistacceptoffer\ndisagreeinit-price\nquit\ninform\nagree\nintro\nreject\ncounter-price\nvague-price\ninquiry\nunknown\ninsist\naccept\noffer\ndisagree\ninit-price\nquit\nZero-shot ProCoT\n0\n50\n100\n150\n200\n250\n300\n350\n0\n100\n200\n300\n400\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nFigure 2: Heatmaps on the relationships between target and predicted dialogue acts. As no dialogue act is predicted\nin standard prompting, a dialogue act classifier is trained to identify the dialogue act of the generated response.\nAppendix D.3. However, compared with prompt-\ning with reference strategies, all the other prompt-\ning schemes fall short of generating persuasive re-\nsponses for negotiation ( Persuasive), indicating\ntheir shortcomings on strategy learning. This is also\nvalidated by the objective judgement on Sale-to-\nList Ratio, which shows that ChatGPT can reach\na better deal for itself when being conditioned on\nreference strategies. Similarly, Chen et al. (2023c)\nempirically show that, given the optimal planned\nstrategy, ChatGPT achieves strong performance\non controllable response generation in some other\nstrategy-based dialogues.\n4.3.4 Analysis of Strategy Learning\nFigure 2 presents the analysis of the relationships\nbetween the target and predicted dialogue acts by\nChatGPT. As for the standard prompting, we ob-\nserve two typical mistakes: 1) The system tends\nto propose the initial bargain price (init-price),\ninstead of greetings ( intro) and waiting for the\nbuyer to initialize the bargain. 2) The system often\ndirectly accepts the buyer’s offer (accept) when it\nis supposed to offer another price for negotiation\n(offer). This also explains why the Sale-to-List\nRatio is relatively low when using standard prompt-\ning in Table 7. On the other hand, Proactive and\nProCoT prompting share similar patterns of mis-\ntakes, where ChatGPT tends to propose a counter\nprice (counter-price) to negotiate with the buyer.\nAppendix E presents the analysis of the distri-\nbution of selected strategies by ChatGPT. In the\nreference responses, the seller often shows posi-\ntive/negative sentiment to negotiate with the buyer.\nHowever, ChatGPT inclines to adopt conservative\nor concessionary strategies, such as using hedge\nwords, show gratitude, or propose a counter price.\nOverall, we conclude that ChatGPT tends to\nmake compromise with the buyer during the\nnegotiation, rather than strategically taking ac-\ntions to maximize its own benefit.\n5 Conclusion\nIn this work, we conduct the first comprehensive\nevaluation on the capability of LLM-based dia-\nlogue systems in handling proactive dialogues,\nincluding clarification, target-guided, and non-\ncollaborative dialogues. To enhance the proactiv-\nity of LLM-based dialogue systems, we propose a\nproactive chain-of-thought prompting scheme that\ntriggers the reasoning and planning capability of\nLLMs. The empirical analysis sheds light on the\npotentials of LLMs for proactive dialogues: 1) Pro-\nCoT largely enhances the originally poor perfor-\nmance of LLMs in asking clarification questions,\nbut still limits in handling domain-specific appli-\ncations. 2) LLM-based dialogue systems perform\naggressive topic shifting towards the designated\ntarget, while ProCoT enables the topic planning to\nbe smoother. 3) Despite the strength on control-\nlable response generation, the capability of strategy\nlearning and planning is a key challenge for LLMs\nin handling non-collaborative dialogues.\nAcknowledgement\nThis research is supported by NExT Research Cen-\nter.\nLimitation\nIn this section, we discuss the limitations of this\nwork from the following perspectives:\nSensitivity of Prompts Similar to other studies\non prompting LLMs for dialogue applications (Lee\net al., 2023; Chen et al., 2023c,a), the evaluation\nresults are likely to be sensitive to the choice of\nprompts. Besides, it is also likely that the designed\nprompts are not the optimal ones for the concerned\n10610\nproblem. In fact, prompt sensitivity and optimality\nthemselves are valuable research problems in dia-\nlogue systems, which can be further investigated\nin the future studies. To facilitate the reproducibil-\nity of this work, we will release all the prompts\nused in the experiments and provide detailed de-\nscriptions about the designs of each prompting\nscheme in Appendix C. The code and data will be\nreleased via https://github.com/dengyang17/\nLLM-Proactive.\nFinancial and Computational Cost of LLMs It\nis financially expensive to call the API of commer-\ncial LLMs for experiments. In our experiments, it\ncosts about $120 to call the OpenAI API for get-\nting all the experimental results of ChatGPT. On\nthe other hand, it is computationally expensive to\nconduct experiments with open-source LLMs in\nlocal machines. In our experiments, we choose Vi-\ncuna 13B as the open-source LLM for evaluation,\nwhich can be adapted to NVIDIA DGX-1 V100\n32G for inference. If more budgets and better ex-\nperimental environment are permitted, it would be\ngreat to evaluate how other larger LLMs performs\nin the concerned proactive dialogue problems, such\nas GPT-4, LLaMA/Vicuna 65B, etc.\nCapability of Planning and Decision Making\nThe proposed ProCoT prompting scheme can be\nregarded as a preliminary attempt at triggering the\ncapability of planning and decision making from\nLLM-based dialogue systems. Compared with fine-\ntuned methods, such ability of LLMs is still weak\nas we learn from the empirical analysis. Moreover,\nsimply prompting LLMs to be proactive may fall\nshort of handling decision making under dynamic\nenvironments in real-world applications. It is worth\nstudying how LLM-based dialogue systems handle\nthe proactive dialogue problems in an interactive\nsetting with more diverse user simulation (Lei et al.,\n2022; Fu et al., 2023).\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Le. 2020. Towards a human-like open-\ndomain chatbot. CoRR, abs/2001.09977.\nMohammad Aliannejadi, Julia Kiseleva, Aleksandr\nChuklin, Jeff Dalton, and Mikhail S. Burtsev. 2021.\nBuilding and evaluating open-domain dialogue cor-\npora with clarifying questions. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021 , pages 4473–\n4484.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. CoRR, abs/2302.04023.\nLiang Chen, Hongru Wang, Yang Deng, Wai-Chung\nKwan, Zezhong Wang, and Kam-Fai Wong. 2023a.\nTowards robust personalized dialogue generation via\norder-insensitive representation regularization. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023, pages 7337–7345.\nMaximillian Chen, Alexandros Papangelis, Chenyang\nTao, Seokhwan Kim, Andy Rosenbaum, Yang Liu,\nZhou Yu, and Dilek Hakkani-Tur. 2023b. PLACES:\nprompting language models for social conversation\nsynthesis. In Findings of the Association for Compu-\ntational Linguistics: EACL 2023, pages 814–838.\nMaximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi,\nand Zhou Yu. 2023c. Controllable mixed-initiative\ndialogue generation through prompting. CoRR,\nabs/2305.04147.\nZhi Chen, Lu Chen, Bei Chen, Libo Qin, Yuncong Liu,\nSu Zhu, Jian-Guang Lou, and Kai Yu. 2022. Unidu:\nTowards A unified generative dialogue understand-\ning framework. In Proceedings of the 23rd Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, SIGDIAL 2022, pages 442–455.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality.\nYang Deng, Wenqiang Lei, Minlie Huang, and Tat-Seng\nChua. 2023a. Goal awareness for conversational\nAI: proactivity, non-collaborativity, and beyond. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics: Tutorial\nAbstracts, ACL 2023, pages 1–10.\nYang Deng, Wenqiang Lei, Wai Lam, and Tat-Seng\nChua. 2023b. A survey on proactive dialogue sys-\ntems: Problems, methods, and prospects. In Proceed-\nings of the Thirty-Second International Joint Con-\nference on Artificial Intelligence, IJCAI 2023, pages\n6583–6591.\nYang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam,\nand Tat-Seng Chua. 2022a. PACIFIC: towards proac-\ntive conversational question answering over tabular\nand textual data in finance. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022 , pages 6970–\n6984.\n10611\nYang Deng, Wenxuan Zhang, Wai Lam, Hong Cheng,\nand Helen Meng. 2022b. User satisfaction estima-\ntion with sequential dialogue act modeling in goal-\noriented conversational systems. In WWW ’22: The\nACM Web Conference 2022, pages 2998–3008.\nYang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\nLei, Tat-Seng Chua, and Wai Lam. 2023c. A unified\nmulti-task learning framework for multi-goal con-\nversational recommender systems. ACM Trans. Inf.\nSyst., 41(3):77:1–77:25.\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\nAlexander H. Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\nLowe, Shrimai Prabhumoye, Alan W. Black, Alexan-\nder I. Rudnicky, Jason D. Williams, Joelle Pineau,\nMikhail S. Burtsev, and Jason Weston. 2019. The sec-\nond conversational intelligence challenge (convai2).\nCoRR, abs/1902.00098.\nRitam Dutt, Sayan Sinha, Rishabh Joshi, Surya Shekhar\nChakraborty, Meredith Riggs, Xinru Yan, Haogang\nBao, and Carolyn P. Rosé. 2021. Resper: Computa-\ntionally modelling resisting strategies in persuasive\nconversations. In EACL.\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.\n2023. Improving language model negotiation with\nself-play and in-context learning from AI feedback.\nCoRR, abs/2305.10142.\nMeiqi Guo, Mingda Zhang, Siva Reddy, and Malihe\nAlikhani. 2021. Abg-coqa: Clarifying ambiguity in\nconversational question answering. In 3rd Confer-\nence on Automated Knowledge Base Construction,\nAKBC 2021.\nHe He, Derek Chen, Anusha Balakrishnan, and Percy\nLiang. 2018. Decoupling strategy and generation in\nnegotiation dialogues. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2333–2343.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-\nshot planners: Extracting actionable knowledge for\nembodied agents. In International Conference on\nMachine Learning, ICML, volume 162, pages 9118–\n9147.\nRishabh Joshi, Vidhisha Balachandran, Shikhar\nVashishth, Alan W. Black, and Yulia Tsvetkov. 2021.\nDialograph: Incorporating interpretable strategy-\ngraph networks into negotiation dialogues. In ICLR.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021.\nDialogue state tracking with a language model using\nschema-driven prompting. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021 , pages 4937–\n4949.\nGibbeum Lee, V olker Hartmann, Jongho Park, Dimitris\nPapailiopoulos, and Kangwook Lee. 2023. Prompted\nllms as chatbot modules for long open-domain con-\nversation. CoRR, abs/2305.04533.\nWenqiang Lei, Yao Zhang, Feifan Song, Hongru Liang,\nJiaxin Mao, Jiancheng Lv, Zhenglu Yang, and Tat-\nSeng Chua. 2022. Interacting with non-cooperative\nuser: A new paradigm for proactive dialogue policy.\nIn SIGIR ’22: The 45th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 212–222.\nYu Li, Kun Qian, Weiyan Shi, and Zhou Yu. 2020. End-\nto-end trainable non-collaborative dialog system. In\nThe Thirty-Fourth AAAI Conference on Artificial In-\ntelligence, AAAI 2020, pages 8293–8302.\nLizi Liao, Grace Hui Yang, and Chirag Shah. 2023.\nProactive conversational agents. In Proceedings of\nthe Sixteenth ACM International Conference on Web\nSearch and Data Mining, WSDM 2023, pages 1244–\n1247. ACM.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022. Multi-stage prompting for\nknowledgeable dialogue generation. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1317–1337.\nFei Mi, Yasheng Wang, and Yitong Li. 2022. CINS:\ncomprehensive instruction for few-shot learning in\ntask-oriented dialog systems. In Thirty-Sixth AAAI\nConference on Artificial Intelligence, AAAI 2022 ,\npages 11076–11084.\nJinghui Qin, Zheng Ye, Jianheng Tang, and Xiaodan\nLiang. 2020. Dynamic knowledge routing network\nfor target-guided open-domain conversation. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, pages 8657–8664.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. Coqa: A conversational question answering\nchallenge. Trans. Assoc. Comput. Linguistics, 7:249–\n266.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In EACL.\nTeven Le Scao and Alexander M. Rush. 2021. How\nmany data points is a prompt worth? In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\npages 2627–2636.\nKarin Sevegnani, David M. Howcroft, Ioannis Konstas,\nand Verena Rieser. 2021. Otters: One-turn topic tran-\nsitions for open-domain dialogue. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\n10612\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, pages 2492–2504.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n373–393.\nJianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xi-\naodan Liang, Eric P. Xing, and Zhiting Hu. 2019.\nTarget-guided open-domain conversation. In Pro-\nceedings of the 57th Conference of the Association for\nComputational Linguistics, ACL 2019, pages 5624–\n5634.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. GitHub repos-\nitory.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHongru Wang, Rui Wang, Fei Mi, Zezhong Wang,\nRuifeng Xu, and Kam-Fai Wong. 2023a. Chain-of-\nthought prompting for responding to in-depth dia-\nlogue questions with LLM. CoRR, abs/2305.11792.\nJian Wang, Dongding Lin, and Wenjie Li. 2023b. Di-\nalogue planning via brownian bridge stochastic pro-\ncess for goal-directed proactive dialogue. CoRR,\nabs/2305.05290.\nRui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang,\nYasheng Wang, Yitong Li, Lifeng Shang, Kam-Fai\nWong, and Ruifeng Xu. 2023c. Retrieval-free knowl-\nedge injection through multi-document traversal for\ndialogue models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, pages\n6608–6619.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020.\nXuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh,\nSijia Yang, Jingwen Zhang, and Zhou Yu. 2019. Per-\nsuasion for good: Towards a personalized persuasive\ndialogue system for social good. In Proceedings of\nthe 57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, pages 5635–5649.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. CoRR,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-shot information extraction via\nchatting with chatgpt. CoRR, abs/2302.10205.\nChien-Sheng Wu, Steven C. H. Hoi, Richard Socher,\nand Caiming Xiong. 2020. TOD-BERT: pre-trained\nnatural language understanding for task-oriented di-\nalogue. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2020, pages 917–929.\nWenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu,\nXiyuan Zhang, Rongzhong Lian, and Haifeng Wang.\n2019. Proactive human-machine conversation with\nexplicit conversation goal. In Proceedings of the 57th\nConference of the Association for Computational Lin-\nguistics, ACL 2019, pages 3794–3804.\nZhitong Yang, Bo Wang, Jinfeng Zhou, Yue Tan, Dong-\nming Zhao, Kun Huang, Ruifang He, and Yuexian\nHou. 2022. Topkg: Target-oriented dialog via global\nplanning on knowledge graph. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, COLING 2022, pages 745–755.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. CoRR, abs/2210.03629.\n10613\nHamed Zamani, Johanne R. Trippas, Jeff Dalton, and\nFilip Radlinski. 2022. Conversational information\nseeking. CoRR.\nHaolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua,\nSuraj Sharma, Zhuang Li, Lizhen Qu, and Gholam-\nreza Haffari. 2022. Let’s negotiate! A survey of\nnegotiation dialogue systems. CoRR.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020a. Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020.\nXiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou,\nand Helen Meng. 2023. Sgp-tod: Building task bots\neffortlessly via schema-guided llm prompting.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020b. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, ACL 2020, pages\n270–278.\nZheng Zhang, Ryuichi Takanobu, Qi Zhu, MinLie\nHuang, and XiaoYan Zhu. 2020c. Recent advances\nand challenges in task-oriented dialog systems. Sci-\nence China Technological Sciences , 63(10):2011–\n2027.\nWeixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang,\nYanpeng Tong, and Bing Qin. 2023. Is chat-\ngpt equipped with emotional dialogue capabilities?\nCoRR, abs/2304.09582.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, volume 139, pages\n12697–12706.\nPeixiang Zhong, Yong Liu, Hao Wang, and Chunyan\nMiao. 2021. Keyword-guided neural conversational\nmodel. In Thirty-Fifth AAAI Conference on Artificial\nIntelligence, AAAI 2021, pages 14568–14576.\nYiheng Zhou, Yulia Tsvetkov, Alan W. Black, and Zhou\nYu. 2020. Augmenting non-collaborative dialog sys-\ntems with explicit semantic and strategic dialog his-\ntory. In 8th International Conference on Learning\nRepresentations, ICLR 2020.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-\nSeng Chua. 2021. TAT-QA: A question answering\nbenchmark on a hybrid of tabular and textual content\nin finance. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing, ACL/IJCNLP 2021, pages\n3277–3287.\nA Details of Datasets\nIn the experiment, we adopt the test sets from five\ndatasets for evaluation, including Abg-CoQA (Guo\net al., 2021), PACIFIC (Deng et al., 2022a), Ot-\nters (Sevegnani et al., 2021), TGConv (Yang et al.,\n2022), and CraigslistBargain (He et al., 2018). De-\ntailed descriptions of each dataset are as follows:\n• Abg-CoQA3 is constructed based on the CoQA\ndataset (Reddy et al., 2019) by truncating a par-\ntial conversation from the full conversation and\nselecting ambiguous questions.\n• PACIFIC4 is constructed based on the TAT-QA\ndataset (Zhu et al., 2021), an question answering\ndataset in the financial domain, whose contexts\ncontain a hybrid of tables and texts. Deng et al.\n(2022a) rewrite the questions to be ambiguous\nfor introducing clarification turns in the conver-\nsation.\n• OTTers is a next-turn target-oriented dialogue\ndataset, which requires the agent proactively gen-\nerate a transition utterance to approach the des-\nignated target. We adopt the processed version5\nby Yang et al. (2022) for evaluation. The topic is\nrepresented as a set of topical keywords.\n• TGConv is constructed based on ConvAI2 (Di-\nnan et al., 2019) and is split to two settings, in-\ncluding \"easy-to-reach\" and \"hard-to-reach\". The\ntopic is also represented as a set of topical key-\nwords.\n• CraigslistBargain was created in a negotiation\nsetting where two crowdsourced workers play\nthe roles of the buyer and the seller to bargain\nthe price of an item. We adopt the processed\nversion6 by Joshi et al. (2021) for evaluation,\nwhich assigns 10 dialogue acts and 21 negotiation\nstrategies to the utterances.\nB Error Analysis Details for Clarification\nDialogues\nAs shown in Table 9, we categorize these failure\ncases into the following four groups:\n3https://github.com/MeiqiGuo/\nAKBC2021-Abg-CoQA/tree/main/abg-coqa\n4https://github.com/dengyang17/PACIFIC/tree/\nmain/data/pacific. Since the labels in the test set is not\npublicly released, we adopt the validation set for evaluation.\n5https://github.com/yyyyyyzt/topkgchat\n6https://github.com/rishabhjoshi/DialoGraph_\nICLR21/tree/main\n10614\n0 1 2 3 4 5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nAbg-CoQA F1\n0 1 2 3 4 5\n0\n2\n4\n6\n8\n10\n12\n14\nOTT ers Hit@1\n0 1 2 3 4 5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nCraigslistBargain BLEU\nStandard ProCoT One-shot (B) One-shot (C) ProCoT Annotation (B) ProCoT Annotation (C)\nFigure 3: Performance in terms of different prompts, including different one-shot examples and different ProCoT\nannotations. The reported results are based on Vicuna-13B.\nAvg. Avg.\nDataset #Dialog #Turns #Words\nAbg-CoQA 1055 5.04 4.87\nPACIFIC 278 7.10 6.80\nOTTers 372 3.00 9.19\nTGConv 751 8.97 12.47\nCraigslistBargain 567 9.78 11.65\nTable 8: Statistics of datasets.\nUnder-\nspecified\nQ: What is the 2019 average defined\nschemes?\nClarification G: What kind of defined schemes are you\nasking about?\nP: Could you please clarify what you mean\nby \"average defined schemes\"?\nWrong\nAspect\nQ: What was the increase / (decrease) in the\ncost?\nG: Which period are you asking about?\nP: Which cost are you referring to?\nOver-\nspecified\nQ: How much amount of goodwill acquisi-\ntions was done in 2019?\nClarification G: What kind of goodwill acquisitions are\nyou asking about?\nP: Are you asking about the total amount of\ngoodwill activity or specifically about acqui-\nsitions in 2019?\nGeneration Q: How much was the average operating\nError income?\nG: Which period are you asking about?\nP: The document does not provide informa-\ntion on the average operating income. There-\nfore, the question cannot be answered.\nTable 9: Error Analysis (Q: User Query, G: Ground-\ntruth, P: Prediction).\n• Wrong Aspect: The model generates a question\nfor clarifying a wrong aspect of the user query.\n• Under-specified Clarification: The model gen-\nerates an under-specified clarification question,\nwhere the requested information is too general so\nthat it would be difficult for the user to provide\nfeedbacks.\n• Over-specified Clarification : The model gen-\nerates an over-specified clarification question,\nwhere the requested information is already clear\nin the user query.\n• Generation Error: Although the model identifies\nthe need for clarification, but it doesn’t generate\nthe output as the required format, such as no\nclarification question.\nC Designs of Prompts\nTable 10, 11, and 12 present the example of\nprompts for clarification, target-guided, non-\ncollaborative dialogues, respectively. As for the\nzero-shot setting, the overall prompt is composed\nby the task instruction and the sample. As for the\nfew-shot setting, the overall prompt is composed\nby the task instruction, a number of samples with\ndemonstrations, and the test sample.\nIn particular, we clarify several questions regard-\ning the prompt designs as follows:\nHow to construct the task instructions? The\ntask instructions first follow the problem definition\nfor each proactive dialogue problem. Then, similar\nto other studies on applying LLMs for different\ntasks (Wei et al., 2023; Bang et al., 2023), we fur-\nther instruct the LLMs to generate the response\nfollowing the desired output format for evaluation.\n10615\nReference Response  \nDo you want to know why he feeds them candy or why he feeds \nthem a good meal? \nA: Why he feeds them candy — to keep up their energy  \nB: Why he feeds them a good meal — to make sure he doesn't \nhave to share the cookies kids leave for him \nGenerated Responses \n(a) Standard Prompting (ChatGPT): To ensure that they have \nenough energy for their long journey delivering gifts. \n(b) Proactive Prompting (ChatGPT): The clarifying question is \n“Who are you referring to when you say ‘he'?” \n(c) ProCoT Prompting (ChatGPT): The document mentions \nseveral reasons why Santa makes sure the reindeer are well \ntaken care of, including giving them coffee to stay awake, \ncandy to keep up their energy, and a good meal before they go. \nTherefore, the question is ambiguous. The clarifying question \nis \"Which reason are you referring to?\" \nTask Background: The grounded document is “Santa Claus \nmakes sure that every year at Christmas time that his \nreindeer are ready to deliver gifts to girls and boys all over \nthe world. Without the reindeer, how would Santa fly from \nhome to home? The most important thing that Santa has to do \nfor the reindeer is to make sure they have their coffee on \nChristmas Eve. Without the coffee, they won't stay awake for \nvery long. Sleeping reindeer are not very helpful. Santa also \ngives the reindeer candy to keep up their energy. In the North \nPole, the main candy is candy canes in red and white. Reindeer \nlove the candy canes that are different colors best and Santa \nsays that helps them fly faster. Next, Santa has to make sure \nthat the reindeer have a good meal before they go. Santa \nwants to make sure he doesn't have to share the cookies kids \nleave for him. The last thing Santa has to do is make sure the \nreindeer know where they are going. Santa has only been lost \none time on Christmas Eve and he does not want that to \nhappen again.” \nConversation History: [“User”:  “What candy does he feed \nthem most often?”, “System”: “Candy canes.”, “User”: “How \nmany times has he been lost?”, “System”: “Once.”, “User”: \n“Why does he make sure they are well fed?”]\nClarification Dialogues: Abg-CoQA\nFigure 4: Case study from the Abg-CoQA dataset.\nHow to choose the one-shot sample? Due to\nthe input length limitation of LLMs, we could only\nadopt one-shot in-context learning (ICL). In or-\nder to testify the sensitivity of the choice of the\none-shot sample, we report the results with three\ndifferent one-shot samples in Figure 3. Despite\nthe variance among different one-shot examples as\nexpected when using exemplar-based ICL (Scao\nand Rush, 2021; Zhao et al., 2021), the observa-\ntions of adopting ProCoT for different proactive\ndialogues remain unchanged. Since the variance\nof one-shot ICL is inevitable, we simply adopt the\nfirst dialogue sample in the original training set of\neach dataset as the one-shot sample for facilitating\nreproducibility.\nHow to construct the demonstration of proac-\ntive chain-of-thoughts? The demonstration of\nReference Transition Response and Next Topics \nA: My dogs fall asleep to music. [“dog”, “fall”, “music”] \nB: I love dogs too and I also love music. [“love”, “dog”, “music”] \nC: My dog calms down when listening to music. [“dog”, “music”, \n“calm”, “listen”] \nGenerated Responses \n(a) Standard Prompting (ChatGPT): Dogs are amazing \ncompanions, aren't they? Have you ever tried listening to \nmusic while playing the piano? It can be a great way to relax \nand unwind. \n(b) Proactive Prompting (ChatGPT): The next topics are \n[\"pets\", \"music\", \"favorite\"]. The response is \"Pets and music \nare two of my favorite things. Do you have a favorite type of \npet or music to listen to?\" \n(c) ProCoT Prompting (ChatGPT): The current topic is [\"dogs\", \n\"love\"]. To bridge the current topic with the target topics, the \nnext topics are [\"pets\", \"music\", \"hobbies\", \"passion\"]. Based \non the predicted next topics, the response is \"Pets can bring \nso much joy and love into our lives. Speaking of hobbies and \npassions, do you enjoy listening to music or playing any \ninstruments like the piano?\" \nTask Background: The target topic is [“piano”, “listen”] \nConversation History: [“User”:  “I really love dogs.”]\nTarget-guided Dialogues: OTTers\nFigure 5: Case study from the OTTers dataset.\nproactive chain-of-thoughts is written by human\nannotators, which represents their own chain-of-\nthoughts of the planning or decision making for the\nproactive dialogues. Following Wei et al. (2022b),\nwe also report the results with the demonstrations\nfrom three different annotations. As shown in Fig-\nure 3, it can be observed that the variance is much\nsmaller than the one-shot sample. We adopt the\nbest ProCoT annotation for each dataset in the eval-\nuation.\nD Case Study\nIn order to intuitively compare the three prompting\nschemes, we conduct case studies on the gener-\nated responses regarding three proactive dialogue\nproblems.\nD.1 Clarification Dialogues\nFigure 4 presents a case study from the Abg-CoQA\ndataset. There are two possible answers for the\nambiguous user question in this case. When using\nstandard prompting (a), ChatGPT generates a re-\nsponse based on a random guess, potentially result-\ning in an answer that may not align with the user’s\ndesired outcome. When using proactive prompting\n(b), although ChatGPT decides to ask a clarification\nquestion, the concerned aspect of clarification is\nnot the focus of this conversation. ProCoT prompt-\n10616\nSelf-play Dialogues \n(a) Standard Prompting (Vicuna-13B) \nUser: I don't enjoy wine but I do enjoy psychology journals \nSystem: That's interesting. Have you ever read any articles on \nthe psychology of wine consumption? \nUser: I haven't read any articles on the psychology of wine \nconsumption specifically, but I have read some articles on the \npsychology of food and drink consumption in general. \nSystem: I'm glad you're interested in the psychology of wine \nconsumption.   \n…  \nFailed to reach the target in 8 turns. \n(b) Proactive Prompting (Vicuna-13B) \nUser: I don't enjoy wine but I do enjoy psychology journals \nSystem: I do enjoy reading, especially psychology journals \nUser: That’s great to hear!  What’s your favorite psychology \njournal to read? \nSystem: My favorite psychology journal to read is the journal \nof personality and social psychology \n…  \nFailed to reach the target in 8 turns. \n(c) ProCoT Prompting (Vicuna-13B) \nUser: I don't enjoy wine but I do enjoy psychology journals \nSystem: I am interested in psychology and reading about it in \njournals \nUser: That’s great to hear!  There are many interesting \npsychology journals out there.  Have you read any particularly \ngood ones lately? \nSystem: I have been reading a lot of gaullist psychology \njournals lately, they have some really interesting studies on \nthe effects of wine on the mind. \nSuccessful to reach the target in 4 turns! \n(d) ProCoT Prompting (ChatGPT) \nUser: I don't enjoy wine but I do enjoy psychology journals \nSystem: Interesting, have you ever read any Gaullist political \nphilosophy? It's a fascinating topic that combines both politics \nand psychology. \nSuccessful to reach the target in 2 turns! \nTask Background: The target topic is “Gaullist” \nConversation History: [“User”:  “I don't enjoy wine but I do \nenjoy psychology journals”]\nTarget-guided Dialogues: TGConv\nFigure 6: Case study from the TGConv dataset.\ning (c) first provides a thorough analysis to identify\nthe underlying reasons for the ambiguity in the\nuser’s question. This analysis serves as the founda-\ntion for generating an effective clarifying question,\nwhich helps to clarify and disambiguate the user’s\nintended meaning.\nD.2 Target-guided Dialogues\nFigure 5 presents a case study from the OTTers\ndataset, where the target topical keywords include\n\"piano\" and \"listen\", and the system is required to\ngenerate a transition response to approach the tar-\nget topics. It can be observed that the current topics\nin the user utterance, i.e., \"dog\", is completely dif-\nferent from the target topics. When using standard\nTurn 3 \nConversation History: ['Seller': 'Hi there, how are you \ntoday?', 'Buyer': 'What are the dimensions?’] \nReference: 40 by 60 inches. Fairly large, in good condition. \nStandard (ChatGPT): The dimensions are [insert dimensions \nhere]. Are you interested in purchasing it for the listed price \nof $50 or were you hoping to negotiate a better deal? \nProactive (ChatGPT): The most appropriate set of negotiation \nstrategies is ['informal', 'greet', 'inform'] and the most \nappropriate dialogue act is ['inquiry']. The response is \"The \ndimensions are [insert dimensions here]. Is there anything else \nyou would like to know?” \nProCoT (ChatGPT): The negotiation has not yet started, as the \nbuyer is asking for more information about the item. The \nappropriate goal is to provide the requested information and \nengage the buyer in the negotiation. To reach this goal, the \nmost appropriate set of negotiation strategies is ['informal', \n'greet', 'inform'] and the most appropriate dialogue act is \n['inquiry']. Based on the selected negotiation strategies and \ndialogue act, the response is \"Hi, I'm doing well, thank you for \nasking. The dimensions of the computer table are [insert \ndimensions here]. Is there anything else you would like to know \nabout it?\" \nTurn 9 \nConversation History: ['Seller': 'Hi there, how are you \ntoday?', 'Buyer': 'What are the dimensions?', 'Seller': '40 by \n60 inches. Fairly large, in good condition.', 'Buyer': 'Does the \nchair have rollers?', 'Seller': 'yes, it does', 'Buyer': 'It's \nbigger than I had wanted but might be able to make it work. \nWould you take $30?’, 'Seller': 'I would be wiling to do $40 \nwith delivery included.', 'Buyer': 'I can pick it up but thanks \nfor offering. Best I could do is $32'] \nReference: $35 and you have a deal. \nStandard (ChatGPT): I understand your budget constraints, \nbut I was hoping to get closer to my target selling price of \n$50. Would you be willing to meet me halfway at $45? \nProactive (ChatGPT): The most appropriate set of negotiation \nstrategies is [‘hedge’, 'informal', 'gratitude', 'neg_sentiment'] \nand the most appropriate dialogue act is ['counter-price']. The \nresponse is \"I appreciate your offer, but the lowest I can go is \n$45. It's already a great deal with the swivel chair included.\" \nProCoT (ChatGPT): The buyer has proposed a price that is \nlower than the target selling price. The seller has already \noffered a counter-price with delivery included, but the buyer \nis still negotiating. The next step should be to politely reject \nthe buyer's offer and propose a new counter-price closer to \nthe target selling price. To reach this goal, the most \nappropriate set of negotiation strategies is ['please', \n'certainty', 'propose', 'hedge', 'gratitude'] and the most \nappropriate dialogue act is ['counter-price']. Based on the \nselected negotiation strategies and dialogue act, the response \nis \"I appreciate your offer, but I cannot go lower than $38. \nWould that work for you?”\nTask Background: The item description is “Furniture. It’s a \nsolid wood tan color computer table with swivel chair for sale. \nIt’s in good condition as shown in pics. Moving out sale \ncomputer table and chair”. The target selling price is 50.\nNon-collaborative Dialogues: CraigslistBargain\nFigure 7: Case study from the CraigslistBargain dataset.\nprompting, ChatGPT leverages its overwhelming\nability of controllable generation to generate the\nresponse with aggressive topic transition. Despite\nits fluency, it is not a natural utterance with log-\n10617\n1st_pluralassertive\n3rd_singular\nhedge\nneg_senttrade_ininformalconcernfriend\ncertainty\n1st_singular3rd_plural\ngreet\ndiff_dic_neg\nfactive\ngratitude\ndiff_dic_pos\nfamily pleasepos_sentpropose\nNegotiation Strategy\n0.00\n0.05\n0.10\n0.15\n0.20Distribution\nReference\nZero-shot Standard\nZero-shot Proactive\nZero-shot ProCoT\nFigure 8: Distribution of selected negotiation strategies. Similarly, a negotiation strategy classifier is trained to\nidentify the negotiation strategies of the generated response in standard prompting.\nical or topical coherency. When using proactive\nprompting, although the predicted next topics are\nless aggressive than the standard prompting, the\ngenerated transition response just blends the cur-\nrent topics and the next topics together without\nnaturally connecting the topics. Finally, ChatGPT\nwith the ProCoT prompting generates a relatively\nsmoother transition response to bridge the current\ntopic with the target topic through planning about\nthe topic shifting process.\nFigure 6 presents a case study from the TGConv\ndataset, where the hard-to-reach target of this case\nis \"Gaullist\", and the system is required to reach\nthis target within 8 turns of conversations under the\nself-play simulation. As for Vicuna, it is struggled\nto lead the conversation towards this hard-to-reach\ntarget, when using standard and proactive prompt-\ning. ProCoT prompting enables Vicuna to effec-\ntively and smoothly drive the conversation towards\nthe designated target. In addition, owing to the\npowerful capability of controllable text generation,\nChatGPT directly responds with the target topic to\nthe initial user utterance. However, the topic tran-\nsition is relatively aggressive, which might down-\ngrade the user engagement or experience during\nthe conversation.\nD.3 Non-collaborative Dialogues\nFigure 7 presents a case study from the Craigslist-\nBargain dataset, where the system plays the seller\nrole to bargain with the buyer. At turn 3, even\nthough the buyer just inquires about the item in-\nformation without showing the bargain intention,\nChatGPT with standard prompting tends to initi-\nate the negotiation, which may put the seller in a\ndisadvantageous position. Proactive and ProCoT\nprompting enable the dialogue act and strategy pre-\ndiction of the next response. Especially for the\nanalysis of the current negotiation status, ProCoT\npoints out that the negotiation has not yet started.\nAt turn 9, we observe that the seller has already\nlowered down the bargain price to $40 in a previous\nturn. Without the reasoning and planning process,\nChatGPT with standard and proactive prompting\ngenerates the response with contradictory state-\nment, i.e., propose a higher counter price ($45)\nfor bargain, which is unreasonable in negotiation\ndialogues. With proactive CoTs, ChatGPT effec-\ntively summarizes the current negotiation progress\nand makes a better decision on the next negotiation\ngoal.\nE Analysis of Strategy Learning (Cont.)\nFigure 8 presents the analysis of the distribution of\nselected strategies by ChatGPT. As for the refer-\nence responses, we observe that the seller tends to\nexpress their positive/negative sentiment as well as\nnegotiate in a positive/negative manner. Differently,\nChatGPT with standard and proactive prompting\nprefers to use hedge words or polite expressions\n(e.g., please and gratitude), indicating that Chat-\nGPT essentially plays a nice role in negotiation.\nChatGPT with ProCoT prompting makes more de-\ncisions to use assertive words or trade in, compared\nwith other distributions. This shows that ProCoT\ncan enable ChatGPT to involve certain negotiation\nstrategies.\n10618\nClarification Dialogues\nStandard Prompting: Given the document and the conversation history, generate the response.\nProactive Prompting: Given the document and the conversation history, answer the question or ask a\nclarifying question. The response should start with \"The answer is\" or \"The clarifying question is\".\nProCoT Prompting: Given the document and the conversation history, first identify whether the\nquestion is ambiguous or not. If it is ambiguous, ask a clarifying question. If it is not ambiguous,\nanswer the question. The response should start with the ambiguity analysis of the question and then\nfollow by \"Therefore, the question is not ambiguous. The answer is\" or \"Therefore, the question is\nambiguous. The clarifying question is\".\nSample:\nDocument: \"Angie went to the library with her mother. First she had to turn in the books she was\nreturning at the return desk. They said hello to the man there. He took their books. Then they went\ninto the adult reading room. Angie sat in a brown chair at the table. She made a drawing of her\nmother. Her mother found a large red book. Then they went to the Mystery section. Angie sat in\na blue chair. She drew a picture of her brother. Her mother found the book. It was a green book.\nFinally it was time to go to the children’s room. It was Story Hour. Miss Hudson was there to read\nto all the children. She read a book about friendship. After the story Angie sat in the red chair and\nbegan drawing. They were drawing pictures of friends. Angie drew a picture of her best friend Lilly.\nMiss Hudson hung the pictures on the wall. Then Angie and her mother picked out 8 books to read\nat home. They checked the books out and went home.\"\nConversation history: [“User”: “What did she draw?”, “System”: “Her mother”, “User”: “What did\nher mother find?”, “System”: “The book”]\nQuestion: “What color was it?”\nDemonstration (Standard): Do you mean the first book?\nDemonstration (Proactive): The clarifying question is \"Do you mean the first book?\"\nDemonstration (ProCoT): There are two books that book that Angie’s mother found. It is uncertain\nwhich book is referred to. Therefore, the question is ambiguous. The clarifying question is \"Do you\nmean the first book?\"\nTable 10: Examples of prompting LLMs for clarification dialogues.\n10619\nTarget-guided Dialogues\nStandard Prompting: Given the target topic and the conversation history, generate the response.\nProactive Prompting: Given the target topic and the conversation history, predict the appropriate\nnext topics that can bridge the current conversation topics to approach the target topics smoothly.\nThen based on the predicted next topics, generate the response. Please reply by completing the output\ntemplate \"The next topics are []. The response is\".\nProactive Prompting w/ CoT: Given the target topic and the conversation history, consider the rela-\ntionship between the current conversation topics and the target topics, and then predict the appropriate\nnext topics that can bridge the current conversation topics to approach the target topics smoothly.\nThen based on the predicted next topics, generate the response. Please reply by completing the output\ntemplate \"The current topics are []. To bridge the current topics with the target topics, the next topics\nare []. Based on the predicted next topics, the response is\".\nSample:\nTarget topic: \"Chicken\"\nConversation history: [\"User\": \"I also remodel homes when I am not out bow hunting.\", \"System\":\n\"That’s neat. When I was in high school I placed 6th in 100m dash!\", \"User\": \"That’s awesome. Do\nyou have a favorite season or time of year?\"]\nDemonstration (Standard): I do not. But I do have a favorite meat since that is all I eat exclusively.\nDemonstration (Proactive): The next topics are [\"eat\", \"meat\"]. The response is \"I do not. But I do\nhave a favorite meat since that is all I eat exclusively.\"\nDemonstration (ProCoT): The current topics are [\"season\", \"time\", \"year\"]. To bridge the current\ntopics with the target topics, the next topics are [\"eat\", \"meat\"]. Based on the predicted next topics, the\nresponse is \"I do not. But I do have a favorite meat since that is all I eat exclusively.\"\nTable 11: Examples of prompting LLMs for target-guided dialogues.\n10620\nNon-collaborative Dialogues\nStandard Prompting: Assume you are the seller. Given the item description, the target selling price,\nand the conversation history, generate the response.\nProactive Prompting: Assume you are the seller. Given the item description, the target selling\nprice, and the conversation history, in order to reach a better deal with the buyer, first select the most\nappropriate set of negotiation strategies and the most appropriate dialogue act to reach the bargain\nprice. Based on the selected negotiation strategies and dialogue act, generate the response. The reply\nshould be in the form \"The most appropriate set of negotiation strategies is [] and the most appropriate\ndialogue act is []. Based on the selected negotiation strategies and dialogue act, the response is\"\nProCoT Prompting: Assume you are the seller. Given the item description, the target selling price,\nand the conversation history, in order to reach a better deal with the buyer, first analyse the current\nnegotiation progress and consider an appropriate negotiation goal, then select the most appropriate set\nof negotiation strategies and the most appropriate dialogue act to reach the bargain price. Based on the\nselected negotiation strategies and dialogue act, generate a response. The reply should start with the\nanalysis of the current negotiation progress and an appropriate goal, and then follow by \"To reach this\ngoal, the most appropriate set of negotiation strategies is [] and the most appropriate dialogue act is [].\nBased on the selected negotiation strategies and dialogue act, the response is\"\nPre-defined Dialogue Acts: [Answer a question, Proposing a counter price, Accept the offer, ...]\nPre-defined Negotiation Strategies: [Communicate politely, Build rapport, Show dominance, ...]\nSample:\nItem description: \"Phone charge two devices simultaneously on the go. This vehicle charger with an\nadditional USB port delivers enough power to charge two devices at once. The push button activated\nled connector light means no more fumbling in the dark trying to connect your device. Auto detect\nIC technology automatically detects the device type and its specific charging needs for improved\ncompatibility. And the built in indicator light illuminates red to let you know the charger is receiving\npower and the power socket is working properly. Verizon car charger with dual output micro USB\nand led light.\" Target selling price: 10.\nConversation history: [\"Buyer\": \"Hi, not sure if the charger would work for my car. can you sell\nit to me for $5?\", \"Seller\": \"It will work, I have never seen a car without a cigarette lighter port.\",\n\"Buyer\": \"Still, can i buy it for $5? I’m on a tight budget.\"]\nDemonstration (Standard): I think the lowest I would want to go is 8.\nDemonstration (Proactive): The most appropriate set of negotiation strategies is [\"Propose price\",\n\"Show dominance\", ’Certainty words’] and the most appropriate dialogue act is [\"Proposing a counter\nprice\"]. Based on the selected negotiation strategies and dialogue act, the response is \"I think the\nlowest I would want to go is 8.\"\nDemonstration (ProCoT): The buyer proposes a low price, which is unacceptable. The next step\nshould assertively raise the bargain price. To reach this goal, the most appropriate set of negotiation\nstrategies is [\"Propose price\", \"Show dominance\", ’Certainty words’] and the most appropriate dialogue\nact is [\"Proposing a counter price\"]. Based on the selected negotiation strategies and dialogue act, the\nresponse is \"I think the lowest I would want to go is 8.\"\nTable 12: Examples of prompting LLMs for non-collaborative dialogues.\n10621",
  "topic": "Proactivity",
  "concepts": [
    {
      "name": "Proactivity",
      "score": 0.9697858095169067
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6254173517227173
    },
    {
      "name": "Computer science",
      "score": 0.5952374935150146
    },
    {
      "name": "Key (lock)",
      "score": 0.5689468383789062
    },
    {
      "name": "Knowledge management",
      "score": 0.44910311698913574
    },
    {
      "name": "Work (physics)",
      "score": 0.4405592978000641
    },
    {
      "name": "Process management",
      "score": 0.3506723940372467
    },
    {
      "name": "Psychology",
      "score": 0.35036343336105347
    },
    {
      "name": "Social psychology",
      "score": 0.20511996746063232
    },
    {
      "name": "Computer security",
      "score": 0.11393839120864868
    },
    {
      "name": "Business",
      "score": 0.09541094303131104
    },
    {
      "name": "Engineering",
      "score": 0.08350604772567749
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}