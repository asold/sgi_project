{
    "title": "Large language models versus traditional textbooks: optimizing learning for plastic surgery case preparation",
    "url": "https://openalex.org/W4411888865",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4311821267",
            "name": "Chandler Hinson",
            "affiliations": [
                "T-Mobile (United States)",
                "University of South Alabama",
                "University of Mobile"
            ]
        },
        {
            "id": "https://openalex.org/A2981570032",
            "name": "Cybil Sierra Stingl",
            "affiliations": [
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2583862803",
            "name": "Rahim Nazerali",
            "affiliations": [
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4311821267",
            "name": "Chandler Hinson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2981570032",
            "name": "Cybil Sierra Stingl",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2583862803",
            "name": "Rahim Nazerali",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3209030113",
        "https://openalex.org/W4280557200",
        "https://openalex.org/W4200092215",
        "https://openalex.org/W4363679509",
        "https://openalex.org/W4282916947",
        "https://openalex.org/W4392938070",
        "https://openalex.org/W4393054152",
        "https://openalex.org/W4389577521",
        "https://openalex.org/W4393119065",
        "https://openalex.org/W3170344956",
        "https://openalex.org/W4323354733",
        "https://openalex.org/W4389917881",
        "https://openalex.org/W4392109210",
        "https://openalex.org/W2957974465",
        "https://openalex.org/W4386553312",
        "https://openalex.org/W4390521419",
        "https://openalex.org/W4385827730",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4403103282",
        "https://openalex.org/W4386117324",
        "https://openalex.org/W4400889289",
        "https://openalex.org/W4386023503"
    ],
    "abstract": "While LLMs show promise in generating thorough educational content, they require improvement in conciseness, accuracy, and utility for practical case preparation. ChatGPT generally outperforms Gemini, indicating variability in LLM capabilities. Further development should focus on enhancing accuracy and consistency to establish LLMs as reliable tools in medical education and practice.",
    "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nHinson et al. BMC Medical Education          (2025) 25:984 \nhttps://doi.org/10.1186/s12909-025-07550-8\nBMC Medical Education\n*Correspondence:\nChandler Hinson\ncsh2121@jagmail.southalabama.edu\n1Frederick P . Whiddon College of Medicine, University of South Alabama, \n5851 USA North Drive, Mobile, AL 36688, USA\n2Stanford Department of Surgery, Division of Plastic and Reconstructive \nSurgery, 770 Welch Road, Palo Alto, CA 94304, USA\n35795 USA North Drive, Mobile, AL 36608, USA\nAbstract\nBackground Large language models (LLMs), such as ChatGPT-4 and Gemini, represent a new frontier in surgical \neducation by offering dynamic, interactive learning experiences. Despite their potential, concerns about the \naccuracy, depth of knowledge, and bias in LLM responses persist. This study evaluates the effectiveness of LLMs in \naiding surgical trainees in plastic and reconstructive surgery through comparison with traditional case-preparation \ntextbooks.\nMethods Six representative cases from key areas of plastic and reconstructive surgery—craniofacial, hand, \nmicrosurgery, burn, gender-affirming, and aesthetics—were selected. Four types of questions were developed \nfor each case to cover clinical anatomy, indications, contraindications, and complications. Responses from LLMs \n(ChatGPT-4 and Gemini) and textbooks were compared using surveys distributed to medical students, research \nfellows, residents, and attending surgeons. Reviewers rated each response on accuracy, thoroughness, usefulness for \ncase preparation, brevity, and overall quality using a 5-point Likert scale. Statistical analyses, including ANOVA and \nunpaired T-tests, were conducted to assess the differences between LLM and textbook responses.\nResults A total of 90 surveys were completed. LLM responses were rated as more thorough (p < 0.001) but \nless concise (p < 0.001) than textbook responses. Textbooks were rated superior for answering questions on \ncontraindications (p = 0.027) and complications (p = 0.014). ChatGPT was perceived as more accurate (p = 0.018), \nthorough (p = 0.002), and useful (p = 0.026) than Gemini. Gemini was rated lower in quality (p = 0.30) compared \nto ChatGPT along with being inferior to textbook answers for burn-related questions (p = 0.017) and anatomical \nquestions (p = 0.013).\nConclusion While LLMs show promise in generating thorough educational content, they require improvement in \nconciseness, accuracy, and utility for practical case preparation. ChatGPT generally outperforms Gemini, indicating \nvariability in LLM capabilities. Further development should focus on enhancing accuracy and consistency to establish \nLLMs as reliable tools in medical education and practice.\nKeywords Artificial intelligence, Education, Plastic and reconstructive surgery, Medical school, Residency, Surgery, \nChatgpt, Large language models, Gemini\nLarge language models versus traditional \ntextbooks: optimizing learning for plastic \nsurgery case preparation\nChandler Hinson1,3*, Cybil Sierra Stingl2 and Rahim Nazerali2\nPage 2 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \nIntroduction\nIn the realm of surgical education, the implementation \nof large language models (LLMs) marks a significant \nmilestone. Large language models are a type of artificial \nintelligence algorithm that leverages deep learning tech -\nniques and large datasets to understand, summarize, and \ngenerate text-based content [ 1]. These models, such as \nOpenAI’s Chat Generative Pre-Trained Transformer-4 \n(ChatGPT-4) and Google’s Gemini, formally known as \nBard, are reshaping the landscape of information acqui -\nsition by providing a quicker and more accessible gate -\nway to necessary information [ 2, 3]. Unlike traditional \nsources, LLMs can process and generate human-like \ntext based on vast amounts of data, making them pow -\nerful tools for a variety of applications, such as medical \neducation. LLMs like GPT-4 and Gemini represent the \nforefront of artificial intelligence (AI) technology. The \nplatforms have access to large amounts of data and uti -\nlizes sophisticated algorithms in understanding user \nrequests and generating human-like responses. They can \nengage in detailed, nuanced conversations, provide sum -\nmaries of extensive documents, and even generate cre -\native content.\nThe integration of LLMs into surgical education has \nbecome a heavily studied area, with many papers stating \nthe promising application for LLMs in surgical education \n[4–8]. The use of AI offers a dynamic, interactive experi -\nence that contrasts sharply with the static nature of text -\nbooks. By allowing learners to pose specific questions, \nLLMs can provide easily accessible, immediate, tailored \nresponses.\nHowever, it is crucial to acknowledge the reservations \nsurrounding LLMs. As a relatively novel technology, \nthere are concerns about the accuracy of LLM responses \nalong with the depth of knowledge the platforms’ pos -\nsess [ 9, 10]. While there has been extensive research of \nLLMs with regards to evidence-based applications in \nmedical diagnostics, there is a level of skepticism about \ntheir reliability and effectiveness in medical education. \nFurthermore, issues such as potential biases in the train -\ning data, the inability to access proprietary medical data -\nbases, and the models’ reliance on pre-existing internet \ndata can lead to questions about the comprehensiveness \nand objectivity of the information it provides to learners \n[11–16]. \nThis study aims to address these concerns by examin -\ning how LLMs respond to case questions for surgical \ntrainees. This study analyzes the quality and efficiency \nof LLM responses and their utility in aiding surgical \ntrainees’ preparation, specifically for plastic and recon -\nstructive surgery. By conducting surveys amongst medi -\ncal students, research fellows, residents, and attendings, \nreviewers compared responses of LLMs with textbook \nquestion-answer pairs, the current gold-standard in \nsurgical education and case preparation. We aim to dis -\ncern strengths, limitations, and potential implications of \nintegrating LLMs into plastic and reconstructive surgical \neducation.\nMethodology\nSix common cases that trainees must be familiar with \nwere selected for each major field of plastic and recon -\nstructive surgery. The fields (and cases) included were \ncraniofacial (cleft lip and palate), hand (carpal tunnel \nrelease), microsurgery (autologous breast reconstruc -\ntion), burn (split-thickness skin graft), gender affirming \n(phalloplasty), and aesthetics (liposuction). Four types of \nquestions were used to encompassed important areas of \ncase preparation: clinical anatomy, indications of the pro-\ncedure, contraindications of the procedure, and common \ncomplications. The list of questions is shown in appendix \n1.\nBased on the case, question-answer pairs were \nextracted from prevalent plastic surgery junior trainee \ncase-preparation textbooks, namely Essentials of Plas -\ntic Surgery (Second Edition)  and Plastic & Reconstruc -\ntive Surgery Board Review (Third Edition)  [ 17, 18]. Two \nleading AI LLMs, Gemini and ChatGPT-4, were also uti -\nlized to generate responses to each question, verbatim \nfrom those listed in the two previously mentioned case-\npreparation textbooks. No priming questions or prompts \nwere employed in this study. Answers from these plat -\nforms were placed into surveys that were administered \nto reviewers of different surgical education levels. The \nLLMs question-answer pairs were compared directly to \nthe answers provided in the traditional case-preparation \ntextbooks.\nThe developed surveys were initially piloted amongst \nselected medical students, residents, and attendings. This \nstudy was granted IRB exempted status from the Univer -\nsity of South Alabama. After the pilot, the surveys were \ndistributed from October 1, 2023, to July 1, 2024, across \nmultiple institutions to attendings, residents, research \nfellows, and medical students practicing or interested in \nplastic surgery. For the purposes of this study, “research \nfellows” refers specifically to medical students completing \na dedicated research year between their third and fourth \nyear of medical school, not post-graduate trainees in sur -\ngical residency or fellowship programs. Informed con -\nsent was obtained from all participants prior to enrolling \nin the study. Respondents assessed the textbook and \nAI responses using a 5-point Likert scale with 0 being \nthe lowest/worst score and 5 being the highest/greatest \nscore. Question-answer pairs were scored across the fol -\nlowing categories: accuracy, thoroughness, usefulness \nfor case preparation, brevity, and overall quality. Statis -\ntical analyses, including one-way ANOVA and unpaired \nT-tests, were performed to compare the performance \nPage 3 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \nof LLMs versus textbook responses and evaluate the \nstrength of responses for each question type or plastic \nsurgery subspecialty. Statistical analysis was conducted in \nSTATA SE 16.0 with a p-value < 0.05 being designated as \nstatistical significance.\nResults\nA total of 90 surveys were administered. Each clinical \ncase had a total 15 responses. Table  1 shows the number \nof responses by surgical education levels.\nDue to the limited sample size of each type of trainee, \nthe authors were unable to stratify responses based on \ntrainee type.\nTextbook vs. LLMs\nCompared to textbook responses, LLMs were viewed as \nmore thorough ( p < 0.001) and less concise ( p < 0.001). \nAdditionally, textbooks responses were seen as superior \nin providing information on contraindications ( p = 0.027) \nand complications (p = 0.014) compared to LLMs. Table 2 \nshows the ANOVA outputs comparing LLMs and text -\nbook views.\nTextbook vs. ChatGPT\nWhen comparing textbook answers strictly to Chat -\nGPT outputs, ChatGPT responses were again viewed \nas being more thorough ( p < 0.001) and less concise \n(p < 0.001). When stratifying by clinical case, there were \nno significant differences between textbook and Chat -\nGPT responses. By question type, textbooks responses \nwere seen as superior compared to ChatGPT ( p = 0.036). \nTable 3 shows the unpaired T-Test outputs when com -\nparing textbook and ChatGPT answers.\nTextbook vs. Gemini\nWhen comparing textbook answers to Gemini responses, \nGemini responses were viewed as being less accurate \n(p = 0.043), more thorough ( p = 0.001), less useful in \ncase preparation ( p = 0.041), and less concise ( p < 0.001). \nWhen stratifying by clinical case, Gemini was perceived \nas being inferior in quality when answering questions \nrelated to burns (p = 0.017). When stratifying by question \ntype, Gemini was also perceived as being inferior when \nproviding answers related to anatomy ( p = 0.013), con-\ntraindications ( p < 0.001), and complications ( p < 0.001). \nTable 4 shows the unpaired T-Test outputs when com -\nparing textbook and Gemini answers.\nChatGPT vs. Gemini\nWhen comparing ChatGPT directly to Gemini, ChatGPT \nis perceived as being more accurate ( p = 0.018), more \nthorough ( p = 0.002), more useful in case preparation \n(p = 0.026), and better in overall quality ( p = 0.03). When \nstratifying by clinical case, ChatGPT was perceived as \nhigher quality for cases within aesthetics ( p = 0.034). \nChatGPT was also perceived as being superior when \nanswering questions about anatomy ( p = 0.016) and con-\ntraindications ( p < 0.001). Table  5 shows the unpaired \nT-Test outputs when comparing ChatGPT and Gemini \nanswers.\nDiscussion\nThe findings of this study underscore both the potential \nbenefits and challenges of integrating LLMs into medical \neducation and clinical practice. The perceived thorough -\nness of LLM responses suggests that these models can \nserve as valuable tools for in-depth learning. However, \nthe trade-off with brevity indicates a significant area for \nimprovement, particularly for use in time-sensitive envi -\nronments. This may negate the often-discussed benefit \nof a faster, compact, mobile resource in LLMs. While the \nthoroughness of LLM responses is commendable, practi -\ncal application in clinical settings often requires concise \nand directly actionable information. The comprehensive \ndetail provided by LLMs, while beneficial for educational \npurposes, can be overwhelming when quick decision-\nmaking is necessary. This suggests that LLMs need to be \nrefined to strike a balance between providing detailed \ninformation and maintaining brevity to ensure they can \nbe effective in both educational and clinical contexts.\nThe study also revealed differences in perceived accu -\nracy and utility between ChatGPT and Gemini, with \nGemini generally receiving lower ratings. These dispari -\nties highlight a critical area for improvement, particularly \nin ensuring the reliability of LLM-generated informa -\ntion. Future iterations of these models should focus on \nenhancing the accuracy and reliability of their responses, \npotentially through more rigorous training and validation \nprocesses that incorporate a broader range of medical \ndata and scenarios.\nStratified analysis by clinical scenario further illumi -\nnates the variability in LLM performance. For instance, \nGemini’s perceived lower quality in addressing burn-\nrelated questions compared to textbook responses sug -\ngests that both platforms may be trained on different \nTable 1 Frequency of responses by surgical education level by clinical case. GAS = Gender affirming surgery\nCraniofacial GAS Microsurgery Hand Burn Aesthetic\nMedical Students 3 4 6 4 3 3\nResearch Fellows 3 3 3 3 3 3\nResidents 5 4 2 4 5 5\nAttendings 4 4 4 4 4 4\nPage 4 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \ndatasets and may lack uniform access to sub-specialty \nknowledge. This is also exemplified by the perception \nthat Gemini’s responses to aesthetic-related questions are \ninferior to those from ChatGPT. Ensuring consistency \nand reliability across all domains of medical knowledge \nis essential for these models to be trusted and widely \nadopted by healthcare professionals and trainees.\nImportantly, this variability is not unique to plastic sur-\ngery. Similar concerns about accuracy, specialty-specific \nknowledge, and clinical applicability have been reported \nin other surgical specialties. For instance, in orthopedic \nsurgery, LLMs like ChatGPT have demonstrated mixed \nperformance, with acceptable explanations for straight -\nforward topics but notable inaccuracies in more complex \nor nuanced clinical questions [ 19, 20]. In neurosurgery, \nwhile ChatGPT has been praised for its educational \npotential, it has also been shown to produce factual inac -\ncuracies and lack subspecialty depth in procedural con -\ntexts [21]. These parallels suggest a broader trend: while \nLLMs have general utility across surgical domains, their \nperformance may fall short when detailed specialty-spe -\ncific knowledge is required.\nThese findings are also echoed in several non-surgical \nspecialties, suggesting that some challenges and benefits \nof LLM integration may be broadly translatable across \nclinical disciplines. In internal medicine, for instance, \nLLMs have demonstrated strong performance on stan -\ndardized exams such as the USMLE but have been criti -\ncized for producing hallucinations and lacking clinical \nnuance in complex patient scenarios (Kung et al., 2023) \nTable 2 ANOVA results comparing views of LLM and textbook responses stratified by view of answer characteristic, clinical case, and \nquestion type. LLM = large Language models. GAS = Gender affirming surgery\nAnswer Characteristics\nMean Standard Deviation F-Ratio P-Value\nAccuracy Textbook 4.314 0.287 1.163 0.297\nLLM 4.109 0.416\nThoroughness Textbook 3.092 0.328 37.158 < 0.001*\nLLM 4.139 0.350\nUtility in Case Preparation Textbook 3.794 0.274 1.238 0.282\nLLM 3.597 0.385\nConcise Textbook 4.648 0.227 183.987 < 0.001*\nLLM 3.044 0.241\nQuality Textbook 3.876 0.262 0.606 0.448\nLLM 3.726 0.429\nClinical Case\nMean Standard Deviation F-Ratio P-Value\nCraniofacial Textbook 4.087 0.630 0.050 0.827\nLLM 4.023 0.459\nGAS Textbook 3.570 0.539 0.258 0.620\nLLM 3.427 0.504\nMicrosurgery Textbook 3.871 0.585 0.384 0.546\nLLM 3.696 0.481\nHand Textbook 4.070 0.585 0.541 0.475\nLLM 3.860 0.484\nBurn Textbook 4.283 0.448 2.569 0.133\nLLM 4.448 0.531\nAesthetics Textbook 3.787 0.766 0.647 0.436\nLLM 3.490 0.629\nQuestion Type\nMean Standard Deviation F-Ratio P-Value\nAnatomy Textbook 4.190 0.567 2.668 0.106\nLLM 3.848 0.894\nIndications Textbook 3.869 0.652 0.196 0.659\nLLM 3.936 0.625\nContraindications Textbook 3.927 0.713 5.094 0.027*\nLLM 3.563 0.723\nComplications Textbook 3.888 0.666 6.291 0.014*\nLLM 3.506 0.576\n*Statistically significant value\nPage 5 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \n[22]. In dermatology, studies have found that while Chat -\nGPT can provide detailed information about common \nconditions, it lacks the diagnostic specificity required for \nmore complex or rare skin diseases, reinforcing the need \nfor human oversight [ 23]. Similarly, in psychiatry, LLMs \nhave been explored for their potential in therapeutic dia -\nlogue generation and patient education, but concerns \nremain about their appropriateness, tone, and potential \nto reinforce harmful biases [ 24]. These parallels across \nnon-surgical domains support the idea that while LLMs \ncan augment education and preliminary clinical decision-\nmaking, domain-specific fine-tuning and careful integra -\ntion remain essential for their safe and effective use.\nUser perception and trust are paramount in the adop -\ntion of LLMs in clinical practice. The study indicates that \nChatGPT is perceived as more accurate and useful than \nGemini, suggesting a higher level of trust in its responses. \nBuilding and maintaining this trust involves not only \nimproving the technical performance of these models but \nalso ensuring transparency in how LLMs source, process, \nand deliver information. Communicating their limita -\ntions and design clearly can help users better understand \nwhen and how to rely on their outputs.\nThe comprehensive nature of LLM responses can be a \nsignificant asset in medical education. These models can \nprovide students with a rich source of information, facili -\ntating a deeper understanding of complex medical con -\ncepts. However, educators should guide students on how \nto critically appraise and integrate LLM outputs with \ntrusted resources. Encouraging thoughtful engagement \nTable 3 Unpaired T-Test outputs comparing textbook answers to ChatGPT outputs stratified by answer characteristics, clinical case, \nand question type. GAS = Gender affirming surgery\nAnswer Characteristics\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAccuracy Textbook 4.314 0.287 0.687 0.415 -0.386, 0.265 0.146 0.687\nChatGPT 4.370 0.214\nThoroughness Textbook 3.092 0.328 < 0.001 8.709 -1.649, -0.977 0.151 < 0.001*\nChatGPT 4.405 0.170\nUtility in Case Preparation Textbook 3.794 0.274 0.815 0.24 -0.382, 0.307 0.155 0.815\nChatGPT 3.830 0.261\nConcise Textbook 4.648 0.227 < 0.001 11.061 1.282, 1.928 0.145 < 0.001*\nChatGPT 3.043 0.273\nQuality Textbook 3.876 0.262 0.524 0.661 -0.467, 0.254 0.162 0.524\nChatGPT 3.980 0.298\nClinical Case\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nCraniofacial Textbook 4.087 0.630 0.825 0.2291 -0.885, 0.725 0.349 0.825\nChatGPT 4.170 0.463\nGAS Textbook 3.570 0.539 0.795 0.2686 -0.895, 0.708 0.347 0.795\nChatGPT 3.660 0.559\nMicrosurgery Textbook 3.871 0.585 0.575 0.584 -0.663, 1.113 0.385 0.575\nChatGPT 3.650 0.631\nHand Textbook 4.070 0.585 1 0 -0.822, 0.822 0.357 1\nChatGPT 4.070 0.543\nBurn Textbook 4.283 0.448 0.644 0.480 -0.596, 0.909 0.326 0.644\nChatGPT 4.127 0.576\nAesthetics Textbook 3.787 0.766 0.815 0.2414 -1.0903, 0.884 0.428 0.815\nChatGPT 3.890 0.574\nQuestion Type\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAnatomy Textbook 4.190 0.567 0.892 0.136 -0.328, 0.287 0.154 0.892\nChatGPT 4.211 0.715\nIndications Textbook 3.869 0.652 0.100 1.668 -0.577, 0.052 0.158 0.100\nChatGPT 4.131 0.667\nContraindications Textbook 3.924 0.718 0.975 0.031 -0.358, 0.369 0.182 0.975\nChatGPT 3.918 0.803\nComplications Textbook 3.888 0.666 0.036 2.141 0.023, 0.659 0.159 0.036*\nChatGPT 3.547 0.666\n*Statistically significant value\nPage 6 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \nwith LLMs—rather than passive consumption—will be \nessential for fostering analytical skills among trainees.\nMoreover, the specific strengths and weaknesses of \nChatGPT and Gemini in different clinical scenarios \nsuggest potential for targeted optimization. ChatGPT’s \nstrength in aesthetic-related questions and Gemini’s rela-\ntive weakness in burn-related questions indicate that \nfine-tuning LLMs to specific medical domains could \nenhance their relevance and reliability. This specializa -\ntion may mirror the trajectory of LLMs in other surgical \nfields, where custom training on subspecialty datasets \nhas been proposed as a way to overcome generalization \nlimitations [25, 26]. \nDespite the valuable insights provided by this study, \nseveral limitations must be acknowledged. First, the small \nsample size limited our ability to explore differences \nin perceptions across training levels—such as between \nmedical students, research fellows, residents, and attend -\nings. This limitation has important implications, as indi -\nviduals at different stages of training may possess varying \nlevels of clinical knowledge and familiarity with surgical \ndecision-making. Medical students, in particular, may \nnot yet have the depth of clinical experience to critically \nevaluate the accuracy or clinical relevance of LLM-gener-\nated responses. As a result, their assessments may differ \nfrom those of more advanced trainees or practicing sur -\ngeons, potentially introducing bias into perceived ratings \nof accuracy, utility, or quality.\nSurvey fatigue may have also affected the reliability of \nresponses, as the survey’s length and cognitive demand \nTable 4 Unpaired T-Test outputs comparing textbook answers to gemini outputs stratified by answer characteristics, clinical case, and \nquestion type. GAS = Gender affirming surgery\nAnswer Characteristics\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAccuracy Textbook 4.314 0.287 0.043 2.319 0.018, 0.923 0.203 0.043*\nGemini 3.843 0.406\nThoroughness Textbook 3.092 0.328 0.001 4.527 -1.164, -0.396 0.172 0.001*\nGemini 3.872 0.266\nUtility in Case Preparation Textbook 3.794 0.274 0.041 2.351 0.023, 0.839 0.183 0.041*\nGemini 3.363 0.355\nConcise Textbook 4.648 0.227 < 0.001 12.164 1.310, 1.900 0.132 < 0.001*\nGemini 3.044 0.230\nQuality Textbook 3.876 0.262 0.063 2.089 -0.027, 0.840 0.195 0.063\nGemini 3.469 0.400\nClinical Case\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nCraniofacial Textbook 4.087 0.630 0.569 0.594 -0.595, 1.009 0.348 0.569\nGemini 3.880 0.456\nGAS Textbook 3.570 0.539 0.221 1.326 -0.281, 1.041 0.287 0.221\nGemini 3.190 0.346\nMicrosurgery Textbook 3.871 0.585 0.691 0.412 -0.574, 0.824 0.303 0.691\nGemini 3.746 0.343\nHand Textbook 4.070 0.585 0.209 1.366 -0.287, 1.120 0.305 0.209\nGemini 3.653 0.352\nBurn Textbook 4.283 0.448 0.017 3.018 0.173, 1.294 0.243 0.017*\nGemini 3.550 0.308\nAesthetics Textbook 3.787 0.766 0.109 1.805 -0.193, 1.587 0.386 0.109\nGemini 3.090 0.399\nQuestion Type\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAnatomy Textbook 4.190 0.567 0.013 2.553 0.105, 0.859 0.189 0.013*\nGemini 3.709 0.962\nIndications Textbook 3.869 0.652 0.970 0.038 -0.303, 0.292 0.149 0.970\nGemini 3.874 0.595\nContraindications Textbook 3.924 0.718 < 0.001 4.394 0.360, 0.958 0.150 < 0.001*\nGemini 3.265 0.521\nComplications Textbook 3.888 0.666 < 0.001 3.566 0.213, 0.755 0.136 < 0.001*\nGemini 3.404 0.448\n*Statistically significant value\nPage 7 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \nmay have decreased participant engagement over time. \nAdditionally, the subjective nature of Likert-scale ratings \nand the possibility of individual biases toward or against \ntechnology introduce variability, as user perceptions may \nnot always align with objective measures of LLM per -\nformance. The limited scope of clinical scenarios and \nquestion types examined further restricts the generaliz -\nability of our findings, as the full breadth and complexity \nof plastic surgery practice were not fully captured.\nAddressing these limitations through larger and more \ndiverse samples, minimizing survey fatigue, incorporat -\ning objective performance assessments, and expanding \nthe clinical breadth of the questionnaire will be essen -\ntial in future work. In addition, we propose the use of \nprompt priming—for example, instructing the LLM to \nrespond as a plastic surgery expert communicating with \na medical colleague—to potentially improve the quality \nand relevance of generated responses. Further research is \nneeded to evaluate whether such strategies enhance the \neducational and clinical utility of LLMs for subspecialty \ntraining.\nConclusion\nWhile ChatGPT and Gemini offer promising capabilities \nin generating thorough and detailed responses, there are \nsignificant areas for improvement, particularly in con -\nciseness, accuracy, and utility for practical case prepara -\ntion. ChatGPT generally outperforms Gemini, suggesting \nthat different LLMs may have varying strengths and \nweaknesses. Future developments should aim to enhance \nTable 5 Unpaired T-Test outputs comparing ChatGPT answers to Gemini outputs stratified by answer characteristics, clinical case, and \nquestion type. GAS = Gender Affirming Surgery\nAnswer Characteristics\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAccuracy ChatGPT 4.370 0.214 0.018 2.835 0.114, 0.949 0.187 0.018*\nGemini 3.843 0.406\nThoroughness ChatGPT 4.405 0.170 0.002 4.134 0.245, 0.820 0.129 0.002*\nGemini 3.872 0.266\nUtility in Case Preparation ChatGPT 3.830 0.261 0.026 2.601 0.067, 0.869 0.180 0.026*\nGemini 3.363 0.355\nConcise ChatGPT 3.043 0.273 0.995 0.007 -0.326, 0.324 0.146 0.995\nGemini 3.044 0.230\nQuality ChatGPT 3.980 0.298 0.030 2.529 0.061, 0.966 0.203 0.030*\nGemini 3.469 0.400\nClinical Case\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nCraniofacial ChatGPT 4.170 0.463 0.353 0.987 -0.383, 0.957 0.291 0.353\nGemini 3.880 0.456\nGAS ChatGPT 3.660 0.559 0.146 1.610 -0.205, 1.151 0.294 0.146\nGemini 3.190 0.346\nMicrosurgery ChatGPT 3.650 0.631 0.764 0.311 -0.841, 0.641 0.321 0.764\nGemini 3.746 0.343\nHand ChatGPT 4.070 0.543 0.188 1.44 -0.250, 1.084 0.289 0.188\nGemini 3.653 0.352\nBurn ChatGPT 4.127 0.576 0.084 1.974 -0.097, 1.250 0.292 0.084\nGemini 3.550 0.308\nAesthetics ChatGPT 3.890 0.574 0.034 2.56 0.079, 1.521 0.313 0.034*\nGemini 3.090 0.399\nQuestion Type\nMean Standard Deviation Two-Tailed T-Value 95% Confidence Interval Standard Error P-Value\nAnatomy ChatGPT 4.211 0.715 0.016 2.482 0.099, 0.907 0.203 0.016*\nGemini 3.709 0.962\nIndications ChatGPT 4.131 0.667 0.093 1.702 -0.044, 0.559 0.151 0.093\nGemini 3.874 0.595\nContraindications ChatGPT 3.918 0.803 < 0.001 4.036 0.330, 0.976 0.162 < 0.001*\nGemini 3.265 0.521\nComplications ChatGPT 3.547 0.666 0.296 1.052 -0.128, 0.414 0.136 0.296\nGemini 3.404 0.448\n*Statistically significant value\nPage 8 of 8\nHinson et al. BMC Medical Education          (2025) 25:984 \nthe balance between detail and brevity, improve accuracy, \nand ensure consistent performance across different clini -\ncal scenarios and question types. These advancements \nwill be critical in establishing LLMs as reliable and effec -\ntive tools in both medical education and clinical practice. \nAs these models evolve, their integration into healthcare \nsettings holds the potential to transform medical educa -\ntion and support healthcare professionals in delivering \nhigh-quality patient care.\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  1 2 9 0 9 - 0 2 5 - 0 7 5 5 0 - 8.\nSupplementary Material 1\nAcknowledgements\nNot applicable.\nAuthor contributions\nCH: Study Design, Data Collection, Data Analysis, Manuscript; CS: Study \nDesign, Data Collection, Data Analysis, Manuscript; RN: Study Design, Data \nAnalysis, Manuscript.\nFunding\nThe study received no financial support from any external funding source. No \nauthors were supported by any grants or funding sources.\nData availability\nThe datasets used and/or analyzed during the current study are available from \nthe corresponding author on reasonable request.\nDeclarations\nEthics approval and consent to participate\nIRB approval (expedited) was granted from the University of South Alabama. \nInformed consent was collected from each participant prior to partaking in \nthe study.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nClinical trial number\nNot applicable.\nReceived: 11 September 2024 / Accepted: 16 June 2025\nReferences\n1. Amazon Web Solutions. What are large language models? - LLM AI explained \n- AWS. Amazon Web Services, Inc. Accessed July 21. 2024.  h t t p  s : /  / a w s  . a  m a z  o \nn .  c o m /  w h  a t -  i s /  l a r g  e -  l a n g u a g e - m o d e l /\n2. OpenAi. OpenAI - ChatGPT. 2024. Accessed July 21, 2024.  h t t p s : / / o p e n a i . c o m \n/ r e s e a r c h /       \n3. Google. Gemini. Gemini. 2024. Accessed July 21, 2024.  h t t p s : / / g e m i n i . g o o g l e . \nc o m       \n4. Kirubarajan A, Young D, Khan S, Crasto N, Sobel M, Sussman D. Artificial intel-\nligence and surgical education: a systematic scoping review of interventions. \nJ Surg Educ. 2022;79(2):500–15.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . j s  u r g  . 2 0 2  1 .  0 9 . 0 1 2\n5. Guerrero DT, Asaad M, Rajesh A, Hassan A, Butler CE. Advancing surgical \neducation: the use of artificial intelligence in surgical training. Am Surgeon™. \n2023;89(1):49–54.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 7 7  / 0  0 0 3 1 3 4 8 2 2 1 1 0 1 5 0 3\n6. Bilgic E, Gorgy A, Yang A, et al. Exploring the roles of artificial intelligence in \nsurgical education: a scoping review. Am J Surg. 2022;224(1, Part A):205–16.  h \nt t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / j  . a m  j s u  r g . 2  0 2  1 . 1 1 . 0 2 3\n7. Satapathy P , Hermis AH, Rustagi S, Pradhan KB, Padhi BK, Sah R. Artificial \nintelligence in surgical education and training: opportunities, challenges, and \nethical considerations– correspondence. Int J Surg. 2023;109(5):1543.  h t t p  s : /  / \nd o i  . o  r g /  1 0 .  1 0 9 7  / J  S 9 .  0 0 0  0 0 0 0  0 0  0 0 0 0 3 8 7\n8. Vedula SS, Ghazi A, Collins JW, et al. Artificial intelligence methods and arti-\nficial intelligence-enabled metrics for surgical education: a multidisciplinary \nconsensus. J Am Coll Surg. 2022;234(6):1181.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 7  / X  C S .  0 0 0  \n0 0 0 0  0 0  0 0 0 0 1 9 0\n9. Tan S, Xin X, Wu D. ChatGPT in medicine: prospects and challenges: a review \narticle. Int J Surg. 2024;110(6):3701.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 9 7  / J  S 9 .  0 0 0  0 0 0 0  0 0  0 0 \n0 1 3 1 2\n10. Gradon KT. Generative artificial intelligence and medical disinformation. BMJ. \n2024;384:q579.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 3 6  / b  m j . q 5 7 9\n11. Nguyen T. ChatGPT in medical education: a precursor for automation bias?? \nJMIR Med Educ. 2024;10(1):e50174.  h t t p  s : /  / d o i  . o  r g /  1 0 .  2 1 9 6  / 5  0 1 7 4\n12. Haltaufderheide J, Ranisch R. The ethics of ChatGPT in medicine and health-\ncare: a systematic review on large language models (LLMs). Npj Digit Med. \n2024;7(1):1–11.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 1 7 4 6 - 0 2 4 - 0 1 1 5 7 - x\n13. Abid A, Farooqi M, Zou J. Large language models associate Muslims with \nviolence. Nat Mach Intell. 2021;3(6):461–3.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 3 8  / s  4 2 2 5 6 - 0 2 \n1 - 0 0 3 5 9 - 2\n14. Yeung JA, Kraljevic Z, Luintel A et al. AI chatbots not yet ready for clinical use. \nPublished online March 20, 2023:2023.03.02.23286705.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 0 \n1  / 2  0 2 3  . 0 3  . 0 2 .  2 3  2 8 6 7 0 5\n15. Zack T, Lehman E, Suzgun M, et al. Assessing the potential of GPT-4 to \nperpetuate Racial and gender biases in health care: a model evaluation study. \nLancet Digit Health. 2024;6(1):e12–22.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 0 1 6  / S  2 5 8 9 - 7 5 0 0 ( 2 3 \n) 0 0 2 2 5 - X\n16. Liu Z, Zhang L, Wu Z, et al. Surviving ChatGPT in healthcare. Front Radiol. \n2024;3:1224682.  h t t p  s : /  / d o i  . o  r g /  1 0 .  3 3 8 9  / f  r a d i . 2 0 2 3 . 1 2 2 4 6 8 2\n17. Janis JE, editor. Essentials of plastic surgery. 2nd ed. CRC; 2015.  h t t p  s : /  / d o i  . o  r g \n/  1 0 .  1 2 0 1  / b  1 6 6 1 0\n18. Lin SJ, Hijjawi JB. Plastic and reconstructive surgery board review: pearls of \nwisdom, third edition. 3rd edition. McGraw Hill / Medical; 2016.\n19. Kung JE, Marshall C, Gauthier C, Gonzalez TA, Jackson JB. Evaluating ChatGPT \nperformance on the orthopaedic in-training examination. JB JS Open Access. \n2023;8(3):e23.00056.\n20. Jain N, Gottlich C, Fisher J, Campano D, Winston T. Assessing chatgpt’s ortho-\npedic in-service training exam performance and applicability in the field. J \nOrthop Surg Res. 2024;19(1):27.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 8 6  / s  1 3 0 1 8 - 0 2 3 - 0 4 4 6 7 - 0\n21. Ali R, Tang OY, Connolly ID, et al. Performance of ChatGPT and GPT-4 on \nneurosurgery written board examinations. Neurosurgery. 2023;93(6):1353–65.  \nh t t p  s : /  / d o i  . o  r g /  1 0 .  1 2 2 7  / n  e u .  0 0 0  0 0 0 0  0 0  0 0 0 2 6 3 2\n22. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on \nUSMLE: potential for AI-assisted medical education using large language \nmodels. PLOS Digit Health. 2023;2(2):e0000198.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 3 7 1  / j  o u r  n \na l  . p d i  g .  0 0 0 0 1 9 8\n23. Goktas P , Grzybowski A. Assessing the impact of ChatGPT in dermatology: a \ncomprehensive rapid review. J Clin Med. 2024;13(19):5909.  h t t p  s : /  / d o i  . o  r g /  1 0 .  \n3 3 9 0  / j  c m 1 3 1 9 5 9 0 9\n24. Cheng S, Chang C, Chang W, et al. The now and future of ChatGPT and GPT in \npsychiatry. Psychiatry Clin Neurosci. 2023;77(11):592–6.  h t t p  s : /  / d o i  . o  r g /  1 0 .  1 1 1 \n1  / p  c n . 1 3 5 8 8\n25. Khalpey Z, Kumar U, King N, Abraham A, Khalpey AH. Large language models \ntake on cardiothoracic surgery: a comparative analysis of the performance of \nfour models on American Board of Thoracic Surgery Exam Questions in 2023. \nCureus. 16(7):e65083.  h t t p  s : /  / d o i  . o  r g /  1 0 .  7 7 5 9  / c  u r e u s . 6 5 0 8 3\n26. Long C, Subburam D, Lowe K et al. ChatENT: augmented large language \nmodels for expert knowledge retrieval in otolaryngology - head and neck \nsurgery. Published online August 21, 2023:2023.08.18.23294283.  h t t p s :   /  / d o  i .  o \nr  g  /  1 0  . 1 1   0 1  / 2   0 2 3   . 0  8  . 1  8 . 2 3 2 9 4 2 8 3\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}