{
  "title": "MKD: a Multi-Task Knowledge Distillation Approach for Pretrained Language Models",
  "url": "https://openalex.org/W3021805648",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1031846015",
      "name": "Liu Linqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1379294413",
      "name": "Wang Huan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751801181",
      "name": "Xiong, Caiming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2237537322",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2951815760",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2162456950",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2091432990",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2937297214",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2952603081",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W2072566913",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2507974895",
    "https://openalex.org/W3104240813",
    "https://openalex.org/W2295072214",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3127787589",
    "https://openalex.org/W2552027021",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Pretrained language models have led to significant performance gains in many NLP tasks. However, the intensive computing resources to train such models remain an issue. Knowledge distillation alleviates this problem by learning a light-weight student model. So far the distillation approaches are all task-specific. In this paper, we explore knowledge distillation under the multi-task learning setting. The student is jointly distilled across different tasks. It acquires more general representation capacity through multi-tasking distillation and can be further fine-tuned to improve the model in the target domain. Unlike other BERT distillation methods which specifically designed for Transformer-based architectures, we provide a general learning framework. Our approach is model agnostic and can be easily applied on different future teacher model architectures. We evaluate our approach on a Transformer-based and LSTM based student model. Compared to a strong, similarly LSTM-based approach, we achieve better quality under the same computational constraints. Compared to the present state of the art, we reach comparable results with much faster inference speed.",
  "full_text": "MKD: a Multi-Task Knowledge Distillation Approach\nfor Pretrained Language Models\nLinqing Liu,∗1 Huan Wang,2 Jimmy Lin,1 Richard Socher,2 and Caiming Xiong2\n1 David R. Cheriton School of Computer Science, University of Waterloo\n2 Salesforce Research\n{linqing.liu, jimmylin}@uwaterloo.ca, {huan.wang, rsocher, cxiong}@salesforce.com\nAbstract\nPretrained language models have led to signif-\nicant performance gains in many NLP tasks.\nHowever, the intensive computing resources to\ntrain such models remain an issue. Knowledge\ndistillation alleviates this problem by learning\na light-weight student model. So far the dis-\ntillation approaches are all task-speciﬁc. In\nthis paper, we explore knowledge distillation\nunder the multi-task learning setting. The stu-\ndent is jointly distilled across different tasks.\nIt acquires more general representation capac-\nity through multi-tasking distillation and can\nbe further ﬁne-tuned to improve the model in\nthe target domain. Unlike other BERT distilla-\ntion methods which speciﬁcally designed for\nTransformer-based architectures, we provide\na general learning framework. Our approach\nis model agnostic and can be easily applied\non different future teacher model architectures.\nWe evaluate our approach on a Transformer-\nbased and LSTM based student model. Com-\npared to a strong, similarly LSTM-based ap-\nproach, we achieve better quality under the\nsame computational constraints. Compared to\nthe present state of the art, we reach compara-\nble results with much faster inference speed.\n1 Introduction\nPretrained language models learn highly effective\nlanguage representations from large-scale unla-\nbeled data. A few prominent examples include\nELMo (Peters et al., 2018), BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019c), and XL-\nNet (Yang et al., 2019), all of which have achieved\nstate of the art in many natural language process-\ning (NLP) tasks, such as natural language infer-\nence, sentiment classiﬁcation, and semantic textual\nsimilarity. However, such models use dozens, if\nnot hundreds, of millions of parameters, invariably\n∗All work was done while the ﬁrst author was a research\nintern at Salesforce Research.\nTask 2\nTask N\nTask 1\n…\nTeacher 1\nTeacher 2\nTeacher N\n…\nShared\nStudent \nNetwork\nLayers\nTeacher\nTask 2\nTask N\nTask 1\n…\nFigure 1: The left ﬁgure represents task-speciﬁc KD.\nThe distillation process needs to be performed for each\ndifferent task. The right ﬁgure represents our proposed\nmulti-task KD. The student model consists of shared\nlayers and task-speciﬁc layers.\nleading to resource-intensive inference. The con-\nsensus is that we need to cut down the model size\nand reduce the computational cost while maintain-\ning comparable quality.\nOne approach to address this problem is knowl-\nedge distillation (KD; Ba and Caruana, 2014; Hin-\nton et al., 2015), where a large model functions as\na teacher and transfers its knowledge to a small\nstudent model. Previous methods focus on task-\nspeciﬁc KD, which transfers knowledge from a\nsingle-task teacher to its single-task student. Put\nit another way, the knowledge distillation process\nneeds to be conducted all over again when perform-\ning on a new NLP task. The inference speed of the\nlarge-scale teacher model remains the bottleneck\nfor various downstream tasks distillation.\nOur goal is to ﬁnd a distill-once-ﬁts-many so-\nlution. In this paper, we explore the knowledge\ndistillation method under the setting of multi-task\nlearning (MTL; Caruana, 1997; Baxter, 2000). We\npropose to distill the student model from different\ntasks jointly. The overall framework is illustrated\nin Figure 1. The reason is twofold: ﬁrst, the dis-\ntilled model learns a more universal language rep-\nresentation by leveraging cross-task data. Second,\nthe student model achieves both comparable qual-\nity and fast inference speed across multiple tasks.\narXiv:1911.03588v2  [cs.CL]  30 Apr 2020\nMTL is based on the idea (Maurer et al., 2016) that\ntasks are related by means of a common low dimen-\nsional representation. We also provide an intuitive\nexplanation on why using shared structure could\npossibly help by assuming some connections over\nthe conditional distribution of different tasks.\nWe evaluate our approach on two different stu-\ndent model architectures. One uses three layers\nTransformers (Vaswani et al., 2017), since most of\nthe KD works (Sun et al., 2019; Jiao et al., 2019)\nuse Transformers as their students. Another is\nLSTM based network with bi-attention mechanism.\nPreviously Tang et al. (2019) examine the represen-\ntation capacity of a simple, single-layer Bi-LSTM\nonly, so we are interested in whether adding more\nprevious effective modules, such as an attention\nmechanism, will further improve its effectiveness.\nIt exempliﬁes that our approach is model agnostic,\ni.e., the choice of student model does not depend on\nthe teacher model architecture; The teacher model\ncan be easily switched to other powerful language\nmodels other than BERT.\nWe further study several important problems in\nknowledge distillation, such as the choice of mod-\nules in student model, the inﬂuence of different\ntokenization methods, and the inﬂuence of MTL in\nKD. We evaluate our approach on seven datasets\nacross four different tasks. For LSTM based stu-\ndent, our approach keeps the advantage of infer-\nence speed while maintaining comparable perfor-\nmances as those speciﬁcally designed for Trans-\nformer methods. For our Transformer based stu-\ndent, it does provide a modest gain, and outper-\nforms other KD methods without using external\ntraining data.\n2 Related Work\nLanguage model pretraining. Given a sequence\nof tokens, pretrained language models encode\neach token as a general language representational\nembedding. A large body of literature has ex-\nplored this area. Traditional pretrained word rep-\nresentations (Turian et al., 2010) presume singu-\nlar word meanings and thus adapt poorly to mul-\ntiple contexts—for some notable examples, see\nword2vec (Mikolov et al., 2013), GloVe (Pen-\nnington et al., 2014), and FastText (Bojanowski\net al., 2017). For more ﬂexible word represen-\ntations, a few advancements exist: Neelakantan\net al. (2015) learn multiple embeddings per word\ntype; context2vec (Melamud et al., 2016) uses bidi-\nrectional LSTM to encode contexts around target\nwords; CoVe (McCann et al., 2017) trains LSTM\nencoders on some machine translation datasets,\nshowing that these encoders are well-transferable\nto other tasks. Prominently, ELMo (Peters et al.,\n2018) learns deep word representations using a bidi-\nrectional language model. It can be easily added\nto an existing model and boost performance across\nsix challenging NLP tasks.\nFine-tuning approaches are mostly employed\nin more recent work. They pretrain the language\nmodel on a large-scale unlabeled corpus and then\nﬁne-tune it with in-domain labeled data for a super-\nvised downstream task (Dai and Le, 2015; Howard\nand Ruder, 2018). BERT (Devlin et al., 2019),\nGPT (Radford et al., 2018) and GPT-2 (Radford\net al.) are some of the prominent examples. Fol-\nlowing BERT, XLNet (Yang et al., 2019) proposes\na generalized autoregressive pretraining method\nand RoBERTa (Liu et al., 2019c) optimizes BERT\npretraining approach. These pretrained models are\nlarge in size and contain millions of parameters.\nWe target the BERT model and aim to address this\nproblem through knowledge distillation. Our ap-\nproach can be easily applied to other models as\nwell.\nKnowledge distillation. Knowledge distillation\n(Ba and Caruana, 2014; Hinton et al., 2015) trans-\nfers knowledge from a large teacher model to a\nsmaller student model. Since the distillation only\nmatches the output distribution, the student model\narchitecture can be completely different from that\nof the teacher model. There are already many ef-\nforts trying to distill BERT into smaller models.\nBERT-PKD (Sun et al., 2019) extracts knowledge\nnot only from the last layer of the teacher, but also\nfrom previous layers. TinyBERT (Jiao et al., 2019)\nintroduces a two-stage learning framework which\nperforms transformer distillation at both pretrain-\ning and task-speciﬁc stages. Zhao et al. (2019)\ntrain a student model with smaller vocabulary and\nlower hidden states dimensions. DistilBERT (Sanh\net al., 2019) reduces the layers of BERT and uses\nthis small version of BERT as its student model.\nAll the aforementioned distillation methods are per-\nformed on a single task, speciﬁcally designed for\nthe transformer-based teacher architecture, result-\ning in poor generalizability to other type of mod-\nels. Our objective is to invent a general distillation\nframework, applicable to either transformer-based\nmodels or other architectures as well. Tang et al.\n(2019) distill BERT into a single-layer BiLSTM. In\nour paper, we hope to extract more knowledge from\nBERT through multi-task learning, while keeping\nthe student model simple.\nMulti-task learning. Multi-task learning (MTL)\nhas been successfully applied on different appli-\ncations (Collobert and Weston, 2008; Deng et al.,\n2013; Girshick, 2015). MTL helps the pretrained\nlanguage models learn more generalized text rep-\nresentation by sharing the domain-speciﬁc infor-\nmation contained in each related task training sig-\nnal (Caruana, 1997). Liu et al. (2019b, 2015) pro-\npose a multi-task deep neural network (MT-DNN)\nfor learning representations across multiple tasks.\n(Clark et al., 2019) propose to use knowledge distil-\nlation so that single task models can teach a multi-\ntask model. Liu et al. (2019a) train an ensemble of\nlarge DNNs and then distill their knowledge to a\nsingle DNN via multi-task learning to ensemble its\nteacher performance.\n3 Model Architecture\nIn this section, we introduce the teacher model\nand student model for our distillation approach.\nWe explored two different student architectures: a\ntraditional bidirectional long short-term memory\nnetwork (BiLSTM) with bi-attention mechanism\nin 3.2, and the popular Transformer in 3.3.\n3.1 Multi-Task Reﬁned Teacher Model\nWe argue that multi-task learning can leverage the\nregularization of different natural language under-\nstanding tasks. Under this setting, language models\ncan be more effective in learning universal lan-\nguage representations. To this end, we consider the\nbidirectional transformer language model (BERT;\nDevlin et al., 2019) as bottom shared text encoding\nlayers, and ﬁne-tune the task-speciﬁc top layers\nfor each type of NLU task. There are mainly two\nstages for the training procedure: pretraining the\nshared layer and multi-task reﬁning.\nShared layer pretraining. Following Devlin et al.\n(2019), the input token is ﬁrst encoded as the\nthe summation of its corresponding token embed-\ndings, segmentation embeddings and position em-\nbeddings. The input embeddings are then mapped\ninto contextual embeddings C through a multi-\nlayer bidirectional transformer encoder. The pre-\ntraining of these shared layers use the cloze task\nand next sentence prediction task. We use the pre-\ntrained BERTLARGE to initialize these shared lay-\ners.\nMulti-task reﬁning. The contextual embeddings\nCare then passed through the upper task-speciﬁc\nlayers. Following Liu et al. (2019b), our cur-\nrent NLU training tasks on GLUE (Wang et al.,\n2018) can be classiﬁed into four categories: single-\nsentence classiﬁcation (CoLA and SST-2), pairwise\ntext classiﬁcation (RTE, MNLI, WNLI, QQP, and\nMRPC), pairwise text similarity (STS-B), and rele-\nvance ranking (QNLI). Each category corresponds\nto its own output layer.\nHere we take the text similarity task as an ex-\nample to demonstrate the implementation details.\nFollowing Devlin et al. (2019), we consider the\ncontextual embedding of the special [CLS] token\nas the semantic representation of the input sentence\npair (X1,X2). The similarity score can be pre-\ndicted by the similarity ranking layer:\nSim(X1,X2) =W⊤\nSTSx (1)\nwhere WSTS is a task-speciﬁc learnable weight\nvector and x is the contextual embedding of the\n[CLS] token.\nIn the multi-task reﬁning stage, all the model pa-\nrameters, including bottom shared layers and task-\nspeciﬁc layers, are updated through mini-batch\nstochastic gradient descent (Li et al., 2014). The\ntraining data are packed into mini-batches and each\nmini-batch only contains samples from one task.\nRunning all the mini-batches in each epoch ap-\nproximately optimizes the sum all of all multi-task\nobjectives. In each epoch, the model is updated\naccording to the selected mini-batch and its task-\nspeciﬁc objective. We still take the text similarity\ntask as an example, where each pair of sentences\nis labeled with a real-value similarity score y. We\nuse the mean-squared error loss as our objective\nfunction:\n∥y−Sim(X1,X2)∥2\n2 (2)\nFor text classiﬁcation task, we use the cross-\nentropy loss as the objective function. For rel-\nevance ranking task, we minimize the negative\nlog likelihood of the positive examples (Liu et al.,\n2019b). We can also easily add other tasks by\nadding its own task-speciﬁc layer.\n3.2 LSTM-based Student Model\nWe’re interested in exploring whether a simple ar-\nchitecture, such as LSTM, has enough represen-\ntation capacity to transfer knowledge from the\nInput  #1\nMLP\n                X                 Y\nMax; Min; Mean; Self-attn Max; Min; Mean; Self-attn\nFC Layer\nBiattention\nIntegrate\nPool\nMLP\nInput  #2\nShared\nLayers\nBilSTM\nFC Layer FC LayerTask\nSpecific\nLayers\nTask 1 Task 2 Task n...\nstack stack\nFigure 2: Architecture for the bi-attentive student neu-\nral network.\nteacher model. We also incorporate bi-attention\nmodule since it’s widely used between pairs of\nsentences (Peters et al., 2018; Wang et al., 2018).\nAnd the inputs in our experiments are mostly two\nsentences. Our LSTM-based bi-attentive student\nmodel is depicted in Figure 2. For equation rep-\nresentations, the embedding vectors of input se-\nquences are denoted as wx and wy. For single-\nsentence input tasks, wy is the same as wx. ⊕\nrepresents vectors concatenation.\nwx and wy are ﬁrst converted into ˆwx and ˆwy\nthrough a feedforward network with ReLU activa-\ntion (Nair and Hinton, 2010) function. For each\ntoken in ˆwx and ˆwy, we then use a bi-directional\nLSTM encoder to compute its hidden states and\nstack them over time axis to form matrices X and\nY separately.\nNext, we apply the biattention mechanism\n(Xiong et al., 2016; Seo et al., 2016) to compute\nthe attention contexts A = XY⊤of the input se-\nquences. The attention weight Ax and Ay is ex-\ntracted through a column-wise normalization for\neach sequence. The context vectors Cx and Cy for\neach token is computed as the multiplication of its\ncorresponding representation and attention weight:\nAx = softmax(A) Cx = A⊤\nxX (3)\nSame as (McCann et al., 2017), we concatenate\nthree different computations between original rep-\nresentations and context vector to reinforce their\nrelationships. The concatenated vectors are then\npassed through one single-layer BiLSTM:\nXy = BiLSTM([X⊕X−Cy ⊕X⊙Cy])\nYx = BiLSTM([Y ⊕Y −Cx ⊕Y ⊙Cx]) (4)\nThe pooling operations are then applied on the out-\nputs of BiLSTM. We use max, mean, and self-\nattentive pooling to extract features. These three\npooled representations are then concatenated to get\none context representation. We feed this context\nrepresentation through a fully-connected layer to\nget ﬁnal output.\n3.3 Transformer-based Student Model\nMost of the pre-trained language models, which\ncan be employed as teachers, are built with Trans-\nformers. Transformer (Vaswani et al., 2017) now is\nan ubiquitous model architecture. It draws global\ndependencies between input and output entirely re-\nlying on an self-attention mechanism. Our student\nmodel uses three layers of Transformers. Same\nas BERT (Devlin et al., 2019), [CLS] is added in\nfront of every input example, and [SEP] is added\nbetween two input sentences. We use the average\n[CLS] representation from each layer as the ﬁnal\noutput.\n4 Multi-Task Distillation\nThe parameters of student models, introduced in\nSection 3.2 and 3.3, are shared across all tasks.\nEach task has its individual layer on top of it. We\nbegin by describing the task-speciﬁc layers: for\neach task, the hidden representations are ﬁrst fed\nto a fully connected layer with rectiﬁed linear units\n(ReLU), whose outputs are passed to another linear\ntransformation to get logitsz= Wh. During multi-\ntask training, the parameters from both the bottom\nstudent network and upper task-speciﬁc layers are\njointly updated.\nConsidering one text classiﬁcation problem, de-\nnoted as task t, a softmax layer will perform the\nfollowing operations on the ith dimension of zto\nget the predicted probability for the ith class:\nsoftmax(zt\ni) = exp{zt\ni}∑\nj exp{zt\nj} (5)\nAccording to Ba and Caruana (2014), training\nthe student network on logits will make learning\neasier. There might be information loss from trans-\nferring logits into probability space, so it follows\nthat the teacher model’s logits provides more in-\nformation about the internal model behaviour than\nits predicted one-hot labels. Then, our distillation\nobjective is to minimize the mean-squared error\n(MSE) between the student network logits zt\nS and\nthe teacher’s logitszt\nT:\nLt\ndistill = ∥zt\nT −zt\nS∥2\n2 (6)\nThe training samples are selected from each\ndataset and packed into task-speciﬁc batches. For\ntask t, we denote the current selected batch as bt.\nFor each epoch, the model running through all the\nbatches equals to attending over all the tasks:\nLdistill = L1\ndistill + L2\ndistill + ...+ Lt\ndistill (7)\nDuring training, the teacher model ﬁrst uses the\npretrained BERT model (Devlin et al., 2019) to\ninitialize its parameters of shared layers. It then\nfollows the multi-task reﬁning procedure described\nin Section 3.1 to update both the bottom shared-\nlayers and upper task-speciﬁc layers.\nFor student model, the shared parameters are ran-\ndomly initialized. During training, for each batch,\nthe teacher model ﬁrst predicts teacher logits. The\nstudent model then updates both the bottom shared\nlayer and the upper task-speciﬁc layers according\nto the teacher logits. The complete procedure is\nsummarized in Algorithm 1.\nAlgorithm 1 Multi-task Distillation\nInitialize the shared layers with BERTLarge then\nmulti-task reﬁne the teacher model\nRandomly initialize the student model parame-\nters\nSet the max number of epoch: epochmax\n// Pack the data for T Tasks into batches\nfor t←1 to T do\n1. Generate augmented data: taug\n2. Pack the dataset tand taug into batch Dt\nend for\n// Train the student model\nfor epoch←1 to epochmax do\n1. Merge all datasets:\nD= D1 ∪D2 ... ∪DT\n2. Shufﬂe D\nfor bt in D do\n3. Predict logits zT from teacher model\n4. Predict logits zS from student model\n5. Compute loss Ldistill(θ)\n6. Update student model:\nθ= θ−α∇θLdistill\nend for\nend for\n5 An Intuitive Explanation\nIn this section we give an intuitive explanation on\nwhy using some shared structure during the multi-\ntask training could possibly help. Suppose the sam-\nples of the task T are independent and identically\ndistributed xT,yT ∼PT\nXY, where xT, yT are the\nfeature and labels of the samples in task T respec-\ntively. The joint density can be decomposed as\npT(x,y) = pT(x)pT(y|x). During the discrimi-\nnative learning process, one tries to estimate the\nconditional distributionpT(·|x). For different tasks,\npT(·|X) could be very different. Indeed if there is\nno connections in pT(·|X) for different tasks, then\nit is hard to believe training on one task may help\nanother. However if we assume some smoothness\nover pT(·|X), then some connections can be built\nacross tasks.\nWithout loss of generality, we investigate the\ncase of two tasks. For task T1 and T2, let’s assume\nthere exist some common domain of representa-\ntions H, and two functions: hT1 (x),hT2 (x) :X↦→\nH, such that\npT1 (·|x) =gT1 ◦hT1 (x), (8)\npT2 (·|x) =gT2 ◦hT2 (x), (9)\n∀x1,x2, ∥hT1 (x1) −hT2 (x2)∥≤ η∥x1 −x2∥,\n(10)\nwhere gT : H↦→Y T is a function that maps from\nthe common domain Hto the task labels YT for\ntask T, ◦denotes function composition, and ηis a\nsmoothness constant.\nThe Lipschitz-ish inequality (10) suggests the\nhidden representation hT1 on task T1 may help the\nestimation of hT2 , since hT2 (x2) will be close to\nhT1 (x1) if x1 and x2 are close enough. This is\nimplicitly captured if we use one common network\nto model both hT1 and hT2 since the neural network\nwith ReLU activation is Lipschitz.\n6 Experimental Setup\n6.1 Datasets\nWe conduct the experiments on seven most widely\nused datasets in the General Language Understand-\ning Evaluation (GLUE) benchmark (Wang et al.,\n2018): one sentiment dataset SST-2 (Socher et al.,\n2013), two paraphrase identiﬁcation datasets QQP\n1 and MRPC (Dolan and Brockett, 2005), one text\nsimilarity dataset STS-B (Cer et al., 2017), and\nthree natural language inference datasets MNLI\n(Williams et al., 2018), QNLI (Rajpurkar et al.,\n2016) and RTE. For the QNLI dataset, version 1\nexpired on January 30, 2019; the result is evaluated\non QNLI version 2.\n1https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\n6.2 Implementation Details\nWe use the released MT-DNN model 2 to initial-\nize our teacher model. We further reﬁne the\nmodel against the multi-task learning objective for\n1 epoch with learning rate set to 5e-4. The per-\nformance of our reﬁned MT-DNN is lower than\nreported results in Liu et al. (2019b).\nThe LSTM based student model (MKD-LSTM)\nis initialized randomly. For multi-task distillation,\nWe use the Adam optimizer (Kingma and Ba, 2014)\nwith learning rates of 5e-4. The batch size is set\nto 128, and the maximum epoch is 16. We clip the\ngradient norm within 1 to avoid gradient exploding.\nThe number of BiLSTM hidden units in student\nmodel are all set to 256. The output feature size of\ntask-speciﬁc linear layers is 512. The Transformer-\nbased student model (MKD-Transformer) consists\nof three layers of Transformers. Following the\nsettings of BERT-PKD, it is initialized with the\nﬁrst three layers parameters from pre-trained BERT-\nbase.\nWe also ﬁne-tune the multi-task distilled student\nmodel for each task. During ﬁne-tuning, the param-\neters of both shared layers and upper task-speciﬁc\nlayers are updated. The learning rate is chosen\nfrom {1,1.5,5}×10−5 according to the validation\nset loss on each task. Other parameters remain\nthe same as above. For both teacher and student\nmodels, we use WordPiece embeddings (Wu et al.,\n2016) with a 30522 token vocabulary.\nData augmentation. The training data for typi-\ncal natural language understanding tasks is usually\nvery limited. Larger amounts of data are desirable\nfor the teacher model to fully express its knowl-\nedge. Tang et al. (2019) proposes two methods for\ntext data augmentation: masking and POS-guided\nword replacement. We employ the only ﬁrst mask-\ning technique which randomly replaces a word in\nthe sentence with [MASK], because, as shown in\nboth Tang et al. (2019) and our own experiments,\nPOS-guided word replacement does not lead to\nconsistent improvements in quality across most of\nthe tasks. Following their strategies, for each word\nin a sentence, we perform masking with probability\npmask = 0.1. We use the combination of original\ncorpus and augmentation data in distillation pro-\ncedure. For smaller datasets STS-B, MRPC and\nRTE, the size of the augmented dataset is 40 times\nthe sizes of the original corpus; 10 times for other\nlarger datasets.\n2https://github.com/namisan/mt-dnn\n6.3 Methods and Baselines\nResults on test data reported by the ofﬁcial GLUE\nevaluation server are summarized in Table 1. Each\nentry in the table is brieﬂy introduced below:\nMTL-BERT.We use the multi-task reﬁned BERT\n(described in Section 3.1) as our teacher model.\nWe tried to replicate the results of the released MT-\nDNN (Liu et al., 2019b) model.\nOpenAI GPT. A generative pre-trained\nTransformer-based language model (Radford et al.,\n2018). In contrast to BERT, GPT is auto-regressive,\nonly trained to encode uni-directional context.\nELMo. Peters et al. (2018) learns word represen-\ntations from the concatenation of independently\ntrained left-to-right and right-to-left LSTMs. We\nreport the results of a BiLSTM-based model with\nbi-attention baseline (Wang et al., 2018) trained on\ntop of ELMo.\nDistilled BiLSTM. Tang et al. (2019) distill BERT\ninto a simple BiLSTM. They use different models\nfor single and pair sentences tasks.\nBERT-PKD.The Patient-KD-Skip approach (Sun\net al., 2019) which student model patiently learns\nfrom multiple intermediate layers of the teacher\nmodel. We use their student model consisting of\nthree layers of Transformers.\nTinyBERT Jiao et al. (2019) propose a knowl-\nedge distillation method specially designed for\ntransformer-based models. It requires a general dis-\ntillation step which is performed on a large-scale\nEnglish Wikipedia (2,500 M words) corpus.\nBERTEXTREME. Zhao et al. (2019) aims to train a\nstudent model with smaller vocabulary and lower\nhidden state dimensions. Similar to BERT-PKD,\nthey use the same training corpus to train BERT to\nperform KD.\n7 Result and Discussions\nThe results of our model are listed as MKD-LSTM\nand MKD-Transformer in the tables.\n7.1 Model Quality Analysis\nComparison with GPT / ELMo. Our model has\nbetter or comparable performance compared with\nELMo and OpenAI GPT. MKD-LSTM has higher\nperformance than ELMo over all seven datasets:\nnotably 8.4 points for RTE, 8.6 points in Spear-\nman’s ρ for STS-B, 7.6 points in F-1 measure\nfor QQP, and 0.6 to 5.6 points higher for other\nModel Size\n# Param\nSST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE\nAcc F 1/Acc r/ρ F1/Acc Acc Acc Acc\nMTL-BERT (Teacher) 303.9M 94.7 84.7/79.7 84.0/83.3 72.3/89.6 85.9/85.7 90.5 77.7\nOpenAI GPT 116.5M 91.3 82.3/75.7 82.0/80.0 70.3/88.5 82.1/81.4 - 56.0\nELMo 93.6M 90.4 84.4/78.0 74.2/72.3 63.1/84.3 74.1/74.5 79.8 58.9\nDistilled BiLSTM 1.59M 91.6 82.7/75.6 79.6/78.2 68.5/88.4 72.5/72.4 - -\nBERT-PKD 21.3M 87.5 80.7/72.5 - 68.1/87.8 76.7/76.3 84.7 58.2\nTinyBERT 5.0M 92.6 86.4/81.2 81.2/79.9 71.3/89.2 82.5/81.8 87.7 62.9\nBERTEXTREME 19.2M 88.4 84.9/78.5 - - 78.2/77.7 -\nMKD-LSTM 10.2M 91.0 85.4/79.7 80.9/80.9 70.7/88.6 78.6/78.4 85.4 67.3\nMKD-Transformer 21.3M 90.1 86.2/79.8 81.5/81.5 71.1/89.4 79.2/78.5 83.5 67.0\nTable 1: Results from the GLUE test server. The ﬁrst group contains large-scale pretrained language models.\nThe second group lists previous knowledge distillation methods for BERT. Our MKD results based on LSTM and\nTransformer student model architectures are listed in the last group. The number of parameters doesn’t include\nembedding layer.\n# Model SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE\n1 Biatt LSTM 85.8 80.4/69.9 12.24/11.33 81.1/86.5 73.0/73.7 80.3 53.1\n2 Single Task Distilled Biatt LSTM 89.2 82.5/72.1 20.2/20.0 84.6/88.4 74.7/75.0 82.0 52.0\n3 BiLSTMMTL 87.5 83.2/72.8 71.6/72.6 81.6/87.0 70.2/71.3 75.4 56.3\n4 MKD-LSTMWord-level Tokenizer 87.3 84.2/75.7 72.2/72.6 71.1/79.3 69.4/70.9 75.1 54.9\n5 MKD-LSTM 89.3 86.8/81.1 84.5/84.5 85.2/89.0 78.4/79.2 83.0 67.9\nTable 2: Ablation studies on GLUE dev set of different training procedures. All models are not ﬁne-tuned. Line\n1 is our bi-attentive LSTM student model trained without distillation. Line 2 is our bi-attentive LSTM student\ndistilled from single task. Line 3 is the Multi-task distilled BiLSTM. Line 4 is the Multi-task distilled model using\nword-level tokenizer.\ndatasets. Compared with OpenAI GPT, MKD-\nLSTM is 11.3 points higher for RTE and 4 points\nhigher for MRPC.\nComparison with Distilled BiLSTM / BERT-\nPKD. While using the same Transformer layers\nand same amount of parameters, MKD-Tranformer\nsigniﬁcantly outperforms BERT-PKD by a range\nof 0.4 ∼9.1 points. MKD-LSTM leads to sig-\nniﬁcant performance gains than BERT-PKD while\nusing far less parameters, and compensate for the\neffectiveness loss of Distlled BiLSTM.\nComparison with TinyBERT / BERT EXTREME.\nThese two approaches both use the large-scale un-\nsupervised text corpus, same as the ones to train the\nteacher model, to execute their distillation process.\nHowever, we only use the data within downstream\ntasks. There are two caveats for their methods: (1)\nDue to massive training data, KD still requires in-\ntensive computing resources, e.g. BERTEXTREME\ntakes 4 days on 32 TPU cores to train their stu-\ndent model. (2) The text corpus to train the teacher\nmodel is not always available due to data privacy.\nUnder some conditions we can only access to the\npretrained models and their approach are not appli-\ncable.\nWhile not resorting to external training data, our\nmodel has the best performance across the state-\nof-the-art KD baselines (i.e., BERT-PKD). It also\nachieves comparable performance compared to in-\ntensively trained KD methods (i.e, BERTEXTREME)\non external large corpus.\n7.2 Ablation Study\nWe conduct ablation studies to investigate the con-\ntributions of: (1) the different training procedures\n(in Table 2); (2) Different training tasks in multi-\ntask distillation (in Table 3). We also compare\nthe inference speed of our models and previous\ndistillation approach. (in Table 4). The ablation\nstudies are all conducted on LSTM-based student\nmodel since it has the advantage of model size and\ninference speed compared to Transformers.\nModel SST-2 MRPC STS-B QQP MNLI-m/mm QNLI RTE\nSentiment Task ✓\nMKD-LSTM 89.9 81.4/70.8 51.2/49.9 84.9/88.3 74.3/74.7 83.2 50.9\nPI Tasks ✓ ✓\nMKD-LSTM 89.3 85.2/77.2 83.4/83.3 84.9/88.7 73.2/73.9 83.8 59.6\nNLI Tasks ✓ ✓ ✓\nMKD-LSTM 90.4 87.9/82.1 84.1/84.1 84.8/88.4 77.1/78.1 84.5 66.8\nAll Tasks ✓ ✓ ✓ ✓ ✓ ✓ ✓\nMKD-LSTM 90.5 86.9/80.2 85.0/84.8 84.8/89.0 77.4/78.3 84.9 68.2\nTable 3: Ablation experiments on the dev set use different training tasks in multi-task distillation. The results are\nreported with the original corpus, without augmentation data. The model is ﬁne-tuned on each individual task.\nDo we need attention in the student model?Yes.\nTang et al. (2019) distill BERT into a simple BiL-\nSTM network. Results in Table 1 demonstrates\nthat our model is better than Distilled BiLSTM and\nachieves an improvement range of2.2 ∼6.1 points\nacross six datasets. To make fair comparison, we\nalso list the results of multi-task distilled BiLSTM\nin Line 3 in Table 2. It’s obvious that Line 5, which\nis the model with bi-attentive mechanism, signif-\nicantly outperform Line 3. We surmise that the\nattention module is an integral part of the student\nmodel for sequence modeling.\nBetter vocabulary choices? WordPiece works\nbetter than the word-level tokenizers in our experi-\nments. The WordPiece-tokenized vocabulary size\nis 30522, while the word-level tokenized vocabu-\nlary size is much larger, along with more unknown\ntokens. WordPiece effectively reduces the vocab-\nulary size and improves rare-word handling. The\ncomparison between Line 4 and Line 5 in Table 2\ndemonstrates that the method of tokenization inﬂu-\nences all the tasks.\nThe inﬂuence of MTL in KD? The single-task\ndistilled results are represented in Line 2 of Ta-\nble 2. Compared with Line 5, all the tasks beneﬁt\nfrom information sharing through multi-task distil-\nlation. Especially for STS-B, the only regression\ntask, greatly beneﬁt from the joint learning from\nother classiﬁcation tasks.\nWe also illustrate the inﬂuence of different num-\nber of tasks for training. In Table 3, the training\nset incorporates tasks of the same type individually.\nEven for the tasks which are in the training sets,\nthey still perform better in the all tasks training\nsetting. For example, for RTE, the All Tasks setting\nincreases 1.4 points than NLI Tasks setting. For\nother training settings which RTE is excluded from\ntraining set, All Tasks leads to better performance.\nDistilled BiLSTM BERT-PKD TinyBERT MKD-LSTM\nInf. Time 1.36 8.41 3.68 2.93\nTable 4: The inference time (in seconds) for baselines\nand our model. The inference is performed on QNLI\ntraining set and on a single NVIDIA V100 GPU.\n7.3 Inference Efﬁciency\nTo test the model efﬁciency, we ran the experiments\non QNLI training set. We perform the inference\non a single NVIDIA V100 GPU with batch size\nof 128, maximum sequence length of 128. The\nreported inference time is the total running time of\n100 batches.\nFrom Table 4, the inference time for our model\nis 2.93s. We re-implemented Distilled BiLSTM\nfrom Tang et al. (2019) and their inference time\nis 1.36s. For fair comparison, we also ran infer-\nence procedure using the released BERT-PKD and\nTinyBERT model on the same machine. Our model\nsigniﬁcantly outperforms Distilled BiLSTM with\nsame magnitude speed. It also achieves compara-\nble results but is faster in efﬁciency compared with\nother distillation models.\n8 Conclusion\nIn this paper, we propose a general framework for\nmulti-task knowledge distillation. The student is\njointly distilled across different tasks from a multi-\ntask reﬁned BERT model (teacher model). We\nevaluate our approach on Transformer-based and\nLSTM-based student model. Compared with previ-\nous KD methods using only data within tasks, our\napproach achieves better performance. In contrast\nto other KD methods using large-scale external text\ncorpus, our approach balances the problem of com-\nputational resources, inference speed, performance\ngains and availability of training data.\nReferences\nJimmy Ba and Rich Caruana. 2014. Do deep nets really\nneed to be deep? In Advances in neural information\nprocessing systems, pages 2654–2662.\nJonathan Baxter. 2000. A model of inductive bias\nlearning. Journal of artiﬁcial intelligence research,\n12:149–198.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5(1):135–146.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D Manning, and Quoc V Le.\n2019. Bam! born-again multi-task networks for\nnatural language understanding. arXiv preprint\narXiv:1907.04829.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th international conference on Ma-\nchine learning, pages 160–167. ACM.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nLi Deng, Geoffrey Hinton, and Brian Kingsbury. 2013.\nNew types of deep neural network learning for\nspeech recognition and related applications: An\noverview. In 2013 IEEE International Conference\non Acoustics, Speech and Signal Processing , pages\n8599–8603. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nRoss Girshick. 2015. Fast r-cnn. In Proceedings of the\nIEEE international conference on computer vision ,\npages 1440–1448.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMu Li, Tong Zhang, Yuqiang Chen, and Alexander J\nSmola. 2014. Efﬁcient mini-batch training for\nstochastic optimization. In Proceedings of the 20th\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining , pages 661–670.\nACM.\nXiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,\nKevin Duh, and Ye-Yi Wang. 2015. Representation\nlearning using multi-task deep neural networks for\nsemantic classiﬁcation and information retrieval. In\nProceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 912–921.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019a. Improving multi-task deep\nneural networks via knowledge distillation for\nnatural language understanding. arXiv preprint\narXiv:1904.09482.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint\narXiv:1901.11504.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndreas Maurer, Massimiliano Pontil, and Bernardino\nRomera-Paredes. 2016. The beneﬁt of multitask rep-\nresentation learning. The Journal of Machine Learn-\ning Research, 17(1):2853–2884.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional lstm. In Proceedings of\nthe 20th SIGNLL conference on computational natu-\nral language learning, pages 51–61.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nVinod Nair and Geoffrey E Hinton. 2010. Rectiﬁed\nlinear units improve restricted boltzmann machines.\nIn Proceedings of the 27th international conference\non machine learning (ICML-10), pages 807–814.\nArvind Neelakantan, Jeevan Shankar, Alexandre Pas-\nsos, and Andrew McCallum. 2015. Efﬁcient\nnon-parametric estimation of multiple embeddings\nper word in vector space. arXiv preprint\narXiv:1504.06654.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2016. Bidirectional attention\nﬂow for machine comprehension. arXiv preprint\narXiv:1611.01603.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th annual meeting of the association for compu-\ntational linguistics, pages 384–394. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nCaiming Xiong, Victor Zhong, and Richard Socher.\n2016. Dynamic coattention networks for question\nanswering. arXiv preprint arXiv:1611.01604.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019. Extreme language model compres-\nsion with optimal subwords and shared projections.\narXiv preprint arXiv:1909.11687.",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7610334157943726
    },
    {
      "name": "Distillation",
      "score": 0.7135988473892212
    },
    {
      "name": "Computer science",
      "score": 0.642339825630188
    },
    {
      "name": "Natural language processing",
      "score": 0.4865361750125885
    },
    {
      "name": "Language model",
      "score": 0.43299010396003723
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39290428161621094
    },
    {
      "name": "Chemistry",
      "score": 0.1440087854862213
    },
    {
      "name": "Engineering",
      "score": 0.13362523913383484
    },
    {
      "name": "Chromatography",
      "score": 0.09591516852378845
    },
    {
      "name": "Systems engineering",
      "score": 0.08899852633476257
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ],
  "cited_by": 19
}