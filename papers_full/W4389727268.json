{
  "title": "Mathematical discoveries from program search with large language models",
  "url": "https://openalex.org/W4389727268",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5088601308",
      "name": "Bernardino Romera‐Paredes",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5078666530",
      "name": "Mohammadamin Barekatain",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5101733098",
      "name": "Alexander Novikov",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5077302579",
      "name": "Matej Balog",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5018728658",
      "name": "Manish Kumar",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5082290231",
      "name": "Emilien Dupont",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5101992030",
      "name": "Francisco J. R. Ruiz",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5071941381",
      "name": "Jordan S. Ellenberg",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A5028269821",
      "name": "Pengming Wang",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5023562540",
      "name": "Omar Fawzi",
      "affiliations": [
        "Institut national de recherche en sciences et technologies du numérique",
        "Laboratoire de l'Informatique du Parallélisme",
        "Université Claude Bernard Lyon 1",
        "École Normale Supérieure de Lyon"
      ]
    },
    {
      "id": "https://openalex.org/A5013834379",
      "name": "Pushmeet Kohli",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5057393538",
      "name": "Alhussein Fawzi",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4388139328",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W6800166007",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4366735548",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W6811216112",
    "https://openalex.org/W6849739958",
    "https://openalex.org/W2040492000",
    "https://openalex.org/W4321855065",
    "https://openalex.org/W4402270921",
    "https://openalex.org/W4400413686",
    "https://openalex.org/W2895170202",
    "https://openalex.org/W2912209775",
    "https://openalex.org/W4252732074",
    "https://openalex.org/W173368591",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W2559529531",
    "https://openalex.org/W2963631653",
    "https://openalex.org/W2963589591",
    "https://openalex.org/W1607996835",
    "https://openalex.org/W1513028911",
    "https://openalex.org/W1535858642",
    "https://openalex.org/W1979722653",
    "https://openalex.org/W4297899343",
    "https://openalex.org/W1519941054",
    "https://openalex.org/W2048628062",
    "https://openalex.org/W2016393589",
    "https://openalex.org/W2078659331",
    "https://openalex.org/W4247440730",
    "https://openalex.org/W90213227",
    "https://openalex.org/W2093188170",
    "https://openalex.org/W4285601124",
    "https://openalex.org/W1969678783",
    "https://openalex.org/W2582697902",
    "https://openalex.org/W4213350211",
    "https://openalex.org/W2030792885",
    "https://openalex.org/W4226243662",
    "https://openalex.org/W4281491203",
    "https://openalex.org/W2026723619",
    "https://openalex.org/W2462548332",
    "https://openalex.org/W2072473332",
    "https://openalex.org/W2117336417",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W6801858553",
    "https://openalex.org/W3215529831",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4312050693",
    "https://openalex.org/W4321277276",
    "https://openalex.org/W4226098701",
    "https://openalex.org/W1979769287",
    "https://openalex.org/W6849565481",
    "https://openalex.org/W2132304387",
    "https://openalex.org/W2072441740",
    "https://openalex.org/W6668783100",
    "https://openalex.org/W1820078070",
    "https://openalex.org/W2137654273",
    "https://openalex.org/W2090103383",
    "https://openalex.org/W2171341281",
    "https://openalex.org/W4299968636",
    "https://openalex.org/W6802007172",
    "https://openalex.org/W4302010773",
    "https://openalex.org/W4379739787",
    "https://openalex.org/W3171967876",
    "https://openalex.org/W4200124985"
  ],
  "abstract": null,
  "full_text": "Mathematical discoveries from program \nsearch with large language models\nBernardino R om er a- Pa re de s, M oh am ma damin Barekatain, Alexander Novikov, Matej Balog, \nM. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, \nOmar Fawzi, Pushmeet Kohli & Alhussein Fawzi\nThis is a PDF file of a peer-reviewed paper that has been accepted for publication. \nAlthough unedited, the content has been subjected to preliminary formatting. Nature \nis providing this early version of the typeset paper as a service to our authors and \nreaders. The text and figures will undergo copyediting and a proof review before the \npaper is published in its final form. Please note that during the production process \nerrors may be discovered which could affect the content, and all legal disclaimers \napply.\nReceived: 12 August 2023\nAccepted: 30 November 2023\nAccelerated Article Preview \nPublished online xx xx xxxx\nCite this article as: Romera-Paredes, B. et al. \nMathematical discoveries from program \nsearch with large language models. Nature  \nhttps://doi.org/10.1038/s41586-023-06924-6 \n(2023)\nhttps://doi.org/10.1038/s41586-023-06924-6\nNature | www.nature.com\nAccelerated Article Preview\nACCELERATED  \nARTICLE  \nPREVIEW \nMathematical discoveries from program search with large1\nlanguage models2\nBernardino Romera-Paredes1∗ Mohammadamin Barekatain1∗\n3\nAlexander Novikov1∗ Matej Balog1∗ M. Pawan Kumar1∗\n4\nEmilien Dupont1∗ Francisco J. R. Ruiz1∗ Jordan S. Ellenberg2\n5\nPengming Wang1 Omar Fawzi3 Pushmeet Kohli1 Alhussein Fawzi1∗\n6\n1Google DeepMind, London, UK7\n2University of Wisconsin-Madison, Madison, Wisconsin, USA8\n3Universit´ e de Lyon (Inria, ENS Lyon, UCBL, LIP), Lyon, France9\nAbstract10\nLarge Language Models (LLMs) have demonstrated tremendous capabilities in solving com-11\nplex tasks, from quantitative reasoning to understanding natural language. However, LLMs12\nsometimes suffer from confabulations (or hallucinations) which can result in them making plau-13\nsible but incorrect statements (Bang et al., 2023; Borji, 2023). This hinders the use of current14\nlarge models in scientific discovery. Here we introduce FunSearch (short for searching in the15\nfunction space), an evolutionary procedure based on pairing a pre-trained LLM with a system-16\natic evaluator. We demonstrate the effectiveness of this approach to surpass the best known re-17\nsults in important problems, pushing the boundary of existing LLM-based approaches (Lehman18\net al., 2022). Applying FunSearch to a central problem in extremal combinatorics — the cap19\nset problem — we discover new constructions of large cap sets going beyond the best known20\nones, both in finite dimensional and asymptotic cases. This represents the first discoveries made21\nfor established open problems using LLMs. We showcase the generality of FunSearch by apply-22\ning it to an algorithmic problem, online bin packing, finding new heuristics that improve upon23\nwidely used baselines. In contrast to most computer search approaches, FunSearch searches for24\nprograms that describe how to solve a problem, rather than what the solution is. Beyond being25\nan effective and scalable strategy, discovered programs tend to be more interpretable than raw26\nsolutions, enabling feedback loops between domain experts and FunSearch, and the deployment27\nof such programs in real-world applications.28\nMany problems in mathematical sciences are “easy to evaluate,” despite being typically “hard to29\nsolve.” For example, in computer science, NP-complete optimization problems admit a polynomial-30\ntime evaluation procedure (measuring the quality of the solution), despite the widespread belief that31\nno polynomial-time algorithms to solve such problems exist. We focus in this paper on problems32\nadmitting an efficient evaluate function, which measures the quality of a candidate solution. Promi-33\nnent examples include the maximum independent set problem and maximum constraint satisfaction34\nproblems (such as finding the ground state energy of a Hamiltonian). Our goal is to generate a35\nsolve program, such that its outputs receive high scores from evaluate (when executed on inputs36\nof interest), and ultimately improve over the best known solutions.37\n∗Equal contributors.\n1\nACCELERATED ARTICLE PREVIEW\nWhile Large Language Models (LLMs) have recently seen dramatic improvements in their coding38\ncapabilities [5–9], with applications including debugging [10, 11], solving code competitions [12, 13]39\nand improving code performance [14], synthesizing solve programs for open problems requires find-40\ning new ideas that are verifiably correct. This is very hard for LLMs, as they tend to confabulate or41\nultimately fall short of going beyond existing results. To surpass the “nominal” capabilities of LLMs,42\nrecent works [3] have combined them with evolutionary algorithms [15, 16], leading to important43\nimprovements on diverse synthetic problems [17], searching for neural network architectures [18–20],44\nand solving puzzles [21]. Our proposed method, FunSearch, pushes the boundary of LLM-guided45\nevolutionary procedures to a new level: the discovery of new scientific results for established open46\nproblems, and the discovery of new algorithms. Surpassing state-of-the-art results on established47\nopen problems provides a clear indication that the discoveries are truly new, as opposed to being48\nretrieved from the LLM’s training data.49\nFunSearch (short for searching in the function space) combines a pre-trained (frozen) Large Lan-50\nguage Model, whose goal is to provide creative solutions, with an evaluator, which guards against51\nconfabulations and incorrect ideas. FunSearch iterates over these two components, evolving initial52\nlow-scoring programs into high-scoring ones discovering new knowledge. Key to the success of this53\nsimple procedure is a combination of multiple essential ingredients. First, we sample best performing54\nprograms and feed them back into prompts for the LLM to improve on; we refer to this as best-shot55\nprompting. Second, we start with a program in the form of a skeleton (containing boilerplate code56\nand potentially prior structure about the problem), and only evolve the part governing the critical57\nprogram logic. For example, by setting a greedy program skeleton, we evolve a priority function58\nused to make decisions at every step. Third, we maintain a large pool of diverse programs by using59\nan island-based evolutionary method that encourages exploration and avoids local optima. Finally,60\nleveraging the highly parallel nature of FunSearch, we scale it asynchronously, considerably broad-61\nening the scope of this approach to find new results, while keeping the overall cost of experiments62\nlow.63\nWe show the surprising effectiveness ofFunSearch on several use-cases. We consider a fundamen-64\ntal problem in extremal combinatorics, namely, the cap set problem [22,23]. FunSearchdemonstrates65\nthe existence of hitherto unknown constructions that go beyond existing ones, including the largest66\nimprovement in 20 years to the asymptotic lower bound. To the best of our knowledge, this shows67\nthe first scientific discovery — a new piece of verifiable knowledge about a notorious scientific prob-68\nlem — using an LLM. Using FunSearch, we also find new algorithms for the online bin packing69\nproblem that improve upon traditional ones on well-studied distributions of interest [24, 25], with70\npotential applications to improving job scheduling algorithms.71\nWhile most computer search techniques output directly what the solution is (e.g., a list of vectors72\nforming a cap set), FunSearch produces programs generating the solution. For structured problems,73\nsuch programs tend to be more interpretable — facilitating interactions with domain experts —74\nand concise — making it possible to scale to large instances — compared to a mere enumeration75\nof the solution. In addition, decision procedures (such as for bin packing) described by code in a76\nstandard programming language are crucially easier to deploy compared to other types of descriptions77\n(e.g., neural networks), which typically require specialized hardware and for which verifying design78\nspecifications is notoriously hard.79\n1 FunSearch80\nAn overview of FunSearch is shown in Figure 1, and its components are described in more detail81\nbelow. For more details and ablations showing the importance of each component, see Methods and82\n2\nACCELERATED ARTICLE PREVIEW\nAppendix A in Supplementary Information.83\nSpecification. The input to FunSearchis a specification of the problem in the form of anevaluate84\nfunction, which scores candidate solutions. In addition, we provide an initial program (which can85\nbe trivial) to evolve. While in principle these are the minimum requirements, we found that perfor-86\nmance tends to improve significantly if we write the initial solve program in the form of a skeleton87\n(containing boilerplate code and prior knowledge of the problem in the form of a program struc-88\nture), and only use FunSearch to evolve the critical part that governs its logic. Figure 2 (a) shows89\nan example where the skeleton takes the form of a simple greedy algorithm, and the crucial part to90\nevolve by FunSearch is the priority function that is used to make the greedy decision at every step.91\nThis delegates to FunSearch precisely the part that is usually the hardest to come up with. While92\na fixed skeleton may constrain the space of programs that can be discovered, we find it improves93\noverall results because it focuses the LLM resources on evolving the critical part only, instead of also94\nusing the LLM to recreate already known program structures (with more opportunities for mistakes95\nthat would render the entire program incorrect). If available, the user can optionally provide addi-96\ntional known information about the problem at hand, in the form of docstrings, relevant primitive97\nfunctions, or import packages, which FunSearch may use.98\nPre-trained LLM. The LLM is the creative core of FunSearch, in charge of coming up with99\nimprovements to the functions presented in the prompt and sending these for evaluation. Perhaps100\nsurprisingly, we obtain our results with a pre-trained model, i.e., without any fine-tuning on our101\nproblems. We use Codey, an LLM built on top of the PaLM2 model family [26], which has been102\nfinetuned on a large corpus of code and is publicly accessible through its API [27]. BecauseFunSearch103\nrelies on sampling from an LLM extensively, an important performance-defining tradeoff is between104\nthe quality of the samples and the inference speed of the LLM. In practice, we have chosen to work105\nwith a fast-inference model (rather than slower-inference, higher-quality), and the results in the106\npaper are obtained using a total number of samples on the order of 10 6. Beyond this tradeoff, we107\nhave empirically observed that the results obtained in this paper are not too sensitive to the exact108\nchoice of LLM, as long as it has been trained on a large enough corpus of code. See Appendix A in109\nSupplementary Information for a comparison to StarCoder [7], a state-of-the-art open-source LLM110\nfor code.111\nEvaluation. Programs generated by the LLM are evaluated and scored on a set of inputs. For112\nexample, in the cap set problem (Section 2.1) the inputs are the values of the dimensionality n113\nthat we are interested in, and in combinatorial optimization (Section 2.2), the inputs correspond114\nto different bin packing instances. The scores across different inputs are then combined into an115\noverall score of the program using an aggregation function, such as the mean. The scored programs116\nare then sent to the programs database. Programs that were incorrect (did not execute within the117\nimposed time and memory limits, or produced invalid outputs) are discarded, and the remaining118\nscored programs are then sent to the programs database.119\nPrograms database. The programs database keeps a population of correct programs, which are120\nthen sampled to create prompts. Preserving and encouraging diversity of programs in the database is121\ncrucial to enable exploration and avoid being stuck in local optima. To encourage diversity we adopt122\nan islands model, also known as multiple population and multiple-deme model [28, 29], a genetic123\nalgorithm approach. A number of islands, or subpopulations, are created and evolved independently.124\nTo sample from the program database, we first sample an island and then sample a program within125\n3\nACCELERATED ARTICLE PREVIEW\nthat island, favoring higher-scoring and shorter programs (see Methods for the exact mechanism).126\nCrucially, we let information flow between the islands by periodically discarding the programs in the127\nworst half of the islands (corresponding to the ones whose best individuals have the lowest scores).128\nWe replace the programs in those islands with a new population, initialized by cloning one of the129\nbest individuals from the surviving islands.130\nPrompt. New prompts are created by “best-shot prompting” from the programs database, and131\nare then fed to the LLM to generate a new program. We first sample k programs from a single island132\nin the programs database, according to the procedure described above. Sampled programs are then133\nsorted according to their score, and a version is assigned to each ( v0 for the lowest scoring program,134\nv1 for the second lowest scoring, etc.). These programs are then combined into a single prompt —135\nwith the version appended as a suffix to the function name; e.g., in the case of Figure 2 (a), this136\nwould be priority v0, priority v1, ... — and the header of the function we wish to generate137\n(e.g., priority vk) is added to the end of the prompt. In practice, we set k = 2, as two functions138\nlead to better results compared to just one, with diminishing returns beyond that. Constructing a139\nprompt by combining several programs (as opposed to only one) enables the LLM to spot patterns140\nacross the different programs and generalize those. Related approaches to prompt building have141\nbeen recently considered; e.g., [17], and were shown to perform well on different domains.142\nDistributed approach. We implement FunSearch as a distributed system that has three types143\nof workers: a programs database, samplers, and evaluators, which communicate asynchronously. The144\nprograms database stores and serves programs, samplers generate new functions using the pre-trained145\nLLM, while evaluators assess programs, as shown in Figure F.26 in Supplementary Information. In146\nthe example of Figure 2 (a), the programs database stores priority functions, samplers generate147\nnew implementations of priority, while evaluators score the proposals by executing the main func-148\ntion on user-specified inputs. Our distributed system offers several advantages: first, it naturally149\nleverages parallelism across different tasks, e.g., LLM sampling and evaluation are performed con-150\ncurrently. Second, it enables scaling to more than one sampler and evaluator, which would be a151\nvery limiting setup, considering that evaluation can take minutes for many problems of interest.152\nRunning evaluators in parallel considerably broadens the scope of this approach to such problems.153\nThe distributed setting enables running many evaluator nodes on inexpensive CPU hardware, while154\nfew samplers run on machines with accelerators for fast LLM inference; this keeps the overall cost155\nand energy usage of experiments low. In our experiments, we typically use 15 samplers and 150 CPU156\nevaluators (can be served on 5 CPU servers each running 32 evaluators in parallel). See Appendix157\nA in Supplementary Information for more details. Also, due to the randomness of LLM sampling158\nand of the evolutionary procedure, for some problems we run several experiments to get the best159\nreported results. See Methods and Appendix A.3 in Supplementary Information for a full statistical160\nanalysis.161\n2 Results162\nWe now describe some of the new discoveries made by FunSearch in two different fields: pure math-163\nematics and applied computer science. Additional discoveries on other problems (namely, corners164\nproblem and Shannon capacity of cycle graphs) are presented in Appendix B in Supplementary165\nInformation. Full discovered programs are available in Appendix C in Supplementary Information.166\n4\nACCELERATED ARTICLE PREVIEW\n2.1 Extremal combinatorics167\nWe apply FunSearch to two related problems in extremal combinatorics — a branch of mathematics168\nthat studies the maximal (or minimal) possible sizes of sets satisfying certain properties.169\nCap sets. The cap set problem [22], once described by Terence Tao as “perhaps my favourite open170\nquestion” [30], refers to the task of finding the largest possible set of vectors in Zn\n3 (known as a cap171\nset) such that no three vectors sum to zero. Geometrically, no three points of a cap set lie on a line172\n(see Figure 3 for an example with n = 2).173\nThe problem has drawn much interest for a variety of reasons. For one, it is an analogue of174\nthe classical number theory problem of finding large subsets of primes in which no three are in175\narithmetic progression. For another, it differs from many problems in combinatorics in that there176\nis no consensus among mathematicians regarding what the right answer should be. Finally, the177\nproblem serves as a model for the many other problems involving “three-way interactions.” For178\ninstance, progress towards improved upper bounds for the cap set problem [31, 32] immediately led179\nto a series of other combinatorial results, e.g., on the Erd¨ os-Radio sunflower problem [33].180\nThe exact size of the largest possible cap set in n dimensions is known only for n ≤ 6. A181\nbrute force approach is not practical as the search space quickly becomes enormous with growing182\nn, e.g., around 31600 for n = 8. Previous methods impose potentially suboptimal restrictions on the183\nsearch space [34, 35]. In contrast, we search the full space via an algorithm skeleton that utilises a184\nfunction priority : Zn\n3 → R. Intuitively, this function provides a priority with which each x ∈ Zn\n3185\nshould be included in the cap set. Our algorithm starts with an empty set and iteratively adds the186\nvector x ∈ Zn\n3 with the highest priority that does not violate the cap set constraint; see Figure 2187\n(a). Starting from a trivial constant function, we evolve the crucial priority component of our188\napproach to result in large cap sets.189\nUsing this approach we discovered cap sets of sizes shown in Figure 4 (a). Notably, in dimension190\nn = 8, FunSearch found a larger cap set than what was previously known, thus illustrating the191\npower of FunSearch to discover novel constructions. This also shows the scalability of FunSearch to192\nlarger dimensions, where the previously best known construction relied on a complex combination193\nof cap sets in lower dimensions [34, 35]. In contrast, FunSearch discovered a larger cap set from194\nscratch, without having to be explicitly taught any way of combining cap sets. Moreover, we do not195\njust discover the set of 512 8-dimensional vectors in itself, but a program that generates it: we show196\nthis program in Figure 4 (b). Through inspecting the code, we obtain a degree of understanding197\nof what this set is: specifically, manual simplification of Figure 4 (b) provides the construction in198\nFigure 4 (c). Some properties of this construction are strikingly similar to the construction of the199\nHill cap [36, 37], which results in the optimal 112-cap in Z6\n3.200\nAdmissible sets. Beyond finding the size of the largest cap set cn in dimension n, a fundamental201\nproblem in additive combinatorics [23] is determining the capacityC = supn c1/n\nn . The breakthrough202\nresult of [ 32] established an upper bound of C ≤ 2.756. In this work, we are interested in lower203\nbounds on C. To this end, we use the framework of constant weight admissible sets (or admissible204\nsets for short) [35], which has established the current state-of-the-art.205\nFormally, admissible sets A(n, w) are collections of vectors in {0, 1, 2}n satisfying two properties:206\ni) each vector has the same number w of non-zero elements but a unique support (thereby implying207\n|A| ≤\n\u0000n\nw\n\u0001\n); ii) for any three distinct vectors there is a coordinate in which their three respective208\nvalues are {0, 1, 2}, {0, 0, 1}, or {0, 0, 2}. Informally, an admissible set describes how to combine209\ncap sets in smaller dimensions into large cap sets in higher dimensions [ 35]. We denote the set of210\n5\nACCELERATED ARTICLE PREVIEW\nfull-size admissible sets (with |A| =\n\u0000n\nw\n\u0001\n) as I(n, w). The current state-of-the-art [39] has relied on211\nSAT solvers to construct large admissible sets.212\nAs before, we evolve a function priority : {0, 1, 2}n → R, which is used to iteratively grow213\nadmissible sets. Starting from a trivial constant function, we discover one that provides us with214\nan I(12, 7) admissible set; the discovered program is shown in Figure 5 (b). This discovery alone215\nalready improves the lower bound on the cap set capacity from 2.2180 [39] to 2.2184. Yet, interpreting216\nthe program found by FunSearch (Figure 5 b) helps us significantly push the boundaries of what217\nadmissible sets we can construct. Specifically, we notice that the discovered priority function218\ntreats the n coordinates in a highly symmetric way, and indeed it turns out that the admissible set219\nit constructs is preserved under independent cyclic permutations of coordinates within four disjoint220\ngroups of coordinate triples. Hereinafter we call such admissible sets symmetric (see Appendix D in221\nSupplementary Information for a formal definition).222\nWe now use FunSearch to directly search for symmetric admissible sets. Note that this is a more223\nrestricted but also much smaller search space, which allows for significantly higher dimensions and224\nweights than were previously possible. This led us to discovering a full-size I(15, 10) admissible set225\n(implying C ≥ 2.219486) and a partial admissible set in A(24, 17) of size 237 984, which implies226\na new lower bound on the cap set capacity of 2 .2202 (see Figure 5 a). While this is the largest227\nimprovement to the lower bound in the last 20 years, we note it is still far from the upper bound,228\nand we hope our results inspire future work on this problem.229\nNot only does FunSearch scale to much larger instances than traditional combinatorial solvers230\n(see Appendix A.4 in Supplementary Information), it is a unique feature of searching in function231\nspace that we were able to inspect the code discovered by FunSearch and infer a new insight into232\nthe problem, in the form of a new symmetry. The procedure we followed in this section is a concrete233\nexample of how LLM-based approaches can be used in mathematical sciences: FunSearch suggests234\na solution, which is examined by researchers, who may note features of interest. These features are235\nused to refine the search, leading to better solutions. This process can be iterated, with both human236\nand search consistently in the loop.237\n2.2 Bin packing238\nCombinatorial optimization is a subfield of mathematics which plays an important role across a wide239\nrange of areas, from theoretical computer science to practical problems in logistics and scheduling.240\nWhile many combinatorial optimization problems are provably hard to solve for large instances, it241\nis typically possible to achieve strong performance using heuristics to guide the search algorithm.242\nThe choice of a heuristic is crucial for obtaining strong performance, but designing a good heuristic243\nis difficult in practice. In this section, we show that FunSearch can be used to discover effective244\nheuristics for one of the central problems in combinatorial optimization: bin packing [4].245\nThe goal of bin packing is to pack a set of items of various sizes into the smallest number of246\nfixed-sized bins. Bin packing finds applications in many areas, from cutting materials to scheduling247\njobs on compute clusters. We focus on the online setting where we pack an item as soon as it is248\nreceived (as opposed to the offline setting where we have access to all items in advance). Solving249\nonline bin packing problems then requires designing a heuristic for deciding which bin to assign an250\nincoming item to.251\nHeuristics for online bin packing are well studied and several variants exist with strong worst252\ncase performance [40–45]. However, they often exhibit poor performance in practice [4]. Instead, the253\nmost commonly used heuristics for bin packing are first fit and best fit. First fit places the incoming254\nitem in the first bin with enough available space, while best fit places the item in the bin with least255\navailable space where the item still fits. Here, we show that FunSearch discovers better heuristics256\n6\nACCELERATED ARTICLE PREVIEW\nOR1 OR2 OR3 OR4 Weibull 5k Weibull 10k Weibull 100k\nFirst Fit 6.42% 6.45% 5.74% 5.23% 4.23% 4.20% 4.00%\nBest Fit 5.81% 6.06% 5.37% 4.94% 3.98% 3.90% 3.79%\nFunSearch 5.30% 4.19% 3.11% 2.47% 0.68% 0.32% 0.03%\nTable 1: Online bin packing results. Fraction of excess bins (lower is better) for various bin\npacking heuristics on the OR and Weibull datasets. FunSearch outperforms first fit and best fit\nacross problems and instance sizes.\nthan first fit and best fit on simulated data.257\nTo achieve this, we define a heuristic as a program that takes as input an item and an array258\nof bins (containing the remaining capacity of each bin) and returns a priority score for each bin.259\nThe solve function picks the bin with the highest score according to the heuristic (see Figure 2 b).260\nFunSearch is then used to evolve this heuristic, starting from best fit.261\nWe first evaluate FunSearch on the well-known OR-Library bin packing benchmarks [24], con-262\nsisting of four datasets, OR1 to OR4, containing bin packing instances with an increasing number263\nof items (see Appendix E.4 in Supplementary Information for details). We evolve our heuristic on264\na training set of generated bin packing instances with the same number of items as those in OR1265\nand, after the evolutionary process is concluded, test it on the OR1 to OR4 datasets. We measure266\nperformance as the fraction of excess bins used over the L2 lower bound [46] of the optimal offline267\npacking solution (which is generally not achievable in the online setting).268\nAs can be seen in Table 1, FunSearch outperforms both first fit and best fit across all datasets.269\nFurther, the learned heuristic generalizes: even though it has only seen instances of the same size as270\nOR1 during training, it generalizes across problem sizes, performing even better on large instances271\nand widening the gap to best fit. In addition to the OR benchmarks, we also use FunSearch to evolve272\nheuristics on bin packing instances sampled from a Weibull distribution, as these closely follow many273\nreal-world scheduling problems [25,47] (see Appendix E.4 in Supplementary Information for details).274\nAs shown in Table 1, the performance of FunSearch is very strong on this dataset, significantly275\noutperforming first fit and best fit across instances, as well as scaling gracefully to large instances276\n(being only 0.03% off the lower bound on the optimum for 100 000 items). In addition, FunSearch is277\nrobust and consistently outperforms these baselines as shown in the statistical analysis in Appendix278\nA.3 in Supplementary Information.279\nWe observed that several heuristics discovered by FunSearch use the same general strategy for280\nbin packing (see Figure 6 for an example). Instead of packing items into bins with the least capacity281\n(like best fit), the FunSearch heuristics assign items to least capacity bins only if the fit is very tight282\nafter placing the item. Otherwise, the item is typically placed in another bin which would leave283\nmore space after the item is placed. This strategy avoids leaving small gaps in bins that are unlikely284\nto ever be filled (see Appendix E.5 in Supplementary Information for example visualizations of such285\npackings).286\nAs this example demonstrates, the benefits of FunSearch extend beyond theoretical and mathe-287\nmatical results to practical problems like bin packing. Indeed, bin packing, and related combinatorial288\noptimization problems, are ubiquitous and find applications across a range of industries. We are289\noptimistic that FunSearch could be applied to several such use-cases with potential for real-world290\nimpact.291\n7\nACCELERATED ARTICLE PREVIEW\n3 Discussion292\nThe effectiveness of FunSearch in discovering new knowledge for hard problems might seem intrigu-293\ning. We believe that the LLM used within FunSearch does not use much context about the problem;294\nthe LLM should instead be seen as a source of diverse (syntactically correct) programs with occa-295\nsionally interesting ideas. When further constrained to operate on the crucial part of the algorithm296\nwith a program skeleton, the LLM provides suggestions that marginally improve over existing ones297\nin the population, which ultimately results in discovering new knowledge on open problems when298\ncombined with the evolutionary algorithm. Another crucial component of the effectiveness of Fun-299\nSearch is that it operates in the space of programs: rather than directly searching for constructions300\n(which is typically an enormous list of numbers), FunSearch searches for programs generating those301\nconstructions. Because most problems we care about are structured (highly non-random), we hy-302\npothesize that solutions are described more concisely with a computer program, compared to other303\nrepresentations. For example, the trivial representation of the admissible set A(24, 17) consists of304\nmore than 200 000 vectors, but the program generating this set consists only of a few lines of code.305\nBecause FunSearch implicitly encourages concise programs, it scales to much larger instances com-306\npared to traditional search approaches in structured problems. In a loose sense, FunSearch attempts307\nto find solutions that have low Kolmogorov complexity [48–50] (which is the length of the short-308\nest computer program that produces a given object as output), while traditional search procedures309\nhave a very different inductive bias. We believe that such Kolmogorov-compressed inductive bias310\nis key to FunSearch scaling up to the large instances in our use-cases. In addition to scale, we311\nhave empirically observed that FunSearch outputs programs that tend to be interpretable — that312\nis, they are clearly easier to read and understand compared to a list of numbers. For example, by313\nscrutinizing FunSearch’s output for the admissible set problem, we found a new symmetry, which314\nwas then subsequently used to improve the results even further. Despite the rarity of symmetric315\nsolutions, we observe that FunSearch preferred symmetric ones, as these are more parsimonious316\n(that is, they require less information to specify), in addition to the natural bias of LLMs (trained317\non human-produced code) in outputting code with similar traits to human code. This is in contrast318\nto traditional genetic programming which do not have this bias (and in addition require hand-tuning319\nthe mutation operators [51]).320\nWe note that FunSearch currently works best for problems having the following characteristics:321\na) availability of an efficient evaluator; b) a “rich” scoring feedback quantifying the improvements322\n(as opposed to a binary signal); c) ability to provide a skeleton with an isolated part to be evolved.323\nFor example, the problem of generating proofs for theorems [52–54] falls outside this scope, since324\nit is unclear how to provide a rich enough scoring signal. In contrast, for MAX-SAT, the number325\nof satisfied clauses can be used as a scoring signal. In this paper, we have explicitly striven for326\nsimplicity and we are confident that FunSearch can be further extended to improve its performance327\nand be applicable to more classes of problems. In addition, the rapid development of LLMs is328\nlikely to result in samples of far superior quality at a fraction of the cost, making FunSearch more329\neffective at tackling a broad range of problems. As a result, we envision that automatically-tailored330\nalgorithms will soon become common practice and deployed in real-world applications.331\nReferences332\n[1] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung,333\net al., A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucina-334\ntion, and interactivity, arXiv preprint arXiv:2302.04023 (2023).335\n8\nACCELERATED ARTICLE PREVIEW\n[2] A. Borji, A categorical archive of ChatGPT failures, arXiv preprint arXiv:2302.03494 (2023).336\n[3] J. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, K. O. Stanley, Evolution through large337\nmodels, arXiv preprint arXiv:2206.08896 (2022).338\n[4] E. G. Coffman, M. R. Garey, D. S. Johnson, Approximation algorithms for bin-packing—an339\nupdated survey, Algorithm design for computer system design (1984) 49–106.340\n[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,341\nN. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv342\npreprint arXiv:2107.03374 (2021).343\n[6] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,344\nQ. Le, et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732345\n(2021).346\n[7] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li,347\nJ. Chim, et al., StarCoder: may the source be with you!, arXiv preprint arXiv:2305.06161348\n(2023).349\n[8] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettle-350\nmoyer, M. Lewis, Incoder: A generative model for code infilling and synthesis, in: International351\nConference on Learning Representations, 2022.352\n[9] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, C. Xiong, CodeGen:353\nAn open large language model for code with multi-turn program synthesis, in: International354\nConference on Learning Representations, 2022.355\n[10] X. Chen, M. Lin, N. Sch¨ arli, D. Zhou, Teaching large language models to self-debug, arXiv356\npreprint arXiv:2304.05128 (2023).357\n[11] V. Liventsev, A. Grishina, A. H¨ arm¨ a, L. Moonen, Fully autonomous programming with large358\nlanguage models, arXiv preprint arXiv:2304.10423 (2023).359\n[12] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,360\nF. Gimeno, A. Dal Lago, et al., Competition-level code generation with alphacode, Science 378361\n(2022) 1092–1097.362\n[13] E. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, N. Haber, Parsel: A (de-) compositional363\nframework for algorithmic reasoning with language models, arXiv preprint arXiv:2212.10561364\n(2023).365\n[14] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan, Y. Yang, G. Neubig, A. Yazdan-366\nbakhsh, Learning performance-improving code edits, arXiv preprint arXiv:2302.07867 (2023).367\n[15] D. E. Goldberg, Optimization and machine learning, 1989.368\n[16] J. R. Koza, Genetic programming as a means for programming computers by natural selection,369\nStatistics and computing 4 (1994) 87–112.370\n[17] E. Meyerson, M. J. Nelson, H. Bradley, A. Moradi, A. K. Hoover, J. Lehman, Language model371\ncrossover: Variation through few-shot prompting, arXiv preprint arXiv:2302.12170 (2023).372\n9\nACCELERATED ARTICLE PREVIEW\n[18] A. Chen, D. M. Dohan, D. R. So, EvoPrompting: Language models for code-level neural373\narchitecture search, arXiv preprint arXiv:2302.14838 (2023).374\n[19] M. Zheng, X. Su, S. You, F. Wang, C. Qian, C. Xu, S. Albanie, Can GPT-4 perform neural375\narchitecture search?, arXiv preprint arXiv:2304.10970 (2023).376\n[20] M. U. Nasir, S. Earle, J. Togelius, S. James, C. Cleghorn, LLMatic: Neural architecture search377\nvia large language models and quality-diversity optimization, arXiv preprint arXiv:2306.01102378\n(2023).379\n[21] P. Haluptzok, M. Bowers, A. T. Kalai, Language models can teach themselves to program380\nbetter (2022).381\n[22] J. Grochow, New applications of the polynomial method: the cap set conjecture and beyond,382\nBulletin of the American Mathematical Society 56 (2019) 29–64.383\n[23] T. Tao, V. H. Vu, Additive combinatorics, volume 105, Cambridge University Press, 2006.384\n[24] J. E. Beasley, Or-library: distributing test problems by electronic mail, Journal of the opera-385\ntional research society 41 (1990) 1069–1072.386\n[25] I. Casti˜ neiras, M. De Cauwer, B. O’Sullivan, Weibull-based benchmarks for bin packing, in:387\nInternational Conference on Principles and Practice of Constraint Programming, Springer, 2012,388\npp. 207–222.389\n[26] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,390\nP. Bailey, Z. Chen, et al., Palm 2 technical report, arXiv preprint arXiv:2305.10403 (2023).391\n[27] Code models overview, https://cloud.google.com/vertex-ai/docs/generative-ai/code/392\ncode-models-overview, 2023. [Online; accessed July-2023].393\n[28] R. Tanese, Distributed genetic algorithms for function optimization, University of Michigan,394\n1989.395\n[29] E. Cant´ u-Paz, A survey of parallel genetic algorithms, Calculateurs paralleles, reseaux et396\nsystems repartis 10 (1998) 141–171.397\n[30] T. Tao, Open question: best bounds for cap sets, https://terrytao.wordpress.com/2007/398\n02/23/open-question-best-bounds-for-cap-sets/, 2009.399\n[31] E. Croot, V. F. Lev, P. P. Pach, Progression-free sets in are exponentially small, Annals of400\nMathematics (2017) 331–337.401\n[32] J. S. Ellenberg, D. Gijswijt, On large subsets of Fn\nq with no three-term arithmetic progression,402\nAnnals of Mathematics (2017) 339–343.403\n[33] E. Naslund, W. Sawin, Upper bounds for sunflower-free sets, in: Forum of Mathematics, Sigma,404\nvolume 5, Cambridge University Press, 2017, p. e15.405\n[34] Y. Edel, J. Bierbrauer, Large caps in small spaces, Designs, Codes and Cryptography 23 (2001)406\n197–212.407\n[35] Y. Edel, Extensions of generalized product caps, Designs, Codes and Cryptography 31 (2004)408\n5–14.409\n10\nACCELERATED ARTICLE PREVIEW\n[36] R. Hill, On the largest size of cap in S5,3, Atti della Accademia Nazionale dei Lincei. Classe di410\nScienze Fisiche, Matematiche e Naturali. Rendiconti 54 (1973) 378–384.411\n[37] P. J. Cameron, J. H. Van Lint, Designs, graphs, codes and their links, volume 3, Cambridge412\nUniversity Press Cambridge, 1991.413\n[38] A. R. Calderbank, P. C. Fishburn, Maximal three-independent subsets of {0, 1, 2} n, Designs,414\nCodes and Cryptography 4 (1994) 203–211.415\n[39] F. Tyrrell, New lower bounds for cap sets, arXiv preprint arXiv:2209.10045 (2022).416\n[40] C. C. Lee, D. T. Lee, A simple on-line bin-packing algorithm, Journal of the ACM (JACM) 32417\n(1985) 562–572.418\n[41] P. Ramanan, D. J. Brown, C.-C. Lee, D.-T. Lee, On-line bin packing in linear time, Journal of419\nAlgorithms 10 (1989) 305–326.420\n[42] S. S. Seiden, On the online bin packing problem, Journal of the ACM (JACM) 49 (2002)421\n640–671.422\n[43] J. Balogh, J. B´ ek´ esi, G. D´ osa, J. Sgall, R. v. Stee, The optimal absolute ratio for online423\nbin packing, in: Proceedings of the twenty-sixth annual ACM-SIAM symposium on discrete424\nalgorithms, SIAM, 2014, pp. 1425–1438.425\n[44] J. Balogh, J. B´ ek´ esi, G. D´ osa, L. Epstein, A. Levin, A new and improved algorithm for426\nonline bin packing, in: 26th Annual European Symposium on Algorithms (ESA 2018), Schloss427\nDagstuhl–Leibniz-Zentrum fuer Informatik, 2018, pp. 5:1–5:14.428\n[45] E. G. Coffman, J. Csirik, G. Galambos, S. Martello, D. Vigo, Bin packing approximation429\nalgorithms: survey and classification, Handbook of combinatorial optimization (2013) 455–531.430\n[46] S. Martello, P. Toth, Lower bounds and reduction procedures for the bin packing problem,431\nDiscrete applied mathematics 28 (1990) 59–70.432\n[47] S. Angelopoulos, S. Kamali, K. Shadkami, Online bin packing with predictions 36 (2022)433\n4574–4580.434\n[48] G. J. Chaitin, On the length of programs for computing finite binary sequences, Journal of the435\nACM (JACM) 13 (1966) 547–569.436\n[49] M. Li, P. Vit´ anyi, et al., An introduction to Kolmogorov complexity and its applications,437\nvolume 3, Springer, 2008.438\n[50] R. J. Solomonoff, A formal theory of inductive inference. part i, Information and control 7439\n(1964) 1–22.440\n[51] M. O’Neill, L. Vanneschi, S. Gustafson, W. Banzhaf, Open issues in genetic programming,441\nGenetic Programming and Evolvable Machines 11 (2010) 339–363.442\n[52] S. Polu, I. Sutskever, Generative language modeling for automated theorem proving, arXiv443\npreprint arXiv:2009.03393 (2020).444\n[53] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, I. Sutskever, Formal mathematics445\nstatement curriculum learning, arXiv preprint arXiv:2202.01344 (2022).446\n11\nACCELERATED ARTICLE PREVIEW\n[54] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzyg´ o´ zd´ z, P. Mi lo´ s, Y. Wu, M. Jam-447\nnik, Thor: Wielding hammers to integrate language models and automated theorem provers,448\nAdvances in Neural Information Processing Systems 35 (2022) 8360–8373.449\n12\nACCELERATED ARTICLE PREVIEW\nFigure 1: Overview of FunSearch. The input to FunSearch is a specification of the problem\nin the form of an evaluate function, an initial implementation of the function to evolve, which can\nbe trivial, and potentially a skeleton. At each iteration, FunSearch builds a prompt by combining\nseveral programs sampled from the programs database (favouring high-scoring ones). The prompt is\nthen fed to the pre-trained LLM, and new programs are created. Newly created programs are then\nscored and stored in the programs database (if correct), thus closing the loop. The user can at any\npoint retrieve the highest-scoring programs discovered so far.\nFigure 2: Examples of FunSearch specifications for two problems. The evaluate function\ntakes as input a candidate solution to the problem, and returns a score assessing it. The solve\nfunction contains the algorithm skeleton, which calls the function to evolve that contains the crucial\nlogic. For (a), the function to evolve is called priority, and for (b) it is called heuristic. The main\nfunction implements the evaluation procedure by connecting the pieces together. Specifically, it uses\nthe solve function to solve the problem, and then scores the resulting solutions using evaluate. In\nsimplest cases, main just executes solve once and uses evaluate to score the output, e.g., see (a).\nIn specific settings such as online algorithms, the main function implements some additional logic,\ne.g., see (b).\nFigure 3: Diagram of a cap set of size 4 in Z2\n3. The circles are the elements of Z2\n3 with the\nones belonging to the cap set shown in blue. The possible lines in Z2\n3 are also shown (with colors\nindicating lines that wrap around in arithmetic modulo 3). No three elements of the cap set are in\na line.\nFigure 4: Result of applying FunSearch to the cap set problem. (a) Size of the largest\ncap set in Zn\n3 for different dimensions n. (b) The function priority : Zn\n3 → R discovered by\nFunSearch that results in a cap set of size 512 in n = 8 dimensions. One feature to note is that\nthe priority is affected by whether the same entry appears in positions i and -i (-i denotes the\ni-th position counting from the end). This motivates the notion of reflections, used in (c). (c)\nAn explicit construction of this new 512-cap, which we were able to manually construct thanks to\nhaving discovered the cap set by searching in function space. See Appendix E.2 in Supplementary\nInformation for more details and for relation to Hill cap.\nFigure 5: Results on the cap set problem via admissible sets. (a) Summary of lower bounds\non the cap set capacity C. (b) The priority function {0, 1, 2}n → R discovered by FunSearch that\nresults in an I(12, 7) admissible set. The source code reveals that when n = 12, the function treats\nthe four triples of coordinates {0, 4, 8}, {1, 5, 9}, {2, 6, 10}, and {3, 7, 11} together. We then checked\nthat the admissible set is in fact symmetric under independent cyclic permutations of coordinates\nwithin each of these four triples. See Appendix D and Appendix E.3 in Supplementary Information\nfor more details.\n13\nACCELERATED ARTICLE PREVIEW\nFigure 6: Example of a short online bin packing heuristic discovered by FunSearch for\nthe OR dataset. This example illustrates frequently observed behavior: instead of always packing\nitems into the best fit bin, the heuristic encourages packing the item only if the fit is tight (line 11).\nComments in the code were manually added. See Appendix C in Supplementary Information for\nmore discovered heuristics.\n14\nACCELERATED ARTICLE PREVIEW\nA Methods450\nA.1 Implementation details of FunSearch451\nDistributed system. We implement FunSearch as a distributed system that has three types of452\nworkers: a programs database, samplers, and evaluators. The programs database stores the initial453\nuser-provided program, as well as all programs received from the evaluators. The samplers are in454\ncharge of performing the LLM inference step; to do so they repeatedly query the programs database455\nfor prompts. To achieve higher sampling throughput, samplers generate multiple samples from each456\nprompt. The samples from the LLM (i.e., the generated programs) are sent to the evaluators,457\nwhich score programs by executing them on inputs of interest and assessing the outputs using458\nevaluate. Programs that are correct are sent to the programs database to be stored. Each of459\nthe three FunSearch components is provided as both Python code and pseudocode (Appendix F in460\nSupplementary Information).461\nPrompt building. When queried for a prompt, the programs database samples k programs to462\nencourage the LLM to merge ideas from them (we typically set k = 2; see Appendix E.1 in Sup-463\nplementary Information). Programs are sorted according to their score in increasing order, starting464\nfrom “version 0” (v0). Using these k programs, the prompt is built as explained next.465\nFor the sake of clarity, we use here the problem specification from Figure 2 (a) to precisely466\ndescribe the prompting mechanism. The overall structure of the prompt mimics the structure of467\nthe program skeleton, with the following differences: (i) The priority function is stripped out, and468\nreplaced with the k = 2 programs sampled, first priority v0 and then priority v1. (ii) After469\nthat, a priority v2 function with no body is appended — the LLM will be in charge of completing470\nthe body of that function. (iii) All other functions that appear before priority v0 are removed.471\nSee Extended Data Figure 1 for an example of the structure of a prompt.472\nEvolutionary method and program selection. Another key feature ofFunSearchis the method473\nused for evolution of the population of programs from the programs database, as well as for program474\nselection, i.e., how the programs database samples programs when queried for a prompt. For this,475\nwe use the islands model, a parallel genetic algorithm [28, 29]. Specifically, we split the population476\ninto m separate groups, or islands. Each island is initialized with a copy of the user-provided initial477\nprogram and is evolved separately. That is, whenever a prompt is required, we first uniformly sample478\nan island and then sample k = 2 programs from that island to build the prompt. The programs479\ngenerated from the LLM based on that prompt will later be stored in the same island. Every four480\nhours, we discard all the programs from the m/2 islands whose best instances have the lowest score.481\nEach of these islands is then seeded with a single program, obtained by first choosing one of the482\nsurviving m/2 islands uniformly at random, and then retrieving the highest-scoring program from483\nthat island (breaking ties in favour of older programs). The evolutionary process is then restarted484\nfrom this state, in which the reset islands contain one high-performing program each (see Extended485\nData Figure 2).486\nThis method has several advantages. First, drawing the analogy where an island corresponds487\nto an experiment, this approach effectively allows us to run several smaller experiments in parallel,488\ninstead of a single large experiment. This is beneficial because single experiments can get stuck in489\nlocal minima, where the majority of programs in the population are not easily mutated and combined490\ninto stronger programs. The multiple island approach allows us to bypass this and effectively kill491\noff such experiments to make space for new ones starting from more promising programs. Second,492\n15\nACCELERATED ARTICLE PREVIEW\npromising experiments are run for longer, since the islands that survive a reset are the ones with493\nhigher scores.494\nWithin each island, we further cluster programs according to their signature. We define the495\nsignature of a program as the tuple containing the program’s scores on each of the inputs (e.g., the496\ncap set size for each input n). Programs with the same signature are clustered together. When497\nsampling a program within an island, we first sample an island’s cluster, and then a program within498\nthat cluster (see Extended Data Figure 3). This approach, which aims at preserving diversity499\n[55, 56], is related to Lexicase [57] in that both approaches consider a set of test cases for scoring an500\nindividual, and it is related to fitness uniform optimization [58], which also clusters individuals based501\non their fitness value, however we sample the clusters based on their score instead of uniformly, as502\ndetailed next.503\nWhen sampling a cluster, we favor those with larger score values. Specifically, let si denote the504\nscore of the i-th cluster, defined as an aggregation (e.g., mean) of all the scores in the signature that505\ncharacterizes that cluster. The probability pi of choosing cluster i is506\npi = exp (si/Tcluster)P\ni′ exp (si′/Tcluster), T cluster = T0 ·\n\u0012\n1 − n mod N\nN\n\u0013\n, (1)\nwhere Tcluster is the temperature parameter, n is the current number of programs in the island,507\nand T0 and N are hyperparameters (given in Appendix E.1 in Supplementary Information). This508\napproach is sometimes referred to as the Boltzmann selection procedure [59].509\nWhen sampling a program within a cluster, we favor shorter programs. In particular, let ℓi510\ndenote the negative length of the i-th program within the chosen cluster (measured as the number511\nof characters), and let eℓi = ℓi−mini′ ℓi′\nmaxi′ ℓi′+10−6 . We set the probability of each program proportional to512\nexp(eℓi/Tprogram), where Tprogram is a temperature hyperparameter.513\nRobustness. Due to randomness in LLM sampling and in the evolutionary procedure, repeating514\nan experiment can lead to different results. For some problems (e.g. cap set through the admissible515\nset problem, and online bin packing) every single run of FunSearch surpasses the baseline, with only516\nsome variation in the magnitude of the difference. For example, all experiments on admissible sets517\nimprove upon the previous best capacity lower bound, with 60% of experiments on I(12, 7) finding518\na full-size admissible set. For other problems, multiple independent repetitions of an experiment519\nmay be necessary to improve upon prior best results. In particular, the case of cap set by direct520\nconstruction in n = 8 dimensions is particularly challenging, with only 4 out of 140 experiments521\ndiscovering a cap set of size 512. See Appendix A.3 in Supplementary Information for more details.522\nA.2 Related work523\nLarge Language Models. The rise of powerful LLMs such as [60] has been followed by systems524\nin which an LLM core is enveloped by a “programmatic scaffold” [61], and multiple LLM calls are525\nconnected together in some way to accomplish larger and more intricate tasks beyond what would be526\npossible using a single prompt and the raw LLM, possibly using external tools or external memory527\nstreams [62–66]. LLMs have also been paired with evaluators; for example, [21, 67] fine-tune an528\nLLM on data that has been previously generated by the LLM itself (respectively on puzzle problems529\nand solutions, and on justifications/explanations for answers to questions), and use an evaluator530\nto assess the correctness of this data, ensuring that the fine-tuning dataset contains correct solu-531\ntions/explanations only. More related to our approach is the use of LLMs as a mutation operator532\non code. [3] was the first work to show that coupling an LLM with a programatic way of scoring a533\n16\nACCELERATED ARTICLE PREVIEW\nsolution can lead to a self-improvement loop. In [17–20], the LLM is used as a crossover operator534\nrather than a mutation one, i.e., the LLM prompts are composed of several functions, similarly to535\nFunSearch. In [3, 17], the task is to improve code that generates bidimensional virtual robots that536\ncan move as far as possible in a given simulated terrain ([17] additionally considers the tasks of537\nsymbolic regression, natural language sentences, and image generation), in [18–20] the task is to538\nfind neural network architectures (described with Python code), and in [68] the task is continuous539\nexploration in the game of Minecraft. In contrast, in this paper we tackle open problems in math-540\nematics and algorithm design, and we surpass human-designed constructions. We achieve that by541\ncombining multiple ingredients together: a distributed system with multiple samplers and evaluators542\nthat communicate asynchronously, a user-provided program specification and skeleton, as well as543\nan evolutionary mechanism based on islands that preserves the diversity of programs. FunSearch544\nachieves that using an off-the-shelf LLM without fine-tuning.545\nMore broadly, LLMs have been used for program synthesis as one of its main applications [5–9].546\nThere are many use cases being explored, such as automatically editing code to improve performance547\n[14], automatically debugging code [10, 11], generating code from natural language descriptions [69–548\n71], and doing so to solve problems in code competitions [12, 13]. Unlike the above approaches549\nwhich provide tools to increase the productivity of software engineers, we combine in this paper550\nthe creativity of LLMs with the power of evolutionary procedures to push the boundaries of human551\nknowledge through solving open hard problems. Another line of research uses LLMs to guide the552\nsearch for formal proofs for automatic theorem proving [52–54]. While this approach has the potential553\nof eventually finding new knowledge, the achievements of these methods still lag behind the frontier554\nof human knowledge.555\nGenetic programming. Genetic programming (GP) is a subfield of computer science concerned556\nwith automatically generating or discovering computer programs using evolutionary methods [16,557\n72, 73] and is employed for symbolic regression applications [74, 75] and discovery of optimization558\nalgorithms [76] among others. In this broad sense, combining LLMs with evolution can be seen559\nas an instance of GP with the LLM acting as a mutation and crossover operator. However, using560\nan LLM mitigates several issues in traditional GP [51], as shown in Appendix A in Supplementary561\nInformation and discussed in [3]. Indeed, GP methods require defining a number of parameters,562\nchief among them the set of allowed mutation operations (or primitives) [16]. Designing such a set563\nof operations is non-trivial and problem-specific, requiring domain knowledge about the problem at564\nhand or its plausible solution [51]. While research has been done to mitigate this limitation, through565\nfor example the reuse of subprograms [77] or modeling the distribution of high-performing programs566\n[78], designing effective and general code mutation operators remains difficult. In contrast, LLMs567\nhave been trained on vast amounts of code and as such have learned about common patterns and568\nroutines from human-designed code. The LLM can leverage this, as well as the context given in the569\nprompt, to generate more effective suggestions than the random ones typically used in GP.570\nRelated to GP, the field of hyper-heuristics [79, 80] seeks to design learning methods for gen-571\nerating heuristics applied to combinatorial optimization problems. In practice, these heuristics are572\noften programs discovered through GP, typically by evolving a heuristic on a set of instances of a573\ngiven combinatorial optimization problem, such as bin packing [81]. Indeed, like FunSearch, hyper-574\nheuristics have also been applied to online bin packing, with the learned heuristics able to match the575\nperformance of first fit [82] and best fit [83] on a set of generated bin packing instances. Augmenting576\nthe heuristics with memory of previously seen items can even lead to heuristics outperforming best577\nfit [84]. In addition, these evolved heuristics can sometimes generalize to larger instances than the578\nones they were trained on [85], similar to the learned FunSearch heuristics. However, as is the case579\nwith GP, one of the fundamental limitations of hyper-heuristics is that the components of the evolved580\n17\nACCELERATED ARTICLE PREVIEW\nheuristic must be manually defined by the user and often need to be tailored to a specific problem581\nto be effective. The LLM in FunSearch allows us to bypass this limitation and learn heuristics for582\nbin packing and job scheduling as well as discovering novel mathematical constructions, all within583\na single pipeline without problem specific tuning.584\nProgram superoptimization and software engineering. Searching for the best way of mod-585\nifying source code is a task that appears in multiple branches of computer science and software586\ndevelopment. These occurrences can be broadly classified into two groups: first, where the goal is to587\nfind semantic-preserving modifications (this arises in program optimization and superoptimization,588\nwhere the aim is to modify the program so that it executes faster while maintaining its input-output589\nbehaviour), and second, where the goal is to find programs with different semantics (this arises, e.g.,590\nin automatic program repair and mutation testing ). With some exceptions discussed below, most591\nof these areas use relatively simple and hard-coded mutation operators on either the source code592\ndirectly (such as deleting or swapping lines) or on the abstract syntax tree (AST).593\nMachine learning approaches have been used for program superoptimization. For example, [86]594\nused reinforcement learning to learn the sampling probabilities used within a hierarchical prob-595\nabilistic model of simple program edits introduced by STOKE [87]. Neural networks have also596\nbeen proposed as a mutation operator for program optimization in [88]. These works operated on597\ncode written in Assembly (perhaps because designing meaningful and rich edit distributions on pro-598\ngrams in higher-level languages is challenging). More recently, [14] used LLMs to find performance-599\nimproving edits to code written in C++ or Python. We also note that reinforcement learning has600\nrecently been applied to discover new faster algorithms for fundamental operations such as matrix601\nmultiplication [89] and sorting [90].602\nIn this paper, we have not explicitly explored semantic-preserving applications such as discovering603\nperformance-improving code edits, but we believe that FunSearch could be an effective method for604\nthat setting too. In both use cases presented in Section 2, the goal is to evolve programs with new605\nsemantics, but the application is different from program repair or mutation testing: in Section 2.1 we606\nused FunSearch to discover a program that constructs a previously unknown mathematical object,607\nand in Section 2.2 we used FunSearch to discover a program that corresponds to a more efficient608\nheuristic for online bin packing.609\nData availability. The experiments carried out in this paper do not require any data corpus other610\nthan the publicly available OR-Library bin packing benchmarks [24]. The output functions of interest611\nproduced by FunSearch are shown across the main paper and in text files in the Supplementary612\nInformation.613\nCode availability. The discovered functions as well as the evolutionary algorithm, code manipula-614\ntion routines, and a single-threaded implementation of theFunSearchpipeline are available as Python615\ncode in the Supplementary information and athttps://github.com/google-deepmind/funsearch.616\nAdditionally, the software library launchpad [91], and a sandbox for safely executing generated code617\non our internal distributed system were used. No training or fine-tuning of a large language model618\nis required; API access for inference is sufficient. We used Codey [27], which is available through its619\nAPI, and StarCoder [7], which is open source.620\n18\nACCELERATED ARTICLE PREVIEW\nReferences621\n[55] J.-B. Mouret, S. Doncieux, Overcoming the bootstrap problem in evolutionary robotics using622\nbehavioral diversity, in: 2009 IEEE Congress on Evolutionary Computation, 2009, pp. 1161–623\n1168.624\n[56] J. K. Pugh, L. B. Soros, K. O. Stanley, Quality diversity: A new frontier for evolutionary625\ncomputation, Frontiers in Robotics and AI 3 (2016) 40.626\n[57] T. Helmuth, L. Spector, J. Matheson, Solving uncompromising problems with lexicase selection,627\nIEEE Transactions on Evolutionary Computation 19 (2015) 630–643.628\n[58] M. Hutter, S. Legg, Fitness uniform optimization, IEEE Transactions on Evolutionary Com-629\nputation 10 (2006) 568–589.630\n[59] M. de la Maza, An analysis of selection procedures with particular attention paid to propor-631\ntional and boltzmann selection, in: Proceedings of the fifth international conference on genetic632\nalgorithms, 1993, Morgan Kaufmann, 1993.633\n[60] OpenAI, GPT-4 technical report, 2023. arXiv:2303.08774.634\n[61] B. Millidge, Scaffolded LLMs as natural language computers, https://www.beren.io/635\n2023-04-11-Scaffolded-LLMs-natural-language-computers, 2023. [Online; accessed July-636\n2023].637\n[62] T. Schick, J. Dwivedi-Yu, R. Dess` ı, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda,638\nT. Scialom, Toolformer: Language models can teach themselves to use tools, arXiv preprint639\narXiv:2302.04761 (2023).640\n[63] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, M. S. Bernstein, Generative agents:641\nInteractive simulacra of human behavior, arXiv preprint arXiv:2304.03442 (2023).642\n[64] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, P. Christiano, Recursively643\nsummarizing books with human feedback, arXiv preprint arXiv:2109.10862 (2021).644\n[65] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan,645\nA. Lewkowycz, M. Bosma, D. Luan, C. Sutton, A. Odena, Show your work: Scratchpads646\nfor intermediate computation with language models, arXiv preprint arXiv:2112.00114 (2021).647\n[66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, Y. Cao, ReAct: Synergizing648\nreasoning and acting in language models, in: International Conference on Learning Represen-649\ntations, 2023.650\n[67] E. Zelikman, Y. Wu, J. Mu, N. Goodman, Star: Bootstrapping reasoning with reasoning,651\nAdvances in Neural Information Processing Systems 35 (2022) 15476–15488.652\n[68] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, A. Anandkumar, Voyager:653\nAn open-ended embodied agent with large language models, arXiv preprint arXiv:2305.16291654\n(2023).655\n[69] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta,656\nH. Michalewski, et al., Natural language to code generation in interactive data science note-657\nbooks, arXiv preprint arXiv:2212.09248 (2022).658\n19\nACCELERATED ARTICLE PREVIEW\n[70] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, X. V. Lin, Lever: Learning to659\nverify language-to-code generation with execution, in: International Conference on Machine660\nLearning, PMLR, 2023, pp. 26106–26128.661\n[71] S. Zhou, U. Alon, F. F. Xu, Z. Jiang, G. Neubig, Docprompting: Generating code by retrieving662\nthe docs, in: International Conference on Learning Representations, 2022.663\n[72] W. Banzhaf, P. Nordin, R. E. Keller, F. D. Francone, Genetic programming: an introduction:664\non the automatic evolution of computer programs and its applications, Morgan Kaufmann665\nPublishers Inc., 1998.666\n[73] W. B. Langdon, R. Poli, Foundations of genetic programming, Springer Science & Business667\nMedia, 2013.668\n[74] H. Ma, A. Narayanaswamy, P. Riley, L. Li, Evolving symbolic density functionals, Science669\nAdvances 8 (2022).670\n[75] M. Schmidt, H. Lipson, Distilling free-form natural laws from experimental data, science 324671\n(2009) 81–85.672\n[76] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong, C.-J.673\nHsieh, et al., Symbolic discovery of optimization algorithms, arXiv preprint arXiv:2302.06675674\n(2023).675\n[77] J. R. Koza, Genetic programming II: automatic discovery of reusable programs, MIT press,676\n1994.677\n[78] R. Salustowicz, J. Schmidhuber, Probabilistic incremental program evolution, Evolutionary678\ncomputation 5 (1997) 123–141.679\n[79] E. Burke, G. Kendall, J. Newall, E. Hart, P. Ross, S. Schulenburg, Hyper-heuristics: An680\nemerging direction in modern search technology, Handbook of metaheuristics (2003) 457–474.681\n[80] P. Ross, Hyper-heuristics, Search methodologies: introductory tutorials in optimization and682\ndecision support techniques (2005) 529–556.683\n[81] E. K. Burke, M. Gendreau, M. Hyde, G. Kendall, G. Ochoa, E. ¨Ozcan, R. Qu, Hyper-heuristics:684\nA survey of the state of the art, Journal of the Operational Research Society 64 (2013) 1695–685\n1724.686\n[82] E. K. Burke, M. R. Hyde, G. Kendall, Evolving bin packing heuristics with genetic program-687\nming, in: International Conference on Parallel Problem Solving from Nature, Springer, 2006,688\npp. 860–869.689\n[83] E. K. Burke, M. R. Hyde, G. Kendall, J. Woodward, Automatic heuristic generation with690\ngenetic programming: evolving a jack-of-all-trades or a master of one, in: Proceedings of the691\n9th annual conference on Genetic and evolutionary computation, 2007, pp. 1559–1565.692\n[84] E. K. Burke, M. R. Hyde, G. Kendall, Providing a memory mechanism to enhance the evo-693\nlutionary design of heuristics, in: IEEE Congress on Evolutionary Computation, IEEE, 2010,694\npp. 1–8.695\n20\nACCELERATED ARTICLE PREVIEW\n[85] E. K. Burke, M. Hyde, G. Kendall, J. R. Woodward, The scalability of evolved on line bin696\npacking heuristics, in: 2007 IEEE Congress on Evolutionary Computation, IEEE, 2007, pp.697\n2530–2537.698\n[86] R. Bunel, A. Desmaison, P. Kohli, P. H. Torr, M. P. Kumar, Learning to superoptimize699\nprograms, in: International Conference on Learning Representations, 2017.700\n[87] E. Schkufza, R. Sharma, A. Aiken, Stochastic superoptimization, ACM SIGARCH Computer701\nArchitecture News 41 (2013) 305–316.702\n[88] A. Shypula, P. Yin, J. Lacomis, C. L. Goues, E. Schwartz, G. Neubig, Learning to superoptimize703\nreal-world programs, in: Deep Learning for Code Workshop (ICLR 2022 Workshop), 2022.704\n[89] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov,705\nF. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al., Discovering faster matrix multiplication706\nalgorithms with reinforcement learning, Nature 610 (2022) 47–53.707\n[90] D. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal,708\nJ.-B. Lespiau, A. Ahern, et al., Faster sorting algorithms discovered using deep reinforcement709\nlearning, Nature 618 (2023) 257–263.710\n[91] F. Yang, G. Barth-Maron, P. Sta´ nczyk, M. Hoffman, S. Liu, M. Kroiss, A. Pope, A. Rrustemi,711\nLaunchpad: a programming model for distributed machine learning research, arXiv preprint712\narXiv:2106.04516 (2021).713\n21\nACCELERATED ARTICLE PREVIEW\nAcknowledgments. We would like to thank Rohan Anil, Vlad Feinberg, Emanuel Taropa, Thomas714\nHubert, Julian Schrittwieser, Sebastian Nowozin for their LLM support; Tom Schaul, Chrisantha715\nFernando, Andre Barreto, Prateek Gupta for discussions on evolutionary algorithms; Michael Fig-716\nurnov and Taylan Cemgil for reviewing the paper; Federico Piccinini, Sultan Kenjeyev for their717\nsupport on job scheduling, Sam Blackwell for technical support; Olaf Ronneberger, Felix Gimeno,718\nBlanca Huergo, Abbas Mehrabian and Ankit Anand for useful advice; George Holland for program719\nmanagement support.720\nAuthor Contributions. BRP conceived the project with help from AF and PK. AF scoped721\nproblems and developed project vision. BRP and AN developed the initial FunSearch codebase.722\nAN, BRP, M. Balog, FR, M. Barekatain, ED, AF implemented and refined the different components723\nof the system. M. Barekatain, AN imported and experimented with LLMs. M. Barekatain, AN, M.724\nBalog worked on evaluating, debugging, and improving the efficiency of experiments. M. Balog,725\nM. Barekatain, BRP, AN, AF, OF, JE contributed to the cap set problem. MPK, M. Balog,726\nJE researched and analyzed results about the admissible sets problem. ED, M. Barekatain, PW727\ncontributed to the online bin packing problem. FR, OF researched and did experiments on other728\nproblems (Shannon capacity and corners problem), PK contributed technical advice and ideas. AF,729\nBRP, ED, FR, MPK, M. Balog, AN, JE, M. Barekatain wrote the paper. These authors contributed730\nequally: BRP, M. Barekatain, AN, M. Balog, MPK, ED, FR, AF.731\nCorresponding authors. Correspondence to Bernardino Romera-Paredes (brp@google.com), Push-732\nmeet Kohli (pushmeet@google.com) or Alhussein Fawzi (afawzi@google.com).733\nCompeting interests. The authors of the paper are planning to file a patent application relating734\nto subject matter contained in this paper in the name of Google DeepMind.735\nAdditional information. Supplementary Information is available for this paper.736\n22\nACCELERATED ARTICLE PREVIEW\nExtended Data Figure 1: Example of best-shot prompting, based on the skeleton from\nFigure 2 (a). The prompt includes k = 2 implementations sampled from the programs database,\nwith higher-scoring implementations being more likely to be included.\nExtended Data Figure 2: Evolutionary method. The initial programs are separated into\nislands and each of them are evolved separately. After a number of iterations, the islands with\nthe worst score are wiped and the best program from the islands with the best score are placed in\nthe empty islands. Evolution then proceeds separately again until the next reset. This process is\nrepeated until termination.\nExtended Data Figure 3: Program clusters within islands. Within each island, programs are\ngrouped into clusters based on their signature (i.e., their scores on several inputs). We first sample\nclusters, favoring the ones with higher score. Within the chosen clusters, we sample a program,\nfavoring shorter programs. The sampled programs are used to prompt the LLM which generates a\nnew program. If the new program is correct, it is added to the island, either in an existing cluster\nor a new one if its signature was not yet present.\n23\nACCELERATED ARTICLE PREVIEW\n??\nPre-trained LLM\nEvaluation\nPrograms\ndatabase\nSpecification\n?\n?\nPrompt\nFunSearch\nNovel program\nACCELERATED ARTICLE PREVIEW\n\"\"\"Finds large cap sets.\"\"\"\nimport numpy as np\nimport utils_capset\n# Function to be executed by FunSearch.\ndef main(n):\n\"\"\"Runs `solve` on `n`-dimensional cap set and\nevaluates\nthe output.\"\"\"֒→\ns\nolution = solve(n)\nreturn evaluate(solution, n)\ndef evaluate(candidate_set, n):\n\"\"\"Returns size of candidate_set if it is a cap\nset,\nNone otherwise.\"\"\"֒→\nif utils_capset.is_capset(candidate_set, n):\nreturn len(candidate_set)\nelse:\nreturn None\nd\nef solve(n):\n\"\"\"Builds a cap set of dimension `n` using\n`priority`\nfunction.\"\"\"֒→\nPrecompute all priority scores.\nelements = utils_capset.get_all_elements(n)\ns\ncores = [priority(el, n) for el in elements]\n# Sort elements according to the scores.\nelements = elements[np.argsort(scores,\nk\nind='stable')[::-1]]֒→\nBuild `capset` greedily, using scores for\nprioritization.֒→\nc\napset = []\nfor element in elements:\nif utils_capset.can_be_added(element, capset):\nc\napset.append(element)\nreturn capset\n# Function to be evolved by FunSearch.\ndef priority(element, n):\n\"\"\"Returns the priority with which we want to add\n`element`\nto the cap set.\"\"\"֒→\nreturn 0.0\n(a) C ap set.\n\"\"\"Finds good assignment for online 1d bin\npacking.\"\"\"֒→\nimport numpy as np\nimport utils_packing\n# Function to be executed by FunSearch.\ndef main(problem):\n\"\"\"Runs `solve` on online 1d bin packing instance,\nand\nevaluates the output.\"\"\"֒→\nb\nins = problem.bins\n# Packs `problem.items` into `bins` online.\nfor item in problem.items:\n# Extract bins that have space to fit item.\nvalid_bin_indices =\nutils_packing.get_valid_bin_indices(item,\nb\nins)\n֒→\n֒→\nbest_index = solve(item,\nb\nins[valid_bin_indices])֒→\nAdd item to the selected bin.\nbins[valid_bin_indices[best_index]] -= item\nreturn evaluate(bins, problem)\ndef evaluate(bins, problem):\n\"\"\"Returns the negative of the number of bins\nrequired\nto pack items in `problem`.\"\"\"֒→\nif utils_packing.is_valid_packing(bins, problem):\nreturn -utils_packing.count_used_bins(bins,\np\nroblem)֒→\nelse:\nreturn None\nd\nef solve(item, bins):\n\"\"\"Selects the bin with the highest value according\nto\n`heuristic`.\"\"\"֒→\ns\ncores = heuristic(item, bins)\nreturn np.argmax(scores)\n# Function to be evolved by FunSearch.\ndef heuristic(item, bins):\n\"\"\"Returns priority with which we want to add\n`item`\nto each bin.\"\"\"֒→\nreturn -(bins - item)\n(b) O nline bin packing.\nACCELERATED ARTICLE PREVIEW\nACCELERATED ARTICLE PREVIEW\nn 3 4 5 6 7 8\nBest known 9 20 45 112 236 496\nFunSearch 9 20 45 112 236 512\n(a)\ndef priority(el: tuple[int, ...],\nn\n: int) -> float:֒→\ns\ncore = n\ni\nn_el = 0\nel_count = el.count(0)\nif el_count == 0:\ns\ncore += n ** 2\nif el[1] == el[-1]:\ns\ncore *= 1.5\nif el[2] == el[-2]:\ns\ncore *= 1.5\nif el[3] == el[-3]:\ns\ncore *= 1.5\nelse:\nif el[1] == el[-1]:\ns\ncore *= 0.5\nif el[2] == el[-2]:\ns\ncore *= 0.5\nfor e in el:\nif e == 0:\nif in_el == 0:\ns\ncore *= n * 0.5\nelif in_el == el_count - 1:\ns\ncore *= 0.5\nelse:\ns\ncore *= n * 0.5 ** in_el\ni\nn_el += 1\nelse:\ns\ncore += 1\nif el[1] == el[-1]:\ns\ncore *= 1.5\nif el[2] == el[-2]:\ns\ncore *= 1.5\nreturn score\n(b)\ndef build_512_cap() -> list[tuple[int, ...]]:\n\"\"\"Returns a cap set of size 512 in `n=8` dimensions.\"\"\"\nn = 8\nV = np.array(list(itertools.product(range(3), repeat=n)), dtype=np.int32)\ns\nupport = lambda v: tuple(i for i in range(n) if v[i] != 0)\nr\neflections = lambda v: sum(1 for i in range(1, n // 2) if v[i] == v[-i])\n# Add all 128 weight-8 vectors that have >= 2 reflections.\nweight8_vectors = [v for v in V\nif np.count_nonzero(v) == 8 # Weight is 8.\nand reflections(v) >= 2] # At least 2 reflections.\n#\nAdd all 128 weight-4 vectors that have specific support.\nsupports_16 = [(0, 1, 2, 3), (0, 1, 2, 5), (0, 3, 6, 7), (0, 5, 6, 7),\n(1, 3, 4, 6), (1, 4, 5, 6), (2, 3, 4, 7), (2, 4, 5, 7)]\nw\neight4_vectors = [v for v in V\nif support(v) in supports_16]\n# Add all 128 weight-4 vectors with specific support and 1 reflection.\nsupports_8 = [(0, 1, 2, 7), (0, 1, 2, 6), (0, 1, 3, 7), (0, 1, 6, 7),\n(0, 1, 5, 7), (0, 2, 3, 6), (0, 2, 6, 7), (0, 2, 5, 6),\n(1, 2, 4, 7), (1, 2, 4, 6), (1, 3, 4, 7), (1, 4, 6, 7),\n(1, 4, 5, 7), (2, 3, 4, 6), (2, 4, 6, 7), (2, 4, 5, 6)]\nw\neight4_vectors_2 = [v for v in V\nif support(v) in supports_8\nand reflections(v) == 1] # Exactly 1 reflection.\n#\nAdd 128 weight-5 vectors with <= 1 reflections and one more condition.\nallowed_zeros = [(0, 4, 7), (0, 2, 4), (0, 1, 4), (0, 4, 6),\n(1, 2, 6), (2, 6, 7), (1, 2, 7), (1, 6, 7)]\nw\neight5_vectors = [\nv for v in V\nif tuple(i for i in range(n) if v[i] == 0) in allowed_zeros\nand reflections(v) <= 1 # At most 1 reflection.\nand (v[1] * v[7]) % 3 != 1 and (v[2] * v[6]) % 3 != 1]\nreturn weight8_vectors + weight4_vectors + weight4_vectors_2 +\nweight5_vectors֒→\nc)\nACCELERATED ARTICLE PREVIEW\nBound\non C\nAdmissible set\ningredient Source\n2.2 101 I( 90 ,89) (Calderbank and Fishburn, 1994)\n2.2 173 I( 10 ,5) (Edel, 2004)\n2.2 180 I( 11 ,7) (Tyrrell, 2022)\n2.2 184 I( 12 ,7) FunSearch\n2.2 194 I( 15 ,10) FunSearch\n2.2 202 A( 24 ,17) FunSearch\n(a)\ndef priority(el: tuple[int, ...], n: int, w: int) -> float:\ns\ncore = 0.0\nfor i in range(n):\nif el[i] == 1:\ns\ncore -= 0.9 ** ( i % 4 )\nif el[i] == 2:\ns\ncore -= 0.98 ** (30 - ( i % 4 ))\nif el[i] == 1 and el[i - 4] == 1:\ns\ncore -= 0.98 ** (30 - ( i % 4 ))\nif el[i] == 2 and el[i - 4] != 0:\ns\ncore -= 0.98 ** (30 - ( i % 4 ))\nif el[i] == 2 and el[i - 4] == 1 and el[i - 8] == 2:\ns\ncore -= 0.98 ** (30 - ( i % 4 ))\ns\ncore -= 6.3\nif el[i] == 2 and el[i - 4] == 2 and el[i - 8] == 1:\ns\ncore -= 0.98 ** (30 - ( i % 4 ))\nif el[i] == 2 and el[i - 4] == 1 and el[i - 8] == 1:\ns\ncore -= 6.3\nif el[i] == 2 and el[i - 4] == 0 and el[i - 8] == 2:\ns\ncore -= 6.3\nif el[i] == 1 and el[i - 4] == 1 and el[i - 8] == 0:\ns\ncore -= 2.2\nreturn score\n(b)\nACCELERATED ARTICLE PREVIEW\ndef heuristic(item: float, bins: np.ndarray) -> np.ndarray:\n\"\"\"Online bin packing heuristic discovered with FunSearch.\"\"\"\nscore = 1000 * np.ones(bins.shape)\n# Penalize bins with large capacities.\nscore -= bins * (bins - item)\n# Extract index of bin with best fit.\nindex = np.argmin(bins)\n# Scale score of best fit bin by item size.\nscore[index] *= item\n# Penalize best fit bin if fit is not tight.\nscore[index] -= (bins[index] - item)**4\nreturn score\nACCELERATED ARTICLE PREVIEW\n\"\"\"Finds large cap sets.\"\"\"\nimport numpy as np\nimport utils_capset\ndef priority_v0(element, n):\n\"\"\"Returns the priority with which we want to add `element` to the cap set.\"\"\"\n#######\n# Code from lowest-scoring sampled program.\nreturn ...\n#######\ndef priority_v1(element, n):\n\"\"\"Improved version of `priority_v0`.\"\"\"\n#######\n# Code from highest-scoring sampled program.\nreturn ...\n#######\ndef priority_v2(element, n):\n\"\"\"Improved version of `priority_v1`.\"\"\"\nExtended Data Fig. 1\nACCELERATED ARTICLE PREVIEW\nExtended Data Fig. 2\nACCELERATED ARTICLE PREVIEW\nExtended Data Fig. 3\nACCELERATED ARTICLE PREVIEW",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.533184289932251
    },
    {
      "name": "Programming language",
      "score": 0.37348881363868713
    },
    {
      "name": "Data science",
      "score": 0.3207654058933258
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210113297",
      "name": "Google (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1326498283",
      "name": "Institut national de recherche en sciences et technologies du numérique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210144566",
      "name": "Laboratoire de l'Informatique du Parallélisme",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I100532134",
      "name": "Université Claude Bernard Lyon 1",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I113428412",
      "name": "École Normale Supérieure de Lyon",
      "country": "FR"
    }
  ],
  "cited_by": 214
}