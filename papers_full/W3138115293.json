{
    "title": "Multi-view Analysis of Unregistered Medical Images Using Cross-View Transformers",
    "url": "https://openalex.org/W3138115293",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4283151746",
            "name": "van Tulder, Gijs",
            "affiliations": [
                "Radboud University Nijmegen"
            ]
        },
        {
            "id": "https://openalex.org/A2149017131",
            "name": "Tong Yao",
            "affiliations": [
                "Radboud University Nijmegen"
            ]
        },
        {
            "id": "https://openalex.org/A2744922180",
            "name": "Marchiori, Elena",
            "affiliations": [
                "Radboud University Nijmegen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2280650650",
        "https://openalex.org/W2580611662",
        "https://openalex.org/W2111096131",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W2776937175",
        "https://openalex.org/W3035230119",
        "https://openalex.org/W2984544647",
        "https://openalex.org/W2972101069",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2793701907",
        "https://openalex.org/W2980030301",
        "https://openalex.org/W3015397771"
    ],
    "abstract": null,
    "full_text": "Multi-view Analysis of Unregistered Medical\nImages Using Cross-View Transformers\nGijs van Tulder, Yao Tong, and Elena Marchiori\nData Science Group, Faculty of Science\nRadboud University, Nijmegen, the Netherlands\nAbstract. Multi-view medical image analysis often depends on the com-\nbination of information from multiple views. However, diﬀerences in per-\nspective or other forms of misalignment can make it diﬃcult to combine\nviews eﬀectively, as registration is not always possible. Without regis-\ntration, views can only be combined at a global feature level, by joining\nfeature vectors after global pooling. We present a novel cross-view trans-\nformer method to transfer information between unregistered views at the\nlevel of spatial feature maps. We demonstrate this method on multi-view\nmammography and chest X-ray datasets. On both datasets, we ﬁnd that\na cross-view transformer that links spatial feature maps can outperform\na baseline model that joins feature vectors after global pooling.\nKeywords: Multi-view medical images · Transformers · Attention\n1 Introduction\nMany medical imaging tasks use data from multiple views or modalities, but\nit can be diﬃcult to combine those eﬀectively. While multi-modal images can\nusually be registered and treated as multiple input channels in a neural net-\nwork, images from diﬀerent views can be diﬃcult to register correctly (e.g., [2]).\nTherefore, most multi-view models process views separately and only combine\nthem after global pooling, which removes any local correlations between views.\nIf these local correlations are important for the interpretation of the images,\nmodels could be improved by linking views at an earlier, spatial feature level.\nWe discuss two example tasks: mammography classiﬁcation with craniocau-\ndal (CC) and mediolateral oblique (MLO) views, and chest X-ray classiﬁcation\nwith frontal and lateral views. In both applications, multi-view models can out-\nperform single-view models (e.g., [3,16]) However, the diﬀerent perspectives make\nregistration challenging and make a channel-based approach unsuitable.\nWe propose a method that can link unregistered views at the level of spatial\nfeature maps. Inspired by the attention-based transformer models [13] that model\nlinks between distant parts of a sequence or image, our model uses attention to\nlink relevant areas between views. We apply this transformer to the intermediate\nfeature maps produced by a CNN. Based on a trainable attention mechanism,\nthe model retrieves features from one view and transfers them to the other,\nwhere they can be used to add additional context to the original view.\n© Springer Nature Switzerland 2021\nM. de Bruijne et al. (Eds.): MICCAI 2021, LNCS 12903, pp. 104–113, 2021.\nThe ﬁnal publication is available at Springer via https://doi.org/10.1007/978-3-030-87199-4 10\narXiv:2103.11390v2  [cs.CV]  23 Sep 2021\nMulti-view Analysis Using Cross-View Transformers 105\nOur approach does not require pixel-wise correspondences – it compares all\npixels in the feature maps from view A to all pixels in the feature maps from view\nB – but combines views using a trainable attention model. By applying this to\nfeature maps instead of directly to the input, we allow the model to link higher-\nlevel features and reduce computational complexity. Since linking all pixel pairs\ncan still be expensive, we additionally investigate an alternative implementation\nthat groups pixels with similar features in visual tokens [15].\nIn this paper, we present these novel pixel-wise and token-based cross-view\ntransformer approaches and apply them to two public datasets. Although com-\nbining features after global pooling is a relatively common way to handle multi-\nview information with unregistered medical images, to our knowledge there are\nno methods that use a transformer-based approach to do this at the spatial\nfeature level. The proposed model can be easily embedded as a module within\nbaseline multi-view architectures that combine views after global pooling. We\nevaluate our method on the CBIS-DDSM mammography dataset [5,7] and the\nCheXpert chest X-ray dataset [6]. Based on our experiments, we think that early\ncombination of features can improve the classiﬁcation of multi-view images.\n2 Related Work\nThere are many works on multi-view classiﬁcation of medical images. In this\nsection we focus on methods applied to mammography and chest X-ray data.\nMost methods combine views at a late stage, usually by concatenating feature\nvectors obtained from the diﬀerent views, followed by a fully connected part to\nmake a ﬁnal prediction. We use this approach in our multi-view baseline.\nCombining features at a global level is common for mammography images,\nwhich are diﬃcult to register [2]. For example, Bekker et al. [1] combined binary\npredictions from view-speciﬁc classiﬁers. Carneiro et al. [2] combined feature\nfrom view-speciﬁc CNN branches after global pooling. Wu et al. [16] discuss\nmultiple ways to combine views in a single network, all with view-speciﬁc con-\nvolution branches. Similar architectures were proposed elsewhere (e.g., [10,12]).\nOther works combine views at a regional level. Wang et al. [14] proposed a\nregion-based three-step method: after extracting mass ROIs (regions of interest)\nfrom each view, they used a CNN with an attention-driven approach to extract\nview-speciﬁc features from each ROI. Finally, the features from both views are\ncombined with additional clinical features by an LSTM-based fusion model. Sim-\nilarly, Ma et al. [9] proposed using Faster RCNNs to detect ROIs in each view,\nwhich they then converted to feature vectors and combined in a multi-view net-\nwork. Liu et al. [8] used a model with bipartite graph convolution to link views\nbased on pseudo landmarks, while satisfying geometric constraints.\nMost similar to our approach is the work by Zhao et al. [17], who applied\na joint attention mechanism that combined two views or two sides (left and\nright breasts) to produce channel-wise and spatial attention maps that high-\nlight asymmetric regions. The outputs of the attention-weighted, view-speciﬁc\nbranches are pooled and concatenated to produce a ﬁnal classiﬁcation. Diﬀerent\n106 G. van Tulder et al.\nInput view A\nPrediction\nInput view B\nPrediction\nResNet blocks 1–4\nGlobal average pooling\nFully connected layer\n(a) Single view\nResNet blocks 1–4\nInput view A Input view B\nJoint prediction\nGlobal average pooling\nFully connected layer\nConcatenated features\n (b) Late join\nInput view A Input view B\nJoint prediction\nResNet blocks 1–3\nResNet block 4\nGlobal average pooling\nFully connected layer\nConcatenated features\nCross-view\ntransformers (c) Cross-view transformer\nFig. 1: Schematic overview of the three architectures.\nfrom our approach, which transfers feature values between views, Zhao et al. use\ncross-view information only to compute cross-view attention weights.\nFor chest X-rays, many datasets only include the frontal view, since the\nlateral view is harder to read, is mostly useful for speciﬁc diagnoses, and is\nsometimes replaced by a CT scan [3]. Rubin et al. [11] evaluated a model with\nview-speciﬁc convolution branches, global average pooling and a shared fully\nconnected layer, and report that combining frontal and lateral views improved\nclassiﬁcation performance. Recently, Hashir et al. [3] compared several multi-\nview models on a large chest X-ray dataset, showing that while multi-view data\nis useful for some diagnosis tasks, the frontal view can be suﬃcient for others.\n3 Methods\nIn this section, we describe two baseline models and our cross-view transformer\nmodels. All models are designed for a classiﬁcation task with unregistered, dual-\nview image pairs. We use a ResNet-based [4] architecture for the view-speciﬁc\nconvolution branches, similar to what is used in related work (e.g., [6,16]).\n3.1 Baseline Models\nOur single-view baseline (Fig. 1a) follows the basic ResNet architecture. The\nnetwork consists of a stack of ResNet blocks with convolution and pooling layers,\nfollowed by global average pooling and a fully connected part that computes the\nﬁnal output. Our late-join baseline (Fig. 1b) extends this model to multiple\nunregistered views, by using a separate convolution branch for each view. After\nglobal pooling, the feature vectors for all views are concatenated and fed into\na shared fully connected part to compute the prediction. This is similar to how\nmulti-view data is commonly combined in other work, such as [16].\nMulti-view Analysis Using Cross-View Transformers 107\nDepending on the type of data, the view-speciﬁc branches can be linked\nthrough weight sharing. This can be a useful regularization if the same low-level\nfeatures can be used for both views, but might be too restrictive if the views\nhave very diﬀerent appearances. We do not use this for our models.\n3.2 Cross-View Transformer Models\nUsing view-speciﬁc branches and combining feature vectors after global pooling\nworks for unregistered views, but limits the model to learning global correlations.\nWe propose a transformer-based method that links views at feature map level,\nwithout requiring pixel-wise correspondences. Instead of the self-attention used\nin standard transformers [13] to transfer information within a single sequence, we\nuse cross-view attention to transfer information between views. This approach\ncan be used in any dual-view model, such as our late-join baseline.\nThe cross-view transformer (Fig. 1c) works on the intermediate level of the\nconvolutional part of the model. In our case, we apply the module to the feature\nmaps after the third ResNet block, leaving one ResNet block before global pool-\ning. At this level, we use the cross-view transformer to transfer features from the\nsource view to the target view. We make the model bidirectional by applying a\nsecond cross-view transformer module in the opposite direction.\nWe will deﬁne two variants of this model: a pixel-wise variant that links pixels\nfrom the source and target views, and a token-based variant in which the pixels\nin the target view are linked to visual tokens in the source view. For conciseness,\nwe will use the term ‘pixels’ to refer to pixels in the intermediate feature maps.\nCross-View Attention.We use a multi-head attention model [13] with scaled\ndot-product attention. For each attention head, we use a 1 ×1 convolution with\nview-speciﬁc weights to compute an embedding for the source and target pixels.\nWe reshape the embedded feature maps for the target view to a query matrix\nQ ∈Rn×d and the reshape feature maps for the source view to a key matrix\nK ∈Rm×d, where d is the size of the embedding and m and n are the number\nof source and target pixels. We also reshape the original source feature maps to\nthe value matrix V ∈Rm×f , where f is the number of feature maps. Next, we\nuse the scaled dot-product attention function [13] to compute\nAttention(Q,K,V ) = softmax\n(QK⊤\n√\nd\n)\nV ∈Rn×f . (1)\nFor each target pixel, this computes a weighted sum of the features from the\nsource view, resulting in f new features per attention head. We reshape the\noutput to m×f feature maps with the shape of the target feature maps and\napply 1 ×1 convolution to reduce these to f attention-based feature maps.\nWe combine the attention-based feature maps a with the original feature\nmaps x of the target data to obtain the combined feature maps y:\ny= LayerNorm(x+ Dropout(Linear(a))), (2)\n108 G. van Tulder et al.\nwhere LayerNorm is layer normalization, Dropout applies dropout, and Linear a\n1×1 convolution that maps the attention features to the feature space of x. The\nresulting feature maps y are used as the input for the following ResNet block.\nUnlike standard transformer networks [13], we do not include a positional\nencoding. This encoding encodes the relative location of each pixel, allowing the\nmodel to distinguish between nearby and faraway pixels within a single image.\nThis is useful in self-attention for natural images, but is less suitable for cross-\nview attention. However, an alternative positional encoding that works across\nviews might help the network to exploit spatial constraints. We leave this for\nfuture work.\nSemantic Visual Tokens.Computing the attention between all pairs of source\nand target pixels can be computationally expensive, even when it is applied to\nthe smaller feature maps at the later stages of the CNN. We therefore evaluate a\nvariant that uses the tokenization method from [15] to replace the source pixels\nwith a smaller number of visual tokens, by grouping semantically related pixels.\nWe apply a three-layer tokenization procedure to the source feature maps. In\nthe ﬁrst layer, given ﬂattened feature maps X ∈Rm×f , where m is the number\nof source pixels and f is the number of feature maps, we compute tokens T:\nT = softmaxm (XWA)⊤X. (3)\nThe softmax over the spatial dimension uses the tokenizer weights WA ∈Rf,L\nto compute a spatial attention map, which is then used to compute a weighted\nsum of features for each of the L tokens in T ∈RL×f .\nIn the second layer, we use the previous tokens Tin to obtain new tokens T:\nWR = TinWT→R (4)\nT = softmaxm (XWR)⊤X. (5)\nwhere WT→R ∈Rf×f . We repeat this with a diﬀerent WT→R in the third\ntokenization layer to obtain the ﬁnal set of tokensT. We use these tokens instead\nof the source pixels in our token-based cross-view transformer.\n4 Data\nCBIS-DDSM. The CBIS-DDSM [5,7] is a public mammography dataset with\ncraniocaudal (CC) and mediolateral-oblique (MLO) views with manual annota-\ntions. We solve a binary classiﬁcation problem on the scans with mass abnor-\nmalities, predicting benign vs. malignant for each CC/MLO pair. We created\nﬁve subsets for cross-validation, using stratiﬁed sampling while ensuring that all\nscans for a patient remain in the same subset. In total, we used image pairs of\n708 breasts (636 unique patients), with approximately 46% labelled malignant.\nDuring preprocessing, we cropped the scans using the method described by\nWu et al. [16], using thresholding to position a ﬁxed-size cropping window that\nMulti-view Analysis Using Cross-View Transformers 109\nincludes the breast but excludes most of the empty background. We downsam-\npled the cropped images to 1/16th of the original resolution to obtain images of\n305 ×188 pixels. We normalized the intensities to µ = 0 and σ = 1, measured\non the nonzero foreground pixels of each scan.\nCheXpert. The CheXpert dataset [6] is a large public dataset of frontal and\nlateral chest X-ray scans, annotated for 13 diﬀerent observations with labels\nnegative, positive, uncertain, or unknown (see supplement). We used the down-\nsampled version of the dataset as provided on the website. We selected the visits\nwith complete frontal and lateral views and divided the patients in random sub-\nsets for training (23628 samples for 16810 unique patients), validation (3915s,\n2802p) and testing (3870s, 2802p). We normalized the images toµ= 0 and σ= 1\nand used zero-padding to obtain a constant size of 390×390 pixels for each view.\nTo handle the uncertain and unknown labels, we followed [6] and used a sin-\ngle network with a three-class softmax output for each task (negative/uncertain/\npositive). We excluded the samples with an unknown label from the loss compu-\ntation for that speciﬁc task. At test time, following [6], we remove the uncertain\nlabel and compute the softmax only for the negative and positive outputs.\n5 Experiments\nModels. We compare four models: the single-view model, the late-join model,\nand the token-based and pixel-based cross-view transformers. All models use\nthe same ResNet-18 architecture [4] for the convolution and pooling blocks up\nto the global average pooling layer. We use pre-trained weights on ImageNet, as\nprovided by PyTorch. After global average pooling, we concatenate the feature\nvectors for both views and use this as input for a single fully connected layer\nthat computes the output. (See the supplementary material for a detailed view.)\nIn the cross-view transformers, we use bidirectional attention and apply the\ncross-view transformer before the ﬁnal ResNet block, adding the transformer\nfeatures to the input for the ﬁnal convolution and pooling layers. For the CBIS-\nDDSM dataset, we evaluated models with 12 or 18 attention heads and 16, 32 or\n48 tokens, as well as the pixel-based transformer. For the CheXpert dataset, we\nuse token-based transformers with 6 or 12 attention heads and 16 or 32 tokens.\nIn all cases, the embedding size is set to 32 features per head.\nImplementation. We implemented the models in PyTorch1 and trained using\nthe Adam optimizer with cosine learning rate annealing from 0.0001 to 0.000001,\nwith linear warm-up in the ﬁrst epochs. We used rotation, scaling, translation,\nﬂipping and elastic deformations as data augmentation.\nOn the CBIS-DDSM dataset, we used a weighted binary cross-entropy loss to\ncorrect for the slight class imbalance. We trained for 300 epochs (30 warm-up),\n1 The code for the experiments is available at\nhttps://vantulder.net/code/2021/miccai-transformers/.\n110 G. van Tulder et al.\nTable 1: Area under the ROC curve for the CBIS-DDSM dataset. Mean and\nstandard deviation computed over three runs. p-values for a two-sided Wilcoxon\nsigned-rank test against the late-join baseline model.\nModel Views ROC-AUC ± std.dev. p-value\nSingle view CC 0 .750 ± 0.007 0.005\nMLO 0 .763 ± 0.003 0.036\nLate join CC + MLO 0 .788 ± 0.008\nCross-view transformer (tokens) CC + MLO 0 .803 ± 0.007 0.061\nCross-view transformer (pixels) CC + MLO 0 .801 ± 0.003 0.006\nTable 2: Area under the ROC curve for tasks in the CheXpert dataset. Mean\nand standard deviation computed over four runs.\nSingle view Cross-view\nTask Frontal Lateral Late join (token-based)\nOverall 0 .827 ± 0.007 0 .817 ± 0.009 0 .829 ± 0.010 0 .834 ± 0.002\nAtelectasis 0 .812 ± 0.009 0 .809 ± 0.004 0 .812 ± 0.016 0 .833 ± 0.009\nCardiomegaly 0 .924 ± 0.004 0 .902 ± 0.003 0 .919 ± 0.003 0 .925 ± 0.004\nConsolidation 0 .863 ± 0.005 0 .848 ± 0.006 0 .867 ± 0.004 0 .867 ± 0.004\nEdema 0 .882 ± 0.005 0 .861 ± 0.005 0 .889 ± 0.002 0 .889 ± 0.005\nEnlarged Cardiomed. 0 .812 ± 0.008 0 .796 ± 0.003 0 .814 ± 0.005 0 .810 ± 0.006\nFracture 0 .775 ± 0.003 0 .764 ± 0.018 0 .766 ± 0.019 0 .769 ± 0.013\nLung Lesion 0 .744 ± 0.013 0 .726 ± 0.010 0 .747 ± 0.018 0 .748 ± 0.007\nLung Opacity 0 .808 ± 0.005 0 .782 ± 0.006 0 .806 ± 0.008 0 .805 ± 0.004\nPleural Eﬀusion 0 .945 ± 0.001 0 .946 ± 0.001 0 .955 ± 0.002 0 .954 ± 0.001\nPleural Other 0 .789 ± 0.025 0 .808 ± 0.030 0 .786 ± 0.036 0 .803 ± 0.030\nPneumonia 0 .750 ± 0.004 0 .740 ± 0.009 0 .766 ± 0.011 0 .754 ± 0.008\nPneumothorax 0 .869 ± 0.003 0 .853 ± 0.004 0 .868 ± 0.004 0 .872 ± 0.002\nSupport Devices 0 .773 ± 0.006 0 .786 ± 0.015 0 .786 ± 0.007 0 .803 ± 0.013\nwhich was suﬃcient for all models to converge, and used the model of the ﬁnal\nepoch. We report the mean of three runs over all ﬁve folds.\nOn the CheXpert dataset, we optimized the unweighted cross-entropy loss\naveraged over all tasks, training for 60 epochs (6 warm-up) to account for the\nlarger dataset. We computed the AUC-ROC for each task separately and used\nthe performance on the validation set to choose the best epoch for each task.\nWe report the mean performance over four runs.\nWe ran all experiments on NVIDIA GeForce RTX 2080 Ti GPUs with 11GB\nVRAM. For the CBIS-DDSM dataset, median training times were approximately\n23 minutes for a single-view model, 36 minutes for a late-join model, and 37\nminutes for the cross-view transformers. For the much larger CheXpert dataset,\nwe trained for approximately 5 hours per single-view model, versus 10 hours for\nthe late-join and cross-view models.\nMulti-view Analysis Using Cross-View Transformers 111\n6 Results\nOn the CBIS-DDSM dataset (Table 1) the late-join baselines outperformed the\nsingle-view baselines. Adding the cross-view transformer improved the perfor-\nmance further, both for the token-based and pixel-wise variants. The transformer\nperformance was not very sensitive to the number of heads or tokens: all settings\nproduced similar results (see the table in the supplementary results).\nOn the CheXpert dataset, the cross-view transformer model also shows an\nimprovement over the baselines (Table 2). The improvement is visible for the\naverage performance over all tasks, with the cross-view model performing better\nthan the late-join and single-view frontal models. The single-view model with\nthe lateral view is less successful.\nFor individual chest X-ray tasks, the results are quite varied. For some tasks,\nsuch as atelectasis, cardiomegaly and support devices, the cross-view models\nshow an improvement over the late-join model. For others, such as consolidation\nand edema, the models are closer together. This is consistent with observations\nin other work [3]. In general, with a few exceptions, the cross-view model has a\nperformance that is equal to or better than the late-join models.\n7 Discussion and Conclusion\nThe common multi-view approach to merge views after global pooling restricts\nmodels to learning global correlations between views. This may be suﬃcient for\nsome applications, but a more local approach may be required for others. This is\nrelatively easy if the images are spatially aligned and can be treated as multiple\ninput channels, but is diﬃcult when there are diﬀerent perspectives or other\nmisalignments that make it impossible to register the images correctly.\nIn this paper, we proposed a cross-view transformer approach to link unreg-\nistered dual-view images based on feature maps. Our experiments on two unreg-\nistered multi-view datasets indicate that this approach can outperform a model\nthat links views on a global level. The cross-view transformer module is easy to\nintegrate in any multi-view model with view-speciﬁc convolution branches.\nWhether a cross-view transformer approach is useful depends on the appli-\ncation, since some tasks will beneﬁt more from multi-view information and local\ncorrelations than others. We can see an indication of this in the results for the\nchest X-ray data (Table 2), where the late-join and cross-view transformer mod-\nels sometimes do and sometimes do not have an advantage over the single-view\nmodels. This is consistent with results from Hashir et al. [3], who made similar\nobservations about multi-view features on a diﬀerent chest X-ray dataset.\nThe cross-view transformer mechanism can be computationally expensive.\nThis can be reduced by applying the transformer later in the network, when the\nnumber of pixels is smaller. The memory requirements can be reduced with an\neﬃcient gradient implementation, by recomputing the pairwise attention scores\non the ﬂy. Using the token-based approach further reduces the requirements. In\npractice, we found that the additional computational and memory requirements\nwere relatively limited, compared with those of the convolution layers.\n112 G. van Tulder et al.\nFor this paper we focussed on an evaluation of the cross-view transformer,\npresenting experiments with downsampled images and with the relatively small\nCBIS-DDSM dataset. While this allowed us to run more experiments, a better\nabsolute performance could be achieved with higher-resolution data and more\ntraining images. For mammography classiﬁcation, the state-of-the-art methods\n[16] use similar ResNet-based architectures, trained with diﬀerent and larger\ndatasets. For chest X-rays, the best-performing methods on the CheXpert dataset\n[6] use the full-resolution dataset and ensemble methods.\nIn summary, we presented a novel cross-view transformer approach that can\ntransfer information between views, by linking feature maps before global pool-\ning. On two datasets, we found that combining multi-view information on a\nspatial level can achieve better results than a model that merges features at an\nimage level. We believe this can be an interesting addition for models that need\nto learn inter-view correlations in applications with unregistered images.\nAcknowledgments\nThe research leading to these results is part of the project “MARBLE”, funded\nfrom the EFRO/OP-Oost under grant number PROJ-00887. Some of the exper-\niments were carried out on the Dutch national e-infrastructure with the support\nof SURF Cooperative.\nReferences\n1. Bekker, A.J., Shalhon, M., Greenspan, H., Goldberger, J.: Multi-View Probabilistic\nClassiﬁcation of Breast Microcalciﬁcations. IEEE Transactions on Medical Imaging\n35(2), 645–653 (Oct 2015). https://doi.org/10.1109/TMI.2015.2488019\n2. Carneiro, G., Nascimento, J., Bradley, A.P.: Deep learning models for classify-\ning mammogram exams containing unregistered multi-view images and segmen-\ntation maps of lesions. In: Zhou, S.K., Greenspan, H., Shen, D. (eds.) Deep\nLearning for Medical Image Analysis, pp. 321–339. Academic Press (Jan 2017).\nhttps://doi.org/10.1016/B978-0-12-810408-8.00019-5\n3. Hashir, M., Bertrand, H., Cohen, J.P.: Quantifying the Value of Lateral Views in\nDeep Learning for Chest X-rays. In: Medical Imaging with Deep Learning. pp.\n288–303. PMLR (Sep 2020)\n4. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.\nIn: CVPR. pp. 770–780 (2016). https://doi.org/10.3389/fpsyg.2013.00124\n5. Heath, M., Bowyer, K., Kopans, D., Moore, R., Jr, P.K.: The Digital Database for\nScreening Mammography (2001)\n6. Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund,\nH., Haghgoo, B., Ball, R., Shpanskaya, K., Seekins, J., Mong, D.A., Halabi, S.S.,\nSandberg, J.K., Jones, R., Larson, D.B., Langlotz, C.P., Patel, B.N., Lungren,\nM.P., Ng, A.Y.: CheXpert: A Large Chest Radiograph Dataset with Uncertainty\nLabels and Expert Comparison. arXiv:1901.07031 [cs, eess] (Jan 2019)\n7. Lee, R.S., Gimenez, F., Hoogi, A., Miyake, K.K., Gorovoy, M., Rubin, D.L.: A\ncurated mammography data set for use in computer-aided detection and diagnosis\nresearch. Scientiﬁc Data 4 (2017). https://doi.org/10.1038/sdata.2017.177\nMulti-view Analysis Using Cross-View Transformers 113\n8. Liu, Y., Zhang, F., Zhang, Q., Wang, S., Wang, Y., Yu, Y.: Cross-View Correspon-\ndence Reasoning Based on Bipartite Graph Convolutional Network for Mammo-\ngram Mass Detection. In: 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR). pp. 3811–3821. IEEE, Seattle, WA, USA (Jun 2020).\nhttps://doi.org/10.1109/CVPR42600.2020.00387\n9. Ma, J., Liang, S., Li, X., Li, H., Menze, B.H., Zhang, R., Zheng, W.S.: Cross-view\nRelation Networks for Mammogram Mass Detection. arXiv:1907.00528 [cs] (Jun\n2019)\n10. Nasir Khan, H., Shahid, A.R., Raza, B., Dar, A.H., Alquhayz, H.: Multi-\nView Feature Fusion Based Four Views Model for Mammogram Classiﬁcation\nUsing Convolutional Neural Network. IEEE Access 7, 165724–165733 (2019).\nhttps://doi.org/10.1109/ACCESS.2019.2953318\n11. Rubin, J., Sanghavi, D., Zhao, C., Lee, K., Qadir, A., Xu-Wilson, M.: Large Scale\nAutomated Reading of Frontal and Lateral Chest X-Rays using Dual Convolutional\nNeural Networks. arXiv:1804.07839 [cs, stat] (Apr 2018)\n12. Sun, L., Wang, J., Hu, Z., Xu, Y., Cui, Z.: Multi-View Convolutional Neural Net-\nworks for Mammographic Image Classiﬁcation. IEEE Access 7, 126273–126282\n(2019). https://doi.org/10.1109/ACCESS.2019.2939167\n13. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in Neural Information Pro-\ncessing Systems 30, 5998–6008 (2017)\n14. Wang, H., Feng, J., Zhang, Z., Su, H., Cui, L., He, H., Liu, L.:\nBreast mass classiﬁcation via deeply integrating the contextual informa-\ntion from multi-view data. Pattern Recognition 80, 42–52 (Aug 2018).\nhttps://doi.org/10.1016/j.patcog.2018.02.026\n15. Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., Tomizuka, M., Gonzalez, J.,\nKeutzer, K., Vajda, P.: Visual Transformers: Token-based Image Representation\nand Processing for Computer Vision. arXiv:2006.03677 [cs, eess] (Nov 2020)\n16. Wu, N., Phang, J., Park, J., Shen, Y., Huang, Z., Zorin, M., Jastrzebski, S., Fevry,\nT., Katsnelson, J., Kim, E., Wolfson, S., Parikh, U., Gaddam, S., Lin, L.L.Y.,\nHo, K., Weinstein, J.D., Reig, B., Gao, Y., Pysarenko, H.T.K., Lewin, A., Lee, J.,\nAirola, K., Mema, E., Chung, S., Hwang, E., Samreen, N., Kim, S.G., Heacock,\nL., Moy, L., Cho, K., Geras, K.J.: Deep Neural Networks Improve Radiologists’\nPerformance in Breast Cancer Screening. IEEE Transactions on Medical Imaging\n(2019). https://doi.org/10.1109/tmi.2019.2945514\n17. Zhao, X., Yu, L., Wang, X.: Cross-View Attention Network for Breast Cancer\nScreening from Multi-View Mammograms. In: ICASSP 2020 - 2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.\n1050–1054 (May 2020). https://doi.org/10.1109/ICASSP40776.2020.9054612\n114 G. van Tulder et al.\nA Supplementary material\nTask Negative Uncertain Positive Unknown\nAtelectasis 296 (0 %) 3738 (11 %) 3539 (11 %) 23840 (75 %)\nCardiomegaly 3175 (10 %) 1332 (4 %) 3472 (11 %) 23434 (74 %)\nConsolidation 8259 (26 %) 3243 (10 %) 1733 (5 %) 18178 (57 %)\nEdema 4678 (14 %) 1105 (3 %) 2437 (7 %) 23193 (73 %)\nEnlarged\nCardiomediastinum 6170 (19 %) 2068 (6 %) 1559 (4 %) 21616 (68 %)\nFracture 498 (1 %) 137 (0 %) 1540 (4 %) 29238 (93 %)\nLung Lesion 483 (1 %) 365 (1 %) 2091 (6 %) 28474 (90 %)\nLung Opacity 1512 (4 %) 1141 (3 %) 10969 (34 %) 17791 (56 %)\nPleural Eﬀusion 9850 (31 %) 2002 (6 %) 9002 (28 %) 10559 (33 %)\nPleural Other 107 (0 %) 822 (2 %) 996 (3 %) 29488 (93 %)\nPneumonia 899 (2 %) 2690 (8 %) 1318 (4 %) 26506 (84 %)\nPneumothorax 8293 (26 %) 428 (1 %) 1703 (5 %) 20989 (66 %)\nSupport Devices 773 (2 %) 156 (0 %) 8570 (27 %) 21914 (69 %)\nTable 3: Number of samples per class in the CheXpert dataset.\nMulti-view Analysis Using Cross-View Transformers 115\nLayer Input channels Output channels Kernel size Stride Padding\nConv 1 3 64 7 × 7 2 × 2 3 × 3\nBatchNorm 64 64\nReLU 64 64\nMaxPool 64 64 3 × 3 2 × 2 1 × 1\nResNet block 1 64 64 3 × 3 1 × 1\nResNet block 2 64 128 3 × 3 2 × 2 1 × 1\nResNet block 3 128 256 3 × 3 2 × 2 1 × 1\nResNet block 4 256 512 3 × 3 2 × 2 1 × 1\nGlobal average pooling 512 512 3 × 3 2 × 2 1 × 1\nFully connected layer 512 1\nSigmoid 1 1\nTable 4: Architecture of the single-view network based on ResNet-18. The other\nnetworks use the same convolutional architecture.\nAttention heads\n12 18\n16 tokens 0 .793 ± 0.002 0 .799 ± 0.018\n32 tokens 0 .796 ± 0.005 0 .798 ± 0.002\n48 tokens 0 .802 ± 0.002 0 .798 ± 0.007\nPixels 0 .799 ± 0.007 0 .804 ± 0.004\nTable 5: Area under the ROC curve for the CBIS-DDSM dataset for diﬀerent\nconﬁgurations of the token-based and pixel-wise cross-view transformers. Mean\nand standard deviation computed over three runs."
}