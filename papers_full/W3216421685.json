{
  "title": "DuDoTrans: Dual-Domain Transformer Provides More Attention for Sinogram Restoration in Sparse-View CT Reconstruction",
  "url": "https://openalex.org/W3216421685",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100600891",
      "name": "Ce Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101431758",
      "name": "Kun Shang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001445572",
      "name": "Haimiao Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100340545",
      "name": "Qian Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101437651",
      "name": "Hui Yuan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028465673",
      "name": "S. Kevin Zhou",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3199376581",
    "https://openalex.org/W3103372211",
    "https://openalex.org/W3032096772",
    "https://openalex.org/W1968238516",
    "https://openalex.org/W2469946482",
    "https://openalex.org/W3183452672",
    "https://openalex.org/W3204146347",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3105465046",
    "https://openalex.org/W2584483805",
    "https://openalex.org/W2743780012",
    "https://openalex.org/W3203168750",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3198035652",
    "https://openalex.org/W2335945936",
    "https://openalex.org/W3208757304",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2621228599",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2982220924",
    "https://openalex.org/W1972150100",
    "https://openalex.org/W2574952845",
    "https://openalex.org/W3134475970",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2058583833",
    "https://openalex.org/W3214175684",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W1580389772",
    "https://openalex.org/W3134839993",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W3103586216",
    "https://openalex.org/W3203397703",
    "https://openalex.org/W2963891322"
  ],
  "abstract": "While Computed Tomography (CT) reconstruction from X-ray sinograms is necessary for clinical diagnosis, iodine radiation in the imaging process induces irreversible injury, thereby driving researchers to study sparse-view CT reconstruction, that is, recovering a high-quality CT image from a sparse set of sinogram views. Iterative models are proposed to alleviate the appeared artifacts in sparse-view CT images, but the computation cost is too expensive. Then deep-learning-based methods have gained prevalence due to the excellent performances and lower computation. However, these methods ignore the mismatch between the CNN's \\textbf{local} feature extraction capability and the sinogram's \\textbf{global} characteristics. To overcome the problem, we propose \\textbf{Du}al-\\textbf{Do}main \\textbf{Trans}former (\\textbf{DuDoTrans}) to simultaneously restore informative sinograms via the long-range dependency modeling capability of Transformer and reconstruct CT image with both the enhanced and raw sinograms. With such a novel design, reconstruction performance on the NIH-AAPM dataset and COVID-19 dataset experimentally confirms the effectiveness and generalizability of DuDoTrans with fewer involved parameters. Extensive experiments also demonstrate its robustness with different noise-level scenarios for sparse-view CT reconstruction. The code and models are publicly available at https://github.com/DuDoTrans/CODE",
  "full_text": "DuDoTrans: Dual-Domain Transformer Provides More\nAttention for Sinogram Restoration in Sparse-View CT Reconstruction\nCe Wang1,2 Kun Shang3 Haimiao Zhang4 Qian Li1,2 Yuan Hui1,2 S. Kevin Zhou5,∗\n1Institute of Computing Technology, CAS, Beijing, China\n2Suzhou Institute of Intelligent Computing Technology, CAS, Suzhou, China\n3Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China\n4Beijing Information Science and Technology University, Beijing, China\n5University of Science and Technology of China, Suzhou, China\ns.kevin.zhou@gmail.com\nAbstract\nWhile Computed Tomography (CT) reconstruction from\nX-ray sinograms is necessary for clinical diagnosis, iodine\nradiation in the imaging process induces irreversible in-\njury, thereby driving researchers to study sparse-view CT\nreconstruction, that is, recovering a high-quality CT im-\nage from a sparse set of sinogram views. Iterative models\nare proposed to alleviate the appeared artifacts in sparse-\nview CT images, but the computation cost is too expensive.\nThen deep-learning-based methods have gained prevalence\ndue to the excellent performances and lower computation.\nHowever, these methods ignore the mismatch between the\nCNN’s local feature extraction capability and the sino-\ngram’s global characteristics. To overcome the problem,\nwe propose Dual-Domain Transformer (DuDoTrans) to si-\nmultaneously restore informative sinograms via the long-\nrange dependency modeling capability of Transformer and\nreconstruct CT image with both the enhanced and raw sino-\ngrams. With such a novel design, reconstruction perfor-\nmance on the NIH-AAPM dataset and COVID-19 dataset\nexperimentally conﬁrms the effectiveness and generalizabil-\nity of DuDoTrans with fewer involved parameters. Ex-\ntensive experiments also demonstrate its robustness with\ndifferent noise-level scenarios for sparse-view CT recon-\nstruction. The code and models are publicly available at\nhttps://github.com/DuDoTrans/CODE.\n1. Introduction and Motivation\nComputed Tomography (CT) is a widely used clinically\ndiagnostic imaging procedure aiming to reconstruct a clean\nCT image X from observed sinograms Y, but its accom-\npanying radiation heavily limits its practical usage. To de-\ncrease the induced radiation dose and reduce the scanning\nFigure 1. Parameters versus performances of DuDoTrans and\ndeep-learning-based CT reconstruction methods. L1 and L2 are\ntwo light versions while N represents the normal version. Firstly,\nwe ﬁnd Transformer-based reconstruction methods have consis-\ntently achieved better performance with fewer parameters.\nFurther, our DuDoTrans derives better results than at a less com-\nputation cost.\ntime, Sparse-View (SV) CT is commonly applied. How-\never, the deﬁciency of projection views brings severe arti-\nfacts in the reconstructed images, especially when common\nreconstruction methods such as analytical Filtered Back-\nprojection (FBP) and algebraic reconstruction technique\n(ART) [25] are used, which poses a signiﬁcant challenge\nto image reconstruction.\nTo tackle the artifacts, some iterative methods are pro-\nposed to impose the well-designed prior knowledge (ideal\nimage properties) via additional regularization termsR(X),\nsuch as Total Variation (TV) based methods [23, 27],\nNonlocal-based methods [38], and sparsity-based meth-\narXiv:2111.10790v2  [eess.IV]  25 Nov 2021\nods [2, 16]. Although these models have achieved better\nqualitative and quantitative performances, they suffer from\nover-smoothness. Besides, the iterative optimization proce-\ndure is often computationally expensive and requires care-\nful case-by-case hyperparameter tuning, which is practi-\ncally less applicable.\nWith the success of CNNs in various vision tasks [13,\n14, 34, 45, 46], CNN-based models are carefully designed\nand exhibit potential to a fast and efﬁcient CT image re-\nconstruction [1, 7, 8, 11, 15, 35]. These methods render the\npotential to learn a better mapping between low-quality im-\nages, such as reconstructed results of FBP, and ground-truth\nimages. Recently, Vision Transformer [5,6,9,18] has gained\nattention with its long-range dependency modeling capa-\nbility, and numerous models have been proposed in med-\nical image analysis [3, 6, 37, 39, 44]. For example, Tran-\nsCT [40] is proposed as an efﬁcient method for low-dose\nCT reconstruction, while it suffers from memory limitation\nwith involved patch-based operations. Besides, these deep\nlearning-based methods ignore the informative sinograms,\nwhich makes their reconstruction inconsistent with the ob-\nserved sinograms.\nTo alleviate the problem, a series of dual-domain (DuDo)\nreconstruction models [21, 30, 42, 43] are proposed to si-\nmultaneously enhance raw sinograms and reconstruct CT\nimages with both enhanced and raw sinograms, experimen-\ntally showing that enhanced sinograms contribute to the\nlatter reconstruction. Although these DuDo methods have\nshown satisfactory performances, they neglect the global\nnature of the sinogram’s sampling process, which is in-\nherently hard to be captured via CNNs, as CNNs are\nknown for extracting local spatial features. This moti-\nvates us to go a step further and design a more applica-\nble architecture for sinogram restoration.\nInspired by the long-range dependency modeling capa-\nbility & shifted window self-attention mechanism of Swin\nTransformer [22], we speciﬁcally design the Sinogram\nRestoration Transformer (SRT) by considering the time-\ndependent characteristics of sinograms, which restore infor-\nmative sinograms and overcome the mismatch between the\nglobal characteristics of sinograms and local feature model-\ning of CNNs. Based on the SRT module, we ﬁnally pro-\npose Dual-Domain Transformer ( DuDoTrans) to recon-\nstruct CT image. Compared with previous image recon-\nstruction methods, we summarize several beneﬁts of Du-\nDoTrans as follows:\n• Considering the global sampling process of sinograms,\nwe introduce SRT module, which has the advantages\nof both Swin-Transformer and CNNs. It has the de-\nsired long-range dependency modeling ability, which\nhelps better restore the sinograms and has been exper-\nimentally veriﬁed in CNN-based, Transformer-based,\nand deep-unrolling-based reconstruction framework.\n• With the powerful SRT module for sinogram restora-\ntion, we further propose Residual Image Reconstruc-\ntion Module (RIRM) for image-domain reconstruc-\ntion. To compensate for the drift error between the\ndual-domain optimization directions, we ﬁnally utilize\nthe proposed differentiable DuDo Consistency Layer\nto keep the restored sinograms consistent with recon-\nstructed CT images, which induces the ﬁnal DuDo-\nTrans. Hence, DuDoTrans not only has the desired\nlong-range dependency and local modeling ability, but\nalso has the beneﬁt of dual-domain reconstruction.\n• Reconstruction performance on the NIH-AAPM\ndataset and COVID-19 dataset experimentally con-\nﬁrms the effectiveness, robustness, and generalizabil-\nity of the proposed method. Besides, by adaptively em-\nploying Swin-Transformer and CNNs, our DuDoTrans\nhas achieved better performance with fewer parame-\nters as shown in Figure 1 and similar FLOPs (shown in\nlater experiments), which makes the model practical in\nvarious applications.\n2. Backgrounds and Related Works\n2.1. Tomographic Image Reconstruction\nHuman body tissues, such as bones and organs, have dif-\nferent X-ray attenuation coefﬁcients µ. When considering\na 2D CT image, the distribution of the attenuation coefﬁ-\ncients X = µ(a,b), where (a,b) indicate positions, repre-\nsents the underlying anatomical structure. The principle of\nCT imaging is based on the fundamental Fourier Slice The-\norem, which guarantees that the 2D image function X can\nbe reconstructed from the obtained dense projections (called\nsinograms). When imaging, projections of the anatomical\nstructure X are indeed inferred by the emitted and received\nX-ray intensities according to the Lambert-Beer Law. Fur-\nther, when under a polychromatic X-ray source with an en-\nergy distribution η(E), the CT imaging process is given as:\nY = −log\n∫\nη(E) exp{−PX}, (1)\nwhere Prepresents the sinogram generation process, i.e.,\nRadon transformation (commonly deﬁned with a fan-beam\nimaging geometry). With the above forward process, CT\nimaging aims to reconstruct X from the obtained projec-\ntions Y = PX (abbreviation for simplicity) with the es-\ntimated/learned function P†. In practical SVCT, the pro-\njection data Y is incomplete, where the total αmax pro-\njection views are sampled uniformly in a circle around the\npatient. This reduced sinogram information heavily limits\nthe performance of previous methods and results in arti-\nfacts. In order to alleviate the phenomena, many works have\nbeen recently proposed, which can be categorized into the\nfollowing two groups: Iterative-based reconstruction meth-\nods [2, 16, 23, 27, 38] and Deep-learning-based reconstruc-\ntion methods [13,14,34,45,46]. Different from these preva-\nlent works, our method DuDoTrans is based on deep-\nlearning, but is the only dual-domain method based on\nTransformer.\n2.2. Transformer in Medical Imaging\nBased on the powerful attention mechanism [4, 10, 26,\n29, 36, 41] and patch-based operations, Transformer is ap-\nplied to many vision tasks [9, 12, 22, 28]. Especially,\nSwin Transformer [22] incorporates such an advantage with\nthe local feature extraction ability of CNNs. With such\nan intuitive manner, Swin-Transformer-based models [19]\nhave relieved the limitation of memory in previous Vision\nTransformer-based models. Based on these successes and\nfor better modeling the global features of medical images,\nTransformer has been applied to medical image segmenta-\ntion [3, 6, 20, 44], registration [39], classiﬁcation [31, 37],\nand achieved surprising improvements. Nevertheless, few\nworks explore Transformer structures in SVCT reconstruc-\ntion. Although TransCT [40] attempts to suppress the noise\nartifacts in low-dose CT with Transformer,they neglect the\nconsideration of global sinogram characteristics in their\ndesign, which is taken into account in DuDoTrans.\n3. Method\nAs shown in Fig. 2, we build DuDoTrans with three\nmodules: (a) Sinogram Restoration Transformer (SRT), (b)\nDuDo Consistency Layer, and (c) Residual Image Recon-\nstruction Module (RIRM). Assume that a sparse-view sino-\ngram Y ∈RHs×Ws is given, we ﬁrst use FBP [25] to re-\nconstruct a low-quality CT image ˜X1. Simultaneously, the\nSRT module is introduced to output an enhanced sinogram\n˜Y, followed by the DuDo Consistency Layer to yield an-\nother estimation ˜X2. At last, these low-quality images ˜X1\nand ˜X2 are concatenated and fed into RIRM to predict the\nCT image ˜X, which will be supervised with the correspond-\ning clean CT imageXgt ∈RHI×WI . We next introduce the\nabove-involved modules in detail.\n3.1. Sinogram Restoration Transformer\nSinogram restoration is extremely challenging since the\nintrinsic information not only contains spatial structures of\nhuman bodies, but follows the global sampling process.\nSpeciﬁcally, each line{Yi}Hs\ni=1 of a sinogramY are sequen-\ntially sampled with overlapping information of surrounding\nsinograms. In other words, 1-D components of sinograms\nheavily correlate with each other. The global characteris-\ntic makes it difﬁcult to be captured with traditional CNNs,\nwhich are powerful in local feature extraction. For this rea-\nson, we equip this module with the Swin-Transformer struc-\nture, which enables it with long-range dependency model-\ning ability. As shown in Fig. 2, SRT consists of m suc-\ncessive residual blocks, and each block contains nnormal\nSwin-Transformer Module (STM) and a spatial convolu-\ntional layer, which have the capacity of both global and lo-\ncal feature extraction. Given the degraded sinograms, we\nﬁrst use a convolutional layer to extract the spatial structure\nFconv. Considering it as FSTM 0 , then nSTM components\nof each residual block output {FSTM i}m\ni=1 with the follow-\ning formulation:\nFSTM i = Mconv(\nn∏\nj=1\nMj\nswin(FSTM i−1 )) +FSTM i−1 ,\n(2)\nwhere Mconv denotes a convolutional layer, {Mj\nswin}n\nj=1\ndenotes n Swin-Transformer layers, and ∏ represents the\nsuccessive operation of Swin-Transformer Layers. Finally,\nthe enhanced sinograms are estimated with:\n˜Y = Y + Mconv(Mconv(FSTM m) +FSTM 0 ). (3)\nAs a restoration block,LSRT is used to supervise the output\nof the SRT:\nLSRT = ∥˜Y −Ygt∥2, (4)\nwhere Ygt is the ground truth sinogram, and it should be\ngiven when training.\n3.2. DuDo Consistency Layer\nAlthough input sinograms have been enhanced via the\nSRT module, directly learning from the concatenation of\n˜X1 and ˜X2 leaves a drift between the optimization direc-\ntions of SRT and RIRM. To compensate for the drift, we\nmake use of a differentiable DuDo Consistency LayerMDC\nto back-propagate the gradients of RIRM. In this way, the\noptimization direction imposes the preferred sinogram char-\nacteristics to ˜Y, and vice versa. To be speciﬁc, given the\ninput fan-beam sinogram ˜Y, the DuDo Consistency Layer\nﬁrst converts it into parallel-beam geometry, followed with\nFiltered Backprojection:\n˜X2 = MDC( ˜Y). (5)\nTo additionally keep the restored sinograms consistent with\nthe ground-truth CT image Xgt, LDC is proposed as fol-\nlows:\nLDC = ∥˜X2 −Xgt∥2. (6)\n3.3. Residual Image Reconstruction Module\nAs a long-standing clinical problem, the ﬁnal goal of CT\nimage reconstruction is to recover a high-quality CT im-\nage for diagnosis. With the initially estimated low-quality\nimages that help rectify the geometric deviation between\nthe sinogram and image domains, we next employ Shallow\nResidual Image Reconstruction Module(RIRM)\nShallow Layer Deep Feature \nExtraction Layers Recon Layer\nDuDo \nConsistency Layer\nFBP\nNorm\nSW-MSA\nNorm\nMLP\nInput Output\nSwin-Transformer Module (STM)\n Conv\nSTM…\nSTM\n Conv\n Conv\n Conv…\nSTM…\nSTM\n Conv\nSinogram Restoration Transformer (SRT)\nFigure 2. The framework of proposed DuDoTrans for SV CT image reconstruction. When under-sampled sinograms are given, our\nDuDoTrans ﬁrst restores clean sinograms with SRT, followed by RIRM to reconstruct the CT image with both restored and raw sinograms.\nLayer Msl to obtain shallow features of input low-quality\nimage ˜X:\nFsl = Msl([ ˜X1, ˜X2]). (7)\nThen a series of Deep Feature Extraction Layers {Mi\nd f}n\ni=1\nare introduced to extract deep features:\nFi\nd f= Mi\nd f(Fi−1\nd f), i= 1,2,...,n, (8)\nwhere F0\nd f= Fsl. Finally, we utilize a Recon Layer Mre to\npredict the clean CT image with residual learning:\n˜X = Mre(Fn\nd f) +˜X1. (9)\nTo supervise our network optimization, the below LRIRM\nloss is used for this module:\nLRIRM = ∥˜X −Xgt∥2. (10)\nThe full objective of our model is:\nL= LSRT + λ1LDC + λ2LRIRM , (11)\nwhere λ1 and λ2 are blending coefﬁcients, which are both\nempirically set as 1 in experiments.\nNote that the intermediate convolutional layers are used\nto communicate between image space RHI×WI and patch-\nbased feature space R\nHI\nw ×WI\nw ×w2\n. Further, by dynamically\ntuning the depth mand width n, SRT modules are ﬂexible\nin practice depending on the balance between memory and\nperformance. We will explore this balancing issue in later\nexperiments.\n4. Experimental Results\n4.1. Experimental Setup\nDatasets. We ﬁrst train and test our model with the\n“2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand\nTable 1. The effect of each module (CNNs v.s. Transformer) on\nthe reconstruction performance. Obviously, our DuDoTrans de-\nsign achieves the best performance.\nMethod PSNR SSIM RMSE\nFBPConvNet 31.47 0.8878 0.0268\nDuDoNet 31.57 0.8920 0.0266\nFBPConvNet+SRT 32.13 0.8989 0.0248\nImgTrans 32.50 0.9010 0.0238\nDuDoTrans 32.68 0.9047 0.0233\nChallenge” [24] dataset. Speciﬁcally, we choose a total of\n1746 slices (resolution 512×512) from ﬁve patients to train\nour models, and use 314 slices of another patient for test-\ning. We employ a scanning geometry of Fan-Beam X-Ray\nsource with 800 detector elements. There are four SV sce-\nnarios in our experiment, corresponding to αmax = [24, 72,\n96, 144] views. Note that these views are uniformly dis-\ntributed around the patient. The original dose data are col-\nlected from the chest to the abdomen under a protocol of\n120 kVp and 235 effective mAs (500mA/0.47s). To simu-\nlate the photon noise in numerical experiments, we add to\nsinograms mixed noise that is by default composed of 5%\nGaussian noise and Poisson noise with an intensity of 5e6.\nImplementation details and training settings.Our mod-\nels are implemented using the PyTorch framework. We use\nthe Adam optimizer [17] with(β1,β2) = (0.9, 0.999) to train\nthese models. The learning rate starts from 0.0001. Models\nare all trained on a Nvidia 3090 GPU card for 100 epochs\nwith a batch size of 1.\nEvaluation metrics. Reconstructed CT images are quan-\ntitatively measured by the multi-scale Structural Similarity\nIndex Metric (SSIM) (with level = 5, Gaussian kernel size =\n11, and standard deviation = 1.5) [32, 33] and Peak Signal-\nto-Noise Ratio (PSNR).\n(a)\n(d)\n(b)\n(e)\n(c)\n(f)\nFigure 3. The ﬁrst row compares the effect of RIRM depth, RIRM width, and SRT size on reconstruction. The second row inspects the\nconvergence, robustness on noise, and the effect of training dataset scale on DuDoTrans.\n4.2. Ablation Study and Analysis\nWe next prove the effectiveness of our proposed SRT\nmodule and exhaust the best structure for DuDoTrans.\nFirstly, we conduct the experiments with the ﬁve models:\n(a) FBPConvNet [15], (b) DuDoNet [21], (c) FBPCon-\nvNet+SRT, which combines (a) with our proposed SRT, (d)\nImgTrans, which replaces the image-domain model in (a)\nwith Swin-Transformer [22], and (e) our DuDoTrans. The\nexperimental settings are by default with αmax = 96, and\nthe results are shown in Table 1.\nThe Effectiveness of SRT.Comparing models (a) and (c) in\nTable 1, the performance is improved 0.66 dB, which con-\nﬁrms that the SRT module output ˜Y indeed provides useful\ninformation for the image-domain reconstruction.\nThe exploration of RIRM. Inspired by the success of\nSwin-Transformer in low-level vision tasks, we simply re-\nplace the post-precessing module of FBPConvNet with\nSwin-Transformer, named ImgTrans. Comparing it with the\nbaseline model (a), the achieved 1 dB improvement con-\nﬁrms that Transformer is skilled at characterizing deep fea-\ntures of images, and a thorough exploration is worthy.\nThe effectiveness of DuDoTrans. When comparing (d)\nand (e), the boosted 0.18 dB proves SRT effectiveness\nagain. Further, comparing (b) and (e), both dual-domain ar-\nchitectures with corresponding CNNs and Transformer, the\nimprovement demonstrates that Transformer is very suit-\nable in CT reconstruction.\nWe then investigate the impact of each sub-module on\nthe performance of DuDoTrans:\nRIRM depth and width. Similar to the SRT structure,\nthe RIRM depth represents the number of sub-modules of\nRIRM, and the RIRM width describes the number of suc-\ncessive Swin-Transformers in each sub-module. Results in\nFig 3 (a) and (b) show the corresponding effect of the RIRM\nwidth and RIRM depth on the reconstruction performance.\nWhen increasing the RIRM depth (with ﬁxed RIRM width\n2), the performance is improved quickly when RIRM depth\nis smaller than 4. Then the PSNR improvement slows down,\nwhile the introduced computational cost is increased. Then\nwe ﬁx RIRM depth equal to 3 (blue) and 4 (yellow) and\nincrease RIRM width, and ﬁnd that the performance is im-\nproved fast till RIRM width = 3. After balancing the com-\nputation cost and performance improvement, we set RIRM\nwidth and depth to 4 and 2, respectively, which is a small\nmodel with similar FLOPs to FBPConvNet.\nThe SRT size. With a similar procedure, we explore the\nmost suitable architecture for the SRT module. As Fig 3 (c)\nshows, with ﬁxed RIRM depth and width. the performance\nis not inﬂuenced when we enlarge the SRT size (depthnand\nwidth mas introduced in Section 3.1). Speciﬁcally, we test\nﬁve paired models whose RIRM depth and width are set to\n{(2, 1), ( 3, 1), (3, 2), (4, 2), (5, 2) }, respectively. Then we\nincrease (m,n) from (3, 1) to (4, 2), but the PSNR is even\nreduced sometimes. Therefore, we set SRT depth and with\nto (3, 1) as default in later experiments.\nFurther, we analyze the convergence, robustness, and ef-\nTable 2. Quantitative results on NIH-AAPM dataset. Our DuDoTrans achieves the best results in all cases. The inferring time is tested\nwhen αmax is ﬁxed as 96.\nNIH-AAPM Param(M) αmax = 24 αmax = 72 αmax = 96 αmax = 144 Time(ms)PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nFBP [25] – 14.58 0.2965 17.61 0.5085 18.13 0.5731 18.70 0.6668 –\nFBPCovNet [15] 13.39 27.10 0.8158 30.80 0.8671 31.47 0.8878 32.74 0.9084 155.53\nDuDoNet [21] 25.80 26.47 0.7977 30.94 0.8816 31.57 0.8920 32.96 0.9106 145.65\nImgTrans 0.22 27.46 0.8405 31.76 0.8899 32.50 0.9010 33.50 0.9157 225.56\nDuDoTrans 0.44 27.55 0.8431 31.91 0.8936 32.68 0.9047 33.70 0.9191 243.81\nTable 3. We test the robustness of DuDoTrans with varied Poisson noise levels. As follows, the intensity is varied to 1e6 (H1), 5e5 (H2),\nand 1e5, respectively. DuDoTrans keeps the best performance except when the Poisson noise level is enlarged to 1e5, which is too hard to\nrestore clean sinograms.\nNoise-H1 αmax = 24 αmax = 72 αmax = 96 αmax = 144 Time(ms)PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nFBP [25] 14.45 0.2815 17.52 0.4898 18.04 0.5541 18.63 0.6483 –\nFBPCovNet [15] 27.12 0.8171 30.74 0.8798 31.44 0.8874 32.65 0.9070 148.58\nDuDoNet [21] 26.40 0.7932 30.84 0.8792 31.47 0.8900 32.87 0.9090 146.36\nImgTrans 27.35 0.8395 31.65 0.8882 32.42 0.8993 33.36 0.9133 244.64\nDuDoTrans 27.45 0.8411 31.80 0.8911 32.55 0.9021 33.48 0.9156 242.38\nNoise-H2\nFBP [25] 14.29 0.2652 17.40 0.4688 17.94 0.5325 18.55 0.6267 –\nFBPCovNet [15] 27.11 0.8168 30.61 0.8764 31.40 0.8865 32.52 0.9047 151.39\nDuDoNet [21] 26.28 0.7857 30.68 0.8755 31.34 0.8871 32.73 0.9066 152.11\nImgTrans 27.18 0.8361 31.49 0.8855 32.26 0.8963 33.15 0.9096 244.16\nDuDoTrans 27.29 0.8377 31.64 0.8881 32.36 0.8986 33.24 0.9113 261.45\nNoise-H3\nFBP [25] 13.24 0.1855 16.56 0.3543 17.20 0.4121 17.96 0.5018 –\nFBPCovNet [15] 26.08 0.7512 29.16 0.8294 30.37 0.8592 31.02 0.8706 152.04\nDuDoNet [21] 24.77 0.6820 29.01 0.8216 29.86 0.8456 31.25 0.8736 151.50\nImgTrans 25.39 0.7933 30.20 0.8624 31.13 0.8707 31.52 0.8691 220.78\nDuDoTrans 24.77 0.7844 30.26 0.8632 30.86 0.8753 31.58 0.8747 241.71\nfect of the training dataset scale.\nConvergence. In Fig. 3 (d), we plot the convergence curve\nof FBPConvNet, ImgTrans, and DuDoTrans. Evidently, the\nintroduction of Transformer Structure not only improves the\nﬁnal results, but also stabilizes the training process. Be-\nsides, our Dual-Domain design achieves consistently better\nresults, compared with ImgTrans.\nRobustness. In practice, the photon noise in the imaging\nprocess inﬂuences the reconstructed images, therefore the\nrobustness to such noise is important for application usage.\nHere, we simulate such noise with mixed noise (Gaussian\n& Poisson noise). Speciﬁcally, we train models with de-\nfault noise level and test them with varied Poisson noise lev-\nels (with ﬁxed Gaussian noise), whose intensity correspond\nto [1e5, 5e5, 1e6, 5e6], and show the results in Fig. 3 (e).\nEvidently, our models achieve better performances except\nwhen the intensity is 1 e5, which noise is extremely hard to\nbe suppressed, while DuDoTrans is still better than CNN-\nbased methods, which conﬁrms its robustness.\nTraining dataset scale. Vision Transformers need large-\nscale data to exhibit performance, and thus limits its de-\nvelopment in medical imaging. To investigate it, we train\nFBPConvNet, ImgTrans, and DuDoTrans with [20%, 40%,\n60%, 80%, 100%] of our original training dataset, and show\nthe performance in Fig. 3 (f). Obviously, reconstruction\nperformance of DuDoTrans is very stable till the training\ndataset decreases to 20%, in which case training data is\ntoo less for all models to perform well and DuDoTrans still\nachieves the best performance.\nGround Truth\n FBP\n FBPConvNet\n DuDoNet\n ImgTrans\n DuDoTrans\nFigure 4. Qualitative comparison on NIH-AAPM dataset, each row from top to bottom corresponds to SV Reconstruction with αmax =\n[72, 96 ,144]. The display window is [-1000, 800] HU. We outline the improvements of images with green bounding boxes, and obviously,\nimprovements on visualization are more clear when αmax increases.\nTable 4. Testing performances on COVID-19 dataset. Given these unobserved data, our DuDoTrans still performs the best in all cases.\nCOVID-19 αmax = 24 αmax = 72 αmax = 96 αmax = 144 Time(ms)PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nFBP [25] 14.82 0.3757 18.16 0.5635 18.81 0.6248 19.36 0.7070 –\nFBPCovNet [15] 26.43 0.8015 32.84 0.9407 33.72 0.9409 34.62 0.9651 149.48\nDuDoNet [21] 26.97 0.8558 33.10 0.9429 32.57 0.9380 36.13 0.9722 153.25\nImgTrans 27.24 0.8797 35.58 0.9580 37.31 0.9699 39.90 0.9801 222.14\nDuDoTrans 27.74 0.8897 35.62 0.9596 37.83 0.9727 40.20 0.9794 244.46\n4.3. Sparse-View CT Reconstruction Analysis\nWe next conduct thorough experiments to test the per-\nformance of DuDoTrans on various sparse-view scenarios.\nSpeciﬁcally, we ﬁrst train models when αmax is 24, 72,\n96, 144, respectively. The results are shown in Table 3,\nand DuDoTrans have achieved consistently better results.\nBesides, we observe that ImgTrans and DuDoTrans are\nmore stable in training, and the learned parameters are both\nextremely small, compared with CNN-based models. Fur-\nthermore, the improvement of DuDoTrans over ImgTrans\nbecomes larger when the αmax increases, which conﬁrms\nthe usefulness of restored sinograms in reconstruction.\nQualitative comparison. We also visualize the recon-\nstructed images of these methods in Fig. 4 with αmax =\n[72, 96 ,144] (See more visualizations in Appendix). In all\nthree rows, our DuDoTrans shows better detail recovery,\nand sparse-view artifacts are suppressed. Further, when\ndecreasing αmax, where raw sinograms are too messy to be\nrestored and low-quality images from FBP are too hard to\ncapture global features, Transformer-based models exhibit\nreduced performance. The phenomena suggests that we\nshould design suitable structures with the Transformer and\nCNNs, facing with different cases.\nRobustness with noise in SVCT.With the decrease of view\nnumber αmax, input sinograms would be messier, which\nmakes SVCT more difﬁcult. Therefore, we test the ro-\nbustness of all trained models with aforementioned Pois-\nson noise levels when αmax = [24, 72, 96, 144], and re-\nport performances in Table. 3. The included notation Noise-\nH1, Noise-H2, and Noise-H3 correspond to Poisson inten-\nsity [1e6, 5e5, 1e5]. Compared with CNN-based methods,\nImgTrans and DuDoTrans show better robustness. Across\ninvolved cases, DuDoTrans shows the best performance.\nNevertheless, when Poisson intensity is 1 e5, DuDoTrans\nGround Truth\n FBP\n FBPConvNet\n DuDoNet\n ImgTrans\n DuDoTrans\nFigure 5. Qualitative comparison on COVID-19 dataset when αmax = 96. The display window is [-1000, 800] HU. We outline the\nimprovements of images with green bounding boxes, which shows that DuDoTrans performs better that others.\nFigure 6. FLOPs versus performances of these methods. Similar\nto Fig 1, L1, L2 are two light versions and N represents the normal\nversion. ImgTrans-L1 and DuDoTrans-L1 have achieved 0.5-0.7\ndB improvements with less than 80G FLOPs, while CNN-based\nmethods needs over 120G FLOPs. Further, DuDoTrans-N has en-\nlarged the improvement to 1.2 dB with similar FLOPs.\nfails to exceed ImgTrans and FBPConvNet, which is caused\nby the extremely messy sinograms. In this case, restoring\nsinograms is too difﬁcult and DuDoNet also fails.\nGeneralizablity on COVID-19 dataset. Finally, we use\nslices of another patient in the COVID-19 dataset to test the\ngeneralizability of trained models, and quantitative perfor-\nmances are compared in Table 4. ImgTrans and DuDoTrans\nhave achieved a larger improvement about 4-5 dB over\nCNN-based methods, which shows that the long-range de-\npendency modeling ability helps capture the intrinsic global\nproperty of general CT images. Further, our DuDoTrans ex-\nceeds ImgTrans about 0.4 dB in all cases, even larger than\nthe original NIH-AAPM dataset. The improvement ensures\nthat DuDoTrans generalizes well to out-of-distribution CT\nimages. Besides, we also show the visualization images in\nFig. 5 when αmax = 96. Coinciding with the quantitative\ncomparison, our DuDoTrans show better reconstruction on\nboth global patterns and local details.\n4.4. Computation comparison\nAs a practical problem, reconstruction speed is neces-\nsary when deployed in modern CT machines. Therefore, we\nTable 5. Test performances of various models w/ v.s. w/o SRT\nmodule. Obviously, SRT improves performances of CNN-based,\nTransformer-based, and Deep unrolloing methods. Note that we\nreplace RIRM with PDNet in our framework, named PDNet+SRT.\nMethod PSNR SSIM\nFBPConvNet [15] 31.47 0.8878\nw/o SRT PDNet [1] 31.62 0.8894\nImgTrans 32.50 0.9010\nFBPConvNet+SRT 32.13 0.8989\nw/ SRT PDNet+SRT 32.38 0.9045\nDuDoTrans 32.68 0.9047\ncompare the parameters and FLOPs versus performances in\nFig 1 and Fig. 6, respectively. We ﬁnd that Transformer-\nbased methods have achieved better performances with\nfewer parameters, and our DuDoTrans exceeds ImgTrans\nwith only a few additional parameters. As known, the patch-\nbased operations and the attention mechanism are compu-\ntationally expensive, which limits their application usage.\nTherefore, we further compare the FLOPs of these methods.\nAs shown, light versions (DuDoTrans-L1, DuDoTrans-L2)\nhave achieved 0.8-1 dB improvement with fewer FLOPs,\nand DuDoTrans-N with default size has enlarged the im-\nprovement to 1.2 dB. Besides, we report the inferring time\nin each Table 2 3 4, and computation time is very similar\nto CNN-based methods, whose additional consumption is\nbecause of the patch-based operations.\n4.5. Discussion\nAs in Table 1, we have shown the effectiveness of SRT\nwith FBPConvNet [15] and ImgTrans [22], which are two\npost-processing methods. Recently deep-unrolling meth-\nods have attracted much attention in reconstruction. To\nconcretely verify the SRT module’s effectiveness, we fur-\nther combine it with PDNet [1], which is a deep-unrolling\nmethod. Results of involved three paired models (w/ v.s.\nw/o SRT) are shown in Table 5 with default experimental\nsettings when αmax = 96 (See other cases when αmax =\n[24, 72, 144] in Appendix). All these three reconstruction\nmethods have been improved by the use of the SRT module.\nFurthermore, our DuDoTrans still performs the best without\nany unrolling design. Thus, our SRT is ﬂexible and power-\nful probably in any existing reconstruction framework.\n5. Conclusion\nWe propose a transformer-based SRT module with\nlong-range dependency modeling capability to exploit the\nglobal characteristics of sinograms, and verify it in CNN-\nbased, Transformer-based, and Deep-unrolling reconstruc-\ntion framework. Further, via combining SRT and similarly-\ndesigned RIRM, we yield DuDoTrans for SVCT recon-\nstruction. Experimental results on the NIH-AAPM dataset\nand COVID-19 dataset show that DuDoTrans achieves\nstate-of-the-art reconstruction. To further beneﬁt DuDo-\nTrans with the accordingly designing advantage of deep-\nunrolling methods, we will explore “DuDoTrans + un-\nrolling” in the future.\nAcknowledge. This work was supported by the National\nNatural Science Foundation of China under Grant No.\n12001180 and 12101061.\nReferences\n[1] Jonas Adler and Ozan ¨Oktem. Learned primal-dual re-\nconstruction. IEEE transactions on medical imaging ,\n37(6):1322–1332, 2018. 2, 8\n[2] Peng Bao, Wenjun Xia, Kang Yang, Weiyan Chen, Mi-\nanyi Chen, Yan Xi, Shanzhou Niu, Jiliu Zhou, He Zhang,\nHuaiqiang Sun, et al. Convolutional sparse coding for com-\npressed sensing ct reconstruction. IEEE transactions on\nmedical imaging, 38(11):2607–2619, 2019. 2, 3\n[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\naopeng Zhang, Qi Tian, and Manning Wang. Swin-unet:\nUnet-like pure transformer for medical image segmentation.\narXiv preprint arXiv:2105.05537, 2021. 2, 3\n[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision Workshops, pages\n0–0, 2019. 3\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213–229. Springer, 2020.\n2\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12299–12310, 2021. 2, 3\n[7] Hu Chen, Yi Zhang, Mannudeep K Kalra, Feng Lin, Yang\nChen, Peixi Liao, Jiliu Zhou, and Ge Wang. Low-dose ct\nwith a residual encoder-decoder convolutional neural net-\nwork. IEEE transactions on medical imaging, 36(12):2524–\n2535, 2017. 2\n[8] Weilin Cheng, Yu Wang, Hongwei Li, and Yuping Duan.\nLearned full-sampling reconstruction from incomplete data.\nIEEE Transactions on Computational Imaging , 6:945–957,\n2020. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[10] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 3146–\n3154, 2019. 3\n[11] Harshit Gupta, Kyong Hwan Jin, Ha Q Nguyen, Michael T\nMcCann, and Michael Unser. Cnn-based projected gradient\ndescent for consistent ct image reconstruction. IEEE trans-\nactions on medical imaging, 37(6):1440–1453, 2018. 2\n[12] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer.arXiv preprint\narXiv:2103.00112, 2021. 3\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2, 3\n[14] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017. 2, 3\n[15] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey,\nand Michael Unser. Deep convolutional neural network for\ninverse problems in imaging. IEEE Transactions on Image\nProcessing, 26(9):4509–4522, 2017. 2, 5, 6, 7, 8\n[16] Kyungsang Kim, Jong Chul Ye, William Worstell, Jin-\nsong Ouyang, Yothin Rakvongthai, Georges El Fakhri, and\nQuanzheng Li. Sparse-view spectral ct reconstruction using\nspectral patch-based low-rank penalty. IEEE transactions on\nmedical imaging, 34(3):748–760, 2014. 2, 3\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 4\n[18] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021. 2\n[19] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration us-\ning swin transformer. InProceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1833–1844,\n2021. 3\n[20] Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, and\nGuangming Lu. Ds-transunet: Dual swin transformer\nu-net for medical image segmentation. arXiv preprint\narXiv:2106.06716, 2021. 3\n[21] Wei-An Lin, Haofu Liao, Cheng Peng, Xiaohang Sun, Jing-\ndan Zhang, Jiebo Luo, Rama Chellappa, and Shaohua Kevin\nZhou. Dudonet: Dual domain network for ct metal arti-\nfact reduction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10512–\n10521, 2019. 2, 5, 6, 7\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 2, 3, 5, 8\n[23] Faisal Mahmood, Nauman Shahid, Ulf Skoglund, and Pierre\nVandergheynst. Adaptive graph-based total variation for to-\nmographic reconstructions. IEEE Signal Processing Letters,\n25(5):700–704, 2018. 1, 3\n[24] C McCollough. Tu-fg-207a-04: Overview of the low dose ct\ngrand challenge. Medical physics, 43(6Part35):3759–3760,\n2016. 4\n[25] Frank Natterer. The mathematics of computerized tomogra-\nphy. SIAM, 2001. 1, 3, 6, 7\n[26] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 3\n[27] Emil Y . Sidky and Xiaochuan Pan. Image reconstruction in\ncircular cone-beam computed tomography by constrained,\ntotal-variation minimization. Physics in Medicine & Biol-\nogy, 53(17):4777, 2008. 1, 3\n[28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347–10357. PMLR, 2021. 3\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 3\n[30] Ce Wang, Haimiao Zhang, Qian Li, Kun Shang, Yuanyuan\nLyu, Bin Dong, and S Kevin Zhou. Improving generaliz-\nability in limited-angle ct reconstruction with sinogram ex-\ntrapolation. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention , pages 86–\n96. Springer, 2021. 2\n[31] Xiyue Wang, Sen Yang, Jun Zhang, Minghui Wang, Jing\nZhang, Junzhou Huang, Wei Yang, and Xiao Han. Transpath:\nTransformer-based self-supervised learning for histopatho-\nlogical image classiﬁcation. In International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion, pages 186–195. Springer, 2021. 3\n[32] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004. 4\n[33] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multi-\nscale structural similarity for image quality assessment. In\nThe Thrity-Seventh Asilomar Conference on Signals, Sys-\ntems & Computers, 2003, volume 2, pages 1398–1402. Ieee,\n2003. 4\n[34] Zidi Xiu, Junya Chen, Ricardo Henao, Benjamin Goldstein,\nLawrence Carin, and Chenyang Tao. Supercharging imbal-\nanced data learning with energy-based contrastive represen-\ntation transfer. In Advances in Neural Information Process-\ning Systems. Curran Associates, Inc., 2021. 2, 3\n[35] Qingsong Yang, Pingkun Yan, Yanbo Zhang, Hengyong Yu,\nYongyi Shi, Xuanqin Mou, Mannudeep K Kalra, Yi Zhang,\nLing Sun, and Ge Wang. Low-dose ct image denoising using\na generative adversarial network with wasserstein distance\nand perceptual loss. IEEE transactions on medical imaging,\n37(6):1348–1357, 2018. 2\n[36] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,\nStephen Lin, and Han Hu. Disentangled non-local neural net-\nworks. In European Conference on Computer Vision, pages\n191–207. Springer, 2020. 3\n[37] Shuang Yu, Kai Ma, Qi Bi, Cheng Bian, Munan Ning, Nan-\njun He, Yuexiang Li, Hanruo Liu, and Yefeng Zheng. Mil-vt:\nMultiple instance learning enhanced vision transformer for\nfundus image classiﬁcation. In International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion, pages 45–54. Springer, 2021. 2, 3\n[38] Dong Zeng, Jing Huang, Hua Zhang, Zhaoying Bian,\nShanzhou Niu, Zhang Zhang, Qianjin Feng, Wufan Chen,\nand Jianhua Ma. Spectral ct image restoration via an average\nimage-induced nonlocal means ﬁlter. IEEE Transactions on\nBiomedical Engineering, 63(5):1044–1057, 2015. 1, 3\n[39] Yungeng Zhang, Yuru Pei, and Hongbin Zha. Learning dual\ntransformer network for diffeomorphic registration. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention , pages 129–138. Springer,\n2021. 2, 3\n[40] Zhicheng Zhang, Lequan Yu, Xiaokun Liang, Wei Zhao, and\nLei Xing. Transct: Dual-path transformer for low dose com-\nputed tomography. arXiv preprint arXiv:2103.00634, 2021.\n2, 3\n[41] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076–10085, 2020. 3\n[42] Bo Zhou, Xiongchao Chen, S Kevin Zhou, James S Dun-\ncan, and Chi Liu. Dudodr-net: Dual-domain data consistent\nrecurrent network for simultaneous sparse view and metal\nartifact reduction in computed tomography. Medical Image\nAnalysis, page 102289, 2021. 2\n[43] Bo Zhou and S Kevin Zhou. Dudornet: Learning a dual-\ndomain recurrent network for fast mri reconstruction with\ndeep t1 prior. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 4273–\n4282, 2020. 2\n[44] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu,\nLiansheng Wang, and Yizhou Yu. nnformer: Interleaved\ntransformer for volumetric segmentation. arXiv preprint\narXiv:2109.03201, 2021. 2, 3\n[45] S Kevin Zhou, Hayit Greenspan, Christos Davatzikos,\nJames S Duncan, Bram Van Ginneken, Anant Madabhushi,\nJerry L Prince, Daniel Rueckert, and Ronald M Summers. A\nreview of deep learning in medical imaging: Imaging traits,\ntechnology trends, case studies with progress highlights, and\nfuture promises. Proceedings of the IEEE, 2021. 2, 3\n[46] S Kevin Zhou, Hoang Ngan Le, Khoa Luu, Hien V Nguyen,\nand Nicholas Ayache. Deep reinforcement learning in med-\nical imaging: A literature review. Medical Image Analysis,\n2021. 2, 3",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6880833506584167
    },
    {
      "name": "Iterative reconstruction",
      "score": 0.6392134428024292
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.604417622089386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5967950820922852
    },
    {
      "name": "Computation",
      "score": 0.5377715826034546
    },
    {
      "name": "Generalizability theory",
      "score": 0.45928382873535156
    },
    {
      "name": "Transformer",
      "score": 0.4135976731777191
    },
    {
      "name": "Algorithm",
      "score": 0.39615577459335327
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3947805166244507
    },
    {
      "name": "Computer vision",
      "score": 0.3560182452201843
    },
    {
      "name": "Mathematics",
      "score": 0.17249831557273865
    },
    {
      "name": "Physics",
      "score": 0.11694586277008057
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 20
}