{
  "title": "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark",
  "url": "https://openalex.org/W4395467677",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2102522444",
      "name": "Fenglin Liu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116032841",
      "name": "Hong-Jian Zhou",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2364554682",
      "name": "Qingyu Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099665068",
      "name": "Jingfeng Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098171518",
      "name": "Xianfeng Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982861718",
      "name": "Chen Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1987661727",
      "name": "Ming Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2472586093",
      "name": "Haoming Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110436783",
      "name": "Yifan Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2200273890",
      "name": "Priyanka Nigam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2770432684",
      "name": "Sreyashi Nag",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104414332",
      "name": "Bing Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2807260057",
      "name": "Yining Hua",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2097962523",
      "name": "Xuan Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2757931577",
      "name": "Omid Rohanian",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2343297152",
      "name": "Anshul Thakur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121652955",
      "name": "Lei Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2057169223",
      "name": "David A Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2102522444",
      "name": "Fenglin Liu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2116032841",
      "name": "Hong-Jian Zhou",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2807260057",
      "name": "Yining Hua",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2757931577",
      "name": "Omid Rohanian",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2121652955",
      "name": "Lei Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2057169223",
      "name": "David A Clifton",
      "affiliations": [
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391334956",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2048296798",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W3173667512",
    "https://openalex.org/W4386530347",
    "https://openalex.org/W4378711639",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2404369708",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W4387560284",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W4391973028",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W4318925155",
    "https://openalex.org/W4391940656",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W4389267014",
    "https://openalex.org/W4366330426",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W4319301505",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3082330004",
    "https://openalex.org/W4385789461",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W2515682654",
    "https://openalex.org/W3186054896",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4381253519",
    "https://openalex.org/W4388014051",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W4362655261",
    "https://openalex.org/W4385681988",
    "https://openalex.org/W4323076364",
    "https://openalex.org/W4378468480",
    "https://openalex.org/W4387929270"
  ],
  "abstract": "Abstract The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the closeended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench . We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs 1 .",
  "full_text": "Large Language Models in Healthcare: A Comprehensive Benchmark\nFenglin Liu1, Hongjian Zhou1, Yining Hua2, Omid Rohanian1, Lei Clifton3, David A. Clifton1\n1Institute of Biomedical Engineering, University of Oxford, UK\n2 Harvard T.H. Chan School of Public Health, USA\n3 Nuffield Department of Population Health, University of Oxford, UK\nAbstract\nThe adoption of large language models (LLMs)\nto assist clinicians has attracted remarkable at-\ntention. Existing works mainly adopt the close-\nended question-answering task with answer op-\ntions for evaluation. However, in real clini-\ncal settings, many clinical decisions, such as\ntreatment recommendations, involve answering\nopen-ended questions without pre-set options.\nMeanwhile, existing studies mainly use accu-\nracy to assess model performance. In this pa-\nper, we comprehensively benchmark diverse\nLLMs in healthcare, to clearly understand their\nstrengths and weaknesses. Our benchmark con-\ntains seven tasks and thirteen datasets across\nmedical language generation, understanding,\nand reasoning. We conduct a detailed evalu-\nation of existing sixteen LLMs in healthcare\nunder both zero-shot and few-shot (i.e., 1,3,5-\nshot) learning settings. We report the results on\nfive metrics (i.e. matching, faithfulness, com-\nprehensiveness, generalizability, and robust-\nness) that are critical in achieving trust from\nclinical users. We further invite medical experts\nto conduct human evaluation.\n1 Introduction\nLarge language models (LLMs), such as ChatGPT\n(Brown et al., 2020; OpenAI, 2023b), LLaMA\n(Touvron et al., 2023a), and PaLM (Chowdhery\net al., 2022), are increasingly being recognized for\ntheir potential in healthcare to aid clinical decision-\nmaking and provide innovative solutions for com-\nplex healthcare problems (Patel et al., 2023; Shen\net al., 2023), e.g., discharge summary generation\n(Patel and Lam, 2023), health education (Safranek\net al., 2023), and care planning (Fleming et al.,\n2023). Several recent efforts have been made to\nfine-tune publicly available general LLMs, e.g.,\nLLaMA (Touvron et al., 2023b) and ChatGLM\n(Tsinghua KEG, 2023), to develop medical LLMs\n(Singhal et al., 2023a,c), resulting in ChatDoctor\n(Li et al., 2023b), MedAlpaca (Han et al., 2023),\nBenTsao (Wang et al., 2023a), and ClinicalCamel\n(Toma et al., 2023). Previous research shows that\nmedical LLMs outperform human experts across a\nvariety of medical tasks. In particular, MedPrompt\n(Nori et al., 2023) and MedPaLM-2 (Singhal et al.,\n2023b) have respectively achieved a competitive ac-\ncuracy of 90.2 and 86.5 compared to human experts\n87.0 (Wu et al.,2023) on the United States Medical\nLicensing Examination (USMLE) (Jin et al., 2021).\nAdmittedly, responsibility and reliability are es-\nsential requirements for tools designed to assist\nclinicians. Despite the promising results of existing\nmedical LLMs, several issues need to be addressed\nfor the responsible and reliable use of LLMs in\nassisting clinicians:\n• (i) Limited evaluation: Most existing works\nonly focus on evaluating LLM performance in\nthe close-ended medical question answering\n(QA) task, overlooking evaluation in other sce-\nnarios, such as medical language understand-\ning and generation (Thirunavukarasu et al.,\n2023; He et al.,2023; Zhou et al.,2023a). This\nlimited evaluation hinders a thorough under-\nstanding of LLM ability in diverse healthcare\napplications.\n• (ii) Limited metric: Existing works primarily\nutilize matching-based metrics (e.g., Accuracy\nand F1) to evaluate LLM performance. These\nmetrics fail to assess important attributes in\ngenerated responses, such as reliability and\ntrustworthiness, which are of paramount im-\nportance for clinicians and in regulatory ap-\nprovals that are essential for reliable deploy-\nment in clinical practice (Shen et al., 2023;\nKitamura, 2023).\n• (iii) Limited comparison: Existing works\nmainly compare LLM performance with their\nown basic models or use private datasets for\nevaluation (Tian et al., 2024). Such an ap-\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nproach falls short of providing a thorough\ncomparative analysis among different LLMs\nunder standardized conditions. Consequently,\nit hampers a comprehensive understanding of\nthe distinct advantages and limitations of vari-\nous LLMs in healthcare.\nAs a result, the accuracy, generalizability, and reli-\nability of existing LLMs in diverse healthcare ap-\nplications remain unclear. In response, (i) we con-\nstruct the BenchHealth from the representative pub-\nlic health data to benchmark LLMs in healthcare.\nAs shown in Table 1, BenchHealth encompasses\nthree different evaluation scenarios (i.e., reason-\ning, generation, and understanding) and includes\nseven popular downstream tasks and thirteen rep-\nresentative datasets; Previous popular benchmarks,\ne.g., BLUE (Peng et al., 2019) and BLURB (Gu\net al., 2021), only include the medical language\nunderstanding and close-ended question answering.\n(ii) In addition to the commonly used matching-\nrelated metrics, as shown in Table 2, we design\nadditional metrics to provide insights into the relia-\nbility of LLMs in clinical settings, i.e., analyzing\ntheir ability to provide faithfulness, comprehensive,\ngeneralized, and robust information; (iii) As shown\nin Table 3, we collect sixteen representative LLMs\nthat vary in the number of model parameters and\nstructural designs. We evaluate their performance\non BenchHealth for a comprehensive comparison.\nThe main insights from our experiments are:\n• Commercial LLMs vs. Public LLMs:\nClosed-source commercial LLMs, especially\nGPT-4, outperform all existing open-source\npublic LLMs on all tasks and datasets.\n• LLMs vs. State-of-the-art: All LLMs have\na strong reasoning ability to predict accurate\nanswers from the provided options, but per-\nform very poorly in open-ended questions, lan-\nguage generation, and language understanding\ntasks (i.e., there are significant gaps between\nthe state-of-the-art and LLM performance).\n• Medical LLMs vs. General LLMs: Fine-\ntuning general LLMs on medical data to ob-\ntain medical LLMs can improve the reasoning\nand understanding of medical data, but could\ndecrease the summarization ability of LLMs.\n•\nModel parameters: A larger number of\nmodel parameters can clearly improve per-\nformance on all tasks, datasets, and metrics.\n• Few-shot learning: It leads to significant im-\nprovements in performance on medical lan-\nguage reasoning and generation tasks, but im-\npairs performance on understanding tasks. On\nreasoning tasks, 1-shot or 3-shot learning per-\nforms the best; more examples do not lead to\nfurther improvements. On generation tasks,\nmore samples lead to better performance.\n• Clinical usefulness: Medical LLMs can pro-\nvide more faithful answers than general LLMs\n(avoiding misdiagnosis) and generalize well\nto diverse medical tasks; General LLMs can\nprovide more comprehensive answers than\nmedical LLMs, which may be due to “hal-\nlucinations”, thus avoiding missed diagnoses;\nGeneral LLMs have better robustness and can\ntherefore better understand a variety of diverse\ninputs compared to medical LLMs.\nOverall, our results show that among all types of\ntasks, the close-ended QA task is the only type\nof task in which current LLMs are comparable to\nstate-of-the-art models and human experts. How-\never, real-world open clinical practice diverges far\nfrom the structured nature of exam-taking. Clinical\ndecisions, such as diagnosis and treatment recom-\nmendations, are often confronted with open-ended\nquestions that lack pre-determined answer choices.\nThis paradigm shift from a controlled test environ-\nment to the unpredictable and subtle domain of\npatient care challenges the conventional approach,\ndemanding a more sophisticated understanding and\napplication of medical knowledge. Our results also\ndemonstrate that all LLMs display insufficient per-\nformance on crucial metrics necessary for ensuring\nthe trustworthiness of LLMs in clinical settings.\nThis unsatisfactory performance suggests that the\ncurrent state of LLMs falls short of readiness for\ndeployment in clinical settings to aid healthcare\nprofessionals. We hope that this work can offer\na holistic view of LLMs in healthcare, aiming to\nbridge the current gaps and advance the integration\nof LLMs in clinical applications.\n2 Benchmark\nOur benchmark is shown in Table 1.\n2.1 Medical Language Reasoning\nWe include the question answering and treatment\nrecommendation tasks in our benchmark.\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nScenarios Tasks Datasets Data Domains Sizes Matching Metrics\nMedical\nLanguage\nReasoning\nQuestion\nAnswering\nMedQA (USMLE) (Jin et al., 2021) Medical Licensing Examination 1,273 Accuracy\nMedMCQA (Pal et al., 2022) Medical Entrance Examination 4,183 Accuracy\nMMLU-Medicine (Hendrycks et al., 2020) Professional&College Medicine 272 Accuracy\nPubMedQA (Jin et al., 2019) Medical Literature 500 Accuracy\nTreatment\nRecommendation ChatDoctor (Li et al., 2023b) Patient-Clinician Conversations 796 Micro F1\nMedical\nLanguage\nGeneration\nRadiology Report\nSummarization\nMIMIC-CXR (Johnson et al., 2019) Radiography 3,269 ROUGE-L\nIU-Xray (Demner-Fushman et al., 2016) Radiography 341 ROUGE-L\nDischarge Instruction\nGeneration MIMIC-III (Johnson et al., 2016) Critical Care 3,633 BLEU-4\nMedical\nLanguage\nUnderstanding\nNamed Entity\nRecognition\nBC5-disease (Li et al., 2016) Scientific Literature 4,797 F1 entity-level\nNCBI-Disease (Do˘gan et al., 2014) Scientific Literature 940 F1 entity-level\nRelation\nExtraction\nDDI (Segura-Bedmar et al., 2013) Drug 5,716 Micro F1\nGAD (Becker et al., 2004) Genetic 534 Micro F1\nDocument\nClassification HoC (Baker et al., 2016) Scientific Literature 315 Micro F1\nTable 1: Overview of the benchmark BenchHealth for evaluating LLMs in healthcare.\nQuestion Answering aims to predict the correct\nanswer to the given question. For example, the\nmodel should answer ‘D’ to the question: “Which\nof the following conditions does not show mul-\ntifactorial inheritance? (A) Pyloric stenosis (B)\nSchizophrenia (C) Spina bifida (neural tube de-\nfects) (D) Marfan syndrome”. Thus, QA evaluates\nthe correctness of the medical knowledge learned\nby LLMs. We include four popular datasets, i.e.,\nMedQA (USMLE) (Jin et al., 2021), MedMCQA\n(Pal et al., 2022), MMLU-Medicine (Hendrycks\net al., 2020), PubMedQA (Jin et al., 2019).\nTreatment Recommendation is an open-ended\ncomplex task and requires the models to first under-\nstand the real-world patient-clinician conversations,\nin which the conversation describes the conditions\nand symptoms, and then recommend all possible\ndrugs for the treatment of patients. We use Chat-\nDoctor (Li et al., 2023b) for evaluation.\n2.2 Medical Language Generation\nWe evaluate two popular generation tasks, i.e., radi-\nology report summarization and discharge instruc-\ntion generation.\nRadiology Report Summarization aims to dis-\ntill a concise summary ‘Impression’ from the\nlengthy ‘Findings’ section in a radiology report.\n‘Findings\" contains detailed abnormal and normal\nclinical findings from radiology images like X-\nrays, CT scans, or MRI scans, and ‘Impression’\nhighlights the key diagnostic information and sig-\nnificant results, which are critical for accurate\ndiagnosis and treatment (Jing et al., 2018; Liu\net al., 2021b). We adopt the widely-used datasets,\nMIMIC-CXR (Johnson et al., 2019) and IU-Xray\n(Demner-Fushman et al., 2016).\nDischarge Instruction Generation aims to gen-\nerate a discharge instruction according to the pa-\ntient’s health records during hospitalization when\na patient is discharged from the hospital. The dis-\ncharge instruction should consider diagnosis, medi-\ncation, and procedure, e.g., demographics, labora-\ntory results, admission notes, nursing notes, radi-\nology notes, and physician notes (Liu et al., 2022).\nIt contains multiple instructions to help the patient\nor carer to manage their conditions at home. We\nfollow previous works (Liu et al., 2022) to use the\nMIMIC-III (Johnson et al., 2016) for evaluation.\n2.3 Medical Language Understanding\nWe include three representative tasks, i.e., named\nentity extraction, relation extraction, and document\nclassification, into our benchmark.\nNamed Entity Extraction can help organize and\nmanage patient data (Perera et al., 2020). For ex-\nample, it can extract medical entities mentioned\nin clinical notes and classify them according to\nrelevant symptoms, medication, dosage, and pro-\ncedures (Song et al., 2021). We adopt two repre-\nsentative datasets BC5-disease (Li et al., 2016) and\nNCBI-Disease (Do˘gan et al., 2014) for evaluation.\nRelation Extraction requires the model to iden-\ntify the relation between medical entities. The ex-\ntracted relations provide a solid basis to link the\nentities in a structured knowledge base or a stan-\ndardized terminology system, e.g., SNOMED CT\n(Chang and Mostafa, 2021; Donnelly et al., 2006)\nand UMLS (Bodenreider, 2004), which is critical\nin clinical decision support systems. We employ\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nMetrics\nMatching (Accuracy, F1, ROUGE-L, BLEU-4)\nMeasure the match between the generated content and the ground truth content.\nFaithfulness\nThe model can not generate content that appears reasonable but is factually\nincorrect and sometimes even harmful, thus avoiding misdiagnosis.\nComprehensiveness\nThe model can not leave out the important content, which can be used to alert\nclinicians to avoid missed diagnoses.\nRobustness\nFor the same scenario and task, the model provides consistent and reliable\nperformance across different formats/types/terminologies of input data (instead\nof overfitting specific data), measuring model stability to a range of inputs.\nGeneralizability\nThe model should maintain competitive performance across different scenarios\nand tasks (not limited to QA), to effectively assist clinicians.\nTable 2: Metrics used in our work for evaluation.\nthe DDI (Segura-Bedmar et al., 2013) and GAD\n(Becker et al., 2004) to evaluate LLMs.\nDocument Classification is a document-level\nlanguage understanding task aiming to predict mul-\ntiple correct labels to the input medical document,\nand can be used to improve clinical management\nsystems. We use the representative dataset HoC\n(Baker et al., 2016) for evaluation.\n3 Metrics\nAs shown in Table 2, we use five metrics to bench-\nmark LLMs in healthcare.\nMatching We follow the common practice to\ncalculate the classification accuracy, F1 score,\nROUGE-L (Lin, 2004), and BLEU-4 (Papineni\net al., 2002) to report the matching performance.\nDetails of used metrics for different tasks are shown\nin Table 1. However, matching-based metrics are\nnot specialized for evaluating the usefulness of the\nLLMs in clinical practice. To assist clinicians, it is\nnecessary to provide faithful, comprehensive, and\nrobust content (Thirunavukarasu et al.,2023; Arora\nand Arora, 2023; Safranek et al., 2023).\nFaithfulness LLMs are susceptible to “hallucina-\ntions” (Li et al., 2023a; Ji et al., 2023), i.e., fluent\ncontent that appears credible but factually incor-\nrect or potentially harmful. Therefore, it is crucial\nto ensure that LLMs generate faithful content, so\nthat the models do not generate contents that “do\nnot exist” according to clinicians (Liu et al., 2022).\nFor instance, if clinicians annotate the ground truth\ncontents as [Content_A, Content_B], but the model\ngenerates [Content_A, Content_C], it becomes ev-\nident that the model has introduced ‘Content_C’,\nwhich does not exist in the annotations. Such in-\naccuracies could lead to misdiagnoses, particularly\nTypes Methods # ParamsGeneral\nLLMs\nClaude-2 (Anthropic, 2023) -\nGPT-3.5-turbo (OpenAI, 2023a) -\nGPT-4 (OpenAI, 2023c) -\nChatGLM (Tsinghua KEG, 2023) 6B\nAlpaca (Taori et al., 2023) 7B\nVicuna (Chiang et al., 2023) 7B\nLLaMA-2-7B (Touvron et al., 2023c) 7B\nLLaMA-2-13B (Touvron et al., 2023c) 13B\nLLaMA-2-70B (Touvron et al., 2023c) 70B\nMedical\nLLMs\nChatGLM-Med (Wang et al., 2023b) 6B\nDoctorGLM (Xiong et al., 2023) 6B\nHuatuo (Zhang et al., 2023a) 7B\nChatDoctor (Li et al., 2023b) 7B\nBaize-Healthcare (Xu et al., 2023) 7B\nMedAlpaca-7B (Han et al., 2023) 7B\nMedAlpaca-13B (Han et al., 2023) 13B\nTable 3: We collect 16 LLMs, including 9 general LLMs\nand 7 medical LLMs, covering both open-source pub-\nlic LLMs and closed-source commercial LLMs (gray-\ncolored), across different numbers of parameters from\n6 billion to 70 billion, and different model backbones\n(GLM and GPT).\nwith clinicians who have less experience. We no-\ntice that the precision scores can measure the rates\nof such generated non-existent content. To this end,\nwe calculate and sum the precision scores of tasks\nto measure the ‘faithfulness’ scores\nComprehensiveness Given the ground truth con-\ntents [Content_A, Content_B], generating compre-\nhensive content [Content_A, Content_B] dimin-\nishes the chance of leaving out important content.\nThey can also be used to alert clinicians to avoid\nmissed diagnoses, improving precision medicine.\nThe recall score measures the percentage of gen-\nerated accurate content out of all correct answers.\nTherefore, to evaluate the comprehensiveness of\nmodel-generated contents, we calculate and sum\nthe recall scores of different tasks to measure the\n‘comprehensiveness’ scores.\nRobustness Clinicians may express the same\ntexts, questions, and conditions using varying for-\nmats and terminologies. For example, in the ra-\ndiology report summarization task, both ‘enlarge-\nment of the cardiac silhouette’ and ‘the heart size\nis enlarged’ express the condition ’cardiomegaly’.\nTherefore, the model needs to accurately identify\n’cardiomegaly’ for both these two different inputs.\nAs shown in Table 1, for the report summarization\ntask, we can compute the variance in model perfor-\nmance on the two datasets, IU-Xray and MIMIC-\nCXR (collected from different hospitals and re-\ngions, thus having different expression habits), to\nobtain the robustness of the model on this task. As\na result, to measure the robustness scores of the\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nPrompts Sources\nMedQA (USMLE), MedMCQA, MMLU-Medicine\nThe following are multiple-choice questions about medical knowledge. Solve them in a step-by-step fashion, starting by summarizing the available information. Output a single option\nfrom the four options as the final answer. (Singhal et al., 2023b)\nPubMedQA\nThis is a multiple-choice question about medical research. Determine the answer to the question based on the strength of the scientific evidence provided in the context. Valid answers\nare yes, no, or maybe. Answer yes or no if the evidence in the context supports a definitive answer. Answer maybe if the evidence in the context does not support a definitive answer,\nsuch as when the context discusses both conditions where the answer is yes and conditions where the answer is no.\n(Singhal et al., 2023b)\nChatDoctor\n\"task\": \"Your task is to list the medications based on the provided content related to the symptom or disease mentioned in the question. Understand the question, extract relevant\ninformation, analyze it, and provide a concise and accurate answer.\",\n\"answer format\": Analysis: Provide an analysis that logically leads to the answer based on the relevant content. Final Answer: Provide the final answer, which should be a list of\nmedications related to the symptom or disease.\n\"not to dos\": \"Do not make assumptions not supported by the content. Avoid providing personal opinions or interpretations. Summarize and interpret the information as objectively and\naccurately as possible. You are providing an analysis, not diagnosing or treating medical conditions.\"\n(Zhou et al., 2023b)\nMIMIC-CXR, IU-Xray\nYou are a helpful radiology assistant. The following are questions about radiology reports. Summarize the findings in the report into diagnostic statements in a coherent paragraph. Given\nthe findings: {Findings}. Q: Summarize the findings. A: (Tu et al., 2023)\nMIMIC-III\nProvide plain language discharge instructions, containing the following three main components from patients’ perspective: (1) What is my main health condition? (i.e., why was I in the\nhospital?) (2) What do I need to do? (i.e., how do I manage at home, how should I best care for myself, what medications to take, and which appointments to go to next (if available)) (3)\nWhy is it important for me to do this?\n(Fleming et al., 2023)\nBC5-disease, NCBI-Disease\nParagraph: <Paragraph ID> | <text> Please extract all chemicals/genes/diseases mentioned in the paragraph. Answer with the format \"<Paragraph ID> | <recognized entities>\" (Chen et al., 2023)\nDDI\n@DRUG$ an anionic-binding resin, has a considerable effect in lowering the rate and extent of @DRUG$ bioavailability.\nTarget: You need to identify the relationship between the two @DRUG$.\nRequire: you must start with choose one from the [“mechanism,” “effect,” “advice,” “int,” “None”],\nSpecific Explanation: mechanism: This type is used to annotate DDIs that are described by their PK mechanism (e.g. Grepafloxacin may inhibit the metabolism of theobromine). effect:\nThis type is used to annotate DDIs describing an effect (e.g. In uninfected volunteers, 46% developed rash while receiving SUSTIV A and clarithromycin) or a PD mechanism (e.g.\nChlorthali done may potentiate the action of other antihypertensive drugs). advice: This type is used when a recommendation or advice Regarding a drug interaction is given (e.g.\nUROXATRAL should not be used in combination with other alpha-blockers). int: This type is used when a DDI appears in the text without providing any additional information (e.g. the\ninteraction of Omeprazole and ketoconazole have been established). You should mark the final category with < >.\n(Chen et al., 2023)\nGAD\nGiven a sentence that introduces a gene (denoted as ”@GENE$”) and a disease (denoted as ”@DISEASE$”), predict whether the gene and disease have a relation or not. The relation\nbetween the gene and disease can be any functional, causal, or associative connection. If there is a relation, then the label should be “Yes”, otherwise “No”. (Tang et al., 2023)\nHoC\ndocument: < text>; target: The correct category for this document is ? You must choose from the given list of answer categories (introduce what each category is ...)” (Chen et al., 2023)\nTable 4: The prompts used for different evaluation tasks and datasets. We collect optimal prompts from existing\nstate-of-the-art work.\nlanguage reasoning, generation, and understanding\nscenarios, we respectively calculate the variance\nin model performance on the representative ques-\ntion answering, radiology report summarization,\nand named entity recognition tasks. Finally, we\nsum the variance up to obtain the overall robust-\nness scores of the LLMs, reflecting whether their\naccuracy is significantly impacted by variations in\nthe inputs.\nGeneralizability To effectively support clini-\ncians in different settings, LLMs should perform\nwell in a wide range of scenarios and tasks (not\nlimited to QA). For clarity, we directly average all\nthe matching scores to obtain the ‘generalizabil-\nity’ scores to evaluate how LLMs perform across a\nrange of scenarios and tasks.\n4 Large Language Models\nAs shown in Table3, to provide a comprehensive\nevaluation of LLMs in healthcare, we evaluate both\nthe general and medical LLMs. Please refer to\nZhao et al. (2023); Yang et al. (2023a) and Zhou\net al. (2023a); He et al. (2023) for a detailed in-\ntroduction to general LLMs and medical LLMs,\nrespectively.\nGeneral Large Language Models We include\nnine general LLMs, including three leading closed-\nsource commercial LLMs, i.e., Claude-2 (An-\nthropic, 2023), GPT-3.5-turbo (OpenAI, 2023a),\nand GPT-4 (OpenAI, 2023c), and six open-source\npublic LLMs, i.e., ChatGLM (Tsinghua KEG,\n2023; Du et al., 2022; Zeng et al., 2022), Alpaca\n(Taori et al., 2023), Vicuna (Chiang et al., 2023),\nand LLaMA-2-7B/13B/70B (Touvron et al., 2023c).\nThese general LLMs are trained on a large general-\npurpose corpus with more than 1T tokens (Zhao\net al., 2023; Yang et al., 2023a; Zhou et al., 2023a).\nMedical Large Language Models We choose\nseven medical LLMs with different numbers of pa-\nrameters and different types of fine-tuning data.\nIn detail, as shown in Table 3, ChatGLM-Med\n(Wang et al., 2023b) and DoctorGLM (Xiong et al.,\n2023) are fine-tuned on the ChatGLM-6B (Ts-\ninghua KEG, 2023; Du et al., 2022; Zeng et al.,\n2022) using QA pairs and dialogues, respectively.\nHuatuo (Zhang et al., 2023a), ChatDoctor (Li et al.,\n2023b), Baize-Healthcare (Xu et al., 2023), and\nMedAlpaca-7B/13B (Han et al., 2023) are built\nupon the LLaMA-series models. During fine-\ntuning, both Huatuo and MedAlpaca employ the\nQA pairs collected from the medical knowledge\ngraphs and medical texts, respectively. ChatDoc-\ntor and Baize-Healthcare are fine-tuned on medical\ndialogues generated by commercial LLMs (e.g.,\nChatGPT).\nPrompts Prompt designs are crucial for the per-\nformance of LLMs. Therefore, to ensure LLMs\nachieve optimal performance across different tasks,\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nTypes Methods # Params\nLanguage Reasoning Language Generation Language Understanding\nMedQA MedMCQA MMLU PubMedQA ChatDoc. MIMIC-CXR IU-Xray MIMIC-III BC5 NCBI DDI GAD HoC\nTask-specific SOTA - 44.6 43.0 - 60.2 - 46.1 67.9 30.5 90.0 89.4 84.1 84.0 85.1\nGeneral\nLLMs\nClaude-2 - 65.1 60.3 78.7 70.8 9.1 13.3 9.4 26.1 52.9 44.2 50.4 50.7 70.8\nGPT-3.5-turbo - 61.2 59.4 73.5 70.2 7.3 14.1 10.3 28.6 52.3 46.1 49.3 50.8 66.4\nGPT-4 - 81.2 74.6 90.8 76.6 13.7 15.2 11.4 30.1 65.7 55.3 62.6 64.4 78.1\nChatGLM 6B 25.7 24.2 33.5 53.0 2.9 13.3 7.5 18.6 37.2 31.9 34.1 36.6 47.5\nAlpaca 7B 34.2 30.1 40.8 65.2 3.5 12.6 8.7 20.4 41.2 36.5 37.4 36.9 52.6\nVicuna 7B 34.5 33.4 43.4 64.8 2.6 13.8 8.2 23.4 44.5 37.0 39.4 41.2 53.8\nLLaMA-2-7B 7B 32.9 30.6 42.3 63.4 3.3 12.3 8.6 20.2 40.1 34.8 37.9 39.3 48.6\nLLaMA-2-13B 13B 38.1 35.5 46.0 66.8 4.8 12.0 9.1 21.1 46.6 38.3 39.7 41.2 55.9\nLLaMA-2-70B 70B 45.8 42.7 54.0 67.4 5.5 13.9 8.0 23.2 47.8 41.5 45.6 44.7 63.2\nMedical\nLLMs\nChatGLM-Med 6B 27.3 25.8 35.3 58.8 3.3 9.5 4.7 19.4 40.5 35.2 37.4 33.6 49.3\nDoctorGLM 6B 25.9 23.1 36.8 57.4 3.1 6.5 3.5 15.2 38.7 33.6 35.6 34.7 50.8\nHuatuo 7B 28.4 24.8 31.6 61.0 3.8 8.7 3.8 17.8 43.6 37.5 40.1 38.2 50.2\nChatDoctor 7B 33.2 31.5 40.4 63.8 5.3 8.9 4.2 20.7 45.8 40.9 41.2 40.1 55.7\nBaize-Healthcare 7B 34.9 31.3 41.9 64.4 4.7 9.8 4.4 19.3 44.4 38.5 41.9 45.8 54.5\nMedAlpaca-7B 7B 35.1 32.9 48.5 62.4 4.8 10.4 7.6 22.1 47.3 39.0 43.5 44.0 58.7\nMedAlpaca-13B 13B 37.3 35.7 51.5 65.6 5.1 11.7 8.6 24.7 49.2 41.6 44.1 44.5 59.4\nTable 5: Performance (measured by traditional matching scores) of LLMs under the zero-shot learning setting. We\ndenote the results of three commercial LLMs (gray-colored) as upper bounds on the performance of open-source\npublic LLMs. For comparison, in the first row, we also report the results of task-specific state-of-the-art (SOTA)\nmodels, which are fine-tuned in a fully supervised manner on downstream data and tasks. The close-ended QA task\nis the only task for which the current LLMs are comparable to the SOTA.\nwe use tailored prompts for each task, so that LLMs\ncan effectively understand the task and questions.\nIn implementation, we adopt prompts used in the\ncurrent state-of-the-art work for each task in the\nbenchmark to evaluate the LLMs. Table 4 shows\nthe prompts we used and their references.\n5 Results\n5.1 Medical Language Reasoning\nFrom Table 5, we observe that on all datasets, the\nthree leading commercial LLMs, i.e., Claude-2,\nGPT-3.5-turbo, and GPT-4, significantly outper-\nform other LLMs, general or medical. In partic-\nular, on the close-ended QA task with provided\noptions, GPT-4 even achieves a competitive accu-\nracy of 81.2 compared to human experts (87.0) (Wu\net al., 2023). In terms of open-source public LLMs,\nmedical LLMs, e.g., ChatGLM-Med and Doctor-\nGLM, achieve better results than general LLMs,\ne.g., ChatGLM, on all datasets. It indicates that\nfine-tuning the general LLMs on medical data can\nimprove their performances on reasoning tasks.\nDiscussion The results show that, on all close-\nended QA datasets, all LLMs significantly outper-\nform existing task-specific SOTA models, e.g., Pub-\nMedBERT (Gu et al., 2022). It proves that existing\nLLMs have a strong reasoning ability to give ac-\ncurate answers from the options. However, on the\nopen-ended treatment recommendation task, com-\npared with SOTA models, all LLMs achieve poor\nF1 scores (<15%) on the ChatDoctor dataset. This\nindicates a considerable need for advancement be-\nfore LLMs can be integrated into actual clinical\ndecision-making processes.\n5.2 Medical Language Generation\nThis application is particularly useful in reducing\nthe heavy workload of clinicians in medical text\nwriting. Table 5 show that, among all LLMs, GPT-\n4 (OpenAI, 2023c) consistently achieves the best\nresults on all generation tasks, showcasing its ex-\nceptional capability in capturing and summariz-\ning important clinical findings compared to other\nLLMs. Nonetheless, the task-specific SOTA model\n(Hu et al., 2022) achieves 46.1 and 67.9 ROUGE-L\nscores on MIMIC-CXR and IU-Xray, respectively,\nsignificantly higher than all LLMs.\nDiscussion On the MIMIC-CXR and IU-Xray ra-\ndiology report summarization datasets, most med-\nical LLMs that have been fine-tuned on medical\ndata, perform worse than general LLMs. In con-\ntrast, on the discharge instruction generation task,\nwhich requires the model to understand various\ntypes of medical data to provide accurate discharge\ninstructions, medical LLMs perform better than the\ngeneral LLMs. These observations may imply that\nthe instruction fine-tuning on medical data could\ndecrease the summarization ability of LLMs, but\nimprove the understanding of medical data.\n5.3 Medical Language Understanding\nAll LLMs exhibit poor performances in this sce-\nnario, including named entity extraction, relation\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \n25.0\n29.0\n33.0\n37.0\n41.0\n45.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nAverage Scores\n(a) Language Reasoning 0-shot 1-shot 3-shots 5-shots\n2.0\n7.0\n12.0\n17.0\n22.0\n27.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nAverage Scores\n(b) Language Generation 0-shot 1-shot 3-shots 5-shots\n25.0\n30.0\n35.0\n40.0\n45.0\n50.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nAverage Scores\n(c) Language Understanding 0-shot 1-shot 3-shots 5-shots\nFigure 1: Performance (measured by traditional matching scores) of LLMs under few (1,3,5)-shot learning setting.\nextraction, and document classification tasks. For\nexample, the best results of LLMs are achieved\nby GPT-4 on the BC5-Disease and NCBI-Disease\ndatasets, with 65.7 and 55.3 F1 scores, which are\nsignificantly far from current state-of-the-art per-\nformances, i.e., 90.0 F1 score achieved by Science-\nBERT (Beltagy et al., 2019) and 89.4 F1 score\nachieved by BioBERT (Lee et al., 2020), respec-\ntively. The medical LLMs have better language\nunderstanding than general LLMs in healthcare.\nWith the same parameters, all medical LLMs out-\nperform the general LLMs over datasets.\nDiscussion The inadequate performance of all\nLLMs may be attributed to the missing of task-\nspecific supervised training and thus a lack of nec-\nessary medical knowledge, such as the medical\nterminologies for named entity extraction, the med-\nical relations between drugs, conditions, and symp-\ntoms for relation extraction, and the background of\ndiseases for document classification (Chen et al.,\n2023). As a result, existing LLMs fail to compre-\nhend texts that typically require extensive expert\nknowledge to interpret. This observation under-\nscores the effectiveness of efficiently using clinical-\nstandard knowledge of diseases, symptoms, and\nmedications, to fine-tune the LLMs.\n5.4 Few-shot Learning Setting\nWe further evaluate the performance of LLMs on\nthe few-shot learning settings, i.e., 1-shot, 3-shot,\nand 5-shot learning settings. We analyze the three\nscenarios, i.e., reasoning, generation, and under-\nstanding. For reasoning and understanding sce-\nnarios, we calculate the average performance of\nall datasets under that scenario to report the per-\nformance of LLMs. For the generation scenario,\nsince the text length of the input for the discharge\nreport generation task is long, we do not report the\nfew-shot learning performance on the MIMIC-III\ndataset. Therefore, we compute the average of the\nperformance of the other generation datasets to ob-\ntain the generation results of the LLMs. The results\nare reported in Figure 1.\n(a) We observe that the few-shot learning can\nsignificantly boost the performances of LLMs in\nlanguage reasoning. It proves the effectiveness of\nfew-shot learning, in which the provided examples\ncould provide efficient knowledge of medical rea-\nsoning to reason about the correct answers. How-\never, most LLMs achieve the best results under the\n1-shot and 3-shot settings. More examples (e.g., 5\nshots) may not only make it difficult for LLMs to\ndeal with long inputs but also potentially introduce\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \n25.0\n30.0\n35.0\n40.0\n45.0\n50.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nFaithfulness\nGeneral LLMs Medical LLMs\n25.0\n28.0\n31.0\n34.0\n37.0\n40.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nGeneralizability\nGeneral LLMs Medical LLMs\n15.0\n17.0\n19.0\n21.0\n23.0\n25.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nRobustness\nGeneral LLMs Medical LLMs\n25.0\n29.0\n33.0\n37.0\n41.0\n45.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nComprehensiveness\nGeneral LLMs Medical LLMs\n25.0\n30.0\n35.0\n40.0\n45.0\n50.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nFaithfulness\nGeneral LLMs Medical LLMs\n25.0\n28.0\n31.0\n34.0\n37.0\n40.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nGeneralizability\nGeneral LLMs Medical LLMs\n15.0\n17.0\n19.0\n21.0\n23.0\n25.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nRobustness\nGeneral LLMs Medical LLMs\n25.0\n29.0\n33.0\n37.0\n41.0\n45.0\nChatGLM\nAlpaca\nVicuna\nLLaMA-2-7B\nLLaMA-2-13B\nLLaMA-2-70B\nChatGLM-Med\nDoctorGLM\nHuatuo\nChatDoctor\nBaize-Healthcare\nMedAlpaca-7B\nMedAlpaca-13B\nHealthLLaMA\nScore (%)\nComprehensiveness\nGeneral LLMs Medical LLMs\nFigure 2: Performance of existing LLMs on our BenchHealth benchmark in terms of clinical usefulness. Higher\nfaithfulness, comprehensiveness, and generalizability scores are better. Lower robustness scores are better.\nnoise into the LLMs, i.e., the provided examples\nmay not be relevant to the input problem, thus af-\nfecting performance. As a result, providing more\nexamples does not lead to further improvements.\n(b) In text generation, few-shot learning can di-\nrectly demonstrate how to capture and summarize\nimportant clinical information and provide a desir-\nable writing style. As a result, few-shot learning\ncan consistently and substantially improve the per-\nformance of the LLMs, with more samples leading\nto better performance. It proves the effectiveness\nof using few-shot learning to significantly boost the\nperformance of medical text generation.\n(c) However, in the case of language understand-\ning, it clearly shows that few-shot learning impairs\nperformance. This may be because, in language un-\nderstanding tasks, the characteristics of different in-\nput data are usually very different from each other,\nresulting in the entities or knowledge involved in\nthe examples often being irrelevant to the test data,\nmaking the model unable to effectively utilize the\nexamples to improve performance.\n5.5 Clinical Usefulness\nIn Figure 2, we report the performances of LLMs\nin terms of clinical usefulness.\n(a) In terms of faithfulness, all medical LLMs\noutperform general LLMs, resulting in providing\nmore faithful answers than general LLMs, avoiding\nmisdiagnosis.\n(b) In contrast, general LLMs demonstrate better\nresults than medical LLMs in terms of compre-\nhensiveness, likely due to their susceptibility to\n“hallucinations”, meaning the LLMs tend to gen-\nerate massive content including both correct and\nincorrect information.\n(c) In terms of generalizability, we notice that\nmedical LLMs achieve optimal results, showing\nthat fine-tuning using the medical data can boost\nthe overall performance of LLMs in healthcare.\n(d) The general LLMs have better robustness\nand achieve lower robustness values than medi-\ncal LLMs. For example, ChatGLM achieves 21.1\npoints, lower than ChatGLM-Med (22.4) and Doc-\ntorGLM (21.3).\nDiscussion We hypothesize that the better com-\nprehensiveness of the general LLMs could poten-\ntially be due to that a certain degree of hallucina-\ntion may offer benefits. This hypothetical advan-\ntage might assist clinicians by providing a broader\nspectrum of diagnostic suggestions, which could\nbe advantageous in the diagnosis and treatment of\nrare diseases. However, any content generated by\nLLMs must be supported by factual knowledge and\nevidence to provide reliable, rather than mislead-\ning, results. General LLMs have better robustness,\nand thus can better understand a variety of diverse\ninputs. We speculate that the reason may be the\nlimited diversity of fine-tuning data and tasks used\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nTypes Methods # Params\nFaithfulness Comprehensiveness Generalizability Robustness\nClaude-2 GPT-3.5 GPT-4 Claude-2 GPT-3.5 GPT-4 Claude-2 GPT-3.5 GPT-4 Claude-2 GPT-3.5 GPT-4\nGeneral\nLLMs\nAlpaca 7B 29.5 35.0 9.5 29.0 40.0 18.0 21.0 26.0 17.5 32.0 42.5 23.5\nVicuna 7B 34.0 39.5 14.0 33.5 43.5 30.5 35.5 41.0 22.0 39.0 37.5 20.0\nLLaMA-2-7B 7B 32.5 37.0 13.0 40.5 48.0 26.5 29.0 34.5 21.5 44.5 46.5 27.5\nLLaMA-2-13B 13B 39.5 44.0 16.0 47.0 52.5 37.0 43.0 49.0 32.5 52.5 49.5 31.0\nLLaMA-2-70B 70B 43.0 49.5 19.5 52.5 58.0 41.5 54.5 56.5 39.0 58.5 61.0 45.0\nMedical\nLLMs\nChatDoctor 7B 38.0 46.5 23.0 18.0 16.5 8.0 25.0 27.0 15.5 20.0 24.5 11.0\nBaize-Healthcare 7B 44.5 52.0 28.5 29.5 33.5 18.5 39.5 45.5 28.0 36.0 42.0 22.5\nMedAlpaca-7B 7B 47.0 55.5 31.5 26.0 33.0 15.5 33.5 31.0 19.0 30.5 37.5 17.0\nMedAlpaca-13B 13B 50.5 61.0 34.0 31.0 35.5 19.5 38.5 39.0 20.5 37.5 43.0 24.0\nTable 6: Performance of human evaluation on our BenchHealth benchmark. We compare open-source public LLM\nwith three leading commercial LLMs. All values are Win+Tie rates for public LLM. Higher is better in all columns.\nto develop medical LLMs (Rohanian et al., 2023).\nIt leads to overfitting to specific types of data and\nthus reduces the robustness of the model during\ninstruction fine-tuning.\n5.6 Human Evaluation\nWe invite two junior annotators (medical students)\nand a senior annotator (medical professor) to con-\nduct the human evaluation. All three annotators\nhave sufficient medical knowledge. In implementa-\ntions, we follow previous works (Li et al., 2023b;\nZhang et al., 2023b) to randomly select 200 real\npatient-doctor conversations from Li et al. (2023b).\nWe require the LLMs to simulate a doctor and pro-\nvide responses based on various patient inquiries.\nEach junior annotator is assigned to independently\ncompare the responses from public LLMs and\nthose from the leading commercial LLMs, i.e.,\nClaude-2, GPT-3.5-turbo, and GPT-4, in terms of\nthe perceived quality of the responses. It includes\nfaithfulness, comprehensiveness, generalizability,\nand robustness. The senior annotator re-evaluates\nthe cases that are difficult for junior annotators\nto decide. The annotators are unaware of which\nmodel generates these reports. We report the results\n(win+tie rates) of public LLMs in Table 6.\nWe observe that with the same number of model\nparameters, medical LLMs outperform general\nLLMs in terms of faithfulness and generalizability,\nbut underperform general LLMs in comprehensive-\nness and robustness. These results are consistent\nwith those shown in Figure 2, which demonstrates\nthe validity and appropriateness of our metric and\nbenchmark.\n6 Conclusions\nThis paper introduces BenchHealth, a healthcare\nbenchmark encompassing medical language rea-\nsoning, generation, and comprehension scenarios.\nIt employs metrics that extend beyond mere accu-\nracy, aiming to evaluate the utility and reliability\nof LLMs for clinical applications. Although LLMs\nhave made promising advances, our analysis uncov-\ners a gap between the capabilities of LLMs and the\nrequirements for clinical application, especially in\nopen-ended non-QA tasks that lack pre-determined\nanswer choices, underscoring the challenges LLMs\nface in providing reliable support in healthcare.\nLimitations\nA limitation of this work is that the recent develop-\nment of LLMs is rapid and we do not evaluate the\nlatest LLMs, e.g., GPT-4.5 and Qwen (Bai et al.,\n2023), and medical LLMs, e.g., Zhongjing (Yang\net al., 2023b) and Qilin-Med (Ye et al., 2023).\nReferences\nAnthropic. 2023. Claude-2.\nAnmol Arora and Ananya Arora. 2023. The promise\nof large language models in health care. The Lancet,\n401(10377):641.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenhang Ge, Yu Han,\nFei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang\nLin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang\nLu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren,\nXuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang\nWu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian\nYang, Jian Yang, Shusheng Yang, Yang Yao, Bowen\nYu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei\nZhang, Xing Zhang, Yichang Zhang, Zhenru Zhang,\nChang Zhou, Jingren Zhou, Xiaohuan Zhou, and\nTianhang Zhu. 2023. Qwen technical report. ArXiv,\nabs/2309.16609.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nHögberg, Ulla Stenius, and Anna Korhonen. 2016.\nAutomatic semantic classification of scientific litera-\nture according to the hallmarks of cancer. Bioinfor-\nmatics, 32(3):432–440.\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nKevin G Becker, Kathleen C Barnes, Tiffani J Bright,\nand S Alex Wang. 2004. The genetic association\ndatabase. Nature genetics, 36(5):431–432.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. arXiv\npreprint arXiv:1903.10676.\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Annual Conference on Neural Informa-\ntion Processing Systems.\nEunsuk Chang and Javed Mostafa. 2021. The use of\nsnomed ct, 2013-2020: a literature review. Journal\nof the American Medical Informatics Association,\n28(9):2017–2026.\nQijie Chen, Haotong Sun, Haoyang Liu, Yinghui Jiang,\nTing Ran, Xurui Jin, Xianglu Xiao, Zhimin Lin,\nHongming Chen, and Zhangmin Niu. 2023. An\nextensive benchmark study on biomedical text gen-\neration and mining with chatgpt. Bioinformatics,\n39(9):btad557.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nDina Demner-Fushman, Marc D. Kohli, Marc B.\nRosenman, Sonya E. Shooshan, Laritza Rodriguez,\nSameer K. Antani, George R. Thoma, and Clement J.\nMcDonald. 2016. Preparing a collection of radiology\nexaminations for distribution and retrieval. J. Am.\nMedical Informatics Assoc., 23(2):304–310.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nKevin Donnelly et al. 2006. Snomed-ct: The advanced\nterminology and coding system for ehealth. Studies\nin health technology and informatics, 121:279.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nScott L Fleming, Alejandro Lozano, William J\nHaberkorn, Jenelle A Jindal, Eduardo P Reis, Rahul\nThapa, Louis Blankemeier, Julian Z Genkins, Ethan\nSteinberg, Ashwin Nayak, et al. 2023. Medalign:\nA clinician-generated dataset for instruction follow-\ning with electronic medical records. arXiv preprint\narXiv:2308.14089.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jian-\nfeng Gao, and Hoifung Poon. 2022. Domain-specific\nlanguage model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Heal.,\n3(1):2:1–2:23.\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioan-\nnou, Paul Grundmann, Tom Oberhauser, Alexander\nLöser, Daniel Truhn, and Keno K Bressem. 2023.\nMedalpaca–an open-source collection of medical\nconversational ai models and training data. arXiv\npreprint arXiv:2304.08247.\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan,\nMengling Feng, and Erik Cambria. 2023. A survey\nof large language models for healthcare: from data,\ntechnology, and applications to accountability and\nethics. arXiv preprint arXiv:2310.05694.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nJinpeng Hu, Zhihong Chen, Yang Liu, Xiang Wan, and\nTsung-Hui Chang. 2022. Improving radiology sum-\nmarization with radiograph and anatomy prompts.\narXiv preprint arXiv:2210.08303.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nBaoyu Jing, Pengtao Xie, and Eric P. Xing. 2018. On the\nautomatic generation of medical imaging reports. In\nAnnual Meeting of the Association for Computational\nLinguistics.\nAlistair E. W. Johnson, Tom J. Pollard, Lu Shen,\nLi wei H. Lehman, Mengling Feng, Moham-\nmad Mahdi Ghassemi, Benjamin Moody, Peter\nSzolovits, Leo Anthony Celi, and Roger G. Mark.\n2016. MIMIC-III, a freely accessible critical care\ndatabase. Scientific Data, 3.\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-\nying Deng, Roger G Mark, and Steven Horng.\n2019. Mimic-cxr, a de-identified publicly available\ndatabase of chest radiographs with free-text reports.\nScientific data, 6(1):317.\nFelipe C Kitamura. 2023. Chatgpt is shaping the future\nof medical writing but still requires human judgment.\nRadiology, page 230171.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023a. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models. arXiv e-prints, pages arXiv–2305.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve\nJiang, and You Zhang. 2023b.Chatdoctor: A medical\nchat model fine-tuned on a large language model\nmeta-ai (llama) using medical domain knowledge.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In ACL.\nFenglin Liu, Shen Ge, and Xian Wu. 2021a.\nCompetence-based multimodal curriculum learning\nfor medical report generation. In Annual Meeting of\nthe Association for Computational Linguistics.\nFenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian\nZou. 2021b. Exploring and distilling posterior and\nprior knowledge for radiology report generation. In\nIEEE Conference on Computer Vision and Pattern\nRecognition.\nFenglin Liu, Bang Yang, Chenyu You, Xian Wu, Shen\nGe, Zhangdaihong Liu, Xu Sun, Yang Yang, and\nDavid Clifton. 2022. Retrieve, reason, and refine:\nGenerating accurate and faithful patient instructions.\nAdvances in Neural Information Processing Systems,\n35:18864–18877.\nFenglin Liu, Chenyu You, Xian Wu, Shen Ge, Sheng\nWang, and Xu Sun. 2021c. Auto-encoding knowl-\nedge graph for unsupervised medical report genera-\ntion. In Advances in Neural Information Processing\nSystems.\nHarsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carig-\nnan, Richard Edgar, Nicolo Fusi, Nicholas King,\nJonathan Larson, Yuanzhi Li, Weishung Liu, et al.\n2023. Can generalist foundation models outcom-\npete special-purpose tuning? case study in medicine.\narXiv preprint arXiv:2311.16452.\nOpenAI. 2023a. Chatgpt [large language model].\nhttps://chat.openai.com.\nOpenAI. 2023b. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nOpenAI. 2023c. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on Health,\nInference, and Learning, pages 248–260. PMLR.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a Method for automatic\nevaluation of machine translation. In ACL.\nSajan B Patel and Kyle Lam. 2023. Chatgpt: the future\nof discharge summaries? The Lancet Digital Health,\n5(3):e107–e108.\nSajan B Patel, Kyle Lam, and Michael Liebrenz. 2023.\nChatgpt: friend or foe. Lancet Digit. Health, 5:e102.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and elmo on ten bench-\nmarking datasets. In BioNLP@ACL, pages 58–65.\nNadeesha Perera, Matthias Dehmer, and Frank Emmert-\nStreib. 2020. Named entity recognition and rela-\ntion detection for biomedical information extraction.\nFrontiers in cell and developmental biology, page\n673.\nOmid Rohanian, Mohammadmahdi Nouriborji, and\nDavid A Clifton. 2023. Exploring the effectiveness\nof instruction tuning in biomedical language process-\ning. arXiv preprint arXiv:2401.00579.\nConrad W Safranek, Anne Elizabeth Sidamon-Eristoff,\nAidan Gilson, and David Chartash. 2023. The role\nof large language models in medical education: ap-\nplications and implications.\nIsabel Segura-Bedmar, Paloma Martínez Fernández, and\nMaría Herrero Zazo. 2013. Semeval-2013 task 9: Ex-\ntraction of drug-drug interactions from biomedical\ntexts (ddiextraction 2013). In Proceedings of the\nSeventh International Workshop on Semantic Evalua-\ntion (SemEval 2013). Association for Computational\nLinguistics.\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \nYiqiu Shen, Laura Heacock, Jonathan Elias, Keith D\nHentel, Beatriu Reig, George Shih, and Linda Moy.\n2023. Chatgpt and other large language models are\ndouble-edged swords. Radiology, page 230163.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2023a. Large language models encode clinical\nknowledge. Nature, pages 1–9.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl,\nHeather Cole-Lewis, Darlene Neal, Mike Schaeker-\nmann, Amy Wang, Mohamed Amin, Sami Lachgar,\nPhilip Mansfield, Sushant Prakash, Bradley Green,\nEwa Dominowska, Blaise Aguera y Arcas, Nenad\nTomasev, Yun Liu, Renee Wong, Christopher Sem-\nturs, S. Sara Mahdavi, Joelle Barral, Dale Web-\nster, Greg S. Corrado, Yossi Matias, Shekoofeh Az-\nizi, Alan Karthikesalingam, and Vivek Natarajan.\n2023b. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,\nEllery Wulczyn, Le Hou, Kevin Clark, Stephen\nPfohl, Heather Cole-Lewis, Darlene Neal, et al.\n2023c. Towards expert-level medical question an-\nswering with large language models. arXiv preprint\narXiv:2305.09617.\nBosheng Song, Fen Li, Yuansheng Liu, and Xiangxiang\nZeng. 2021. Deep learning methods for biomed-\nical named entity recognition: a survey and qual-\nitative comparison. Briefings in Bioinformatics,\n22(6):bbab282.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and\nXia Hu. 2023. Does synthetic data generation of\nllms help clinical text mining? arXiv preprint\narXiv:2303.04360.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nArun James Thirunavukarasu, Darren Shu Jeng Ting,\nKabilan Elangovan, Laura Gutierrez, Ting Fang Tan,\nand Daniel Shu Wei Ting. 2023. Large language\nmodels in medicine. Nature medicine, 29(8):1930–\n1940.\nShubo Tian, Qiao Jin, Lana Yeganova, Po-Ting Lai,\nQingqing Zhu, Xiuying Chen, Yifan Yang, Qingyu\nChen, Won Kim, Donald C Comeau, et al. 2024. Op-\nportunities and challenges for chatgpt and large lan-\nguage models in biomedicine and health. Briefings\nin Bioinformatics, 25(1):bbad493.\nAugustin Toma, Patrick R Lawler, Jimmy Ba, Rahul G\nKrishnan, Barry B Rubin, and Bo Wang. 2023. Clini-\ncal camel: An open-source expert-level medical lan-\nguage model with dialogue-based knowledge encod-\ning. arXiv preprint arXiv:2305.12031.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023b. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023c. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nTsinghua KEG. 2023. Chatglm-6b: A large-scale\nlanguage model. https://github.com/THUDM/\nChatGLM-6B/blob/main/README_en.md. Ac-\ncessed: 2023-11-05.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaek-\nermann, Mohamed Amin, Pi-Chuan Chang, Andrew\nCarroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, Basil\nMustafa, Aakanksha Chowdhery, Yun Liu, Simon\nKornblith, David Fleet, Philip Mansfield, Sushant\nPrakash, Renee Wong, Sunny Virmani, Christopher\nSemturs, S Sara Mahdavi, Bradley Green, Ewa Domi-\nnowska, Blaise Aguera y Arcas, Joelle Barral, Dale\nWebster, Greg S. Corrado, Yossi Matias, Karan Sing-\nhal, Pete Florence, Alan Karthikesalingam, and Vivek\nNatarajan. 2023. Towards generalist biomedical ai.\narXiv preprint arXiv:2307.14334.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,\nSendong Zhao, Bing Qin, and Ting Liu. 2023a. Hu-\natuo: Tuning llama model with chinese medical\nknowledge. arXiv preprint arXiv:2304.06975.\nHaochun Wang, Chi Liu, Sendong Zhao, Bing Qin, and\nTing Liu. 2023b. Chatglm-med. https://github.\ncom/SCIR-HI/Med-ChatGLM.\nChaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang,\nand Weidi Xie. 2023. Pmc-llama: Further fine-\ntuning llama on medical papers. arXiv preprint\narXiv:2304.14454.\nHonglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao,\nYuxiao Liu, Qian Wang, and Dinggang Shen. 2023.\nDoctorglm: Fine-tuning your chinese doctor is not a\nherculean task. arXiv preprint arXiv:2304.01097.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023a. Harnessing the power of llms in\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint \npractice: A survey on chatgpt and beyond. arXiv\npreprint arXiv:2304.13712.\nSonghua Yang, Hanjia Zhao, Senbin Zhu, Guangyu\nZhou, Hongfei Xu, Yuxiang Jia, and Hongying Zan.\n2023b. Zhongjing: Enhancing the chinese medical\ncapabilities of large language model through expert\nfeedback and real-world multi-turn dialogue. arXiv\npreprint arXiv:2308.03549.\nQichen Ye, Junling Liu, Dading Chong, Peilin Zhou,\nYining Hua, and Andrew Liu. 2023. Qilin-\nmed: Multi-stage knowledge injection advanced\nmedical large language model. arXiv preprint\narXiv:2310.09089.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An\nopen bilingual pre-trained model. In The Eleventh In-\nternational Conference on Learning Representations.\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhi-\nhong Chen, Jianquan Li, Guiming Chen, Xiangbo\nWu, Zhiyi Zhang, Qingying Xiao, et al. 2023a. Hu-\natuogpt, towards taming language model to be a doc-\ntor. arXiv preprint arXiv:2305.15075.\nXinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang\nChen, Zekun Li, and Linda Ruth Petzold. 2023b.\nAlpacare: Instruction-tuned large language mod-\nels for medical application. arXiv preprint\narXiv:2310.14558.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nHongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li,\nSam S Chen, Peilin Zhou, Junling Liu, Yining\nHua, Chengfeng Mao, Xian Wu, et al. 2023a.\nA survey of large language models in medicine:\nProgress, application, and challenge. arXiv preprint\narXiv:2311.05112.\nHongjian Zhou, Fenglin Liu, Wenjun Zhang, Guowei\nHuang, Lei Clifton, David Eyre, Haochen Luo,\nFengyuan Liu, Kim Branson, Patrick Schwab, et al.\n2023b. Druggpt: A knowledge-grounded collabora-\ntive large language model for evidence-based drug\nanalysis. Preprint.\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 25, 2024. ; https://doi.org/10.1101/2024.04.24.24306315doi: medRxiv preprint ",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.705028772354126
    },
    {
      "name": "Computer science",
      "score": 0.4426061511039734
    },
    {
      "name": "Geography",
      "score": 0.1622776985168457
    },
    {
      "name": "Cartography",
      "score": 0.08674311637878418
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}