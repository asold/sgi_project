{
  "title": "UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation.",
  "url": "https://openalex.org/W3006320872",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2783658490",
      "name": "Huaishao Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070400137",
      "name": "Lei Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952117771",
      "name": "Botian Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2235093108",
      "name": "Haoyang Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966337374",
      "name": "Nan Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169559621",
      "name": "Tianrui Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099541584",
      "name": "Xilin Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096867225",
      "name": "Ming Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2565656701",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2951383777",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2885775891",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2738581557",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2526286384",
    "https://openalex.org/W2116435618",
    "https://openalex.org/W2985185987",
    "https://openalex.org/W2530794863",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2963125871",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2972780057",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2799247651",
    "https://openalex.org/W2519656895",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2922303317",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2794481829",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2962795934",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W1957706851",
    "https://openalex.org/W2952453038",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2401640538",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2923016229",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W3023633125",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W2990629080",
    "https://openalex.org/W219040644",
    "https://openalex.org/W3035119608",
    "https://openalex.org/W2951098185",
    "https://openalex.org/W2964216663"
  ],
  "abstract": "With the recent success of the pre-training technique for NLP and image-linguistic tasks, some video-linguistic pre-training works are gradually developed to improve video-text related downstream tasks. However, most of the existing multimodal models are pre-trained for understanding tasks, leading to a pretrain-finetune discrepancy for generation tasks. This paper proposes UniVL: a Unified Video and Language pre-training model for both multimodal understanding and generation. It comprises four components, including two single-modal encoders, a cross encoder, and a decoder with the Transformer backbone. Five objectives, including video-text joint, conditioned masked language model (CMLM), conditioned masked frame model (CMFM), video-text alignment, and language reconstruction, are designed to train each of the components. We further develop two pre-training strategies, stage by stage pre-training (StagedP) and enhanced video representation (EnhancedV), to make the training process of the UniVL more effective. The pre-train is carried out on a sizeable instructional video dataset HowTo100M. Experimental results demonstrate that the UniVL can learn strong video-text representation and achieves state-of-the-art results on five downstream tasks.",
  "full_text": "UniVL: A Uniﬁed Video and Language Pre-Training Model for\nMultimodal Understanding and Generation\nHuaishao Luo1∗, Lei Ji2,3,4, Botian Shi5, Haoyang Huang2,\nNan Duan2, Tianrui Li1, Jason Li6, Taroon Bharti6, Ming Zhou2\n1Southwest Jiaotong University, Chengdu, China\n2Microsoft Research Asia, Beijing, China\n3Institute of Computing Technology, Chinese Academy of Science, Beijing, China\n4University of Chinese Academy of Sciences, Beijing, China\n5Beijing Institute of Technology, Beijing, China\n6Microsoft STCA, Beijing, China\nhuaishaoluo@gmail.com, leiji@microsoft.com\nAbstract\nWith the recent success of the pre-training\ntechnique for NLP and image-linguistic tasks,\nsome video-linguistic pre-training works are\ngradually developed to improve video-text re-\nlated downstream tasks. However, most of\nthe existing multimodal models are pre-trained\nfor understanding tasks, leading to a pretrain-\nﬁnetune discrepancy for generation tasks. This\npaper proposes UniVL: a Uniﬁed Video and\nLanguage pre-training model for both multi-\nmodal understanding and generation. It com-\nprises four components, including two single-\nmodal encoders, a cross encoder, and a de-\ncoder with the Transformer backbone. Five\nobjectives, including video-text joint, condi-\ntioned masked language model (CMLM), con-\nditioned masked frame model (CMFM), video-\ntext alignment, and language reconstruction,\nare designed to train each of the components.\nWe further develop two pre-training strategies,\nstage by stage pre-training (StagedP) and en-\nhanced video representation (EnhancedV), to\nmake the training process of the UniVL more\neffective. The pre-train is carried out on a size-\nable instructional video dataset HowTo100M.\nExperimental results demonstrate that the\nUniVL can learn strong video-text represen-\ntation and achieves state-of-the-art results on\nﬁve downstream tasks.\n1 Introduction\nWith the recent advances of self-supervised learn-\ning, pre-training techniques play a vital role in\nlearning visual and language representation. The\nparadigm is to pre-train the model on a large scale\nunlabeled data and ﬁne-tune the downstream tasks\n∗This work was done during the ﬁrst author’s internship\nin MSR Asia\n…\nstart\nwith\nsome\nregular\nflour\nand\ncornstarch\nso\nit's\nkind\nof\nsimilar\nto\na…\nmedium\nyellow\ndo\nnot\nonions\ncut\nhim\nany\nthicker…Transcript\nwe're\ngoing\nto\ndump\nin\na\nfew\nrings\n…\nVideo Clip\nUniVL\nPre-Trained \nModel\n… m ake your specially cut tomatoes …\nplace the bacon slices \non a baking pan and \ncook them in an oven\ntoast the bread slices in the toaster\n……\nRetrieval\nCaption\n… …\nFigure 1: A showcase of video and language pre-train\nbased model for multimodal understanding (e.g., re-\ntrieval) and generation (e.g., captioning).\nusing task-speciﬁc labeled data. Inspired by the\nBERT (Devlin et al., 2019) model’s success for\nNLP tasks, numerous multimodal image-language\npre-training models (Lu et al., 2019; Li et al.,\n2019a,b) have been proposed. Their results have\ndemonstrated the effectiveness of pre-training on\nvarious visual and language tasks such as visual\nquestion answering. Different from previous text\npre-training or image-language pre-training, we fo-\ncus on video-linguistic pre-training in this paper.\nVideos contain rich visual, acoustic, and lan-\nguage information for people to acquire knowledge\nor learn how to perform a task. This motivates\nresearchers to investigate whether AI agents can\nlearn task completion from videos like humans\nwith both low-level visual and high-level semantic\nlanguage signals. Therefore, multimodal video-\nlanguage tasks are of great importance to inves-\ntigate for both research and applications. In this\nwork, we ﬁrst propose to pre-train a uniﬁed video-\nlanguage model using video and acoustic speech\narXiv:2002.06353v3  [cs.CV]  15 Sep 2020\nrecognition (ASR) transcript in instructional videos\nto learn a joint representation of both video and\nlanguage. Then, we ﬁne-tune this model on ﬁve\ntypical multimodal tasks, including understanding\nand generation targets. Figure 1 presents a show-\ncase of our pre-training and ﬁne-tuning ﬂow. Take\nmultimodal video captioning as an example. The\nmodel inputs video and ASR transcript and predicts\na captioning sentence.\nVideoBERT (Sun et al., 2019b) and CBT (Sun\net al., 2019a) are the ﬁrst pioneers to investigate\nvideo-language pre-training with regard to video\nrepresentation on instructional videos. They have\ndemonstrated the effectiveness of the BERT based\nmodel for capturing video temporal and language\nsequential features. Besides the above two works,\nthere is some concurrent progress to our model.\nActBERT (Zhu and Yang, 2020) leverages global\naction information to catalyze mutual interactions\nbetween linguistic texts and local regional objects.\nMoreover, a transformer block is introduced to en-\ncode global actions, local regional objects, and lin-\nguistic descriptions. HERO (Li et al., 2020) hierar-\nchically encodes multimodal inputs. Furthermore,\ntwo new pre-training tasks, video-subtitle matching\nand frame order modeling, are designed to improve\nthe representation learning. VideoAsMT (Korbar\net al., 2020) takes a generative modeling approach\nthat poses the objective as a translation problem\nbetween modalities.\nHowever, most of previous models only pre-train\nthe model on understanding tasks. In this paper,\nwe pre-train on both understanding and genera-\ntion tasks through an encoder-decoder paradigm.\nAlthough the concurrent work VideoAsMT has a\nsimilar encoder-decoder as ours, it is not ﬂexible\nfor downstream tasks with only one single uniﬁed\nframework. In this paper, we develop a ﬂexible\napproach to learn video and language joint repre-\nsentation and adapt downstream multimodal tasks.\nWe propose UniVL: a Uniﬁed Video and\nLanguage pre-training model for multimodal un-\nderstanding and generation. Our UniVL model\nadopts Transformer (Vaswani et al., 2017) as the\nbackbone and has four components, including two\nsingle-modal encoders, a cross encoder, and a de-\ncoder. In detail, we ﬁrst encode the text and visual\nseparately by two single-modal encoders. A video-\ntext joint objective performs on these two encoders,\nwhich aims to learn better representation for each\nmodality before fusing them. Such a two-stream de-\nsign is natural to retrieval tasks due to its scalability\nto very large datasets. The proposed representation\ncan be indexed and has linear complexity in the\nnumber of videos. Then we adopt the Transformer\nbased encoder-decoder model to perform the under-\nstanding and generation pre-training by four tasks:\nconditioned masked language model (CMLM for\nlanguage corruption), conditioned masked frame\nmodel (CMFM for video corruption), video-text\nalignment, and language reconstruction.\nFurthermore, we design two pre-training strate-\ngies, including stage by stage pre-training strategy\n(StagedP) and Enhanced video representation (En-\nhancedV), to promote the UniVL pre-training. The\nStagedP has two parts in our setting. We only pre-\ntrain the text encoder and video encoder by the\nvideo-text joint objective for the ﬁrst stage. Then\nall modules will be pre-trained under the whole\nobjectives in the second stage. Besides, we adopt\nan entire masking strategy EnhancedV on text to\nenhance video representation.\nOur contributions are summarized as follows:\n1) We propose a multimodal video-language pre-\ntraining model trained on a large-scale instructional\nvideo dataset. It is a ﬂexible model for both video-\nlanguage understanding and generation tasks.\n2) The pre-training consists of ﬁve objectives,\nincluding video-text joint, conditioned masked lan-\nguage model, conditioned masked frame model,\nvideo-text alignment, and language reconstruction.\nTwo pre-training strategies are proposed to make\nthese objectives work harmoniously.\n3) We ﬁne-tune our pre-trained model on ﬁve typ-\nical multimodal video-language tasks: text-based\nvideo retrieval, multimodal video captioning, ac-\ntion segmentation, action step localization, and\nmultimodal sentiment analysis. Extensive exper-\niments demonstrate our model’s effectiveness on\ndownstream tasks and achieve state-of-the-art re-\nsults.\n2 Related Works\n2.1 Single Modal Pre-Training\nSelf-supervised representation learning has been\nshown to be effective for sequential data, including\nlanguage and video. Language pre-training mod-\nels, including BERT (Devlin et al., 2019), GPT\n(Radford et al., 2018), RoBERTa (Liu et al., 2019),\nXLNet (Yang et al., 2019), MASS (Song et al.,\n2019), UniLM (Dong et al., 2019), and BART\n(Lewis et al., 2019), have achieved great success\nEncoder (Text & Vision)\nText & Vision Text Vision\nEncoder (Text) Encoder \n(Vision)\nCross \nEncoder\nCross \nEncoder\nText Vision\nCross-modal\nEncoder\nEncoder (Text) Encoder \n(Vision)\n(a) Share Type (b) Cross Type (c) Joint Type\nFigure 2: Various paradigms for multimodal pre-training.\non NLP tasks. BERT (Devlin et al., 2019) is a de-\nnoising auto-encoder network using Transformer\nwith MLM (masked language model) and NSP\n(next sentence prediction) as pre-training tasks. It\nhas a strong performance for understanding tasks.\nMASS (Song et al., 2019) focuses on pre-training\nfor generation tasks. UniLM (Dong et al., 2019)\nand BART (Lewis et al., 2019) continuously study\na uniﬁed pre-training model for both understanding\nand generation tasks.\nVideo representation learning mostly focuses\non the video sequence reconstruction or future\nframes prediction as pre-training (pretext) tasks.\nEarly works like (Mathieu et al., 2015; Srivastava\net al., 2015; Han et al., 2019) aim to synthetic\nvideo frames through the image patches. Similarly,\nWang and Gupta (2015) adopt a siamese-triplet net-\nwork to rank continuous patches more similar than\npatches of different videos. Other works predict the\nfeature vectors in latent space using auto-regressive\nmodels with the noise-contrastive estimation (NCE)\n(Lotter et al., 2016; Oord et al., 2018). Sun et al.\n(2019a) adopt NCE to predict corrupted (masked)\nlatent space using the auto-encoder model.\n2.2 Multimodal Pre-Training\nRecently, numerous visual-linguistic pre-training\nmodels are proposed for multimodal tasks. For\nimage and text pre-training, ViLBERT (Lu et al.,\n2019), LXMERT (Tan and Bansal, 2019) adopt two\nseparate Transformers for image and text encod-\ning independently. Other models like Visualbert\n(Li et al., 2019b), Unicoder-VL (Li et al., 2019a),\nVL-BERT (Su et al., 2020), UNITER (Zhou et al.,\n2019) use one shared BERT model. These mod-\nels employ MLM and image-text matching as pre-\ntraining tasks which are effective for downstream\nmultimodal tasks. VLP (Zhou et al., 2019) pro-\nposes a uniﬁed image-language model for under-\nstanding and generation tasks. Different from these\nworks, we focus on video and text pre-training for\nuniversal representation.\nFor video and text pre-training, VideoBERT\n(Sun et al., 2019b) and CBT (Sun et al., 2019a)\nare the ﬁrst works to explore the capability of pre-\ntraining models. Although VideoBERT and CBT\npre-train the model on multimodal data, the down-\nstream tasks mainly take video representation for\nfurther prediction. ActBERT (Zhu and Yang, 2020)\nleverages global action information to catalyze mu-\ntual interactions between linguistic texts and lo-\ncal regional objects, and introduces a transformer\nblock to encode global actions, local regional ob-\njects, and linguistic descriptions. HERO (Li et al.,\n2020) encodes multimodal inputs in a hierarchi-\ncal fashion. Besides, two new pre-training tasks,\nvideo-subtitle matching and frame order modeling,\nare designed to improve the representation learning.\nHowever, ActBERT and HERO are only pre-train\nthe models on understanding tasks. VideoAsMT\n(Korbar et al., 2020) takes a generative modeling\napproach that poses the objective as a translation\nproblem between modalities. The difference be-\ntween our work with VideoAsMT is that our model\ncontains two more separate encoders instead of\none uniﬁed encoder-decoder, while VideoAsMT is\ninﬂexible for downstream tasks due to one single\nuniﬁed framework.\nWe summarize three pre-training paradigms to\ncover the previous vision-text pre-training model\nconsidering different encoder architecture in lit-\nerature, as presented in Figure 2. Unicoder-VL\n(Li et al., 2019a), VL-BERT (Su et al., 2020),\nUNITER (Zhou et al., 2019), VLP (Zhou et al.,\n2019), VideoBERT (Sun et al., 2019b), ActBERT\n(Zhu and Yang, 2020), and VideoAsMT (Korbar\net al., 2020) belong to share-type in Figure 2(a),\nwhere the text and vision sequences are combined\nas the input of one shared Transformer encoder.\nViLBERT (Lu et al., 2019) and LXMERT (Tan and\nBansal, 2019) are cross-type shown in Figure 2(b).\nCBT (Sun et al., 2019a) and HERO (Li et al., 2020)\nare joint-type shown in Figure 2(c). The cross-\ntype and joint-type architectures have two-stream\ninput, and the difference is the interaction across\nboth modalities. Compared with the single-stream\ninput in the share-type, the two-stream input can\naccommodate each modality’s different processing\nneeds and interact at varying representation depths\n(Lu et al., 2019). Besides, the joint-type structure\nhas one cross-modal encoder for full interaction\nbetween the two streams comparing with the cross-\ntype. We adopt the joint-type as our encoder in this\npaper.\n3 Method\nThe problem is deﬁned as: given the input video\nand the corresponding ASR transcript pairs, pre-\ntrain a model to learn joint video and text repre-\nsentation with the self-supervision approach, and\nﬁne-tune downstream tasks. In this section, we\ndescribe the architecture and pre-training tasks in\ndetail.\n3.1 Model Architecture\nFigure 3 presents the UniVL as an encoder-decoder\narchitecture. First, the model extracts representa-\ntions of the input text tokens and the video frame\nsequences using various feature extractors. A text\nencoder then adopts the BERT model to embed\nthe text, and a video encoder utilizes the Trans-\nformer encoder to embed the video frames. Next,\nwe employ a Transformer based cross encoder for\ninteracting between the text and the video. Finally,\na Transformer decoder is used to reconstruct the\ninput text.\n3.1.1 Pre-processing.\nWe ﬁrst pre-process video and language before\nfeeding to the UniVL. For the input text, we to-\nkenize all words by WordPieces (Wu et al., 2016)\nfollowing the pre-processing method in BERT to\nobtain the token sequence t =\n{\nti|i ∈ [1,n]\n}\n,\nwhere ti is the i-th token and nis the length of the\ntoken sequence. For each video clip, we sample\na frame sequence v =\n{\nvj|j ∈[1,m]\n}\nand adopt\nthem to extract features, where vj is the j-th group\nof video frames and mis the group length of the\nframe sequence.\n3.1.2 Single Modal Encoders.\nWe encode the text and video separately. Such\na two-stream design has two advantages: module\nreusing and retrieval orienting. The module reusing\nmeans the text module can beneﬁt from the exist-\ning text-based pretrained model, e.g., BERT. The\nretrieval orienting means the two-stream design is\nnatural to retrieval tasks due to its scalability to ex-\ntensive datasets. The extracted representation can\nbe indexed, and the calculation of similarity has\nlinear complexity in the number of videos. In this\npaper, we adopt the BERT-Base uncased model to\ngenerate the text representation T ∈Rn×d after\nfeeding the token sequence t,\nT = BERT(t), (1)\nwhere dis the hidden size of text representation.\nFor the video frame sequence v, we adopt the\noff-the-shelf image feature extractors, e.g., S3D\n(Xie et al., 2018), to generate video feature Fv ∈\nRm×df\nv , where df\nv is the hidden size. A Trans-\nformer encoder is utilized to embed the contextual\ninformation of video as follows,\nV = Transformer(Fv). (2)\nThe dimension of V is Rm×d.\n3.1.3 Cross Encoder.\nThe text encoder and video encoder mainly focus\non individual modality. To make the text and video\nfully interact, we design across encoder, which\ntakes both the text and video modality features\nas input. Speciﬁcally, we ﬁrst combine the text\nencoding T and the video encoding V to get the\nencoding M ∈R(n+m)×d. Then, a Transformer\nencoder takes the encoding M as input to generate\nthe attended encoding M ∈R(n+m)×d,\nM = Transformer([T; V]), (3)\nwhere [; ]denotes the combination operation. It is\nnoted that the combination is operated along with\nthe dimension of sequence, not the dimension of\nhidden size. One reason is that the text lengthnand\nvideo clip length mare always different. Another\nreason is that the semantic between text and video\nare not absolutely aligned. People are likely to\ndescribe an event after or before performing it in\nthe video (Miech et al., 2020).\nVideo Clip\n[CLS] toast the [MASK] \n[MASK] in the toaster [SEP]\nTranscript Cross Encoder\n(Transformer Encoder)\nCMFM\nCMLM\nbread slices\nDecoder \n(Transformer Decoder)Joint\n0/1\nRetrieval\nText\n[CLS] peanuts \nand …\nVideo Clip\nText Encoder\nVideo Encoder\nCaption\nCross \nEncoder\nDecoder\nplace the \nbacon \nslices …\nText Encoder\nVideo Encoder\n[CLS] you \nspecially …\nTranscript\nVideo Clip\nAction  Tasks\nsegmentation &  step localization\n...\nVideo Clip\nMultimodal Classification\nCross \nEncoder\nText Encoder\nVideo Encoder\n[CLS] you \nspecially …\nTranscript\nVideo Clip\nText Encoder\n(Transformer Encoder)\nVideo Encoder\n(Transformer Encoder)\nAlignment\n0/1\nGeneration\ntoast the bread slices \nin the toaster [SEP]\nVideo Encoder\nfeatur e\nmasked\nStep Description\n[CLS] pour juice\nText Encoder\nJoint\nCross \nEncoder\nAligment\n0.12 0.12\nor\nFigure 3: The main structure of our UniVL, which comprises four components, including two single-modal en-\ncoders, a cross encoder, and a decoder. The model is ﬂexible for many text and video downstream tasks. Four\npossible tasks are listed.\n3.1.4 Decoder.\nWe empower our pre-trained model to have the ca-\npability of learning from and then beneﬁting for\ngeneration tasks by attaching a decoder, which is\nusually a unidirectional recurrent/attention model\nto generate tokens one by one. Such a decoder mod-\nule is proved useful in text-based pre-training tasks,\ne.g., T5 (Raffel et al., 2019) and BART (Lewis\net al., 2020). It is noted that the decoder has a dif-\nferent target at different phases. The decoder learns\nto reconstruct the input text (e.g., transcripts) dur-\ning pre-training because of no available text label.\nWhen ﬁne-tuning, the decoder is used to generate\nresults, e.g., video caption, where inputs transcripts\nand video and outputs caption. The input is the\nattended encoding M of text and video. We unex-\nceptionally exploit Transformer to get the decoded\nfeature D ∈Rl×d from M,\nD = Transformer(M), (4)\nwhere lis the decoder length.\n3.2 Pre-training Objectives\nWe have ﬁve pre-training objectives: 1) video-\ntext joint, 2) conditioned masked language model\n(for text corruption), 3) conditioned masked frame\nmodel (for video corruption), 4) video-text align-\nment, and 5) language reconstruction.\n3.2.1 Video-Text Joint.\nAs our text encoder, the BERT-Base uncased model\nis a robust extractor of text representation. So, we\nutilize a video-text joint objective to enhance the\ncapability of the video encoder. It seems a retrieval\norienting operation, which is to align the space of\nrepresentation between text and video. Considering\nthe misalignment between the text and video clip in\nnarrated videos, we adopt MIL-NCE (Miech et al.,\n2020) on T and V as our joint objective,\nLJoint(θ) =−E(t,v)∼B log MIL-NCE (t,v) ,\n(5)\nMIL-NCE (t,v) =\n∑\n(ˆ v,ˆt)∈Pv,t exp(ˆ vˆt⊤)\nZ , (6)\nZ=\n∑\n(ˆ v,ˆt)∈Pv,t\nexp(ˆ vˆt⊤) +\n∑\n(˜ v,˜t)∈Nv,t\nexp(˜ v˜t⊤),\n(7)\nwhere Pv,t is a set of positive video-transcript pairs.\nE.g., {(v,t),(v,t−1),(v,t+1)}, where t−1 and\nt+1 are two closest transcripts in time to t. The\nnegative pairs Nv,t take negative transcripts (or\nvideo clips) from other instances within the batch\nB after ﬁxing v (or t). ˆ v,˜ vand ˆt,˜t are generated\nthrough mean-pooling on V and T, respectively. θ\nis the trainable parameters.\n3.2.2 CMLM: Conditioned Masked\nLanguage Model.\nFollowing BERT, we also randomly mask 15% to-\nkens with the special token [MASK] in the sen-\ntence and re-produce the masked tokens under the\ncondition of video input and known tokens. This\nloss function is deﬁned on the feature matrix of the\ntext part in M as:\nLCMLM(θ) =−Etm∼t log Pθ(tm |t¬m,v) ,\n(8)\nwhere t¬m means the contextual tokens surround-\ning the masked token tm, θis the trainable parame-\nters.\n3.2.3 CMFM: Conditioned Masked Frame\nModel.\nSimilarly, we also propose a masked frame model\nto predict the correct frames given contextual\nframes and the input text for semantic constraints.\nHowever, it is hard to reconstruct the original RGB\nframe. We adopt the contrastive learning method\nto maximize the MI (Mutual information) between\nthe masked output features and the original fea-\ntures. This loss function is NCE (Sun et al., 2019a).\nWe randomly mask 15% vectors (also 15% frames)\nwith zeros. The objective is to identify the correct\nframe compared to negative distractors. The loss is\ndeﬁned as:\nLCMFM (θ) =−Evm∼v log NCE (vm |v¬m,t) ,\n(9)\nNCE (vm |v¬m,t) =exp(fvmm⊤\nvm)\nZ , (10)\nZ= exp(fvmm⊤\nvm) +\n∑\nvj∈N(vm)\nexp(fvmm⊤\nvj ),\n(11)\nwhere v¬m means the surrounding frames except\nvm, fvm ∈R1×d is a linear output of fv\nvm ∈Fv, Fv\nis the real-valued vectors of video features, mvm ∈\nM(v), and M(v) is the feature matrix of the video\npart in M. We take other frames in the same batch\nas negative cases deﬁned as N(vm).\n3.2.4 Video-Text Alignment.\nWe use the fused representation that corresponds\nto the special token [CLS] to predict scores for the\nvideo-text alignment, which is similar to the BERT\nsentence pair classiﬁcation task. We adopt the NCE\nloss to learn to discriminate against the positive\nfrom negative video-text pairs. To enhance this\ncapability, we not only randomly sample negative\ncases but also re-sample video clips from the same\nvideo (Han et al., 2019). The reason is that the\nframes inside the same video are more similar than\nframes of different videos. This loss function is\ndeﬁned as follows,\nLAlign(θ) =−E(t,v)∼B log exp\n(\ns(t,v)\n)\nZ , (12)\nZ= exp\n(\ns(t,v)\n)\n+\n∑\nu∈N(v)\nexp\n(\ns(t,u)\n)\n,\n(13)\nwhere s(·) means two linear layers with aTanh ac-\ntivation function between them, which is performed\non the ﬁrst hidden state of M. We take other video\nclips in the same batch B as negative cases N(v).\n3.2.5 Language Reconstruction.\nTo reconstruct the input sentence to endow the pre-\ntrained model with the generation capability, we\nemployed an auto-regressive decoder with recon-\nstruction objective, and the loss function is,\nLDecoder(θ) =−Eˆti∼ˆt log Pθ\n(ˆti |ˆt<i,t,v\n)\n.\n(14)\nIt is noted that t is the masked version of ground-\ntruth text ˆt when pre-training. As shown in BART\n(Lewis et al., 2019), pre-training decoder beneﬁts\ngeneration tasks.\nWe jointly optimize our model by a weighted\nloss:\nLUniVL =LJoint + LCMLM + LCMFM\n+ LAlign + LDecoder. (15)\n3.3 Pre-training Strategies\nWe develop two pre-training strategies to train the\nUniVL model effectively.\n3.3.1 StagedP: Stage by Stage Pre-training.\nThe UniVL can beneﬁt from the pre-trained BERT-\nBase uncased model in the text encoder module.\nThe natural idea is to train a peer to peer video\nencoder as the BERT-Base. We adopt a two-stage\ntraining fashion. For the ﬁrst stage, we only pre-\nserve the text BERT and video Transformer to learn\nthe weights using the Video-Text Joint loss Eq. (5).\nNext, we decrease the learning rate and continue to\nfurther pre-train the UniVL by all ﬁve objectives.\nOne advantage is to fasten the pre-training speed,\nand the other advantage is to make the pre-training\nprogress more smoothing on weights.\n3.3.2 EnhancedV: Enhanced Video\nRepresentation.\nTo further enhance the video representation, we\nadopt a masked modality strategy to make the video\nto generate transcripts without text input. Specif-\nically, we mask the whole text tokens with a 15%\npossibility. In other words, there are 15% text-\nvideo pairs with entire text tokens masked in each\nmini-batch, and the model utilizes the video infor-\nmation to complete generation. Such a strategy is\na more challenging task for the model to learn a\nbetter video representation.\n4 Experiments\nWe ﬁrst pre-train our model on the large scale\ndataset. We download videos with ASR transcripts\nfrom Howto100M dataset (Miech et al., 2019) 1.\nAfter ﬁltering the unavailable ones, we get 1.2M\nvideos for pre-training our model. On average, the\nduration of each video is 6.5 minutes with 110\nclip-text pairs.\nThen, we ﬁne-tune our pre-trained model on\nﬁve diverse downstream tasks using ﬁve datasets,\nincluding text-based video retrieval, multimodal\nvideo captioning, action segmentation, action step\nlocalization, and multimodal sentiment analysis.\n4.1 Datasets\n4.1.1 Youcook2\nYoucook2 (Zhou et al., 2018a) contains 2,000 cook-\ning videos on 89 recipes with 14K video clips. The\noverall duration is 176 hours (5.26 minutes on aver-\nage). Each video clip is annotated with one caption-\ning sentence. We evaluate both text-based video\nretrieval and multimodal video captioning task on\nthis dataset.\nFor the text-based video retrieval task, we fol-\nlow the same experimental setting in (Miech et al.,\n2019), and use the captions as the input text queries\nto ﬁnd the corresponding video clips. For the video\ncaptioning task, we use the same setting as in (Shi\net al., 2019). We ﬁlter the data and make sure there\nis no overlap between pre-training and evaluation\ndata. In all, we have 1,261 training videos and 439\ntest videos, that is, 9,776 training clip-text pairs\nand 3,369 test clip-text pairs.\n4.1.2 MSR-VTT\nMSR-VTT (Xu et al., 2016) is the open-domain\ndataset for video retrieval tasks. It has open do-\nmain video clips, and each clip has 20 captioning\nsentences labeled by human. In all, there are 200K\nclip-text pairs from 10K videos in 20 categories in-\ncluding sports, music, etc. Following JSFusion (Yu\net al., 2018), we randomly sampled 1,000 clip-text\npairs as test data to evaluate the performance of our\nmodel on text-based video retrieval task.\n4.1.3 COIN\nCOIN (Tang et al., 2019) is to evaluate action seg-\nmentation task, which contains 180 different tasks\nand 11,827 videos. Each video is labeled with 3.91\n1https://www.di.ens.fr/willow/research/howto100m/\nstep segments. In total, the dataset contains videos\nof 476 hours, with 46,354 annotated segments.\n4.1.4 CrossTask\nCrossTask (Zhukov et al., 2019) is to evaluate the\naction step localization task. It contains 83 different\ntasks and 4.7k videos. For each task, an ordered\nlist of steps with manual descriptions are provided.\n4.1.5 CMU-MOSI\nMultimodal Opinion Sentiment and Emotion Inten-\nsity (Zadeh et al., 2016) is sentence-level sentiment\nanalysis and emotion recognition in online videos.\nCMU-MOSI contains 2,199 opinion video clips,\neach annotated with real-valued sentiment intensity\nannotations in the range [-3, +3]. We evaluate the\nperformance of our model on multimodal sentiment\nanalysis.\n4.2 Experimental Details\nFor text encoding, we apply WordPiece embed-\ndings (Wu et al., 2016) with a 30,000 token vo-\ncabulary to input to BERT model. We exploit the\nBERT-base model (Devlin et al., 2019) with 12\nlayers of Transformer blocks. Each block has 12\nattention heads and the hidden size is 768.\nFor video encoding, we ﬁrst extract the 3D fea-\nture from video clips using the S3D model pre-\ntrained by Miech et al. (2020). The basic visual\nfeature can signiﬁcantly affect the results from our\npreliminary experiments. The fps of the 3D feature\nextractor is 16 and the dimension is 1,024. We then\nemploy Transformer Encoder with 6 layers to cap-\nture the sequential information on the 3D feature.\nEach block has 12 attention heads and the hidden\nsize is 768.\nThe model consumes the clip-text pairs. The\nmaximal input tokens of text is 32 and the max-\nimal number of video features is 48. For short\nsentence and clip, we concatenate contextual to-\nkens and frames. For cross encoder and decoder,\nwe use a 2 layers Transformer Encoder as the en-\ncoder and a 3 layer Transformer Decoder as the\ndecoder with 12 heads and 768 hidden size. For\ngeneration task during the inference stage, we use\nthe beam search with the size of 5. As previously\nmentioned, the generated sequence is the ground-\ntruth input transcripts in the pre-training phase. Its\ntarget is to sequentially learn full information from\nthe masked transcripts and video features.\nWe pre-train our model on 8 NVIDIA Tesla\nV100 GPUs. There are two sets of hyper-\nMethods R@1 R@5 R@10 Median R\nRandom 0.03 0.15 0.3 1675\nHGLMM (Klein et al., 2015) 4.6 14.3 21.6 75\nHowTo100M (Miech et al., 2019) 8.2 24.5 35.3 24\nMIL-NCE (Miech et al., 2020) 15.1 38.0 51.2 10\nActBERT (Zhu and Yang, 2020) 9.6 26.7 38.0 19\nVideoAsMT (Korbar et al., 2020) 11.6 - 43.9 -\nUniVL (FT-Joint) 22.2 52.2 66.2 5\nUniVL (FT-Align) 28.9 57.6 70.0 4\nTable 1: Results of text-based video retrieval on Youcook2 dataset.\nMethods R@1 R@5 R@10 Median R\nRandom 0.1 0.5 1.0 500\nC+LSTM+SA (Torabi et al., 2016) 4.2 12.9 19.9 55\nVSE (Kiros et al., 2014) 3.8 12.7 17.1 66\nSNUVL (Yu et al., 2016) 3.5 15.9 23.8 44\nKaufman et al. (2017) 4.7 16.6 24.1 41\nCT-SAN (Yu et al., 2017) 4.4 16.6 22.3 35\nJSFusion (Yu et al., 2018) 10.2 31.2 43.2 13\nHowTo100M (Miech et al., 2019) 14.9 40.2 52.8 9\nMIL-NCE (Miech et al., 2020) 9.9 24.0 32.4 29.5\nActBERT (Zhu and Yang, 2020) 8.6 23.4 33.1 36\nVideoAsMT (Korbar et al., 2020) 14.7 - 52.8 -\nUniVL (FT-Joint) 20.6 49.1 62.9 6\nUniVL (FT-Align) 21.2 49.6 63.1 6\nTable 2: Results of text-based video retrieval on MSR-VTT dataset.\nparameters considering the stage by stage pre-\ntraining strategy. In the ﬁrst stage, the batch size\nis set to 600 and the model is trained 50 epochs\nfor 1.5 days. In the second stage, the batch size is\nset to 48 and the model is trained 50 epochs for 12\ndays. We use the Adam optimizer (Kingma and\nBa, 2015) with an initial learning rate of 1e-3 in\nthe ﬁrst stage and 1e-4 in the second stage, and\nemploy a linear decay learning rate schedule with\na warm-up strategy.\n4.3 Main Results\n4.3.1 Text-based Video Retrieval.\nText-based video retrieval is deﬁned to retrieve a\nrelevant video/clip given an input text query. As\nshown in Figure 3 (retrieval block), the model en-\ncodes the input text query and candidate video clips\nthrough the text encoder and video encoder respec-\ntively. Then calculate the matching scores using\ntwo different approaches: one is UniVL (FT-Joint),\nwhich calculates the score through dot product as in\nEq. (6), and use LJoint as the loss during the ﬁne-\ntuning stage; the other is UniVL (FT-Align), which\nfeeds the encodings to both single encoders and the\ncross encoder to get uniﬁed representation and pre-\ndict the match score through s(·) in Eq. (12) on the\nﬁrst token ‘[CLS]’. During the ﬁne-tuning stage,\nthe loss is LAlign. We use the Adam optimizer\nwith an initial learning rate of 3e-5 and a batch size\nof 32 video-caption pairs for Youcook2, an initial\nlearning rate of 5e-5 and a batch size of 128 video-\ncaption pairs for MSR-VTT as hyper-parameters\nto ﬁne-tune for 5 epochs.\nWe ﬁne-tune our pre-trained model for text-\nbased video retrieval task on both Youcook2 and\nMSR-VTT datasets. The evaluation metrics are Re-\ncall@n (R@n) and Median R. Tables 1 and 2 list\nthe retrieval results of all baselines and our model\non Youcook2 and MSR-VTT separately. We can\nsee that our model achieves the best performance\nMethods Input B-3 B-4 M R-L CIDEr\nBi-LSTM (Zhou et al., 2018a) V - 0.87 8.15 - -\nEMT (Zhou et al., 2018b) V - 4.38 11.55 27.44 0.38\nVideoBERT (Sun et al., 2019b) V 6.80 4.04 11.01 27.50 0.49\nCBT (Sun et al., 2019a) V - 5.12 12.97 30.44 0.64\nActBERT (Zhu and Yang, 2020) V 8.66 5.41 13.30 30.56 0.65\nVideoAsMT (Korbar et al., 2020) V - 5.3 13.4 - -\nAT (Hessel et al., 2019) T - 8.55 16.93 35.54 1.06\nDPC (Shi et al., 2019) V + T 7.60 2.76 18.08 - -\nAT+Video (Hessel et al., 2019) V + T - 9.01 17.77 36.65 1.12\nUniVL V 16.46 11.17 17.57 40.09 1.27\nUniVL T 20.32 14.70 19.39 41.10 1.51\nUniVL V + T 23.87 17.35 22.35 46.52 1.81\nTable 3: The multimodal video captioning results on Youcook2 dataset. ‘V’ means video and ‘T’ means Transcript.\nover all baselines to a large extent. We present\nseveral baseline methods with or without pre-\ntraining. Our model outperforms the Howto100M\nand VideoAsMT models pre-trained on the same\ndataset on all metrics. Besides, the experimental\nresults present the a large performance gain with\npre-training.\nWe also notice that UniVL (FT-Align) performs\nbetter than UniVL (FT-Joint), which demonstrates\nthat fusion representation generated by the cross en-\ncoder is better. Nevertheless, the UniVL (FT-Joint)\ninference speed is 50 times for Youcook2 and 10\ntimes for MSR-VTT faster than that of the UniVL\n(FT-Align). Therefore, it is a trade-off between per-\nformance and efﬁciency in practical applications.\nIn the following ablation experiment, we exploit\nUniVL (FT-Joint) in the retrieval task.\n4.3.2 Multimodal Video Captioning.\nMultimodal video captioning aims to generate a\nsequence of descriptive sentences. As shown in\nFigure 3 (caption block), the model encodes the\ninput video frames as well as transcripts inside the\nclips through the video encoder and text encoder\nrespectively, then feeds the encodings to the cross\nencoder to get uniﬁed representation, and ﬁnally\ngenerates token sequence by the decoder. We use\nLDecoder as the loss during the ﬁne-tuning stage.\nThe hyper-parameters are an initial learning rate of\n3e-5, a batch size of 32 samples, and ﬁne-tune for\n5 epochs.\nTable 3 lists the caption results of all baselines\nand our models on Youcook2. This generation task\nadopts the corpus-level generation evaluation met-\nric using the pen-source tool 2, including BLEU\n(BLEU-3, B-3; BLEU-4, B-4) (Papineni et al.,\n2002), METEOR (M) (Banerjee and Lavie, 2005),\nROUGE-L (R-L) (Lin and Och, 2004), and CIDEr\n(Vedantam et al., 2015). We compare our pre-\ntrained model with several baseline methods. We\nclassify the methods with the setting that the in-\nput is video-only or video+transcript. Zhou et al.\n(2018a) propose an end-to-end model for both pro-\ncedural segmentation and captioning. Sun et al.\n(2019b,a); Zhu and Yang (2020); Korbar et al.\n(2020) adopt the pre-training strategy and evalu-\nate the captioning with the only video as input. Shi\net al. (2019) and Hessel et al. (2019) discuss the\nmultimodal input with both video and transcript.\nOur pre-trained model achieves state-of-the-art re-\nsults and outperforms the existing pre-trained mod-\nels, even only considering video as input.\n4.3.3 Action Segmentation.\nWe ﬁne-tune our pre-train model on action seg-\nmentation task using COIN dataset, which is to\npredict one pre-deﬁned label for each frame of the\ngiven video. As shown in Figure 3 (action tasks\nblock), the model encodes the input video frames\nthrough the video encoder, followed by a linear\nclassiﬁer upon the output encodings for frame label-\ning. We do not use the text encoder due to no text\ndescription in the dataset. The evaluation metric is\nframe-wise accuracy (FA). The hyper-parameters\nare an initial learning rate of 3e-5, a batch size of\n32 samples, and ﬁne-tune for 5 epochs. The results\nare shown in Table 4. The UniVL signiﬁcantly\noutperforms the baselines with more than 14% im-\n2https://github.com/Maluuba/nlg-eval\nMethods Frame Accuracy (%)\nNN-Viterbi (Richard et al., 2018) 21.17\nVGG (Simonyan and Zisserman, 2014) 25.79\nTCFPN-ISBA (Ding and Xu, 2018) 34.30\nCBT (Sun et al., 2019a) 53.90\nMIL-NCE (Miech et al., 2020) 61.00\nActBERT (Zhu and Yang, 2020) 56.95\nUniVL 70.02\nTable 4: Action segmentation results on COIN.\nMethods Average Recall (%)\nAlayrac et al. (2016) 13.3\nZhukov et al. (2019) 22.4\nSupervised (Zhukov et al., 2019) 31.6\nHowTo100M (Miech et al., 2019) 33.6\nMIL-NCE (Miech et al., 2020) 40.5\nActBERT (Zhu and Yang, 2020) 41.4\nUniVL 42.0\nTable 5: Action step localization results on CrossTask.\nprovements. It shows that the pre-trained UniVL\nactually learns a good visual representation, even\nabsent of linguistic descriptions.\n4.3.4 Action Step Localization.\nWe evaluate the action step localization on\nCrossTask dataset. As shown in Figure 3 (action\ntasks block), the model encodes the step description\n(action) and video clip through the text encoder and\nthe video encoder respectively. And then calculate\nthe relevance scores through dot product similar\nto the retrieval task. To fairly compare to (Miech\net al., 2019, 2020; Zhu and Yang, 2020), we do not\nﬁne-tune on the CrossTask dataset. We perform the\nevaluation protocol by reporting the average recall\n(CTR) metric for the localization task3. The results\nare shown in Table 5. Our results are even better\nthan the supervised baseline, which demonstrates\nour UniVL model can learn better joint text-video\nrepresentation.\n4.3.5 Multimodal Sentiment Analysis.\nWe evaluate the multimodal sentiment analysis on\nCMU-MOSI dataset, the goal of which is to iden-\ntify the sentiment of speaker based on the speakers\ndisplay of verbal and nonverbal behaviors. We\n3The result is generated following the evaluation process\nof ofﬁcial project: https://github.com/DmZhukov/CrossTask\nemploy video and corresponding transcripts to ac-\ncomplish this task. As shown in Figure 3 (multi-\nmodal classiﬁcation block), the model encodes the\ninput video frames as well as transcripts inside the\nclips through the video encoder and text encoder,\nrespectively. Then feeds the encodings to the cross\nencoder to get uniﬁed representation, and ﬁnally\npredicts the sentiment score by a linear on the ﬁrst\ntoken ‘[CLS]’. The hyper-parameters are an initial\nlearning rate of 1e-5, a batch size 32, and ﬁne-tune\nfor 3 epochs.\nThe results are shown in Table 6. Following\n(Zadeh et al., 2019), the evaluation metrics are\nbinary accuracy (BA), F1 score, Mean-Absolute\nError (MAE), and Pearson Correlation Coefﬁcient\n(Corr). Compared with the baseline using video,\ntranscript, and audio inputs, our model trained with\nvideo and language still achieves the best results\nwithout audio information.\n4.4 Ablation Studies\nWe analyze the effectiveness of our model design\non pre-training objectives and strategies through\nablation studies over text-based video retrieval and\nmultimodal video captioning tasks. We also discuss\nthe effectiveness of various visual features.\nMethods BA F1 MAE Corr\nMV-LSTM (Rajagopalan et al., 2016) 73.9/- 74.0/- 1.019 0.601\nTFN (Zadeh et al., 2017) 73.9/ 73.4/- 1.040 0.633\nMARN (Zadeh et al., 2018b) 77.1/ 77.0/- 0.968 0.625\nMFN (Zadeh et al., 2018a) 77.4/ 77.3/- 0.965 0.632\nRMFN (Liang et al., 2018) 78.4/ 78.0/- 0.922 0.681\nRA VEN (Wang et al., 2019) 78.0/ -/- 0.915 0.691\nMulT (Tsai et al., 2019) /83.0 -/82.8 0.870 0.698\nFMT (Zadeh et al., 2019) 81.5/83.5 81.4/83.5 0.837 0.744\nUniVL 83.2/84.6 83.3/84.6 0.781 0.767\nTable 6: Multimodal sentiment analysis results on CMU-MOSI dataset. BA means binary accuracy, MAE is Mean-\nabsolute Error, and Corr is Pearson Correlation Coefﬁcient. For BA and F1, we report two numbers following\nZadeh et al. (2019): the number on the left side of / is calculated based on the approach from Zadeh et al. (2018b),\nand the right side is by Tsai et al. (2019).\nMethods Dataset R@1 R@5 R@10 Median R\nUniVL Youcook2 22.2 52.2 66.2 5\n-w/o Joint Youcook2 19.5 48.0 62.7 6\n-w/o Alignment Youcook2 16.3 42.3 56.2 8\n-w/o EnhancedV Youcook2 16.1 41.3 55.8 8\n-w/o Decoder Youcook2 14.6 40.3 55.5 8\n-w/o StagedP Youcook2 11.9 35.0 48.9 11\n-w/o Pre-training Youcook2 7.7 23.9 34.7 21\nUniVL MSR-VTT 20.6 49.1 62.9 6\n-w/o Joint MSR-VTT 19.6 45.9 62.6 6\n-w/o Alignment MSR-VTT 19.3 44.6 60.1 7\n-w/o EnhancedV MSR-VTT 18.0 45.3 59.3 7\n-w/o Decoder MSR-VTT 18.9 44.9 57.8 7\n-w/o StagedP MSR-VTT 18.0 44.3 57.7 8\n-w/o Pre-training MSR-VTT 16.7 44.0 55.9 8\nTable 7: Ablation study on retrieval task. ‘-w/o’ means reducing the condition above the previous line.\nMethods B-3 B-4 M R-L CIDEr\nUniVL 23.87 17.35 22.35 46.52 1.81\n-w/o Joint 23.96 17.54 22.48 46.77 1.84\n-w/o Alignment 23.51 17.24 22.02 45.90 1.77\n-w/o EnhancedV 23.15 17.04 21.83 45.89 1.76\n-w/o Decoder 19.01 13.22 19.43 43.62 1.53\n-w/o StagedP 18.13 12.49 18.78 42.64 1.46\n-w/o Pre-training 14.23 9.46 16.27 37.44 1.15\nTable 8: Ablation study on caption task of Youcook2 dataset. ‘-w/o’ means reducing the condition above the\nprevious line.\nMethod Visual Feature R@1 R@5 R@10 Median R\nUniVL RS152 + RX101 11.5 29.1 40.1 17\non Youcook2 S3D 22.2 52.2 66.2 5\nUniVL RS152 + RX101 18.7 44.4 58.9 7\non MSR-VTT S3D 20.6 49.1 62.9 6\nTable 9: Ablation study of visual features for re-\ntrieval task. RS152 denotes ResNet-152, RX101 means\nResNeXt-101.\nMethod Visual Feature B-3 B-4 M R-L CIDEr\nUniVL RS152 + RX101 20.42 14.31 19.92 42.35 1.47\nS3D 23.87 17.35 22.35 46.52 1.81\nTable 10: Ablation study of visual features for multi-\nmodal video captioning results on Youcook2 dataset.\nRS152 denotes ResNet-152, RX101 means ResNeXt-\n101.\n4.4.1 Modules and Strategies.\nTable 7 shows the effectiveness of each objective\nor strategy on the retrieval task. The results are re-\nported on both Youcook2 and MSR-VTT datasets.\nSimultaneously, Table 8 demonstrates the effective-\nness of each objective or strategy on the caption\ntask. For the retrieval task, we exploit UniVL (FT-\nJoint) ﬁne-tuning strategy to study the objectives:\nJoint loss, Alignment loss, and Decoder loss, and\nthe strategies: StagedP and EnhancedV show con-\nsistent improvement. From the result, we can see\nthat the cross encoder and decoder modules can\npromote the joint representation of video and text.\nFor the caption task, we ﬁnd that the decoder mod-\nule shows great advantage and achieves more than\n3 points gain on the BLUE-4 metric. Another ﬁnd-\ning is that the Joint loss decreases the generation\ntask a little, although it performs well in the re-\ntrieval task. Excessive emphasis on coarse-grained\nmatching can affect the ﬁne-grained description at\nthe generation task.\n4.4.2 Visual Features.\nWe compare the S3D video feature pre-trained on\nHowto100M and ResNet-152 plus ResNeXt-101\npre-trained on labeled ImageNet and Kinetics re-\nspectively. The ResNet-152 (RS152) and ResNeXt-\n101 (RX101) are used to extract 2D and 3D features\nfrom video clips respectively similar to Miech et al.\n(2019)’s work.\nAs shown in Table 9 and Table 10, the visual\nfeature is important in our pre-training model and\nthe downstream tasks. It is worth studying an\nend to end training from raw videos instead of\nextracted ﬁxed video features in the future. How-\never, the time-cost and the memory-cost are enor-\nmous. The key bottleneck is visual representation,\nand we propose two possible approaches: design-\ning a lightweight training scheme, e.g., training on\nkeyframes of video, using a small feature dimen-\nsion size.\n5 Conclusion and Discussion\nThis paper proposes UniVL with self-supervised\nlearning for video and language representation on\nlarge scale videos. The UniVL is designed with\nfour modules and ﬁve objectives for both video-\nlanguage understanding and generation tasks. It is\na ﬂexible model for most of the multimodal down-\nstream tasks considering both efﬁciency and effec-\ntiveness. We conduct extensive experiments on\nevaluating our model for ﬁve downstream tasks,\ne.g., text-based video retrieval and multimodal\nvideo captioning. The experimental results demon-\nstrate that our pre-trained model can improve the\nperformance to a large extent over the baseline\nmodels and achieve state-of-the-art results on ﬁve\ntypical multimodal tasks. Besides, we will investi-\ngate our model’s performance on more massive\ndatasets and more downstream tasks for future\nwork.\nReferences\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant\nAgrawal, Josef Sivic, Ivan Laptev, and Simon\nLacoste-Julien. 2016. Unsupervised learning from\nnarrated instruction videos. In CVPR, pages 4575–\n4583.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186.\nLi Ding and Chenliang Xu. 2018. Weakly-supervised\naction segmentation with iterative soft boundary as-\nsignment. In CVPR, pages 6508–6516.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197.\nTengda Han, Weidi Xie, and Andrew Zisserman. 2019.\nVideo representation learning by dense predictive\ncoding. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, pages\n0–0.\nJack Hessel, Bo Pang, Zhenhai Zhu, and Radu Soricut.\n2019. A case study on combining asr and visual fea-\ntures for generating instructional video captions. In\nCoNLL.\nDotan Kaufman, Gil Levi, Tal Hassner, and Lior Wolf.\n2017. Temporal tessellation: A uniﬁed approach for\nvideo analysis. In ICCV, pages 94–104.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. ICLR.\nRyan Kiros, Ruslan Salakhutdinov, and Richard S\nZemel. 2014. Unifying visual-semantic embeddings\nwith multimodal neural language models. arXiv\npreprint arXiv:1411.2539.\nBenjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf.\n2015. Associating neural word embeddings with\ndeep image representations using ﬁsher vectors. In\nCVPR, pages 4437–4446.\nBruno Korbar, Fabio Petroni, Rohit Girdhar, and\nLorenzo Torresani. 2020. Video understand-\ning as machine translation. arXiv preprint\narXiv:2006.07203.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In ACL, pages 7871–7880.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. 2019a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. arXiv preprint arXiv:1908.06066.\nLinjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan,\nLicheng Yu, and Jingjing Liu. 2020. Hero: Hi-\nerarchical encoder for video+ language omni-\nrepresentation pre-training. arXiv preprint\narXiv:2005.00200.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019b. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-\nPhilippe Morency. 2018. Multimodal language anal-\nysis with recurrent multistage fusion. In EMNLP,\npages 150–161.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In ACL, page 605.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWilliam Lotter, Gabriel Kreiman, and David Cox. 2016.\nDeep predictive coding networks for video predic-\ntion and unsupervised learning. arXiv preprint\narXiv:1605.08104.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In NeurIPS, pages 13–23.\nMichael Mathieu, Camille Couprie, and Yann Le-\nCun. 2015. Deep multi-scale video predic-\ntion beyond mean square error. arXiv preprint\narXiv:1511.05440.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira,\nIvan Laptev, Josef Sivic, and Andrew Zisserman.\n2020. End-to-End Learning of Visual Represen-\ntations from Uncurated Instructional Videos. In\nCVPR.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\n2019. Howto100m: Learning a text-video embed-\nding by watching hundred million narrated video\nclips. ICCV.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL, pages 311–\n318.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\nShyam Sundar Rajagopalan, Louis-Philippe Morency,\nTadas Baltrusaitis, and Roland Goecke. 2016. Ex-\ntending long short-term memory for multi-view\nstructured learning. In ECCV, pages 338–353.\nAlexander Richard, Hilde Kuehne, Ahsan Iqbal, and\nJuergen Gall. 2018. Neuralnetwork-viterbi: A\nframework for weakly supervised video learning. In\nCVPR, pages 7386–7395.\nBotian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,\nZhendong Niu, and Ming Zhou. 2019. Dense proce-\ndure captioning in narrated instructional videos. In\nACL, pages 6382–6391.\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. arXiv preprint\narXiv:1905.02450.\nNitish Srivastava, Elman Mansimov, and Ruslan\nSalakhudinov. 2015. Unsupervised learning of\nvideo representations using lstms. In ICML, pages\n843–852.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn ICLR.\nChen Sun, Fabien Baradel, Kevin Murphy, and\nCordelia Schmid. 2019a. Contrastive bidirectional\ntransformer for temporal representation learning.\narXiv preprint arXiv:1906.05743.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019b. Videobert: A\njoint model for video and language representation\nlearning. ICCV.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,\nDanyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.\n2019. COIN: A large-scale dataset for comprehen-\nsive instructional video analysis. In CVPR, pages\n1207–1216.\nAtousa Torabi, Niket Tandon, and Leonid Sigal. 2016.\nLearning language-visual embedding for movie un-\nderstanding with natural-language. arXiv preprint\narXiv:1609.08124.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In ACL,\npages 6558–6569.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998–6008.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR, pages 4566–4575.\nXiaolong Wang and Abhinav Gupta. 2015. Unsuper-\nvised learning of visual representations using videos.\nIn ICCV, pages 2794–2802.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency. 2019.\nWords can shift: Dynamically adjusting word rep-\nresentations using nonverbal behaviors. In AAAI,\npages 7216–7223.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu,\nand Kevin Murphy. 2018. Rethinking spatiotempo-\nral feature learning: Speed-accuracy trade-offs in\nvideo classiﬁcation. In ECCV, pages 318–335.\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In CVPR, pages 5288–5296.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nYoungjae Yu, Jongseok Kim, and Gunhee Kim. 2018.\nA joint sequence fusion model for video question an-\nswering and retrieval. In ECCV, pages 487–503.\nYoungjae Yu, Hyungjin Ko, Jongwook Choi, and Gun-\nhee Kim. 2016. Video captioning and retrieval mod-\nels with semantic attention. In ECCVLSMDC2016\nWorkshop.\nYoungjae Yu, Hyungjin Ko, Jongwook Choi, and Gun-\nhee Kim. 2017. End-to-end concept word detection\nfor video captioning, retrieval, and question answer-\ning. In CVPR, pages 3261–3269.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency. 2017. Tensor fu-\nsion network for multimodal sentiment analysis. In\nEMNLP, pages 1103–1114.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning. AAAI.\nAmir Zadeh, Paul Pu Liang, Soujanya Poria, Pra-\nteek Vij, Erik Cambria, and Louis-Philippe Morency.\n2018b. Multi-attention recurrent network for human\ncommunication comprehension. In AAAI.\nAmir Zadeh, Chengfeng Mao, Kelly Shi, Yiwei Zhang,\nPaul Pu Liang, Soujanya Poria, and Louis-Philippe\nMorency. 2019. Factorized multimodal transformer\nfor multimodal sequential learning. arXiv preprint\narXiv:1911.09826.\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016. Multimodal sentiment in-\ntensity analysis in videos: Facial gestures and verbal\nmessages. IEEE Intelligent Systems, 31(6):82–88.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong\nHu, Jason J Corso, and Jianfeng Gao. 2019. Uni-\nﬁed vision-language pre-training for image caption-\ning and vqa. arXiv preprint arXiv:1909.11059.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. 2018a.\nTowards automatic learning of procedures from web\ninstructional videos. In AAAI.\nLuowei Zhou, Yingbo Zhou, Jason J Corso, Richard\nSocher, and Caiming Xiong. 2018b. End-to-end\ndense video captioning with masked transformer. In\nCVPR, pages 8739–8748.\nLinchao Zhu and Yi Yang. 2020. Actbert: Learning\nglobal-local video-text representations. In CVPR.\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gok-\nberk Cinbis, David Fouhey, Ivan Laptev, and Josef\nSivic. 2019. Cross-task weakly supervised learning\nfrom instructional videos. In CVPR.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8448830246925354
    },
    {
      "name": "Encoder",
      "score": 0.7105716466903687
    },
    {
      "name": "Transformer",
      "score": 0.6514797806739807
    },
    {
      "name": "Representation (politics)",
      "score": 0.5095663070678711
    },
    {
      "name": "Language model",
      "score": 0.44704464077949524
    },
    {
      "name": "Multimedia",
      "score": 0.4425889849662781
    },
    {
      "name": "Natural language processing",
      "score": 0.4196354150772095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40189069509506226
    },
    {
      "name": "Speech recognition",
      "score": 0.36283230781555176
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}