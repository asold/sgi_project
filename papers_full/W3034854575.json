{
  "title": "SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics",
  "url": "https://openalex.org/W3034854575",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2123677600",
      "name": "Da Yin",
      "affiliations": [
        "Peking University",
        "King University",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A1966014882",
      "name": "Tao Meng",
      "affiliations": [
        "King University",
        "Peking University",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2208999240",
      "name": "Kai-Wei Chang",
      "affiliations": [
        "Peking University",
        "University of California, Los Angeles",
        "King University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473104040",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2066551267",
    "https://openalex.org/W2983772459",
    "https://openalex.org/W2137607259",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1889268436",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W2097726431",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2963454111",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2985866243",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W193524605",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2251189452",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2251869843",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2250243742",
    "https://openalex.org/W2891430112",
    "https://openalex.org/W2955429306",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4313490656",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2964126298",
    "https://openalex.org/W2108646579",
    "https://openalex.org/W2251939518"
  ],
  "abstract": "We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT. We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3695–3706\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3695\nSentiBERT: A Transferable Transformer-Based Architecture for\nCompositional Sentiment Semantics\nDa Yin♣, Tao Meng♠, Kai-Wei Chang♠\n♣ Peking University\n♠ University of California, Los Angeles\nwade yin9712@pku.edu.cn, tmeng@cs.ucla.edu, kw@kwchang.net\nAbstract\nWe propose SentiBERT, a variant of BERT\nthat effectively captures compositional sen-\ntiment semantics. The model incorporates\ncontextualized representation with binary con-\nstituency parse tree to capture semantic com-\nposition. Comprehensive experiments demon-\nstrate that SentiBERT achieves competi-\ntive performance on phrase-level sentiment\nclassiﬁcation. We further demonstrate that\nthe sentiment composition learned from the\nphrase-level annotations on SST can be trans-\nferred to other sentiment analysis tasks as\nwell as related tasks, such as emotion clas-\nsiﬁcation tasks. Moreover, we conduct ab-\nlation studies and design visualization meth-\nods to understand SentiBERT. We show\nthat SentiBERT is better than baseline ap-\nproaches in capturing negation and the con-\ntrastive relation and model the compositional\nsentiment semantics.\n1 Introduction\nSentiment analysis is an important language pro-\ncessing task (Pang et al., 2002, 2008; Liu, 2012).\nOne of the key challenges in sentiment analysis is\nto model compositional sentiment semantics. Take\nthe sentence“Frenetic but not really funny. ”in Fig-\nure 1 as an example. The two parts of the sentence\nare connected by“but”, which reveals the change\nof sentiment. Besides, the word“not” changes the\nsentiment of“really funny”. These types of nega-\ntion and contrast are often difﬁcult to handle when\nthe sentences are complex (Socher et al., 2013; Tay\net al., 2018; Xu et al., 2019).\nIn general, the sentiment of an expression is de-\ntermined by the meaning of tokens and phrases and\nthe way how they are syntactically combined. Prior\nstudies consider explicitly modeling compositional\nsentiment semantics over constituency structure\nwith recursive neural networks (Socher et al., 2012,\n/g85/g72/g68/g79/g79/g92/g73/g88/g81/g81/g92\n/g17\n/g81/g82/g87/g69/g88/g87/g41/g85/g72/g81/g72/g87/g76/g70\n/g49/g72/g74/g68/g87/g76/g89/g72\n/g49/g72/g88/g87/g85/g68/g79\n/g51/g82/g86/g76/g87/g76/g89/g72\nFigure 1: Illustration of the challenges of learning sen-\ntiment semantic compositionality. The blue nodes rep-\nresent token nodes. The colors of phrase nodes in\nthe binary constituency tree represent the sentiment\nof phrases. The red boxes show that the sentiment\nchanges from the child node to the parent node due to\nnegation and contrast.\n2013). However, these models that generate repre-\nsentation of a parent node by aggregating the local\ninformation from child nodes, overlook the rich\nassociation in context.\nIn this paper, we propose SentiBERT to in-\ncorporate recently developed contextualized rep-\nresentation models (Devlin et al., 2019; Liu et al.,\n2019) with the recursive constituency tree structure\nto better capture compositional sentiment seman-\ntics. Speciﬁcally, we build a simple yet effective\nattention network for composing sentiment seman-\ntics on top of BERT (Devlin et al., 2019). During\ntraining, we follow BERT to capture contextual\ninformation by masked language modeling. In ad-\ndition, we instruct the model to learn composition\nof meaning by predicting sentiment labels of the\nphrase nodes.\nResults on phrase-level sentiment classiﬁcation\non Stanford Sentiment Treebank (SST) (Socher\net al., 2013) indicate thatSentiBERT improves\nsigniﬁcantly over recursive networks and the base-\n3696\n/g20/g90 /g21/g90 /g81/g90/g20/g16/g81/g90/g17/g17/g17/g17/g17/g17\n/g36/g87/g87/g72/g81/g87/g76/g82/g81\n/g51/g75/g85/g68/g86/g72/g3/g49/g82/g71/g72/g3\n/g51/g85/g72/g71/g76/g70/g87/g76/g82/g81\n/g17/g17/g17/g17/g17/g17\n/g38/g82/g80/g83/g82/g86/g76/g87/g76/g82/g81\n/g20/g75 /g21/g75 /g20/g16/g81/g75 /g81/g75\n/g17/g17/g17/g17/g17/g17\n/g55/g82/g78/g72/g81/g3/g44/g81/g83/g88/g87\n/g48/g82/g71/g88/g79/g72/g3/g44\n/g48/g82/g71/g88/g79/g72/g3/g44/g44\n/g48/g82/g71/g88/g79/g72\n/g3/g44/g44/g44\n/g47/g68/g92/g72/g85/g3/g20/g29/g47/g68/g92/g72/g85/g3/g21/g29\n/g17/g17/g17/g17/g17/g17 /g17/g17/g17/g17/g17/g17\n/g86/g87/g20/g14/g86/g87 /g72/g81/g20/g16/g72/g81 /g86/g87/g20/g14/g86/g87 /g72/g81/g20/g16/g72/g81\n/g37/g40/g53/g55\nFigure 2: The architecture of SentiBERT. Module I is the BERT encoder; Module II denotes the semantic\ncomposition module based on an attention mechanism; Module III is a predictor for phrase-level sentiment. The\nsemantic composition module is a two layer attention-based network (see Section3.1) The ﬁrst layer (Attention\nto Tokens) generates representation for each phrase based on the token it covers and the second layer (Attention\nto Children) reﬁnes the phrase representation obtained from the ﬁrst layer based on its children.\nline BERT model. As phrase-level sentiment labels\nare expensive to obtain, we further explore if the\ncompositional sentiment semantics learned from\none task can be transferred to others. In particular,\nwe ﬁnd thatSentiBERT trained on SST can be\ntransferred well to other related tasks such as twit-\nter sentiment analysis (Rosenthal et al., 2017) and\nemotion intensity classiﬁcation (Mohammad et al.,\n2018) and contextual emotion detection (Chatter-\njee et al., 2019). Furthermore, we conduct com-\nprehensive quantitative and qualitative analyses to\nevaluate the effectiveness ofSentiBERT under\nvarious situations and to demonstrate the seman-\ntic compositionality captured by the model. The\nsource code is available athttps://github.com/\nWadeYin9712/SentiBERT.\n2 Related Work\nSentiment Analysis V arious approaches have\nbeen applied to build a sentiment classiﬁer, includ-\ning feature-based methods (Hu and Liu, 2004; Pang\nand Lee, 2004), recursive neural networks (Socher\net al., 2012, 2013; Tai et al., 2015), convolution\nneural networks (Kim, 2014) and recurrent neu-\nral networks (Liu et al., 2015). Recently, pre-\ntrained language models such as ELMo (Peters\net al., 2018), BERT (Devlin et al., 2019) and Sen-\ntiLR (Ke et al., 2019) achieve high performance in\nsentiment analysis by constructing contextualized\nrepresentation. Inspired by these prior studies, we\ndesign a transformer-based neural network model\nto capture compositional sentience semantics by\nleveraging binary constituency parse tree.\nSemantic Compositionality Semantic composi-\ntion (Pelletier, 1994) has been widely studied in\nNLP literature. For example, Mitchell and Lap-\nata (2008) introduce operations such as addition\nor element-wise product to model compositional\nsemantics. The idea of modeling semantic compo-\nsition is applied to various areas such as sentiment\nanalysis (Socher et al., 2013; Zhu et al., 2016),\nsemantic relatedness (Marelli et al., 2014) and cap-\nturing sememe knowledge (Qi et al., 2019). In this\npaper, we demonstrate that the syntactic structure\ncan be combined with contextualized representa-\ntion such that the semantic compositionality can\nbe better captured. Our approach resembles to a\nfew recent attempts (Harer et al., 2019; Wang et al.,\n2019) to integrate tree structures into self-attention.\nHowever, our design is speciﬁc for the semantic\ncomposition in sentiment analysis.\n3 Model\nWe introduceSentiBERT, a model that captures\ncompositional sentiment semantics based on con-\nstituency structures of sentences. SentiBERT\nconsists of three modules: 1) BERT; 2) a semantic\ncomposition module based on an attention network;\n3) phrase and sentence sentiment predictors. The\nthree modules are illustrated in Figure2 and we\nprovide an overview in below.\nBERT We incorporate BERT ( Devlin et al. ,\n2019) as the backbone to generate contextualized\n3697\nrepresentation of input sentence.\nSemantic Composition Module This module\naims to obtain effective phrase representation\nguided by the contextualized representation and\nconstituency parsing tree. To reﬁne phrase repre-\nsentation based on the structural information and\nits constituencies, we design a two-level attention\nmechanism: 1) Attention to Tokensand 2)Attention\nto Children.\nPhrase Node Prediction SentiBERT is super-\nvised by phrase-level sentiment labels. We use\ncross-entropy as the loss function for learning the\nsentiment predictor.\n3.1 Attention Networks for Sentiment\nSemantic Composition\nIn this section, we describe the attention networks\nfor sentiment semantic composition in detail.\nWe ﬁrst introduce the notations. s =\n[w1,w2,...,w n] denotes a sentence which consists\nof n words. phr = [phr1,phr2,..., phrm] denotes\nthe phrases on the binary constituency tree of sen-\ntence s. h =[ h1,h2,..., hn] is the contextualized\nrepresentation of tokens after forwarding to a fully-\nconnected layer, whereht ∈ Rd. Suppose sti and\neni are beginning and end indices of thei-th phrase\nwhere wsti ,wsti+1,...,w eni are constituent tokens\nof thei-th phrase. The corresponding token repre-\nsentation is[hsti ,hsti+1,..., heni ]. pi is the phrase\nrepresentation of thei-th phrase.\nAttention to Tokens Given the contextualized\nrepresentations of the tokens covered by a phrase.\nWe ﬁrst generate phrase representation vi for a\nphrase i by the following attention network.\nqi = 1\neni −sti +1\neni∑\nj=sti\nhj,\ntj = Attention(qi,hj),sti ≤ j ≤ eni,\naj = exp(tj)∑eni\nk=sti exp(tk),\noi =\neni∑\nj=sti\naj ·hj.\n(1)\nIn Eq. (1), we ﬁrst treat the averaged representa-\ntion for each token as the query, and then allocate\nattention weights according to the correlation with\neach token. aj represents the weight distributed\nto the j-th token. We concatenate the weighted\nsum oi and qi and feed it to forward networks.\nLastly, we obtain the initial representation for the\nphrase vi ∈ Rd based on the representation of\nconstituent tokens. The detailed computation of\nattention mechanism is shown in AppendixA.1.\nAttention to Children Furthermore, we reﬁne\nphrase representations in the second layer based on\nconstituency parsing tree and the representations\nobtained in the ﬁrst layer. To aggregate information\nbased on hierarchical structure, we develop the\nfollowing network. For each phrase, the attention\nnetwork computes correlation with its children in\nthe binary constituency parse tree and itself.\nAssume that the indices of child nodes of the\ni-th phrase are lson and rson. Their representa-\ntions generated from the ﬁrst layer arevi, vlson,\nand vrson, respectively. We generate the attention\nweights rlson, rrson and ri over thei-th phrase and\nits left and right children by the following.\nclson = Attention(vi,vlson),\ncrson = Attention(vi,vrson),\nci = Attention(vi,vi),\nrlson,rrson,ri = Softmax(clson,crson,ci).\n(2)\nThen the reﬁned representation of phraseiis com-\nputed by\nfi = rlson ·vlson + rrson ·vrson + ri ·vi.\nFinally, we concatenate the weighted sum fi\nand vi and feed it to forward networks with\nSeLU (Klambauer et al., 2017) and GeLU acti-\nvations (Hendrycks and Gimpel, 2017) and layer\nnormalization (Ba et al., 2016), similar to Joshi\net al.(2020) to generate the ﬁnal phrase represen-\ntation pi ∈ Rd. Note that when the child ofi-th\nphrase is token node, the attention mechanism will\nattend to the representation of all the subtokens the\ntoken node covers.\n3.2 Training Objective of SentiBERT\nInspired by BERT, the training objective of\nSentiBERT consists of two parts: 1) Masked Lan-\nguage Modeling. Some texts are masked and the\nmodel learn to predict them. This objective allows\nthe model learn to capture the contextual informa-\ntion as in the original BERT model. 2) Phrase Node\nPrediction. We further consider training the model\nto predict the phrase-level sentiment label based on\nthe aforementioned phrase representations. This\nallows SentiBERT lean to capture the composi-\ntional sentiment semantics. Similar to BERT, in the\n3698\ntransfer learning setting, pre-trainedSentiBERT\nmodel can be used to initialize the model parame-\nters of a downstream model.\n4 Experiments\nWe evaluateSentiBERT on the SST dataset. We\nthen evaluate SentiBERT in a transfer learning\nsetting and demonstrate that the compositional sen-\ntiment semantics learned on SST can be transferred\nto other related tasks.\n4.1 Experimental Settings\nWe evaluate how effectiveSentiBERT captures\nthe compositional sentiment semantics on SST\ndataset (Socher et al., 2013).\nThe SST dataset has several variants.\n• SST-phrase is a 5-class classiﬁcation task\nthat requires to predict the sentiment of all\nphrases on a binary constituency tree. Dif-\nferent from Socher et al.(2013), we test the\nmodel only on phrases (non-terminal con-\nstituents) and ignore its performance on to-\nkens.\n• SST-5 is a 5-class sentiment classiﬁcation task\nthat aims at predicting the sentiment of a sen-\ntence. We use it to test ifSentiBERT learns\na better sentence representation through cap-\nturing compositional sentiment semantics.\n• Similar to SST-5, SST-2 and SST-3 are 2-\nclass and 3-class sentiment classiﬁcation tasks.\nHowever, the granularity of the sentiment\nclasses is different.\nBesides, to test the transferability of\nSentiBERT, we consider several related datasets,\nincluding Twitter Sentiment Analysis (Rosenthal\net al., 2017), Emotion Intensity Classiﬁcation (Mo-\nhammad et al., 2018) and Contextual Emotion\nDetection (EmoContext) (Chatterjee et al., 2019).\nDetails are shown in AppendixA.2.\nWe buildSentiBERT on the HuggingFace li-\nbrary1 and initialize the model parameters using\npre-trained BERT-base and RoBERTa-base models\nwhose maximum length is 128, layer number is 12,\nand embedding dimension is 768. For the train-\ning on SST-phrase, the learning rate is\n2 × 10−5,\nbatch size is 32 and the number of training epochs\nis 3. For masking mechanism, to put emphasis on\n1https://github.com/huggingface\nmodeling sentiments, the probability of masking\nopinion words which can be retrieved from Senti-\nWordNet (Baccianella et al., 2010) is set to 20%,\nand for the other words, the probability is 15%. For\nﬁne-tuning on downstream tasks, the learning rate\nis {1×10−5 −1×10−4}, batch size is{16,32}and\nthe number of training epochs is1−5. We use Stan-\nford CoreNLP API (Manning et al., 2014) to obtain\nbinary constituency trees for the sentences of these\ntasks to keep consistent with the settings on SST-\nphrase. Note that when ﬁne-tuning on sentence-\nlevel sentiment and emotion classiﬁcation tasks,\nthe objective is to correctly label the root of tree,\ninstead of targeting at the[CLS] token representa-\ntion as in the original BERT.\n4.2 Effectiveness of SentiBERT\nWe ﬁrst compare the proposed attention networks\n(SentiBERT w/o BERT) with the following base-\nline models trained on SST-phrase corpus to evalu-\nate the effectiveness of the architecture design: 1)\nRecursive NN (Socher et al., 2013); 2) GCN (Kipf\nand Welling, 2017); 3) Tree-LSTM ( Tai et al.,\n2015); 4) BiLSTM (Hochreiter and Schmidhuber,\n1997) w/ Tree-LSTM. To further understand the\neffect of using contextualized representation, we\ncompare SentiBERT with the vanilla pre-trained\nBERT and its variants which combine the four men-\ntioned baselines and BERT. The training settings\nremain the same with\nSentiBERT. We also ini-\ntialize SentiBERT with pre-trained parameters of\nRoBERTa (SentiBERT w/ RoBERTa) and further\ncompare it with its variants.\nAs shown in Table 1, SentiBERT and\nSentiBERT w/ RoBERTa substantially outper-\nforms their corresponding variants and the net-\nworks merely built on the tree. Speciﬁcally,\nwe ﬁrst observe that though our attention net-\nwork (\nSentiBERT w/o BERT) is simple, it is\ncompetitive with Recursive NN, GCN and Tree-\nLSTM. Besides,SentiBERT largely outperforms\nSentiBERT w/o BERT by leveraging contextual-\nized representation. Moreover, the results manifest\nthat SentiBERT and SentiBERT w/ RoBERTa\noutperform the BERT and RoBERTa, indicating the\nimportance of incorporating syntactic guidance.\n4.3 Transferability of SentiBERT\nThough the designed models are effective, we are\ncurious how beneﬁcial the compositional sentiment\nsemantics learned on SST can be transferred to\nother tasks. We compareSentiBERT with pub-\n3699\nModels SST-phrase SST-5\nRecursive NN 58.33 46.53\nGCN 60.89 49.34\nTree-LSTM 61.71 50.07\nBiLSTM w/ Tree-LSTM 61.89 50.45\nBERT w/ Mean pooling 64.53 50.68\nBERT w/ GCN 65.23 54.56\nBERT w/ Tree-LSTM 67.39 55.89\nRoBERTa w/ Mean pooling 67.73 56.34\nSentiBERT w/o BERT 61.04 50.31\nSentiBERT 68.31 56.10\nSentiBERT w/ RoBERTa 68.98 56.87\nTable 1: The averaged accuracies on SST-phrase and\nSST-5 tasks (%) for 5 runs. For baselines vanilla BERT\nand RoBERTa, we use mean-pooling on token repre-\nsentation of top layer to get phrase and sentence repre-\nsentation.\nModels SST-2 (Dev) SST-3 Twitter\nBERT 92.39 73.78 70.0\nBERT w/ Mean pooling 92.33 74.35 69.7\nXLNet 93.23 75.89 70.7\nRoBERTa 94.31 78.04 71.1\nSentiBERT w/o BERT 86.57 68.32 64.9\nSentiBERT w/o Masking 92.48 76.95 70.7\nSentiBERT w/o Pre-training 92.44 76.78 70.8\nSentiBERT 92.78 77.11 70.9\nSentiBERT w/ RoBERTa 94.72 78.69 71.5\nTable 2: The averaged results on sentence-level senti-\nment classiﬁcation (%) for 5 runs. For SST-2,3, the\nmetric is accuracy; for Twitter Sentiment Analysis, we\nuse averaged recall value.\nlished models BERT, XLNet, RoBERTa and their\nvariants on benchmarks mentioned in Section4.1.\nSpeciﬁcally, ‘BERT’ indicates the model trained on\nthe raw texts of the SST dataset. ‘BERT w/ Mean\npooling’ denotes the model trained on SST, whose\nphrase and sentence representation is computed by\nmean pooling on tokens. ‘BERT w/ Mean pooling’\nmerely leverages the phrases’ range information\nrather than syntactic structural information.\nSentiment Classiﬁcation Tasks The evaluation\nresults of sentence-level sentiment classiﬁcation\non the three tasks are shown in Table2. Despite\nthe difference among tasks and datasets, from ex-\nperimental results, we ﬁnd that\nSentiBERT has\ncompetitive performance compared with various\nbaselines.\nSentiBERT achieves higher perfor-\nmance than the vanilla BERT and XLNet in tasks\nsuch as SST-3 and Twitter Sentiment Analysis.\nBesides,\nSentiBERT signiﬁcantly outperform\nModels Emotion Intensity EmoContext\nBERT 65.2 73.49\nRoBERTa 66.4 74.20\nSentiBERT w/o Pre-training 66.0 73.81\nSentiBERT 66.5 74.23\nSentiBERT w/ RoBERTa 67.2 74.67\nTable 3: The averaged results on several emotion clas-\nsiﬁcation tasks (%) for 5 runs. For Emotion Intensity\nClassiﬁcation task, the metric is averaged Pearson Cor-\nrelation value of the four subtasks; for EmoContext,\nwe follow the standard metrics used inChatterjee et al.\n(2019) and use F1 score as the evaluation metric.\nSentiBERT w/o BERT. This demonstrates the\nimportance of leveraging pre-trained BERT model.\nMoreover, SentiBERT outperforms BERT w/\nMean pooling. This indicates the importance of\nmodeling the compositional structure of sentiment.\nEmotion Classiﬁcation Tasks Emotion detec-\ntion is different from sentiment classiﬁcation. How-\never, these two tasks are related. The task aims\nto classify ﬁne-grained emotions, such as happi-\nness, fearness, anger, sadness, etc. It is challenging\ncompared to sentiment analysis because of vari-\nous emotion types. We ﬁne-tuneSentiBERT and\nSentiBERT w/ RoBERTa on Emotion Intensity\nClassiﬁcation and EmoContext. Table3 shows the\nresults on the two emotion classiﬁcation tasks. Sim-\nilar to the results in sentiment classiﬁcation tasks,\nSentiBERT obtains the best results, further justi-\nfying the transferability ofSentiBERT.\n5 Analysis\nWe conduct experiments on SST-phrase us-\ning BERT-base model as backbone to demon-\nstrate the effectiveness and interpretability of the\nSentiBERT architecture in terms of semantic\ncompositionality. We also explore potential of the\nmodel when lacking phrase-level sentiment infor-\nmation. In order to simplify the analysis of the\nchange of sentiment polarity, we convert the 5-class\nlabels to to 3-class: the classes ‘very negative’ and\n‘negative’ are converted to be ‘negative’; the classes\n‘very positive’ and ‘positive’ are converted to be\n‘positive’; the class ‘neutral’ remains the same. The\ndetails of statistical distribution in this part is shown\nin AppendixA.3.\nWe consider the following baselines to eval-\nuate the effectiveness of each component in\nSentiBERT. First we design BERT w/ Mean\npooling as a base model, to demonstrate the ne-\n3700\nFigure 3: Evaluation for local difﬁculty. The ﬁgure\nshows the accuracy difference on phrase node senti-\nment prediction with BERT w/ Mean pooling for dif-\nferent local difﬁculty.\ncessity of incorporating syntactic guidance and\nimplementing aggregation on it. Then we com-\npare SentiBERT with alternative aggregation ap-\nproaches, Tree-LSTM, GCN and w/o Attention to\nChildren.\n5.1 Semantic Compositionality\nWe investigate how effectivelySentiBERT cap-\ntures compositional sentiment semantics. We focus\non how the representation inSentiBERT captures\nthe sentiments when the children and parent in the\nconstituency tree have different sentiments (i.e.,\nsentiment switch) as shown in the red boxes of Fig-\nure 1. Here we focus on the sentiment switches\nbetween phrases. We assume that the more the\nsentiment switches, the harder the prediction is.\nWe analyze the model under the following two\nscenarios: local difﬁcultyand global difﬁculty. Lo-\ncal difﬁculty is deﬁned as the number of sentiment\nswitches between a phrase and its children. As we\nconsider binary constituency tree. The maximum\nnumber of sentiment switches for each phrase is\n2. Global difﬁculty indicates number of sentiment\nswitches in the entire constituency tree. The maxi-\nmum number of sentiment switches in the test set\nis 23. The former is a phrase-level analysis and the\nlatter is sentence level.\nWe compareSentiBERT with aforementioned\nbaselines. We group all the nodes and sentences\nin the test set by local and global difﬁculty. Re-\nsults are shown in Figure 3 and Figure 4. Our\nmodel achieves better performance than baselines\nin all situations. Also, we ﬁnd that with the in-\ncrease of difﬁculty, the gap between our models\nFigure 4: Evaluation for global difﬁculty. The ﬁgure\nshows the accuracy difference on phrase node senti-\nment prediction with BERT w/ Mean pooling for dif-\nferent global difﬁculty.\nand baselines becomes larger. Especially, when the\nsentiment labels of both children are different from\nthe parent node (i.e., local difﬁculty is 2), the per-\nformance gap betweenSentiBERT and BERT w/\nTree-LSTM is about 7% accuracy. It also outper-\nforms the baseline BERT model with mean pooling\nby 15%. This validates the necessity of structural\ninformation for semantic composition and the ef-\nfectiveness of our designed attention networks for\nleveraging the hierarchical structures.\n5.2 Negation and Contrastive Relation\nNext, we investigate howSentiBERT deals with\nnegations and contrastive relation.\nNegation: Since the negation words such as‘no’,\n‘n’t’ and ‘not’ will cause the sentiment switches,\nthe number of negation words also reﬂects the difﬁ-\nculty of understanding sentence and its constituen-\ncies. We ﬁrst group the sentences by the number of\nnegation words, and then calculate the accuracy of\nthe prediction on their constituencies respectively.\nIn test set, as there are at most six negation words\nand the amount of sentences with above three nega-\ntion words is small, we separate all the data into\nthree groups.\nResults are provided in Figure5. We observe\nSentiBERT performs the best among all the mod-\nels. Similar to the trend in local and global difﬁ-\nculty experiments, the gap betweenSentiBERT\nand other baselines becomes larger with increase\nof negation words. The results show the ability of\nSentiBERT when dealing with negations.\n3701\nFigure 5: Evaluation for negation. We show the accu-\nracy difference with BERT w/ Mean pooling.\nModels Accuracy\nBERT w/ Mean pooling 26.1\nBERT w/ Tree-LSTM 28.5\nBERT w/ GCN 29.4\nSentiBERT w/o Attention to Children 29.8\nSentiBERT 30.7\nTable 4: Evaluation for contrastive relation (%). We\nshow the accuracy for triple-lets (‘X but Y’, ‘X’, ‘Y’).\nX and Y must be phrases in our experiments.\nContrastive Relation: We evaluate the effective-\nness ofSentiBERT with regards to tackling con-\ntrastive relation problem. Here, we focus on the\ncontrastive conjunction “but”. We pick up the\nsentences containing word ‘but’ of which the sen-\ntiments of left and right parts are different. In our\nanalysis, a ‘X but Y’ can be counted as correct if\nand only if the sentiments of all the phrases in triple-\nlet (‘X but Y’, ‘X’ and ‘Y’) are predicted correctly.\nTable 4 demonstrates the results.SentiBERT out-\nperforms other variants of BERT about 1%, demon-\nstrating its ability in capturing contrastive relation\nin sentences.\n5.3 Case Study\nWe showcase several examples to demonstrate how\nSentiBERT performs sentiment semantic compo-\nsition. We observe the attention distribution among\nhierarchical structures. In Figure7, we demonstrate\ntwo sentences of which the sentiments of all the\nphrases are predicted correctly. We also visualize\nthe attention weights distributed to the child nodes\nand the phrases themselves to see which part might\ncontribute more to the sentiment of those phrases.\nSentiBERT performs well in several aspects.\nFirst, SentiBERT tends to attend to adjectives\nsuch as‘frenetic’ and ‘funny’, which contribute to\nthe phrases’ sentiment. Secondly, facing negation\nwords, SentiBERT considers them and a switch\ncan be observed between the phrases with and with-\nout negation word (e.g.,‘not really funny’and ‘re-\nally funny’). Moreover,SentiBERT can correctly\nanalyze the sentences expressing different senti-\nments in different parts. For the ﬁrst case, the\nmodel concentrates more on the part after‘but’.\n5.4 Amount of Phrase-level Supervision\nWe are also interested in analyzing how much\nphrase-level supervisionSentiBERT needs in or-\nder to capture the semantic compositionality. We\nvary the amount of phrase-level annotations used\nin trainingSentiBERT. Before training, we ran-\ndomly sample 0% to 100% with a step of 10% of\nlabels from SST training set. After pre-training\non them, we ﬁne-tuneSentiBERT on tasks SST-\n5, SST-3 and Twitter Sentiment Analysis. During\nﬁne-tuning, for the tasks which use phrase-level an-\nnotation, such as SST-5 and SST-3, we use the same\nphrase-level annotation during pre-training and the\nsentence-level annotation; for the tasks which do\nnot have phrase-level annotation, we merely use\nthe sentence-level annotation.\nResults in Figure6 show that with about 30%-\n50% of the phrase labels on SST-5 and SST-3, the\nmodel is able to achieve competitive results com-\npared with XLNet. Even without any phrase-level\nsupervision, using 70%-80% of phrase labels in\npre-training allowsSentiBERT competitive with\nXLNet on the Twitter Sentiment Analysis dataset.\nFurthermore, we ﬁnd the conﬁdence of about\n40-50% of phrase nodes in SST-3 task is above\n0.9 and the accuracy of predicting these phrases is\nabove 90% on the SST dataset. Considering the\nprevious results, we speculate if we produce part\nof the phrase labels on generic texts, choose the\npredicted labels with high conﬁdence and add them\nto the original SST training set during the training\nprocess, the results might be further improved.\n6 Conclusion\nWe proposed SentiBERT, an architecture de-\nsigned for capturing better compositional sentiment\nsemantics.\nSentiBERT considers the necessity of\ncontextual information and explicit syntactic guide-\nlines for modeling semantic composition. Exper-\niments show the effectiveness and transferability\n3702\n(a) SST-5 (b) SST-3 (c) Twitter Sentiment Analysis\nFigure 6: The results of SentiBERT trained with part of the phrase-level labels on SST-5, SST-3 and Twitter\nSentiment Analysis. We show the averaged results of 5 runs.\n/g58/g75/g76/g79/g72\n/g87/g75/g72\n/g74/g79/g68/g86/g86 /g86/g79/g76/g83/g83/g72/g85\n/g71/g82/g72/g86 /g81/g10/g87\n/g84/g88/g76/g87/g72\n/g73/g76/g87\n/g15\n/g51/g88/g80/g83/g78/g76/g81\n/g76/g86 /g71/g72/g73/g76/g81/g76/g87/g72/g79/g92/g68\n/g88/g81/g76/g84/g88/g72\n/g17\n/g80/g82/g71/g72/g85/g81/g73/g68/g76/g85/g92/g87/g68/g79/g72\n/g85/g72/g68/g79/g79/g92/g73/g88/g81/g81/g92\n/g17\n/g81/g82/g87/g69/g88/g87/g41/g85/g72/g81/g72/g87/g76/g70\n/g49/g72/g74/g68/g87/g76/g89/g72\n/g49/g72/g88/g87/g85/g68/g79\n/g51/g82/g86/g76/g87/g76/g89/g72\nFigure 7: Cases for interpretability of compositional\nsentiment semantics. The three color blocks between\nparents and children are the attention weights dis-\ntributed to left child, the phrase itself and right child.\nof SentiBERT. Further analysis demonstrates its\ninterpretability and potential with less supervision.\nFor future work, we will extendSentiBERT to\nother applications involving phrase-level annota-\ntions.\nAcknowledgement\nWe would like to thank the anonymous reviewers\nfor the helpful discussions and suggestions. Also,\nwe would thank Liunian Harold Li, Xiao Liu, Wasi\nAhmad and all the members of UCLA NLP Lab\nfor advice about experiments and writing. This\nmaterial is based upon work supported in part by a\ngift grant from Taboola.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer Normalization. arXiv preprint\narXiv:1607.06450.\nStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-\ntiani. 2010. Sentiwordnet 3.0: An Enhanced Lexical\nResource for Sentiment Analysis and Opinion Min-\ning. In LREC, volume 10, pages 2200–2204.\nAnkush Chatterjee, Kedhar Nath Narahari, Meghana\nJoshi, and Puneet Agrawal. 2019. Semeval-2019\nTask 3: Emocontext Contextual Emotion Detection\nin Text. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation, pages 39–48.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers),\npages 4171–4186.\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\nTransformer: A Transformer-Based Method for Cor-\nrection of Tree-Structured Data. arXiv preprint\narXiv:1908.00449.\nDan Hendrycks and Kevin Gimpel. 2017. A Baseline\nfor Detecting Misclassiﬁed and Out-of-Distribution\nExamples in Neural Networks.Proceedings of Inter-\nnational Conference on Learning Representations.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation,\n9(8):1735–1780.\nMinqing Hu and Bing Liu. 2004. Mining and Sum-\nmarizing Customer Reviews. InProceedings of the\nTenth ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 168–\n177. ACM.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving Pre-training by Representing\nand Predicting Spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. 2019. SentiLR: Linguistic Knowledge\nEnhanced Language Representation for Sentiment\nAnalysis. arXiv preprint arXiv:1911.02493.\nY oon Kim. 2014. Convolutional Neural Networks for\nSentence Classiﬁcation. In Proceedings of the 2014\n3703\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1746–1751. As-\nsociation for Computational Linguistics.\nThomas N. Kipf and Max Welling. 2017. Semi-\nSupervised Classiﬁcation with Graph Convolutional\nNetworks. In ICLR.\nG¨unter Klambauer, Thomas Unterthiner, Andreas\nMayr, and Sepp Hochreiter. 2017. Self-Normalizing\nNeural Networks. In Advances in Neural Informa-\ntion Processing Systems, pages 971–980.\nBing Liu. 2012. Sentiment Analysis and Opinion Min-\ning. Synthesis Lectures on Human Language Tech-\nnologies, 5(1):1–167.\nPengfei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and\nXuanjing Huang. 2015. Multi-timescale Long Short-\nTerm Memory Neural Network for Modelling Sen-\ntences and Documents. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2326–2335.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP Natural\nLanguage Processing Toolkit. In Association for\nComputational Linguistics (ACL) System Demon-\nstrations, pages 55–60.\nMarco Marelli, Luisa Bentivogli, Marco Baroni, Raf-\nfaella Bernardi, Stefano Menini, and Roberto Zam-\nparelli. 2014. Semeval-2014 Task 1: Evaluation of\nCompositional Distributional Semantic Models on\nFull Sentences through Semantic Relatedness and\nTextual Entailment. In Proceedings of the 8th In-\nternational Workshop on Semantic Evaluation (Se-\nmEval 2014), pages 1–8.\nJeff Mitchell and Mirella Lapata. 2008. V ector-based\nModels of Semantic Composition. In Proceedings\nof ACL-08: HLT, pages 236–244.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. Semeval-\n2018 Task 1: Affect in Tweets. In Proceedings of\nthe 12th International Workshop on Semantic Evalu-\nation, pages 1–17.\nBo Pang and Lillian Lee. 2004. A Sentimental Edu-\ncation: Sentiment Analysis Using Subjectivity Sum-\nmarization Based on Minimum Cuts. In Proceed-\nings of the 42nd Annual Meeting on Association for\nComputational Linguistics, page 271. Association\nfor Computational Linguistics.\nBo Pang, Lillian Lee, and Shivakumar V aithyanathan.\n2002. Thumbs Up?: Sentiment Classiﬁcation Using\nMachine Learning techniques. InProceedings of the\nACL-02 Conference on Empirical Methods in Natu-\nral Language Processing-V olume 10, pages 79–86.\nAssociation for Computational Linguistics.\nBo Pang, Lillian Lee, et al. 2008. Opinion Mining and\nSentiment Analysis. F oundations and Trends\nR⃝ in\nInformation Retrieval, 2(1–2):1–135.\nFrancis Jeffry Pelletier. 1994. The Principle of Seman-\ntic Compositionality. Topoi, 13(1):11–24.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long Papers), pages 2227–\n2237.\nFanchao Qi, Junjie Huang, Chenghao Y ang, Zhiyuan\nLiu, Xiao Chen, Qun Liu, and Maosong Sun. 2019.\nModeling Semantic Compositionality with Sememe\nknowledge. ACL.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemeval-2017 Task 4: Sentiment Analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518.\nRichard Socher, Brody Huval, Christopher D Manning,\nand Andrew Y Ng. 2012. Semantic Composition-\nality through Recursive Matrix-Vector Spaces. In\nProceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning, pages\n1201–1211. Association for Computational Linguis-\ntics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved Semantic Representa-\ntions From Tree-Structured Long Short-Term Mem-\nory Networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (V olume 1: Long\nPapers), pages 1556–1566, Beijing, China. Associa-\ntion for Computational Linguistics.\nYi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su.\n2018. Attentive Gated Lexicon Reader with Con-\ntrastive Contextual Co-attention for Sentiment Clas-\nsiﬁcation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3443–3453.\n3704\nY aushian Wang, Hung-Yi Lee, and Y un-Nung Chen.\n2019. Tree Transformer: Integrating Tree Struc-\ntures into Self-Attention. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1061–1070, Hong Kong,\nChina. Association for Computational Linguistics.\nHu Xu, Bing Liu, Lei Shu, and Philip S Y u. 2019.\nA Failure of Aspect Sentiment Classiﬁers and an\nAdaptive Re-weighting Solution. arXiv preprint\narXiv:1911.01460.\nXiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2016.\nDAG-Structured Long Short-Term Memory for Se-\nmantic Compositionality. In Proceedings of the\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 917–926, San\nDiego, California. Association for Computational\nLinguistics.\n3705\nA Appendix\nA.1 Details of Correlation Computation in\nAttention Networks\nFor vectorsa and b, the correlation between them\nis computed as below:\nAttention(a,b) =tanh(1\nαSeLU((W1 ×a)T ×W3\n×SeLU(W2 ×b))),\n(3)\nwhere SeLU (Klambauer et al., 2017)i sa na c t i v a -\ntion function and α equals 4. The two layers of\nattention networks do not share the parameters.\nA.2 Details of Downstream Tasks\nWe adopt the following tasks for evaluation of\nsentence-level sentiment classiﬁcations:\nSST-2,3 (Socher et al., 2013) These tasks all\nshare with the text of the SST dataset and are single-\nsentence sentiment classiﬁcation task, of which the\nnumbers behind indicate the number of classes.\nSince two of ﬁve classes in SST-5 correspond to\npositive and another two indicate negative, with ad-\nditional neutral ones, the dataset is separated into\nthree groups in SST-3 task. We convert the 5-class\nphrase-level labels in SST-5 into three classes and\nleverage them in the training of SST-3 task.\nTwitter Sentiment Analysis ( Rosenthal et al. ,\n2017) For Twitter Sentiment Analysis, given a\ntweet, model needs to decide which sentiment it\nexpresses: positive, negative or neutral.\nEmotion Intensity Ordinal Classiﬁcation (Mo-\nhammad et al., 2018) The task is, given a tweet\nand an emotion, categorizing the tweet into one\nof four classes of intensity that best represents\ntweeter’s mental state. For Emotion Intensity Clas-\nsiﬁcation task, the metric is averaged Pearson Cor-\nrelation value of the four subtasks, ‘happiness’,\n‘sadness’, ‘anger’ and ‘fearness’.\nEmotions in Textual Conversations (Chatterjee\net al., 2019) In a dialogue, given a sentence with\ntwo turns of conversation, the models needs to clas-\nsify the emotion expressed in the last sentence. For\nEmoContext, we follow the standard metrics used\nin Chatterjee et al.(2019) and use F1 score on the\nthree classes ‘happy’, ‘sad’ and ‘angry’, except\n‘others’ class, as the evaluation metric.\nThe statistics of datasets is shown in Table5.\nDataset Data Split # of Classes\nSST-phrase 8379 / 2184 5\nSST-2 66475 / 859 2\nSST-3 8379 / 2184 3\nSST-5 8379 / 2184 5\nTwitter 50284 / 12273 3\nEmoContext 30141 / 2754 3\nEmoInt\nsad: 1533 / 975\n4angry: 1701 / 1001\nfear: 2252 / 986\njoy: 1616 / 1105\nTable 5: Statistics of benchmarks.\nLocal Difﬁculty 0 1 2\nNumber 28136 10174 1342\nTable 6: The distribution of nodes in terms of local dif-\nﬁculty.\nGlobal Difﬁculty 0-4 5-9 10-14 15-19 20-23\nNumber 930 861 326 59 8\nTable 7: The distribution of nodes in terms of global\ndifﬁculty.\n# of Negation Words 0 1 2-\nNumber 1825 325 34\nTable 8: The distribution of nodes in terms of negation\nwords.\n3706\nModels SST-phrase SST-5\nSentiBERT w/ token 68.23 56.02\nSentiBERT w/ token and RoBERTa 68.78 56.91\nSentiBERT 68.31 56.10\nSentiBERT w/ RoBERTa 68.98 56.87\nTable 9: The results after incorporating token node pre-\ndiction. ‘Token’ denotes token node prediction.\nA.3 Details of Analysis Part\nThe distribution of nodes and sentences in terms\nof local difﬁculty, global difﬁculty and negation\nwords is shown in Table6, 7 and 8, respectively.\nA.4 Incorporating Token Node Prediction\nSince the SST dataset also provides token-level\nsentiment labels, we combine the token node pre-\ndiction with phrase node prediction learning ob-\njective together to model compositional sentiment\nsemantics.\nResults are shown in Table9. We observe that\nthe results drops a bit after additionally incorporat-\ning token-level sentiment information. This may be\nbecause the phrase sentiment is composed but the\ntoken sentiment mainly depends on the meaning\nof the lexicon itself rather than a kind of compo-\nsitional sentiment semantics. The inconsistency\nof the training objectives may result in the perfor-\nmance drop.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8608846068382263
    },
    {
      "name": "Natural language processing",
      "score": 0.6696376800537109
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6426733732223511
    },
    {
      "name": "Negation",
      "score": 0.6360681653022766
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6022093892097473
    },
    {
      "name": "Phrase",
      "score": 0.5740536451339722
    },
    {
      "name": "Parsing",
      "score": 0.49619871377944946
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.45865345001220703
    },
    {
      "name": "Distributional semantics",
      "score": 0.4175180196762085
    },
    {
      "name": "Programming language",
      "score": 0.1631896197795868
    },
    {
      "name": "Semantic similarity",
      "score": 0.15398633480072021
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ],
  "cited_by": 125
}