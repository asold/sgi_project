{
  "title": "TMU Feedback Comment Generation System Using Pretrained Sequence-to-Sequence Language Models",
  "url": "https://openalex.org/W4411630355",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5105266463",
      "name": "Naoya Ueda",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061931124",
      "name": "Mamoru Komachi",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 16th International Natural Language Generation Conference: Generation Challenges, pages 68–73\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n68\nTMU Feedback Comment Generation System Using Pretrained\nSequence-to-Sequence Language Models\nNaoya Ueda and Mamoru Komachi\nTokyo Metropolitan University\nueda-naoya@ed.tmu.ac.jp, komachi@tmu.ac.jp\nAbstract\nIn this paper, we introduce our Tokyo\nMetropolitan University Feedback Comment\nGeneration system submitted to the feedback\ncomment generation task for INLG 2023 Gen-\neration Challenge. In this task, a source sen-\ntence and offset range of preposition uses are\ngiven as the input. Then, a system gener-\nates hints or explanatory notes about prepo-\nsition uses as the output. To tackle this genera-\ntion task, we finetuned pretrained sequence-to-\nsequence language models. The models using\nBART and T5 showed significant improvement\nin BLEU score, demonstrating the effective-\nness of the pretrained sequence-to-sequence\nlanguage models in this task. We found that\nusing part-of-speech tag information as an aux-\niliary input improves the generation quality of\nfeedback comments. Furthermore, we adopt a\nsimple postprocessing method that can enhance\nthe reliability of the generation. As a result, our\nsystem achieved the F1 score of 47.4 points\nin BLEU-based evaluation and 60.9 points in\nmanual evaluation, which ranked second and\nthird on the leaderboard. 1.\n1 Introduction\nThis paper describes our submission to the feed-\nback comment generation task for INLG 2023 Gen-\neration Challenge (Nagata et al., 2021). Feedback\ncomment generation is a task of automatically gen-\nerating hints or explanatory notes about errors for\nthe purpose of helping the language learner im-\nprove their writing skills (Nagata, 2019). In this\ntask, the target of the feedback comment genera-\ntion is limited to preposition uses, such as missing\nprepositions, to-infinitives, and deverbal preposi-\ntions. Table 1 shows the overview of this task.\nIn the previous study (Hanawa et al., 2021),\nPointer Generator Network (See et al., 2017) was\nused as a sequence-to-sequence method and found\n1Our source code is available at https://github.com/\nNOIRUED/T5_FCG.git\nInput\u0013 \u0010\nSource sentence: I can not agree you in this\ncase.\nOffeset Ranges: 9:18\n\u0012 \u0011\nOutput\u0013 \u0010\nSince the <verb> «agree» is an <intransitive\nverb>, a <preposition> needs to precede the\n<object>. Look up the <verb> «agree» in the\ndictionary to find the appropriate <preposi-\ntion>.\n\u0012 \u0011\nFigure 1: Overview of the feedback comment generation\ntask.\nto be effective in a setting with few variations\nof feedback comments such as preposition uses.\nWhile this study shows the effectiveness of non-\npretrained sequence-to-sequence models such as\nPointer Generator Network, no experiments using\npretrained language models have been conducted.\nSince pretrained sequence-to-sequence language\nmodels, such as T5 (Raffel et al., 2020), show\nsignificant performance in the generation task, it\nis conceivable that using pretrained sequence-to-\nsequence language models improves the generation\nquality.\nIn this paper, we examined the performance of\npretrained sequence-to-sequence language models\nin the feedback comment generation task. We em-\nploy BART (Lewis et al., 2020) and T5 (Raffel\net al., 2020) as the pretrained sequence-to-sequence\nlanguage models. Both models have improved\nthe generation quality compared with the non-\npretrained sequence-to-sequence model. Also, we\nconfirmed that using part-of-speech (POS) tags as\nan auxiliary input improves the generation quality\nof feedback comments in the T5 model. Further-\nmore, we adopted a simple postprocessing method\n69\nOur government even restricted\nfor no selling cigarette to all\nunder 18 youngsters . [\\t] 31:34\nGenerate a feedback comment: I\ncan not [BOE]agree you[EOE] in\nthis case . </s> \nPOS_information: PRP VB RB\n[BOE]VB PRP[EOE] IN DT NN .\nT5\nAs the <verb> <<limit>> is a <transitive\nverb>, the <object> does not need to\nbe preceded by a <preposition>.\nIs <<limit>>  \nor its inflections included in the\ninput sentence? \nSubstitute\n<<limit>> with\n<<restrict>>\nAre there any  \nsynonyms within the input\nsentence? \n<NO_COMMENT>\nOutputInput\nAs the <verb> <<restrict>> is a\n<transitive verb>, the <object> does not\nneed to be preceded by a <preposition>.\nFalse\nTrue\nFalse\nTrue\nFigure 2: Overview of our method.\nto enhance the reliability of the generation. By us-\ning this model and methods, we achieved the F1\nscore of 47.4 points in the BLEU-based evaluation\nand 60.9 points in the manual evaluation.\n2 Feedback Comment Generation Task\n2.1 Task Description\nThe task focuses on the feedback comment gener-\nation targeted on preposition uses. As the input,\nthe source sentence and the offset ranges indicating\nwhere to comment is given. From the input, a sys-\ntem is required to generate an appropriate feedback\ncomment or the special token <NO_COMMENT>\nindicating that the system cannot generate any reli-\nable feedback comment.\n2.2 Evaluation\nThe performance of the system is evaluated auto-\nmatically and manually. As an automatic evalua-\ntion, BLEU (Papineni et al., 2002) score is calcu-\nlated between the system output and the reference\nusing SacreBLEU (Post, 2018). A manual eval-\nuation is done by the shared task organizers on\nthe final submission. Both evaluations are mea-\nsured by recall, precision, and F1. System outputs\nwith <NO_COMMENT> are excluded from both the nu-\nmerator and the denominator of precision and the\nnumerator of recall.\n2.3 Official Baseline System\nThe official baseline system is Pointer Generator\nNetwork model (See et al., 2017) implemented\nbased on fairseq (Ott et al., 2019). It is a sequence-\nto-sequence neural network with attention and copy\nmechanisms. We refer to this model as a non-\npretrained sequence-to-sequence model and com-\npare it with pretrained sequence-to-sequence mod-\nels.\n3 Our Method\nWe frame the feedback comment generation task\nas a sequence-to-sequence generation task. We\nfinetuned the pretrained sequence-to-sequence lan-\nguage models with the official distributed datasets.\nSince it is difficult for the models to learn the mean-\ning of the offset ranges, instead of using offset as it\nis, we inserted the special tokens [BOE] and [EOE]\nin the position of offset ranges. Figure 2 shows the\noverview of our proposed method.\n3.1 Auxiliary Input\nAs shown in Figure 2, there are cases that POS\ninformation is needed in the output. However, the\ninput sequence does not contain such information,\nwhich might lead a system to generate a feedback\ncomment with wrong POS information.\n70\nTrain Dev Test\nOfficial Datasets 4,868 170 215\nTable 1: Number of data instances used in the experi-\nment.\nTo address this problem, we used POS tag in-\nformation as an auxiliary input in the T5 model.\nWe used Natural Language Toolkit (NLTK) (Bird\net al., 2009) to obtain POS tags of the source sen-\ntence. Using the obtained POS tags, we concate-\nnated them with the source sentence as follows:\n[Source sentence] <\\s> POS: [POS tags]\nwhere <\\s> is special token in T5. This method\n(we will refer to as POSTAG hereafter) allows the T5\nmodel to learn the POS information of the source\nsentence, which makes better auxiliary inputs.\n3.2 Postprocessing\nIn this task, the quotations from the source sentence\nshould be bracketed using double-angle brackets.\nConversely, if the double-angle bracketed words\nare not present in the source text, the feedback com-\nment is considered unreliable. However, there are\ncases where the T5 model quotes the words that\ndo not exist in the source sentence. To overcome\nthis problem, we adopted a simple postprocessing\nmethod (we will refer to it as EDIT hereafter). In\nthis postprocessing method, if the double-bracketed\nwords do not exist in the source sentence, it finds\nthe 10-best synonyms using FastText (Bojanowski\net al., 2017). If any of the 10-best synonyms are\nincluded in the text, the system replaces the brack-\neted word with the synonym. Conversely, if none\nof the 10-best synonyms are included in the text, it\nchanges the outputs to <NO_COMMENT>.\n4 Experimental Settings\n4.1 Dataset\nIn this paper, we only used the official datasets dis-\ntributed in the shared task. Since there are some\ntypographical errors and orthographic variants in\nthe datasets, we preprocessed the datasets to cor-\nrect typographical errors and unify orthographic\nvariants. The number of data instances is shown in\nTable 1.\n4.2 Model\nIn this study, we employ BART (Lewis et al.,\n2020) and T5 (Raffel et al., 2020) as the pretrained\nSystem BLEU\nPrecision Recall F1\nOfficial Baseline 46.3 46.3 46.3\nBART-base 51.9 51.9 51.9\nBART-large 51.6 51.6 51.6\nT5-base 64.0 64.0 64.0\nT5-large 60.4 60.4 60.4\nTable 2: Experimental results for each system.\nSystem BLEU\nPrecision Recall F1\nT5-base 64.0 64.0 64.0\n+POSTAG 64.7 64.7 64.7\n+EDIT 64.9 64.4 64.6\nTable 3: Experimental results for POSTAG and EDIT set-\ntings.\nsequence-to-sequence language models. We used\nthe Huggingface Transformer (Wolf et al., 2020) to\nimplement the models.\nBART For the BART-based model, we use the\nBART-base 2 and BART-large3. For fine-tuning,\nthe models are optimized using AdamW optimizer\nwith the constant learning rate of 1e-5, the batch\nsize 16, and trained for 20 epochs.\nT5 For the T5-based model, we use the T5-base4\nand T5-large 5. For fine-tuning, the models are op-\ntimized using AdamW optimizer with a constant\nlearning rate of 5e-4, a batch size of 16, and trained\nfor 30 epochs. To specify a task, the prefix “Gener-\nate a feedback comment: ” is added at the begin-\nning of input sequences.\n5 Results\n5.1 Exeperimental Results\nTable 2 shows the experimental results against the\ndevelopment set. Compared with the official base-\nline system, BART and T5 models improved the\nBLEU scores, demonstrating the effectiveness of\nthe pretrained sequence-to-sequence language mod-\nels in this task. In our case, the T5-base model\n2https://huggingface.co/facebook/bart-base\n3https://huggingface.co/facebook/bart-large\n4https://huggingface.co/t5-base\n5https://huggingface.co/t5-large\n71\nSource sentence But smoking in the restaurant will cause both the smokers and\nsurrounding people facing with the those problems more than\npublic places .\nSystem System Output BLEU\nGold A <verb> part representing the cause of <verb> «cause» takes the\nform of a <to-infinitive> rather than the <ing-form>.\n100.00\nT5-base A <verb> part representing the cause of <verb> «cause» takes the\nform of a <to-infinitive> rather than the <base form>.\n89.53\nPOSTAG A <verb> part representing the cause of <verb> «cause» takes the\nform of a <to-infinitive> rather than the <ing-form>.\n100.00\nTable 4: Example of the result in POSTAG setting. The underline indicates the offset ranges.\nSource sentence With the development of society , we , college students , should\ndo more to adjust it .\nSystem System Output BLEU\nGold As the <verb> «adjust» is an <intransitive verb> when used to\nexpress “to adapt to something” , [...]\n100.00\nPOSTAG The <verb> «adapt» does not take an <indirect object> to indicate\nwhat one adjusts to. Use the <verb> «adapt» as an <intransitive\nverb> with a <preposition>. [...]\n37.72\nEDIT The <verb> «adjust» does not take an <indirect object> to indicate\nwhat one adjusts to. Use the <verb> «adjust» as an <intransitive\nverb> with a <preposition>. [...]\n44.97\nTable 5: Example of the result in EDIT setting. The underline indicates the offset ranges.\nperformed best in this task. We expected large-\nsized models to perform better than the base-sized\nmodels, but contrary to our expectations, the base-\nsized models outperformed the large-sized models.\nWe consider this odd finding comes from a lack of\nsufficient parallel data or unreliableness of BLEU\nscores in the feedback comment generation task.\nWe leave for future work a more detailed examina-\ntion of these model differences.\nTable 3 shows the experimental results in the\nPOSTAG and EDIT settings. Compared with the T5-\nbase model, POSTAG setting improved the score by\n0.7 points. The improvements of the BLEU score\nare relatively small because the superficial differ-\nences in the generated outputs were small. Table 4\nshows the example that the model has successfully\nused POS tag information. From the table, we can\nconfirm that POSTAG setting generated feedback\ncomments with correct POS information, but the\nBLEU score only improved by 10.5 points. These\nresults indicate that using POS tag information as\nan auxiliary input does not improve the overall\nBLEU score, but is effective in this task to generate\nreliable feedback comments.\nCompared with the POSTAG setting, EDIT setting\nimproved the precision, but lowered recall and F1\nscore. Although, the EDIT setting does not improve\nthe BLEU score, it actually enhances the reliabil-\nity of the feedback comments. Table 5 shows the\nexample that had successfully edited an unreliable\nfeedback comment into a reliable feedback com-\nment. These results show that our postprocessing\nmethod is effective to enhance the reliability of the\ngeneration.\n5.2 Official Results\nFrom the experimental results, we submitted the\nT5-base with POSTAG and EDIT as our final submis-\nsion to the shared task. As shown in Table 6, our\nsystem obtained a BLEU score of 47.4 and a man-\nual evaluation score of 60.9, which ranked second\nand third on the leaderboard.\n72\nSystem BLEU Manual Evaluation\nPrecision Recall F1 Precision Recall F1\nOfficial Baseline 33.4 33.4 33.4 31.2 31.2 31.2\nOur System 47.7 47.1 47.4 61.3 60.5 60.9\nTable 6: Official results.\n6 Conclusion\nIn this paper, we described our submission to the\nfeedback comment generation task for INLG 2023\nGeneration Challenge. The result of the experi-\nments showed that using pretrained sequence-to-\nsequence language models is effective in the feed-\nback comment generation for preposition uses. Fur-\nthermore, we found that using POS tags as an auxil-\niary input improves the generation quality, and con-\nfirmed that our postprocessing method enhances\nthe quality of the feedback comments by editing un-\nreliable feedback comments into reliable feedback\ncomments. Future work will explore additional\npostprocessing methods that can better identify and\nappropriately edit unreliable feedback comments.\nReferences\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. O’Reilly Media,\nInc.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nKazuaki Hanawa, Ryo Nagata, and Kentaro Inui. 2021.\nExploring methods for generating feedback com-\nments for writing learning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9719–9730, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nRyo Nagata. 2019. Toward a task of feedback comment\ngeneration for writing learning. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3206–3215, Hong Kong,\nChina. Association for Computational Linguistics.\nRyo Nagata, Masato Hagiwara, Kazuaki Hanawa,\nMasato Mita, Artem Chernodub, and Olena Nahorna.\n2021. Shared task on feedback comment generation\nfor language learners. In Proceedings of the 14th\nInternational Conference on Natural Language Gen-\neration, pages 320–324, Aberdeen, Scotland, UK.\nAssociation for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\n73\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.",
  "topic": "Sequence (biology)",
  "concepts": [
    {
      "name": "Sequence (biology)",
      "score": 0.8010295033454895
    },
    {
      "name": "Computer science",
      "score": 0.7171503901481628
    },
    {
      "name": "Natural language processing",
      "score": 0.5446382164955139
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4989662170410156
    },
    {
      "name": "Language model",
      "score": 0.4772196412086487
    },
    {
      "name": "Speech recognition",
      "score": 0.3991991877555847
    },
    {
      "name": "Biology",
      "score": 0.05321204662322998
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "cited_by": 1
}