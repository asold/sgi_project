{
  "title": "Large-scale Evaluation of Notebook Checkpointing with AI Agents",
  "url": "https://openalex.org/W4409721310",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5001927645",
      "name": "Hanxi Fang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5039513625",
      "name": "Supawit Chockchowwat",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5018532037",
      "name": "Hari Sundaram",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5023168280",
      "name": "Yongjoo Park",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3037564822",
    "https://openalex.org/W3012094108",
    "https://openalex.org/W3032086959",
    "https://openalex.org/W2941629559",
    "https://openalex.org/W2796040126",
    "https://openalex.org/W2968484890",
    "https://openalex.org/W4283328246",
    "https://openalex.org/W4366548004",
    "https://openalex.org/W2784241156",
    "https://openalex.org/W2942846919",
    "https://openalex.org/W4224991060",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W2302890201",
    "https://openalex.org/W2609517807"
  ],
  "abstract": "Saving, or checkpointing, intermediate results during interactive data exploration can potentially boost user productivity. However, existing studies on this topic are limited, as they primarily rely on small-scale experiments with human participants - a fundamental constraint of human subject studies. To address this limitation, we employ AI agents to simulate a large number of complex data exploration scenarios, including revisiting past states and branching into new exploration paths. This strategy enables us to accurately assess the impact of checkpointing while closely mimicking the behavior of real-world data practitioners. Our evaluation results, involving more than 1,000 exploration paths and 2,848 executed code blocks, show that a checkpointing framework for computational notebooks can indeed enhance productivity by minimizing unnecessary code re-executions and redundant variables or code.",
  "full_text": "Large-scale Evaluation of Notebook Checkpointing with\nAI Agents\nHanxi Fang\nUniversity of Illinois Urbana-Champaign\nUrbana, Illinois, USA\nhanxif2@illinois.edu\nSupawit Chockchowwat\nUniversity of Illinois Urbana-Champaign\nUrbana, Illinois, USA\nsupawit2@illinois.edu\nHari Sundaram\nUniversity of Illinois Urbana-Champaign\nUrbana, Illinois, USA\nhs1@illinois.edu\nYongjoo Park\nUniversity of Illinois Urbana-Champaign\nUrbana, Illinois, USA\nyongjoo@illinois.edu\nAbstract\nSaving, or checkpointing, intermediate results during interactive\ndata exploration can potentially boost user productivity. However,\nexisting studies on this topic are limited, as they primarily rely on\nsmall-scale experiments with human participants‚Äîa fundamental\nconstraint of human subject studies. To address this limitation, we\nemploy AI agents to simulate a large number of complex data explo-\nration scenarios, including revisiting past states and branching into\nnew exploration paths. This strategy enables us to accurately assess\nthe impact of checkpointing while closely mimicking the behavior\nof real-world data practitioners. Our evaluation results, involving\nmore than 1,000 exploration paths and 2,848 executed code blocks,\nshow that a checkpointing framework for computational notebooks\ncan indeed enhance productivity by minimizing unnecessary code\nre-executions and redundant variables/code.\nCCS Concepts\n‚Ä¢ Human-centered computing ‚ÜíUsability testing; User stud-\nies; Interactive systems and tools ; ‚Ä¢ Information systems ‚Üí\nVersion management.\nKeywords\nAI agent-based evaluation, computational notebooks, version con-\ntrol systems, notebook checkpointing, interactive data science\nACM Reference Format:\nHanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park.\n2025. Large-scale Evaluation of Notebook Checkpointing with AI Agents.\nIn Extended Abstracts of the CHI Conference on Human Factors in Computing\nSystems (CHI EA ‚Äô25), April 26-May 1, 2025, Yokohama, Japan. ACM, New\nYork, NY, USA, 8 pages. https://doi.org/10.1145/3706599.3719928\n1 Introduction\nCheckpointing computational notebooks can improve user produc-\ntivity [5]. Specifically, notebook systems like Jupyter [14], Colab [8],\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ‚Äô25, Yokohama, Japan\n¬© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1395-8/2025/04\nhttps://doi.org/10.1145/3706599.3719928\nR Markdown [3] let data scientists run a code block one at a time\nfor analyzing tabular data, training machine learning models, visu-\nalizing results, etc. By checkpointing intermediate code/data, users\ncan undo undesirable executions, explore alternative hypotheses,\nand restore from crashes. A recent work [5] proposes a checkpoint-\ning interface for computational notebooks and demonstrates its\nproductivity benefits.\nUnfortunately, the existing evaluation is limited to employing a\nhandful of human subjects working on a fixed set of tasks. While\nthose participants are allowed to freely explore, the study remains\nconstrained by the number of participants, the variety of tasks, and\nthe duration of the study. We recognize that these limitations are\ninherent to human subject studies: the limitations are difficult to\novercome without adopting an entirely different approach.\nIn this work, we tackle them by devising an AI agent-based strat-\negy. Our observation is that AI agents can generate high-quality\ncode akin to that produced by real data scientists, and exhibit it-\nerative refinement behaviors that closely resemble the testing, de-\nbugging, and code improvement processes typically employed by\nhuman practitioners. Specifically, we employ a pre-trained AI agent\n(i.e., ChatGPT 4o) to simulate real-world data scientists who explore\ndata with and without a notebook checkpointing tool. For each\nof the scenarios, the only independent variable is the assistance\noffered by the checkpointing tool; however, the actual variables\nresiding in sessions and also elapsed times vary significantly.\nThis ‚Äúlate-breaking work‚Äù, as a sequel to the recent research\npaper [5], expands the current state-of-the-art in two significant\naspects. First, we systematically evaluate the effectiveness of the\nnew code+data space checkpointing framework with hundreds of\ndata exploration scenarios. This work is the first that evaluates data\nscience checkpointing frameworks [20, 21, 24, 30] in such a large\nscale. Second, we reason why our agent-based approach is valid,\nensuring two types of consistencies: consistency of generated code\nand consistency of branching strategy.\n2 Background\nWe overview notebook checkpointing frameworks (Section 2.1) and\ndiscuss the limitations of the existing evaluation (Section 2.2).\n2.1 Notebook Checkpointing\nComputational notebooks (e.g., Jupyter, Colab) are designed around\na linear, cell-by-cell execution model, even though real-world data\narXiv:2504.01377v1  [cs.HC]  2 Apr 2025\nCHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan Hanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park\nHistory Graph\nCommits with messages.\nNavigation Popup\nAllows code+data state changes through\ncheckout and execution rollback.\nCode View (top) & Variable Panel (bottom)\nShows information of a selected commit (in\nthe left history graph) without actually altering\nconnected Jupyter sessions.\nSearch Bar\nIf keywords are entered, only relevant com-\nmits are highlighted.\nFig. 1. Background: Kishuboard (image reproduced with the original author‚Äôs permission). The history graph (purple box) shows\npast commits. The code and variable panes (yellow box) display the information of a selected commit. From any past commit, users\ncan load data only (i.e., execution rollback) or load both code and data (i.e., checkout) using the navigation popup (red box).\n1\nFigure 1: Background about Kishuboard. The history graph (purple box) shows past commits. The code and variable panes\n(yellow box) display the information of a selected commit. From any past commit, users can load data only (i.e., execution\nrollback) or load both code and data (i.e., checkout) using the navigation popup (red box). The image is reproduced with the\noriginal authors‚Äô permission.\nscience exploration is often iterative and non-linear. Practitioners\ncommonly revisit older states, branch the code to try different\nalternatives, and switch among branches. [17, 27] To accommodate\nbranching, they either start a new notebook for each path [ 4] or\nmix branch-specific code cells in a single notebook [ 18, 22, 29].\nBoth approaches are time-consuming (re-running shared cells) and\ncognitively taxing (tracking which cells belong to each branch).\nInspired by code version control systems [23], Kishuboard [5] ad-\ndresses these issues by introducing two-dimensional code+data\ncheckpointing for computational notebooks (see Fig. 1 for its user\ninterface). Unlike one-dimensional version control tools that track\nonly code revisions, Kishuboard also version-controls the evolving\n‚Äúvariable state in the kernel‚Äù or ‚Äúdata state‚Äù produced by each cell‚Äôs\nexecution. Based on that, users can checkout the code and data\nstate of notebook to any previous point and start exploring a new\nbranch from that point. This feature, together with Kishuboard‚Äôs\nUI that visualizes the branched structure of exploration similar to\nGit GUI [7], enables fast, nonlinear data science exploration and\nreduces user‚Äôs cognitive burden of managing multiple exploration\nbranches simultaneously.\n2.2 Existing Evaluation of Notebook\nCheckpointing and its Limitations\nThe existing evaluation is based on human subject studies [ 5].\nSpecifically, to evaluate whether notebook checkpointing im-\nproves data science productivity, a user study was conducted with\n20 student participants randomly split into experimental (with\nKishuboard to checkout states) and control (without Kishuboard)\ngroups. Both groups performed the same notebook-based tasks‚Äî\nbuilding models, branching workflows, reporting metrics, retrieving\nprevious variable states, debugging, and recovering from system\ncrashes. While data practitioners may write their own code, par-\nticipants in this study chose from a set of pre-written code snip-\npets (with the option to modify them), ensuring that notebook\ncheckpointing was the primary independent variable. The study\nmeasured each participant‚Äôs task completion time and gathered\nfeedback through surveys, then compared performance between\nthe two groups.\nEvaluation using traditional user study has several limitations:\n(1) As it relies on a small group of participants, its statistical\npower is limited.\n(2) The short study duration restricts participants‚Äô ability to en-\ngage in extended exploration. Even if they are allowed to\nbranch and iterate, the limited time reduces opportunities to\ntest multiple hypotheses or recover from mistakes.\n(3) The task steps and reference code snippets are provided, lim-\niting diverse exploration.\nThese limitations are inherent to resource-constrained human-\nsubject user study. To address them under the user-study paradigm,\none would need to recruit more participants, extend the study\nduration, and ensure uniform expertise in data science and Python\ncoding‚Äîall of which may involve substantial costs.\n3 AI-Agent-Based Experiment Design\nWe tackle the limitations mentioned in Section 2.2 using AI agents.\nAs AI agents can generate diverse code in a short amount of time,\ncompared to humans, it allows us to evaluate Kishuboard‚Äôs effective-\nness at a much larger scale in terms of code diversity at a relatively\nsmall cost. We first describe our experiment design in this section.\nThen, we examine its scientific validity in Section 4.\nSimulating Branched Data Exploration. We use a large language\nmodel (LLM), i.e., ChatGPT-4o [ 2], to iteratively generate code\ncells based on a generic task specification. We first provide the\ntask: ‚ÄúMy data is in file_name. The columns and their meanings\nare column_names_and_descriptions. I want to build a model to\npredict requirements. ‚Äù then prompt the LLM to outline a step-by-\nstep plan. Afterward, we prompt the agent to generate code cells\nand execute the generated code according to the plan step by step.\nSpecifically, the execution results and code for previous steps are\nfed to the agent when generating the code for next step.\nLarge-scale Evaluation of Notebook Checkpointing with AI Agents CHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan\nimport libraries\ndf = spark_load()\ndf = df.dropna() df.fillna(0.0)\nmodel = train_svm(df) model = train_rbf(df)\n21\n22\n23\n24\n230\n240\n(a) Intended data exploration\n21 22 23 24 Restart 21 22 230 240\n21 22 23 24 230 240\n21 22 23 24 Checkout +2 230 240\nNa√ØveRestart:\nNa√ØveContinue:\nKishuboard:\n(b) Strategies to execute the exploration\nFig. 1. A toy example of user‚Äôs intended data exploration and strategies to execute it. Na√ØveRestart repetitively executes cells21 and\n22. Na√ØveContinue executes new cells (230 and 240 ) without any kernel restart. Na√ØveContinue may lead to branch interferences, for\nexample, N/A values were already dropped by23, making data imputation in230 ineÔøøective. Kishuboard restores checkpointed data\nto explore a new path, thus removing the repetitive work and preventing potential branch interferences.\n1\nFigure 2: A toy example of user‚Äôs intended data exploration and strategies to execute it. Na√ØveRestart repetitively executes\ncells ùëê1 and ùëê2. Na√ØveContinue executes new cells ( ùëê3‚Ä≤ and ùëê4‚Ä≤) without any kernel restart. Na√ØveContinue may lead to branch\ninterferences, for example, N/A values were already dropped by ùëê3, making data imputation in ùëê3‚Ä≤ineffective. Kishuboard restores\ncheckpointed data to explore a new path, thus removing the repetitive work and preventing potential branch interferences.\nData scientists often revisit previous explored phases [9, 11, 18,\n19, 22, 26]. To simulate such a behavior, we randomly select a past\ncell to revisit and ask the LLM to generate alternative code cells\nto continue from the selected cell. Each time creates a new branch\nof exploration. Whenever a runtime error occurs, the LLM can\nretry twice to self-correct according to the error message. If the\nLLM fails to fix the error, we checkout randomly and start a new\nbranch. Appendix A presents concrete examples of our prompts\nand generated code by the LLM.\nWe repeat the generation process 10 times to simulate10 explo-\nration sessions of data scientists exploring data independently\nwhere each session explores 100 branches . The dataset is the\n32 KB Titanic dataset downloaded from Kaggle [15]. Overall, the\nLLM generated 2,848 cells in this study.\nTo test Kishuboard‚Äôs performance with memory-intensive tasks,\nwe also use the Spotify podcast dataset (i.e., 45 MB and 450 MB) [16]\nto simulate 3 exploration sessions with each exploring 3 branches.\nBaselines and Kishuboard. Given an exploration session, we test\nthe following existing baseline strategies and Kishuboard to sup-\nport branched data exploration (see Figure 2 for an example). (1)\nNa√ØveRestart: To explore a new branch, Na√ØveRestart restarts the\nkernel and re-executes common cells to restore the selected past\nstate. This strategy follows some user‚Äôs behaviour of starting a\nnew notebook for a new exploration path [4]. (2) Na√ØveContinue:\nInstead, Na√ØveContinue appends new cells to the current notebook\nand continues execution from the current kernel state without any\nrestart. It mimics a common practice to mix code from different cells\nwithin the same notebook [ 18, 22, 29]. Na√ØveContinue can intro-\nduce both explicit errors (e.g., operating on columns dropped in an\nearlier branch, thus observing execution errors) or implicit errors\n(e.g., unknowingly working on a dataframe updated by an earlier\nbranch). (3) Kishuboard: This approach leverages Kishuboard to\ncheck out the code+data version to the latest common history state\nand continue execution from there.\nMetrics. In addition to performance metrics (i.e., execution times),\nthis experiment is also interested in measuring correctness and\nexploration complexity using the following metrics. (1) End-to-\nend time to explore all branches : This metric measures the\ntime elapsed to execute and checkpoint/checkout code for all the\ngenerated branches. It focuses solely on system efficiency during\nexecution and state management (if any) as the time for AI-agents to\ngenerate code is neglected. (2) Number of branch interferences :\nIn the real world, a branch interference occurs when a user reuses\na modified variable assuming it was unmodified. Unfortunately,\nit may lead to misleading exploration outcomes which are hard\nto detect and debug. To study this type of mistake, we count the\nnumber of times a variable is incorrectly accessed across branches.\nFor example, in Fig. 2, cell ùëê3 from the first branch drops rows\nwith missing values in df, but with naively continue executing, cell\nùëê3‚Ä≤ would ineffectively impute missing-but-already-dropped values\nwhich is a branch interference. We detect such branch interference\nby analyzing variable access and modification in each cell as well\nas manually verifying each detected branch interference. (3) Peak\nnumber of variables : This metric tracks the maximum number of\nvariables in the kernel‚Äôs namespace. A higher number indicates a\ngreater memory burden and increased cognitive overhead for the\nuser to remember and manage the variables. (4) Peak number\nof cells: This metric tracks the maximum number of cells in the\nnotebook at any given time. A larger number of cells indicates a\nmessier notebook cell organization, leading to difficulties working\nwith the notebook and cognitive burden.\n4 Validation of the Experiment Design\nIn this section, we aim to justify the validity of AI-agent-based ex-\nperiment design. Specifically, we discuss the similarity between AI\nagents and human behavior in terms of code generation. We also ex-\nplain why the experiment setting effectively measures the efficiency\ngain of notebook checkpointing from a causal perspective.\nSimilarity to Human Exploration Behavior. Our decision to em-\nploy an AI agent for code generation and exploration is founded on\nrecent research that demonstrates the capability of large language\nmodels (LLMs) to produce code (including data science code) and\niterative refinement patterns that closely resemble those of human\nprogrammers [10, 13, 31, 32]. Concretely, LLMs has been shown to:\n(1) Generate high-quality data science code: LLMs can gen-\nerate high-quality code for data science tasks. For example,\nrecent research shows LLM-based agent can achieve accuracy\nof 94.9% on certain data analysis task benchmarks [13] and\nCHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan Hanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park\n1 2 3 4 5 6 7 8 9 100\n500\n1,000\n1,500\n2,000\n1 28 42 55 64 6 23\n11\n19 86Detected number of branch interferences\nExploration session ID (each session explored 100 branches)\nTotal time to explore (secs)\nNa√ØveRestart Na√ØveContinue (fast-but-Ôøøaky)Kishuboard\nFig. 1. End-to-end execution time for Kishuboard and baseline methods. We generated1000 branches of code using LLM-Agent,\ndivided into 10 exploration sessions with 100 branches each. The sessions are sorted in ascending order by Na√ØveRestart time.\nNa√ØveContinue method is the fastest in terms of execution time, as it only runs newly added cells without checkpoint or checkout\noverhead. However, it is faulty, oÔøøen producing incorrect results that do not trigger explicit errors, which may require significant\ndebugging time. The additional time for the Kishuboard group is due entirely to checkpointing and checkout overhead, with the\nworst-case average overhead being just 2 seconds per branch. The red annotations indicate the number of implicit incorrect results\nfor each session.\n1\nFigure 3: End-to-end execution time for Kishuboard and baseline methods. We generated 1000 branches of code using LLM-\nAgent, divided into 10 exploration sessions with 100 branches each. The sessions are sorted in ascending order by Na√ØveRestart\ntime. Na√ØveContinue method is the fastest in terms of execution time, as it only runs newly added cells without checkpoint\nor checkout overhead. However, it is faulty, often producing incorrect results that do not trigger explicit errors, which may\nrequire significant debugging time. The additional time for the Kishuboard group is due entirely to checkpointing and checkout\noverhead, with the worst-case average overhead being just 2 seconds per branch. The red annotations indicate the number of\nimplicit incorrect results for each session.\nget mean ROC AUC performance of 0.82 on some feature\nengineering benchmarks [12].\n(2) Iteratively refine code: Research in LLM agents [28, 31, 33]\nshows that LLMs can parse feedback from external contexts\nto improve outputs. In our setup, the AI agent generates code\nstep-by-step, receives execution feedback at each stage. Like a\nhuman data scientist, the AI agent can iteratively test, debug,\nand refine their work.\nWhile the AI agent is not subject to the same cognitive constraints\nas a human (e.g., short-term memory limitations or domain biases),\nthese overlaps in behavior patterns provide reasonable assurance\nthat our experimental tasks and the corresponding branching strate-\ngies align with real-world exploration scenarios. Moreover, as many\ndata scientists use AI for coding assistance [1, 6, 25], their code may\nemploy part of AI-agent-generated code.\nCausal Perspective on Measuring Efficiency Gains. A key rationale\nfor our experiment is to measure, in a controlled manner, how much\nefficiency researchers can gain by using a notebook checkpointing\nmechanism (i.e.,Kishuboard). From a causal inference perspective,\nthe central challenge is to isolate the effect of the checkpointing\nstrategy from other factors, such as task difficulty or participants‚Äô\nskill level. Our design addresses this issue in two ways:\n(1) Consistency of Generated Code: By using the same gener-\nated code across different checkpointing strategies, we ensure\nthat our experiment holds task complexity and baseline code\nconstant. Because the generated code and data do not change,\nany observed differences in runtime behavior or outcomes\nmust stem from the checkpointing mechanism itself.\n(2) Consistency of Branching Strategy: We introduce random\nselection to revisit previous cells, mirroring real-world data\nexploration where users backtrack to earlier steps [ 18, 22].\nEach branch emerges from the same generative process but\ndiffers in terms of whether it restarts the kernel, continues\nwith the existing kernel state, or checks out a prior code-data\nversion. This design ensures that all strategies face the same\nexploration tasks, allowing a fair comparison of performance.\n5 Experiment Result and Discussion\nUsing the AI-agent-based methodology described earlier, we\npresent and discuss the experiment results in this section in terms\nof correctness, efficiency, and notebook simplicity. In summary, we\nget the following experiment results:\n(1) Kishuboard speeds up data exploration: up to 36% faster\nin both compute-intensive/light tasks, than naively restart-\ning and re-execution. On average, data exploration with\nKishuboard is 23% faster in compute-intensive tasks and 15%\nfaster in compute-light tasks in terms of execution times.\n(2) Kishuboard prevents incorrect results that often occur when\nusers na√Øvely explore alternative paths by appending cells\nand/or reusing variables.\n(3) Kishuboard maintains a clean notebook with only cells and\nvariables in the current branch, which is significantly more\neffective than keeping track of branched structures with (al-\nmost) redundant code and variables.\n(4) Kishuboard‚Äôs space overhead‚Äîcheckpoint sizes‚Äîis small.\nKishuboard Speeds up Data Exploration. We compare end-to-end\ntimes to explore using different strategies. Figure 3 shows that\nKishuboard enables faster data explorations than Na√ØveRestart does.\nIn Session 10, Kishuboard would save the user about 10 minutes\nof real-time or 36% faster than Na√ØveRestart. As the only excep-\ntion, Kishuboard is slightly slower than Na√ØveRestart in Session 2\nbecause cells before the branching point are all lightweight, mak-\ning recomputation faster than using recovering from checkpoints.\nNa√ØveContinue is expectedly faster than Kishuboard as Na√ØveCon-\ntinue overwrites the existing kernel state by running only the addi-\ntional cells (for a new exploration path); however, Na√ØveContinue\ncauses significant issues such as branch interferences and incorrect\nresults as described below.\nKishuboard Ensures Correctness. Furthermore, Kishuboard guar-\nantees correctness (Figure 3) by recovering exact data states from\ncheckpoints. In contrast, while the alternative method Na√ØveCon-\ntinue is the fastest, it is error-prone. In particular, we detect 355\nbranch interferences which are hard to detect and debug manually.\nFor example, in Session 9, Branch 5 modifies a variable X_test\nLarge-scale Evaluation of Notebook Checkpointing with AI Agents CHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan\n1 2 3 4 5 6 7 8 9 10\n200\n400\n600\n103 85 73 78 91 83 96 77 112 74\n225\n292\n161\n264 267\n168\n285 326\n419\n162\n105 87 75 80 93 85 98 79 114 76\nExploration session ID (each session explored 100 branches)\nPeak number of variables\nNa√ØveRestart Na√ØveContinue Kishuboard (including its hidden metadata variables)\nFig. 1. The peak number of kernel variables for each session during exploration. Smaller numbers of variables may be preferred for\neasier understanding. Na√ØveContinue creates excessive variables as it never clears old branch data, increasing cognitive load to keep\ntrack of variable versions. Kishuboard has slightly more variables than Na√ØveRestart due to additional user-invisible metadata.\n1\nFigure 4: The peak number of kernel variables for each session during exploration. Smaller numbers of variables may be\npreferred for easier understanding. Na√ØveContinue produces excessive variables, increasing cognitive load to keep track of\nvariables across branches. Kishuboard has exactly two more variables than Na√ØveRestart for user-invisible metadata.\nby resampling from the original dataset and re-assigning it to the\nsame variable name. Later on, Branch 7 evaluates its model based\non X_test which leads to a misleading accuracy because the re-\nsampled X_test overlaps with the training data. Unsurprisingly,\nNa√ØveRestart also guarantees correctness but incurs computation\ncosts to re-execute cells common across branches.\nKishuboard Maintains Clean Code and Data States. To assess note-\nbook complexity, we measure the peak number of variables during\nexploration, as shown in Figure 4. Compared to Na√ØveRestart and\nKishuboard, Na√ØveContinue results in significantly more variables,\nbecause Na√ØveContinue mixes data states from multiple exploration\nbranches. We also measure the peak number of cells in the notebook\nduring exploration and find that the Na√ØveContinue sessions have\nat least 24√ómore cells than the other two groups, as it appends\ncode from different branches to the same notebook. Consequently,\nthe user of Na√ØveContinue would experience an increased cognitive\nload to manage code cells across branches and remember variables.\nSimulating Memory-intensive Tasks. Using the AI agent to ex-\nplore both 45 MB and 450 MB Spotify dataset [ 16], our results\npresented in Table 1 show that Kishuboard can checkpoint vari-\nables efficiently without requiring extensive IO overhead or storage\nresources. For the 45 MB dataset, the total checkpoint size was\n56 MB, and for the 450 MB dataset, it was 358 MB‚Äîsmaller than\nthe dataset itself in a CSV format. Furthermore, Kishuboard acceler-\nates data exploration in both memory-light and memory-intensive\nscenarios, demonstrating the scalability of the method.\nWe observed that during experiments with larger datasets,\nKishuboard occasionally fails with ‚ÄúOverflowError: BLOB longer\nthan INT_MAX bytes‚Äù due to a limitation in the existing backend\nimplementation [20]. Because this issue is an implementation flaw\nand not inherent to the technique, we believe that it could be ad-\ndressed by chunking data before writing to storage.\n6 Limitations of Experiment Setup and Result\nWhile our AI-agent-based evaluation provides a large-scale and\nsystematic analysis of notebook checkpointing, several limitations\nmust be acknowledged. These primarily stem from the differences\nin behavior between AI agents and human practitioners.\nAI Agents Do Not Replicate How Humans Iteratively Explore\nBranches. The simulation of 100 randomly generated branches\nwithin a 10-cell notebook may not fully capture real-world data sci-\nence workflows. While AI agents randomly revert to past commits,\nhuman users make such decisions based on logical reasoning or\nexecution results. Furthermore, human users vary in exploration\ndepth, leading to different numbers of executed cells per branch.\nThis discrepancy may influence our findings, particularly in assess-\ning the efficiency of different checkpointing strategies. For example,\nif humans always choose to checkout to a very early stage, then\nthe efficiency gap between Na√ØveRestart and Kishuboard may be\nsmaller.\nUnlike AI Agents, Humans Can Mitigate Variable Overwrites. The\ndrawbacks observed in the Na√ØveContinue group may not be as\nsevere in real-world scenarios. Experienced users often mitigate\nvariable overwrites by explicitly renaming or copying variables\nwhen switching between branches. As a result, the frequency of\nbranch interferences leading to execution errors may be lower than\nwhat our AI-agent simulations suggest. Moreover, while AI agents\nappend all executed cells into a single notebook, real users often\nrefactor their workflow by removing obsolete cells or reorganizing\ntheir notebooks dynamically. However, it is important to note that\nthese manual interventions take time‚Äîtime that Kishuboard aims\nto save by automating refactoring and state management.\nAI Agents Do Not Experience Cognitive Load. In real-world sce-\nnarios, users must actively navigate commit histories and decide\nwhich version to restore using Kishuboard. Similarly, those in the\nNa√ØveContinue and Na√ØveRestart groups would spend additional\ntime figuring out how to organize cells or new notebooks. These\ninteractions, which require human judgment and effort, are not\nfully accounted for in our AI-driven evaluation.\nGiven these factors, while our AI-agent-based approach offers\nvaluable insights into checkpointing strategies at scale, complemen-\ntary user studies are needed to validate its real-world applicability.\n7 Conclusion\nIn this study, we demonstrate the efficacy of using AI agents to eval-\nuate notebook checkpointing systems at scale, addressing limita-\ntions inherent in traditional human-subject studies. Traditional user\nstudies are constrained by limited participant pools, short durations,\nand the substantial costs of ensuring uniform skill validation, which\ncollectively hinder the scale and diversity of such evaluations. In\ncontrast, AI agents offer a scalable alternative by generating diverse\nCHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan Hanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park\nTable 1: End-to-end execution time for exploring three randomly generated branches on small (45 MB) and large (450 MB)\ndatasets. ‚ÄúTime‚Äù denotes the duration from executing the first cell of the first branch to completing the last cell of the final\nbranch. ‚Äú# of Cells‚Äù and ‚Äú# of Variables‚Äù indicate the peak number of cells in the notebook and the peak number of variables in\nthe kernel, respectively. Kishuboard outperforms Na√ØveRestart in efficiency for both dataset sizes, demonstrating the scalability\nof the method. Additionally, Kishuboard requires only 56 MB of storage for checkpoints on the small dataset and 358 MB on\nthe large dataset throughout the exploration. While Na√ØveContinue is faster than Kishuboard, as observed in the previous\nexperiments, Na√ØveContinue often causes hard-to-detect errors (Fig. 3) and creates significantly more variables(Fig. 4) and cells.\nMethod Small Dataset (45 MB) Large Dataset (450 MB)\nTime (secs) # of Cells # of Variables Time (secs) # of Cells # of Variables\nNa√ØveRestart 38 9 63 271 9 63\nNa√ØveContinue 18 14 69 141 14 69\nKishuboard 24 9 65 169 9 65\ncode in a short amount of time, mimicking real-world exploratory\nbehaviors, and producing high-quality code. By simulating diverse\nand complex data exploration scenarios‚Äîencompassing a total of\n1,000 branches‚Äî we assessed the performance of Kishuboard, a\nnovel code+data checkpointing framework, against baselines. The\nresults show that Kishuboard significantly enhances exploration\nefficiency, reduces cognitive burden, and prevents branching errors.\nOur work is the first of its kind to demonstrate the utility of AI\nagents in evaluating notebook checkpointing systems.\nReferences\n[1] [n.d.]. GitHub Copilot. https://github.com/features/copilot. Accessed: 2025-01-23.\n[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[3] Benjamin Baumer and Dana Udwin. 2015. R markdown. Wiley Interdisciplinary\nReviews: Computational Statistics 7, 3 (2015), 167‚Äì177.\n[4] Souti Chattopadhyay, Ishita Prasad, Austin Z Henley, Anita Sarma, and Titus\nBarik. 2020. What‚Äôs wrong with computational notebooks? Pain points, needs,\nand design opportunities. In Proceedings of the 2020 CHI conference on human\nfactors in computing systems . 1‚Äì12.\n[5] Hanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park. 2025.\nEnhancing Computational Notebooks with Code+Data Space Versioning. In\nProceedings of the 2025 CHI Conference on Human Factors in Computing Systems\n(conditionally accepted with minor revisions) .\n[6] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. Incoder: A\ngenerative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999\n(2022).\n[7] Git SCM. 2024. Git GUI Clients. https://git-scm.com/downloads/guis A list of\ngraphical user interface (GUI) clients for Git.\n[8] Google. [n.d.]. Google Colab. https://colab.research.google.com/.\n[9] Phillip Guo. 2013. Data Science Workflow: Overview and Challenges . https:\n//cacm.acm.org/blogcacm/data-science-workflow-overview-and-challenges/\n[10] Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang.\n2024. DS-Agent: Automated Data Science by Empowering Large Language\nModels with Case-Based Reasoning. arXiv preprint arXiv:2402.17453 (2024).\n[11] Fred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. 2020.\nUnderstanding and visualizing data iteration in machine learning. In Proceedings\nof the 2020 CHI conference on human factors in computing systems . 1‚Äì13.\n[12] Noah Hollmann, Samuel M√ºller, and Frank Hutter. 2024. Large language models\nfor automated data science: Introducing caafe for context-aware automated\nfeature engineering. Advances in Neural Information Processing Systems 36 (2024).\n[13] Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang,\nChenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, et al. 2024. Data interpreter:\nAn llm agent for data science. arXiv preprint arXiv:2402.18679 (2024).\n[14] Project Jupyter. 2023. Jupyter Notebook. https://jupyter.org/.\n[15] Kaggle. [n.d.]. Titanic - Machine Learning from Disaster.\nhttps://www.kaggle.com/competitions/titanic.\n[16] Kaggle. [n.d.]. Top Spotify Podcast Episodes (Daily Updated).\nhttps://www.kaggle.com/datasets/daniilmiheev/top-spotify-podcasts-daily-\nupdated.\n[17] Mary Beth Kery, Bonnie E John, Patrick O‚ÄôFlaherty, Amber Horvath, and Brad A\nMyers. 2019. Towards effective foraging by data scientists to find past analysis\nchoices. In Proceedings of the 2019 CHI Conference on Human Factors in Computing\nSystems. 1‚Äì13.\n[18] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A\nMyers. 2018. The story in the notebook: Exploratory data science using a literate\nprogramming tool. In Proceedings of the 2018 CHI conference on human factors in\ncomputing systems . 1‚Äì11.\n[19] David Koop and Jay Patel. 2017. Dataflow notebooks: encoding and tracking\ndependencies of cells. In 9th USENIX Workshop on the Theory and Practice of\nProvenance (TaPP 2017) .\n[20] Zhaoheng Li, Supawit Chockchowwat, Ribhav Sahu, Areet Sheth, and\nYongjoo Park. 2024. Kishu: Time-Traveling for Computational Notebooks.\narXiv:2406.13856 [cs.DB] https://arxiv.org/abs/2406.13856\n[21] Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, and Yongjoo Park.\n2023. ElasticNotebook: Enabling Live Migration for Computational Notebooks.\narXiv preprint arXiv:2309.11083 (2023).\n[22] Jiali Liu, Nadia Boukhelifa, and James R Eagan. 2019. Understanding the role\nof alternatives in data analysis practices. IEEE transactions on visualization and\ncomputer graphics 26, 1 (2019), 66‚Äì76.\n[23] Jon Loeliger and Matthew McCullough. 2012. Version Control with Git: Powerful\ntools and techniques for collaborative software development . O‚ÄôReilly Media, Inc.\n[24] Naga Nithin Manne, Shilvi Satpati, Tanu Malik, Amitabha Bagchi, Ashish\nGehani, and Amitabh Chaudhary. 2022. CHEX: multiversion replay with or-\ndered checkpoints. Proc. VLDB Endow. 15, 6 (feb 2022), 1297‚Äì1310. https:\n//doi.org/10.14778/3514061.3514075\n[25] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474\n(2022).\n[26] Deepthi Raghunandan, Aayushi Roy, Shenzhi Shi, Niklas Elmqvist, and Leilani\nBattle. 2023. Code code evolution: Understanding how people change data science\nnotebooks over time. In Proceedings of the 2023 CHI Conference on Human Factors\nin Computing Systems . 1‚Äì12.\n[27] Adam Rule, Aur√©lien Tabard, and James D Hollan. 2018. Exploration and expla-\nnation in computational notebooks. In Proceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems . 1‚Äì12.\n[28] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning.\nAdvances in Neural Information Processing Systems 36 (2024).\n[29] Krishna Subramanian, Ilya Zubarev, Simon V√∂lker, and Jan Borchers. 2019. Sup-\nporting data workers to perform exploratory programming. InExtended Abstracts\nof the 2019 CHI Conference on Human Factors in Computing Systems . 1‚Äì6.\n[30] April Yi Wang, Will Epperson, Robert A DeLine, and Steven M Drucker. 2022.\nDiff in the loop: Supporting data comparison in exploratory data analysis. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems .\n1‚Äì10.\n[31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824‚Äì24837.\n[32] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A\nsystematic evaluation of large language models of code. In Proceedings of the 6th\nACM SIGPLAN International Symposium on Machine Programming . 1‚Äì10.\n[33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao. 2022. React: Synergizing reasoning and acting in language models.\narXiv preprint arXiv:2210.03629 (2022).\nLarge-scale Evaluation of Notebook Checkpointing with AI Agents CHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan\nA AI-Agent Prompts and Generated Notebooks\nA.1 Prompts\nWe first prompt ChatGPT-4 with the following prompts to generate\nsteps it is planing to take, here‚Äôs a real example:\n\"My dataset is in top_podcasts.csv, it has header. It has the\nfollowing fields:<all fields name in the dataset>. I want to build a\nmodel to track how podcast rankings change over time to identify\nemerging trends.\nIt will be an interactive data science process using notebook.Tell\nme the steps to do this (without code). The steps should include\nsome computation intensive ones such as hyperparameter tuning, model\nselection, neural network training, etc.\nThe step should be listed as:\nThere are XXX steps in total.\n1. XXX\n2. XXX\nNext, we‚Äôll ask ChatGPT-4 to generate code step by step, along\nwith the result of the last step feeded, here‚Äôs an example:\n\"The output for this code cell is: <execution output>. Please\ngenerate the code for next step <step number>.\nWhen the agent‚Äôs output code contains error when executing,\nwe give it 2 chances to self-correct, with the following prompt:\nThe output of the given code is: {output}, please help me debug it\nby generating the correct cell code.\nNote that, when interacting with ChatGPT, we also include the\nhistory messages in the current prompt. When checking out, we\nalso checkout the messages to the right point, so that ChatGPT\nonly has the information about previous steps before the current\nstep.\nA.2 Example Generated Code\nHere‚Äôs a real example of the generated code branch, with step\ngenerated one by one as separate cells:\n# Step 1: Data Loading\nimport pandas as pd\n# Load the dataset\nfile_path = 'titanic.csv'\ntitanic_data = pd.read_csv(file_path)\n# Display the first few rows of the dataset\ntitanic_data.head()\n# Step 2: Data Exploration\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Display basic information about the dataset\ntitanic_data.info()\n# Check for missing values\nmissing_values = titanic_data.isnull().sum()\nprint(\"Missing values in each column:\\n\", missing_values)\n# Visualize the distribution of the 'Survived' variable\nsns.countplot(x='Survived', data=titanic_data)\nplt.title('Survival Counts')\nplt.xlabel('Survived (0 = No, 1 = Yes)')\nplt.ylabel('Count')\nplt.show()\n# Visualize the distribution of 'Age' with respect to 'Survived'\nplt.figure(figsize=(12, 6))\nsns.histplot(data=titanic_data, x='Age', hue='Survived', multiple='\nstack', bins=30)\nplt.title('Age Distribution by Survival')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n# Visualize the relationship between 'Pclass' and 'Survived'\nsns.countplot(x='Pclass', hue='Survived', data=titanic_data)\nplt.title('Survival Counts by Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Count')\nplt.show()\n# Pairplot to visualize relationships between features\nsns.pairplot(titanic_data, hue='Survived', diag_kind='kde')\nplt.show()\n# Step 3: Data Preprocessing\n# Make a copy of the original dataset\ntitanic_data = titanic_data.copy()\n# Handle missing values for 'Age' by filling with the median age\ntitanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=\nTrue)\n# Check again for missing values to confirm they have been addressed\nmissing_values_after = titanic_data.isnull().sum()\nprint(\"Missing values after preprocessing:\\n\", missing_values_after)\n# Encode categorical variables using one-hot encoding for 'Embarked'\nand label encoding for\n'Sex'\ntitanic_data = pd.get_dummies(titanic_data, columns=['Embarked'],\ndrop_first=True)\ntitanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female':\n1}) # Label encoding\n# Drop columns that are not needed for model training\ntitanic_data.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace\n=True)\n# Display the preprocessed dataset\ntitanic_data.head()\n# Step 4: Feature Selection\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n# Define the features and target variable\nX = titanic_data.drop('Survived', axis=1)\ny = titanic_data['Survived']\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size\n=0.2, random_state=42)\n# Train a Random Forest classifier to evaluate feature importance\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train, y_train)\n# Get feature importances\nimportances = rf_model.feature_importances_\n# Create a DataFrame for feature importances\nfeature_importance_df = pd.DataFrame({'Feature': X.columns, '\nImportance': importances})\nfeature_importance_df = feature_importance_df.sort_values(by='\nImportance', ascending=False)\n# Display feature importances\nCHI EA ‚Äô25, April 26-May 1, 2025, Yokohama, Japan Hanxi Fang, Supawit Chockchowwat, Hari Sundaram, and Yongjoo Park\nprint(\"Feature Importances:\\n\", feature_importance_df)\n# Select features that are important (e.g., importance >= 0.05)\nselected_features = feature_importance_df[feature_importance_df['\nImportance'] >= 0.05]\n['Feature'].tolist()\nprint(\"Selected Features:\\n\", selected_features)\n# Create a new dataset with selected features\nX_train_selected = X_train[selected_features]\nX_test_selected = X_test[selected_features]\n# Step 5: Data Splitting\n# As we have already split the data in the previous step,\n# we'll confirm the split and prepare for modeling.\n# Display the shape of the training and testing sets\nprint(\"Training set shape (features):\", X_train_selected.shape)\nprint(\"Testing set shape (features):\", X_test_selected.shape)\nprint(\"Training set shape (target):\", y_train.shape)\nprint(\"Testing set shape (target):\", y_test.shape)\n# Optionally, you may want to reset the indices of the training\n# and testing sets for consistency\nX_train_selected.reset_index(drop=True, inplace=True)\nX_test_selected.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n# Display the first few rows of the training set\nprint(\"First few rows of the training set (features):\\n\",\nX_train_selected.head())\nprint(\"First few rows of the training set (target):\\n\", y_train.head\n())\n# Step 6: Model Selection\n# Import necessary libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report,\nconfusion_matrix\n# Define a list of models to evaluate\nmodels = {\n'Logistic Regression': LogisticRegression(),\n'Decision Tree': DecisionTreeClassifier(random_state=42),\n'Random Forest': RandomForestClassifier(random_state=42),\n'Support Vector Machine': SVC(random_state=42),\n'K-Nearest Neighbors': KNeighborsClassifier()\n}\n# Initialize a dictionary to hold model performance\nmodel_performance = {}\n# Fit each model and evaluate its performance\nfor model_name, model in models.items():\nmodel.fit(X_train_selected, y_train) # Fit the model\ny_pred = model.predict(X_test_selected) # Make predictions\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nmodel_performance[model_name] = accuracy\n# Print classification report and confusion matrix for each model\nprint(f\"Model: {model_name}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Classification Report:\\n\", classification_report(y_test,\ny_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"-\" * 50)\n# Display performance summary for all models\nprint(\"Model Performance Summary:\\n\", model_performance)\n# Step 7: Hyperparameter Tuning\n# Import necessary libraries for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n# Define the model to tune (example: Random Forest)\nmodel_to_tune = RandomForestClassifier(random_state=42)\n# Define the hyperparameters to test\nparam_grid = {\n'n_estimators': [50, 100, 200], # Number of trees in the forest\n'max_depth': [None, 10, 20, 30], # Maximum depth of the tree\n'min_samples_split': [2, 5, 10], # Minimum number of samples\nrequired to\n#split an internal node\n'min_samples_leaf': [1, 2, 4] # Minimum number of samples\nrequired to be at a leaf node\n}\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=model_to_tune, param_grid=\nparam_grid,\nscoring='accuracy', cv=5, verbose=1, n_jobs\n=-1)\n# Fit the model to the training data\ngrid_search.fit(X_train_selected, y_train)\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\nprint(\"Best Parameters from Grid Search:\", best_params)\nprint(\"Best Cross-Validated Accuracy:\", best_score)\n# Optionally, we can also evaluate the best model on the test set\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test_selected)\n# Evaluate the best model on the test set\nbest_accuracy = accuracy_score(y_test, y_pred_best)\nprint(f\"Best Model Test Accuracy: {best_accuracy:.4f}\")\nprint(\"Classification Report of Best Model:\\n\",\nclassification_report(y_test, y_pred_best))\nprint(\"Confusion Matrix of Best Model:\\n\", confusion_matrix(y_test,\ny_pred_best))\n# Step 8: Model Training\n# As we have already identified the best model through\nhyperparameter tuning,\n# we will now re-train this best model on the entire training\ndataset.\n# Fit the best model on the entire training set\nbest_model.fit(X_train_selected, y_train)\n# Make predictions on the test set\ny_pred_final = best_model.predict(X_test_selected)\n# Evaluate the final model's performance on the test set\nfinal_accuracy = accuracy_score(y_test, y_pred_final)\nprint(f\"Final Model Test Accuracy: {final_accuracy:.4f}\")\nprint(\"Final Classification Report:\\n\", classification_report(y_test,\ny_pred_final))\nprint(\"Final Confusion Matrix:\\n\", confusion_matrix(y_test,\ny_pred_final))\n# Optionally, save the model for future use\nimport joblib\njoblib.dump(best_model, 'best_random_forest_model.pkl')\nprint(\"Best model saved as 'best_random_forest_model.pkl'.\")",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7283086180686951
    },
    {
      "name": "Scale (ratio)",
      "score": 0.6069724559783936
    },
    {
      "name": "Embedded system",
      "score": 0.3846067190170288
    },
    {
      "name": "Computer architecture",
      "score": 0.34966957569122314
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ]
}