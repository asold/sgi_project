{
    "title": "Pre-trained models for natural language processing: A survey",
    "url": "https://openalex.org/W3011574394",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2378878701",
            "name": "Qiu Xipeng",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2695815753",
            "name": "Sun, Tianxiang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A3179877050",
            "name": "Xu Yige",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A4225672153",
            "name": "Shao, Yunfan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2098653865",
            "name": "Dai Ning",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2378608106",
            "name": "Huang, Xuanjing",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2120615054",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W2602753196",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2963355447",
        "https://openalex.org/W2963964898",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2740711318",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2163922914",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W6759826913",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W637144538",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6683738474",
        "https://openalex.org/W6629028937",
        "https://openalex.org/W2507974895",
        "https://openalex.org/W2555428947",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2970119519",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2966892770",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2917551568",
        "https://openalex.org/W2887997457",
        "https://openalex.org/W2980360762",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2994811754",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2998653236",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2983772459",
        "https://openalex.org/W2971207485",
        "https://openalex.org/W2970193165",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W6802852670",
        "https://openalex.org/W2971871542",
        "https://openalex.org/W2985537030",
        "https://openalex.org/W6770131634",
        "https://openalex.org/W2994980856",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2969876226",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2949178656",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W6768807518",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2974875810",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3005444338",
        "https://openalex.org/W2108783283",
        "https://openalex.org/W2250382531",
        "https://openalex.org/W2908854766",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W1847618513",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2995856824",
        "https://openalex.org/W6767594909",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2998557616",
        "https://openalex.org/W2970161131",
        "https://openalex.org/W2972119829",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W6767002648",
        "https://openalex.org/W2999854190",
        "https://openalex.org/W2991612931",
        "https://openalex.org/W2158028897",
        "https://openalex.org/W2250807343",
        "https://openalex.org/W2499696929",
        "https://openalex.org/W2558476738",
        "https://openalex.org/W2951561177",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W2997012196",
        "https://openalex.org/W342285082",
        "https://openalex.org/W2251765408",
        "https://openalex.org/W2798762751",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2995015695",
        "https://openalex.org/W6771823989",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W3006320872",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2979977993",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W2996888978",
        "https://openalex.org/W2294370754",
        "https://openalex.org/W3203309275",
        "https://openalex.org/W2982041622",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W2969601108",
        "https://openalex.org/W3015298864",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W6753640285",
        "https://openalex.org/W2970557265",
        "https://openalex.org/W2937297214",
        "https://openalex.org/W2924902521",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2964028111",
        "https://openalex.org/W2994928925",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2997090102",
        "https://openalex.org/W2964914104",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W3006963874",
        "https://openalex.org/W2927746189",
        "https://openalex.org/W2970927596",
        "https://openalex.org/W2793978524",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W3008219293",
        "https://openalex.org/W3007595536",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2964223283",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2975291987",
        "https://openalex.org/W2997759614",
        "https://openalex.org/W2944958965",
        "https://openalex.org/W2952357537",
        "https://openalex.org/W2971292190",
        "https://openalex.org/W3003957883",
        "https://openalex.org/W3005929844",
        "https://openalex.org/W2979155592",
        "https://openalex.org/W2963563735",
        "https://openalex.org/W2963972328",
        "https://openalex.org/W2987972786",
        "https://openalex.org/W2922709902",
        "https://openalex.org/W2986562961",
        "https://openalex.org/W2989276524",
        "https://openalex.org/W2962785754",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W3035050380",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W3011279327",
        "https://openalex.org/W3018458867",
        "https://openalex.org/W3006647218",
        "https://openalex.org/W6776215604",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2766839578",
        "https://openalex.org/W2905266130",
        "https://openalex.org/W2981731882",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W2970727289",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W2963374479",
        "https://openalex.org/W3174784402",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W3004117589",
        "https://openalex.org/W2995923603",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2407776548",
        "https://openalex.org/W3030236966",
        "https://openalex.org/W2967690619",
        "https://openalex.org/W3014568172",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2950726992",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3033737024",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2954278700",
        "https://openalex.org/W3165511581",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2951873722",
        "https://openalex.org/W3034292689",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W2971600926",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2985056549",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W3006881356",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W3112673818",
        "https://openalex.org/W2963153906",
        "https://openalex.org/W2606321545",
        "https://openalex.org/W2995983533",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W2912225506",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W3135934234",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3021191241",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2913946806",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3006381853",
        "https://openalex.org/W2979314664",
        "https://openalex.org/W2925618549",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2948740140",
        "https://openalex.org/W2968880719",
        "https://openalex.org/W3099668342",
        "https://openalex.org/W2969477568",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2097732278",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2965595599",
        "https://openalex.org/W3034715004",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W2995040292",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W2999168658",
        "https://openalex.org/W3038035611",
        "https://openalex.org/W2951244744",
        "https://openalex.org/W3207342693",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W3127787589",
        "https://openalex.org/W2985008383",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W2946676565",
        "https://openalex.org/W3139080614",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2979949198",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2997710335",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2907947679",
        "https://openalex.org/W2951585248",
        "https://openalex.org/W2968289784",
        "https://openalex.org/W2138857742",
        "https://openalex.org/W3010293452",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3152497014",
        "https://openalex.org/W3001393026",
        "https://openalex.org/W3160106041",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W3017003177",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962677625",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2799166683",
        "https://openalex.org/W3011718307",
        "https://openalex.org/W3174461835",
        "https://openalex.org/W3098065087",
        "https://openalex.org/W3152956381",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2981458636",
        "https://openalex.org/W2983915252",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W2923978210",
        "https://openalex.org/W3006057906",
        "https://openalex.org/W3170113752",
        "https://openalex.org/W2152790380",
        "https://openalex.org/W2949547296",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2763990719",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W3032532958",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W3017231514",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W3107826490",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W3026408381",
        "https://openalex.org/W2976132230",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W1879966306",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3171649327"
    ],
    "abstract": null,
    "full_text": ". Invited Review .\nPre-trained Models for Natural Language Processing: A Survey\nXipeng Qiu*, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai & Xuanjing Huang\nSchool of Computer Science, Fudan University, Shanghai 200433, China;\nShanghai Key Laboratory of Intelligent Information Processing, Shanghai200433, China\nRecently, the emergence of pre-trained models (PTMs) * has brought natural language processing (NLP) to a new era. In this\nsurvey, we provide a comprehensive review of PTMs for NLP. We ﬁrst brieﬂy introduce language representation learning and its\nresearch progress. Then we systematically categorize existing PTMs based on a taxonomy from four diﬀerent perspectives. Next,\nwe describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for\nfuture research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP\ntasks.\nDeep Learning, Neural Network, Natural Language Processing, Pre-trained Model, Distributed Representation, Word\nEmbedding, Self-Supervised Learning, Language Modelling\n1 Introduction\nWith the development of deep learning, various neural net-\nworks have been widely used to solve Natural Language Pro-\ncessing (NLP) tasks, such as convolutional neural networks\n(CNNs) [1–3], recurrent neural networks (RNNs) [4, 5], graph-\nbased neural networks (GNNs) [ 6–8] and attention mecha-\nnisms [9, 10]. One of the advantages of these neural models\nis their ability to alleviate the feature engineering problem.\nNon-neural NLP methods usually heavily rely on the discrete\nhandcrafted features, while neural methods usually use low-\ndimensional and dense vectors (aka. distributed representa-\ntion) to implicitly represent the syntactic or semantic features\nof the language. These representations are learned in speciﬁc\nNLP tasks. Therefore, neural methods make it easy for people\nto develop various NLP systems.\nDespite the success of neural models for NLP tasks, the\nperformance improvement may be less signiﬁcant compared\nto the Computer Vision (CV) ﬁeld. The main reason is that\ncurrent datasets for most supervised NLP tasks are rather small\n(except machine translation). Deep neural networks usually\nhave a large number of parameters, which make them overﬁt\non these small training data and do not generalize well in\npractice. Therefore, the early neural models for many NLP\ntasks were relatively shallow and usually consisted of only\n1∼3 neural layers.\nRecently, substantial work has shown that pre-trained mod-\nels (PTMs), on the large corpus can learn universal language\nrepresentations, which are beneﬁcial for downstream NLP\ntasks and can avoid training a new model from scratch. With\nthe development of computational power, the emergence of\nthe deep models (i.e., Transformer [ 10]), and the constant\nenhancement of training skills, the architecture of PTMs has\nbeen advanced from shallow to deep. The ﬁrst-generation\nPTMs aim to learn good word embeddings. Since these mod-\nels themselves are no longer needed by downstream tasks, they\n* Corresponding author (email: xpqiu@fudan.edu.cn)\n*PTMs are also known as pre-trained language models (PLMs). In this survey, we use PTMs for NLP instead of PLMs to avoid confusion with the narrow\nconcept of probabilistic (or statistical) language models.\narXiv:2003.08271v4  [cs.CL]  23 Jun 2021\n2 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nare usually very shallow for computational eﬃciencies, such\nas Skip-Gram [11] and GloVe [12]. Although these pre-trained\nembeddings can capture semantic meanings of words, they are\ncontext-free and fail to capture higher-level concepts in con-\ntext, such as polysemous disambiguation, syntactic structures,\nsemantic roles, anaphora. The second-generation PTMs focus\non learning contextual word embeddings, such as CoVe [13],\nELMo [14], OpenAI GPT [15] and BERT [16]. These learned\nencoders are still needed to represent words in context by\ndownstream tasks. Besides, various pre-training tasks are also\nproposed to learn PTMs for diﬀerent purposes.\nThe contributions of this survey can be summarized as\nfollows:\n1. Comprehensive review. We provide a comprehensive\nreview of PTMs for NLP, including background knowl-\nedge, model architecture, pre-training tasks, various\nextensions, adaption approaches, and applications.\n2. New taxonomy. We propose a taxonomy of PTMs for\nNLP, which categorizes existing PTMs from four dif-\nferent perspectives: 1) representation type, 2) model\narchitecture; 3) type of pre-training task; 4) extensions\nfor speciﬁc types of scenarios.\n3. Abundant resources. We collect abundant resources\non PTMs, including open-source implementations of\nPTMs, visualization tools, corpora, and paper lists.\n4. Future directions. We discuss and analyze the limi-\ntations of existing PTMs. Also, we suggest possible\nfuture research directions.\nThe rest of the survey is organized as follows. Section 2\noutlines the background concepts and commonly used nota-\ntions of PTMs. Section 3 gives a brief overview of PTMs\nand clariﬁes the categorization of PTMs. Section 4 provides\nextensions of PTMs. Section 5 discusses how to transfer the\nknowledge of PTMs to downstream tasks. Section 6 gives the\nrelated resources on PTMs. Section 7 presents a collection of\napplications across various NLP tasks. Section 8 discusses the\ncurrent challenges and suggests future directions. Section 9\nsummarizes the paper.\n2 Background\n2.1 Language Representation Learning\nAs suggested by Bengio et al. [17], a good representation\nshould express general-purpose priors that are not task-speciﬁc\nbut would be likely to be useful for a learning machine to solve\nAI-tasks. When it comes to language, a good representation\nshould capture the implicit linguistic rules and common sense\nknowledge hiding in text data, such as lexical meanings, syn-\ntactic structures, semantic roles, and even pragmatics.\nThe core idea of distributed representation is to describe the\nmeaning of a piece of text by low-dimensional real-valued vec-\ntors. And each dimension of the vector has no corresponding\nsense, while the whole represents a concrete concept. Figure\n1 illustrates the generic neural architecture for NLP. There are\ntwo kinds of word embeddings: non-contextual and contex-\ntual embeddings. The diﬀerence between them is whether the\nembedding for a word dynamically changes according to the\ncontext it appears in.\nex1 ex2 ex3 ex4 ex5 ex6 ex7\nNon-contextual\nEmbeddings\nh1 h2 h3 h4 h5 h6 h7\nContextual\nEmbeddings\nContextual Encoder\nTask-Specifc Model\nFigure 1: Generic Neural Architecture for NLP\nNon-contextual Embeddings The ﬁrst step of represent-\ning language is to map discrete language symbols into a dis-\ntributed embedding space. Formally, for each word (or sub-\nword) x in a vocabularyV, we map it to a vectorex ∈RDe with\na lookup table E ∈RDe×|V|, where De is a hyper-parameter\nindicating the dimension of token embeddings. These em-\nbeddings are trained on task data along with other model\nparameters.\nThere are two main limitations to this kind of embeddings.\nThe ﬁrst issue is that the embeddings are static. The embed-\nding for a word does is always the same regardless of its\ncontext. Therefore, these non-contextual embeddings fail to\nmodel polysemous words. The second issue is the out-of-\nvocabulary problem. To tackle this problem, character-level\nword representations or sub-word representations are widely\nused in many NLP tasks, such as CharCNN [18], FastText [19]\nand Byte-Pair Encoding (BPE) [20].\nContextual Embeddings To address the issue of polyse-\nmous and the context-dependent nature of words, we need\ndistinguish the semantics of words in diﬀerent contexts. Given\na text x1,x2,··· ,xT where each token xt ∈V is a word or\nsub-word, the contextual representation of xt depends on the\nwhole text.\n[h1,h2,··· ,hT ] = fenc(x1,x2,··· ,xT ), (1)\nwhere fenc(·) is neural encoder, which is described in Sec-\ntion 2.2, ht is called contextual embedding or dynamical em-\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 3\nh1 h2 h3 h4 h5\nx1 x2 x3 x4 x5\n(a) Convolutional Model\nh1 h2 h3 h4 h5\nx1 x2 x3 x4 x5 (b) Recurrent Model\nh1 h2 h3 h4 h5\nx1 x2 x3 x4 x5 (c) Fully-Connected Self-Attention Model\nFigure 2: Neural Contextual Encoders\nbedding of token xt because of the contextual information\nincluded in.\n2.2 Neural Contextual Encoders\nMost of the neural contextual encoders can be classiﬁed into\ntwo categories: sequence models and non-sequence models.\nFigure 2 illustrates three representative architectures.\n2.2.1 Sequence Models\nSequence models usually capture local context of a word in\nsequential order.\nConvolutional Models Convolutional models take the em-\nbeddings of words in the input sentence and capture the mean-\ning of a word by aggregating the local information from its\nneighbors by convolution operations [2].\nRecurrent Models Recurrent models capture the contextual\nrepresentations of words with short memory, such as LSTMs\n[21] and GRUs [ 22]. In practice, bi-directional LSTMs or\nGRUs are used to collect information from both sides of a\nword, but its performance is often aﬀected by the long-term\ndependency problem.\n2.2.2 Non-Sequence Models\nNon-sequence models learn the contextual representation with\na pre-deﬁned tree or graph structure between words, such\nas the syntactic structure or semantic relation. Some popu-\nlar non-sequence models include Recursive NN [ 6], TreeL-\nSTM [7, 23], and GCN [24].\nAlthough the linguistic-aware graph structure can provide\nuseful inductive bias, how to build a good graph structure is\nalso a challenging problem. Besides, the structure depends\nheavily on expert knowledge or external NLP tools, such as\nthe dependency parser.\nFully-Connected Self-Attention Model In practice, a\nmore straightforward way is to use a fully-connected graph\nto model the relation of every two words and let the model\nlearn the structure by itself. Usually, the connection weights\nare dynamically computed by the self-attention mechanism,\nwhich implicitly indicates the connection between words. A\nsuccessful instance of fully-connected self-attention model is\nthe Transformer [10, 25], which also needs other supplement\nmodules, such as positional embeddings, layer normalization,\nresidual connections and position-wise feed-forward network\n(FFN) layers.\n2.2.3 Analysis\nSequence models learn the contextual representation of the\nword with locality bias and are hard to capture the long-range\ninteractions between words. Nevertheless, sequence models\nare usually easy to train and get good results for various NLP\ntasks.\nIn contrast, as an instantiated fully-connected self-attention\nmodel, the Transformer can directly model the dependency\nbetween every two words in a sequence, which is more power-\nful and suitable to model long range dependency of language.\nHowever, due to its heavy structure and less model bias, the\nTransformer usually requires a large training corpus and is\neasy to overﬁt on small or modestly-sized datasets [15, 26].\nCurrently, the Transformer has become the mainstream\narchitecture of PTMs due to its powerful capacity.\n2.3 Why Pre-training?\nWith the development of deep learning, the number of model\nparameters has increased rapidly. The much larger dataset is\nneeded to fully train model parameters and prevent overﬁt-\nting. However, building large-scale labeled datasets is a great\nchallenge for most NLP tasks due to the extremely expen-\nsive annotation costs, especially for syntax and semantically\nrelated tasks.\nIn contrast, large-scale unlabeled corpora are relatively easy\nto construct. To leverage the huge unlabeled text data, we can\nﬁrst learn a good representation from them and then use these\nrepresentations for other tasks. Recent studies have demon-\nstrated signiﬁcant performance gains on many NLP tasks with\nthe help of the representation extracted from the PTMs on the\nlarge unannotated corpora.\nThe advantages of pre-training can be summarized as fol-\nlows:\n1. Pre-training on the huge text corpus can learn universal\n4 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nlanguage representations and help with the downstream\ntasks.\n2. Pre-training provides a better model initialization,\nwhich usually leads to a better generalization perfor-\nmance and speeds up convergence on the target task.\n3. Pre-training can be regarded as a kind of regularization\nto avoid overﬁtting on small data [27].\n2.4 A Brief History of PTMs for NLP\nPre-training has always been an eﬀective strategy to learn the\nparameters of deep neural networks, which are then ﬁne-tuned\non downstream tasks. As early as 2006, the breakthrough\nof deep learning came with greedy layer-wise unsupervised\npre-training followed by supervised ﬁne-tuning [28]. In CV , it\nhas been in practice to pre-train models on the huge ImageNet\ncorpus, and then ﬁne-tune further on smaller data for diﬀerent\ntasks. This is much better than a random initialization because\nthe model learns general image features, which can then be\nused in various vision tasks.\nIn NLP, PTMs on large corpus have also been proved to be\nbeneﬁcial for the downstream NLP tasks, from the shallow\nword embedding to deep neural models.\n2.4.1 First-Generation PTMs: Pre-trained Word Embeddings\nRepresenting words as dense vectors has a long history [29].\nThe “modern” word embedding is introduced in pioneer work\nof neural network language model (NNLM) [30]. Collobert\net al. [31] showed that the pre-trained word embedding on the\nunlabelled data could signiﬁcantly improve many NLP tasks.\nTo address the computational complexity, they learned word\nembeddings with pairwise ranking task instead of language\nmodeling. Their work is the ﬁrst attempt to obtain generic\nword embeddings useful for other tasks from unlabeled data.\nMikolov et al. [11] showed that there is no need for deep\nneural networks to build good word embeddings. They pro-\npose two shallow architectures: Continuous Bag-of-Words\n(CBOW) and Skip-Gram (SG) models. Despite their sim-\nplicity, they can still learn high-quality word embeddings to\ncapture the latent syntactic and semantic similarities among\nwords. Word2vec is one of the most popular implementations\nof these models and makes the pre-trained word embeddings\naccessible for di ﬀerent tasks in NLP. Besides, GloVe [ 12]\nis also a widely-used model for obtaining pre-trained word\nembeddings, which are computed by global word-word co-\noccurrence statistics from a large corpus.\nAlthough pre-trained word embeddings have been shown ef-\nfective in NLP tasks, they are context-independent and mostly\ntrained by shallow models. When used on a downstream task,\nthe rest of the whole model still needs to be learned from\nscratch.\nDuring the same time period, many researchers also try to\nlearn embeddings of paragraph, sentence or document, such\nas paragraph vector [ 32], Skip-thought vectors [ 33], Con-\ntext2Vec [34]. Diﬀerent from their modern successors, these\nsentence embedding models try to encode input sentences\ninto a ﬁxed-dimensional vector representation, rather than the\ncontextual representation for each token.\n2.4.2 Second-Generation PTMs: Pre-trained Contextual En-\ncoders\nSince most NLP tasks are beyond word-level, it is natural to\npre-train the neural encoders on sentence-level or higher. The\noutput vectors of neural encoders are also called contextual\nword embeddings since they represent the word semantics\ndepending on its context.\nDai and Le [35] proposed the ﬁrst successful instance of\nPTM for NLP. They initialized LSTMs with a language model\n(LM) or a sequence autoencoder, and found the pre-training\ncan improve the training and generalization of LSTMs in many\ntext classiﬁcation tasks. Liu et al. [5] pre-trained a shared\nLSTM encoder with LM and ﬁne-tuned it under the multi-task\nlearning (MTL) framework. They found the pre-training and\nﬁne-tuning can further improve the performance of MTL for\nseveral text classiﬁcation tasks. Ramachandran et al. [36]\nfound the Seq2Seq models can be signiﬁcantly improved by\nunsupervised pre-training. The weights of both encoder and\ndecoder are initialized with pre-trained weights of two lan-\nguage models and then ﬁne-tuned with labeled data. Besides\npre-training the contextual encoder with LM, McCann et al.\n[13] pre-trained a deep LSTM encoder from an attentional\nsequence-to-sequence model with machine translation (MT).\nThe context vectors (CoVe) output by the pre-trained encoder\ncan improve the performance of a wide variety of common\nNLP tasks.\nSince these precursor PTMs, the modern PTMs are usually\ntrained with larger scale corpora, more powerful or deeper\narchitectures (e.g., Transformer), and new pre-training tasks.\nPeters et al. [14] pre-trained 2-layer LSTM encoder with\na bidirectional language model (BiLM), consisting of a for-\nward LM and a backward LM. The contextual representations\noutput by the pre-trained BiLM, ELMo (Embeddings from\nLanguage Models), are shown to bring large improvements\non a broad range of NLP tasks. Akbik et al. [37] captured\nword meaning with contextual string embeddings pre-trained\nwith character-level LM. However, these two PTMs are usu-\nally used as a feature extractor to produce the contextual\nword embeddings, which are fed into the main model for\ndownstream tasks. Their parameters are ﬁxed, and the rest\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 5\nparameters of the main model are still trained from scratch.\nULMFiT (Universal Language Model Fine-tuning) [ 38] at-\ntempted to ﬁne-tune pre-trained LM for text classiﬁcation\n(TC) and achieved state-of-the-art results on six widely-used\nTC datasets. ULMFiT consists of 3 phases: 1) pre-training\nLM on general-domain data; 2) ﬁne-tuning LM on target data;\n3) ﬁne-tuning on the target task. ULMFiT also investigates\nsome eﬀective ﬁne-tuning strategies, including discrimina-\ntive ﬁne-tuning, slanted triangular learning rates, and gradual\nunfreezing.\nMore recently, the very deep PTMs have shown their pow-\nerful ability in learning universal language representations:\ne.g., OpenAI GPT (Generative Pre-training) [15] and BERT\n(Bidirectional Encoder Representation from Transformer) [16].\nBesides LM, an increasing number of self-supervised tasks\n(see Section 3.1) is proposed to make the PTMs capturing\nmore knowledge form large scale text corpora.\nSince ULMFiT and BERT, ﬁne-tuning has become the\nmainstream approach to adapt PTMs for the downstream tasks.\n3 Overview of PTMs\nThe major diﬀerences between PTMs are the usages of con-\ntextual encoders, pre-training tasks, and purposes. We have\nbrieﬂy introduced the architectures of contextual encoders in\nSection 2.2. In this section, we focus on the description of\npre-training tasks and give a taxonomy of PTMs.\n3.1 Pre-training Tasks\nThe pre-training tasks are crucial for learning the universal\nrepresentation of language. Usually, these pre-training tasks\nshould be challenging and have substantial training data. In\nthis section, we summarize the pre-training tasks into three\ncategories: supervised learning, unsupervised learning, and\nself-supervised learning.\n1. Supervised learning (SL) is to learn a function that maps\nan input to an output based on training data consisting\nof input-output pairs.\n2. Unsupervised learning (UL) is to ﬁnd some intrinsic\nknowledge from unlabeled data, such as clusters, densi-\nties, latent representations.\n3. Self-Supervised learning (SSL) is a blend of supervised\nlearning and unsupervised learning 1). The learning\nparadigm of SSL is entirely the same as supervised\nlearning, but the labels of training data are generated\nautomatically. The key idea of SSL is to predict any part\nof the input from other parts in some form. For example,\nthe masked language model (MLM) is a self-supervised\ntask that attempts to predict the masked words in a\nsentence given the rest words.\nIn CV , many PTMs are trained on large supervised training\nsets like ImageNet. However, in NLP, the datasets of most\nsupervised tasks are not large enough to train a good PTM.\nThe only exception is machine translation (MT). A large-scale\nMT dataset, WMT 2017, consists of more than 7 million sen-\ntence pairs. Besides, MT is one of the most challenging tasks\nin NLP, and an encoder pre-trained on MT can beneﬁt a va-\nriety of downstream NLP tasks. As a successful PTM, CoVe\n[13] is an encoder pre-trained on MT task and improves a\nwide variety of common NLP tasks: sentiment analysis (SST,\nIMDb), question classiﬁcation (TREC), entailment (SNLI),\nand question answering (SQuAD).\nIn this section, we introduce some widely-used pre-training\ntasks in existing PTMs. We can regard these tasks as self-\nsupervised learning. Table 1 also summarizes their loss func-\ntions.\n3.1.1 Language Modeling (LM)\nThe most common unsupervised task in NLP is probabilistic\nlanguage modeling (LM), which is a classic probabilistic den-\nsity estimation problem. Although LM is a general concept,\nin practice, LM often refers in particular to auto-regressive\nLM or unidirectional LM.\nGiven a text sequence x1:T = [x1,x2,··· ,xT ], its joint prob-\nability p(x1:T ) can be decomposed as\np(x1:T ) =\nT∏\nt=1\np(xt|x0:t−1), (2)\nwhere x0 is special token indicating the begin of sequence.\nThe conditional probability p(xt|x0:t−1) can be modeled by\na probability distribution over the vocabulary given linguistic\ncontext x0:t−1. The context x0:t−1 is modeled by neural encoder\nfenc(·), and the conditional probability is\np(xt|x0:t−1) = gLM\n(\nfenc(x0:t−1)\n)\n, (3)\nwhere gLM(·) is prediction layer.\nGiven a huge corpus, we can train the entire network with\nmaximum likelihood estimation (MLE).\nA drawback of unidirectional LM is that the representa-\ntion of each token encodes only the leftward context tokens\nand itself. However, better contextual representations of text\nshould encode contextual information from both directions.\n1) Indeed, it is hard to clearly distinguish the unsupervised learning and self-supervised learning. For clariﬁcation, we refer “unsupervised learning” to the\nlearning without human-annotated supervised labels. The purpose of “self-supervised learning” is to learn the general knowledge from data rather than standard\nunsupervised objectives, such as density estimation.\n6 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nTable 1: Loss Functions of Pre-training Tasks\nTask Loss Function Description\nLM LLM = −\nT∑\nt=1\nlog p(xt|x<t) x<t = x1,x2,··· ,xt−1.\nMLM LMLM = −\n∑\nˆx∈m(x)\nlog p\n(\nˆx|x\\m(x)\n)\nm(x) and x\\m(x) denote the masked words from x and the rest\nwords respectively.\nSeq2Seq MLM LS2SMLM = −\nj∑\nt=i\nlog p\n(\nxt|x\\xi: j ,xi:t−1\n)\nxi: j denotes an masked n-gram span from i to j in x.\nPLM LPLM = −\nT∑\nt=1\nlog p(zt|z<t) z = perm(x) is a permutation of x with random order.\nDAE LDAE = −\nT∑\nt=1\nlog p(xt|ˆx,x<t) ˆx is randomly perturbed text from x.\nDIM LDIM = s(ˆxi: j,xi: j) −log\n∑\n˜xi: j∈N\ns(ˆxi: j,˜xi: j) xi: j denotes an n-gram span from i to j in x, ˆxi: j denotes a\nsentence masked at position i to j, and ˜xi: j denotes a randomly-\nsampled negative n-gram from corpus.\nNSP/SOP LNSP/SOP = −log p(t|x,y) t = 1 if x and y are continuous segments from corpus.\nRTD LRTD = −\nT∑\nt=1\nlog p(yt|ˆx) yt = 1( ˆxt = xt), ˆx is corrupted from x.\n1 x = [x1,x2,··· ,xT ] denotes a sequence.\nAn improved solution is bidirectional LM (BiLM), which con-\nsists of two unidirectional LMs: a forward left-to-right LM\nand a backward right-to-left LM. For BiLM, Baevski et al.\n[39] proposed a two-tower model that the forward tower oper-\nates the left-to-right LM and the backward tower operates the\nright-to-left LM.\n3.1.2 Masked Language Modeling (MLM)\nMasked language modeling (MLM) is ﬁrst proposed by Tay-\nlor [40] in the literature, who referred to this as a Cloze task.\nDevlin et al. [16] adapted this task as a novel pre-training task\nto overcome the drawback of the standard unidirectional LM.\nLoosely speaking, MLM ﬁrst masks out some tokens from the\ninput sentences and then trains the model to predict the masked\ntokens by the rest of the tokens. However, this pre-training\nmethod will create a mismatch between the pre-training phase\nand the ﬁne-tuning phase because the mask token does not\nappear during the ﬁne-tuning phase. Empirically, to deal with\nthis issue, Devlin et al. [16] used a special [MASK] token 80%\nof the time, a random token 10% of the time and the original\ntoken 10% of the time to perform masking.\nSequence-to-Sequence MLM (Seq2Seq MLM) MLM is\nusually solved as classiﬁcation problem. We feed the masked\nsequences to a neural encoder whose output vectors are fur-\nther fed into a softmax classiﬁer to predict the masked token.\nAlternatively, we can use encoder-decoder (aka. sequence-to-\nsequence) architecture for MLM, in which the encoder is fed\na masked sequence, and the decoder sequentially produces\nthe masked tokens in auto-regression fashion. We refer to\nthis kind of MLM as sequence-to-sequence MLM (Seq2Seq\nMLM), which is used in MASS [ 41] and T5 [42]. Seq2Seq\nMLM can beneﬁt the Seq2Seq-style downstream tasks, such\nas question answering, summarization, and machine transla-\ntion.\nEnhanced Masked Language Modeling (E-MLM) Con-\ncurrently, there are multiple research proposing diﬀerent en-\nhanced versions of MLM to further improve on BERT. Instead\nof static masking, RoBERTa [43] improves BERT by dynamic\nmasking.\nUniLM [44, 45] extends the task of mask prediction on\nthree types of language modeling tasks: unidirectional, bidi-\nrectional, and sequence-to-sequence prediction. XLM [ 46]\nperforms MLM on a concatenation of parallel bilingual sen-\ntence pairs, called Translation Language Modeling (TLM).\nSpanBERT [47] replaces MLM with Random Contiguous\nWords Maskingand Span Boundary Objective (SBO) to inte-\ngrate structure information into pre-training, which requires\nthe system to predict masked spans based on span bound-\naries. Besides, StructBERT [ 48] introduces the Span Order\nRecovery task to further incorporate language structures.\nAnother way to enrich MLM is to incorporate external\nknowledge (see Section 4.1).\n3.1.3 Permuted Language Modeling (PLM)\nDespite the wide use of the MLM task in pre-training, Yang\net al. [49] claimed that some special tokens used in the pre-\ntraining of MLM, like [MASK], are absent when the model is\napplied on downstream tasks, leading to a gap between pre-\ntraining and ﬁne-tuning. To overcome this issue, Permuted\nLanguage Modeling (PLM) [ 49] is a pre-training objective\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 7\nto replace MLM. In short, PLM is a language modeling task\non a random permutation of input sequences. A permutation\nis randomly sampled from all possible permutations. Then\nsome of the tokens in the permuted sequence are chosen as\nthe target, and the model is trained to predict these targets,\ndepending on the rest of the tokens and the natural positions of\ntargets. Note that this permutation does not aﬀect the natural\npositions of sequences and only deﬁnes the order of token pre-\ndictions. In practice, only the last few tokens in the permuted\nsequences are predicted, due to the slow convergence. And a\nspecial two-stream self-attention is introduced for target-aware\nrepresentations.\n3.1.4 Denoising Autoencoder (DAE)\nDenoising autoencoder (DAE) takes a partially corrupted input\nand aims to recover the original undistorted input. Speciﬁc to\nlanguage, a sequence-to-sequence model, such as the standard\nTransformer, is used to reconstruct the original text. There are\nseveral ways to corrupt text [50]:\n(1) Token Masking: Randomly sampling tokens from the\ninput and replacing them with [MASK] elements.\n(2) Token Deletion: Randomly deleting tokens from the in-\nput. Diﬀerent from token masking, the model needs to decide\nthe positions of missing inputs.\n(3) Text Inﬁlling: Like SpanBERT, a number of text spans\nare sampled and replaced with a single [MASK] token. Each\nspan length is drawn from a Poisson distribution (λ= 3). The\nmodel needs to predict how many tokens are missing from a\nspan.\n(4) Sentence Permutation: Dividing a document into sen-\ntences based on full stops and shu ﬄing these sentences in\nrandom order.\n(5) Document Rotation: Selecting a token uniformly at\nrandom and rotating the document so that it begins with that\ntoken. The model needs to identify the real start position of\nthe document.\n3.1.5 Contrastive Learning (CTL)\nContrastive learning [51] assumes some observed pairs of text\nthat are more semantically similar than randomly sampled\ntext. A score function s(x,y) for text pair (x,y) is learned to\nminimize the objective function:\nLCTL = Ex,y+,y−\n[\n−log exp (s(x,y+))\nexp (s(x,y+)) + exp (s(x,y−))\n]\n, (4)\nwhere (x,y+) are a similar pair and y−is presumably dissimi-\nlar to x. y+ and y−are typically called positive and negative\nsample. The score function s(x,y) is often computed by a\nlearnable neural encoder in two ways: s(x,y) = f T\nenc(x) fenc(y) or\ns(x,y) = fenc(x ⊕y).\nThe idea behind CTL is “learning by comparison”. Com-\npared to LM, CTL usually has less computational complex-\nity and therefore is desirable alternative training criteria for\nPTMs.\nCollobert et al. [31] proposed pairwise ranking task to dis-\ntinguish real and fake phrases. The model needs to predict\na higher score for a legal phrase than an incorrect phrase\nobtained by replacing its central word with a random word.\nMnih and Kavukcuoglu [52] trained word embeddings e ﬃ-\nciently with Noise-Contrastive Estimation (NCE) [53], which\ntrains a binary classiﬁer to distinguish real and fake samples.\nThe idea of NCE is also used in the well-known word2vec\nembedding [11].\nWe brieﬂy describe some recently proposed CTL tasks in\nthe following paragraphs.\nDeep InfoMax (DIM) Deep InfoMax (DIM) [54] is origi-\nnally proposed for images, which improves the quality of the\nrepresentation by maximizing the mutual information between\nan image representation and local regions of the image.\nKong et al. [55] applied DIM to language representation\nlearning. The global representation of a sequence x is deﬁned\nto be the hidden state of the ﬁrst token (assumed to be a spe-\ncial start of sentence symbol) output by contextual encoder\nfenc(x). The objective of DIM is to assign a higher score for\nfenc(xi: j)T fenc(ˆxi: j) than fenc(˜xi: j)T fenc(ˆxi: j), where xi: j denotes\nan n-gram2) span from i to j in x, ˆxi: j denotes a sentence\nmasked at position i to j, and ˜xi: j denotes a randomly-sampled\nnegative n-gram from corpus.\nReplaced Token Detection (RTD) Replaced Token Detec-\ntion (RTD) is the same as NCE but predicts whether a token\nis replaced given its surrounding context.\nCBOW with negative sampling (CBOW-NS) [11] can be\nviewed as a simple version of RTD, in which the negative\nsamples are randomly sampled from vocabulary with simple\nproposal distribution.\nELECTRA [56] improves RTD by utilizing a generator to\nreplacing some tokens of a sequence. A generator G and a dis-\ncriminator D are trained following a two-stage procedure: (1)\nTrain only the generator with MLM task for n1 steps; (2) Ini-\ntialize the weights of the discriminator with the weights of the\ngenerator. Then train the discriminator with a discriminative\ntask for n2 steps, keeping G frozen. Here the discriminative\ntask indicates justifying whether the input token has been re-\nplaced by G or not. The generator is thrown after pre-training,\nand only the discriminator will be ﬁne-tuned on downstream\ntasks.\n2) n is drawn from a Gaussian distribution N(5,1) clipped at 1 (minimum length) and 10 (maximum length).\n8 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nRTD is also an alternative solution for the mismatch prob-\nlem. The network sees [MASK] during pre-training but not\nwhen being ﬁne-tuned in downstream tasks.\nSimilarly, WKLM [57] replaces words on the entity-level\ninstead of token-level. Concretely, WKLM replaces entity\nmentions with names of other entities of the same type and\ntrain the models to distinguish whether the entity has been\nreplaced.\nNext Sentence Prediction (NSP) Punctuations are the nat-\nural separators of text data. So, it is reasonable to construct\npre-training methods by utilizing them. Next Sentence Predic-\ntion (NSP) [16] is just a great example of this. As its name\nsuggests, NSP trains the model to distinguish whether two\ninput sentences are continuous segments from the training cor-\npus. Speciﬁcally, when choosing the sentences pair for each\npre-training example, 50% of the time, the second sentence\nis the actual next sentence of the ﬁrst one, and 50% of the\ntime, it is a random sentence from the corpus. By doing so, it\nis capable to teach the model to understand the relationship\nbetween two input sentences and thus beneﬁt downstream\ntasks that are sensitive to this information, such as Question\nAnswering and Natural Language Inference.\nHowever, the necessity of the NSP task has been questioned\nby subsequent work [47, 49, 43, 63]. Yang et al. [49] found\nthe impact of the NSP task unreliable, while Joshi et al. [47]\nfound that single-sentence training without the NSP loss is\nsuperior to sentence-pair training with the NSP loss. More-\nover, Liu et al. [43] conducted a further analysis for the NSP\ntask, which shows that when training with blocks of text from\na single document, removing the NSP loss matches or slightly\nimproves performance on downstream tasks.\nSentence Order Prediction (SOP) To better model inter-\nsentence coherence, ALBERT [63] replaces the NSP loss with\na sentence order prediction (SOP) loss. As conjectured in\nLan et al. [63], NSP conﬂates topic prediction and coherence\nprediction in a single task. Thus, the model is allowed to make\npredictions merely rely on the easier task, topic prediction.\nDiﬀerent from NSP, SOP uses two consecutive segments from\nthe same document as positive examples, and the same two\nconsecutive segments but with their order swapped as negative\nexamples. As a result, ALBERT consistently outperforms\nBERT on various downstream tasks.\nStructBERT [48] and BERTje [88] also take SOP as their\nself-supervised learning task.\n3.1.6 Others\nApart from the above tasks, there are many other auxiliary\npre-training tasks designated to incorporate factual knowledge\n(see Section 4.1), improve cross-lingual tasks (see Section 4.2),\nmulti-modal applications (see Section 4.3), or other speciﬁc\ntasks (see Section 4.4).\n3.2 Taxonomy of PTMs\nTo clarify the relations of existing PTMs for NLP, we build the\ntaxonomy of PTMs, which categorizes existing PTMs from\nfour diﬀerent perspectives:\n1. Representation Type: According to the representation\nused for downstream tasks, we can divide PTMs into\nnon-contextual and contextual models.\n2. Architectures: The backbone network used by PTMs,\nincluding LSTM, Transformer encoder, Transformer\ndecoder, and the full Transformer architecture. “Trans-\nformer” means the standard encoder-decoder architec-\nture. “Transformer encoder” and “Transformer decoder”\nmean the encoder and decoder part of the standard\nTransformer architecture, respectively. Their diﬀerence\nis that the decoder part uses masked self-attention with\na triangular matrix to prevent tokens from attending\ntheir future (right) positions.\n3. Pre-Training Task Types: The type of pre-training tasks\nused by PTMs. We have discussed them in Section 3.1.\n4. Extensions: PTMs designed for various scenarios, in-\ncluding knowledge-enriched PTMs, multilingual or\nlanguage-speciﬁc PTMs, multi-model PTMs, domain-\nspeciﬁc PTMs and compressed PTMs. We will particu-\nlarly introduce these extensions in Section 4.\nFigure 3 shows the taxonomy as well as some correspond-\ning representative PTMs. Besides, Table 2 distinguishes some\nrepresentative PTMs in more detail.\n3.3 Model Analysis\nDue to the great success of PTMs, it is important to understand\nwhat kinds of knowledge are captured by them, and how to in-\nduce knowledge from them. There is a wide range of literature\nanalyzing linguistic knowledge and world knowledge stored\nin pre-trained non-contextual and contextual embeddings.\n3.3.1 Non-Contextual Embeddings\nStatic word embeddings are ﬁrst probed for kinds of knowl-\nedge. Mikolov et al. [117] found that word representa-\ntions learned by neural network language models are able\nto capture linguistic regularities in language, and the rela-\ntionship between words can be characterized by a relation-\nspeciﬁc vector o ﬀset. Further analogy experiments [ 11]\ndemonstrated that word vectors produced by skip-gram model\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 9\nPTMs\nContextual?\nNon-Contextual CBOW/Skip-Gram [11],GloVe [12]\nContextual ELMo [14], GPT [15], BERT [16]\nArchitectures\nLSTM LM-LSTM [35], Shared LSTM[5], ELMo [14], CoVe [13]\nTransformer Enc. BERT [16], SpanBERT [47], XLNet [49], RoBERTa [43]\nTransformer Dec. GPT [15], GPT-2 [58],GPT-3 [59]\nTransformer MASS [41], BART [50],T5 [42], XNLG [60], mBART [61]\nPre-Training\nTasks\nSupervised MT CoVe [13]\nUnsupervised/\nSelf-Supervised\nLM ELMo [14], GPT [15], GPT-2 [58], UniLM [44]\nMLM\nBERT [16], SpanBERT [47], RoBERTa [43], XLM-R [62]\nTLM XLM [46]\nSeq2Seq MLM MASS [41], T5 [42]\nPLM XLNet [49]\nDAE BART [50]\nCTL\nRTD CBOW-NS [11], ELECTRA [56]\nNSP BERT [16], UniLM [44]\nSOP ALBERT [63], StructBERT [48]\nTuning\nStrategies\nFine-Tuning Two-stage FT [64–66],Multi-task FT [67], Extra Adaptor [68, 69]\nPrompt-Tuning\nDiscrete PET [70],AutoPrompt [71],LM-BFF [72]\nContinuous W ARP [73],Preﬁx-Tuning [74],P-Tuning [75]\nExtensions\nKnowledge-Enriched ERNIE(THU) [76], KnowBERT [77], K-BERT [78], SentiLR [79], KEPLER [80]\nWKLM [57], CoLAKE [81]\nMultilingual\nXLU mBERT [16], Unicoder [82], XLM [46], XLM-R [62], MultiFit [83]\nXLG MASS [41], mBART [61], XNLG [60]\nLanguage-Speciﬁc ERNIE(Baidu) [84], BERT-wwm-Chinese [85], NEZHA [86], ZEN [87], BERTje [88]\nCamemBERT [89], FlauBERT [90], RobBERT [91]\nMulti-Modal\nImage ViLBERT [92], LXMERT [93], VisualBERT [94], B2T2 [95],VL-BERT [96]\nVideo VideoBERT [97], CBT [98]\nSpeech SpeechBERT [99]\nDomain-Speciﬁc SentiLR [79], BioBERT [100], SciBERT [101], PatentBERT [102]\nModel Compression\nModel Pruning CompressingBERT [103]\nQuantization Q-BERT [104], Q8BERT [105]\nParameter Sharing ALBERT [63]\nDistillation DistilBERT [106], TinyBERT [107], MiniLM [108]\nModule Replacing BERT-of-Theseus [109]\nEarly Exit DeeBERT [110], RightTool [111], FastBERT [112], PABEE [113], Liao et al. [114], Sun et al. [115]\nSentEE/TokEE [116]\nFigure 3: Taxonomy of PTMs with Representative Examples\n10 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nTable 2: List of Representative PTMs\nPTMs Architecture† Input Pre-Training Task Corpus Params GLUE ‡ FT?♯\nELMo [14] LSTM Text BiLM WikiText-103 No\nGPT [15] Transformer Dec. Text LM BookCorpus 117M 72.8 Yes\nGPT-2 [58] Transformer Dec. Text LM WebText 117M ∼1542M No\nBERT [16] Transformer Enc. Text MLM & NSP WikiEn +BookCorpus 110M ∼340M 81.9 ∗ Yes\nInfoWord [55] Transformer Enc. Text DIM +MLM WikiEn +BookCorpus =BERT 81.1 ∗ Yes\nRoBERTa [43] Transformer Enc. Text MLM BookCorpus +CC-\nNews+OpenWebText+ STORIES\n355M 88.5 Yes\nXLNet [49] Two-Stream\nTransformer Enc.\nText PLM WikiEn + BookCorpus+Giga5\n+ClueWeb+Common Crawl\n≈BERT 90.5 § Yes\nELECTRA [56] Transformer Enc. Text RTD +MLM same to XLNet 335M 88.6 Yes\nUniLM [44] Transformer Enc. Text MLM ⋄+ NSP WikiEn +BookCorpus 340M 80.8 Yes\nMASS [41] Transformer Text Seq2Seq MLM *Task-dependent Yes\nBART [50] Transformer Text DAE same to RoBERTa 110% of BERT 88.4 ∗ Yes\nT5 [42] Transformer Text Seq2Seq MLM Colossal Clean Crawled Corpus (C4) 220M ∼11B 89.7 ∗ Yes\nERNIE(THU) [76] Transformer Enc. Text +Entities MLM +NSP+dEA WikiEn + Wikidata 114M 79.6 Yes\nKnowBERT [77] Transformer Enc. Text MLM +NSP+EL WikiEn + WordNet/Wiki 253M ∼523M Yes\nK-BERT [78] Transformer Enc. Text +Triples MLM +NSP WikiZh + WebtextZh + CN-DBpedia +\nHowNet + MedicalKG\n=BERT Yes\nKEPLER [80] Transformer Enc. Text MLM +KE WikiEn + Wikidata/WordNet Yes\nWKLM [57] Transformer Enc. Text MLM +ERD WikiEn + Wikidata =BERT Yes\nCoLAKE [81] Transformer Enc. Text +Triples MLM WikiEn + Wikidata =RoBERTa 86.3 Yes\n†“Transformer Enc.” and “Transformer Dec.” mean the encoder and decoder part of the standard Transformer architecture respectively. Their di ﬀerence is that the\ndecoder part uses masked self-attention with triangular matrix to prevent tokens from attending their future (right) positions. “Transformer” means the standard\nencoder-decoder architecture.\n‡the averaged score on 9 tasks of GLUE benchmark (see Section 7.1).\n∗ without WNLI task.\n§indicates ensemble result.\n♯ means whether is model usually used in ﬁne-tuning fashion.\n⋄The MLM of UniLM is built on three versions of LMs: Unidirectional LM, Bidirectional LM, and Sequence-to-Sequence LM.\ncan capture both syntactic and semantic word relationships,\nsuch as vec(“China”) −vec(“Beijing”) ≈vec(“Japan”) −\nvec(“Tokyo”). Besides, they ﬁnd compositionality property of\nword vectors, for example, vec(“Germany”)+ vec(“capital”)\nis close to vec(“Berlin”). Inspired by these work, Rubin-\nstein et al. [118] found that distributional word representations\nare good at predicting taxonomic properties (e.g., dog is an\nanimal) but fail to learn attributive properties (e.g., swan is\nwhite). Similarly, Gupta et al. [119] showed that word2vec\nembeddings implicitly encode referential attributes of entities.\nThe distributed word vectors, along with a simple supervised\nmodel, can learn to predict numeric and binary attributes of\nentities with a reasonable degree of accuracy.\n3.3.2 Contextual Embeddings\nA large number of studies have probed and induced diﬀerent\ntypes of knowledge in contextual embeddings. In general,\nthere are two types of knowledge: linguistic knowledge and\nworld knowledge.\nLinguistic Knowledge A wide range of probing tasks are\ndesigned to investigate the linguistic knowledge in PTMs. Ten-\nney et al. [120], Liu et al. [121] found that BERT performs\nwell on many syntactic tasks such as part-of-speech tagging\nand constituent labeling. However, BERT is not good enough\nat semantic and ﬁne-grained syntactic tasks, compared with\nsimple syntactic tasks.\nBesides, Tenney et al. [122] analyzed the roles of BERT’s\nlayers in diﬀerent tasks and found that BERT solves tasks in a\nsimilar order to that in NLP pipelines. Furthermore, knowl-\nedge of subject-verb agreement [123] and semantic roles [124]\nare also conﬁrmed to exist in BERT. Besides, Hewitt and Man-\nning [125], Jawahar et al. [126], Kim et al. [127] proposed\nseveral methods to extract dependency trees and constituency\ntrees from BERT, which proved the BERT’s ability to encode\nsyntax structure. Reif et al. [128] explored the geometry of\ninternal representations in BERT and ﬁnd some evidence: 1)\nlinguistic features seem to be represented in separate semantic\nand syntactic subspaces; 2) attention matrices contain gram-\nmatical representations; 3) BERT distinguishes word senses\nat a very ﬁne level.\nWorld Knowledge Besides linguistic knowledge, PTMs\nmay also store world knowledge presented in the training\ndata. A straightforward method of probing world knowledge\nis to query BERT with “ﬁll-in-the-blank” cloze statements, for\nexample, “Dante was born in [MASK]”. Petroni et al. [129]\nconstructed LAMA (Language Model Analysis) task by manu-\nally creating single-token cloze statements (queries) from sev-\neral knowledge sources. Their experiments show that BERT\ncontains world knowledge competitive with traditional infor-\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 11\nmation extraction methods. Since the simplicity of query\ngeneration procedure in LAMA, Jiang et al. [130] argued that\nLAMA just measures a lower bound for what language models\nknow and propose more advanced methods to generate more\neﬃcient queries. Despite the surprising ﬁndings of LAMA, it\nhas also been questioned by subsequent work [131, 132]. Sim-\nilarly, several studies induce relational knowledge [133] and\ncommonsense knowledge [134] from BERT for downstream\ntasks.\n4 Extensions of PTMs\n4.1 Knowledge-Enriched PTMs\nPTMs usually learn universal language representation from\ngeneral-purpose large-scale text corpora but lack domain-\nspeciﬁc knowledge. Incorporating domain knowledge from\nexternal knowledge bases into PTM has been shown to be\neﬀective. The external knowledge ranges from linguistic [135,\n79, 77, 136], semantic [137], commonsense [138], factual [76–\n78, 57, 80], to domain-speciﬁc knowledge [139, 78].\nOn the one hand, external knowledge can be injected dur-\ning pre-training. Early studies [140–143] focused on learning\nknowledge graph embeddings and word embedding jointly.\nSince BERT, some auxiliary pre-training tasks are designed\nto incorporate external knowledge into deep PTMs. LIB-\nERT [135] (linguistically-informed BERT) incorporates lin-\nguistic knowledge via an additional linguistic constraint task.\nKe et al. [79] integrated sentiment polarity of each word to\nextend the MLM to Label-Aware MLM (LA-MLM). As a re-\nsult, their proposed model, SentiLR, achieves state-of-the-art\nperformance on several sentence- and aspect-level sentiment\nclassiﬁcation tasks. Levine et al. [137] proposed SenseBERT,\nwhich is pre-trained to predict not only the masked tokens but\nalso their supersenses in WordNet. ERNIE(THU) [ 76] inte-\ngrates entity embeddings pre-trained on a knowledge graph\nwith corresponding entity mentions in the text to enhance the\ntext representation. Similarly, KnowBERT [77] trains BERT\njointly with an entity linking model to incorporate entity repre-\nsentation in an end-to-end fashion. Wang et al. [80] proposed\nKEPLER, which jointly optimizes knowledge embedding and\nlanguage modeling objectives. These work inject structure\ninformation of knowledge graph via entity embedding. In con-\ntrast, K-BERT [78] explicitly injects related triples extracted\nfrom KG into the sentence to obtain an extended tree-form\ninput for BERT. CoLAKE [81] integrates knowledge context\nand language context into a uniﬁed graph, which is then pre-\ntrained with MLM to obtain contextualized representation for\nboth knowledge and language. Moreover, Xiong et al. [57]\nadopted entity replacement identiﬁcation to encourage the\nmodel to be more aware of factual knowledge. However, most\nof these methods update the parameters of PTMs when inject-\ning knowledge, which may suﬀer from catastrophic forgetting\nwhen injecting multiple kinds of knowledge. To address this,\nK-Adapter [136] injects multiple kinds of knowledge by train-\ning diﬀerent adapters independently for diﬀerent pre-training\ntasks, which allows continual knowledge infusion.\nOn the other hand, one can incorporate external knowledge\ninto pre-trained models without retraining them from scratch.\nAs an example, K-BERT [78] allows injecting factual knowl-\nedge during ﬁne-tuning on downstream tasks. Guan et al.\n[138] employed commonsense knowledge bases, ConceptNet\nand ATOMIC, to enhance GPT-2 for story generation. Yang\net al. [144] proposed a knowledge-text fusion model to acquire\nrelated linguistic and factual knowledge for machine reading\ncomprehension.\nBesides, Logan IV et al. [145] and Hayashi et al. [146] ex-\ntended language model to knowledge graph language model\n(KGLM) and latent relation language model (LRLM) respec-\ntively, both of which allow prediction conditioned on knowl-\nedge graph. These novel KG-conditioned language models\nshow potential for pre-training.\n4.2 Multilingual and Language-Speciﬁc PTMs\n4.2.1 Multilingual PTMs\nLearning multilingual text representations shared across lan-\nguages plays an important role in many cross-lingual NLP\ntasks.\nCross-Lingual Language Understanding (XLU) Most of\nthe early works focus on learning multilingual word em-\nbedding [147–149], which represents text from multiple lan-\nguages in a single semantic space. However, these methods\nusually need (weak) alignment between languages.\nMultilingual BERT3) (mBERT) is pre-trained by MLM with\nthe shared vocabulary and weights on Wikipedia text from the\ntop 104 languages. Each training sample is a monolingual doc-\nument, and there are no cross-lingual objectives speciﬁcally\ndesigned nor any cross-lingual data. Even so, mBERT per-\nforms cross-lingual generalization surprisingly well [150]. K\net al. [151] showed that the lexical overlap between languages\nplays a negligible role in cross-lingual success.\nXLM [ 46] improves mBERT by incorporating a cross-\nlingual task, translation language modeling (TLM), which\nperforms MLM on a concatenation of parallel bilingual sen-\ntence pairs. Unicoder [ 82] further propose three new cross-\nlingual pre-training tasks, including cross-lingual word recov-\nery, cross-lingual paraphrase classiﬁcation and cross-lingual\n3) https://github.com/google-research/bert/blob/master/multilingual.md\n12 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nmasked language model (XMLM).\nXLM-RoBERTa (XLM-R) [62] is a scaled multilingual\nencoder pre-trained on a signiﬁcantly increased amount of\ntraining data, 2.5TB clean CommonCrawl data in 100 diﬀer-\nent languages. The pre-training task of XLM-RoBERTa is\nmonolingual MLM only. XLM-R achieves state-of-the-arts\nresults on multiple cross-lingual benchmarks, including XNLI,\nMLQA, and NER.\nCross-Lingual Language Generation (XLG) Multilin-\ngual generation is a kind of tasks to generate text with diﬀerent\nlanguages from the input language, such as machine transla-\ntion and cross-lingual abstractive summarization.\nDiﬀerent from the PTMs for multilingual classiﬁcation, the\nPTMs for multilingual generation usually needs to pre-train\nboth the encoder and decoder jointly, rather than only focusing\non the encoder.\nMASS [41] pre-trains a Seq2Seq model with monolingual\nSeq2Seq MLM on multiple languages and achieves signiﬁcant\nimprovement for unsupervised NMT. XNLG [60] performs\ntwo-stage pre-training for cross-lingual natural language gen-\neration. The ﬁrst stage pre-trains the encoder with monolin-\ngual MLM and Cross-Lingual MLM (XMLM) tasks. The\nsecond stage pre-trains the decoder by using monolingual\nDAE and Cross-Lingual Auto-Encoding (XAE) tasks while\nkeeping the encoder ﬁxed. Experiments show the beneﬁt of\nXNLG on cross-lingual question generation and cross-lingual\nabstractive summarization. mBART [61], a multilingual exten-\nsion of BART [50], pre-trains the encoder and decoder jointly\nwith Seq2Seq denoising auto-encoder (DAE) task on large-\nscale monolingual corpora across 25 languages. Experiments\ndemonstrate that mBART produces signiﬁcant performance\ngains across a wide variety of machine translation (MT) tasks.\n4.2.2 Language-Speciﬁc PTMs\nAlthough multilingual PTMs perform well on many languages,\nrecent work showed that PTMs trained on a single language\nsigniﬁcantly outperform the multilingual results [89, 90, 152].\nFor Chinese, which does not have explicit word bound-\naries, modeling larger granularity [ 85, 87, 86] and multi-\ngranularity [84, 153] word representations have shown great\nsuccess. Kuratov and Arkhipov [154] used transfer learn-\ning techniques to adapt a multilingual PTM to a monolin-\ngual PTM for Russian language. In addition, some monolin-\ngual PTMs have been released for diﬀerent languages, such\nas CamemBERT [89] and FlauBERT [ 90] for French, Fin-\nBERT [152] for Finnish, BERTje [88] and RobBERT [91] for\nDutch, AraBERT [155] for Arabic language.\n4.3 Multi-Modal PTMs\nObserving the success of PTMs across many NLP tasks, some\nresearch has focused on obtaining a cross-modal version of\nPTMs. A great majority of these models are designed for\na general visual and linguistic feature encoding. And these\nmodels are pre-trained on some huge corpus of cross-modal\ndata, such as videos with spoken words or images with cap-\ntions, incorporating extended pre-training tasks to fully utilize\nthe multi-modal feature. Typically, tasks like visual-based\nMLM, masked visual-feature modeling and visual-linguistic\nmatching are widely used in multi-modal pre-training, such as\nVideoBERT [97], VisualBERT [94], ViLBERT [92].\n4.3.1 Video-Text PTMs\nVideoBERT [97] and CBT [98] are joint video and text mod-\nels. To obtain sequences of visual and linguistic tokens used\nfor pre-training, the videos are pre-processed by CNN-based\nencoders and oﬀ-the-shelf speech recognition techniques, re-\nspectively. And a single Transformer encoder is trained on the\nprocessed data to learn the vision-language representations\nfor downstream tasks like video caption. Furthermore, Uni-\nViLM [156] proposes to bring in generation tasks to further\npre-train the decoder using in downstream tasks.\n4.3.2 Image-Text PTMs\nBesides methods for video-language pre-training, several\nworks introduce PTMs on image-text pairs, aiming to ﬁt down-\nstream tasks like visual question answering(VQA) and vi-\nsual commonsense reasoning(VCR). Several proposed models\nadopt two separate encoders for image and text representation\nindependently, such as ViLBERT [ 92] and LXMERT [ 93].\nWhile other methods like VisualBERT [94], B2T2 [95], VL-\nBERT [96], Unicoder-VL [157] and UNITER [158] propose\nsingle-stream uniﬁed Transformer. Though these model ar-\nchitectures are di ﬀerent, similar pre-training tasks, such as\nMLM and image-text matching, are introduced in these ap-\nproaches. And to better exploit visual elements, images are\nconverted into sequences of regions by applying RoI or bound-\ning box retrieval techniques before encoded by pre-trained\nTransformers.\n4.3.3 Audio-Text PTMs\nMoreover, several methods have explored the chance of PTMs\non audio-text pairs, such as SpeechBERT [99]. This work tries\nto build an end-to-end Speech Question Answering (SQA)\nmodel by encoding audio and text with a single Transformer\nencoder, which is pre-trained with MLM on speech and text\ncorpus and ﬁne-tuned on Question Answering.\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 13\n4.4 Domain-Speciﬁc and Task-Speciﬁc PTMs\nMost publicly available PTMs are trained on general do-\nmain corpora such as Wikipedia, which limits their appli-\ncations to speciﬁc domains or tasks. Recently, some studies\nhave proposed PTMs trained on specialty corpora, such as\nBioBERT [100] for biomedical text, SciBERT [101] for scien-\ntiﬁc text, ClinicalBERT [159, 160] for clinical text.\nIn addition to pre-training a domain-speciﬁc PTM, some\nwork attempts to adapt available pre-trained models to target\napplications, such as biomedical entity normalization [161],\npatent classiﬁcation [102], progress notes classiﬁcation and\nkeyword extraction [162].\nSome task-oriented pre-training tasks were also proposed,\nsuch as sentiment Label-Aware MLM in SentiLR [79] for sen-\ntiment analysis, Gap Sentence Generation (GSG) [163] for\ntext summarization, and Noisy Words Detection for disﬂuency\ndetection [164].\n4.5 Model Compression\nSince PTMs usually consist of at least hundreds of millions\nof parameters, they are diﬃcult to be deployed on the on-line\nservice in real-life applications and on resource-restricted de-\nvices. Model compression [ 165] is a potential approach to\nreduce the model size and increase computation eﬃciency.\nThere are ﬁve ways to compress PTMs [ 166]: (1) model\npruning, which removes less important parameters, (2) weight\nquantization [167], which uses fewer bits to represent the pa-\nrameters, (3) parameter sharing across similar model units,\n(4) knowledge distillation [168], which trains a smaller student\nmodel that learns from intermediate outputs from the original\nmodel and (5) module replacing, which replaces the modules\nof original PTMs with more compact substitutes.\nTable 3 gives a comparison of some representative com-\npressed PTMs.\n4.5.1 Model Pruning\nModel pruning refers to removing part of neural network (e.g.,\nweights, neurons, layers, channels, attention heads), thereby\nachieving the eﬀects of reducing the model size and speeding\nup inference time.\nGordon et al. [103] explored the timing of pruning (e.g.,\npruning during pre-training, after downstream ﬁne-tuning) and\nthe pruning regimes. Michel et al. [174] and V oita et al.[175]\ntried to prune the entire self-attention heads in the transformer\nblock.\n4.5.2 Quantization\nQuantization refers to the compression of higher precision\nparameters to lower precision. Works from Shen et al. [104]\nand Zafrir et al. [105] solely focus on this area. Note that\nquantization often requires compatible hardware.\n4.5.3 Parameter Sharing\nAnother well-known approach to reduce the number of pa-\nrameters is parameter sharing, which is widely used in CNNs,\nRNNs, and Transformer [176]. ALBERT [63] uses cross-layer\nparameter sharing and factorized embedding parameteriza-\ntion to reduce the parameters of PTMs. Although the number\nof parameters is greatly reduced, the training and inference\ntime of ALBERT are even longer than the standard BERT.\nGenerally, parameter sharing does not improve the compu-\ntational eﬃciency at inference phase.\n4.5.4 Knowledge Distillation\nKnowledge distillation (KD) [168] is a compression technique\nin which a small model called student model is trained to re-\nproduce the behaviors of a large model called teacher model.\nHere the teacher model can be an ensemble of many models\nand usually well pre-trained. Di ﬀerent to model compres-\nsion, distillation techniques learn a small student model from\na ﬁxed teacher model through some optimization objectives,\nwhile compression techniques aiming at searching a sparser\narchitecture.\nGenerally, distillation mechanisms can be divided into three\ntypes: (1) distillation from soft target probabilities, (2) dis-\ntillation from other knowledge, and (3) distillation to other\nstructures:\n(1) Distillation from soft target probabilities. Bucilua et al.\n[165] showed that making the student approximate the teacher\nmodel can transfer knowledge from teacher to student. A com-\nmon method is approximating the logits of the teacher model.\nDistilBERT [106] trained the student model with a distillation\nloss over the soft target probabilities of the teacher as:\nLKD-CE =\n∑\ni\nti ·log(si), (5)\nwhere ti and si are the probabilities estimated by the teacher\nmodel and the student, respectively.\nDistillation from soft target probabilities can also be used\nin task-speciﬁc models, such as information retrieval [ 177],\nand sequence labeling [178].\n(2) Distillation from other knowledge . Distillation from\nsoft target probabilities regards the teacher model as a black\nbox and only focus on its outputs. Moreover, decomposing\n14 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nTable 3: Comparison of Compressed PTMs\nMethod Type #Layer Loss Function ∗ Speed Up Params Source PTM GLUE ‡\nBERTBASE [16] Baseline 12 LMLM + LNSP 110M 79.6\nBERTLARGE [16] 24 LMLM + LNSP 340M 81.9\nQ-BERT [104] Quantization 12 HAWQ + GWQ - BERT BASE ≈99% BERT⋄\nQ8BERT [105] 12 DQ + QAT - BERT BASE ≈99% BERT\nALBERT§[63] Param. Sharing 12 LMLM + LSOP ×5.6 ∼0.3 12 ∼235M 89.4 (ensemble)\nDistilBERT [106]\nDistillation\n6 LKD-CE+CosKD+ LMLM ×1.63 66M BERT BASE 77.0 (dev)\nTinyBERT§†[107] 4 MSE embed+MSEattn+ MSEhidn+LKD-CE ×9.4 14.5M BERT BASE 76.5\nBERT-PKD [169] 3 ∼6 LKD-CE+PTKD+ LTask ×3.73 ∼1.64 45.7 ∼67 M BERT BASE 76.0 ∼80.6♯\nPD [170] 6 LKD-CE+LTask+ LMLM ×2.0 67.5M BERT BASE 81.2♯\nMobileBERT§[171] 24 FMT +AT+PKT+ LKD-CE+LMLM ×4.0 25.3M BERT LARGE 79.7\nMiniLM [108] 6 AT +AR ×1.99 66M BERT BASE 81.0♭\nDualTrain§†[172] 12 Dual Projection +LMLM - 1.8 ∼19.2M BERT BASE 75.8 ∼81.9♮\nBERT-of-Theseus [109] Module Replacing 6 LTask ×1.94 66M BERT BASE 78.6\n1 The desing of this table is borrowed from [109, 173].\n‡The averaged score on 8 tasks (without WNLI) of GLUE benchmark (see Section 7.1). Here MNLI-m and MNLI-mm are regarded as two diﬀerent tasks. ‘dev’ indicates the result\nis on dev set. ‘ensemble’ indicates the result is from the ensemble model.\n∗ ‘LMLM ’, ‘LNSP’, and ‘LSOP’ indicate pre-training objective (see Section 3.1 and Table 1).‘LTask’ means task-speciﬁc loss.\n‘HAWQ’, ‘GWQ’, ‘DQ’, and ‘QAT’ indicate Hessian AWare Quantization, Group-wise Quantization, Quantization-Aware Training, and Dynamically Quantized, respectively.\n‘KD’ means knowledge distillation. ‘FMT’, ‘AT’, and ‘PKT’ mean Feature Map Transfer, Attention Transfer, and Progressive Knowledge Transfer, respectively. ‘AR’ means\nSelf-Attention value relation.\n§The dimensionality of the hidden or embedding layers is reduced.\n†Use a smaller vocabulary.\n♭ Generally, the F1 score is usually used as the main metric of the QQP task. But MiniLM reports the accuracy, which is incomparable to other works.\n⋄Result on MNLI and SST-2 only.\n♯ Result on the other tasks except for STS-B and CoLA.\n♮ Result on MRPC, MNLI, and SST-2 only.\nthe teacher model and distilling more knowledge can bring\nimprovement to the student model.\nTinyBERT [107] performs layer-to-layer distillation with\nembedding outputs, hidden states, and self-attention distribu-\ntions. MobileBERT [171] also perform layer-to-layer distil-\nlation with soft target probabilities, hidden states, and self-\nattention distributions. MiniLM [ 108] distill self-attention\ndistributions and self-attention value relation from teacher\nmodel.\nBesides, other models distill knowledge through many ap-\nproaches. Sun et al. [169] introduced a “ patient” teacher-\nstudent mechanism, Liu et al. [179] exploited KD to improve\na pre-trained multi-task deep neural network.\n(3) Distillation to other structures. Generally, the structure\nof the student model is the same as the teacher model, except\nfor a smaller layer size and a smaller hidden size. However,\nnot only decreasing parameters but also simplifying model\nstructures from Transformer to RNN [180] or CNN [181] can\nreduce the computational complexity.\n4.5.5 Module Replacing\nModule replacing is an interesting and simple way to reduce\nthe model size, which replaces the large modules of original\nPTMs with more compact substitutes. Xu et al. [109] pro-\nposed Theseus Compression motivated by a famous thought\nexperiment called “Ship of Theseus”, which progressively\nsubstitutes modules from the source model with modules of\nfewer parameters. Diﬀerent from KD, Theseus Compression\nonly requires one task-speciﬁc loss function. The compressed\nmodel, BERT-of-Theseus, is 1.94×faster while retaining more\nthan 98% performance of the source model.\n4.5.6 Early Exit\nAnother eﬃcient way to reduce the inference time is early exit,\nwhich allows the model to exit early at an oﬀ-ramp instead of\npassing through the entire model. The number of layers to be\nexecuted is conditioned on the input.\nThe idea of early exit is ﬁrst applied in computer vision,\nsuch as BranchyNet [182] and Shallow-Deep Network [183].\nWith the emergence of deep pre-trained language models,\nearly exit is recently adopted to speedup Transformer-based\nmodels. As a prior work, Universal Transformer [ 176] uses\nthe Adaptive Computation Time (ACT) mechanism [184] to\nachieve input-adaptive computation. Elbayad et al. [185] pro-\nposed Depth-adaptive transformer for machine translation,\nwhich learns to predict how many decoding layers are re-\nquired for a particular sequence or token. Instead of learning\nhow much computation is required, Liu et al. [186] proposed\ntwo estimation approaches based on Mutual Information (MI)\nand Reconstruction Loss respectively to directly allocate the\nappropriate computation to each sample.\nMore recently, DeeBERT [ 110], RightTool [ 111], Fast-\nBERT [112], ELBERT [ 187], PABEE [ 113] are proposed\nto reduce the computation of transformer encoder for natural\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 15\nlanguage understanding tasks. Their methods usually contain\ntwo steps: (a) Training the injected o ﬀ-ramps (aka internal\nclassiﬁers), and (b) Designing an exiting strategy to decide\nwhether or not to exit.\nTypically, the training objective is a weighted sum of the\ncross-entropy losses at all oﬀ-ramps, i.e.\nLearly-exit =\nM∑\ni=1\nwi ·Li, (6)\nwhere M is the number of oﬀ-ramps. FastBERT [112] adopted\nthe self-distillation loss that trains each oﬀ-ramp with the soft\ntarget generated by the ﬁnal classiﬁer. Liao et al. [114] im-\nproved the objective by considering both the past and the\nfuture information. In particular, the o ﬀ-ramps are trained\nto aggregate the hidden states of the past layers, and also ap-\nproximate the hidden states of the future layers. Moreover,\nSun et al. [115] developed a novel training objective from\nthe perspective of ensemble learning and mutual information,\nby which the o ﬀ-ramps are trained as an ensemble. Their\nproposed objective not only optimizes the accuracy of each\noﬀ-ramp but also the diversity of the oﬀ-ramps.\nDuring inference, an exiting strategy is required to decide\nwhether to exit early or continue to the next layer. Dee-\nBERT [110], FastBERT [ 112], Liao et al. [114] adopt the\nentropy of the prediction distribution as the exiting criterion.\nSimilarly, RightTool [111] use the maximum softmax score to\ndecide whether to exit. PABEE developed a patience-based\nstrategy that allows a sample to exit when the prediction is\nunchanged for successive layers. Further, Sun et al. [115]\nadopt a voting-based strategy to let all of the past oﬀ-ramps\ntake a vote to decide whether or not to exit. Besides, Li et al.\n[116] proposed a window-based uncertainty as the exiting cri-\nterion to achieve token-level early exit (TokEE) for sequence\nlabeling tasks.\n5 Adapting PTMs to Downstream Tasks\nAlthough PTMs capture the general language knowledge from\na large corpus, how eﬀectively adapting their knowledge to\nthe downstream task is still a key problem.\n5.1 Transfer Learning\nTransfer learning [ 188] is to adapt the knowledge from a\nsource task (or domain) to a target task (or domain). Fig-\nure 4 gives an illustration of transfer learning.\nThere are many types of transfer learning in NLP, such as\ndomain adaptation, cross-lingual learning, multi-task learning.\nAdapting PTMs to downstream tasks is sequential transfer\nlearning task, in which tasks are learned sequentially and the\ntarget task has labeled data.\nSource Dataset Target Dataset\nSource Model Target Model\nKnowledge\nTransfer\nFigure 4: Transfer Learning\n5.2 How to Transfer?\nTo transfer the knowledge of a PTM to the downstream NLP\ntasks, we need to consider the following issues:\n5.2.1 Choosing appropriate pre-training task, model archi-\ntecture and corpus\nDiﬀerent PTMs usually have di ﬀerent eﬀects on the same\ndownstream task, since these PTMs are trained with various\npre-training tasks, model architecture, and corpora.\n(1) Currently, the language model is the most popular pre-\ntraining task and can more eﬃciently solve a wide range of\nNLP problems [ 58]. However, di ﬀerent pre-training tasks\nhave their own bias and give di ﬀerent eﬀects for di ﬀerent\ntasks. For example, the NSP task [16] makes PTM understand\nthe relationship between two sentences. Thus, the PTM can\nbeneﬁt downstream tasks such as Question Answering (QA)\nand Natural Language Inference (NLI).\n(2) The architecture of PTM is also important for the down-\nstream task. For example, although BERT helps with most\nnatural language understanding tasks, it is hard to generate\nlanguage.\n(3) The data distribution of the downstream task should be\napproximate to PTMs. Currently, there are a large number of\noﬀ-the-shelf PTMs, which can just as conveniently be used\nfor various domain-speciﬁc or language-speciﬁc downstream\ntasks.\nTherefore, given a target task, it is always a good solution\nto choose the PTMs trained with appropriate pre-training task,\narchitecture, and corpus.\n5.2.2 Choosing appropriate layers\nGiven a pre-trained deep model, diﬀerent layers should cap-\nture diﬀerent kinds of information, such as POS tagging, pars-\ning, long-term dependencies, semantic roles, coreference. For\nRNN-based models, Belinkov et al. [189] and Melamud et al.\n[34] showed that representations learned from diﬀerent layers\nin a multi-layer LSTM encoder beneﬁt di ﬀerent tasks (e.g.,\npredicting POS tags and understanding word sense). For\ntransformer-based PTMs, Tenney et al. [122] found BERT\nrepresents the steps of the traditional NLP pipeline: basic\n16 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nsyntactic information appears earlier in the network, while\nhigh-level semantic information appears at higher layers.\nLet H(l)(1 ⩽ l ⩽ L) denotes the l-th layer representation\nof the pre-trained model with L layers, and g(·) denote the\ntask-speciﬁc model for the target task.\nThere are three ways to select the representation:\na) Embedding Only. One approach is to choose only the\npre-trained static embeddings, while the rest of the model still\nneeds to be trained from scratch for a new target task.\nThey fail to capture higher-level information that might\nbe even more useful. Word embeddings are only useful in\ncapturing semantic meanings of words, but we also need to\nunderstand higher-level concepts like word sense.\nb) Top Layer.The most simple and eﬀective way is to feed\nthe representation at the top layer into the task-speciﬁc model\ng(H(L)).\nc) All Layers. A more ﬂexible way is to automatic choose\nthe best layer in a soft version, like ELMo [14]:\nrt = γ\nL∑\nl=1\nαlh(l)\nt , (7)\nwhere αl is the softmax-normalized weight for layer l and γis\na scalar to scale the vectors output by pre-trained model. The\nmixup representation is fed into the task-speciﬁc model g(rt).\n5.2.3 To tune or not to tune?\nCurrently, there are two common ways of model transfer: fea-\nture extraction (where the pre-trained parameters are frozen),\nand ﬁne-tuning (where the pre-trained parameters are unfrozen\nand ﬁne-tuned).\nIn feature extraction way, the pre-trained models are re-\ngarded as oﬀ-the-shelf feature extractors. Moreover, it is im-\nportant to expose the internal layers as they typically encode\nthe most transferable representations [190].\nAlthough both these two ways can signiﬁcantly beneﬁt\nmost of NLP tasks, feature extraction way requires more com-\nplex task-speciﬁc architecture. Therefore, the ﬁne-tuning way\nis usually more general and convenient for many di ﬀerent\ndownstream tasks than feature extraction way.\nTable 4 gives some common combinations of adapting\nPTMs.\nTable 4: Some common combinations of adapting PTMs.\nWhere FT /FE?† PTMs\nEmbedding Only FT /FE Word2vec [11], GloVe [12]\nTop Layer FT BERT [16], RoBERTa [43]\nTop Layer FE BERT §[191, 192]\nAll Layers FE ELMo [14]\n†FT and FE mean Fine-tuning and Feature Extraction respectively.\n§BERT used as feature extractor.\n5.3 Fine-Tuning Strategies\nWith the increase of the depth of PTMs, the representation cap-\ntured by them makes the downstream task easier. Therefore,\nthe task-speciﬁc layer of the whole model is simple. Since\nULMFit and BERT, ﬁne-tuning has become the main adaption\nmethod of PTMs. However, the process of ﬁne-tuning is often\nbrittle: even with the same hyper-parameter values, distinct\nrandom seeds can lead to substantially diﬀerent results [193].\nBesides standard ﬁne-tuning, there are also some useful\nﬁne-tuning strategies.\nTwo-stage ﬁne-tuning An alternative solution is two-stage\ntransfer, which introduces an intermediate stage between pre-\ntraining and ﬁne-tuning. In the ﬁrst stage, the PTM is trans-\nferred into a model ﬁne-tuned by an intermediate task or cor-\npus. In the second stage, the transferred model is ﬁne-tuned\nto the target task. Sun et al. [64] showed that the “further pre-\ntraining” on the related-domain corpus can further improve\nthe ability of BERT and achieved state-of-the-art performance\non eight widely-studied text classiﬁcation datasets. Phang\net al. [194] and Garg et al. [195] introduced the intermedi-\nate supervised task related to the target task, which brings a\nlarge improvement for BERT, GPT, and ELMo. Li et al.[65]\nalso used a two-stage transfer for the story ending prediction.\nThe proposed TransBERT (transferable BERT) can transfer\nnot only general language knowledge from large-scale unla-\nbeled data but also speciﬁc kinds of knowledge from various\nsemantically related supervised tasks.\nMulti-task ﬁne-tuning Liu et al. [67] ﬁne-tuned BERT un-\nder the multi-task learning framework, which demonstrates\nthat multi-task learning and pre-training are complementary\ntechnologies.\nFine-tuning with extra adaptation modules The main\ndrawback of ﬁne-tuning is its parameter ine ﬃciency: every\ndownstream task has its own ﬁne-tuned parameters. There-\nfore, a better solution is to inject some ﬁne-tunable adaptation\nmodules into PTMs while the original parameters are ﬁxed.\nStickland and Murray [68] equipped a single share BERT\nmodel with small additional task-speciﬁc adaptation modules,\nprojected attention layers (PALs). The shared BERT with\nthe PALs matches separately ﬁne-tuned models on the GLUE\nbenchmark with roughly 7 times fewer parameters. Similarly,\nHoulsby et al. [69] modiﬁed the architecture of pre-trained\nBERT by adding adapter modules. Adapter modules yield a\ncompact and extensible model; they add only a few trainable\nparameters per task, and new tasks can be added without re-\nvisiting previous ones. The parameters of the original network\nremain ﬁxed, yielding a high degree of parameter sharing.\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 17\nOthers Motivated by the success of widely-used ensemble\nmodels, Xu et al.[196] improved the ﬁne-tuning of BERT with\ntwo eﬀective mechanisms: self-ensemble and self-distillation,\nwhich can improve the performance of BERT on downstream\ntasks without leveraging external resource or signiﬁcantly de-\ncreasing the training eﬃciency. They integrated ensemble and\ndistillation within a single training process. The teacher model\nis an ensemble model by parameter-averaging several student\nmodels in previous time steps.\nInstead of ﬁne-tuning all the layers simultaneously, grad-\nual unfreezing [38] is also an e ﬀective method that gradu-\nally unfreezes layers of PTMs starting from the top layer.\nChronopoulou et al. [197] proposed a simpler unfreezing\nmethod, sequential unfreezing, which ﬁrst ﬁne-tunes only the\nrandomly-initialized task-speciﬁc layers, and then unfreezes\nthe hidden layers of PTM, and ﬁnally unfreezes the embedding\nlayer.\nLi and Eisner [198] compressed ELMo embeddings us-\ning variational information bottleneck while keeping only the\ninformation that helps the target task.\nGenerally, the above works show that the utility of PTMs\ncan be further stimulated by better ﬁne-tuning strategies.\n5.3.1 Prompt-based Tuning\nNarrowing the gap between pre-training and ﬁne-tuning can\nfurther boost the performance of PTMs on downstream tasks.\nAn alternative approach is reformulating the downstream\ntasks into a MLM task by designing appropriate prompts.\nPrompt-based methods have shown great power in few-shot\nsetting [199, 200, 70, 72], zero-shot setting [ 129, 201], and\neven fully-supervised setting [74, 75]. Current prompt-based\nmethods can be categorized as two branches according to the\nprompt is whether discrete or continuous.\nDiscrete prompts Discrete prompt is a sequence of words\nto be inserted into the input text, which helps the PTM to bet-\nter model the downstream task. Sun et al. [202] constructed\nan auxiliary sentence by transforming aspect-based sentiment\nanalysis (ABSA) task to a sentence pair classiﬁcation task, but\nits model parameters still need to be ﬁne-tuned. GPT-3 [59]\nproposed the in-context learning that concatenates the original\ninput with the task description and a few examples. By this,\nGPT-3 can achieve competitive performance without tuning\nthe parameters. Besides, Petroni et al. [129] found that with\nproper manual prompt, BERT can perform well on entity pre-\ndiction task (LAMA) without training. In addition to LAMA,\nSchick and Sch ¨utze [200, 70] proposed PET that designed\ndiscrete prompts for various text classiﬁcation and entailment\ntasks. However, the manually designed prompts can be sub-\noptimal, as a result, many methods are developed to automate\nthe generation of prompts. LPAQA [201] uses two methods,\ni.e., mining-based generation and paraphrasing-based gen-\neration, to ﬁnd the optimal patterns that express particular\nrelations. AutoPrompt [ 71] ﬁnds the optimal prompt with\ngradient-guided search. LM-BFF [ 72] employs T5 [ 42] to\nautomatically generate prompts.\nContinuous prompts Instead of ﬁnding the optimal con-\ncrete prompt, another alternative is to directly optimize the\nprompt in continuous space, i.e. the prompt vectors are not\nnecessarily word type embeddings of the PTM. The opti-\nmized continuous prompt is concatenated with word type\nembeddings, which is then fed into the PTM. Qin and Eisner\n[203] and Zhong et al. [204] found that the optimized con-\ntinuous prompt can outperform concrete prompts (including\nmanual [129], mined (LPAQA [201]), and gradient-searched\n(AutoPrompt [71]) prompts) on relational tasks. W ARP [73]\ninserts trainable continuous prompt tokens before, between,\nand after the input sequence while keeping the parameters\nof the PTM ﬁxed, resulting in considerable performance on\nGLUE benchmark. Preﬁx-Tuning [ 74] inserts continuous\nprompt as preﬁx of the input of GPT-2 for table-to-text gen-\neration and BART for summarization. Preﬁx-Tuning, as a\nparameter-eﬃcient tuning technique, achieved comparable per-\nformance in fully-supervised setting and outperformed model\nﬁne-tuning in few-shot setting. Further, P-Tuning [75] showed\nthat, with continuous prompt, GPT can also achieve compa-\nrable or even better performance to similar-sized BERT on\nnatural language understanding (NLU) tasks. Very recently,\nLester et al. [205] showed that prompt tuning becomes more\ncompetitive with scale. When the PTM exceeds billions of\nparameters, the gap between model ﬁne-tuning and prompt\ntuning can be closed, which makes the prompt-based tuning\na very promising method for eﬃcient serving of large-scale\nPTMs.\n6 Resources of PTMs\nThere are many related resources for PTMs available online.\nTable 5 provides some popular repositories, including third-\nparty implementations, paper lists, visualization tools, and\nother related resources of PTMs.\nBesides, there are some other good survey papers on PTMs\nfor NLP [211, 212, 173].\n7 Applications\nIn this section, we summarize some applications of PTMs in\nseveral classic NLP tasks.\n18 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nTable 5: Resources of PTMs\nResource Description URL\nOpen-Source Implementations §\nword2vec CBOW,Skip-Gram https: //github.com/tmikolov/word2vec\nGloVe Pre-trained word vectors https: //nlp.stanford.edu/projects/glove\nFastText Pre-trained word vectors https: //github.com/facebookresearch/fastText\nTransformers Framework: PyTorch&TF, PTMs: BERT, GPT-2, RoBERTa, XLNet, etc. https: //github.com/huggingface/transformers\nFairseq Framework: PyTorch, PTMs:English LM, German LM, RoBERTa, etc. https: //github.com/pytorch/fairseq\nFlair Framework: PyTorch, PTMs:BERT, ELMo, GPT, RoBERTa, XLNet, etc. https: //github.com/ﬂairNLP/ﬂair\nAllenNLP [206] Framework: PyTorch, PTMs: ELMo, BERT, GPT-2, etc. https: //github.com/allenai/allennlp\nfastNLP Framework: PyTorch, PTMs: RoBERTa, GPT, etc. https: //github.com/fastnlp/fastNLP\nUniLMs Framework: PyTorch, PTMs: UniLM v1&v2, MiniLM, LayoutLM, etc. https: //github.com/microsoft/unilm\nChinese-BERT [85] Framework: PyTorch&TF, PTMs: BERT, RoBERTa, etc. (for Chinese) https: //github.com/ymcui/Chinese-BERT-wwm\nBERT [16] Framework: TF, PTMs: BERT, BERT-wwm https: //github.com/google-research/bert\nRoBERTa [43] Framework: PyTorch https: //github.com/pytorch/fairseq/tree/master/examples/roberta\nXLNet [49] Framework: TF https: //github.com/zihangdai/xlnet/\nALBERT [63] Framework: TF https: //github.com/google-research/ALBERT\nT5 [42] Framework: TF https: //github.com/google-research/text-to-text-transfer-transformer\nERNIE(Baidu) [84, 153] Framework: PaddlePaddle https: //github.com/PaddlePaddle/ERNIE\nCTRL [207] Conditional Transformer Language Model for Controllable Generation. https: //github.com/salesforce/ctrl\nBertViz [208] Visualization Tool https: //github.com/jessevig/bertviz\nexBERT [209] Visualization Tool https: //github.com/bhoov/exbert\nTextBrewer [210] PyTorch-based toolkit for distillation of NLP models. https: //github.com/airaria/TextBrewer\nDeepPavlov Conversational AI Library. PTMs for the Russian, Polish, Bulgarian,\nCzech, and informal English.\nhttps://github.com/deepmipt/DeepPavlov\nCorpora\nOpenWebText Open clone of OpenAI’s unreleased WebText dataset. https: //github.com/jcpeterson/openwebtext\nCommon Crawl A very large collection of text. http: //commoncrawl.org/\nWikiEn English Wikipedia dumps. https: //dumps.wikimedia.org/enwiki/\nOther Resources\nPaper List https://github.com/thunlp/PLMpapers\nPaper List https://github.com/tomohideshibata/BERT-related-papers\nPaper List https://github.com/cedrickchee/awesome-bert-nlp\nBert Lang Street A collection of BERT models with reported performances on di ﬀerent\ndatasets, tasks and languages.\nhttps://bertlang.unibocconi.it/\n§Most papers for PTMs release their links of oﬃcial version. Here we list some popular third-party and oﬃcial implementations.\n7.1 General Evaluation Benchmark\nThere is an essential issue for the NLP community that how\ncan we evaluate PTMs in a comparable metric. Thus, large-\nscale-benchmark is necessary.\nThe General Language Understanding Evaluation (GLUE)\nbenchmark [213] is a collection of nine natural language under-\nstanding tasks, including single-sentence classiﬁcation tasks\n(CoLA and SST-2), pairwise text classiﬁcation tasks (MNLI,\nRTE, WNLI, QQP, and MRPC), text similarity task (STS-\nB), and relevant ranking task (QNLI). GLUE benchmark is\nwell-designed for evaluating the robustness as well as general-\nization of models. GLUE does not provide the labels for the\ntest set but set up an evaluation server.\nHowever, motivated by the fact that the progress in recent\nyears has eroded headroom on the GLUE benchmark dra-\nmatically, a new benchmark called SuperGLUE [ 214] was\npresented. Compared to GLUE, SuperGLUE has more chal-\nlenging tasks and more diverse task formats (e.g., coreference\nresolution and question answering).\nState-of-the-art PTMs are listed in the corresponding leader-\nboard4) 5) .\n7.2 Question Answering\nQuestion answering (QA), or a narrower concept machine\nreading comprehension (MRC), is an important application in\nthe NLP community. From easy to hard, there are three types\nof QA tasks: single-round extractive QA (SQuAD) [ 215],\nmulti-round generative QA (CoQA) [216], and multi-hop QA\n(HotpotQA) [217].\nBERT creatively transforms the extractive QA task to the\nspans prediction task that predicts the starting span as well\nas the ending span of the answer [ 16]. After that, PTM as\nan encoder for predicting spans has become a competitive\nbaseline. For extractive QA, Zhang et al. [218] proposed a ret-\n4) https://gluebenchmark.com/\n5) https://super.gluebenchmark.com/\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 19\nrospective reader architecture and initialize the encoder with\nPTM (e.g., ALBERT). For multi-round generative QA, Ju et al.\n[219] proposed a “PTM+Adversarial Training+Rationale Tag-\nging+Knowledge Distillation” model. For multi-hop QA, Tu\net al. [220] proposed an interpretable “Select, Answer, and\nExplain” (SAE) system that PTM acts as the encoder in the\nselection module.\nGenerally, encoder parameters in the proposed QA model\nare initialized through a PTM, and other parameters are ran-\ndomly initialized. State-of-the-art models are listed in the\ncorresponding leaderboard. 6) 7) 8)\n7.3 Sentiment Analysis\nBERT outperforms previous state-of-the-art models by simply\nﬁne-tuning on SST-2, which is a widely used dataset for senti-\nment analysis (SA) [16]. Bataa and Wu [221] utilized BERT\nwith transfer learning techniques and achieve new state-of-the-\nart in Japanese SA.\nDespite their success in simple sentiment classiﬁcation,\ndirectly applying BERT to aspect-based sentiment analysis\n(ABSA), which is a ﬁne-grained SA task, shows less signif-\nicant improvement [ 202]. To better leverage the powerful\nrepresentation of BERT, Sun et al. [202] constructed an auxil-\niary sentence by transforming ABSA from a single sentence\nclassiﬁcation task to a sentence pair classiﬁcation task. Xu\net al. [222] proposed post-training to adapt BERT from its\nsource domain and tasks to the ABSA domain and tasks. Fur-\nthermore, Rietzler et al. [223] extended the work of [ 222]\nby analyzing the behavior of cross-domain post-training with\nABSA performance. Karimi et al. [224] showed that the per-\nformance of post-trained BERT could be further improved\nvia adversarial training. Song et al. [225] added an additional\npooling module, which can be implemented as either LSTM\nor attention mechanism, to leverage BERT intermediate lay-\ners for ABSA. In addition, Li et al. [226] jointly learned as-\npect detection and sentiment classiﬁcation towards end-to-end\nABSA. SentiLR [79] acquires part-of-speech tag and prior sen-\ntiment polarity from SentiWordNet and adopts Label-Aware\nMLM to utilize the introduced linguistic knowledge to capture\nthe relationship between sentence-level sentiment labels and\nword-level sentiment shifts. SentiLR achieves state-of-the-art\nperformance on several sentence- and aspect-level sentiment\nclassiﬁcation tasks.\nFor sentiment transfer, Wu et al. [227] proposed “Mask\nand Inﬁll” based on BERT. In the mask step, the model disen-\ntangles sentiment from content by masking sentiment tokens.\nIn the inﬁll step, it uses BERT along with a target sentiment\nembedding to inﬁll the masked positions.\n7.4 Named Entity Recognition\nNamed Entity Recognition (NER) in information extraction\nand plays an important role in many NLP downstream tasks.\nIn deep learning, most of NER methods are in the sequence-\nlabeling framework. The entity information in a sentence\nwill be transformed into the sequence of labels, and one label\ncorresponds to one word. The model is used to predict the\nlabel of each word. Since ELMo and BERT have shown their\npower in NLP, there is much work about pre-trained models\nfor NER.\nAkbik et al. [37] used a pre-trained character-level language\nmodel to produce word-level embedding for NER. TagLM\n[228] and ELMo [14] use a pre-trained language model’s last\nlayer output and weighted-sum of each layer output as a part\nof word embedding. Liu et al. [229] used layer-wise pruning\nand dense connection to speed up ELMo’s inference on NER.\nDevlin et al. [16] used the ﬁrst BPE’s BERT representation\nto predict each word’s label without CRF. Pires et al. [150]\nrealized zero-shot NER through multilingual BERT. Tsai et al.\n[178] leveraged knowledge distillation to run a small BERT\nfor NER on a single CPU. Besides, BERT is also used on\ndomain-speciﬁc NER, such as biomedicine [230, 100], etc.\n7.5 Machine Translation\nMachine Translation (MT) is an important task in the NLP\ncommunity, which has attracted many researchers. Almost\nall of Neural Machine Translation (NMT) models share the\nencoder-decoder framework, which ﬁrst encodes input tokens\nto hidden representations by the encoder and then decodes\noutput tokens in the target language from the decoder. Ra-\nmachandran et al. [36] found the encoder-decoder models can\nbe signiﬁcantly improved by initializing both encoder and\ndecoder with pre-trained weights of two language models.\nEdunov et al. [231] used ELMo to set the word embedding\nlayer in the NMT model. This work shows performance im-\nprovements on English-Turkish and English-German NMT\nmodel by using a pre-trained language model for source word\nembedding initialization.\nGiven the superb performance of BERT on other NLP\ntasks, it is natural to investigate how to incorporate BERT into\nNMT models. Conneau and Lample [46] tried to initialize\nthe entire encoder and decoder by a multilingual pre-trained\nBERT model and showed a signiﬁcant improvement could be\nachieved on unsupervised MT and English-Romanian super-\nvised MT. Similarly, Clinchant et al. [232] devised a series\n6) https://rajpurkar.github.io/SQuAD-explorer/\n7) https://stanfordnlp.github.io/coqa/\n8) https://hotpotqa.github.io/\n20 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nof diﬀerent experiments for examining the best strategy to\nutilize BERT on the encoder part of NMT models. They\nachieved some improvement by using BERT as an initializa-\ntion of the encoder. Also, they found that these models can get\nbetter performance on the out-of-domain dataset. Imamura\nand Sumita [233] proposed a two stages BERT ﬁne-tuning\nmethod for NMT. At the ﬁrst stage, the encoder is initialized\nby a pre-trained BERT model, and they only train the decoder\non the training set. At the second stage, the whole NMT\nmodel is jointly ﬁne-tuned on the training set. By experiment,\nthey show this approach can surpass the one stage ﬁne-tuning\nmethod, which directly ﬁne-tunes the whole model. Apart\nfrom that, Zhu et al. [192] suggested using pre-trained BERT\nas an extra memory to facilitate NMT models. Concretely,\nthey ﬁrst encode the input tokens by a pre-trained BERT and\nuse the output of the last layer as extra memory. Then, the\nNMT model can access the memory via an extra attention mod-\nule in each layer of both encoder and decoder. And they show\na noticeable improvement in supervised, semi-supervised, and\nunsupervised MT.\nInstead of only pre-training the encoder, MASS (Masked\nSequence-to-Sequence Pre-Training) [41] utilizes Seq2Seq\nMLM to pre-train the encoder and decoder jointly. In the\nexperiment, this approach can surpass the BERT-style pre-\ntraining proposed by Conneau and Lample [46] both on un-\nsupervised MT and English-Romanian supervised MT. Dif-\nferent from MASS, mBART [ 61], a multilingual extension\nof BART [ 50], pre-trains the encoder and decoder jointly\nwith Seq2Seq denoising auto-encoder (DAE) task on large-\nscale monolingual corpora across 25 languages. Experiments\ndemonstrated that mBART could signiﬁcantly improve both\nsupervised and unsupervised machine translation at both the\nsentence level and document level.\n7.6 Summarization\nSummarization, aiming at producing a shorter text which pre-\nserves the most meaning of a longer text, has attracted the\nattention of the NLP community in recent years. The task\nhas been improved signiﬁcantly since the widespread use of\nPTM. Zhong et al. [191] introduced transferable knowledge\n(e.g., BERT) for summarization and surpassed previous mod-\nels. Zhang et al. [234] tries to pre-trained a document-level\nmodel that predicts sentences instead of words, and then apply\nit on downstream tasks such as summarization. More elabo-\nrately, Zhang et al. [163] designed a Gap Sentence Generation\n(GSG) task for pre-training, whose objective involves generat-\ning summary-like text from the input. Furthermore, Liu and\nLapata [235] proposed BERTSUM. BERTSUM included a\nnovel document-level encoder, and a general framework for\nboth extractive summarization and abstractive summarization.\nIn the encoder frame, BERTSUM extends BERT by inserting\nmultiple [CLS] tokens to learn the sentence representations.\nFor extractive summarization, BERTSUM stacks several inter-\nsentence Transformer layers. For abstractive summarization,\nBERTSUM proposes a two-staged ﬁne-tuning approach using\na new ﬁne-tuning schedule. Zhong et al. [236] proposed a\nnovel summary-level framework MATCHSUM and conceptu-\nalized extractive summarization as a semantic text matching\nproblem. They proposed a Siamese-BERT architecture to\ncompute the similarity between the source document and the\ncandidate summary and achieved a state-of-the-art result on\nCNN/DailyMail (44.41 in ROUGE-1) by only using the base\nversion of BERT.\n7.7 Adversarial Attacks and Defenses\nThe deep neural models are vulnerable to adversarial examples\nthat can mislead a model to produce a speciﬁc wrong predic-\ntion with imperceptible perturbations from the original input.\nIn CV , adversarial attacks and defenses have been widely stud-\nied. However, it is still challenging for text due to the discrete\nnature of languages. Generating of adversarial samples for\ntext needs to possess such qualities: (1) imperceptible to hu-\nman judges yet misleading to neural models; (2) ﬂuent in\ngrammar and semantically consistent with original inputs. Jin\net al. [237] successfully attacked the ﬁne-tuned BERT on text\nclassiﬁcation and textual entailment with adversarial exam-\nples. Wallace et al. [238] deﬁned universal adversarial triggers\nthat can induce a model to produce a speciﬁc-purpose predic-\ntion when concatenated to any input. Some triggers can even\ncause the GPT-2 model to generate racist text. Sun et al.[239]\nshowed BERT is not robust on misspellings.\nPTMs also have great potential to generate adversarial sam-\nples. Li et al. [240] proposed BERT-Attack, a BERT-based\nhigh-quality and eﬀective attacker. They turned BERT against\nanother ﬁne-tuned BERT on downstream tasks and success-\nfully misguided the target model to predict incorrectly, out-\nperforming state-of-the-art attack strategies in both success\nrate and perturb percentage, while the generated adversarial\nsamples are ﬂuent and semantically preserved.\nBesides, adversarial defenses for PTMs are also promis-\ning, which improve the robustness of PTMs and make them\nimmune against adversarial attack.\nAdversarial training aims to improve the generalization\nby minimizes the maximal risk for label-preserving perturba-\ntions in embedding space. Recent work [ 241, 242] showed\nthat adversarial pre-training or ﬁne-tuning can improve both\ngeneralization and robustness of PTMs for NLP.\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 21\n8 Future Directions\nThough PTMs have proven their power for various NLP tasks,\nchallenges still exist due to the complexity of language. In\nthis section, we suggest ﬁve future directions of PTMs.\n(1) Upper Bound of PTMs Currently, PTMs have not yet\nreached its upper bound. Most of the current PTMs can be\nfurther improved by more training steps and larger corpora.\nThe state of the art in NLP can be further advanced by\nincreasing the depth of models, such as Megatron-LM [243]\n(8.3 billion parameters, 72 Transformer layers with a hidden\nsize of 3072 and 32 attention heads) and Turing-NLG 9) (17\nbillion parameters, 78 Transformer layers with a hidden size\nof 4256 and 28 attention heads).\nThe general-purpose PTMs are always our pursuits for\nlearning the intrinsic universal knowledge of languages (even\nworld knowledge). However, such PTMs usually need deeper\narchitecture, larger corpus, and challenging pre-training tasks,\nwhich further result in higher training costs. However, train-\ning huge models is also a challenging problem, which needs\nmore sophisticated and eﬃcient training techniques such as\ndistributed training, mixed precision, gradient accumulation,\netc. Therefore, a more practical direction is to design more\neﬃcient model architecture, self-supervised pre-training tasks,\noptimizers, and training skills using existing hardware and\nsoftware. ELECTRA [ 56] is a good solution towards this\ndirection.\n(2) Architecture of PTMs The Transformer has been\nproved to be an eﬀective architecture for pre-training. How-\never, the main limitation of the Transformer is its computation\ncomplexity, which is quadratic to the input length. Limited\nby the memory of GPUs, most of current PTMs cannot deal\nwith the sequence longer than 512 tokens. Breaking this limit\nneeds to improve the architecture of the Transformer. Al-\nthough many works [ 25] tried to improve the e ﬃciency of\nTransformer, there remains much room for improvement.\nBesides, searching for more e ﬃcient alternative non-\nTransformer architecture for PTMs is important to capture\nlonger-range contextual information. The design of deep\narchitecture is challenging, and we may seek help from\nsome automatic methods, such as neural architecture search\n(NAS) [245].\n(3) Task-oriented Pre-training and Model Compression\nIn practice, diﬀerent downstream tasks require the diﬀerent\nabilities of PTMs. The discrepancy between PTMs and down-\nstream tasks usually lies in two aspects: model architecture\nand data distribution. A larger discrepancy may result in that\nthe beneﬁt of PTMs may be insigniﬁcant. For example, text\ngeneration usually needs a speciﬁc task to pre-train both the\nencoder and decoder, while text matching needs pre-training\ntasks designed for sentence pairs.\nBesides, although larger PTMs can usually lead to better\nperformance, a practical problem is how to leverage these\nhuge PTMs on special scenarios, such as low-capacity devices\nand low-latency applications. Therefore, we can carefully de-\nsign the speciﬁc model architecture and pre-training tasks for\ndownstream tasks or extract partial task-speciﬁc knowledge\nfrom existing PTMs.\nInstead of training task-oriented PTMs from scratch, we\ncan teach them with existing general-purpose PTMs by us-\ning techniques such as model compression (see Section 4.5).\nAlthough model compression is widely studied for CNNs in\nCV [246], compression for PTMs for NLP is just beginning.\nThe fully-connected structure of the Transformer also makes\nmodel compression more challenging.\n(4) Knowledge Transfer Beyond Fine-tuning Currently,\nﬁne-tuning is the dominant method to transfer PTMs’ knowl-\nedge to downstream tasks, but one deﬁciency is its parameter\nineﬃciency: every downstream task has its own ﬁne-tuned\nparameters. An improved solution is to ﬁx the original pa-\nrameters of PTMs and by adding small ﬁne-tunable adap-\ntion modules for speciﬁc task [ 68, 69]. Thus, we can use\na shared PTM to serve multiple downstream tasks. Indeed,\nmining knowledge from PTMs can be more ﬂexible, such as\nfeature extraction, knowledge distillation [210], data augmen-\ntation [247, 248], using PTMs as external knowledge [ 129].\nMore eﬃcient methods are expected.\n(5) Interpretability and Reliability of PTMs Although\nPTMs reach impressive performance, their deep non-linear\narchitecture makes the procedure of decision-making highly\nnon-transparent.\nRecently, explainable artiﬁcial intelligence (XAI) [249] has\nbecome a hotspot in the general AI community. Unlike CNNs\nfor images, interpreting PTMs is harder due to the complex-\nities of both the Transformer-like architecture and language.\nExtensive eﬀorts (see Section 3.3) have been made to analyze\nthe linguistic and world knowledge included in PTMs, which\nhelp us understand these PMTs with some degree of trans-\nparency. However, much work on model analysis depends on\nthe attention mechanism, and the eﬀectiveness of attention for\ninterpretability is still controversial [250, 251].\nBesides, PTMs are also vulnerable to adversarial attacks\n(see Section 7.7). The reliability of PTMs is also becoming\nan issue of great concern with the extensive use of PTMs in\nproduction systems. The studies of adversarial attacks against\nPTMs help us understand their capabilities by fully exposing\n9) https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\n22 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\ntheir vulnerabilities. Adversarial defenses for PTMs are also\npromising, which improve the robustness of PTMs and make\nthem immune against adversarial attack.\nOverall, as key components in many NLP applications,\nthe interpretability and reliability of PTMs remain to be ex-\nplored further in many respects, which helps us understand\nhow PTMs work and provides a guide for better usage and\nfurther improvement.\n9 Conclusion\nIn this survey, we conduct a comprehensive overview of\nPTMs for NLP, including background knowledge, model ar-\nchitecture, pre-training tasks, various extensions, adaption\napproaches, related resources, and applications. Based on\ncurrent PTMs, we propose a new taxonomy of PTMs from\nfour diﬀerent perspectives. We also suggest several possible\nfuture research directions for PTMs.\nAcknowledgements\nWe thank Zhiyuan Liu, Wanxiang Che, Minlie Huang, Dan-\nqing Wang and Luyao Huang for their valuable feedback on\nthis manuscript. This work was supported by the National\nNatural Science Foundation of China (No. 61751201 and\n61672162), Shanghai Municipal Science and Technology Ma-\njor Project (No. 2018SHZDZX01) and ZJLab.\nReferences\n[1] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.\nA convolutional neural network for modelling sentences. In\nACL, 2014.\n[2] Yoon Kim. Convolutional neural networks for sentence classi-\nﬁcation. In EMNLP, pages 1746–1751, 2014.\n[3] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats,\nand Yann N Dauphin. Convolutional sequence to sequence\nlearning. In ICML, pages 1243–1252, 2017.\n[4] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to\nsequence learning with neural networks. In NeurIPS, pages\n3104–3112, 2014.\n[5] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent\nneural network for text classiﬁcation with multi-task learning.\nIn IJCAI, 2016.\n[6] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,\nChristopher D Manning, Andrew Y Ng, and Christopher Potts.\nRecursive deep models for semantic compositionality over\na sentiment treebank. In EMNLP, pages 1631–1642. ACL,\n2013.\n[7] Kai Sheng Tai, Richard Socher, and Christopher D. Manning.\nImproved semantic representations from tree-structured long\nshort-term memory networks. In ACL, pages 1556–1566,\n2015.\n[8] Diego Marcheggiani, Joost Bastings, and Ivan Titov. Ex-\nploiting semantics in neural machine translation with graph\nconvolutional networks. In NAACL-HLT, pages 486–492,\n2018.\n[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In ICLR, 2014.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Cor-\nrado, and Jeﬀrey Dean. Distributed representations of words\nand phrases and their compositionality. In NeurIPS, 2013.\n[12] Jeﬀrey Pennington, Richard Socher, and Christopher D. Man-\nning. GloVe: Global vectors for word representation. In\nEMNLP, 2014.\n[13] Bryan McCann, James Bradbury, Caiming Xiong, and Richard\nSocher. Learned in translation: Contextualized word vectors.\nIn NeurIPS, 2017.\n[14] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In NAACL-HLT,\n2018.\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. URL https: //s3-us-west-2.amazonaws.\ncom/openai-assets/researchcovers/languageunsupervised/\nlanguageunderstandingpaper.pdf.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n[17] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-\nresentation learning: A review and new perspectives. IEEE\ntransactions on pattern analysis and machine intelligence, 35\n(8):1798–1828, 2013.\n[18] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M\nRush. Character-aware neural language models. In AAAI,\n2016.\n[19] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas\nMikolov. Enriching word vectors with subword informa-\ntion. TACL, 5:135–146, 2017. doi: https: //doi.org/10.1162/\ntacl a 00051.\n[20] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural\nmachine translation of rare words with subword units. In ACL,\n2016.\n[21] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term\nmemory. Neural Computation, 1997. doi: https: //doi.org/10.\n1162/neco.1997.9.8.1735.\n[22] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and\nYoshua Bengio. Empirical evaluation of gated recurrent\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 23\nneural networks on sequence modeling. arXiv preprint\narXiv:1412.3555, 2014.\n[23] Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long\nshort-term memory over recursive structures. In International\nConference on Machine Learning, pages 1604–1612, 2015.\n[24] Thomas N Kipf and Max Welling. Semi-supervised classiﬁca-\ntion with graph convolutional networks. In ICLR, 2017.\n[25] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.\nA survey of transformers. arXiv preprint arXiv:2106.04554,\n2021.\n[26] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xi-\nangyang Xue, and Zheng Zhang. Star-transformer. In NAACL-\nHLT, pages 1315–1325, 2019.\n[27] Dumitru Erhan, Yoshua Bengio, Aaron C. Courville, Pierre-\nAntoine Manzagol, Pascal Vincent, and Samy Bengio. Why\ndoes unsupervised pre-training help deep learning? J. Mach.\nLearn. Res., 11:625–660, 2010. doi: https: //dl.acm.org/doi/10.\n5555/1756006.1756025.\n[28] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing\nthe dimensionality of data with neural networks. Science, 313\n(5786):504–507, 2006. doi: https: //doi.org/10.1126/science.\n1127647.\n[29] GE Hinton, JL McClelland, and DE Rumelhart. Distributed\nrepresentations. In Parallel distributed processing: explo-\nrations in the microstructure of cognition, vol. 1: foundations,\npages 77–109. 1986.\n[30] Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Chris-\ntian Jauvin. A neural probabilistic language model. Jour-\nnal of machine learning research, 3:1137–1155, 2003. doi:\nhttps://dl.acm.org/doi/10.5555/944919.944966.\n[31] Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen,\nKoray Kavukcuoglu, and Pavel P. Kuksa. Natural language\nprocessing (almost) from scratch. J. Mach. Learn. Res., 2011.\ndoi: https://dl.acm.org/doi/10.5555/1953048.2078186.\n[32] Quoc Le and Tomas Mikolov. Distributed representations of\nsentences and documents. In ICML, pages 1188–1196, 2014.\n[33] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard\nZemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\nSkip-thought vectors. In NeurIPS, pages 3294–3302, 2015.\n[34] Oren Melamud, Jacob Goldberger, and Ido Dagan. Con-\ntext2Vec: Learning generic context embedding with bidirec-\ntional LSTM. In CoNLL, pages 51–61, 2016.\n[35] Andrew M Dai and Quoc V Le. Semi-supervised sequence\nlearning. In NeurIPS, pages 3079–3087, 2015.\n[36] Prajit Ramachandran, Peter J Liu, and Quoc Le. Unsupervised\npretraining for sequence to sequence learning. In EMNLP,\npages 383–391, 2017.\n[37] Alan Akbik, Duncan Blythe, and Roland V ollgraf. Contextual\nstring embeddings for sequence labeling. In COLING, pages\n1638–1649, 2018.\n[38] Jeremy Howard and Sebastian Ruder. Universal language\nmodel ﬁne-tuning for text classiﬁcation. In ACL, pages 328–\n339, 2018.\n[39] Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettle-\nmoyer, and Michael Auli. Cloze-driven pretraining of self-\nattention networks. In Kentaro Inui, Jing Jiang, Vincent Ng,\nand Xiaojun Wan, editors, EMNLP-IJCNLP, pages 5359–\n5368, 2019.\n[40] Wilson L. Taylor. “cloze procedure”: A new tool for measur-\ning readability. Journalism Quarterly, 30(4):415–433, 1953.\ndoi: https://doi.org/10.1177/107769905303000401.\n[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan\nLiu. MASS: masked sequence to sequence pre-training for\nlanguage generation. In ICML, volume 97 of Proceedings of\nMachine Learning Research, pages 5926–5936, 2019.\n[42] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Pe-\nter J. Liu. Exploring the limits of transfer learning with a uni-\nﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683,\n2019.\n[43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[44] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu,\nYu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.\nUniﬁed language model pre-training for natural language un-\nderstanding and generation. In NeurIPS, pages 13042–13054,\n2019.\n[45] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,\nXiaodong Liu, Yu Wang, Songhao Piao, Jianfeng Gao, Ming\nZhou, et al. UniLMv2: Pseudo-masked language models\nfor uniﬁed language model pre-training. arXiv preprint\narXiv:2002.12804, 2020.\n[46] Alexis Conneau and Guillaume Lample. Cross-lingual lan-\nguage model pretraining. In NeurIPS, pages 7057–7067, 2019.\n[47] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke\nZettlemoyer, and Omer Levy. SpanBERT: Improving pre-\ntraining by representing and predicting spans. Transactions of\nthe Association for Computational Linguistics, 8:64–77, 2019.\ndoi: https://doi.org/10.1162/tacl a 00300.\n[48] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei\nPeng, and Luo Si. StructBERT: Incorporating language struc-\ntures into pre-training for deep language understanding. In\nICLR, 2020.\n[49] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuss R Salakhutdinov, and Quoc V Le. XLNet: General-\nized autoregressive pretraining for language understanding. In\nNeurIPS, pages 5754–5764, 2019.\n[50] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-\nanov, and Luke Zettlemoyer. BART: denoising sequence-to-\n24 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nsequence pre-training for natural language generation, transla-\ntion, and comprehension. arXiv preprint arXiv:1910.13461,\n2019.\n[51] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail\nKhodak, and Hrishikesh Khandeparkar. A theoretical analysis\nof contrastive unsupervised representation learning. In ICML,\npages 5628–5637, 2019.\n[52] Andriy Mnih and Koray Kavukcuoglu. Learning word embed-\ndings eﬃciently with noise-contrastive estimation. InNeurIPS,\npages 2265–2273, 2013.\n[53] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized sta-\ntistical models. In AISTATS, pages 297–304, 2010.\n[54] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\nKaran Grewal, Philip Bachman, Adam Trischler, and Yoshua\nBengio. Learning deep representations by mutual information\nestimation and maximization. In ICLR, 2019.\n[55] Lingpeng Kong, Cyprien de Masson d’Autume, Lei Yu, Wang\nLing, Zihang Dai, and Dani Yogatama. A mutual information\nmaximization perspective of language representation learning.\nIn ICLR, 2019.\n[56] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christo-\npher D. Manning. ELECTRA: Pre-training text encoders as\ndiscriminators rather than generators. In ICLR, 2020.\n[57] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin\nStoyanov. Pretrained encyclopedia: Weakly supervised\nknowledge-pretrained language model. In ICLR, 2020.\n[58] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI Blog, 2019.\n[59] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-\nshot learners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Pro-\ncessing Systems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual, 2020.\n[60] Zewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling\nMao, and Heyan Huang. Cross-lingual natural language gen-\neration via pre-training. In AAAI, 2019.\n[61] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,\nMarjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.\nMultilingual denoising pre-training for neural machine trans-\nlation. arXiv preprint arXiv:2001.08210, 2020.\n[62] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav\nChaudhary, Guillaume Wenzek, Francisco Guzm´an, Edouard\nGrave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.\nUnsupervised cross-lingual representation learning at scale.\narXiv preprint arXiv:1911.02116, 2019.\n[63] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite\nBERT for self-supervised learning of language representa-\ntions. In International Conference on Learning Representa-\ntions, 2020.\n[64] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. How\nto ﬁne-tune BERT for text classiﬁcation? In China National\nConference on Chinese Computational Linguistics, pages 194–\n206, 2019.\n[65] Zhongyang Li, Xiao Ding, and Ting Liu. Story ending predic-\ntion by transferable bert. In IJCAI, pages 1800–1806, 2019.\n[66] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta,\nKyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t\nstop pretraining: Adapt language models to domains and tasks.\nIn ACL, pages 8342–8360, 2020.\n[67] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng\nGao. Multi-task deep neural networks for natural language\nunderstanding. In ACL, 2019.\n[68] Asa Cooper Stickland and Iain Murray. BERT and PALs:\nProjected attention layers for eﬃcient adaptation in multi-task\nlearning. In ICML, pages 5986–5995, 2019.\n[69] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-e ﬃcient transfer\nlearning for NLP. In ICML, pages 2790–2799, 2019.\n[70] Timo Schick and Hinrich Sch¨utze. It’s not just size that mat-\nters: Small language models are also few-shot learners. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 2339–2352. Association for Computational\nLinguistics, 2021.\n[71] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wal-\nlace, and Sameer Singh. Autoprompt: Eliciting knowledge\nfrom language models with automatically generated prompts.\nIn Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 4222–4235. Association for\nComputational Linguistics, 2020.\n[72] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-\ntrained language models better few-shot learners. arXiv\npreprint arXiv:2012.15723, 2020.\n[73] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan\nMay. WARP: word-level adversarial reprogramming. arXiv\npreprint arXiv:2101.00121, 2021.\n[74] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimiz-\ning continuous prompts for generation. arXiv preprint\narXiv:2101.00190, 2021.\n[75] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 25\nQian, Zhilin Yang, and Jie Tang. GPT understands, too. arXiv\npreprint arXiv:2103.10385, 2021.\n[76] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong\nSun, and Qun Liu. ERNIE: enhanced language representation\nwith informative entities. In ACL, 2019.\n[77] Matthew E. Peters, Mark Neumann, Robert L. Logan IV , Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith.\nKnowledge enhanced contextual word representations. In\nEMNLP-IJCNLP, 2019.\n[78] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. K-BERT: Enabling language\nrepresentation with knowledge graph. In AAAI, 2019.\n[79] Pei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Min-\nlie Huang. SentiLR: Linguistic knowledge enhanced lan-\nguage representation for sentiment analysis. arXiv preprint\narXiv:1911.02493, 2019.\n[80] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu,\nJuanzi Li, and Jian Tang. KEPLER: A uniﬁed model for\nknowledge embedding and pre-trained language representa-\ntion. arXiv preprint arXiv:1911.06136, 2019.\n[81] Tianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru\nHu, Xuanjing Huang, and Zheng Zhang. Colake: Contextual-\nized language and knowledge embedding. In Proceedings of\nthe 28th International Conference on Computational Linguis-\ntics, COLING 2020, Barcelona, Spain (Online), December\n8-13, 2020, pages 3660–3670. International Committee on\nComputational Linguistics, 2020.\n[82] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun\nShou, Daxin Jiang, and Ming Zhou. Unicoder: A universal\nlanguage encoder by pre-training with multiple cross-lingual\ntasks. In EMNLP-IJCNLP, pages 2485–2494, 2019.\n[83] Julian Eisenschlos, Sebastian Ruder, Piotr Czapla, Marcin\nKadras, Sylvain Gugger, and Jeremy Howard. MultiFiT: Eﬃ-\ncient multi-lingual language model ﬁne-tuning. In EMNLP-\nIJCNLP, pages 5701–5706, 2019.\n[84] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen,\nHan Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua\nWu. ERNIE: enhanced representation through knowledge\nintegration. arXiv preprint arXiv:1904.09223, 2019.\n[85] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang,\nShijin Wang, and Guoping Hu. Pre-training with whole word\nmasking for chinese BERT. arXiv preprint arXiv:1906.08101,\n2019.\n[86] Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang,\nYi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen,\nand Qun Liu. NEZHA: Neural contextualized representa-\ntion for chinese language understanding. arXiv preprint\narXiv:1909.00204, 2019.\n[87] Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yong-\ngang Wang. ZEN: pre-training chinese text encoder enhanced\nby n-gram representations. arXiv preprint arXiv:1911.00720,\n2019.\n[88] Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza,\nTommaso Caselli, Gertjan van Noord, and Malvina Nis-\nsim. BERTje: A Dutch BERT model. arXiv preprint\narXiv:1912.09582, 2019.\n[89] Louis Martin, Benjamin M ¨uller, Pedro Javier Ortiz Su ´arez,\nYoann Dupont, Laurent Romary, ´Eric Villemonte de la Clerg-\nerie, Djam´e Seddah, and Benoˆıt Sagot. CamemBERT: a tasty\nFrench language model. arXiv preprint arXiv:1911.03894,\n2019.\n[90] Hang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin\nCoavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoˆıt\nCrabb´e, Laurent Besacier, and Didier Schwab. FlauBERT:\nUnsupervised language model pre-training for French. arXiv\npreprint arXiv:1912.05372, 2019.\n[91] Pieter Delobelle, Thomas Winters, and Bettina Berendt. Rob-\nBERT: a Dutch RoBERTa-based language model. arXiv\npreprint arXiv:2001.06286, 2020.\n[92] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-\nBERT: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In NeurIPS, pages 13–23,\n2019.\n[93] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP-IJCNLP, pages 5099–5110, 2019.\n[94] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and\nKai-Wei Chang. VisualBERT: A simple and performant base-\nline for vision and language.arXiv preprint arXiv:1908.03557,\n2019.\n[95] Chris Alberti, Jeﬀrey Ling, Michael Collins, and David Re-\nitter. Fusion of detected objects in text for visual question\nanswering. In EMNLP-IJCNLP, pages 2131–2140, 2019.\n[96] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: Pre-training of generic visual-\nlinguistic representations. In ICLR, 2020.\n[97] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. VideoBERT: A joint model for video and\nlanguage representation learning. In ICCV, pages 7463–7472.\nIEEE, 2019.\n[98] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia\nSchmid. Contrastive bidirectional transformer for temporal\nrepresentation learning. arXiv preprint arXiv:1906.05743 ,\n2019.\n[99] Yung-Sung Chuang, Chi-Liang Liu, and Hung-yi Lee.\nSpeechBERT: Cross-modal pre-trained language model for\nend-to-end spoken question answering. arXiv preprint\narXiv:1910.11559, 2019.\n[100] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim,\nSunkyu Kim, Chan Ho So, and Jaewoo Kang. BioBERT:\na pre-trained biomedical language representation model for\nbiomedical text mining. Bioinformatics, 2019.\n[101] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pre-\ntrained language model for scientiﬁc text. InEMNLP-IJCNLP,\n26 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\npages 3613–3618, 2019.\n[102] Jieh-Sheng Lee and Jieh Hsiang. PatentBERT: Patent clas-\nsiﬁcation with ﬁne-tuning a pre-trained BERT model. arXiv\npreprint arXiv:1906.02124, 2019.\n[103] Mitchell A Gordon, Kevin Duh, and Nicholas Andrews. Com-\npressing BERT: Studying the e ﬀects of weight pruning on\ntransfer learning. arXiv preprint arXiv:2002.08307, 2020.\n[104] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,\nAmir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-\nBERT: Hessian based ultra low precision quantization of\nBERT. In AAAI, 2020.\n[105] Oﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. Q8BERT: Quantized 8bit BERT. arXiv preprint\narXiv:1910.06188, 2019.\n[106] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas\nWolf. DistilBERT, a distilled version of BERT: smaller, faster,\ncheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[107] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,\nLinlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling\nBERT for natural language understanding. arXiv preprint\narXiv:1909.10351, 2019.\n[108] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,\nand Ming Zhou. MiniLM: Deep self-attention distillation for\ntask-agnostic compression of pre-trained transformers. arXiv\npreprint arXiv:2002.10957, 2020.\n[109] Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and\nMing Zhou. BERT-of-Theseus: Compressing BERT by pro-\ngressive module replacing. arXiv preprint arXiv:2002.02925,\n2020.\n[110] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy\nLin. DeeBERT: Dynamic early exiting for accelerating BERT\ninference. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 2246–2251,\nOnline, July 2020. Association for Computational Linguistics.\n[111] Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta,\nJesse Dodge, and Noah A. Smith. The right tool for the\njob: Matching model and instance complexities. In Dan Ju-\nrafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault,\neditors, Proceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 6640–6651. Association for Computational\nLinguistics, 2020.\n[112] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang\nDeng, and Qi Ju. FastBERT: a self-distilling BERT with\nadaptive inference time. In ACL, 2020.\n[113] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley,\nKe Xu, and Furu Wei. Bert loses patience: Fast and robust\ninference with early exit. arXiv preprint arXiv:2006.04152,\n2020.\n[114] Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, Xu Sun, and\nBin He. A global past-future early exit method for accelerat-\ning inference of pre-trained language models. In Proceedings\nof the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npages 2013–2023. Association for Computational Linguistics,\n2021.\n[115] Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang,\nHao Jiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu.\nEarly exiting with ensemble internal classiﬁers. arXiv preprint\narXiv: 2105.13792, 2021.\n[116] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng\nQiu, and Xuanjing Huang. Accelerating bert inference for\nsequence labeling via early-exit. In ACL, 2021.\n[117] Tomas Mikolov, Wen-tau Yih, and Geoﬀrey Zweig. Linguis-\ntic regularities in continuous space word representations. In\nHLT-NAACL, pages 746–751, 2013.\n[118] Dana Rubinstein, Eﬃ Levi, Roy Schwartz, and Ari Rappoport.\nHow well do distributional models capture diﬀerent types of\nsemantic knowledge? In ACL, pages 726–730, 2015.\n[119] Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian\nPad´o. Distributional vectors encode referential attributes. In\nEMNLP, pages 12–21, 2015.\n[120] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Po-\nliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme,\nSamuel R. Bowman, Dipanjan Das, and Ellie Pavlick. What\ndo you learn from context? probing for sentence structure in\ncontextualized word representations. In ICLR, 2019.\n[121] Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E.\nPeters, and Noah A. Smith. Linguistic knowledge and transfer-\nability of contextual representations. In NAACL-HLT, pages\n1073–1094, 2019.\n[122] Ian Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscov-\ners the classical NLP pipeline. In Anna Korhonen, David R.\nTraum, and Llu´ıs M`arquez, editors, ACL, pages 4593–4601,\n2019.\n[123] Yoav Goldberg. Assessing BERT’s syntactic abilities.arXiv\npreprint arXiv:1901.05287, 2019.\n[124] Allyson Ettinger. What BERT is not: Lessons from a new suite\nof psycholinguistic diagnostics for language models. TACL,\n8:34–48, 2020.\n[125] John Hewitt and Christopher D. Manning. A structural probe\nfor ﬁnding syntax in word representations. In NAACL-HLT,\npages 4129–4138, 2019.\n[126] Ganesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. What does\nBERT learn about the structure of language? In ACL, pages\n3651–3657, 2019.\n[127] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang goo Lee.\nAre pre-trained language models aware of phrases? simple\nbut strong baselines for grammar induction. In ICLR, 2020.\n[128] Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda B Vie-\ngas, Andy Coenen, Adam Pearce, and Been Kim. Visualizing\nand measuring the geometry of BERT. In NeurIPS, pages\n8592–8600, 2019.\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 27\n[129] Fabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick\nS. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H.\nMiller. Language models as knowledge bases? In EMNLP-\nIJCNLP, pages 2463–2473, 2019.\n[130] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neu-\nbig. How can we know what language models know? arXiv\npreprint arXiv:1911.12543, 2019.\n[131] Nina P¨orner, Ulli Waltinger, and Hinrich Sch¨utze. BERT is not\na knowledge base (yet): Factual knowledge vs. name-based\nreasoning in unsupervised QA. CoRR, abs/1911.03681, 2019.\n[132] Nora Kassner and Hinrich Sch ¨utze. Negated LAMA: birds\ncannot ﬂy. arXiv preprint arXiv:1911.03343, 2019.\n[133] Zied Bouraoui, Jos´e Camacho-Collados, and Steven Schock-\naert. Inducing relational knowledge from BERT. In AAAI,\n2019.\n[134] Joe Davison, Joshua Feldman, and Alexander M. Rush. Com-\nmonsense knowledge mining from pretrained models. In\nEMNLP-IJCNLP, pages 1173–1178, 2019.\n[135] Anne Lauscher, Ivan Vulic, Edoardo Maria Ponti, Anna Ko-\nrhonen, and Goran Glavas. Informing unsupervised pre-\ntraining with external linguistic knowledge. arXiv preprint\narXiv:1909.02339, 2019.\n[136] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing\nHuang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou.\nK-adapter: Infusing knowledge into pre-trained models with\nadapters. arXiv preprint arXiv:2002.01808, 2020.\n[137] Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir,\nShai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham.\nSenseBERT: Driving some sense into BERT. arXiv preprint\narXiv:1908.05646, 2019.\n[138] Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie\nHuang. A knowledge-enhanced pretraining model for com-\nmonsense story generation. arXiv preprint arXiv:2001.05139,\n2020.\n[139] Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. Integrating graph contex-\ntualized knowledge into pre-trained language models. arXiv\npreprint arXiv:1912.00147, 2019.\n[140] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\nKnowledge graph and text jointly embedding. In EMNLP,\npages 1591–1601, 2014.\n[141] Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and\nZheng Chen. Aligning knowledge and text embeddings by\nentity descriptions. In EMNLP, pages 267–272, 2015.\n[142] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and\nMaosong Sun. Representation learning of knowledge graphs\nwith entity descriptions. In IJCAI, 2016.\n[143] Jiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang.\nKnowledge graph representation with jointly structural and\ntextual encoding. In IJCAI, pages 1318–1324, 2017.\n[144] An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua\nWu, Qiaoqiao She, and Sujian Li. Enhancing pre-trained\nlanguage representations with rich knowledge for machine\nreading comprehension. In ACL, pages 2346–2357, 2019.\n[145] Robert L. Logan IV, Nelson F. Liu, Matthew E. Peters, Matt\nGardner, and Sameer Singh. Barack’s wife hillary: Using\nknowledge graphs for fact-aware language modeling. In ACL,\n2019.\n[146] Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Graham\nNeubig. Latent relation language models. In AAAI, 2019.\n[147] Manaal Faruqui and Chris Dyer. Improving vector space word\nrepresentations using multilingual correlation. InEACL, pages\n462–471, 2014.\n[148] Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\nBilingual word representations with monolingual quality in\nmind. In Proceedings of the 1st Workshop on Vector Space\nModeling for Natural Language Processing, pages 151–159,\n2015.\n[149] Karan Singla, Do˘gan Can, and Shrikanth Narayanan. A multi-\ntask approach to learning multilingual representations. In\nACL, pages 214–220, 2018.\n[150] Telmo Pires, Eva Schlinger, and Dan Garrette. How multi-\nlingual is multilingual BERT? In ACL, pages 4996–5001,\n2019.\n[151] Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth.\nCross-lingual ability of multilingual BERT: An empirical\nstudy. In ICLR, 2020.\n[152] Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani\nLuotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo.\nMultilingual is not enough: BERT for Finnish. arXiv preprint\narXiv:1912.07076, 2019.\n[153] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian,\nHua Wu, and Haifeng Wang. ERNIE 2.0: A continual pre-\ntraining framework for language understanding. In AAAI,\n2019.\n[154] Yuri Kuratov and Mikhail Arkhipov. Adaptation of deep bidi-\nrectional multilingual transformers for russian language.arXiv\npreprint arXiv:1905.07213, 2019.\n[155] Wissam Antoun, Fady Baly, and Hazem Hajj. AraBERT:\nTransformer-based model for Arabic language understanding.\narXiv preprint arXiv:2003.00104, 2020.\n[156] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan,\nTianrui Li, Xilin Chen, and Ming Zhou. UniViLM: A uniﬁed\nvideo and language pre-training model for multimodal under-\nstanding and generation. arXiv preprint arXiv:2002.06353,\n2020.\n[157] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou.\nUnicoder-vl: A universal encoder for vision and language by\ncross-modal pre-training. In AAAI, 2020.\n[158] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: learning universal image-text representations.arXiv\npreprint arXiv:1909.11740, 2019.\n[159] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clin-\n28 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\nicalBERT: Modeling clinical notes and predicting hospital\nreadmission. arXiv preprint arXiv:1904.05342, 2019.\n[160] Emily Alsentzer, John R. Murphy, Willie Boag, Wei-Hung\nWeng, Di Jin, Tristan Naumann, and Matthew B. A. McDer-\nmott. Publicly available clinical BERT embeddings. arXiv\npreprint arXiv:1904.03323, 2019.\n[161] Zongcheng Ji, Qiang Wei, and Hua Xu. BERT-based rank-\ning for biomedical entity normalization. arXiv preprint\narXiv:1908.03548, 2019.\n[162] Matthew Tang, Priyanka Gandhi, Md Ahsanul Kabir, Christo-\npher Zou, Jordyn Blakey, and Xiao Luo. Progress notes clas-\nsiﬁcation and keyword extraction using attention-based deep\nlearning models with BERT.arXiv preprint arXiv:1910.05786,\n2019.\n[163] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J\nLiu. PEGASUS: Pre-training with extracted gap-sentences for\nabstractive summarization. arXiv preprint arXiv:1912.08777,\n2019.\n[164] Shaolei Wang, Wanxiang Che, Qi Liu, Pengda Qin, Ting Liu,\nand William Yang Wang. Multi-task self-supervised learning\nfor disﬂuency detection. In AAAI, 2019.\n[165] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-\nMizil. Model compression. In KDD, pages 535–541, 2006.\n[166] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,\nYin Yang, Deming Chen, Marianne Winslett, Hassan Sajjad,\nand Preslav Nakov. Compressing large-scale transformer-\nbased models: A case study on BERT. arXiv preprint\narXiv:2002.11985, 2020.\n[167] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Ma-\nhoney, and Kurt Keutzer. Hawq: Hessian aware quantization\nof neural networks with mixed-precision. In ICCV, pages\n293–302, 2019.\n[168] Geoﬀrey Hinton, Oriol Vinyals, and Je ﬀ Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n[169] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowl-\nedge distillation for BERT model compression. In EMNLP-\nIJCNLP, pages 4323–4332, 2019.\n[170] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Well-read students learn better: The impact of\nstudent initialization on knowledge distillation. arXiv preprint\narXiv:1908.08962, 2019.\n[171] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yim-\ning Yang, and Denny Zhou. MobileBERT: a compact task-\nagnostic BERT for resource-limited devices. arXiv preprint\narXiv:2004.02984, 2020.\n[172] Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou.\nExtreme language model compression with optimal subwords\nand shared projections. arXiv preprint arXiv:1909.11687 ,\n2019.\n[173] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer\nin BERTology: What we know about how BERT works.arXiv\npreprint arXiv:2002.12327, 2020.\n[174] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen\nheads really better than one? In NeurIPS, pages 14014–14024,\n2019.\n[175] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and\nIvan Titov. Analyzing multi-head self-attention: Specialized\nheads do the heavy lifting, the rest can be pruned. In ACL,\npages 5797–5808, 2019.\n[176] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\nUszkoreit, and Lukasz Kaiser. Universal transformers. In\nICLR, 2019.\n[177] Wenhao Lu, Jian Jiao, and Ruofei Zhang. TwinBERT: Distill-\ning knowledge to twin-structured BERT models for eﬃcient\nretrieval. arXiv preprint arXiv:2002.06275, 2020.\n[178] Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Arivazha-\ngan, Xin Li, and Amelia Archer. Small and practical BERT\nmodels for sequence labeling. In EMNLP-IJCNLP, pages\n3632–3636, 2019.\n[179] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng\nGao. Improving multi-task deep neural networks via knowl-\nedge distillation for natural language understanding. arXiv\npreprint arXiv:1904.09482, 2019.\n[180] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechto-\nmova, and Jimmy Lin. Distilling task-speciﬁc knowledge\nfrom BERT into simple neural networks. arXiv preprint\narXiv:1903.12136, 2019.\n[181] Yew Ken Chia, Sam Witteveen, and Martin Andrews. Trans-\nformer to CNN: Label-scarce distillation for e ﬃcient text\nclassiﬁcation. arXiv preprint arXiv:1909.03508, 2019.\n[182] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung.\nBranchynet: Fast inference via early exiting from deep neural\nnetworks. In 23rd International Conference on Pattern Recog-\nnition, ICPR 2016, Canc´ un, Mexico, December 4-8, 2016,\npages 2464–2469. IEEE, 2016.\n[183] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\nShallow-deep networks: Understanding and mitigating net-\nwork overthinking. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, volume 97 of Proceed-\nings of Machine Learning Research, pages 3301–3310. PMLR,\n2019.\n[184] Alex Graves. Adaptive computation time for recurrent neural\nnetworks. arXiv preprint arXiv:1603.08983, 2016.\n[185] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.\nDepth-adaptive transformer. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net, 2020.\n[186] Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, and Jinan\nXu. Faster depth-adaptive transformers. In Thirty-Fifth AAAI\nConference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third\nConference on Innovative Applications of Artiﬁcial Intelli-\ngence, IAAI 2021, The Eleventh Symposium on Educational\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 29\nAdvances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event,\nFebruary 2-9, 2021, pages 13424–13432. AAAI Press, 2021.\n[187] Keli Xie, Siyuan Lu, Meiqi Wang, and Zhongfeng Wang. El-\nbert: Fast albert with conﬁdence-window based early exit. In\nICASSP 2021-2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , pages 7713–\n7717. IEEE, 2021.\n[188] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-\ning. IEEE Transactions on knowledge and data engineering,\n22(10):1345–1359, 2009. doi: https: //doi.org/10.1109/TKDE.\n2009.191.\n[189] Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad,\nand James Glass. What do neural machine translation models\nlearn about morphology? In ACL, pages 861–872, 2017.\n[190] Matthew E. Peters, Sebastian Ruder, and Noah A. Smith. To\ntune or not to tune? adapting pretrained representations to\ndiverse tasks. In Proceedings of the 4th Workshop on Repre-\nsentation Learning for NLP , RepL4NLP@ACL 2019, Florence,\nItaly, August 2, 2019, pages 7–14, 2019.\n[191] Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and\nXuanjing Huang. Searching for e ﬀective neural extractive\nsummarization: What works and what’s next. In ACL, pages\n1049–1058, 2019.\n[192] Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang\nZhou, Houqiang Li, and Tieyan Liu. Incorporating BERT into\nneural machine translation. In ICLR, 2020.\n[193] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi,\nHannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained\nlanguage models: Weight initializations, data orders, and early\nstopping. arXiv preprint arXiv:2002.06305, 2020.\n[194] Jason Phang, Thibault F´evry, and Samuel R Bowman. Sen-\ntence encoders on STILTs: Supplementary training on inter-\nmediate labeled-data tasks. arXiv preprint arXiv:1811.01088,\n2018.\n[195] Siddhant Garg, Thuy Vu, and Alessandro Moschitti. Tanda:\nTransfer and adapt pre-trained transformer models for answer\nsentence selection. In AAAI, 2019.\n[196] Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang.\nImproving BERT ﬁne-tuning via self-ensemble and self-\ndistillation. arXiv preprint arXiv:2002.10345, 2020.\n[197] Alexandra Chronopoulou, Christos Baziotis, and Alexandros\nPotamianos. An embarrassingly simple approach for transfer\nlearning from pretrained language models. In NAACL-HLT,\npages 2089–2095, 2019.\n[198] Xiang Lisa Li and Jason Eisner. Specializing word embed-\ndings (for parsing) by information bottleneck. In EMNLP-\nIJCNLP, pages 2744–2754, 2019.\n[199] Teven Le Scao and Alexander M. Rush. How many data\npoints is a prompt worth? In Proceedings of the 2021 Con-\nference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , pages 2627–\n2636. Association for Computational Linguistics, 2021.\n[200] Timo Schick and Hinrich Sch¨utze. Exploiting cloze-questions\nfor few-shot text classiﬁcation and natural language inference.\nIn Proceedings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics: Main\nVolume, EACL 2021, Online, April 19 - 23, 2021, pages 255–\n269. Association for Computational Linguistics, 2021.\n[201] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig.\nHow can we know what language models know. Trans. Assoc.\nComput. Linguistics, 8:423–438, 2020.\n[202] Chi Sun, Luyao Huang, and Xipeng Qiu. Utilizing BERT\nfor aspect-based sentiment analysis via constructing auxiliary\nsentence. In NAACL-HLT, 2019.\n[203] Guanghui Qin and Jason Eisner. Learning how to ask: Query-\ning lms with mixtures of soft prompts. In Proceedings of\nthe 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npages 5203–5212. Association for Computational Linguistics,\n2021.\n[204] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual prob-\ning is [MASK]: learning vs. learning to recall. In Proceedings\nof the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11, 2021 ,\npages 5017–5033. Association for Computational Linguistics,\n2021.\n[205] Brian Lester, Rami Al-Rfou, and Noah Constant. The power\nof scale for parameter-eﬃcient prompt tuning. arXiv preprint\narXiv:2104.08691, 2021.\n[206] Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord,\nPradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael\nSchmitz, and Luke S. Zettlemoyer. Allennlp: A deep semantic\nnatural language processing platform. 2017.\n[207] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caim-\ning Xiong, and Richard Socher. CTRL: A conditional trans-\nformer language model for controllable generation. arXiv\npreprint arXiv:1909.05858, 2019.\n[208] Jesse Vig. A multiscale visualization of attention in the trans-\nformer model. In ACL, 2019.\n[209] Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann.\nexbert: A visual analysis tool to explore learned rep-\nresentations in transformers models. arXiv preprint\narXiv:1910.05276, 2019.\n[210] Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting\nLiu, Shijin Wang, and Guoping Hu. Textbrewer: An open-\nsource knowledge distillation toolkit for natural language pro-\ncessing. arXiv preprint arXiv:2002.12620, 2020.\n[211] Yuxuan Wang, Yutai Hou, Wanxiang Che, and Ting Liu. From\nstatic to dynamic word representations: a survey.International\nJournal of Machine Learning and Cybernetics, pages 1–20,\n2020. doi: https: //doi.org/10.1007/s13042-020-01069-8.\n30 QIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020)\n[212] Qi Liu, Matt J Kusner, and Phil Blunsom. A survey on con-\ntextual embeddings. arXiv preprint arXiv:2003.07278, 2020.\n[213] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language under-\nstanding. In ICLR, 2019.\n[214] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet\nSingh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. SuperGLUE: A stickier benchmark for general-\npurpose language understanding systems. In NeurIPS, pages\n3261–3275, 2019.\n[215] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy\nLiang. Squad: 100, 000 + questions for machine comprehen-\nsion of text. In Jian Su, Xavier Carreras, and Kevin Duh,\neditors, EMNLP, pages 2383–2392, 2016.\n[216] Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA:\nA conversational question answering challenge. TACL, 7:249–\n266, 2019. doi: https: //doi.org/10.1162/tacl a 00266.\n[217] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam W. Cohen, Ruslan Salakhutdinov, and Christopher D.\nManning. HotpotQA: A dataset for diverse, explainable multi-\nhop question answering. In EMNLP, pages 2369–2380, 2018.\n[218] Zhuosheng Zhang, Junjie Yang, and Hai Zhao. Retrospective\nreader for machine reading comprehension. arXiv preprint\narXiv:2001.09694, 2020.\n[219] Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng\nYang, and Yunfeng Liu. Technical report on conversational\nquestion answering. arXiv preprint arXiv:1909.10772, 2019.\n[220] Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xi-\naodong He, and Bowen Zhou. Select, answer and explain:\nInterpretable multi-hop reading comprehension over multiple\ndocuments. In AAAI, 2020.\n[221] Enkhbold Bataa and Joshua Wu. An investigation of transfer\nlearning-based sentiment analysis in japanese. In ACL, 2019.\n[222] Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. BERT post-\ntraining for review reading comprehension and aspect-based\nsentiment analysis. In NAACL-HLT, 2019.\n[223] Alexander Rietzler, Sebastian Stabinger, Paul Opitz, and Ste-\nfan Engl. Adapt or get left behind: Domain adaptation through\nBERT language model ﬁnetuning for aspect-target sentiment\nclassiﬁcation. arXiv preprint arXiv:1908.11860, 2019.\n[224] Akbar Karimi, Leonardo Rossi, Andrea Prati, and Katharina\nFull. Adversarial training for aspect-based sentiment analysis\nwith BERT. arXiv preprint arXiv:2001.11316, 2020.\n[225] Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, and\nTao Jiang. Utilizing BERT intermediate layers for aspect based\nsentiment analysis and natural language inference. arXiv\npreprint arXiv:2002.04815, 2020.\n[226] Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. Exploit-\ning BERT for end-to-end aspect-based sentiment analysis. In\nW-NUT@EMNLP, 2019.\n[227] Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and\nSonglin Hu. ”mask and inﬁll” : Applying masked language\nmodel to sentiment transfer. In IJCAI, 2019.\n[228] Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula,\nand Russell Power. Semi-supervised sequence tagging with\nbidirectional language models. In ACL, pages 1756–1765,\n2017.\n[229] Liyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu, Jian Peng,\nand Jiawei Han. Eﬃcient contextualized representation: Lan-\nguage model pruning for sequence labeling. In EMNLP, pages\n1215–1225, 2018.\n[230] Kai Hakala and Sampo Pyysalo. Biomedical named entity\nrecognition with multilingual BERT. In BioNLP Open Shared\nTasks@EMNLP, pages 56–61, 2019.\n[231] Sergey Edunov, Alexei Baevski, and Michael Auli. Pre-trained\nlanguage model representations for language generation. In\nJill Burstein, Christy Doran, and Thamar Solorio, editors,\nNAACL-HLT, pages 4052–4059, 2019.\n[232] Stephane Clinchant, Kweon Woo Jung, and Vassilina\nNikoulina. On the use of BERT for neural machine translation.\nIn Proceedings of the 3rd Workshop on Neural Generation\nand Translation, Hong Kong, 2019.\n[233] Kenji Imamura and Eiichiro Sumita. Recycling a pre-trained\nBERT encoder for neural machine translation. In Proceedings\nof the 3rd Workshop on Neural Generation and Translation,\nHong Kong, November 2019.\n[234] Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: Docu-\nment level pre-training of hierarchical bidirectional transform-\ners for document summarization. In ACL, pages 5059–5069,\n2019.\n[235] Yang Liu and Mirella Lapata. Text summarization with pre-\ntrained encoders. In EMNLP/IJCNLP, 2019.\n[236] Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuan-Jing Huang. Extractive summarization\nas text matching. In ACL, 2020.\n[237] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is\nBERT really robust? natural language attack on text classiﬁ-\ncation and entailment. In AAAI, 2019.\n[238] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and\nSameer Singh. Universal adversarial triggers for attacking and\nanalyzing NLP. In EMNLP-IJCNLP, pages 2153–2162, 2019.\n[239] Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai,\nJia Li, Philip Yu, and Caiming Xiong. Adv-BERT: BERT\nis not robust on misspellings! generating nature adversarial\nsamples on BERT. arXiv preprint arXiv:2003.04985, 2020.\n[240] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and\nXipeng Qiu. BERT-ATTACK: Adversarial attack against\nBERT using BERT. arXiv preprint arXiv:2004.09984, 2020.\n[241] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein,\nand Jingjing Liu. FreeLB: Enhanced adversarial training for\nnatural language understanding. In ICLR, 2020.\n[242] Xiulei Liu, Hao Cheng, Peng cheng He, Weizhu Chen,\nYu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial\nQIU XP, et al. Pre-trained Models for Natural Language Processing: A Survey March (2020) 31\ntraining for large neural language models. arXiv preprint\narXiv:2004.08994, 2020.\n[243] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick\nLeGresley, Jared Casper, and Bryan Catanzaro. Megatron-\nLM: Training multi-billion parameter language models using\ngpu model parallelism. arXiv preprint arXiv:1909.08053 ,\n2019.\n[244] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc Le, and Ruslan Salakhutdinov. Transformer-XL: Atten-\ntive language models beyond a ﬁxed-length context. In ACL,\npages 2978–2988, 2019.\n[245] Barret Zoph and Quoc V Le. Neural architecture search with\nreinforcement learning. In ICLR, 2017.\n[246] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of\nmodel compression and acceleration for deep neural networks.\narXiv preprint arXiv:1710.09282, 2017.\n[247] Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and\nSonglin Hu. Conditional BERT contextual augmentation. In\nInternational Conference on Computational Science, pages\n84–95, 2019.\n[248] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data\naugmentation using pre-trained transformer models. arXiv\npreprint arXiv:2003.02245, 2020.\n[249] Alejandro Barredo Arrieta, Natalia D ´ıaz-Rodr´ıguez, Javier\nDel Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado,\nSalvador Garc´ıa, Sergio Gil-L´opez, Daniel Molina, Richard\nBenjamins, et al. Explainable artiﬁcial intelligence (xai):\nConcepts, taxonomies, opportunities and challenges toward\nresponsible ai. Information Fusion, 58:82–115, 2020.\n[250] Sarthak Jain and Byron C Wallace. Attention is not explana-\ntion. In NAACL-HLT, pages 3543–3556, 2019.\n[251] Soﬁa Serrano and Noah A Smith. Is attention interpretable?\nIn ACL, pages 2931–2951, 2019."
}