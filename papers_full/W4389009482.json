{
  "title": "The Next Chapter: A Study of Large Language Models in Storytelling",
  "url": "https://openalex.org/W4389009482",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2806626100",
      "name": "Zhuohan Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095936123",
      "name": "Jey Han Lau",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963101081",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3105484636",
    "https://openalex.org/W3174489133",
    "https://openalex.org/W2970574558",
    "https://openalex.org/W3035807844",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W4307106501",
    "https://openalex.org/W4385572789",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4313854414",
    "https://openalex.org/W4320854883",
    "https://openalex.org/W4385571140",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4206532177",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4226116544",
    "https://openalex.org/W4385574077",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W2556468274",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W4362655923",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3212002806",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571454",
    "https://openalex.org/W3101652466",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2963730239",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2517226069",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3174264667",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W3102401511",
    "https://openalex.org/W4327810450",
    "https://openalex.org/W3100714086",
    "https://openalex.org/W2963878748",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2250770256"
  ],
  "abstract": "To enhance the quality of generated stories, recent story generation models have been investigating the utilization of higher-level attributes like plots or commonsense knowledge. The application of prompt-based learning with large language models (LLMs), exemplified by GPT-3, has exhibited remarkable performance in diverse natural language processing (NLP) tasks. This paper conducts a comprehensive investigation, utilizing both automatic and human evaluation, to compare the story generation capacity of LLMs with recent models across three datasets with variations in style, register, and length of stories. The results demonstrate that LLMs generate stories of significantly higher quality compared to other story generation models. Moreover, they exhibit a level of performance that competes with human authors, albeit with the preliminary observation that they tend to replicate real stories in situations involving world knowledge, resembling a form of plagiarism.",
  "full_text": "Proceedings of the 16th International Natural Language Generation Conference, pages 323–351\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n323\nThe Next Chapter: A Study of Large Language Models in Storytelling\nZhuohan Xie Trevor Cohn ∗ Jey Han Lau\nSchool of Computing and Information Systems\nThe University of Melbourne\nzhuohanx@student.unimelb.edu.au, {t.cohn, laujh}@unimelb.edu.au\nAbstract\nTo enhance the quality of generated stories, re-\ncent story generation models have been investi-\ngating the utilization of higher-level attributes\nlike plots or commonsense knowledge. The ap-\nplication of prompt-based learning with large\nlanguage models (LLMs), exemplified by GPT-\n3, has exhibited remarkable performance in di-\nverse natural language processing (NLP) tasks.\nThis paper conducts a comprehensive inves-\ntigation, utilizing both automatic and human\nevaluation, to compare the story generation\ncapacity of LLMs with recent models across\nthree datasets with variations in style, regis-\nter, and length of stories. The results demon-\nstrate that LLMs generate stories of signifi-\ncantly higher quality compared to other story\ngeneration models. Moreover, they exhibit a\nlevel of performance that competes with human\nauthors, albeit with the preliminary observation\nthat they tend to replicate real stories in situa-\ntions involving world knowledge, resembling a\nform of plagiarism.\n1 Introduction\nAutomatic story generation poses a significant chal-\nlenge as it requires more than just individually co-\nherent sentences. A good story should exhibit a\nnatural flow, adhere to commonsense logic, and\nbe captivating to the reader. In recent times, the\nprevailing approach in story generation involves\nfine-tuning pre-trained language models (PLMs)\nlike GPT-2 (Radford et al., 2019) or BART (Lewis\net al., 2020) on specific datasets. These models gen-\nerally excel in generating fluent sentences, devoid\nof glaring grammar issues. However, they often\nstruggle to construct a coherent story that adheres\nto commonsense and fails to create an engaging\nnarrative (See et al., 2019; Guan et al., 2021a). To\novercome these challenges, state-of-the-art (SOTA)\nstory generation models integrate higher-level fea-\ntures, such as plots and commonsense knowledge.\n∗Now at Google DeepMind.\nPrompt-based learning (Liu et al., 2021) is a\nrecent paradigm specifically tailored for large lan-\nguage models (LLMs) that possess in-context learn-\ning capabilities (Brown et al., 2020; Zhao et al.,\n2023). In contrast to the conventional “pre-train\nand fine-tune” approach, which necessitates a sub-\nstantial amount of data for fine-tuning, prompt-\nbased learning enables LLMs to learn a task by pro-\nviding them with multiple examples as a “prompt”,\neliminating the need for gradient-based fine-tuning\n(Liu et al., 2021). Recently, LLMs have demon-\nstrated remarkable performance across various lan-\nguage generation tasks, with notable attention on\nmodels such as ChatGPT and GPT-4 (Qin et al.,\n2023; Liu et al., 2023b; OpenAI, 2023). For in-\nstance, a comparative analysis by Qin et al. (2023)\nhighlighted the superior performance of fine-tuned\nLLMs over smaller pre-trained models in zero-shot\nscenarios for tasks like dialogue and summariza-\ntion. It is worthwhile to note though, that story\ngeneration was not specifically examined in their\nexperiments.\nThis paper aims to address this research gap by\nconducting a comprehensive evaluation of auto-\nmatic story generation. Specifically, we compare\nthe performance of LLMs, with a particular focus\non GPT-3, using prompt-based learning, against\nSOTA models. We compare generated stories in\nterms of various automatic evaluation metrics from\nlexical and semantic matching ones to recently pro-\nposed model-based ones. We follow the best prac-\ntice in literature to conduct rigorous human evalua-\ntions including both crowdworkers from Amazon\nMechanical Turk and in-house judges, and assess\nstory quality at a fine-grained level, such as coher-\nence and logicality. To summarise, our contribu-\ntions are:\n• We conduct an empirical comparison between\nGPT-3 and other SOTA techniques for open-\nended story generation on three different cor-\npora that differ in style, register, and length.\n324\n• We test with a wide variety of automatic story\nevaluation metrics, and find that recent model-\nbased ones work better, consistent with the\nliterature.\n• We conducted experiments using two types\nof annotators: crowdworkers and in-house\njudges, to assess the quality of stories on var-\nious aspects. The results obtained from two\ngroups are consistent. We release this anno-\ntated resource as a testbed for developing new\nautomatic metrics in story generation tasks.1\n• Our experimental findings provide compre-\nhensive evidence that the stories produced by\nGPT-3 exhibit significant improvement com-\npared to SOTA techniques, and are compa-\nrable to stories authored by humans across\nvarious aspects.\n• We conduct a preliminary study on story pla-\ngiarism and find that GPT-3 tends to (soft)\n“plagiarise” real stories when generating news,\neven though it does not directly copy the\nsource text, raising further questions as to\nwhat extent GPT-3 recycles stories in its mem-\nory rather than generating new narratives.\n2 Related Work\nStory Generation See et al. (2019) find that fine-\ntuned GPT-2 can already generate stories with flu-\nent sentences, but more attentions are needed to\nincorporate commonsense and higher-level story\nplanning. Most works then use PLMs such as GPT-\n2 or BART as the backbone and incorporate higher\nlevel features to aid the generation process. Specif-\nically, Rashkin et al. (2020); Goldfarb-Tarrant et al.\n(2020); Tan et al. (2021) construct a storyline to\nguide the generation process. Guan et al. (2021a);\nYu et al. (2021); Xie et al. (2021) incorporate inter-\nsentence relationships such as coherence and dis-\ncourse relationships into the generation process.\nGuan et al. (2020); Peng et al. (2021) explore using\nexternal knowledge such as commonsense for story\ngeneration. Xu et al. (2020); Ammanabrolu et al.\n(2021) combine storyline planning and common-\nsense reasoning.\nThere are also studies which explore the use of\nGPT-3 for story generation. For example, Clark\net al. (2021) conducts a Turing test between GPT-\n3 generated and human-written stories and Lucy\n1https://github.com/ZhuohanX/TheNextChapter\nand Bamman (2021) probe for gender and repre-\nsentation bias in GPT-3 generated stories. These\nstudies, however, do not provide a systematic eval-\nuation that assesses GPT-3 against the SOTA story\ngeneration models.\nStory Evaluation Automatic story evaluation is\nadmittedly a challenging task, and the lack of stan-\ndardized evaluation metrics has somewhat impeded\nprogress of story generation (Guan et al., 2021b).\nHuman evaluation is usually considered as the gold\nstandard for story quality evaluation, but it is expen-\nsive and time-consuming (Guan and Huang, 2020)\nand it can not capture diversity (Hashimoto et al.,\n2017). Subsequently, several automated evaluation\nmetrics are introduced as alternative measures to\nevaluate the quality (the degree of readability) and\ndiversity (the extent of variation) of the generated\nstories. For quality, most metrics measure lexical\noverlap between strings (Papineni et al., 2002; Lin,\n2004; Tan et al., 2021) or semantic similarity by\ncomparing embedding of models (Zhao et al., 2019;\nZhang et al., 2020) between generated stories and\ntheir human references. Recently, learning (Sel-\nlam et al., 2020) and generation (Yuan et al., 2021)\nbased methods are explored and they are based on\npre-trained language models such as BERT (De-\nvlin et al., 2019) and BART. Nevertheless, these\nevaluation metrics are limited in that they provide\na single score to indicate the overall quality of the\nstory, and few metrics are specifically designed to\nassess specific aspects such as logicality (the adher-\nence to commonsense) or interestingness (the level\nof reader engagement) (Chhun et al., 2022).\n3 Experimental Setup\n3.1 Story Generation Models\nTo ensure a comprehensive comparison, we con-\nducted extensive experiments involving GPT-3 and\na wide range of SOTA story generation models.\nIn our experiments, we utilized the largest initial\nversion of GPT-3, namely text-davinci-001, which\nwas initially introduced in June 2020 and comprises\n175B parameters. It is perhaps worth noting that\nthis model was considered the most powerful at\nthe time of our experiment (March 2022), although\nsubsequent models like GPT-4 have since been re-\nleased, boasting even greater capabilities. As such,\nthe results we report here can be interpreted as a\n“lower bound” of LLM’s story generation perfor-\nmance. To adapt GPT-3 to the story domain without\n325\nexplicit fine-tuning, we employed a prompt-based\nlearning approach. We selected a small number of\nstories, typically 2 or 3, to serve as exemplars for\nGPT-3 in the target domain.\nFor SOTA story generation models, we use 1)\nknowledge enhanced based models: KGGPT2\n(Guan et al., 2020) and HINT (Guan et al., 2021a);\n2) storyline planning based model: PROGEN (Tan\net al., 2021); and 3) MTCL (Xu et al., 2020) that\ncombines both storyline planning and common-\nsense reasoning. We also fine-tune BART as an\nadditional baseline. For consistency, all models\nuse nucleus sampling (Holtzman et al., 2020) with\np = 0.95 as the decoding method. We summarise\nthese models in Table 1, and more details can be\nfound in Appendix A.\n3.2 Story Datasets\nThe most popular story dataset is ROCStories\n(ROC) (Mostafazadeh et al., 2016), which is com-\nposed of short commonsense stories and is used\nby most story generation works. There are also\nmore difficult and longer story datasets, such as\nWritingPrompts (WP) (Fan et al., 2018) and CNN\nNews (CNN) (Hermann et al., 2015) which are\ncomposed of fictional and news stories (two differ-\nent domains). In our experimental setup, we uti-\nlized all three datasets. The ROC dataset was used\nto evaluate the generation of short stories compris-\ning 5 sentences. The WP dataset was employed to\nassess medium-length stories, which were trimmed\ndown to 10 sentences. Lastly, the CNN dataset\nwas utilized to evaluate the generation of long sto-\nries, and each story have around 20 sentences. For\nfurther details about these datasets, please refer to\nAppendix B.\nWhenever possible we evaluate all models on\neach story dataset. However, this is sometimes\ninfeasible because some models are designed to\nwork on a particular dataset and thus cannot be\nadapted to other datasets easily. Moreover, we\nfocus on conditional story generation in this work,\nthis means there is some context upon which we\ngenerate the stories (details below).\nROC We evaluate all models in this dataset. The\ncontext we use to generate stories is the first sen-\ntence, and so the models are trained to generate the\nlast 4 sentences. Evaluation results are computed\nover 800 generated stories using randomly sampled\nleading sentences from the test partition.\nWP We assess HINT, PROGEN, GPT-3 and\nBART on this dataset. The context is a short para-\ngraph (“prompt”) that describes the idea of the\nstory. We randomly sample 1000 prompts from the\ntest partition for automatic evaluation.\nCNN We only run GPT-3, BART, PROGEN on\nCNN, as HINT is developed for ROC and WP orig-\ninally and it does not work well when applied to\nCNN. Stories of CNN are generated conditioned\non the news titles. We randomly sample 600 titles\nfrom the test partition for automatic evaluation.\n4 Automatic Evaluation\n4.1 Evaluation Metrics\nWe use two types of automatic evaluation metrics:\n1) reference-based metrics, where we compare the\ngenerated stories to human reference stories based\non the same conditioning context; and 2) reference-\nfree metrics, where we assess the quality of the\nstories directly.\n4.1.1 Reference-based Metrics\nMost reference-based metrics measure the lexi-\ncal or semantic closeness between generated sto-\nries and their human references. We experiment\nwith metrics based on string based matching (CBL,\nMSJ) and embedding based matching (BES) and a\nlearning based metric (BRT), to assess the quality\nof generated stories. We also use a recall based met-\nric (BBL) to assess the diversity of generated sto-\nries. Specifically, Corpus BLEU (CBL) computes\nthe average BLEU scores (Papineni et al., 2002) for\neach generated story against all human references\n(Caccia et al., 2020; Xie et al., 2021). MS-Jaccard\n(MSJ) measures lexical overlap by computing the\nn-gram overlap between generated and referenced\nstories using the Jaccard index (Alihosseini et al.,\n2019). BERTScore (BES) measures the maximum\nsimilarity of each token’s contextual embedding\nbetween generated and referenced stories (Zhang\net al., 2020). BLEURT (BRT) is trained on syn-\nthetic data to predict a similarity score between gen-\nerated and referenced stories (Sellam et al., 2020).\nBackward BLEU (BBL) computes the coverage\nof n-grams in the reference stories against the set\nof generated stories (Shi et al., 2018).2\n2We use BLEU4 for CBL and BBL; 4-grams overlap for\nMSJ; roberta-large model for BES; bert-base-128 for BRT.\n326\nModel Backbone Size Method Story Datasets\nGPT-3 text-davinci-\n001\n175B Prompt-based learning with several examples from the\nstory dataset (3 for ROC and WP and 2 for CNN)\nROC, WP, CNN\nKGGPT2 GPT-2 small 124M Fine-tuned on commonsense data before more fine-\ntuning with auxiliary classification tasks\nROC\nPROGEN BART large 400M Three-stage generation where at each stage a fine-tuned\nBART generates stories based on word importance in\nthe story datasets\nROC, WP, CNN\nMTCL GPT-2 small\nBERT large\n124M\n336M\n(1) a GPT-2 model to generate keywords; (2) a BERT\nmodel to rank retrieved knowledge triples; and (3) a\nsecond GPT-2 model that takes top-ranked knowledge\ntriples and context as input for story generation\nROC\nHINT BART base 140M BART is first fine-tuned on BookCorpus with additional\nobjectives to learn internal structure in a story and then\nfurther fine-tuned on the story datasets\nROC, WP\nBART BART large 400M Baseline model that is fine-tuned on the story datasets\nusing a standard language modelling objective\nROC, WP, CNN\nTable 1: The backbone (“Backbone”) of the story generation models and their number of parameters (“Size”).\n“Story Datasets” indicates which datasets are used to generate stories for a particular model. KGGPT2 and MTCL\nstories are obtained from the original authors; for PROGEN and HINT we re-run the implementation provided by\nthe authors.\n4.1.2 Reference-free Metrics\nReference-free metrics evaluate generated stories\nwithout comparing them to their human-authored\nreferences. We experiment with diversity metrics\nbased on intra-story (D-3, LR-n) and inter-story\ndiversity (SBL). We also compute negative log-\nlikelihood from BART of a story conditioned on\nthe context (BAS) for relatedness, and story length\nin terms of words (LEN) for complexity.\nSpecifically, Lexical Repetition (LR-n) com-\nputes the average percentage of 4-grams appearing\nat least n times in the generated stories (Shao et al.,\n2019). Distinct-3 (D-3) computes the average ratio\nof distinct 3-grams to all 3-grams (Li et al., 2016).\nSelf-BLEU (SBL) measures inter-story diversity\nthat computes the average BLEU score of each gen-\nerated story using all generated stories as reference\n(Zhu et al., 2018). BARTScore (BAS) computes\ngenerative likelihood of a story conditioned on the\ncontext (i.e., leading sentence for ROC, prompt\nfor WP and title for CNN) to measure the extent\nto which a generated story relates to its condition\n(Yuan et al., 2021).3 Length (LEN) measures the\naverage length of the generated stories, which is\nused as a rough indicator of generation complexity.\n3We setn = 3/8/8 for ROC, WP and CNN respectively and\nuse BLEU4 for SBL. We use the “PARA” version of BART\nand direction as “from source to hypothesis”.\n4.2 Results\nTable 2 and Table 3 present the reference-based and\nreference-free evaluation results, respectively. At a\nglance, these metrics do not appear to agree with\neach other even though some of them are designed\nto evaluate the same aspect (e.g., the best model in\nterms of fluency/coherence or diversity is different\ndepending on the metric). Overall, GPT-3 seems\nto have weaker performance than most of other\nmodels in terms of quality (CBL and MSJ) and\ndiversity (BBL, SBL, D-3 and LR-n) metrics.\nHowever, when we look at recent model-based\nmetrics (BERTScore, BLEURT and BARTScore),\nGPT-3 appears to be a much better model (a finding\nwe will return to when we look at human evaluation\nresults). Interestingly, we notice that human writ-\nten stories have very poor performance in terms of\nBARTScore (BAS). We suspect BARTScore may\nexhibit a bias towards machine-generated stories,\nas the metric primarily evaluates quality based on\nthe generative likelihood of a sequence. Machine-\ngenerated stories are specifically designed to maxi-\nmize this likelihood, while human-authored stories\noften incorporate distinct elements, such as sur-\nprising or creative word choices (Holtzman et al.,\n2020). In general, all models are capable of gen-\nerating stories of the appropriate length, except\nfor GPT-3 in the CNN dataset. GPT-3 in the\nCNN dataset exhibits difficulty in generating sto-\nries longer than 150 words, whereas human-written\n327\nModel\nFlu./Coh. Div.\nCBL MSJ BES BRT BBL\n⇑ ⇑ ⇑ ⇓ ⇑\nROC\nGPT-3 27.2 11.6 86.6 8.6 24.0\nKGGPT2 33.5 15.0 87.0 9.5 25.6\nPROGEN3 26.6 14.6 86.7 9.7 25.0\nMTCL 31.4 14.2 86.9 9.7 24.0\nHINT 39.6 13.7 87.0 8.6 24.6\nBART 27.5 14.7 86.8 9.5 25.1\nWP\nGPT-3 28.6 12.3 81.6 11.7 24.4\nPROGEN3 32.3 16.4 81.4 13.3 27.6\nHINT 45.5 12.8 80.8 12.1 23.7\nBART 32.6 16.2 81.4 13.0 27.2\nCNN\nGPT-3 33.2 11.0 83.5 7.5 19.8\nPROGEN3 29.6 14.8 82.2 9.3 26.2\nBART 29.1 14.7 82.2 9.8 25.7\nTable 2: Reference-based Evaluation Results. CBL,\nMSJ, BES and BRT evaluate the closeness between the\ngenerated stories and the whole test reference data as\nan indicator of general fluency (Flu.) and coherence\n(Coh.). BBL focus on the recall of generated stories as\nan indicator of diversity (Div.). ⇑: higher is better; ⇓:\nlower is better. BRT values are negated here.\nstories typically consist of around 500 words on\naverage. Considering the overall assessment using\nvarious automatic metrics, there is no single winner\nthat consistently outperforms other models.\n5 Human Evaluation\nTo obtain a comprehensive assessment of the gen-\nerated stories, we recruit human annotators to eval-\nuate their quality. In order to gain insights into con-\nsistency, we employed both crowdsourced workers\nand in-house annotators. This approach allows us\nto gather diverse perspectives and obtain a more\nnuanced understanding of the story quality.\n5.1 Crowdsource Annotation\nWe first collect human judgements using the Ama-\nzon Mechanical Turk (AMT) platform.4 Following\nthe approach suggested by Karpinska et al. (2021),\nwe assessed four aspects, namely fluency, coher-\nence, relatedness, and interestingness. Addition-\nally, we introduced a new aspect called logicality,\nwhich assesses the extent to which the story com-\nplies with commonsense. Each of these five as-\npects is evaluated on an ordinal scale ranging from\n1 (worst) to 5 (best). We randomly sample 20 con-\nditional contexts (e.g., titles) from each dataset and\ncollect stories generated by all models for human\nevaluation. Each story (including human-written\n4https://requester.mturk.com/\nModel\nDiv. Rel. Com.\nSBL D-3 LR-n BAS LEN\n⇓ ⇑ ⇓ ⇓ ⇑\nROC\nGPT-3 38.5 67.7 39.1 4.2 47.3\nKGGPT2 41.9 67.2 51.9 4.6 38.4\nPROGEN3 30.0 76.9 39.5 5.0 40.9\nMTCL 39.4 69.6 44.4 4.9 49.7\nHINT 55.1 54.3 68.1 4.3 35.8\nBART 30.5 77.4 37.8 5.0 40.6\nhuman 33.1 80.2 35.8 5.2 40.3\nWP\nGPT-3 37.5 69.6 9.7 4.3 120.6\nPROGEN3 35.2 77.2 2.6 5.4 136.9\nHINT 64.1 33.9 67.4 4.1 119.0\nBART 35.3 77.5 1.6 5.4 129.2\nhuman 27.1 83.7 1.5 5.7 150.0\nCNN\nGPT-3 26.5 82.9 9.8 4.4 147.3\nPROGEN3 28.9 82.3 2.3 5.2 395.8\nBART 27.9 83.2 0.8 5.2 374.1\nhuman 27.3 83.8 6.3 5.4 498.6\nTable 3: Reference-free Evaluation Results. SBL mea-\nsures inter-story diversity by assessing differences be-\ntween different stories while D-3 and LR-n (3 for ROC,\n8 for WP and CNN) focus on repetition n-grams within\nthe same story. We also include LEN as an indicator\nof story complexity (Com.). We compute BAS of story\ngiven condition for story relatedness (Rel.).\none) is judged by 3 annotators, and so we have\nannotations for 320 stories in total (140/100/80 for\nROC, WP and CNN, respectively). Amazon Quali-\nfication requirements on AMT and question details\ncan be found in Appendix C. Quality control de-\ntails can be found in Appendix D.\nTable 4 presents the human evaluation results.\nOverall, GPT-3 generates stories that are consis-\ntently of higher quality than those generated by\nother SOTA models. To understand whether the\ndifference is significant, we perform a paired t-test\nby comparing GPT-3 to other models (including\nhuman) and find that in most cases these results are\nsignificant with p-value < 0.05 (‘*’ in the table).\nCompared with human authors, GPT-3 appears\nthat it is generating stories that are just as good\nas (ROC) or better than (WP and CNN) human au-\nthors, confirming the findings of Clark et al. (2021).\nFor WP, in particular, human stories are trimmed to\nthe first 10 sentences (data pre-processing for train-\ning the story generation models). This abruptly\nshortens the stories so they might not provide a\nproper conclusion, and inevitably are penalised\n(see examples in Appendix K). For CNN, GPT-\n3 appears to be “plagiarising” real stories, where\nmany story elements are not a product of creative\ngeneration but details copied from real news stories\n328\nModel Flu. Coh. Rel. Log. Int.\nROC\nGPT-3 4.40 4.43 4 .37 4 .37 3 .57\nKGGPT2 3.90∗ 3.48∗ 3.53∗ 3.00∗ 2.62∗\nPROGEN3 3.88∗ 3.45∗ 3.37∗ 2.95∗ 2.57∗\nMTCL 3.55∗ 3.12∗ 3.18∗ 2.73∗ 2.42∗\nHINT 3.90∗ 3.27∗ 3.33∗ 3.12∗ 2.58∗\nBART 3.92∗ 3.38∗ 3.48∗ 3.03∗ 2.60∗\nhuman 4.22 4.58 4 .42 4 .48 3 .77\nWP\nGPT-3 4.37 4.67 4.28 4.48 3.47\nPROGEN3 3.45∗ 3.08∗ 2.35∗ 2.57∗ 1.98∗\nHINT 3.32∗ 2.63∗ 2.02∗ 2.25∗ 1.77∗\nBART 3.42∗ 2.73∗ 2.08∗ 2.27∗ 1.87∗\nhuman 4.13∗ 4.22∗ 3.05∗ 3.75∗ 2.97∗\nCNN\nGPT-3 4.22 4 .52 4 .58 4 .60 3.20\nPROGEN3 3.63∗ 3.32∗ 3.30∗ 3.22∗ 2.28∗\nBART 3.58∗ 3.37∗ 3.30∗ 3.27∗ 2.17∗\nhuman 4.10 4.10∗ 4.23∗ 4.18∗ 3.72∗\nTable 4: Crowdsource Human Evaluation Results. We\ncalculate the average score of models for each aspect:\nfluency (Flu.), coherence (Coh.), relatedness (Rel.), log-\nicality (Log.) and interestingness (Int.). Model scores\nthat are marked with ∗ indicate the performance differ-\nence between the model and GPT-3 is significant.\n(Section 6). Another reason could be that GPT-3\nstories are much shorter than those generated by\nother models and human authors (150 vs. 300-400\nwords; Table 3), which makes them easier to read\nand thus leads to better scores. Note that this is a\ndownside of GPT-3 where it is difficult to get it to\ngenerate long stories (Section 7).\nWhen considering the various aspects for SOTA\nmodels, including KGGPT2, PROGEN3, MTCL,\nHINT, and BART, these models exhibit strong per-\nformance in terms of fluency, with scores consis-\ntently exceeding 3.5 in most cases. This indicates\nthat the models can generate sentences that are nat-\nural and fluent. However, coherence performance\ndiffers depending on the dataset. Most models per-\nform well on the ROC and CNN datasets, while\nthey tend to struggle on WP, with coherence scores\nfalling below 3.1. The observation that these mod-\nels struggle with shorter WP stories compared to\nlonger CNN stories might be because the PLMs\nthat they are built on are mostly trained on web\ndata which contains plenty of news articles. For\nrelatedness, logicality and interestingness, we see\na similar trend where the models perform best in\nROC and worst in WP. We also observe a consis-\ntent decrease in performance from relatedness to\nlogicality and interestingness, suggesting that the\nmodels particularly struggle to generate interesting\nand sensible stories. Interestingness is perhaps the\nmost difficult aspect to optimise, as it is difficult to\nModel Flu. Coh. Rel. Log. Int.\nROC\nGPT-3 4.78 4 .73 4 .50 4 .82 3 .37\nKGGPT2 4.52∗ 3.67∗ 3.57∗ 3.47∗ 2.50∗\nPROGEN3 4.27∗ 3.47∗ 3.78∗ 3.23∗ 2.48∗\nMTCL 4.27∗ 3.27∗ 3.45∗ 3.15∗ 2.37∗\nHINT 4.38∗ 4.03∗ 3.38∗ 3.70∗ 2.38∗\nBART 4.37∗ 3.95∗ 3.85∗ 3.53∗ 2.70∗\nhuman 4.52∗ 4.38∗ 4.22 4 .32∗ 3.18\nWP\nGPT-3 4.57 4.65 4.08 4.22 3.82\nPROGEN3 3.55∗ 3.03∗ 2.23∗ 2.57∗ 2.45∗\nHINT 3.60∗ 2.72∗ 2.07∗ 2.68∗ 2.08∗\nBART 3.45∗ 2.77∗ 2.08∗ 2.38∗ 2.30∗\nhuman 4.05∗ 4.07∗ 3.73 3 .87∗ 3.78\nCNN\nGPT-3 4.50 4 .33 4 .48 4 .40 3 .45\nPROGEN3 3.80∗ 3.45∗ 3.63∗ 3.45∗ 2.52∗\nBART 3.73∗ 3.25∗ 3.58∗ 3.32∗ 2.57∗\nhuman 4.22∗ 4.00∗ 4.35 4 .13∗ 3.22\nTable 5: In-house Human Evaluation Results.\ndefine what makes a narrative interesting.\n5.2 In-house Annotation\nWe next recruit university volunteers to collect in-\nhouse judgements.5 We ask them to evaluate the\nsame 5 aspects using the same scale. We sample\n20 disjoint conditional contexts from each dataset\nfor story generation here, as we are interested to\ntest the robustness of our previous findings (with\ndifferent workers and set of stories). As with crowd-\nsource annotation, each story is also judged by 3\nannotators. Details of the agreement between an-\nnotators can be found in Appendix G.\nTable 5 presents the scores of story quality from\nin-house annotators. Interestingly, the magnitude\nof the in-house scores are generally somewhat\nhigher than the crowdworker scores (across all met-\nrics and datasets and models). We hypothesise\nthat this may be because our in-house workers are\nmore “tolerant” to mistakes as they have been ex-\nposed to machine-generated text more compared to\ncrowdworkers. That said, the overall findings are\nconsistent between the two groups of annotators: 1)\nGPT-3 is the best story generation model and out-\nperforms both SOTA models and human stories; 2)\nThe SOTA models do well in fluency, but poorly in\nmost other aspects (interestingness worst); and 3)\nThe SOTA models face notable challenges specifi-\ncally in WP, as evidenced by their poor coherence,\nrelatedness, logicality scores compared to other\ndomains.\nWhen comparing the results of automatic met-\n5Demographically, 14 are PhD students and 1 is university\nstaff; all of them are proficient in English.\n329\nrics (Section 4.2) to the human evaluation results, a\nnotable discrepancy emerges, leading to a different\nconclusion regarding the performance of GPT-3\nand the identification of a clear “best” story gener-\nation model. That said, if we consider only model-\nbased metrics such as BERTScore, BLEURT for\nfluency/coherence, and BARTScore for relatedness,\na more aligned conclusion can be drawn, suggest-\ning these metrics may be more reliable (though the\ntrend is still less conclusive compared to human\nevaluation results). This observation is consistent\nwith recent literature, which highlights the better\ncorrelation of modern model-based metrics with hu-\nman evaluations (Chiang and Lee, 2023; Ke et al.,\n2023; Xie et al., 2023).\n6 Plagiarism\nConsidering the strong performance of GPT-3 on\nstory generation, we next provide a preliminary in-\nvestigation to understand the extent to which GPT-3\ncopies from its training data.\nIThenticate We use iThenticate 6 — a profes-\nsional plagiarism detection software that has com-\nprehensive coverage over online articles — to as-\nsess how much GPT-3 plagiarises. We include only\nthe generated content (without the condition) when\nchecking for plagiarism. Results show that there is\nno strong plagiarism: similarity score for ROC, WP\nand CNN is 4%, 3% and 14% respectively. This is\nconsistent with findings from McCoy et al. (2021)\nthat the language models do not simply memorize,\nbut combine familiar parts into novel way. How-\never, iThenticate looks for lexical overlap to iden-\ntify plagiarism. A more subtle form of plagiarism\nis one that copies the ideas without parroting the\nwords (Lee et al., 2022), motivating us to perform\nmanual check.\nManual Check We randomly sample 10 GPT-3\ngenerated stories for WP and CNN respectively\nand manually check whether they copy from web\narticles by searching for keywords, named entities\nand long phrases. For WP, in most instances we\ndo not find web articles that resemble our stories,\nexcept for one instance where it copies a story from\nWikipedia (noting that it is not a direct word-to-\nword copy). For CNN, we find 7 out of 10 instances\nare based on real stories from news articles, where\nmost details (e.g., names, incident origins, sentence\npenalty) are faithful to the original stories. These\n6https://www.ithenticate.com\nresults highlight that GPT-3 can recall details from\ntraining data, calling into question how much of its\ngeneration is creative. More details can be found\nin Appendix M.\n7 Discussion\nIn this paper, we only experiment with a simple\nprompt-based learning approach to adapt GPT-3 to\nthe different story datasets and one can argue more\nprompt engineering can be explored (Liu et al.,\n2022; Lu et al., 2022; Mishra et al., 2022). We con-\ntend, however, that our goal here is not to propose a\nbetter prompt-based method to do story generation\nwith GPT-3, and ultimately even with our simple\napproach we find that GPT-3 generates high qual-\nity stories, suggesting that with additional prompt\nengineering it can do better. We next discuss qual-\nitatively some of the strengths and drawbacks of\nGPT-3 in story generation.\n7.1 Strengths\nUnlike other conditional generation tasks such as\nmachine translation or summarisation, where the\ninput context is rich in information and the goal is\nto translate or compress the input information, story\ngeneration works in the reverse manner where the\nmodel needs to “hallucinate” new information and\ndetails given a succinct context. This means that\nin order to do the task well, having strong world\nknowledge is important. Reading some of the GPT-\n3 stories, we observe GPT-3’s advantage in this,\nparticularly in the WP dataset where some of the\nprompts require niche knowledge about characters.\nIn Table 6 we show an example in WP where the\nprompt is cthulhu calls your cell , he needs to be\npicked up, where cthulhu a fictional cosmic entity,\nand only GPT-3 is able to produce a coherent story\nand the SOTA models struggle.\n7.2 Drawbacks\nEven though GPT-3 demonstrates excellent gen-\nerative capability and outperforms SOTA models\nsignificantly, we still find GPT-3 has many genera-\ntion errors that can be improved.\nStory length GPT-3 has a parameter to control\nthe maximum number of generated tokens but does\nnot provides a way to control the minimum number\nof tokens. As one can see from Table 3, GPT-3\ncan not generate stories longer than 150 words for\nCNN, even though the prompts have long stories.\nWe also attempted to encourage longer stories by\n330\nModel Story\nGPT-3 “ yes , lord cthulhu ? ” you answer the phone . “ i need you to pick me up from the airport . ” “ of course ,\nmy lord . when will you be arriving ? ” “ in about an hour . i ’ll text you the flight number . ” you hang up\nand start getting ready . you know cthulhu is a demanding deity , but you do n’t mind . you enjoy serving\nhim .\nPROGEN3 **but everything ’s different ! i miss my child in there ...** *what are you talking about ! you have to be\nmad . you must have misheard ! ! you come and get your mommy ... ” momma always grabbed him before\nthey climbed into the bathtub . after that happened they were always late . * bam . * wait , it ’s never like\nthis . ctlulhu always puts up a fight . i repeat : it ’s never like this ! it ’s always for the umpteenth time , now\nor never . *the phone rings . i answer it and ctlulhu ’s voice booms out .\nHINT “ sir , i ’m afraid i ’m late for work. “ “ i ’m sorry sir , “ the technician said , “ but i have to get my cell phone\nback. “ “ sir , you have to pick up this cell. “ “ yes sir , i do. “ “ and how are you going to find it ? “ “ i ’m\njust going to have to pick it up. “ “ well , sir , “ the technician said , “ it ’s just that i do n’t have the time to\npick it up. “ “ why not ? “ “ sir , “ the technician said , “ i do n’t have the time. “ “ i ’m just going to make\nsure it ’s picked up. “\nBART “ ah it ’ s here . i thought you wanted to leave , a phone call . * ” my coworkers looked at me confused .\n“ i wasn ’ t in the mood to go running around scared . but it was important . you need to pick up on your\ncravings . the kid-ta-bye , gabriel , payliver , cilia de los angeles . and four people ’ s fault. ” he was right\nabout the fifth , at least my attention was fixed on it . three girls were outside screaming in the street , and\na family of four was picking up one of them . the two men yelled and approached as they walked to the\nmailbox , visibly distressed .\nTable 6: Stories generated by GPT-3, HINT, BART and PROGEN3 given the same title “cthulhu calls your cell , he\nneeds to be picked up .”\nadding specific instructions as part of the prompt\nof GPT-3, but this did not work.\nNull generation Occasionally GPT-3 decides to\ngenerate no output. This is usually not an issue,\nsince this can be solved by forcing it to generate\nagain, although it is unclear why this occurs.\nDirect copy Besides the soft plagiarism issue\n(Section 6), GPT-3 does occasionally copy long\nchunks of text, e.g., the title or prompt in the story.\nMultilingual GPT-3 sometimes generates sto-\nries in languages other than English, despite the\ngiven prompts always being in English. In terms of\nstatistics, out of 1000 generations we find 14 non-\nEnglish stories (5 Chinese, 4 German, 1 Japanese,\n1 French, 1 Russian, 1 Norwegian Nynorsk and 1\nmixture of Chinese and English). Interestingly, in\nmost of these cases the stories are related to the\ncondition (even though in different languages) al-\nthough sometimes we observe the outputs are direct\ntranslation of the prompt and not a creative story.\nTokenisation issue GPT-3 generations occasion-\nally feature “sticky” words where there are missing\nwhite spaces (e.g., understand.With and timewhen).\nWe suspect this is due to Byte-Pair Encoding of\nGPT-3 where white spaces are “glued” to each sub-\nword and so every subword has two versions (one\nwith the white space and one without). This is-\nsue arises when GPT-3 generates using a subword\nwithout the white space suffix.\nExpletives GPT-3 would occasionally generate\nstories with expletives. Interestingly, it would\nsometimes self-censor them (e.g., b****).\n8 Conclusion\nWe present an extensive comparison of GPT-3 with\nSOTA models for story generation, and found that\nstories generated by GPT-3 are substantially better\nthan SOTA models on multiple aspects and even\nrival human authors. The findings of this study\nindicate that we have entered a new chapter in\nstory generation with LLMs. Future research is\nlikely to concentrate on prompt-engineering LLMs\nto achieve enhanced customization, such as varying\ntheir style and length, further advancing the capa-\nbilities of story generation models. In terms of eval-\nuation metrics, our work: 1) reveals a weak corre-\nlation between automatic lexical-based evaluation\nmetrics and human evaluation, and that recently\nproposed model-based metrics appear to more reli-\nable; and 2) contributes a new test bed for metric\ndevelopment, through the release of a dataset that\ncontains story quality annotations by two groups of\njudges. In spite of the positive results of GPT-3 in\nstory generation, we discuss some of its issues, the\nchief one being that it has a tendency to reproduce\ndetails or plots from its memories, raising founda-\ntional questions about its generation creativity.\n331\nLimitations\nAs observed by Mishra et al. (2022), engineering\nappropriate prompts can significantly influence the\nperformance of language models. In our current\nstudy, we randomly sample a few training examples\nas demonstrations for GPT-3 (in-context learning).\nHowever, a more effective approach could involve\nstrategically selecting contextually more relevant\nexamples.\nAlthough text-davinci-001 was the best model at\nthe time of our experiment, recent advancements in\nthe field have led to the release of more powerful\nLLMs. Despite these improved models, we hold\nthe view that they are unlikely to substantially alter\nthe conclusions drawn in this study. The findings\nstrongly suggest that LLMs will remain the domi-\nnant approach in story generation in the foreseeable\nfuture. Also, we only explore with GPT-3 in our\nexperiments, and although we think our findings\nare likely to generalise to other LLMs, this has not\nbeen empirirically validated.\nSince we started this work in 2022, there has\nbeen quite a development in terms of text genera-\ntion evaluation metrics (Chiang and Lee, 2023; Ke\net al., 2023; Xie et al., 2023; Fu et al., 2023; Liu\net al., 2023a), and some of these uses LLMs them-\nselves. Although we claim that human evaluation\nremains the gold standard for story generation, it\nremains to be seen how much these new metrics\nclose the gap. We foresee that the question of circu-\nlarity, i.e. using LLMs to evaluate LLM-generated\ntext, will be the next challenge that the field needs\nto address.\nIn our work, we acknowledge that we did not in-\nvolve domain experts (e.g., story writers) for a more\nspecialized assessment. It would be intriguing to\ninvestigate the potential variations in judgments\nbetween lay individuals and expert evaluators in\nstory assessment (Chiang and Lee, 2023). Recent\nresearch has indicated that certain practices in the\nhuman annotation process, such as the use of Lik-\nert scales, have limitations in capturing the true\npreferences of humans (Ethayarajh and Jurafsky,\n2022; Liu et al., 2023c). We contend, however, that\nthe fact that we found consistent results between\ntwo different groups of annotators suggest that our\nfindings are likely to be robust.\nEthics Statement\nAll mechanical turk experiments conducted in this\npaper were approved by internal ethics review\nboard from our institution. (Ethics ID Number:\n21961). Our evaluators were paid based on an es-\ntimated US$14.83 per hour rate. For each dataset,\nwe estimate the time they would spend and vary\nthe payment according to the estimated time. Each\nHIT contains 7 stories (5 stories to be evaluated\nand 2 controlled stories to control the evaluation\nquality on AMT). We pay US$2.50 per HIT for\nROC, US$3.50 for WP and US$4.50 for CNN.\nWe remind the workers in our consent form that\nthe potential risks about this work, which they\nmight have to read and evaluate stories with filthy\nwords or offended storyline and they are welcome\nto quit the task and we will still pay them according\nto the efforts they spend.\nAcknowledgements\nWe extend our thanks to the reviewers for their\nvaluable feedback, which has greatly contributed to\nthe improvement of this work. We also appreciate\nthe efforts of our online annotators and colleagues\nfor their helpful annotation contributions. Zhuohan\nXie is supported by Melbourne Research Scholar-\nship, and would like to expresses his sincere appre-\nciation to the program.\nReferences\nDanial Alihosseini, Ehsan Montahaei, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diversity\nand quality in text generation models. InProceedings\nof the Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation, pages 90–98,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O. Riedl. 2021. Automated sto-\nrytelling via causal, commonsense plot ordering. In\nThirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 5859–5867. AAAI\nPress.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n332\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin. 2020.\nLanguage gans falling short. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nCyril Chhun, Pierre Colombo, Fabian M. Suchanek, and\nChloé Clavel. 2022. Of human criteria and automatic\nmetrics: A benchmark of the evaluation of story gen-\neration. In Proceedings of the 29th International\nConference on Computational Linguistics, COLING\n2022, Gyeongju, Republic of Korea, October 12-17,\n2022, pages 5794–5836. International Committee on\nComputational Linguistics.\nDavid Cheng-Han Chiang and Hung-yi Lee. 2023. Can\nlarge language models be an alternative to human\nevaluations? In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023, pages 15607–15631. Asso-\nciation for Computational Linguistics.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7282–7296, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKawin Ethayarajh and Dan Jurafsky. 2022. The authen-\nticity gap in human evaluation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022, pages\n6056–6070. Association for Computational Linguis-\ntics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. CoRR,\nabs/2302.04166.\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 4319–4338, Online. Association for\nComputational Linguistics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nJian Guan and Minlie Huang. 2020. UNION: An Un-\nreferenced Metric for Evaluating Open-ended Story\nGeneration. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 9157–9166, Online. Association\nfor Computational Linguistics.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021a. Long text\ngeneration by modeling sentence-level and discourse-\nlevel coherence. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6379–6393, Online. Association for\nComputational Linguistics.\nJian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wen-\nbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie\nHuang. 2021b. OpenMEV A: A benchmark for eval-\nuating open-ended story generation metrics. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 6394–6407,\nOnline. Association for Computational Linguistics.\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-\nruoka, and Richard Socher. 2017. A joint many-task\nmodel: Growing a neural network for multiple NLP\ntasks. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1923–1933, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nKarl Moritz Hermann, Tomás Kociský, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Information\nProcessing Systems 28: Annual Conference on Neu-\nral Information Processing Systems 2015, December\n7-12, 2015, Montreal, Quebec, Canada, pages 1693–\n1701.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\n333\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nMarzena Karpinska, Nader Akoury, and Mohit Iyyer.\n2021. The perils of using Mechanical Turk to evalu-\nate open-ended text generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1265–1285, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nPei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xi-\naoyan Zhu, and Minlie Huang. 2023. DecompEval:\nEvaluating generated texts as unsupervised decom-\nposed question answering. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n9676–9691, Toronto, Canada. Association for Com-\nputational Linguistics.\nJey Han Lau, Carlos Armendariz, Shalom Lappin,\nMatthew Purver, and Chang Shu. 2020. How fu-\nriously can colorless green ideas sleep? sentence\nacceptability in context. Transactions of the Associa-\ntion for Computational Linguistics, 8:296–310.\nJooyoung Lee, Thai Le, Jinghui Chen, and Dongwon\nLee. 2022. Do language models plagiarize? CoRR,\nabs/2203.07618.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nCoRR, abs/2107.13586.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023a. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment. CoRR, abs/2303.16634.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong\nLi, Mengshen He, Zhengliang Liu, Zihao Wu,\nDajiang Zhu, Xiang Li, Ning Qiang, Dinggang\nShen, Tianming Liu, and Bao Ge. 2023b. Sum-\nmary of chatgpt/gpt-4 research and perspective to-\nwards the future of large language models. CoRR,\nabs/2304.01852.\nYixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun\nZhao, Linyong Nan, Ruilin Han, Simeng Han,\nShafiq Joty, Chien-Sheng Wu, Caiming Xiong, and\nDragomir Radev. 2023c. Revisiting the gold stan-\ndard: Grounding summarization evaluation with ro-\nbust human evaluation. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023,\nToronto, Canada, July 9-14, 2023, pages 4140–4170.\nAssociation for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLi Lucy and David Bamman. 2021. Gender and rep-\nresentation bias in GPT-3 generated stories. In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nR. Thomas McCoy, Paul Smolensky, Tal Linzen, Jian-\nfeng Gao, and Asli Celikyilmaz. 2021. How much\ndo language models copy from their training data?\nevaluating linguistic novelty in text generation using\nRA VEN.CoRR, abs/2111.09509.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2022. Reframing\ninstructional prompts to GPTk’s language. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, pages 589–612, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nNasrin Mostafazadeh, Lucy Vanderwende, Wen-tau Yih,\nPushmeet Kohli, and James Allen. 2016. Story cloze\nevaluator: Vector space representation evaluation by\npredicting what happens next. In Proceedings of\nthe 1st Workshop on Evaluating Vector-Space Repre-\nsentations for NLP, pages 24–29, Berlin, Germany.\nAssociation for Computational Linguistics.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\n334\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nXiangyu Peng, Siyan Li, Sarah Wiegreffe, and Mark O.\nRiedl. 2021. Inferring the reader: Guiding auto-\nmated story generation with commonsense reasoning.\nCoRR, abs/2105.01311.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? CoRR, abs/2302.06476.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nHannah Rashkin, Asli Celikyilmaz, Yejin Choi, and\nJianfeng Gao. 2020. PlotMachines: Outline-\nconditioned generation with dynamic plot state track-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 4274–4295, Online. Association\nfor Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nATOMIC: an atlas of machine commonsense for\nif-then reasoning. In The Thirty-Third AAAI Con-\nference on Artificial Intelligence, AAAI 2019, The\nThirty-First Innovative Applications of Artificial In-\ntelligence Conference, IAAI 2019, The Ninth AAAI\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA,\nJanuary 27 - February 1, 2019 , pages 3027–3035.\nAAAI Press.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 843–861, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu,\nand Xiaoyan Zhu. 2019. Long and diverse text gen-\neration with planning-based hierarchical variational\nmodel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3257–3268, Hong Kong, China. Association for Com-\nputational Linguistics.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Toward diverse text generation with in-\nverse reinforcement learning. In Proceedings of the\nTwenty-Seventh International Joint Conference on\nArtificial Intelligence, IJCAI 2018, July 13-19, 2018,\nStockholm, Sweden, pages 4361–4367. ijcai.org.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\nCoRR, abs/1909.08053.\nRobyn Speer and Catherine Havasi. 2012. Representing\ngeneral relational knowledge in ConceptNet 5. In\nProceedings of the Eighth International Conference\non Language Resources and Evaluation (LREC’12),\npages 3679–3686, Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA).\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric\nXing, and Zhiting Hu. 2021. Progressive generation\nof long text with pretrained language models. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4313–4324, Online. Association for Computational\nLinguistics.\nZhuohan Xie, Jey Han Lau, and Trevor Cohn. 2021. Ex-\nploring story generation with multi-task objectives in\nvariational autoencoders. In Proceedings of the The\n19th Annual Workshop of the Australasian Language\nTechnology Association, pages 97–106, Online. Aus-\ntralasian Language Technology Association.\nZhuohan Xie, Miao Li, Trevor Cohn, and Jey Han Lau.\n2023. Deltascore: Evaluating story generation with\ndifferentiating perturbations. CoRR, abs/2303.08991.\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul\nPuri, Pascale Fung, Anima Anandkumar, and Bryan\nCatanzaro. 2020. MEGATRON-CNTRL: Control-\nlable story generation with external knowledge using\nlarge-scale language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2831–2845,\nOnline. Association for Computational Linguistics.\nWenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo,\nand Meng Jiang. 2021. Sentence-permuted para-\ngraph generation. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5051–5062, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems 34: Annual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, De-\ncember 6-14, 2021, virtual, pages 27263–27277.\n335\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\nSIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,\npages 1097–1100. ACM.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\n336\nA SOTA Story Models Details\nKnowledge Enhanced GPT-2 (KGGPT2) Guan\net al. (2020) use heuristic rules to translate com-\nmonsense triples from commonsense knowledge\nbases (e.g., ConceptNet (Speer and Havasi, 2012)\nand ATOMIC (Sap et al., 2019)) into natural lan-\nguage sentences and fine-tune GPT-2 small using\nthese sentences. They also use rules to construct\nnegative samples from the original stories to cre-\nate “bad stories” and perform additional training to\nencourage the model to learn representations that\ncan distinguish the original and negative stories on\nROC.\nProgressive Generation of Long Text (PRO-\nGEN) Tan et al. (2021) divide the story genera-\ntion process into multiple stages where words are\ngenerated based on their order of importance (es-\ntimated using TF-IDF). In other words, PROGEN\ndoes not generate stories in a left to right man-\nner. They fine-tune BART-large in different stages\nwhere the early stages focus on generated keywords\nand the intermediate stages focus on generating the\nnext set of content words. We use PROGEN3 in\nour experiment which has 3 stages where it gener-\nates 15%/25%/100% of the story words after each\npass.\nMEGATRON-CNTRL (MTCL) Xu et al.\n(2020) combines commonsense reasoning and sto-\nryline planning. They first train a keyword predic-\ntor with GPT-2 and the predicted keywords are\nused to retrieve related knowledge triples from\na knowledge base. They then train a contextual\nknowledge ranker with BERT to rank the top- N\npredicted knowledge triples. A second GPT-2 is\ntrained as a conditional generator that takes both\ntop ranked knowledge triples and other condition-\ning (e.g., titles) as input when generating stories.\nNote that the parameters of the two GPT-2 and\nBERT models are initialised using MEGATRON\nparameters (Shoeybi et al., 2019).\nHigh-Level Representations for Long Text Gen-\neration (HINT) Guan et al. (2021a) pre-train\nBART-base on BookCorpus (Zhu et al., 2015) with\nadditional objectives that capture sentence-level\nsimilarity and sentence-order to learn the internal\nstructure within a story. The model is then further\nfine-tuned on story datasets to generate stories in a\nparticular dataset.\nBART This is a baseline model where we fine-\ntune BART-large on the story datasets with the\nstandard next word prediction objective.\nB Datasets Details\nROCStories (ROC) ROC was developed by\nMostafazadeh et al. (2016) and it contains 98K\ncommonsense stories of five sentences. To\nobtain a more generalised lexicon, we follow\nthe delexicalisation process from prior studies\n(Guan et al., 2020; Xu et al., 2020) where\nmale/female/unknown names are replaced by\n[MALE]/[FEMALE]/[NEUTRAL] sentinels. For\neach story, the first (leading) sentence is used as\nconditioning context, and models are trained to\ngenerate the remaining 4 sentences.\nWritingPrompts (WP) WP consists of 303K\nhuman-written stories mined from Reddit’s Writ-\ning Prompts forum Fan et al. (2018).7 Each story\nis trimmed to contain only the first 10 sentences\n(following Guan et al. (2021a)). For WP, we use\nthe prompt (which is typically a paragraph of text\nthat sets the scene of the story) as conditioning for\nstory generation.\nCNN News (CNN) CNN News (Hermann et al.,\n2015) is a dataset that contains long news articles\nwith titles. CNN is a very large dataset, with 311K\nnews articles and highlights. We sub-sample the\nstandard training, validation and testing splits to\nproduce splits with 10K/5K/1K stories each, re-\nspectively, for our experiments. The title of a news\nstory is used as conditioning for story generation.\nC Amazon Mechanic Turk Setting\nQualification Requirements We set following\nqualification requirements for our annotators: 1)\nTheir accept rate is greater than or equal to 97%. 2)\nTheir location is in US. 3) They have to complete\nmore than 1000 HITs.\nQuestions We ask the following questions in our\nquestionnaire.\n1. Fluency: “How grammatically correct is the\ntext of the story?”\n2. Coherence: “How well do the sentences in the\nstory fit together?”\n3. Relatedness: “How relevant is the story to the\ntitle?”\n7https://www.reddit.com/r/WritingPrompts/\n337\n4. Logicality: “How much does the story obey\ncommonsense?”\n5. Interestingness: “How enjoyable do you find\nthe story?”\nD Amazon Mechanic Turk Pilot Study\nWhile AMT is convenient to find workers for anno-\ntation work, it can be rather difficult to obtain reli-\nable workers (Karpinska et al., 2021; Clark et al.,\n2021). One of our workers told us many work-\ners install website plugins to help them to manage\nthe workflow with AMT so that they can hoard\nmany HITs at the same time. Therefore, HITs\nwith high payment can easily attract irresponsible\nworkers even though previous qualifications are set\nsince most AMT requesters will not bother to reject\nwork.\nTherefore, we set a pilot study to aid us to help\nreliable workers. We randomly select 5 stories\ngenerated from different models on ROC and 1\nstory from the test dataset. We then train a trigram\nlanguage model on ROC to mimic the style and\ngenerate 1 story from the trigram model. All stories\nhave different titles. We randomly shuffle these 7\nstories and the task is to ask people to evaluate all\nstories with questions mentioned in Appendix C\nand we will judge the quality of their evaluation\nbased on human and trigram stories.\nWe invite 7 of our colleagues, which are all from\nnon-English speaking countries to have a rough\nidea of the difficulty degree of the task. We calcu-\nlate the average score of all quality metrics except\nthe interestingness aspect since it is subjective. On\naverage, our colleagues rank the human story as\n4.5 and trigram story as 1.425, which shows our\ntask is not hard to distinguish human and trigram\nstories. We set a rather lenient standard as “ranking\nhuman story >= 3.5 and trigram story <= 2.0\" to\nselect workers from our pilot study.\nWe create 100 assignments of the same HIT at\ndifferent times with the qualification mentioned\nin Appendix C. We find running the same pilot\nstudy at different times can obtain quite different\nresults from AMT, which align to the findings in\nKarpinska et al. (2021). Generally, we find that\nmore reliable workers can be found in the evening\nof Eastern Daylight Time (EDT). We have 10 out\nof 100 people pass the pilot study but only 5 peo-\nple pass it on the day. It shows the difficulty of\nobtaining reliable workers on AMT nowadays and\nthe economic importance of running a pilot study\nIAA Flu. Coh. Rel. Log. Int.\nROC\nr 0.64 0.81 0.79 0.80 0.68\nTA 17.24 24.98 25.57 27.37 22.03\nWP\nr 0.51 0.70 0.74 0.71 0.54\nTA 18.37 17.01 32.65 19.73 12.93\nCNN\nr 0.46 0.54 0.61 0.59 0.50\nTA 15.13 12.61 15.97 11.76 14.29\nTable 7: Inter Annotator Agreement (IAA) results for\neach aspect: fluency (Flu.), coherence (Coh.), related-\nness (Rel.), logicality (Log.) and interestingness (Int.).\nWe use one-vs-rest Pearson’s r to assess the extent to\nwhich each annotator agrees with the consensus. Total\nAgreement (TA) means the percentage where all 3 an-\nnotators choose the same score.\nbefore conducting real research. We grant those\nreliable workers the customised qualification and\nonly invite them to our real study, we also have\ncontrolled stories to monitor the quality of workers,\nas 2 controlled stories inserted into each HIT.\nE Amazon Mechanic Turk Issue\nOur human evaluation is conducted over AMT,\neven though it is convenient and affordable, we\nfind a big disagreement between our annotators.\nWe first conduct a pilot study to test the capabil-\nity of annotators to evaluate English stories and\nonly invite workers that pass our proficient English\nstories reading tests to the evaluation of sampled\nstories. We only gave them two examples showing\nhow we assess the example stories but we did not\nprovide detailed English stories evaluation training\nto our annotators. We did not have a main annota-\ntors that can provide a standard score for example\nstories, which increase the difficulty of judging the\nquality of evaluation work we receive from AMT.\nAlso, as pointed out in Karpinska et al. (2021),\nthe quality of work from annotators on AMT plat-\nform can be of high variance and have poor cali-\nbration, therefore, we would obtain more reliable\nhuman evaluation results if we hire expert raters\nsuch as professional authors or English language\nteachers.\nF Inter-annotator Agreement for MTurk\nWorkers\nWe follow Lau et al. (2020) to estimate one-vs-rest\nagreement using Pearson’s r. For each story, we\nsingle out an annotator’s score and compare it to\nthe mean scores given by the other two annotators,\nand we repeat this process for every score in a story\n338\nIAA Flu. Coh. Rel. Log. Int.\nROC\nr 0.42 0.54 0.66 0.59 0.32\nTA 38.57 25.0 25.71 25.71 8.57\nWP\nr 0.36 0.57 0.73 0.49 0.54\nTA 10.0 10.0 18.57 10.0 10.0\nCNN\nr 0.36 0.41 0.47 0.37 0.35\nTA 17.14 10.71 14.29 10.0 4.29\nTable 8: In-house IAA Results.\nand for all stories to compute Pearson’s r over the\ntwo sets of scores (singled-out scores vs. mean\nscores). We also compute the percentage where all\n3 annotators choose the same score, noting that this\nis a much stricter agreement metric (as it does not\ncapture the ordinal scale of the scores). Random\nscoring would produce 4% for this metric.\nIAA results are presented in Table 7. In terms\nof one-vs-rest agreement (r), we find overall good\nagreement with 9 strong agreement results ( r >=\n0.6) and 6 moderate agreement results (0.45 <= r\n<= 0.6). We see some correlation between story\nlength and agreement, as ROC has the highest\nagreement (shortest with 5 sentences) and CNN\nhas the lowest (over 20 sentences). When it comes\nto aspects, coherence, relatedness and logicality\nhave higher agreement compared to fluency and\ninterestingness. While it is intuitive to see inter-\nestingness being subjective, fluency is somewhat a\nsurprise. Manual inspection reveals that annotators\nhave very different standards when it comes to flu-\nency, with some workers being more strict about\ngrammar, which contributes to the low agreement.\nFor total agreement (TA), the numbers range be-\ntween 10–25%, which is encouraging as it shows\nthat there is still a good proportion of cases where\nall annotators agree on a score.\nG Inter-annotator Agreement for\nIn-house Workers\nThe In-house annotation agreement results are\nshown in Table 8. Surprisingly, we see similar\ntrends in compared to Table 7, which means our\nfindings are consistent even with a different group\nof workers on a different set of stories.\nWe again see fluency and interestingness are two\naspects that have lower agreements. However, it\nis interesting that we find fluency has highest TA\nbut rather lower r. Manual inspection reveals that\nannotators mainly select 4 or 5 for fluency (also can\nbe seen from high fluency scores for ROC in Ta-\nble 5), which benefits TA because they have higher\nchance to agree on the same values, but might harm\nr because the score can be negatively influenced\nby other rare values. Relevance again has the high-\nest agreement, which might be because annotators\nagree that many stories generated by SOTA models\nare not related to the given condition. However,\ncompared to IAA results from crowdsource annota-\ntors, in-house annotators have lower agreement in\ngeneral. we find 2 strong agreement results (r >=\n0.6) and 6 moderate agreement results (0.45 <= r\n<= 0.6).\nH GPT-3 World knowledge Strengths\nTable 9 and 10 demonstrates that GPT-3 can gener-\nate better stories than SOTA models thanks to its\nwide world knowledge strengths.\nI Mechanic Turk Instructions\nFigure 1 and 2 demonstrate our instructions and\nrubrics.\nJ Example Prompts\nTable 11 shows some examples we use in our\nprompts for generation.\nK GPT-3 Stories vs Human References\nTable 12, 13 and 14 show some examples where\nGPT-3 obtains higher scores than its human refer-\nence.\nL Example Generations\nTable 15 shows randomly picked generation exam-\nple for each dataset.\nM Soft Plagiarism Examples\nTable 16 shows some GPT-3 generated news stories\nwhere we find true online stories.\nN GPT-3 Generation Issues\nTable 17 shows some GPT-3 generated stories with\nseveral issues.\nO Pearson Correlations between Each\nStory Aspects\nFigure 3 and 4 present Pearson Correlations be-\ntween Each Story Aspects for MTurk and in-house\nworkers respectively.\n339\nModel Story\nGPT-3 as soon as i realized that i had died , the first thing that came to mind was how pointless\nmy life had been . i had devoted myself to a cause that was ultimately fruitless and\ndestructive , and in the end i had caused the deaths of millions of people .but then i\nrealized that there was still a chance for redemption . in the after life , i was given the\nopportunity to travel to different moments in history and try to make things right . i\ncould see the consequences of my actions , and i could try to make different choices that\nwould lead to a better outcome .i started with the holocaust , and i was able to save many\npeople from being killed . but it was difficult , and i had to make a lot of tough choices .\neventually i moved on to other moments in history , trying to make things better wherever\ni could .it was a difficult journey , but it was worth it . i was finally able to make up for\nthe mistakes that i had made in life .\nPROGEN3 i knew i was in a bad mood . the blood dripped from my nose and down my chin like the\nburly balding drifter . was it dark yet ? i could feel my brain becoming small dots of the\nrotten material . the blood stopped flowing . almost instantly , the world dissolved in my\nhands . that ’s when i noticed something . i was still standing in the middle of the street .\ni turned to my left and saw the madman .\nHINT “ hi , “ i said , opening my eyes to a bright light . “ this is hitler . “ i looked at the clock .\n5:00 am . “ what ’s your name ? “ “ hitler . “ “ hitler . “ “ hitler . “ “ hitler . “ “ hitler . “\nBART i ’m struck with a fierce sense of shame at that thought . unfortunately it ’s been a while\nsince i ’ve felt this way , and normally it ’s * bad * to feel that way , but i think that too\nshould be expected when you ’re still alive . there has been so much turmoil since the war\n. we ’ve lost children , we ’ve lost society , we ’ve even lost ourselves . but now , we ’re\nfree . we can finally go on with our lives , like we always should . but something happens\nthat seems wrong , and it ’s really hard to forget . there are my reasons for feeling so sad .\nwhy will i be mad today ?\nTable 9: Stories generated from GPT-3, HINT, BART and PROGEN3 given the same title “you ’re hitler , and you\n’ve just died . there is an after life . tell me about it .\"\n340\nModel Story\nGPT-3 the young man had lost everything . his family , his home , his country . he had nothing\nleft to lose . when he was offered the chance to join isis , he saw it as a way to finally\nhave a purpose in life . he was given a sense of belonging and a sense of worth . he was\ntold that he was fighting for a good cause and that he was helping to protect his people .\nhe was given a sense of identity and a sense of purpose . he was told that he was part of\nsomething bigger than himself .\nPROGEN3 they ’ve crippled the ranks of these countries . their leaders not going to the president ’s\nparty , everyone having their noses to the grindstone , the war . political parties shooting\neach other with a firing squad , only to end up murdering each other . “ the one who\nexecuted your followers was the first one to hell houng taqibs . “ he ’s right , ha is not ’d\nunderstand * reason * if he knew that . “ you ’re killing my people . “\nHINT my eyes stare at the ceiling . it ’s just a window , it ’s just a window . i ’m not sure if i\nshould close it or close it again . i ’m afraid i ca n’t move it . i do n’t want to be here . i\ndo n’t want to be here . i want to be able to talk to the people who will help me . to be\nable to hear the voices of people who will help me . to be able to see the lights . to be\nable to see the people who will help me .\nBART missdamie looked at her young daughter sat with c and o . she ’d put on a radiant glow\nand bound herself up in an artificial bubble . she was beautiful . she twitched at the\nvibrancy in her hands . her hair was beautiful and silver . her smile was off . her black\nnails placed on gentle curves . when she smiled her smile widened and the small eyes\nof her daughter ’s eyes danced . “ isis ? “ i asked , shakily , behind the mask . ms. de\nlos angeles nodded and smiled . “ yes , my name is isis de los angeles , and “ her mother\nheld her hand . “ my son is wondering if isis can cause such a thing . “\nTable 10: Stories generated from GPT-3, HINT, BART and PROGEN3 given the same title “make me sympathise\nwith an isis member .\"\n341\nFigure 1: A screenshot of our evaluation questions.\n342\nFigure 2: Explanation and Rubric in the Instruction.\n343\nExample\nROC prompt: [MALE] wants to move to a new house . completion: his wife [FEMALE]\ndoes not want to move . [MALE] convinces her to look at a couple new houses . she\ndoes not see any that she likes . [FEMALE] refuses to move .\nWP prompt: the real reason we ’re not supposed to look at the sun during a total eclipse\nis because it reveals something horrifying . completion: the harvest was here . father\nxander made his way through priory halls toward the sanctuary in the back garden\n. at his advanced years , the trip was a struggle ; each step brought a hefty dose of\npain and unpleasant memories . it was eclipse day , and while he always prayed for\na good outcome , in his eighty years as the priory ’ s eyes , there never was . “ the\ninitiates are assembled in the sanctuary , father , ” spoke a fellow priest in the hall . “\nthank you , my son , ” father xander replied . he paused and looked at the young man\n. memories came flooding back like sour wine . his name was thomas hampstead ,\nand at thirty years of age , he should have been a robust healthy lad ; but he was not .\nthomas ’ s skin had a greyish tint and his body was frail .\nCNN prompt: (CNN) – Usain Bolt rounded off the world championships Sunday by\nclaiming his third gold in Moscow as he anchored Jamaica to victory in the men’s\n4x100m relay. completion: The fastest man in the world charged clear of United\nStates rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-\nCole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in\n37.56 seconds with Canada taking the bronze after Britain were disqualified for a\nfaulty handover. The 26-year-old Bolt has now collected eight gold medals at world\nchampionships, equaling the record held by American trio Carl Lewis, Michael\nJohnson and Allyson Felix, not to mention the small matter of six Olympic titles. The\nrelay triumph followed individual successes in the 100 and 200 meters in the Russian\ncapital. Ï’m proud of myself and I’ll continue to work to dominate for as long as\npossible, ¨Bolt said, having previously expressed his intention to carry on until the\n2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely\nin hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem\nSalaam had problems. Gatlin strayed out of his lane as he struggled to get full control\nof their baton and was never able to get on terms with Bolt. Earlier, Jamaica’s women\nunderlined their dominance in the sprint events by winning the 4x100m relay gold,\nanchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their\nquartet recorded a championship record of 41.29 seconds, well clear of France, who\ncrossed the line in second place in 42.73 seconds. Defending champions, the United\nStates, were initially back in the bronze medal position after losing time on the\nsecond handover between Alexandria Anderson and English Gardner, but promoted\nto silver when France were subsequently disqualified for an illegal handover. The\nBritish quartet, who were initially fourth, were promoted to the bronze which eluded\ntheir men’s team. Fraser-Pryce, like Bolt aged 26, became the first woman to achieve\nthree golds in the 100-200 and the relay. In other final action on the last day of the\nchampionships, France’s Teddy Tamgho became the third man to leap over 18m in\nthe triple jump, exceeding the mark by four centimeters to take gold. Germany’s\nChristina Obergfoll finally took gold at global level in the women’s javelin after five\nprevious silvers, while Kenya’s Asbel Kiprop easily won a tactical men’s 1500m\nfinal.\nTable 11: Prompts and GPT-3 Completions, including one example each for ROC, WP and CNN.\n344\nTitle [SEP] Story From Flu. Coh. Rel. Log. Int.\ntell me the story of a totally inconsequential character\nwho does n’t realize he ‘s wrapped up in a massive high-\nstakes plotline - and keep them in the dark about it . [SEP]\nivan was just an ordinary guy who led an ordinary life .\nhe worked a dead-end job at a local factory , and spent his\nevenings and weekends playing his favorite online game\nwith his friends . one day , out of the blue , ivan was\ncontacted by a representative of the game company , who\noffered him a chance to participate in a closed beta test\nfor their new game . ivan was excited to participate and\neagerly agreed to the terms and conditions .little did he\nknow that he was about to become embroiled in a global\nconspiracy . the game company was actually a front for\na secret organization that was planning to use the game\nto recruit new members . ivan was the perfect candidate ,\nbecause he was someone who was looking for an escape\nfrom his ordinary life .the organization was planning to\nuse the game to recruit new members from all over the\nworld , and they were counting on ivan to help them . ivan\nhad no idea what he was getting himself into , and the\norganization was careful to keep him in the dark about\ntheir true intentions . ivan thought he was just playing\na game , but he was actually a pawn in a much larger\nscheme .\nGPT-3 5.0 5.0 5.0 5.0 5.0\ntell me the story of a totally inconsequential character\nwho does n’t realize he ‘s wrapped up in a massive high-\nstakes plotline - and keep them in the dark about it . [SEP]\nsomeone was moving around on screen three . * probably\njust another junkie stumbling around . fucking dumbass\n. * barry did n’t really give a shit what the junkie was\nup to anyway . his job did n’t pay enough for that . it ’s\nnot like they thought he was actually all that important ,\nanyway . a security guard ? minimum wage , take your\npaycheck , shut up , and move along , thank you very\nmuch . you have to sit for twelve hours in the dead of\nnight and devote all of your attention to watching the\nsame twelve tiny boxes ? that ’s nice . how ’s $ 7.25 an\nhour sound ? you ’re preventing vandalism and break-ins\n? how wonderful . would you like a cookie ?\nhuman 3.67 2.33 2.0 3.33 2.33\nTable 12: GPT-3 Generated Story Obtains Higher Scores than Its Human Reference.\n345\nTitle [SEP] Story From Flu. Coh. Rel. Log. Int.\nyou meet the grim reaper at a crossroads . you are fearful\nas he approaches , but notice he seems just as fearful of\nyou . [SEP] the grim reaper was a fearful figure , draped\nin a cloak of blackness with a face that was hidden in\nshadow . as he came closer , you could see the fear in his\neyes . “ why are you so afraid of me ? “ you asked . “\nbecause you ’re the first person to ever see me like this , “\nhe replied . “ i ’m not here to take you , i ’m here to ask\nfor your help . “\nGPT-3 4.0 4.67 4.67 3.67 3.67\nyou meet the grim reaper at a crossroads . you are fearful\nas he approaches , but notice he seems just as fearful of\nyou . [SEP] my eyes jump from snowbank to snowbank\n, one featureless as the next . the road i walk upon is a\ndark grey slush , the result of many cars and snow plows\nmaking their way through the void . yet i had not seen\na car in miles . i shiver . but not from the cold . the\ncompelling . it had awakened me . this ominous feeling\nthat i had to be at a certain place at a certain time . it\ncontrolled me . it made me get dressed . it made me open\nthe door . it had even made me ignore my wife , who i\nusually kiss goodbye . the road seemed endless , but i felt\nno reason to stop .\nhuman 3.33 3.33 1.67 2.33 2.33\nTable 13: GPT-3 Generated Story Obtains Higher Scores than Its Human Reference.\n346\nTitle [SEP] Story From Flu. Coh. Rel. Log. Int.\nyou are able to see the number of days a person has left\nto live above their heads . for the first time in your life ,\nyou see someone with a negative number . [SEP] as you\nwalk the streets , dodging the rushing crowds , you notice\nsomething different about one of the people walking by\n. their number is displayed in a negative value . for\na moment , you feel a pang of sadness and empathy .\nyou know what that means - that person is going to die\nsoon . you watch as they go about their day , completely\nunaware of the timer ticking away above their head . you\nwonder who they are , what they ’re doing , and who will\nbe there with them when their time runs out .\nGPT-3 4.67 4.33 4.67 4.0 5.0\nyou are able to see the number of days a person has left\nto live above their heads . for the first time in your life ,\nyou see someone with a negative number . [SEP] crowds\nare the worst . frank squints through the flurry of hov-\nering digits as he shoulders his way through the masses\nof shoppers . the cacophony of vociferous patrons , dis-\ntressed employees , and typical black friday commotion\ncrescendos around him . he pauses to watch two shoppers\nvie over the last remaining television monitor ( only $\n600 ! ) . one is a blonde woman whose pigtails have been\njostled into disarray ; above her hovers the number 10951\n. the other is n’t as fortunate ; the number 582 glows\nabove her greying hair . frank sighs . he ’s sure there\n’s something . . . vaguely poignant about fighting over\na product priced at a greater value than your remaining\ndays of life . socio-rhetorical commentary on modern\nconsumerism , perhaps . he has little time to contemplate\n, however , as another shopper impatiently shoves him\nforward , and frank resumes his pace . while he walks ,\nfrank studies the various faces passing by : a woman in\nan elegant , blue hijab ; a portly , bearded man ; a fatigued\nmother clutching her two children by the hands ... .\nhuman 3.67 3.0 3.33 4.0 3.33\nTable 14: GPT-3 Generated Story Obtain Higher Scores than Its Human Reference.\n347\nTitle Generation\nthe boy cried . the boy cried because he was sad and scared . his parents were getting\na divorce and he did n’t know what was going to happen to him . he\nwas going to have to leave his home and his friends and move to a\nnew house . his mother said that he could choose where he wanted to\nlive , but his father said that he had to live with him . the boy did n’t\nknow what to do .\nyour extreme apathy\nhas been mistaken\nas spiritual enlighten-\nment , and now peo-\nple are building a re-\nligion around you .\nmy name is john and i have a problem . well , actually , a lot of\npeople have a problem with me but that ’s not the point . the point is\nthat i don ’t care about anything . at all . not the way most people\ncare about things anyway . when most people see a homeless person\non the street they feel compassion and want to help . when i see a\nhomeless person on the street i think “ well , there goes another lazy\nbum who ’s too good for work “ . and i ’m not joking when i say that\nmost people see the world in a completely different way than i do .\nthey see the good in people and want to help them . i see the bad in\npeople and want to help them . see the difference ? i ’m not saying\nthat my way of seeing the world is wrong and theirs is right . i ’m\njust saying that we see the world differently and that ’s okay . or at\nleast it should be . but it ’s not because people can ’t accept that i don\n’t care about anything . they think that i ’m enlightened because i don\n’t feel the pain and suffering that they do . they think that because i\ndon ’t care about anything i must be in a better place than they are\n.and that ’s why they ’re building a religion around me .\n(CNN) – Hurricane\nIda moved into the\nsouthern Gulf of\nMexico Sunday,\nprompting a decla-\nration of emergency\nin Louisiana and\nconcern along the\nU.S. Gulf Coast.\nida Louisiana Gov. Bobby Jindal declared a state of emergency\nSaturday as the Category 1 hurricane churned in the Gulf of Mexico,\naccording to a news release from the governor’s office. \"Based on\nthe latest forecasts, Hurricane Ida is expected to make landfall in\nLouisiana as a Category 1 hurricane and we must be prepared for\nsignificant impacts,\" Jindal said in the release. \"I have directed my\nteam to do everything possible to protect the lives and property of\nthe people of Louisiana.\" The hurricane could bring storm surges,\nhigh winds and heavy rains to the coast, the National Hurricane\nCenter said. The storm was about 220 miles (355 kilometers) south-\nsoutheast of the mouth of the Mississippi River on Sunday morning\nand was moving north-northwest at about 9 mph (15 kph), the center\nsaid. The hurricane had maximum sustained winds of 75 mph (120\nkph). Forecasters said Ida could strengthen a bit before making\nlandfall. The center of the storm is expected to come ashore early\nTuesday morning.\nTable 15: Randomly Chosen GPT-3 Generation Examples from Each Dataset.\n348\nGPT-3 Generated Story (Link to the Online Resource)\nWashington (CNN) – With the posting of 400,000 classified documents from the Iraq war,\nWikiLeaks has shown a much heavier hand redacting compared to its previous publication\nof documents. In an interview with Wolf Blitzer on Wednesday, WikiLeaks founder Julian\nAssange said that the organization redacted the names of Iraqis who provided information\nto the United States military in order to protect them from retaliation.\"We have a duty to\nprotect some of the people who are named in these documents,\" Assange said.WikiLeaks\nhas redacted the names of coalition forces and informants, as well as the names of Iraqis\nwho cooperated with the United States military, in all of its recent releases of classified doc-\numents. (https://www.reuters.com/article/britain-assange-idUSL4N2GD2PZ)\n(CNN) – A former University of Virginia lacrosse player convicted of killing his ex-\ngirlfriend was sentenced to 23 years in prison on Thursday, according to the Common-\nwealth Attorney’s office in Charlottesville, Virginia. George Huguely V was convicted\nin February of second-degree murder in the May 2010 death of Yeardley Love. He was\nalso convicted of grand larceny and burglary.Prosecutors had asked for a sentence of\n30 years, while Huguely’s defense attorneys had asked for 10 years.\"Yeardley was my\neverything,\" Huguely said in a statement before sentencing. \"I was not thinking, I was\nnot rationale, and I was not in control of my own actions.\"Love was found beaten to\ndeath in her off-campus apartment. Huguely’s DNA was found on Love’s body and in\nher bedroom.During the trial, prosecutors said Huguely had sent Love a series of emails\nin which he threatened to kill her.\"I’m going to kill you. You deserve it. You took\nthe one thing I loved the most,\" Huguely wrote in one email, according to prosecutors.\n(https://en.wikipedia.org/wiki/Murder_of_Yeardley_Love)\n(CNN) – Crisis club Racing Santander has been thrown out of next season’s Copa del\nRey after its players staged a bizarre on-pitch boycott of a quarterfinal tie against Real\nSociedad. Racing Santander’s players staged a bizarre on-pitch boycott of a Copa del\nRey quarterfinal tie against Real Sociedad on Wednesday, leading to their expulsion from\nthe competition.The players refused to take to the field in the first leg of the last-eight\ntie at the Anoeta Stadium in San Sebastian, protesting at the club’s failure to pay their\nwages.After a delay of around 45 minutes, the referee finally called off the game, with the\nSpanish Football Federation (RFEF) confirming Racing Santander’s expulsion from the\ncompetition.\"The Competition Committee has decided to expel Racing Santander from the\nCopa del Rey for not appearing in the match,\" said an RFEF statement.\"Real Sociedad have\nbeen awarded a 3-0 win and will now face the winners of the match between Real Madrid\nand Valencia, who will play the semi-final.\"Racing Santander, who are currently bottom\nof the Spanish first division, released a statement on their website protesting at the club’s\ntreatment by the RFEF.\"Racing Santander has today been expelled from the Copa del Rey\nfor not appearing in the match against Real Sociedad, despite the fact that our players were\nprepared to play,\" said the statement.\"We believe that this decision is unjust, given that our\nplayers have not been paid their wages for several months and have made it clear that they\nwould not be able to compete in the match.\"The club also released a video of the players\nrefusing to take to the field, with captain David Lafuente leading the protest.Racing\nSantander’s expulsion from the Copa del Rey means they will not compete in next\nseason’s competition, while Real Sociedad will face the winners of the match between\nReal Madrid and Valencia in the semi-final. (http://edition.cnn.com/2014/01/30/\nsport/football/spanish-cup-boycott-santander-football/index.html )\nTable 16: GPT-3 generated CNN News and link to the Online Resource\n349\nIssue Title Generation\nGenerate\nChinese\nfrom an\nEnglish title\nyou are an alien\nbody snatcher , and\nthis is your first day\nin your new host\nbody . you have no\nidea what his life is\n.\n你不是人，你是外星人。这是你第一天在新的主人\n身上。你不知道他的生活是什么。你只是偶然得到\n他的身体，他的生活已经开始了，而你还不知道这是\n什么。你的主人是一个普通人，他没有你的外星人能\n力。你不知道你的主人在做什么，也不知道他有没有\n你的朋友。你只是在他的身体里，完全不知道他的生\n活。\nGenerate\nJapanese\nfrom an\nEnglish title\nwhat it means to\nlive ( 250 words ) .\n人生とは何かを探して生きることである。探しける\nことで、生きるをつける。 例えばを探しけること\nで、そのが 叶う率を高める。を 叶えるために 努力\nをしけることで、そのが叶う可能性が高まる。しか\nし、を叶えるために努力をしけることはしい。を叶\nえるためには、そのにして情を持つことが大切だ。\nそして、そのを叶えるためには、自分を信じること\nが大切だ。を叶えるためには、そのにチャレンジす\nることが大切だ。そして、を叶えるためには、失を\n恐れずに前することが大切だ。\nStick word\ntogether to\ncreate “new\"\nwords\nwrite a short story\nbased off the lyrics\nto any song .\nthere was a timewhen we were youngand our dreams were\nalivewe wanted to be rock starsand travel the worldwe\nwanted to be someonesomeone famoussomeone lovednow\nwe ’re just oldour dreams have diedour passions gonewe\njust want to be left aloneto die in peace\nCopy from\nthe leading\ntext\n[MALE] was a very\nsweet dog .\n[MALE] was a very sweet dog . he loved his family and\nwas always happy to see them . even though he was old ,\nhe never stopped wagging his tail and licking everyone ’s\nfaces .\nCopy from\nthe prompt\n[MALE] is a man-\nager at kfc .\nhe started as a fry cook [MALE] wants to move to a new\nhouse , but his wife [FEMALE] does not want to move .\n[MALE] convinces her to look at a couple new houses , and\nshe does not see any that she likes . [FEMALE] refuses to\nmove . (Table 11)\nGenerate pro-\nfanity words\n[FEMALE] was eat-\ning lunch at school\n.\na boy came up to her and asked her to go out with him . she\nsaid no and he called her a stuck up b * * * * .\nTable 17: Examples of some interesting generation errors we find for GPT-3.\n350\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.68 0.47 0.33 0.29\n0.68 1 0.68 0.58 0.52\n0.47 0.68 1 0.5 0.51\n0.33 0.58 0.5 1 0.89\n0.29 0.52 0.51 0.89 1\nROC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) ROC\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.74 0.54 0.73 0.57\n0.74 1 0.71 0.92 0.8\n0.54 0.71 1 0.78 0.69\n0.73 0.92 0.78 1 0.85\n0.57 0.8 0.69 0.85 1\nWP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) WP\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.71 0.56 0.71 0.47\n0.71 1 0.75 0.86 0.69\n0.56 0.75 1 0.82 0.58\n0.71 0.86 0.82 1 0.72\n0.47 0.69 0.58 0.72 1\nCNN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) CNN\nFigure 3: Pearson Correlations between Each Aspect from Crowdsourcing annotations.\n351\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.51 0.27 0.43 0.38\n0.51 1 0.48 0.78 0.63\n0.27 0.48 1 0.48 0.49\n0.43 0.78 0.48 1 0.68\n0.38 0.63 0.49 0.68 1\nROC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) ROC\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.66 0.42 0.59 0.48\n0.66 1 0.64 0.8 0.72\n0.42 0.64 1 0.65 0.67\n0.59 0.8 0.65 1 0.76\n0.48 0.72 0.67 0.76 1\nWP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) WP\nFlu. Coh. Rel. Log. Int.\nFlu.\nCoh.\nRel.\nLog.\nInt.\n1 0.57 0.45 0.59 0.61\n0.57 1 0.59 0.81 0.64\n0.45 0.59 1 0.55 0.52\n0.59 0.81 0.55 1 0.6\n0.61 0.64 0.52 0.6 1\nCNN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) CNN\nFigure 4: Pearson Correlations between Each Aspect from in-house annotations.",
  "topic": "Storytelling",
  "concepts": [
    {
      "name": "Storytelling",
      "score": 0.8154270648956299
    },
    {
      "name": "Computer science",
      "score": 0.6213712692260742
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5744940638542175
    },
    {
      "name": "Style (visual arts)",
      "score": 0.550406277179718
    },
    {
      "name": "Replicate",
      "score": 0.5036045908927917
    },
    {
      "name": "Natural language processing",
      "score": 0.4942781925201416
    },
    {
      "name": "Natural language generation",
      "score": 0.4801182746887207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43321818113327026
    },
    {
      "name": "Narrative",
      "score": 0.3546273112297058
    },
    {
      "name": "Natural language",
      "score": 0.34297674894332886
    },
    {
      "name": "Linguistics",
      "score": 0.22735542058944702
    },
    {
      "name": "History",
      "score": 0.15570366382598877
    },
    {
      "name": "Epistemology",
      "score": 0.08731082081794739
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}