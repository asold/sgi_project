{
  "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
  "url": "https://openalex.org/W4385571011",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099221992",
      "name": "Cheng-Yu Hsieh",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2629527409",
      "name": "Chun-Liang Li",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4209280989",
      "name": "Chih-Kuan Yeh",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A928169443",
      "name": "Hootan Nakhost",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2257381379",
      "name": "Yasuhisa Fujii",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3039494716",
      "name": "Alex Ratner",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2118252886",
      "name": "Ranjay Krishna",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2688524499",
      "name": "Chen-Yu Lee",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2798413642",
      "name": "Tomas Pfister",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W2793383859",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W3110300144",
    "https://openalex.org/W4308243058",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W4306295157",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4228998172",
    "https://openalex.org/W2963082289",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4308900254",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2153332911",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4309212061",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4288631924",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W2963560987",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2963798744",
    "https://openalex.org/W4385571219",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W3115894062",
    "https://openalex.org/W3212191244",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W3023690688",
    "https://openalex.org/W3095273266",
    "https://openalex.org/W4226479682",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4303648545",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3170224286",
    "https://openalex.org/W4303648559"
  ],
  "abstract": "Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, Tomas Pfister. Findings of the Association for Computational Linguistics: ACL 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8003–8017\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDistilling Step-by-Step! Outperforming Larger Language Models\nwith Less Training Data and Smaller Model Sizes\nCheng-Yu Hsieh1∗, Chun-Liang Li 2, Chih-Kuan Yeh3, Hootan Nakhost 2,\nYasuhisa Fujii3, Alexander Ratner 1, Ranjay Krishna 1, Chen-Yu Lee 2, Tomas Pﬁster 2\n1University of Washington, 2Google Cloud AI Research, 3Google Research\ncydhsieh@cs.washington.edu\nAbstract\nDeploying large language models (LLMs) is\nchallenging because they are memory inef-\nﬁcient and compute-intensive for practical\napplications. In reaction, researchers train\nsmaller task-speciﬁc models by either ﬁne-\ntuning with human labels or distilling using\nLLM-generated labels. However, ﬁnetuning\nand distillation require large amounts of train-\ning data to achieve comparable performance to\nLLMs. We introduce Distilling step-by-step, a\nnew mechanism that (a) trains smaller models\nthat outperform LLMs, and (b) achieves so by\nleveraging less training data needed by ﬁnetun-\ning or distillation. Our method extracts LLM\nrationales as additional supervision for train-\ning small models within a multi-task frame-\nwork. We present three ﬁndings across 4\nNLP benchmarks: First, compared to both\nﬁnetuning and distillation, our mechanism\nachieves better performance with much fewer\nlabeled/unlabeled training examples. Second,\ncompared to few-shot prompted LLMs, we\nachieve better performance using substantially\nsmaller model sizes. Third, we reduce both\nthe model size and the amount of data required\nto outperform LLMs; our ﬁnetuned 770M\nT5 model outperforms the few-shot prompted\n540B PaLM model using only 80% of avail-\nable data on a benchmark, whereas standard\nﬁnetuning the same T5 model struggles to\nmatch even by using 100% of the dataset.1\n1 Introduction\nDespite the impressive few-shot ability offered by\nlarge language models (LLMs) (Brown et al., 2020;\nChowdhery et al., 2022; Thoppilan et al., 2022;\nHoffmann et al., 2022; Smith et al., 2022b; Zhang\net al., 2022), these models are challenging to de-\nploy in real world applications due to their sheer\n∗Work done while the author was a student researcher at\nGoogle Cloud AI Research.\n1Source code is available at: https://github.com/\ngoogle-research/distilling-step-by-step .\nFigure 1: While large language models (LLMs) offer\nstrong zero/few-shot performance, they are challeng-\ning to serve in practice. Traditional ways of training\nsmall task-speciﬁc models, on the other hand, requires\nlarge amount of training data. We propose Distilling\nstep-by-step, a new paradigm that extracts rationales\nfrom LLMs as informative task knowledge into training\nsmall models, which reduces both the deployed model\nsize as well as the data required for training.\nsize. Serving a single 175 billion LLM requires\nat least 350GB GPU memory using specialized in-\nfrastructure (Zheng et al., 2022). To make matters\nworse, today’s state-of-the-art LLMs are composed\nof over 500B parameters (Chowdhery et al., 2022),\nrequiring signiﬁcantly more memory and compute.\nSuch computational requirements are far beyond\naffordable for most product teams, especially for\napplications that require low latency performance.\nTo circumvent these deployment challenges of\nlarge models, practitioners often choose to de-\nploy smaller specialized models instead. These\nsmaller models are trained using one of two\ncommon paradigms: ﬁnetuning or distillation.\nFinetuning updates a pretrained smaller model\n(e.g. BERT (Devlin et al., 2018) or T5 (Raffel\net al., 2020)) using downstream human annotated\ndata (Howard and Ruder, 2018). Distillation trains\nthe same smaller models with labels generated by\na larger LLM (Tang et al., 2019; Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022). Unfortu-\nnately, these paradigms reduce model size at a cost:\nto achieve comparable performance to LLMs, ﬁne-\ntuning requires expensive human labels, and dis-\n8003\ntillation requires large amounts of unlabeled data\nwhich can be hard to obtain (Tang et al., 2019;\nLiang et al., 2020).\nIn this work, we introduce Distilling step-by-\nstep, a new simple mechanism for training smaller\nmodels with less training data. Our mechanism re-\nduces the amount of training data required for both\nﬁnetuning and distillation of LLMs into smaller\nmodel sizes. Core to our mechanism is changing\nour perspective from viewing LLMs as a source\nof noisy labels to viewing them as agents that can\nreason: LLMs can produce natural language ratio-\nnales justifying their predicted labels (Wei et al.,\n2022; Kojima et al., 2022). For example, when\nasked “Jesse’s room is 11 feet long and 15 feet\nwide. If she already has 16 square feet of carpet.\nHow much more carpet does she need to cover\nthe whole ﬂoor? ”, an LLM can be prompted by\nchain-of-thought (CoT) technique (Wei et al., 2022)\nto provide intermediate rationales “Area = length\n×width. Jesse’s room has 11 ×15 square feet.”\nthat better connects the input to the ﬁnal answer\n“(11 ×15) −16”. These rationales can contain\nrelevant task knowledge, such as “Area = length ×\nwidth”, that may originally require many data for\nsmall task-speciﬁc models to learn. We thus utilize\nthese extracted rationales as additional, richer infor-\nmation to train small models through a multi-task\ntraining setup, with both label prediction and ratio-\nnale prediction tasks (Raffel et al., 2020; Narang\net al., 2020).\nDistilling step-by-step allows us to learn task-\nspeciﬁc smaller models that outperform LLMs us-\ning over 500×less model parameters, and it does\nso with far fewer training examples compared to\ntraditional ﬁnetuning or distillation (Figure 1). Our\nresults show three promising empirical conclusions\nacross 4 NLP benchmarks. First, compared to both\nﬁnetuning and distillation, our resulting models\nachieve better performance with over 50% less\ntraining examples on average across datasets (and\nup to over 85% reduction). Second, our models\noutperform LLMs with much smaller model sizes\n(up to 2000×smaller), drastically reducing the\ncomputation cost required for model deployment.\nThird, we simultaneously reduce the model size\nas well as the amount of data required to outper-\nform LLMs. We surpass the performance of 540B\nparameter LLMs using a 770M T5 model; this\nsmaller model only uses 80% of a labeled dataset\nthat would otherwise be required if using an exist-\ning ﬁnetuning method. When only unlabeled data\nis present, our small models still perform on par or\nbetter than LLMs. We outperform 540B PaLM’s\nperformance with only a11B T5 model. We further\nshow that when a smaller model performs worse\nthan an LLM, Distilling step-by-step can more efﬁ-\nciently leverage additional unlabeled data to match\nthe LLM performance compared to the standard\ndistillation approach.\n2 Related work\nOur work distills task-speciﬁc knowledge of LLMs\ninto smaller specialist models by leveraging the\nemergent reasoning capabilities of today’s LLMs.\nWe draw on recent knowledge distillation research\nand other methods that learn from both human-\ngenerated rationales and LLM-generated ratio-\nnales.\nKnowledge distillation from large models.\nKnowledge distillation has been successfully used\nto transfer knowledge from larger, more competent\nteacher models into smaller student models afford-\nable for practical applications (Buciluˇa et al., 2006;\nHinton et al., 2015; Beyer et al., 2022; West et al.,\n2021). It supports learning from limited labeled\ndata, since the larger teacher model is often used\nto generate a training dataset with noisy pseudo\nlabels (Chen et al., 2020; Iliopoulos et al., 2022;\nWang et al., 2021; Smith et al., 2022a; Arora et al.,\n2022; Agrawal et al., 2022). The one limitation that\nknowledge distillation often faces is its reliance on\nlarge amounts of unlabelled data required to cre-\nate a useful noisy training dataset. Although prior\nwork has explored using data augmentation tech-\nniques to reduce this hunger for data (Tang et al.,\n2019; Liang et al., 2020; Srinivas and Fleuret, 2018;\nMilli et al., 2019), we propose an alternative ap-\nproach: we reduce the need for large unlabeled data\nby distilling not just labels but also the teacher’s\nrationales.\nLearning with human rationales. While utiliz-\ning LLM-generated rationales is a new exciting\narea of investigation, using human-generated ratio-\nnales has a rich history (Hase and Bansal, 2021).\nFor instance, human rationales can be used to reg-\nularize model behavior (Ross et al., 2017); it can\nbe used as additional inputs to guide a model’s\npredictions (Rajani et al., 2019); it can be used to\nimprove overall model performance (Zaidan et al.,\n2007; Zhang et al., 2016; Camburu et al., 2018;\n8004\nFigure 2: Overview on Distilling step-by-step. We ﬁrst utilize CoT prompting to extract rationales from an LLM\n(Section 3.1). We then use the generated rationales to train small task-speciﬁc models within a multi-task learning\nframework where we prepend task preﬁxes to the input examples and train the model to output differently based\non the given task preﬁx (Section 3.2).\nHancock et al., 2019; Pruthi et al., 2022); and hu-\nman rationales can be used as gold standard labels\nto make models more interpretable by generating\nsimilar rationales (Wiegreffe et al., 2021; Narang\net al., 2020; Eisenstein et al., 2022). Unfortunately,\nhuman rationales are expensive.\nLearning with LLM generated rationales. To-\nday’s LLMs are capable of explaining their pre-\ndictions by generating high-quality reasoning\nsteps (Wei et al., 2022; Kojima et al., 2022). These\nreasoning steps have been used to augment input\nprompts to LLMs, improving their few-shot or zero-\nshot performance (Wei et al., 2022; Kojima et al.,\n2022; Wang et al., 2022b); reasoning steps have\nalso been used as additional ﬁnetuning data “self-\nimprove” LLMs (Zelikman et al., 2022; Huang\net al., 2022). Unfortunately, regardless of how\nLLMs are improved, their large size limits their\nutility in most test-time applications.\nBy contrast, we leverage generated rationales\nas informative supervision to train smaller task-\nspeciﬁc models, i.e. models that can be deployed\nwithout incurring large computation or memory\ncosts. In the past few months, three concurrent\nworks have also proposed a similar idea to ours\n– that of using extracted rationales as supervi-\nsion (Wang et al., 2022a; Ho et al., 2022; Magister\net al., 2022). Amongst them, PINTO (Wang et al.,\n2022a) relies on an LLM to generate rationales\nat test-time, and thus does not fully solve deploy-\nment challenges. Compared with Ho et al. (2022)\nand Magister et al. (2022), we go beyond their ex-\nperiments to provide a granular study by varying\ntraining dataset size, exploring downstream model\nsizes, and demonstrating the effectiveness of our\nmethod on fully unlabeled datasets.\n3 Distilling step-by-step\nWe propose a new paradigm, Distilling step-by-\nstep, that leverages the ability of LLMs to reason\nabout their predictions to train smaller models in\na data-efﬁcient way. Our overall framework is il-\nlustrated in Figure 2. Our paradigm has two sim-\nple steps: First, given an LLM and an unlabeled\ndataset, we prompt the LLM to generate output\nlabels along with rationales to justify the labels.\nRationales are natural language explanations that\nprovide support for the model’s predicted label\n(see Figure 2). Second, we leverage these ratio-\nnales in addition to the task labels to train smaller\ndownstream models. Intuitively, rationales provide\nricher, more detailed information about why an in-\nput is mapped to a speciﬁc output label, and often\ncontain relevant task knowledge that may be hard\nto infer solely from the original inputs.\n3.1 Extracting rationales from LLMs\nRecent studies observe one intriguing emerging\nproperty of LLMs: their ability to generate ra-\ntionales that support their predictions (Wei et al.,\n2022; Kojima et al., 2022). While the studies have\nlargely focused on how to elicit such reasoning ca-\npability from LLMs (Nye et al., 2021; Wei et al.,\n2022; Kojima et al., 2022), we use them in training\nsmaller downstream models.\nSpeciﬁcally, we utilize Chain-of-Thought (CoT)\n8005\nFigure 3: We use few-shot CoT prompting that contains\nboth an example rationale (highlighted in green) and a\nlabel (highlighted in blue) to elicit rationales from an\nLLM on new input examples.\nprompting (Wei et al., 2022) to elicit and extract\nrationales from LLMs. As illustrated in Figure 3,\ngiven an unlabeled dataset xi ∈D, we ﬁrst cu-\nrate a prompt template pthat articulates how the\ntask should be solved. Each prompt is a triplet\n(xp,rp,yp), where xp is an example input, yp is\nits corresponding label and rp is a user-provided\nrationale that explains why xp can be categorized\nas yp. We append each input xi to pand use it as\nan input to prompt the LLM to generate rationales\nand labels for each xi ∈D. With the demonstra-\ntions seen in p, the LLM is able to mimic the triplet\ndemonstration to generate the rationale ˆri and out-\nput ˆyi for xi.\n3.2 Training smaller models with rationales\nWe ﬁrst describe the current framework for learn-\ning task-speciﬁc models. With this framework in\nplace, we extend it to incorporate rationales into\nthe training process. Formally, we denote a dataset\nas D= {(xi,yi)}N\ni=1 where each xi represents an\ninput and yi is the corresponding desired output\nlabel. While our framework supports inputs and\noutputs of any modality, our experiments limits\nxand y to be natural language. This text-to-text\nframework (Raffel et al., 2020) encompasses a va-\nriety of NLP tasks: classiﬁcation, natural language\ninference, question answering and more.\nStandard ﬁnetuning and task distillation. The\nmost common practice to train a task-speciﬁc\nmodel is to ﬁnetune a pretrained model with su-\npervised data (Howard and Ruder, 2018). In the\nabsence of human-annotated labels, task-speciﬁc\ndistillation (Hinton et al., 2015; Tang et al., 2019)\nuses LLM teachers to generates pseudo noisy train-\ning labels, ˆyi in place of yi (Wang et al., 2021;\nSmith et al., 2022a; Arora et al., 2022).\nFor both scenarios, the smaller model f is\ntrained to minimize the label prediction loss:\nLlabel = 1\nN\nN∑\ni=1\nℓ(f(xi),ˆyi), (1)\nwhere ℓis the cross-entropy loss between the pre-\ndicted and target tokens. Note that for ease of\nexposition, we overload ˆyi in Eq. 1 to be either\nhuman-annotated labels yi for the standard ﬁnetun-\ning case, or LLM-predicted labels ˆyi for the model\ndistillation case.\nMulti-task learning with rationales. To create\na more explicit connection between xi’s toˆyi’s, we\nuse extracted rationales ˆri as additional supervi-\nsion. There are several ways to incorporate ratio-\nnales into the downstream model’s training process.\nOne straightforward approach is feed ˆri as an ad-\nditional input—as proposed by other concurrent\nresearch (Rajani et al., 2019; Wang et al., 2022a).\nIn other words, the f(xi,ˆri) →ˆyi is trained with\nboth text and rationale [x,r] as inputs:\nL= 1\nN\nN∑\ni=1\nℓ(f(xi,ˆri),ˆyi). (2)\nUnfortunately, this design requires an LLM to ﬁrst\ngenerate a rationale before the f can make a pre-\ndiction. The LLM is still necessary during deploy-\nment, limited its deployability.\nIn this work, instead of using rationales as ad-\nditional model inputs, we frame learning with ra-\ntionales as a multi-task problem. Speciﬁcally, we\ntrain the model f(xi) →(ˆyi,ˆri) to not only predict\nthe task labels but also generate the corresponding\nrationales given the text inputs:\nL= Llabel + λLrationale, (3)\nwhere Llabel is the label prediction loss in Eq. 1\nand Lrationale is the rationale generation loss:\nLrationale = 1\nN\nN∑\ni=1\nℓ(f(xi),ˆri). (4)\nThe rationale generation loss enables the model to\nlearn to generate the intermediate reasoning steps\nfor the prediction, and could therefore guide the\nmodel in better predicting the resultant label. This\nis our proposed Distilling step-by-step. Compared\nwith Eq. 2, the rationale ˆri is not required in the\ntest time, which removes the need for an LLM at\ntest-time.\n8006\nWe prepend “task preﬁxes” ( [label],\n[rationale]) to the input examples and\ntrain the smaller model to output ˆyi when\n[label] is provided and to produce ˆri with\n[rationale] (Raffel et al., 2020).\n4 Experiments\nWe empirically validate the effectiveness of Dis-\ntilling step-by-step. First, we show that when\ncompared to standard ﬁnetuning and task distil-\nlation approaches, Distilling step-by-step achieves\nbetter performance with much fewer number of\ntraining examples, substantially improving the\ndata efﬁciency to learn small task-speciﬁc mod-\nels (Sec. 4.1). Second, we show that Distilling\nstep-by-step surpasses the performance of LLMs\nwith much smaller model size, drastically lowering\nthe deployment cost compared to LLMs (Sec. 4.2).\nThird, we investigate the minimum resources re-\nquired, w.r.t. both number of training examples and\nmodel size, for Distilling step-by-step to outper-\nform LLMs. We show that Distilling step-by-step\noutperforms LLMs by using less data and smaller\nmodel, simultaneously improving both data- and\ndeployability-efﬁciency (Sec. 4.3). Finally, we per-\nform ablation studies to understand the inﬂuence\nof different components and design choices in the\nDistilling step-by-step framework (Sec. 4.4).\nSetup. In the experiments, we consider the 540B\nPaLM model (Chowdhery et al., 2022) as the LLM.\nFor task-speciﬁc downstream models, we use T5\nmodels (Raffel et al., 2020) where we initialize the\nmodels with pretrained weights obtained from pub-\nlicly available sources2. For CoT prompting, we\nfollow Wei et al. (2022) when available, and curate\nour own examples for new datasets. We include\nmore implementation details in Appendix A.1.\nDatasets. We conduct the experiments on 4\npopular benchmark datasets across 3 different\nNLP tasks: e-SNLI (Camburu et al., 2018) and\nANLI (Nie et al., 2020) for natural language infer-\nence; CQA (Talmor et al., 2019; Rajani et al., 2019)\nfor commonsense question answering; SVAMP (Pa-\ntel et al., 2021) for arithmetic math word problems.\nWe include more dataset details in Appendix A.2.\n4.1 Reducing training data\nWe compare Distilling step-by-step to two most\ncommon methods in learning task-speciﬁc models:\n2https://huggingface.co/\n(1) STANDARD FINETUNING when human-labeled\nexamples are available, and (2) STANDARD TASK\nDISTILLATION when only unlabeled examples are\navailable. Speciﬁcally, standard ﬁnetuning refers to\nthe prevailing pretrain-then-ﬁnetune paradigm that\nﬁnetunes a model with ground-truth labels via stan-\ndard label supervision (Howard and Ruder, 2018).\nOn the other hand, when only unlabeled examples\nare available, standard task distillation learns the\ntask-speciﬁc model by treating a teacher LLM’s\npredicted labels as ground-truths (Hinton et al.,\n2015; Chen et al., 2020; Wang et al., 2021; Smith\net al., 2022a; Arora et al., 2022).\nIn the following set of experiments, we ﬁx the\ntask-speciﬁc models to be 220M T5-Base models,\nand compare the task performances achieved by dif-\nferent methods under varying number of available\ntraining examples.\nDistilling step-by-step outperforms standard\nﬁnetuning with much less labeled examples.\nWhen ﬁnetuned with human-labeled examples, Fig-\nure 4 shows that Distilling step-by-step consistently\nachieves better performance than standard ﬁnetun-\ning across varying numbers of labeled examples\nused. Furthermore, we see that Distilling step-by-\nstep can achieve the same performance as stan-\ndard ﬁnetuning with much less labeled examples.\nIn particular, by using only 12.5% of the full e-\nSNLI dataset, Distilling step-by-step can outper-\nform standard ﬁnetuning trained with 100% of the\nfull dataset. Similarly, we achieve 75%, 25%, and\n20% reduction in training examples required to out-\nperform standard ﬁnetuning on ANLI, CQA, and\nSV AMP respectively.\nDistilling step-by-step outperforms standard\ndistillation with much less unlabeled examples.\nWhen only unlabeled data is available, we compare\nDistilling step-by-step to standard task distillation.\nIn Figure 5, we observe an overall similar trend to\nthe ﬁnetuning setup. Speciﬁcally, we see that Dis-\ntilling step-by-step outperforms standard task distil-\nlation on all 4 datasets under different numbers of\nunlabeled data used. We as well see that Distilling\nstep-by-step requires much less unlabeled data to\noutperform standard task distillation. For instance,\nwe need only 12.5% of the full unlabeled dataset\nto outperform the performance achieved by stan-\ndard task distillation using 100% of the training\nexamples on e-SNLI dataset.\n8007\nFigure 4: We compare Distilling step-by-step and Standard ﬁnetuning using 220M T5 models on varying sizes of\nhuman-labeled datasets. On all datasets, Distilling step-by-step is able to outperform Standard ﬁnetuning, trained\non the full dataset, by using much less training examples (e.g., 12.5% of the full e-SNLI dataset).\nFigure 5: Similar to the plots above, we compare Distilling step-by-step and Standard task distillation using 220M\nT5 models on varying sizes of unlabeled datasets. Distilling step-by-step is able to outperform Standard task\ndistillation by using only a small subset of the full unlabeled dataset (e.g., 12.5% on ANLI dataset).\n4.2 Reducing model size\nIn the following set of experiments, we hold the\ntraining set size ﬁxed (using 100% of the datasets),\nand compare varying sizes of small T5 models\ntrained with Distilling step-by-step and standard\napproaches to LLMs. Speciﬁcally, we consider 3\ndifferent sizes of T5 models, i.e., 220M T5-Base,\n770M T5-Large, and 11B T5-XXL. For LLMs,\nwe include two baseline methods: (1) FEW-SHOT\nCOT (Wei et al., 2022), and (2) PINTO TUN -\nING (Wang et al., 2022a). Few-shot CoT directly\nutilizes CoT demonstrations to prompt the 540B\nPaLM to generate intermediate steps before pre-\ndicting the ﬁnal labels without any further ﬁne-\ntuning of the LLM. PINTO tuning refers to our\nextension of Wang et al. (2022a) to handle tasks\nbeyond question-answering, which are not stud-\nied by Wang et al. (2022a). Here, we ﬁnetune a\n220M T5-Base model on top of the outputs gener-\nated from the PaLM model, which can be viewed\nas a ﬁnetuning method for LLMs with additional\nparameters (Zhang et al., 2020; Lester et al., 2021).\nWe present the experimental results under the\ntwo broad scenarios of having access to labeled\ndatasets or unlabeled datasets in Figure 6 and Fig-\nure 7, respectively. We plot each method by their\ndeployed model sizes for prediction (x-axis), and\ntheir corresponding task performances (y-axis).\nDistilling step-by-step improves over standard\nbaselines across varying model sizes used. In\nFigure 6 and Figure 7 respectively, we see that\nDistilling step-by-step consistently improves over\nstandard ﬁnetuning and standard distillation across\nall sizes of T5 models. The improvements are most\npronounced on ANLI, where Distilling step-by-\nstep outperforms standard ﬁnetuning and distilla-\ntion by an average of8% and 13% on task accuracy\nrespectively.\nDistilling step-by-step outperforms LLMs by\nusing much smaller task-speciﬁc models. In\nFigure 6 when human-labeled datasets are avail-\nable, Distilling step-by-step can always outper-\nform Few-shot CoT and PINTO tuning on all 4\ndatasets considered, by using much smaller T5\nmodels. For instance, we can achieve better perfor-\nmances than 540B PaLM model’s Few-shot CoT\n8008\nFigure 6: We perform Distilling step-by-step and Standard ﬁnetuning, using the full human-labeled datasets, on\nvarying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO\nTuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over\n700×smaller model on ANLI. Standard ﬁnetuning fails to match LLM’s performance using the same model size.\nFigure 7: Using unlabeled datasets, we perform Distilling step-by-step and Standard task distillation on varying\nsizes of T5 models and compare them to Few-shot CoT. Distilling step-by-step outperforms Few-shot CoT by using\n2000×smaller models on e-SNLI and 45×smaller models on ANLI and CQA. On SV AMP, by adding unlabeled\nexamples from ASDiv, we close the gap to Few-shot CoT whereas Standard distillation still struggles to catch up.\nwith 220M (over 2000×smaller) T5 model on e-\nSNLI, 770M (over 700×smaller) T5 models on\nANLI and SV AMP, and11B (over 45×smaller)\nT5 model on CQA. These results hold true even\nby further ﬁnetuning the 540B PaLM model on\navailable labeled data with PINTO tuning3.\nIn Figure 7, by only utilizing unlabeled exam-\nples, Distilling step-by-step also outperforms the\nteacher LLM on 3 out of 4 datasets. Speciﬁcally,\nDistilling step-by-step surpasses the 540B PaLM\nmodel’s Few-shot CoT performance by using11B\nT5 with less than 3% of PaLM’s size. On SV AMP\nwhere the distilled model underperforms, we hy-\npothesize that the performance gap is due to the\nrelatively small number of data points in the dataset\n(i.e., 800). In reaction, we propose to augment the\ndataset with additional unlabeled examples to close\nthe performance gap as shown in next.\n3We note that PETuning methods may outperform PINTO\ntuning. However, they require massive resource in both train-\ning and deployment, which is not the focus of this work.\nUnlabeled data augmentation further im-\nproves Distilling step-by-step. We augment the\nSV AMP training set with unlabeled examples from\nthe ASDiv dataset (Miao et al., 2020). ASDiv\ndataset contains a total of 2,305 examples, where\neach example is a math word problem similar to the\nones in SV AMP. In Figure 7 on SV AMP, we show\nthe performances of Distilling step-by-step and\nstandard task distillation using 11B T5 model after\naugmenting the training set with ASDiv. We see\nthe data augmentation much improves the perfor-\nmance for both Distilling step-by-step and standard\ntask distillation. However, even with the added\nunlabeled examples, standard task distillation still\nunderperforms Few-shot CoT. On the other hand,\nDistilling step-by-step is able to much more efﬁ-\nciently exploit the value of the added examples to\nachieve the same performance level of Few-shot\nCoT, again, using a T5 model of size less than 3%\nof the 540B PaLM.\n8009\nFigure 8: We show the minimum size of T5 models and the least amount of human-labeled examples required\nfor Distilling step-by-step to outperform LLM’s Few-shot CoT by a coarse-grained search. Distilling step-by-step\nis able to outperform Few-shot CoT using not only much smaller models, but it also achieves so with much less\ntraining examples compared to Standard ﬁnetuning. On ANLI, we outperform the LLM CoT with a 770M model\nusing only 80% of the dataset, whereas Standard ﬁnetuning struggles to match even using 100% of the dataset.\nFigure 9: Similar to Figure 8 but using only unlabeled examples, Distilling step-by-step is able to outperform\nFew-shot CoT using much smaller models and with much less examples compared to Standard task distillation.\nOn SV AMP, thex-axis corresponds to the size of ASDiv dataset used for augmenting the original SV AMP dataset,\ni.e., x= 0is without augmentation and x= 100corresponds to adding the full ASDiv dataset.\n4.3 Outperforming LLMs using minimum\nmodel size and least training data\nHere, using the LLM’s performance as an anchor\npoint, we explore the most efﬁcient resource re-\nquirements in terms of both number of training\nexamples and deployed model size, that Distill-\ning step-by-step and standard ﬁnetuning/distillation\nneed to outperform the LLM. We present the re-\nsults, again under human-labeled setting and unla-\nbeled setting, in Figure 8 and Figure 9 respectively.\nWe visualize the results by plotting different resul-\ntant models by (1) the number of training exam-\nples used (x-axis), (2) the ﬁnal task performance\nachieved (y-axis), and (3) the size of the model\n(visualized by the size of the shaded area).\nDistilling step-by-step outperforms LLMs with\nmuch smaller models by using less data. On\nall datasets in Figure 8, we see that Distilling step-\nby-step outperforms PaLM’s Few-shot CoT with\nmuch smaller T5 models using only a subset of\nthe available training examples. Speciﬁcally, on\ne-SNLI, Distilling step-by-step can achieve bet-\nter performance than Few-shot CoT with a model\nover 2000×smaller (220M T5) and only 0.1% of\nthe full dataset. In Figure 9 where only unlabeled\ndatasets are available, we observe the same trend\nthat Distilling step-by-step can, at most time, out-\nperform Few-shot CoT with smaller model as well\nas less data. For instance, on ANLI, Distilling step-\nby-step outperforms the LLM with a 45×smaller\nmodel and 50% of the full unlabeled set.\nStandard ﬁnetuning and distillation require\nmore data and larger model. Finally, in Fig-\nure 8 and Figure 9, we see that standard ﬁnetuning\nand distillation often need either more data or larger\nmodels to match LLM’s performance. For instance,\non e-SNLI in Figure 8, we observe that Distilling\nstep-by-step outperform the LLM using only 0.1%\nof the dataset while standard ﬁnetuning requires\nmore data to match the performance. Furthermore,\non ANLI in Figure 8, we observe that Distilling\nstep-by-step can outperform PaLM using 770M\nmodel with only 80% of the training set while stan-\ndard ﬁnetuning struggles to match the LLM even\n8010\nTable 1: Distilling step-by-step works with different\nsizes of LLMs. When rationales are extracted from a\n20B GPT-NeoX model, Distilling step-by-step is still\nable to provide performance lift compared to standard\nﬁnetuning on 220M T5 models.\nDataset\nMethod LLM e-SNLI ANLI CQA SV AMP\nSTANDARD FINETUNINGN/A 88.38 43.58 62.19 62.63\nDISTILLING STEP-BY-STEP 20B 89.12 48.15 63.25 63.00\nDISTILLING STEP-BY-STEP 540B 89.51 49.58 63.29 65.50\nusing the full dataset and thus requires larger model\nto close the performance gap.\n4.4 Further ablation studies\nSo far, we have focused on showing the effective-\nness of Distilling step-by-step on reducing the train-\ning data required for ﬁnetuning or distilling smaller\ntask-speciﬁc models. In this section, we perform\nfurther studies to understand the inﬂuence of dif-\nferent components in the Distilling step-by-step\nframework. Speciﬁcally, we study (1) how differ-\nent LLMs, from which the rationales are extracted,\naffect the effectiveness of Distilling step-by-step,\nand (2) how the multi-task training approach com-\npares to other potential design choices in training\nsmall task-speciﬁc models with LLM rationales.\nHere, we ﬁx the small task-speciﬁc models to be\n220M T5 models, and utilize 100% of the data on\nall datasets.\nDistilling step-by-step works with different\nsizes of decently trained LLMs. In addition\nto using 540B PaLM as the LLM, here we con-\nsider a relatively smaller LLM, 20B GPT-NeoX\nmodel (Black et al., 2022), from which we extract\nrationales for Distilling step-by-step. In Table 1,\nwe see that when coupled with LLMs of different\nsizes, Distilling step-by-step can still provide per-\nformance improvements compared to standard ﬁne-\ntuning. However, the performance lift is smaller\nwhen rationales are extracted from the 20B GPT-\nNeoX model instead of from the 540B PaLM. This\ncan be due to the fact that the larger PaLM model\nprovides higher-quality rationales that are more\nbeneﬁcial for learning the task.\nMulti-task training is much more effective than\nsingle-task rationale and label joint prediction.\nThere are different possible ways to train task-\nspeciﬁc models with LLM-rationales as output su-\npervisions. One straightforward approach is to con-\ncatenate the rationale ˆri and label ˆyi into a single\nTable 2: Our proposed multi-task training framework\nconsistently leads to better performances than treating\nrationale and label predictions as a single task. Single-\ntask training can at times lead to worse performance\nthan standard ﬁnetuning.\nDataset\nMethod e-SNLI ANLI CQA SV AMP\nSTANDARD FINETUNING 88.38 43.58 62.19 62.63\nSINGLE-TASK TRAINING 88.88 43.50 61.37 63.00\nMULTI-TASK TRAINING 89.51 49.58 63.29 65.50\nsequence [ˆri,ˆyi] and treat the entire sequence as\nthe target output in training small models, as con-\nsidered in (Magister et al., 2022; Ho et al., 2022):\nLsingle = 1\nN\nN∑\ni=1\nℓ(f(xi),[ˆri,ˆyi]). (5)\nIn Table 2, we compare this single-task training\napproach to our proposed multi-task training ap-\nproach for utilizing LLM-rationales. We see that\nnot only multi-task training consistently leads to\nbetter performance, single-task training with LLM-\nrationales can at times leads to worse performance\nthan standard ﬁnetuning, e.g., on ANLI and CQA.\nIn fact, similar results have also been observed\nin (Wiegreffe et al., 2021; Magister et al., 2022;\nHo et al., 2022) that simply treating rationale and\nlabel predictions as a single joint task may harm the\nmodel’s performance on label prediction. This val-\nidates our use of the multi-task training approach,\nand highlights the need to treat the rationales care-\nfully so as to unleash their actual beneﬁts.\n5 Discussion\nWe propose Distilling step-by-step to extract ra-\ntionales from LLMs as informative supervision in\ntraining small task-speciﬁc models. We show that\nDistilling step-by-step reduces the training dataset\nrequired to curate task-speciﬁc smaller models; it\nalso reduces the model size required to achieve,\nand even surpass, the original LLM’s performance.\nDistilling step-by-step proposes a resource-efﬁcient\ntraining-to-deployment paradigm compared to ex-\nisting methods. Further studies demonstrate the\ngeneralizability and the design choices made in\nDistilling step-by-step. Finally, we discuss the lim-\nitations, future directions and ethics statement of\nour work below.\n8011\nLimitations\nThere are a number of limitations with our ap-\nproach. First, we require users to produce a few\nexample demonstrations (∼10-shot for all tasks)\nin order to use the few-shot CoT (Wei et al., 2022)\nprompting mechanism. This limitation can be\nimproved by using recent advances that suggest\nthat rationales can be elicited without any user-\nannotated demonstrations (Kojima et al., 2022).\nSecond, training task-speciﬁc models with ratio-\nnales incur slight training-time computation over-\nhead. However, at test time, our multi-task design\nnaturally avoids the computation overhead since it\nallows one to only predict labels without generat-\ning the rationales. Finally, while we observe suc-\ncess using LLM rationales, there is evidence that\nLLMs exhibit limited reasoning capability on more\ncomplex reasoning and planning tasks (Valmeekam\net al., 2022). Future work should characterize how\nrationale quality affects Distilling step-by-step.\nEthics statement\nIt is worth noting that the behavior of the our down-\nstream smaller models is subject to biases inherited\nfrom the larger teacher LLM. We envision that the\nsame research progress in reducing anti-social be-\nhaviors in LLMs can also be applied to improve\nsmaller language models.\nReferences\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua\nMaynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev,\nDipanjan Das, and Mirella Lapata. 2022. Qameleon:\nMultilingual qa with only 5 examples. arXiv\npreprint arXiv:2211.08264.\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\nrel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-\neric Sala, and Christopher Ré. 2022. Ask me any-\nthing: A simple strategy for prompting language\nmodels. arXiv preprint arXiv:2210.02441.\nLucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Mar-\nkeeva, Rohan Anil, and Alexander Kolesnikov. 2022.\nKnowledge distillation: A good teacher is patient\nand consistent. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10925–10934.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In Pro-\nceedings of the 12th ACM SIGKDD international\nconference on Knowledge discovery and data min-\ning, pages 535–541.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-\nural language inference with natural language expla-\nnations. Advances in Neural Information Process-\ning Systems, 31.\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\nmad Norouzi, and Geoffrey E Hinton. 2020. Big\nself-supervised models are strong semi-supervised\nlearners. Advances in neural information process-\ning systems, 33:22243–22255.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJacob Eisenstein, Daniel Andor, Bernd Bohnet,\nMichael Collins, and David Mimno. 2022. Hon-\nest students from untrusted teachers: Learning\nan interpretable question-answering pipeline from\na pretrained language model. arXiv preprint\narXiv:2210.02498.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\nMazare, and Jason Weston. 2019. Learning from\ndialogue after deployment: Feed yourself, chatbot!\narXiv preprint arXiv:1901.05415.\nPeter Hase and Mohit Bansal. 2021. When can mod-\nels learn from explanations? a formal framework for\nunderstanding the roles of explanation data. arXiv\npreprint arXiv:2102.02201.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\n2022. Large language models are reasoning teach-\ners. arXiv preprint arXiv:2212.10071.\n8012\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nFotis Iliopoulos, Vasilis Kontonis, Cenk Baykal, Gau-\nrav Menghani, Khoa Trinh, and Erik Vee. 2022.\nWeighted distillation with unlabeled examples. In\nAdvances in Neural Information Processing Sys-\ntems.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. arXiv\npreprint arXiv:2205.11916.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nKevin J Liang, Weituo Hao, Dinghan Shen, Yufan\nZhou, Weizhu Chen, Changyou Chen, and Lawrence\nCarin. 2020. Mixkd: Towards efﬁcient distilla-\ntion of large-scale language models. arXiv preprint\narXiv:2011.00593.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and develop-\ning english math word problem solvers. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 975–984.\nSmitha Milli, Ludwig Schmidt, Anca D Dragan, and\nMoritz Hardt. 2019. Model reconstruction from\nmodel explanations. In Proceedings of the Confer-\nence on Fairness, Accountability, and Transparency,\npages 1–9.\nSharan Narang, Colin Raffel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\nWt5?! training text-to-text models to explain their\npredictions. arXiv preprint arXiv:2004.14546.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra,\nLivio Baldini Soares, Michael Collins, Zachary C\nLipton, Graham Neubig, and William W Cohen.\n2022. Evaluating explanations: How much do ex-\nplanations from the teacher aid students? Transac-\ntions of the Association for Computational Linguis-\ntics, 10:359–375.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain yourself!\nleveraging language models for commonsense rea-\nsoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4932–4942, Florence, Italy. Association\nfor Computational Linguistics.\nAndrew Slavin Ross, Michael C Hughes, and Finale\nDoshi-Velez. 2017. Right for the right reasons:\nTraining differentiable models by constraining their\nexplanations. arXiv preprint arXiv:1703.03717.\nRyan Smith, Jason A Fries, Braden Hancock, and\nStephen H Bach. 2022a. Language models in the\nloop: Incorporating prompting into weak supervi-\nsion. arXiv preprint arXiv:2205.02318.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022b. Using\ndeepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990.\nSuraj Srinivas and François Fleuret. 2018. Knowledge\ntransfer with jacobian matching. In International\nConference on Machine Learning, pages 4723–4731.\nPMLR.\n8013\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498.\nPeifeng Wang, Aaron Chan, Filip Ilievski, Muhao\nChen, and Xiang Ren. 2022a. Pinto: Faithful lan-\nguage reasoning using prompt-generated rationales.\narXiv preprint arXiv:2211.01562.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce\nlabeling cost? gpt-3 can help. arXiv preprint\narXiv:2108.13487.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing\nLu, Sean Welleck, and Yejin Choi. 2021. Sym-\nbolic knowledge distillation: from general language\nmodels to commonsense models. arXiv preprint\narXiv:2110.07178.\nSarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith.\n2021. Measuring association between labels and\nfree-text rationales. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 10266–10284, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007.\nUsing “annotator rationales” to improve machine\nlearning for text categorization. In Human Lan-\nguage Technologies 2007: The Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics; Proceedings of the Main\nConference, pages 260–267, Rochester, New York.\nAssociation for Computational Linguistics.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reasoning.\narXiv preprint arXiv:2203.14465.\nJeffrey O Zhang, Alexander Sax, Amir Zamir,\nLeonidas Guibas, and Jitendra Malik. 2020. Side-\ntuning: a baseline for network adaptation via ad-\nditive side networks. In European Conference on\nComputer Vision, pages 698–714. Springer.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nYe Zhang, Iain Marshall, and Byron C. Wallace. 2016.\nRationale-augmented convolutional neural networks\nfor text classiﬁcation. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 795–804, Austin, Texas.\nAssociation for Computational Linguistics.\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao\nZhuang, Zhifeng Chen, Yanping Huang, Yida Wang,\nYuanzhong Xu, Danyang Zhuo, Joseph E Gonza-\nlez, et al. 2022. Alpa: Automating inter-and intra-\noperator parallelism for distributed deep learning.\narXiv preprint arXiv:2201.12023.\n8014\nA Experiment detail\nA.1 Implementation\nWe perform our experiments on cloud A100 ×16\nGPU instances. We train the T5 models with\nthe following hyperparameters, using publicly\navailable packages from https://github.com/\nhuggingface/transformers:\n• T5-Base (220M) and T5-Large (770M): We\ntrain the models with learning rate = 5 ×\n10−5, batch size = 64, max input length =\n1024, for a maximum of 10000 steps.\n• T5-XXL ( 11B): We train the models with\nlearning rate = 5 ×10−5, batch size = 32,\nmax input length = 1024, for a maximum of\n4000 steps.\nWe report all the results over 4 random runs, and\ninclude the standard error in the presented plots.\nA.2 Datasets\nWe provide more detailed descriptions on the\ndatasets used in our experiments. We include the\nsources from which we obtain the datasets as well\nas their original sources released from the authors.\nWe refer readers to these sources for their license or\nterms for use and/or distribution. To the best of our\nknowledge, the datasets used do not contain infor-\nmation that names or uniquely identiﬁes individual\npeople or offensive content.\n• e-SNLI: The dataset was originally re-\nleased in (Camburu et al., 2018), and made\npublicly available at https://github.com/\nOanaMariaCamburu/e-SNLI. We obtain\nthe dataset from https://huggingface.co/\ndatasets/esnli.\n• ANLI: The dataset was originally released\nin (Nie et al., 2020), and made pub-\nlicly available at https://github.com/\nfacebookresearch/anli. We obtain the\ndataset from https://huggingface.co/\ndatasets/anli. We use the R1 split in our\nexperiments.\n• CQA: The dataset was originally released\nin (Talmor et al., 2019), and made publicly\navailable at https://www.tau-nlp.sites.\ntau.ac.il/commonsenseqa. It was then\naugmented with human-labeled explanations\nTable 3: Dataset statistics used in our experiments.\nDataset Train Validation Test\ne-SNLI 549,367 9,842 9,824\nANLI 16,946 1,000 1,000\nCQA 8,766 975 1,221\nSV AMP 720 80 200\nby (Rajani et al., 2019), which is avail-\nable at https://github.com/salesforce/\ncos-e. We obtain the dataset used in our ex-\nperiments from https://huggingface.co/\ndatasets/cos_e.\n• SV AMP: The dataset was originally re-\nleased in (Patel et al., 2021). We ob-\ntain the dataset from https://github.com/\narkilpatel/SVAMP.\n• ASDiv: The dataset was originally re-\nleased in (Miao et al., 2020). We ob-\ntain the dataset from https://github.com/\nchaochun/nlu-asdiv-dataset.\nFor each dataset, we randomly subsample 10%\nof the original training set to serve as validation set\nwhen validation set is not originally provided. For\nCQA, we use the original validation set to serve\nas our test set since the ground-truth labels are not\navailable for the original test set. We provide the\ndataset statistics in Table 3.\n8015\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 6\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nAppendix\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4, Appendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n8016\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4, Appendix\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4, Appendix\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4, Appendix\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n8017",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.7437925338745117
    },
    {
      "name": "Association (psychology)",
      "score": 0.5724758505821228
    },
    {
      "name": "Computer science",
      "score": 0.568211555480957
    },
    {
      "name": "Natural language processing",
      "score": 0.4529796838760376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4478708505630493
    },
    {
      "name": "Data modeling",
      "score": 0.4236030876636505
    },
    {
      "name": "Computational linguistics",
      "score": 0.412163645029068
    },
    {
      "name": "Algorithm",
      "score": 0.3553122878074646
    },
    {
      "name": "Linguistics",
      "score": 0.3301701545715332
    },
    {
      "name": "Philosophy",
      "score": 0.2714349627494812
    },
    {
      "name": "Epistemology",
      "score": 0.10222834348678589
    },
    {
      "name": "Geology",
      "score": 0.0950273871421814
    },
    {
      "name": "Database",
      "score": 0.08214634656906128
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}