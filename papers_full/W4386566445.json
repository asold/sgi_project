{
  "title": "Combining Psychological Theory with Language Models for Suicide Risk Detection",
  "url": "https://openalex.org/W4386566445",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5056192898",
      "name": "Daniel Izmaylov",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A5089712688",
      "name": "Avi Segal",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A5024303852",
      "name": "Kobi Gal",
      "affiliations": [
        "University of Edinburgh",
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A5016543509",
      "name": "Meytal Grimland",
      "affiliations": [
        "Ruppin Academic Center"
      ]
    },
    {
      "id": "https://openalex.org/A5051456030",
      "name": "Yossi Levi‐Belz",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285232264",
    "https://openalex.org/W3127069001",
    "https://openalex.org/W4287366208",
    "https://openalex.org/W3175475730",
    "https://openalex.org/W3100359458",
    "https://openalex.org/W3049024012",
    "https://openalex.org/W3092071453",
    "https://openalex.org/W2998535576",
    "https://openalex.org/W3121559951",
    "https://openalex.org/W2031597326",
    "https://openalex.org/W2970205254",
    "https://openalex.org/W2900152803",
    "https://openalex.org/W2807452501",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3007531441",
    "https://openalex.org/W2914765137",
    "https://openalex.org/W4230435932",
    "https://openalex.org/W3109558947",
    "https://openalex.org/W1932968309",
    "https://openalex.org/W4295689568",
    "https://openalex.org/W78677904",
    "https://openalex.org/W2123612404",
    "https://openalex.org/W172260869",
    "https://openalex.org/W3164870625",
    "https://openalex.org/W3090352232",
    "https://openalex.org/W2957144537",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2889391310",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1901708246",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W3036340576",
    "https://openalex.org/W2911378332"
  ],
  "abstract": "With the increased awareness of situations of mental crisis and their societal impact, online services providing emergency support are becoming commonplace in many countries. Computational models, trained on discussions between help-seekers and providers, can support suicide prevention by identifying at-risk individuals. However, the lack of domain-specific models, especially in low-resource languages, poses a significant challenge for the automatic detection of suicide risk. We propose a model that combines pre-trained language models (PLM) with a fixed set of manually crafted (and clinically approved) set of suicidal cues, followed by a two-stage fine-tuning process.Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly outperforming an array of strong baselines even early on in the conversation, which is critical for real-time detection in the field. Moreover, the model performs well across genders and age groups.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2430–2438\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nCombining Psychological Theory with Language Models for Suicide Risk\nDetection\nDaniel Izmaylov1\nizmaylov@post.bgu.ac.il\nAmir Bialer1\namirbial@post.bgu.ac.il\nAvi Segal1\navisegal@gmail.com\nMeytal Grimland2\nmeytal.grimland@gmail.com\nYossi Levi-Belz2\nyossil@ruppin.ac.il\nKobi Gal1,3\nkobig@bgu.ac.i\n1Ben-Gurion University of the Negev 2Ruppin Academic Center 3University of Edinburgh\nAbstract\nRecent years saw a dramatic increase in the pop-\nularity of online counseling services providing\nemergency mental health support. This paper\nprovides a new language model for automatic\ndetection of suicide risk in online chat sessions\nbetween help-seekers and counselors. The\nmodel adapts a hierarchical BERT language\nmodel for this task. It extends the state of the art\nin capturing aspects of the conversation struc-\nture in the counseling session and in integrating\npsychological theory into the model. We test\nthe performance of our approach in a leading\nnational online counseling service that operates\nin the Hebrew language. Our model outper-\nformed other non-hierarchical approaches from\nthe literature, achieving a 0.76 F2 score and\n0.92 ROC-AUC. Moreover, we demonstrate\nour model’s superiority over strong baselines\neven early on in the conversation, which is key\nfor real-time detection in the field. This is a first\nstep towards incorporating suicide predictive\nmodels in online support services and advanc-\ning NLP tools for resource-bounded languages.\n1 Introduction\nSuicide accounts for more than 700,000 lives lost\nacross the world every year. It is the second leading\ncause of death for adolescents and adults from 15\nto 29 years of age in many countries. A key effort\nin suicide prevention is to identify individuals at\nrisk of suicide as early as possible (World-Health-\nOrganization, 2021).\nIn the past decade, online counseling services\nfor mental health support have become common-\nplace in many countries, providing chat support\nand guidance to at-risk individuals (see fictitious\nexample in Figure 1). Online counseling services\naim to provide mental support and address a va-\nriety of mental health crises through specialist\ncounselors. These counselors are trained to de-\ntect suicide risk during conversations and inter-\nvene quickly as needed. These services have ex-\nFigure 1: A fictitious example of a conversation.\nperienced tremendous growth in traffic since the\ncommencement of the COVID pandemic (Zalsman\net al., 2021). Any kind of technological support to\nhelp counselors in this critical task can potentially\nsave lives.\nThis paper provides a computational model for\ndetection of suicide risk from anonymous text-\nbased discussions between help-seekers and coun-\nselors. Our data is taken from an online counsel-\ning service in a low-resource language (Hebrew).\nThere are several challenges towards solving sui-\ncide risk detection in our setting: State of the art\npre-trained language models for suicide prevention\nusually focus on posts from social media which\nare very different in structure from online conver-\nsations between counselors and help-seekers. Ex-\nisting works that do consider conversations in this\ndomain ignore the conversation structure or are\nlimited in the size of the conversation they con-\nsider. Also, the set of NLP resources available for\nlow-resource languages is extremely limited when\ncompared to English.\nTo address this gap, we present a hierarchical\nlanguage model called SR-BERT that includes a\nbase layer for encoding the conversation text and\nan additional layer for capturing aspects of con-\nversation structure. The hierarchical structure of\nSR-BERT encodes each of the messages in the con-\nversation separately and is not limited by the size\n2430\nof the conversation. We hypothesized that incorpo-\nrating knowledge from suicide risk theory as part\nof the pre-training step can improve downstream\nperformance of the detection model. To this end,\nwe develop a new domain knowledge-based pre-\ntraining step that embeds a Suicide Risks Factor\nlexicon (SRF) into SR-BERT. The SRF lexicon was\ncreated by a team of psychologists who are experts\non suicide risk theory and prevention.\nIn empirical studies, SR-BERT significantly out-\nperforms alternative classifiers for suicide risk (SR)\ndetection, including the state-of-the-art (Bialer\net al., 2022). We show that adding the domain-\nexpert information to SR-BERT plays a critical part\nin its performance. In particular, it obtained consis-\ntently better performance than Bialer et al. (2022)\nwhen processing different portions of the conver-\nsation. These findings suggest that SR-BERT can\nperform well in the field when analyzing conversa-\ntions in real-time.\nWe extend the state-of-the-art hierarchical lan-\nguage models to combine conversation structure\nand expert-based knowledge in the pretraining step.\nWe show this approach leads to significant in-\ncreases in performance for detecting suicide risk\nfrom chat conversations.\n2 Related Work\nThis paper relates to past studies in suicide risk\ndetection in online settings, representing domain\nknowledge and conversation structure in deep lan-\nguage models and NLP tools for low-resource lan-\nguages. We expand on each of these topics in turn.\nFor a review on using machine learning in suicide\nprevention, we refer the reader to Ji et al. (2021).\nThe majority of work using machine learning to\npredict suicide risk analyzes posts from social me-\ndia (Coppersmith et al., 2018; Zirikly et al., 2019;\nShing et al., 2018; Sawhney et al., 2018; Tadesse\net al., 2019). Recent works in this social media\nsuicide prediction space includes Cao et al. (2019)\nwho used an LSTM with an additional attention\nlayer to predict SR from social media posts, and\nWang et al. (2021) who combined a generic BERT\nmodel with predefined rules for scoring suicide risk\nin social media. Additionally, Ophir et al. (2020)\nshowed that psychological questionnaires can im-\nprove the performance of neural networks to iden-\ntify at-risk individuals from Facebook posts. We\nsignificantly differ from the social media setting\nin our focus on conversations from online coun-\nseling services, where messages are significantly\nlonger than social media posts, and messages are\npart of a conversational structure and exhibit psy-\nchological dynamics. We show that capturing these\naspects in the conversation model is necessary for\nrecognizing SR in our setting.\nThere are few works on suicide detection in on-\nline counseling conversations, but none of these\nreasons about the conversation structure in the ses-\nsion. Most relevant to our approach is the model by\nBialer et al. (2022) who combined a pre-trained lan-\nguage model based on BERT (Devlin et al., 2018)\nwith a lexicon of suicide terms that were manu-\nally extracted from conversations. This model was\nable to represent only part of the conversation (512\ntokens) and ignored the input from the counselor.\nOur SR-BERT model used a lexicon extracted from\npsychological theory, which was embedded in the\npre-training process. The model significantly out-\nperforms that of Bialer et al. (2022) on the same\ndataset, both for entire conversations as well as\nwhen considering early detection on parts of the\nconversation.\nWe mention two approaches for detecting SR in\ncounseling services that did not consider early de-\ntection. Xu et al. (2021) combined a word2vec rep-\nresentation of suicide concepts with a bi-directional\nLSTM network for SR prediction in Korean online\ncounseling service. Each side of the conversation\nwas represented by an independent BI-LSTM. This\napproach used a knowledge graph to represent a\npsychological lexicon which may be more time-\nconsuming for human experts to construct. Our\nmodel is shown to outperform a baseline using\na similar representation (doc2vec) on our dataset.\nBantilan et al. (2021) used TF-IDF embedding with\nXGBoost to predict SR in transcribed phone calls\nfrom an English counseling service. This model\ndid not use a lexicon.\nThere is ample evidence on the benefits of in-\ncorporating domain knowledge in language models\nfor downstream tasks (Childs and Washburn, 2019;\nCao et al., 2019; Lee et al., 2020; Colon-Hernandez\net al., 2021; Gaur et al., 2019). Notable examples\ninclude Gaur et al. (2019) and Wang et al. (2021)\nwho showed that using lexicon-based features can\nimprove machine learning prediction of suicide risk\nin Chinese blogs. They use lexicons to map terms\nfrom online discussions to clinically relevant sets\nof categories. We extend these approaches by pre-\nsenting a new method for incorporating domain\n2431\nknowledge in the pre-training phase of deep learn-\ning models.\nIn general, NLP models and solutions for low-\nresource languages are extremely limited. In He-\nbrew, two pre-trained language models were pub-\nlished, HeBERT(Chriqui and Yahav, 2021) and\nAlephBERT (Seker et al., 2022). We used Aleph-\nBERT which is freely available and was trained on\na larger dataset than HeBERT and was able to out-\nperform HeBERT on a variety of natural language\ntasks. We are first to use hierarchical transformer\narchitecture to model conversation structures in a\nlow-resource language.\n3 The Sahar domain\nSahar (Hebrew acronym for Online Mental Health\nSupport 1) was established in 2000 and is the lead-\ning internet-based emotional support and suicide\nprevention organization in Israel. It provides anony-\nmous, confidential, and free crisis support via a\nchat hotline (in Hebrew and in Arabic). The orga-\nnization handles more than 40,000 chat sessions\nper year, and these numbers have increased signifi-\ncantly during the COVID-19 pandemic (Zalsman\net al., 2021).\nSahar counselors are volunteers who receive\nyear-round guidance and supervision from a team\nof mental health professionals. Shifts take place in\nthe evening hours and are accompanied by trained\ntherapists who monitor the conversations and pro-\nvide professional support to counselors as needed.\nDuring the shifts, counselors work in a high-stress\nenvironment and usually handle multiple chat ses-\nsions in parallel at any given time. Counselors pro-\nvide a written summary of each of their conversa-\ntions, as well as indicate whether the conversation\nexhibits suicide risk.\nThe Sahar corpus contains more than 40,000\nchat sessions (conversations) that took place over\nthe span of five years (2017-2022). Each conversa-\ntion includes the messages generated by the help-\nseekers and the counselors, ordered by time sig-\nnatures. Table 1 presents general statistics about\nthe dataset. We note that 39.5% of the sessions are\nlabeled with either positive or negative SR label\nand 17% of these sessions are SR positive.\nTo validate the SR labels, a sample of 600 con-\nversations (300 positive SR, 300 negative SR) was\nlabeled separately by clinical psychologists with\nexpertise in suicide theory. The Krippendorff’s α\n1https://sahar.org.il\nTable 1: General statistics for Sahar corpus\nTotal num. of sessions 44,506\nNum. of labeled sessions 17,564\nSR positive label ratio 17%\nMean(Median) num. of messages 57(46)\nMean(Median) num. of turn exchanges 27(25)\nMean(Median) num. of tokens 617(566)\nfor inter-annotator agreement between the psychol-\nogists and the SR label in the conversation is 0.766,\nwhich is en par with other works. We note that the\ninconsistencies found in the samples were debated\nby the psychologists and resolved in the data set.\n4 The SRF Psychological Lexicon\nAs part of our research, a team of psychology ex-\nperts from a national center for suicide prevention\nin Israel has constructed a Suicide-Risk Factors\nLexicon (SRF) in Hebrew that is based on psycho-\nlogical theory.\nThe SRF lexicon contains terms relating to per-\nsonal and situational variables associated with an\nincrease in suicidal thinking, based on valid self-\nreport questionnaires in the psychological and psy-\nchiatric literature (Klonsky and May, 2015; Turecki\nand Brent, 2016; Nock et al., 2008).\nEach of the 3,094 sentences in the lexicon be-\nlonged to one of 25 categories. Specifically, terms\nrelating to depression are taken from the Patient\nHealth Questionnaire Depression Module (PHQ-\n9) (Kroenke et al., 2001). Terms relating to a sense\nof burdensomeness are taken from the Interper-\nsonal Needs Questionnaire (INQ) (Van Orden et al.,\n2012). Terms relating to a sense of hopelessness\nare taken from the Beck hopelessness scale (Beck\net al., 1996). Terms relating to suicide behavior\nwere taken from the Columbia questionnaire (Pos-\nner et al., 2008) which is a standard tool to measure\nsuicide risk.\nExamples of sentences for the category “per-\nceived burdensomeness” (translated) included sen-\ntences such as “better without me”, “I am a bur-\nden”, “I spoil everything for my spouse”; and the\nlexicon category “explicit suicide mentions” con-\ntains phrases such as: “to die”, “to commit suicide”,\n“kill myself” etc.\n5 The SR-BERT Language Model\nOur main contribution is SR-BERT, a two-layer hi-\nerarchical language model that extends the generic\nDialogBERT (Gu et al., 2021) to reason about con-\n2432\nFigure 2: Model architecture. (a) SR-BERT base architecture, encoding conversation and speaker roles. (b)\nPre-training procedure on 4 self-supervised tasks including psychological knowledge learning using the SRF lexicon.\n(c) Fine-tuning procedure learning to predict Suicide Risk (SR)\nversation structure in suicide risk prediction set-\ntings and harness psychological domain knowledge.\nThe SR-BERT architecture is shown in Figure 2(a).\nThe architecture is composed of two part: A\ntransformer based layer performing message encod-\ning, and on top of it an additional transformer layer,\nwhich captures conversation structure, named Con-\ntext Encoder Transformer.\nThe base layer uses the AlephBERT (Seker et al.,\n2022) pre-trained language model to encode each\nmessage in the dialogue to a vector. The received\nmessage encoding is then combined with speaker\nrole representation (help-seeker vs. counselor) to\ncapture important conversation aspects such as turn-\ntaking. The Context Encoder Transformer is a\ntransformer based encoder applied at the message\nlevel (instead of the single token level) which trans-\nforms the series of message vectors into a context-\nsensitive repression of the conversation. The Con-\ntext Encoder Transformer included 12 attention\nlayers, and 12 hidden layers, each with a vector\nsize of 780. The hidden layer size is 780 rather\nthan 768 in AlephBert to account for the additional\nspeaker role encoding.\nThe hierarchical structure of the architecture en-\nables the model to capture multiple messages in-\ncluding turn exchanges and speaker roles. Fur-\nthermore, it enable the encoding of each message\nindependently, thus avoiding the need to truncate\nconversations (due to AlephBert’s 512 token limit)\nas in past work.\n5.1 Pre-training with Self Supervised\nKnowledge\nIn this section we describe the use of several pre-\ntraining tasks for adapting SR-BERT to conversa-\ntion structure of online counseling, including a new\npre-training task for incorporating the SRF lexicon.\nThis procedure uses the entire Sahar dataset, and is\nshown in Figure 2 (b).\nThe first step in this process is to represent con-\nversations as a 25 dimension vector representing\nthe different categories in the lexicon. For a given\nconversation, the value at index k is the number\nof sentences in the conversation with at least one\noccurrence in the kth lexicon category.\nWe also considered a reduced 5-dimension rep-\nresentation of conversations on the SRF lexicon\nspace. To this end we selected the top categories us-\ning XGBoost feature selection (Chen and Guestrin,\n2016) on the SR prediction task of entire conver-\nsations. We identified the top 5 categories as “self\nperceived burdensomeness”, “previous suicide at-\ntempt”, “loss of hope” “self injury” and “suicidal\nthinking”. The 5-dimension representation out-\nperformed the 25-dimension representation on the\nvalidation set, leading us to use this representation\nin the subsequent pre-training phase.\nThe second step, called the Self Supervised\nKnowledge task, applies a new pre-training task\nfor predicting Sahar conversations in the SRF rep-\nresentation space. For a given prefix of a conver-\nsation, we mask a message in this subset with a\nfixed probability of 80%. We then use SR-BERT to\npredict the conversation subset’s representation in\nthe SRF space using a fully connected layer. The\nloss is obtained by calculating the mean squared\nerror (MSE) between the original subset represen-\ntation and the predicted (masked) representation\nin the SRF space. This process is repeated for in-\ncreasing size of conversation prefixes, to simulate\nconversations of varying sizes.\nIn addition to the SSK task, we implemented the\nthree pre-training tasks defined by DialogBERT(Gu\net al., 2021) for capturing several aspects of the\nconversation structure: message-level semantics,\n2433\nconversation structure, and underlying dialogue\nsequential order. We describe them briefly here and\nrefer the reader to the full paper for more details.\n• Next Utterance GenerationThe goal of this\ntask is to generate the next message in the\nconversation when the previous messages are\ngiven. The task tries to minimize the cross-\nentropy loss between the predicted words and\nthe original words of the next message.\n• Masked Utterance RegressionThe goal of\nthis task is to predict a randomly masked mes-\nsage in a conversation from its context. The\nloss is obtained by calculating the MSE be-\ntween the original and the predicted message\nvectors.\n• Distributed Order Ranking NetworkThis\ntask predicts the order index of each message\nfrom a shuffled order of a conversation. The\ntask tries to minimize the KL divergence be-\ntween the predicted order and the true order.\nThe calculated loss for the model propagation\nover the four self supervised tasks is the weighted\nsum of each loss function in the pre-training stage.\nThe AdamW optimizer is employed with a linear\nplanned warm-up technique and an initial learn-\ning rate of 5e-5. Additionally, we use an adap-\ntive learning-rate scheduler with 0.01 weight decay,\n15,000 warm-up steps, and a batch size of 32. The\nmodel is trained for 20 epochs. All experiments\nare conducted on a GeForce RTX 3090 GPU using\nthe PyTorch package.\n5.2 Fine-tuning\nIn the fine-tuning step Figure 2(c), SR-BERT is\nadapted for the suicide risk prediction task using a\nstandard approach (Sun et al., 2020). To this end we\nadd a binary classification head to SR-BERT. The\nclassification head consists of a dense layer with an\noutput size of 2 and a softmax activation function.\nBy maximizing the log-likelihood of the actual la-\nbel, we fine-tune the Context Encoder Transformer\nand the classification head. We employ the AdamW\noptimizer with a linear planned warm-up technique\nand an initial learning rate of 2e-5. Additionally,\nwe use an adaptive learning-rate scheduler with\n0.01 weight decay, and a batch size of 16. The\nmodel is trained for 10 epochs.\n6 Empirical Methodology\nWe randomly split the labeled Sahar dataset to a\ntrain (70%) validation (15%) and test (15%) sets.\nThese data sets were used throughout the exper-\niments described in the following section. The\nvalidation set was used for training model hyper\nparameters.\nWe follow prior work in evaluating model perfor-\nmance using ROC-AUC which is widely employed\nin suicide detection research (Bernert et al., 2020).\nAdditionally, we report on the F2-score (Sokolova\net al., 2006) for predicting the positive SR label.\nThis measure concentrates on reducing false nega-\ntives (rather than false positives) and is thus well\nsuited for SR detection where missing a positive\nclass has life threatening implications.\nWe compare SR-BERT with SSK to the follow-\ning baseline models:\n6.1 SR-BERT w.o.SSK\nThis model omits the SSK pre-training task from\nSR-BERT w. SSK. Apart from the SSK pre-\ntraining task this model is identical to SR-BERT\nw. SSK. including the hierarchical structure and\npre-training on the other 3 tasks.\n6.2 Explicit based lexicon + XGBoost\nWe used an XGBoost classifier that was based on an\nencoding of conversations over the explicit suicide\nrelated terms proposed by Bialer et al. (2022). This\nlist includes 67 terms such as “commit suicide”,\n“cut wrists”, “wish to die” etc. We note that explicit\nterms carry very weak signal for SR detection.\n6.3 Ensemble SI-BERT (Bialer et al., 2022)\nThis is a non-hierarchical Hebrew language model\nensembled with a classifier based on the Explicit\nlexcion, that represents the state of the art for SR de-\ntection. It was trained on the same dataset from the\nSahar organization. To bypass BERT’s constraint\nof 512 tokens, Ensemble SI-BERT only utilized\nthe help seeker text and truncated text greater than\n512 tokens. We re-implemented this model with\nthe code and parameters provided by the authors\nand run it on the dataset provided for this research.\nThis is the reported state of the art for this domain\nin the Hebrew language.\n6.4 SRF based lexicon + XGBoost\nAn XGBoost (Chen and Guestrin, 2016) classifier\nbased on the 5-dimension SRF conversations rep-\n2434\nTable 2: SR prediction results of compared models. Bold highlights highest value.\nModel Recall [%] Precision [%] ROC-AUC [%] F2 [%] F1 [%]\nDoc2Vec+XGBoost 31.3 69 .2 64 .7 35 .1 43 .1\nExplicit lexicon+XGBoost 49.2 67 .1 76 .9 52 .3 57 .7\nSRF lexicon + XGBoost 55.1 67 .2 76 .5 57 .1 60 .0\nEnsemble SI-BERT 60.4 70.9 91.3 62 .3 65 .3\nSR-BERT w.o. SSK 72.9 68 .4 92.1 71.9 70 .6\nSR-BERT w. SSK 78.3 68.9 92.1 76 .2 73 .3\nresentation over the SRF lexicon. We note that\nXGBoost outperformed Random Forest and Logis-\ntic Regression as the classifier for this baseline (and\nfor the next two baselines)\nConsider for example one of the sessions which\nincludes the statement “I am having strong stomach\naches since yesterday, I want to die.”. This session\nincludes a term from the Explicit lexicon while it\nis not an SR positive session.\n6.5 Doc2Vec + XGBoost\nAn XGBoost classifier based on an encoding of\neach conversation to a 300-dimensional space us-\ning the Doc2Vec representation(Le and Mikolov,\n2014).\n7 Results\nWe first present the performance of the SR-BERT\nmodel in predicting SR on labeled conversations\ncompared to the proposed baselines. Results are\nthen reported for early SR detection, when increas-\ning percentages of conversation information are\navailable.\n7.1 SR Detection from Complete\nConversation\nTable 2 compares the performance of the SR-BERT\nmodel to the baselines when predicting suicide risk\nfrom complete conversations. As seen in the ta-\nble, both SR-BERT-based models (with and with-\nout SSK pre-training) outperformed the Ensemble\nSI-BERT model in terms of recall, F1, F2, and\nROC-AUC metrics. Most notable improvement\nwas in the recall metric where SR-BERT w.o. SSK\nachieved a 12.5% improvement over the Ensemble\nSI-BERT model, which led to a 9.6% improve-\nment in the F2 metric. Moreover, the additional\nSSK pre-training improved on the SR-BERT w.o.\nSSK results for all metrics except the ROC-AUC\nscore, where it hasn’t change. Ensemble SI-BERT\nachieved the highest precision, which was slightly\nFigure 3: Classification results for early detection of\ntop-performing SR detection approaches\nbetter than SR-BERT w. SSK. It exhibited a sub-\nstantially lower recall score, which correlates to\nlower F1 and F2 values.\nThe SRF lexicon + XGBoost based classifier was\nbetter than the Explicit lexicon + XGBoost classi-\nfier in all measures apart from ROC-AUC. We also\nnote that the BERT based models outperformed the\nnone BERT models on all tested metrics.\nWe used the McNemar paired test for labeling\ndisagreements (Gillick and Cox, 1989) to compare\nbetween the predictions of the different models.\nStatistical significance with p< 0.05 was demon-\nstrated for SR-BERT w. SSK vs. SR-BERT w.o.\nSSK and for SR-BERT w. SSK vs. Ensemble\nSI-BERT.\nOverall SR-BERT w. SSK achieved a substan-\ntial improvement in recall and F2 compared the\nEnsemble SI-BRET of 17.9% and 13.9% respec-\ntively, with only a slight decrease in precision per-\nformance. This is critical in the suicide risk de-\ntection realm where recall is key to identifying\nhelp-seekers at risk and enabling targeted support.\n7.2 Early SR Detection\nEvaluating the ability of SR-BERT to predict SR\nrisk from partial sessions provides an indication\nof its performance in real time, when only part of\nthe session is available. To this end, Figure 3 com-\npares the performance of the different models after\n2435\nreceiving the first {20,40,60,80,100} percent of\nmessages in the session. As seen in the figure, the\nperformance of all models improved as the sessions\nprogressed. However, SR-BERT w. SSK model\nconsistently outperformed the other models, fol-\nlowed by SR-BERT w.o. SSK. The difference in\nperformance between SR-BERT with SSK and SR-\nBERT w.o. SSK was the largest at the beginning of\nthe session and reduced as the sessions advanced.\nThis may indicate the contribution of SR-BERT w.\nSSK to identify risk variables from the lexicon in\nearly stages of the dialogue when information is\nlacking. In contrast, the difference in performance\nbetween SR-BERT w.o. SSK and the Ensemble SI-\nBERT model increases as sessions advance. This\ncould be due to the inability of Ensemble SI-BERT\nto process the lengthy dialogue without having to\ntruncate it, which may result in the loss of impor-\ntant information as sessions develop.\n8 Conclusion and Future Work\nThis work has provided a new automatic approach\nfor suicide risk detection in online conversations\nbetween help-seekers and counselors. Early detec-\ntion of at-risk individuals is a key goal of suicide\nprevention. Our approach extends the state-of-the-\nart in deep language modeling by 1) incorporating\ndomain knowledge relevant to suicide risk detec-\ntion as part of the pre-training step; 2) reasoning\nabout the structure of the conversation between\nhelp-seekers and counselors; 3) adapting to a low-\nresource language (Hebrew). The presented ap-\nproach was able to significantly outperform the\nstate-of-the-art approaches when detecting SR from\ncomplete conversations, as well as early detection\nwhen only part of the conversation is available.\nThese results suggest the model may be able to\nsupport the work of counselors in real chat ses-\nsions, alerting them in real-time to at-risk individu-\nals and enabling quick and focused responses. For\nfuture work, we intend to improve our approach by\ncapturing more aspects of conversations, such as\nprosody (Wilson and Wharton, 2006; Kliper et al.,\n2010) as well as model the mental state dynam-\nics of the help-seeker. We are also extending the\nmodel with explanations to be able to provide jus-\ntifications for predictions made and point to key\nexchanges and phrases that triggered specific pre-\ndictions.\n9 Limitations\nWe note several limitations of this study.\nFirst, our model was evaluated only in the He-\nbrew language. We have not directly compared\nSR-BERT to approaches for detecting suicidal risk\nin non-Hebrew domains, and note that the effec-\ntiveness of the model may vary across different\nlanguages and cultural contexts. It is difficult to\nmake this comparison given the lack of public data\nsets from online counseling services.\nSecond, the proposed approach relies on the exis-\ntence of psychological knowledge for pre-training\nthe SR-BERT model which requires human effort.\nOn the one hand, psychological lexicons already\nexist in English (Lee et al., 2020) and possibly in\nother languages. On the other hand, lexicons inher-\nently suffer from limited coverage, lack of context\nand are expensive to maintain. Sharing domain\nknowledge across research tasks may go a long\nway to overcome these issues. We intend to make\nthe lexicon developed for this research publicly\navailable.\nThird, the annotation of the help seekers’ men-\ntal state was performed by the counselors, rather\nthan the help seekers themselves. While the coun-\nselors underwent a thorough training process last-\ning several months and were monitored by certified\nclinical psychologists, there is still the possibility\nthat they may have misclassified the mental state\nof the help seekers. This issue is prevalent in many\nstudies that rely on observer-reported data.\nFinally, the current model does not provide any\nexplanations for its predictions, which are of high\nimportance in order to support counselors in the\nfield. This is essential in order to ensure that the\nmodel is not merely a means of classification but\ninstead is able to provide valuable insights and\nassistance to counselors. This is a key focus of our\nfuture development plans.\n10 Ethics Statement\nThe present study has been conducted in accor-\ndance with the highest ethical standards and has\nbeen approved by the relevant institutional review\nboard of the participating institutions. All data uti-\nlized in this study, including the Sahar corpus of\nconversations between help-seekers and counselors,\nand the SRF psychological lexicon, have been ob-\ntained in compliance with the IRB. Specifically, the\nSahar dataset has been anonymized and encrypted\nto protect the privacy of the participants, and all\n2436\nhelp-seekers who have provided data for this study\nhave given informed consent for the anonymous\nuse of their sessions for research purposes. The\ncounselors signed consent papers to allow the us-\nage of their text data for the study.\nIt is important to note that despite the model’s\nability to successfully predict SR during the con-\nversation and its demonstrated gender fairness, it\nis not intended to replace human volunteer coun-\nselors. We believe that human involvement is es-\nsential in providing support to help-seekers, and\nthe role of an automated model is to serve as an\naid to counselors, enhancing their ability to assess\nSR rather than replacing them. Our take is that in\nthe future, when such models could be deployed\nin the field (after all necessary approvals and adap-\ntations), they may only act as a \"friendly parrot\"\non the counselors’ shoulders, providing additional\ninsights and supporting their decision-making pro-\ncess in the high load situations these counselors are\nfacing on a daily basis.\nReferences\nNiels Bantilan, Matteo Malgaroli, Bonnie Ray, and\nThomas D. Hull. 2021. Just in time crisis response:\nSuicide alert system for telemedicine psychotherapy\nsettings. 31(3):302–312.\nAaron T Beck, Robert A Steer, and Gregory Brown.\n1996. Beck depression inventory–ii. Psychological\nassessment.\nRebecca A Bernert, Amanda M Hilberg, Ruth Melia,\nJane Paik Kim, Nigam H Shah, and Freddy Abnousi.\n2020. Artificial intelligence and suicide prevention: a\nsystematic review of machine learning investigations.\nInternational journal of environmental research and\npublic health, 17(16):5929.\nAmir Bialer, Daniel Izmaylov, Avi Segal, Oren Tsur,\nYossi Levi-Belz, and Kobi Gal. 2022. Detecting Sui-\ncide Risk in Online Counseling Services: A Study in\na Low-Resource Language. The 29th International\nConference on Computational Linguistics (COLING-\n22) http://shorturl.at/crXY2.\nLei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin\nWang, Ningyun Li, and Xiaohao He. 2019. Latent\nSuicide Risk Detection on Microblog via Suicide-\nOriented Word Embeddings and Layered Attention.\nTianqi Chen and Carlos Guestrin. 2016. Xgboost: A\nscalable tree boosting system. In Proceedings of\nthe 22nd acm sigkdd international conference on\nknowledge discovery and data mining , pages 785–\n794.\nChristopher M. Childs and Newell R. Washburn. 2019.\nEmbedding domain knowledge for machine learning\nof complex material systems. 9(3):806–820.\nAvihay Chriqui and Inbal Yahav. 2021. HeBERT &\nHebEMO: A Hebrew BERT Model and a Tool for\nPolarity Analysis and Emotion Recognition.\nPedro Colon-Hernandez, Catherine Havasi, Jason\nAlonso, Matthew Huggins, and Cynthia Breazeal.\n2021. Combining pre-trained language models and\nstructured knowledge.\nGlen Coppersmith, Ryan Leary, Patrick Crutchley, and\nAlex Fine. 2018. Natural language processing of so-\ncial media as screening for suicide risk. Biomedical\ninformatics insights, 10:1178222618792860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nManas Gaur, Amanuel Alambo, Joy Prakash Sain, Ugur\nKursuncu, Krishnaprasad Thirunarayan, Ramakanth\nKavuluru, Amit Sheth, Randy Welton, and Jyotish-\nman Pathak. 2019. Knowledge-aware Assessment\nof Severity of Suicide Risk for Early Intervention.\nIn The World Wide Web Conference on - WWW ’19,\npages 514–525. ACM Press.\nLaurence Gillick and Stephen J Cox. 1989. Some statis-\ntical issues in the comparison of speech recognition\nalgorithms. pages 532–535.\nXiaodong Gu, Kang Min Yoo, and Jung-Woo Ha. 2021.\nDialogbert: Discourse-aware response generation via\nlearning to recover and rank utterances. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 12911–12919.\nShaoxiong Ji, Shirui Pan, Xue Li, Erik Cambria,\nGuodong Long, and Zi Huang. 2021. Suicidal\nIdeation Detection: A Review of Machine Learning\nMethods and Applications. 8(1):214–226.\nRoi Kliper, Yonatan Vaizman, Daphna Weinshall, and\nShirley Portuguese. 2010. Evidence for depression\nand schizophrenia in speech prosody. In Proceedings\nof the 3rd ICSA Tutorial and Research Workshop on\nExperimental Linguistics 2010, pages 85–88.\nE David Klonsky and Alexis M May. 2015. The three-\nstep theory (3st): A new theory of suicide rooted in\nthe “ideation-to-action” framework. International\nJournal of Cognitive Therapy, 8(2):114–129.\nKurt Kroenke, Robert L Spitzer, and Janet BW Williams.\n2001. The phq-9: validity of a brief depression sever-\nity measure. Journal of general internal medicine,\n16(9):606–613.\nQuoc Le and Tomas Mikolov. 2014. Distributed repre-\nsentations of sentences and documents. 32(2):1188–\n1196.\n2437\nDaeun Lee, Soyoung Park, Jiwon Kang, Daejin Choi,\nand Jinyoung Han. 2020. Cross-lingual suicidal-\noriented word embedding toward suicide prevention.\npages 2208–2217.\nMatthew K Nock, Guilherme Borges, Evelyn J Bromet,\nChristine B Cha, Ronald C Kessler, and Sing Lee.\n2008. Suicide and suicidal behavior. Epidemiologic\nreviews, 30(1):133–154.\nYaakov Ophir, Refael Tikochinski, Christa S. C. Aster-\nhan, Itay Sisso, and Roi Reichart. 2020. Deep neural\nnetworks detect suicide risk from textual facebook\nposts. 10(1):16685.\nK Posner, D Brent, C Lucas, M Gould, B Stanley,\nG Brown, P Fisher, J Zelazny, A Burke, MJNY\nOquendo, et al. 2008. Columbia-suicide severity\nrating scale (c-ssrs). New York, NY: Columbia Uni-\nversity Medical Center, 10.\nRamit Sawhney, Prachi Manchanda, Puneet Mathur, Ra-\njiv Shah, and Raj Singh. 2018. Exploring and learn-\ning suicidal ideation connotations on social media\nwith deep learning. In Proceedings of the 9th work-\nshop on computational approaches to subjectivity,\nsentiment and social media analysis, pages 167–175.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Greenfeld, and Reut Tsarfaty.\n2022. Alephbert: Language model pre-training and\nevaluation from sub-word to sentence level. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 46–56.\nHan-Chin Shing, Suraj Nair, Ayah Zirikly, Meir Frieden-\nberg, Hal Daumé III, and Philip Resnik. 2018. Expert,\ncrowdsourced, and machine assessment of suicide\nrisk via online postings. In Proceedings of the fifth\nworkshop on computational linguistics and clinical\npsychology: from keyboard to clinic, pages 25–36.\nMarina Sokolova, Nathalie Japkowicz, Stan Szpakow-\nicz, and Stan Szpakowicz. 2006. Beyond accuracy,\nf-score and roc: a family of discriminant measures for\nperformance evaluation. In Australasian joint con-\nference on artificial intelligence, pages 1015–1021.\nSpringer.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2020. How to Fine-Tune BERT for Text Classifica-\ntion?\nMichael Mesfin Tadesse, Hongfei Lin, Bo Xu, and\nLiang Yang. 2019. Detection of suicide ideation in\nsocial media forums using deep learning. Algorithms,\n13(1):7.\nGustavo Turecki and David A Brent. 2016. Suicide and\nsuicidal behaviour. The Lancet, 387(10024):1227–\n1239.\nKimberly A Van Orden, Kelly C Cukrowicz, Tracy K\nWitte, and Thomas E Joiner Jr. 2012. Thwarted\nbelongingness and perceived burdensomeness: con-\nstruct validity and psychometric properties of the\ninterpersonal needs questionnaire. Psychological as-\nsessment, 24(1):197.\nRui Wang, Bing Xiang Yang, Yujun Ma, Peilin Wang,\nQiao Yu, Xiaofen Zong, Zhen Huang, Simeng Ma,\nLong Hu, Kai Hwang, and Zhongchun Liu. 2021.\nMedical-Level Suicide Risk Analysis: A Novel Stan-\ndard and Evaluation Model. 8(23):16825–16834.\nDeirdre Wilson and Tim Wharton. 2006. Relevance and\nprosody. Journal of pragmatics, 38(10):1559–1579.\nWorld-Health-Organization. 2021. Live life: an imple-\nmentation guide for suicide prevention in countries.\nZhongzhi Xu, Yucan Xu, Florence Cheung, Mabel\nCheng, Daniel Lung, Yik Wa Law, Byron Chiang,\nQingpeng Zhang, and Paul S.F. Yip. 2021. Detecting\nsuicide risk using knowledge-aware natural language\nprocessing and counseling service data. 283:114176.\nGil Zalsman, Yael Levy, Eliane Sommerfeld, Avi Se-\ngal, Dana Assa, Loona Ben-Dayan, Avi Valevski,\nand J John Mann. 2021. Suicide-related calls to a\nnational crisis chat hotline service during the covid-\n19 pandemic and lockdown. Journal of psychiatric\nresearch.\nAyah Zirikly, Philip Resnik, Ozlem Uzuner, and Kristy\nHollingshead. 2019. Clpsych 2019 shared task: Pre-\ndicting the degree of suicide risk in reddit posts. In\nProceedings of the sixth workshop on computational\nlinguistics and clinical psychology, pages 24–33.\n2438",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.7434382438659668
    },
    {
      "name": "Computer science",
      "score": 0.6544480323791504
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5623713135719299
    },
    {
      "name": "Language model",
      "score": 0.5413406491279602
    },
    {
      "name": "Process (computing)",
      "score": 0.5259642601013184
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4423953592777252
    },
    {
      "name": "Field (mathematics)",
      "score": 0.44194042682647705
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4302001893520355
    },
    {
      "name": "Natural language processing",
      "score": 0.4203474521636963
    },
    {
      "name": "Suicide Risk",
      "score": 0.419467031955719
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39729517698287964
    },
    {
      "name": "Machine learning",
      "score": 0.3709374666213989
    },
    {
      "name": "Psychology",
      "score": 0.3044777512550354
    },
    {
      "name": "Suicide prevention",
      "score": 0.28374266624450684
    },
    {
      "name": "Poison control",
      "score": 0.25166264176368713
    },
    {
      "name": "Medicine",
      "score": 0.12354829907417297
    },
    {
      "name": "Medical emergency",
      "score": 0.10410240292549133
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I124227911",
      "name": "Ben-Gurion University of the Negev",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802112877",
      "name": "Ruppin Academic Center",
      "country": "IL"
    }
  ]
}