{
  "title": "Molecular Generation and Optimization of Molecular Properties Using a Transformer Model",
  "url": "https://openalex.org/W4390205041",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2291920097",
      "name": "Zhong-yin Xu",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1993658660",
      "name": "Xiujuan Lei",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2111994055",
      "name": "Mei Ma",
      "affiliations": [
        "Shaanxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2105260931",
      "name": "Pan Yi",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2023818227",
    "https://openalex.org/W2087563523",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W6737665993",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W6733974131",
    "https://openalex.org/W6752245542",
    "https://openalex.org/W6747927160",
    "https://openalex.org/W4289436753",
    "https://openalex.org/W6738599175",
    "https://openalex.org/W2805002767",
    "https://openalex.org/W2793945656",
    "https://openalex.org/W6635084905",
    "https://openalex.org/W6714644935",
    "https://openalex.org/W6752910514",
    "https://openalex.org/W4225983336",
    "https://openalex.org/W4212815277",
    "https://openalex.org/W3209708391",
    "https://openalex.org/W6804214328",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3087716567",
    "https://openalex.org/W4284898351",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W4220802400",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W2997058986",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W2803615944",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W2768118983",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W2020859943",
    "https://openalex.org/W4236316962",
    "https://openalex.org/W1997764055",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3112113844",
    "https://openalex.org/W4297796727",
    "https://openalex.org/W4289761690",
    "https://openalex.org/W2618625858",
    "https://openalex.org/W4297798428",
    "https://openalex.org/W1583912456",
    "https://openalex.org/W4221138292",
    "https://openalex.org/W4229590462"
  ],
  "abstract": "Generating novel molecules to satisfy specific properties is a challenging task in modern drug discovery, which requires the optimization of a specific objective based on satisfying chemical rules. Herein, we aim to optimize the properties of a specific molecule to satisfy the specific properties of the generated molecule. The Matched Molecular Pairs (MMPs), which contain the source and target molecules, are used herein, and logD and solubility are selected as the optimization properties. The main innovative work lies in the calculation related to a specific transformer from the perspective of a matrix dimension. Threshold intervals and state changes are then used to encode logD and solubility for subsequent tests. During the experiments, we screen the data based on the proportion of heavy atoms to all atoms in the groups and select 12365, 1503, and 1570 MMPs as the training, validation, and test sets, respectively. Transformer models are compared with the baseline models with respect to their abilities to generate molecules with specific properties. Results show that the transformer model can accurately optimize the source molecules to satisfy specific properties.",
  "full_text": " \nMolecular Generation and Optimization of Molecular\nProperties Using a Transformer Model\nZhongyin Xu, Xiujuan Lei*, Mei Ma, and Yi Pan*\nAbstract: Generating  novel  molecules  to  satisfy  specific  properties  is  a  challenging  task  in  modern  drug\ndiscovery, which requires the optimization of a specific objective based on satisfying chemical rules. Herein, we\naim  to  optimize  the  properties  of  a  specific  molecule  to  satisfy  the  specific  properties  of  the  generated\nmolecule.  The  Matched  Molecular  Pairs  (MMPs),  which  contain  the  source  and  target  molecules,  are  used\nherein, and logD and solubility are selected as the optimization properties. The main innovative work lies in the\ncalculation related to a specific transformer from the perspective of a matrix dimension. Threshold intervals and\nstate changes are then used to encode logD and solubility for subsequent tests. During the experiments, we\nscreen the data based on the proportion of heavy atoms to all atoms in the groups and select  12 365, 1503, and\n1570 MMPs as the training, validation, and test sets, respectively. Transformer models are compared with the\nbaseline models with respect to their abilities to generate molecules with specific properties. Results show that\nthe transformer model can accurately optimize the source molecules to satisfy specific properties.\nKey words:  molecular optimization; transformer; Matched Molecular Pairs (MMPs); logD; solubility\n1　Introduction\n1023\n1060\nGenerating new molecules with desirable properties is\na substantial and challenging task in drug discovery. A\ndrug  requires  balancing  of  multiple  properties,\nincluding  Absorption,  Distribution,  Metabolism,\nElimination,  and  Toxicity  (ADMET),  physical  and\nchemical  properties.  Finding  molecules  with  specific\nfeatures  in  a  vast  chemical  environment,  where  the\ntotal number of potential drug candidates is presumed\nto be − [1], is difficult.\nMolecular  optimization  improves  the  properties  of\ninitial molecules. Controlling some properties of initial\ndrug molecules to create directional changes is crucial\nin increasing drug efficiency. Four aspects are typically\ninvolved  in  molecular  generation.  (1)  Database\nselection involves choosing appropriate molecular data\naccording  to  the  task  requirements.  (2)  Molecular\nrepresentation  is  the  translation  of  the  selected\nmolecular  data  into  the  input  form  that  can  be\nunderstood by the computer. (3) Selection of a suitable\ngenerative  model  is  closely  related  to  the  chosen\nrepresentation  method  and  the  performance  of  the\nmodel.  (4)  Evaluation  metrics  are  used  to  assess  the\ngenerated new molecules with appropriate indexes.\nA  series  of  molecular  representations  have  been\ndevised over the past few years. The de novo molecular\ndesign has two common representations. The first is a\nsequence-based  representation.  Two  current  examples\nof one-dimensional linear representations are SMILES\nand International CHemical Identifier (InCHI) [2]. Some\nimproved  molecular  representations  based  on\n \n  Zhongyin  Xu,  Xiujuan  Lei,  and  Mei  Ma are  with School  of\nComputer Science, Shaanxi Normal University, Xi’an 710119,\nChina. E -mail: 20212687@snnu.edu.cn ; xjlei@snnu.edu.cn ;\nmm1016@qhnu.edu.cn.\n  Yi  Pan is  with Faculty  of  Computer  Science  and  Control\nEngineering,  Shenzhen  Institute  of  Advanced  Technology,\nChinese  Academy  of  Sciences,  Shenzhen  518055,  China. E -\nmail: yipan@siat.ac.cn.\n* To whom correspondence should be addressed.\n    Manuscript  received: 2023 -01-06;  revised: 2023 -03-27;\naccepted: 2023-05-04 \nBIG   DATA   MINING   AND    ANALYTICS\nISSN  2096 -0654    10/15   pp142 −155\nDOI:  10.26599/BDMA.2023.9020009\nVolume  7, Number  1,  March    2024\n \n©  The author(s) 2024. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nSMILES[3, 4] have been recently proposed. The second\nrepresentation  is  a  graph-based  representation,  where\neach  molecule  can  be  represented  as  an  undirected\ngraph,  with  nodes  and  edges  representing  atoms  and\nchemical bonds, respectively. Figure 1 shows a simple\nexample of this representation.\nDifferent  molecular  generative  models,  such  as\nRecurrent  Neural  Networks  (RNNs)[5−7],  Variational\nAutoEncoders  (VAEs)[8−14],  Generative  Adversarial\nNetworks (GANs)[15−18], and flow-based models[19−21],\nhave  been  recently  used  for  molecular  generation.\nSome  novel  approaches  have  emerged  from  these\nhighly  mature  models.  MGCVAE[22] can  effectively\ngenerate drug-like molecules with target properties. Li\net  al.[23] generated  molecules  based  on  geometric\nconvolution  with  specific  protein  constraints.\nGFVAE[24] inherited the advantages of VAE and flow-\nbased  methods.  Luo  et  al.[25] proposed  an\nautoregressive  sampling  scheme  to  generate  3D\nmolecules.  MolGPT[26] took  masked  self-attention[27]\nfor  generating  drug-like  molecules.  Langevin  et  al.[28]\nintroduced  a  new  algorithm  named  scaffold-\nconstrained  molecular  generation  to  perform  scaffold-\nconstrained  molecular  design.  Zhang  and  Chen[29]\nproposed a novel molecular deep generative model that\nadopts  an  RNN  architecture  coupled  with  a  ligand-\nprotein  interaction  fingerprint  as  constraints.  The\nsequence-to-sequence model with attention mechanism\nand the transformer model are employed in Ref. [30] to\ngenerate molecules with desirable properties. A further\nstudy[31] provides  a  general  methodology  for  highly\ngeneral  structural  modifications  beyond  Matched\nMolecular Pairs (MMPs).\nThe  metrics  of  evaluating  generative  models  of\nperformance  can  be  roughly  classified  into  three\ncatalogs  according  to  different  evaluation  objects.  (1)\nThe evaluation metrics of the entire molecule set aim to\nassess  the  difference  between  the  generated  and  test\nsets,  such  as  the  average  Tanimoto  similarity\ncoefficient  between  two  molecule  sets,  and  generate\nthe  validity  of  the  molecular  set,  novelty,  and\nuniqueness.  (2)  Evaluation  metrics  for  evaluating\nindividual molecules in the molecular set are utilized in\nthe  following  examples.  Bickerton  et  al.[32] used\nQuantitative  Estimate  of  Drug-likeness  (QED),  which\nis the concept of desirability, to measure drug-likeness.\nFrechet  ChemNet  Distance  (FCD)[33] is  a  measure  of\ndistribution  between  training  sets  and  generated\nmolecules.  Synthetic  accessibility  and  ring  sizes  are\nconsidered  in  penalized  logP[34].  (3)  Assessment  of\ngenerative  models  with  benchmark  suites  is\ndemonstrated in GuacaMol[35] and MOSES[36].\nThe issue of molecular optimization can be framed as\na  machine  translation  problem[37] in  natural  language\nprocessing,  where  a  text  is  translated  from  one\nlanguage  to  another.  A  way  to  translate  initial\nmolecules  into  target  molecules  with  optimized\nproperties based on SMILES must be determined. The\ndatasets  comprise  a  set  of  MMPs.  The  application  of\nMMPs  is  a  widely  used  design  strategy  by  medicinal\nchemists  due  to  its  intuitive  nature.  The  MMPs\n(including  reverse  transformations)  are  extracted  from\nChEMBL[38] using  an  open-source  MMP  tool[39].  A\ntransformer  is  used  in  this  study  to  generate  new\nmolecules  with  specific  properties  from  initial\nmolecules.\n2　Method\n2.1　Molecule and property representation\nThe SMILES representation of molecules[40] is used in\nmodels to train a set of MMPs. Figures 2a and 2c show\nexamples.\nlogD and solubility are chosen as specific properties\nin  this  article.  logD  is  the  logarithm  of  the  partition\ncoefficient  of  a  compound  between  an  organic  phase\n(e.g., octanol) and an aqueous phase (e.g., buffer) at a\ngiven pH. logD mainly affects the physicochemical and\nmetabolic properties, as well as the activity and toxicity\nof compounds. Solubility is a type of physical property;\nfor example, the maximum number of certain materials\nis  dissolved  in  100  g  of  solvent  under  a  certain\ntemperature. One of the most crucial characteristics in\n \nSMILES\nRepresentation\n2D structure 3D conformer\nCCOC(=O)C1=C(C)N(c2cccc(C(F)(F)F)c2)\nC(=O)N(C)C1c1ccc(C#N)cc1C(=O)NC\n \n \nFig. 1    Example of molecular representation.\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 143\n \nthe  search  for  new  drugs  lies  in  their  solubility.\nCompounds  with  low  solubility  can  have  numerous\nundesirable  effects,  such  as  poor  absorption  and\nbioavailability  after  oral  administration  of  drugs  and\naggravated patient burden due to frequent high doses of\nmedication.  Moreover,  several  drug  properties\naffecting  ADMET  are  related  to  the  two  properties.\nDigitally  labeling  the  features  that  have  an  important\nimpact on the task is necessary to reduce the learning\ndifficulty  of  models  and  enable  them  to  learn  the\nchemical  structures  of  molecules  and  the  complex\nrelationships  between  the  physics,  chemistry,  and\nbiology of molecules effectively. Molecular properties\nare  generally  measured  by  quantified  property\nfractions.  Therefore,  logD  and  solubility  must  be\nencoded  as  corresponding  vectors[41] by  one-hot\nencoding  before  inputting  specific  properties  into\nmodels.  The  fractions  for  the  properties  of  each  drug\nare different. Requiring consistency for every property\nfraction  is  unreasonable  when  coding  molecular\nproperties,  and  a  reasonable  coding  method  must  be\ndefined. The solubility can be encoded in the following\nthree states: from low to high, from high to low, and no\nchange.  Thresholds  must  be  set  to  distinguish  and\nmeasure these states. However, the value range of logD\nis  relatively  large  and  needs  a  detailed  description;\ntherefore,  logD  is  encoded  as  the  range  interval.\nFigure 2b provides additional details.\n \nlogD change\nlogD_change_(−inf,−5.7]\n...\nlogD_change_(−0.1,0.1]\nlogD_change_(0.1,0.3]\nlogD_change_(0.3,0.5]\n...\nlogD_change_(5.9,inf)\nSolubility change\nSolubility_low->high\nSolubility_high->low\nSolubility_no_change\nThreshold for low/high\nsolubility:\nlogD change\nSolubility change  low->high\n(−1.1, −0.9]\nCC(C)(C)c1nc(N2CCN(CCCCn3ccc(-\nc4ccccc4)nc3=O)CC2)cc(C(F)(F)F)n1\nlogD: 3.989\nSolubility: 0.705\nO=C1C(=O)N(Cc2ccc(O)cc2)c2ccc(S(=\nO)(=O)N3CCCC3COc3cccnc3)cc21\nlogD: 2.301\nSolubility: 0.715\nCN(C)C1(C(=O)Nc2cccc(c3cccnc3)c2)CC\nN(c2ncnc3[nH]c4c(c23)CCCC4)CC1\nlogD: 4.184\nSolubility: 0.633\nlog10 (50μM)≈1.7\nCCC(O)c1ccn(CCCCN2CCN(c3cc(C(F)\n(F)F)nc(C(C)(C)C)n3)CC2)c(=O)n1\nlogD: 3.153\nSolubility: 2.713\nCC(=O)NCC1CCCN1S(=O)(=O)c1ccc2\nc(c1)C(=O)C(=O)N2Cc1ccc(O)cc1\nlogD: 1.087\nSolubility: 2.023\nCN(C)C1(C(=O)Nc2cccc(CN3CCOCC3)c\n2)CCN(c2ncnc3[nH]c4c(c23)CCCC4)CC1\nlogD: 3.536\nSolubility: 1.082\nAttention\nFeedforward\nneural\nnetwork\nFeedforward\nneural\nnetwork\nAttention\n(a) Source molecule\n(c) Generated molecule\n(b) Property coding\n(d) Transformer \n \n \nFig. 2    Molecule and property representation.\n    144 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \nSource  molecules  must  be  connected  with  the\nencoded  SMILES  with  property  changes  in  the  input\nprocess  to  transform  source  molecules  into  target\nmolecules  with  specific  properties,  and  the  generated\ntarget  sequences  are  the  SMILES  of  target  molecules.\nThe general process is shown in Fig. 3.\nfA; B; Cg\n(A; C) 2 A\u0002 C \u0000 !B 2 B\nA\u0002 C\nB\n(A; C) 2 A\u0002 C\nThis  task  aims  to  provide  a  set  of  MMPs ,\nwhere A, B,  and C represent  the  source  molecule,  the\ntarget  molecule,  and  the  property  change  between A\nand B,  respectively.  The  model  will  learn  a  mapping\n  during  the  training  process,\nwhere  represents input space including all source\nmolecules  with  property  changes,  and  represents\noutput space including all target molecules. Given any\ngroup , the model can generate a diverse\nset  of  target  molecules  with  certain  properties  during\nthe testing process.\n2.2　Transformer  architecture  and  calculation\ndetails\nThe  transformer  architecture  will  be  comprehensively\ninterpreted  for  the  generation  of  molecules  with\nspecific  properties  proposed  in  this  article.  The\ntransformer  is  also  a  classical  encoder-decoder  mode.\nThe specific flow chart is shown in Fig. 4. The specific\ncalculation  process  of  the  transformer  is  analyzed  on\nthe basis of matrix operation.\n2.2.1　Encoder\nI 2 Rn \u0002 l \u0002 d\nP 2 Rn \u0002 l \u0002 d\nThe input part of the encoder comprises two parts: the\nword-coding  matrix  and  the  position-\ncoding matrix , where n, l, and d represent\nthe number of source sequences, the maximum number\nof  words,  and  the  dimension  of  the  word  vector,\nrespectively.  The  position-coding  matrix P represents\nthe  position  information  of  each  word  in  a  sentence.\n \nInput:\nTransformer block 1\nTransformer block 6\n…\nTransformer block 2\nOutput: \nlogD_change_(0.7,0.9]      \n Solubility_high->low\nProperty condition\nC=CCCCCSc1nc(N)c2ncn(C\n3OC(CO)C(O)C3O)c2n1\nSMILES 1 SMILES n\nNc1nc(SCCC2CCCCC2)nc2\nc1ncn2C1OC(CO)C(O)C1O\nSMILES 1\n…\nSMILES n\nlogD change\nlogD_change_(-inf,-5.7]\n...\nlogD_change_(-0.1,0.1]\nlogD_change_(0.1,0.3]\nlogD_change_(0.3,0.5]\n...\nlogD_change_(5.9,inf)\nSolubility change\nSolubility_low->high\nSolubility_high->low\nSolubility_no_change\nThreshold for low/high\nSolubility:\nLog10 (50μM)≈1.7\nPositional encoding Input embeddingPE (pos, 2i)=sin(            )10002i/d\npos\nPE (pos, 2i+1)=cos(            )10002i/d\npos\nC\n0\n1\n0\n0\n0\n0\n(0.7,0.9]\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\nS_no_change\n \n \nFig. 3    Training process of molecular optimization work.\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 145\n \nNumerous ways to generate position-coding vectors are\navailable. The commonly used method is to generate a\none-hot  location  code  according  to  the  position  of  a\nword in a sentence. Trigonometric functions are used in\nthis  article  for  position  coding,  as  shown  in  the\nfollowing:\n \n8>>>>><>>>>>:\nPE (pos;2i) = sin\n( pos\n10002i=d\n)\n;\nPE (pos;2i +1) = cos\n( pos\n10002i=d\n) (1)\n \n \nDecoder\nEncoder\nMultihead self-attention\nFeed forward\nneural network\nLayer normalization\nLayer normalization\nI '\nI+P+I '\nSkip\nI ''\nI ''' ...\nIN\nSkip\nI+P\nK VQ\nMask multihead self-attention\n Multihead encoder-decoder \nself-attention\nSkip\nSkip\nSkip\nON\nFeedforward\nneural network\nLayer normalization\nLayer normalization\nLayer normalization\nO+PO\nO '\nO+PO+O '\nQN KN V '\n...\nQ K V\nO ''+O '''\nI ''+I ''' O '''\nO ''''\nO '''''\nO ''''+O '''''\nO ''\n \n \nFig. 4    Flowchart of the transformer block.\n    146 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \nPE\nI′ 2 Rn \u0002 l \u0002 d\nwhere  represents the position encoding vector, pos\nrepresents the position of the word in the sentence, and\ni represents the position index of the encoding vector.\nThe  input  matrix I + P is  transformed  linearly  to\ngenerate  the  matrices Q, K,  and V.  However,  in  the\nactual training process, I + P is directly assigned to Q,\nK,  and V.  The  word  length  is  different.  Therefore,\npadding,  which  uses  zero  to  fill  each  word  to  the\nmaximum word length, must be employed. Q, K, and V\nare inputted into the multihead self-attention module to\ncalculate the attention distribution to obtain the matrix\n , \nI′ = MultiHead (Q; K;V) (2)\n \nI′\nI + P + I′ 2 Rn \u0002 l \u0002 d\nThe residual of the original input I + P and attention\ndistribution  are  then  computed  to  obtain  the  output\nmatrix .\nI + P + I′ = fxi jkgn \u0002 l \u0002 d\nI′′ 2 Rn \u0002 l \u0002 d\nAfterward,  layer  normalization  is  performed  on\n  to  obtain ,  and  the\nspecific calculation is shown in the following:\n \n\u0016i j=\nn∑\nk=1\nxi jk;\n\u001bi j=\nn\nvut d∑\nk=1\n(xi jk\u0000 \u0016i j)2;\nˆxi jk= xi jk\u0000 \u0016i j\n\u001bi j ;\ni 2 f1; 2; :::; ng; j 2 f1; 2; :::; lg; and k 2 f1; 2; :::; dg\n(3)\n \nI′′\nI′′′ 2 Rn \u0002 l \u0002 d\nI′′\nI′′′\nI′′ + I′′′\n  is  inputted  into  a  fully  connected  feedforward\nneural  network  to  obtain ,  and  residual\ncalculation between  and  is performed to acquire\n .\nI′′ + I′′′\nLayer  normalization  operation  is  then  performed  on\n .  The  above  equation  presents  the  detailed\noperation of a block. The number of blocks set in this\ntraining  is  six.  The  entire  encoder  module  comprises\nsix blocks, and the final output is obtained.\n2.2.2　Decoder\nO 2 Rn1 \u0002 l1 \u0002 d\nPO 2 Rn1 \u0002 l1 \u0002 d\nThe  decoder  input  also  comprises  the  following  two\nparts:  the  word-coding  matrix  and  the\nposition-coding  matrix .  In  the  model\ntraining  process,  the  decoder’s  input  word-coding\nmatrix is the target sequence. In addition, the decoder’s\ninput  is  time  sequential  (that  is,  the  output  of  the\nprevious  step  is  the  input  of  the  current  step).\nTherefore,  introducing  the  mask  matrix  in  order  is\nnecessary to calculate the attention distribution.\nO + PO\nQ\nK\nV\nO + PO\nQ\nK\nV\nThe  input  matrix  is  transformed  linearly  to\ngenerate  the  matrices , ,  and .  However,  in  the\nactual  training  process,  is  directly  assigned  to\n , , and . Similar to the encoder, 0 must be added\nto short word vectors.\nQ\nK\nV\nM\nO′ 2 Rn1 \u0002 l1 \u0002 d\n , , , and mask matrix  are then inputted into\nthe  mask  multihead  self-attention  module  to  calculate\nattention  distribution,  and  matrix  is\nobtained, \nO′ = MaskMultiHead\n(\nQ; K; V; M\n)\n(4)\n \nO + PO\nO′\nO + PO+O′ 2 Rn1 \u0002 l1 \u0002 d\nO + PO +O′\nO′′ 2 Rn1 \u0002 l1 \u0002 d\nAfterward,  the  residual  between  the  original  input\n  and the attention distribution  is computed to\nobtain  the  output  matrix ,  and\nthe  layer  normalization  of  is  then\nperformed to acquire .\nIN\nQN\nKN\nO′\nV\n′\nQN\nKN\nV\n′\nO′′′\nThe encoder information should be inputted into the\ndecoder.  Encoder  output  can  obtain  and ,\nand  can obtain  through a linear transformation.\nThe cross-attention distribution of , , and  can\nbe used to obtain , \nO′′′ = MultiHead\n(\nQN; KN; V\n′)\n(5)\n \nIN\nQN\nKN\nO′′\nV\n′\nO′′\nO′′′\nO′′ +O′′′\nO′′ +O′′′\nO′′′′\nO′′′′\nO′′′′′\nO′′′′ +O′′′′′\nThe  cross-attention  distribution  synthesizes  the\noutput  of  the  encoder  and  the  intermediate  result\ninformation  of  the  decoder.  However,  in  the  actual\ntraining process,  is directly assigned to  and ,\nand  is  directly  assigned  to .  Next,  the  residual\nbetween  and  is calculated to obtain the output\nmatrix .  The  layer  normalization  operation  is\nperformed  on  to  obtain ,  and  is\ninputted into a fully connected neural network to obtain\n .  Residual  operation  is  conducted  to  obtain\n ,  and  the  layer  normalization  operation  is\nfinally conducted again.\nON 2 Rn1 \u0002 l1 \u0002 d\nThe above is the detailed operation of a block. In this\nexperiment,  six  blocks  are  stacked  to  form  the  entire\ndecoder  module,  and  the  obtained  output  is\n .  The  word  in  the  vocabulary  that\ncurrently  predicts  the  maximum  probability  is\nidentified, and the word vector is used as input for the\nnext  stage.  The  steps  are  repeated  until  the ”end”\ncharacter is selected.\n2.2.3　Multihead self-attention\nNumerous  works  related  to  transformers  do  not\ncomprehensively  introduce  the  calculation  method  of\nmultihead  self-attention  through  examples.  For  the\nconvenience of illustration, the head is set to 2, and the\nhead of the actual training process is 8.\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 147\n \na1\na2\na3 2 Rdl \u0002 1\nqi 2 Rdk \u0002 1\nki 2 Rdk \u0002 1\nvi 2 Rdl \u0002 1\nWq 2 Rdk \u0002 dl\nWk 2 Rdk \u0002 dl\nWv 2 Rdl \u0002 dl\nqi\nWq1 2 Rdm \u0002 dk\nWq2 2 Rdm \u0002 dk\nqi1 2 Rdm \u0002 1\nqi2 2 Rdm \u0002 1\nki\nvi\nki1 2 Rdm \u0002 1\nki2 2 Rdm \u0002 1\nvi1 2 R(dl=2) \u0002 1\nvi2 2 R(dl=2) \u0002 1\nTake Fig. 5 as an example. Given the input vector ,\n , ,  query  vector ,  key  vector\n , and value vector  can be obtained\nby  linear  transformation  through  matrix ,\n , and . A linear transformation\nis  then  performed  on  the  query  vector  through\nmatrix  and  to  obtain\n  and .  Similarly,  the  same\noperation  is  performed  on  key  vector  and  value\nvector  to  obtain ,  and\n , ,  respectively.  The\nspecific calculation is as follows:\n \n8>>>>><>>>>>:\nqih = Wqh \u0001 Wq \u0001 ai;\nkih = Wkh \u0001 Wk \u0001 ai;\nvih = Wvh \u0001 Wv \u0001 ai;\ni 2 f1; 2; 3g; h 2 f1; 2g (6)\n \nwhere h represents the number of heads.\nThe  vectors  are  then  integrated  for  the  matrix\ncalculation,\n \n8>>>>><>>>>>:\nQ1 = (q11; q21; q31) = Wq1 \u0001 Wq \u0001 A;\nK1 = (k11; k21; k31) = Wk1 \u0001 Wk \u0001 A;\nV1 = (v11; v21; v31) = Wv1 \u0001 Wv \u0001 A;\n \n \n8>>>>><>>>>>:\nQ2 = (q12; q22; q32) = Wq2 \u0001 Wq \u0001 A;\nK2 = (k12; k22; k32) = Wk2 \u0001 Wk \u0001 A;\nV2 = (v12; v22; v32) = Wv2 \u0001 Wv \u0001 A (7)\n \n\u000bih\nFor each head, the query and key vectors should be\nused to calculate the corresponding attention score. The\ncalculation formula of the l components of the attention\nvector  is as follows: \n\u000bih\nl = (qih)T \u0001 kih;\ni; l 2 f1; 2; 3g; h 2 f1; 2g (8)\n \nThen,  the  attention  score  matrix  is  obtained  as\nfollows: \n8>>>>>>><>>>>>>>:\n\u00031 = (\u000b11; \u000b 21; \u000b 31) = (K1)T \u0001 Q1\npdm\n;\n\u00032 = (\u000b12; \u000b 22; \u000b 32) = (K2)T \u0001 Q2\npdm\n(9)\n \n\fih\nThe normalized attention distribution  is obtained\nafter the softmax layer, \n\fih\nj = e\u000bih\nj\n3∑\nn=1\ne\u000bihn\n;\ni; j 2 f1; 2; 3g; h 2 f1; 2g (10)\n \n \nSoftmax\nAttention calculation\nα1\nα11 α12\nb11 b12 b21 b22 b31\nb3b2b1\nb32\nα21 α22 α31 α32\nβ11 β12 β21 β22 β31 β32\nq1\nq11 q12 k11 k12 v11\nv11\nv12\nv12\nq21 q22 k21 k22 v21 v22\nv21 v22\nq31 q32 k31 k32 v31 v32\nv31 v32\nk1 v1 q2 k2 v2 q3 k3 v3\nα2 α3\nW v\nW v2W v1W k2W k1W q2W q1W v2W v1W k2W k1W q2W q1W v2W v1\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\nW k2W k1W q2W q1\nW kW qW vW kW qW vW kW q\n* * * * * *\nWO\n \n \nFig. 5    Flow chart of multihead self-attention.\n    148 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \n\fih\nVh\nbih 2 R(dl=2) \u0002 1\nAttention distribution vector  and value matrix \nshould  be  used  for  each  head  to  obtain  the  output\n , as shown in the following:\n \nbih =\n3∑\nl=1\n\fih\nl \u0001 vlh;\ni 2 f1; 2; 3g; h 2 f1; 2g (11)\n \nConcatenating the two head vectors,\n \nB =\n( b11 b21 b31\nb12 b22 b32\n)\n(12)\n \nWO 2 Rdl \u0002 dl\nThe  given  parameter  matrix  is\ncombined, and the output matrix is obtained as follows:\n \nO = WO \u0001 B 2 Rdl \u0002 3 (13)\n \nFinally, the overall calculation can be obtained,\n \nO = WO \u0001 Concat\n0BBBBBBBBBBBBBBB@\nV1 \u0001 softmax\n((K1)T \u0001 Q1\npdm\n)\nV2 \u0001 softmax\n((K2)T \u0001 Q2\npdm\n)\n1CCCCCCCCCCCCCCCA\n(14)\n \n2.2.4　Additional transformer information\nSpecific  parameters  trained  with  the  transformer  are\nshown  in Table  1,  where H represents  the  number  of\nheads  in  the  multihead  attention  mechanism,  and N\nrepresents the number of transformer blocks, as well as\nthe training parameters of the Adam optimizer and the\nparameters set during the training process.\nUsing  multihead,  the  parameter  matrix  can  form\nmultiple  subspaces,  and  the  overall  size  of  the  matrix\nremains  the  same.  However,  the  dimension  size\ncorresponding to each head is changed. Therefore, the\nmatrix can learn various pieces of information, but the\ncomputation amount is similar to that of a single head.\nIn Eq. (14), when softmax is used to process attention\npdm\nscores,  division  by  is  necessary  to  control  the\nvanishing  gradient  problem  effectively.  Residuals  and\nlayer  normalization  can  be  used  in  the  transformer.\nThese steps can also be added to other training models\nto improve the training performance.\n3　Experiment and Result\n3.1　Baseline\nThe  transformer  is  compared  with  the  Seq2Seq[30],\nMGCVAE[22], and JT-VAE[13] baselines.\nTransformer  uses  sequences  to  represent  molecules.\nSeq2Seq also utilizes sequences, while MGCVAE and\nJT-VAE  employ  graphs  to  represent  molecules.  The\nthree  comparison  methods  and  transformer  belong  to\nthe  encoder-decoder  mode;  therefore,  the  comparison\nexperiment will be highly sufficient. The main purpose\nis  to  compare  the  efficiency  of  the  transformer  with\nother  models  in  generating  molecules  that  satisfy  the\nrequired properties.\n3.2　Data preparation\n<\n<\n<\nTransformer and baseline models are trained on a set of\nMMPs[42] extracted  from  ChEMBL[43] with  property\nchanges  between  source  and  target  sequences.  The\nselected  molecules  are  standardized  using  MolVS[44].\nThe  molecular  pairs  are  processed  in  accordance  with\nthe  following  constraints:  number  of  heavy  atoms \n50; number of heavy atoms in the R group  10; ratio\nof  heavy  atoms  in  the  R  group  0.5;  AZFilter = \nCORE[45] to  filter  out  low-quality  compounds.  Each\nmolecule’s  property  values  are  within  three  standard\ndeviations  of  all  molecule  property  values.  Then,\n12 365, 1503, and 1570 MMPs are randomly sampled\nas  the  training,  validation,  and  test  sets,  respectively,\nfrom full molecules.\n3.3　Evaluation metrics\nFor each initial molecular test set, 10 unique and valid\nmolecules  are  generated,  and  these  generated\nmolecules are not identical. The number of molecules\nsatisfying the specific changes of logD and solubility is\nthen  calculated.  The  ADMET  property  prediction\nmodel[30] is  used  to  compute  the  properties  of\ngenerated  molecules.  The  model  error  (test  RMSE)  is\nconsidered  if  a  generated  molecule  satisfies  its\ndesirable  properties.  For  logD,  the  error  between  the\ngenerated  and  target  logD  values  is  guaranteed  to  be\nless  than  0.4.  For  solubility,  a  standard  must  also  be\nestablished to frame the threshold range.\n \nTable 1    Training parameters of transformer.\nParameter Description Value\nH Number of heads in self-attention 8\nN Number of transformer blocks 6\nd_model Dimensions of the input and output 256\nd_ff Extended dimension of the\nfeedforward layer 2048\nbatch_size Size of batch 128\nnum_epoch Number of epoch 60\nadam_beta1 Exponential decay rate for the 1st\nmoment estimates 0.9\nadam_beta2 Exponential decay rate for the 2nd\nmoment estimates 0.98\nadam_eps Small value for numerical stability\n10\u00009 \n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 149\n \nThe  generated  and  initial  molecules  are  then\nanalyzed,  and  the  performance  between  the  models  is\ncompared  with  the  proportion  of  molecules  satisfying\nspecific  properties. Figure  6 shows  the  property\ndistribution  of  source  and  target  molecules  on  the\ntraining  set.  Dark  areas  have  highly  concentrated\nmolecules. The reverse transformation is represented in\nthe  data  set.  Thus,  the  distribution  of  the  source\nmolecule properties is comparable to that of the target\nmolecule properties.\n3.4　Conditional vs. unconditional transformer\nThis part of the test compares a conditional model with\ninput  source  molecules  and  specific  properties  to  an\nunconditional model with input source molecules only.\nFrom  1570  initial  test  molecules,  10  unique  and  valid\nmolecules  are  generated,  which  must  differ  from  the\noriginal molecules.\nFigure  7 shows  the  performance  of  the  conditional\nand unconditional transformers satisfying two specific\nproperties.  This  experiment  uses  the K-sample\nAnderson-Darling  test[46].  This  test  is  employed  to\nexamine if a data sample came from a population with\na  specific  distribution.  This  figure  shows  that  the\nperformance  of  the  conditional  transformer  is  better\nthan  that  of  the  unconditional  transformer.  Of  the  10\nmolecules  generated  per  initial  molecule,  conditional\nand  unconditional  transformers  averaged  6  and  3,\nrespectively.\n3.5　Transformer vs. baseline models\nThis  experiment  compares  the  performance  of  the\ntransformer  and  the  three  other  methods  under  the\nsame  test  set  and  provides  the  same  specific\nperformance constraints.\nTable  2 shows  the  ratio  of  generated  molecules\n \nSource logD\nTarget logD\n0\n−2 −1 0 1 2 3 4 5\n−2 −1 0 1 2 3 4 5\n5\n4\n3\n2\n1\n0\n−1\n6\n5\n4\n3\n2\n1\n0\n−1\n−2 −1 0 1 2 3 4 65\n−2 −1 0 1 2 3 4 5\n−2 −1 0 1 2 3 4 65\n−2 −1 0 1 2 3 4 65\n100\n200\n300\n400\n0\n100\n200\n300\n400\n0\n100\n200\n300\n400\n600\n500\n700\n0\n100\n200\n300\n400\n600\n500\n700\nSource solubility\nTarget solubility\nSource logD value\nTarget logD value\nSource solubility value\nTarget solubility value\n logD value\n logD value\nSolubility value\nSolubility value\nNumber of  moleculesNumber of  moleculesNumber of  moleculesNumber of  molecules\n100\n600\n350\nNumber of  molecules\nNumber of  molecules\n50\n300\n175\n \n \nFig. 6    Distributions and properties of source and target molecules in training.\n    150 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \n<\n<\nsatisfying specific properties to all generated molecules\nfor  different  methods.  The  second  test  set  is  selected\nunder  the  condition  that  R  group  0.5  (the  ratio  of\nheavy atoms in R group  0.5). The performance of the\ntransformer is better than that of Seq2Seq. MGCVAE\nand JT-VAE also show good performance.\nMoreover, the transformer is compared with baseline\nmodels  based  on  the  evaluation  metrics  of  MOSES.\nValidity refers to the generation of satisfying the rules\nof  chemical  molecules  that  account  for  the  proportion\nof  all  generated  molecules.  Uniqueness  is  the\nproportion of different molecules that are formed, and\nUnique@10 000 stands  for  the  uniqueness  value\nobtained per 10 000 generated molecules. Novelty refers\nto  the  proportion  of  generated  molecules  that  are\ndifferent  from  the  source  dataset  of  those  in  all\ngenerated  molecules.  FCD  is  calculated  using  the\nfeatures of generated molecules and those molecules in\nthe dataset. A low FCD value means that the model has\nsuccessfully  captured  the  statistical  information  of  the\ndataset.  IntDiv  evaluates  the  chemical  diversity  of\ngenerated molecules and detects whether the model has\na pattern collapse, and the value range is [0, 1].\nAs  shown  in Table  3 shows  100% validity  because\nthe  formation  of  molecules  requires  the  generation  of\n10  unique  valid  molecules  for  each  initial  molecule.\nAdditional unique molecules can be generated through\nJT-VAE,  thus  ensuring  the  diversity  of  molecules.\nTransformer  achieves  the  best  performance  in  novelty\nand  FCD,  ensuring  the  differentiation  between\ngenerated  and  training  set  molecules  as  well  as\neffectively  capturing  the  distribution  information  of\ntraining  set.  The  four  methods  for  Internal  Diversity\n(IntDiv) are the same, which can achieve high chemical\ndiversity.\n<\n<\n<\nThe numerical transformation of logD and solubility\nis analyzed. The test set is framed, and test molecules\nwith  high  logD  and  low  solubility  are  selected  (2 \nlogD  4.5  and  solubility  1.7).  Initial  molecules\nmust be optimized to produce new molecules with low\nlogD  and  high  solubility.  As  shown  in Fig.  8,  the\nproperties  of  the  source  molecule  are  plotted  on  the\nleft,  and  those  of  the  predicted  molecules  are  on  the\nright  (the  predicted  values  of  the  properties  of  the  10\npredicted molecules generated for each initial molecule\nare  numerically  averaged).  The  prediction  of  specific\nproperties  by  a  transformer  can  extensively  widen  the\ndistribution  of  generated  molecules,  which  markedly\nimproves  the  diversity  of  molecules  with  specific\nproperties. Most compounds become slightly soluble as\nlogD  increases,  and  a  link  is  observed  between  logD\nand solubility[47]. Some molecules may not follow this\npattern  because  other  properties  might  affect\n \nConditional \ntransformer\nUnconditional \ntransformer\nNumber of molecules that\nsatisfy specific properties\n0\n300\nConditional transformer\nUnconditional transformer\n250\n200\n150\n100\n50\n0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1 2 3 4 5 6 7 8 9 10\nDifferent transformers that satisfy specific properties\nNumber of molecules that satisfy specific properties\nNumber of initial molecules\n \n \nFig. 7    Conditional transformer vs. unconditional transformer.\n \nTable 2    Comparison  of  the  ratio  of  generated  molecules\nsatisfying specific properties based on different methods.\n (%)\nTest set Transformer Seq2Seq MGCVAE JT-VAE\nOriginal 55.23 44.93 54.03 54.56\nR < 0.5 94.45 87.23 92.46 91.68\n \nTable 3    Comparison  of  transformer  and  baseline  models\nbased on MOSES.\nModel Validity Unique@10000 Novelty FCD/test IntDiv\nTransformer 1.000 0.996 0.981 5.442 0.867\nSeq2Seq 1.000 0.979 0.963 8.560 0.866\nMGCVAE 1.000 0.998 0.979 6.096 0.866\nJT-VAE 1.000 0.999 0.964 6.139 0.867\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 151\n \nmolecules.  Future  work  may  need  to  consider\nadditional  properties,  such  as  stability  and  toxicity,\ncomprehensively.\nFigure  9 shows  an  example  of  different  molecules\nwith  specific  properties  generated  by  transformer  and\nbaseline models concerning the initial molecule and the\nspecific properties. Three molecules are selected from\neach  model  for  observation.  Each  of  the  resulting\nmolecules  has  some  modifications  to  the  original\nmolecules and has the required specific properties. The\nmolecules  in  the  wireframe  of  the  same  color  are\nidentical. Thus, the molecules produced by the baseline\nand  transformer  models  may  overlap.  Some  details\nmust  still  be  uncovered.  For  example,  the  error  of\nmolecules  generated  by  transformers  MGCVAE  and\nJT-VAE  in  specific  properties  is  small,  and  the\ngenerated molecules can effectively satisfy the required\nproperties.  Overall,  the  transformer  and  other  models\ncan  generate  different  sets  of  molecules  with  specific\nproperties.\n4　Conclusion and Challenge\nThe  molecular  property  optimization  problem  can  be\ndefined  as  a  machine  translation  problem,  which\n \nSource logD value\n−1\n−1\n0\n1\n2\n3\n4\n−2\n−1\n0\n1\n2\n3\n4\n−2\n0 1 2 3 4 5 −1 0 1 2 3 4 5\nSource solubility value\nPredicted logD value\nPredicted solubility value\n \n \nFig. 8    Property distribution of initial molecules and generated molecules.\n \nlogD: 1.08\nSolubility: 2.39\nlogD: 1.39\nSolubility: 2.31\nlogD: 1.59\nSolubility: 2.41\nlogD: 0.99\nSolubility: 2.77\n(b) Transformer\nlogD: 2.34\nSolubility: 1.05\nSpecific properties:\nlogD: 1.34 (±0.4)\nSolubility: low->high (>1.45)\n(a)  Initial molecule and specific properties\nlogD: 1.89\nSolubility: 1.99\nlogD: 0.80\nSolubility: 2.66\nlogD: 1.09\nSolubility: 2.46\nlogD: 0.97\nSolubility: 2.50\n(d) MGCVAE\nlogD: 1.08\nSolubility: 2.39\nlogD: 1.22\nSolubility: 2.70\nlogD: 1.39\nSolubility: 2.31\n(c) Seq2Seq\n(e) JT-VAE\nlogD: 1.09\nSolubility: 2.46\n \n \nFig. 9    Examples of different molecules with specific properties generated from transformer and baseline models.\n    152 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \ngenerates  new  molecules  with  specific  properties  by\ninputting  the  initial  molecules  and  specific  properties\ninto  the  model.  The  optimization  performance  of  the\ntransformer  is  better  than  Seq2Seq  and  has  its\nadvantages  compared  with  MGCVAE  and  JT-VAE.\nThe  transformer  and  these  methods  have  produced\ndifferent new molecules with specific properties.\nTen unique and valid molecules are generated in this\narticle,  satisfying  specific  properties  for  each  initial\nmolecule  in  the  test  set  and  ensuring  their  uniqueness\nfrom  the  initial  molecules.  The  conditional  and\nunconditional  transformer  models  are  compared.  At\nleast 50% of the conditionally constrained transformer\ncan  generate  six  molecules  satisfying  certain\nproperties,  while  the  unconditional  transformer  can\nonly  have  three  molecules.  The  optimization\nperformance of the transformer and baseline models is\nalso compared. The transformer ensures the difference\nbetween  the  generated  molecules  and  the  training  set\nmolecules, and can effectively capture the distribution\ninformation of the training set. The performance of the\nthree  baseline  methods  is  different  from  that  of  the\ntransformer.  However,  these  methods  still  produce\nsome  molecules  that  satisfy  specific  properties.  The\noptimized use of these models will help in substantially\nenriching  different  molecules.  The  current  study\nencounter  some  challenges.  A  transformer  is  a  model\nbased  on  sequence  representation.  In  the  subsequent\nwork,  additional  models  based  on  the  graph\nrepresentation should also be included for comparison.\nIn  addition  to  logD  and  solubility,  other  specific\nproperties  should  also  be  regarded  to  generate\nadditional  new  molecules.  These  conditions  and  other\ndifficulties will be considered in future research efforts.\nAcknowledgment\nThis work was supported by the National Natural Science\nFoundation  of  China  (Nos.  62272288,  61972451,  and\nU22A2041)  and  the  Shenzhen  Key  Laboratory  of\nIntelligent  Bioinformatics  (No.\nZDSYS20220422103800001).\nReferences \n P.  G.  Polishchuk,  T.  I.  Madzhidov,  and  A.  Varnek,\nEstimation  of  the  size  of  drug-like  chemical  space  based\non GDB-17 data, J. Comput. Aided Mol. Des., vol. 27, no. 8,\npp. 675–679, 2013.\n[1]\n S. Heller, A. McNaught, S. Stein, D. Tchekhovskoi, and I.\nPletnev, InChI − the  worldwide  chemical  structure\nidentifier  standard, J.  Cheminform.,  vol. 5,  no. 1,  p. 7,\n[2]\n2013.\n N.  M.  O’Boyle  and  A.  Dalke,  DeepSMILES: An\nadaptation  of  SMILES  for  use  in  machine-learning  of\nchemical structures, doi:10.26434/chemrxiv.7097960.\n[3]\n M.  Krenn,  F.  Häse,  A.  K.  Nigam,  P.  Friederich,  and  A.\nAspuru-Guzik, Self-Referencing  Embedded  Strings\n(SELFIES): A  100% robust  molecular  string\nrepresentation, Mach. Learn. Sci. Technol., vol. 1, no. 4, p.\n045024, 2020.\n[4]\n E. J. Bjerrum and R. Threlfall, Molecular generation with\nrecurrent  neural  networks (RNNs),  arXiv  preprint  arXiv:\n1705.04612, 2017.\n[5]\n A. Gupta, A. T. Müller, B. J. H. Huisman, J. A. Fuchs, P.\nSchneider,  and  G.  Schneider, Generative  recurrent\nnetworks  for de  novo drug  design, Mol.  Inform.,  vol. 37,\nnos. 1&2, p. 1700111, 2018.\n[6]\n M. H. S. Segler, T. Kogej, C. Tyrchan, and M. P. Waller,\nGenerating  focused  molecule  libraries  for  drug  discovery\nwith recurrent neural networks, ACS Cent. Sci., vol. 4, no. 1,\npp. 120–131, 2018.\n[7]\n R.  Gómez-Bombarelli,  J.  N.  Wei,  D.  Duvenaud,  J.  M.\nHernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J.\nAguilera-Iparraguirre,  T.  D.  Hirzel,  R.  P.  Adams,  and  A.\nAspuru-Guzik, Automatic  chemical  design  using  a  data-\ndriven continuous representation of molecules, ACS Cent.\nSci., vol. 4, no. 2, pp. 268–276, 2018.\n[8]\n J.  Lim,  S.  Ryu,  J.  W.  Kim,  and  W.  Y.  Kim, Molecular\ngenerative  model  based  on  conditional  variational\nautoencoder  for de  novo molecular  design, J.\nCheminform., vol. 10, p. 31, 2018.\n[9]\n M.  J.  Kusner,  B.  Paige,  and  J.  M.  Hernández-Lobato,\nGrammar variational autoencoder, in Proc. 34th Int. Conf.\nMachine Learning, Sydney, Australia, 2017, pp. 1945–1954.\n[10]\n H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, Syntax-\ndirected  variational  autoencoder  for  molecule  generation,\nin Proc.  Int.  Conf.  Learning  Representations,  https://doi.\norg/10.48550/arXiv.1802.08786, 2018.\n[11]\n Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt,\nConstrained  graph  variational  autoencoders  for  molecule\ndesign,  in Proc.  32nd Int.  Conf.  Neural  Information\nProcessing  Systems,  Montréal,  Canada,  2018,  pp.\n7806–7815.\n[12]\n W.  Jin,  R.  Barzilay,  and  T.  Jaakkola,  Junction  tree\nvariational autoencoder for molecular graph generation, in\nProc.  35th Int.  Conf.  Machine  Learning,  Stockholm,\nSweden, 2018, pp. 2323–2332.\n[13]\n M. Simonovsky and N. Komodakis, GraphVAE: Towards\ngeneration of small graphs using variational autoencoders,\nin Proc.  27th Int.  Conf.  Artificial  Neural  Networks,\nRhodes, Greece, 2018, pp. 412–422.\n[14]\n G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L.\nC.  Farias,  and  A.  Aspuru-Guzik,  Objective-reinforced\ngenerative  adversarial  networks (ORGAN) for  sequence\ngeneration  models,  arXiv  preprint  arXiv: 1705.10843,\n2018.\n[15]\n E.  Putin,  A.  Asadulaev,  Y.  Ivanenkov,  V.  Aladinskiy,  B.\nSanchez-Lengeling,  A.  Aspuru-Guzik,  and  A.\nZhavoronkov, Reinforced adversarial neural computer for\nde  novo molecular  design, J.  Chem.  Inf.  Model.,  vol. 58,\nno. 6, pp. 1194–1204, 2018.\n[16]\n E.  Putin,  A.  Asadulaev,  Q.  Vanhaelen,  Y.  Ivanenkov,  A.[17]\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 153\n \nV.  Aladinskaya,  A.  Aliper,  and  A.  Zhavoronkov,\nAdversarial  threshold  neural  computer  for  molecular de\nnovo design, Mol. Pharm., vol. 15, no. 10, pp. 4386–4397,\n2018.\n N. De Cao and T. Kipf, MolGAN: An implicit generative\nmodel  for  small  molecular  graphs,  arXiv  preprint  arXiv:\n1805.11973, 2022.\n[18]\n L.  Dinh,  D.  Krueger,  and  Y.  Bengio,  NICE: Non-linear\nindependent components estimation, arXiv preprint arXiv:\n1410.8516, 2015.\n[19]\n L.  Dinh,  J.  Sohl-Dickstein,  and  S.  Bengio,  Density\nestimation  using  real  NVP,  arXiv  preprint  arXiv:\n1605.08803, 2017.\n[20]\n D.  P.  Kingma  and  P.  Dhariwal,  Glow: Generative  flow\nwith  invertible  1x1  convolutions,  arXiv  preprint  arXiv:\n1807.03039, 2018.\n[21]\n M.  Lee  and  K.  Min, MGCVAE: Multi-objective  inverse\ndesign  via  molecular  graph  conditional  variational\nautoencoder, J.  Chem.  Inf.  Model.,  vol. 62,  no. 12,  pp.\n2943–2950, 2022.\n[22]\n C. Li, J. Yao, W. Wei, Z. Niu, X. Zeng, J. Li, and J. Wang,\nGeometry-based  molecular  generation  with  deep\nconstrained  variational  autoencoder, IEEE  Trans.  Neural\nNetw. Learn. Syst., doi: 10.1109/TNNLS.2022.3147790.\n[23]\n C. Ma and X. Zhang, GF-VAE: A flow-based variational\nautoencoder  for  molecule  generation,  in Proc.  30th ACM\nInt. Conf. Information & Knowledge Management, Virtual\nEvent, Queensland, Australia, 2021, pp. 1181–1190.\n[24]\n S.  Luo,  J.  Guan,  J.  Ma,  and  J.  Peng,  A  3D  generative\nmodel  for  structure-based  drug  design,  arXiv  preprint\narXiv: 2203.10446, 2022.\n[25]\n V.  Bagal,  R.  Aggarwal,  P.  K.  Vinod,  and  U.  D.\nPriyakumar, MolGPT: Molecular  generation  using  a\ntransformer-decoder model, J. Chem. Inf. Model., vol. 62,\nno. 9, pp. 2064–2076, 2022.\n[26]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all\nyou  need,  in Proc.  31st Int.  Conf.  Neural  Information\nProcessing  Systems,  Long  Beach,  CA,  USA,  2017.  pp.\n6000–6010.\n[27]\n M.  Langevin,  H.  Minoux,  M.  Levesque,  and  M.\nBianciotto, Scaffold-constrained  molecular  generation, J.\nChem. Inf. Model., vol. 60, no. 12, pp. 5637–5646, 2020.\n[28]\n J.  Zhang  and  H.  Chen, De  novo molecule  design  using\nmolecular generative models constrained by ligand-protein\ninteractions, J.  Chem.  Inf.  Model.,  vol. 62,  no. 14,  pp.\n3291–3306, 2022.\n[29]\n J. He, H. You, E. Sandström, E. Nittinger, E. J. Bjerrum,\nC.  Tyrchan,  W.  Czechtizky,  and  O.  Engkvist, Molecular\noptimization  by  capturing  chemist’s  intuition  using  deep\nneural  networks, J.  Cheminform.,  vol. 13,  no. 1,  p. 26,\n2021.\n[30]\n J.  He,  E.  Nittinger,  C.  Tyrchan,  W.  Czechtizky,  A.\nPatronov,  E.  J.  Bjerrum,  and  O.  Engkvist, Transformer-\nbased  molecular  optimization  beyond  matched  molecular\npairs, J. Cheminform., vol. 14, no. 1, p. 18, 2022.\n[31]\n G.  R.  Bickerton,  G.  V.  Paolini,  J.  Besnard,  S.  Muresan,\nand  A.  L.  Hopkins, Quantifying  the  chemical  beauty  of\ndrugs, Nat. Chem., vol. 4, no. 2, pp. 90–98, 2012.\n[32]\n K. Preuer, P. Renz, T. Unterthiner, S. Hochreiter, and G.\nKlambauer, Fréchet  ChemNet  distance: A  metric  for\ngenerative  models  for  molecules  in  drug  discovery, J.\nChem. Inf. Model., vol. 58, no. 9, pp. 1736–1741, 2018.\n[33]\n T.  Fu,  C.  Xiao,  and  J.  Sun,  CORE: Automatic  molecule\noptimization  using  copy & refine  strategy, Proc.  AAAI\nConf. Artif. Intell., vol. 34, no. 1, pp. 638–645, 2020.\n[34]\n N.  Brown,  M.  Fiscato,  M.  H.  S.  Segler,  and  A.  C.\nVaucher, GuacaMol: Benchmarking  models  for  de  novo\nmolecular design, J. Chem. Inf. Model., vol. 59, no. 3, pp.\n1096–1108, 2019.\n[35]\n D.  Polykovskiy,  A.  Zhebrak,  B.  Sanchez-Lengeling,  S.\nGolovanov,  O.  Tatanov,  S.  Belyaev,  R.  Kurbanov,  A.\nArtamonov, V. Aladinskiy, M. Veselov, et al., Molecular\nSets (MOSES): A  benchmarking  platform  for  molecular\ngeneration models, Front. Pharmacol., vol. 11, p. 565644,\n2020.\n[36]\n D.  Bahdanau,  K.  Cho,  and  Y.  Bengio,  Neural  machine\ntranslation by jointly learning to align and translate, arXiv\npreprint arXiv: 1409.0473, 2016.\n[37]\n A.  Gaulton,  L.  J.  Bellis,  A.  P.  Bento,  J.  Chambers,  M.\nDavies,  A.  Hersey,  Y.  Light,  S.  McGlinchey,  D.\nMichalovich,  B.  Al-Lazikani,  et  al., ChEMBL: a  large-\nscale  bioactivity  database  for  drug  discovery, Nucleic\nAcids Res., vol. 40, no. D1, pp. D1100–D1107, 2012.\n[38]\n A. Dalke, J. Hert, and C. Kramer, mmpdb: An open-source\nmatched  molecular  pair  platform  for  large  multiproperty\ndata sets, J. Chem. Inf. Model., vol. 58, no. 5, pp. 902–910,\n2018.\n[39]\n D.  Weininger,  SMILES, a  chemical  language  and\ninformation  system.  1.  Introduction  to  methodology  and\nencoding rules, J. Chem. Inf. Comput. Sci., vol. 28, no. 1,\npp. 31–36, 1988.\n[40]\n K. Yang, K. Swanson, W. G. Jin, C. Coley, P. Eiden, H.\nGao, A. Guzman-Perez, T. Hopper, B. Kelley, M. Mathea,\net  al., Analyzing  learned  molecular  representations  for\nproperty  prediction, J.  Chem.  Inf.  Model.,  vol. 59,  no. 8,\npp. 3370–3388, 2019.\n[41]\n S. Turk, B. Merget, F. Rippmann, and S. Fulle, Coupling\nmatched molecular pairs with machine learning for virtual\ncompound optimization, J. Chem. Inf. Model., vol. 57, no.\n12, pp. 3079–3085, 2017.\n[42]\n D. Mendez, A. Gaulton, A. P. Bento, J. Chambers, M. De\nVeij,  E.  Félix,  M.  P.  Magariños,  J.  F.  Mosquera,  P.\nMutowo,  M.  Nowotka,  et  al., ChEMBL: Towards  direct\ndeposition  of  bioassay  data, Nucleic  Acids  Res.,  vol. 47,\nno. D1, pp. D930–D940, 2019.\n[43]\n M.  Swain,  MolVS: Molecule  validation  and  standardi-\nzation, https://pypi.org/project/Molvs, 2018.\n[44]\n J. G. Cumming, A. M. Davis, S. Muresan, M. Haeberlein,\nand  H.  Chen, Chemical  predictive  modelling  to  improve\ncompound quality, Nat. Rev. Drug Discov., vol. 12, no. 12,\npp. 948–962, 2013.\n[45]\n F.  W.  Scholz  and  M.  A.  Stephens, K-sample  Anderson-\ndarling  tests, J.  Am.  Stat.  Assoc.,  vol. 82,  no. 399,  pp.\n918–924, 1987.\n[46]\n J.  B.  Dressman  and  C.  Reppas, In  vitro-in  vivo\ncorrelations  for  lipophilic,  poorly  water-soluble  drugs,\nEur. J. Pharm. Sci., vol. 11, no. S2, pp. S73–S80, 2000.\n[47]\n    154 Big Data Mining and Analytics, March 2024, 7(1): 142−155\n \nZhongyin  Xu received  the  BEng  degree\nfrom  Shaanxi  Normal  University,  Xi’an,\nChina  in  2016.  He  is  currently  a  master\nstudent  at  School  of  Computer  Science,\nShaanxi Normal University, Xi’an, China.\nHis  current  research  interests  include\nbioinformatics  and  intelligent  computing,\nand deep learning.\nXiujuan Lei received the MEng and PhD\ndegrees  from  Northwestern  Polytechnical\nUniversity, Xi’an, China in 2001 and 2005,\nrespectively. She is currently a professor at\nSchool  of  Computer  Science,  Shaanxi\nNormal  University,  Xi’an,  China.  Her\nresearch  interests  include  bioinformatics,\nswarm  intelligent  optimization,  data\nmining, and deep learning.\nMei  Ma received  the  BEng  and  MEng\ndegrees  from  Qinghai  Normal  University,\nQinghai,  China  in  2010  and  2014,\nrespectively.  She  is  currently  a  PhD\ncandidate  at  School  of  Computer  Science,\nShaanxi Normal University, Xi’an, China.\nHer  current  research  interests  include\nbioinformatics  and  intelligent  computing,\nand deep learning.\nYi  Pan received  the  BEng  (computer\nengineering)  and  MEng  (computer\nengineering)  degrees  from  Tsinghua\nUniversity,  Beijing,  China  in  1982  and\n1984,  respectively,  and  the  PhD  degree\n(computer  science)  from  University  of\nPittsburgh,  USA  in  1991.  He  is  currently\nworking  as  a  dean  and  chair  professor  at\nFaculty  of  Computer  Science  and  Control  Engineering,\nShenzhen Institute of Advanced Technology, Chinese Academy\nof  Sciences,  Shenzhen,  China.  He  has  published  over  250\nacademic  papers  in  SCI-indexed  journals,  including  more  than\n100  papers  in  top  IEEE/ACM  transactions/journals.  His\npublications  have  been  cited  more  than  19  000  times,  and  his\ncurrent H-index is 88. He has been awarded IEEE Distinguished\nAchievement Award, IEEE Distinguished Service Award, IEEE\nTransactions  Best  Paper  Award,  IEEE  and  other  international\nconference  best  paper  awards  many  times,  IBM  Professor\nAward  four  times,  Andrew  Mellon  Award,  and  other  awards.\nHis research interests include parallel and distributed processing\nsystems, Internet technology, and bioinformatics.\n   Zhongyin Xu et al.:  Molecular Generation and Optimization of Molecular Properties Using a Transformer Model 155\n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6809401512145996
    },
    {
      "name": "Solubility",
      "score": 0.6015073657035828
    },
    {
      "name": "Molecule",
      "score": 0.5675190687179565
    },
    {
      "name": "ENCODE",
      "score": 0.5413560271263123
    },
    {
      "name": "Biological system",
      "score": 0.5288318991661072
    },
    {
      "name": "Computer science",
      "score": 0.4641519784927368
    },
    {
      "name": "Chemistry",
      "score": 0.39071330428123474
    },
    {
      "name": "Physics",
      "score": 0.2416868507862091
    },
    {
      "name": "Voltage",
      "score": 0.11125335097312927
    },
    {
      "name": "Organic chemistry",
      "score": 0.0884363055229187
    },
    {
      "name": "Biochemistry",
      "score": 0.08599728345870972
    },
    {
      "name": "Biology",
      "score": 0.08445408940315247
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I88830068",
      "name": "Shaanxi Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    }
  ],
  "cited_by": 12
}