{
  "title": "Household Energy Consumption Prediction Using the Stationary Wavelet Transform and Transformers",
  "url": "https://openalex.org/W4205809857",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2020965364",
      "name": "Lyes Saad Saoud",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3213273780",
      "name": "Hasan Al Marzouqi",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2553562683",
      "name": "Ramy Hussein",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2020965364",
      "name": "Lyes Saad Saoud",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3213273780",
      "name": "Hasan Al Marzouqi",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2553562683",
      "name": "Ramy Hussein",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3043685378",
    "https://openalex.org/W2081733273",
    "https://openalex.org/W2948490758",
    "https://openalex.org/W2763500568",
    "https://openalex.org/W2592453717",
    "https://openalex.org/W2890188851",
    "https://openalex.org/W2952133925",
    "https://openalex.org/W2067493648",
    "https://openalex.org/W1991277158",
    "https://openalex.org/W1998230236",
    "https://openalex.org/W2775155156",
    "https://openalex.org/W2064469609",
    "https://openalex.org/W2586634262",
    "https://openalex.org/W2938562256",
    "https://openalex.org/W3036110098",
    "https://openalex.org/W2604099671",
    "https://openalex.org/W2329258917",
    "https://openalex.org/W3038178920",
    "https://openalex.org/W3022720643",
    "https://openalex.org/W2776741657",
    "https://openalex.org/W2597866042",
    "https://openalex.org/W2754252319",
    "https://openalex.org/W3008235510",
    "https://openalex.org/W2995894307",
    "https://openalex.org/W2893898383",
    "https://openalex.org/W3033993664",
    "https://openalex.org/W3033027149",
    "https://openalex.org/W2810273060",
    "https://openalex.org/W2899934327",
    "https://openalex.org/W3000313095",
    "https://openalex.org/W3010938659",
    "https://openalex.org/W2999520752",
    "https://openalex.org/W3007211815",
    "https://openalex.org/W2994209110",
    "https://openalex.org/W3017024051",
    "https://openalex.org/W3029357900",
    "https://openalex.org/W3022039226",
    "https://openalex.org/W2997861936",
    "https://openalex.org/W2982252459",
    "https://openalex.org/W3112325088",
    "https://openalex.org/W3044720376",
    "https://openalex.org/W3038202720",
    "https://openalex.org/W3004454690",
    "https://openalex.org/W3181576298",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6773071120",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W3101467051",
    "https://openalex.org/W1996944908",
    "https://openalex.org/W6765602453",
    "https://openalex.org/W3084610728",
    "https://openalex.org/W1653130573",
    "https://openalex.org/W3024458572",
    "https://openalex.org/W2343086726",
    "https://openalex.org/W59771946",
    "https://openalex.org/W2621204779",
    "https://openalex.org/W2996374824",
    "https://openalex.org/W2975836901",
    "https://openalex.org/W2899714726",
    "https://openalex.org/W2543643230",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4288283362",
    "https://openalex.org/W3104996215",
    "https://openalex.org/W3102977103"
  ],
  "abstract": "In this paper, we present a new method for forecasting power consumption. Household power consumption prediction is essential to manage and plan energy utilization. This study proposes a new technique using machine learning models based on the stationary wavelet transform (SWT) and transformers to forecast household power consumption in different resolutions. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from household power consumption data. The SWT and its inverse are used to decompose and reconstruct the actual and the forecasted household power consumption data, respectively, and deep transformers are used to forecast the SWT subbands. Experimental findings show that our hybrid approach achieves superior prediction performance compared to the existing power consumption prediction methods.",
  "full_text": "Received December 15, 2021, accepted December 31, 2021, date of publication January 6, 2022, date of current version January 14, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3140818\nHousehold Energy Consumption Prediction Using\nthe Stationary Wavelet Transform\nand Transformers\nLYES SAAD SAOUD\n1, HASAN AL-MARZOUQI\n 1, (Senior Member, IEEE),\nAND RAMY HUSSEIN2\n1Electrical and Computer Engineering Department, Khalifa University, Abu Dhabi, United Arab Emirates\n2Radiological Sciences Laboratory (RSL), Stanford University, Stanford, CA 94305, USA\nCorresponding author: Lyes Saad Saoud (lyes.saoud@ku.ac.ae)\nThis work was supported by the Information and Communication Technology (ICT) Fund, Telecommunications Regulatory Authority\n(TRA), Abu Dhabi, United Arab Emirates.\nABSTRACT In this paper, we present a new method for forecasting power consumption. Household power\nconsumption prediction is essential to manage and plan energy utilization. This study proposes a new\ntechnique using machine learning models based on the stationary wavelet transform (SWT) and transformers\nto forecast household power consumption in different resolutions. This approach works by leveraging self-\nattention mechanisms to learn complex patterns and dynamics from household power consumption data. The\nSWT and its inverse are used to decompose and reconstruct the actual and the forecasted household power\nconsumption data, respectively, and deep transformers are used to forecast the SWT subbands. Experimental\nﬁndings show that our hybrid approach achieves superior prediction performance compared to the existing\npower consumption prediction methods.\nINDEX TERMS Household power consumption, transformers, stationary wavelet transform, time series\nforecasting.\nI. INTRODUCTION\nElectric energy consumption has recently risen worldwide,\ndriven by economic advancements and increasing popula-\ntion [1]. According to the 2019 World Energy Outlook\nreleased by International Energy Agency (IEA), the world-\nwide electricity demand increases at 2.1% per year to 2040,\ndouble the stated policies scenario’s primary energy produc-\ntion rate. Therefore, the total ﬁnal energy consumption is\nexpected to rise from 19% in 2018 to 24% in 2040 [2].\nThe housing market accounts for 27% of global electric-\nity demand and signiﬁcantly affects aggregate electricity\nusage [3]. Because electricity is used simultaneously during\nthe production at the power plant, it is necessary to fore-\ncast energy consumption in advance for a reliable power\nsupply [4]. Over the last few decades, a growing number\nof models have been developed to predict building energy\nconsumption [4]–[9]. In what follows, we review some of the\nrecently published papers in the literature.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Amjad Anvari-Moghaddam\n.\nForecasting energy consumption is a challenging time\nseries prediction problem. Intelligent sensors collect data\nthat may contain redundancy, missing values, outliers, and\nuncertainties [6]. Moreover, it is hard to predict electrical\nenergy consumption using traditional forecasting techniques\nsince energy usage has erratic trend components, including\nregular seasonal patterns [4], [10]. Appropriate operating\napproaches should be implemented in energy control schemes\nto maximize buildings’ energy efﬁciency [7]. Therefore,\nvarious forecasting techniques have been recently proposed\nto predict energy consumption. Energy consumption fore-\ncasting has been studied using a variety of different tech-\nniques that can be divided into conventional and artiﬁcial\nintelligence (AI) models [8]. Wei et al. [8] have reviewed\n128 models in 116 published studies used to forecast energy\nconsumption; among them, 62.48% are AI-based models.\nWe have divided energy consumption forecasting systems\ninto three primary categories, statistical models, machine\nlearning models, and hybrid models.\nStatistical techniques were used mainly in the past to\npredict energy demand. For example, in [9], the seasonal\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 5171\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nautoregressive integrated moving average (SARIMA) model\nwas compared to the neuro-fuzzy model for forecasting elec-\ntric load. Both linear regression (with one predictor and\nmultiple predictors) and quadratic regression models were\napplied to the hourly and daily energy consumption of the\nresearch household [11]. Also, in [12], the multiple regres-\nsion approach together with a genetic engineering technique\nwere proposed to estimate the administration building’s daily\nenergy use. Both models’ signiﬁcant drawbacks include the\nunavailability of occupancy data and the reality that none\nof these models have been studied to estimate comparable\nbuildings’ energy usage. Bootstrap aggregating autoregres-\nsive integrated moving average (ARIMA) and the exponen-\ntial smoothing methods have been used to forecast energy\ndemand for different countries [13]. Generally, the statistical\ntechniques showed their weakness in long-term forecasting\nand capturing the nonlinear behavior of the energy consump-\ntion data.\nFurthermore, computational approaches have shown lim-\nited prediction performance due to the non-stationarity nature\nand serious trends in the energy demand; therefore, many\nprediction models have been tested using machine learn-\ning methods to improve the forecasting quality [14]–[16].\nFor instance, Liu et al. [17] have developed a support vec-\ntor machine (SVM) model to forecast and analyze public\nbuildings’ energy consumption. Driven by the solid nonlin-\near supporting vector regression capacities, Chen et al.[18]\nproposed a model that forecasts the electrical load based\non the ambient temperature. Energy consumption has been\nforecasted based on evaluating the usage of aggregated people\ndynamics in [19]. An artiﬁcial neural network-based cuckoo\nsearch learning algorithm was proposed to forecast the elec-\ntricity consumption of the organization of petroleum export-\ning countries (OPEC) [20]. Pinto et al. [21] proposed an\nensemble learning model containing three machine learning\nalgorithms: random forests, gradient boosted regression trees,\nand Adaboost to forecast energy consumption. Nevertheless,\ncurrent machine learning approaches severely suffer from\noverﬁtting as the dynamic correlation between variables is\nchallenging, and data characteristics change over time. It is\nhard to ascertain long-term and reliable usage when overﬁt-\nting happens.\nLikewise, many deep sequential learning neural networks\nhave been established to forecast electricity use. A recurrent\nneural network model was used to predict medium-to-long\nterm electricity consumption proﬁles in commercial and res-\nidential buildings at one-hour resolution predictions [22].\nA pooling-based recurrent neural network (RNN) approach\nhas been proposed to address the over-ﬁtting issue by increas-\ning data diversity and volume [23]. An RNN architecture with\nlong-short term memory (LSTM) cells was used to forecast\nenergy load in [24]. A model based on LSTM networks was\nalso proposed in [25] to forecast regular energy consumption.\nIn addition, an advanced optimization method focused on the\nbagged echo state network (ESN) and improved by a differen-\ntial evolution algorithm was proposed in [26] to approximate\nenergy usage. The performance of deep extreme learning\nmachines was investigated for energy consumption prediction\nin residential buildings [27]. The proposed model outper-\nformed other artiﬁcial neural and neuro-fuzzy networks.\nTo achieve adequate predictability based on the weak\nknowledge and lack of a multitude of historical evidence in\nenergy consumption, Gao et al. [28] suggested using two\ndeep learning models, a sequence-to-sequence model and\na two-dimensional attention-based convolutional neural net-\nwork model. Deep learning models can extract the crucial\nand hidden features needed for accurate prediction, even from\nnon-stationary data with dynamic features and/or different\nbiomarkers. However, conventional deep learning models\nhave difﬁculties identifying the spatiotemporal properties\npertinent to energy use [4].\nSeveral variables, such as the market cycle and regional\neconomic policies, have a signiﬁcant impact on energy\nusage. Therefore, it is very challenging that a single intel-\nligent algorithm would sufﬁce [29]. Thus, combining efﬁ-\ncient pre-processing techniques and feature learning models\nfor forecasting power consumption has a great poten-\ntial for improving prediction performance. For instance,\nthe stacked autoencoders and extreme learning machines\nwere used to efﬁciently extract the energy consumption-\nrelated features and achieve more robust prediction perfor-\nmance in [5]. AdaBoost ensemble technology was hybridized\nwith a neural network, support vector regression machine,\ngenetic programming, and radial basis function network\nto better forecast energy consumption [30]. The hybrid\nSARIMA-metaheuristic ﬁreﬂy algorithm-least squares sup-\nport vector regression model was used to forecast energy\nconsumption in [8].\nWell-known artiﬁcial intelligence methods have been used\nto evaluate energy use in single and ensemble situations.\nAn in-depth study and examination of the hybrid model,\nintegrating forecasting and optimization approaches, were\ndiscussed. A thorough analysis revealed that the combi-\nnation conﬁguration is more reliable than the single and\nassembly models. A hybrid convolutional neural network -\nLSTM (CNN-LSTM) model has been established for elec-\ntricity forecasting [4], [31]. The CNN was used to extract\nthe features, and the LSTM layer was used to deal with\nthe temporal behavior of the time series data. A predictive\nmodel of energy consumption using LSTM and sine cosine\noptimization algorithm was proposed [32]. Hu et al. [33]\ncombined echo state network, bagging, and differential evolu-\ntion algorithm to forecast energy consumption. Logarithmic\nMean Divisia Index, empirical mode decomposition, least-\nsquare support vector machine, and particle swarm optimiza-\ntion were hybridized to forecast energy consumption [34].\nKaytez [35] proposed forecasting energy consumption using\nthe least-square SVM (LSSVM) and an autoregressive inte-\ngrated moving average.\nA mixture of three deep reinforcement learning mod-\nels, including asynchronous advantage Actor-Critic, deep\ndeterministic policy gradient, and recurrent deterministic\n5172 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\npolicy gradient, was introduced in [36] to address nonlin-\near and complex energy consumption forecasting results.\nAn ensemble model was proposed in [37], in which the\nenergy consumption data was divided into stable and stochas-\ntic elements. A hybrid model based on ARIMA, artiﬁcial neu-\nral network, and the combined Particle Swarm Optimization\nSupport Vector Regression, was proposed and used for load\nand energy forecasting [38]. Complete ensemble empirical\nmode decomposition with adaptive noise and machine learn-\ning model–extreme gradient boosting was proposed to predict\nbuilding energy consumption [39]. A hybrid model has been\nproposed by combining CNN with multilayer bi-directional\nLSTM [40]. The hybrid energy-based sequential learning\nprediction model that used a coherent structure for the reliable\nenergy usage prediction was brought forward using CNN and\nGated Recurrent Units (GRU) [1]. The Stationary wavelet\ntransform (SWT) was combined with the ensemble LSTM to\nforecast energy consumption [41].\nThe k −means clustering based CNN-LSTM (kCNN-\nLSTM) model was proposed to provide a precise forecast of\nbuilding energy consumption [42]. The k-CNN-LSTM was\nfound to achieve superior performance when compared to\nother existing machine learning and deep learning energy\ndemand forecast models. In [43], Liu et al. developed a\nhybrid model for the short-term predictions of residential\nelectricity consumption based on the Holt-Winters method\nand Extreme Learning Machine (ELM) network. They also\ncompared their hybrid model with non-hybrid deep learning\nmodels such as ELM and LSTM. For a training data set\nof 50 days, the proposed model reduced the prediction error\nrate by 53.39-87.98%. Another integrated approach, consist-\ning of feature extraction, optimization, and adaptive deep\nneural networks (DNNs), was proposed in [44] to forecast\nweek-ahead hourly building energy consumption. The feature\nextraction procedure was carried out through the k-means\nclustering technique, while the DNN was the forecasting\nengine of the proposed model. A genetic algorithm was also\ndeployed to identify the DNN architecture that yields supe-\nrior prediction performance. The proposed hybrid predictive\nmodel was implemented on an actual ofﬁce building in the\nUK, and it was found to achieve an 11.9-24.6% decrease\nin the mean absolute percentage error when compared with\nother DNNs of ﬁxed architectures. In an attempt to provide\nmore robust forecasting of building energy consumption, the\nauthors of [45] proposed to use an LSTM recurrent neural\nnetwork together with an improved sine cosine optimization\nalgorithm. They also introduced a novel Haar wavelet-based\nmutation operator to optimize the hyper-parameters of the\nLSTM network and improve the divergence of the sine cosine\noptimization method.\nThe proposed model showed accurate and reliable\npredictions for short, mid, and long-term energy consumption\nforecasting problems. In [46], another integrated machine\nlearning model was proposed to boost the prediction per-\nformance of building energy consumption, and it showed\nlower prediction error rates compared to individual machine\nlearning models. Similarly, a hybrid approach including\nonline search data for household power consumption fore-\ncasting was developed to increase forecasting accuracy [47].\nTo forecast residential electricity usage, an extreme learning\nmachine model optimized by the Jaya algorithm was pro-\nposed, along with the selected search keywords. This hybrid\nmodel showed the ability to better predict residential electric-\nity consumption.\nRecently, transformer networks were introduced to resolve\nthe parallelization issue of the LSTM [48]. With the aid of\nattention, the intermediate distance between the source and\nthe target sequences is no longer constrained. Rather than\nproducing a single context vector from the last hidden state\nof the encoder, attention establishes shortcuts between the\ninput sequence and the whole source entry. For each output\nelement, the weights of these shortcuts can be customized.\nThis approach beneﬁts from eliminating recursion,\nso those parallel calculations help minimize the training\ntime and tackle the reduction in efﬁciency related to long-\nterm dependencies and the corresponding vanishing gradient\nproblem. Transformers have been successfully applied to\nhealthcare problems such as inﬂuenza-like illness predic-\ntion [49]. In general, deep transformers have two limitations:\n(1) they cannot represent greater than one ﬁxed length of\nrelationships, and (2) the divisions do not generally follow\nthe limitations of the sequence and result in segmentation in\nthe context, which results in ineffective optimization [50].\nIn contrast to model-aligned sequences, transformer net-\nworks don’t handle input in a sequence-ordered way. Instead,\nit analyses the whole series of information and utilizes mech-\nanisms for self-service to learn dependencies in the sequential\ndata. Transformer-based models do indeed have the ability to\ndescribe complicated time-series data dynamics that are dif-\nﬁcult for conventional sequence models such as RNNs [49].\nFor individual homes, energy consumption patterns are\nusually erratic due to many causes like weather and holi-\ndays. Therefore, the use of methodologies based solely on\nenergy consumption data to forecast energy use is unreli-\nable. Univariate time-series data analysis, such as household\nenergy consumption prediction, is challenging even with deep\nlearning models [41]. Thus, integrating other observations\n(whether the observed point is an anomaly, change point,\nor part of the patterns) may help improve the prediction\nperformance [51]. The similarities between the different data\nencoding variables in transformers (e.g., queries and keys) are\ncalculated based on their point-speciﬁc values without explic-\nitly considering local contexts [52]. This weakness could\nbe addressed either by introducing new attention algorithms\nreplacing the classical self-attention of the original trans-\nformer, e.g., Spring Time Warping Matrix [52], or providing\nmore information about the surroundings of the observed\npoint to the transformer externally [41]. This later approach\nforms the basis of the proposed method in this paper. In this\npaper, we propose to use SWT as an efﬁcient pre-processing\ntechnique that decomposes a given signal into sub-signals\nwith high and low frequencies, which offers an efﬁcient\nVOLUME 10, 2022 5173\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nFIGURE 1. (a) The proposed deep transformer SWT model for the household power consumption forecasting, (b) the transformer encoder/decoder,\n(c) Time2Vec block.\nrepresentation of the signal’s content and behavior. Next,\nwe use transformer networks to predict SWT sub-bands.\nHence, the novelty of this work is in the development of a\nhybrid approach for household energy consumption forecast-\ning based on SWT and transformers [48].\nOur contributions are in particular as follows:\n• We developed a hybrid SWT-Transformer model for\nhousehold power consumption time-series forecasting.\nThe developed transformer model forecasts the features\nproduced by the SWT. This combination helps tackle the\nproblem of irregular patterns in the univariate household\nenergy data. To the best of our knowledge, this is the ﬁrst\ntime SWT and transformers are combined to develop an\nefﬁcient energy consumption predictive model.\n• Experimental results, based on several energy consump-\ntion datasets from real-world households, show that our\nproposed SWT-Transformer approach can accurately\nforecast household energy usage, achieving superior\nprediction performance compared to existing methods.\nII. THE PROPOSED MODEL\nWe propose a hybrid approach based on the stationary\nwavelet transform and deep transformers for forecasting\nhousehold energy consumption. First, the initial univariate\nenergy input data is decomposed into sub-bands using the\nSWT to extract the local trends and patterns. Second, the\ndeep transformer is adopted to forecast the next wavelet sub-\nband. Finally, the inverse SWT is applied to the deep trans-\nformer outputs to reconstruct the predicted household energy\nconsumption. The overall proposed method is summarized in\nFig. 1a.\nA. DATA DESCRIPTION\nWe use the open-source energy consumption data in ﬁve\nseparate family homes in London, UK (UK), under the project\nname ’UK-DALE’ [53], to test the validity and strength of\nthe proposed model. To have a fair comparison with other\nexisting models, we used the same strategy and data used\nin [29], [41], and [54]. They combined multiple entries of the\noriginal data collected in 6-seconds intervals and converted it\nto a dataset with a time interval of 5 minutes.\nB. TRANSFORMER-SWT MODEL\nOur hybrid energy consumption Transformer-SWT model\nfollows the original Transformer architecture [48], which\nconsists of time to vector, encoder, and decoder layers.\n5174 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nThe transformer is a deep learning architecture that exclu-\nsively employs attention mechanisms for sequence-based\ndata processing. Therefore, it does not utilize recurrent and\nconvolutional layers that are widely used in sequence mod-\neling. Instead, it maintains an encoder design and employs\nstacked multi-head self-attention and fully connected lay-\ners. Each encoder layer includes a multi-head self-attention\nlayer followed by two feedforward layers. Both multi-head\nattention and feedforward layers are followed by dropout and\nAdd&Normlize layers.\nThe encoder consists of sub-encoders that handle the input\nof each layer sequentially, while the decoder includes layers\nthat do the same with the output of the encoder. Each encoder\nlayer aims to create encodings of critical information on\nwhich sections of the inputs are relevant to each other. The\nencodings are sent to the next encoder layer. Every decoder\nlayer does the reverse, takes all the encodings, and uses them\nto produce a series of outputs.\nTo this end, attention is used in each encoder and decoder\nlayer. For each input, attention measures and calls attention to\nthe pertinence of each input. The decoder layer is similar to\nthe encoder layer but uses one feedforward layer rather than\ntwo. Encoder and decoder layers have feedforward networks\nfor further output processing and have residual connections\nand layer normalization processes (Fig. 1b).\nEach multi-head attention has three learnable weights, the\nquery weights Q, the key weights K, and the value weights\nV [48]. Each attention head extracts a layer of ‘relevance’\nbetween input parameters.\nIn more detail, the multi-head attention module of the\ntransformer performs its calculations in parallel. The atten-\ntion module performs an attention mechanism several times\nin parallel (Fig. 2). The independent attention outputs are\nthen concatenated and linearly transformed into the desired\ndimension. Multi-head attention allows the transformer to\nencode many associations and subtleties for each input vari-\nable. A single attention module output is given by [48]:\nAttention (Q,K,V ) =softmax\n(QKT\n√dk\n)\nV (1)\nwhere dk is the dimension of query and key vectors.\nThe multi-head attention score is the concatenation of the\noutput of h heads given by Eq. (1) multiplied with a learnable\nprojection parameters W , i.e.:\nMultiheadAtt. =Concat (Att.1,··· ,Att.h)W (2)\nThe number of parallel attention layers used in the proposed\nmodel is h =12.\nThe Time2Vec [55], [56] is a learnable layer that is an\nextended version of the original positional encoding of the\ntransformer. It allows learning the input frequencies rather\nthan using a ﬁxed representation. We use this layer because\nit is invariant to time rescaling and can capture periodic\nand non-periodic patterns of the input signal (Fig. 1c). The\nFIGURE 2. Multi-head attention. V, K, and Q are learned linear projections\nof input data, and h is the number of parallel attention layers [48].\nFIGURE 3. n levels decomposition with SWT.\nTime2Vec operation implements the following equation [56]:\nTime2Vec(τ)[i] =\n{\nωiτ+ϕi, if i =0\nℑ(ωiτ+ϕi), if 1 ≤i ≤k (3)\nwhere, Time2Vec (τ)[i] is the i th element of Time2Vec (τ)\nthat has k elements, ℑis a periodic function, and ωi and ϕi\nare learnable parameters.\nC. THE STATIONARY WAVELET TRANSFORM\nThe stationary wavelet transform (SWT) is a wavelet trans-\nform algorithm proposed by Nason and Silverman [57] to\nsolve the shift-invariance and the non-redundancy issues in\nthe discrete wavelet transform [58]. SWT does not decimate\nthe initial signal. Instead, it changes the ﬁlters at each stage\nby padding zeroes instead of utilizing the down-sampling\ntechnique after implementing the low-pass or high-pass ﬁlters\non the signal [59]. The SWT sub-signals from the decompo-\nsition has the same length as the initial signal, which creates\nan appealing function compared to traditional wavelets. This\nVOLUME 10, 2022 5175\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nTABLE 1. Comparison results of the proposed model with other existing machine learning models for forecasting household energy consumption in\nfive-minute intervals.\nTABLE 2. Comparison results of the proposed model with other existing machine learning model for forecasting household energy consumption in\n10-minute intervals.\nfeature makes SWT an optimal choice for data used in neural\nnetworks and allows for more accurate knowledge of the\ncorresponding approximation and detail coefﬁcients. SWT\nalso demonstrated low-cost computing [60]. We, therefore,\nadopted SWT to analyze the energy consumption time-series\ndata, produce distinguishable low- and high-frequency com-\nponents, called approximations and details, and then provide\nsuch components as an input to the transformer.\nThe SWT approximation sub-band reﬂects the general\ntrend of the time-series, while the detail sub-band indicates\n5176 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nFIGURE 4. A selected portion from the validation part for two houses to show the performance of the proposed method for\nforecasting 5 minutes step size.\nminor series variations. The SWT breaks down the time\nseries with a hierarchical combination of low-pass and high-\npass wavelet ﬁlters, enabling the separation of high and low\nfrequencies.\nThe decomposition is seen as a dyadic tree shape [61].\nFig. 3 gives an illustration of one-dimensional signal\ndecomposition at n-level using SWT. For a given signal\nu(t) of length N, SWT decomposes u(t) into two coefﬁ-\ncients: approximation A1(t) and detail D1(t). Besides, the\napproximation coefﬁcients A1(t) are split down into two\npieces using up-sampled low and high-pass ﬁlters.\nThis procedure is repeated until achieving the n th decom-\nposition level. We tested the different decomposition levels\nand wavelet families. We experimented with different wavelet\nfamilies and different decomposition levels, and our experi-\nments demonstrate that Daubechies (db2) with three levels\nshow the best results, so we use them in all experiments.\nD. MODEL EVALUATION CRITERIA\nThe mean squared error (MSE), the root mean squared error\n(RMSE), the mean absolute error (MAE), and the mean\nabsolute percentage error (MAPE) are chosen as models\nevaluation metrics. They are deﬁned as follows:\nRMSE =\n\n√\n1\nN\nN∑\nk=1\n(\nyk −ˆyk\n)2 (4)\nMAE = 1\nN\nN∑\nk=1\n⏐\n⏐yk −ˆyk\n⏐\n⏐ (5)\nMAPE =100%\nN\nN∑\nk=1\n⏐\n⏐⏐⏐\nyk −ˆyk\nyk\n⏐\n⏐⏐⏐ (6)\nwhere yk is the kth sample value in y, ˆyk is the kth forecasted\nvalue, and N is the total number of samples.\nFIGURE 5. Bar graph of the four best results of Table 1.\nE. TRAINING\nThe deep transformer is used to forecast the SWT sublevels\nsignals based on the historical sublevels. Our experiments\nuse twelve lags from the SWT decomposed signal to fore-\ncast the next SWT sublevels. Let us take U the SWT n\ndecomposition of the energy consumption time series U =\n[An D1 D2 ... Dn]. The Transformer input is fed with histori-\ncal time series of decomposed household energy consumption\n(U(t −11),U(t −10),... U(t)) to forecast the next SWT\nlevels U(t +1), which can be described as\nˆy(t +1) =\n[ˆAn(t +1) ˆD1(t +1) ˆD2(t +1) ··· ˆDn(t +1)\n]\n.\nIn our case, the goal is to forecast one step, and this does not\nrequire feeding the decoder with predicted output. Therefore,\nwe ditch the decoder part altogether.\nWe used the RMSProp optimizer [14], as a learning algo-\nrithm to train our model using the parameters: η=0.001 and\nβ =0.999. The model is regularized using a dropout rate of\n0.1 for each sub-layer in the encoder layer, which contains\nVOLUME 10, 2022 5177\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nFIGURE 6. Selected portion from the validation part for two houses to show the performance of the proposed method for forecasting 10 minutes step\nsize.\na multi-attention sub-layer, a feedforward sub-layer, and a\nnormalization sub-layer. The number of query weights Q, key\nweights K, and value weights V used is 256.\nIII. RESULTS AND DISCUSSIONS\nThis study aims to forecast household energy consumption\nfor several time scales. The same problem, i.e., energy con-\nsumption prediction, has been addressed in several recent\nstudies [29], [41], allowing comparison with the literature\nperformance. We use the same data as in [54] that consists\nof ﬁve separate houses datasets collected by the UK-DALE\nproject for the whole year of 2015 [55]. From the whole\ndataset (comprising 36000 samples from each house), we use\ntwo-thirds of the samples for training and the remaining\none-third of the samples for validation. As described in\nsection II.C, each energy consumption data sample is decom-\nposed into three levels, producing three approximation and\nthree detail sub-signals. The deep transformer is then used\nto forecast the coefﬁcient representing the next predicted\nsample from SWT coefﬁcients representing the previous\n12 samples, i.e., the approximation and details. Finally,\nwe reconstruct the signal using the inverse SWT to compute\nthe forecasted household energy consumption. The results of\nthe proposed hybrid prediction model are compared with the\nfollowing state-of-art methods: the persistent method [14],\nARIMA [63], the multilayer perceptron (MLP) network [64],\nSVM [65], LSTM [59], CNN-LSTM [29], the hybrid\nSWT-LSTM [41], and the deep transformer [49].\nThe proposed architecture has been implemented in Ten-\nsorﬂow with Keras backend [67], [68]. The coefﬁcients of\nSWT approximation and detail sub-bands were standardized\nto have zero arithmetic mean and standard deviation of 1. This\npre-processing step helps speed up the training of deep neural\nnetworks.\nThe prediction results for the time steps of 5 minutes,\n10 minutes, 20 minutes, and 30 minutes are presented in\nFIGURE 7. Bar graph of the four best results of Table 2.\ntables 1, 2, 3, and 4, respectively. Table 1 shows the obtained\nresults for the case of 5 minutes step. We can see that all\nother strategies underperform our model and for all metrics.\nAccording to the data provider, houses 1 and 3 are relatively\nmore active, whereas houses 2, 4, and 5 are less volatile [41].\nWe show a selected timestamp from the validation part houses\n1 and 2, representing both categories, using our model and\ndeep transformer in Fig. 4.\nOne can see that our model forecasts better energy con-\nsumption than the transformer-based model solely without\nSWT, which predicts the global and the local features of\nenergy consumption. Fig. 3 demonstrates that the proposed\napproach well forecasts the irregular energy consumption\npattern in the case of 5 minutes forecasting. Fig. 4 presents\nthe bar graph of the four best models of Table 1, which\nare in decreasing order, CNN-LSTM, deep transformer,\nLSTM-SWT, and transformer-SWT. Our model improves the\naverage RMSE, MAE, and MAPE values by 48%, 47% and,\n51%, respectively, compared to the LSTM-SWT model [41]\nthat produces the nearest results.\n5178 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nFIGURE 8. Selected portion from the validation part for two houses to show the performance of the proposed method for forecasting 20 minutes\nstep size.\nResults for forecasting 10 minutes timestep are given in\nTable 2, in which we can see that our model outperforms\nall the other models by at least 59%, 56%, and 53% in\nRMSE, MAE, and MAPE, respectively. The performance of\nthe proposed model is shown in Fig. 6 and Fig. 7. In Fig. 6,\nwe plot the actual energy consumption and the two forecasted\noutputs based on our model and the deep transformer that\npresent the nearest result we have run, in this case. Fig. 7\nshows the bar graph of the four best results in Table 2: MLP,\ndeep transformer, LSTM-SWT, and our model.\nThe results for the cases of forecasting 20 and 30 minutes\ntimes steps are presented in tables 3 and 4. Again, we can see\nthat our model achieves superior prediction performance of\ntotal energy consumption.\nFig. 8 shows comparison results between the proposed\nmodel and the transformer without SWT for the prediction\nof 20 minutes step size, in which we can see the improve-\nments provided by using the SWT. We have to note that,\nin the cases of 20 and 30 minutes steps, the transformer alone\nperforms better than the hybrid models of CNN-LSTM [30],\nas shown in the bar graph of Fig. 9.\nOur model improves forecasting quality by 48%, 38%,\nand 40% in RMSE, MAE, and MAPE values compared\nto the LSTM-SWT model, which is the hybrid model that\npresents the nearest results for the case of 20 minutes time\nstep.\nFor the case of the forecasting 30 minutes step, our model\nimproves the RMSE, MAE, and MAPE values by 65 %, 57%,\nand 38%, respectively.\nIV. ANALYSIS\nIn this section, we study the robustness and performance\nof the proposed model in different situations that could\nhappen in real-life situations, like noise, magnitude, and dips\ndisturbances. House 1 is the sample used in this analysis.\nFIGURE 9. Bar graph of the four best results of Table 3.\nWe ﬁrst injected noisy Gaussian signal with different values\nof standard deviation σ to a signal from the testing dataset S\naccording to the following equation\nSn =S +σ ·std (S)·Gaussian (0,1) (7)\nwhere std (S)is the standard of deviation of the testing signal,\nSn is the noisy signal, and Gaussian (0,1) is a Gaussian signal\nwith zero mean and unit standard deviation.\nForecasting results reported in Table 5 shows that the\nperformance of the model understandably decreases with\nhigher level of noise, but maintains a robust energy consump-\ntion prediction performance under very high levels of noise\n(e.g. σ =2 and σ =3).\nNext, we studied the performance of the developed model\nafter injecting constant magnitude disturbances with different\ndurations, as depicted in Fig. 10. The injection was done\nin both parts of the day (night and day). First, we applied\nVOLUME 10, 2022 5179\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nTABLE 3. Comparison results of the proposed model with other existing machine learning models for forecasting household energy consumption in\n20-minute intervals.\nTABLE 4. Comparison results of the proposed model with other existing machine learning models for forecasting household energy consumption in\n30-minutes intervals.\na disturbance with a large magnitude during the night for\nmore than 4 hours of duration. Second, we injected a low-\nhigh-low magnitude sequence of disturbances during the day\nto see the model’s response to sudden electricity changes\n(inexistence or very high usage of electricity). One can\nobserve that the model reacts to these disturbances and can\nadequately forecast them even if the injected signals are\nsigniﬁcantly different from usual energy consumption data.\n5180 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\nFIGURE 10. (a) Forecasting model without disturbance, (b) Model reaction to constant disturbances with different durations and magnitudes Four\nhours of high magnitude, 1.5 hors of low magnitude, 1.5 hours of high magnitude, and 2.5 hours of low magnitude.\nTABLE 5. Robustness of the proposed model checking for house 1.\nV. LIMITATIONS\nDespite the proposed model’s effectiveness, we still have two\nlimitations. First, it is a learning-based system and can fail\nwhen faced with unknown circumstances. One possible way\nto alleviate this issue is to dynamically update the model\nwith new training data to increase the size and variability of\ninput data. The second limitation is that the decomposition\nmethod expects a regularly spaced signal, and this makes\nsignal reconstruction difﬁcult in multistep prediction prob-\nlems. The use of the recursive predicted output in the SWT\nreconstruction may resolve this issue. Our future work will\nfocus on resolving these two issues.\nVI. CONCLUSION\nIn this study, we have proposed a hybrid predictive model\nbased on SWT and deep transformers for reliable forecasting\nof residential energy consumption. Our model forecasts the\nlocal feature of the electrical energy consumption by using\nSWT and modeling the local tends through the deep trans-\nformer. Comparison with other existing machine learning\nmodels has shown the utility and superiority of the proposed\nmodel. For three signiﬁcant factors, the beneﬁt of using our\nmodel over other current models can be eligible.\n• SWT can efﬁciently analyze the energy usage time-\nseries data. Thus, each aspect, trend, or biomarker in the\ndata can be captured more quickly and precisely.\n• The deep transformer can predict different frequency\nlevels by SWT of energy consumption rather than\npredicting the entire signal that includes all sub-\nfrequency signals.\n• Taking the beneﬁt of our approach, the hybrid model can\nwell catch the sophisticated features of energy usage and\nproduce more precise forecasting performance for the\nfour-time scales with an average improvement of more\nthan 45 % in RMSE.\nAs a general conclusion, electric energy forecasting has\nimportant consequences for reliable power supply, effec-\ntive operation, and electricity generation systems sustainabil-\nity. The proposed strategy would incorporate a forecasting\napproach to reduce costs and control rising energy consump-\ntion. The proposed approach might also mitigate economic\nloss from unplanned activities of power plants.\nREFERENCES\n[1] M. Sajjad, Z. A. Khan, A. Ullah, T. Hussain, W. Ullah, M. Y. Lee, and\nS. W. Baik, ‘‘A novel CNN-GRU-based hybrid approach for short-term\nresidential load forecasting,’’ IEEE Access, vol. 8, pp. 143759–143768,\n2020, doi: 10.1109/ACCESS.2020.3009537.\n[2] World Energy Outlook 2019, IEA, Paris, France, 2019. [Online]. Available:\nhttps://www.iea.org/reports/world-energy-outlook-2019\n[3] P. Nejat, F. Jomehzadeh, M. M. Taheri, M. Gohari, and M. Z. A. Majid,\n‘‘A global review of energy consumption, CO 2 emissions and policy in\nthe residential sector (with an overview of the top ten CO 2 emitting\ncountries),’’Renew. Sustain. Energy Rev., vol. 43, pp. 843–862, Mar. 2015.\n[4] T.-Y. Kim and S.-B. Cho, ‘‘Predicting residential energy consump-\ntion using CNN-LSTM neural networks,’’ Energy, vol. 182, pp. 72–81,\nSep. 2019.\n[5] C. Li, Z. Ding, D. Zhao, J. Yi, and G. Zhang, ‘‘Building energy consump-\ntion prediction: An extreme deep learning approach,’’ Energies, vol. 10,\nno. 10, p. 1525, Oct. 2017.\nVOLUME 10, 2022 5181\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\n[6] C. Deb, F. Zhang, J. Yang, S. E. Lee, and K. W. Shah, ‘‘A review on time\nseries forecasting techniques for building energy consumption,’’ Renew.\nSustain. Energy Rev., vol. 74, pp. 902–924, Jul. 2017.\n[7] J.-S. Chou and D.-S. Tran, ‘‘Forecasting energy consumption time series\nusing machine learning techniques based on usage patterns of residential\nhouseholders,’’Energy, vol. 165, pp. 709–726, Dec. 2018.\n[8] N. Wei, C. Li, X. Peng, F. Zeng, and X. Lu, ‘‘Conventional models and\nartiﬁcial intelligence-based models for energy consumption forecasting: A\nreview,’’J. Petroleum Sci. Eng., vol. 181, Oct. 2019, Art. no. 106187.\n[9] V. G. Tran, V. Debusschere, and S. Bacha, ‘‘One week hourly electricity\nload forecasting using neuro-fuzzy and seasonal ARIMA models,’’ IFAC\nProc. Volume, vol. 45, no. 21, pp. 97–102, 2012.\n[10] M. I. Ahmad, ‘‘Seasonal decomposition of electricity consumption data,’’\nRev. Integr. Bus. Econ. Res., vol. 6, no. 4, pp. 271–275, 2017.\n[11] N. Fumo and M. A. R. Biswas, ‘‘Regression analysis for prediction of\nresidential energy consumption,’’ Renew. Sustain. Energy Rev., vol. 47,\npp. 332–343, Jul. 2015.\n[12] K. P. Amber, M. W. Aslam, and S. K. Hussain, ‘‘Electricity consumption\nforecasting models for administration buildings of the UK higher education\nsector,’’Energy Buildings, vol. 90, pp. 127–136, Mar. 2015.\n[13] E. M. de Oliveira and F. L. C. Oliveira, ‘‘Forecasting mid-long term electric\nenergy consumption through bagging ARIMA and exponential smoothing\nmethods,’’Energy, vol. 144, pp. 776–788, Feb. 2018.\n[14] R. K. Jain, K. M. Smith, P. J. Culligan, and J. E. Taylor, ‘‘Forecasting\nenergy consumption of multi-family residential buildings using support\nvector regression: Investigating the impact of temporal and spatial mon-\nitoring granularity on performance accuracy,’’ Appl. Energy, vol. 123,\npp. 168–178, Jun. 2014.\n[15] Y. Yaslan and B. Bican, ‘‘Empirical mode decomposition based denoising\nmethod with support vector regression for time series prediction: A case\nstudy for electricity load forecasting,’’ Measurement, vol. 103, pp. 52–61,\nJun. 2017.\n[16] R. Rueda, M. P. Cuéllar, M. C. Pegalajar, and M. Delgado, ‘‘Straight line\nprograms for energy consumption modelling,’’ Appl. Soft Comput., vol. 80,\npp. 310–328, Jul. 2019.\n[17] Y. Liu, H. Chen, L. Zhang, X. Wu, and X.-J. Wang, ‘‘Energy consumption\nprediction and diagnosis of public buildings based on support vector\nmachine learning: A case study in China,’’ J. Cleaner Prod., vol. 272,\nNov. 2020, Art. no. 122542.\n[18] Y. Chen, P. Xu, Y. Chu, W. Li, Y. Wu, L. Ni, Y. Bao, and K. Wang, ‘‘Short-\nterm electrical load forecasting using the support vector regression (SVR)\nmodel to calculate the demand response baseline for ofﬁce buildings,’’\nAppl. Energy, vol. 195, pp. 659–670, Jun. 2017.\n[19] A. Bogomolov, B. Lepri, R. Larcher, F. Antonelli, F. Pianesi, and\nA. Pentland, ‘‘Energy consumption prediction using people dynamics\nderived from cellular network data,’’ EPJ Data Sci., vol. 5, no. 1, pp. 1–13,\nDec. 2016.\n[20] A. Khan, H. Chiroma, M. Imran, A. Khan, J. I. Bangash, M. Asim,\nM. F. Hamza, and H. Aljuaid, ‘‘Forecasting electricity consumption based\non machine learning to improve performance: A case study for the orga-\nnization of petroleum exporting countries (OPEC),’’ Comput. Electr. Eng.,\nvol. 86, Sep. 2020, Art. no. 106737.\n[21] T. Pinto, I. Praça, Z. Vale, and J. Silva, ‘‘Ensemble learning for electricity\nconsumption forecasting in ofﬁce buildings,’’ Neurocomputing, vol. 423,\npp. 747–755, Jan. 2021.\n[22] A. Rahman, V. Srikumar, and A. D. Smith, ‘‘Predicting electricity con-\nsumption for commercial and residential buildings using deep recurrent\nneural networks,’’ Appl. Energy, vol. 212, pp. 372–385, Feb. 2018.\n[23] H. Shi, M. Xu, and R. Li, ‘‘Deep learning for household load forecasting—\nA novel pooling deep RNN,’’ IEEE Trans. Smart Grid, vol. 9, no. 5,\npp. 5271–5280, Sep. 2018.\n[24] W. Kong, Z. Y. Dong, Y. Jia, D. J. Hill, Y. Xu, and Y. Zhang, ‘‘Short-term\nresidential load forecasting based on LSTM recurrent neural network,’’\nIEEE Trans. Smart Grid, vol. 10, no. 1, pp. 841–851, Jan. 2019.\n[25] J. Q. Wang, Y. Du, and J. Wang, ‘‘LSTM based long-term energy\nconsumption prediction with periodicity,’’ Energy, vol. 197, Apr. 2020,\nArt. no. 117197.\n[26] H. Hu, L. Wang, L. Peng, and Y.-R. Zeng, ‘‘Effective energy consumption\nforecasting using enhanced bagged echo state network,’’ Energy, vol. 193,\nFeb. 2020, Art. no. 116778.\n[27] M. Fayaz and D. Kim, ‘‘A prediction methodology of energy consumption\nbased on deep extreme learning machine and comparative analysis in\nresidential buildings,’’ Electronics, vol. 7, no. 10, p. 222, Sep. 2018.\n[28] Y. Gao, Y. Ruan, C. Fang, and S. Yin, ‘‘Deep learning and transfer learning\nmodels of energy consumption forecasting for a building with poor infor-\nmation data,’’ Energy Buildings, vol. 223, Sep. 2020, Art. no. 110156.\n[29] G.-F. Fan, X. Wei, Y.-T. Li, and W.-C. Hong, ‘‘Forecasting electricity\nconsumption using a novel hybrid model,’’ Sustain. Cities Soc., vol. 61,\nOct. 2020, Art. no. 102320.\n[30] J. Xiao, Y. Li, L. Xie, D. Liu, and J. Huang, ‘‘A hybrid model based on\nselective ensemble for energy consumption forecasting in China,’’ Energy,\nvol. 159, pp. 534–546, Sep. 2018.\n[31] K. Yan, X. Wang, Y. Du, N. Jin, H. Huang, and H. Zhou, ‘‘Multi-step short-\nterm power consumption forecasting with a hybrid deep learning strategy,’’\nEnergies, vol. 11, no. 11, p. 3089, Nov. 2018.\n[32] N. Somu, M R. G. Raman, and K. Ramamritham, ‘‘A hybrid model for\nbuilding energy consumption forecasting using long short term memory\nnetworks,’’Appl. Energy, vol. 261, Mar. 2020, Art. no. 114131.\n[33] H. Hu, L. Wang, and S.-X. Lv, ‘‘Forecasting energy consumption and wind\npower generation using deep echo state network,’’ Renew. Energy, vol. 154,\npp. 598–613, Jul. 2020.\n[34] C. Xia and Z. Wang, ‘‘Drivers analysis and empirical mode decomposition\nbased forecasting of energy consumption structure,’’ J. Cleaner Prod.,\nvol. 254, May 2020, Art. no. 120107.\n[35] F. Kaytez, ‘‘A hybrid approach based on autoregressive integrated mov-\ning average and least-square support vector machine for long-term fore-\ncasting of net electricity consumption,’’ Energy, vol. 197, Apr. 2020,\nArt. no. 117200.\n[36] T. Liu, Z. Tan, C. Xu, H. Chen, and Z. Li, ‘‘Study on deep reinforcement\nlearning techniques for building energy consumption forecasting,’’ Energy\nBuildings, vol. 208, Feb. 2020, Art. no. 109675.\n[37] G. Zhang, C. Tian, C. Li, J. J. Zhang, and W. Zuo, ‘‘Accurate forecasting\nof building energy consumption via a novel ensembled deep learning\nmethod considering the cyclic feature,’’ Energy, vol. 201, Jun. 2020,\nArt. no. 117531.\n[38] M.-R. Kazemzadeh, A. Amjadian, and T. Amraee, ‘‘A hybrid data mining\ndriven algorithm for long term electric peak load and energy demand\nforecasting,’’Energy, vol. 204, Aug. 2020, Art. no. 117948.\n[39] H. Lu, F. Cheng, X. Ma, and G. Hu, ‘‘Short-term prediction of building\nenergy consumption employing an improved extreme gradient boosting\nmodel: A case study of an intake tower,’’ Energy, vol. 203, Jul. 2020,\nArt. no. 117756.\n[40] F. U. M. Ullah, A. Ullah, I. U. Haq, S. Rho, and S. W. Baik, ‘‘Short-\nterm prediction of residential power energy consumption via CNN\nand multi-layer bi-directional LSTM networks,’’ IEEE Access, vol. 8,\npp. 123369–123380, 2020.\n[41] K. Yan, W. Li, Z. Ji, M. Qi, and Y. Du, ‘‘A hybrid LSTM neural network for\nenergy consumption forecasting of individual households,’’ IEEE Access,\nvol. 7, pp. 157633–157642, 2019, doi: 10.1109/ACCESS.2019.2949065.\n[42] N. Somu, M. R. G. Raman, and K. Ramamritham, ‘‘A deep learning\nframework for building energy consumption forecast,’’ Renew. Sustain.\nEnergy Rev., vol. 137, Mar. 2021, Art. no. 110591.\n[43] C. Liu, B. Sun, C. Zhang, and F. Li, ‘‘A hybrid prediction model for\nresidential electricity consumption using holt-winters and extreme learning\nmachine,’’Appl. Energy, vol. 275, Oct. 2020, Art. no. 115383.\n[44] X. J. Luo, L. O. Oyedele, A. O. Ajayi, O. O. Akinade, H. A. Owolabi, and\nA. Ahmed, ‘‘Feature extraction and genetic algorithm enhanced adaptive\ndeep neural network for energy consumption prediction in buildings,’’\nRenew. Sustain. Energy Rev., vol. 131, Oct. 2020, Art. no. 109980.\n[45] N. Somu, M. R. G. Raman, and K. Ramamritham, ‘‘A hybrid model for\nbuilding energy consumption forecasting using long short term memory\nnetworks,’’Appl. Energy, vol. 261, Mar. 2020, Art. no. 114131.\n[46] R. Wang, S. Lu, and W. Feng, ‘‘A novel improved model for building\nenergy consumption prediction based on model integration,’’ Appl. Energy,\nvol. 262, Mar. 2020, Art. no. 114561.\n[47] F. Gao, H. Chi, and X. Shao, ‘‘Forecasting residential electricity consump-\ntion using a hybrid machine learning model with online search data,’’ Appl.\nEnergy, vol. 300, Oct. 2021, Art. no. 117393.\n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advances in\nNeural Information Processing Systems, vol. 30, I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.\nRed Hook, NY, USA: Curran Associates, 2017, pp. 5998–6008.\n[49] N. Wu, B. Green, X. Ben, and S. O’Banion, ‘‘Deep transformer mod-\nels for time series forecasting: The inﬂuenza prevalence case,’’ 2020,\narXiv:2001.08317.\n5182 VOLUME 10, 2022\nL. Saad Saoudet al.: Household Energy Consumption Prediction Using SWT and Transformers\n[50] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[51] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y.-X. Wang, and X. Yan,\n‘‘Enhancing the locality and breaking the memory bottleneck of trans-\nformer on time series forecasting,’’ in Proc. Conf. Neural Inf. Process. Syst.\n(NeurIPS), 2019, pp. 5243–5253.\n[52] Y. Lin, I. Koprinska, and M. Rana, ‘‘SpringNet: Transformer and spring\nDTW for time series forecasting,’’ in Neural Information Processing\n(Lecture Notes in Computer Science), vol. 12534, H. Yang, K. Pasupa,\nA. C. S. Leung, J. T. Kwok, J. H. Chan, and I. King, Eds. Cham,\nSwitzerland: Springer, 2020.\n[53] J. Kelly and W. Knottenbelt, ‘‘The UK-DALE dataset, domestic appliance-\nlevel electricity demand and whole-house demand from ﬁve UK homes,’’\nSci. Data, vol. 2, no. 1, Mar. 2015, Art. no. 150007.\n[54] K. Yan. CNN-LSTM Framework. Accessed: Aug. 3, 2021. [Online].\nAvailable: http://keddiyan.com/ﬁles/PowerForecast.html\n[55] S. M. Kazemi, R. Goel, S. Eghbali, J. Ramanan, J. Sahota, S. Thakur,\nS. Wu, C. Smyth, P. Poupart, and M. Brubaker, ‘‘Time2Vec: Learning a\nvector representation of time,’’ 2019, arXiv:1907.05321.\n[56] Y. Shen, X. Jiang, Y. Wang, X. Jin, and X. Cheng, ‘‘Dynamic rela-\ntion extraction with a learnable temporal encoding method,’’ in Proc.\nIEEE Int. Conf. Knowl. Graph (ICKG), Aug. 2020, pp. 235–242, doi:\n10.1109/ICBK50248.2020.00042.\n[57] G. P. Nason and B. W. Silverman, ‘‘The stationary wavelet transform and\nsome statistical applications,’’ in Wavelets and Statistics(Lecture Notes in\nStatistics), vol. 103, A. Antoniadis and G. Oppenheim, Eds. New York,\nNY, USA: Springer, 1995, pp. 281–299.\n[58] U. B. Tayab, A. Zia, F. Yang, J. Lu, and M. Kashif, ‘‘Short-term load\nforecasting for microgrid energy management system using hybrid HHO-\nFNN model with best-basis stationary wavelet packet transform,’’ Energy,\nvol. 203, Jul. 2020, Art. no. 117857.\n[59] C. Yang, P. Liu, G. Yin, H. Jiang, and X. Li, ‘‘Defect detection in magnetic\ntile images based on stationary wavelet transform,’’ NDT & E Int., vol. 83,\npp. 78–87, Oct. 2016.\n[60] R. R. Coifman and D. L. Donoho, ‘‘Translation-invariant denoising,’’ in\nWavelets and Statistics(Lecture Notes in Statistics), vol. 103. New York,\nNY, USA: Springer-Verlag, 1995, pp. 125–150.\n[61] S. Supratid, T. Aribarg, and S. Supharatid, ‘‘An integration of station-\nary wavelet transform and nonlinear autoregressive neural network with\nexogenous input for baseline and future forecasting of reservoir inﬂow,’’\nWater Resour. Manage., vol. 31, no. 12, pp. 4023–4043, Sep. 2017.\n[62] G. Hinton, ‘‘Neural networks for machine learning—Lecture 6a—\nOverview of mini-batch gradient descent,’’ Univ. Toronto, Toronto,\nON, Canada, 2012. Accessed: Dec. 20, 2020. [Online]. Available:\nhttps://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n[63] B. Nepal, M. Yamaha, A. Yokoe, and T. Yamaji, ‘‘Electricity load fore-\ncasting using clustering and ARIMA model for energy management in\nbuildings,’’Jpn. Archit. Rev., vol. 3, no. 1, pp. 62–76, Jan. 2020.\n[64] J. Moon, S. Park, S. Rho, and E. Hwang, ‘‘A comparative analysis of\nartiﬁcial neural network architectures for building energy consumption\nforecasting,’’ Int. J. Distrib. Sensor Netw., vol. 15, no. 9, Sep. 2019,\nArt. no. 155014771987761.\n[65] Z. Ma, C. Ye, H. Li, and W. Ma, ‘‘Applying support vector machines to\npredict building energy consumption in China,’’ Energy Proc., vol. 152,\npp. 780–786, Oct. 2018.\n[66] D. L. Marino, K. Amarasinghe, and M. Manic, ‘‘Building energy load\nforecasting using deep neural networks,’’ in Proc. 42nd Annu. Conf. IEEE\nInd. Electron. Soc., Oct. 2016, pp. 7046–7051.\n[67] Keras: The Python Deep Learning Library. Accessed: Aug. 3, 2021.\n[Online]. Available: http://keras.io/\n[68] M. Abadi et al., ‘‘TensorFlow: A system for large-scale machine learning,’’\nin Proc. 12th USENIX Conf. Oper. Syst. Design Implement., Savannah, GA,\nUSA, Nov. 2016, pp. 265–283.\nLYES SAAD SAOUD received the D.Sc. and\nM.C.A. degrees (empowerment to direct research)\nin physics, specialty in renewables energies, from\nthe University of Boumerdès, Algeria, in January\n2015 and February 2017, respectively. He was\nan Associate Professor with the University of\nBoumerdès. From February 2019 to October 2019,\nhe was the Head and a Scientiﬁc Member of the\nDepartment of Electrical Systems Engineering and\na Scientiﬁc Member of the Faculty of Technology,\nBoumerdès. From October 2019, he joined Khalifa University as a postdoc-\ntoral fellow in artiﬁcial intelligence and computer vision. He is currently an\nEngineer and a Magister in electronics. His recent research interests include\nadvanced neural networks, fuzzy logic, hybrid systems, renewable energy,\nsustainability and the environment, energy engineering, and cybersecurity.\nHe has served as a member of the Technical Committee for several inter-\nnational specialized conferences. He is a Reviewer of IEEE TRANSACTIONS\nON NEURAL NETWORKS AND LEARNING SYSTEMS, IEEE ACCESS, Transactions of\nthe Institute of Measurement and Control(SAGE Publications), Journal of\nRenewable and Sustainable Energy(American Institute of Physics), Neuro-\ncomputing (Elsevier), and Neural Processing Letters(Springer).\nHASAN AL-MARZOUQI(Senior Member, IEEE)\nreceived the bachelor’s (Hons.) and M.S. degrees\nin electrical and computer engineering from\nVanderbilt University, Nashville, TN, USA, in\n2004 and 2006, respectively, and the Ph.D. degree\nin electrical and computer engineering from the\nGeorgia Institute of Technology, in 2014. He is\ncurrently an Assistant Professor with the Electrical\nand Computer Engineering Department, Khalifa\nUniversity, Abu Dhabi, United Arab Emirates. His\nresearch interests include machine learning, image and video processing,\nand digital rock physics. He is an Organizing Committee Member and the\nChallenge Sessions Co-Chair of ICIP 2020 in Abu Dhabi and a Technical\nProgram Committee Member of EUVIP 2020 in Paris. He is an active\nreviewer of many international conferences and journals.\nRAMY HUSSEIN received the Ph.D. degree in\nelectrical and computer engineering from The\nUniversity of British Columbia, Canada. He is\ncurrently a Research Scientist with the Center\nfor Artiﬁcial Intelligence in Medicine & Imag-\ning (AIMI), Stanford University. He is interested\nin developing and optimizing artiﬁcial intelli-\ngence solutions for the early diagnosis and pre-\ndiction of cerebrovascular and neurodegenrative\ndiseases, with more focus on ischemic stroke and\nalzheimer’s disease. He combines computer vision with signal/image pro-\ncessing for better acquisition, detection, and also prediction of biomedical\ntime-series data, CT scans, MRI, and also PET images.\nVOLUME 10, 2022 5183",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7370678186416626
    },
    {
      "name": "Power consumption",
      "score": 0.626462996006012
    },
    {
      "name": "Wavelet transform",
      "score": 0.6228664517402649
    },
    {
      "name": "Energy consumption",
      "score": 0.6151745319366455
    },
    {
      "name": "Transformer",
      "score": 0.5547755360603333
    },
    {
      "name": "Wavelet",
      "score": 0.5200157165527344
    },
    {
      "name": "Consumption (sociology)",
      "score": 0.4778949022293091
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4168933033943176
    },
    {
      "name": "Econometrics",
      "score": 0.376832515001297
    },
    {
      "name": "Power (physics)",
      "score": 0.3368304371833801
    },
    {
      "name": "Data mining",
      "score": 0.3359954357147217
    },
    {
      "name": "Engineering",
      "score": 0.12263479828834534
    },
    {
      "name": "Voltage",
      "score": 0.12002292275428772
    },
    {
      "name": "Mathematics",
      "score": 0.1095905601978302
    },
    {
      "name": "Electrical engineering",
      "score": 0.09301385283470154
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I176601375",
      "name": "Khalifa University of Science and Technology",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 51
}