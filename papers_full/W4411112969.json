{
  "title": "CLaMP 2: Multimodal Music Information Retrieval Across 101 Languages Using Large Language Models",
  "url": "https://openalex.org/W4411112969",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4375892914",
      "name": "Shangda Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103747309",
      "name": "Yashan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3111091201",
      "name": "Ruibin Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2021181949",
      "name": "Guo Zhancheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983219134",
      "name": "Xu Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096591353",
      "name": "Ge Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5101254865",
      "name": "Monan Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964638924",
      "name": "Jing Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095916268",
      "name": "Xuefeng Mu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4375999453",
      "name": "Yuejie Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296787690",
      "name": "Yuanliang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109915202",
      "name": "Jiafeng Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1571171104",
      "name": "Xiaobing Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2058280834",
      "name": "Feng Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588"
  ],
  "abstract": null,
  "full_text": "Findings of the Association for Computational Linguistics:\nNAACL 2025, pages 435–451\nApril 29 - May 4, 2025 ©2025 Association for Computational Linguistics\nCLaMP 2: Multimodal Music Information Retrieval\nAcross 101 Languages Using Large Language Models\nShangda Wu Yashan Wang Ruibin Yuan Zhancheng Guo Xu Tan\nGe Zhang Monan Zhou Jing Chen Xuefeng Mu Yuejie Gao\nYuanliang Dong Jiafeng Liu Xiaobing Li Feng Yu Maosong Sun\nDetails of contributors, correspondence, and affiliations are on Page 9\nhttps://github.com/sanderwood/clamp2\nAbstract\nChallenges in managing linguistic diversity and\nintegrating various musical modalities are faced\nby current music information retrieval systems.\nThese limitations reduce their effectiveness in\na global, multimodal music environment. To\naddress these issues, we introduce CLaMP 2,\na system compatible with 101 languages that\nsupports both ABC notation (a text-based musi-\ncal notation format) and MIDI (Musical Instru-\nment Digital Interface) for music information\nretrieval. CLaMP 2, pre-trained on 1.5 mil-\nlion ABC-MIDI-text triplets, includes a multi-\nlingual text encoder and a multimodal music\nencoder aligned via contrastive learning. By\nleveraging large language models, we obtain\nrefined and consistent multilingual descriptions\nat scale, significantly reducing textual noise\nand balancing language distribution. Our ex-\nperiments show that CLaMP 2 achieves state-\nof-the-art results in both multilingual semantic\nsearch and music classification across modali-\nties, thus establishing a new standard for inclu-\nsive and global music information retrieval.\n1 Introduction\nAs a cross-cultural art form that transcends geo-\ngraphical boundaries, music is being accessed glob-\nally more than ever, as people seek diverse content\nto enhance their aesthetic experience. However,\ncurrent Music Information Retrieval (MIR) sys-\ntems struggle to meet this demand, particularly in\nthe area of multilingual retrieval. For example, a\nJapanese user searching for \"Brazilian choro mu-\nsic with themes of celebration and carefreeness\"\nin their native language may face significant chal-\nlenges. Keyword-based retrieval methods might\nreturn choro music, but they often fail to capture\nthe specific themes the user is searching for. Mean-\nwhile, existing cross-modal MIR models remain\nheavily focused on English (Huang et al., 2022;\nElizalde et al., 2023; Doh et al., 2023b), making\neffective multilingual semantic search challenging.\nA key limitation in the development of multilin-\ngual MIR systems is that most music-text datasets\nare predominantly in English (Agostinelli et al.,\n2023; Lanzendörfer et al., 2023; Manco et al.,\n2023). As a result, MIR models struggle to pro-\ncess text queries in non-English languages. Addi-\ntionally, textual noise—such as inconsistent meta-\ndata and variations in terminology—complicates\nthe task of matching descriptions to the appropri-\nate music. Addressing these challenges requires\nadvanced techniques to manage multilingual data\nmore effectively and reduce noise, allowing MIR\nsystems to bridge linguistic and aesthetic gaps.\nRecent Large Language Models (LLMs) (Ope-\nnAI, 2023; Meta, 2024; Google, 2024) have demon-\nstrated robust performance in language-related\ntasks. LLMs have been used in previous cross-\nmodal MIR models and music-text dataset curation\nto generate coherent descriptions and annotations\n(Doh et al., 2023a; Wu et al., 2023b; Lu et al.,\n2023; Melechovský et al., 2024). This has proven\neffective in improving text quality and enhancing\nmodel performance. Since LLMs are typically mul-\ntilingual, they hold significant potential for gener-\nating high-quality music descriptions in multiple\nlanguages. This could overcome the limitations\nof current MIR systems and significantly enhance\nglobal music accessibility.\nTo leverage these advancements, we introduce\nCLaMP 2, a cross-modal MIR model designed to\neffectively link multilingual text with diverse music\ndata. The model includes a text encoder (Conneau\net al., 2020) and a music encoder (Wu et al., 2023a),\nwhich are aligned by contrastive learning (Sohn,\n2016; van den Oord et al., 2018). Pre-trained on a\nsubstantial dataset of 1.5 million ABC-MIDI-text\ntriplets, CLaMP 2 incorporates LLM-generated\ntext to boost its multilingual processing capabilities.\nThis enables the model to gain a deep understand-\ning of musical concepts and their subtleties across\nvarious languages. Notably, CLaMP 2 supports\n435\n101 languages and unifies two symbolic music for-\nmats—ABC notation and MIDI—with new encod-\ning methods into one framework. By enhancing\nmultilingual semantic search and integrating di-\nverse music data, CLaMP 2 sets a new standard for\nglobal MIR, enabling users to access music from a\nwide range of linguistic and cultural contexts.\nThe contributions of this paper are as follows:\n• We utilized GPT-4 (OpenAI, 2023) to refine\nthe multilingual corpus used for contrastive\nlearning. This reduced noise, balanced lan-\nguage distribution, and improved the overall\nquality of the pre-training dataset.\n• We enhanced an existing music encoder (Wu\net al., 2023a) to support both ABC notation\nand MIDI data using novel encoding tech-\nniques for better musical representation. Em-\npirical results prove that joint training on both\nmodalities enhances extracted feature quality.\n• CLaMP 2 achieves state-of-the-art results\nin multiple MIR tasks, showing that LLM-\ngenerated data significantly boosts multilin-\ngual retrieval performance.\n2 Related Work\n2.1 Multilingual Language Models\nMultilingual Language Models (MLMs), trained\non text from various languages, play a crucial role\nin Natural Language Processing (NLP) and related\nfields. Early MLM research used word embed-\ndings to represent words of different languages in a\nshared representation space. For instance, fastText\n(Joulin et al., 2017) provided pre-trained word em-\nbeddings for multilingual NLP tasks, enabling the\ncalculation of cross-language similarities.\nIn recent years, more advanced MLMs based\non complex neural network architectures (Vaswani\net al., 2017) have been introduced. Examples in-\nclude mBERT1, mBART (Liu et al., 2020), and\nmT5 (Xue et al., 2021), all of which evolved from\ntheir monolingual counterparts (Devlin et al., 2019;\nLewis et al., 2020; Raffel et al., 2020) and are well-\nsuited to multilingual environments. XLM-R (Con-\nneau et al., 2020) has shown strong performance\nin low-resource languages, demonstrating the effi-\ncacy of large-scale multilingual modeling. In con-\ntrast to English-centric models, M2M-100 (Fan\n1https://github.com/google-research/bert/blob/\nmaster/multilingual.md\net al., 2021) allows direct translation between 100\nlanguages, marking a major step forward in mul-\ntilingual translation. Additionally, SeamlessM4T\n(Meta, 2023b) overcomes the limitations of tradi-\ntional translation models by supporting up to 100\nlanguages and enabling translation between speech\nand text, as well as within the same modality, all in\na unified framework.\nLately, LLMs (Zhipu, 2024; Mistral, 2024; Al-\nibaba, 2024) have become increasingly multilin-\ngual to better serve a global audience. By utilizing\ndiverse linguistic data from large training corpora,\nLLMs have improved both their accessibility and\nusefulness for users around the world. Similarly,\ncross-modal MIR systems must evolve to support\nmultilingual queries, enabling more inclusive re-\ntrieval and interaction across languages.\n2.2 Applications of LLMs in Music\nRecent advancements in LLMs have greatly influ-\nenced the music field. Specifically, many models\nand datasets now leverage LLM-generated text to\nimprove both music understanding and generation.\nMuseCoco (Lu et al., 2023) uses LLMs to trans-\nlate musical attributes into coherent, detailed de-\nscriptions, enabling more precise control over mu-\nsic generation. Similarly, Noise2Music (Huang\net al., 2023) leverages pre-trained LLMs to gen-\nerate musical descriptions paired with audio data,\nenriching the dataset with semantically rich cap-\ntions. Beyond generative models, TTMR++ (Doh\net al., 2024) enhances text-to-music retrieval by\nincorporating detailed descriptions from a fine-\ntuned LLaMA 2 (Meta, 2023a) model alongside\nmetadata, leading to more relevant and accurate\nsearch results. For dataset curation, MidiCaps\n(Melechovský et al., 2024) provides over 168 thou-\nsand MIDI files, each paired with detailed musi-\ncal attributes like tempo, key, and instrumentation.\nThese attributes are then utilized by Claude 3 Opus\n(Anthropic, 2024) to generate fluent captions for\nthe MIDI files. LP-MusicCaps (Doh et al., 2023a)\nemploys GPT-3.5 Turbo (Ouyang et al., 2022) to\ngenerate music descriptions and explores different\ninstructions to create diverse captions, resulting in\n2.2 million captions and 0.5 million audio clips.\nNevertheless, the aforementioned efforts mainly\nfocus on improving text coherence and fluency and\nare English-exclusive. To the best of our knowl-\nedge, CLaMP 2 is the first to leverage the multilin-\ngual capabilities of LLMs to improve multilingual\nperformance in the music field.\n436\nClair de Lune (Moonlight)\nSymphony No. 9 in D minor, Op. 125\nEl Choclo (The Corn Cob)\nAmazing Grace\n十面埋伏 (Ambush from All Sides)\nLarge\nLanguage\nModel\nUna vivace melodia di danza argentina\n(A lively Argentine dance tune)\n역사적인전투를묘사한강렬한곡\n(An intense piece depicting a historic battle)\nΜια ψυχική ύμνος σωτηρίας\n(A soulful hymn of redemption)\nA tranquil piano piece evoking nostalgia\nएक भ� �सम्फनी जो एकता का ज� मनाती है\n(A grand symphony celebrating unity)\nEnglish-Centric\nNoisy Text Data\nRefined Multilingual Corpus Performance Data & Sheet Music\nShared Representation Space\nMultilingual\nText\nEncoder\nMultimodal\nMusic\nEncoder\nContrastive Learning Framework of CLaMP 2\nFigure 1: CLaMP 2 is a cross-modal MIR model that uses contrastive learning to link multilingual text and\nmultimodal music data. It employs GPT-4 to refine the multilingual corpus, reducing noise and achieving a more\nbalanced language distribution. The refined text data is then encoded by a multilingual text encoder. Meanwhile,\nmusic data in both ABC notation (sheet music) and MIDI (performance data) formats is processed by a multimodal\nmusic encoder. Both encoders project data into a shared representation space to connect text and music.\n3 CLaMP 2\nIn this section, we present the CLaMP 2 framework.\nWe begin with an overview of contrastive learning\nfor modality alignment, followed by discussions\nof the multilingual text and multimodal music en-\ncoders. Finally, we introduce the data sources used\nfor pre-training and elaborate on how we leverage\nGPT-4 to enhance data quality.\n3.1 Contrastive Learning\nContrastive learning (Sohn, 2016; van den Oord\net al., 2018) is a powerful technique in various\napplications for aligning different modalities (Rad-\nford et al., 2021; Girdhar et al., 2023). It minimizes\nthe distance between paired representations and\nmaximizes that for unpaired ones. This effectively\nmaps semantically related features (e.g., an image\nand its caption) close together in a shared represen-\ntation space while separating unrelated ones.\nAs shown in Fig. 1, CLaMP 2 applies contrastive\nlearning to ABC-MIDI-text triplets. The music\nencoder processes both ABC notation and MIDI\ndata, while the text encoder handles the correspond-\ning text inputs. During each training epoch, either\nABC or MIDI data from each triplet is randomly\nselected for the music encoder, while the text en-\ncoder processes either the original metadata or the\nrefined multilingual descriptions generated by GPT-\n4. Additionally, instrument information is removed\nfrom the music data 90% of the time, encourag-\ning the model to focus on broader musical con-\ncepts rather than specific instrumentations. Both\nencoders project data into a shared representation\nspace to learn the underlying connections between\nmusic and text. In this space, similar musical and\ntextual concepts are clustered together, while dis-\nsimilar ones are kept apart.\n3.2 Multilingual Text Encoder\nCLaMP 2 uses XLM-R-base (Conneau et al., 2020),\na multilingual text encoder based on RoBERTa (Liu\net al., 2019). With 270 million parameters, it is pre-\ntrained on a 2.5TB cleaned CommonCrawl corpus\nthat spans a wide range of languages, enabling it to\ncapture diverse linguistic nuances.\nDuring each training epoch, the input text for\neach triplet is randomly selected with the follow-\ning probabilities: 50% for the raw text data, 25%\nfor LLM-generated English descriptions, and 25%\nfor LLM-generated non-English descriptions. This\nselection ensures a balanced exposure to both real-\nworld and LLM-generated multilingual data. Ad-\nditionally, we apply text dropout from the original\nCLaMP framework (Wu et al., 2023a) to the raw\ntext data. It helps the model generalize better by\nreducing overfitting to specific input patterns.\nFor computational efficiency, we set the maxi-\nmum text length to 128. Longer texts are truncated\nin one of three ways with equal probability: using\nthe first, the last, or randomly selecting a segment\nof 128 tokens. This minimizes bias that could arise\nfrom relying on a single truncation method.\n3.3 Multimodal Music Encoder\nCLaMP 2’s multimodal music encoder supports\nmulti-track music encoding in both ABC notation\nand MIDI. Although they can be mutually con-\nverted, they are different in nature. ABC notation\n(sheet music), a text-based sheet music represen-\ntation like stave notation, is theory-oriented and\nideal for presenting complex musical concepts to\nmusicians for study and analysis. In contrast, MIDI\n(performance data) precisely encodes performance\ninformation related to timing and dynamics, thus\nsuitable for music production and live performance.\n437\nThe music encoder of CLaMP 2 is built on\nM3 (Wu et al., 2023a), a self-supervised model\ndesigned for feature extraction from sheet music\nbased on bar patching. This method divides sheet\nmusic into bar-like segments, maintaining musi-\ncal coherence while improving efficiency. M3\nhas an asymmetric encoder-decoder framework:\nthe patch-level encoder extracts contextualized fea-\ntures from patches, while the char-level decoder\nthen uses these features to autoregressively recon-\nstruct each corresponding bar. During pre-training,\n45% of patches are randomly selected and uni-\nformly processed with corresponding probabilities:\n80% masked, 10% shuffled, and 10% unchanged.\nM3 is optimized via cross-entropy loss to predict\noriginal patches from noisy input.\nCompared to the previous M3 model, we made\nseveral important improvements to CLaMP 2’s mul-\ntimodal music encoder. Notably, it now supports\nMIDI data. MIDI messages are first read losslessly\nfrom the original file using the mido library2 and\nthen converted to the MIDI Text Format (MTF) pro-\nposed in this paper. As MTF is a text-based format,\neach message read from it can be treated as a patch\nfor M3. It offers two main advantages: 1) seamless\nintegration with the M3 framework, enabling the\nsame training methods, and 2) lossless MIDI-to-\nMTF conversion, which preserves all information\nand avoids common quantization errors found in\nexisting MIDI representations (Oore et al., 2020;\nHuang and Yang, 2020; Hsiao et al., 2021).\nAnother improvement is restructuring ABC nota-\ntion into a voice-interleaved form. As previous re-\nsearch has verified (Qu et al., 2024), this can signifi-\ncantly reduce the difficulty of modeling multi-track\nABC notation and is conducive to training. Im-\nportantly, our implementation of interleaved ABC\nnotation adheres to syntax rules, ensuring compati-\nbility with existing ABC notation tools.\nThe patch-level encoder is expanded to 12 layers\nto better capture complex musical features, while\nthe char-level decoder remains at 3 layers, both\nwith a hidden size of 768. Each patch can hold\nup to 64 characters, and with a maximum of 512\npatches per input sequence, M3 can support a total\ninput of 32,768 characters. Longer sequences are\ntruncated by randomly selecting 512 patches from\nthe start, middle, or end, with equal probability.\nFor details on interleaved ABC notation and\nMTF, please see Appendix A and B, respectively.\n2https://github.com/mido/mido\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000028/uni00000051/uni0000000c\n/uni00000036/uni00000058/uni00000050/uni00000050/uni00000044/uni00000055/uni0000005c/uni00000003/uni0000000b/uni00000031/uni00000052/uni00000051/uni00000010/uni00000028/uni00000051/uni0000000c\n/uni00000037/uni0000004c/uni00000057/uni0000004f/uni00000048/uni00000037/uni00000044/uni0000004a/uni00000056\n/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000050/uni00000048/uni00000051/uni00000057/uni00000056/uni00000027/uni00000048/uni00000056/uni00000046/uni00000055/uni0000004c/uni00000053/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000028/uni00000051/uni00000056/uni00000048/uni00000050/uni00000045/uni0000004f/uni00000048/uni00000056/uni00000026/uni00000052/uni00000050/uni00000053/uni00000052/uni00000056/uni00000048/uni00000055\n/uni0000002f/uni0000005c/uni00000055/uni0000004c/uni00000046/uni00000056/uni0000002a/uni00000048/uni00000051/uni00000055/uni00000048/uni00000056\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013\n/uni00000014/uni00000011/uni00000015\n/uni00000014/uni00000011/uni00000017\n/uni00000014/uni00000011/uni00000019/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057\n/uni00000014/uni00000048/uni00000019\n/uni00000014/uni0000000f/uni00000018/uni00000016/uni00000013/uni0000000f/uni0000001c/uni0000001c/uni00000017/uni00000014/uni0000000f/uni00000018/uni00000016/uni00000013/uni0000000f/uni0000001c/uni0000001c/uni00000017/uni00000014/uni0000000f/uni00000018/uni00000016/uni00000013/uni0000000f/uni0000001b/uni0000001b/uni0000001c\n/uni00000014/uni0000000f/uni00000017/uni0000001a/uni0000001b/uni0000000f/uni0000001b/uni00000016/uni00000016\n/uni00000014/uni0000000f/uni00000015/uni00000015/uni00000017/uni0000000f/uni0000001a/uni0000001a/uni0000001a\n/uni0000001a/uni00000014/uni00000014/uni0000000f/uni0000001b/uni0000001b/uni00000013\n/uni00000018/uni00000018/uni00000013/uni0000000f/uni00000014/uni00000014/uni00000015\n/uni00000015/uni0000001b/uni00000016/uni0000000f/uni00000016/uni0000001b/uni00000013/uni00000015/uni00000018/uni00000019/uni0000000f/uni0000001c/uni0000001c/uni0000001a\n/uni00000014/uni00000017/uni00000014/uni0000000f/uni0000001b/uni00000018/uni00000013\nFigure 2: The distribution of counts for different text\ntypes within the LLM-processed pre-training dataset.\n3.4 Data Sources\nThe pre-training dataset for both M3 and CLaMP 2\ncomes from two sources: the Million MIDI Dataset\n(MMD) (Zeng et al., 2021) and the WebMusicText\n(WebMT) dataset (Wu et al., 2023a). They cover\nvarious music genres, such as popular and classical,\nfrom single- to multi-track compositions.\nThe MMD consists of over 1.5 million MIDI\nfiles, compiled by crawling a vast collection of mu-\nsic files and filtering out any malformed or blank\nentries. On the other hand, the WebMT dataset,\ncomprising 1.4 million music-text pairs, includes\nformats like MusicXML, LilyPond, and ABC nota-\ntion. These were standardized into ABC notation\nfollowing an initial conversion to MusicXML. To\nprevent information leakage, natural language ele-\nments were removed from the ABC files.\nTo unify the datasets, we convert MMD to ABC,\nWebMT to MIDI, and merge them to get 3 million\nABC-MIDI-text triplets. Admittedly, converting\nMMD to ABC may lead to the loss of performance\ndetails, and converting WebMT to MIDI may re-\nsult in the loss of certain score-related information.\nNevertheless, the key benefit is that it enriches data\ndiversity, thus enhancing the model’s ability to gen-\neralize across different musical modalities.\nHowever, variations in text quality pose signif-\nicant challenges. A substantial amount of non-\nmusical content in the text data diminishes the ef-\nfectiveness of pre-training by introducing noise that\ndetracts from relevant musical information. Fur-\nthermore, as Fig. 3 shows, the dataset has an imbal-\nanced language distribution (detected by the langid\nlibrary3): English accounts for two-thirds of the\ndata, while most languages contribute less than\n1MB. This imbalance restricts the model’s ability\nto effectively link music with various languages.\n3https://github.com/saffsd/langid.py\n438\n/uni00000048/uni00000051\n/uni00000047/uni00000048\n/uni0000004d/uni00000044\n/uni0000004c/uni00000057\n/uni00000049/uni00000055\n/uni0000004f/uni00000044\n/uni00000048/uni00000056\n/uni00000051/uni0000004f\n/uni00000053/uni00000057\n/uni0000005d/uni0000004b\n/uni0000004e/uni00000052\n/uni0000004c/uni00000047\n/uni00000045/uni00000055\n/uni00000056/uni00000059\n/uni00000047/uni00000044\n/uni00000046/uni00000056\n/uni00000053/uni0000004f\n/uni00000056/uni0000005a\n/uni00000051/uni00000052\n/uni00000049/uni0000004c\n/uni00000055/uni00000058\n/uni00000057/uni0000004f\n/uni00000048/uni00000058\n/uni0000004d/uni00000059\n/uni00000055/uni00000052\n/uni00000059/uni0000004c\n/uni00000050/uni00000056\n/uni0000004b/uni00000058\n/uni00000046/uni00000044\n/uni00000052/uni00000046\n/uni0000004f/uni00000052\n/uni00000048/uni00000052\n/uni00000044/uni00000051\n/uni00000048/uni0000004f\n/uni00000050/uni00000057\n/uni0000004f/uni00000045\n/uni00000048/uni00000057\n/uni00000051/uni00000051\n/uni00000057/uni00000055\n/uni0000004f/uni00000057\n/uni00000056/uni0000004f\n/uni0000004a/uni00000044\n/uni00000046/uni0000005c\n/uni0000004b/uni00000055\n/uni0000004a/uni0000004f\n/uni00000056/uni0000004e\n/uni00000057/uni0000004b\n/uni00000056/uni00000048\n/uni00000044/uni00000049\n/uni0000005b/uni0000004b\n/uni0000004c/uni00000056\n/uni00000058/uni0000004e\n/uni0000004f/uni00000059\n/uni0000004b/uni00000057\n/uni00000055/uni0000005a\n/uni00000051/uni00000045\n/uni0000005a/uni00000044\n/uni00000044/uni00000055\n/uni0000004b/uni00000048\n/uni00000054/uni00000058\n/uni00000050/uni0000004a\n/uni0000005d/uni00000058\n/uni00000059/uni00000052\n/uni0000004e/uni0000005c\n/uni00000045/uni0000004a\n/uni00000045/uni00000056\n/uni00000044/uni0000005d\n/uni00000056/uni00000054\n/uni00000049/uni00000052\n/uni00000045/uni00000048\n/uni0000004e/uni00000050\n/uni00000056/uni00000055\n/uni0000004e/uni0000004e\n/uni0000004e/uni00000058\n/uni00000050/uni00000051\n/uni00000044/uni00000050\n/uni0000004b/uni0000005c\n/uni0000004e/uni00000044\n/uni00000050/uni0000004e\n/uni00000049/uni00000044\n/uni00000045/uni00000051\n/uni00000058/uni0000004a\n/uni00000057/uni00000044\n/uni00000058/uni00000055\n/uni00000056/uni0000004c\n/uni00000052/uni00000055\n/uni00000050/uni0000004f\n/uni0000004b/uni0000004c\n/uni00000053/uni00000056\n/uni00000057/uni00000048\n/uni0000004a/uni00000058\n/uni00000050/uni00000055\n/uni00000051/uni00000048\n/uni0000004e/uni00000051\n/uni00000053/uni00000044\n/uni00000044/uni00000056\n/uni00000047/uni0000005d\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000018\n/uni00000014/uni00000013/uni0000001a\n/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000056/uni0000004c/uni0000005d/uni00000048/uni00000003/uni0000000b/uni0000004c/uni00000051/uni00000003/uni00000045/uni0000005c/uni00000057/uni00000048/uni00000056/uni0000000c\nFigure 3: The amount of data for 97 languages found in the original metadata, displayed in order of magnitude.\n/uni00000047/uni00000048\n/uni00000048/uni00000056\n/uni00000056/uni00000058\n/uni00000056/uni00000052\n/uni0000004b/uni00000044\n/uni00000057/uni0000004f\n/uni0000004d/uni00000059\n/uni00000055/uni00000052\n/uni0000004a/uni0000004f\n/uni0000004f/uni00000044\n/uni00000050/uni0000004a\n/uni00000057/uni00000055\n/uni00000047/uni00000044\n/uni00000058/uni0000005d\n/uni00000048/uni00000058\n/uni00000048/uni00000057\n/uni0000004f/uni00000057\n/uni0000004d/uni00000044\n/uni0000004b/uni00000058\n/uni00000048/uni00000052\n/uni00000053/uni00000057\n/uni00000049/uni0000004c\n/uni00000049/uni0000005c\n/uni00000056/uni00000055\n/uni0000005b/uni0000004b\n/uni00000044/uni00000049\n/uni0000005d/uni0000004b/uni00000010/uni0000004b/uni00000044/uni00000051/uni00000056\n/uni00000056/uni00000054\n/uni00000046/uni0000005c\n/uni00000052/uni00000050\n/uni0000004b/uni00000055\n/uni00000050/uni00000056\n/uni0000004c/uni00000057\n/uni00000053/uni0000004f\n/uni00000056/uni0000004e\n/uni00000046/uni00000044\n/uni0000004f/uni00000059\n/uni00000056/uni0000004f\n/uni00000056/uni0000005a\n/uni0000004c/uni00000056\n/uni00000056/uni00000059\n/uni00000045/uni00000055\n/uni00000049/uni00000055\n/uni0000004c/uni00000047\n/uni0000005d/uni0000004b/uni00000010/uni0000005c/uni00000058/uni00000048\n/uni0000004a/uni00000044\n/uni0000004a/uni00000047\n/uni00000050/uni0000005c/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni0000004e/uni00000052\n/uni00000045/uni00000056\n/uni00000059/uni0000004c\n/uni00000055/uni00000058\n/uni00000044/uni0000005d\n/uni00000057/uni00000048/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni00000058/uni00000055/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni0000005d/uni0000004b/uni00000010/uni0000004b/uni00000044/uni00000051/uni00000057\n/uni00000046/uni00000056\n/uni00000045/uni00000051/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni00000045/uni0000004a\n/uni00000050/uni0000004e\n/uni00000058/uni0000004e\n/uni00000044/uni00000055\n/uni00000051/uni00000052\n/uni00000051/uni0000004f\n/uni0000004b/uni0000004c/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni00000049/uni00000044\n/uni0000004e/uni00000058\n/uni00000057/uni00000044/uni00000010/uni0000004f/uni00000044/uni00000057/uni00000051\n/uni0000004e/uni0000005c\n/uni00000045/uni00000048\n/uni0000004e/uni0000004e\n/uni00000050/uni00000051\n/uni0000004b/uni00000048\n/uni00000058/uni0000004a\n/uni00000057/uni0000004b\n/uni00000053/uni00000056\n/uni00000058/uni00000055\n/uni0000005c/uni0000004c\n/uni00000056/uni00000047\n/uni00000051/uni00000048\n/uni00000048/uni0000004f\n/uni0000004b/uni0000004c\n/uni00000056/uni00000044\n/uni00000050/uni00000055\n/uni00000045/uni00000051\n/uni00000044/uni00000056\n/uni0000004e/uni00000050\n/uni0000004a/uni00000058\n/uni00000053/uni00000044\n/uni00000044/uni00000050\n/uni00000057/uni00000044\n/uni0000004f/uni00000052\n/uni00000057/uni00000048\n/uni00000056/uni0000004c\n/uni0000004e/uni00000051\n/uni00000050/uni0000004f\n/uni0000004b/uni0000005c\n/uni0000004e/uni00000044\n/uni00000050/uni0000005c\n/uni00000052/uni00000055\n/uni00000013\n/uni00000018/uni00000013/uni00000013/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000003/uni00000028/uni00000051/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000056\nFigure 4: Count of text entries for 100 non-English languages generated by GPT-4.\n3.5 LLM-Based Metadata Processing\nTo improve text quality and mitigate imbalanced\nlanguage distribution, we employed GPT-4 (Ope-\nnAI, 2023) to filter and enrich the text data. The\nprompt given to GPT-4 consisted of a system in-\nstruction along with two examples illustrating the\ndesired outputs, which enhanced its understanding\nof our requirements. GPT-4 was tasked with identi-\nfying relevant music-related elements in each entry,\nand subsequently generating concise summaries in\nmultiple languages based on these elements.\nEntries were excluded for lacking specific musi-\ncal details, containing vague comments like \"this is\na good song,\" or having no significant relation to\nthe music. For valid entries, GPT-4 generated con-\ncise summaries in English and a randomly selected\nnon-English language from the 100 languages to-\nkenizable by XLM-R. However, some responses\nin low-resource languages did not conform to the\nexpected format, resulting in fewer entries for these\nlanguages. Nevertheless, as shown in Fig. 4, GPT-4\nsignificantly enhanced language balance, resulting\nin a total of 1.6 million ABC-MIDI-text triplets.\nAs our dataset is derived from two sources, dupli-\ncate entries may occur. To resolve this, we merged\ntriplets with identical components, resulting in 1.5\nmillion unique triplets—approximately 1.3 million\nfrom WebMT and 0.2 million from MMD.\nGPT-4 cleaned the pre-training dataset and en-\nriched it with multilingual descriptions in 101\nlanguages. This significantly enhanced CLaMP\n2’s multilingual MIR capabilities. Details on the\nprompt and text examples are in Appendix C.\n4 Experiments\n4.1 Settings\nWe evaluated the proposed models, M3 and\nCLaMP 2, on music classification and semantic\nsearch tasks. Training both models together on\n8 NVIDIA H800 GPUs took approximately 800\nhours. We split the data, allocating 99% for train-\ning and 1% for validation. The models were\ntrained for up to 100 epochs. We adopted mixed-\nprecision acceleration (Micikevicius et al., 2018) to\nenhance training efficiency. The AdamW optimizer\n(Loshchilov and Hutter, 2019) was utilized, along\nwith a 1,000-step warm-up (Goyal et al., 2017).\nFor M3, the batch size was set to 128, and the\nlearning rate was 1e-4. For CLaMP 2, initialized\nfrom M3’s patch-level encoder and XLM-R, the\nbatch size was set to 1024, the learning rate was\n5e-5, and the logit scale was set to 1.\nThe ablation study for M3 and CLaMP 2 in-\ncluded several variants. For M3, three configura-\ntions were examined to assess the impact of mix-\ning musical modalities on performance: M3-ABC,\ntrained only on ABC data; M3-MIDI, trained only\non MIDI data; and full M3, trained on both. For\nCLaMP 2, five ablations were carried out to under-\nstand the contribution of different text data sources:\nCLaMP 2 (w/o en), excluding LLM-generated En-\nglish data; CLaMP 2 (w/o nen), excluding LLM-\ngenerated non-English data; CLaMP 2 (w/o meta),\nexcluding the original raw text data; CLaMP 2 (w/o\nLLM), excluding all LLM-generated data; and the\nfull CLaMP 2 setup, using all available text data.\n439\nTable 1: Classification performance for ABC notation and MIDI was assessed across three datasets: WikiMT (1,010\npieces, 8 genres), VGMIDI (204 pieces, 4 emotions), and Pianist8 (411 pieces, 8 composers). Underlined values\nindicate the top M3 model, while bold values denote the overall best performance among all models.\nModel Modality WikiMT VGMIDI Pianist8\nF1-macro Accuracy F1-macro Accuracy F1-macro Accuracy\nM3-MIDI MIDI 0.2586 0.4158 0.4700 0.5854 0.8683 0.8674\nM3-ABC ABC 0.2416 0.4010 0.4955 0.6098 0.7339 0.7470\nM3 MIDI 0.2621 0.4257 0.5399 0.6098 0.9199 0.9157\nM3 ABC 0.2349 0.4010 0.6016 0.6341 0.7395 0.7590\nMusicBERT MIDI 0.1746 0.3219 0.5127 0.5850 0.8379 0.8413\nCLaMP ABC 0.3452 0.4267 0.6453 0.6866 0.7067 0.7152\nCLaMP 2 MIDI 0.2898 0.4455 0.5246 0.6585 0.8927 0.8916\nCLaMP 2 ABC 0.3990 0.4653 0.7449 0.8049 0.8025 0.8072\n4.2 Music Classification Across Modalities\nThis evaluation assesses the classification capabili-\nties of various models across three datasets, each\nhighlighting a specific aspect of music.\n• WikiMT (Wu et al., 2023a): It contains 1,010\nlead sheets in ABC notation from Wikifonia4,\nlabeled with 8 genre classes according to the\nrelevant Wikipedia entries.\n• VGMIDI (Ferreira and Whitehead, 2019): It\ncontains 204 MIDI scores from video game\nsoundtracks, annotated with 4 emotion classes\nbased on valence and arousal levels.\n• Pianist8 (Chou et al., 2021): It includes 411\npiano performances automatically transcribed\nfrom audio to performance MIDI (Kong et al.,\n2021) and labeled with 8 composer styles.\nWe evaluated our model against state-of-the-art\nbaselines in symbolic music understanding.\n• CLaMP (Wu et al., 2023a): A cross-modal\nMIR model designed to connect text and sheet\nmusic. It is pre-trained on WebMT using bar\npatching and masked music modeling.\n• MusicBERT (Zeng et al., 2021): A self-\nsupervised MIR model for representation\nlearning, pre-trained on MMD through Oc-\ntupleMIDI encoding and bar-level masking.\nIn this evaluation, we utilized only the represen-\ntations from the music encoder. Given that the text\nencoder was not involved, the multilingual capabil-\nities were not investigated. As a result, CLaMP 2\nunder evaluation included all available text data.\n4http://www.synthzone.com/files/Wikifonia/\nWikifonia.zip\nNotably, we employed a linear classifier on the\ntop layer of each model to assess the quality of\nmusical representations. We evaluated each bench-\nmark in both MIDI and ABC formats to analyze\nhow the models utilize information from different\nmusical modalities.\nThe results in Table 1 indicate that mixing musi-\ncal modalities significantly benefits M3. When\ntrained with both ABC and MIDI, M3 outper-\nformed its single-modality counterparts on all\nbenchmarks. This implies that training with ABC\nand MIDI together improves its feature extraction\ncapability for both modalities.\nDespite being pre-trained on only 0.2 million\nnative MIDI pieces, M3 consistently outperformed\nMusicBERT in MIDI classification tasks. This per-\nformance advantage is attributed to our proposed\nMTF, which preserves all MIDI information during\ntext conversion. In contrast, MusicBERT’s Octu-\npleMIDI encoding suffers from information loss,\nwhich weakens its performance.\nOnce aligned with text data, CLaMP 2 gener-\nally outperforms M3 across benchmarks, though\nperformance varies by modalities. In ABC nota-\ntion, CLaMP 2 achieves top accuracies of 0.4653\nand 0.8049 in WikiMT and VGMIDI, respectively,\nboth of which emphasize score information. How-\never, in Pianist8, which focuses on performance\ndetails, CLaMP 2 excels in MIDI with an accuracy\nof 0.8916, a significant improvement over the orig-\ninal CLaMP. Still, this falls slightly below M3’s\n0.9157, likely due to limited performance MIDI\ndata in the pre-training dataset. This shortage may\nhave caused a slight decline after contrastive learn-\ning. Despite this, CLaMP 2 remains highly effec-\ntive across musical modalities, showing its strong\npotential for music classification.\n440\nTable 2: The semantic search performance of CLaMP 2 across the WikiMT and MidiCaps benchmarks under diverse\nexperimental settings. Both datasets contain texts exclusively in English.\nSetting WikiMT (1,010 ABC-text pairs) MidiCaps (1,010 MIDI-text pairs)\nMRR HR@1 HR@10 HR@100 MRR HR@1 HR@10 HR@100\nCLaMP 2 0.3438 0.2705 0.4870 0.7956 0.2695 0.1653 0.4782 0.8634\nCLaMP 2 (w/o en) 0.3234 0.2455 0.4800 0.7846 0.2708 0.1723 0.4752 0.8436\nCLaMP 2 (w/o nen) 0.3359 0.2615 0.4880 0.7735 0.2490 0.1574 0.4158 0.8297\nCLaMP 2 (w/o meta) 0.2856 0.2104 0.4218 0.7585 0.1940 0.1050 0.3713 0.7901\nCLaMP 2 (w/o LLM) 0.2797 0.2094 0.4068 0.7375 0.2772 0.1762 0.4822 0.8614\nCLaMP 0.2561 0.1931 0.3693 0.7020 0.1236 0.0666 0.2416 0.6412\n4.3 Semantic Search on Native English Data\nBenchmarks in symbolic MIR are relatively scarce.\nTo the best of our knowledge, WikiMT (Wu et al.,\n2023a) and MidiCaps (Melechovský et al., 2024)\nare the only two publicly available music-text\ndatasets for symbolic music. WikiMT pairs 1,010\nABC notation pieces with Wikipedia text, focus-\ning on cultural and historical context. MidiCaps,\nbuilt on the Lakh MIDI dataset (Raffel, 2016), in-\ncludes 168,407 pairs with descriptions of musical\nfeatures like tempo and chord progression. These\ndatasets have different focuses: WikiMT empha-\nsizes cultural-context understanding, while Midi-\nCaps targets musical feature analysis.\nAs the pre-training data includes the Lakh MIDI\ndataset (a subset of MMD), we took precautions\nto prevent data leakage. To this end, we randomly\nselected 1,010 pieces from the MidiCaps validation\nset to match the size of WikiMT, which contains\nonly non-training data. CLaMP 2 uses the original\nformats for testing on these benchmarks. Because\nthe original CLaMP does not support MIDI, we\nconverted the MidiCaps data into ABC notation for\nits evaluation.\nTable 2 shows semantic search results on the\nWikiMT and MidiCaps benchmarks, using Mean\nReciprocal Rank (MRR) and Hit Rate at Top K\n(HR@K) to assess model performance in retrieving\nand ranking relevant music-text pairs.\nIn the WikiMT benchmark, a clear trend is\nobserved: any CLaMP 2 variant using LLM-\ngenerated text, whether in English or non-English,\noutperforms CLaMP 2 (w/o LLM). For example,\nCLaMP 2 achieves an MRR of 0.3438. However,\nwhen excluding LLM-generated text in CLaMP 2\n(w/o LLM), the MRR drops significantly to 0.2797.\nThis indicates that LLM-generated text greatly en-\nhances the CLaMP 2’s ability to capture and convey\ncultural information.\nSpanish\n70.55\nFrench\n68.80\nRussian\n56.13\nChinese\n43.81\nArabic\n55.43\nAmharic\n44.12\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35MRR\nCLaMP 2\nw/o en\nw/o nen\nw/o meta\nw/o LLM\n(a) MRR scores on the WikiMT benchmark.\nSpanish\n58.07\nFrench\n56.99\nRussian\n40.38\nChinese\n28.84\nArabic\n47.34\nAmharic\n36.14\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25MRR\nCLaMP 2\nw/o en\nw/o nen\nw/o meta\nw/o LLM\n(b) MRR scores on the MidiCaps benchmark.\nFigure 5: MRR scores across six non-English languages\nfor (a) WikiMT and (b) MidiCaps benchmarks. BLEU\nscores below each language provide additional context\non translation quality.\nIn the MidiCaps benchmark, CLaMP 2 achieves\nan MRR of 0.2695, demonstrating strong perfor-\nmance. Notably, all CLaMP 2 variants significantly\noutperform CLaMP. This improvement arises from\ntheir native support for MIDI data, enabling a bet-\nter capture of performance details. In contrast to\nthe WikiMT results, excluding LLM-generated text\ndoes not harm performance, as CLaMP 2 (w/o\nLLM) achieves the highest MRR of 0.2772. This\nsuggests that LLM-generated text may not enhance\nthe understanding of musical features.\n441\n4.4 Semantic Search Across Multilingual Data\nTo address the lack of multilingual music-text\nbenchmarks, we translated English texts from\nWikiMT and MidiCaps into six languages: Spanish,\nFrench, Russian, Chinese, Arabic, and Amharic.\nAmong these languages, Amharic is an extremely\nlow-resource language with limited pre-training\ndata—less than 1GB in XLM-R and only 17KB in\nCLaMP 2. We used SeamlessM4T (Meta, 2023b)\nfor its broad translation support, allowing evalu-\nation without native multilingual datasets. Given\nthat translation quality directly affects retrieval ef-\nfectiveness, we used BLEU scores5 to assess the\nsimilarity between back-translations and original\ntexts, serving as an indicator of translation quality.\nThis evaluation lacks baselines, as no comparable\nmodels support multilingual symbolic MIR.\nCLaMP 2’s multilingual retrieval results are\npresented in Fig. 5. Generally, removing LLM-\ngenerated English texts (w/o en) slightly impacts\nperformance. Although in English, they improve\noverall text quality by reducing inconsistencies and\nirrelevancies, thereby enhancing multilingual re-\ntrieval performance. In contrast, excluding LLM-\ngenerated non-English texts (w/o nen) notably hin-\nders retrieval for all languages in both benchmarks,\nespecially for low-resource languages like Amharic.\nComparing CLaMP 2 (w/o en) with CLaMP 2 (w/o\nLLM) further confirms the important role of LLM-\ngenerated non-English texts in enhancing multilin-\ngual retrieval performance.\nNotably, CLaMP 2 (w/o LLM) records the low-\nest MRR across all languages in both benchmarks,\nindicating poor multilingual performance when re-\nlying solely on the original text data. However, ex-\ncluding the original text data (w/o meta) results in a\nsignificant drop in performance. This indicates that\nCLaMP 2 can effectively extract authentic musical\nconcepts from English-centric text data, enabling\nit to transcend language barriers and improve re-\ntrieval across different languages and cultures.\nIn conclusion, the evaluation of CLaMP 2 on\nWikiMT and MidiCaps reveals that LLM-generated\ntexts, particularly non-English texts, significantly\nenhance multilingual semantic search. However,\nrelying solely on them is insufficient, as the original\ndata provides authentic details that LLM-generated\ndata may lack. Together, they enable CLaMP 2 to\nperform better across languages by learning a more\ncomprehensive representation of music semantics.\n5https://github.com/mjpost/sacrebleu\n5 Conclusions\nCLaMP 2 makes substantial progress in cross-\nmodal MIR by integrating multilingual text and\nmultimodal music data via contrastive learning.\nLeveraging GPT-4 to refine the multilingual cor-\npus, it overcomes the limitations of existing models\nthat are exclusively trained on English music-text\ndatasets. This facilitates more precise alignment\nbetween music and text across 101 languages.\nExperimental results demonstrate that CLaMP 2\nachieves state-of-the-art performance across a vari-\nety of MIR tasks. In music classification tasks, the\nM3 model, trained on both ABC and MIDI data,\ndemonstrates improved performance and consis-\ntently outperforms counterparts trained on a single\nmodality. Building on M3, CLaMP 2 achieves su-\nperior performance across diverse benchmarks and\nmodalities. Notably, the incorporation of LLM-\ngenerated text data significantly enhances mul-\ntilingual semantic search. This enhancement is\nachieved by reducing textual noise and balancing\nlanguage distribution, which is particularly benefi-\ncial for low-resource languages.\nCLaMP 2 establishes a new multilingual MIR\nstandard, enabling users worldwide to access a\ndiverse array of musical content across 101 lan-\nguages. Future developments may build on CLaMP\n2 to connect with audio and visual modalities, fa-\ncilitating a more comprehensive and culturally rich\nexperience at a global scale.\n6 Excluded Approaches\nIn CLaMP 2, several experimental strategies were\ntested, yet failed to achieve expected improvements\nand were thus excluded from the final model. It\nshould be noted that these failed attempts are de-\nrived from our practice and may not be generalized.\nThe integration of discretized audio tokens (Dé-\nfossez et al., 2023) failed to match previous au-\ndio models’ performance and was removed. In-\nspired by MidiCaps (Melechovský et al., 2024)\nand MuseCoco (Lu et al., 2023), we attempted to\ninclude musical attributes in the text data. How-\never, this inclusion negatively impacted perfor-\nmance. Additionally, extending the patch mask-\ning pre-training strategy to contrastive learning did\nnot enhance the robustness of CLaMP 2. L2 nor-\nmalization caused convergence problems and was\nalso excluded. Lastly, a learnable logit scale led\nto over-scaling and degraded representations, so a\nfixed logit scale of 1 was used for better stability.\n442\n7 Limitations\nAlthough CLaMP 2 has made progress, it still has\ncertain limitations.\nIn CLaMP 2, the contrastive learning framework\nprimarily extracts global semantic features, result-\ning in a loss of fine-grained temporal information.\nConsequently, tasks that rely on sequential or time-\nrelated details cannot be effectively executed.\nIn addition, the absence of multilingual music-\ntext benchmarks complicates the evaluation of\nCLaMP 2’s performance in non-English languages.\nTo address this, an existing machine translation\nmodel (Meta, 2023b) was used to translate English\nbenchmarks into other languages. However, ma-\nchine translation presents its own challenges. For\ninstance, the BLEU score for MidiCaps translations\nin Chinese is only 28.84, indicating poor transla-\ntion quality and significantly hindering retrieval\nperformance. Notably, Arabic—despite having far\nless training data than Chinese in both XLM-R\nand CLaMP 2—achieves a higher MRR, with a\nBLEU score of 47.34. This suggests that transla-\ntion quality has a significant impact on retrieval\nperformance, outweighing the influence of training\ndata size. Without native, high-quality benchmarks\nfor non-English languages, it remains unclear how\nwell CLaMP 2 will perform in real-world multilin-\ngual retrieval tasks.\nCore Contributors\nShangda Wu1, shangda@mail.ccom.edu.cn\nYashan Wang1, alexis_wang@mail.ccom.edu.cn\nRuibin Yuan2, ryuanab@connect.ust.hk\nContributors\nZhancheng Guo1\nXu Tan3\nGe Zhang2\nMonan Zhou1\nJing Chen4\nXuefeng Mu4\nYuejie Gao4\nYuanliang Dong1\nJiafeng Liu1\nXiaobing Li1\nFeng Yu1\nCorrespondence\nMaosong Sun1, sms@tsinghua.edu.cn\nAffiliations\n1Central Conservatory of Music, China\n2Multimodal Art Projection Research Community\n3Microsoft Research Asia\n4NetEase Cloud Music, China\nAcknowlegdements\nThis work was supported by the following funding\nsources: Special Program of National Natural Sci-\nence Foundation of China (Grant No. T2341003),\nAdvanced Discipline Construction Project of Bei-\njing Universities, Major Program of National So-\ncial Science Fund of China (Grant No. 21ZD19),\nand the National Culture and Tourism Technologi-\ncal Innovation Engineering Project (Research and\nApplication of 3D Music).\nIn addition, we would like to express our grat-\nitude for the use of icons from flaticon 6 in Fig. 1\nand Fig. 8.\nReferences\nAndrea Agostinelli, Timo I. Denk, Zalán Borsos,\nJesse H. Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco\nTagliasacchi, Matthew Sharifi, Neil Zeghidour, and\nChristian Havnø Frank. 2023. Musiclm: Generating\nmusic from text. CoRR, abs/2301.11325.\nAlibaba. 2024. Qwen2 technical report. CoRR,\nabs/2407.10671.\nAnthropic. 2024. The claude 3 model family: Opus,\nsonnet, haiku. Preprint.\nYi-Hui Chou, I-Chun Chen, Chin-Jui Chang, Joann\nChing, and Yi-Hsuan Yang. 2021. Midibert-piano:\nLarge-scale pre-training for symbolic music under-\nstanding. CoRR, abs/2107.05223.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020, pages 8440–8451. Association\nfor Computational Linguistics.\nAlexandre Défossez, Jade Copet, Gabriel Synnaeve, and\nYossi Adi. 2023. High fidelity neural audio compres-\nsion. Trans. Mach. Learn. Res., 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\n6https://www.flaticon.com/\n443\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nSeungheon Doh, Keunwoo Choi, Jongpil Lee, and Juhan\nNam. 2023a. Lp-musiccaps: Llm-based pseudo mu-\nsic captioning. In Proceedings of the 24th Interna-\ntional Society for Music Information Retrieval Con-\nference, ISMIR 2023, Milan, Italy, November 5-9,\n2023, pages 409–416.\nSeungheon Doh, Minhee Lee, Dasaem Jeong, and\nJuhan Nam. 2024. Enriching music descriptions with\nA finetuned-llm and metadata for text-to-music re-\ntrieval. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing, ICASSP 2024,\nSeoul, Republic of Korea, April 14-19, 2024, pages\n826–830. IEEE.\nSeungheon Doh, Minz Won, Keunwoo Choi, and Juhan\nNam. 2023b. Toward universal text-to-music re-\ntrieval. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing ICASSP 2023,\nRhodes Island, Greece, June 4-10, 2023, pages 1–5.\nIEEE.\nBenjamin Elizalde, Soham Deshmukh, Mahmoud Al\nIsmail, and Huaming Wang. 2023. CLAP learning\naudio concepts from natural language supervision. In\nIEEE International Conference on Acoustics, Speech\nand Signal Processing ICASSP 2023, Rhodes Island,\nGreece, June 4-10, 2023, pages 1–5. IEEE.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multi-\nlingual machine translation. J. Mach. Learn. Res. ,\n22:107:1–107:48.\nLucas Ferreira and Jim Whitehead. 2019. Learning\nto generate music with sentiment. In Proceedings\nof the 20th International Society for Music Informa-\ntion Retrieval Conference, ISMIR 2019, Delft, The\nNetherlands, November 4-8, 2019, pages 384–390.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023. Imagebind one embedding\nspace to bind them all. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR\n2023, Vancouver, BC, Canada, June 17-24, 2023 ,\npages 15180–15190. IEEE.\nGoogle. 2024. Gemini 1.5: Unlocking multimodal\nunderstanding across millions of tokens of context.\nCoRR, abs/2403.05530.\nPriya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No-\nordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-\ncurate, large minibatch SGD: training imagenet in 1\nhour. CoRR, abs/1706.02677.\nWen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-\nHsuan Yang. 2021. Compound word transformer:\nLearning to compose full-song music over dynamic\ndirected hypergraphs. In Thirty-Fifth AAAI Confer-\nence on Artificial Intelligence, AAAI 2021, Thirty-\nThird Conference on Innovative Applications of Arti-\nficial Intelligence, IAAI 2021, The Eleventh Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, EAAI 2021, Virtual Event, February 2-9, 2021,\npages 178–186. AAAI Press.\nQingqing Huang, Aren Jansen, Joonseok Lee, Ravi\nGanti, Judith Yue Li, and Daniel P. W. Ellis. 2022.\nMulan: A joint embedding of music audio and natural\nlanguage. In Proceedings of the 23rd International\nSociety for Music Information Retrieval Conference,\nISMIR 2022, Bengaluru, India, December 4-8, 2022,\npages 559–566.\nQingqing Huang, Daniel S. Park, Tao Wang, Timo I.\nDenk, Andy Ly, Nanxin Chen, Zhengdong Zhang,\nZhishuai Zhang, Jiahui Yu, Christian Havnø Frank,\nJesse H. Engel, Quoc V . Le, William Chan, and\nWei Han. 2023. Noise2music: Text-conditioned\nmusic generation with diffusion models. CoRR,\nabs/2302.03917.\nYu-Siang Huang and Yi-Hsuan Yang. 2020. Pop music\ntransformer: Beat-based modeling and generation of\nexpressive pop piano compositions. In MM ’20: The\n28th ACM International Conference on Multimedia,\nVirtual Event / Seattle, WA, USA, October 12-16,\n2020, pages 1180–1188. ACM.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomás Mikolov. 2017. Bag of tricks for efficient text\nclassification. In Proceedings of the 15th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, EACL 2017, Valencia, Spain,\nApril 3-7, 2017, Volume 2: Short Papers, pages 427–\n431. Association for Computational Linguistics.\nQiuqiang Kong, Bochen Li, Xuchen Song, Yuan Wan,\nand Yuxuan Wang. 2021. High-resolution piano tran-\nscription with pedals by regressing onset and offset\ntimes. IEEE ACM Trans. Audio Speech Lang. Pro-\ncess., 29:3707–3717.\nLuca A. Lanzendörfer, Florian Grötschla, Emil Funke,\nand Roger Wattenhofer. 2023. DISCO-10M: A large-\nscale music dataset. In Advances in Neural Infor-\nmation Processing Systems 36: Annual Conference\non Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10\n- 16, 2023.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\n444\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Trans. Assoc.\nComput. Linguistics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nPeiling Lu, Xin Xu, Chenfei Kang, Botao Yu, Chengyi\nXing, Xu Tan, and Jiang Bian. 2023. Musec-\noco: Generating symbolic music from text. CoRR,\nabs/2306.00110.\nIlaria Manco, Benno Weck, Seungheon Doh, Minz\nWon, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu,\nKe Chen, Philip Tovstogan, Emmanouil Benetos,\nElio Quinton, György Fazekas, and Juhan Nam. 2023.\nThe song describer dataset: a corpus of audio cap-\ntions for music-and-language evaluation. In Machine\nLearning for Audio Workshop at NeurIPS 2023.\nJan Melechovský, Abhinaba Roy, and Dorien Herre-\nmans. 2024. Midicaps - A large-scale MIDI dataset\nwith text captions. CoRR, abs/2406.02255.\nMeta. 2023a. Llama 2: Open foundation and fine-tuned\nchat models. CoRR, abs/2307.09288.\nMeta. 2023b. Seamlessm4t-massively multilin-\ngual & multimodal machine translation. CoRR,\nabs/2308.11596.\nMeta. 2024. The llama 3 herd of models. Preprint,\narXiv:2407.21783.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory F. Diamos, Erich Elsen, David García,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed pre-\ncision training. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nMistral. 2024. Mixtral of experts. CoRR,\nabs/2401.04088.\nSageev Oore, Ian Simon, Sander Dieleman, Douglas\nEck, and Karen Simonyan. 2020. This time with\nfeeling: learning expressive musical performance.\nNeural Comput. Appl., 32(4):955–967.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In Advances in Neural\nInformation Processing Systems 35: Annual Confer-\nence on Neural Information Processing Systems 2022,\nNeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022.\nXingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou,\nKa Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min,\nXueling Liu, Tianyu Zhang, Xinrun Du, Shuyue\nGuo, Yiming Liang, Yizhi Li, Shangda Wu, Junt-\ning Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han,\nWei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue,\nChenghua Lin, Xu Tan, Stephen W. Huang, Wenhu\nChen, Jie Fu, and Ge Zhang. 2024. Mupt: A genera-\ntive symbolic music pretrained transformer. CoRR,\nabs/2404.06393.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8748–8763.\nPMLR.\nColin Raffel. 2016. Learning-Based Methods for\nComparing Sequences, with Applications to Audio-\nto-MIDI Alignment and Matching . Ph.D. thesis,\nColumbia University, USA.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nKihyuk Sohn. 2016. Improved deep metric learning\nwith multi-class n-pair loss objective. In Advances in\nNeural Information Processing Systems 29: Annual\nConference on Neural Information Processing Sys-\ntems 2016, December 5-10, 2016, Barcelona, Spain,\npages 1849–1857.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. CoRR, abs/1807.03748.\n445\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nShangda Wu, Dingyao Yu, Xu Tan, and Maosong Sun.\n2023a. Clamp: Contrastive language-music pre-\ntraining for cross-modal symbolic music information\nretrieval. In Proceedings of the 24th International\nSociety for Music Information Retrieval Conference,\nISMIR 2023, Milan, Italy, November 5-9, 2023, pages\n157–165.\nYusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov. 2023b.\nLarge-scale contrastive language-audio pretraining\nwith feature fusion and keyword-to-caption augmen-\ntation. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing ICASSP 2023,\nRhodes Island, Greece, June 4-10, 2023, pages 1–5.\nIEEE.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 483–498. Association\nfor Computational Linguistics.\nMingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao\nQin, and Tie-Yan Liu. 2021. Musicbert: Symbolic\nmusic understanding with large-scale pre-training. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 791–800. Association for Computational\nLinguistics.\nZhipu. 2024. Chatglm: A family of large language\nmodels from GLM-130B to GLM-4 all tools. CoRR,\nabs/2406.12793.\n446\n%%score { 1 | 2 }\nL:1/8\nQ:1/4=120\nM:3/4\nK:G\nV:1 treble nm=\"Piano\" snm=\"Pno.\"\nV:2 bass \nV:1\n!mf!\"^Allegro\" d2 (GA Bc | d2) .G2 .G2 |]\nV:2\n [G,B,D]4 A,2 | B,6 |]\n%%score { 1 | 2 }\nL:1/8\nQ:1/4=120\nM:3/4\nK:G\nV:1 treble nm=\"Piano\" snm=\"Pno.\"\nV:2 bass \n[V:1]!mf!\"^Allegro\" d2 (GA Bc|[V:2][G,B,D]4 A,2|\n[V:1]d2) .G2 .G2|][V:2]B,6|]\nSheet Music\nStandard\nABC Notation\nInterleaved\nABC Notation\nFigure 6: Comparison between standard and interleaved\nABC notation in multi-track piano sheet music. In-\nterleaved ABC notation merges voices and tags them\nin-line for a compact and synchronized representation.\nColors mark patch boundaries for M3 model encoding.\nA Interleaved ABC Notation\nStandard ABC notation encodes each voice sepa-\nrately, which often results in corresponding bars\nbeing spaced far apart. This separation makes it\ndifficult for models to accurately understand the\ninteractions between voices in sheet music that are\nmeant to align musically.\nIn contrast, interleaved ABC notation effectively\naligns multi-track music by integrating multiple\nvoices of the same bar into a single line, ensuring\nthat all parts remain synchronized. As illustrated in\nFig. 6, this format combines voices in-line and tags\neach bar with its corresponding voice (e.g., [V:1]\nfor treble and [V:2] for bass). By directly aligning\nrelated bars, interleaved ABC notation enhances\nthe model’s understanding of how different voices\ninteract within the same bar.\nTo facilitate this reformatting process, we de-\nveloped a script for reversible and lossless conver-\nsion between standard and interleaved notations,\nensuring accuracy without any loss of information.\nThis simplification of multi-track music modeling\nmaintains compatibility with standard ABC syntax,\nallowing for effective processing in existing tools.\nticks_per_beat 480\ntime_signature 3 4 24 8 0\nkey_signature G 0\nset_tempo 500000 0\ncontrol_change 0 0 121 0\nprogram_change 0 0 0\ncontrol_change 0 0 7 100\\t0 0 10 64\\t0 0 91 0\\t0 0 93 0\nmidi_port 0 0\nnote_on 0 0 74 80\nkey_signature G 0\nmidi_port 0 0\nnote_on 0 0 55 80\\t0 0 59 80\\t0 0 62 80\\t455 0 74 0\\t25 0 67 80\nnote_on 239 0 67 0\\t1 0 69 80\\t191 0 55 0\\t0 0 59 0\\t0 0 62 0\nnote_on 48 0 69 0\\t1 0 71 80\\t0 0 57 80\\t239 0 71 0\\t1 0 72 80\nnote_on 215 0 57 0\\t24 0 72 0\\t1 0 74 80\\t0 0 59 80\\t455 0 74 0\nnote_on 25 0 67 80\\t239 0 67 0\\t0 67 80\\t239 0 67 0\\t168 0 59 0\nend_of_track 1\nMIDI Piano-roll\nVisualization\nM3-encoded\nMIDI Text Format\nFigure 7: Illustration of MIDI Text Format (MTF) en-\ncoded by M3. In this format, MIDI messages are treated\nas patches for processing. Consecutive messages of the\nsame type are merged within a patch, with colors indi-\ncating the boundaries between patches.\nB MIDI Text Format\nThe MIDI Text Format (MTF) provides a struc-\ntured, textual representation of MIDI data that pre-\nserves all original information without loss. Each\nMIDI message is accurately represented, allowing\nfull reconstruction from MTF to ensure no musical\nnuances are overlooked during conversion.\nTo generate MTF, the mido library reads raw\nMIDI messages from MIDI files. As shown in\nTable 3, the output retains all necessary informa-\ntion but can be lengthy and redundant. To simplify\nthis, we streamline the representation by directly\nreading parameter values in a fixed order and sepa-\nrating them with spaces. For instance, the raw time\nsignature message, which includes multiple param-\neters—numerator, denominator, clocks per click,\nnotated 32nd notes per beat, and time—is repre-\nsented in MTF as time_signature 3 4 24 8 0,\nas illustrated in Table 4. Other messages, includ-\ning control changes and note events, are similarly\ncompacted while preserving key musical details.\nThis approach improves computational perfor-\nmance and maintains precise control of timing\nand dynamics. Furthermore, when processed by\nM3, consecutive messages of the same type that\nfit within a single patch (under 64 characters) are\ncombined into one line, with only the first mes-\nsage containing the type information. This further\nsimplifies representation and improves processing\nefficiency, as shown in Fig. 7.\n447\nTable 3: Raw MIDI messages extracted from a MIDI\nfile using the mido library.\nMetaMessage (’ time_signature ’,\nnumerator =3 ,\ndenominator =4 ,\nclocks_per_click =24 ,\nnotated_32nd_notes_per_beat =8 ,\ntime =0)\nMetaMessage (’ key_signature ’, key =’G’, time =0)\nMetaMessage (’ set_tempo ’, tempo =500000 , time =0)\ncontrol_change channel =0 control =121 value =0 time =0\nprogram_change channel =0 program =0 time =0\ncontrol_change channel =0 control =7 value =100 time =0\ncontrol_change channel =0 control =10 value =64 time =0\ncontrol_change channel =0 control =91 value =0 time =0\ncontrol_change channel =0 control =93 value =0 time =0\nMetaMessage (’ midi_port ’, port =0 , time =0)\nnote_on channel =0 note =74 velocity =80 time =0\nMetaMessage (’ key_signature ’, key =’G’, time =0)\nMetaMessage (’ midi_port ’, port =0 , time =0)\nnote_on channel =0 note =55 velocity =80 time =0\nnote_on channel =0 note =59 velocity =80 time =0\nnote_on channel =0 note =62 velocity =80 time =0\nnote_on channel =0 note =74 velocity =0 time =455\nnote_on channel =0 note =67 velocity =80 time =25\nnote_on channel =0 note =67 velocity =0 time =239\nnote_on channel =0 note =69 velocity =80 time =1\nnote_on channel =0 note =55 velocity =0 time =191\nnote_on channel =0 note =59 velocity =0 time =0\nnote_on channel =0 note =62 velocity =0 time =0\nnote_on channel =0 note =69 velocity =0 time =48\nnote_on channel =0 note =71 velocity =80 time =1\nnote_on channel =0 note =57 velocity =80 time =0\nnote_on channel =0 note =71 velocity =0 time =239\nnote_on channel =0 note =72 velocity =80 time =1\nnote_on channel =0 note =57 velocity =0 time =215\nnote_on channel =0 note =72 velocity =0 time =24\nnote_on channel =0 note =74 velocity =80 time =1\nnote_on channel =0 note =59 velocity =80 time =0\nnote_on channel =0 note =74 velocity =0 time =455\nnote_on channel =0 note =67 velocity =80 time =25\nnote_on channel =0 note =67 velocity =0 time =239\nnote_on channel =0 note =67 velocity =80 time =241\nnote_on channel =0 note =67 velocity =0 time =239\nnote_on channel =0 note =59 velocity =0 time =168\nMetaMessage (’ end_of_track ’, time =1)\nTable 4: MTF offers a streamlined textual representa-\ntion of MIDI messages extracted using the mido library.\nFor simplicity, ticks_per_beat, though originally an\nattribute of MIDI objects in mido, is included as the first\nmessage at the beginning of the MTF representation.\nticks_per_beat 480\ntime_signature 3 4 24 8 0\nkey_signature G 0\nset_tempo 500000 0\ncontrol_change 0 0 121 0\nprogram_change 0 0 0\ncontrol_change 0 0 7 100\ncontrol_change 0 0 10 64\ncontrol_change 0 0 91 0\ncontrol_change 0 0 93 0\nmidi_port 0 0\nnote_on 0 0 74 80\nkey_signature G 0\nmidi_port 0 0\nnote_on 0 0 55 80\nnote_on 0 0 59 80\nnote_on 0 0 62 80\nnote_on 455 0 74 0\nnote_on 25 0 67 80\nnote_on 239 0 67 0\nnote_on 1 0 69 80\nnote_on 191 0 55 0\nnote_on 0 0 59 0\nnote_on 0 0 62 0\nnote_on 48 0 69 0\nnote_on 1 0 71 80\nnote_on 0 0 57 80\nnote_on 239 0 71 0\nnote_on 1 0 72 80\nnote_on 215 0 57 0\nnote_on 24 0 72 0\nnote_on 1 0 74 80\nnote_on 0 0 59 80\nnote_on 455 0 74 0\nnote_on 25 0 67 80\nnote_on 239 0 67 0\nnote_on 241 0 67 80\nnote_on 239 0 67 0\nnote_on 168 0 59 0\nend_of_track 1\n448\nC Prompt and Text Examples\nTo reduce textual noise and balance language distri-\nbution in pre-training data, we carefully designed\na structured prompt to leverage the capabilities\nof GPT-4. As illustrated in Fig. 8, the prompt\ncomprises a system instruction and two conversa-\ntional examples between a user and the assistant.\nThese examples act as in-context learning refer-\nences, helping GPT-4 understand the desired out-\nput format and the types of information it should\nextract from the provided metadata.\nAfter formulating the prompt, we organized the\nmetadata entries in our pre-training dataset into a\nstructured JSON format. For each entry, GPT-4\ngenerated corresponding summaries in both En-\nglish and a randomly selected non-English lan-\nguage from the 99 non-English languages sup-\nported by XLM-R (Conneau et al., 2020), in addi-\ntion to Cantonese. Including Cantonese, which is\nwell-represented in the dataset and sharing vocabu-\nlary with Mandarin, enables CLaMP 2 to support\n101 languages without increasing vocabulary size.\nTo ensure high-quality outputs, both the English\nand non-English summaries must strictly adhere\nto the specified JSON format. We implemented\nfiltering criteria to exclude entries that do not meet\nthese requirements, including those returning None,\nlacking proper JSON structure, or containing non-\nEnglish summaries in the wrong language. Incon-\nsistencies and structural errors are more prevalent\nin low-resource languages, as shown in Fig. 4.\nTo illustrate the effectiveness of this approach,\nFig. 9 provides examples that demonstrate GPT-\n4’s ability to generate summaries for different mu-\nsical compositions. Each example adheres to a\nstructured format, including key metadata—such\nas the title, composer, genres, description, lyrics,\nand ensemble information—followed by generated\nsummaries in English and a specified non-English\nlanguage.\nIn conclusion, our approach effectively uses\nGPT-4 to generate structured summaries from\nnoisy, English-centric metadata, reducing textual\nnoise and achieving a more balanced distribution\nof various languages. By applying filtering criteria,\nwe first remove entries that lack musical informa-\ntion, followed by those that are poorly structured\nor mismatched with the specified non-English lan-\nguage. This method enhances the quality of our\npre-training dataset and promotes a multilingual\nenvironment to better serve diverse languages.\nYour task is to provide a concise, comprehensive, and \ncoherent summary of the music piece using the \nprovided metadata. Please write the summary in English \nfirst, and then write an equivalent summary in the \nspecified non-English language from the \n\"nen_language\" field. Use this JSON format:\n{\n\"summary_en\": \"Your English summary here.\",\n\"summary_nen\": {\n“language”: “Specified non-English language.\",\n\"summary\": \"Your non-English summary here.\"\n}\n}\nIf there is not enough music-related information, return \n`None` instead.\nSystem\n{\n\"title\": \"Brejeiro\",\n\"composer\": \"Ernesto Nazareth\",\n\"genres\": [\"Choro\", \"Classical\", \"Instrumental\"],\n\"description\": \"\\\"Brejeiro\\\" is in A major and 2/4 time. A \njoyful melody begins at bar six, and a lively tango rhythm \nstarts at bar fourteen. It has a D.C. al Fine at bar fifty-\nthree and ends on two quarter notes in bar thirty-seven. \nThe piece, with its vibrant melodies and rhythms, reflects \ncelebration and carefreeness, embodying the spirit of \nBrazilian music.\",\n\"tags\": [\"Brazilian\", \"Choro\", \"Piano\"],\n\"ensembles\": [\"Solo Piano\", \"Small Ensemble\"],\n\"instruments\": [\"Piano\"],\n\"nen_language\": \"Japanese\"\n}\nUser\n{\n\"title\": \"Untitled\",\n\"composer\": \"Unknown\",\n\"description\": \"This is a good song.\",\n\"nen_language\": \"Russian\"\n}\nUser\n{\n\"summary_en\": \"Brejeiro, composed by Ernesto \nNazareth, is a lively choro piece in A major and 2/4 time. \nIt features a joyful melody that begins at bar six and a \nvibrant tango rhythm introduced at bar fourteen. The \npiece includes a D.C. al Fine at bar fifty-three, \nconcluding on two quarter notes in bar thirty-seven. \nWith its themes of celebration and carefreeness, \nBrejeiro beautifully captures the essence of Brazilian \nmusic and is well-suited for solo piano and small \nensembles.\",\n\"summary_nen\": {\n\"language\": \"Japanese\",\n\"summary\": \"「ブレジェイロ」は、エルネスト・\nナザレが作曲した活気あふれるショーロの作品で、\nイ長調の2/4拍子で書かれています。第6小節から\n始まる喜びに満ちたメロディーと、第14小節で導\n入される活気あるタンゴのリズムが特徴です。こ\nの曲には、第53小節でのD.C. al Fineが含まれ、また\n第37小節で二つの四分音符で締めくくられていま\nす。「ブレジェイロ」は、お祝いと無邪気さの\nテーマを持ち、ブラジル音楽の本質を美しく捉え\nており、ソロピアノや小編成のアンサンブルに\nぴったりの作品です。\"\n}\n}\nAssistant\nNone\nAssistant\nFigure 8: GPT-4 is tasked with generating summaries\nin English and a selected non-English language. The\nprompt includes a system instruction and two exam-\nples: one shows how to process music metadata—like\ntitle, composer, and genre—into clear multilingual sum-\nmaries, while the other identifies entries lacking suffi-\ncient musical information.\n449\n{\n\"title\": \"Hard Times Come Again No More\",\n\"composer\": \"Stephen Foster\",\n\"genres\": [\"Children's Music\", \"Folk\"],\n\"description\": \"\\\"Hard Times Come Again No More\\\" (sometimes referred to as \\\"Hard Times\\\") is an American parlor \nsong written by Stephen Foster, reflecting themes of sorrow and hope.\",\n\"lyrics\": \"Let us pause in life's pleasures and count its many tears,\\nWhile we all sup sorrow with the poor;\\nThere's a \nsong that will linger forever in our ears;\\nOh! Hard times come again no more.\\n\\nChorus:\\n'Tis the song, the sigh of \nthe weary,\\nHard Times, hard times, come again no more.\\nMany days you have lingered around my cabin door;\\nOh! \nHard times come again no more.\\n\\nWhile we seek mirth and beauty and music light and gay,\\nThere are frail forms \nfainting at the door;\\nThough their voices are silent, their pleading looks will say\\nOh! Hard times come again no \nmore.\\nChorus\\n\\nThere's a pale weeping maiden who toils her life away,\\nWith a worn heart whose better days are \no'er:\\nThough her voice would be merry, 'tis sighing all the day,\\nOh! Hard times come again no more.\\nChorus\\n\\n'Tis\na sigh that is wafted across the troubled wave,\\n'Tis a wail that is heard upon the shore\\n'Tis a dirge that is murmured \naround the lowly grave\\nOh! Hard times come again no more.\\nChorus\",\n\"tags\": [\"folk\", \"traditional\", \"bluegrass\", \"nostalgic\", \"heartfelt\", \"acoustic\", \"melancholic\", \"storytelling\", \"American \nroots\", \"resilience\"],\n\"ensembles\": [\"Folk Ensemble\"],\n\"instruments\": [\"Vocal\", \"Violin\", \"Tin whistle\", \"Guitar\", \"Banjo\", \"Tambourine\"],\n\"summary_en\": \"\\\"Hard Times Come Again No More,\\\" composed by Stephen Foster, is a poignant American parlor \nsong that explores themes of sorrow and hope. The lyrics reflect on the contrast between life's pleasures and its \nhardships, inviting listeners to acknowledge both joy and suffering. With a heartfelt chorus that repeats the line \\\"Hard \ntimes come again no more,\\\" the song resonates with nostalgia and resilience. It is often performed by folk ensembles \nand features a variety of instruments, including vocals, violin, guitar, and banjo, encapsulating the spirit of American \nroots music.\",\n\"summary_nen\": {\n\"language\": \"Chinese (Simplified)\",\n\"summary\": \"《艰难时光再无来临》是斯蒂芬·福斯特创作的一首感人至深的美国小歌厅歌曲，探讨了悲伤与希\n望的主题。歌词展现了生活的乐趣与艰辛之间的对比，邀请听众去感受快乐与痛苦的交织。歌曲中那句反复吟唱\n的“艰难时光再无来临”深情地表达了怀旧与坚韧。它常常由民谣乐队演奏，伴随着人声、小提琴、吉他和班卓琴\n等多种乐器，生动地展现了美国根源音乐的独特魅力。\"\n}\n}\n{\n  \"title\": \"ةدﯾﺻﻖ ﯾﺑﺗﺎك,\"\ngenres\": [\"ﯾﻧﺎﻏﺄ ةﯾﺑﻌش\" ,\"ةﯾﺛﺎرت],\n  \"description\": \"ةدﯾﺻﻖ ،ﯾﺑﺗﺎك وأ ءاﻧﺛﺄ ﺑﺎھذﻻ ﯩﻺ ،رادﻛُﺳﺄُ  ﯾﮫ ﻻوم ﯾﻧﺎﻣﺛﻊ رﯾﮭش ﯾف ﻟوﺑﻧطﺳﺈ ﯾوري ةﺻﻖ ھزﻧت ﺑﺗﺎك ةأرﻣﺎو ﯾف رادﻛﺳﺄ ﯾف ﻧرﻗﻼ ﻋﺳﺎﺗﻼ \nرﺷﻊ,\".\nlyrics\": \"ﻟطﮫ رطﻣﻼ ءاﻧﺛﺄ ﺑﺎھذﻻ ﯩﻺ رادﻛُﺳﺄُ \\nﺗﺧطﻠﺗو ﻣﺎﻣﻛﺄ ةرﺗس ﯾﺑﺗﺎك ةﻟﯾوطﻼ ﻧﯾطﻼب\\nظﻘﯾﺗﺳﺎ ﺑﺗﺎﻛﻼ ﻧم ﻣوﻧﻼ ھﻧﯾﻌو ھروﻣﺧم\\nﺑﺗﺎﻛﻼ ﯾل اﻧﺄو ھل \nاﻣف ﻧﺄش ؟ﻧﯾرﺧﺂﻻ\\nﻣك وه ﻗﺋﺎل ﺻﯾﻣﻘﻼ ﯩﺷﻧﻣﻼ ﯩﻠﻊ ﯾﺑﺗﺎك\\nﺗدﺟو اﻟﯾدﻧم ءاﻧﺛﺄ ﺑﺎھذﻻ رادﻛﺳﺄل\\nﺗﺋﻠم ﯾﻠﯾدﻧم ﻧﺑﻠﻣﻼب\\nﺗدﺟو ﯾﺑﺗﺎك ﯾراوﺟب اﻧﺄو ﺛﺣﺑﺄ \nھﻧﻊ\\nﺑﺗﺎﻛﻼ ﯾل اﻧﺄو ھل اﻣف ﻧﺄش ؟ﻧﯾرﺧﺂﻻ\\nﻣك وه ﻗﺋﺎل ﺻﯾﻣﻘﻼ ﯩﺷﻧﻣﻼ ﯩﻠﻊ ﯾﺑﺗﺎك,\"!\ntags\": [\"ﺛﺎرت\" ,\"ﯾﺑﻌش\" ,\"ةﻓﺎﻗث\" ,\"ﺻﺻﻖ\" ,\"ﻧﯾﻧﺢ],\n  \"ensembles\": [\"ةﻗرف ةﯾﺑﻌش],\n  \"instruments\": [\"ﺗوص\" ,\"دوع\" ,\"وﻧﺎﯾب\" ,\"ةﻟﺑط],\n  \"summary_en\": \"\\\"ةدﯾﺻﻖ ﯾﺑﺗﺎك\\\" or \\\"While Going to Üsküdar,\\\" is a popular Ottoman song from Istanbul that tells the \nstory of a writer and a woman strolling in Üsküdar during the 19th century. The lyrics describe a rainy day when the \nwriter's long coat gets muddy, his sleepy state, and his feelings for the woman beside him. With themes of nostalgia and \ncultural heritage, this piece is typically performed by folk ensembles and features vocals, oud, piano, and drums.\",\n  \"summary_nen\": {\n    \"language\": \"Amharic\",\n    \"summary\": \"ቃሊዳ ካታቢ, ወይም \\\"ወደ ኡስክዳር በመሄድ,\\\" በኢስታንቡል የታወቀ ወታዊ ዘፈን ነው፣ ይህ ዘፈን በአስክዳር በወር \nየነበረ ወንጌላዊ ሰውና እንቁላል ወንድ ያንቀሳቅስ ወቅት ይወዳል። የቃልዎቹ የዝንጉ ቀን የተወዳዳል ነገር እና ዝንባል ባይዛው በእምነት \nእና በልብስ ይወዳል። እንደ ምናልባት ይከበሩ፣ በሕይወት የተመለሰ ዝርዝር ይውውዝ።\"\n  }\n}\nFigure 9: Two examples of LLM-processed text data presented in JSON format, representing the original metadata\nand LLM-generated summaries in multiple languages for different music pieces.\n450\nD t-SNE Visualizations of CLaMP 2\nRepresentations\nThis section presents the t-SNE visualizations of\nfeature representations extracted from CLaMP 2\nacross three benchmarks: WikiMT, VGMIDI, and\nPianist8. These visualizations illustrate the clus-\ntering patterns of musical representations and re-\nveal an intriguing alignment between the two data\nmodalities—ABC notation and MIDI—without\nany fine-tuning of the model.\nAs demonstrated in Fig. 10, the clarity of cluster-\ning correlates with the classification performance\nfrom Table 1. Pianist8, which achieves the highest\naccuracy, displays well-defined and tight clusters,\nsignifying that the model adeptly apprehends the\nminute stylistic subtleties at the composer level.\nA particularly notable finding is the mirrored spa-\ntial alignment between ABC and MIDI across all\ndatasets. This implies that, despite their dissimilar\nmusical encodings, CLaMP 2 capture comparable\nlatent structures within the feature space. The align-\nment indicates that CLaMP 2 extracts modality-\ninvariant features—resilient patterns that remain\nconsistent across ABC and MIDI formats. These\nshared representations are likely to reflect profound\nmusical semantics such as harmonic progressions,\nrhythmic architectures, or stylistic themes.\nThis symmetry has practical consequences. It\nsuggests that CLaMP 2 could enable cross-modal\ntasks, for example, retrieving MIDI files based on\nABC queries, without requiring specialized adap-\ntation. It also points to the potential for trans-\nfer learning between modalities, where a model\ntrained on one format (e.g., ABC) could operate\neffectively on another (e.g., MIDI). Future work\ncould explore whether introducing explicit align-\nment techniques, like contrastive learning, could\nfurther enhance cross-modal performance between\nthese two modalities.\nThese results spotlight both the strengths and\nlimitations of CLaMP 2: the model demonstrates\nstrong generalization across datasets, capturing\nmeaningful musical patterns across diverse do-\nmains. However, it struggles with tasks involving\noverlapping or ambiguous genre boundaries, sim-\nilar to human perception, such as distinguishing\nbetween entities like Bethel and Hillsong, which it\nfinds very similar. This suggests that while CLaMP\n2 excels at identifying clear stylistic differences, it\nmay have difficulty differentiating between more\nclosely related or subtle variations.\nCountry\nDance\nFolk\nJazz\nLatin\nPop\nR&B\nRock\nABC\nMIDI\n(a) t-SNE visualizations on the WikiMT benchmark\nV0A0\nV0A1\nV1A0\nV1A1\nABC\nMIDI\n(b) t-SNE visualizations on the VGMIDI benchmark\nBethel\nClayderman\nEinaudi\nHancock\nHillsong\nHisaishi\nRyuichi\nYiruma\nABC\nMIDI\n(c) t-SNE visualizations on the Pianist8 benchmark\nFigure 10: t-SNE visualizations of feature represen-\ntations from CLaMP 2 (without fine-tuning) for three\ndatasets: (a) WikiMT, (b) VGMIDI, and (c) Pianist8.\nA noteworthy observation is the mirrored spatial align-\nment of the ABC and MIDI representations, suggesting\nthat CLaMP 2 effectively extracts modality-invariant\nmusical semantics from both formats.\n451",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7759913206100464
    },
    {
      "name": "Natural language processing",
      "score": 0.5520065426826477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46668821573257446
    },
    {
      "name": "Language model",
      "score": 0.44132310152053833
    },
    {
      "name": "Information retrieval",
      "score": 0.392380028963089
    },
    {
      "name": "Speech recognition",
      "score": 0.3785981833934784
    }
  ]
}