{
  "title": "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal",
  "url": "https://openalex.org/W4200632246",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098196253",
      "name": "Zhaoyang Sun",
      "affiliations": [
        "Wuhan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136392240",
      "name": "Yaxiong Chen",
      "affiliations": [
        "Wuhan University of Technology",
        "Chongqing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136140329",
      "name": "Shengwu Xiong",
      "affiliations": [
        "Wuhan University of Technology",
        "Wuhan University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098196253",
      "name": "Zhaoyang Sun",
      "affiliations": [
        "Wuhan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136392240",
      "name": "Yaxiong Chen",
      "affiliations": [
        "Wuhan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2136140329",
      "name": "Shengwu Xiong",
      "affiliations": [
        "Wuhan University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798600195",
    "https://openalex.org/W2969686176",
    "https://openalex.org/W3169770341",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W2942143931",
    "https://openalex.org/W6678111577",
    "https://openalex.org/W6752851532",
    "https://openalex.org/W2797650215",
    "https://openalex.org/W3035661604",
    "https://openalex.org/W2991526570",
    "https://openalex.org/W2904367110",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3159890710",
    "https://openalex.org/W3020922552",
    "https://openalex.org/W1916003155",
    "https://openalex.org/W2611605760",
    "https://openalex.org/W3188904624",
    "https://openalex.org/W3196813158",
    "https://openalex.org/W6734564793",
    "https://openalex.org/W3146983536",
    "https://openalex.org/W2920879895",
    "https://openalex.org/W2965200717",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3013321121",
    "https://openalex.org/W6664829052",
    "https://openalex.org/W3149806960",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W3016067379",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2963890275",
    "https://openalex.org/W4287239845",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W3034723751",
    "https://openalex.org/W2056566827",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2121293528",
    "https://openalex.org/W3112431590",
    "https://openalex.org/W3034463304",
    "https://openalex.org/W2593414223",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W2809852002",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W2989149111",
    "https://openalex.org/W3186501579",
    "https://openalex.org/W3034832575"
  ],
  "abstract": "Makeup transfer is not only to extract the makeup style of the reference image, but also to render the makeup style to the semantic corresponding position of the target image. However, most existing methods focus on the former and ignore the latter, resulting in a failure to achieve desired results. To solve the above problems, we propose a unified Symmetric Semantic-Aware Transformer (SSAT) network, which incorporates semantic correspondence learning to realize makeup transfer and removal simultaneously. In SSAT, a novel Symmetric Semantic Corresponding Feature Transfer (SSCFT) module and a weakly supervised semantic loss are proposed to model and facilitate the establishment of accurate semantic correspondence. In the generation process, the extracted makeup features are spatially distorted by SSCFT to achieve semantic alignment with the target image, then the distorted makeup features are combined with unmodified makeup irrelevant features to produce the final result. Experiments show that our method obtains more visually accurate makeup transfer results, and user study in comparison with other state-of-the-art makeup transfer methods reflects the superiority of our method. Besides, we verify the robustness of the proposed method in the difference of expression and pose, object occlusion scenes, and extend it to video makeup transfer.",
  "full_text": "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer\nand Removal\nZhaoyang Sun1, Yaxiong Chen1, 2, Shengwu Xiong1, 3, \u0003\n1 School of Computer Science and Artiﬁcial Intelligence, Wuhan University of Technology, Wuhan, 430070\n2 Wuhan University of Technology Chongqing Research Institute, Chongqing, 401122\n3 Sanya Science and Education Innovation Park of Wuhan University of Technology, Sanya, 572000\nzhaoyangsun0304@outlook.com, chenyaxiong@whut.edu.cn, xiongsw@whut.edu.cn\nAbstract\nMakeup transfer is not only to extract the makeup style of the\nreference image, but also to render the makeup style to the\nsemantic corresponding position of the target image. How-\never, most existing methods focus on the former and ignore\nthe latter, resulting in a failure to achieve desired results. To\nsolve the above problems, we propose a uniﬁed Symmetric\nSemantic-Aware Transformer (SSAT) network, which incor-\nporates semantic correspondence learning to realize makeup\ntransfer and removal simultaneously. In SSAT, a novel Sym-\nmetric Semantic Corresponding Feature Transfer (SSCFT)\nmodule and a weakly supervised semantic loss are proposed\nto model and facilitate the establishment of accurate seman-\ntic correspondence. In the generation process, the extracted\nmakeup features are spatially distorted by SSCFT to achieve\nsemantic alignment with the target image, then the distorted\nmakeup features are combined with unmodiﬁed makeup irrel-\nevant features to produce the ﬁnal result. Experiments show\nthat our method obtains more visually accurate makeup trans-\nfer results, and user study in comparison with other state-of-\nthe-art makeup transfer methods reﬂects the superiority of our\nmethod. Besides, we verify the robustness of the proposed\nmethod in the difference of expression and pose, object oc-\nclusion scenes, and extend it to video makeup transfer.\nIntroduction\nMakeup transfer aims to transfer the makeup style of any\nreference image to the target image while preserving the i-\ndentity of the target person. With the booming development\nof the cosmetics market, makeup transfer is widely demand-\ned in many popular beautifying applications and has been re-\nceived extensive attention in the computer vision and graph-\nics. In the early years, traditional approaches (Guo and Sim\n2009; Tong et al. 2007; Li, Zhou, and Lin 2015) mostly used\nimage gradient editing or physical-based modiﬁcation to re-\nalize makeup transfer. Recently, combining generative ad-\nversarial network (Goodfellow et al. 2014) and disentangled\nrepresentation (Lee et al. 2018; Huang et al. 2018), many\nmakeup transfer approaches (Li et al. 2018; Chang et al.\n2018; Chen et al. 2019; Gu et al. 2019; Huang et al. 2020;\nDeng et al. 2021; Nguyen, Tran, and Hoai 2021) have made\n\u0003Corresponding Author\nCopyright c\r 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The diagram of our motivation and PSGAN com-\nparison. The spatial mapping of the same color points be-\ntween the target image and the reference image is the seman-\ntic correspondence expected to be established in this paper.\nsigniﬁcant progress in extracting makeup style and generat-\ning realistic makeup results.\nHowever, little attention has been paid to semantic corre-\nspondence, which plays an important role in makeup trans-\nfer. Consider the actual makeup process following the tuto-\nrial: 1) choose cosmetics of the same color (extract makeup\nstyle); 2) apply this cosmetics to the semantic correspond-\ning position of the face (semantic correspondence), see Fig-\nure 1. Ignoring the semantic correspondence would result\nin the makeup style being transferred to the wrong position,\nsuch as lipstick leaking lips. At the same time, inaccurate se-\nmantic correspondence will lead to the averaging of makeup\ncolors (Jiang et al. 2020), so that the resulting makeup style\nis visually different from the reference makeup. In addition,\nthe robustness of expression and pose and the robustness of\nobject occlusion are greatly reduced.\nThe motivation of this paper is to establish accurate se-\nmantic correspondence between the target and the reference\nfacial images to improve the quality of makeup transfer.\nNote that the dense semantic correspondence of the make-\nup regions is established, not the sparse semantic correspon-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2325\nFigure 2: Our SSAT makeup transfer results. The diversi-\nty of makeup style, the difference of facial expression and\npose, object occlusion (glasses or hair) are considered in the\nselected images. The semantic correspondence is in the low-\ner right corner of the generated result, and the value of each\nspatial position of it is obtained by the different weighted\nsum of the reference image.\ndence shown in the Figure1. For this purpose, a Symmet-\nric Semantic Corresponding Feature Transfer (SSCFT) mod-\nule is proposed to models the spatial position mapping be-\ntween different images. Next there is no effective supervi-\nsion for semantic correspondence, so the face parsing is in-\ntroduced and a weakly supervised semantic loss is proposed.\nThe experiments show that the semantic correspondence es-\ntablished by our method is adaptive, which ignores the se-\nmantic correspondence of makeup irrelevant regions (hair,\nbackground, glasses, inside eyes, inside mouth) and focuses\non the makeup regions, see Figure 2.\nTo sum up, we propose a Symmetric Semantic-Aware\nTransformer network (SSAT) whose framework and pro-\ncess are shown in Figure 3. Compared with PSGAN (Jiang\net al. 2020), there are several obvious differences: 1) The\nproposed method introduces the face parsing, instead of the\nfeature points which sometimes has large error in side face.\n2) Learnable network features instead of hand-designed fea-\ntures to establish semantic correspondence. 3) Using the\nsymmetry of semantic correspondence, our method gener-\nates makeup transfer and removal results simultaneously,\nwhile PSGAN can only realize makeup transfer. The main\ncontributions of this paper are summarized as follows:\n•We propose a novel Symmetric Semantic-Aware Trans-\nformer (SSAT) network for makeup transfer and removal.\nExperiments verify the effectiveness of the transferring\nstrategy in the real-world environment and generated re-\nsults are of higher quality than state-of-the-art methods\nboth qualitatively and quantitatively.\n•A novel SSCFT module and a semantic loss are proposed\nto establishing accurate semantic correspondence, which\nsigniﬁcantly improves the quality of makeup transfer.\n•Combined with face parsing, our method is more ﬂexible\nand could realize partial makeup transfer. Meanwhile, we\nverify the robustness of the proposed method in the dif-\nference of expression and pose, object occlusion (glasses\nor hair) scenes and extend it to video makeup transfer.\nRelated Work\nMakeup Transfer\nIn recent years, makeup transfer has been extensively s-\ntudied. BeautyGAN (Li et al. 2018) addressed the make-\nup transfer and removal task by incorporating both glob-\nal domain adaptation loss and local instance-level makeup\nloss in an dual input/output GAN. PairedCycleGAN (Chang\net al. 2018) extended the CycleGAN (Zhu et al. 2017) to\nasymmetric networks to enable transferring speciﬁc makeup\nstyle. LADN (Gu et al. 2019) and CPM (Nguyen, Tran, and\nHoai 2021) focused on the complex/dramatic makeup styles\ntransfer. Introducing facial feature points, PSGAN (Jiang\net al. 2020; Liu et al. 2021) proposed a pose and expres-\nsion robust spatial-aware GAN for makeup transfer. Recent-\nly, SOGAN (Lyu et al. 2021) explored the shadow and oc-\nclusion robust and (Wan et al. 2021) realized makeup trans-\nfer from the perspective of face attribute editing. Inspired by\nStyleGAN (Karras, Laine, and Aila 2018), SCGAN (Deng\net al. 2021) proposed a style-based controllable GAN mod-\nel. Unlike the above methods, the core idea of this paper\nis to establish accurate semantic correspondence to improve\nthe quality of makeup transfer.\nSemantic Correspondence\nIn recent years, CNN-based features (Simonyan and Zisser-\nman 2015; Krizhevsky, Sutskever, and Hinton 2017) have\nbeen proved to be a powerful tool to express high-level se-\nmantics. Recently, (Liao et al. 2017; Zhang et al. 2020) pro-\nposed a technique for visual attribute transfer across im-\nages using semantic correspondence. The exemplar-based\ncolorization methods (He et al. 2018; Lee et al. 2020) cal-\nculated the semantic correspondence between the target im-\nage and the exemplar image, then transferred the color with\nthe closest semantic similarity to the target image. Inspired\nby the recent exemplar-based image colorization (He et al.\n2018; Lee et al. 2020), our work is expected to transfer the\nmakeup style with the closest semantic similarity to the tar-\nget image.\nOur Approach: SSAT\nFormulation\nOur goal is to transfer the makeup style from an arbitrary\nmakeup reference image to a non-makeup target image.\nHere, X ⊂RH\u0002W\u00023 refers to non-makeup image domain,\nY ⊂ RH\u0002W\u00023 refers to makeup image domain. Given\na target image xt ∈ X and a reference image yr ∈ Y,\nthe goal of makeup transfer is learning a mapping function:\n2326\nFigure 3: The proposed Symmetric Semantic-Aware Transformer (SSAT) network. The process goes through the following\nsteps: 1) Content encoder Ec and makeup encoder Em decompose target image xt and reference image yr respectively, xc\nt =\nEc(xt);xm\nt = Em(xt);yc\nr = Ec(yr);ym\nr = Em(yr). Meanwhile, face parsing st;sr is introduced and semantic features\nare extracted by using semantic encoders Es: xs\nt = Es(st);ys\nr = Es(sr). 2) The Feature Fusion (FF) module fuses content\nfeatures and semantic features to obtain richer features for semantic correspondence, xf\nt = FF(xc\nt;xs\nt );yf\nr = FF(yc\nr;ys\nr). 3)\nThe Symmetric Semantic Corresponding Feature Transfer (SSCFT) module distorts makeup features spatially according to the\nsemantic correspondence established by xf\nt and yf\nr , and outputs ^ym\nr ;^xm\nt = SSCFT (xf\nt ;yf\nr ;xm\nt ;ym\nr ). 4) Distorted makeup\nfeatures ^ym\nr of the reference image are embedded in the content features xc\nt of the target image to generate makeup transfer\nresult ^yt = Dec(xc\nt;^ym\nr ). Similarly, the makeup removal result ^xr = Dec(yc\nr;^xm\nt ).\n\b : xt;yr → ^yt, where ^yt ∈Y has the makeup style with\nyr while preserving the identity of xt. For makeup removal,\nit is assumed that the non-makeup image is a special case of\nthe makeup image (Sun et al. 2020), which uniﬁes makeup\ntransfer and makeup removal. Therefore, the goal of make-\nup removal is learning a mapping function:\b : yr;xt → ^xr,\nwhere ^xr ∈X has the makeup style with xt while preserv-\ning the identity of yr. In this paper, the only difference be-\ntween makeup transfer and removal is whether the reference\nimage is a non-makeup image or a makeup image.\nSSAT\nThe overall framework of Symmetric Semantic-Aware\nTransformer network (SSAT) is shown in Figure 3, which\nconsists of three encoders, a FF module, a SSCFT module\nand a decoder Dec. Next, the function of each module will\nbe introduced in detail.\nEncoders The one main problem of makeup transfer stem-\ns from the difﬁculty of extracting the makeup latent features,\nwhich are required to be disentangled from other make-\nup irrelevant features. Here, this problem is referred to as\ncontent-style separation (Huang et al. 2018; Lee et al. 2018).\nSo a content encoder Ec and a makeup encoder Em are de-\nsigned to extract content features and makeup features re-\nspectively:\nxc\nt = Ec(xt);xm\nt = Em(xt) (1)\nyc\nr = Ec(yr);ym\nr = Em(yr) (2)\nExperiments have found that it is difﬁcult to establish ac-\ncurate semantic correspondences only with content features.\nTherefore, face parsing (Yu et al. 2018) is introduced and\nsemantic features are extracted by using semantic encoders\nEs:\nxs\nt = Es(st);ys\nr = Es(sr) (3)\nwhere st ∈RH\u0002W\u0002L and sr ∈RH\u0002W\u0002L refer to binary\nface parsing of the target image and the reference image,\nrespectively. Lis the number of different semantic regions,\nwhich is set 18 in our experiments. Next, content features\nand semantic features will cooperate to establish semantic\ncorrespondence.\n2327\nFigure 4: The illustration of feature fusion (FF) module.\nFF fuses content features and semantic features to obtain\nricher features for semantic correspondence. ⊙refers to the\nchannel-wise concatenation operator.\nFF In the FF module, content features and semantic fea-\ntures are fused to obtain richer features for feature match-\ning, see Figure 4. Take the target image as an example, af-\nter obtaining content features and semantic features, they\nare connected along the channel dimensions and fed in-\nto the FF module. FF consists of N convolutional layer-\ns and outputs N high-level features (f1;f2;···;fN ). Be-\nsides, the low-level features extracted by Ec and Es are al-\nso combined. Ec and Es consist of M convolutional layer-\ns, producing M low-level features (C1;C2;···;CM ) and\n(S1;S2;···;SM ), where CM = xc\nt;SM = xs\nt . Each layer\nof content features is selected, but the last layer of semantic\nfeatures is selected. Now, we downsample each of Cm and\nupsample each of fn to match the spatial size SM , forming\nthe ﬁnal fusion features xf\nt :\nxf\nt = [\u001e(C1); \u001e(C2); ···; CM ; SM ;\n'(f1); '(f2); ···; '(fN )]\n(4)\nwhere \u001e denotes a spatially downsampling function of an\ninput Cm of different size to the size of CM or SM . Simi-\nlarly, 'denotes a spatially upsampling function. ”;” denotes\nthe channel-wise concatenation operator. In this manner, the\noutput fusion feature xf\nt combines the low-level and high-\nlevel features, while ensuring the accuracy of semantic cor-\nrespondence. With the same operation, we obtain the fusion\nfeatures yf\nr of the reference image.\nSSCFT The SSCFT module is inspired by colorization\n(He et al. 2018) and combines the symmetry of semantic\ncorrespondence. In makeup transfer task, the semantic cor-\nrespondence is a one-to-one mapping relationship. That is,\npoint A corresponds to point B. In turn, point B also corre-\nsponds to point A. And the symmetry of this semantic rela-\ntionship can be applied cleverly to the anti-task of makeup\ntransfer, makeup removal. See Figure 5 for the framework\nof the proposed SSCFT module.\nFirstly, we reshape xf\nt into ^xf\nt = [ x1;x2;···;xhw] ∈\nRd\u0002hw, where xi ∈Rd indicates feature variables of the\nith location of ^xf\nt . Then we obtain channel-wise centralized\nfeatures ^xi and ^yj to make the learning more stable (He et al.\nFigure 5: The illustration of symmetric semantic corre-\nsponding feature transfer (SSCFT) module. Wa;Wb refer to\nthe linear transformation matrix into xf\nt and yf\nr , respective-\nly. ⊗refers to the matrix multiplication operator. T refers to\nthe matrix transpose operator. After this module, the output\ndistorted makeup features ^xm\nt ;^ym\nr are semantic aligned with\nyc\nr;xc\nt in semantic.\n2018), where ^xi = xi −mean(xi);^yj = xy −mean(yj).\nGiven ^xi and ^yj, SSCFT computes a semantic correlation\nmatrix A∈R hw\u0002hw, whose element ai;j is computed by\nthe scaled dot product:\nai;j = ^xT\ni ·^yj\n∥^xi∥∥^yj∥ (5)\nAfter that, we distort the reference makeup features ym\nr to-\nwards xc\nt according to the correlation matrix A. The weight-\ned sum of ^ym\nr is calculated to approximate the makeup sam-\npling from ym\nr :\n^ym\nr (i) =\nX\nj\nsoftmaxj(a(i;j) ·\u001b) ·ym\nr (j) (6)\nwhere \u001bcontrols the sharpness of the softmax and we set its\ndefault value as 100. Now the distorted makeup features ^ym\nr\nof reference image are aligned with the content features xc\nt\nof target image in semantic. In the same way, we obtain the\ndistorted makeup features ^xm\nt , which aligns with the content\nfeatures yc\nr. Note that this step makes our method robust to\nexpression, pose, object occlusion and produce more accu-\nrate makeup transfer results.\nDec Finally, we employ the spatially-adaptive denormal-\nization (SPADE) block (Park et al. 2019) to project the dis-\ntorted makeup styles ^ym\nr ;^xm\nt to content features xc\nt;yc\nr for\nmakeup transfer and removal.\n^yt = Dec(xc\nt;^ym\nr ) (7)\n^xr = Dec(yc\nr;^xm\nt ) (8)\nwhere ^yt is the makeup transfer result and ^xr is the makeup\nremoval result.\n2328\nFigure 6: The process of generating pseudo-paired data.\nObjective\nIn total, there are four loss functions used for network SSAT\nend-to-end training. The overall loss is as follows:\nLoverall =\u0015semLsem + \u0015makeupLmakeup+\n\u0015recLrec + \u0015advLadv\n(9)\nSemantic Loss A weakly supervise semantic loss is pro-\nposed to establish semantic correspondence. The idea is that\nsemantic correspondence should only exist between seman-\ntic regions of the same class:\nLsem = ∥st −^sr∥1 + ∥sr −^st∥1 (10)\nwhere st ∈RH\u0002W\u0002L and sr ∈RH\u0002W\u0002L refer to binary\nface parsing, which only have the values 0 and 1. L is the\nnumber of semantic classes, ^sr(i) = P\nj softmaxj(a(i;j)·\n\u001b) ·sr(j), ^st(i) = P\nj softmaxj(a(i;j) ·\u001b) ·st(j), and\n∥·∥1 refers to L1 loss. For dense semantic correspondence,\nthe Lsem is only a crude region constraint, but experiments\nshow that it plays an important role.\nMakeup Loss Inspired by (Chang et al. 2018), we gener-\nate pseudo pairs of data \u0016xr and \u0016yt according to face feature\npoints to train the network, see Figure 12. The process is\ndescribed in detail in the supplementary materials. Here, we\nintroduce the SPL loss (Sarfraz et al. 2019) instead of theL1\nloss to guide the makeup transfer and removal:\nLmakeup = SPL(^yt;xt;\u0016yt) + SPL(^xr;yr;\u0016xr) (11)\nTake the makeup transfer as an example, SPL constrains the\ngradient consistency between ^yt and xt to ensure the identi-\nty of the target image, and restricts the color consistency be-\ntween ^yt and \u0016yt to guide the makeup transfer. Note thatLsem\nconstrains the corresponding region and does not penalize\none-to-many mappings of the same class. While Lmakeup\nguides makeup transfer, it implicitly constrains the one-to-\none mapping in the makeup regions.\nReconstruction Loss We feed the xc\nt and xm\nt into Decto\ngenerate xself\nt , and yc\nr and ym\nr into Dec to generate yself\nr ,\nwhich should be identical to xt and yr. Here, we introduce\nCycle loss (Zhu et al. 2017) to ensure that the image does not\nlose information during the decoupling process of makeup\nfeatures and content features. So we feed the makeup re-\nmoval result ^xr and makeup transfer ^yt result into SSAT a-\ngain to obtain the xcycle\nt and ycycle\nr , which should also be\nFigure 7: Ablation study of SSCFT module.\nidentical to xt and yr. We use L1 loss to encourage such\nreconstruction consistency:\nLrec =∥xt −xself\nt ∥1 + ∥yr −yself\nr ∥1+\n∥xt −xcycle\nt ∥1 + ∥yr −ycycle\nr ∥1\n(12)\nAdversarial Loss Two discriminatorsDX and DY are in-\ntroduced for the non-makeup domain and makeup domain,\nwhich try to discriminate between real samples and generat-\ned images and help the generator synthesize realistic output-\ns. The least square loss (Mao et al. 2017) is used for steady\ntraining:\nLadv =Ext [(DX(xt))2] + E^xr [(1 −DX(^xr))2]+\nEyr [(DY (yr))2] + E^yt [(1 −DY (^yt))2]\n(13)\nExperiment\nDataset and Implementation Details\nFor the dataset, we randomly selected 300 non-makeup im-\nages as target images and 300 makeup images as reference\nimages from the Makeup Transfer dataset (Li et al. 2018).\nUsing the proposed generation method, a total of 180,000\npairs of pseudo-paired data are generated for makeup trans-\nfer and removal. During the training, all trainable parame-\nters are initialized normally, and the Adam optimizer with\n\f1 = 0 :5;\f2 = 0 :999 is employed for training. We set\n\u0015sem = 1 ;\u0015makeup = 1 ;\u0015rec = 1 ;\u0015adv = 1 for balan-\nceing the different loss functions. The SSAT is implemented\nby MindSpore 1. And the model is trained for 300,000 itera-\ntions on one single Nvidia 2080Ti GPU. The learning rate is\nﬁxed as 0.0002 during the ﬁrst 150,000 iterations and linear-\nly decays to 0 over the next 150,000 iterations. The batch-\nsize is set 1. See the supplementary materials for the spe-\nciﬁc network structure parameters. Code will be available at\nhttps://gitee.com/sunzhaoyang0304/ssat-msp.\nAblation Studies\nEffects of SSCFT The motivation of this paper is to es-\ntablish semantic correspondence to improve the quality of\nmakeup transfer. In order to verify our idea, we remove the\nSSCFT module to analyze the effects of the role of semantic\n1Mindspore. https://www.mindspore.cn/\n2329\nFigure 8: Ablation study of Lsem.\ncorrespondence in makeup transfer. In this case, we direct-\nly skip the SSCFT module and input the features into the\ndecoder. The comparison results are shown in the Figure 7.\nWithout SSCFT, the resulting makeup style is signiﬁcant-\nly lighter than the reference makeup. With the addition of\nSSCFT, the result is more similar visually to the reference\nmakeup , especially eye shadow and blush.\nEffects ofLsem The lack of effective supervision hinders\nthe establishment of semantic correspondence. To solve this\nproblem, we introduce face parsing and propose a weakly\nsupervised semantic loss Lsem. In order to verify their ef-\nfect, we remove Lsem and the face parsing, use the remain-\ning loss to train the network. The comparison of semantic\nand makeup transfer results is shown in the Figure 8. Com-\npared without Lsem, the semantic result using Lsem is more\naccurate and the boundary between makeup area and non-\nmakeup area is also clearer. For the makeup transfer result,\nthe blush is mapped to the wrong semantic position, spread-\ning over the entire face in the result without using Lsem.\nComparisons to Baselines\nTo verify the superiority of our makeup transfer strategy,\nwe choose three state-of-the-art makeup transfer approach-\nes, BeautyGAN (Li et al. 2018), PSGAN (Jiang et al. 2020),\nSCGAN (Deng et al. 2021), as our comparison benchmark.\nWe skip some baselines such as LADN (Gu et al. 2019) and\nCPM (Nguyen, Tran, and Hoai 2021), because they focus on\nthe complex/dramatic makeup styles transfer.\nQualitative Comparison Here, three scenes that often ap-\npear in real life are selected for comparison, frontal face,\ndifferent expressions and poses, and object occlusion. The\nqualitative comparison has been shown in Figure 9. Beau-\ntyGAN produces a realistic result, but fails to transfer\neye shadow and blush. Although PSGAN designs semantic\ncorrespondence modules, its accuracy is limited by hand-\ndesigned features. The imprecise semantic correspondence\nresults in a lighter eye and blush makeup style, which is vi-\nsually different from the reference makeup. Similar to Beau-\ntyGAN, the transfer of eye makeup and blush fails in SC-\nGAN. Meanwhile, the results of SCGAN are too smooth,\nand slightly change makeup irrelevant information, such as\nthe glasses in the sixth row. On the contrary, the makeup\nMethods Rank 1 Rank 2 Rank 3 Rank 4\nBeautyGAN 3.0% 3.9% 34.5% 58.6%\nPSGAN 11.3% 73.2% 11.3% 4.1%\nSCGAN 3.6% 15.3% 48.0% 33.2%\nSSAT 82.1% 7.6% 6.2% 4.1%\nTable 1: User Study.\nFigure 10: The partial makeup transfer results. The last col-\numn is the partial makeup transfer results, which receive per-\nsonal identity from the ﬁrst column, the lips style from the\nsecond column, the eyes style from the third column and the\nface style from the fourth column.\nstyle of our results is highly similar to the reference make-\nup, whether it is lipstick, eye shadow, blush. The semantic\nresults explain why our method has a better makeup transfer\neffect and why our method is robust to expression, pose and\nobject occlusion.\nQuantitative Comparison How quantitative evaluation\nmakeup transfer is still a ﬁeld that needs to explore. Here,\nwe conduct user study to compare different methods quanti-\ntatively. We randomly generate 30 results of makeup transfer\nusing four methods respectively. 45 volunteers are asked to\nrank the results based on the realism and the similarity of\nmakeup styles. To be fair, the results in each selection are\nalso randomly arranged. As shown in the Table 3, our SSAT\noutperforms other methods by a large margin. Our method\nachieves the highest selection rate of 82.1% in Rank 1.\nPartial Makeup Transfer\nPartial makeup transfer refers to transfer partial makeup of\nthe reference image. The distorted makeup features extract-\ned by our method is accurately distributed according to the\nspatial semantic position of the target image, making it pos-\nsible to integrate the partial makeup from different reference\nimages, as shown in the Figure 10.\n^ypart\nt = Dec(xc\nt;\nX\ni\n(^ymi\nr ·Maski)) (14)\nwhere i∈{Lip;Eye;Face }in our experiment, ^ymi\nr means\nthe distorted makeup features extracted are from different\nreference images, Maski represents a binary mask related\nto the makeup area.\n2330\nFigure 9: Comparison with state-of-the-art methods. The ﬁrst two rows: frontal face, the middle two rows: different expressions\nand poses, the last two rows: object occlusion (glasses or hair).\nFigure 11: The video makeup transfer results.\nVideo Makeup Transfer\nVideo makeup transfer is a very challenging task, which has\nhigh requirements for the quality of generated images and\nthe accuracy of semantic correspondence. We download a\nvideo from the Internet and decompose it frame by frame,\nthen apply the SSAT method, and ﬁnally integrate the re-\nsulting images into a video. We chose PSGAN as the com-\nparison baseline, because other methods don’t consider se-\nmantic correspondence. See Figure 11, the results produced\nby PSGAN are visually different from the reference make-\nup and cause ﬂickering and discontinuity. In contrast, Our\nSSAT achieves smooth and accurate video makeup transfer\nresults.\nConclusion\nDifferent from other methods, we focus on semantic cor-\nrespondence learning, propose the SSCFT module and a\nsemantic loss, then integrate them into one Symmetric\nSemantic-Aware Transformer network (SSAT) for makeup\ntransfer and removal. The experiment veriﬁed that semantic\ncorrespondence signiﬁcantly improved the quality of make-\nup transfer visually as expected. The comparison with oth-\ner methods demonstrates that our method achieves state-of-\nthe-art makeup transfer results. In addition, beneﬁts from\nprecise semantic correspondence, our method is robust to\nthe difference of expression and pose, object occlusion and\ncan achieve partial makeup transfer. Moreover, we extend S-\nSAT to the ﬁeld of video makeup transfer, generating smooth\nand stable results. However, the computational complexity\nof proposed SSCFT is quadratic to image size, the focus of\nour later work is to solve this problem.\n2331\nAcknowledgments\nThis work was in part supported by NSFC (Grant\nNo. 62176194) and the Major project of IoV , Tech-\nnological Innovation Projects in Hubei Province (Grant\nNo.2020AAA001, 2019AAA024) and Sanya Science and E-\nducation Innovation Park of Wuhan University of Technol-\nogy (Grant No. 2020KF0054), the National Natural Science\nFoundation of China under Grant 62101393 and the Open\nProject of Wuhan University of Technology Chongqing Re-\nsearch Institute(ZL2021–6).\nSupplementary Materials\nGeneration of Pseudo-paired Data\nFigure 12: The process of generating pseudo-paired data.\nPseudo-paired data is used in the Lmakeup, and the gen-\neration process is shown in Figure 12. Firstly, Face ++ API\ninterface 2 is used to obtain the feature points of the input\nface image. Then, piecewise-afﬁne transformation distorts\nthe reference image into the target image according to the\nposition of feature points. In this step, the inside of the eyes\nand the mouth remain unchanged, because these two areas\nneed to be preserved during makeup transfer. Finally, his-\ntogram matching are applied to change the appearance of\nthe ears and neck according to face parsing. Note that the\nwarping results \u0016yr sometimes possess artifacts, which can\nbe ﬁxed by SSAT in the generated results. For the dataset,\nwe randomly selected 300 non-makeup images as target\nimages and 300 makeup images as reference images from\nthe Makeup Transfer dataset. Using the proposed genera-\ntion method, a total of 180,000 pairs of pseudo-paired data\nare generated for makeup transfer and removal. During the\ntraining, images are resized to286×286, randomly cropped\nto 256 ×256, horizontally ﬂipped with probability of 0.5,\nand randomly rotated from -30 degrees to +30 degrees for\ndata augmentation.\nNetwork Structure\nFor the network structure, the encoders consist of stacked\nconvolution blocks, which contain convolution, Instance\nnormalization, and ReLU activation layers. In the decoder,\nthe SPADE is applyed to embed the makeup features into\nthe content features, with each SPADE followed by a residu-\nal block. For the discriminator, the network structure follows\n2https://www.faceplusplus.com.cn/dense-facial-landmarks/\nLayer Ec and Em\nL1\nConv(I:3,O:64,K:7,P:3,S:1),\nInstance Normalization,\nLeaky ReLU:0.2\nL2\nConv(I:64,O:128,K:3,P:1,S:2),\nInstance Normalization,\nLeaky ReLU:0.2\nL3\nConv(I:128,O:256,K:3,P:1,S:2),\nInstance Normalization,\nLeaky ReLU:0.2\nTable 2: The network architecture of Encoder Ec and Em.\nLayer Es\nL1\nConv(I:18,O:32,K:7,P:3,S:1),\nInstance Normalization,\nLeaky ReLU:0.2\nL2\nConv(I:32,O:64,K:3,P:1,S:2),\nInstance Normalization,\nLeaky ReLU:0.2\nL3\nConv(I:64,O:128,K:3,P:1,S:2),\nInstance Normalization,\nLeaky ReLU:0.2\nTable 3: The network architecture of Encoder Es.\nthe multi-scale discriminator architecture. The speciﬁc net-\nwork structure parameters will be given in detail in the table\nbelow. I, O, K, P, and S denote the number of input channels,\nthe number of output channels, a kernel size, a padding size,\nand a stride size, respectively. The network architecture of\nEncoder Ec and Em has been shown in table 2. The network\narchitecture of Encoder Es has been shown in table 3 The\nnetwork architecture of FF module has been shown in table\n4 The network architecture of Encoder Dechas been shown\nin table 5\nMakeup Removal\nThe makeup image as the target images and the non-makeup\nimages as the reference images, then feed them into SSAT,\nthe makeup removal results can be obtained, see Figure 13.\nIn addition, more of our makeup transfer results are shown\nin Figure 15.\nMakeup Style Interpolation\nAdjusting the shade of makeup style is an essential func-\ntion of existing makeup applications. Due to the separation\nof makeup features, our method could generate continuous\nmakeup transfer results by interpolating makeup features,\nsee Figure 14 . The formula is described as follows:\n^yadjust\nt = Dec(xc\nt;(^ym1\nr ·\u000b1 + ^ym2\nr ·\u000b2)) (15)\nwhere \u000b1 + \u000b2 = 1 . The closer \u000b1 is to 1, the closer the\nresulting makeup style is to ^ym1\nr . The closer \u000b2 is to 1, the\ncloser the resulting makeup style is to ^ym2\nr .\n2332\nLayer FF\nL1\nConv(I:384,O:512,K:3,P:1,S:2),\nInstance Normalization,\nLeaky ReLU:0.2\nL2\nConv(I:512,O:512,K:3,P:1,S:1),\nInstance Normalization,\nLeaky ReLU:0.2\nL3\nConnect [L1, L2, L3 of Ec,\nL3 of Es,\nL1, L2 of FF]\nTable 4: The network architecture of FF module.\nLayer Dec\nL1\nUpsample:2,\nSPADE,\nResnet\nL2\nUpsample:2,\nSPADE,\nResnet\nL3 SPADE,\nResnet\nL4 Conv(I:64,O:3,K:7,P:3,S:1),\ntanh\nTable 5: The network architecture of Encoder Dec.\nFigure 13: Our makeup removal results. The ﬁrst column\nis three makeup target images, the ﬁrst row is three non-\nmakeup reference images, the makeup removal results are\ndisplayed in the lower right corner.\nFigure 14: Results of makeup style interpolation. The ﬁrst\nfour rows have one reference image, and the last four rows\nhave two. The makeup styles of the interpolated images con-\ntinuously transfer from reference 1 to reference 2.\nFigure 15: More of our makeup transfer results. The ﬁrst\ncolumn is three target images, the ﬁrst row is ﬁve reference\nimages, the semantic correspondence results and the makeup\ntransfer results are displayed in the lower right corner..\n2333\nReferences\nChang, H.; Lu, J.; Yu, F.; and Finkelstein, A. 2018. Paired-\nCycleGAN: Asymmetric Style Transfer for Applying and\nRemoving Makeup. In Computer Vision and Pattern Recog-\nnition, 40–48.\nChen, H.-J.; Hui, K.-M.; Wang, S.-Y .; Tsao, L.-W.; Shuai,\nH.-H.; and Cheng, W.-H. 2019. BeautyGlow: On-Demand\nMakeup Transfer Framework With Reversible Generative\nNetwork. In Computer Vision and Pattern Recognition.\nDeng, H.; Han, C.; Cai, H.; Han, G.; and He, S. 2021.\nSpatially-Invariant Style-Codes Controlled Makeup Trans-\nfer. In Computer Vision and Pattern Recognition, 6549–\n6557.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative Adversarial Networks. In Advances in\nNeural Information Processing Systems, 2672–2680.\nGu, Q.; Wang, G.; Chiu, M. T.; Tai, Y .-W.; and Tang, C.-K.\n2019. LADN: Local Adversarial Disentangling Network for\nFacial Makeup and De-Makeup. In International Confer-\nence on Computer Vision, 10481–10490.\nGuo, D.; and Sim, T. 2009. Digital face makeup by example.\nIn Computer Vision and Pattern Recognition, 73–79.\nHe, M.; Chen, D.; Liao, J.; Sander, P. V .; and Yuan, L. 2018.\nDeep Exemplar-based Colorization. In International Con-\nference on Computer Graphics and Interactive Techniques.\nHuang, X.; Liu, M.-Y .; Belongie, S.; and Kautz, J. 2018.\nMultimodal Unsupervised Image-to-Image Translation. In\nEuropean Conference on Computer Vision, 179–196.\nHuang, Z.; Zheng, Z.; Yan, C.; Xie, H.; Sun, Y .; Wang, J.;\nand Zhang, J. 2020. Real-World Automatic Makeup via\nIdentity Preservation Makeup Net. In International Joint\nConference on Artiﬁcial Intelligence.\nJiang, W.; Liu, S.; Gao, C.; Cao, J.; He, R.; Feng, J.; and\nYan, S. 2020. PSGAN: Pose and Expression Robust Spatial-\nAware GAN for Customizable Makeup Transfer. InComput-\ner Vision and Pattern Recognition, 5194–5202.\nKarras, T.; Laine, S.; and Aila, T. 2018. A Style-Based Gen-\nerator Architecture for Generative Adversarial Networks. In\nComputer Vision and Pattern Recognition, 4401–4410.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2017. Im-\nageNet classiﬁcation with deep convolutional neural net-\nworks. Communications of The ACM, 60: 84–90.\nLee, H.-Y .; Tseng, H.-Y .; Huang, J.-B.; Singh, M. K.; and\nYang, M.-H. 2018. Diverse Image-to-Image Translation via\nDisentangled Representations. In European Conference on\nComputer Vision, 36–52.\nLee, J.; Kim, E.; Lee, Y .; Kim, D.; Chang, J.; and Choo, J.\n2020. Reference-Based Sketch Image Colorization Using\nAugmented-Self Reference and Dense Semantic Correspon-\ndence. In Computer Vision and Pattern Recognition, 5800–\n5809.\nLi, C.; Zhou, K.; and Lin, S. 2015. Simulating makeup\nthrough physics-based manipulation of intrinsic image lay-\ners. In Computer Vision and Pattern Recognition, 4621–\n4629.\nLi, T.; Qian, R.; Dong, C.; Liu, S.; Yan, Q.; Zhu, W.; and Lin,\nL. 2018. BeautyGAN: Instance-level Facial Makeup Trans-\nfer with Deep Generative Adversarial Network. In ACM\nMultimedia, 645–653.\nLiao, J.; Yao, Y .; Yuan, L.; Hua, G.; and Kang, S. B. 2017.\nVisual attribute transfer through deep image analogy. In\nACM Transactions on Graphics, volume 36.\nLiu, S.; Jiang, W.; Gao, C.; He, R.; Feng, J.; Li, B.; and\nYan, S. 2021. PSGAN++: Robust Detail-Preserving Makeup\nTransfer and Removal. In ArXiv, abs/2105.12324.\nLyu, Y .; Dong, J.; Peng, B.; Wang, W.; and Tan, T. 2021. SO-\nGAN: 3D-Aware Shadow and Occlusion Robust GAN for\nMakeup Transfer. In ArXiv, abs/2104.10567.\nMao, X.; Li, Q.; Xie, H.; Lau, R. Y . K.; Wang, Z.; and S-\nmolley, S. P. 2017. Least Squares Generative Adversarial\nNetworks. In International Conference on Computer Vision,\n2813–2821.\nNguyen, T.; Tran, A. T.; and Hoai, M. 2021. Lipstick Ain’t\nEnough: Beyond Color Matching for In-the-Wild Make-\nup Transfer. In Computer Vision and Pattern Recognition,\n13305–13314.\nPark, T.; Liu, M.-Y .; Wang, T.-C.; and Zhu, J.-Y . 2019. Se-\nmantic Image Synthesis with Spatially-Adaptive Normaliza-\ntion. In Computer Vision and Pattern Recognition, 2337–\n2346.\nSarfraz, M. S.; Seibold, C.; Khalid, H.; and Stiefelhagen, R.\n2019. Content and Colour Distillation for Learning Image\nTranslations with the Spatial Proﬁle Loss. In British Ma-\nchine Vision Conference, 287.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nInternational Conference on Learning Representations.\nSun, Z.; Liu, F.; Liu, R. W.; Xiong, S.; and Liu, W. 2020.\nLocal Facial Makeup Transfer via Disentangled Representa-\ntion. In Asian Conference on Computer Vision.\nTong, W.-S.; Tang, C.-K.; Brown, M. S.; and Xu, Y .-Q. 2007.\nExample-Based Cosmetic Transfer. In Paciﬁc Conference\non Computer Graphics and Applications, 211–218.\nWan, Z.; Chen, H.; Zhang, J.; Jiang, W.; Yao, C.; and Luo, J.\n2021. Facial Attribute Transformers for Precise and Robust\nMakeup Transfer. In ArXiv, abs/2104.02894.\nYu, C.; Wang, J.; Peng, C.; Gao, C.; Yu, G.; and Sang, N.\n2018. BiSeNet: Bilateral Segmentation Network for Real-\nTime Semantic Segmentation. In European Conference on\nComputer Vision, 334–349.\nZhang, P.; Zhang, B.; Chen, D.; Yuan, L.; and Wen,\nF. 2020. Cross-Domain Correspondence Learning for\nExemplar-Based Image Translation. InComputer Vision and\nPattern Recognition, 5143–5153.\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\npaired Image-to-Image Translation Using Cycle-Consistent\nAdversarial Networks. In International Conference on Com-\nputer Vision.\n2334",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7040849924087524
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6754997968673706
    },
    {
      "name": "Transformer",
      "score": 0.6459336280822754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5476677417755127
    },
    {
      "name": "Semantic feature",
      "score": 0.47364604473114014
    },
    {
      "name": "Focus (optics)",
      "score": 0.43254554271698
    },
    {
      "name": "Computer vision",
      "score": 0.3853268027305603
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.328110933303833
    },
    {
      "name": "Engineering",
      "score": 0.10167589783668518
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}