{
  "title": "Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis",
  "url": "https://openalex.org/W4221151672",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1913798565",
      "name": "Zhang Kai",
      "affiliations": [
        "Hefei University of Technology",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1825159377",
      "name": "ZHANG-Kun",
      "affiliations": [
        "University of Science and Technology of China",
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2353742199",
      "name": "Zhang Mengdi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745764610",
      "name": "Zhao, Hongke",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2108859322",
      "name": "Liu Qi",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2086655376",
      "name": "Wu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A167579018",
      "name": "Chen, Enhong",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035740499",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2251792193",
    "https://openalex.org/W2963168371",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W3159117141",
    "https://openalex.org/W3100060077",
    "https://openalex.org/W2260776682",
    "https://openalex.org/W2916076862",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2904664992",
    "https://openalex.org/W2052935438",
    "https://openalex.org/W2576562514",
    "https://openalex.org/W2963240575",
    "https://openalex.org/W2950090767",
    "https://openalex.org/W2963909901",
    "https://openalex.org/W2743904806",
    "https://openalex.org/W2529550020",
    "https://openalex.org/W2171008261",
    "https://openalex.org/W3209266125",
    "https://openalex.org/W3176719207",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2789132801",
    "https://openalex.org/W2964164368",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W3108490213",
    "https://openalex.org/W2113125055",
    "https://openalex.org/W2963855931",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2068956026",
    "https://openalex.org/W2252057809",
    "https://openalex.org/W2251124635",
    "https://openalex.org/W2144608574",
    "https://openalex.org/W2971220558",
    "https://openalex.org/W3167287584",
    "https://openalex.org/W2953874899",
    "https://openalex.org/W2966157634",
    "https://openalex.org/W4236120890",
    "https://openalex.org/W3034444624",
    "https://openalex.org/W3127623408",
    "https://openalex.org/W2030774563",
    "https://openalex.org/W2997141632",
    "https://openalex.org/W1996815492",
    "https://openalex.org/W2949161734",
    "https://openalex.org/W2891778157",
    "https://openalex.org/W2971088231",
    "https://openalex.org/W3004314094",
    "https://openalex.org/W3115080939",
    "https://openalex.org/W2158935941",
    "https://openalex.org/W2970157301",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W2971087444",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2970583420",
    "https://openalex.org/W3113871136"
  ],
  "abstract": "Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3599 - 3610\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nIncorporating Dynamic Semantics into Pre-Trained Language Model for\nAspect-based Sentiment Analysis\nKai Zhang1, Kun Zhang2, Mengdi Zhang3, Hongke Zhao4, Qi Liu1,∗\nWei Wu3, Enhong Chen1\n1 School of Data Science, University of Science and Technology of China\n2 School of Computer Science and Information Engineering, Hefei University of Technology\n3 Meituan; 4 College of Management and Economics, Tianjin University\nkkzhang0808@mail.ustc.edu.cn; {qiliuql,cheneh}@ustc.edu.cn\n{zhang1028kun,wuwei19850318,mdz}@gmail.com; hongke@tju.edu.cn\nAbstract\nAspect-based sentiment analysis (ABSA) pre-\ndicts sentiment polarity towards a speciﬁc as-\npect in the given sentence. While pre-trained\nlanguage models such as BERT have achieved\ngreat success, incorporating dynamic semantic\nchanges into ABSA remains challenging. To\nthis end, in this paper, we propose to address\nthis problem by Dynamic Re-weighting BERT\n(DR-BERT), a novel method designed to learn\ndynamic aspect-oriented semantics for ABSA.\nSpeciﬁcally, we ﬁrst take the Stack-BERT lay-\ners as a primary encoder to grasp the overall\nsemantic of the sentence and then ﬁne-tune it\nby incorporating a lightweight D ynamic R e-\nweighting Adapter (DRA). Note that the DRA\ncan pay close attention to a small region of the\nsentences at each step and re-weigh the vitally\nimportant words for better aspect-aware senti-\nment understanding. Finally, experimental re-\nsults on three benchmark datasets demonstrate\nthe effectiveness and the rationality of our pro-\nposed model and provide good interpretable in-\nsights for future semantic modeling.\n1 Introduction\nAspect-based sentiment analysis is a branch of sen-\ntiment analysis, which aims to identify sentiment\npolarity of the speciﬁc aspect in a sentence (Jiang\net al., 2011). For example, given a sentence “The\nrestaurant has attentive service, but the food is\nterrible. ”, the task aims to predict the sentiment\npolarities towards “service” and “food”, which\nshould be positive and negative respectively.\nAs a fundamental technology, the ABSA task\nhas broad applications, such as recommender sys-\ntem (Chin et al., 2018; Zhang et al., 2021b) and\nquestion answering (Wang et al., 2019). Therefore,\na great amount of research has been attracted from\nboth academia and industry. Among them, deep\nneural networks (DNN) (Nguyen and Shirai, 2015;\n∗ Corresponding author.\nTang et al., 2015, 2016; Zheng et al., 2020), at-\ntention mechanism (Wang et al., 2016; Ma et al.,\n2017) and graph neural/attention networks (Huang\nand Carley, 2019; Zhang et al., 2019a; Wang et al.,\n2020) have signiﬁcantly improved the performance\nthrough deep feature alignment between the aspect\nrepresentations and context representations.\nRecently, the large-scaled pre-trained language\nmodels, such as Bidirectional Encoder Represen-\ntations from Transformers (BERT) (Devlin et al.,\n2019), realize a breakthrough for improving many\nlanguage tasks, which further attracts considerable\nattention to enhance the semantic representations.\nIn ABSA, Xu et al. (2019a) designed BERT-PT,\nwhich explores a novel post-training approach on\nthe BERT model. Song et al. (2019) further pro-\nposed a text pair classiﬁcation model BERT-SPC,\nwhich prepares the input sequence by appending\nthe aspects into the contextual sentence. Although\ngreat success has been achieved by the above stud-\nies, some critical problems remain when directly\napplying attention mechanisms or ﬁne-tuning the\npre-trained BERT in the task of ABSA.\nSpeciﬁcally, most of the existing approaches se-\nlect all the important words from a contextual sen-\ntence at one time. However, according to neuro-\nscience studies, the essential words during seman-\ntic comprehension are dynamically changing with\nthe reading process and should be repeatedly con-\nsidered (Kuperberg, 2007; Tononi, 2008; Brouwer\net al., 2021). For example, when judging the senti-\nment polarity of the aspect “system memory” in a\nreview sentence “It could be a perfect laptop if it\nwould have faster system memory and its radeon\nwould have DDR5 instead of DDR3” , the impor-\ntant words should change from general sentiment\nwords {“faster”, “perfect”, “laptop” } into aspect-\naware words {“would have”, “faster”, “could”,\n“be”, “perfect” }. Through these dynamic changes,\nthe sentiment polarity will change from positive to\nthe ground truth sentiment label negative.\n3599\nMeanwhile, simply initializing the encoder with\na pre-trained BERT does not effectively boost the\nperformance in ABSA as we expected (Huang and\nCarley, 2019; Xu et al., 2019a; Wang et al., 2020).\nOne possible reason could be that training on two\nspeciﬁc tasks, i.e., Next Sentence Prediction and\nMasked LM, with rich resources leads to better\nsemantic of the overall sentences. However, the\nABSA task is conditional, which means the model\nneeds to understand the regional semantics of sen-\ntences by fully considering the given aspect. For\ninstance, BERT tends to understand the global sen-\ntiment of the above sentence “It could be a perfect\nlaptop ... of DDR3” regardless of which aspect is\ngiven. But in ABSA, the sentence is more likely\nto be different sentiment meanings for different as-\npects (e.g., negative for “system memory” while\npositive for“DDR5”). Therefore, the vanilla BERT\nis hardly to pay closer attention to relevant informa-\ntion for the speciﬁc aspect, especially when there\nare multiple aspects in one sentence.\nTo equip the pre-trained models with the abil-\nity to capture the aspect-aware dynamic semantics,\nwe present a Dynamic Re-weighting BERT (DR-\nBERT) model, which considers the aspect-aware\ndynamic semantics in a pre-trained learning frame-\nwork. Speciﬁcally, we ﬁrst take the Stack-BERT\nlayers as primary sentence encoder to learn overall\nsemantics of the whole sentences. Then, we devise\na Dynamic Re-weighting Adapter (DRA), which\naims to pay most careful attention to a small region\nof the contextual sentence and dynamically select\nand re-weight one critical word at each step for bet-\nter aspect-aware sentiment understanding. Finally,\nto overcome the limitation of vanilla BERT men-\ntioned above, we incorporate the light-weighted\nDRA into each BERT encoder layer and ﬁne-tune\nit to adapt to the ABSA task. We conduct extensive\nexperiments on three widely-used datasets where\nthe results demonstrate the effectiveness, rational-\nity and interpretability of the proposed model.\n2 Related Work\n2.1 Aspect-based Sentiment Analysis\nAspect-based sentiment analysis identiﬁes speciﬁc\naspect’s sentiment polarity in the sentence. Some\napproaches (Ding and Liu, 2007; Jiang et al., 2011;\nKiritchenko et al., 2014) designed numerous rules-\nbased models for ABSA. For example, Ding and\nLiu (2007) ﬁrst performed dependency parsing to\ndetermine sentiment polarity about the aspects.\nIn recent years, most research studies make use\nof the attention mechanism to learn the word’s se-\nmantic relation (Tang et al., 2015, 2016; Wang\net al., 2016; Ma et al., 2017; Xing et al., 2019;\nLiang et al., 2019; Zhang et al., 2021a). Among\nthem, Wang et al. (2016) proposed an attention-\nbased LSTM to identify important information re-\nlating to the aspect. Ma et al. (2017) developed\nan interactive attention to model the aspect and\nsentence interactively. Fan et al. (2018) deﬁned\na multi-grained network to link the words from\naspect and sentence. Li et al. (2018) designed a\ntarget-speciﬁc network to integrate aspect informa-\ntion into sentence. Tan et al. (2019) introduced a\ndual attention to distinguish conﬂicting opinions.\nIn addition, another research trend is to leverage\nsyntactic knowledge to learn syntax-aware features\nof the aspect (Tang et al., 2019; Huang and Car-\nley, 2019; Zhang et al., 2019a; Sun et al., 2019;\nWang et al., 2020; Tang et al., 2020; Chen et al.,\n2020; Li et al., 2021; Tian et al., 2021). For ex-\nample, Tang et al. (2020) developed dependency\ngraph enhanced dual-transformer network to fuse\nthe ﬂat representations. More recently, pre-trained\nmethods have been proved remarkably successful\nin the ABSA task. Song et al. (2019) devised an at-\ntentional encoder and a BERT-SPC model to learn\nfeatures between aspect and context. Wang et al.\n(2020) reshaped the dependency trees and proposed\na relational graph attention network to encode the\nsyntax relation feature. Tian et al. (2021) explicitly\nutilize dependency types with a type-aware graph\nnetworks to learn aspect-aware relations.\nHowever, these methods largely ignore the pro-\ncedure of dynamic semantic comprehension (Ku-\nperberg, 2007; Kuperberg and Jaeger, 2016; Wang\net al., 2017; Zhang et al., 2019c; Brouwer et al.,\n2021) and can not fully reveal dynamic semantic\nchanges of the aspect-related words. Thus, it’s hard\nfor ABSA models to achieve the same performance\nas human-level sentiment understanding.\n2.2 Human Semantic Comprehension\nActually, no matter in the early days or now, im-\nitating the procedure of human semantic compre-\nhension has always been one of the original inten-\ntion of many studies (Bezdek, 1992; Wang et al.,\n2017; Zheng et al., 2019; Li et al., 2019; Zhang\net al., 2019d; Peng et al., 2020; Golan et al., 2020),\nsuch as machine reading comprehension (Zhang\net al., 2019d; Peng et al., 2020), visual object detect-\ning (Spampinato et al., 2017) and relevance estima-\n3600\nFigure 1: An illustration of the proposed framework. The blue blocks constitute a pre-trained BERT model which\nare frozen during ﬁne-tuning, and the right block represents the dynamic re-weighting adapter that is inserted after\neach BERT encoder layer and trained during ﬁne-tuning. Moreover, Sand Arepresent the sentence sequence and\nthe aspect sequence respectively. N indicates the number of layers of the BERT encoder.\ntion (Li et al., 2019). For example, attention mecha-\nnism (Vaswani et al., 2017) has a widespread inﬂu-\nence, which allows the model to focus on important\nparts of the input as human’s attention. Spampinato\net al. (2017) aimed to learn human–based features\nvia brain-based visual object. Wang et al. (2017)\nbuilt a dynamic attention model to model human\npreferences for article recommendation.\nMoreover, some psychologists and psycholin-\nguists have also done many research on the mecha-\nnisms of human semantic comprehension (Kuper-\nberg, 2007; Kuperberg and Jaeger, 2016; Brouwer\net al., 2021). Speciﬁcally, some scholars (Yang and\nMcConkie, 1999; Rayner, 1998) found that most\npeople may focus on 1.5 words. Moreover, Koch\nand Tsuchiya (2007) and Tononi (2008) assumed\nthat people can only remember the meaning of\nabout 7 to 9 words at each time. The phenomenons\nindicate that most people only focused on a small\nregion of the sentence at one time and need to re-\npeatedly process important parts for better semantic\nunderstanding (Sharmin et al., 2015).\nInspired by the above research and linguistic psy-\nchology theories, in this paper, we explore aspect-\naware semantic changes of the ABSA task by incor-\nporating the procedure of dynamic semantic com-\nprehension into the pre-trained language model.\n3 Dynamic Re-weighting BERT\nIn this section, we introduce the technical detail of\nDR-BERT. Speciﬁcally, we start with the problem\ndeﬁnition, followed by an overall architecture of\nDR-BERT as illustrated in Figure 1.\nProblem Deﬁnition In ABSA, a sentence-aspect\npair (S,A) is given. In this paper, the sentence is\nrepresented as S = {ws\n1,ws\n2,...,w s\nls }which con-\nsists of a series of ls words. The speciﬁc aspect\nis denoted as A = {wa\n1,wa\n2,...,w a\nla }which is a\npart of S. la is the length of aspect words. The\ngoal of ABSA is to learn a sentiment classiﬁer that\ncan precisely predict the sentiment polarity of sen-\ntence Sfor speciﬁc aspect A. As the aspect-related\ninformation plays a key role in the prediction (Li\net al., 2018; Zheng et al., 2020), this paper aims\nto dynamically select and encode the aspect-aware\nsemantic information through the proposed model.\nOverall Architecture DR-BERT mainly contains\ntwo components (i.e., BERT encoder and Dynamic\nRe-weighting Adapter), together with two modules\n(i.e., the embedding module and sentiment predic-\ntion module). The technical details of each part\nwill be elaborated on as follows.\n3.1 Embedding Module\nTo represent semantic information of the aspect\nwords and context words better, we ﬁrst map each\nword into a low-dimensional vector. Speciﬁcally,\nthe inputs of DR-BERT are the sentence sequence\nand the corresponding aspect sequence. For the\nsentence sequence, we construct the BERT input\nas “[CLS]” + sentence +“[SEP]” and the sentence\nS = {ws\n1,ws\n2,...,w ls }can be transformed into\nthe hidden states s = {si |i= 1,2,...,l s}with\nBERT embedding. For aspect sequences, we adopt\nthe same method to get the representation vector\nof each word. Thus, through the embedding mod-\nule, the aspect sequence A= {wa\n1,wa\n2,...,w a\nla }is\nmapped to as = {aj |j = 1,2,...,l a}. Note that,\nif the aspect sequence is a single word like “food”,\nthe aspect representation is the embedding of the\n3601\nsingle word “food”. While for the cases where the\nsequence contains multiple words such as “system\nmemory”, the aspect representation is the average\nof each word embedding (Sun et al., 2015). We can\ndenote the aspect embedding process as:\na =\n\n\n\na1, if la = 1 ,\n(∑la\nj=1 aj)/la , if la >1 ,\n(1)\nwhere aj is the embedding of word jin the aspect\nsequence. a denotes the embedding of the aspect.\n3.2 BERT Encoder\nThe architecture of BERT (Devlin et al., 2019) is\nakin to the Transformer (Vaswani et al., 2017). For\nsimplicity, we omit some architecture details such\nas position encoding, layer normalization (Xu et al.,\n2019b) and residual connections (He et al., 2016).\n1) Multi-head Self-attention Mechanism.In re-\ncent years, the multi-head self-attention mechanism\n(MultiHead) has received a wide range of applica-\ntions in natural language processing. In the pa-\nper, we adopt MultiHead with hheads to obtain\nthe overall semantics of the whole sentence. The\nproduct from each self-attention network is then\nconcatenated and ﬁnally transformed as:\nm = {mi |i= 1,2,...,l s}\n= MultiHead(sWQ\nh ,sWK\nh ,sWV\nh ),\n(2)\nwhere hdenotes the h-th attention head, WQ\ni , WK\ni\nand WV\ni are learnable parameters. Finally, the out-\nput feature is m = {mi |i= 1,2,...,l s}. For\ndetailed implementation of MultiHead, please re-\nfer to Transformer (Vaswani et al., 2017).\n2) Position-wise Feed-Forward Network.Since\nthe multi-head attention is a series of linear trans-\nformations, we then apply the position-wise feed-\nforward network (FFN) to learn the feature’s non-\nlinear transformation. Speciﬁcally, the FFN con-\nsists of two linear transformations along with a\nReLU activation in between. More formally:\nf = {fi |i= 1,2,...,l s}\n= max(0,mW1 + b1)W2 + b2, (3)\nwhere W1, b1, W2 and b2 are learnable parame-\nters in the linear transformations.\nSo far, with the input S = {ws\n1,ws\n2,...,w s\nls }, we\nobtain the hidden states f = {fi |i= 1,2,...,l s}\nvia the BERT encoder. Then, for the words’ hidden\nstates of the sentence from FFN, we utilize the max-\npooling operation to fairly select crucial features in\nthe sentence (Lai et al., 2015; Zhang et al., 2019b),\nso as to obtain the original sentence representation\nhs at the beginning of each re-weighting step:\nhs = Max_Pooling(fi |i= 1,2,...,l s). (4)\n3.3 Dynamic Re-weighting Adapter (DRA)\nThe currently attention mechanism in deep learning\nis essentially similar to the selective visual attention\nof human beings (Vaswani et al., 2017; You et al.,\n2016). However, as for the text semantic under-\nstanding, human brain will discover the intentional\nrelationship of words at a sentential level (Taatgen\net al., 2007; Sha et al., 2016; Sen et al., 2020) and\nlink the incoming semantic information with pre-\nexisting information stored within memory. Thus,\nwe design a dynamic re-weighting adapter (DRA)\nwhich can dynamically emphasize the important\naspect-aware words for the ABSA task.\nAs shown in the right part of Figure 1, based\non overall semantics of the whole sentence, DRA\nfurther selects the most important word at each\nstep with consideration of the speciﬁc aspect rep-\nresentation. Speciﬁcally, the inputs of DRA are\nthe ﬁnal outputs of the BERT encoder (i.e., hs)\nand the original aspect embedding (i.e., a). In\neach step, we ﬁrst utilize re-weighting attention to\nchoose the word for current input from the input\nsequence ({si |i= 1,2,...,l s}). Then, we utilize\nGated Recurrent Unit (GRU)(Cho et al., 2014) to\nencode the chosen word and update the semantic\nrepresentation of the review sentence.\nFormally, we regard the calculation process as:\nat = F([s1,s2,..., sls ] ,ht−1,a) ,\nht = GRU(at,ht−1) , t ∈[1,T] (5)\nwhere a is the original embedding vector of the\naspect words. at is the output of re-weighting func-\ntion F. T denotes the dynamic re-weighting length\nover the sentences, which represents the cognitive\nthreshold of human beings. h0 = hs is the initial\nstate and hT is the output hidden states of DRA.\n1) The Re-weighting Function.More speciﬁ-\ncally, we utilize the attention mechanism to achieve\nthe re-weighting function F, which aims to select\nthe most important aspect-related word at each step.\nThe calculation can be formulated as:\nS = [s1,s2,..., sls ] ,\nM = WsS + (Wdht−1 + Waa) ⊗w,\nm = ωT tanh (M) ,\n(6)\n3602\nwhere S denotes the original sentence embedding,\nM is the fusion representation of the aspects and\nthe sentences. Ws, Wd, Wa and ωare trainable\nparameters. w ∈Rls is a row vector of 1 and ⊗\ndenotes the outer product.\nSubsequently, to better encode aspect-aware se-\nmantics, we choose the most important word (i.e.,\none word) at each step for the speciﬁc aspect.\nαi = exp (mi)∑ls\nk=1 exp (mk)\n,\nat = sj,(j = Index(max(αi)))\n(7)\nwhere mi and αi are the hidden state and the atten-\ntion score of i-th word in the sentence. at is the\nchosen word which is most related to the speciﬁc\naspect at t-th step. However, Index(max(·)) oper-\nation has no derivative, which means its gradient\ncould not be calculated. Inspired by softmax func-\ntion, we modify the Eq.7 and employ the following\noperation to re-weight the contextual words:\nat =\nls∑\ni=1\nexp (λmi)∑ls\nk=1 exp (λmk)\nsi . (8)\nNote that, we design a hyper-parameter λto en-\nsure our model achieves the above purpose. Specif-\nically, the softmax function can exponentially in-\ncrease or decrease the signal, thereby highlighting\nthe information we want to enhance. Thus, when λ\nis an arbitrarily large value, the attention score of\nthe chosen word is inﬁnitely close to 1, and other\nwords are inﬁnitely close to 0. In this way, the most\nimportant word (i.e., one word) will be extract from\nthe context at each re-weighting step.\n2) The GRU Function.To better encode seman-\ntic of the whole sentence, we also employ GRU to\nfurther imitate the procedure of human semantic\ncomprehension under the speciﬁc context, which is\nconsistent with the process of people adjusting to\na new text based on their understanding behavior.\nTherefore, given a previous vector embedding, the\nhidden vectors of GRU are calculated by receiving\nit as input:\nzt = σ(Wz ·[ht−1,at])\nrt = σ(Wr ·[ht−1,at])\n˜ht = tanh (W ·[rt ∗ht−1,at])\nht = (1 −zt) ∗ht−1 + zt ∗˜ht ,\n(9)\nwhere σis the logistic sigmoid function. zt and rt\ndenote the update gate and reset gate respectively\nat the time step t.\nDatasets\n#Positive #Negative #Neural #L #M\nTrain Test Train Test Train Test\nRestaurant 2164 728 807 196 637 196 20 45.5\nLaptop 994 341 870 128 464 169 19 36.5\nTwitter 1561 173 1560 173 3127 346 16 10.2\nTable 1: The statistics of three benchmark datasets. #L\nis the average length of sentences. #M is the proportion\n(%) of samples with multiple (i.e., more than 1) aspects.\n3.4 Sentiment Predicting\nAfter applying BERT layers and DRA on the input\nsentence, its root representation (i.e., s) is convert\ninto the feature representation e:\ne = {ei |i= 1,2,...,l s}\n= (Wef + UehT + be) , (10)\nwhere We, Ue and be are trainable parameters. Af-\nter N-th stacked BERT layers, we obtain the ﬁnal\nrepresentation of the sentence (i.e., eN ). Then, we\nfeed it into a Multilayer Perceptron (MLP) and map\nit to the probabilities over the different sentiment\npolarities via a softmax layer:\nRl = Relu(WlRl−1 + bl) ,\nˆy = softmax (WoRh + bo) , (11)\nwhere Wl, Wo , bl and bo are learned parameters.\nRl is the hidden state of l-th layer MLP (R0 = eN ,\nl ∈[1,h]). Rh is the state of ﬁnal layer which\nis also regard as the output of the MLP. ˆy is the\npredicted sentiment polarity distribution.\n3.5 Model Training\nFinally, we applies the cross-entropy loss function\nfor model training:\nL= −\nM∑\ni=1\nC∑\nj=1\nyj\ni log\n(\nˆyj\ni\n)\n+ β∥Θ∥2\n2 , (12)\nwhere yj\ni is the ground truth sentiment polarity. C\nis the number of labels (i.e, 3 in our task). M is the\nnumber of training samples. Θ corresponds to all\nof the trainable parameters.\n4 Experiment\n4.1 Datasets\nWe mainly conduct experiments on three bench-\nmark ABSA datasets, including “Laptop”, “Restau-\nrant” (Pontiki et al., 2014) and “Twitter” (Dong\net al., 2014). Each data item is labeled with three\n3603\nCategory\nMethods\nDatasets Laptop Restaurant Twitter\nAccuracy F1-score Accuracy F1-score Accuracy F1-score\nAttention.\nATAE-LSTM (Wang et al., 2016) 68.57 64.52 76.58 67.39 67.27 66.43\nIAN (Ma et al., 2017) 70.84 65.73 76.88 68.36 68.74 67.61\nMemNet (Tang et al., 2016) 72.32 67.03 78.12 68.99 70.19 68.22\nAOA (Huang et al., 2018) 74.56 68.77 79.42 70.43 71.68 69.25\nMGNet (Fan et al., 2018) 75.37 71.26 81.28 72.07 72.54 70.78\nTNet (Li et al., 2018) 76.54 71.75 80.69 71.27 74.93 73.60\nPre-trained.\nBERT (Devlin et al., 2019) 77.29 73.36 82.40 73.17 73.42 72.17\nBERT-PT (Xu et al., 2019a) 78.07 75.08 84.95 76.96 – –\nBERT-SPC (Song et al., 2019) 78.99 75.03 84.46 76.98 74.13 72.73\nAEN-BERT (Song et al., 2019) 79.93 76.31 83.12 73.76 74.71 73.13\nRGAT-BERT (Wang et al., 2020) 78.21 74.07 86.60 81.35 76.15 74.88\nT-GCN (Tian et al., 2021) 80.88 77.03 86.16 79.95 76.45 75.25\nOurs. DR-BERT 81.45 78.16 87.72 82.31 77.24 76.10\nTable 2: Experimental results (%) in three benchmark datasets. We underline the best performed baseline.\nsentiment polarities (i.e., positive, negative and neu-\ntral). The statistics of the datasets are presented in\nTable 1. Moreover, we follow the dataset conﬁgu-\nrations of previous studies strictly. For all datasets,\nwe randomly sample 10% items from the training\nset and regard them as the development set.\n4.2 Hyperparameters Settings\nIn the implementation, we build our framework\nbased on the ofﬁcial bert-base models (nlayers=12,\nnheads=12, nhidden=768). The hidden size of GRUs\nand re-weighting length of DRA are set to 256 and\n7. The learning rate is tuned amongst [2e-5, 5e-5\nand 1e-3] and the batch size is manually tested in\n[16, 32, 64, 128]. The dropout rate is set to 0.2.\nThe hyper-parameter l , β and λhave been care-\nfully adjusted, and ﬁnal values are set to 3, 0.8 and\n100 respectively. The model is trained using the\nAdam optimizer and evaluated by two widely used\nmetrics. The parameters of baseline models are in\naccordance with the default conﬁguration of the\noriginal paper. We run our model three times with\ndifferent seeds and report the average performance.\n4.3 Baselines\n• Attention-based Models: MemNet (Tang\net al., 2016), ATAE-LSTM (Wang et al.,\n2016), IAN (Ma et al., 2017), AOA (Huang\net al., 2018), MGNet (Fan et al., 2018),\nTNet (Li et al., 2018).\n• Pre-trained Models: Fine-tune BERT (De-\nvlin et al., 2019), BERT-PT (Xu et al.,\n2019a), BERT-SPC, AEN-BERT (Song et al.,\n2019), RGAT-BERT (Wang et al., 2020), T-\nGCN (Tian et al., 2021).\nThe baseline methods have comprehensive cov-\nerage of the recent related SOTA models recently.\nMost of them are detailed in Section 2.1. For space-\nsaving, we do not detail them in this section.\n4.4 Experimental Results\nFrom the results in Table 2, we have the follow-\ning observations. First, BERT-based methods beat\nmost of the attention-based methods (e.g., IAN and\nTNet) in both metrics. The phenomenon indicates\nthe powerful ability of the pre-trained language\nmodels. That is also why we adopt BERT as base\nencoder to learn the overall semantic representation\nof the whole sentences.\nSecond, by comparing non-speciﬁc BERT mod-\nels (i.e., BERT and BERT-PT) with task-speciﬁc\nmodels (e.g., RGAT-BERT) for ABSA, we ﬁnd that\nthe task-speciﬁc BERT models perform better than\nthe non-speciﬁc models. Speciﬁcally, we can also\nobserve the performance trend that T-GCN&RGAT-\nBERT >AEN-BERT>BERT-PT>BERT, which is\nconsistent with the previous assumption that aspect-\nrelated information is the crucial inﬂuence factor\nfor the performance of the ABSA model.\nFinally, despite the outstanding performance of\nprevious models, our DR-BERT still outperforms\nthe most advanced baseline (i.e., T-GCN or RGAT-\nBERT) no matter in terms of Accuracy or F1-score.\nThe results demonstrate the effectiveness of the dy-\nnamic modeling strategy based on the procedure of\nsemantic comprehension. Meantime, it also indi-\ncates that our proposed DRA can better grasp the\naspect-aware semantics of the sentence than other\nBERT plus-in components in previous methods.\n3604\nModel Variants\nLaptop\nAccuracy F1-score\nBERT-Base 77.29 73.36\n(1): + MLP 77.94 74.42\n(2): + DRA 80.66 77.13\n(3): + DRA on top 3 layers 78.64 75.16\n(4): + DRA on top 6 layers 79.17 75.93\n(5): + DRA on top 9 layers 80.22 76.49\n(6): DR-BERT 81.45 78.16\nTable 3: The ablation study on different components\nwhich conducted on the test set of the Laptop dataset.\n“BERT-Base” indicates the vanilla BERT. “+” indicates\nthe setting with plus-in components.\n2 4 6 8 10\n78\n3 5 7 9\n79\n80\n81\n82\n%\n2 4 6 8 10\n84\n3 5 7 9\n85\n86\n87\n88\n%\n2 4 6 8 10\n74\n3 5 7 9\n75\n76\n77\n78\n%\n2 4 6 8 10\n75\n3 5 7 9\n76\n77\n78\n79\n%\n2 4 6 8 10\n79\n3 5 7 9\n80\n81\n82\n83\n%\n2 4 6 8 10\n73\n3 5 7 9\n74\n75\n76\n77\n%\nLaptop F1\nRestaurant Acc. Twitter Acc.\nTwitter F1Restaurant F1\n(a)  The performance  (Accuracy) in three dataset. \n(b)  The performance  (F1-score) in three dataset. \nLaptop Acc.\nFigure 2: The ablation study on the re-weighting length\nof the adapter. Red lines indicate Accuracy/ F1 scores\nwhile blue and green lines indicate the performance of\nthe best baseline and BERT-base model respectively.\n4.5 Ablation Study\nAblations on the Proposed Components. In\nTable 3, we study the inﬂuence of different com-\nponents in our framework, including the DRA and\nMLPs. We can ﬁnd that without utilizing adapters\nand MLPs, DR-BERT degenerates into the BERT\nmodel, which gains the worst performance among\nall the variants. The phenomenon indicates the ef-\nfective of the DRA and MLP modules. Moreover,\nthrough comparing (1) and (2), we can easily con-\nclude that DRA plays a more crucial role in the\nﬁnal sentiment prediction than MLPs.\nSince BERT models are usually quite deep (e.g.,\n12 layers), we only insert the dynamic re-weighting\nadapter into top layers (i.e., 3-th, 6-th, and 9-th lay-\ners) to further verify the effectiveness of the DRA\nmodule. The results are shown in Table 3 (3), (4),\nand (5). We observe that when introducing adapters\nto the top layers of DR-BERT, our framework still\noutperforms the BERT model, showing that the\nDRA is efﬁcient in encoding the aspect-aware se-\nmantics over the whole sentence. In addition, we\ncan also ﬁnd that the more adapter incorporated\nWhile the $20 entree range is not overly \nexpensive, in New York City, there is definitely \nbetter food in that range, and so Sapphire, despite \nit is lovely atmosphere, will most likely not be a \nrestaurant to which I will return .\nfood, better, while, definitely, not, return, …\n(a) Human Cognition\n(b) DRA Chosen Words\nFigure 3: Comparison of the semantic understanding\nprocess between human reading and DRA when judg-\ning the sentiment polarity of aspect “food”. (a) is the\nvisualization of the human understanding process from\nthe eye tracker†. (b) denotes aspect-aware words from\nre-weighting function.\nin BERT layers the higher performance gained,\nillustrating the importance of modeling the deep\ndynamic semantics over the sentence.\nAblations on the Scale of Adapter. In this sub-\nsection, we investigate the inﬂuence of the scale\nof adapters on different datasets. As shown in Fig-\nure 2, we tune the adapter’s dynamic re-weighting\nlength (T) in a wide range (i.e., 2 to 10). Speciﬁ-\ncally, the performance of DR-BERT ﬁrst becomes\nbetter with the increasing of re-weighting length\nand achieving the best result at around 7. Then, as\nthe length continues to increase, the performance\ncontinues to decline. This phenomenon is consis-\ntent with the psychological ﬁndings that human\nmemory focuses on nearly seven words (Tononi,\n2008; Koch and Tsuchiya, 2007), which further\nindicates the effectiveness of DRA in modeling\nhuman-like (dynamic) semantic comprehension.\nBesides, compared with the best-performed base-\nline (blue lines), our model can achieve better per-\nformance with only 4 or 5 times of re-weighting at\nmost test sets, illustrating the efﬁciency of the re-\nweighting adapter. On the other hand, we can also\nﬁnd that DR-BERT always gives superior perfor-\nmance compared to the BERT-based model (green\nlines), even with the lowest re-weighting length.\nAll those results show that DR-BERT could better\ncomprehend aspect-aware dynamic semantics in\naspect-based sentiment analysis.\n4.6 Interpretability Veriﬁcation\nComparison of Semantic Comprehension. To\nevaluate model rationality and interpretability, we\nconduct an study for dynamic semantic compre-\nhension by eye tracker. As shown in Figure 3 (a),\n†The procedure of the human semantic comprehension is\ngenerated by the eye tracker: https://www.tobiipro.\ncom/product-listing/nano/\n3605\nIt could be a perfect laptop if it would have faster system memory \nand its radeon would have DDR5 instead of DDR3.\nIt could be a perfect laptop if it would have faster system memory       \nand its radeon would have DDR5 instead of DDR3.\nPrediction: Negative\nSystem, memory, faster, would, have…\nPrediction: Positive\nDDR5, would, instead, DDR3…\n“System memory”\nNegative\n“DDR5”\nPositive\nIt could be a perfect laptop if it would have faster system memory \nand its radeon would have DDR5 instead of DDR3.\nPrediction: Negative \nDDR3, instead, of, DDR3…\n“DDR3”\nNegative\nFigure 4: Visualization results of multiple aspects in the same sentence. The blue part indicates the aspect and its\nground truth. The middle subﬁgures represent the procedure of human’s semantic comprehension which is targeted\nat one speciﬁc aspect. The green subﬁgures are the predicted labels and the chosen word sequences from DRA.\nCase Examples. The label in brackets represents ground truth. BERT-base RGAT-BERT DR-BERT\nAspects: “system memory”(Neg.), “DDR5”(Pos.), “DDR3”(Neg.) Pos/Neg/Neg Neg/Pos/Pos Neg/Pos/Neg\nSentence: It could be a perfect laptop if it would have faster system\nmemory and its radeon would have DDR5 instead of DDR3. %/ %/ % \"/ \"/ % \"/ \"/ \"\nAspects: “Supplied software” (Neu.), “software” (Pos.), “Windows” (Neg.) Pos/ Pos/ Pos Pos/Pos/Neu Pos/Pos/Neg\nSentence: Supplied software: The software that comes with this machine\nis greatly welcomed compared to what Windows comes with. %/ \"/ % %/ \"/ % %/ \"/ \"\nAspects: “waiter” (Neg.), “served” (Neg.), “specials” (Pos.) Neg/Neg/Neg Neg/Neg/Neu Neg/Neg/Pos\nSentence: First, the waiter who served us neglected to ﬁll us in on the\nspecials, which I would have chosen had I known about them. \"/ \"/ % \"/ \"/ % \"/ \"/ \"\nTable 4: Error analysis of two review items from laptop and restaurant. The colored words in brackets represents\nground truth sentiment label of the corresponding aspect. The symbol ✓ means the predicting sentiment is correct,\nand the other symbol means the predicting sentiment is wrong.\nwhen a person tries to understand a relatively long\nsentence, he/she ﬁrst read the entire sentence. Sub-\nsequently, after giving a speciﬁc aspect, he/she will\ndynamically select related words based on the pre-\nvious memory state until he/she fully understands\nthe sentiment polarity of the given aspect.\nInterestingly, the above phenomenon is consis-\ntent with our dynamic re-weighting adapter’s cho-\nsen result. Speciﬁcally, as Figure 3 (b) shows, with\nthe re-weighting functionF (i.e., Equation 5 and 6),\nour model dynamically choose the words “food,\nbetter, while, deﬁnitely, not, ... ”, which have proven\nto be very important for predicting the sentiment of\naspect “food” in Figure 3 (a). Those experimental\nresults again fully indicate the effectiveness and\ninterpretability of our proposed model in dynamic\nlearning aspect-aware information.\nThe Inﬂuence of multiple Aspects. As aspect-\nrelated information plays a key role in ABSA and at\nleast 10.2% of reviews contain multiple aspects as\nshown in Table 1, we are curious about the model’s\nperformance in the complex scenarios, e.g., a re-\nview sentence contains multiple aspects. Therefore,\nwe randomly choose an example to explore how the\nselection of the context words will correspondingly\nchange with different inputs. The visualization re-\nsults are shown in Figure 4. Speciﬁcally, the chosen\nsentence has three different aspects with their sen-\ntiment polarity, i.e., “System memory” -negative,\n“DDR5”-positive and “DDR3”-negative. Take the\naspect “DDR5” as example, it is positive which is\ncontrary to “DDR3”. After receiving the overall\nsemantic of the whole sentence, readers tend to as-\nsociate “DDR5” with the context words {“would”,\n“have”} to predict the correct sentiment “positive”.\nFor other two aspects, the observations are consis-\ntent with “DDR5”. In summary, all those results\nshow that DR-BERT could dynamically extract the\nvital information to achieve aspect-aware semantic\nunderstanding even in a more complex scenario.\n4.7 Error Analysis\nTable 4 displays three review examples and their\nprediction results by BERT, RGAT-BERT, and our\nDR-BERT. As we can see from the “BERT-base”\ncolumn, when there are multiple aspects, the vanilla\nBERT often makes the wrong classiﬁcation since\nit tends to learn the overall sentiment polarity of\nthe sentences instead of the aspect-aware semantic.\nWhile RGAT-BERT can alleviate the problem to a\ncertain extent, it is also hard to predict the accurate\nsentiment label with few dependency relations. For\n3606\nMethods\nLaptop Restaurant Twitter\nS E T S E T S E T\n(1) DR-BERT 157s 10 26.1m 183s 10 30.5m 379s 10 63.2m\n(2) T-GCN-BERT 168s 10 28.0m 188s 10 31.3m 411s 10 68.5m\n(3) BERT-base 133s 10 22.2m 158s 10 26.3m 242s 10 40.3m\n(4) ATAE-LSTM 3s 30 1.50m 4s 30 2.00m 5s 30 2.50m\nTable 5: Runtime comparison between DR-BERT, T-GCN-BERT, BERT-base and ATAE-LSTM. Speciﬁcally, “S”\nrepresents the training time (seconds) for a single epoch, “E” denotes the number of training epochs, and “T” is\nthe total training time (minutes).\nexample, in the ﬁrst sentence, “DDR3” has few\nhelpful syntactic dependency relations. Therefore,\nRGAT-BERT makes a wrong sentiment prediction.\nHowever, our DR-BERT model, succeeding in pre-\ndicting most sentiment labels by considering the\ndynamic changing of the aspect-aware semantic.\nFor other two case examples, the observations are\nconsistent. Note that, for aspect“Supplied software”\nin second sentence, two overlap aspects appear in\nthe same sentence makes it more difﬁcult to distin-\nguish the different sentiment between them. Thus,\nprecisely determine its sentiment polarity is a big\nchallenge for human, let alone deep learning mod-\nels. This also leaves space for future exploration.\n5 Computation Time Comparison\nWe also compared the computation runtime of three\nbaseline methods. All of the models are performed\non a Linux server with 64 Intel(R) CPUs and 4\nTesla V100 32GB GPUs. From the results shown\nin Table 5, we can ﬁrst observe that the training\ntime of a single epoch in DR-BERT performs better\nthan T-GCN, which is based on GCN. Meanwhile,\nthe training time of all these BERT-based models\nis similar (i.e., there is no signiﬁcant difference).\nThe possible reason is that the ofﬁcial datasets are\nsmall, and it is hard to inﬂuence the overall run-\ntime of PLMs with such a small amount of data.\nSecond, compared with other models, the training\ntime of the ATAE-LSTM model is less (always an\norder of magnitude lower). For example, the ATAE-\nLSTM only needs about two minutes to achieve op-\ntimal performance in the restaurant dataset, while\nBERT-based models require more than 26 minutes.\nTherefore, though DR-BERT contains a Dynamic\nRe-weighting adapter based on GRU, the compu-\ntation time is much lower than the BERT-based\nframework. In summary, the observations above\nshow that the computation time of our DR-BERT\nmodel is within an acceptable range.\n6 Conclusion and Future Works\nThis paper introduced a new approach named Dy-\nnamic Re-weighting BERT (DR-BERT) for aspect-\nbased sentiment analysis. Speciﬁcally, we ﬁrst em-\nployed the BERT layers as a base encoder to learn\nthe overall semantic features of the whole sentence.\nThen, inspired by human semantic comprehension,\nwe devised a new Dynamic Re-weighting Adapter\n(DRA) to enhance aspect-aware semantic features\nin the sentiment learning process. In addition, we\ninserted the DRA into the BERT layers to address\nthe limitations of the vanilla pre-trained model in\nABSA task. Extensive experiments on three bench-\nmark datasets demonstrated the effectiveness and\ninterpretability of the proposed model, with good\nsemantic comprehension insights for future nature\nlanguage modeling. Moreover, the error analysis\nwas performed on incorrectly predicted examples,\nleading to some insights into the ABSA task.\nWe hope our research can help boost excellent\nwork for aspect-based sentiment analysis from dif-\nferent perspectives. In the future, we plan to extend\nour method to other tasks like Sentence Semantic\nMatching, Relation Extraction, etc., which can also\nbeneﬁt from utilizing the dynamic semantics. Be-\nsides, we will explore whether DR-BERT can make\nany positive changes based on previous mistakes\nduring the dynamic semantic understanding.\n7 Acknowledgments\nWe would like to thank the anonymous reviewers\nfor the helpful comments. This research was par-\ntially supported by grants from the National Key\nR&D Program of China (No. 2021YFF0901003),\nand the National Natural Science Foundation of\nChina (No. 61922073, 61727809, 62006066 and\n72101176). We appreciate all the authors for their\nfruitful discussions. In addition, Kai Zhang wants\nto thank, in particular, the patience, care and unwa-\nvering support from Rui Fu over the past years.\n3607\nReferences\nJames C Bezdek. 1992. On the relationship be-\ntween neural networks, pattern recognition and in-\ntelligence. International journal of approximate rea-\nsoning, 6(2):85–107.\nHarm Brouwer, Francesca Delogu, Noortje J Ven-\nhuizen, and Matthew W Crocker. 2021. Neurobe-\nhavioral correlates of surprisal in language compre-\nhension: A neurocomputational model. Frontiers in\nPsychology, 12:110.\nChenhua Chen, Zhiyang Teng, and Yue Zhang. 2020.\nInducing target-speciﬁc latent structures for aspect\nsentiment classiﬁcation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5596–5607.\nJin Yao Chin, Kaiqi Zhao, Shaﬁq Joty, and Gao Cong.\n2018. Anr: Aspect-based neural recommender. In\nProceedings of the 27th ACM International Confer-\nence on Information and Knowledge Management ,\npages 147–156.\nKyunghyun Cho, Bart van, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua Bengio.\n2014. Learning phrase representations using rnn\nencoder–decoder for statistical machine translation.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1724–1734.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4171–4186.\nXiaowen Ding and Bing Liu. 2007. The utility of lin-\nguistic rules in opinion mining. In Proceedings of\nthe 30th annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 811–812.\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming\nZhou, and Ke Xu. 2014. Adaptive recursive neural\nnetwork for target-dependent twitter sentiment clas-\nsiﬁcation. In Proceedings of the 52nd annual meet-\ning of the association for computational linguistics\n(volume 2: Short papers), pages 49–54.\nFeifan Fan, Yansong Feng, and Dongyan Zhao. 2018.\nMulti-grained attention network for aspect-level sen-\ntiment classiﬁcation. In Proceedings of the 2018\nconference on empirical methods in natural lan-\nguage processing, pages 3433–3442.\nTal Golan, Prashant C Raju, and Nikolaus Kriegesko-\nrte. 2020. Controversial stimuli: Pitting neural net-\nworks against each other as models of human cogni-\ntion. Proceedings of the National Academy of Sci-\nences, 117(47):29330–29337.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition (CVPR) ,\npages 770–778.\nBinxuan Huang and Kathleen M Carley. 2019. Syntax-\naware aspect level sentiment classiﬁcation with\ngraph attention networks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5469–5477.\nBinxuan Huang, Yanglan Ou, and Kathleen M Car-\nley. 2018. Aspect level sentiment classiﬁcation with\nattention-over-attention neural networks. In Interna-\ntional Conference on Social Computing, Behavioral-\nCultural Modeling and Prediction and Behavior\nRepresentation in Modeling and Simulation , pages\n197–206. Springer.\nLong Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and\nTiejun Zhao. 2011. Target-dependent twitter senti-\nment classiﬁcation. In Proceedings of the 49th an-\nnual meeting of the association for computational\nlinguistics, pages 151–160.\nSvetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and\nSaif Mohammad. 2014. Nrc-canada-2014: Detect-\ning aspects and sentiment in customer reviews. In\nProceedings of the 8th international workshop on se-\nmantic evaluation (SemEval 2014), pages 437–442.\nChristof Koch and Naotsugu Tsuchiya. 2007. Atten-\ntion and consciousness: two distinct brain processes.\nTrends in cognitive sciences, 11(1):16–22.\nGina R Kuperberg. 2007. Neural mechanisms of lan-\nguage comprehension: Challenges to syntax. Brain\nresearch, 1146:23–49.\nGina R and T Florian Jaeger. 2016. What do we mean\nby prediction in language comprehension? Lan-\nguage, cognition and neuroscience, 31(1):32–59.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassiﬁcation. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 29.\nRuifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi-\naojie Wang, and Eduard Hovy. 2021. Dual graph\nconvolutional networks for aspect-based sentiment\nanalysis. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 6319–6329.\n3608\nXiangsheng Li, Jiaxin Mao, Chao Wang, Yiqun Liu,\nMin Zhang, and Shaoping Ma. 2019. Teach machine\nhow to read: reading behavior inspired relevance es-\ntimation. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 795–804.\nXin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018.\nTransformation networks for target-oriented senti-\nment classiﬁcation. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 946–956.\nYunlong Liang, Fandong Meng, Jinchao Zhang, Ji-\nnan Xu, Yufeng Chen, and Jie Zhou. 2019. A\nnovel aspect-guided deep transition model for as-\npect based sentiment analysis. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5569–5580.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017. Interactive attention networks for\naspect-level sentiment classiﬁcation. In Proceed-\nings of the 26th International Joint Conference on\nArtiﬁcial Intelligence, pages 4068–4074.\nThien Hai Nguyen and Kiyoaki Shirai. 2015.\nPhrasernn: Phrase recursive neural network for\naspect-based sentiment analysis. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 2509–2514.\nWei Peng, Yue Hu, Luxi Xing, Yuqiang Xie, Jing\nYu, Yajing Sun, and Xiangpeng Wei. 2020. Bi-\ndirectional cognitivethinking network for machine\nreading comprehension. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 2613–2623.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos,\nHarris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2014. SemEval-2014 task 4: As-\npect based sentiment analysis. In Proceedings of the\n8th International Workshop on Semantic Evaluation\n(SemEval 2014), pages 27–35, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological bulletin, 124(3):372.\nCansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan\nKong, and Elke Runden. 2020. Human attention\nmaps for text classiﬁcation: Do humans and neural\nnetworks focus on the same words? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4596–4608.\nLei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.\n2016. Reading and thinking: Re-read lstm unit\nfor textual entailment recognition. In Proceedings\nof COLING 2016, the 26th International Confer-\nence on Computational Linguistics: Technical Pa-\npers, pages 2870–2879.\nSelina Sharmin, Oleg Špakov, and Kari-Jouko Räihä.\n2015. Dynamic text presentation in print\ninterpreting–an eye movement study of reading be-\nhaviour. International Journal of Human-Computer\nStudies, 78:17–30.\nYouwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and\nYanghui Rao. 2019. Attentional encoder network\nfor targeted sentiment classiﬁcation. arXiv preprint\narXiv:1902.09314.\nConcetto Spampinato, Simone Palazzo, Isaak Kava-\nsidis, Daniela Giordano, Nasim Souly, and Mubarak\nShah. 2017. Deep learning human mind for auto-\nmated visual classiﬁcation. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6809–6817.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utiliz-\ning bert for aspect-based sentiment analysis via con-\nstructing auxiliary sentence. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 380–385.\nYaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhen-\nzhou Ji, and Xiaolong Wang. 2015. Modeling men-\ntion, context and entity with neural networks for en-\ntity disambiguation. In Twenty-fourth international\njoint conference on artiﬁcial intelligence.\nNiels A Taatgen, Hedderik Van Rijn, and John Ander-\nson. 2007. An integrated theory of prospective time\ninterval estimation: The role of cognition, attention,\nand learning. Psychological review, 114(3):577.\nXingwei Tan, Yi Cai, and Changxi Zhu. 2019. Rec-\nognizing conﬂict opinions in aspect-level sentiment\nclassiﬁcation with dual attention networks. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing (EMNLP-\nIJCNLP), pages 3426–3431.\nDuyu Tang, Bing Qin, Xiaocheng Feng, and\nTing Liu. 2015. Effective lstms for target-\ndependent sentiment classiﬁcation. arXiv preprint\narXiv:1512.01100.\nDuyu Tang, Bing Qin, and Ting Liu. 2016. Aspect\nlevel sentiment classiﬁcation with deep memory net-\nwork. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing,\npages 214–224.\nHao Tang, Donghong Ji, Chenliang Li, and Qiji\nZhou. 2020. Dependency graph enhanced dual-\ntransformer structure for aspect-based sentiment\nclassiﬁcation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 6578–6588.\nJialong Tang, Ziyao Lu, Jinsong Su, Yubin Ge, Linfeng\nSong, Le Sun, and Jiebo Luo. 2019. Progressive self-\nsupervised attention learning for aspect-level senti-\nment analysis. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 557–566.\n3609\nYuanhe Tian, Guimin Chen, and Yan Song. 2021.\nAspect-based sentiment analysis with type-aware\ngraph convolutional networks and layer ensemble.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 2910–2922.\nGiulio Tononi. 2008. Consciousness as integrated in-\nformation: a provisional manifesto. The Biological\nBulletin, 215(3):216–242.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nJingjing Wang, Changlong Sun, Shoushan Li, Xi-\naozhong Liu, Luo Si, Min Zhang, and Guodong\nZhou. 2019. Aspect sentiment classiﬁcation towards\nquestion-answering with reinforced bidirectional at-\ntention network. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3548–3557.\nKai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan,\nand Rui Wang. 2020. Relational graph attention net-\nwork for aspect-based sentiment analysis. In Pro-\nceedings of 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3229–3238.\nXuejian Wang, Lantao Yu, Kan Ren, Guanyu Tao,\nWeinan Zhang, and et al. 2017. Dynamic attention\ndeep model for article recommendation by learning\nhuman editors’ demonstration. In Proceedings of\nthe 23rd international conference on knowledge dis-\ncovery and data mining, pages 2051–2059.\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and\nLi Zhao. 2016. Attention-based lstm for aspect-\nlevel sentiment classiﬁcation. In Proceedings of the\n2016 conference on empirical methods in natural\nlanguage processing, pages 606–615.\nBowen Xing, Lejian Liao, Dandan Song, and et al.\n2019. Earlier attention? aspect-aware lstm for\naspect-based sentiment analysis. In IJCAI.\nHu Xu, Bing Liu, Lei Shu, and S Yu Philip. 2019a.\nBert post-training for review reading comprehension\nand aspect-based sentiment analysis. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2324–2335.\nHu Xu, Lei Shu, S Yu Philip, and Bing Liu. 2020. Un-\nderstanding pre-trained bert for aspect-based senti-\nment analysis. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 244–250.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang\nZhao, and Junyang Lin. 2019b. Understanding and\nimproving layer normalization. Advances in Neural\nInformation Processing Systems, 32.\nHsien-Ming Yang and George W McConkie. 1999.\nReading chinese: Some basic eye-movement charac-\nteristics. Reading Chinese script: A cognitive analy-\nsis, pages 207–222.\nQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,\nand Jiebo Luo. 2016. Image captioning with seman-\ntic attention. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n4651–4659.\nChen Zhang, Qiuchi Li, and Dawei Song. 2019a.\nAspect-based sentiment classiﬁcation with aspect-\nspeciﬁc graph convolutional networks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4568–4578.\nKai Zhang, Qi Liu, Hao Qian, Biao Xiang, Qing Cui,\nJun Zhou, and Enhong Chen. 2021a. Eatn: An efﬁ-\ncient adaptive transfer network for aspect-level sen-\ntiment analysis. IEEE Transactions on Knowledge\nand Data Engineering.\nKai Zhang, Hao Qian, Qi Liu, Zhiqiang Zhang, Jun\nZhou, and et al. 2021b. Sifn: A sentiment-aware in-\nteractive fusion network for review-based item rec-\nommendation. In Proceedings of the 30th ACM\nInternational Conference on Information & Knowl-\nedge Management, pages 3627–3631.\nKai Zhang, Hefu Zhang, Qi Liu, Hongke Zhao, Heng-\nshu Zhu, and Enhong Chen. 2019b. Interactive at-\ntention transfer network for cross-domain sentiment\nclassiﬁcation. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence , volume 33, pages\n5773–5780.\nKai Zhang, Hongke Zhao, Qi Liu, Zhen Pan, and En-\nhong Chen. 2019c. A dynamic and cooperative\ntracking system for crowdfunding. arXiv preprint\narXiv:2002.00847.\nKun Zhang, Guangyi Lv, Linyuan Wang, Le Wu, En-\nhong Chen, Fangzhao Wu, and Xing Xie. 2019d.\nDrr-net: Dynamic re-read network for sentence se-\nmantic matching. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, pages 7442–7449.\nYaowei Zheng, Richong Zhang, Samuel Mensah, and\nYongyi Mao. 2020. Replicate, walk, and stop on syn-\ntax: An effective neural network model for aspect-\nlevel sentiment classiﬁcation. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 9685–9692.\nYukun Zheng, Jiaxin Mao, Yiqun Liu, Zixin Ye, Min\nZhang, and Shaoping Ma. 2019. Human behavior in-\nspired machine reading comprehension. In Proceed-\nings of the 42nd International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 425–434.\n3610",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8575649857521057
    },
    {
      "name": "Sentence",
      "score": 0.6649345755577087
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6076949834823608
    },
    {
      "name": "Weighting",
      "score": 0.5981577038764954
    },
    {
      "name": "Natural language processing",
      "score": 0.5806607007980347
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5760303735733032
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5626562237739563
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5382779240608215
    },
    {
      "name": "GRASP",
      "score": 0.5343906879425049
    },
    {
      "name": "Language model",
      "score": 0.513458788394928
    },
    {
      "name": "Encoder",
      "score": 0.5110443830490112
    },
    {
      "name": "Programming language",
      "score": 0.13023418188095093
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    }
  ],
  "cited_by": 59
}