{
    "title": "Large language models for material property predictions: elastic constant tensor prediction and materials design",
    "url": "https://openalex.org/W4410513059",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2108880704",
            "name": "Siyu Liu",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2763276979",
            "name": "Tongqi Wen",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2810969051",
            "name": "Beilin Ye",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2112927206",
            "name": "Zhuoyuan Li",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2126168353",
            "name": "Han Liu",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2138259234",
            "name": "Yang Ren",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A1892576833",
            "name": "David J. Srolovitz",
            "affiliations": [
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4293499223",
        "https://openalex.org/W3203688505",
        "https://openalex.org/W3080673473",
        "https://openalex.org/W2976766051",
        "https://openalex.org/W4394008305",
        "https://openalex.org/W4404987793",
        "https://openalex.org/W4388560379",
        "https://openalex.org/W4396723768",
        "https://openalex.org/W4383988639",
        "https://openalex.org/W4396650955",
        "https://openalex.org/W4385502405",
        "https://openalex.org/W4391836235",
        "https://openalex.org/W4402643908",
        "https://openalex.org/W4391862446",
        "https://openalex.org/W4391335129",
        "https://openalex.org/W4388787940",
        "https://openalex.org/W4401845100",
        "https://openalex.org/W4283031227",
        "https://openalex.org/W4387928778",
        "https://openalex.org/W4384115439",
        "https://openalex.org/W4400074175",
        "https://openalex.org/W4400064820",
        "https://openalex.org/W4406284830",
        "https://openalex.org/W4403592913",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W3205750961",
        "https://openalex.org/W2032204227",
        "https://openalex.org/W4391547284",
        "https://openalex.org/W4386269388",
        "https://openalex.org/W1992985800",
        "https://openalex.org/W4206119985",
        "https://openalex.org/W3180309787",
        "https://openalex.org/W4224129425",
        "https://openalex.org/W4389305466",
        "https://openalex.org/W2959802414",
        "https://openalex.org/W2015197254",
        "https://openalex.org/W4391670863",
        "https://openalex.org/W2464725281",
        "https://openalex.org/W4362703219",
        "https://openalex.org/W4390722747",
        "https://openalex.org/W2131576953",
        "https://openalex.org/W2064240714",
        "https://openalex.org/W2896712495",
        "https://openalex.org/W2052953557",
        "https://openalex.org/W4308342665",
        "https://openalex.org/W2911665018",
        "https://openalex.org/W4220764574",
        "https://openalex.org/W3212879136",
        "https://openalex.org/W4294755884",
        "https://openalex.org/W4309553208",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4400703072",
        "https://openalex.org/W4387390329",
        "https://openalex.org/W2804431384",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W4391988255",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4393108840",
        "https://openalex.org/W4399695937",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2948715311",
        "https://openalex.org/W4387560005"
    ],
    "abstract": "A multifunctional LLM for predicting finite temperature elastic constant tensor, RAG-enhanced prediction, and inverse material design.",
    "full_text": "Large language models for material property\npredictions: elastic constant tensor prediction and\nmaterials design†\nSiyu Liu, ab Tongqi Wen, *ab Beilin Ye,a Zhuoyuan Li,ab Han Liu, c Yang Renc\nand David J. Srolovitz*ab\nEﬃcient and accurate prediction of material properties is critical for advancing materials design and\napplications. Leveraging the rapid progress of large language models (LLMs), we introduce ElaTBot,\na domain-speciﬁc LLM for predicting elastic constant tensors and enabling materials discovery as a case\nstudy. The proposed ElaTBot LLM enables simultaneous prediction of elastic constant tensors, bulk\nmodulus atﬁnite temperatures, and the generation of new materials with targeted properties. Integrating\ngeneral LLMs (GPT-4o) and Retrieval-Augmented Generation (RAG) further enhances its predictive\ncapabilities. A specialized variant, ElaTBot-DFT, designed for 0 K elastic constant tensor prediction,\nreduces the prediction errors by 33.1% compared with a domain-speciﬁc, materials science LLM (Darwin)\ntrained on the same dataset. This natural language-based approach highlights the broader potential of\nLLMs for material property predictions and inverse design. Their multitask capabilities lay the foundation\nfor multimodal materials design, enabling more integrated and versatile exploration of material systems.\nProperty data are essential for determining the suitability of\nmaterials for specic applications. For example,exible elec-\ntronics require materials with targeted elastic sti ﬀness,1\nthermal management systems rely on materials with suﬃciently\nhigh thermal conductivity,2 and electronic devices depend on\nmaterials with appropriate band structure.3 Given the diverse\nproperty pro le required for individual applications, deep\nunderstanding or at least robust property prediction would be\na great aid to material selection and/or alloy design. While the\nformer is a long-term goal of materials science, high-\nthroughput material property measurements and prediction,\ncoupled with new techniques in articial intelligence (AI) have\nthe potential to revolutionize materials development in the\nshort term.\nWhile experimental approaches for determining materials\nproperties remains the gold standard, they are oen hindered\nby expense and the time required to synthesize materials and\nmeasure properties (and, at times, lead to results that are either\ninconsistent or not suﬃciently accurate), such as in the case of\nelastic constant measurements.\n4 Recent advancements in\nsimulation techniques and computational power have made\ncomputational modeling a critical tool for property prediction.\nGiven the diversity of length and time scales that control\nmaterial properties, multi-scale modeling has emerged as an\noen eﬃcient and suﬃciently accurate approach for materials\nproperty prediction. For example, atomistic simulations with\nquantum-mechanical accuracy can accurately predict the full\nelastic constant tensors and/or band gaps (using hybrid\nexchange– correlation functionals), while phase-eld modeling\nand other continuum based-methods enable microstructure\nevolution and defect property prediction. However, challenges\n(e.g., data transfer and error propagation) oen remain signif-\nicant obstacles to achieving accurate, macroscopic predictions\nin multi-scale modeling frameworks. The emergence of large\nlanguage models (LLMs) presents a new opportunity for mate-\nrials property prediction, with the potential to close the gaps\nbetween experiment data (e.g., sourced from literature data-\nbases) and computational materials simulation approaches.\n5\nLLMs, for example ChatGPT, have demonstrated some\nremarkable successes across a wide range of materials appli-\ncations, including high-throughput discovery of physical laws,\n6\ngeneration of metal– organic frameworks (MOFs),7 design of\nchemical reaction workows,8 determining crystal structure\n(CIF, crystallographic information le),9 electron microscopy\nimage analysis,10 and guiding automated experiments.11 LLMs\nachieve this by leveraging their capabilities such as rapid liter-\nature summarization, 12 prompt engineering 13 and/or\naCenter for Structural Materials, Department of Mechanical Engineering, The\nUniversity of Hong Kong, Hong Kong SAR, China. E-mail: tongqwen@hku.hk; srol@\nhku.hk\nbMaterials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI,\nShenzhen, China\ncDepartment of Physics, JC STEM Lab of Energy and Materials Physics, City University\nof Hong Kong, Hong Kong SAR, China\n† Electronic supplementary information (ESI) available: ESI Notes, ESI Fig. S1– S9,\nESI Tables S1– S8. See DOI:https://doi.org/10.1039/d5dd00061k\nCite this:Digital Discovery,2 0 2 5 ,4,\n1625\nReceived 12th February 2025\nAccepted 19th May 2025\nDOI: 10.1039/d5dd00061k\nrsc.li/digitaldiscovery\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1625\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nintegration with external tools.14 This approach can make them\nsuperior to traditional machine learning (ML) models, partic-\nularly when dealing with complex and multitask processes at\nscale.\n15 One major strength of LLMs is their foundation in\nnatural language-based training,ne-tuning, and application,\nwhich lowers the barrier to entry for researchers without\na strong background in computer science or coding.\n16 More-\nover, underlying pre-trained models encode extensive materials\nscience knowledge, giving LLMs remarkable ability in cases\nwhere datasets are sparse (through transfer learning), an\nachievement that previously required highly specialized\nalgorithms.\n17\nGiven the strong performance of LLMs on a wide range of\nlow-dimensional classi cation and regression tasks in\ncomputer science,18 there is growing interest in leveraging LLMs\nto improve numerical property prediction in materials science.\nRecent studies, including LLM-Prop,\n19 CrysMMNet,20 and\nAtomGPT,21 illustrate two major strategies. LLM-Prop and\nCrysMMNet introduce architectural modications of LLMs fol-\nlowed by ne-tuning, whereas AtomGPT preserves the original\nLLM architecture. Despite their methodological diﬀerences, all\nthree approaches convert crystal structures into text descrip-\ntions, and ne-tune the LLMs to predict individual material\nproperties such as the band gap, formation energy, or bulk\nmodulus. These studies demonstrate that text-based encoding\nof structural information can enhance predictive accuracy.\nRecent studies examined the impact of prompt design on LLM\nproperty prediction performance,\n22 and have benchmarked\nLLM-based methods against conventional models on out-of-\ndistribution datasets.\n23 These comparisons highlight the value\nof prompt design for optimizing LLM materials property\nprediction performance. Although the aforementioned works\nprove that LLMs can outperform traditional models in pre-\ndicting certain scalar properties, there are also contrary results,\nespecially when faced with small datasets.\n24 For example,\nJablonka et al.25 showed that while LLMs can predict properties\nlike HOMO– LUMO gaps, solubility, photoswitching behavior,\nsolvation free energies, and photoconversion e ﬃciency, the\nresults were no better than with traditional ML models.\nEnhancing the quantitative prediction capabilities of LLMs,\nwhile leveraging their strengths in natural language interaction\nand multitasking, can signicantly expand their potential in\nmaterials science applications.\nIn this work, we focus on predicting the elastic constant\ntensor as a case study of quantitative prediction of a material\nproperty. The elastic constant tensor is a fundamental property\nthat describes the elastic response of materials to external\nforces\n26 and serves as a indicator of the nature of intrinsic\nbonding within a material.27 Mechanical (Young's modulus,\nPoisson's ratio,.), thermal (thermal conductivity), and acoustic\n(sound velocity) properties can all be derived starting from the\nelastic constant tensor\n28 (oen together with other basic mate-\nrial properties). Here, we introduce ElaTBot and ElaTBot-DFT\n(DFT is quantum mechanical density functional theory), LLMs\ndeveloped through prompt engineering and knowledge fusion\ntraining. ElaTBot is designed to predict elastic constant tensors,\nbulk modulus at nite temperatures, and propose materials\nwith specic elastic properties.\nTo our knowledge, ElaTBot is the rst model capable of\ndirectly and eﬃciently predicting the full elastic constant tensor\nat nite temperatures. ElaTBot-DFT, a variant specialized for\n0 K elastic constant tensor prediction, reduces prediction error\nby 33.1% compared to the material science LLM Darwin\n29 using\nthe same training and test sets. These results highlight the\npotential of LLMs for numerical materials property predictions.\nTraining specialized LLMs\nDespite the importance of elastic constant tensors in materials\nscience, complete elastic constant tensor data for inorganic\ncrystals remains scarce due to experimental and computational\nlimitations. Fig. 1(a) shows that elastic constant data is scarce in\nthe Materials Project;\n30 ∼7.9% as abundant as crystal structure\ndata and ∼17.2% as abundant as band structure data. The\nelastic constants are a fourth-rank tensor with as many as 21\nindependent components (in static equilibrium) which is oen\nrepresented as a symmetric 6× 6 Voigt matrixC\nij.31 Predicting\nthese components is far more complex than predicting scalar\nproperties, such as formation energy, free energy, or bulk\nmodulus. Fig. 1(b) lists a few ML approaches for predicting\nelastic constant tensors. Chemical composition-based upon\nelement descriptors were used to predict specic components\nor C\nij,32,33 but not the full elastic constant tensor. More recent\nmodels that leverage crystal structure descriptors to predict the\nfull elastic constant tensor face challenges such as long training\ntimes and complex model architectures.\n28,34 These models are\noen restricted to single-task predictions of elastic properties\nand lack the capabilities to propose new materials tailored to\nspecic properties or learn from new data without retraining.\nFig. 1(c) presents an integrated approach, combining ML\nand natural language processing, for predicting material prop-\nerties and identifying materials with targeted properties.\nSpecically, for elastic properties prediction and materials\ngeneration, we developed two domain-specic LLMs: ElaTBot\nand ElaTBot-DFT, which predict elastic properties such as the\nelastic constant tensor, bulk, shear and Young's moduli, as well\nas the Poisson ratio. To further improve user interaction and\ntask handling, we implemented an AI-driven agent capable of\nutilizing tools and databases, and general LLMs to perform\ncomplex, multi-step tasks. This agent can process new (and\nunseen) data by integration of external tools and vector data-\nbases. Its responses can be fed into general LLMs (e.g., GPT-4,\nGemini) to further extend its capabilities and tackle more\ncomplex, multi-step tasks. Fig. 1(d) shows three capabilities of\nour specialized LLM ElaTBot: prediction, Retrieval-Augmented\nGeneration (RAG)-enhanced prediction\n35 without retraining,\nand generation.\nTo train the ElaTBot, werst used robocrystallographer36 to\nextract structural text descriptions, then employed Pymatgen37\nto obtain compositional information. We then integrate these\nelements into text-form prompts, subsequentlyne-tuning the\ngeneral LLM Llama2-7b model to yield ElaTBot-DFT, a special-\nized model for predicting elastic constant tensors at 0 K.\n1626 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nElaTBot-DFT serves as a benchmark for elastic constant tensor\nprediction, particularly since other models are limited in\naddressing nite-temperature predictions. Next, we employ\nseveral steps to enhance LLM performance,\n38 incorporating\nnite-temperature data and fusing this knowledge to develop\nElaTBot. Prompt engineering leads to a reduction of the\nprediction error of the average value of the elastic constant\ntensor\nCij (see Methods) by 33.1% for ElaTBot-DFT compared to\nDarwin,29 a materials science LLM built on the same dataset. We\nran the test set twice to ensure the reliability of the LLM results.\nThrough knowledge fusion, ElaTBot accurately ts the\ntemperature-dependent bulk modulus curves (derived from the\nelastic constant tensor) for new multicomponent alloys, with\nerrors near room temperature approaching the average error for\nFig. 1 Datasets and overview of the ElaTBot for predicting elastic properties. (a) Comparison of the number of materials in the Materials Project\ndatabase30 with available data on crystal structures, band structures, and elastic properties. The availability of elastic constant tensors data is\nsigniﬁcantly lower than that of crystal and band structure data. (b) Overview of existing methods used to predict elastic constant tensors, which\nprimarily relied on element descriptors and structural features (constructed by CIF). (c) Aﬂowchart illustrating the process of using large language\nmodels (LLMs) to acquire material knowledge. This method enables researchers to gain domain-speciﬁc insights, allowing those without\nextensive programming skills or theoretical expertise to conduct research, thereby lowering the entry barrier into materials science. (d)\nCapabilities of our specialized LLM ElaTBot. By incorporating elastic constant tensors data atﬁnite temperatures, we develop an LLM-based\nagent, ElaTBot, which is capable of predicting elastic constant tensors, enhancing prediction without retraining by leveraging external tools and\ndatasets, and generating chemical composition for materials with speciﬁc modulus.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1627\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nthe 0 K test set. RAG-enhanced35 predictions with limitednite-\ntemperature data further improves ElaTBot errors for the bulk\nmodulus from 27.49% to 0.95% across nine alloys at various\ntemperatures without retraining orne-tuning.\nWe integrate ElaTBot with GPT-4o to propose/screen mate-\nrials based upon bulk modulus and other requirements of tar-\ngeted applications. These include materials with low corrosion\nrates and high biocompatibility (measured by median lethal\ndose) with a bulk modulus similar to that of bone for bone\nimplantation, high bulk but low shear modulus materials\nsuitable for exoskeletons of so  robots, corrosion-resistant\nmaterials suitable for saline environments, and materials for\nthe protective layers of LiCoO\n2 electrode.\nElastic constant tensor predictions\nOur model uses text-based inputs to predict the elastic constant\ntensor; this requires carefully designed prompt templates. We\ndevelop a prompt (prompt type 4 in ESI Table S1†) that incor-\nporates both chemical composition and crystal structure\ndescriptions as inputs to the model (Llama2-7b, an open-source\ngeneral LLM). These textual descriptions were generated by\nextracting relevant data from the material composition and\nstructure and converting these into natural language.\nWe conducted a series of“experiments” to assess the eﬀects\nof di ﬀerent input formats on model performance: JSON-\nformatted (JavaScript Object Notation) structure descriptions\n(prompt type 1), textual descriptions of crystal structure\n(prompt type 2), textual descriptions of the composition\n(prompt type 3), and textual description of both chemical\ncomposition and crystal structure (prompt type 4). The model\nwas trained using a 0 K density functional theory (DFT) dataset\ncontaining 9498 materials with elastic constant tensor data\nfrom the Materials Project, with 500 materials for validation and\n522 for testing. Fig. 2(a) and ESI Table S2† show that prompt\ntype 4 achieves a mean absolute error (MAE; the average of the\nabsolute diﬀerences between the predicted and actual values for\nall data points) of 2.32 GPa andR\n2 of 0.965 for predicting the\naverage elastic constant tensor componentðCijÞ, outperforming\nother prompt types (explicit denitions of the MAE andCij are\nin Methods). Compared to prompt type 1 (JSON format), prompt\ntype 4 reduces the MAE by 16.8% and increasesR\n2 by 1.9%.\nWhen compared to prompt type 2 (crystal structure descriptions\nonly), prompt type 4 achieves a 5.3% reduction in MAE and\na 0.8% increase inR\n2. Compared to prompt type 3 (composition\ndescriptions only), prompt type 4 yields a 13.1% reduction in\nMAE and a 0.9% increase inR\n2. These results demonstrate that\nLLMs perform better when trained with natural language-like\ninputs, and that using both structural and compositional\ninformation improves elastic constant tensor prediction. The\nbulk modulus results (derived from the elastic constant tensor)\nin Fig. 2(b) conrm this: prompt type 4 achieves an MAE of\n7.74 GPa and anR\n2 of 0.963, representing a 14.4% reduction in\nMAE and a 1.7% increase inR2 compared to prompt type 1.\nTherefore, prompt type 4 was selected for training Llama2-7b\nfor our ElaTBot-DFT model.\nWe compared the performance of ElaTBot-DFT with two\nwidely-used models for predicting the full elastic constant\ntensor: the random forest model, which utilizes Magpie\n(Materials Agnostic Platform for Informatics and Exploration)\nfeatures based on composition,\n39 and the MatTen model, which\nemploys a crystal structure graph neural network.28 As shown in\nFig. 2(a, b), ESI Fig. S1† and Table S3,† when trained on the\ndataset, ElaTBot-DFT using prompt type 4 achieves a 30.3%\nreduction in MAE and a 4.4% increase inR\n2 for predicting the\naverage elastic constant tensor componentsðCijÞ compared to\nthe random forest model. Compared to the MatTen model,\nElaTBot-DFT reduces MAE by 4.5% and improvesR\n2 by 0.2%.\nThis demonstrates that, even with a relatively small dataset,\nLLMs trained with well-designed textual descriptions can\noutperform traditional methods, contrary to previous studies\nusing QA-based training approaches.\n25 We also examined the\nsymmetry of the generated elastic constant tensors that result\nfrom the rigorous application of crystal symmetries; this\nsymmetry requires certainCij components to be zero and axed\nrelationship between some others. Under strict criteria (error\nmargin of±2 GPa), ElaTBot-DFT achieves a symmetry accuracy\nof 94%, signi cantly outperforming MatTen (5%) and the\nrandom forest model (6%) (Fig. 2(c)). Traditional numerical\nmodels tend to produce small non-zero values due to algo-\nrithmic limitations, while the natural language-based model,\nElaTBot-DFT, accurately outputs a“0” where appropriate. We\nalso tested the elastic stability of all of the materials in the test\n(i.e., the Born condition– the elastic constant tensor is positive\ndenite), as shown in Fig. S9.† For the 519 materials in the test\nset, 518 are found to be elastically stable for predictions of both\nthe random forest model and our ElaTBot-DFT model, whereas\nthe predictions of MatTen failed the stability test in 32 cases.\nWe further compared ElaTBot-DFT predictions with those\nfrom the domain-specic materials LLM, Darwin, for elastic\nconstant tensor prediction. Domain-specic LLMs are widely\nbelieved to outperform general LLMs on specialized problems;\n40\nhowever, as shown in Fig. 2(a, b, d, e) and ESI Table S3,† Darwin\n(even aer ne-tuning on the same dataset) underperforms\nElaTBot-DFT in predicting theCij and bulk modulus. Speci-\ncally, the MAEs of ElaTBot-DFT are 33.1% and 31.8% lower than\nthose of Darwin for\nCij and bulk modulus, respectively. This\nsuggests that integrating the reasoning abilities of a general\nLLM with ne-tuning on a specic dataset may yield better\nresults for tasks requiring quantitative property predictions.\nFine-tuning a model with domain-speci c knowledge (like\nDarwin) can lead to gaps in its abilities and knowledge loss,\nwhich may reduce the eﬀectiveness in specialized tasks.\n41\nWe further examined the performance of ElaTBot-DFT\nacross diﬀerent crystal systems, as summarized in Tables S6\nand S7.† The model demonstrates consistently strong predictive\naccuracy across all crystal systems except for the triclinic\nsystem, for which there are only three/sixty data points in the\ntest/training sets. The performance of ElaTBot-DFT is particu-\nlarly strong for the very common cubic system, withR\n2 > 0.97 for\nboth elastic constant tensor and bulk modulus predictions. The\nperformance is slightly lower in the orthorhombic and mono-\nclinic systems, with R\n2 ∼ 0.94. This demonstrates that while\n1628 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nElatBot-DFT is broadly eﬀective across diﬀerent crystal systems,\npredictive accuracy is inuenced by training/test sample sizes\n(this is an issue only for less common crystal systems).\nFinally, we integrated the nite-temperature dataset and\ndesigned four tasks (ESI Table S4†) with corresponding training\ntext inputs (elastic constant tensor prediction, bulk modulus\nprediction, material generation based on bulk modulus, and\ntext inlling) to conduct multi-task knowledge fusion training.\nThis approach equips ElaTBot with multiple capabilities,\nincluding the ability to predict elastic constant tensors atnite\ntemperatures. Although the text inlling task does not directly\npredict material properties, previous studies have shown that it\nimproves the overall multi-task performance.\n38 To test the\neﬀectiveness of ElaTBot, we selected three multicomponent\nalloys not in the training set (cubic Ni3Al, g0-PE16 (Ni72.1Al10.4-\nFe3.2Cr1.0Ti13.3), and tetragonalg-TiAl (Ti44Al56)) and predicted\ntheir bulk modulus as a function of temperature (based on the\nfull elastic constant tensors). Given the limited nite-\ntemperature training data-just 1266 samples-and the vast\ncompositional space of alloys, predicting accurate values over\na wide range of temperature and compositions is inherently\nchallenging. We predicted the bulk modulus at 11 temperatures\nfor Ni\n3Al and g0-PE16 and 15 temperatures forg-TiAl. Fig. 2(f)\nshows the variation of prediction errors for three alloy systems\n(not in the training set) as a function of temperature; the blue\ndashed lines indicating the error trends. A clear increase in\nprediction error with temperature is observed. We note that in\nthe original training set, there are 10 520 samples at 0 K and\nonly 1266 entries atnite-temperature conditions. The errors\nare larger for the quinary g\n0-PE16 (Ni72.1Al10.4Fe3.2Cr1.0Ti13.3)\nalloy compared with the binary Ni3Al alloy. The original training\nset had 213 times more binary than quinary data. This high-\nlights that the model performance is less reliable for situations\n(alloy and temperature) where the test cases diﬀer greatly from\nthose in the training set. Nonetheless, the model performs\nremarkably well compared across a wide range of composition\nand temperature, especially in light of the fact that experi-\nmental data on compositionally complex materials and at high\nFig. 2 Prediction abilities of ElaTBot-DFT and ElaTBot. (a and b) Performance comparison of the Llama2-7b model using diﬀerent prompt types,\nthe MatTen model, random forest model, and Darwin model in predictingCij (GPa) and bulk modulus based on MAE andR2 on the 0 K DFT test\nset, all trained on the same dataset. (c) Symmetry validation for elastic constant tensors predicted by MatTen, random forest, and ElaTBot-DFT\nmodels. Symmetry correctness is deﬁned as components within±2 GPa where zero values are required by the Voigt format matrix. (d and e)\nPerformance comparison of ElaTBot-DFT model against the pre-trained Llama2-7b model (Darwin, trained with a materials knowledge data-\nbase) for bulk modulus prediction, using the same test set and training data with prompt type 4. (f) The capability of ElaTBot to predictﬁnite\ntemperature bulk modulus. The red line indicates predicted values, the green line shows experimental data,\n42,43 the blue dashed line indicates the\npercentage error trend, and the black dashed line shows the average error (7.05%) for the 0 K temperature test set.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1629\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ntemperatures is rare (and expensive to generate). Fig. 2(f) also\nshows that the tted lines from the predictions of ElaTBot\nclosely align with experimental data, 42,43 particularly at low\ntemperatures, where ElaTBot exhibits smaller errors. This\ndemonstrates that incorporating the 0 K DFT dataset from the\nMaterials Project helps reduce prediction errors, highlighting\nthe eﬀectiveness of the multi-task knowledge fusion training\napproach.\nRAG enhanced predictions\nRetrieval-Augmented Generation (RAG)35 provides an eﬀective\nmethod for LLMs to access up-to-date databases, enabling the\nmost current RAG-enhanced prediction without model retrain-\ning. RAG integrates information retrieval with generative\nmodels, enhancing the knowledge scope and accuracy of LLM\n35\noutput. The retrieval module extracts relevant data from\nexternal sources, which is then combined with the generative\nmodel to deliver more accurate predictions or text generation\n(Fig. 3(a)). This approach allows LLMs to stay current with new\ndata or literature, by incorporating knowledge rather than solely\nrelying on pre-trained models.\nFig. 3(a) compares the ElaTBot bulk modulus predictions for\ng-TiAl at 170 K with and without RAG support. Since thenite\ntemperature data was not in the original ElaTBot training set,\nthe model automatically queries our external database,nds\nbulk modulus data forg-TiAl at similar temperatures (the 170 K\ndata was removed from the database for comparison purposes).\nThe predicted value (110.77 GPa) diﬀers by only 0.1% from the\ntrue value, whereas without RAG, the error increases to 2.4%. To\nensure a fair comparison, we customized the RAG prompt in\norder to isolate its inuence from training prompts. Further\ntesting on alloy data at various temperatures (see Fig. 3(b– d)\nand ESI Table S5†) demonstrates that RAG reduces the average\nerror from 27.49% to 0.95%. RAG prediction performance can\nbe improved by increasing the quantity and quality of data. This\nis demonstrated in Table S8,† where increasing the number of\ndata points by 32% led to a 50% decrease in the error, compared\nwith experiments. By incorporating RAG, ElaTBot achieves RAG-\nFig. 3 Integration of Retrieval-Augmented Generation (RAG) with ElaTBot for enhanced prediction. (a) The steps involved in enabling ElaTBot to\nperform RAG are as follows: (1) document loading: external documents or data sources are ingested into the system for further use; (2) splitting:\nthe documents are broken down into smaller, manageable chunks to optimize retrieval; (3) storage: these chunks are stored in an indexed format\nfor fast and eﬃcient searching; (4) retrieval: the system identiﬁes and retrieves the most relevant chunks in response to the query; (5) output:\nElaTBot generates a more accurate and informed response by incorporating the retrieved information. (b–d) Diﬀerences in predicted bulk\nmodulus with and without the RAG method for Ni\n3Al, g0-PE16 (Ni72.1Al10.4Fe3.2Cr1.0Ti13.3) andg-TiAl (Ti44Al56) as a function of temperature. The\nbulk modulus error percent decreased from 27.49% to 0.95% in 9 alloys with diﬀerent temperatures after using RAG.\n1630 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nenhanced prediction capabilities, allowing it to reason well\nbeyond its training set that contained minimal similar data.\nMaterial discovery\nThe knowledge-fused ElaTBot may also be applied to inverse\nmaterials design. By combining the domain-speci c LLM\n(ElaTBot) with a general LLM (GPT-4o), we can search for\nmaterials with specic bulk modulus for various applications.\nFig. 4(a) shows an example where we identify orthopedic\nmaterials for bone replacement with a bulk modulus similar to\nthat of bone (<50 GPa).\n4 Using the “material generation task”\nprompt template from ESI Table S4,† we identify the bulk\nmodulus target as <50 GPa and request the ElaTBot agent to\ngenerate potential material compositions. This process can be\nautomated via a Python script to obtain multiple material\ncompositions. The generated compositions are then passed to\nGPT-4o (dialogue record in ESI Fig. S3†) with more specic\nrequirements, such as corrosion resistance and biocompati-\nbility; i.e., a corrosion rate <0.3 mm per year and an LD\n50\n(median lethal dose) <3 g per kg body weight.44 This results in\na list of compositions that satisfy both the bulk modulus and\northopedic material criteria, along with explanations for each\nrecommendation. Fig. 4(b– d) and ESI Fig. S4– S6† show that this\nprocess can be extended to discover materials with high bulk\nmodulus (∼250 GPa) and low shear modulus, new corrosion-\nresistant materials with bulk modulus similar to stainless\nsteel (∼160 GPa),\n45 or materials for the protective layers for\nLiCoO2 electrode in lithium battery that could be used to\nstabilize high-capacity battery electrodes by providing structural\nsupport and accommodating volume changes during charge/\ndischarge cycles in lithium batteries. We generated protective\nlayer materials for LiCoO\n2 with bulk moduli ranging from 40 to\n120 GPa in increments of 20 GPa, with 500 materials per\nmodulus value. Aer screening, our workow identied several\npromising candidates. Notably, materials such as Li\n2S,46\nLi3SbS4,47 CaF2,48 Mg2SiO4,49 BaTiO3,50 Ti2AlC,51 and LiNbO3\n52\nfall within or near the generated bulk modulus range and have\nbeen experimentally validated as eﬀective protective layers for\nelectrodes. These applications demonstrate the potential of\nintegrating domain-specic and general LLMs to accelerate new\nmaterial discovery and design.\nFig. 4 Integration of a domain-speciﬁc LLM (ElaTBot) with a general LLM (GPT-4o) for materials discovery. The process begins by requesting\nElaTBot to generate material compositions with a target bulk modulus. Next, GPT-4o reﬁnes the results to identify compositions that meet\nspeciﬁc application requirements. Examples of applications include: (a) generating orthopedic materials with bulk modulus similar to bone, (b)\ndiscovering materials with high bulk modulus and low shear modulus, (c)ﬁnding new corrosion-resistant materials, and (d) identifying materials\nfor the protective layers of LiCoO2 electrode in lithium battery.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1631\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nBased on the prediction and generation abilities above, we\ndeveloped a multi-function agent interface (Fig. S2†) that allows\nresearchers to predict, generate, or engage in RAG-enhanced\nprediction through natural language dialogue, without the\nheavy load of coding.\nDiscussion\nWe demonstrated the potential of LLMs in predicting elastic\nconstant tensors and discovering materials with targeted elastic\nproperties. By introducing ElaTBot-DFT (to accurately predict\nelastic constants at 0 K) and ElaTBot (extention to nite\ntemperatures), we showcase the ability of LLMs to predict\nmaterial properties and discover materials with speci ed\nproperties. Our results show that LLMs, even when trained on\nrelatively small datasets, can outperform traditional ML\napproaches with carefully curated textual descriptions. The\nsuccess of our LLM-based approach can be attributed to several\nkey features. First, the Transformer architecture incorporates\nmechanisms such as LayerNorm, residual connections, and\ndropout to enhance model generalization capabilities. Second,\nrepresenting crystal structures as text tokens rather than\nnumerical features enables more accurate predictions, partic-\nularly for exact zero values that traditional numerical models\nstruggle to represent precisely. Third, the textual representation\nnaturally facilitates multimodal integration of crystal structures\nand chemical compositions. Finally, the text-based approach\nsimplies the development of multi-functional models that can\nperform numerous tasks traditionally requiring separate\nnumerical models. Furthermore, the combination of domain-\nspecic and general LLMs opens new avenues for materials\ndiscovery, while the incorporation of RAG enhances real-time\nlearning and improves the scope and accuracy of LLM\npredictions.\nDespite these promising results, several challenges remain,\nparticularly in ensuring the stability of continuous quantitative\npredictions. For example, minor variations in temperature,\nsuch as between 500.12 K and 500.13 K, may lead to inconsis-\ntencies in property predictions like the bulk modulus. To\naddress these issues, future work will focus on generating larger\ndatasets,\n53 developing multi-agent systems for incremental task-\nsolving,54 exploring novel digital encoding methods for LLMs,55\nand guiding LLMs to learn materials laws (such as the general\ntrend of decreasing elastic constant tensor values with\nincreasing temperature). These improvements, along with the\naddition of constraints or regularization techniques, may\nenhance the stability of numerical predictions.\nOur work presents a fresh perspective on using LLMs for the\nquantitative prediction of material properties and facilitating\ninverse material design. A key benet of domain-specic LLMs\nis the ability to interact with and generate results through\nnatural language, without requiring users to have extensive\nknowledge of the underlying ML techniques. This lowers the\nbarrier to entry for computational materials design and fosters\nbroader participation in theeld. The integration of domain-\nspecic and general-purpose LLMs allows for access to\nbroader research data, enhancing the synergy between\nmaterials science and AI. These advancements have the poten-\ntial to revolutionize both elds by accelerating innovation,\ndiscovery, and application.\nMethods\nData acquisition and processing\nWe trained ElaTBot-DFT, Darwin, the random forest model and\nthe MatTen model using material data containing elastic\nconstant tensors from the Materials Project (MP). Initially, 12\n128 materials with elastic constant tensor data calculated by\nDFT were available from MP (Fig. 1(a)). A er ltering out\nunreasonable entries, 10 520 valid samples remained. From this\nset, we allocated 9498 to the training set, 500 to the validation\nset used during training, and 522 to the test set (521 for MatTen\nbecause it does not support structures with element Ne). The\npartitioned dataset includes the material_id for each entry,\nensuring consistency across all methods with identical training/\nvalidation/test sets. The partitioning methodology involved\nextracting 5% of the materials from each crystal system as the\ntest set, with the remaining data forming a combined pool for\ntraining and validation sets. This combined pool was rst\nshuﬄed and then 5% of these data were randomly selected to\nform the validation set with axed random seed. The distri-\nbution of materials with diﬀerent crystal systems is shown in\nFig. 5(a). We compared the performance of all models on the\ntest set using the mean absolute error (MAE) and the coeﬃcient\nof determination (R\n2). To prepare the data for model input, we\nfollowed procedures appropriate for each model. For ElaTBot,\nwe constructed textual descriptions based on the scheme in\nTable S1,† using pymatgen\n37 and robocrystallographer36 to\nconvert composition and structural information into textual\ndescriptions (see ESI Fig. S7†). Darwin was trained with the\nsame prompt type 4 used for ElaTBot. The random forest model\nwas trained using Magpie feature vectors, which were derived\nfrom the elemental composition with pymatgen and mat-\nminer.\n56 For the MatTen model, we constructed crystal structure\ngraph neural networks following the original settings.28\nIn addition to the 10 520 data points for elastic constant\ntensors at 0 K, we manually extracted 1266 experimental elastic\nconstant tensor data points atnite temperatures from ref. 57.\nThe distribution of elastic constant tensor data at diﬀerent\ntemperatures for this dataset is shown in Fig. 5(b). To enable\nmultitasking in ElaTBot, we designed four tasks (Fig. 5(c)) and\nconverted material composition and structural information\ninto textual descriptions as outlined in ESI Table S4.† Given the\nlimited availability ofnite-temperature data, we did not create\na separate test set for this subset. Instead, we evaluated\npredictive performance on unseen alloy compositions,\nincluding cubic phase Ni\n3Al, g0-PE16, and tetragonal phaseg-\nTiAl.\nModel training and evaluation\nWe trained the ELaTBot-DFT and ElaTBot models using four\nNVIDIA V100, both based on the Llama2-7b pre-trained model.\nTraining the ElaTBot model required approximately 24 hours\n1632 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\non four NVIDIA V100 GPUs (this incurred a total cost of 331\nRMB z45 USD on the Bohrium platform). On the other hand,\ntraining the ElaTBot-DFT model (only based on DFT data at 0 K)\non the same platform required 5 hours (cost: 88 RMBz12\nUSD). These costs reect the computational resource eﬃciency\nof our approach. The Darwin model, also built on Llama2-7b,\nwas ne-tuned using a material science literature database.\nFor comparison, we implemented the random forest model\nusing scikit-learn\n58 and employed a crystal structure graph\nneural network for the MatTen model. The training process for\nElaTBot-DFT is shown in Fig. 5(d), and that of ElaTBot is shown\nin Fig. 5(e). To ensure a fair comparison, we standardized both\nthe training duration and the number of samples processed\nacross all models. Techniques such as early stopping were used\nto nalize the model when performance gains stagnated.\nHyperparameters for model training are detailed in the ESI.†\nFor ne-tuning, we applied LoRA+,\n59 a parameter-eﬃcient\nadaptation technique that extends basic LoRA.60 LORA+ allows\nthe adapter matrices to bene-tuned at diﬀerent learning rates,\nreducing GPU memory usage by approximately half without\nFig. 5 Dataset and model architecture for ElaTBot and ElaTBot-DFT. (a and b) Data distribution for Materials Project (MP) 0 K DFT dataset and\nﬁnite temperature dataset. For ElaTBot-DFT, we used data from the MP dataset, converting it to prompt type 4 (shown in Table S1†). The\ntransformed textual descriptions were separated to the training set, validation set, and test set and then used for training ElaTBot-DFT. For\nElaTBot, as shown in the lower part of (b), we combined data from the MP dataset and theﬁnite temperature dataset, then converted it into\nquestion and answer (Q&A, each Q&A pair is a task and we designed four tasks) as input and output for training ElaTBot, enabling ElaTBot to\nacquire multiple capabilities. (c) Number of Q&A entries used for training ElatBot, categorized by the speciﬁc tasks in the training. The elastic\nconstant tensor prediction task involves training the ElatBot to predict elastic constant tensors based on textual descriptions of materials. The\nbulk modulus prediction task requires the ElatBot to predict the bulk modulus from material textual descriptions. The material generation task\naims to enable the ElatBot to generate material chemical formulas based on given bulk modulus and temperature. The description inﬁlling task,\ngiven a description of the chemical formula and compositions, masks the formula with [MASK], and the ElaTBot is then expected toﬁll in [MASK]\nwith the correct chemical formula. (d) Model architecture for training ElaTBot-DFT. (e) Knowledge fusion training workﬂow for ElaTBot, detailing\nhow external data and tools are integrated to enhance model capabilities.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1633\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ncompromising data input capacity, thereby accelerating\ntraining. A detailed comparison between LoRA and LoRA+ is\nprovided in Fig. S8 of ESI.†\nThe LLMs (ElaTBot-DFT, ElaTBot, and Darwin) were\nmanaged using the Llama-factory61 framework, which facilitates\nmodel loading and parameter tuning. The architectural\nadvantages of the Transformer architecture of Llama-2,\nincluding LayerNorm, residual connections, and dropout\nmechanisms limit overtting on small datasets. We imple-\nmented warm-up strategies (a neural network training tech-\nnique where the learning rate is gradually increased to the\ninitial learning rate during therst few training epochs\n62) and\na cosine learning rate scheduler (that gradually reduces the\nlearning rate during training\n63) to ensure smooth gradient\nupdates when training on small datasets. The random forest\nand MatTen models were trained directly using Python and\nPyTorch. The LLMs were optimized by calculating cross-entropy\nloss, L\nCE ¼/C0 PN\ni¼1 yi logðpiÞ, whereyi is the true label andpi is\nthe predicted probability. The random forest model used\nsquared error loss,L SE ¼ 1\nN\nXN\ni¼1 ðyi /C0 ^yiÞ2, whereyi is the true\nvalue andˆyi is the predicted value. The MatTen model employed\nmean squared error lossL MSE ¼ 1\nN\nXN\ni¼1 ðyi /C0 ^yiÞ2 for optimi-\nzation, consistent with the method specied in ref. 28. These\nlosses guide the models in learning to accurately predict the\nelastic constant tensor.\nThe predicted elastic constant tensor is expressed in Voigt\nform:\nC\nij ¼\n2\n666\n6\n666\n6\n666\n6\n4\nC\n11 C12 C13 C14 C15 C16\nC21 C22 C23 C24 C25 C26\nC31 C32 C33 C34 C35 C36\nC41 C42 C43 C44 C45 C46\nC51 C52 C53 C54 C55 C56\nC61 C62 C63 C64 C65 C66\n3\n777\n7\n777\n7\n777\n7\n5\nThe average value of elastic constant tensorC\nij are calculated\nas follows:\nCij ¼ 1\n36\nX6\ni¼1\nX6\nj¼1\nCij\nThe bulk modulusK are calculated by pymatgen37 as follows:\nKvoigt ¼ 1\n9\nX3\ni¼1\nX3\nj¼1\nCij\nThe mean absolute error (MAE) and coeﬃcient of determi-\nnation (R2) are used to evaluate the model performance. The\nMAE is calculated as follows:\nMAE ¼ 1\nn\nXn\ni¼1\njyi /C0 ^yij\nR2 is\nR2 ¼ 1 /C0\nPn\ni¼1\nðyi /C0 ^yiÞ2\nPn\ni¼1\nðyi /C0 yÞ2\n;\nwhere yi represents the labeled average value of the elastic\nconstant tensor or bulk modulus from the dataset for each data\npoint i. ˆy\ni is the predicted average value of the elastic constant\ntensor or bulk modulus from the models for each data pointi.\nThe symbol/C22y represents the mean ofyi across all data points,\nand n is the total number of data points. The symmetry of the\nelastic constant tensor is checked by comparing the predicted\ntensors with the Voigt format matrix. Diﬀerent crystal symme-\ntries imply that certain components ofC\nij are zero and specic\nrelations exist betweenCij components (e.g., see ref. 31).\nFor nite-temperature predictions, ElaTBot generated elastic\nconstant tensors for Ni3Al andg0-PE16 atT = 90, 113, 142, 162,\n192, 223, 253, 283, 303, 333, and 363 K, and forg-TiAl atT = 30,\n50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, and\n298 K. Due to the limited training data and the discrete nature\nof the prediction of ElaTBot, we did a lineart to the predicted\nvalues and used this to evaluate the deviation from experi-\nmental data and analyze error trends.\nMaterials generation and RAG-enhanced prediction\nWe used gradio\n64 to build a user-friendly chat interface for\ninteracting with ElaTBot. This interface allows users to predict,\ngenerate, and perform RAG-enhanced prediction tasks through\nnatural language input without the need to engage directly with\ncode. While a default prompt, used during training, is pre-\nloaded into the interface, users can modify it as needed for\nspecic tasks.\nThe RAG-enhanced prediction ability of ElaTBot was enabled\nthrough the integration of RAG, which allows the model to\nperform real-time learning without requiring retraining. The\nknowledge base consists ofnite temperature, experimentally-\nmeasured, elastic constant tensor data for three materials\nfrom the literature: Ni\n3Al atT = 90, 113, 142, 162, 192, 223, 253,\n283, 300, 303, 333, 363, 400, 500, 600, 700, 800, 900, 1000 and\n1100 K,g\n0-PE16 atT = 90, 113, 142, 162, 192, 223, 253, 283, 300,\n303, 333, and 363 K, andg-TiAl atT = 30, 50, 70, 90, 110, 130,\n150, 170, 190, 210, 230, 250, 270, 290, and 298 K.42,43,65 To ensure\na rigorous, unbiased evaluation of our RAG-based system, the\ndatabase accessed by the RAG system excluded 18, randomly\nselected data points from the full knowledge base; these 18\npoints formed the test-set shown in ESI Table S5.† For materials\nabsent from the constructed knowledge base (specically those\nnot related to the three example alloy systems investigated in\nthis study), the RAG was not activated. To ensure reliability in\ncases of uncertainty, the prompt included the explicit instruc-\ntion: ‘If you don't know the answer, just say that you don't\nknow.’ In instances where the language model responded with\n‘don't know’ even aer RAG was applied, the agent system\nreverts to using the base LLM without retrieval assistance. The\nRAG module was implemented using langchain,\n66 and follows\n1634 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\na multi-step process including document loading, splitting,\nstorage, retrieval, and output generation. This process enhances\nthe ability of ElaTBot to update its knowledge and handle new\ndata eﬃciently, as outlined in Fig. 3(a).\nCode availability\nAll codes used in the paper are publicly accessible on GitHub\n(https://github.com/Grenzlinie/ElaTBot).\nData availability\nThe method section provides the models and algorithms\nemployed in this study, while specic parameter implementa-\ntions are available in ESI.† The data, codes, and processing\nscripts used in this study can be found at Figshare (DOI:https://\ndoi.org/10.6084/m9.gshare.28399757.v1).\nConﬂicts of interest\nThe authors declare no competing interests.\nAcknowledgements\nThe Authors would like to thank for startup funding from\nMaterials Innovation Institute for Life Sciences and Energy\n(MILES), HKU-SIRI in Shenzhen for support of this manuscript.\nThis work is supported by Research Grants Council, Hong Kong\nSAR through the General Research Fund (17210723, 17200424).\nT. W. acknowledges additional support by The University of\nHong Kong (HKU) via seed funds (2201100392, 2409100597)\nand Guangdong Natural Science Fund (2025A1515012129). We\nalso thank Xiaoguo Gong and Huaiyi Liu for their help in\nnding thenite temperature dataset.\nReferences\n1 L. Hu, P. L. Chee, S. Sugiarto, Y. Yu, C. Shi, R. Yan,et al.,\nHydrogel-Based Flexible Electronics, Adv. Mater. , 2023,\n35(14), 2205326, DOI:10.1002/adma.202205326.\n2 J. Luo, D. Zou, Y. Wang, S. Wang and L. Huang, Battery\nthermal management systems (BTMs) based on phase\nchange material (PCM): A comprehensive review, Chem.\nEng. J. , 2022, 430, 132741. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS1385894721043199.\n3 A. Chaves, J. G. Azadani, H. Alsalman, D. R. da Costa,\nR. Frisenda, A. J. Chaves, et al., Bandgap engineering of\ntwo-dimensional semiconductor materials, npj 2D Mater.\nAppl., 2020,4(1), 29, DOI:10.1038/s41699-020-00162-4.\n4 C. T. Wu, H. T. Chang, C. Y. Wu, S. W. Chen, S. Y. Huang,\nM. Huang,et al., Machine learning recommends aﬀordable\nnew Ti alloy with bone-like modulus,Mater. Today, 2020,\n34,4 1– 50. available from: https://www.sciencedirect.com/\nscience/article/pii/S136970211930759X.\n5 M. Fan, T. Wen, S. Chen, Y. Dong and C. A. Wang,\nPerspectives Toward Damage-Tolerant Nanostructure\nCeramics, Adv. Sci., 2024, 11(24), 2309834, DOI: 10.1002/\nadvs.202309834.\n6 B. Hu, S. Liu, B. Ye, Y. Hao and T. Wen, A Multi-agent\nFramework for Materials Laws Discovery, arXiv, 2024,\npreprint, arXiv:241116416, DOI:10.48550/arXiv.2411.16416,\navailable from:https://arxiv.org/abs/2411.16416.\n7 Z. Zheng, O. Zhang, H. L. Nguyen, N. Rampal,\nA. H. Alawadhi, Z. Rong,et al., ChatGPT Research Group\nfor Optimizing the Crystallinity of MOFs and COFs, ACS\nCent. Sci. , 2023, 9(11), 2161 – 2170, DOI: 10.1021/\nacscentsci.3c01087.\n8 A. M Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White and\nP. Schwaller, Augmenting large language models with\nchemistry tools, Nat. Mach. Intell. , 2024, 6(5), 525– 535,\nDOI: 10.1038/s42256-024-00832-8.\n9 L. M. Antunes, K. T. Butler and R. Grau-Crespo, Crystal\nStructure Generation with Autoregressive Large Language\nModeling, arXiv, 2024, preprint, arXiv:230704340, DOI:\n10.48550/arXiv.2307.04340\n, available from:https://arxiv.org/\nabs/2307.04340.\n10 P. Verma, M. H. Van and X. Wu. Beyond Human Vision: The\nRole of Large Vision Language Models in Microscope Image\nAnalysis, arXiv, 2024, preprint, arXiv:240500876, DOI:\n10.48550/arXiv.2405.00876, available from: https://\narxiv.org/abs/2405.00876.\n11 Z. Ren, Z. Ren, Z. Zhang, T. Buonassisi and J. Li,\nAutonomous experiments using active learning and AI,\nNat. Rev. Mater., 2023, 8(9), 563– 564, DOI: 10.1038/s41578-\n023-00588-4.\n12 J. Dagdelen, A. Dunn, S. Lee, N. Walker, A. S. Rosen,\nG. Ceder, et al., Structured information extraction from\nscientic text with large language models,Nat. Commun.,\n2024, 15(1), 1418, DOI:10.1038/s41467-024-45563-x.\n13 S. Liu, T. Wen, A. S. L. S. Pattamatta and D. J. Srolovitz, A\nprompt-engineered large language model, deep learning\nworkow for materials classication, Mater. Today, 2024,\n240– 249, DOI:10.1016/j.mattod.2024.08.028.\n14 B. Ni and M. J. Buehler, MechAgents: Large language model\nmulti-agent collaborations can solve mechanics problems,\ngenerate new data, and integrate knowledge,Extreme Mech.\nLett., 2024, 67, 102131. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS2352431624000117.\n15 Q. Zhang, K. Ding, T. Lyv, X. Wang, Q. Yin, Y. Zhang,et al.,\nScientic large language models: A survey on biological &\nchemical domains, arXiv, 2024, preprint, arXiv:240114656,\nDOI: 10.48550/arXiv.2401.14656, available from: https://\narxiv.org/abs/2401.14656.\n16 S. Ouyang, Z. Zhang, B. Yan, X. Liu, Y. Choi, J. Han,et al.,\nStructured Chemistry Reasoning with Large Language\nModels, arXiv, 2024, preprint, arXiv:231109656, DOI:\n10.48550/arXiv.2311.09656, available from: https://\narxiv.org/abs/2311.09656.\n17 S. Yu, N. Ran and J. Liu, Large-language models: The game-\nchangers for materials science research,Artif. Intell. Chem.,\n2024, 2(2), 100076. available from: https://\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1635\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nwww.sciencedirect.com/science/article/pii/\nS2949747724000344.\n18 T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput,et al.,\nLIFT: Language-Interfaced Fine-Tuning for Non-Language\nMachine Learning Tasks, arXiv, 2022, preprint,\narXiv:220606565, DOI: 10.48550/arXiv.2206.06565, available\nfrom: https://arxiv.org/abs/2206.06565.\n19 A. N. Rubungo, C. Arnold, B. P. Rand and A. B. Dieng, LLM-\nProp: Predicting Physical And Electronic Properties Of\nCrystalline Solids From Their Text Descriptions, arXiv,\n2023, preprint, arXiv:231014029, DOI: 10.48550/\narXiv.2310.14029, available from: https://arxiv.org/abs/\n2310.14029.\n20 K. Das, P. Goyal, S. C. Lee, S. Bhattacharjee and N. Ganguly,\nCrysMMNet: Multimodal Representation for Crystal\nProperty Prediction,arXiv, 2023, preprint, arXiv:230705390,\nDOI: 10.48550/arXiv.2307.05390, available from: https://\narxiv.org/abs/2307.05390.\n21 K. Choudhary, AtomGPT: Atomistic Generative Pretrained\nTransformer for Forward and Inverse Materials Design,J.\nPhys. Chem. Lett., 2024, 15(27), 6909– 6917, DOI: 10.1021/\nacs.jpclett.4c01126.\n22 N. Alampara, S. Miret and K. M. Jablonka. MatText: Do\nLanguage Models Need More than Text & Scale for\nMaterials Modeling?, arXiv, 2024, preprint,\narXiv:240617295, DOI: 10.48550/arXiv.2406.17295, available\nfrom: https://arxiv.org/abs/2406.17295.\n23 K. Li, A. N. Rubungo, X. Lei, D. Persaud, K. Choudhary,\nB. DeCost, et al., Probing out-of-distribution generalization\nin machine learning for materials,Commun. Mater., 2025,\n6(1), 9. available from: https://www.nature.com/articles/\ns43246-024-00731-w#citeas.\n24 R. Jacobs, M. P. Polak, L. E. Schultz, H. Mahdavi, V. Honavar\nand D. Morgan, Regression with Large Language Models for\nMaterials and Molecular Property Prediction, arXiv, 2024,\npreprint, arXiv:240906080, DOI:10.48550/arXiv.2409.06080,\navailable from:https://arxiv.org/abs/2409.06080.\n25 K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and B. Smit,\nLeveraging large language models for predictive chemistry,\nNat. Mach. Intell. , 2024, 6(2), 161 – 169, DOI: 10.1038/\ns42256-023-00788-1.\n26 P. R. Spackman, A. Grosjean, S. P. Thomas, D. P. Karothu,\nP. Naumov and M. A. Spackman, Quantifying Mechanical\nProperties of Molecular Crystals: A Critical Overview of\nExperimental Elastic Tensors,Angew. Chem., Int. Ed., 2022,\n61(6), e202110716, DOI:10.1002/anie.202110716.\n27 J. B. Levine, J. B. Betts, J. D. Garrett, S. Q. Guo, J. T. Eng,\nA. Migliori, et al., Full elastic tensor of a crystal of the\nsuperhard compound ReB2,Acta Mater., 2010, 58(5), 1530–\n1535. available from: https://www.sciencedirect.com/\nscience/article/pii/S1359645409007630.\n28 M. Wen, M. K. Horton, J. M. Munro, P. Huck and\nK. A. Persson, An equivariant graph neural network for the\nelasticity tensors of all seven crystal systems,Digit. Discov.,\n2024, 3, 869– 882, DOI:10.1039/D3DD00233K.\n29 T. Xie, Y. Wan, W. Huang, Z. Yin, Y. Liu, S. Wang,et al.,\nDARWIN Series: Domain Specic Large Language Models\nfor Natural Science,arXiv, 2023, preprint, arXiv:230813565,\nDOI: 10.48550/arXiv.2308.13565, available from: https://\narxiv.org/abs/2308.13565.\n30 A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards,\nS. Dacek, et al., Commentary: The Materials Project: A\nmaterials genome approach to accelerating materials\ninnovation, APL Mater., 2013,1(1), 011002.\n31 Y. Li, L. Voˇcadlo and J. P. Brodholt, ElasT: A toolkit for\nthermoelastic calculations, Comput. Phys. Commun., 2022,\n273, 108280. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS0010465521003921.\n32 V. Revi, S. Kasodariya, A. Talapatra, G. Pilania and\nA. Alankar, Machine learning elastic constants of multi-\ncomponent alloys, Comput. Mater. Sci., 2021, 198, 110671.\navailable from: https://www.sciencedirect.com/science/\narticle/pii/S0927025621003980.\n33 G. Vazquez, P. Singh, D. Sauceda, R. Couperthwaite, N. Britt,\nK. Youssef,et al.,E ﬃcient machine-learning model for fast\nassessment of elastic properties of high-entropy alloys,\nActa Mater. , 2022, 232, 117924. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS1359645422003068.\n34 T. Pakornchote, A. Ektarawong and T. Chotibut,\nStrainTensorNet: Predicting crystal structure elastic\nproperties using SE(3)-equivariant graph neural networks,\nPhys. Rev. Res. , 2023, 5, 043198, DOI: 10.1103/\nPhysRevResearch.5.043198.\n35 P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,\nN. Goyal, et al. , Retrieval-augmented generation for\nknowledge-intensive nlp tasks, Adv. Neural Inf. Process.\nSyst., 2020,33\n, 9459– 9474, DOI:10.5555/3495724.3496517.\n36 A. M. Ganose and A. Jain, Robocrystallographer: automated\ncrystal structure text descriptions and analysis, MRS\nCommun., 2019,9(3), 874– 881, DOI:10.1557/mrc.2019.94.\n37 S. P. Ong, W. D. Richards, A. Jain, G. Hautier, M. Kocher,\nS. Cholia, et al., Python Materials Genomics (pymatgen): A\nrobust, open-source python library for materials analysis,\nComput. Mater. Sci. , 2013, 68, 314– 319. available from:\nhttps://www.sciencedirect.com/science/article/pii/\nS0927025612006295.\n38 N. Gruver, A. Sriram, A. Madotto, A. G. Wilson, C. L. Zitnick\nand Z. Ulissi, Fine-Tuned Language Models Generate Stable\nInorganic Materials as Text, arXiv, 2024, preprint,\narXiv:240204379, DOI: 10.48550/arXiv.2402.04379, available\nfrom: https://arxiv.org/abs/2402.04379.\n39 L. Ward, A. Agrawal, A. Choudhary and C. Wolverton, A\ngeneral-purpose machine learning framework for\npredicting properties of inorganic materials, npj Comput.\nMater., 2016, 2(1), 16028, DOI: 10.1038/\nnpjcompumats.2016.28.\n40 T. Xie, Y. Wan, W. Huang, Y. Zhou, Y. Liu, Q. Linghu,et al.,\nLarge language models as master key: unlocking the secrets\nof materials science with GPT,arXiv, 2023, arXiv:230402213,\nDOI: 10.48550/arXiv.2304.02213, available from: https://\narxiv.org/abs/2304.02213.\n1636 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n41 C. A. Li and H. Y. Lee, Examining Forgetting in Continual\nPre-training of Aligned Large Language Models, arXiv,\n2024, arXiv:240103129, DOI: 10.48550/arXiv.2401.03129,\navailable from:https://arxiv.org/abs/2401.03129.\n42 F. Wallow, G. Neite, W. Schröer and E. Nembach, Stiﬀness\nconstants, dislocation line energies, and tensions of Ni3Al\nand of the gamma ’-phases of NIMONIC 105 and of\nNIMONIC PE16, Phys. Status Solidi A, 1987, 99(2), 483– 490,\nDOI: 10.1002/pssa.2210990218.\n43 K. Tanaka, Single-crystal elastic constants of gamma-TiAl,\nPhilos. Mag. Lett. , 1996, 73(2), 71 – 78, DOI: 10.1080/\n095008396181019.\n44 R. Gorejov´a, L. Haverov ´a, R. Ori ˇnakov´a, A. Ori ˇnak and\nM. Ori ˇnak, Recent advancements in Fe-based\nbiodegradable materials for bone repair, J. Mater. Sci. ,\n2019, 54(3), 1913– 1947, DOI:10.1007/s10853-018-3011-z.\n45 ToolBox TE, Metals and Alloys - Bulk Modulus Elasticity,\n2008, accessed: 26 September 2024, available from:https://\nwww.engineeringtoolbox.com/bulk-modulus-metals-\nd_1351.html.\n46 A. Sakuda, A. Hayashi, T. Ohtomo, S. Hama and\nM. Tatsumisago, All-solid-state lithium secondary batteries\nusing LiCoO2 particles with pulsed laser deposition\ncoatings of Li2S– P2S5 solid electrolytes, J. Power Sources,\n2011, 196(16), 6735– 6741.\n47 R. Matsuda, H. Muto and A. Matsuda, Air-Stable Li3SbS4– LiI\nElectrolytes Synthesized via an Aqueous Ion-Exchange\nProcess and the Unique Temperature Dependence of\nConductivity, ACS Appl. Mater. Interfaces , 2022, 14(46),\n52440– 52447.\n48 M. Li, H. Wang, L. Zhao, F. Zhang and D. He, The combined\neﬀect of CaF2 and graphite two-layer coatings on improving\nthe electrochemical performance of Li-rich layer oxide\nmaterial, J. Solid State Chem., 2019, 272,3 8– 46. available\nfrom: https://www.sciencedirect.com/science/article/pii/\nS0022459619300349.\n49 C. Bian, R. Fu, Z. Shi, J. Ji, J. Zhang, W. Chen,et al., Mg2SiO4/\nSi-Coated Disproportionated SiO Composite Anodes with\nHigh Initial Coulombic E ﬃciency for Lithium Ion\nBatteries, ACS Appl. Mater. Interfaces, 2022, 14(13), 15337–\n15345.\n50 C. Wang, M. Liu, M. Thijs, F. G. B. Ooms, S. Ganapathy and\nM. Wagemaker, High dielectric barium titanate porous\nscaﬀold for eﬃcient Li metal cycling in anode-free cells,\nNat. Commun., 2021,12(1), 6536.\n51 L. X. Yang, Y. B. Mu, R. J. Liu, H. J. Liu, L. Zeng, H. Y. Li,et al.,\nA facile preparation of submicro-sized Ti2AlC precursor\ntoward Ti2CTx MXene for lithium storage, Electrochim.\nActa, 2022, 432, 141152. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS0013468622013093.\n52 C. Wei, C. Yu, S. Chen, S. Chen, L. Peng, Y. Wu,et al.,\nUnraveling the LiNbO3 coating layer on battery\nperformances of lithium argyrodite-based all-solid-state\nbatteries under diﬀerent cut-oﬀ voltages, Electrochim. Acta,\n2023, 438, 141545. available from: https://\nwww.sciencedirect.com/science/article/pii/\nS0013468622017029.\n53 J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child,et al., Scaling Laws for Neural Language\nModels, arXiv, 2020, preprint, arXiv:200108361, DOI:\n10.48550/arXiv.2001.08361, available from: https://\narxiv.org/abs/2001.08361.\n54 A. Ghafarollahi and M. J. Buehler, AtomAgents: Alloy design\nand discovery through physics-aware multi-modal multi-\nagent arti cial intelligence, arXiv, 2024, preprint,\narXiv:240710022, DOI: 10.48550/arXiv.2407.10022, available\nfrom: https://arxiv.org/abs/2407.10022.\n55 S. Golkar, M. Pettee, M. Eickenberg, A. Bietti, M. Cranmer,\nG. Krawezik, et al., xVal: A Continuous Number Encoding\nfor Large Language Models, arXiv, 2023, preprint,\narXiv:230902989, DOI: 10.48550/arXiv.2310.02989, available\nfrom: https://arxiv.org/abs/2310.02989.\n56 L. Ward, A. Dunn, A. Faghaninia, N. E. R. Zimmermann,\nS. Bajaj, Q. Wang,et al., Matminer: An open source toolkit\nfor materials data mining,Comput. Mater. Sci., 2018, 152\n,\n60– 69. available from: https://www.sciencedirect.com/\nscience/article/pii/S0927025618303252.\n57 G. Simmons and H. Wang,Single Crystal Elastic Constants\nand Calculated Aggregate Properties: A Handbook,M .I .T .\nPress, Cambridge, Mass, 1971.\n58 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel,et al., Scikit-learn: Machine Learning\nin Python, arXiv, 2018, preprint, arXiv:12010490, DOI:\n10.48550/arXiv.1201.0490, available from: https://arxiv.org/\nabs/1201.0490.\n59 S. Hayou, N. Ghosh and B. Yu, LoRA+: Eﬃcient Low Rank\nAdaptation of Large Models, arXiv, 2024, preprint,\narXiv:240212354, DOI: 10.48550/arXiv.2402.12354, available\nfrom: https://arxiv.org/abs/2402.12354.\n60 E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,et al.,\nLoRA: Low-Rank Adaptation of Large Language Models,\narXiv, 2021, preprint, arXiv:210609685, DOI: 10.48550/\narXiv.2106.09685, available from: https://arxiv.org/abs/\n2106.09685.\n61 Y. Zheng, R. Zhang, J. Zhang, Y. Ye and Z. Luo, Llamafactory:\nUnied eﬃcient ne-tuning of 100+ language models,arXiv,\n2024, preprint, arXiv:240313372, DOI: 10.48550/\narXiv.2403.13372, available from: http://arxiv.org/abs/\n2403.13372.\n62 D. S. Kalra and M. Barkeshli, Why Warmup the Learning\nRate? Underlying Mechanisms and Improvements, arXiv,\n2024, preprint, arXiv:240609405, DOI: 10.48550/\narXiv.2406.09405, available from: https://arxiv.org/abs/\n2406.09405.\n63 I. Loshchilov and F. Hutter, SGDR: Stochastic Gradient\nDescent with Warm Restarts, arXiv, 2017, preprint,\narXiv:160803983, DOI: 10.48550/arXiv.1608.03983, available\nfrom: https://arxiv.org/abs/1608.03983.\n64 A. Abid, A. Abdalla, A. Abid, D. Khan, A. Alfozan and J. Zou,\nGradio: Hassle-Free Sharing and Testing of ML Models in\nthe Wild, arXiv, 2019, preprint, arXiv:190602569, DOI:\n© 2025 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 | 1637\nPaper Digital Discovery\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n10.48550/arXiv.1906.02569, available from:https://arxiv.org/\nabs/1906.02569.\n65 S. V. Prikhodko, H. Yang, A. J. Ardell, J. D. Carnes and\nD. G. Isaak, Temperature and composition dependence of\nthe elastic constants of Ni3Al,Metall. Mater. Trans. A, 1999,\n30(9), 2403– 2408, DOI:10.1007/s11661-999-0248-9#Abs1.\n66 K. Pandya and M. Holia, Automating Customer Service using\nLangChain: Building custom open-source GPT Chatbot for\norganizations, arXiv, 2023, preprint, arXiv:231005421, DOI:\n10.48550/arXiv.2310.05421, available from: https://\narxiv.org/abs/2310.05421.\n1638 | Digital Discovery,2 0 2 5 ,4,1 6 2 5–1638 © 2025 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 May 2025. Downloaded on 11/5/2025 5:37:20 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online"
}