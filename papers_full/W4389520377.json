{
  "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
  "url": "https://openalex.org/W4389520377",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101045986",
      "name": "Myra Cheng",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2954368363",
      "name": "Tiziano Piccardi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4298269718",
    "https://openalex.org/W4297847082",
    "https://openalex.org/W2059984441",
    "https://openalex.org/W1693521980",
    "https://openalex.org/W2930706539",
    "https://openalex.org/W3154151289",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W4287902409",
    "https://openalex.org/W90655423",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W4385572796",
    "https://openalex.org/W4229888985",
    "https://openalex.org/W3098567718",
    "https://openalex.org/W2109453861",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4225081783",
    "https://openalex.org/W3105994699",
    "https://openalex.org/W4221152344",
    "https://openalex.org/W4283155548",
    "https://openalex.org/W2106969952",
    "https://openalex.org/W3045889512",
    "https://openalex.org/W4384659608",
    "https://openalex.org/W3196026655",
    "https://openalex.org/W4366729489",
    "https://openalex.org/W2605604066",
    "https://openalex.org/W4366549000",
    "https://openalex.org/W1686131003",
    "https://openalex.org/W4316829821",
    "https://openalex.org/W4385570549",
    "https://openalex.org/W2154288935",
    "https://openalex.org/W2085918881",
    "https://openalex.org/W2992319600",
    "https://openalex.org/W2797580082",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W4321102425",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W3177828345",
    "https://openalex.org/W2478955812",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W2039210770",
    "https://openalex.org/W4283157985",
    "https://openalex.org/W3153888608",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W3123328377",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2983073293",
    "https://openalex.org/W2064367438",
    "https://openalex.org/W2993053740",
    "https://openalex.org/W1981331496",
    "https://openalex.org/W2124956094",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4283450324",
    "https://openalex.org/W4317734474",
    "https://openalex.org/W4385572088",
    "https://openalex.org/W4385571220",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W4236521339",
    "https://openalex.org/W1993584577",
    "https://openalex.org/W2038411619",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4378468563",
    "https://openalex.org/W2032228855",
    "https://openalex.org/W4377865902",
    "https://openalex.org/W4380354660",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W4283166422",
    "https://openalex.org/W2304206229",
    "https://openalex.org/W4378718328",
    "https://openalex.org/W2788897568"
  ],
  "abstract": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10853‚Äì10875\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nCoMPosT: Characterizing and Evaluating Caricature in LLM Simulations\nMyra Cheng, Tiziano Piccardi, Diyi Yang\nStanford University\nDepartment of Computer Science\n{myra, piccardi, diyiy}@cs.stanford.edu\nAbstract\nRecent work has aimed to capture nuances of\nhuman behavior by using LLMs to simulate\nresponses from particular demographics in set-\ntings like social science experiments and public\nopinion surveys. However, there are currently\nno established ways to discuss or evaluate the\nquality of such LLM simulations. Moreover,\nthere is growing concern that these simulations\nare flattened caricatures of the personas that\nthey aim to simulate, failing to capture the\nmultidimensionality of people and perpetuating\nstereotypes. To bridge these gaps, we present\nCoMPosT, a framework to characterize LLM\nsimulations using four dimensions: Context,\nModel, Persona, and Topic. We use this frame-\nwork to measure open-ended LLM simulations‚Äô\nsusceptibility to caricature, defined via two cri-\nteria: individuation and exaggeration. We eval-\nuate the level of caricature in scenarios from\nexisting work on LLM simulations. We find\nthat for GPT-4, simulations of certain demo-\ngraphics (political and marginalized groups)\nand topics (general, uncontroversial) are highly\nsusceptible to caricature.\n1 Introduction\nLarge language models (LLMs) have shown\npromise in capturing social nuances and human\nbehavior. For instance, researchers have repro-\nduced results from social science experiments and\npublic opinion surveys using LLMs (Argyle et al.,\n2023; Aher et al., 2023; Santurkar et al., 2023,inter\nalia). More broadly, interest in LLM simulation is\nrapidly growing, and the possibility of using LLMs\nto simulate human behaviors has far-reaching ap-\nplications in fields like education (Markel et al.,\n2023), product design (Park et al., 2022, 2023a),\npsychology (Binz and Schulz, 2023), healthcare\n(Weizenbaum, 1966; Bassett, 2019), skill training\n(Hollan et al., 1984; Jones et al., 1999), and law\n(Hamilton, 2023). These simulations are a sort of\ndigital compost‚Äîany new insight into human be-\nThe CoMPosT Framework\nContext Where and when does the simulated\nscenario occur?\nModel What LLM is used?\nPersona Whose opinion/action is simulated?\nTopic What is the simulation about?\nTable 1: Dimensions of the CoMPosT framework. We\nuse these dimensions to characterize LLM simulations\nand measure their susceptibility to caricature.\nhavior that they provide draws upon the organic\nmaterial (human data) used to train LLMs.\nSuch applications currently have little to no\nmechanisms for comprehensive evaluation or care-\nful deployment. Evaluation of such simulations has\nbeen limited to either (1) replicating existing results\nor (2) assessing believability. Both paradigms have\ndrawbacks: (1) Replication limits us to only re-\nproducing already-known behavior, and does not\nsupport the validation or evaluation of any simula-\ntion behaviors beyond those highly correlated with\nexisting results from human studies. Also, existing\nresults are typically quantified as categorical dis-\ntributions across multiple-choice answers, so there\nis no way to directly evaluate open-ended genera-\ntions, and such results may have been ‚Äúmemorized‚Äù\nfrom the LLMs‚Äô training data (Lewis et al., 2021;\nElangovan et al., 2021). Thus, replication does not\nfacilitate new insight into human behavior. Further-\nmore, while (2) Believability is useful in certain\nsettings, such as entertainment (Bates et al., 1994),\nit is susceptible to the biases and fallacies of hu-\nman judgment: psychology literature shows that\npeople are more likely to believe stereotypes about\ngroups with which they have less personal experi-\nence (Plous, 2003; Bar-Tal et al., 2013), and beliefs\nare easily influenced (Blair et al., 2001; Jussim\net al., 2016). Recent work has also demonstrated\nthat human judgment is insufficient for assessing\nAI (Schneider et al., 2020; Peng et al., 2022; V o-\ndrahalli et al., 2022; Veselovsky et al., 2023).\n10853\nPaper Context Model Persona Topic\nPark et al. (2022) discussion guided by \ncommunity norms (O)\nGPT-3 davinci users of various Reddit communities topics of various Reddit communities\nArgyle et al. (2022) public opinion survey (O) GPT-3 davinci ideology, partisanship, race, gender, \nincome, age\npolitical groups\nHorton (2023) economics experiments \n(MC)\nGPT-3.5 \ndavinci\npolitical/economical ideologies social preferences, fairness in market \nscenarios, status quo bias, etc.\nHamilton (2023) Supreme Court cases (MC) GPT-2 Supreme Court justices Supreme Court cases\nJiang et al. (2022) posting on Twitter (O) GPT-2, GPT-3 Republican & Democrat Twitter users controversial political figures & groups\nFigure 1: Mapping Existing Work Using CoMPosT.Existing work on LLM simulations can be compared using\nour framework. MC and O denote multiple-choice and open-response respectively. More examples are in Table A6.\nToward clearer documentation of this emerging\nline of work, we first present a descriptive frame-\nwork that taxonomizes LLM simulations using four\ndimensions: Context, Model, Persona, and Topic\n(CoMPosT) (Table 1). Our framework facilitates\ncomparison across existing work on LLM simula-\ntions (Figure 1).\nNext, we introduce a new evaluation metric that\naddresses growing concerns of modal responses\nand essentializing narratives in LLM outputs (San-\nturkar et al., 2023; Cheng et al., 2023b; Shumailov\net al., 2023). Our metric focuses on a simulation‚Äôs\nsusceptibility to caricature: an exaggerated narra-\ntive of the persona (the demographic that we aim\nto simulate) rather than a meaningful response to\nthe topic (Figure 2). Caricatured simulations are\nconcerning because they a) fail to capture the real\nnuances of human behavior, thus limiting the use-\nfulness of simulations and b) perpetuate mislead-\ning descriptions, stereotypes, and essentializing\nnarratives about demographic groups. We define\ncaricature using two criteria: individuation and ex-\naggeration (Section 3). To measure individuation,\nwe assess whether outputs from the given simula-\ntion can be differentiated from the default response\nfor the topic. To measure exaggeration, we use\na ‚Äúcontextualized semantic axis‚Äù whose two poles\nare the defining characteristics of the persona and\ntopic dimensions respectively.\nWe evaluate the level of caricature in scenar-\nios from existing work on LLM simulations. We\nfind that for GPT-4, simulations of certain demo-\ngraphics (political and marginalized race/ethnicity\ngroups) and topics (general, uncontroversial) are\nmore susceptible to caricature. Our main contri-\nbutions are: (1) CoMPosT, a framework for char-\nacterizing the dimensions of LLM simulations of\nhuman behavior (Section 2), (2) a novel method\nthat relies on the persona and topic dimensions of\nCoMPosT to measure simulations‚Äô susceptibility to\nSimulation Topic: Computers and Electronics\nGenerated nonbinary person responses:\n‚ÄúI'm interested in getting some recommendations for any cool devices \nthat might particularly appeal to nonbinary individuals or help increase \nour visibility and representation. üòäüåàüíª\n‚Äù ‚ÄúAs a nonbinary individual, I \nwant to create an inclusive and comfortable gaming/streaming space for \nmyself, as well as others in the LGBTQ+ community. ‚Äù\n‚ÄúI recently upgraded my desktop PC with a new graphics card and SSD, \nand I'm really impressed with the performance boost I got from these \nupgrades. ‚Äù ‚ÄúIt's interesting to see how rapidly technology has evolved \nover the past few decades. From the first personal computers to \nsmartphones, and now we have AI and IoT making significant impacts... ‚Äù\nGenerated person responses:\nFigure 2: Examples of GPT-4 generated responses for\nsimulations with the topic Computers and Electronics.\nFor the simulation of a nonbinary person‚Äôs response,\nthe generations are focused on identity-related issues,\nwhile the simulation of a person‚Äôs response is topical.\nThe former constructs a homogenous narrative that de-\nfines nonbinary people only by LGBTQ+ activism. We\nprovide more qualitative examples in Appendix A.\ncaricatures (Section 4), and (3) experiments in dif-\nferent contexts (Section 5) toward an analysis of the\ndimensions that are most susceptible to caricature\n(Section 6). We conclude with actionable recom-\nmendations and considerations for those interested\nin LLM simulation (Section 7).1\n2 CoMPosT: Taxonomizing Simulations\nWe introduce CoMPosT, a descriptive framework\nwith four dimensions to characterize LLM simu-\nlations: Context, Model, Persona, and Topic. In-\nspired by existing descriptive frameworks for AI\nfairness (Tubella et al., 2023), our framework pro-\nvides a shared language to understand and articu-\nlate similarities and differences across LLM simu-\nlations. Context, persona, and topic are specified in\nthe prompt, while model is determined externally.\nWe map existing work on LLM simulations using\nthese dimensions (Figure 1 and Table A4).\n1The code and data is available at\nhttps://github.com/myracheng/lm_caricature.\n10854\nContext The output from a simulation is neces-\nsarily affected by the context of where and when\nthe imagined situation takes place. For instance, a\nformal interview response varies drastically from a\nuser‚Äôs comment on Twitter or Reddit. The context\nincludes relevant structural factors and embeds in-\nformation about the norms of the situation. Each\ncontext has its own unique set of norms, which may\nbe explicit, as in the case of online communities\nwith written-down rules, or implicit (Chandrasekha-\nran et al., 2018; Ziems et al., 2023). Context also\nincludes the phrasing of the prompt itself, which\naffects the output‚ÄîLLMs are notoriously sensitive\nto prompt engineering (Zamfirescu-Pereira et al.,\n2023). The desired granularity of outcome also\narises from the phrasing of the prompt and thus\nis embedded in the context: a simulation scenario\nmay be worded to ask for a choice between binary-\nor multiple-choice options (such as in many so-\ncial science experiments and public opinion polls)\nor for an open-ended output. Previous work on\nevaluating simulations has largely focused on us-\ning LLMs to reproduce scenarios in which humans\nare asked to choose among a fixed number of op-\ntions without specifying the context, as it is more\nchallenging to evaluate the quality of open-ended\nresponses. We bridge this gap by offering a metric\nfor the latter.\nModel The LLM used to produce the simula-\ntion affects the quality and other characteristics\nof a simulation. Differences may arise from varia-\ntions in models‚Äô training data and processes, includ-\ning instruction-tuning, fine-tuning, and/or value\nalignment efforts (Solaiman and Dennison, 2021;\nOuyang et al., 2022; Bakker et al., 2022).\nPersona The persona refers to the entity whose\nopinions/actions the simulation aims to study and\nreplicate. This persona may include attributes that\nare relatively static (e.g., race/ethnicity), slowly\nchange over time (e.g., age), or temporary and of-\nthe-moment (e.g., emotional status) (Yang, 2019).\nIt may also refer to a specific individual.\nTopic The topic of the simulation may be a partic-\nular subject of discussion, question, or other event\nto which a response is desired. Topics vary in speci-\nficity, from very general (such as a single word that\ncaptures a broad conversation category) to very\nspecific (such as a specific situational question in a\npsychology experiment).\nThese four dimensions capture a wide range of\npossible simulation scenarios, many of which have\nnot yet been well-explored. Across existing work,\nwe find that researchers typically use a state-of-the-\nart model and choose a particular context while\nvarying the more salient dimensions of persona\nand topic. We denote the simulation scenario as\nSp,t,c, as it is associated with a prompt containing\npersona p, topic t, and context c (Table A4). Our\nevaluation methodology and results uses these di-\nmensions of CoMPosT to understand how different\nsimulations may result in caricatures. Specifically,\nwe explore how the relationship between the di-\nmensions of persona and topic help characterize\nthe extent of caricature in simulations.\n3 Background: Caricature\n3.1 Definition of Caricature\nBuilding upon Lynch (1927)‚Äôs discussion of how\ncaricatures are misrepresentations that have some\nsense of truth to the subject by reflecting ‚Äúsalient\npeculiarities,‚Äù Perkins (1975) define caricature as\n‚Äúa symbol that exaggerates measurements relative\nto individuating norms.‚Äù In so doing, Perkins iden-\ntifies two key characteristics of caricature: exagger-\nation and individuation. A caricature is a depiction\nthat not only exaggerates particular features of the\nsubject but also exaggerates in a manner that mean-\ningfully differentiates the subject from others. The\nexaggeration is done in such a way that it individ-\nuates by remaining faithful to the properties that\ndistinguish the subject from others (thus, a com-\nplete distortion is not a caricature).\nThis inspires our definition of caricature in the\nLLM simulation context: given that a subject has\nsome defining characteristics, a caricature exagger-\nates these characteristics in a way that amplifies\nthe ability to identify (i.e., individuate) the subject\nfrom the caricature. In CoMPosT terms, a simula-\ntion‚Äôs level of caricature is the degree to which it\nexaggerates the individuating characteristics that\nare emblematic of the persona beyond a meaning-\nful, topical response to the scenario.\nNote that in some cases, it may be acceptable for\nthe persona to influence the simulation, i.e., individ-\nuation alone does not entail caricature. For exam-\nple, opinions on some topics differ greatly based on\ndemographic. A caricature occurs when the simula-\ntion both individuates and exaggerates the defining\ncharacteristics of the imagined generic responses\nof that persona. Previous work has documented\nhow such imagined personas reflect stereotypes,\n10855\nboth inside and outside the LLM context (Marsden\nand Haag, 2016; Cheng et al., 2023b). Thus, carica-\ntures not only fail to capture the diversity of human\nbehavior but also may rely on stereotypes.\n3.2 Implications of Caricature\nStereotypes We rely on psychology literature\nthat broadly defines stereotypes as generalizations\nabout the characteristics of a social category, such\nas associating a social category with a particular\nrole or trait (Heilman, 2001; Fiske et al., 2002; Cao\net al., 2022; Kambhatla et al., 2022). The norma-\ntive value of stereotypes is context-dependent; for\ninstance, stereotypes can help foster a sense of au-\nthenticity (Marsden and Haag, 2016), while even\nseemingly-positive stereotypes can have harmful\nimplications (Fiske et al., 2002; Czopp et al., 2015).\nStereotypes and caricature, while closely related,\nare distinct in that a caricature may be a specific de-\npiction of a stereotype: scholars have documented\ncaricatures of stereotypes in various domains and\nhow they facilitate misogyny, racism, and other\nforms of oppression (Slavney, 1984; Brown, 2010;\nGottschalk and Greenberg, 2011; Takayama, 2017;\nBow, 2019). Such caricatures have historically\nbeen used in literature and media to justify slavery,\nimperialism, and war (Demm, 1993; Kriz, 2008).\nBut even when caricatures do not contain stereo-\ntypes, they have concerning implications of ho-\nmogenous narratives.\nMisleading Homogeneity Caricatures foster ho-\nmogenous narratives that do not reflect the full di-\nversity of the personas they aim to simulate, which\nlimits the utility of the simulation. This concern\nbuilds upon previous work: Grudin (2006) discuss\nhow personas can result in systematic errors in un-\nderstanding human behavior, Cheng et al. (2023b)\ncharacterize the harms of LLMs reflecting essen-\ntializing narratives about demographic groups, and\nSanturkar et al. (2023) show that certain instruction-\ntuned LLMs tend to generate modal responses. Oth-\ners have explored the linguistic nuances within\ncomplex social categories and the ramifications of\nignoring heterogeneity within social groups (Bam-\nman et al., 2014; Hanna et al., 2020; Cheng et al.,\n2023a).\nThese harms of caricature are also articulated\nby feminist scholars who have discussed how\n‚Äúwomen in the Two-Thirds World...are constructed\nas one-dimensional, oppressed caricatures without\nan understanding of their real experiences, agency,\nand struggles‚Äù (Mohanty, 1988; Aneja, 1993; Ku-\nmar et al., 2019). This literature reveals that\neven when the caricatures are not overtly negative,\nsuch one-dimensional depictions are still damaging\nand harmful. Overlooking diversity within demo-\ngraphic groups has been connected to real-world\nharms including misprediction and medical mis-\ndiagnosis (McCracken et al., 2007; Borrell et al.,\n2021; Read et al., 2021; Wang et al., 2022).\n4 Caricature Detection Method\nThe two key aspects of caricature are individua-\ntion and exaggeration. To measure the amount of\ncaricature in a given simulation Sp,t,c, our method\nhas three steps, each of which rely on the persona\nand topic dimensions of CoMPosT (Figure 3): (1)\ndefining defaults, (2) measuring individuation, and\n(3) measuring exaggeration. Note that this frame-\nwork is sequential, as (3) is only necessary if the\nsimulations can be individuated. Otherwise, we can\nhalt after step (2) since individuation is a necessary\ncriterion for caricature.\n4.1 Defining Defaults\nA simulation Sp,t,c is a caricature if it has more\nof the defining characteristics associated with the\npersona p and less of the defining characteristics\nassociated with the topic t. We first identify these\ndefining characteristics using the default-persona\nsimulation S_,t,c (simulation that does not men-\ntion any specific persona) and the default-topic\nsimulation Sp,_,c (simulation that does not men-\ntion any specific topic). Note that this default does\nnot reflect a universal default but rather a model-\nand context-specific default: previous work has\nshown that LLMs implicitly default to a particu-\nlar set of perspectives (Western, white, masculine,\netc.) (Santy et al., 2023). We use these defaults\nas a comparison point for caricature to isolate the\ndefining characteristics of particular dimensions.\nFor the default-persona simulation, we use a\nprompt where in lieu of a specific persona, we use\nan unmarked default term like ‚Äúperson‚Äù or ‚Äúuser.‚Äù\nThus, the outputs reflect the topic and context rather\nthan any particular persona. (Again, such words\nare not true defaults and are inextricably tied to\nsocietal norms: in English, the word ‚Äúperson‚Äù is\noften conflated with ‚Äúman,‚Äù a phenomenon also\npresent in web data and language models (Bailey\net al., 2022; Wolfe and Caliskan, 2022).)\nFor the default-topic simulation , we use a\n10856\nprompt where no topic is specified. Thus, the\noutputs from these prompts reflect the particular\npersona rather than a response to the topic. It is\nwell-documented that this type of prompt results in\noutputs that reflect stereotypes, both when asked\nto humans and to LLMs (Kambhatla et al., 2022;\nCheng et al., 2023b). Note that even if we expect\nthe output to change based on the persona, the\nresponse should still be distinct and not defined\nby the same distinguishing characteristics as the\ndefault-topic simulation for a given persona.\n4.2 Measuring Individuation\nWe operationalize the desiderata of individuation\nusing differentiability from default: we examine\nwhether the given simulation Sp,t,c is differentiable\nfrom the default-persona simulation S_,t,c. If not,\nthen Sp,t,c cannot be a caricature.\nWe use a binary classifier (specifically, a random\nforest classifier implemented using scikit-learn) to\ndifferentiate between outputs from the target simu-\nlation of interest Sp,t,c and those from the default-\ntopic simulation S_,t,c based on the outputs‚Äô contex-\ntualized embeddings. We compute contextualized\nembeddings using the pre-trained Sentence-BERT\nmodel all-mpnet-base-v2 (Reimers and Gurevych,\n2019). To create the training and test datasets, we\nuse a stratified 80/20 split onSp,t,c and S_,t,c to pre-\nserve the balance between the classes. We report\nthe accuracy2 of the classifier on the test dataset\nas our measure of individuation. Note that this\nmeasure is agnostic to the particular choice of dif-\nferentiator and contextualized embedding model,\nand we show results with other choices in Appendix\nB.\nThis score alone is necessary but insufficient for\nidentifying caricature, as a caricature must also\nexaggerate, which we measure next.\n4.3 Measuring Exaggeration\nWe define caricature as text having more of (and\nexaggerating) the defining characteristics associ-\nated with persona and less of those associated with\ntopic. Unlike individuation, exaggeration requires\na more nuanced measure than differentiation from\nthe default-topic simulation: if an output mentions\nthe topic frequently, it can easily be differentiated,\nbut it may still be a caricature. (Note that it is\n2In our experiments, the classes are balanced since we\ngenerate 100 outputs for each simulation, but this measure\ngeneralizes to contexts with imbalanced classes.\nacceptable for a simulation to have many topic-\nrelated words.) Instead, we measure the extent\nto which the defining characteristics of the per-\nsona are exaggerated in the target simulation via\npersona-topic semantic axes.\nSpecifically, we construct contextualized seman-\ntic axes, a method introduced by Lucy et al. (2022),\nto capture whether Sp,t,c is more similar to the\ndefining characteristics of the personap or the topic\nt. Our semantic axes have two poles, Pp and Pt,\nreflecting the persona p and the topic t. To con-\nstruct the set of seed words, we use the Fightin‚Äô\nWords method (Monroe et al., 2008) to identify\nthe words that statistically distinguish Sp,_,c from\nS_,t,c. We first compute the weighted log-odds\nratios of the words between S_,t,c vs. Sp,_,c. To\ncontrol for variance in words‚Äô frequencies, we use\nthe following prior distribution: other texts where\nthe personas/topics are either p/t respectively or\n_ (i.e., the default). Then, we take the words that\nare statistically significant (have z-score > 1.96)\nas the sets of seed words Wp and Wt for the corre-\nsponding poles Pp and Pt (Table A3).\nWe represent each word w ‚ààW as the mean\nof the contextualized embeddings of the sentences\ncontaining that word w across S_,t,c and Sp,_,c. We\ndefine the semantic axis\nVp,t = 1\nk\nk‚àë\ni=1\npi ‚àí 1\nm\nm‚àë\nj=1\ntj, (1)\nwhere pi/tj is a word in Wp/Wt respectively, i.e.,\nwe represent Pp/Pt as the mean of the embeddings\nof Wp/Wt respectively. This subtraction-based\naxis allows for scaling relative to how closely-\nrelated the topic and persona are.\nTo evaluate exaggeration, we compute the aver-\nage cosine similarity of a given simulation‚Äôs con-\ntextualized embedding to this axis:\ncos(Sp,t,c, Vp,t) =\n‚àën\ni=1 cos(Si\np,t,c, Vp,t)\nn , (2)\nwhere Si\np,t,c refers to individual outputs, i.e. i =\n1, 2, ..., nfor n outputs from the same simulation\nSp,t,c.\nThe final value we report as the measure of exag-\ngeration is this value normalized to lie between 0\nand 1 (by scaling it relative to the cosine similarity\nof the default-persona and default-topic simulations\nwith the axis):\ncos(Sp,t,c, Vp,t) ‚àícos(S_,t,c, Vp,t)\ncos(Sp,_,c, Vp,t) ‚àícos(S_,t,c, Vp,t). (3)\n10857\nGiven simulation S with persona p and topic t‚Ä¶\n1. Generate default-topic & \ndefault-persona simulations\n2. Measure Individuation: \nDiÔ¨Äerentiability From Default\nDefault-persona: ‚ÄúA person ºs \ncomment about t‚Ä¶ ‚Äù\np\nt\n3. Measure Exaggeration: \nPersona-Topic semantic axis\nDefault-topic: ‚ÄúA p ºs \ncomment‚Ä¶ ‚Äù\nAccuracy of classifier that \ndiÔ¨Äerentiates between \ndefault-persona and S\nCosine similarity\nof S and axis\nFigure 3: Our method to measure caricature in LLM simulations. We rely on comparing the defining character-\nistics of the persona and topic dimensions to measure individuation and exaggeration.\nWe perform internal and external validation of these\npersona-topic semantic axes (Appendix C).\n5 Experiments\nWe use our method to evaluate simulations in vari-\nous contexts that have been used in previous work\nto demonstrate the capabilities of LLM simulation\n(Park et al., 2022; Santurkar et al., 2023; Jiang\net al., 2022). Our experiments are focused on the\ntwo most widely-used contexts of (1) an online fo-\nrum setting and (2) a question-answering interview\nsetting. We also evaluate the Twitter context as an\nadditional robustness study (Appendix G). These\nchoices are based on our survey of the literature on\nLLM simulations (Figure 1, Table A6): Among 15\npapers in this area, we found that six use the con-\ntext of a virtual community or society and four rely\non an open-ended interview or survey context. The\nremaining five are in various question-answering\ncontexts, so conclusions about the (2) can also pro-\nvide insight into these types of simulations.\nWe use the state-of-the-art GPT-4 model for all\nexperiments (OpenAI, 2023); like others (Dubois\net al., 2023), we find that open-source LLMs and\nolder models are worse at simulation tasks, yield-\ning outputs that are unrealistic and significantly\nlower in quality. For each simulation setting Sp,t,c,\nwe generate 100 outputs and average all results\nacross them. See Appendix E for a power analysis\nof this sample size. The full details for each set-\nting, including topic lists, persona lists, and default-\ntopic/persona prompts, are in Appendix D.\n5.1 Online Forum\nPark et al. (2022) demonstrate the believability of\nLLM simulations of users in online forums. Follow-\ning their prompting format, we use the prompt: ‚ÄúA\n[persona] posted the following comment on [topic]\nto an online forum:‚Äù3 We explore such simulations\nusing 15 different personas (5 race/ethnicities, 3\ngenders, 3 political ideologies, 3 ages, and the neu-\ntral ‚Äúperson‚Äù) and 30 pairs of topics.\nFor topics, we aim to cover a wide range of\ncommon topics that vary in (1) specificity (e.g.,\novercoming fear of driving is much more specific\nthan cars) and (2) level of controversy (e.g., abor-\ntion is much more controversial than health). To\ncover both dimensions, we use topics from Wiki-\nHow, which is a knowledge base with a wide range\nof topics (Koupaee and Wang, 2018), and from\nProCon.org, which is a website that lists popular\ncontroversial topics and has been used in the NLP\ncontext to study stance and argumentation (Misra\net al., 2016; Hosseinia et al., 2020). We use the first\n15 categories from WikiHow‚Äôs ‚Äúpopular categories‚Äù\nwebpage and randomly sample an associated spe-\ncific ‚Äúhow-to‚Äù for each category. For ProCon.org,\nwe randomly sample 15 topics from ProCon.org‚Äôs\n‚Äúdebate topics‚Äù webpage. Each topic on the page is\nlisted in both more general and more specific word-\ning, e.g., abortion and should abortion be legal?\nThus, for each sampled subject, we use both the\ngeneral and specific versions as topics.\n5.2 Interview\nVarious previous works simulate opinions of\ndifferent demographics using an interview-style\nprompt (Argyle et al., 2023; Santurkar et al., 2023;\nH√§m√§l√§inen et al., 2023, inter alia). We reproduce\nthe public opinion survey simulation context from\nSanturkar et al. (2023), using the prompt:\n‚ÄúBelow you will be asked to provide a\nshort description of your identity and\n3Park et al. (2022) also include an HTML <span> tag to\nimprove content quality. We find that this is only necessary\nfor older models and not GPT-4.\n10858\nthen answer some questions.\nDescription: I am [persona].\nQuestion: [topic]\nAnswer:‚Äù\nFor topics, we randomly sample 30 questions\nfrom the Pew Research‚Äôs American Trends Panel\nsurvey questions that Santurkar et al. (2023) iden-\ntify as ‚Äúmost contentious‚Äù in their OpinionQA\ndataset. We convert the multiple-choice questions\ninto open-ended ones by removing the multiple-\nchoice answer options. For personas, we use the\nsame 15 personas as described in Section 5.1.\n6 Results and Discussion\nWe apply our caricature detection method to eval-\nuate the simulations produced in these different\ncontexts. We further operationalize the CoM-\nPosT framework by aggregating the individuation\nand exaggeration scores across the dimensions of\ntopic and persona. This enables us to analyze the\ntopics and personas that lead to the most caricatures\nacross different contexts. For instance, to exam-\nine what personas lead to the most caricatures in\na particular context, we compute the mean score\nfor each persona across all simulations (varying\nin topic) for that persona and then compare these\nscores. We also report results from additional ex-\nperiments that explore the influence of the context\ndimension in Appendix F.\n6.1 Simulations of all personas can be\nindividuated from the default-persona\nThe mean individuation score and 95% confidence\ninterval for every persona is > 0.5, i.e., each per-\nsona can be meaningfully individuated from the\ndefault-persona at a rate better than random chance.\nMean individuation scores across the online fo-\nrum and interview contexts are in Figure 4. We\nsee that the woman and man personas have lower\nmean scores in the online forum context, while in\nthe interview context, the mean score is > 0.95\nfor every persona‚Äîthus, this score is not informa-\ntive for comparing caricature across personas. The\ndifference in score between contexts arises from\ndifferences in the sample distributions: compared\nto the online forum context, the interview context\nsimulations have lower variability, so they are eas-\nier to individuate.\n6.2 Exaggeration scores reveal the personas\nand topics most susceptible to caricature\nNext, we examine the exaggeration scores, i.e., the\nsimilarities across the simulations to their corre-\nsponding persona-topic axes (Figure 5). Since al-\nmost all the simulations are able to be individuated,\nwe use the exaggeration score as a proxy for cari-\ncature, i.e., a given simulation is highly susceptible\nto caricature if it has a high exaggeration score.\n6.2.1 Caricature ‚Üë: Topic specificity ‚Üì\nIn the online forum context, among the topics, we\nfind that the more general topics resulted in higher\nexaggeration scores, and thus higher rates of car-\nicature, while the more specific topics had much\nlower rates of caricature (Figure 5). In particular,\nthe general, uncontroversial topics have highest\nexaggeration scores. The top five topics with the\nhighest mean rates of caricature are Health, Phi-\nlosophy and Religion , Education and Communi-\ncations, Relationships, and Finance and Business.\nTo explore this pattern further, we experiment on\na fine-grained range of topic specificity: For the\ntopic with highest exaggeration score (Health), we\ngenerate simulations for a range of related topics\nwith 5 levels of specificity (Appendix D.1.1). We\nfind that this pattern holds: the level of caricature\ndecreases as the specificity of the topic increases\n(Figure A2). We find no correlation between topic\nlength and caricature otherwise.\nIn the interview context, the exaggeration scores\nare broadly comparable to the scores for the more\nspecific topics in the online forum context (Figure\nA5). After controlling for context (Appendix F),\nwe hypothesize that this is because they are similar\nin specificity. We also observe this inverse relation-\nship between topic specificity and susceptibility to\ncaricature in the Twitter context (Appendix G).\n6.2.2 Caricature ‚Üë: Political ideology, race,\nand marginalized personas\nIn the online forum context, the personas nonbi-\nnary, Black, Hispanic, Middle-Eastern, and conser-\nvative have highest mean exaggeration score (Fig-\nure 5b-d). Similarly, in the interview context, the\nhighest mean scores are for the personas nonbinary,\nHispanic, 80-year-old, conservative and Middle-\nEastern. Broadly, nonbinary gender, non-white\nrace/ethnicity, and political leanings are most sus-\nceptible to caricature, while binary gender groups\nhave the least caricatures. This corresponds to pre-\nvious findings that NLP systems broadly align with\n10859\nage 20 age 40 age 80 conser-\nvative \nliberal moderate white Black Asian Hispanic Middle-\nEastern \nman non-\nbinary \nwoman\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nOnline Forum\nInterview\nFigure 4: Mean individuation scores (differentiability from default). The standard error for each point is < 0.02\nand thus not visible on the plot. Based on a classifier between the default-persona and the target persona, many of\nthe personas are easy to individuate (have high accuracy scores). The only personas that are slightly challenging to\ndifferentiate are the gender groups woman and man in the online forum context (blue circles). All personas have\nmean score > 0.95 in the interview context (green stars).\n20 40 80\n(a)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Exaggeration (Similarity to Axis)\nAge\nWikihow general\nProCon general\nWikiHow specific\nProCon specific\nConser-\nvative\nLiberal Moderate\n(b)\nPolitical\nAsian Black Hispanic Middle-\nEastern\nWhite\n(c)\nRace/Ethnicity\nMan Non-\nBinary\nWoman\n(d)\nGender\nFigure 5: Mean exaggeration scores ¬±standard error in the online forum context. We measure exaggeration as\nnormalized cosine similarity to the persona-topic axis. The more general topics (purple, larger marker) have higher\nrates of exaggeration, and thus caricature, than the specific topics (orange, smaller marker). The uncontroversial\n(WikiHow, squares) topics have higher rates of caricature than the controversial (ProCon.org, stars) topics. Personas\nrelated to political leanings, race/ethnicity, and nonbinary gender broadly have the highest rates of caricature.\nthe perspectives of liberal, white, and younger pop-\nulations, while the perspectives of non-binary peo-\nple are poorly represented by these systems (Santy\net al., 2023; Santurkar et al., 2023). It is surprising\nthat although Asian and woman reflect marginal-\nized groups, they have relatively low rates of car-\nicature; this may further reflect implicit defaults\nin LLM outputs. Certain personas and topics have\nmore nuanced relationships, e.g., conservative and\nliberal personas in the online forum context have\nthe widest gap between the scores of the uncontro-\nversial and controversial topics. The Twitter con-\ntext also reveals that, beyond analyzing the persona\nand topic dimensions separately, some persona-\ntopic combinations are particularly susceptible to\ncaricature (Figure A4).\n6.3 Stereotypes\nWe note that our framework is not an exhaustive\ntest for bias or failure modes, but rather a measure\nfor one way in which simulations may fail. Thus,\nsimulations that seem caricature-free may still con-\ntain stereotypes, as our method captures how much\na simulation exaggerates the persona in a particular\nsetting, which is not an all-encompassing catalog of\nstereotypes. We see this in the simulated ‚Äúwoman‚Äù\nresponses in the online forum context: The default-\ntopic generations contain specific stereotypes, e.g.,\n‚ÄúI recently purchased a new vacuum cleaner and I\nhave to say, I am extremely satisfied with its perfor-\nmance! It has made my cleaning routine so much\neasier and efficient.‚Äù This response reflects gender\nbias in that it focuses on cleaning and other domes-\ntic tasks, while simulations of other personas do\nnot. Although the simulated ‚Äúwomen‚Äù responses\ncontain various other gender stereotypes/biases be-\nyond association with domestic tasks, they have\nlow caricature scores.\n10860\n7 Recommendations\nWe conclude with several recommendations for\nthose interested in LLM simulations.\nMitigating Caricature Researchers should use\nour method to test their simulation in their partic-\nular context and critically examine whether a sim-\nulation helps illuminate the desired phenomenon.\nWhile the relationship between topic, persona, and\ncontext in causing caricature is nuanced, we gen-\nerally encourage researchers and practitioners to\nuse more specific topics to mitigate caricature. Any\nattempt to simulate a group‚Äîespecially a politi-\ncized or marginalized group‚Äîought to be done\nwith particular care and attention.\nDocumenting Positionality Research on LLM\nsimulations face the well-documented challenges\nof human-focused, value-laden interdisciplinary\nwork (Marsden and Haag, 2016). For instance,\nresearchers themselves may be subject to the out-\ngroup homogeneity effect, i.e., the tendency to rely\non stereotypes and generalizations for groups to\nwhich they do not belong (Plous, 2003). Following\nwork on model, dataset, and system documenta-\ntion (Bender and Friedman, 2018; Mitchell et al.,\n2019; Gebru et al., 2021; Adkins et al., 2022), we\ncall for increased transparency and documentation\nfor simulations, including the dimensions of CoM-\nPosT and less-visible aspects such as the creators‚Äô\npositions, motivations, and process. Drawing upon\nHCI work on reflexivity and positionality (Keyes\net al., 2020; Liang et al., 2021; Bowman et al.,\n2023), we encourage researchers to report how their\nidentity and perspectives may influence their work.\nUnderstanding Difference Although some ap-\nplications of LLM simulations focus on aggre-\ngates rather than individuals, it is critical to un-\nderstand the landscape of individuals from which\nthese groupings arise, and it is often necessary to\nuse more subtle forms of aggregation. Otherwise,\nminority opinions and subgroup disparities may\nbe overlooked (Herring and Paolillo, 2006; Hanna\net al., 2020; Wang et al., 2022). Takayama (2017)\nsuggests countering caricature ‚Äúby providing fully\ncontextualized, balanced, and nuanced descrip-\ntion,‚Äù and in HCI, Marsden and Pr√∂bster (2019)\nexplore how to explicitly capture users‚Äô multidi-\nmensional identities.\nDrawing inspiration from these works, one fu-\nture direction is injecting variation and using mul-\ntifaceted personas into simulations. Our goal in\navoiding caricature is not to erase difference, but\nrather the opposite: capturing relevant differences\nthat reflect meaningful insights rather than shallow,\nmisleading generalizations.\n8 Positionality\nThe perspectives introduced in this paper have un-\ndeniably been shaped and influenced by our po-\nsitionality. Myra Cheng identifies as a Chinese-\nAmerican woman. The authors are a PhD stu-\ndent, postdoctoral scholar, and professor respec-\ntively in the Stanford University Computer Sci-\nence department, which is predominantly male and\nwhite/Asian.\n9 Ethical Considerations\nFrom impersonation to pornography, LLM simu-\nlations can have deeply problematic applications.\nWe are strongly against such applications, and we\nalso do not condone research and development that\nmay enable such applications by bad actors with-\nout guardrails in place. Our CoMPosT framework\noffers a shared language to meaningfully critique\nsuch work. For instance, one might imagine com-\ning to a consensus to avoid simulating certain top-\nics, personas, and contexts entirely. Introducing a\nmethod to measure caricature offers a way to make\nknown this concerning limitation. Lack of carica-\nture based on our measure does not mean that a\nsimulation is necessarily acceptable or high quality\n(see Section 10).\nImplicit Defaults The least caricatured personas\nare also those that others have found to be implicit\ndefaults in LLMs. Implicit defaults in LLMs may\nshift depending on the prompt, context, etc., as well\nas including aspects of identity and social factors\nthat may be invisible or underrepresented in exist-\ning empirical data and surveys4. Given the increas-\ning proliferation of generated content and a limited\nquantity of human-written text (Xue et al., 2023;\nShumailov et al., 2023), caricatures become only\nmore relevant with the prospect of future LLMs\nthat are trained on generated data: what will their\n4Existing studies often exclude various demographics:\nstudies on human behavior oversample from the population\nof American college students, who have been shown to dif-\nfer from other populations in significant ways (Segall et al.,\n1966). Psychology and social science research subjects are\ndisproportionately Western, educated, and from industrialized,\nrich, and democratic countries (Henrich et al., 2010).\n10861\ndefaults be, and how might they further amplify\ncaricature?\n10 Limitations\nWhile we fill a critical gap since there is no ex-\nisting work on systematically detecting stereo-\ntypes/caricatures in simulations or evaluating simu-\nlations in this manner at all, our measure is limited\nin scope: it is not a comprehensive evaluation of the\nquality of a simulation. We quantify susceptibility\nto caricature, which is a particular failure case of a\nsimulation. Our method may yield false positives\n(simulations that seem acceptable and caricature-\nfree based on our method but have other problems).\nAvoiding caricature is a necessary but insuffi-\ncient criterion for simulation quality; our metric\nshould be used in tandem with other evaluations.\nAs a pilot study for a recently-emerging direction\nof work, we hope to lay the groundwork for a more\ncomprehensive evaluation of simulations in the fu-\nture, perhaps in tandem with human evaluation.\nAs we provide a first step toward characterizing\nand evaluating LLM simulations, an area which\ncurrently lacks a shared language for discussion\nand comparison, we focus only on simulations of\npersonas that reflect subpopulations such as social\ngroups and on one-round response formats. How-\never, our framework easily extends to other more\ncomplex or open-ended settings. For instance, for a\nmulti-round simulation, one could apply our frame-\nwork by using the full text of the simulation across\nthe rounds. Depending on the length and structure\nof the simulation, one could also split the simula-\ntion into multiple parts and characterize each part‚Äôs\npropensity to caricature.\nAlso, note that in the semantic axes, Pp does not\nnecessarily reflect a universal notion of the model‚Äôs\nrepresentation or description of that particular de-\nmographic group. It merely characterizes the words\nthat distinguish a simulation of that persona, given\nthe particular context c, such as an opinion from\nthat demographic‚Äôs perspective. This enables us to\nmeasure if and when a simulation is dominated by\nlanguage that is a caricature of the persona, but our\nwork is not a comprehensive evaluation of stereo-\ntypes or representations of demographic groups.\nAcknowledgments\nThank you to Tony Wang, Joon Sung Park, Omar\nShaikh, Caleb Ziems, Camille Harris, and Matthias\nGerstgrasser for their feedback throughout this\nproject. This work was funded in part by an\nNSF Graduate Research Fellowship (Grant DGE-\n2146755) and Stanford Knight-Hennessy Scholars\ngraduate fellowship to MC, as well as a Meta grant\nand an NSF grant IIS-2247357 to DY .\nReferences\nDavid Adkins, Bilal Alsallakh, Adeel Cheema, Narine\nKokhlikyan, Emily McReynolds, Pushkar Mishra,\nChavez Procope, Jeremy Sawruk, Erin Wang, and\nPolina Zvyagina. 2022. Prescriptive and descrip-\ntive approaches to machine-learning transparency. In\nCHI Conference on Human Factors in Computing\nSystems Extended Abstracts, pages 1‚Äì9.\nGati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.\n2023. Using large language models to simulate mul-\ntiple humans and replicate human subject studies. In\nProceedings of the 40th International Conference on\nMachine Learning, ICML‚Äô23. JMLR.org.\nAnu Aneja. 1993. ‚ÄúJasmine,‚Äù the sweet scent of exile.\nPacific Coast Philology, pages 72‚Äì80.\nLisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.\nGubler, Christopher Rytting, and David Wingate.\n2023. Out of one, many: Using language mod-\nels to simulate human samples. Political Analysis,\n31(3):337‚Äì351.\nApril H Bailey, Adina Williams, and Andrei Cimpian.\n2022. Based on billions of words on the Internet,\npeople= men. Science Advances, 8(13):eabm2463.\nMichiel Bakker, Martin Chadwick, Hannah Sheahan,\nMichael Tessler, Lucy Campbell-Gillingham, Jan\nBalaguer, Nat McAleese, Amelia Glaese, John\nAslanides, Matt Botvinick, et al. 2022. Fine-tuning\nlanguage models to find agreement among humans\nwith diverse preferences. Advances in Neural Infor-\nmation Processing Systems, 35:38176‚Äì38189.\nDavid Bamman, Jacob Eisenstein, and Tyler Schnoebe-\nlen. 2014. Gender identity and lexical variation in\nsocial media. Journal of Sociolinguistics, 18(2):135‚Äì\n160.\nDaniel Bar-Tal, Carl F Graumann, Arie W Kruglanski,\nand Wolfgang Stroebe. 2013. Stereotyping and prej-\nudice: Changing conceptions . Springer Science &\nBusiness Media.\nCaroline Bassett. 2019. The computational therapeutic:\nExploring Weizenbaum‚Äôs ELIZA as a history of the\npresent. AI & SOCIETY, 34:803‚Äì812.\nJoseph Bates et al. 1994. The role of emotion in believ-\nable agents. Communications of the ACM, 37(7):122‚Äì\n125.\nEmily M Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\n10862\nTransactions of the Association for Computational\nLinguistics, 6:587‚Äì604.\nMarcel Binz and Eric Schulz. 2023. Using cognitive\npsychology to understand GPT-3. Proceedings of the\nNational Academy of Sciences, 120(6):e2218523120.\nIrene V Blair, Jennifer E Ma, and Alison P Lenton. 2001.\nImagining stereotypes away: The moderation of im-\nplicit stereotypes through mental imagery. Journal\nof personality and social psychology, 81(5):828.\nLuisa N Borrell, Jennifer R Elhawary, Elena Fuentes-\nAfflick, Jonathan Witonsky, Nirav Bhakta, Alan HB\nWu, Kirsten Bibbins-Domingo, Jos√© R Rodr√≠guez-\nSantana, Michael A Lenoir, James R Gavin III, et al.\n2021. Race and genetic ancestry in medicine‚Äîa time\nfor reckoning with racism. New England Journal of\nMedicine, 384(5):474‚Äì480.\nLeslie Bow. 2019. Racist cute: Caricature, kawaii-style,\nand the Asian thing. American Quarterly, 71(1):29‚Äì\n58.\nRobert Bowman, Camille Nadal, Kellie Morrissey, Anja\nThieme, and Gavin Doherty. 2023. Using thematic\nanalysis in healthcare HCI at CHI: A scoping review.\nIn Proceedings of the 2023 CHI Conference on Hu-\nman Factors in Computing Systems, pages 1‚Äì18.\nJames DJ Brown. 2010. A stereotype, wrapped in a\nclich√©, inside a caricature: Russian foreign policy\nand orientalism. Politics, 30(3):149‚Äì159.\nYang Cao, Anna Sotnikova, Hal Daum√© III, Rachel\nRudinger, and Linda Zou. 2022. Theory-grounded\nmeasurement of US social stereotypes in english lan-\nguage models. In Proceedings of the 2022 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1276‚Äì1295.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In The Eleventh International Confer-\nence on Learning Representations.\nEshwar Chandrasekharan, Mattia Samory, Shagun\nJhaver, Hunter Charvat, Amy Bruckman, Cliff\nLampe, Jacob Eisenstein, and Eric Gilbert. 2018. The\ninternet‚Äôs hidden rules: An empirical study of Reddit\nnorm violations at micro, meso, and macro scales.\nProceedings of the ACM on Human-Computer Inter-\naction, 2(CSCW):1‚Äì25.\nMyra Cheng, Maria De-Arteaga, Lester Mackey, and\nAdam Tauman Kalai. 2023a. Social norm bias:\nResidual harms of fairness-aware algorithms. Data\nMining and Knowledge Discovery, pages 1‚Äì27.\nMyra Cheng, Esin Durmus, and Dan Jurafsky. 2023b.\nMarked personas: Using natural language prompts to\nmeasure stereotypes in language models. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics, pages 1276‚Äì1295.\nAlexander M Czopp, Aaron C Kay, and Sapna Cheryan.\n2015. Positive stereotypes are pervasive and\npowerful. Perspectives on Psychological Science ,\n10(4):451‚Äì463.\nEberhard Demm. 1993. Propaganda and caricature in\nthe first world war. Journal of Contemporary History,\n28(1):163‚Äì192.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\nFarm: A simulation framework for methods that learn\nfrom human feedback.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization: Quantify-\ning data leakage in NLP performance evaluation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 1325‚Äì1335, Online.\nAssociation for Computational Linguistics.\nSusan T Fiske, Amy JC Cuddy, Peter Glick, and Jun Xu.\n2002. A model of (often mixed) stereotype content:\ncompetence and warmth respectively follow from per-\nceived status and competition. Journal of personality\nand social psychology, 82(6):878.\nTimnit Gebru, Jamie Morgenstern, Briana Vec-\nchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum√© Iii, and Kate Crawford. 2021. Datasheets\nfor datasets. Communications of the ACM, 64(12):86‚Äì\n92.\nPeter Gottschalk and Gabriel Greenberg. 2011. From\nMuhammad to Obama: Caricatures, cartoons, and\nstereotypes of Muslims. Islamophobia: The chal-\nlenge of pluralism in the 21st century , pages 191‚Äì\n210.\nJonathan Grudin. 2006. Why personas work: The psy-\nchological evidence. The persona lifecycle, 12:642‚Äì\n664.\nPerttu H√§m√§l√§inen, Mikke Tavast, and Anton Kunnari.\n2023. Evaluating large language models in gener-\nating synthetic HCI research data: a case study. In\nProceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems, pages 1‚Äì19.\nSil Hamilton. 2023. Blind judgement: Agent-based\nsupreme court modelling with GPT. arXiv preprint\narXiv:2301.05327.\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila\nSmith-Loud. 2020. Towards a critical race method-\nology in algorithmic fairness. In Proceedings of the\n2020 conference on Fairness, Accountability, and\nTransparency, pages 501‚Äì512.\nMadeline E Heilman. 2001. Description and prescrip-\ntion: How gender stereotypes prevent women‚Äôs as-\ncent up the organizational ladder. Journal of Social\nIssues, 57(4):657‚Äì674.\n10863\nJoseph Henrich, Steven J Heine, and Ara Norenzayan.\n2010. The weirdest people in the world? Behavioral\nand brain sciences, 33(2-3):61‚Äì83.\nSusan C Herring and John C Paolillo. 2006. Gender and\ngenre variation in weblogs. Journal of Sociolinguis-\ntics, 10(4):439‚Äì459.\nJames D Hollan, Edwin L Hutchins, and Louis Weitz-\nman. 1984. Steamer: An interactive inspectable\nsimulation-based training system. AI magazine ,\n5(2):15‚Äì15.\nMarjan Hosseinia, Eduard Dragut, and Arjun Mukher-\njee. 2020. Stance prediction for contemporary is-\nsues: Data and experiments. In Proceedings of the\nEighth International Workshop on Natural Language\nProcessing for Social Media, pages 32‚Äì40, Online.\nAssociation for Computational Linguistics.\nHang Jiang, Doug Beeferman, Brandon Roy, and Deb\nRoy. 2022. CommunityLM: Probing partisan world-\nviews from language models. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 6818‚Äì6826, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nRandolph M Jones, John E Laird, Paul E Nielsen,\nKaren J Coulter, Patrick Kenny, and Frank V Koss.\n1999. Automated intelligent pilots for combat flight\nsimulation. AI magazine, 20(1):27‚Äì27.\nLee Jussim, Jarret T Crawford, Stephanie M Anglin,\nSean T Stevens, and Jose L Duarte. 2016. Inter-\npretations and methods: Towards a more effectively\nself-correcting social psychology. Journal of Experi-\nmental Social Psychology, 66:116‚Äì133.\nGauri Kambhatla, Ian Stewart, and Rada Mihalcea.\n2022. Surfacing racial stereotypes through identity\nportrayal. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency, pages 1604‚Äì1615.\nOs Keyes, Burren Peil, Rua M Williams, and Katta Spiel.\n2020. Reimagining (women‚Äôs) health: HCI, gender\nand essentialised embodiment. ACM Transactions on\nComputer-Human Interaction (TOCHI), 27(4):1‚Äì42.\nMahnaz Koupaee and William Yang Wang. 2018. Wiki-\nhow: A large scale text summarization dataset. arXiv\npreprint arXiv:1810.09305.\nKay Dian Kriz. 2008. Slavery, sugar, and the culture of\nrefinement: Picturing the British West Indies, 1700‚Äì\n1840. Paul Mellon Centre.\nNeha Kumar, Naveena Karusala, Azra Ismail, Marisol\nWong-Villacres, and Aditya Vishwanath. 2019. En-\ngaging feminist solidarity for comparative research,\ndesign, and practice. Proceedings of the ACM on\nHuman-Computer Interaction, 3(CSCW):1‚Äì24.\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021. Question and answer test-train overlap in open-\ndomain question answering datasets. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1000‚Äì1008.\nCalvin A Liang, Sean A Munson, and Julie A\nKientz. 2021. Embracing four tensions in human-\ncomputer interaction research with marginalized peo-\nple. ACM Transactions on Computer-Human Inter-\naction (TOCHI), 28(2):1‚Äì47.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny\nZhou, Andrew M Dai, Diyi Yang, and Soroush\nV osoughi. 2023. Training socially aligned language\nmodels in simulated human society. arXiv preprint\narXiv:2305.16960.\nLi Lucy, Divya Tadimeti, and David Bamman. 2022.\nDiscovering differences in the representation of peo-\nple using contextualized semantic axes. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3477‚Äì3494,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nBohun Lynch. 1927. A history of caricature. Faber and\nGwyer.\nJulia M. Markel, Steven G. Opferman, James A. Lan-\nday, and Chris Piech. 2023. GPTeach: Interactive\nTA training with GPT-based students. In Proceed-\nings of the Tenth ACM Conference on Learning @\nScale, L@S ‚Äô23, page 226‚Äì236, New York, NY , USA.\nAssociation for Computing Machinery.\nNicola Marsden and Maren Haag. 2016. Stereotypes\nand politics: Reflections on personas. In Proceedings\nof the 2016 CHI conference on human factors in\ncomputing systems, pages 4017‚Äì4031.\nNicola Marsden and Monika Pr√∂bster. 2019. Personas\nand identity: Looking at multiple identities to inform\nthe construction of personas. In Proceedings of the\n2019 CHI Conference on Human Factors in Comput-\ning Systems, pages 1‚Äì14.\nMelissa McCracken, Miho Olsen, Moon S Chen Jr,\nAhmedin Jemal, Michael Thun, Vilma Cokkinides,\nDennis Deapen, and Elizabeth Ward. 2007. Can-\ncer incidence, mortality, and associated risk factors\namong Asian Americans of Chinese, Filipino, Viet-\nnamese, Korean, and Japanese ethnicities. CA: a\ncancer journal for clinicians, 57(4):190‚Äì205.\nAmita Misra, Brian Ecker, and Marilyn Walker. 2016.\nMeasuring the similarity of sentential arguments in\ndialogue. In Proceedings of the 17th Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 276‚Äì287, Los Angeles. Association for\nComputational Linguistics.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2019. Model cards for model reporting. In\nProceedings of the Conference on Fairness, Account-\nability, and Transparency, pages 220‚Äì229.\n10864\nChandra Mohanty. 1988. Under Western eyes: Feminist\nscholarship and colonial discourses. Feminist review,\n30(1):61‚Äì88.\nBurt L Monroe, Michael P Colaresi, and Kevin M Quinn.\n2008. Fightin‚Äôwords: Lexical feature selection and\nevaluation for identifying the content of political con-\nflict. Political Analysis, 16(4):372‚Äì403.\nOpenAI. 2023. GPT-4 technical report. arXiv.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nJoon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023a. Generative agents: Interac-\ntive simulacra of human behavior. arXiv preprint\narXiv:2304.03442.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S Bern-\nstein. 2022. Social simulacra: Creating populated\nprototypes for social computing systems. In Proceed-\nings of the 35th Annual ACM Symposium on User\nInterface Software and Technology, pages 1‚Äì18.\nPeter S Park, Philipp Schoenegger, and Chongyang Zhu.\n2023b. Artificial intelligence in psychology research.\narXiv preprint arXiv:2302.07267.\nAndi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen,\nand Ece Kamar. 2022. Investigations of performance\nand bias in human-AI teamwork in hiring. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n36(11):12089‚Äì12097.\nDavid Perkins. 1975. A definition of caricature and\ncaricature and recognition. Studies in Visual Commu-\nnication, 2(1):1‚Äì24.\nScott Ed Plous. 2003. Understanding prejudice and\ndiscrimination. McGraw-Hill.\nJen‚Äônan Ghazal Read, Scott M Lynch, and Jessica S\nWest. 2021. Disaggregating heterogeneity among\nnon-hispanic whites: evidence and implications for\nus racial/ethnic health disparities. Population re-\nsearch and policy review, 40:9‚Äì31.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) . Association\nfor Computational Linguistics.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose opinions do language models reflect? In\nProceedings of the 40th International Conference on\nMachine Learning, ICML‚Äô23. JMLR.org.\nSebastin Santy, Jenny Liang, Ronan Le Bras, Katharina\nReinecke, and Maarten Sap. 2023. NLPositionality:\nCharacterizing design biases of datasets and models.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 9080‚Äì9102, Toronto, Canada.\nAssociation for Computational Linguistics.\nJohannes Schneider, Christian Meske, and Michalis Vla-\nchos. 2020. Deceptive AI explanations: Creation and\ndetection. In International Conference on Agents\nand Artificial Intelligence. SciTePress.\nMarshall H Segall, Donald Thomas Campbell, and\nMelville Jean Herskovits. 1966. The influence of\nculture on visual perception , volume 310. Bobbs-\nMerrill Indianapolis.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao,\nYarin Gal, Nicolas Papernot, and Ross Anderson.\n2023. The curse of recursion: Training on gen-\nerated data makes models forget. arXiv preprint\narxiv:2305.17493.\nPhillip R Slavney. 1984. Histrionic personality and anti-\nsocial personality: caricatures of stereotypes? Com-\nprehensive Psychiatry, 25(2):129‚Äì141.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (palms) with\nvalues-targeted datasets. Advances in Neural Infor-\nmation Processing Systems, 34:5861‚Äì5873.\nKeita Takayama. 2017. Imagining east asian educa-\ntion otherwise: Neither caricature, nor scandalization.\nAsia Pacific Journal of Education, 37(2):262‚Äì274.\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\nand Armen Aghajanyan. 2022. Memorization with-\nout overfitting: Analyzing the training dynamics of\nlarge language models. Advances in Neural Informa-\ntion Processing Systems, 35:38274‚Äì38290.\nAndrea Aler Tubella, Dimitri Coelho Mollo,\nAdam Dahlgren Lindstr√∂m, Hannah Devinney,\nVirginia Dignum, Petter Ericson, Anna Jonsson,\nTimotheus Kampik, Tom Lenaerts, Julian Alfredo\nMendez, et al. 2023. ACROCPoLis: A descriptive\nframework for making sense of fairness. In 2023\nACM Conference on Fairness, Accountability, and\nTransparency, pages 1604‚Äì1615.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and\nRobert West. 2023. Artificial artificial artificial intel-\nligence: Crowd workers widely use large language\nmodels for text production tasks. arXiv preprint\narXiv:2306.07899.\nKailas V odrahalli, Roxana Daneshjou, Tobias Gersten-\nberg, and James Zou. 2022. Do humans trust advice\nmore if it comes from AI? an analysis of human-AI\ninteractions. In Proceedings of the 2022 AAAI/ACM\nConference on AI, Ethics, and Society , pages 763‚Äì\n777.\n10865\nAngelina Wang, Vikram V Ramaswamy, and Olga Rus-\nsakovsky. 2022. Towards intersectionality in ma-\nchine learning: Including more identities, handling\nunderrepresentation, and performing evaluation. In\n2022 ACM Conference on Fairness, Accountability,\nand Transparency, pages 336‚Äì349.\nJoseph Weizenbaum. 1966. ELIZA‚Äîa computer pro-\ngram for the study of natural language communica-\ntion between man and machine. Communications of\nthe ACM, 9(1):36‚Äì45.\nRobert Wolfe and Aylin Caliskan. 2022. Markedness\nin visual semantic AI. 2022 ACM Conference on\nFairness, Accountability, and Transparency.\nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei\nZheng, and Yang You. 2023. To repeat or not to\nrepeat: Insights from scaling llm under token-crisis.\narXiv preprint arXiv:2305.13230.\nDiyi Yang. 2019. Computational Social Roles. Ph.D.\nthesis, Carnegie Mellon University Pittsburgh, PA,\nUSA.\nJD Zamfirescu-Pereira, Richmond Y Wong, Bjoern\nHartmann, and Qian Yang. 2023. Why Johnny can‚Äôt\nprompt: how non-AI experts try (and fail) to design\nllm prompts. In Proceedings of the 2023 CHI Con-\nference on Human Factors in Computing Systems ,\npages 1‚Äì21.\nCaleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon\nHalevy, and Diyi Yang. 2023. NormBank: A knowl-\nedge bank of situational social norms. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 7756‚Äì7776, Toronto, Canada. Association for\nComputational Linguistics.\nA Examples of Caricatures in Simulation\nWhich our paper focuses on how to discover prob-\nlematic patterns of caricature at the larger scale\nused in LLM simulations, these patterns are clearly\nevident on an individual example level. For ex-\nample, for the ‚Äúnonbinary person‚Äù persona in the\nonline forum setting, which has some of the high-\nest exaggeration scores, some randomly-sampled\ndefault-topic responses are:\n‚ÄúAs a nonbinary individual, I often feel like I don‚Äôt\nfit into the traditional boxes society has created\nfor gender. This can be difficult when navigat-\ning spaces that are dominated by cisnormativity.\nHowever, I have found solace and empowerment\nin connecting with other nonbinary people and\nadvocating for inclusivity and understanding. It\nis important to remember that gender is a social\nconstruct, and it is up to us to define our own\nidentities and push for a more accepting world.‚Äù\n‚ÄúAs a nonbinary individual, I often find myself\nin situations where people are not aware of or\ndon‚Äôt understand my gender identity. While it\ncan be frustrating at times, I choose to take it as\nan opportunity to educate others and promote a\nmore inclusive society. It‚Äôs important to recognize\nand respect everyone‚Äôs experiences and identities,\neven if they differ from our own.‚Äù\nThese descriptions focus solely on general state-\nments regarding gender identity and inclusion. We\nfind the same phenomenon in simulations of a non-\nbinary person‚Äôs perspective on other topics. For the\ntopic ‚Äúhow to read guitar music,‚Äù some randomly-\nsampled simulated non-binary person‚Äôs responses\nare:\n‚ÄúHey everyone! I just wanted to share my expe-\nrience as a nonbinary individual learning how to\nread guitar music. It‚Äôs been a really empower-\ning journey for me, as it allows me to express\nmyself in a way that transcends the limits of\ngender. Moreover, I‚Äôve found that the guitar com-\nmunity is incredibly accepting and supportive,\nwhich has been a huge help in overcoming some\nof the challenges that I‚Äôve faced as a nonbinary\nperson. Keep rocking out, friends!‚Äù\n‚ÄúAs a nonbinary guitarist, I wanted to share some\nadvice and encouragement for others who may be\nnavigating the world of guitar music. Remember\nthat music is a universal language and it does not\nadhere to any specific gender norms or societal\nexpectations. Allow yourself the freedom to ex-\nplore different styles, techniques, and genres of\nguitar music that resonate with you. Embrace\nyour unique perspective and use it to create music\nthat reflects your authentic self. And most im-\nportantly, don‚Äôt forget to have fun and enjoy the\njourney of learning and growing as a musician.‚Äù\nRather than actually discussing ‚Äúhow to read gui-\ntar music,‚Äù these outputs disproportionately focus\non generic statements about gender, authenticity,\nand inclusion (bolded). These responses exagger-\nate this particular imagined aspect of a nonbinary\nperson‚Äôs perspective. Similarly, for the topic ‚Äúcars\nand vehicles,‚Äù some randomly-sampled simulated\nnon-binary person‚Äôs responses are:\n‚ÄúI just wanted to share my experience as a nonbi-\nnary individual when it comes to cars and other\nvehicles. For me, it‚Äôs important that my mode of\ntransportation reflects my identity and personal\naesthetics. I‚Äôve found that customizing my vehi-\ncle with various accessories, paint jobs, and even\ncustom license plates has been a fun and empow-\nering way to express myself. I also appreciate a\nvehicle that has a gender-neutral design, as that\naligns with my identity. I‚Äôve noticed that more\nand more auto manufacturers are starting to cre-\nate designs that feel more inclusive and neutral,\nwhich is great for people like me. Ultimately,\nthe vehicle you choose should be a reflection of\nyour personality and preferences, no matter your\ngender identity. Happy driving, everyone!‚Äù\n‚ÄúHey everyone! As a nonbinary individual, I\nwanted to share my perspective on car preferences\n10866\nTopic Output for \"Man\" persona\nDefault ‚ÄúI recently purchased a new smartphone and I‚Äôm really impressed with its features and performance. The\ncamera quality is amazing, and the battery life lasts all day without any issues. The interface is easy to\nuse, and the overall design is sleek and modern. I would highly recommend this phone to anyone in the\nmarket for a new device.‚Äù\n‚ÄúI recently purchased a new laptop and I‚Äôm absolutely loving it so far. The performance is top-notch, the\ndesign is sleek, and the battery life has been really impressive. The only issue I‚Äôve come across is a minor\nsoftware bug, but that was easily fixed with an update. Overall, I highly recommend this model to anyone\nin the market for a reliable and high-performing laptop.‚Äù\nHow to read\nguitar music\n‚ÄúI recently started learning how to play the guitar and I found this guide on How to Read Guitar Music\nextremely helpful. The step-by-step explanation, along with images and examples, made it easy to\nunderstand and follow. I‚Äôm now more confident in reading sheet music and applying it to my guitar\npractice. Highly recommend this guide for any beginner guitarist looking to learn how to read music.\nKudos to the author for making it so simple and easy to follow!‚Äù\n‚ÄúI recently started learning how to read guitar music and found this article very helpful. It provided clear\nexplanations and easy-to-follow steps that have greatly improved my understanding of how to read sheet\nmusic for the guitar. The tips and examples given were a great reference for me as a beginner, and I feel\nmuch more confident in my ability now. Thanks for sharing this informative guide!‚Äù\nCars and ve-\nhicles\n‚ÄúI recently purchased a 2020 Ford Mustang GT and I couldn‚Äôt be happier with my decision. The drive is\nsmooth, the car is powerful, and the interior is incredibly comfortable and stylish. I have always loved the\nlook of the Ford Mustang and this new model definitely lives up to my expectations. The sound system is\nfantastic as well. I highly recommend this car to anyone considering a sporty, powerful, and fun vehicle.‚Äù\n‚ÄúI recently bought a 2015 Honda Civic and I‚Äôm absolutely loving it! The fuel efficiency is amazing and the\nride is smooth. I‚Äôve also noticed a significant improvement in the handling compared to my old car. The\ninterior is quite spacious and comfortable too. My only complaint so far is the touchscreen entertainment\nsystem - it‚Äôs a bit slow and not very user-friendly. But overall, I highly recommend the 2015 Civic to\nanyone in the market for a reliable, fuel-efficient, and enjoyable vehicle!‚Äù\nTable A1: Examples of Simulated ‚ÄúMan‚Äù Responses\nand how certain vehicles cater to different gen-\nder expressions. I‚Äôve noticed that car manufactur-\ners and marketers often gear their advertisements\ntowards a stereotypical male or female audience,\nbut I believe there‚Äôs a vast array of options out\nthere that can appeal to people of all gender identi-\nties. For example, I personally love sleek, modern\ndesigns that have a balance of form and function\nand don‚Äôt scream ‚Äômasculinity‚Äô or ‚Äôfemininity.‚Äô\nI‚Äôd love to hear about your favorite vehicles and\nhow they play a role in expressing your unique\npersonality and identity. Let‚Äôs have an open and\ninclusive conversation about our diverse expe-\nriences with cars and other vehicles!‚Äù\nIn these outputs, the bolded phrases also present\nunrealistic caricatures that exaggerate specific as-\npects of the persona rather than reflecting the full\nrange of possibilities of what a non-binary person\nmight say about these topics. In stark contrast, for\nthe ‚Äúman‚Äù persona, the default-topic responses are\nnot identity-related, and responses for particular\ntopics are much more topical and do not exagger-\nate any aspect of the ‚Äúman‚Äù persona (Table A1).\nB Robustness of Individuation Measure\nThe notinon of measuring individuation is broadly\nagnostic to the choice of differentiator and model\nused to embed the texts. In Figure A1, we show\nindividuation results from (a) using the unsuper-\nvised V-measure to measure differentiation and\n(b) using the all-distilroberta-v1 model to com-\npute embeddings instead of all-mpnet-base-v2\n(all-distilroberta-v1 is the next highest-performing\nmodel for general-use purposes). For (a), we first\nuse K-Means to cluster the embeddings into two\nclusters, and then report the v-score. We find that\nthe scores are overall slightly lower than using a\nbinary classifier, which makes sense since the un-\nsupervised method is less powerful in this context,\nbut most of the personas can still be differentiated\nat a rate higher than random chance. We use the su-\npervised binary classifier in the main results since\nthe purpose of this metric is to reflect a reader‚Äôs ca-\npability to differentiate between the two categories,\nwhich is more accurately reflected by the more pow-\nerful classifier. The broad patterns remain the same\nas the findings reported in the paper: all the per-\nsonas can be individuated in the Interview context,\nand in the Online Forum context, marginalized per-\nsonas are the most easily individuated, while the\npersonas of man and woman are challenging to\nindividuate at all.\nC Internal and External Validation of\nSemantic Axes\nFor internal validation, following Lucy et al. (2022),\nwe verify that the constructed axes are internally\nvalid using a leave-one-out method: consider a pole\nP1 with a set of words W, where the other pole\non the semantic axis is P2. We remove one of the\nseed words w ‚ààW and construct a new pole P‚Ä≤\n1\n10867\nage 20 age 40 age 80 conser-\nvative \nliberal moderate white Black Asian Hispanic Middle-\nEastern \nman non-\nbinary \nwoman\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n Online Forum\nInterview\nage 20 age 40 age 80 conser-\nvative \nliberal moderate white Black Asian Hispanic Middle-\nEastern \nman non-\nbinary \nwoman\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nOnline Forum\nInterview\nFigure A1: Top: Using the unsupervised V-measure to measure differentiation results in similar patterns as our\nmain result in Figure 4. Bottom: Using an alternative pre-trained model to encode the outputs also results in similar\npatterns.\nthat does not include w. Then, we measure the\ncosine similarity of w‚Äôs contextualized embedding\nto P‚Ä≤\n1 and P2. We found that the former is larger\nfor all of the persona-topic semantic axes that we\nconstructed.\nFor external validation, we manually inspect the\nsets of seed words and find that for each axis, it is\neasy to differentiate which set is associated with\nthe corresponding persona versus topic. We find\nthat for race/ethnicity and gender personas, the sets\nof top words reflect similar stereotypes as reported\nby Cheng et al. (2023b). See Table A2 and A3 for\nrandomly-sampled examples of persona-topic axes\nseed word sets.\nD Experimental Details: Topics and\nPersonas\nExamples of each setting are in Table A4. Follow-\ning Santurkar et al. (2023) and Cheng et al. (2023b),\nthe personas used in the online forum and interview\ncontexts are:\n‚Ä¢ age: 20-year-old person, 40-year-old person,\n80-year-old person\n‚Ä¢ political ideology: conservative person, lib-\neral person, moderate person\n‚Ä¢ race/ethnicities: Asian person, Black per-\nson, Hispanic person, Middle-Eastern person,\nwhite person\n‚Ä¢ gender: man, non-binary person, woman\n‚Ä¢ default: person\nD.1 Online Forum Context\nThe full list of topics collected from WikiHow and\nProCon.org for the online forum context are in\ntopics/onlineforumtopics.txt in the supple-\nmentary material.\nThe general prompt for simulation with persona\np and topic t is: ‚ÄòA(n) p posted the following com-\nment on t to an online forum.‚Äù For persona p, the\ndefault-topic prompt is: ‚ÄúA(n) p posted the follow-\ning comment to an online forum.‚Äù For topic t, the\ndefault-persona prompt is: ‚ÄúA person posted the\nfollowing comment on t to an online forum.‚Äù\nD.1.1 Fine-Grained Specificity Experiment\nThe topics for the fine-grained specificity experi-\nment are:\n‚Ä¢ Specificity Level 1: health\n‚Ä¢ Specificity Level 2: mental health\n‚Ä¢ Specificity Level 3: things to do for mental\nhealth, social media‚Äôs effect on mental health\n‚Ä¢ Specificity Level 4: What is the best thing\nyou did for your mental health?, Is social me-\ndia bad for your mental health?\n‚Ä¢ Specificity Level 5: During a very dark pe-\nriod, what was the best thing you ever did\nfor your mental health?, To what extent do\nyou think social media is bad for your mental\nhealth?\n10868\n20 40 80\n0.0\n0.2\n0.4\n0.6\n0.8Exaggeration (Similarity to Axis)\nAge\nSpecificity\n1\n2\n3\n4\n5\nConser-\nvative\nLiberal Moderate\nPolitical\nAsian Black HispanicMiddle-\nEastern\nWhite\nRace/Ethnicity\nMan Non-\nBinary\nWoman\nGender\nFigure A2: Exaggeration scores in the online forum context for topics varying in specificity. Across different\npersonas (x-axis), exaggeration score (y-axis) is negatively correlated with the specificity of the topic (marker size).\nThe Level 1 topic is the topic with highest rate\nof caricature. The Level 5 topics are from the top\nsearch results for ‚Äúhealth‚Äù in the subreddit com-\nmunity AskReddit. The authors constructed the in-\ntermediate specificity levels by interpolating speci-\nficity between these. The resulting exaggeration\nscores are in Figure A2. We see that specificity and\nexaggeration are negatively correlated.\nD.2 Interview Context\nThe full list of Pew opinion survey questions\nthat we use as topics in the interview setting are\nin topics/pewtopics.txt in the supplementary\nmaterial. The default-topic and default-persona\nprompts are in Table A5.\nE Power Analysis\nTo justify 100 examples per simulation setting, we\nprovide the following power analysis. Note that\nfor individuation, a simulation that cannot be in-\ndividuated would have score 0.5 (random chance).\nAcross personas and topics, the lowest mean score\nwas 0.65, and the highest standard deviation was\n0.2. A power analysis using a t-test for two indepen-\ndent samples reveals that the necessary sample size\nis 28 given the effect size(0.65 ‚àí0.5)/0.2 = 0.75,\nalpha = 0.5, and desired power = 0.8. Similarly,\nfor exaggeration, a simulation with no exaggera-\ntion of a persona would have score0. We found that\nsimulations with less-specific topics and personas\nof many political ideology, race, and marginalized\ngroups result in high exaggeration scores. Among\nthese simulations, the lowest mean score was 0.23,\nand the highest standard deviation was 0.37. Again\nusing a power analysis, the necessary sample size is\n41 using effect size (0.23 ‚àí0)/0.37 = 0.62, alpha\n= 0.5, and desired power = 0.8. Thus, our choice\nof obtaining 100 samples per simulation is more\nthan sufficient to achieve the desired power for both\nthe individuation and exaggeration metrics.\nF Influence of the Context Dimension\nIn this section, we explore the effect of the context\ndimension. To verify that the trends we observe\nare not due to the difference in context alone, we\nalso experiment with switching the contexts and\ntopics, i.e., 1) simulations with the online forum\ntopics in the interview context 2) simulations with\nthe interview topics in the online forum setting.\nFigure A3 reveals that the trends in caricature rates\npersist for the same topics in different contexts\nrather than being based on context alone. Interest-\ningly, the exaggeration scores are overall slightly\nhigher in these switched contexts than in the origi-\nnal contexts. This may be impacted by memoriza-\ntion dynamics (Elangovan et al., 2021; Tirumala\net al., 2022; Carlini et al., 2023), a relationship to\nexplore in future work (e.g., are memorized topics\nless susceptible to caricature?).\nG Twitter Context\nWe additionally conduct and analyze experiments\nin the Twitter context introduced by Jiang et al.\n(2022), and we find similar trends that corroborate\nthe results of the main paper.\nG.1 Experimental Details\nJiang et al. (2022) demonstrate how LLM simula-\ntions of Republican and Democrat Twitter users\nresult in opinions about public figures and groups\nthat correspond to the outcomes of the Ameri-\n10869\n20 40 80\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Exaggeration (Similarity to Axis)\nAge\nWikihow general\nProCon general\nWikiHow specific\nProCon specific\nConser-\nvative\nLiberal Moderate\nPolitical\nAsian Black Hispanic Middle-\nEastern\nWhite\nRace/Ethnicity\nMan Non-\nBinary\nWoman\nGender\nOnline Forum Context Topics (WikiHow and ProCon) in Interview Context\n20 40 80\n0.0\n0.1\n0.2\n0.3Exaggeration (Similarity to Axis)\nAge\nConser-\nvative\nLiberal Moderate\nPolitical\nAsian Black HispanicMiddle-\nEastern\nWhite\nRace/Ethnicity\nPew Survey Questions,\nOnline Forum Context\nMan Non-\nBinary\nWoman\nGender\nInterview Context Topics (Pew Survey Questions) in Online Forum Context\nFigure A3: Exaggeration scores for simulations with the topics used in the online forum context (from WikiHow\nand ProCon.org), but using the interview context instead (top), and vice versa (bottom). We find similar patterns,\nindiciating that we can attribute the trends in scores to topics rather than context alone.\ncan National Election Studies (ANES) 2020 Ex-\nploratory Testing Survey. To evaluate susceptibil-\nity to caricature in this context, we use their best-\nperforming prompt, [persona] user posted on Twit-\nter: [topic] is/are. Following their work, we use\nthe same two personas (Republican and Democrat)\nand 30 topics that they use: ‚Äú(a) 16 people: Donald\nTrump, Barack Obama, Joe Biden, Elizabeth War-\nren, Bernie Sanders, Pete Buttigieg, Kamala Harris,\nAmy Klobuchar, Mike Pence, Andrew Yang, Nancy\nPelosi, Marco Rubio, Alexandria Ocasio-Cortez,\nNikki Haley, Clarence Thomas, Dr. Anthony Fauci,\n(b) 14 groups: black people, white people, Hispanic\npeople, Asian people, illegal immigrants, feminists,\nthe #MeToo movement, transgender people, so-\ncialists, capitalists, big business, labor unions, the\nRepublican Party, the Democratic Party.‚Äù\nG.2 Results\nIndividuation In the Twitter context, the average\nscore for the Democrat and Republican personas\nare 0.94 and 0.88 respectively.\nExaggeration In the Twitter context (Figure A4),\nRepublican personas resulted in higher mean exag-\ngeration scores than Democrat ones. Topics related\nto race and other groups broadly have higher ex-\naggeration scores, while topics of specific public\nfigures have lower ones, reflecting the pattern that\nmore general topics are more caricatured. Discus-\nsion of Republican public figures yielded more\nexaggeration than those of Democratic public fig-\nures. Examining topic subcategories reveals more\nnuanced relationships between topic and persona.\nFor example, in simulations with Democrat per-\nsonas, topics related to race yielded higher rates of\ncaricature. We found that among all the public fig-\nures that are used as topics, the topic Dr. Anthony\nFauci had the lowest rate of caricature. Dr. Fauci\nis also the only non-politician among them.\nH Interview Setting Result Details\nExaggeration scores in the interview setting are in\nFigure A5. The topics with highest exaggeration\nscores relate to confidence in the military ( How\nmuch confidence, if any, do you have in the mil-\nitary to act in the best interests of the public? ),\nself-identified social class ( If you were asked to\nuse one of these commonly used names for the so-\ncial classes, which would you say you belong in?),\nand ties to community (How important is it to you,\n10870\nRepublican Democrat \n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Exaggeration (Similarity to Axes)\nT opics\nall topics\nRepublican Democrat \nT opics\nRepublican public figures\nDemocrat public figures\nOther Groups\nRaces\nFigure A4: Mean exaggeration scores for topics and personas in the Twitter context, averaged over all topics\n(left) and by topic category (right). On average, simulations of Republican personas (left) and simulations\nwith Republican topics (right, teal stars) result in higher exaggeration scores. However, the relationship between\ntopics and personas is nuanced: for simulations of Democrat users, topics related to race have relatively higher\nexaggeration scores.\n20 40 80\n‚àí0.05\n0.00\n0.05\n0.10\n0.15\nExaggeration (Similarity to Axis)\nAge\nConser-\nvative\nLiberal Moderate\nPolitical\nAsian Black HispanicMiddle-\nEastern\nWhite\nRace/Ethnicity\nPew Survey Questions,\nInterview Context\nMan Non-\nBinary\nWoman\nGender\nFigure A5: Exaggeration scores for topics and personas in the interview context. The topics are randomly-\nsampled controversial questions from OpinionQA (Santurkar et al., 2023). These scores are comparable to those for\nthe specific topics in the online forum context.\npersonally, to live in a community with a strong\nsense of community?).\n10871\nSimulation Persona Pole Seed Words Topic Pole Seed Words\nPersona: white person, Topic:\nwhether the United States\nshould implement a universal\nbasic income\ni, racism, white, color, under-\nstand, experiences, inclusive,\nperson, together, we, privilege,\nlisten, educate, important, race\nincome, ubi, would, basic,\ncould, universal, states, united,\nimplementing, poverty, finan-\ncial, net, potential, safety, the\nPersona: Black person, Topic:\nHistoric Statue Removal\nblack, experiences, person, to-\ngether, race, work, and, conver-\nsations, racism, for, share, open,\neveryone, my, understanding\nstatues, history, past, removing,\nthem, the, instead, context, his-\ntoric, represent, mistakes, fig-\nures, erase, these, they\nPersona: Asian person, Topic:\nHow to Write a Business Case\nour, asian, we, cultures, person,\npeople, stereotypes, us, an, and,\nlearn, world, lets, culture, diver-\nsity\ncase, business, the, a, project,\narticle, helpful, writing, tips,\nwrite, stepbystep, solution,\nproblem, examples, found\nPersona: Hispanic person ,\nTopic: Relationships\nhispanic, our, community, soci-\nety, culture, person, heritage, di-\nverse, of, and, proud, rich, tradi-\ntions, important, celebrate\nrelationship, been, partner, but,\nadvice, how, years, now, any,\nive, dont, them, if, things, lately\nPersona: Middle-Eastern per-\nson, Topic: Universal Basic In-\ncome (UBI)\nmiddle, east, our, we, and, re-\ngion, culture, understanding,\nhistory, from, cultures, diverse,\nrich, share, us\nubi, income, basic, universal,\ncould, idea, poverty, society,\njobs, their, automation, provide,\npotential, net, safety\nPersona: woman, Topic: Holi-\ndays and Traditions\nvacuum, cleaner, i, it, my, this,\nrecently, product, anyone, new,\nskin, has, recommend, highly,\nsuction\ntraditions, holidays, holiday,\nour, together, love, bring, cul-\ntures, of, celebrate, favorite,\nworld, special, memories, fam-\nily\nPersona: man, Topic: How to\nKnow if You Love Someone\nlaptop, new, the, performance,\nbattery, recently, purchased,\nrecommend, highly, extremely,\noverall, features, am, anyone,\nimpressed\nlove, you, them, if, someone,\nwhen, their, happiness, truly,\nfeelings, about, article, feel,\nthink, that\nPersona: nonbinary person ,\nTopic: How to Dress For a Fu-\nneral\ngender, nonbinary, identity,\nidentities, understanding, my,\nbinary, everyone, society, iden-\ntify, experiences, male, female,\npeople, share\nfuneral, family, dress, col-\nors, deceased, wearing, black,\nthe, appropriately, dark, wear,\nattended, show, conservative,\ndressing\nTable A2: Persona-topic semantic axes seed words for randomly-sampled simulations with race/ethnicity and\ngender personas. Top persona and topic words identified by our method (Section 4.3) to construct persona-topic\nsemantic axes for measuring exaggeration. We display the seed words corresponding to simulations of each persona\nalongside a randomly-sampled topic in the online forum context; this table is continued with word sets for age and\npolitical ideology personas in Table A3. Each word list is truncated to 15 words in this table, but we use the full set\nin constructing the axes. We find that the race/ethnicity and nonbinary persona seed words reflect the stereotypes\nand essentializing narratives documented by Cheng et al. (2023b).\n10872\nSimulation Persona Pole Seed Words Topic Pole Seed Words\nPersona: 20-year-old person,\nTopic: How to Say Happy Birth-\nday\nim, yearold, forward, advice,\nforum, from, ive, discussions,\nlooking, hey, everyone, been,\nexperiences, thoughts, now\nyear, day, filled, birthday, happy,\nwishing, joy, may, laughter,\nlove, another, memories, , hap-\npiness, lots\nPersona: 80-year-old person,\nTopic: Food and Entertaining\ntechnology, am, all, world,\nhello, yearold, learn, how, on-\nline, learning, changes, discus-\nsions, from, has, experiences\ndinner, recipes, love, hosting, a,\nparties, out, party, food, dishes,\nabsolutely, favorite, delicious,\nfriends, tips\nPersona: 40-year-old person,\nTopic: whether the federal mini-\nmum wage should be increased\ni, my, experiences, im, ive,\nyearold, and, am, forward, from,\nage, discussions, cheers, hello,\nyou\nwage, minimum, federal, work-\ners, the, increased, raising, busi-\nnesses, increasing, would, liv-\ning, higher, economy, could, not\nPersona: conservative person,\nTopic: Personal Care and Style\ngovernment, our, values, re-\nsponsibility, believe, society,\ntraditional, strong, limited, we,\nintervention, individual, princi-\nples, that, should\nmy, care, style, products,\nstarted, feel, me, confidence,\ntaking, skincare, ive, recently,\nskin, tips, your\nPersona: liberal person, Topic:\nwhether colleges and universi-\nties should pay college athletes\nwe, society, everyone, our, re-\ngardless, social, equal, health-\ncare, race, rights, gender, jus-\ntice, policies, together, inclu-\nsive\nathletes, college, their, sports,\nuniversities, colleges, they,\nschools, financial, these,\nrevenue, would, paying,\ncompensation, could\nPersona: moderate person ,\nTopic: Electoral College\nopinions, perspectives, and, dif-\nferent, find, ground, common,\nopen, understanding, discus-\nsions, various, important, re-\nspectful, us, engage\nthe, vote, system, popular, elec-\ntoral, college, states, outdated,\ncandidate, votes, of, direct, pres-\nident, gives, win\nTable A3: Persona-topic semantic axes seed words for randomly-sampled simulations with age and political\nideology personas. Examples continued from Table A2.\n‚ÄúAn Asian person posted the following comment about health to an online forum‚Äù (Similar to\n(Park et al., 2022))\n‚ÄúBelow you will be asked to provide a short description of your identity and then answer some\nquestions.\nDescription: In politics today, I would describe my political views as liberal.\nHow much, if at all, do you worry about the following happening to you? Being the victim of a\nterrorist attack:‚Äù (Similiar to (Santurkar et al., 2023))\n‚ÄúA Republican user posted on Twitter: labor unions are‚Äù (Jiang et al., 2022)\nTable A4: Mapping dimensions in different simulations. Each prompt containspersona (bolded), context (italicized),\nand topic (highlighted).\n10873\nDefault-Topic (persona p) Prompt\nAge\nBelow you will be asked to provide a short description of your identity\nand then answer some questions.\nDescription: I am p.\nComment:\nPolitical Ideology\nBelow you will be asked to provide a short description of your identity\nand then answer some questions.\nDescription: In politics today, I would describe my political views as p.\nComment:\nRace/Ethnicity\nBelow you will be asked to provide a short description of your identity\nand then answer some questions.\nDescription: I am p.\nComment:\nGender\nBelow you will be asked to provide a short description of your identity\nand then answer some questions.\nDescription: I identify as p.\nComment:\nDefault-Persona (topic t)\nBelow you will be asked to provide a short description of your identity\nand then answer some questions.\nDescription: I am a person.\nQuestion: t\nAnswer:\nTable A5: Prompts for default-persona and default-topic simulations in the interview context.\n10874\nPaper Context Models Personas Topics\nArgyle\net al.\n(2023)\npublic survey (MC) GPT-3\ndavinci\npublic survey participants (po-\nlitical ideology, church atten-\ndance, state, race, gender, in-\ncome, age)\npresidential election\npublic survey (MC) GPT-3\ndavinci\nvariety of ANES participants\nwith specific backstories\ndemographic data about self\nAher\net al.\n(2023)\nsocial science experi-\nments (MC)\nGPT-3\ndavinci\nsocial science experiment par-\nticipants with varying names\nand genders\nUltimatum Game, Garden Path\nSentences, Milgram Shock\nExperiment, and Wisdom of\nCrowds\nPark et al.\n(2023b)\npsychology surveys and\nquestionnaires (MC)\nGPT-\n3.5\nliberals and conservatives psychology studies from the\nMany Labs 2 replication\nproject\nSanturkar\net al.\n(2023)\npublic survey (MC) GPT-\n3.5 &\nAI21\nmodels\n60 U.S. demographic groups\nincluding age, gender, race, po-\nlitical leaning, political party\nPew public opinion polls\nBinz and\nSchulz\n(2023)\ncanonical cognitive\npsychology experiments\n(vignette-based and\ntask-based) (MC)\nGPT-3 psychology experiment partici-\npants\nquestions to evaluate decision-\nmaking, information search,\ndeliberation, and causal reason-\ning abilities\nH√§m√§l√§inen\net al.\n(2023)\nHCI research interview\n(O)\nGPT-3 interviewees for HCI re-\nsearcher\nexperiencing video games as\nart\nPark et al.\n(2023a)\ninteractive sandbox en-\nvironment inspired by\nThe Sims, small town of\ntwenty-five agents (O)\nGPT-\n3.5\nturbo\n25 agents of Smallville, incl.\noccupation and relationship\nwith other agents\ninteractions with other agents\nand reactions to changes in en-\nvironment\nMarkel\net al.\n(2023)\noffice hours for a com-\nputer science class with\nstudents and a teaching\nassistant (O)\nGPT-3 students (descriptions include\nage, major, characteristics,\nmindset)\ncomputer science homework\nassignments\nLiu et al.\n(2023)\nvirtual societies with so-\ncial norms (O)\nGPT-\n3.5,\nGPT-4\nsocial agents social interactions and dynam-\nics\nDubois\net al.\n(2023)\npairwise preference\ntasks that require human\nannotation (MC)\nGPT-4 human annotators (crowdwork-\ners)\nquestions from OASST, An-\nthropic, Vicuna, and Koala\nevaluations\nTable A6: Additional Examples Mapping Existing Work Using the CoMPosT Framework. Extended from\nFigure 1. MC and O mean multiple-choice and open-response respectively. Note that all works use versions of GPT\nfor the model dimension.\n10875",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6531137228012085
    },
    {
      "name": "Demographics",
      "score": 0.6272776126861572
    },
    {
      "name": "Persona",
      "score": 0.5527344346046448
    },
    {
      "name": "Computer science",
      "score": 0.4633382558822632
    },
    {
      "name": "Exaggeration",
      "score": 0.46127697825431824
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.42846786975860596
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4203764796257019
    },
    {
      "name": "Psychology",
      "score": 0.33689433336257935
    },
    {
      "name": "Data science",
      "score": 0.3223814070224762
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.2907349765300751
    },
    {
      "name": "Sociology",
      "score": 0.2649517059326172
    },
    {
      "name": "Epistemology",
      "score": 0.16555777192115784
    },
    {
      "name": "Geography",
      "score": 0.10323137044906616
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Demography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 26
}