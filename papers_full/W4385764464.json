{
  "title": "CiT-Net: Convolutional Neural Networks Hand in Hand with Vision Transformers for Medical Image Segmentation",
  "url": "https://openalex.org/W4385764464",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4304011449",
      "name": "Lei T",
      "affiliations": [
        "Shaanxi University of Science and Technology",
        "First Affiliated Hospital of Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2711418541",
      "name": "Sun R",
      "affiliations": [
        "Shaanxi University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3121676728",
      "name": "Wang, X.",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2247379936",
      "name": "Wang Y",
      "affiliations": [
        "Shaanxi University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2351387268",
      "name": "He, X.",
      "affiliations": [
        "Shaanxi University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4282096780",
      "name": "Nandi, A.",
      "affiliations": [
        "Brunel University of London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3125218802",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3096812112",
    "https://openalex.org/W3013198566",
    "https://openalex.org/W4211221612",
    "https://openalex.org/W4317515509",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3035414587",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4221066005",
    "https://openalex.org/W4285707640",
    "https://openalex.org/W4294226146",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W3115153039",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034421924",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312428231",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W4292314562",
    "https://openalex.org/W4287274255",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W3169808228",
    "https://openalex.org/W4310466409",
    "https://openalex.org/W3205816419",
    "https://openalex.org/W4389977645",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4310418908",
    "https://openalex.org/W4214546759"
  ],
  "abstract": "The hybrid architecture of convolutional neural networks (CNNs) and Transformer are very popular for medical image segmentation. However, it suffers from two challenges. First, although a CNNs branch can capture the local image features using vanilla convolution, it cannot achieve adaptive feature learning. Second, although a Transformer branch can capture the global features, it ignores the channel and cross-dimensional self-attention, resulting in a low segmentation accuracy on complex-content images. To address these challenges, we propose a novel hybrid architecture of convolutional neural networks hand in hand with vision Transformers (CiT-Net) for medical image segmentation. Our network has two advantages. First, we design a dynamic deformable convolution and apply it to the CNNs branch, which overcomes the weak feature extraction ability due to fixed-size convolution kernels and the stiff design of sharing kernel parameters among different inputs. Second, we design a shifted-window adaptive complementary attention module and a compact convolutional projection. We apply them to the Transformer branch to learn the cross-dimensional long-term dependency for medical images. Experimental results show that our CiT-Net provides better medical image segmentation results than popular SOTA methods. Besides, our CiT-Net requires lower parameters and less computational costs and does not rely on pre-training. The code is publicly available at https://github.com/SR0920/CiT-Net.",
  "full_text": "CiT-Net: Convolutional Neural Networks Hand in Hand with Vision Transformers\nfor Medical Image Segmentation\nTao Lei1;2 , Rui Sun1 , Xuan Wang3 , Yingbo Wang1 , Xi He1 , Asoke Nandi4\n1Shaanxi Joint Laboratory of Artiﬁcial Intelligence, Shaanxi University of Science and Technology\n2Department of Geriatric Surgery, First Afﬁliated Hospital, Xi’an Jiaotong University\n3Unmanned System Research Institute, Northwestern Polytechnical University\n4Department of Electronic and Electrical Engineering, Brunel University London\nleitao@sust.edu.cn, siri0920@163.com, wangxuan@nwpu.edu.cn, fwangyingbo, xiheg@sust.edu.cn,\nAsoke.Nandi@brunel.ac.uk\nAbstract\nThe hybrid architecture of convolutional neural net-\nworks (CNNs) and Transformer are very popular\nfor medical image segmentation. However, it suf-\nfers from two challenges. First, although a CNNs\nbranch can capture the local image features us-\ning vanilla convolution, it cannot achieve adap-\ntive feature learning. Second, although a Trans-\nformer branch can capture the global features, it\nignores the channel and cross-dimensional self-\nattention, resulting in a low segmentation accuracy\non complex-content images. To address these chal-\nlenges, we propose a novel hybrid architecture of\nconvolutional neural networks hand in hand with\nvision Transformers (CiT-Net) for medical image\nsegmentation. Our network has two advantages.\nFirst, we design a dynamic deformable convolu-\ntion and apply it to the CNNs branch, which over-\ncomes the weak feature extraction ability due to\nﬁxed-size convolution kernels and the stiff design\nof sharing kernel parameters among different in-\nputs. Second, we design a shifted-window adap-\ntive complementary attention module and a com-\npact convolutional projection. We apply them to the\nTransformer branch to learn the cross-dimensional\nlong-term dependency for medical images. Experi-\nmental results show that our CiT-Net provides bet-\nter medical image segmentation results than popu-\nlar SOTA methods. Besides, our CiT-Net requires\nlower parameters and less computational costs and\ndoes not rely on pre-training. The code is publicly\navailable at https://github.com/SR0920/CiT-Net.\n1 Introduction\nMedical image segmentation refers to dividing a medical im-\nage into several speciﬁc regions with unique properties. Med-\nical image segmentation results can not only achieve abnor-\nmal detection of human body regions but also be used to\nguide clinicians. Therefore, accurate medical image segmen-\ntation has become a key component of computer-aided diag-\nnosis and treatment, patient condition analysis, image-guided\nsurgery, tissue and organ reconstruction, and treatment plan-\nning. Compared with common RGB images, medical images\nusually suffer from the problems such as high density noise,\nlow contrast, and blurred edges. So how to quickly and accu-\nrately segment speciﬁc human organs and lesions from med-\nical images has always been a huge challenge in the ﬁeld of\nsmart medicine.\nIn recent years, with the rapid development of computer\nhardware resources, researchers have continuously developed\nmany new automatic medical image segmentation algorithms\nbased on a large number of experiments. The existing med-\nical image segmentation algorithms can be divided into two\ncategories: based on convolutional neural networks (CNNs)\nand based on the Transformer networks.\nThe early traditional medical image segmentation algo-\nrithms are based on manual features designed by medical ex-\nperts using professional knowledge [Suetens, 2017]. These\nmethods have a strong mathematical basis and theoretical\nsupport, but these algorithms have poor generalization for dif-\nferent organs or lesions of the human body. Later, inspired\nby the full convolutional networks (FCN) [Long et al., 2015]\nand the encoder-decoder, Ronnebreger et al. designed the U-\nNet [Ronneberger et al., 2015] network that was ﬁrst applied\nto medical image segmentation. After the network was pro-\nposed, its symmetric U-shaped encoder and decoder structure\nreceived widespread attention. At the same time, due to the\nsmall number of parameters and the good segmentation effect\nof the U-Net network, deep learning has made a breakthrough\nin medical image segmentation. Then a series of improved\nmedical image segmentation networks are inspired based on\nthe U-Net network, such as 2D U-Net++ [Zhou et al., 2018],\nResDO-UNet [Liu et al., 2023], SGU-Net [Lei et al., March\n2023], 2.5D RIU-Net [Lv et al., 2022], 3D Unet [C ¸ ic ¸eket al.,\n2016], V-Net[Milletari et al., 2016], etc. Among them, Alom\net al. designed R2U-Net [Alom et al., 2018] by combining U-\nNet, ResNet [Song et al., 2020], and recurrent neural network\n(RCNN) [Girshick et al., 2014 ]. Then Gu et al. introduced\ndynamic convolution[Chen et al., 2020] into U-Net proposed\nCA-Net [Gu et al., 2020]. Based on U-Net, Yang et al. pro-\nposed DCU-Net [Yanget al., 2022] by referring to the idea of\nresidual connection and deformable convolution [Dai et al.,\n2017].Lei et al. [Lei et al., 2022 ] proposed a network ASE-\nNet based on adversarial consistency learning and dynamic\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1017\nconvolution.\nThe rapid development of CNNs in the ﬁeld of medical im-\nage segmentation is largely due to the scale invariance and in-\nductive bias of convolution operation. Although this ﬁxed re-\nceptive ﬁeld improves the computational efﬁciency of CNNs,\nit limits its ability to capture the relationship between distant\npixels in medical images and lacks the ability to model med-\nical images in a long range.\nAiming at the shortcomings of CNNs in obtaining global\nfeatures of medical images, scholars have proposed a Trans-\nformer architecture. In 2017, Vaswani et al. [Vaswani et al.,\n2017] proposed the ﬁrst Transformer network. Because of\nits unique structure, Transformer obtains the ability to pro-\ncess indeﬁnite-length input, establish long-range dependency\nmodeling, and capture global information. With the excel-\nlent performance of Transformer in NLP ﬁelds, ViT [Doso-\nvitskiy et al., 2020 ] applied Transformer to the ﬁeld of im-\nage processing for the ﬁrst time. Then Chen et al. put for-\nward TransUNet [Chen et al., 2021 ], which initiates a new\nperiod of Transformer in the ﬁeld of medical image segmen-\ntation. Valanarasu et al. proposed MedT [Valanarasu et al.,\n2021] in combination with the gating mechanism. Cao et al.\nproposed a pure Transformer network Swin-Unet [Cao et al.,\n2021] for medical image segmentation, in combination with\nthe shifted-window multi-head self-attention (SW-MSA) in\nSwin Transformer [Liu et al., 2021b]. Subsequently, Wang et\nal. designed the BAT[Wanget al., 2021a] network for dermo-\nscopic images segmentation by combining the edge detection\nidea [Sun et al., 2022 ]. Hatamizadeh et al. proposed Swin\nUNETR [Tang et al., 2022] network for 3D brain tumor seg-\nmentation. Wang et al. proposed the UCTransNet [Wang et\nal., 2022] network that combines the channel attention with\nTransformer.\nThese methods can be roughly divided into based on the\npure Transformer architecture and based on the hybrid archi-\ntecture of CNNs and Transformer. The pure Transformer net-\nwork realizes the long-range dependency modeling based on\nself-attention. However, due to the lack of inductive bias of\nthe Transformer itself, Transformer cannot be widely used\nin small-scale datasets like medical images [Shamshad et al.,\n2022]. At the same time, Transformer architecture is prone to\nignore detailed local features, which reduces the separability\nbetween the background and the foreground of small lesions\nor objects with large-scale changes in the medical image.\nThe hybrid architecture of CNNs and Transformer realizes\nthe local and global information modeling of medical im-\nages by taking advantage of the complementary advantages\nof CNNs and Transformer, thus achieving a better medical\nimage segmentation effect [Azad et al., 2022]. However, this\nhybrid architecture still suffers from the following two prob-\nlems. First, it ignores the problems of organ deformation and\nlesion irregularities when modeling local features, resulting\nin weak local feature expression. Second, it ignores the corre-\nlation between the feature map space and the channels when\nmodeling the global feature, resulting in inadequate expres-\nsion of self-attention. To address the above problems, our\nmain contributions are as follows:\n• A novel dynamic deformable convolution (DDConv) is\nproposed. Through task adaptive learning, DDConv can\nﬂexibly change the weight coefﬁcient and deformation\noffset of convolution itself. DDConv can overcome the\nproblems of ﬁxation of receptive ﬁelds and sharing of\nconvolution kernel parameters, which are common prob-\nlems of vanilla convolution and its variant convolutions,\nsuch as Atrous convolution and Involution, etc. Im-\nproves the ability to perceive tiny lesions and targets\nwith large-scale changes in medical images.\n• A new (shifted)-window adaptive complementary atten-\ntion module ((S)W-ACAM) is proposed. (S)W-ACAM\nrealizes the cross-dimensional global modeling of med-\nical images through four parallel branches of weight co-\nefﬁcient adaptive learning. Compared with the current\npopular attention mechanisms, such as CBAM and Non-\nLocal, (S)W-ACAM fully makes up for the deﬁciency\nof the conventional attention mechanism in modeling\nthe cross-dimensional relationship between spatial and\nchannels. It can capture the cross-dimensional long-\ndistance correlation features in medical images, and en-\nhance the separability between the segmented object and\nthe background in medical images.\n• A new parallel network structure based on dynamically\nadaptive CNNs and cross-dimensional feature fusion\nTransformer is proposed for medical image segmenta-\ntion, called CiT-Net. Compared with the current popular\nhybrid architecture of CNNs and Transformer, CiT-Net\ncan maximize the retention of local and global features\nin medical images. It is worth noting that CiT-Net not\nonly abandons pre-training but also has fewer parame-\nters and less computational costs, which are 11.58 M\nand 4.53 GFLOPs respectively.\nCompared with the previous vanilla convolution [Ron-\nneberger et al., 2015 ], dynamic convolution [Chen et al.,\n2020] [Li et al., 2021 ], and deformable convolution [Dai et\nal., 2017], our DDConv can not only adaptively change the\nweight coefﬁcient and deformation offset of the convolution\naccording to the medical image task, but also better adapt to\nthe shape of organs and small lesions with large-scale changes\nin the medical image, and additionally, it can improve the lo-\ncal feature expression ability of the segmentation network.\nCompared with the self-attention mechanism in the existing\nTransformer architectures [Cao et al., 2021 ] [ Wang et al.,\n2021a], our (S)W-ACAM requires fewer parameters and less\ncomputational costs while it’s capable of capturing the global\ncross-dimensional long-range dependency in the medical im-\nage, and improving the global feature expression ability of the\nsegmentation network. Our CiT-Net does not require a large\nnumber of labeled data for pre-training, but it can maximize\nthe retention of local details and global semantic information\nin medical images. It has achieved the best segmentation per-\nformance on both dermoscopic images and liver datasets.\n2 Method\n2.1 Overall Architecture\nThe fusion of local and global features are clearly helpful for\nimproving medical image segmentation. CNNs capture lo-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1018\nFigure 1: (a) The architecture of CiT-Net. CiT-Net consists of a dual-branch interaction between dynamically adaptive CNNs and cross-\ndimensional feature fusion Transformer. The DDConv in the CNNs branch can adaptively change the weight coefﬁcient and deformation\noffset of the convolution itself, which improves the segmentation accuracy of irregular objects in medical images. The (S)W-ACAM in the\nTransformer branch can capture the cross-dimensional long-range dependency in medical images, improving the separability of segmented\nobjects and backgrounds in medical images. The lightweight perceptron module (LPM) greatly reduces the parameters and calculations\nof the original Transformer network by using the Ghost strategy. (b) Two successive Transformer blocks. W-ACAM and SW-ACAM are\ncross-dimensional self-attention modules with shifted windows and compact convolutional projection conﬁgurations.\ncal features in medical images through convolution operation\nand hierarchical feature representation. In contrast, the Trans-\nformer network realizes the extraction of global features in\nmedical images through the cascaded self-attention mecha-\nnism and the matrix operation with context interaction. In\norder to make full use of local details and global semantic\nfeatures in medical images, we design a parallel interactive\nnetwork architecture CiT-Net. The overall architecture of the\nnetwork is shown in Figure 1 (a). CiT-Net fully considers the\ncomplementary properties of CNNs and Transformer. During\nthe forward propagation process, CiT-Net continuously feeds\nthe local details extracted by the CNNs to the decoder of the\nTransformer branch. Similarly, CiT-Net also feeds the global\nlong-range relationship captured by the Transformer branch\nto the decoder of the CNNs branch. Obviously, the proposed\nCiT-Net provides better local and global feature representa-\ntion than pure CNNs or Transformer networks, and it shows\ngreat potential in the ﬁeld of medical image segmentation.\nSpeciﬁcally, CiT-Net consists of a patch embedding model,\ndynamically adaptive CNNs branch, cross-dimensional fu-\nsion Transformer branch, and feature fusion module. Among\nthem, the dynamically adaptive CNNs branch and the cross-\ndimensional fusion Transformer branch follow the design of\nU-Net and Swin-Unet, respectively. The dynamically adap-\ntive CNNs branch consists of seven main stages. By using\nthe weight coefﬁcient and deformation offset adaptive DD-\nConv in each stage, the segmentation network can better un-\nderstand the local semantic features of medical images, better\nperceive the subtle changes of human organs or lesions, and\nimprove the ability of extracting multi-scale change targets\nin medical images. Similarly, the cross-dimensional fusion\nTransformer branch also consists of seven main stages. By\nusing (S)W-ACAM attention in each stage, as shown in Fig-\nure 1 (b), the segmentation network can better understand the\nglobal dependency of medical images to capture the position\ninformation between different organs, and improve the sep-\narability of the segmented object and the background in the\nmedical images.\nAlthough our CiT-Net can effectively improve the feature\nrepresentation of medical images, it requires a large num-\nber of training data and network parameters due to the dual-\nbranch structure. As the conventional Transformer network\ncontains a lot of MLP layers, which not only aggravates the\ntraining burden of the network but also makes the number of\nmodel parameters rise sharply, resulting in the slow training\nof the model. Inspired by the idea of the Ghost network [Han\net al., 2020 ], we redesign the MLP layer in the original\nTransformer and proposed a lightweight perceptron module\n(LPM). The LPM can help our CiT-Net not only achieve bet-\nter medical image segmentation results than MLP but also\ngreatly reduced the parameters and computational complex-\nity of the original Transformer block, even the Transformer\ncan achieve good results without a lot of labeled data train-\ning. It is worth mentioning that the dual-branch structure\ninvolves mutually symmetric encoders and decoders so that\nthe parallel interaction network structure can maximize the\npreservation of local features and global features in medical\nimages.\n2.2 Dynamic Deformable Convolution\nVanilla convolution has spatial invariance and channel speci-\nﬁcity, so it has a limited ability to change different visual\nmodalities when dealing with different spatial locations. At\nthe same time, due to the limitations of the receptive ﬁeld, it\nis difﬁcult for vanilla convolution to extract features of small\ntargets or targets with blurred edges. Therefore, vanilla con-\nvolution inevitably has poor adaptability and weak general-\nization ability for complex medical images. Although the ex-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1019\nFigure 2: The module of the proposed DDConv. Compared with the current popular convolution strategy, DDConv can dynamically adjust\nthe weight coefﬁcient and deformation offset of the convolution itself during the training process, which is conducive to the feature capture\nand extraction of irregular targets in medical images. \u000b and \f represent the different weight values of DDConv in different states.\nisting deformable convolution [Dai et al., 2017] and dynamic\nconvolution [Chen et al., 2020] [Li et al., 2021] outperforms\nvanilla convolution to a certain extent, they still have the un-\nsatisﬁed ability to balance the performance and size of net-\nworks when dealing with medical image segmentation.\nIn order to solve the shortcomings of current convolution\noperations, this paper proposes a new convolution strategy,\nDDConv, as shown in Figure 2. It can be seen that DDConv\ncan adaptively learn the kernel deformation offset and weight\ncoefﬁcients according to the speciﬁc task and data distribu-\ntion, so as to realize the change of both the shapes and the\nvalues of convolution kernels. It can effectively deal with the\nproblems of large data distribution differences and large tar-\nget deformation in medical image segmentation. Also, DD-\nConv is plug-and-play and can be embedded in any network\nstructure.\nThe shape change of the convolutional kernel in DDConv\nis based on the network learning of the deformation offsets.\nThe segmentation network ﬁrst samples the input feature map\nXusing a square convolutional kernelS, and then performs a\nweighted sum with a weight matrix M. The square convolu-\ntion kernel Sdetermines the range of the receptive ﬁeld, e.g.,\na 3 \u00023 convolution kernel can be expressed as:\nS = f(0;0);(0;1);(0;2);:::; (2;1);(2;2)g; (1)\nthen the output feature map Y at the coordinate 'n can be\nexpressed as:\nY ('n) =\nX\n'm2S\nS('m) \u0001X('n + 'm) ; (2)\nwhen the deformation offset 4'm = fm= 1;2;3;:::;N g\nis introduced in the weight matrix M, N is the total length of\nS. Thus the Equation (2) can be expressed as:\nY ('n) =\nX\n'm2S\nS('m) \u0001X('n + 'm + 4'm) : (3)\nThrough network learning, an offset matrix with the same\nsize as the input feature map can be ﬁnally obtained, and the\nmatrix dimension is twice that of the input feature map.\nTo show the convolution kernel of DDConv is dynamic, we\nﬁrst present the output feature map of vanilla convolution:\ny= \u001b(W \u0001x); (4)\nwhere \u001bis the activation function,W is the convolutional ker-\nnel weight matrix andyis the output feature map. In contrast,\nthe output of the feature map of DDConv is:\n^y= \u001b((\u000b1 \u0001W1 + ::: + \u000bn \u0001Wn) \u0001x) ; (5)\nwhere nis the number of weight coefﬁcients,\u000bn is the weight\ncoefﬁcients with learnable parameters and ^y is the output\nfeature map generated by DDConv. DDConv achieves dy-\nnamic adjustment of the convolution kernel weights by lin-\nearly combining different weight matrices according to the\ncorresponding weight coefﬁcients before performing the con-\nvolution operation.\nAccording to the above analysis, we can see that DDConv\nrealizes the dynamic adjustment of the shape and weights of\nthe convolution kernel by combining the convolution kernel\ndeformation offset and the convolution kernel weight coefﬁ-\ncient with a minimal number of calculation. Compared with\ndirectly increasing the number and size of convolution ker-\nnels, the DDConv is simpler and more efﬁcient. The pro-\nposed DDConv not only solves the problem of poor adaptive\nfeature extraction ability of ﬁxed-size convolution kernels but\nalso overcomes the defect that different inputs share the same\nconvolution kernel parameters. Consequently, our DDConv\ncan be used to improve the segmentation accuracy of small\ntargets and large targets with blurred edges in medical im-\nages.\n2.3 Shifted Window Adaptive Complementary\nAttention Module\nThe self-attention mechanism is the core computing unit in\nTransformer networks, which realizes the capture of long-\nrange dependency of feature maps by utilizing matrix oper-\nations. However, the self-attention mechanism only consid-\ners the dependency in the spatial dimension but not the cross-\ndimensional dependency between spatial and channels[Hong\net al., 2021 ]. Therefore, when dealing with medical image\nsegmentation with low contrast and high density noise, the\nself-attention mechanism is easy to confuse the segmentation\ntargets with the background, resulting in poor segmentation\nresults.\nTo solve the problems mentioned above, we propose a\nnew cross-dimensional self-attention module called (S)W-\nACAM. As shown in Figure 3, (S)W-ACAM has four par-\nallel branches, the top two branches are the conventional\ndual attention module [Liu et al., 2021a ] and the bottom\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1020\nFigure 3: The module of the proposed (S)W-ACAM. Unlike con-\nventional self-attention, (S)W-ACAM has the advantages of spatial\nand channel attention, and can also capture long-distance correla-\ntion features between spatial and channels. Through the shifted win-\ndow operation, the spatial resolution of the image is signiﬁcantly re-\nduced, and through the compact convolutional projection operation,\nthe channel dimension of the image is also signiﬁcantly reduced.\nThus, the overall computational costs and complexity of the network\nare reduced. \u00151, \u00152, \u00153 and \u00154 are learnable weight parameters.\ntwo branches are cross-dimensional attention modules. Com-\npared to popular self-attention modules such as spatial self-\nattention, channel self-attention, and dual self-attention, our\nproposed (S)W-ACAM can not only fully extract the long-\nrange dependency of spatial and channels, but also capture\nthe cross-dimensional long-range dependency between spa-\ntial and channels. These four branches complement each\nother, provide richer long-range dependency relationships,\nenhance the separability between the foreground and back-\nground, and thus improve the segmentation results for medi-\ncal images.\nThe standard Transformer architecture [Dosovitskiy et al.,\n2020] uses the global self-attention method to calculate the\nrelationship between one token and all other tokens. This\ncalculation method is complex, especially in the face of high-\nresolution and intensive prediction tasks like medical images\nwhere the computational costs will increase exponentially. In\norder to improve the calculation efﬁciency, we use the shifted\nwindow calculation method similar to that in Swin Trans-\nformer [Liu et al., 2021b ], which only calculates the self-\nattention in the local window. However, in the face of our\n(S)W-ACAM four branches module, using the shifted win-\ndow method to calculate self-attention does not reduce the\noverall computational complexity of the module. Therefore,\nwe also designed the compact convolutional projection. First,\nwe reduce the local size of the medical image through the\nshifted window operation, then we compress the channel di-\nmension of feature maps through the compact convolutional\nprojection, and ﬁnally calculate the self-attention. It is worth\nmentioning that this method can not only better capture the\nglobal high-dimensional information of medical images but\nalso signiﬁcantly reduce the computational costs of the mod-\nule. Suppose an image containsh\u0002wwindows, each window\nsize is M\u0002M, then the complexity of the (S)W-ACAM, the\nglobal MSA in the original Transformer, and the (S)W-MSA\nin the Swin Transformer are compared as follows:\n\n (MSA) = 4 hwC2 + 2(hw)2C; (6)\n\n ((S)W-MSA) = 4 hwC2 + 2M2hwC; (7)\n\n ((S)W-ACAM) = hwC2\n4 + M2hwC: (8)\nif the former term of each formula is a quadratic function\nof the number of patches hw, the latter term is linear when\nM is ﬁxed (the default is 7). Then the computational costs\nof (S)W-ACAM are smaller compared with MSA and (S)W-\nMSA.\nAmong the four parallel branches of (S)W-ACAM, two\nbranches are used to capture channel correlation and spatial\ncorrelation, respectively, and the remaining two branches are\nused to capture the correlation between channel dimension\nC and space dimension H and vice versa (between channel\ndimension C and space dimension W). After adopting the\nshifted window partitioning method, as shown in Figure 2\n(b), the calculation process of continuous Transformer blocks\nis as follows:\n^Tl = W-ACAM\n\u0000\nLN\n\u0000\nTl\u00001\u0001\u0001\n+ Tl\u00001; (9)\nTl = LPM\n\u0010\nLN\n\u0010\n^Tl\n\u0011\u0011\n+ ^Tl; (10)\n^Tl+1 = SW-ACAM\n\u0000\nLN\n\u0000\nTl\u0001\u0001\n+ Tl; (11)\nTl+1 = LPM\n\u0010\nLN\n\u0010\n^Tl+1\n\u0011\u0011\n+ ^Tl+1: (12)\nwhere ^Tl and Tl represent the output features of (S)W-\nACAM and LPM, respectively. W-ACAM represents win-\ndow adaptive complementary attention, SW-ACAM repre-\nsents shifted window adaptive complementary attention, and\nLPM represents lightweight perceptron module. For the spe-\nciﬁc attention calculation process of each branch, we follow\nthe same principle in Swin Transformer as follows:\nAttention(Q;K;V ) = SoftMax\n \nQKT\np\nC=8\n+ B\n!\nV;\n(13)\nwhere relative position bias B 2 RM2\u0002M2\n, Q;K;V 2\nRM2\u0002C\n8 are query, key, and value matrices respectively. C\n8\nrepresents the dimension of query/key, andM2 represents the\nnumber of patches.\nAfter four parallel attention branches Out1, Out2, Out3\nand Out4 are calculated, the ﬁnal feature fusion output is:\nOut= \u00151 \u0001Out1 + \u00152 \u0001Out2 + \u00153 \u0001Out3 + \u00154 \u0001Out4; (14)\nwhere \u00151, \u00152, \u00153 and \u00154 are learnable parameters that enable\nadaptive control of the importance of each attention branch\nfor spatial and channel information in a particular segmen-\ntation task through the back-propagation process of the seg-\nmentation network.\nDifferent from other self-attention mechanisms, the (S)W-\nACAM in this paper can fully capture the correlation between\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1021\nMethod DI\" JA\" SE\" AC\" SP\" Para. (M)# GFLOPs\nCNNs\nU-Net [Ronneberger et al., 2015] 86.54 79.31 88.56 93.16 96.44 34.52 65.39\nR2UNet [Alom et al., 2018] 87.92 80.28 90.92 93.38 96.33 39.09 152.82\nAttention Unet [Oktay et al., 2018] 87.16 79.55 88.52 93.17 95.62 34.88 66.57\nCENet [Gu et al., 2019] 87.61 81.18 90.71 94.03 96.35 29.02 11.79\nCPFNet † [Feng et al., 2020] 90.18 82.92 91.66 94.68 96.63 30.65 9.15\nTransformer\nSwin-Unet † [Cao et al., 2021] 89.26 80.47 90.36 94.45 96.51 41.40 11.63\nTransUNet † [Chen et al., 2021] 89.39 82.10 91.43 93.67 96.54 105.30 15.21\nBAT †[Wang et al., 2021a] 90.21 83.49 91.59 94.85 96.57 45.56 13.38\nCvT † [Wu et al., 2021] 88.23 80.21 87.60 93.68 96.28 21.51 20.53\nPVT [Wang et al., 2021b] 87.31 79.99 87.74 93.10 96.21 28.86 14.92\nCrossForm [Wang et al., 2021c] 87.44 80.06 88.25 93.39 96.40 38.66 13.57\nCiT-Net-T (our) 90.72 84.59 92.54 95.21 96.83 11.58 4.53\nCiT-Net-B (our) 91.23 84.76 92.68 95.56 98.21 21.24 13.29\nTable 1: Performance comparison of the proposed method against the SOTA approaches on the ISIC2018 benchmarks. Red indicates the\nbest result, and blue displays the second-best.\n† indicates the model is initialized with pre-trained weights on the ImageNet21K. “Para.” refers to the number of parameters. “GFLOPs”\nis calculated under the input scale of 224 \u0002224. Since the dermoscopic images are 2D medical images, the comparison methods are all\n2D networks.\nspatial and channels, and reasonably use the context infor-\nmation of medical images to achieve long-range dependence\nmodeling. Since our (S)W-ACAM effectively overcomes bet-\nter feature representation of the defect that the conventional\nself-attention only focuses on the spatial self-attention of\nimages and ignores the channel and cross-dimensional self-\nattention, it achieves the best image suffers from large noise,\nlow contrast, and complex background.\n2.4 Architecture Variants\nWe have built a CiT-Net-T as a base network with a model\nsize of 11.58 M and a computing capacity of 4.53 GFLOPs.\nIn addition, we built the CiT-Net-B network to make a fair\ncomparison with the latest networks such as CvT [Wu et al.,\n2021] and PVT [Wang et al., 2021b]. The window size is set\nto 7, and the input image size is 224 \u0002224. Other network\nparameters are set as follows:\n• CiT-Net-T: layer number = f2; 2; 6; 2; 6; 2; 2g,\nH = f3; 6; 12; 24; 12; 6; 3g, D= 96\n• CiT-Net-B: layer number= f2; 2; 18; 2; 18; 2; 2g,\nH = f4; 8; 16; 32; 16; 8; 4g, D= 96,\nD represents the number of image channels when en-\ntering the ﬁrst layer of the dynamically adaptive CNNs\nbranch and the cross-dimensional fusion Transformer branch,\nlayernumber represents the number of Transformer blocks\nused in each stage, and H represents the number of multiple\nheads in self-attention.\n3 Experiment and Results\n3.1 Datasets\nWe conducted experiments on the skin lesion segmenta-\ntion dataset ISIC2018 from the International Symposium on\nBiomedical Imaging (ISBI) and the Liver Tumor Segmenta-\ntion Challenge dataset (LiTS) from the Medical Image Com-\nputing and Computer Assisted Intervention Society (MIC-\nCAI). The ISIC2018 contains 2,594 dermoscopic images\nfor training, but the ground truth images of the testing set\nhave not been released, thus we performed a ﬁve-fold cross-\nvalidation on the training set for a fair comparison. The LiTS\ncontains 131 3D CT liver scans, where 100 scans of which\nare used for training, and the remaining 31 scans are used\nfor testing. In addition, all images are empirically resized to\n224 \u0002224 for efﬁciency.\n3.2 Implementation Details\nAll the networks are implemented on NVIDIA GeForce RTX\n3090 24GB and PyTorch 1.7. We utilize Adam with an initial\nlearning rate of 0.001 to optimize the networks. The learning\nrate decreases in half when the loss on the validation set has\nnot dropped by 10 epochs. We used mean squared error loss\n(MSE) and Dice loss as loss functions in our experiment.\n3.3 Evaluation and Results\nIn this paper, we selected the mainstream medical image seg-\nmentation networks U-Net [Ronneberger et al., 2015], Atten-\ntion Unet [Oktay et al., 2018], Swin-Unet [Cao et al., 2021],\nPVT [Wang et al., 2021b ], CrossForm [Wang et al., 2021c ]\nand the proposed CiT-Net to conduct a comprehensive com-\nparison of the two different modalities datasets, ISIC2018 and\nthe LiTS.\nIn the experiment of the ISIC2018 dataset, we made an\noverall evaluation of the mainstream medical image segmen-\ntation network by using ﬁve indicators: Dice (DI), Jaccard\n(JA), Sensitivity (SE), Accuracy (AC), and Speciﬁcity (SP).\nTable 1 shows the quantitative analysis of the results of the\nproposed CiT-Net and the current mainstream CNNs and\nTransformer networks in the ISIC2018 dataset. From the\nexperimental results, we can conclude that our CiT-Net has\nthe minimum number of parameters and the lowest compu-\ntational costs, and can obtain the best segmentation effect on\nthe dermoscopic images without adding pre-training. More-\nover, our CiT-Net-T network has only 11.58 M of parameters\nand 4.53 GFLOPs of computational costs, but still achieves\nthe second-best segmentation effect. Our CiT-Net-B network,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1022\nMethod DI \" VOE # RVD # ASD # RMSD # Para. (M)# GFLOPs\nCNNs\nU-Net [Ronneberger et al., 2015] 93.99±1.23 11.13±2.47 3.22±0.20 5.79±0.53 123.57±6.28 34.52 65.39\nR2UNet [Alom et al., 2018] 94.01±1.18 11.12±2.37 2.36±0.15 5.23±0.45 120.36±5.03 39.09 152.82\nAttention Unet [Oktay et al., 2018] 94.08±1.21 10.95±2.36 3.02±0.18 4.95±0.48 118.67±5.31 34.88 66.57\nCENet [Gu et al., 2019] 94.04±1.15 11.03±2.31 6.19±0.16 4.11±0.51 115.40±5.82 29.02 11.79\n3D Unet [C ¸ ic ¸eket al., 2016] 94.10±1.06 11.13±2.23 1.42±0.13 2.61±0.45 36.43±5.38 40.32 66.45\nV-Net [Milletari et al., 2016] 94.25±1.03 10.65±2.17 1.92±0.11 2.48±0.38 38.28±5.05 65.17 55.35\nTransformer\nSwin-Unet † [Cao et al., 2021] 95.62±1.32 9.73±2.16 2.78±0.21 2.35±0.35 38.85±5.42 41.40 11.63\nTransUNet † [Chen et al., 2021] 95.79±1.09 9.82±2.10 1.98±0.15 2.33±0.41 37.22±5.23 105.30 15.21\nCvT † [Wu et al., 2021] 95.81±1.25 9.66±2.31 1.77±0.16 2.34±0.29 36.71±5.09 21.51 20.53\nPVT [Wang et al., 2021b] 94.56±1.15 9.75±2.19 1.69±0.12 2.42±0.34 37.35±5.16 28.86 14.92\nCrossForm [Wang et al., 2021c] 94.63±1.24 9.72±2.24 1.65±0.15 2.39±0.31 37.21±5.32 38.66 13.57\nCiT-Net-T (our) 96.48±1.05 9.53±2.11 1.45±0.12 2.29±0.33 36.21±4.97 11.58 4.53\nCiT-Net-B (our) 96.82±1.22 9.46±2.33 1.38±0.13 2.21±0.35 36.08±4.88 21.24 13.29\nTable 2: Performance comparison of the proposed method against the SOTA approaches on the LiTS-Liver benchmarks. Red indicates the\nbest result, and blue displays the second-best.\n† indicates the model initialized with pre-trained weights on ImageNet21K. “Para.” refers to the number of parameters. “GFLOPs” is\ncalculated under the input scale of 224 \u0002224. Compared with the comparison experiment oin the ISIC2018 dataset, 3D Unet and V-Net\nare introduced into the comparison experiment oin the LiTS-Liver dataset.\nBAT, CvT, and CrossForm have similar parameters or compu-\ntational costs, but in the ISIC2018 dataset, the division Dice\nvalue of our CiT-Net-B is 1.02%, 3.00%, and 3.79% higher\nthan that of the BAT, CvT, and CrossForm network respec-\ntively. In terms of other evaluation indicators, our CiT-Net-B\nis also signiﬁcantly better than other comparison methods.\nIn the experiment of the LiTS-Liver dataset, we conducted\nan overall evaluation of the mainstream medical image seg-\nmentation network by using ﬁve indicators: DI, VOE, RVD,\nASD and RMSD. Table 2 shows the quantitative analysis of\nthe results of the proposed CiT-Net and the current main-\nstream networks in the LiTS-Liver dataset. It can be seen\nfrom the experimental results that our CiT-Net has great ad-\nvantages in medical image segmentation, which further ver-\niﬁes the integrity of CiT-Net in preserving local and global\nfeatures in medical images. It is worth noting that the CiT-\nNet-B and CiT-Net-T networks have achieved good results\nin medical image segmentation in the ﬁrst and second place,\nwith the least number of model parameters and computational\ncosts. The division Dice value of our CiT-Net-B network\nwithout pre-training is 1.20%, 1.03%, and 1.01% higher than\nthat of the Swin-Unet, TransUNet, and CvT network with\npre-training. In terms of other evaluation indicators, our CiT-\nNet-B is also signiﬁcantly better than other comparison meth-\nods.\nBackbone DDConv(S)W-ACAMLPMPara. (M)DI (%) \"\nU-Net+Swin-Unet 46.92 87.45\nU-Net+Swin-Unet p 48.25 89.15\nU-Net+Swin-Unet p 30.26 89.62\nU-Net+Swin-Unet p 15.45 88.43\nU-Net+Swin-Unet p p 32.16 90.88\nU-Net+Swin-Unet p p 16.93 89.12\nU-Net+Swin-Unet p p 9.67 89.46\nCiT-Net-T (our) p p p 11.58 90.72\nTable 3: Ablation experiments of DDConv, (S)W-ACAM and LPM\nin CiT-Net in the ISIC2018 dataset.\n3.4 Ablation Study\nIn order to fully prove the effectiveness of different modules\nin our CiT-Net, we conducted a series of ablation experiments\non the ISIC2018 dataset. As shown in Table 3, we can see\nthat the Dynamic Deformable Convolution (DDConv) and\n(Shifted) Window Adaptive Complementary Attention Mod-\nule ((S)W-ACAM) proposed in this paper show good perfor-\nmance, and the combination of these two modules, CiT-Net\nshows the best medical image segmentation effect. At the\nsame time, the Lightweight Perceptron Module (LPM) can\nsigniﬁcantly reduce the overall parameters of the CiT-Net.\n4 Conclusion\nIn this study, we have proposed a new architecture CiT-\nNet that combines dynamically adaptive CNNs and cross-\ndimensional fusion Transformer in parallel for medical im-\nage segmentation. The proposed CiT-Net integrates the ad-\nvantages of both CNNs and Transformer, and retains the lo-\ncal details and global semantic features of medical images to\nthe maximum extent through local relationship modeling and\nlong-range dependency modeling. The proposed DDConv\novercomes the problems of ﬁxed receptive ﬁeld and param-\neter sharing in vanilla convolution, enhances the ability to ex-\npress local features, and realizes adaptive extraction of spatial\nfeatures. The proposed (S)W-ACAM self-attention mecha-\nnism can fully capture the cross-dimensional correlation be-\ntween feature spatial and channels, and adaptively learn the\nimportant information between spatial and channels through\nnetwork training. In addition, by using the LPM to replace the\nMLP in the traditional Transformer, our CiT-Net signiﬁcantly\nreduces the number of parameters, gets rid of the dependence\nof the network on pre-training, avoids the challenge of the\nlack of labeled medical image data and easy over-ﬁtting of\nthe network. Compared with popular CNNs and Transformer\nmedical image segmentation networks, our CiT-Net shows\nsigniﬁcant advantages in terms of operational efﬁciency and\nsegmentation effect.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1023\nAcknowledgments\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grants 62271296, 62201334\nand 62201452, in part by the Natural Science Basic Research\nProgram of Shaanxi under Grant 2021JC-47, and in part by\nthe Key Research and Development Program of Shaanxi un-\nder Grants 2022GY-436 and 2021ZDLGY08-07.\nReferences\n[Alom et al., 2018] Md Zahangir Alom, Mahmudul Hasan,\nChris Yakopcic, Tarek M Taha, and Vijayan K Asari. Re-\ncurrent residual convolutional neural network based on\nu-net (r2u-net) for medical image segmentation. arXiv\npreprint arXiv:1802.06955, 2018.\n[Azad et al., 2022] Reza Azad, Ehsan Khodapanah Aghdam,\nAmelie Rauland, Yiwei Jia, Atlas Haddadi Avval, Afshin\nBozorgpour, Sanaz Karimijafarbigloo, Joseph Paul Co-\nhen, Ehsan Adeli, and Dorit Merhof. Medical image seg-\nmentation review: The success of u-net. arXiv preprint\narXiv:2211.14830, 2022.\n[Cao et al., 2021] Hu Cao, Yueyue Wang, Joy Chen, Dong-\nsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning\nWang. Swin-unet: Unet-like pure transformer for medi-\ncal image segmentation.arXiv preprint arXiv:2105.05537,\n2021.\n[Chen et al., 2020] Yinpeng Chen, Xiyang Dai, Mengchen\nLiu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dy-\nnamic convolution: Attention over convolution kernels. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11030–11039,\n2020.\n[Chen et al., 2021] Jieneng Chen, Yongyi Lu, Qihang Yu,\nXiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L\nYuille, and Yuyin Zhou. Transunet: Transformers make\nstrong encoders for medical image segmentation. arXiv\npreprint arXiv:2102.04306, 2021.\n[C ¸ ic ¸eket al., 2016] ¨Ozg¨un C ¸ ic ¸ek, Ahmed Abdulkadir, So-\neren S Lienkamp, Thomas Brox, and Olaf Ronneberger.\n3d u-net: learning dense volumetric segmentation from\nsparse annotation. In International conference on medi-\ncal image computing and computer-assisted intervention,\npages 424–432. Springer, 2016.\n[Dai et al., 2017] Jifeng Dai, Haozhi Qi, Yuwen Xiong,\nYi Li, Guodong Zhang, Han Hu, and Yichen Wei. De-\nformable convolutional networks. In Proceedings of the\nIEEE international conference on computer vision, pages\n764–773, 2017.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Feng et al., 2020] Shuanglang Feng, Heming Zhao, Fei Shi,\nXuena Cheng, Meng Wang, Yuhui Ma, Dehui Xiang,\nWeifang Zhu, and Xinjian Chen. Cpfnet: Context\npyramid fusion network for medical image segmenta-\ntion. IEEE transactions on medical imaging, 39(10):3008–\n3018, 2020.\n[Girshick et al., 2014] Ross Girshick, Jeff Donahue, Trevor\nDarrell, and Jitendra Malik. Rich feature hierarchies for\naccurate object detection and semantic segmentation. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 580–587, 2014.\n[Gu et al., 2019] Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang\nZhou, Huaying Hao, Yitian Zhao, Tianyang Zhang,\nShenghua Gao, and Jiang Liu. Ce-net: Context encoder\nnetwork for 2d medical image segmentation. IEEE trans-\nactions on medical imaging, 38(10):2281–2292, 2019.\n[Gu et al., 2020] Ran Gu, Guotai Wang, Tao Song, Rui\nHuang, Michael Aertsen, Jan Deprest, S´ebastien Ourselin,\nTom Vercauteren, and Shaoting Zhang. Ca-net: Com-\nprehensive attention convolutional neural networks for ex-\nplainable medical image segmentation. IEEE transactions\non medical imaging, 40(2):699–711, 2020.\n[Han et al., 2020] Kai Han, Yunhe Wang, Qi Tian, Jianyuan\nGuo, Chunjing Xu, and Chang Xu. Ghostnet: More\nfeatures from cheap operations. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 1580–1589, 2020.\n[Hong et al., 2021] Luminzi Hong, Risheng Wang, Tao Lei,\nXiaogang Du, and Yong Wan. Qau-net: Quartet atten-\ntion u-net for liver and liver-tumor segmentation. In 2021\nIEEE International Conference on Multimedia and Expo\n(ICME), pages 1–6. IEEE, 2021.\n[Lei et al., 2022] Tao Lei, Dong Zhang, Xiaogang Du, Xuan\nWang, Yong Wan, and Asoke K Nandi. Semi-supervised\nmedical image segmentation using adversarial consistency\nlearning and dynamic convolution network. IEEE Trans-\nactions on Medical Imaging, 2022.\n[Lei et al., March 2023] Tao Lei, Rui Sun, Xiaogang Du,\nHuazhu Fu, Changqing Zhang, and Asoke K Nandi. Sgu-\nnet: Shape-guided ultralight network for abdominal image\nsegmentation. IEEE Journal of Biomedical and Health In-\nformatics, 27(3):1431–1442, March, 2023.\n[Li et al., 2021] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li,\nQi She, Lei Zhu, Tong Zhang, and Qifeng Chen. Invo-\nlution: Inverting the inherence of convolution for visual\nrecognition. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n12321–12330, 2021.\n[Liu et al., 2021a] Xin Liu, Guobao Xiao, Luanyuan Dai,\nKun Zeng, Changcai Yang, and Riqing Chen. Scsa-net:\nPresentation of two-view reliable correspondence learn-\ning via spatial-channel self-attention. Neurocomputing,\n431:137–147, 2021.\n[Liu et al., 2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1024\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012–\n10022, 2021.\n[Liu et al., 2023] Yanhong Liu, Ji Shen, Lei Yang, Guibin\nBian, and Hongnian Yu. Resdo-unet: A deep residual net-\nwork for accurate retinal vessel segmentation from fun-\ndus images. Biomedical Signal Processing and Control,\n79:104087, 2023.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 3431–\n3440, 2015.\n[Lv et al., 2022] Peiqing Lv, Jinke Wang, and Haiying\nWang. 2.5 d lightweight riu-net for automatic liver and\ntumor segmentation from ct. Biomedical Signal Process-\ning and Control, 75:103567, 2022.\n[Milletari et al., 2016] Fausto Milletari, Nassir Navab, and\nSeyed-Ahmad Ahmadi. V-net: Fully convolutional neural\nnetworks for volumetric medical image segmentation. In\n2016 fourth international conference on 3D vision (3DV),\npages 565–571. IEEE, 2016.\n[Oktay et al., 2018] Ozan Oktay, Jo Schlemper, Loic Le Fol-\ngoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa,\nKensaku Mori, Steven McDonagh, Nils Y Hammerla,\nBernhard Kainz, et al. Attention u-net: Learning where to\nlook for the pancreas. arXiv preprint arXiv:1804.03999,\n2018.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In International\nConference on Medical image computing and computer-\nassisted intervention, pages 234–241. Springer, 2015.\n[Shamshad et al., 2022] Fahad Shamshad, Salman Khan,\nSyed Waqas Zamir, Muhammad Haris Khan, Munawar\nHayat, Fahad Shahbaz Khan, and Huazhu Fu. Trans-\nformers in medical imaging: A survey. arXiv preprint\narXiv:2201.09873, 2022.\n[Song et al., 2020] Jiarui Song, Beibei Li, Yuhao Wu, Yaxin\nShi, and Aohan Li. Real: A new resnet-alstm based in-\ntrusion detection system for the internet of energy. In\n2020 IEEE 45th Conference on Local Computer Networks\n(LCN), pages 491–496. IEEE, 2020.\n[Suetens, 2017] Paul Suetens. Fundamentals of medical\nimaging. Cambridge university press, 2017.\n[Sun et al., 2022] Rui Sun, Tao Lei, Qi Chen, Zexuan Wang,\nXiaogang Du, Weiqiang Zhao, and A Nandi. Survey of im-\nage edge detection. Frontiers in Signal Processing, 2(1):1–\n13, 2022.\n[Tang et al., 2022] Yucheng Tang, Dong Yang, Wenqi Li,\nHolger R Roth, Bennett Landman, Daguang Xu, Vish-\nwesh Nath, and Ali Hatamizadeh. Self-supervised pre-\ntraining of swin transformers for 3d medical image anal-\nysis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 20730–\n20740, 2022.\n[Valanarasu et al., 2021] Jeya Maria Jose Valanarasu, Poo-\njan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Med-\nical transformer: Gated axial-attention for medical im-\nage segmentation. In International Conference on Med-\nical Image Computing and Computer-Assisted Interven-\ntion, pages 36–46. Springer, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems,\n30, 2017.\n[Wang et al., 2021a] Jiacheng Wang, Lan Wei, Liansheng\nWang, Qichao Zhou, Lei Zhu, and Jing Qin. Boundary-\naware transformers for skin lesion segmentation. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 206–216. Springer,\n2021.\n[Wang et al., 2021b] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 568–578, 2021.\n[Wang et al., 2021c] Wenxiao Wang, Lu Yao, Long Chen,\nBinbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Cross-\nformer: A versatile vision transformer hinging on cross-\nscale attention. arXiv preprint arXiv:2108.00154, 2021.\n[Wang et al., 2022] Haonan Wang, Peng Cao, Jiaqi Wang,\nand Osmar R Zaiane. Uctransnet: rethinking the skip\nconnections in u-net from a channel-wise perspective with\ntransformer. In Proceedings of the AAAI conference on\nartiﬁcial intelligence, volume 36, pages 2441–2449, 2022.\n[Wu et al., 2021] Haiping Wu, Bin Xiao, Noel Codella,\nMengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 22–31, 2021.\n[Yang et al., 2022] Xin Yang, Zhiqiang Li, Yingqing Guo,\nand Dake Zhou. Dcu-net: a deformable convolutional\nneural network based on cascade u-net for retinal ves-\nsel segmentation. Multimedia Tools and Applications ,\n81(11):15593–15607, 2022.\n[Zhou et al., 2018] Zongwei Zhou, Md Mahfuzur Rah-\nman Siddiquee, Nima Tajbakhsh, and Jianming Liang.\nUnet++: A nested u-net architecture for medical image\nsegmentation. In Deep learning in medical image analy-\nsis and multimodal learning for clinical decision support,\npages 3–11. Springer, 2018.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7945058345794678
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7511616349220276
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7342425584793091
    },
    {
      "name": "Segmentation",
      "score": 0.6528470516204834
    },
    {
      "name": "Image segmentation",
      "score": 0.5492661595344543
    },
    {
      "name": "Transformer",
      "score": 0.5263251662254333
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5081112384796143
    },
    {
      "name": "Deep learning",
      "score": 0.5077265501022339
    },
    {
      "name": "Feature extraction",
      "score": 0.49487608671188354
    },
    {
      "name": "Kernel (algebra)",
      "score": 0.4607982933521271
    },
    {
      "name": "Computer vision",
      "score": 0.45518702268600464
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.43134766817092896
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.4222722053527832
    },
    {
      "name": "Artificial neural network",
      "score": 0.3368711471557617
    },
    {
      "name": "Engineering",
      "score": 0.09085464477539062
    },
    {
      "name": "Mathematics",
      "score": 0.08493676781654358
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210121771",
      "name": "First Affiliated Hospital of Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I51622183",
      "name": "Shaanxi University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17145004",
      "name": "Northwestern Polytechnical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I59433898",
      "name": "Brunel University of London",
      "country": "GB"
    }
  ]
}