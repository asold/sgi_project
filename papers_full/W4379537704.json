{
  "title": "Temporal fusion transformer-based prediction in aquaponics",
  "url": "https://openalex.org/W4379537704",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5108624637",
      "name": "Ahmet Metin",
      "affiliations": [
        "Bursa Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5049248460",
      "name": "Ahmet Ka≈üif",
      "affiliations": [
        "Bursa Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5038073619",
      "name": "Cagatay Catal",
      "affiliations": [
        "Qatar University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2271174547",
    "https://openalex.org/W3107980734",
    "https://openalex.org/W3016836618",
    "https://openalex.org/W2909877301",
    "https://openalex.org/W4205141961",
    "https://openalex.org/W3120460349",
    "https://openalex.org/W2900296911",
    "https://openalex.org/W4313472348",
    "https://openalex.org/W2533750305",
    "https://openalex.org/W3096731251",
    "https://openalex.org/W4220968027",
    "https://openalex.org/W3005205558",
    "https://openalex.org/W4229054687",
    "https://openalex.org/W4293063320",
    "https://openalex.org/W3127112266",
    "https://openalex.org/W4317569465",
    "https://openalex.org/W4283214670",
    "https://openalex.org/W2084678032",
    "https://openalex.org/W2905238323",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W2111072639",
    "https://openalex.org/W2949676527"
  ],
  "abstract": null,
  "full_text": "Vol:.(1234567890)\nThe Journal of Supercomputing (2023) 79:19934‚Äì19958\nhttps://doi.org/10.1007/s11227-023-05389-8\n1 3\nTemporal fusion transformer‚Äëbased prediction \nin¬†aquaponics\nAhmet¬†Metin1¬†¬∑ Ahmet¬†Kasif1¬†¬∑ Cagatay¬†Catal2\nAccepted: 6 May 2023 / Published online: 6 June 2023 \n¬© The Author(s) 2023\nAbstract\nAquaponics offers a soilless farming ecosystem by merging modern hydroponics \nwith aquaculture. The fish food is provided to the aquaculture, and the ammonia \ngenerated by the fish is converted to nitrate using specialized bacteria, which is an \nessential resource for vegetation. Fluctuations in the ammonia levels affect the gen-\nerated nitrate levels and influence farm yields. The sensor-based autonomous con-\ntrol of aquaponics can offer a highly rewarding solution, which can enable much \nmore efficient ecosystems. Also, manual control of the whole aquaponics operation \nis prone to human error. Artificial Intelligence-powered Internet of Things solutions \ncan reduce human intervention to a certain extent, realizing more scalable environ-\nments to handle the food production problem. In this research, an attention-based \nTemporal Fusion Transformers deep learning model was proposed and validated to \nforecast nitrate levels in an aquaponics environment. An aquaponics dataset with \ntemporal features and a high number of input lines has been employed for validation \nand extensive analysis. Experimental results demonstrate significant improvements \nof the proposed model over baseline models in terms of MAE, MSE, and Explained \nVariance metrics considering one-hour sequences. Utilizing the proposed solution \ncan help enhance the automation of aquaponics environments.\nKeywords Time series forecasting¬†¬∑ Aquaponics¬†¬∑ Transformers¬†¬∑ Anomaly \ndetection¬†¬∑ Deep learning\n * Cagatay Catal \n ccatal@qu.edu.qa\n Ahmet Metin \n ahmet.metin@btu.edu.tr\n Ahmet Kasif \n ahmet.kasif@btu.edu.tr\n1 Computer Engineering, Bursa Technical University, Bursa, Turkey\n2 Computer Science and¬†Engineering, Qatar University, Doha, Qatar\n19935\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n1 Introduction\nThe increasing global population has led to an increase in food demand, putting \na strain on traditional agriculture. This demand is further exacerbated by climate \nchange, leading to challenges in water availability [1 ]. The adoption of chemical \nfertilizers and pesticides in traditional agriculture has helped increase crop yields \nbut has also had negative impacts on the environment and human health. The \nexcessive use of fertilizers and pesticides has led to soil degradation, water pollu-\ntion, and health problems, leading to debates worldwide about the sustainability \nof agriculture. Organic agriculture proposes a healthier alternative to reduce the \nimpact of pesticides but provides less food compared to traditional agriculture [2 , \n3]. In recent years, there has been a growing interest in sustainable farming prac-\ntices that can address these challenges, such as the use of aquaponics.\nAquaponics is a new farming method that has gained popularity due to its ben-\nefits over traditional agriculture. It eliminates the need for synthetic fertilizers \nand pesticides, making it an organic farming method. The aquaponics technique \nproposes a more complex yet highly rewarding alternative to conventional agri-\nculture using a combination of hydroponics, fish farms, and bacteria. In aqua-\nponics, fish waste provides nutrients to the plants, which in turn filters the water \nand provides clean water back to the fish. Fish waste produces ammonia, which \nis toxic to fish but can be converted into nitrate by beneficial bacteria. Figure¬† 1 \ndepicts a sample aquaponics ecosystem. Hydroponic farms need nutritious water \nto reproduce. Fish waste contains ammonia, which can be collected through water \npumps and turned into nitrate using beneficial bacteria.\nThe only input to the aquaponics ecosystem is fish food. Many existing aqua-\nponics ecosystems require careful maintenance through human intervention, \nwhich can be time-consuming and labor-intensive. Keeping pH between certain \nlevels, feeding time for fish, and maintaining temperature [4 ] are all things that \nmust be periodically checked. This manual maintenance can be more challenging \nfor larger aquaponics systems that require more frequent monitoring. With the \ndevelopments in the Internet of Things (IoT) technology, there is now an oppor -\ntunity to reduce human intervention and improve food yields by implementing a \nsmart environment capable of mass production.\nTo predict nitrate levels in an aquaponics environment accurately, highly accu-\nrate prediction models are needed. While several models are possible using tra-\nditional machine learning algorithms, their performance needs to be improved. \nTherefore, this study utilized the most recent machine learning and deep learn-\ning algorithms. The algorithms include Long Short-Term Memory (LSTM), \nEncoder‚ÄìDecoder LSTM, Attention LSTM, Extreme Learning Machine (ELM), \nand Temporal Fusion Transformer (TFT). Since LSTM models are effective in \ncapturing temporal dependencies, which are often present in time series data, \nthey have been successfully used for several problems involving time series \ndata [5 ]. Recently, a new deep learning algorithm named Transformer has been \ndeveloped, and several variations of this algorithm have been implemented [6 ]. \nFor longer sequences with complex dependencies, transformer-based models \n19936 A.¬†Metin et al.\n1 3\nprovide relatively better performance than LSTM models. To the best of our \nknowledge, Temporal Fusion Transformers have not been applied in this problem \nbefore. Therefore, we aimed to utilize these algorithms in this research. Since the \nmain objective is to achieve a highly accurate prediction model, state-of-the-art \nmachine learning algorithms were applied and compared.\nThe contributions of the study are given as follows:\n‚Ä¢ A novel deep learning model has been developed for forecasting aquaponics. The \nmodel comprises a TFT-based solution.\n‚Ä¢ The TFT network improves the hourly forecasting performance for the aquapon-\nics environment over the previous works regarding Mean Absolute Error (MAE) \nand Explained Variance.\n‚Ä¢ The high-performing forecasting accuracy provides opportunities for automated \nprocesses.\nThe paper is organized as follows: Sect.¬† 2 provides the related work. Section¬† 3 \nexplains the methods, including the ELM, LSTM, Encoder‚ÄìDecoder networks, the \nTFT technique, and with Evaluation Metrics. Section¬† 4 presents the dataset and the \nexperimental results. Section¬† 5 includes the discussion, and Sect.¬† 6 concludes the \npaper.\nFig. 1  Aquaponics ecosystem\n19937\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n2  Related work\nIn this section, previous studies and the current state of literature in the context \nof the maintenance of smart aquaponics systems based on Artificial Intelligence \n(AI) are discussed. Monitoring and maintaining self-sufficient smart aquaponics \nsystems requires autonomous control through sensors. Arvind et¬† al. developed \na miniature smart aquaponics ecosystem through several IoT sensors and used \nthe produced data to implement an autoML regressor [7 ]. The regressor is then \nutilized to create autonomous anomaly signals, which can be used to reduce the \nmaintenance burden of the proposed ecosystem. Mehra et¬†al. proposed an artifi-\ncial neural network (ANN) to classify several anomalies, such as lack of nutrients \nand changes in levels of humidity or lighting, through sensor reports [8 ], though \nthe system lacks reproducibility as the accuracy metrics are not provided. Hydro-\nponics systems require a certain pH level to operate properly. One of the main \nfactors affecting the pH level is the presence of heavy metals in the ecosystem. \nDhal et¬† al. proposed a real-time machine learning-based solution supported by \na real-life application to monitor and detect anomalies in heavy metal levels [9 ]. \nThe solution only analyzes calcium, sulfate, and phosphate and can be expanded \nto include other heavy metals such as iron, copper, and zinc. Another limitation \nof the work was that, while there was a high-dimensional feature space, the obser -\nvation size was small, resulting in reduced prediction performance.\nAdvancements in modern cameras improved industrial image quality in the \nlast decade and enabled more powerful image processing techniques to be applied \nin the management of aquaponics. Handling these images through inference is \nstill a challenging topic, as these systems consist of several low-capacity IoT \ndevices. Another solution requires server-based handling and fast communication \ntechnologies. With the adoption of 5¬† G, improved data transfer speeds provide \nbetter opportunities to establish autonomous aquaponics systems. Kumar et¬† al. \nprovided an end-to-end aquaponics system to detect anomalies in the physical \nconditions of fish [10]. The fish tanks are periodically imaged and subjected to \nclassification through Bayesian classification. The tanks are connected through \n6LowPAN, which provides the bandwidth for image transfer. Another advan-\ntage of vision systems has been that the stage of growth can become monitorable \nand the study proposed by Lauguico et¬†al. has shown that crop yielding can be \nassisted by Machine Learning (ML) based algorithms [11]. The analysis lacks \nnumber of observations and does not provide an autonomous handling and reso-\nlution strategy.\nIn recent years, studies have shown great opportunities for handling both aqua-\nculture, hydroponics and aquaponics anomalies through time-series-based analy -\nsis and future forecasting. Cardenas et¬† al. proposed an RNN-based solution to \nforecast sudden changes in pH using Recurrent Neural Networks (RNNs) [12]. \nThai-Nghe et¬†al. conducted univariate time-series analysis to monitor water qual-\nity in real-time [13]. The study has shown that the LSTM algorithm can produce \nbetter results against baseline ML methods when tackled with univariate repre-\nsentations. Liu et¬† al. implemented a water quality forecasting framework using \n19938 A.¬†Metin et al.\n1 3\na Bi-directional Stacked Simple Recurrent Unit (Bi-S-SRU) [14]. The Bi-S-SRU \nframework, compared to a vanilla RNN, shows improved forecasting accuracy in \nterms of longer sequences while also providing good inference time. Both the \nfish and the plant need certain conditions for a healthy aquaponics environment. \nThus, nutrient-based analysis to detect input anomalies in the environment is an \nimportant problem. Dhal et¬†al. provided the much needed research with propor -\ntional nutrient data analysis [15]. The researchers deployed an IoT-based aqua-\nponics laboratory and collected inputs with high-dimensional feature space. Still, \nthe research lacked a proper number of observations and is supported by data \naggregation techniques to enable AI-based assistance. The drawback of having \nsmall datasets is investigated in other studies using baseline ML algorithms [16, \n17]. While the studies offered a benchmarking viewpoint on small datasets, the \ngeneral applicability of the results remained low.\nTable¬†1 presents the relevant studies and the current research. It is clearly seen \nthat most papers used proprietary datasets instead of public datasets.\n3  Materials and¬†methods\nIn this section, the proposed TFT model as well as the baseline techniques lead-\ning to the development of the proposed model (LSTMs, encoder‚Äìdecoder networks, \nthe attention concept) are briefly presented. The proposed model is also compared \nwith the Extreme Learning Machine (ELM) as another baseline method. ELM is \na simple and fast-converging algorithm, which can excel at representing complex \ndatasets. Still, the need to employ high number of neurons makes the algorithm \nslower on inference time. Liu et¬†al. demonstrate the power of ELM against LSTM in \nthe estimation of photovoltaic power, where the ELM algorithm is both more accu-\nrate and also computationally more efficient [18]. LSTM is appropriate for mod-\neling sequences with long-term dependencies, whereas encoder‚Äìdecoder networks \nare appropriate for modeling complicated sequences with variable-length input and \noutput. Encoder‚Äìdecoder networks function better when attention mechanisms are \nused, and the Temporal Fusion Transformer is a neural network architecture that was \nspecifically designed for time series forecasting tasks. It combines the strengths of \nLSTMs, attention mechanisms, and transformers to produce accurate and robust pre-\ndictions for multivariate time series data. The inefficiency of simpler models like \nLSTM or GRU in comparison with more sophisticated models like TFT is also dem-\nonstrated by research that estimates energy usage [19].\nThe quality of the data and how it is represented determines how well time series \nanalysis turns out. The characteristics of the dataset and the procedures to prepare \nit for time forecasting models are described in the section on the dataset and data \npreparation. The section ends with an explanation of the evaluation metrics applied \nin the study. This entire method process is illustrated in the workflow diagram in \nFig.¬†2.\nAquaponics environments, when integrated with several sensors, produce a \nseries of time-stamped measurements such as levels of nitrite, ammonia, and pH. \nA fusion-based transformer deep learning model to perform a precise forecast of \n19939\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  Table 1  The relevant studies\nStudy Dataset Method Metrics Gaps\nProposed approach Sensor-based aquaponics TFT MAE, RMSE Multi-variate time-series analysis with,\nMulti-head attention-based TFT architecture\nArvind et¬†al. [7] PASCAL VOC 2017 and 2012 AutoML ROC, F1 Non-temporal analysis\nMehra et¬†al. [8] Proprietary ANN, Bayesian Network Accuracy Small number of observations,\nNon-temporal analysis\nLauguico et¬†al. [11] Proprietary Logistic regression, KNN, L-SVM F1 Small number of observations,\nNon-temporal analysis\nLiu et. al. [14] Water quality dataset Bi-S-SRU MSE Non-temporal analysis\nCardenas et. al. [12] Proprietary MLP, LSTM, GRU MSE Method is susceptible to small-scale noises\nDhal et¬†al. [15] Proprietary XG-boost, extra trees classifier F-Score Small number of observations,\nNon-temporal analysis\nThai-Nghe et¬†al. [13] Water quality‚ÄîTomales Bay LSTM, SVM RMSE Only univariate analysis\n19940 A.¬†Metin et al.\n1 3\nnitrate levels for the upcoming time window using historical data is proposed. \nFrom a machine learning perspective, the problem can be classified as a regres-\nsion problem. Classic machine learning techniques are inadequate for modeling \nthe complexity of the problem due to the high input dimension; instead, more \ncomplex models are required.\n3.1  Dataset\nThis study utilized the sensor-based aquaponics dataset proposed by Ogbuokiri \net¬† al. [20]. This dataset was selected because it is a recent dataset, has high-\nquality data points with reliable sensor measurements, includes several relevant \nparameters, is easily accessible, and has a suitable frequency of data collection. \nThe prediction of nitrate levels in aquaponic systems was performed for the first \ntime using a time-stamped dataset with a high-dimensional feature space. The \ndataset contains 6 parameters of water quality sensors (i.e., temperature, turbid-\nity, dissolved oxygen, pH, ammonia, nitrate), time, and physical conditions of \nfish (i.e., length, width, population). The dataset‚Äôs default data collection inter -\nval is five seconds and contains sensor data for nine freshwater catfish ponds. \nEach sensor was initially calibrated in accordance with industry standards before \nbeing tested [20]. The trustworthiness of the data and the lack of a prior time-\nstamped investigation led to the selection of the proposed dataset. The basic sta-\ntistical analysis of dataset parameters is depicted in Table¬†2 .\nFig. 2  Workflow diagram\n19941\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n3.2  Data preparation\nNormalization is required if the features have drastically different values. The Aqua-\nponics dataset shows a high variational difference between parameters, which leads \nto some features being dominant, as shown in Fig.¬†3. The proposed features are nor-\nmalized to similar sizes because they should be equally important for estimating the \nnitrate level in an aquaponic system analysis. Without normalization, the training \nmodel could blow up with NaNs if the gradient update is too large. By defining a \nunique effective learning rate for each feature, optimizers like Adagrad and Adam \nprovide protection against this problem. Still, ELM is not a gradient-based algo-\nrithm, meaning that the optimizer functionality cannot be utilized. Unlike gradient-\nbased models, high-variance input data causes input saturation in the ELM model, \nwhere the activation function gets saturated at spiked values, limiting the model‚Äôs \ncapacity to understand the underlying patterns in the data. This can be addressed \nTable 2  Dataset statistics for sensor-based attributes\nAttribute Definition Average SD\nTime Timestamp ‚Äì ‚Äì\nTemperature Temperature sensor (DS18B20) 24.565268 0.899205\nTurbidity DF Robot Turbidity sensor 69.490202 43.233901\nDissolved oxygen DF robot dissolved oxygen sensor 10.583218 10.673741\npH DF robot pH sensor V2.2 6.033098 2.949616\nAmmonia MQ-137 ammonia sensor 229841283.6471 9104453144.3300\nNitrate MQ-135 nitrate sensor 699.520674 550.081504\nFig. 3  Box-plot for aquaponics dataset before normalization\n19942 A.¬†Metin et al.\n1 3\nby applying normalization techniques to the dataset. According to studies [21, 22], \nmin-max normalization performs better than its counterparts in time-series-based \nanalysis. Each piece of input is normalized to a value between 0 and 1, which mini-\nmizes the impact of noise and guarantees that neural networks update parameters \neffectively, accelerating the training of the network. Therefore, min-max normaliza-\ntion is utilized in the study as given in Eq. [1] to normalize the features.\nX shows the input variable; min and max values point to the lowest and highest \npoints present in the series, and /uni0303.s1X indicates the normalized value. The dataset also \ncontained missing values with a percentage of around 0.001%. These missing values \nhave been cleaned from the dataset. The original dataset reports at an interval of \n20¬†s, which leads to a noisy structure. Thus, the interval length has been selected \nas 60¬†s for the study, effectively merging each of the three sensor reports by using \nthe arithmetical mean operator. There are 421140 data points in chronological order. \nThere are three subsets of the entire data set: the training dataset, the validation \ndataset, and the test dataset. The data set consists of 90% training data, 8% valida-\ntion data, and 2% test data.\n3.3  LSTM\nRNN and ELM differ in that RNN is better at handling sequence data. It has the \nability to back-propagate as well. The gradient‚Äôs functionality is to update the recur-\nrent neural network‚Äôs weight value. If the weight is kept too low, the gradient van-\nishes, and the hidden layer‚Äôs ability to learn diminishes. The gradient also explodes \nif the specified weight is too high. RNNs also include feedback connections in the \nhidden layer units of their architecture. They are able to process temporal informa-\ntion and learn sequences as a result of this capability. The hidden layer functions as \na memory and has the capacity to store sequential data.\nThe LSTM method was developed as an improved version of RNNs, where the \nvanishing gradient problem is addressed [23]. The term ‚Äúgated‚Äù cell refers to this \ntype of cell because it allows the user to choose whether to retain or disregard stored \ninformation. LSTM comprises three gates, namely an input gate, a forget gate, and \nan output gate (Fig.¬†4).\nForget gate selectively decides what information from earlier time steps should be \nretained. Equation¬†2 is applied.\nIn order to determine which information should be retained in the LSTM memory, \nthis control gate employs a sigmoid function. The values of h(t ‚àí 1) and x(t) are \nlargely responsible for the selection. f(t) produces output values between 0 and 1. \nThe values close to 0 denote the total loss of the previously acquired information, \nwhile the values close to 1 preserve the entire information.\n(1)/uni0303.s1X = x ‚àí min\nmax ‚àí min\n(2)f(t)=/u1D70E(x(t)U f + h(t‚àí 1)W f)\n19943\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \nInput gate that determines which information from the most recent time step should \nbe added. According to Eqs.¬†3,¬†4 and¬†5, this gate is made up of a sigmoid layer and a \nhyperbolic tangent (tanh) layer.\nA vector of new candidate values that will be added to the LSTM memory is repre-\nsented by i2 and i1 represents whether the value needs to be modified or not. Follow-\ning that, element-wise multiplication is applied to the tanh and sigmoid outputs. Cell \nstate, which carries information during the entire sequence and in Eq.¬† 6, serves as a \nrepresentation of the network‚Äôs memory.\nFirst, the forget gate‚Äôs output is multiplied element-wise by the cell state from the \nprevious time step. This makes it possible to reject values in the cell state when they \nare multiplied by values close to 0. Next, the cell state is enhanced by adding the \ninput gate‚Äôs output element by element. The new cell state is what is produced in \nEq.¬†7. The output gate decides the value of the output at the current time step.\n(3)i1 = /u1D70E(x(t)U i + h(t‚àí 1)W i)\n(4)i2 = tanh(x(t)U j + h(t‚àí 1)W g)\n(5)i(t)= i1(t)‚àói 2(t)\n(6)C (t)=f (t)‚àóC (t ‚àí 1)+i(t )\n(7)o(t)= /u1D70E(x(t)U 0 + h(t‚àí 1)W 0)\n(8)h(t)= tanh(C t)‚àóo(t )\nFig. 4  LSTM architecture\n19944 A.¬†Metin et al.\n1 3\nThis gate first determines which side of the LSTM memory contributes to the output \nusing a sigmoid layer. The information that the hidden state should contain is ulti-\nmately determined by multiplying the tanh output by the sigmoid output in Eq.¬† 8. \nThe new hidden state is the output.\n3.4  Encoder‚Äìdecoder networks\nThe Encoder‚ÄìDecoder standard model [24] is generally incapable of accurately han-\ndling long input sequences. The encoder processes the input sequence and compresses \nthe information into a context vector of fixed length. Therefore, only the last hidden \nstate of the encoder RNN is used as the context vector for the decoder. It is expected \nthat this representation will be a good summary of the full capture sequence. On the \ncontrary, the first part is mostly forgotten once it completes the processing of the entire \nentry. In the Encoder‚ÄìDecoder model, an encoder reads the input sentence, a sequence \nof vectors x =( x1,...,xT ) , into a vector c. At each time step t, the hidden state h t of \nthe RNN is updated by using Eqs.¬† 9 and¬† 10 where f  and q are nonlinear activation \nfunctions.\nThe suggested model‚Äôs decoder is conditioned to produce the output sequence by \nanticipating the subsequent symbol yt given the hidden state h t . Additionally, yt and \nh t are dependent on yt‚àí1 and the input sequence‚Äôs summary c.\nConsequently, the decoder‚Äôs hidden state at time t is computed.\n3.5  Attention mechanism\nInitially, attention focused on solving the main problem around the Encoder‚ÄìDecoder \nmodel and achieved great success. The attention was presented by Bahdanau [25], who \nrevolutionized the field of deep learning with the concept of parallel treatment of words \ninstead of processing them sequentially. The central idea of this layer is as follows: \nEach time the model predicts an output word, it only uses parts of the input where the \nmost relevant information is concentrated, rather than the whole sequence. It only pays \nattention to the most relevant inputs and the computing process, as follows:\nThe input sentence is mapped by an encoder to a sequence of annotations (h1 , ...,hT ) , \nwhich determine the context vector ci . Although each annotation h i contains details on \nthe input sequence, only a specific portion of the input is highlighted. As a weighted \nsum, the context vector is subsequently calculated as follows:\n(9)h t = f(xt,h t‚àí1 )\n(10)c = q({h1 ,‚Ä¶ ,hT })\n(11)h t = f(h t‚àí1 ,yt‚àí1 ,c)\n(12)et,i = a(st‚àí1 ,h i),\n19945\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \nThis is known as the alignment model. Based on how closely the input at position \ni and the output at position t match, the alignment model gives a score e t,i . The \nweights a t,i are computed in Eq.¬† 13 by applying a softmax operation to the previ-\nously computed alignment scores.\nEach time step, the decoder receives a distinct context vector, ct . It is calculated \nusing the weighted total of all the hidden states of the encoder in Eq.¬†14.\n3.6  The temporal fusion transformer\nThe TFT provides a neural network architecture that combines the workings of \na number of existing neural architectures, such as LSTM layers, encoder‚Äìdecod-\ners, and the attention heads used in transformers, as shown in Fig.¬† 5 [26]. The \ntransformer primarily consists of an encoder and a decoder, where the encoder \npart uses the time series data as input and the decoder part produces context-\naware embeddings to predict future values. LSTM encoders and decoders sum-\nmarize shorter patterns, whereas long-range relationships are left to the attention \nheads. The temporal multi-head attention block finds and prioritizes the most \nimportant long-range patterns that the time series may include. Each attention \nhead can focus on a different temporal pattern.\nThe context vector is supplied to the Gate layer and then, to the Add & Norm \nlayer. The dropout layer is only used during training and helps prevent the over-\nfitting of the network by randomly eliminating some weights at a rate set by \nthe user. The gated layer merely regulates the bandwidth of information flow \nwithin a particular neuron. Self-attention gathers information from a couple of \ndifferent neurons. The layer first combines the weights from the gated layer with \nthe residual connection weights in the Add & Norm layer. The dependency on \nbatches is eliminated by later normalizing each input to a particular layer across \nall features. Because of this feature, sequence models like transformers and \nrecurrent neural networks are well suited for layer normalization.\nIn terms of processing and predicting time series data, TFT models have \nproven to be more sophisticated than conventional LSTM models. By taking \nadvantage of self-attention, this model offers a novel multi-head attention mech-\nanism that, when analyzed, sheds further light on feature significance. Therefore, \nin contrast to other deep neural networks, these features are no longer regarded \nas black box. The following is a list of TFT‚Äôs primary components:\n(13)a t,i =\nexp(et,i)\n‚àëT\nj=1 exp(et,j)\n(14)ct =\nT/uni2211.s1\ni=1\nat,ihi\n19946 A.¬†Metin et al.\n1 3\n3.6.1  Gated residual network (GRN)\nGRNs are used to eliminate unnecessary and unimportant inputs. In order to avoid \nover-fitting, nodes can be dropped arbitrarily. A more sophisticated model may not \nalways produce greater prediction performance for machine learning models. The \nELU (Exponential Linear Unit) and GLU (Gated Linear Units) activation func-\ntions assist the network in determining which input transformations are straightfor -\nward and which require more complex modeling. The output is the standard Layer \nFig. 5  TFT multi-head attention model architecture\n19947\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \nNormalized before being output. Additionally, the GRN has a residual connection, \nwhich enables the network to learn, if required, to ignore the input. The GRN has \ntwo different inputs: an optional context vector c and a primary input p, which are \ndepicted in Eqs.¬†15-17 as follows:\nWhere ELU is the activation function, /u1D7021 ‚àà Rdmodel ,/u1D7022 ‚àà Rdmodel  are intermediate lay -\ners, LayerNorm is standard layer normalization and the index w indicates weight \nsharing. Following is a description of the GLU:\nIf the input is /u1D6FE , the sigmoid activation function is represented by /u1D70E . In addition, w \nand b represent weights and biases, respectively. The element-wise Hadamard prod-\nuct is ‚®Ä . The model‚Äôs structure can be managed by GRN through the GLU, and \nextra layers can be disregarded. Because nonlinear contributions may be suppressed \nby having all the GLU‚Äôs outputs close to zero, this layer may be completely omitted \nif necessary (Fig.¬†6).\n3.6.2  Variable selection network (VSN)\nTFT‚Äôs variable selection networks are able to decide which input variables are suit-\nable for each time step. Additionally, to enhance forecast accuracy, this module can \nremove the impact of irrelevant variables. TFT uses three instances of the Variable \nSelection Network (VSN) because there are three different input modalities. As a \nresult, each instance has a distinct weight. Categorical variables are represented with \nentity embeddings, and continuous variables are represented with linear transforms. \nGRN is managed internally by the VSN for filtering. Following is a description of \nthe VSN:\n(15)GRN w(p,c)= LayerNorm(p+ GLU w(/u1D7021))\n(16)/u1D7021 =W1,w/u1D7022 + b1,w\n(17)/u1D7022 = ELU(W 2,wp+ W 3,wc)b2,w\n(18)GLU w(/u1D6FE)=/u1D70E(W 4,w/u1D6FE+ b4,w)\n/uni2A00.s1\n(W5,w/u1D6FE+ b5,w)\nFig. 6  Gated residual network model architecture\n19948 A.¬†Metin et al.\n1 3\nThe flattened vector of all previous inputs, called Œût , which is from the correspond-\ning lookback period, is fed through a GRN unit and a softmax function at time t \nto produce a normalized vector of weights, denoted by the vxt\n . The context vector, \nabbreviated as cs , comes from a static covariate encoder. ÃÉùúâ(i)\nt  is the output of a gated \nresidual network. ÃÉùúâ(i)\nt  was calculated by feeding /u1D709(i)\nt  in GRN.\n3.6.3  Interpretable multi‚Äëhead attention\nThe self-attention mechanism is used in this step to assist the model in learning \nlong-range dependencies across various time steps. Contrary to the standard imple-\nmentation, the novel Multi-Head Attention mechanism proposed by TFT provides \nfeature interpretability. To project the input into different representation subspaces, \nthe original architecture included various ‚Äúheads‚Äù like Query, Key, and Value weight \nmatrices. This method‚Äôs disadvantage is that there is no common ground between \nthe weight matrices, making it impossible to interpret them. The addition of a \nnew matrix by TFT‚Äôs multi-head attention allows the various heads to share some \nweights, which can then be explained in terms of seasonality analysis. According \nto the following relationships between keys K ‚àà ‚ÑùN √ódattn and queries Q ‚àà ‚ÑùN √ódattn , \nattention mechanisms scale values V ‚àà ‚ÑùN √ódv generally as follows:\nA() is a function that normalizes data. The scaled dot-product for attention values is \ntypically given as follows:\nTo improve the model‚Äôs capacity for fitting, the TFT employs a multi-head atten-\ntion structure. The mathematical interpretation of Multi-head attention is given as \nfollows:\nAttention weights alone would not be a good indicator of the significance of a \nparticular feature because of the different values used in each head. Therefore, a \n(19)vxt\n= softmax(GRN vx\n(Œût,cs))\n(20)ÃÉùúât =\nm x/uni2211.s1\ni=1\nv(i)\nxt\nÃÉùúâ(i)\nt\n(21)ÃÉùúâ(i)\nt = GRNÃÉùúâ(i)(ùúâ(i)\nt )\n(22)Attention(Q,K,V)=A(Q, K)V\n(23)A(Q,K)= Softmax\nÔøΩ\nQK T\n‚àö\ndattn\nÔøΩ\n(24)MultiHead (Q,K,V)= /bracketleft.s1H1,‚Ä¶ ,HmH\n/bracketright.s1WH\n(25)Hh = Attention(QW (h)\nQ ,KW (h)\nK ,VW(h)\nV )\n19949\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \nmulti-head attention technique to share values across different heads and use addi-\ntive head aggregation is utilized. This approach particularly improves the multi-fea-\nture representative capability of the proposed model. The characteristics of an inter -\npretable multi-head are described as follows:\nIt is clear to see that the result of interpretable multi-head attention is very similar to \nthat of a single attention layer, with the main distinction being the process used to \nproduce attention weights ÃÉA(Q, K) . While paying to a common set of input features \nV, each head can learn various temporal patterns A(QW (h)\nQ , KW (h)\nK ) , which can be \nunderstood as a simple ensemble over attention weights into combined matrix \nÃÉA(Q, K) . When compared to A(Q, K) , ÃÉA(Q, K) , the representation capacity is suc-\ncessfully enhanced.\n3.7  ELM\nBackpropagation employs gradients as a basis. The structural nature of the algorithm \nprovides high computational capacity to model complex problems. Gradient-based \nneural network algorithms also show a high predisposition to local optimus. Thus, \nregarding the nature of the problem analyzed, the algorithm is also included in the \nstudy for comparative purposes.\nThe Extreme Learning Machine (ELM) offers a rapid and powerful alternative for \nboth Machine Learning and Deep Learning-based solutions [27]. ELM is a training \napproach for a single hidden layer feed-forward neural network (SLFN). The architec-\nture employs the following three layers: an input layer, a hidden layer, and an output \nlayer. The hidden layer bias and input weights for the Extreme Learning Machine are \ndetermined at random and frozen during training. The ELM only optimizes the hidden \nlayer weights. A single training iteration and random hidden layer weights enable faster \nconvergence to the global optimum.\nMathematically, ELM can be formulated according to the following equation:\n(26)InterpretableMultiHead(Q,K,V)= ÃÉHW H\n(27)ÃÉH = ÃÉA(Q, K)VWV\n(28)=\n/braceleft.s4\n1\nmH\nmH/uni2211.s1\nh=1\nA(QW (h)\nQ ,KW (h)\nK )\n/braceright.s4\nVWV\n(29)= 1\nmH\nmH/uni2211.s1\nh=1\nAttention(QW (h)\nQ ,KW (h)\nK ,VWV)\n(30)\nÃÇN/uni2211.s1\ni=1\nùõΩig(w ixi + bi)=o j,j= 1, ‚Ä¶ ,N\n19950 A.¬†Metin et al.\n1 3\n3.8  Experimental environment\nThe development language for both pre-processing and network implementation \nwas Python version 3.7. The proposed LSTM networks were implemented using \nthe Keras framework with version 2.9.0, and the TFT network was developed with \nPytorch Lightning version 1.8.0.post1 and Pytorch Forecasting version 0.10.1.\nThe hyperparameter optimization has been conducted at the B.T.U. High-Per -\nformance Clustering Laboratory (HPCLAB). 1 The model is trained on Nvidia 3090 \nGPUs with CUDA version 8. Eight GPUs were used in parallel to accelerate the \noverall training process.\n3.9  Evaluation metrics\nThree measures, namely Mean Absolute Error (MAE), Explained Variance Score, \nand Mean Square Error (MSE), were used to evaluate the proposed model‚Äôs predic-\ntive ability. The Mean Absolute Error is the difference between the expected and \nactual values expressed in absolute terms. In MSE, the average squared difference \nbetween the observed and predicted values is assessed. Any unfavorable indications \nare changed by the squaring. The error measures are defined in Eqs. [31] and [32].\nExplained Variance Score is a metric for measuring the disparity between a model‚Äôs \npredictions and the actual data. In other words, it is the portion of the model‚Äôs total \nvariance that is not attributable to error variance but is explained by factors that \nare actually present. Scores close to 1.0 are highly desired, and error measures are \ndefined in Eqs. [33]. The variance of the predicted errors and the variance of the \nactual values are denoted by Var(ya ‚àí yp) and Var(ya) , respectively.\nR2 Eqs. [34], where N is the total number of the forecasting value, yp is the predicted \nvalue, ya is the original actual value, and yaverage is the average of the original value. \nPerfect forecasting results in a value of 1, whereas a value of 0 means that the per -\nformance is identical to that of a simple model that consistently forecasts the mean \nvalue of the data. There is minimal association between the findings and the dataset \nwhen the R2 value is negative.\n(31)MAE = 1\nN ‚àó\nN/uni2211.s1\nL=1\n‚à£ yp ‚àí ya ‚à£\n(32)MSE = 1\nN ‚àó\nN/uni2211.s1\nL=1\n(yp ‚àí ya)2\n(33)Explained Variance(ya,yp)=1 ‚àí\nVar(ya ‚àí yp)\nVar(ya)\n1 BTU HPCLAB Website: http:// ceng. btu. edu. tr/ hpclab/\n19951\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n4  Experimental results\n4.1  Hyperparameter optimization\nThe selection of hyperparameters has a significant impact on how well deep learning \nmodels perform. Because of this, fine-tuning becomes crucial in the training stage to \nproduce a successful model. This work uses the Keras Bayesian optimizer within the \nKeras-Tuner framework [28] to construct a hyperparameter optimization utilizing a \nrandom search method for the LSTM models. The ELM hyperparameters are opti-\nmized using a hand-made search technique. The hyperparameters for the suggested \nTFT network for predicting aquaponics systems have also been determined using \nOptuna [29] optimizer techniques within the Pytorch framework. The resulting com-\nbinations are shown in Table¬† 3. Random combinations of all accessible hyperpa-\nrameters defining the search spaces are produced, with the number of combinations \ngenerated depending on the maximum number of trials and the number of models to \nbe trained for each test. A model is trained for each of these combinations, and the \none that performs the best is saved as the best model.\n4.2  Discussion of¬†results\nTable¬†4 and Fig.¬† 7 present the error rates of the proposed study for different meth-\nods concerning the metrics RMSE, MAE, Explained Variance and R2 . The proposed \nTFT algorithm demonstrates remarkable improvements over baseline models in all \nmetrics. The higher rates of Explained Variance represent better association of vari-\nance between the original data-space and the generated data-space.\nIn all time windows, the performance improvement is also competitive. As the \nmeasurements deteriorate, the proposed model‚Äôs limitation of a one-hour forecast-\ning window remains. The proposed TFT algorithm performs better than all baseline \n(34)R2 (ya,yp)=1 ‚àí\n‚àëN\nL=1 (ya ‚àí yp)2\n‚àëN\nL=1 (ya ‚àí yaverage)2\nTable 3  Hyperparameter search \nspace and best hyperparameters Hyperparameter Search space Selected value\nLearning rate [30.0 - - - 0.00001] 0.0912\nEpochs [10 - - - 400] 27\nDropout rate [0.1 - - - 0.5] 0.36411\nGradient clip value [0.01 - - - 1.0] 0.03279\nNumber of attention heads [1 - - - 4] 2\nHidden size [8 - - - 128] 13\nHidden continuous size [8 - - - 128] 12\n19952 A.¬†Metin et al.\n1 3\nmethods over an all period. As the value of the forecasting period rises, the forecast-\ning performance of all models gradually deteriorates.\nThe proposed model also shows clear improvements in terms of metric scores \nover the previous works. Table¬† 5 shows a comparison of the proposed study with \nsimilar studies.\nIn the aquaponics nitrate forecasting study, having lots of training data can \nimprove accuracy and produce more effective outcomes. Training, validation, and \ntest rates must be very carefully adjusted in order to prevent over-fitting. Otherwise, \nthe likelihood of receiving inaccurate forecasts is high. Figure¬†7 displays the estima-\ntion graph that the entire method generated. The test results made using the hourly \nnitrate data gathered for the following all test data are displayed.\nTable 4  Performance comparison of the proposed model with baseline methods in different time win-\ndows\nMethods Metrics Time Windows\n15 30 45 60 90 120\nLstm RMSE: 0.03559 0.03904 0.04524 0.04830 0.04928 0.05732\nMae: 0.02341 0.02686 0.03246 0.03495 0.03404 0.04329\nExplained variance score: 0.76193 0.72133 0.63764 0.58409 0.51928 0.43771\nR2 Score: 0.70776 0.64832 0.52807 0.46250 0.44105 0.24616\nAttention RMSE: 0.03441 0.04220 0.04275 0.04416 0.04679 0.05352\nMae: 0.02132 0.03049 0.02941 0.02951 0.03033 0.03864\nExplained variance score: 0.73827 0.68339 0.64295 0.02951 0.49679 0.44146\nR2 Score: 0.72683 0.58925 0.57854 0.55075 0.49607 0.34273\nEncoderDecoder RMSE: 0.03553 0.04136 0.04466 0.04523 0.05119 0.05615\nMae: 0.02300 0.02962 0.03132 0.02906 0.03508 0.04111\nExplained variance score: 0.75628 0.71764 0.63622 0.55144 0.43935 0.39943\nR2 Score: 0.70885 0.60529 0.54000 0.52870 0.39686 0.27651\nELM RMSE: 0.04487 0.04808 0.04850 0.05113 0.05379 0.05730\nMae: 0.02757 0.03167 0.03131 0.03449 0.03591 0.03961\nExplained variance score: 0.53905 0.48314 0.45864 0.40324 0.35113 0.28430\nR2 Score: 0.53550 0.46666 0.45758 0.39770 0.33413 0.24656\nTFT RMSE: 0.02556 0.02658 0.03171 0.03222 0.03792 0.04039\nMae: 0.01665 0.01850 0.02233 0.02261 0.02687 0.02899\nExplained variance score: 0.81492 0.80844 0.70583 0.67483 0.55179 0.46643\nR2 Score: 0.81491 0.79612 0.69783 0.67474 0.52035 0.44584\nTable 5  Comparison of the \nproposed model with other \nstudies\nStudy Target Methods Metrics Scores\n[12] pH RNN MSE 0.0032\n[14] Diss. oxygen Bi-S-SRU MSE 0.1058\nProposed model Nitrate TFT MSE 0.0322\n19953\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \nShort-term representational strength is lacking in the baseline models. Figure¬† 7 \nshows that the models can pick up certain aspects of the long-term structure. \nAlthough the dataset‚Äôs characteristics have been normalized, the internal struc-\nture of the dataset poses a complex modeling challenge for the baseline mod-\nels. Because of the sudden jumps it contains, the basic models fail to simulate \ntemporal representation. The result is that the look-ahead output is skewed and \nvulnerable to temporal drift. Shifts were always present in the forecasts, despite \nthe ELM and LSTM models in Fig.¬† 7 being able to detect the noise. However, \nFig. 7  All test data forecasting performance for methods: a ELM b LSTM c Encoder‚Äìdecoder LSTM d \nAttention LSTM e temporal fusion transformer (TFT)\n19954 A.¬†Metin et al.\n1 3\nalternative algorithms produce forecasts with a smoother transition while not \ncapturing noisy data. This guarantees that the models produce results that are \naccurate. When looking at Fig.¬† 7e, it is clear that the TFT‚Äôs predicted and actual \nnitrate level values overlap and that there is no excess in the deviations or vari-\nances between them. Given the near values of the data and the similarity of the \ndirectional breaks, it is clear why the graph is successful.\nThe results of the various temporal test data sets of the proposed model are \nshown in Fig.¬† 8, and it is clear that even though it does not exactly match the data, \nthe model enables the noise to be estimated with a smooth transition. The model is \nquite good at forecasting long-term predictions, but due to the enormous noise in \nshort-term projections, it cannot capture such predictions accurately. However, the \nshort-term model can sufficiently describe the pattern.\nThe computational performance of the models can be examined through the \nmodels‚Äô parameter sizes or the total time spent on the training phase [14]. In terms \nof both model parameter size and total training time (Table¬† 6), the suggested TFT \nmodel is higher to the basic models. The Encoder‚ÄìDecoder layers operate with \nmuch fewer parameters while providing the model with improved learning capacity. \nThe proposed model needs more epochs to train properly and requires more time \nin total. Prediction timeframes are the longest of all methodologies yet provide for \nsufficient time to run in production settings. The proposed TFT model outperforms \nexisting algorithms in real-world settings in terms of performance needs.\nFig. 8  Performance prediction for the TFT model using test data in hourly forecasting: a 800 test data b \n3500 test data c 8000 test data\n19955\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n5  Conclusions\nSoil usage in agriculture poses a great limitation in matching the increasing demand \nfor food. The soilless farming proposed by aquaponics creates an ecosystem where \nthe only input is fish food and fertilizers are moved out of the system. Autonomous \nhandling of aquaponics with less human intervention provides increased efficiency \nfor higher throughput and fewer maintenance costs.\nThe baseline solutions for simulating aquaponics environments include LSTM, \nEncoder‚ÄìDecoder architectures and Attention-based methods. As shown in Table¬† 5, \nthe proposed model provided 0.0322 MSE value for predicting the nitrate level. \nAlso, different metrics such as RMSE, MAE, and explained variance scores per \nmethod are shown in Table¬† 4. The TFT has a more intricate architecture and better \nlearning capabilities than other baseline deep learning algorithms. In addition, the \nTFT is capable of taking into account a variety of factors in datasets, including static \ninput, known input, and observed input. In contrast, traditional deep learning archi-\ntectures may overemphasize factors that are irrelevant to the target variable.\nTFT proposes several improvements over these baseline methods by combining \nthe Encoder‚ÄìDecoder LSTMs, which model short-term relations with good accu-\nracy, with the feature weighting mechanism of attention matrices to capture long-\nterm relations. Transformer, a recently created encoder‚Äìdecoder model based on the \nattention mechanism, accurately calculates sequences without the use of any repeat-\ning neural networks. As a result, fewer parameters are needed to produce signifi-\ncantly better results. The architecture also improves the predictive performance of \nmulti-variate forecasting due to the masked multi-head attention method used in the \nattentive layer. Thus, a transformer-based Deep Learning solution to forecast nitrate \nlevels using multiple input-features in an aquaponics environment is utilized.\nThe predictive performance of the proposed method shows clear improvements \nover baseline models in terms of metrics MAE, MSE and Explained Variance \nwhen considering all sequences. The problem is impractical for sequences longer \nthan an hour due to the memory space needed to process the attention matrix. \nThus, rendering the simulation infeasible with limited computational resources. \nBesides, multi-step forecasting performance for longer sequences deteriorates \nafter a certain sequence length. The proposed method offers increased sequence \nmodeling capacity, enabling longer sequences to be represented. The employ -\nment of auto-regressive models along with TFT would mean even better sequence \nTable 6  Computational complexity of models\nMethod Param. size Training time (s) Prediction time (s)\nProposed TFT model 31.9k 12828.23 11.97\nAttention LSTM 34.2k 147.65 0.40\nEncoder‚Äìdecoder LSTM 4.5k 102.73 0.24\nLSTM 31k 215.05 0.38\nELM ‚Äì 0.05 0.05\n19956 A.¬†Metin et al.\n1 3\nhandling. Also, the Memory Transformers can be employed in multi-head atten-\ntion architectures to lower the memory requirement of the general approach. To \nfurther capture noisy data in short-time predictions as in the LSTM model, new \nLSTM or RNN layers might be added to the proposed model. Short-time noise \ndata are believed to be properly learned in this manner.\nThe proposed solution improves the overall viewpoint of aquaponics han-\ndling with better simulation performance. Obtained results can be used to han-\ndle anomalies in the ecosystem, such as fish diseases, pump failures, or bacte-\nrial problems. The employment of real-life applications along with the simulation \ncan increase the effectiveness of the aquaponics architecture. Furthermore, the \nimprovements obtained will bring us closer to the reality of the unmanned agri-\nculture of aquaponics.\nAuthor Contributions All authors contributed to writing the manuscript, reviewed it and made necessary \nchanges before submission.\nFunding Open Access funding provided by the Qatar National Library.\nAvailability of data and materials The data supporting the findings of this study are available upon \nrequest.\nDeclarations \nConflict of interest The authors declare no conflict of interest.\nEthical Approval Not applicable. \nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article‚Äôs Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nReferences\n 1. Smit B, Smithers J (1993) Sustainable agriculture: interpretations, analyses and prospects. Can J Regional \nSci 16(3):499‚Äì524\n 2. Reganold JP, Wachter JM (2016) Organic agriculture in the twenty-first century. Nat Plants 2(2):1‚Äì8\n 3. Ka≈üif A, Orta√ß G, Esma ƒ∞, Bilgin TT (2020) Performing similarity analysis on organic farming crop data \nof turkish cities. In: 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pp \n1‚Äì4. IEEE\n 4. Yanes AR, Martinez P, Ahmad R (2020) Towards automated aquaponics: a review on monitoring, iot, and \nsmart systems. J Clean Prod 263:121571\n 5. Siami-Namini S, Tavakoli N, Namin AS (2018) A comparison of arima and lstm in forecasting time series. \nIn: 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pp \n1394‚Äì1401. IEEE\n19957\n1 3Temporal fusion transformer-based prediction in¬†aquaponics  \n 6. Narayan A, Mishra BS, Hiremath PS, Pendari NT, Gangisetty S (2021) An ensemble of transformer and \nlstm approach for multivariate time series data classification. In: 2021 IEEE International Conference \non Big Data (Big Data), pp 5774‚Äì5779. IEEE\n 7. Arvind C, Jyothi R, Kaushal K, Girish G, Saurav R, Chetankumar G (2020) Edge computing based smart \naquaponics monitoring system using deep learning in iot environment. In: 2020 IEEE Symposium \nSeries on Computational Intelligence (SSCI), pp 1485‚Äì1491. IEEE\n 8. Mehra M, Saxena S, Sankaranarayanan S, Tom RJ, Veeramanikandan M (2018) Iot based hydroponics \nsystem using deep neural networks. Comput Electronics Agric 155:473‚Äì486\n 9. Dhal SB, Mahanta S, Gumero J, O‚ÄôSullivan N, Soetan M, Louis J, Gadepally KC, Mahanta S, Lusher J, \nKalafatis S (2023) An iot-based data-driven real-time monitoring system for control of heavy metals to \nensure optimal lettuce growth in hydroponic set-ups. Sensors 23(1):451\n 10. Kumar NH, Baskaran S, Hariraj S, Krishnan V (2016) An autonomous aquaponics system using 6low-\npan based wsn. In: 2016 IEEE 4th International Conference on Future Internet of Things and Cloud \nWorkshops (FiCloudW), pp 125‚Äì132. IEEE\n 11. Lauguico SC, Concepcion R, Alejandrino JD, Tobias RR, Macasaet DD, Dadios EP (2020) A compara-\ntive analysis of machine learning algorithms modeled from machine vision-based lettuce growth stage \nclassification in smart aquaponics. Int J Environ Sci Dev 11(9):442‚Äì449\n 12. Cardenas-Cartagena J, Elnourani M, Beferull-Lozano B (2022) Forecasting aquaponic systems behav-\niour with recurrent neural networks models. In: Proceedings of the Northern Lights Deep Learning \nWorkshop, vol 3\n 13. Thai-Nghe N, Thanh-Hai N, Chi Ngon N (2020) Deep learning approach for forecasting water quality \nin iot systems. Int J Adv Comput Sci Appl 11(8):686‚Äì693\n 14. Liu J, Yu C, Hu Z, Zhao Y, Bai Y, Xie M, Luo J (2020) Accurate prediction scheme of water quality in \nsmart mariculture with deep bi-s-sru learning network. IEEE Access 8:24784‚Äì24798\n 15. Dhal SB, Jungbluth K, Lin R, Sabahi SP, Bagavathiannan M, Braga-Neto U, Kalafatis S (2022) A \nmachine-learning-based iot system for optimizing nutrient supply in commercial aquaponic operations. \nSensors 22(9):3510\n 16. Dhal SB, Bagavathiannan M, Braga-Neto U, Kalafatis S (2022) Can machine learning classifiers be \nused to regulate nutrients using small training datasets for aquaponic irrigation?: A comparative analy-\nsis. Plos One 17(8):0269401\n 17. Dhal SB, Bagavathiannan M, Braga-Neto U, Kalafatis S (2022) Nutrient optimization for plant growth \nin aquaponic irrigation using machine learning for small training datasets. Artif Intell Agric 6:68‚Äì76\n 18. Li Q, Zhang X, Ma T, Jiao C, Wang H, Hu W (2021) A multi-step ahead photovoltaic power prediction \nmodel based on similar day, enhanced colliding bodies optimization, variational mode decomposition, \nand deep extreme learning machine. Energy 224:120094\n 19. Nazir A, Shaikh AK, Shah AS, Khalil A (2023) Forecasting energy consumption demand of customers \nin smart grid using temporal fusion transformer (tft). Res Eng, 100888\n 20. Udanor C, Ossai N, Nweke E, Ogbuokiri B, Eneh A, Ugwuishiwu C, Aneke S, Ezuwgu A, Ugwoke P, \nChristiana A (2022) An internet of things labelled dataset for aquaponics fish pond water quality moni-\ntoring system. Data Brief 43:108400\n 21. Wu C, Chau KW, Fan C (2010) Prediction of rainfall time series using modular artificial neural net-\nworks coupled with data-preprocessing techniques. J Hydrol 389(1‚Äì2):146‚Äì167\n 22. Cao J, Li Z, Li J (2019) Financial time series forecasting model based on ceemdan and lstm. Phys A \nStat Mech Appl 519:127‚Äì139\n 23. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735‚Äì1780\n 24. Cho K, Van Merri√´nboer B, Bahdanau D, Bengio Y (2014) On the properties of neural machine trans-\nlation: encoder-decoder approaches. arXiv preprint arXiv: 1409. 1259\n 25. Chorowski JK, Bahdanau D, Serdyuk D, Cho K, Bengio Y (2015) Attention-based models for speech \nrecognition. Adv Neural Inf Process Syst, 28\n 26. Lim B, Arƒ±k S√ñ, Loeff N, Pfister T (2021) Temporal fusion transformers for interpretable multi-horizon \ntime series forecasting. Int J Forecasting 37(4):1748‚Äì1764\n 27. Huang G-B, Zhu Q-Y, Siew C-K (2006) Extreme learning machine: theory and applications. Neuro-\ncomputing 70(1‚Äì3):489‚Äì501\n 28. O‚ÄôMalley T, Bursztein E, Long J, Chollet F, Jin H, Invernizzi L et¬†al (2019) KerasTuner. https:// github. \ncom/ keras- team/ keras- tuner\n 29. Akiba T, Sano S, Yanase T, Ohta T, Koyama M (2019) Optuna: A next-generation hyperparameter opti-\nmization framework. In: Proceedings of the 25rd ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining\n19958 A.¬†Metin et al.\n1 3\nPublisher‚Äôs Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Aquaponics",
  "concepts": [
    {
      "name": "Aquaponics",
      "score": 0.9624588489532471
    },
    {
      "name": "Computer science",
      "score": 0.7061490416526794
    },
    {
      "name": "Aquaculture",
      "score": 0.4525175094604492
    },
    {
      "name": "Transformer",
      "score": 0.4383033514022827
    },
    {
      "name": "Hydroponics",
      "score": 0.42464566230773926
    },
    {
      "name": "Automation",
      "score": 0.4239642918109894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3846231997013092
    },
    {
      "name": "Machine learning",
      "score": 0.36138084530830383
    },
    {
      "name": "Environmental science",
      "score": 0.33101898431777954
    },
    {
      "name": "Fish <Actinopterygii>",
      "score": 0.1862545609474182
    },
    {
      "name": "Voltage",
      "score": 0.16605299711227417
    },
    {
      "name": "Engineering",
      "score": 0.09842929244041443
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Fishery",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I192257213",
      "name": "Bursa Technical University",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I60342839",
      "name": "Qatar University",
      "country": "QA"
    }
  ]
}