{
    "title": "Activity Graph Transformer for Temporal Action Localization",
    "url": "https://openalex.org/W3123394884",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2753497164",
            "name": "Nawhal Megha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3019606094",
            "name": "Mori, Greg",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962722947",
        "https://openalex.org/W2131042978",
        "https://openalex.org/W3047343286",
        "https://openalex.org/W2519328139",
        "https://openalex.org/W2963321993",
        "https://openalex.org/W2982515679",
        "https://openalex.org/W2962876901",
        "https://openalex.org/W2964214371",
        "https://openalex.org/W2156303437",
        "https://openalex.org/W1944615693",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W2963165299",
        "https://openalex.org/W2597958930",
        "https://openalex.org/W2963247196",
        "https://openalex.org/W2986407524",
        "https://openalex.org/W2342776425",
        "https://openalex.org/W607748843",
        "https://openalex.org/W2751832138",
        "https://openalex.org/W2471143248",
        "https://openalex.org/W2755876276",
        "https://openalex.org/W2105101328",
        "https://openalex.org/W2337252826",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W2962677524",
        "https://openalex.org/W2018068650",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034667697",
        "https://openalex.org/W2895240652",
        "https://openalex.org/W3102393264",
        "https://openalex.org/W2943833595",
        "https://openalex.org/W3034912730",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2964107601",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W2607566495",
        "https://openalex.org/W2604113307",
        "https://openalex.org/W1595717062",
        "https://openalex.org/W2806331055",
        "https://openalex.org/W2964008341",
        "https://openalex.org/W2084341401",
        "https://openalex.org/W2893390896",
        "https://openalex.org/W2461621749",
        "https://openalex.org/W2550143307",
        "https://openalex.org/W3110589170",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W2020163092",
        "https://openalex.org/W2983918066",
        "https://openalex.org/W3095013945",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2472970127",
        "https://openalex.org/W3035392611",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2606461407",
        "https://openalex.org/W2529163075",
        "https://openalex.org/W2463824207",
        "https://openalex.org/W1923332106",
        "https://openalex.org/W2308045930",
        "https://openalex.org/W2014021126",
        "https://openalex.org/W3034623254"
    ],
    "abstract": "We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin.",
    "full_text": "Activity Graph Transformer for Temporal Action Localization\nMegha Nawhal1, Greg Mori1,2\n1 Simon Fraser University, Burnaby, Canada\n2 Borealis AI, Vancouver, Canada\nAbstract\nWe introduce Activity Graph Transformer, an end-to-end\nlearnable model for temporal action localization, that re-\nceives a video as input and directly predicts a set of ac-\ntion instances that appear in the video. Detecting and\nlocalizing action instances in untrimmed videos requires\nreasoning over multiple action instances in a video. The\ndominant paradigms in the literature process videos tem-\nporally to either propose action regions or directly produce\nframe-level detections. However, sequential processing of\nvideos is problematic when the action instances have non-\nsequential dependencies and/or non-linear temporal order-\ning, such as overlapping action instances or re-occurrence\nof action instances over the course of the video. In this\nwork, we capture this non-linear temporal structure by rea-\nsoning over the videos as non-sequential entities in the form\nof graphs. We evaluate our model on challenging datasets:\nTHUMOS14, Charades, and EPIC-Kitchens-100. Our re-\nsults show that our proposed model outperforms the state-\nof-the-art by a considerable margin.\n1. Introduction\nVisual understanding of human activities in untrimmed\nvideos involves reasoning over multiple action instances\nwith varying temporal extents. This problem has been for-\nmally studied in the setup of temporal action localization,\ni.e., given a human activity video, the goal is to predict a\nset of action labels and their corresponding start and end\ntimestamps indicating their occurrence in the video. Rea-\nsoning over untrimmed human activity videos for action\nlocalization is particularly challenging due to the idiosyn-\ncrasies of the videos such as: (1) overlap - the action in-\nstances may have overlaps in their temporal extents indicat-\ning non-sequential temporal ordering of the instances; (2)\nnon-sequential dependencies - some action instances may\nhave temporal dependencies but are separated by other un-\nrelated action instances and/or durations of no action; and\n(3) re-occurrence - instances belonging to same category\nmay appear more than once over the course of the video. In\naction queries\nmix pasta\nput spoon\nopen cupboard close cupboard\ntake colander insert colander\nUntrimmed \nVideo\nground truth\ninstances\npredicted\ninstances\naction label,\nstart time,\nend timeregression\nActivity Graph \nTransformer\nclassification\nFigure 1. Main Idea. Given an untrimmed human activity video,\nwe directly predict the set of action instances (label, start time,\nend time) that appear in the video. We observe that human ac-\ntivity videos contain non-sequential dependencies (illustrated by\nthe ground truth instances as colored bars). In this work, we pro-\npose Activity Graph Transformer that captures this non-sequential\nstructure by reasoning over such videos as graphs. Overall, the\nnetwork receives a video and directly infers a set of action in-\nstances. The network achieves this by transforming a set of graph-\nstructured abstract queries into contextual embeddings which are\nthen used to provide predictions of action instances. It is trained\nend-to-end using classiﬁcation and regression losses.\nthis work, we propose a novel end-to-end learnable model\nfor temporal action localization that receives a video as an\ninput and directly predicts the set of action instances that\nappear in the video.\nExisting approaches for the task of temporal action lo-\ncalization predominantly fall into two paradigms. First is\nthe local-then-global paradigm where the video-level pre-\ndictions are obtained by postprocessing of local (i.e. frame-\nlevel or snippet-level) predictions using sequence modeling\ntechniques such as recurrent neural networks, temporal con-\nvolutions and temporal pooling [11,25,28,32,39,39,41,47,\n63,65]. Second is the proposal-then-classiﬁcation paradigm\nwhich involves generation of a sparse set of class agnostic\nsegment proposals from the overall video followed by clas-\nsiﬁcation of the action categories for each proposal using\neither two-stage learning [3, 4, 12, 18, 42, 44, 68, 69] or end-\nto-end learning [7, 8, 14, 61, 66].\narXiv:2101.08540v2  [cs.CV]  28 Jan 2021\nThe local-then-global paradigm does not utilize the over-\nall temporal context provided by the activity in the video as\nthe local predictions are solely based on visual information\nconﬁned to the frame or the snippet. For instance, consider\nthe example in Figure 1, these approaches would miss out\non important relevant information provided by ‘mix pasta’\nin predicting ‘put spoon’or may produce imprecise predic-\ntions when the temporal extents of instances‘take colander’\nor ‘open cupboard’overlap.\nAlternatively, the proposal-then-classiﬁcation paradigm\ngenerates a subset of proposals by processing the video as\na sequence. As a result, these approaches suffer from lim-\nited receptive ﬁeld for incorporating temporal information,\nand do not capture non-sequential temporal dependencies\neffectively. This problem is further aggravated in the case\nof overlapping action instances. For instance, in the ex-\nample in Figure 1, ‘open cupboard’and ‘close cupboard’\nshare information but are separated by other, potentially\noverlapping, action instances such as ‘take colander’. Due\nto such ordering, when generating proposals correspond-\ning to ‘close cupboard’, these approaches are unlikely to\ncapture the dependency with the visual information pertain-\ning to ‘open cupboard’. Furthermore, these approaches use\nheuristics to perform non-maximal suppression of proposals\nthat might result in imprecise localization outcomes when\nthe action instances vary widely in their temporal extents.\nAs such, both these types of approaches process videos\nsequentially to either generate direct local predictions or ac-\ntion proposals and are problematic when action instances\nreoccur, overlap, or have non-sequential dependencies.\nThese observations suggest that although a video has a lin-\near ordering of frames, the reasoning over the video need\nnot be sequential. We argue that modeling the non-linear\ntemporal structure is a key requirement for effective reason-\ning over untrimmed human activity videos. In this work, we\nseek a temporal action localization model that: (1) captures\nthe temporal structure in complex human activity videos,\n(2) does not rely on heuristics or postprocessing of the pre-\ndictions, and (3) is trained end-to-end.\nTowards this goal, we formulate temporal action local-\nization as a direct set prediction task. We propose a novel\ntemporal action localization model, Activity Graph Trans-\nformer (AGT), an end-to-end learnable model that receives\na video as input and predicts the set of action instances that\nappear in the video. In order to capture the non-linear tem-\nporal structure in videos, we reason over videos as non-\nsequential entities, speciﬁcally, learnable graph structures.\nParticularly, we map the input video to graph-structured em-\nbeddings using an encoder-decoder transformer architecture\nthat operates using graph attention. A ﬁnal feed forward\nnetwork then uses these embeddings to directly predict the\naction instances. Thus, we propose a streamlined end-to-\nend training process that does not require any heuristics.\nTo summarize, our contributions are as follows: (1) we\npropose an encoder-decoder transformer based model Ac-\ntivity Graph Transformer that reasons over videos as graphs\nand can be trained end-to-end, and (2) we achieve state-of-\nthe-art performance on the task of temporal action localiza-\ntion on challenging human activity datasets, namely, THU-\nMOS14 [23], Charades [45], and EPIC-Kitchens100 [10].\n2. Related Work\nIn this section, we discuss the prior work relevant to tempo-\nral action localization and graph based modeling in videos.\nTemporal Action Localization. Early methods for tem-\nporal action localization use temporal sliding windows and\ndesign hand-crafted features to classify action within each\nwindow [13, 22, 35, 50, 64]. However, these approaches are\ncomputationally inefﬁcient as they apply classiﬁers on win-\ndows of all possible sizes and locations in the entire video.\nWith the advances in convolutional neural networks, re-\ncent approaches fall into two dominant paradigms: (1)\nlocal-then-global, (2) proposal-then-classiﬁcation. The\nmethods following the local-then-global paradigm rely on\nobtaining temporal boundaries of actions based on local (i.e.\nframe-level or snippet-level) predictions and perform video-\nlevel reasoning using temporal modeling techniques such as\nexplicit modeling of action durations or transitions [41,65],\nrecurrent neural networks [11, 32, 47, 63], temporal pool-\ning [25], temporal convolutions [28, 39], and temporal at-\ntention [39]. However, these approaches does not utilize the\noverall temporal context of the videos as local predictions\nare computed using only the frame/snippet information.\nThe methods based on proposal-then-classiﬁcation\nparadigm formulate temporal action localization as the mir-\nror problem of object detection in the temporal domain.\nInspired by the progress in object detection [16] tech-\nniques, some methods employ a two-stage training frame-\nwork [3, 4, 12, 18, 42, 44, 68, 69] – they generate a set of\nclass-agnostic segment proposals in the ﬁrst stage and pre-\ndict an action label for each proposal in the second stage.\nMost recent methods in this direction focus on improving\nthe proposal generation stage [1,3,4,12,18,29,30,68], while\na few propose a more accurate classiﬁcation stage [42, 69].\nRecently, some end-to-end trainable architectures have\nalso been proposed [7, 8, 14, 61, 62, 66]. However, these\nmethods also process the video as a sequence and, thus,\nhave limited receptive ﬁeld for capturing temporal infor-\nmation. They do not capture non-sequential temporal de-\npendencies in action instances. Moreover, these approaches\nuse heuristics during training ( e.g. intersection-over-union\nthresholds) to perform non-maximal suppression in the set\nof proposals. This might lead to poor localization perfor-\nmance when the action instances vary widely in their tem-\nporal extents as it might skip some highly overlapping pro-\n2\nposals. To address these problems in object detection, [5]\npropose a transformer based end-to-end learnable architec-\nture that implicitly learns the non-max suppression and per-\nform object detection using proposals as abstract encodings.\nIn contrast to the above approaches, we formulate tem-\nporal action localization as a direct set prediction task. We\npropose to reason over untrimmed videos as non-sequential\nentities ( i.e. graphs) as opposed to existing methods that\nperform sequential reasoning. Our approach is inspired by\n[5] in that we propose an end-to-end learnable transformer\nbased model for direct set prediction. But unlike [5], the\ntransformer model in our approach operates graphs.\nAdditionally, there are other realms of work on temporal\naction localization in weakly supervised setting [21, 43, 58]\nand spatio-temporal action localization [15, 17, 24, 48].\nThese are beyond the scope of this paper.\nAction Recognition. Action recognition methods operate\non short video clips that are trimmed such that a single ac-\ntion instance spans the video duration and, hence, are not\nsuitable for untrimmed videos containing multiple actions.\nNonetheless, models pretrained for the task of action recog-\nnition provide effective feature representations for tasks re-\nlated to untrimmed videos. A wide variety of action recog-\nnition approaches have been proposed ranging from earlier\nmethods based on hand-crafted features [9, 27, 55] to con-\nvolutional models such as I3D [46], 3D-CNN [51] through\nto advanced temporal modeling [57,59,67] and graph mod-\neling [20, 60] techniques. In this paper, we use I3D [46]\npretrained on the Kinetics dataset [6] for feature extraction.\nGraph-based Modeling for Videos.The advances in graph\nconvolutional networks (GCNs) [26] have inspired several\nrecent approaches for video based tasks [34, 36, 60]. Most\nof the graph based approaches for videos represent either\nthe input space ( i.e. videos or derived visual information)\nas graphs [19, 34, 36, 60] or the output space (i.e. labels) as\ngraphs [52]. In contrast, we design our model based on the\ninsight that both the input space ( i.e. features derived from\nvideos) and the output space ( i.e. labels and timestamps\nfor the action) are graph-structured for the task of tempo-\nral action localization. Speciﬁcally, we propose an encoder-\ndecoder transformer architecture to learn the mapping be-\ntween the input and output space. Furthermore, GCNs re-\nquire the information pertaining to the nodes and edges a\npriori. In contrast, we learn the graph structure ( i.e. both\nnodes and edges) from the data itself using self-attention.\n3. Proposed Approach\nIn this section, we present the problem formulation and\nprovide a detailed description of our proposed approach.\nProblem Formulation. The task of temporal action local-\nization involves prediction of the category labels as well\nas start and end timestamps of the actions that occur in\na given video. In this work, we formulate this task as a\ndirect set prediction problem wherein each element in the\npredicted set denotes an action instance in a video. Specif-\nically, given a video V, the goal is to predict a set Awhere\nthe i-th element a(i) = (c(i),t(i)\ns ,t(i)\ne ) denotes an action in-\nstance in the video depicting action category c(i) that starts\nat time 0 ≤ t(i)\ns ≤ T, ends at time 0 ≤ t(i)\ne ≤ T, for\ni ∈{1,2,..., |A|}. Here, |A|is the number of action in-\nstances present in the video and T is the duration of the\nvideo. Thus, |A|and T vary based on the input video.\nTowards this goal, we propose Activity Graph Trans-\nformer (AGT), an end-to-end learnable model that receives\na video as input and directly infers the set of action instances\n(label, start time, end time) in the video. Our approach con-\nsists of: (1) a network that predicts a set of action instances\nin a single forward pass; and (2) a loss function to train the\nnetwork by obtaining a unique alignment between the pre-\ndicted and ground truth action instances. We contend that\neffective reasoning over untrimmed human activity videos\nrequires modeling the non-linear temporal structure in the\nvideos. In our approach, we seek to capture this struc-\nture by employing graphs. Speciﬁcally, we propose a novel\nencoder-decoder transformer network that leverages graph\nbased self-attention to reason over the videos. We describe\nthe details of our approach below.\n3.1. Activity Graph Transformer\nAs shown in Figure 2, Activity Graph Transformer\n(AGT) consists of three components: (1) backbone network\nto obtain a compact representation of the input video; (2)\ntransformer network consisting of an encoder network and\na decoder network that operates over graphs; and (3) pre-\ndiction heads for the ﬁnal prediction of action instances of\nthe form (label, start time, end time). The encoder network\nreceives the compact video-level representation from the\nbackbone network and encodes it to a latent graph repre-\nsentation, referred to as context graph. The decoder net-\nwork receives graph-structured abstract query encodings\n(referred to as action query graph) as input along with the\ncontext graph. The decoder uses the context graph to trans-\nform the action query graph to a graph-structured set of em-\nbeddings. Each node embedding of this decoder output is\nfed into a feed forward network to obtain predictions of\naction instances. The whole AGT network is trained end-\nto-end using a combination of classiﬁcation and regression\nlosses for the action labels and timestamps respectively. Re-\nfer to Algorithm 1 for an overview of one training iteration\nof AGT. We provide detailed description of the components\nbelow.\n3\nBackbone\nPositional \nEncoding\nFFN\nlabel, timestamps\nlabel, timestamps\nlabel, timestamps\nno action\nFFN\nFFN\nFFN\nEncoder Decoder\naction query\ngraph\ntimecontext\ngraph\nGraph\nSelf-Attention\nFFN\nGraph-to-Graph\nAttention\nGraph\nSelf-Attention\nFFN\n+\nFigure 2. Model Overview. Activity Graph Transformer (AGT) receives a video as input and directly predicts a set of action instances\nthat appear in the video. The input video is fed into a backbone network to obtain a compact representation (Section 3.1.1). Then, the\nencoder network (Section 3.1.2) receives the compact video-level representation from the backbone network and encodes it to a latent\ngraph representation context graph. The decoder network (Section 3.1.3) receives the context graph along with graph-structured abstract\nquery encodings action query graph. The decoder transforms the action query graph to a graph-structured set of embeddings. Each node\nembedding of the decoder output is fed into a prediction head (Section 3.1.4). The network is trained end-to-end (Section 3.1.5) using\nclassiﬁcation and regression losses for the action labels and timestamps of the action instances respectively.\n3.1.1 Backbone\nTo obtain a compact representation for the input video V\ncontaining T frames, any 3D convolutional network can\nbe used to extract the features. In our implementation,\nwe chunk the videos into short overlapping segments of\n8 frames and use an I3D model pretrained on the Kinet-\nics [6] dataset to extract features of dimension C (= 2048)\nfrom the segments, resulting in video-level feature v =\n[v(1),v(2) ... v(Nv)] where Nv is the number of chunks\nused in the feature extraction.\n3.1.2 Transformer Encoder\nThe backbone simply provides a sequence of local features\nand does not incorporate the overall context of the video\nor the temporal structure in the video. Therefore, we use\nan encoder network that receives the video-level feature as\ninput and encodes this video representation to a graph (re-\nferred to as the context graph). Intuitively, the encoder is\ndesigned to model the interactions among the local features\nusing self-attention modules.\nThe context graph is initialized with video-level fea-\nture v(i) (of dimension C = 2048 ) as the i-th node for\ni ∈ {1,2,...,N v}. Usually, transformer networks use\nﬁxed positional encoding [37] to provide position informa-\ntion of each element in the input sequence. In contrast, in\nour setting, we contend that the video features have a non-\nlinear temporal structure. Thus, we provide the positional\ninformation using learnable positional encodings pv as ad-\nditional information to the video feature v. The positional\nencoding p(i)\nv corresponds to the i-th node in the graph and\nis of the same dimension as the node. Next, the graph nodes\nare from the same video and hence, they are related to each\nother. However, their connection information (edges) is not\nknown a priori. Thus, we model the interactions among\nthese nodes as learnable edge weights. This is enabled by\nthe graph self-attention module (described below).\nWe design the transformer encoder network E as a se-\nquence of Le blocks, wherein, an encoder block Eℓ for ℓ∈\n{1,2,...,L e}consists of a graph self-attention module fol-\nlowed by a feed forward network. The output of the encoder\nnetwork is the context graph hLe = [ h(1)\nLe ,h(2)\nLe ... h(Nv)\nLe ]\nwhere h(i)\nLe is the i-th node and is of dimension d(same for\neach block). The output of the ℓ-th encoder block hℓ and\nthe ﬁnal output of the encoder hLe are deﬁned as:\nh0 = v\nhℓ = Eℓ(hℓ−1,pv)\nhLe = ELe ◦···◦ E1(v,pv).\n(1)\nGraph Self-Attention.This module aims to model interac-\ntions among graph structured variables along with learnable\nedge weights. Here, we describe the graph self-attention\nmodule in the context of the(ℓ+ 1)-th encoder block Eℓ+1.\nFor simplicity of notation, let x be the output of the ℓ-\nth block of the encoder, i.e., x = Eℓ(hℓ−1,pv). x is a\ngraph contains Nv nodes x(1),x(2),..., x(Nv) which are\nconnected using learnable edge weights. The graph self-\nattention module ﬁrst performs graph message passing (as\ndescribed in [54]) to produce the output x′, with the i-th\nnode of the output deﬁned as\nx′(i) = x(i) +\n⏐⏐⏐\n⏐⏐⏐\nK\nk=1\nσ\n(∑\nj∈Ni\nαk\nijWk\ngx(j)\n)\n, (2)\nwhere\n⏐⏐⏐\n⏐⏐⏐represents concatenation operator, K is the num-\nber of parallel heads in the self-attention module, σ is a\nnon-linear function (leaky ReLU in our case),Nirepresents\nthe set of neighbours of the i-th node, Wk\ng is the learnable\ntransformation weight matrix. αk\nij are the self attention co-\n4\nAlgorithm 1A training iteration of AGT model\nInputs: video V containing T frames; number of action\nquery encodings No; ground truth action instancesA=\n{a(i)}|A|\ni=1\nInitializations: Backbone initialized to I3D model [6] pre-\ntrained on Kinetics dataset; initialize action query graph\nq with No random encodings\n1: compute features using backbone\n2: compute context graph using encoder (see Eq. 1)\n3: compute output embeddings using decoder (see Eq. 4)\n4: for i∈1,2,...,N o do\n5: predict action instance ˜a(i) using prediction head\n6: end for\n7: compute optimal matching ˆφ between {a(i)}|A|\ni=1 and\n{˜a(i)}No\ni=1 using matcher\n8: compute ﬁnal loss LH between {a(i)}|A|\ni=1 and\n{˜a(ˆφ(i))}|A|\ni=1 using Eq. 9\n9: backpropagate LH\nefﬁcients computed by the k-th attention head described as:\nαk\nij =\nexp(f(wT\na,k[Wk\ngx(i)||Wk\ngx(j)]))∑\nm∈Ni\nexp(f(wT\na,k[Wkgx(i)||Wkgx(m)])), (3)\nwhere ·T represents a transpose operator, f is a non-linear\nactivation (leaky ReLU in our case) and wa,k is the atten-\ntion coefﬁcients. αk\nij is the attention weight and denotes the\nstrength of the interaction between i-th and j-th node of the\ninput graph of the module. Subsequent to the message pass-\ning step, we apply batch normalization and a linear layer.\nThis is then followed by a standard multi-head self-attention\nlayer (same as in [53]). Overall, the graph self-attention\nmodule models interactions between the nodes, i.e., local\nfeatures derived from the video.\n3.1.3 Transformer Decoder\nBased on the observation that the action instances in the\nvideo have a non-linear temporal structure, we design\nthe decoder to learn a graph-structured set of embeddings\nwhich would subsequently be used for predicting the action\ninstances. Intuitively, the output graph provided by the de-\ncoder serves as the latent representation for the set of action\ninstances depicted in the video.\nThe inputs of the transformer decoder are: (1) a graph-\nstructured abstract query encodings, referred to as action\nquery graph q, containing No nodes wherein each node is a\nlearnable positional encoding of dimension d(same as the\ndimension used in the encoder); and (2) the context graph\nhLe containing Nv nodes (obtained from the encoder). We\nassume that the number of nodes in the action query graph\nNo is ﬁxed and is sufﬁciently larger than the maximum\nnumber of action instances per video in the dataset. This\nidea of using representations of prediction entities as posi-\ntional query encodings is inspired from [5]. However, un-\nlike the independent queries in [5], we use graph-structured\nencodings for the decoder. To learn the interactions among\nthe graph-structured query embeddings, we use graph self-\nattention modules (same module as used in transformer en-\ncoder). Additionally, we use graph-to-graph attention mod-\nule (described below) to learn interactions between the con-\ntext graph, i.e., the latent representation of the input video,\nand the graph-structured query embeddings, i.e., the latent\nrepresentations of the action queries.\nThe overall decoder network D consists of Ld blocks,\nwherein, a decoder block Dℓ′ for ℓ′ ∈{1,2,...,L d}con-\nsists of a graph self-attention module followed by a graph-\nto-graph attention module, and then a feed forward network.\nThe decoder block Dℓ′ has an output yℓ′ and the ﬁnal out-\nput of the decoder yLd = [y(1)\nLd ,y(2)\nLd ,..., y(No)\nLd ]. They are\ndeﬁned as:\ny0 = q\nyℓ′ = Dℓ′ (yℓ′−1,hLe)\nyLd = DLd ◦···◦ D1(q,hLe)\n(4)\nGraph-to-Graph Attention.The graph-to-graph attention\nmodule aims to learn the interactions between two differ-\nent graphs referred to as a source graph and a target graph.\nHere, we describe this module in the context of the decoder\nblock Dℓ′+1. The input to this block is the output yℓ′ of the\nprevious decoder block Dℓ′ . This is fed to the graph self-\nattention module in the block Dℓ′+1, and the output is used\nas the target graph for the graph-to-graph attention module.\nThe source graph for this module (in any decoder block) is\nthe context graph hLe. For simplicity of notation, let xs\ndenote the source graph ( i.e. hLe) and xt denote the target\ngraph. Here, the source and target graphs may contain dif-\nferent number of nodes. In our case, xs contains Nv nodes\nand xt contains No nodes. The graph-to-graph attention\nmodule ﬁrst performs message passing from source graph\nto target graph to provide an output x′t, with the i-th node\ndeﬁned as\nx′(i)\nt = x(i)\nt +\n⏐⏐⏐\n⏐⏐⏐\nK\nk=1\nσ\n(∑\nj∈Ni\nβk\nijWk\nsx(j)\ns\n)\n, (5)\nwhere Wk\ns is the learnable transformation weight matrix for\nthe source graph. Other symbols denote the same entities as\nin graph self-attention (Section 3.1.2). βk\nij is the attention\ncoefﬁcient for k-th attention head between i-th node of the\nsource graph and j-th node of the target graph computed as:\nβk\nij =\nexp(f(wstT\na,k[Wk\nsx(i)\ns ||Wk\ntx(j)\nt ]))\n∑\nm∈Ni exp(f(wstT\na,k[Wksx(i)\ns ||Wk\ntx(m)\nt ]))\n(6)\n5\nwhere wst\na,k is the graph-to-graph attention coefﬁcients, and\nWk\ns and Wk\nt are the learnable transformation weight matri-\nces for source and target graphs respectively. Other symbols\ndenote the same entities as in graph self-attention. Similar\nto the transformer encoder, subsequent to the message pass-\ning step, we apply batch normalization and a linear layer.\nThis is then followed by a standard multi-head self-attention\nlayer (same as in [53]). Overall, the graph-to-graph atten-\ntion module models the interactions between the latent rep-\nresentations of the input video and the action queries.\n3.1.4 Prediction Heads\nThe decoder network provides a set of embeddings where\nthe embeddings serve as the latent representations for the\naction instances in the video. This output graph yLd con-\ntains No nodes. We use these No node embeddings to ob-\ntain predictions for No action instances using prediction\nheads. The prediction heads consist of a feed forward net-\nwork (FFN) with ReLU activation which provides the start\ntime and end time of the action instance normalized with\nrespect to the overall video duration. Additionally, we use a\nlinear layer with a softmax function to predict the categori-\ncal label corresponding to the action instance.\nTherefore, when provided with the i-th node embed-\nding y(i)\nLd, the prediction head provides prediction ˜a(i) =\n(˜c(i),˜t(i)\ns ,˜t(i)\ne ) where ˜c(i), ˜t(i)\ns and ˜t(i)\ne are the category la-\nbel, start time and end time for the i-th action instance for\ni ∈{1,2,...,N o}. Note that the ground truth set would\ncontain a variable number of action instances, whereas No\nis larger than the maximum number of action instances per\nvideo in the dataset. This calls for a need to suppress irrel-\nevant predictions. We do this by introducing an additional\nclass label ∅ indicating no action (similar to [5]). As such,\nthis non-maximal suppression (typically performed using\nheuristics in existing methods [7]) is learnable in our model.\n3.1.5 Loss functions\nTo train the overall network, we align the predictions with\nthe ground truth action instances using a matcher mod-\nule which optimizes a pair-wise cost function. This pro-\nvides a unique matching between the predicted and ground\ntruth action instances. Subsequently, our model computes\nlosses corresponding to these matched pairs of predicted\nand ground truth action instances to train the overall net-\nwork end-to-end.\nMatcher. The matcher module ﬁnds an optimal matching\nbetween the predicted set of action instances (that contains\nﬁxed number of elements for every video) and the ground\ntruth set of action instances (that contains a variable num-\nber of elements depending on the video). To obtain this\nmatching, we design a matching cost function and employ\nthe Hungarian algorithm to obtain an optimal matching be-\ntween the two sets as described in prior work [49].\nFormally, let Abe the ground truth set of action in-\nstances A= {a(i)}|A|\ni=1, where a(i) = ( c(i),t(i)\ns ,t(i)\ne ) and\n˜Abe the predicted set of action instances ˜A= {˜a(i)}No\ni=1\nwhere ˜a(i) = (˜c(i),˜t(i)\ns ,˜t(i)\ne ). In our model, we assume\nthat No is larger than the number of actions in any video\nin the dataset. Therefore, we assume that ground truth\nset Aalso is a set of size No by padding the remaining\n(No −|A|) elements with ∅ element indicating no action.\nThe optimal bipartite matching between the two sets re-\nduces to choosing the permutation of No elements ˆφfrom\nthe set of all possible permutations ΦNo that results in low-\nest value of the matching cost function Lm. Thus, ˆφ =\nargminφ∈ΦNo\nLm(a(i),˜a(φ(i))), where Lm(a(i),˜a(φ(i))) is\nthe matching cost function between ground truth a(i) and\nprediction with index φ(i). The matching cost function in-\ncorporates the class probabilities of the action instances and\nthe proximity between predicted and ground truth times-\ntamps. Speciﬁcally, we deﬁne the cost function as:\nLm(a(i),˜a(φ(i))) = −1 {c(i)̸=∅}˜pφ(i)(c(i))\n+ 1 {c(i)̸=∅}Ls(s(i),˜s(φ(i))),\n(7)\nwhere s(i) = [ t(i)\ns ,t(i)\ne ] and ˜s(φ(i)) = [ ˜t(φ(i))\ns ,˜t(φ(i))\ne ], and\n˜pφ(i)(c(i)) is the probability of class c(i) for prediction φ(i)\nand Ls represents segment loss that measures proximity in\nthe timestamps of the instances. The segment loss is de-\nﬁned as a weighted combination of an L1 loss (sensitive to\nthe durations of the instances) and an IoU loss (invariant to\nthe durations of the instances) between the predicted and\nground-truth start and end timestamps. It is expressed as:\nLs = λiouLiou(s(i),˜s(φ(i))) + λL1||s(i) −˜s(φ(i))||1, (8)\nwhere λiou,λL1 ∈R are hyperparameters. Subsequent to\nobtaining the optimal permutation ˆφ, we compute the Hun-\ngarian loss LH over all the matched pairs as follows:\nLH =\nNo∑\ni=1\n[\n−log ˜pˆφ(c(i)) + 1 {c(i)̸=∅}Ls(s(i),˜s(ˆφ(i)))\n]\n.\n(9)\nThis loss is used to train our AGT model end-to-end. We\nprovide further implementation details of the model in the\nsupplementary material.\nIn summary, our proposed Activity Graph Transformer\nperforms temporal action localization using an encoder-\ndecoder based architecture leveraging graph based attention\nmodules. We jointly optimize all parameters of our model\nto minimize the regression loss for the start and end times-\ntamps of the action instances and the cross entropy losses\nfor the corresponding action labels.\n6\n4. Experiments\nWe conducted several experiments to demonstrate the ef-\nfectiveness of our proposed approach. In this section, we\nreport the results of our evaluation.\nDatasets. We use three benchmark datasets for evaluation.\nThey vary in their extent of overlap in action instances, the\nnumber of action instances per video, and the number of ac-\ntion categories in the dataset. Thus, these datasets together\nserve as a challenging testbed for our model.\nTHUMOS14 [23] contains 200 videos in training set\nand 213 videos in testing set for the the task of action local-\nization. This dataset has 20 action categories. The videos\ncontain an average of 15 action instances per video with an\naverage of 8% overlapping with other instances.\nCharades [45] is large scale dataset containing 9848\nvideos of daily indoor activities. This dataset has 157 ac-\ntion categories. Videos in the dataset contain an average\nof 6 action instances per video with an average of 79% of\noverlapping instances in a video. This dataset is challenging\nbecause of the high degree of overlap in the action instances.\nEPIC-Kitchens100 [10] contains 700 egocentric videos\nof daily kitchen activities. This dataset contains 289 noun\nand 97 verb classes. Videos in the dataset contain an av-\nerage of 128 action instances per video with an average of\n28% overlapping instances in a video.\nComparison with state-of-the-art. We compare the per-\nformance of our proposed AGT with the state-of-the-art\nmethods. We use mean average precision as the metric to\nevaluate the model. To ensure fair comparison, we use the\nsame evaluation protocol as used by state-of-the-art meth-\nods for each of the datasets. Table 1 shows that the our AGT\nachieves upto 3.5% improvement over state-of-the-art for\nTHUMOS14 dataset and consistently shows performance\nimprovement across all IoU thresholds. Table 2 shows\nthe comparisons with state-of-the-art methods on Charades\ndataset. Our model achieves 13% improvement in the Cha-\nrades dataset. We also perform comparison on recenty re-\nleased EPIC-Kitchens100 dataset for classiﬁcation of verb,\nnoun, and action (i.e. both verb and noun) classes. Table 3\nindicates that our model performs consistently for all three\ntasks for EPIC-Kitchens100 datasets across all IoU thresh-\nolds. Overall, these results clearly show that our proposed\nmethod AGT outperforms the state-of-the-art methods by a\nconsiderable margin.\nImpact of graph based reasoning.To demonstrate the im-\nportance of reasoning over videos as graphs, we conducted\nablation studies by removing the graph based reasoning\ncomponents from either the encoder or the decoder or both\n(i.e. overall transformer network) in our model. Speciﬁ-\ncally, this is implemented by removing the graph message\npassing layers from the attention modules ( i.e., graph self-\nattention module and graph-to-graph attention module) in\nTable 1. Comparison with state-of-the-art (THUMOS14).We\nreport the mean average precision at different intersection over\nunion thresholds (mAP@tIoU) for tIoU∈ {0.1, 0.2, 0.3, 0.4, 0.5}.\n↑ indicates higher is better.\nMethod mAP@tIoU ↑\n0.1 0.2 0.3 0.4 0.5\nOneata et al. [35] 36.6 33.6 27.0 20.8 14.4\nWang et al. [56] 18.2 17.0 14.0 11.7 8.3\nCaba et al. [4] - - - - 13.5\nRichard et al. [41] 39.7 35.7 30.0 23.2 15.2\nShou et al. [44] 47.7 43.5 36.3 28.7 19.0\nYeung et al. [63] 48.9 44.0 36.0 26.4 17.1\nYuan et al. [64] 51.4 42.6 33.6 26.1 18.8\nBuch et al. [3] - - 37.8 - 23.0\nShou et al. [42] - - 40.1 29.4 23.3\nYuan et al. [65] 51.0 45.2 36.5 27.8 17.8\nBuch et al. [2] - - 45.7 - 29.2\nGao et al. [14] 60.1 56.7 50.1 41.3 31.0\nDai et al. [8] - - - 33.3 25.6\nXu et al. [61] 54.5 51.5 44.8 35.6 28.9\nZhao et al. [69] 66.0 59.4 51.9 41.0 29.8\nLin et al. [30] - - 53.5 45.0 36.9\nChao et al. [7] 59.8 57.1 53.2 48.5 42.8\nZeng et al. [66] 69.5 67.8 63.6 57.8 49.1\nXu et al. [62] 66.1 64.2 54.5 47.6 40.2\nAGT (Ours) 72.1 69.8 65.0 58.1 50.2\nTable 2. Comparison with state-of-the-art (Charades).\nWe report mean average precision (mAP) computed using\nCharades v1 localize setting in [45]. ↑: higher is better.\nMethod mAP ↑\nPredictive-corrective (Daveet al. [11]) 8.9\nTwo-stream (Siggurdsonet al. [45]) 8.9\nTwo-stream + LSTM (Siggurdsonet al. [45]) 9.6\nR-C3D (Xuet al. [61]) 12.7\nSSN (Zhaoet al. [69]) 16.4\nI3D baseline [38] 17.2\nSuper-events (Piergiovanniet al. [39]) 19.4\nTGM (Piergiovanniet al. [39]) 22.3\nMavroudiet al. [33] 23.7\n3D ResNet-50 + super-events (Piergiovanniet al. [40]) 25.2\nAGT (Ours) 28.6\nthe encoder and/or decoder blocks in the network. Intu-\nitively, when the graph message passing module is removed\nfrom the whole transformer network, the transformer en-\ncoder treats the input as a sequence and the transformer\ndecoder treats the action queries as independent. Table 4\nshows the performance of these ablated versions of our\nmodel. The results clearly show that eliminating the graph-\nbased reasoning module hurts the localization performance.\nThe results also suggest that graph-based modeling is more\nuseful in the encoder than in the decoder. We believe this\n7\nTable 3. Comparison with state-of-the-art (EPIC-\nKitchens100). We report mean average precision at dif-\nferent intersection over union thresholds (mAP@tIoU) for\ntIoU∈ {0.1, 0.2, 0.3, 0.4, 0.5}. We use the validation split in the\noriginal dataset for testing. ↑ indicates higher is better.\nMethod Task mAP@tIoU↑\n0.1 0.2 0.3 0.4 0.5\nVerb 10.51 9.24 7.67 6.40 5.12\nDamen Noun 10.71 8.73 6.75 5.05 3.35\net al. [10] Action 6.78 6.03 4.94 4.04 3.35\nVerb 12.01 10.25 8.15 7.12 6.14\nAGT Noun 11.63 9.33 7.05 6.57 3.89\n(Ours) Action 7.78 6.92 5.53 4.22 3.86\nTable 4. Ablation Study (Impact of graph based reasoning).\nWe report performance of ablated versions of our AGT model.\nWe report mAP for evaluation performance (higher is better). We\nremove graph reasoning in the encoder ( E) and/or decoder ( D)\nof the transformer. \u0013 and \u0017 indicates whether a component (en-\ncoder or decoder) contains graph message passing module or not\nrespectively. EPIC(A), EPIC (V), EPIC (N) indicates task ‘Ac-\ntion’, ‘Verb’, ‘Noun’ classiﬁcation on EPIC-Kitchens100.\nDataset E: \u0017/ D: \u0017 E: \u0017/ D: \u0013 E: \u0013/ D: \u0017 E: \u0013/ D: \u0013\nTHUMOS14 55.6 56.3 58.3 63.0\nCharades 18.2 19.2 22.5 28.6\nEPIC (A) 3.0 3.3 5.1 5.9\nEPIC (V) 5.7 6.1 7.4 8.7\nEPIC (N) 4.9 5.3 6.3 7.7\nTable 5. Ablation Study (Impact of temporal resolution).Per-\nformance of our AGT for different temporal resolutions of input\nvideo. Here, SR indicates sampling rate of frames for feature ex-\ntraction, i.e., SR=1/k means frames sampled at 1/k -th factor of the\noriginal frame rate. EPIC(A), EPIC (V), EPIC (N) indicate tasks\n‘Action’, ‘Verb’, ‘Noun’ on dataset EPIC-Kitchens100. We report\nmAP for evaluation (higher is better).\nDataset SR = 1/8 SR=1/4\nTHUMOS14 60.2 63.0\nCharades 27.3 28.6\nEPIC (A) 4.1 5.9\nEPIC (V) 7.2 8.7\nEPIC (N) 6.4 7.7\nis because the graph reasoning performed by the encoder is\nmore useful in capturing the non-sequential dependencies\nas it operates directly on the video features. For better read-\nability, here, we provide the mAP values averaged over the\nvarious intersection-over-union thresholds (tIoU) for THU-\nMOS14 and EPIC-Kitchens100. For mAP values at speciﬁc\nthresholds, refer to the supplementary.\nImpact of temporal resolution.To evaluate the impact of\ntemporal resolution, we experimented with different frame\nrates for the input video. Table 5 shows the results sug-\ngesting higher resolution leads to better performance as the\nhigher temporal resolution provides more information in the\ninput. However, our results also show that lower resolution\nground truth\npredicted\ncricket bowling cricket shot\nground truth\npredicted\nholding a cup/glass/bottle\ndrinking from a cup/glass/bottle\nputting a cup/glass/bottle somewhere\ntaking a cup/glass/bottle from somewhere\nputting something on a shelf\nFigure 3. Qualitative Results. Visualization of ground truth and\npredicted action instances.\ndoes not lead to any major drop in performance. For bet-\nter readability, here, we provide the mAP values averaged\nover the various intersection-over-union thresholds (tIoU)\nfor THUMOS14 and EPIC-Kitchens100. mAP values at\nspeciﬁc thresholds are available in the supplementary.\nQualitative Results. We visualize the predictions of the\nmodel on two different samples in Figure 3. The visualiza-\ntions indicate that our model is able to predict the correct\nnumber of action instances as well as correct action cate-\ngories with minimal errors in start and end timestamps. We\nbelieve this is because video content around the start and\nend timestamps in some instances do not contain enough\ninformation pertaining to the action. We provide additional\nvisualizations of predictions in the supplementary.\nAdditionally, refer to the supplementary for experiments\non performance of our model with varied number of layers\nand heads in the transformer and ablations of loss functions.\n5. Conclusion\nIn this paper, we proposed a novel end-to-end learnable\nencoder-decoder transformer model for the task of tempo-\nral action localization in untrimmed human activity videos.\nOur approach aims to model the non-linear temporal struc-\nture in such videos by reasoning over the videos as graphs\nusing graph self-attention mechanisms. The experimental\nevaluation showed that our model achieves state-of-the-art\nperformance on the task of temporal action localization on\nchallenging human activity datasets. Overall, this work\nhighlights the importance of reasoning over videos as non-\nsequential entities and shows that graph-based transformers\nare an effective means to model complex activity videos.\n8\nReferences\n[1] Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang,\nQiyue Liu, and Junhui Liu. Boundary content graph neu-\nral network for temporal action proposal generation. 2020.\n2\n[2] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-\nFei, and Juan Carlos Niebles. End-to-end, single-stream tem-\nporal action detection in untrimmed videos. In Proceedings\nof the British Machine Vision Conference (BMVC), 2017. 7\n[3] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard\nGhanem, and Juan Carlos Niebles. Sst: Single-stream tem-\nporal action proposals. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (CVPR), 2017. 1, 2,\n7\n[4] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard\nGhanem. Fast temporal activity proposals for efﬁcient detec-\ntion of human actions in untrimmed videos. In Proceedings\nof the IEEE International Conference on Computer Vision\n(CVPR), 2016. 1, 2, 7\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. InProceedings of the\nEuropean Conference on Computer Vision (ECCV), 2020. 3,\n5, 6\n[6] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 3, 4, 5, 17\n[7] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Sey-\nbold, David A Ross, Jia Deng, and Rahul Sukthankar. Re-\nthinking the faster r-cnn architecture for temporal action lo-\ncalization. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2018. 1, 2, 6,\n7\n[8] Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S Davis, and\nYan Qiu Chen. Temporal context network for activity local-\nization in videos. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2017. 1, 2, 7\n[9] Navneet Dalal, Bill Triggs, and Cordelia Schmid. Human\ndetection using oriented histograms of ﬂow and appearance.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), 2006. 3\n[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\n, Antonino Furnari, Jian Ma, Evangelos Kazakos, Da-\nvide Moltisanti, Jonathan Munro, Toby Perrett, Will Price,\nand Michael Wray. Rescaling egocentric vision. CoRR,\nabs/2006.13256, 2020. 2, 7, 8\n[11] Achal Dave, Olga Russakovsky, and Deva Ramanan.\nPredictive-corrective networks for action detection. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 1, 2, 7\n[12] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles,\nand Bernard Ghanem. Daps: Deep action proposals for ac-\ntion understanding. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), 2016. 1, 2\n[13] Adrien Gaidon, Zaid Harchaoui, and Cordelia Schmid. Tem-\nporal localization of actions with actoms. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence (TPAMI),\n2013. 2\n[14] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram\nNevatia. Turn tap: Temporal unit regression network for tem-\nporal action proposals. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (ICCV), 2017. 1, 2,\n7\n[15] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 3\n[16] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (ICCV), 2015.\n2\n[17] Georgia Gkioxari and Jitendra Malik. Finding action tubes.\nIn Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 3\n[18] Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia, and\nBernard Ghanem. Scc: Semantic context cascade for efﬁ-\ncient action detection. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (CVPR), 2017. 1, 2\n[19] Noureldien Hussein, Efstratios Gavves, and Arnold WM\nSmeulders. Videograph: Recognizing minutes-long human\nactivities in videos. Proceedings of the ICCV Workshop on\nScene Graph Representation and Learning (ICCV-W), 2019.\n3\n[20] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh\nSaxena. Structural-rnn: Deep learning on spatio-temporal\ngraphs. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016. 3\n[21] Mihir Jain, Amir Ghodrati, and Cees GM Snoek. Action-\nbytes: Learning from trimmed videos to localize actions.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020. 3\n[22] Mihir Jain, Jan Van Gemert, Herv´e J´egou, Patrick Bouthemy,\nand Cees GM Snoek. Action localization with tubelets from\nmotion. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2014. 2\n[23] Y .-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev,\nM. Shah, and R. Sukthankar. THUMOS challenge: Ac-\ntion recognition with a large number of classes. http:\n//crcv.ucf.edu/THUMOS14/, 2014. 2, 7\n[24] Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari,\nand Cordelia Schmid. Action tubelet detector for spatio-\ntemporal action localization. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), 2017.\n3\n[25] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video\nclassiﬁcation with convolutional neural networks. In Pro-\nceedings of the IEEE conference on Computer Vision and\nPattern Recognition (CVPR), 2014. 1, 2\n[26] Thomas N. Kipf and Max Welling. Semi-supervised classi-\nﬁcation with graph convolutional networks. In International\nConference on Learning Representations (ICLR), 2017. 3\n[27] Ivan Laptev. On space-time interest points. International\nJournal of Computer Vision(IJCV), 2005. 3\n9\n[28] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter,\nand Gregory D Hager. Temporal convolutional networks for\naction segmentation and detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 1, 2\n[29] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.\nBmn: Boundary-matching network for temporal action pro-\nposal generation. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2019. 2\n[30] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and\nMing Yang. Bsn: Boundary sensitive network for temporal\naction proposal generation. In Proceedings of the European\nConference on Computer Vision (ECCV), 2018. 2, 7\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. 2017. 18\n[32] Shugao Ma, Leonid Sigal, and Stan Sclaroff. Learning activ-\nity progression in lstms for activity detection and early detec-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016. 1, 2\n[33] Effrosyni Mavroudi, Benjam ´ın B´ejar Haro, and Ren ´e Vidal.\nRepresentation learning on visual-symbolic graphs for video\nunderstanding. In Proceedings of the European Conference\non Computer Vision (ECCV), 2020. 7\n[34] Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, and\nKristen Grauman. Ego-topo: Environment affordances from\negocentric video. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020. 3\n[35] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. Action\nand event recognition with ﬁsher vectors on a compact fea-\nture set. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), 2013. 2, 7\n[36] Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee,\nAdrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles.\nSpatio-temporal graph for video captioning with knowledge\ndistillation. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2020. 3\n[37] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. Proceedings of the International Confer-\nence on Machine Learning (ICML), 2018. 4\n[38] AJ Piergiovanni and Michael Ryoo. Temporal gaussian mix-\nture layer for videos. In Proceedings of the International\nConference on Machine Learning (ICML), 2019. 7\n[39] AJ Piergiovanni and Michael S Ryoo. Learning latent super-\nevents to detect multiple activities in videos. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. 1, 2, 7\n[40] AJ Piergiovanni and Michael S Ryoo. Avid dataset:\nAnonymized videos from diverse countries. In Advances in\nNeural Information Processing Systems (NeurIPS), 2020. 7\n[41] Alexander Richard and Juergen Gall. Temporal action detec-\ntion using a statistical language model. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2016. 1, 2, 7\n[42] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki\nMiyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-\nconvolutional networks for precise temporal action localiza-\ntion in untrimmed videos. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision (CVPR), 2017. 1,\n2, 7\n[43] Zheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa,\nand Shih-Fu Chang. Autoloc: Weakly-supervised tempo-\nral action localization in untrimmed videos. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n2018. 3\n[44] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Tempo-\nral action localization in untrimmed videos via multi-stage\ncnns. In Proceedings of the IEEE International Conference\non Computer Vision (CVPR), 2016. 1, 2, 7\n[45] Gunnar A Sigurdsson, G ¨ul Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding. In Proceedings of the European Conference on\nComputer Vision (ECCV), 2016. 2, 7\n[46] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In Ad-\nvances in Neural Information Processing Systems (NIPS),\n2014. 3\n[47] Bharat Singh, Tim K Marks, Michael Jones, Oncel Tuzel,\nand Ming Shao. A multi-stream bi-directional recurrent neu-\nral network for ﬁne-grained action detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 1, 2\n[48] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS\nTorr, and Fabio Cuzzolin. Online real-time multiple spa-\ntiotemporal action localisation and prediction. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion (ICCV), 2017. 3\n[49] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng.\nEnd-to-end people detection in crowded scenes. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 6\n[50] Kevin Tang, Bangpeng Yao, Li Fei-Fei, and Daphne Koller.\nCombining the right features for complex event recogni-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 2013. 2\n[51] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torre-\nsani, and Manohar Paluri. Learning spatiotemporal fea-\ntures with 3d convolutional networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2015. 3\n[52] Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe\nMorency, Ruslan Salakhutdinov, and Ali Farhadi. Video\nrelationship reasoning using gated spatio-temporal energy\ngraph. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019. 3\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NIPS), 2017. 5, 6, 17\n[54] Petar Veli ˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. In Proceedings of the International Con-\nference on Learning Representations (ICLR), 2018. 4\n10\n[55] Heng Wang and Cordelia Schmid. Action recognition with\nimproved trajectories. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2013. 3\n[56] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition\nand detection by combining motion and appearance features.\nTHUMOS14 Action Recognition Challenge, 2014. 7\n[57] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recogni-\ntion with trajectory-pooled deep-convolutional descriptors.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 3\n[58] Limin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool.\nUntrimmednets for weakly supervised action recognition\nand detection. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition (CVPR), 2017. 3\n[59] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2016. 3\n[60] Xiaolong Wang and Abhinav Gupta. Videos as space-time\nregion graphs. In Proceedings of the European Conference\non Computer Vision (ECCV), 2018. 3\n[61] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region con-\nvolutional 3d network for temporal activity detection. InPro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 1, 2, 7\n[62] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and\nBernard Ghanem. G-tad: Sub-graph localization for tempo-\nral action detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2020. 2, 7\n[63] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-\nFei. End-to-end learning of action detection from frame\nglimpses in videos. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 2678–\n2687, 2016. 1, 2, 7\n[64] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A Kas-\nsim. Temporal action localization with pyramid of score dis-\ntribution features. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016. 2,\n7\n[65] Zehuan Yuan, Jonathan C Stroud, Tong Lu, and Jia Deng.\nTemporal action localization by structured maximal sums.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 1, 2, 7\n[66] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong,\nPeilin Zhao, Junzhou Huang, and Chuang Gan. Graph con-\nvolutional networks for temporal action localization. In Pro-\nceedings of the IEEE International Conference on Computer\nVision (ICCV), 2019. 1, 2, 7\n[67] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli\nWang. Real-time action recognition with enhanced motion\nvector cnns. InProceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 3\n[68] Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang,\nand Qi Tian. Bottom-up temporal action localization with\nmutual regularization. In Proceedings of the European Con-\nference on Computer Vision (ECCV), 2020. 1, 2\n[69] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-\naoou Tang, and Dahua Lin. Temporal action detection with\nstructured segment networks. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (CVPR), 2017.\n1, 2, 7\n11\nA. Appendix\nWe report additional quantitative results and qualitative\nanalysis and provide implementation details of our model.\nSpeciﬁcally, this document contains the following.\n• Code provided in the folder agt code.zip\n• Additional quantitative evaluation\n– Section A.1.1: Supplemental tables for Table 4\nand Table 5 from the main paper to report mAP\nvalues at speciﬁc IoU thresholds\n– Section A.1.2: Ablation study of loss function\n(Eq. 9 in the main paper).\n– Section A.1.3: Impact of different number of lay-\ners in transformer encoder and decoder.\n– Section A.1.4: Impact of different number of\nheads in attention modules of the transformer.\n– Section A.1.5: Impact of different number of\nnodes in the action query graph.\n• Additional qualitative analysis\n– Section A.2.1: Visualization of predictions\n– Section A.2.2: Visualization of graphs learned by\nthe model\n– Section A.2.3: Analysis of AGT predictions\nbased on the duration of action instances\n• Technical details\n– Section A.3.1: Details of the architecture of AGT\n– Section A.3.2: Details of initialization, data aug-\nmentation, and hyperparameters.\nA.1. Additional Quantitative Evaluation\nIn this section, we report the quantitative evaluation of\nour proposed AGT model to supplement the quantitative\nevaluation in the main paper.\nA.1.1 Supplemental Tables\nIn the main paper, we only reported the mAP averaged\nover different IoU thresholds for THUMOS14 and EPIC-\nKitchens100 dataset (Table 4 and Table 5 in main paper).\nFor completeness, we report mAP at speciﬁc IoU thresh-\nolds in Table T1 and Table T2.\nA.1.2 Ablation Study: Loss function\nNote that for any version of the loss function, the model re-\nquires cross entropy loss to be able to classify the action la-\nbel pertaining to an instance. The model also requires some\nform of regression loss to produce predictions pertaining to\nthe start and end timestamps of an action instance. Recall,\nTable T1. Supplemental Tables: Impact of graph based rea-\nsoning. We report performance of ablated versions of our AGT\nmodel. We remove graph reasoning in the encoder (E) and/or de-\ncoder (D) of the transformer. \u0013 and \u0017 indicates whether a compo-\nnent (encoder or decoder) contains graph message passing mod-\nule or not respectively. EPIC (A), EPIC (V), EPIC (N) indicates\ntask ‘Action’, ‘Verb’, ‘Noun’ classiﬁcation on EPIC-Kitchens100.\nWe report the mean average precision at different intersection over\nunion thresholds (mAP@tIoU) for tIoU∈ {0.1, 0.2, 0.3, 0.4, 0.5}.\n↑ indicates higher is better.\nDataset Model mAP@tIoU↑\n0.1 0.2 0.3 0.4 0.5\nE: \u0017/ D: \u0017 64.6 60.8 59.1 51.2 40.3\nTHUMOS14 E: \u0017/ D: \u0013 65.1 62.4 60.3 52.4 41.3\nE: \u0013/ D: \u0017 67.1 64.4 62.5 53.6 44.9\nE: \u0013/ D: \u0013 72.1 69.8 65.0 58.1 50.2\nE: \u0017/ D: \u0017 9.4 6.9 5.2 4.5 2.5\nEPIC (V) E: \u0017/ D: \u0013 9.9 7.5 5.5 4.9 2.7\nE: \u0013/ D: \u0017 11.4 9.0 6.9 6.3 3.4\nE: \u0013/ D: \u0013 12.0 10.3 8.2 7.1 6.1\nE: \u0017/ D: \u0017 8.9 5.4 4.9 3.6 1.7\nEPIC (N) E: \u0017/ D: \u0013 9.2 6.0 5.1 4.2 2.0\nE: \u0013/ D: \u0017 10.1 8.0 6.8 5.2 2.3\nE: \u0013/ D: \u0013 11.6 9.3 7.1 6.6 3.9\nE: \u0017/ D: \u0017 4.8 4.1 2.9 2.1 1.5\nEPIC (A) E: \u0017/ D: \u0013 5.1 4.3 3.2 2.3 1.8\nE: \u0013/ D: \u0017 7.3 6.1 5.0 3.9 3.7\nE: \u0013/ D: \u0013 7.8 6.9 5.5 4.2 3.9\nTable T2. Supplemental Tables: Impact of temporal resolu-\ntion. Performance of our AGT for different temporal resolutions\nof input video. Here, SR indicates sampling rate of frames for\nfeature extraction, i.e., SR=1/k means frames sampled at 1/ k -th\nfactor of the original frame rate. EPIC (A), EPIC (V), EPIC (N)\nindicate tasks ‘Action’, ‘Verb’, ‘Noun’ classiﬁcation on dataset\nEPIC-Kitchens100. We report the mean average precision at dif-\nferent intersection over union thresholds (mAP@tIoU) for tIoU ∈\n{0.1, 0.2, 0.3, 0.4, 0.5}. ↑ indicates higher is better.\nDataset Model mAP@tIoU↑\n0.1 0.2 0.3 0.4 0.5\nTHUMOS14 SR=1/8 70.4 65.8 62.3 54.1 48.6\nSR=1/4 72.1 69.8 65.0 58.1 50.2\nEPIC (V) SR=1/8 10.9 8.5 6.3 5.5 4.9\nSR=1/4 12.0 10.3 8.2 7.1 6.1\nEPIC (N) SR=1/8 10.2 8.1 5.9 5.2 2.8\nSR=1/4 11.6 9.3 7.1 6.6 3.9\nEPIC (A) SR=1/8 7.0 5.0 4.1 2.5 1.9\nSR=1/4 7.8 6.9 5.5 4.2 3.9\nour overall loss (see Eq. (9) in the main paper) is a com-\nbination of cross-entropy loss and regression loss, i.e., seg-\nment loss Ls. The segment loss contains two components:\nL1 loss and IoU loss Liou. Table T3 shows the results of\nthe performance of our model when trained with ablated\n12\nTable T3.Ablation Study: Loss function.We report performance\nof our AGT model when trained with ablated versions of the loss\nfunction. We report mAP for evaluation performance (higher is\nbetter). We train the model with a combination of cross-entropy\nloss and segment loss containing L1 loss and/or IoU loss Liou.\n\u0013 and \u0017 indicate whether the speciﬁc component of the segment\nloss is used or not respectively. EPIC(A), EPIC (V), EPIC (N)\nindicate tasks ‘Action’, ‘Verb’, ‘Noun’ classiﬁcation on EPIC-\nKitchens100.\nDataset L1: \u0013 L1: \u0017 L1: \u0013\nLiou: \u0017 Liou: \u0013 Liou: \u0013\nTHUMOS14 61.3 59.6 63.0\nCharades 26.0 25.3 28.6\nEPIC (A) 4.8 3.7 5.9\nEPIC (V) 7.1 6.4 8.7\nEPIC (N) 6.3 5.0 7.7\nversions of the segment loss. The results indicate that the\nmodels trained with only L1 loss perform better than the\nones trained with only IoU loss Liou. Additionally, mod-\nels trained with both losses are better than the ones trained\nwith only one of the losses. Nonetheless, all versions of our\nAGT model perform better than state-of-the-art methods.\nWe only provide the mAP values averaged over the various\nintersection-over-union thresholds (tIoU) for THUMOS14\nand EPIC-Kitchens100.\nA.1.3 Impact of number of layers\nTable T4 shows the results of the performance of our model\nwith different number of layers in encoder and decoder\ncomponent of the transformer. While increase in number\nof layers increases the training time, we did not observe\nmuch difference in the performance of the model with in-\ncreased depth of the transformer components. We only pro-\nvide the mAP values averaged over the various intersection-\nover-union thresholds (tIoU) for THUMOS14 and EPIC-\nKitchens100.\nA.1.4 Impact of number of heads\nTable T5 shows the results of the performance of our AGT\nmodel with different number of heads in the attention mod-\nules of the transformer. The results suggest a slight im-\nprovement with more number of heads in the transformer\nnetwork. We only provide the mAP values averaged over\nthe various intersection-over-union thresholds (tIoU) for\nTHUMOS14 and EPIC-Kitchens100.\nA.1.5 Impact of action query graph size\nTable T6 shows the results of the performance of our AGT\nmodel with different number of node encodings in the ac-\ntion query graph. Intuitively, a very large size of action\nTable T4. Impact of number of layers.We report performance\nof our AGT model with different number of layers in encoder and\ndecoder. We report mAP for evaluation performance (higher is\nbetter). EPIC (A), EPIC (V), EPIC (N) indicates task ‘Action’,\n‘Verb’, ‘Noun’ classiﬁcation on EPIC-Kitchens100. #E indicates\nnumber of layers in encoder and #D indicates number of layers in\ndecoder.\nDataset #E #D mAP\n4 2 62.5\nTHUMOS14 4 4 63.0\n2 4 62.7\n4 2 28.0\nCharades 4 4 28.6\n2 4 28.2\n4 2 5.5\nEPIC (A) 4 4 5.9\n2 4 5.6\n4 2 8.5\nEPIC (V) 4 4 8.7\n2 4 8.6\n4 2 7.2\nEPIC (N) 4 4 7.7\n2 4 7.3\nTable T5. Impact of number of heads.We report performance\nof our AGT model with different number of heads in the attention\nmodules of the transformer network. We report mAP for evalu-\nation performance (higher is better). EPIC (A), EPIC (V), EPIC\n(N) indicates task ‘Action’, ‘Verb’, ‘Noun’ classiﬁcation on EPIC-\nKitchens100. #heads indicates number of heads in attention mod-\nules of the transformer.\nDataset #heads mAP\nTHUMOS14 8 63.0\n4 61.4\nCharades 8 28.6\n4 26.4\nEPIC (A) 8 5.9\n4 5.2\nEPIC (V) 8 8.7\n4 8.4\nEPIC (N) 8 7.7\n4 7.1\nquery graph implies the model will require more time to\nlearn the non-maximal suppression of the irrelevant predic-\ntions. On the other hand, a very small size of action query\ngraph might limit the ability of model to learn complex\nstructure in the action instances. Note that, any value used\nfor our experiments is higher than the maximum number of\naction instances per video in the dataset. The results suggest\n13\nTable T6. Impact of action query graph size.We report per-\nformance of our AGT model with different number of nodes in\nthe action query graph. We report mAP for evaluation perfor-\nmance (higher is better). EPIC (A), EPIC (V), EPIC (N) indicates\ntask ‘Action’, ‘Verb’, ‘Noun’ classiﬁcation on EPIC-Kitchens100.\n#queries indicates number of nodes in the action query graph.\nDataset #queries mAP\n150 59.1\nTHUMOS14 300 63.0\n900 63.2\n30 24.0\nCharades 50 28.6\n100 28.6\n900 4.3\nEPIC (A) 1500 5.9\n2000 7.0\n900 7.0\nEPIC (V) 1500 8.7\n2000 8.8\n900 6.1\nEPIC (N) 1500 7.7\n2000 7.9\nminor improvement with more number of nodes in the ac-\ntion query graph, however, the models with more number of\nnodes require longer training times. Our experiments also\nsuggest that when the size of the action query graph is re-\nduced, the localization performance of our model degrades.\nWe only provide the mAP values averaged over the various\nintersection-over-union thresholds (tIoU) for THUMOS14\nand EPIC-Kitchens100.\nA.2. Additional Qualitative Analysis\nIn this section, we visualize the results of our proposed\nmodel AGT to supplement the qualitative analysis in the\nmain paper.\nA.2.1 Visualization: Predictions\nWe provide additional visualizations of the predictions of\nour AGT on several diverse samples in Figure F1. The vi-\nsualizations indicate that our model is able to predict the\ncorrect number of action instances as well as most of the\ncorrect action categories with minimal errors in start and\nend timestamps for videos containing overlapping instances\nwith varying temporal extents.\nA.2.2 Visualization: Learned Graphs\nWe visualize the learned action query graph in Figure F2.\nby observing the graph embeddings obtained from the last\nlayer of decoder. For better visibility, we do not plot the\nnodes (or their edges) that are classiﬁed as no action ( i.e.\nclass label ∅) by the prediction head. Note that the edge\nmatrix is also learnable in our model. For the purpose of this\nvisualization, we obtained the edge weights from the atten-\ntion coefﬁcients in the self-attention based graph message\npassing module . We show samples with reoccurring and/or\noverlapping action instances. The visualizations demon-\nstrate that the model indeed learns non-linear dependencies\namong the action instances that appear in the video.\nA.2.3 Analysis: Effect of Action Instance Durations\nWe conduct further analysis to study the performance of our\nmodel in terms of the durations of the action instances. Fig-\nure F3 shows the trend of segmentation error, i.e., L1 norm\ncomputed between the ground truth and predicted times-\ntamps of actions instances plotted against the duration of the\nground truth instances (normalized with respect to the video\nduration). The error is computed over normalized values of\nthe timestamps. This analysis indicates that action instances\nwith larger durations (with respect to the whole video dura-\ntion) have lower segmentation errors in their predictions as\ncompared to the instances with smaller durations.\nA.3. Technical Details\nIn this section, we provide additional implementation de-\ntails to supplement the model section in the main paper.\nA.3.1 Additional details\nDetailed Architecture.Figure F4 presents the architecture\nof our AGT in detail. Activity Graph Transformer (AGT)\nconsists of three components: (1) backbone network to ob-\ntain features corresponding to the input video; (2) trans-\nformer network consisting of an encoder network and a de-\ncoder network that operates over graphs; and (3) prediction\nheads for the ﬁnal prediction of action instances of the form\n(label, start time, end time). The encoder network receives\nthe compact video-level representation from the backbone\nnetwork and encodes it to a latent graph representation, re-\nferred to as context graph. The decoder network receives\ngraph-structured abstract query encodings (referred to as\naction query graph) as input along with the context graph.\nThe decoder uses the context graph to transform the action\nquery graph to a graph-structured set of embeddings. Each\nnode embedding of this decoder output is fed into a pre-\ndiction head to obtain predictions of action instances. The\nwhole AGT network is trained end-to-end using a combi-\nnation of classiﬁcation and regression losses for the action\nlabels and timestamps respectively.\nPositional Encoding. Positional encoding layer consists\nof a layer that retrieves encodings based on an integer in-\n14\nFigure F1. Visualization: Predictions. Visualization of predictions and groundtruth action instances\n15\nFigure F2. Visualization: Learned Graphs. Visualizations of embeddings corresponding to the last layer of the decoder and ground\ntruth instances. The thickness of edges show the strength of interaction between the nodes. For ease of visibility, the nodes have been\nnumbered based on the order of their predictions sorted with respect to the start time ( i.e., node 0 represents the instance that starts ﬁrst).\nThese visualizations demonstrate that the model indeed learns non-linear dependencies between the action instances in a video. The legend\nbelow each ﬁgure shows the action labels corresponding to the color coded elements. For details on the visualization process, please refer\nto Section A.2.2\n.\nFigure F3. Analysis (THUMOS14).Analysis of segmentation er-\nror (L1 loss) with respect to the duration of corresponding ground\ntruth instances. All the values are normalized with respect to the\noverall video duration. We observe that the action instances of\nlonger durations have lower segmentation errors in their predic-\ntions.\ndex provided to it. In our case, given a video feature\nv = [v(1),v(2) ... v(Nv)], the positional encoding layer re-\nceives input iand provides an embedding p(i)\nv correspond-\ning to the i-th element of the video feature v(i) where\ni ∈1,2,...,N v. In our implementation, the embedding\nsize is same as that of the video feature so as to allow\naddition of the positional encodings and input video fea-\ntures. Since the weights of the layer are learnable during\ntraining, the positional encoding layer is learnable. We use\ntorch.nn.Embedding in Pytorch to implement it. This\nlayer initialization requires maximum possible value of Nv\nin the features corresponding to the video.\nAction Query Graph.Similar to positional encoding layer,\nthe No encodings in the action query graph q is obtained\nusing an embedding layer. Speciﬁcally, the layer receives\ni as input to provide i-th node q(i) of the query graph\nwhere i ∈ 1,2,...,N o. In our implementation, we use\ntorch.nn.Embedding in Pytorch to implement this.\nThe weights of this layer are learnable during training.\nLosses. For completeness, we describe the IoU loss ( Liou)\nwhich is used as a component of segment loss Ls to train\nour model. The segment loss is described as:\nLs = λiouLiou(s(i),˜s(φ(i))) +λL1||s(i) −˜s(φ(i))||1, (10)\nwhere λiou,λL1 ∈R are hyperparameters.\nLiou(s(i),˜s(φ(i))) = 1 −|s(i) ∩˜s(φ(i))|\n|s(i) ∪˜s(φ(i))| (11)\nwhere |.|is the duration of the instance, i.e., difference be-\ntween end and start timestamp.\nA.3.2 Training Details\nFeature Extraction & Data augmentation.To obtain I3D\nfeatures corresponding to an input video V containing T\nframes sampled at a speciﬁc sample rate, we ﬁrst divide the\nvideo into short overlapping segments of 8 frames with an\noverlap of 4 frames resulting in T′chunks. We use an I3D\n16\nMulti-head graph message passing\nVideo features\nLinear\n+\nAdd and Normalize\nMulti-head self attention\nFFN\nV K Q\nAdd and Normalize\nEncoder\nMulti-head graph message passing\nLinear\nAdd and Normalize\nMulti-head self attention\nGraph-to-graph message passing\nV K Q\nAdd and Normalize\nAction query graph\nMulti-head self attention\nAdd and Normalize\nAdd and Normalize\nFFN\nAdd and Normalize\nAdd and Normalize\nV\n+\nQK\nDecoder\nPrediction \nHead\nlabel,\ntimestamps\nlabel, \ntimestamps\nlabel, \ntimestamps\nno \naction\nLinear\nPrediction \nHead\nPrediction \nHead\nPrediction \nHead\nPositional encodings\nFigure F4. Detailed ArchitectureArchitecture of Activity Graph Transformer. Please see Section A.3.1 for details. ‘Q’,‘K’,‘V’ are query,\nkey and value to the self-attention layer as described in [53].\nmodel pretrained on the Kinetics [6] dataset to extract fea-\ntures of dimension C (= 2048). In our implementation, we\nobtain two-stream features (both RGB and ﬂow streams).\nWe obtain features for these T′chunks to obtain a tensor of\nsize T′×2048. Here, the length of the video T depends on\nthe duration of the video, and, hence the size of the tempo-\nral channel (i.e. T′) of the feature tensor varies based on the\ninput.\nTo prevent severe overﬁtting, we perform data augmen-\ntation to train our model on the features directly obtained\nfrom I3D model (described above). We use a hyperparame-\nter Nmax\nv as the maximum size of temporal channel used\nfor training. This helps in stabilizing the training as the\nvideo datasets contain high variance in their duration. If\nthe size of temporal channel of the video tensor T′is less\nthan Nmax\nv , we repeat each element in the temporal channel\nγtimes (γ = 4) in our implementation to obtain a modiﬁed\ntensor of size γT′×2048 and then randomly sample T′\nelements from the modiﬁed tensor. If the size of tempo-\nral channel of the video tensor T′is more than Nmax\nv , we\n17\njust randomly sample T′elements from the modiﬁed tensor.\nNote that, positional encoding is applied on this feature of\nsize Nv = min(T′,Nmax\nv ).\nWe ﬁnd such data augmentation during training to be\ncrucial to prevent overﬁtting and obtain good performance\nof our model, especially for smaller datasets such as THU-\nMOS14. During testing, if the size of temporal channel of\nthe video tensor T′ is less than Nmax\nv , we don’t perform\nany augmentation. If the size of temporal channel of the\nvideo tensor T′is more than Nmax\nv , we uniformly sample\nT′ elements from the feature in order to match the maxi-\nmum index of the positional encoding layer.\nFurthermore, to perform training in minibatches, we ap-\nply 0-padding to ensure all elements have the same size as\nthe largest element of the batch. For training efﬁciency and\nminimizing the amount of 0-padding, we sort all the dataset\nbased on the duration of the video. We observe that this type\nof batch formation leads to improvement in training speed\nwithout affecting the model performance.\nHyperparameters. We provide the hyperparameters used\nto train our model below.\nWe train all our models using AdamW optimizer [31]\nwith a learning rate of 1e-5 and a weight decay of 1e-5 for\n3000k steps. We reduce the learning rate by factor of 10\nafter 2000k steps. The hyperparameters in the loss func-\ntions λL1 and λiou are set to 5 and 3 respectively for all our\nexperiments. All the learnable weights are initialized using\nXavier initialization.\nFor our experiments, we sample frames at 1/4 of the orig-\ninal frame rate and obtain the I3D features as decribed ear-\nlier. We do not ﬁnetune the I3D model.\nWe mention the dataset speciﬁc hyperparameters below.\nTHUMOS14. We do not use dropout for this dataset. We\nuse maximum number of nodes in the context graph Nmax\nv\nequal to 256. The size of the action query graph is 300 for\nour experiments (except when conducting ablation on the\nsize of action query graph). We use base model dimension\nin the transformer as 512 and set the number of encoder and\ndecoder layers as 4 (except when conducting ablation on the\nnumber of layers).\nCharades. We use dropout with default probability0.1.\nWe use maximum number of nodes in the context graph\nNmax\nv equal to 64. The size of the action query graph is\n100 for our experiments (except when conducting ablation\non the size of action query graph). We use base model di-\nmension in the transformer as 512 and set the number of\nencoder and decoder layers as 4 (except when conducting\nablation on the number of layers).\nEpic-Kitchens100. We do not use dropout for this\ndataset. We use maximum number of nodes in the context\ngraph Nmax\nv equal to 1024. The size of the action query\ngraph is 1200 for our experiments (except when conduct-\ning ablation on the size of action query graph). We use base\nmodel dimension in the transformer as 512 and set the num-\nber of encoder and decoder layers as 4 (except when con-\nducting ablation on the number of layers).\n18"
}