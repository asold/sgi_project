{
  "title": "Knowledge-Augmented Language Model Verification",
  "url": "https://openalex.org/W4389519208",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5013449404",
      "name": "Jinheon Baek",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088628107",
      "name": "Soyeong Jeong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018777306",
      "name": "Minki Kang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100641120",
      "name": "Jong Cheol Park",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5109493035",
      "name": "Sung Hwang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4322718421",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4312091845",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4366736258",
    "https://openalex.org/W3204877056",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4379933424",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4376632757",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4376312546",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W4224863259",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4302306557",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4385574298",
    "https://openalex.org/W4367628123",
    "https://openalex.org/W4308761000",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3173169192",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4226082499"
  ],
  "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1720–1736\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nKnowledge-Augmented Language Model Verification\nJinheon Baek Soyeong Jeong Minki Kang Jong C. Park Sung Ju Hwang\nKAIST\n{jinheon.baek, starsuzi, zzxc1133, jongpark, sjhwang82}@kaist.ac.kr\nAbstract\nRecent Language Models (LMs) have shown\nimpressive capabilities in generating texts with\nthe knowledge internalized in parameters. Yet,\nLMs often generate the factually incorrect re-\nsponses to the given queries, since their knowl-\nedge may be inaccurate, incomplete, and out-\ndated. To address this problem, previous works\npropose to augment LMs with the knowledge\nretrieved from an external knowledge source.\nHowever, such approaches often show subop-\ntimal text generation performance due to two\nreasons: 1) the model may fail to retrieve the\nknowledge relevant to the given query, or 2) the\nmodel may not faithfully reflect the retrieved\nknowledge in the generated text. To overcome\nthese, we propose to verify the output and the\nknowledge of the knowledge-augmented LMs\nwith a separate verifier, which is a small LM\nthat is trained to detect those two types of errors\nthrough instruction-finetuning. Then, when the\nverifier recognizes an error, we can rectify it\nby either retrieving new knowledge or gener-\nating new text. Further, we use an ensemble\nof the outputs from different instructions with\na single verifier to enhance the reliability of\nthe verification processes. We validate the ef-\nfectiveness of the proposed verification steps\non multiple question answering benchmarks,\nwhose results show that the proposed verifier\neffectively identifies retrieval and generation\nerrors, allowing LMs to provide more factu-\nally correct outputs. Our code is available at\nhttps://github.com/JinheonBaek/KALMV .\n1 Introduction\nRecent Language Models (LMs) (Brown et al.,\n2020; Chowdhery et al., 2022; Chung et al., 2022),\nwhich have a large number of parameters and are\nfurther instruction-finetuned on massive datasets,\nhave achieved remarkable successes on various lan-\nguage tasks. For example, they are able to perform\nclosed-book zero-shot question answering, which\naims to provide an answer to a user’s query without\nupdating the LM parameters while using only the\nknowledge internalized in their parameters. How-\never, while the generated answers from LMs look\nplausible and sound, they are often factually incor-\nrect, which is a problem widely known as halluci-\nnation (Rohrbach et al., 2018; Bang et al., 2023;\nZheng et al., 2023). Hallucination is a critical prob-\nlem when deploying LMs, since it poses a risk of\nspreading misinformation, potentially misleading\nusers who rely on the information.\nTo mitigate hallucination of LMs, recent works\nhave proposed to augment LMs with the knowledge\nretrieved from external knowledge sources (e.g.,\nWikipedia and Wikidata) (Lazaridou et al., 2022;\nMallen et al., 2023; Baek et al., 2023). Moreover,\nsome other works have proposed to check the factu-\nality of generated texts and refine them by using the\nknowledge in LMs themselves or from the external\nknowledge sources (Madaan et al., 2023; Gao et al.,\n2023; Jiang et al., 2023; Gou et al., 2023; Xu et al.,\n2023; Feng et al., 2023). However, while the afore-\nmentioned knowledge-augmentation strategies are\neffective in reducing hallucinations, we find that\nthere still exists a couple of challenges: 1) the re-\ntrieved knowledge may not be relevant to the given\nquestion from the user, and 2) the generated answer\nmay not be grounded in the retrieved knowledge,\nas illustrated in Figure 1 and shown in Figure 2.\nIn this work, we aim to overcome these subop-\ntimalities of knowledge-augmented LMs. In other\nwords, our goal is to verify whether the retrieved\nknowledge used for augmenting LMs is related to\ngenerating the answers for the given questions and\nwhether the generated answers include the relevant\nparts of the retrieved knowledge. To this end, we\npropose to train a small, tailorable LM that is able\nto verify the aforementioned two failure cases of\nknowledge-augmented LMs in retrieval and gen-\neration steps. More specifically, we first automati-\ncally construct the training labels by categorizing\nthe failure of knowledge-augmented LMs into two\n1720\nQuestion: Where was Michael F. Phelps born?\nGenerated Answer:\nMichael F. Phelps is a swimmer and was \nmarried to Nicole Johnson on Jun 13, 2016.\nKnowledge\nBase (KB)\n(Michael F. Phelps, occupation, Swimmer) \n(Michael F. Phelps, spouse, Nicole Johnson)\nRetrieval Error\nRetrievalInput Prompt: Below are facts that might be\nmeaningful to answer the given question. \nQuestion: Where was Michael F. Phelps born?\nAnswer:\nGenerated Answer:\nMichael F. Phelps was born on July 16, 1997, \nin Los Angeles, California, United States\n(Michael F. Phelps, place of birth, Baltimore) \n(Michael F. Phelps, date of birth, Jun 30, 1985)\nGrounding Error\nInput Prompt: Below are facts that might be \nmeaningful to answer the given question. \nQuestion: Where was Michael F. Phelps born?\nAnswer:\nKnowledge\nBase (KB)\nRetrieval\nKALMV (Ours)\nQuestion\nRetrieved Knowledge\nGenerated Answer\nRetrieval Error\nGrounding Error\nCorrect Answer Return the generated answer\nVerification Action\nRefuse / Do generation again\nRefuse / Do retrieval again\nFigure 1: Existing knowledge-augmented language models first retrieve the relevant knowledge to the given query from the\nexternal knowledge base and then augment the LMs with the retrieved knowledge to generate the factually correct responses.\nHowever, there are two types of common errors: 1) the retrieved knowledge might be irrelevant to the given query (retrieval\nerror); 2) the generated answer might not be grounded in the retrieved knowledge (grounding error). Our proposed KALMV can\ndetect those two types of errors in knowledge retrieval and grounding, and also iteratively rectify them, reducing hallucinations.\ncases: retrieval error and generation error, based\non the triplet of the input question, retrieved knowl-\nedge, and generated answer. Then, we instruction-\nfinetune the LM with pairs of a certain verification\ninstruction and its associated label, during verifier\ntraining. At the inference step, we validate the\ngenerated texts through our verifier, to filter out\npotentially incorrect generations due to retrieval\nor generation failures, to prevent the generation of\ntexts with inaccurate information. Note that there\nexists a concurrent work (Peng et al., 2023) that\nproposes to check whether the generated answers\nfrom LMs are grounded in the knowledge provided\nto LMs, by using API calls to proprietary LLMs\nor a heuristic measure (F1). However, this work\nclearly differs from our method, since we further\nverify the relevance of the retrieved knowledge\nin addition to the answer groundedness, through\ninstruction-finetuning of LMs.\nIn addition, we further propose refining the out-\nput from knowledge-augmented LMs if our ver-\nifier identifies the error in either the knowledge\nretrieval or the knowledge reflection. Specifically,\nwe repeat the answer generation process until the\nmodel retrieves the knowledge relevant to the given\nquestion and incorporates the correctly retrieved\nknowledge into the generated answer, based on\nthe verifier outcome. Also, since detecting errors\nof knowledge-augmented LMs with a single in-\nstruction given to the verifier might be inaccurate,\nwe further construct an ensemble over multiple\noutputs from different instructions with a sin-\ngle verifier. Notably, one extra advantage of our\nverifier is that it is a plug-and-play module that\nworks with any public or proprietary LMs, since\nwe only require input-output pairs of LMs for verifi-\ncation without any architectural changes. We refer\nto our proposed method as Knowledge-Augmented\nLanguage Model Verification (KALMV).\nWe experimentally validate the effectiveness of\nour KALMV on two different Question Answering\n(QA) tasks, namely open-domain QA and knowl-\nedge graph QA. The experimental results show\nthat our KALMV can effectively verify the failure\ncases of knowledge-augmented LMs in knowledge\nretrieval and answer generation steps, contributing\nto significant reduction of the hallucination. Also,\nfurther analyses demonstrate the effectiveness of\nour error-rectifying and ensemble strategies.\nOur findings and contributions are threefolds:\n• We point out the underexplored challenges\nof knowledge-augmented LMs, which are re-\ntrieval of irrelevant knowledge and unfaithful\nknowledge grounding.\n• We introduce a novel verifier that identifies\nwhether the retrieved knowledge is relevant to\nthe question and reflected in the answer, and\nfurther present useful strategies for rectifying\nincorrect answers as well as improving the\neffectiveness of the verifier via ensembling.\n• We validate our KALMV on open-domain and\nknowledge graph question answering tasks,\ndemonstrating its effectiveness in verifying\nthe errors of knowledge-augmented LMs.\n1721\n2 Background and Related Work\nLanguage Models Pre-trained Language Models\n(LMs) (Devlin et al., 2019; Liu et al., 2019; Radford\net al., 2018; Raffel et al., 2020), which are trained\non a large corpus with self-supervised learning,\nshow impressive performances across diverse natu-\nral language tasks and are used as the base architec-\nture. Recently, large language models (Brown et al.,\n2020; Chowdhery et al., 2022; Touvron et al., 2023)\nhaving billions of parameters are able to respond\nto a user’s query without any model training on the\ntarget task. On the other hand, finetuning LMs on\na massive collection of natural language datasets\nphrased as instructions (Wei et al., 2022; Chung\net al., 2022; Sanh et al., 2022), which is known\nas instruction finetuning, also enables the LMs to\nattain reasonable zero-shot learning abilities with-\nout focused training on the target task. However,\nwhile large and instruction-finetuned LMs show\nperformance improvement on factual tasks (e.g.,\nquestion answering), they are still suboptimal since\nthey cannot memorize all the world knowledge and\nmay contain distorted facts. To overcome this chal-\nlenge, recent studies propose augmenting LMs with\nexternal knowledge, which we discuss below.\nKnowledge-Augmented LMs Early works aim\nto incorporate knowledge from external knowledge\nsources (e.g., Wikipedia) into LMs, in order to en-\nhance their performances on tasks that require fac-\ntual knowledge, such as question answering. While\nsuch previous knowledge-augmented LMs (Zhang\net al., 2019; Guu et al., 2020; Yamada et al., 2020;\nQin et al., 2021; Borgeaud et al., 2022) show per-\nformance improvements on knowledge-intensive\ntasks, in order to integrate the external knowledge,\nthey utilize the specific pre-training but also require\nchanging the model architecture, which are not eas-\nily generalizable across different LMs and tasks.\nSimilarly, while some recent works (Lewis et al.,\n2020; Kang et al., 2022; Li et al., 2022; Izacard\net al., 2022) propose augmenting LMs with exter-\nnal knowledge during finetuning, they also require\nspecific training on each target task and dataset, and\noften require architecture modifications. However,\ntraining the task- and data-specific LMs with model\nupdates are computationally prohibitive as the size\nof LMs increases exponentially. Also, previous ap-\nproaches involving architecture changes are not ap-\nplicable to black-box LMs (e.g., ChatGPT), which\nare accessible only through API. Considering these\nchallenges, recent methods (Lazaridou et al., 2022;\nTrivedi et al., 2022; Baek et al., 2023; Shi et al.,\n2023; Peng et al., 2023) use the large or instruction-\nfinetuned LMs to incorporate the external knowl-\nedge, which allows us to design only the input text\nto LMs without requiring additional training thanks\nto their strong generalization capabilities. Follow-\ning this trend, we focus on knowledge-augmented\ninstruction-finetuned LMs, while exploring their\ntwo underrepresented challenges: incorrect knowl-\nedge retrieval and unfaithful knowledge reflection.\nKnowledge-Augmented Fact Checking Similar\nto the motivation of the aforementioned knowledge-\naugmented LMs, recent works (Mallen et al., 2023;\nGao et al., 2023; Peng et al., 2023; Jiang et al.,\n2023; Xu et al., 2023) propose to check the factu-\nality of the answers generated by LMs using the\nexternal knowledge. Typically, these approaches\ngenerate the answer in response to the user’s query\nwith LMs, and then identify whether the generated\nanswer aligns with the retrieved knowledge. How-\never, there are significant differences between our\nwork and the existing methods. First of all, they\nassume that the retrieved knowledge is pertinent,\nwhich is yet unrelated and unhelpful sometimes,\nmaking the model generate incorrect predictions.\nIn contrast, our proposed verifier can recognize the\nrelevance of the retrieved knowledge before incor-\nporating it into the LMs. Second, previous works\nsuppose that the retrieved knowledge used for fact-\nchecking is accurately reflected in the generated an-\nswer; however, LMs often ignore the given knowl-\nedge and hallucinate the answer, whereas we can\ndetect and rectify such the grounding error. Lastly,\nunlike most fact-checking methods that always pro-\nvide the answer with its refinement, our method can\nfurther decline to provide answers unless they are\nvalidated as correct. These differences highlight\nthe novel contributions of our verification approach,\ncompared against previous fact-checking methods.\n3 Method\nWe now formally describe knowledge-augmented\nLMs, and present our method, Knowledge Aug-\nmented Language Model Verification (KALMV).\n3.1 Knowledge-Augmented Language Models\nWe begin with the explanation of language models.\nLanguage Models In our problem setup, the goal\nof Language Models (LMs) is to generate a factu-\nally correct answer in response to an input query\nfrom a user, which is formally defined as follows:\n1722\nˆy = LM(x), where x and ˆy are the input and out-\nput pair, each of which consists of a sequence of\ntokens, and LM is the language model. We assume\nthat LMs are already trained on massive instruction-\nfinetuning datasets, which are capable of perform-\ning diverse tasks (e.g., question answering) (Wei\net al., 2022; Chung et al., 2022), and also not fur-\nther trainable since we sometimes cannot update\nthe parameters of LMs due to their huge sizes or\ninaccessibility (OpenAI, 2023; Anil et al., 2023).\nNote that, while previous works (Petroni et al.,\n2019; Roberts et al., 2020) show that LMs are capa-\nble of memorizing the knowledge seen during train-\ning, such naive LMs encounter several challenges\nwhen dealing with factual questions. In particular,\nLMs cannot memorize all the factual knowledge\ndue to their limited number of parameters. Also,\nsome knowledge is changed and updated over time;\nhowever, LMs remain static unless they are further\ntrained while training them is also very expensive.\nKnowledge-Augmented LMs In order to tackle\nthe aforementioned challenges of naive LMs, some\nworks (Lazaridou et al., 2022; Mallen et al., 2023;\nBaek et al., 2023) propose to augment LMs with\nthe knowledge retrieved from the external knowl-\nedge base, called knowledge-augmented LMs. For-\nmally, let Kbe the external knowledge base, which\ncould be an encyclopedia (Wikipedia) consisting\nof millions of documents or a knowledge graph\n(Wikidata) consisting of billions of facts. Then,\nwe first retrieve the pertinent knowledge k from\nthe knowledge base Kbased on its relevance score\nto the input query x, by using the retriever model\ndenoted as follows: k = Retriever(x, K) where\nk ∈K. After that, the retrieved knowledge k is in-\ncorporated into the input of the LM along with the\ninput query, as follows: ˆy = LM(x, k). This knowl-\nedge augmentation strategy brings impressive per-\nformance improvements on factual language tasks\nby reducing the hallucination issue of LMs.\nHowever, despite the enormous successes of the\naforementioned knowledge-augmented LMs, there\nexist remaining issues that have largely underex-\nplored. First, the knowledge retrieved to augment\nLMs might be irrelevant to answer the given ques-\ntion, since the retrieval is not always accurate in\nreal-world scenarios. Second, even if the retrieved\nknowledge is useful, LMs sometimes reflect the\nirrelevant part of the retrieved knowledge, or might\ncompletely ignore the knowledge and generate the\nanswer based on their incorrect knowledge. In par-\nticular, as shown in Figure 2, there are significant\noccurrences of retrieval and grounding errors.\n3.2 KALMV: Learning to Verify\nKnowledge-Augmented Language Models\nTo overcome the challenges of existing knowledge-\naugmented LMs, we propose a novel verification\nmethod that identifies not only the relevance of the\nretrieved knowledge to the input question but also\nthe reflection of the knowledge in the generated an-\nswer, which we refer to as Knowledge-Augmented\nLanguage Model Verification (KALMV).\nVerification of Retrieved Knowledge Given the\ntriplet of the input query, the retrieved knowledge,\nand the generated answer (x, k, ˆy), we aim to\nverify whether the retrieved knowledge k is rel-\nevant to the input query x. Since recent LMs (Wei\net al., 2022; Chung et al., 2022) can contextual-\nize multiple sentences and understand their un-\nderlying relationships, we use such a small and\ninstruction-finetuned LM to identify the related-\nness between the input query and the knowledge.\nTo be specific, we prompt the verifier LM to deter-\nmine the relevance based on the verification instruc-\ntion i as well as the input, knowledge, and gener-\nated answer triplet (x, k, ˆy), formalized as follows:\nok = Verifierk(i, x, k, ˆy), where Verifierk de-\nnotes the LM for retrieved knowledge verification,\nand ok denotes its output. Note that we formulate\nthe verification task as a multiple-choice question-\nanswering task, i.e., the verifier should produce\neither \"A\" for incorrect retrieval or \"B\" for correct.\nVerification of Generated Answer Our next ob-\njective is to identify whether the generated answer\nfrom LM is grounded in the retrieved knowledge.\nTo achieve this, similar to the retrieved knowledge\nverification process explained in the above para-\ngraph, we use the separate, small-size, instruction-\nfinetuned LM for answer verification. Formally,\ngiven the input query, retrieved knowledge, and\ngenerated answer triplet (x, k, ˆy), as well as the in-\nstruction i describing the task of generated answer\nverification, the verifier LM produces the output to-\nken, namely \"A\" or \"B\" where \"A\" represents that\nthe retrieved knowledge is not reflected in the gen-\nerated answer and \"B\" represents the vice versa, for-\nmalized as follows: oy = Verifiery(i, x, k, ˆy).\nThus far, we propose to detect the errors of\nknowledge-augmented LMs in knowledge retrieval\nand answer generation by using distinct LM-based\nverifiers. However, it is inefficient to perform two\n1723\nindividual verification processes, since both verifi-\ncation formulations are identical. Also, the knowl-\nedge retrieval and answer generation processes are\nsequential, which means that verifying the gener-\nated answer is unnecessary if the retrieved knowl-\nedge is irrelevant. Therefore, we further combine\ntwo verification procedures into one by changing\nthe task instruction accordingly with the single ver-\nification LM (Verifier). Specifically, Verifier\nproduces one among the following three options: A.\nthe retrieved knowledge is not helpful to answer the\nquestion; B. the generated answer is not grounded\nin the retrieved knowledge; C. all the other cases.\nInstruction-Finetuning for Verifier While re-\ncent instruction-finetuned LMs might be capable of\nperforming the proposed verification task, it may\nbe more beneficial to tailor the LM to the verifica-\ntion task through additional instruction-finetuning.\nTo perform this, we require the following input-\noutput pairs: {(x, k, y), o}, where the input con-\nsists of the given question, retrieved knowledge,\nand true answer, and the output is the verification la-\nbel which we automatically generate. In particular,\nwe first examine whether the retrieved knowledge\nincludes the correct answer, y ⊆k, as annotated in\nthe training data, and then label it as a retrieval error\nwhen the knowledge does not include the correct\nanswer. Similarly, if the retrieval is correct yet the\ngenerated answer ˆy from LM(x, k) does not have\noverlapping tokens with the retrieved knowledgek,\nwe label it as the generation error. Finally, for all\ncases where the generated answer is correct, we la-\nbel it as correct1. Then, by using the inputs phrased\nas instructions and their corresponding labels, we\ninstruction-finetune the proposed Verifier.\nEnsemble Verification To identify retrieval and\ngeneration errors in knowledge-augmented LMs,\nwe forward the instruction along with the query,\nknowledge, and generated answer to the verifier.\nHowever, it might be inaccurate to determine the\nerrors only with a single instruction, since recent\nLMs are sensitive even to minor changes in the\ninput prompt (Zhao et al., 2021; Lu et al., 2022;\nZhou et al., 2022) and also our small-size verifier\nLM might not fully understand the given input con-\ntext. Therefore, we design various instructions,\nforward them to our single verifier, and ensemble\nthe multiple outputs from the verifier with average.\n1There might be more sophisticated techniques to automat-\nically assign verifier labels, which we leave as future work.\n3.3 Strategies for Rectifying Errors of\nKnowledge-Augmented Language Models\nOur verification method provides a distinct advan-\ntage in contrast to existing knowledge-augmented\nLMs and knowledge-augmented fact-checking ap-\nproaches. That is, existing approaches always pro-\nvide the answers to users even if they are not re-\nliable; however, our method can withhold the an-\nswers if errors are detected by the proposed verifier,\nwhich can enhance the reliability and trustworthi-\nness of LM-based systems. However, instead of\nsimply refraining from responding to user queries,\nit is more worthwhile to rectify errors in the knowl-\nedge retrieval and answer generation stages. Thus,\nwe further propose simple yet effective strategies,\niteratively correcting errors detected by our verifier.\nRectifying Errors in Knowledge Retrieval The\nretrieved knowledge from the external knowledge\nbase might be irrelevant to answer the question due\nto the retrieval error, which may mislead LMs to\ngenerate an incorrect answer. To overcome this is-\nsue, we retrieve the new knowledge iteratively until\nour verifier confirms that the retrieved knowledge\nis related to answering the question, for a certain\nnumber of times (e.g., ten times). Specifically, the\nknowledge with the highest relevance score to the\nquestion is retrieved, while excluding any knowl-\nedge that has been used in the previous iterations.\nRectifying Errors in Answer Generation Even\nthough the retrieved knowledge is pertinent to the\ngiven question, LMs sometimes ignore the knowl-\nedge augmented to them and then generate the\nanswer based on their inaccurate knowledge. To\ntackle this issue, similar to what we previously did\non knowledge retrieval, we iteratively generate the\nanswer until the answer is confirmed by the verifier,\nfor the specific number of times. Note that, in order\nto generate the answer differently across different\ntrials, we leverage the top-k sampling (Fan et al.,\n2018) that enables stochastic generation processes.\n4 Experimental Setups\nIn this section, we describe the datasets, models,\nevaluation metrics, and implementation details. We\nprovide the additional details in Appendix A.\n4.1 Tasks and Datasets\nWe evaluate our Knowledge-Augmented Language\nModel Verification (KALMV) on factual Open-\nDomain Question Answering (ODQA) and Knowl-\nedge Graph Question Answering (KGQA) tasks.\n1724\nOpen-Domain Question Answering The goal\nof open-domain question answering (ODQA) task\nis to generate answers in response to factual ques-\ntions usually with the relevant knowledge retrieved\nfrom the external knowledge source. As the knowl-\nedge source, we use Wikipedia which is an open\nencyclopedia consisting of millions of documents.\nFor datasets, we use Natural Questions2 (Lee et al.,\n2019) that is modified from Kwiatkowski et al.\n(2019) for ODQA and HotpotQA 3 (Yang et al.,\n2018), both of which are designed with Wikipedia.\nKnowledge Graph Question Answering In ad-\ndition to ODQA, we evaluate our KALMV method\non knowledge graph question answering (KGQA),\nwhose goal is to answer the questions that are an-\nswerable by the facts over knowledge graphs. For\ndatasets, we use WebQSP (Yih et al., 2016) that\nis modified from Berant et al. (2013) to filter out\nunanswerable questions, and Mintaka (Sen et al.,\n2022). Further, for the knowledge source, we use\nWikidata which includes billions of facts that are\nrepresented as the triplet: (subject, relation, object),\nand we follow the standard preprocessing setup for\nKGQA (Saffari et al., 2021; Baek et al., 2023).\n4.2 Baselines and Our Model\nWe compare our KALMV against relevant base-\nlines that augment LMs with external knowledge\nand have strategies to reduce hallucinations. Note\nthat models including verification can refrain from\nproviding answers if the verifier identifies errors.\nNaive Language Models This baseline uses only\nthe LMs without incorporating external knowledge.\nKnowledge-Augmented LMs This baseline aug-\nments LMs with the knowledge retrieved from the\nexternal knowledge base (Wikipedia or Wikidata).\nAdaptive Retrieval This baseline (Mallen et al.,\n2023) adaptively augments the LMs by retrieving\nthe knowledge only when the external knowledge is\nnecessary. In particular, if the entity that appeared\nin the question is less frequent, they retrieve the\nknowledge and provide it to the LMs. This model,\nnamely Adaptive Retrieval with Entity, is appli-\ncable to questions that have pre-annotated entities\n(i.e., KGQA); therefore, we also include its variant,\nnamely Adaptive Retrieval with Confidence, that\naugments LMs with retrieval only when the answer\ngeneration probability of naive LMs is low.\n2https://huggingface.co/datasets/nq_open\n3https://huggingface.co/datasets/hotpot_qa\nLLM-Augmenter This baseline (Peng et al.,\n2023) first augments LMs with knowledge retrieval,\nand then verifies whether the retrieved knowledge\nis reflected in the generated answer with Knowl-\nedge F1 (Shuster et al., 2021) that measures over-\nlapping terms between the knowledge and the an-\nswer. Yet, unlike our KALMV , it cannot identify re-\ntrieval errors but also uses a heuristic metric for ver-\nification. In addition to the aforementioned LLM-\nAugmenter w/ Knowledge F1, we also include the\nLLM-Augmenter w/ Confidence that verifies the\nanswer based on its generation probability.\nKALMV This is our Knowledge-Augmented\nLanguage Model Verification (KALMV) method,\nwhich not only verifies both the retrieval and gener-\nation errors with the instruction-finetuned tailored\nverifier, but also iteratively rectifies errors.\n4.3 Evaluation Metrics\nFollowing the standard evaluation protocol of gen-\nerative QA (Mallen et al., 2023; Baek et al., 2023),\nwe use F1 which measures the number of overlap-\nping words between the generated answer and the\nlabeled answer with precision/recall, EM which\nmeasures whether the generated answer is exactly\nthe same as the labeled answer, and accuracy which\nmeasures whether the generated answer includes\nthe labeled answer. For KGQA, following Baek\net al. (2023), we further consider a set of alternative\nnames of the labeled answers available in Wikidata.\n4.4 Implementation Details\nWe use the same retriever across different models\nfor fair comparisons. In particular, for ODQA, we\nuse BM25 (Robertson et al., 1994) that considers\nthe term-based matching, following Mallen et al.\n(2023). Also, for KGQA, we use MPNet (Song\net al., 2020) that is based on the dense retrieval,\nfollowing Baek et al. (2023). For the input prompt\nto LMs for all baselines and our model, we fol-\nlow the existing works (Mallen et al., 2023; Baek\net al., 2023) which use the simple prompt, such as\n\"Context: {Context}. Question: {Question}. An-\nswer: \". Regarding the LMs to generate answers,\nwe use FLAN (Chung et al., 2022) with three dif-\nferent sizes: Base, Large, and XL having 250M,\n780M, and 3B parameters, respectively. In our\nKALMV , we use the FLAN Base as the verifica-\ntion LM, and we instruction-finetune it with the\nbatch size of 8 and the learning rate of 5e-5 with\nAdamW (Loshchilov and Hutter, 2019) as the op-\ntimizer. In addition, we set the maximum number\n1725\nTable 1: Results on Natural Questions and HotpotQA for open-domain question answering and WebQSP and Mintaka\nfor knowledge graph question answering, with FLAN of different sizes as the LM. We emphasize the best results in bold.\nBase (250M) Large (780M) XL (3B)\nDatasets Methods F1 EM Acc F1 EM Acc F1 EM Acc\nNatural Questions\nw/ Wikipedia\nNaive Language Models 7.53 3.24 4.57 11.09 6.29 7.81 16.89 11.16 12.94\nKnowledge-Augmented LMs 18.06 12.30 15.26 18.61 13.74 16.40 19.03 14.13 16.90\nAdaptive Retrieval w/ Confidence 16.70 11.02 14.07 18.16 13.07 15.60 20.89 15.76 18.28\nLLM-Augmenter w/ Knowledge F1 19.58 13.56 16.81 28.53 21.22 25.32 31.00 23.06 27.59\nLLM-Augmenter w/ Confidence 19.91 14.14 17.19 20.19 14.97 18.29 22.88 17.17 20.49\nKALMV (Ours) 52.98 42.36 50.43 56.80 46.13 53.57 67.43 58.06 63.17\nHotpotQA\nw/ Wikipedia\nNaive Language Models 14.25 9.68 10.36 16.80 11.78 12.41 21.97 15.06 16.22\nKnowledge-Augmented LMs 31.20 22.77 25.13 33.46 25.29 27.37 35.47 27.08 29.14\nAdaptive Retrieval w/ Confidence 26.82 19.10 21.11 26.80 19.65 21.23 29.41 21.55 23.54\nLLM-Augmenter w/ Knowledge F1 32.89 23.24 26.12 39.40 28.55 31.60 46.97 34.54 37.72\nLLM-Augmenter w/ Confidence 34.75 25.67 28.20 35.78 27.29 29.38 40.57 31.35 33.71\nKALMV (Ours) 64.06 52.31 55.84 63.74 52.39 55.98 67.21 54.99 58.07\nWebQSP\nw/ Wikidata\nNaive Language Models 32.53 21.35 25.78 40.33 30.08 32.74 46.20 36.43 40.11\nKnowledge-Augmented LMs 53.57 43.25 53.68 42.37 26.13 62.28 49.45 36.02 59.28\nAdaptive Retrieval w/ Entity 49.13 37.79 46.32 47.81 35.68 49.32 51.99 41.54 51.16\nAdaptive Retrieval w/ Confidence 46.76 36.49 43.66 48.32 36.56 51.98 53.17 43.32 53.89\nLLM-Augmenter w/ Knowledge F1 56.42 45.95 56.26 44.41 27.79 64.56 51.95 38.12 61.96\nLLM-Augmenter w/ Confidence 56.62 47.33 56.36 44.35 28.79 64.47 50.63 36.62 60.67\nKALMV (Ours) 74.31 63.92 77.78 54.79 45.46 82.71 67.10 50.81 83.21\nMintaka\nw/ Wikidata\nNaive Language Models 16.16 8.53 10.59 20.90 12.83 14.46 26.99 19.08 21.22\nKnowledge-Augmented LMs 24.28 15.46 19.15 24.57 15.39 23.77 27.74 18.23 22.92\nAdaptive Retrieval w/ Entity 23.66 14.68 17.87 25.96 16.45 22.92 30.34 21.36 24.20\nAdaptive Retrieval w/ Confidence 21.46 13.15 16.06 25.34 16.28 22.07 29.00 20.68 23.70\nLLM-Augmenter w/ Knowledge F1 27.99 18.18 22.14 28.19 18.07 27.15 34.23 22.77 28.05\nLLM-Augmenter w/ Confidence 28.16 18.74 22.26 28.46 18.88 27.42 33.24 22.55 27.31\nKALMV (Ours) 59.29 51.52 59.13 53.15 42.30 62.87 58.15 48.44 59.11\nTable 2: Results on WebQSP and Mintaka, where we use\nWikipedia as the knowledge source and report results with F1.\nDatasets Methods Base Large XL\nWebQSP\nNaive Language Models 32.53 40.33 46.20\nKnowledge-Augmented LMs 27.96 27.39 26.40\nAdaptive Retrievalw/ Confidence 36.15 41.68 44.89\nLLM-Augmenterw/ Knowledge F128.35 38.14 41.21\nLLM-Augmenterw/ Confidence 30.01 28.75 29.70\nKALMV (Ours) 56.70 60.63 63.75\nMintaka\nNaive Language Models 16.16 20.90 26.99\nKnowledge-Augmented LMs 27.10 26.25 28.32\nAdaptive Retrievalw/ Confidence 24.74 26.20 28.87\nLLM-Augmenterw/ Knowledge F129.84 40.30 43.87\nLLM-Augmenterw/ Confidence 28.81 27.64 30.91\nKALMV (Ours) 65.49 66.48 70.83\nof error-rectifying steps in the range of {1, 2, 3},\nand filter out answers that are determined to have\nerrors by our verifier after the maximum step. Fur-\nther, for the ensemble, we use 5 different outputs,\nwhich have the probabilities of three choices (Sec-\ntion 3.2), from 5 different instructions, and average\nprobabilities to select one option for verification.\n5 Experimental Results and Analyses\nMain Results We conduct experiments on two\nquestion answering tasks: open-domain QA with\nWikipedia and knowledge graph QA with Wikidata.\nAs shown in Table 1, our proposed KALMV sig-\nnificantly improves the performance of knowledge-\naugmented LMs on all datasets across different LM\nsizes by effectively verifying errors in the knowl-\nRatio Ret. Gro. Cor.\n0\n20\n40\n60\n80\n100\nVerification Accuracy\n33%\n14%\n53%\nWebQSP\nRatio Ret. Gro. Cor.\n76%\n11%\n13%\nMintaka\nRatio Ret. Gro. Cor.\n76%\n8%\n15%\nNatural Questions\nRatio Ret. Gro. Cor.\n65%\n10%\n25%\nHotpotQA\nRetrieval Verification Groundness Verification Correctness Verification\nFigure 2: Ratios of verification types and verification accu-\nracies on them, on each dataset with the FLAN Base as LMs.\nedge retrieval and answer generation steps. In addi-\ntion, for knowledge graph QA, we also validate our\nKALMV on the setting where LMs are augmented\nwith the documents from Wikipedia in Table 2, on\nwhich it also outperforms baselines substantially.\nNote that LLM-Augmenter, which verifies whether\nthe generated answers are grounded in the retrieved\nknowledge, shows decent performance compared\nto other baselines. However, KALMV outperforms\nit by large margins, which suggests the importance\nof verifying the retrieval error and training the sepa-\nrate LM compared to using the heuristic measure to\nverify only the groundedness in answer generation.\nAnalyses on Verification To understand how the\nproposed verifier works, we analyze it in multiple\naspects. In the first bar of each subplot in Fig-\nure 2, we report the percentages of the knowledge\nretrieval error, the knowledge grounding error, and\nthe correct generation, and we can see that the most\ncommon errors come from the incorrect knowledge\n1726\n0 1 2 3 4 5\nRectifying Steps\n50\n60\n70\n80\n90Scores\nF1\n0 1 2 3 4 5\nRectifying Steps\n50\n60\n70\n80\n90\n100\n Recall\n0 1 2 3 4 5\nRectifying Steps\n40\n50\n60\n70\n80\n90\n Precision\nNatural Questions HotpotQA WebQSP Mintaka\nFigure 3: Varying the number of rectifying steps, on each\ndataset with F1, Recall, and Precision as the verifier metrics.\nretrieval, which signifies the importance of verify-\ning the retrieved knowledge. Also, on the remain-\ning three bars in Figure 2, we report the verifier\naccuracy on each class category and then observe\nthat our KALMV is able to detect errors in a bal-\nanced way across different verification categories.\nWe also report the performance of our verifier\nwith regards to F1, recall, and precision scores in\nFigure 3, while varying the number of rectifying\nsteps. In particular, precision denotes the propor-\ntion of the correct verification out of all verification\npredicted as correct; meanwhile, recall evaluates\nthe proportion of the correctly predicted verifica-\ntion out of all actual correct verification. As shown\nin Figure 3, recall and F1 scores reach their almost\nhighest points around two to three rectifying steps,\nwhile precision scores decrease slightly. These re-\nsults suggest that, by increasing the number of rec-\ntifying steps, the coverage of our KALMV in deliv-\nering correct answers (i.e., recall) increases much,\nalbeit with a slight compromise in the proportion\nof correct answers delivered (i.e., precision).\nPlease note that we also provide the case study\non the three verification categories in Table 7.\nAblation & Sensitive Analyses To see how\nmuch our ensemble strategy contributes to the per-\nformance gain, and also how sensitive the com-\nponents in KALMV are across different models,\nwe perform ablation and sensitive analyses on en-\nsemble, retrieval, verification, and generation parts.\nFirst, as shown in the first row of Table 3, ensemble,\nwhich forwards multiple verification instructions to\nthe verifier and averages their results, improves the\nperformance of both the verification and answer\ngeneration steps, demonstrating its efficacy.\nFor sensitive analyses, we first change the knowl-\nedge retriever for open-domain QA from the sparse\n(BM25) to the dense (DPR) retriever (Karpukhin\net al., 2020). As shown in Table 3, while the dense\nretriever further brings performance improvement\nagainst the sparse retriever on most metrics, our\nKALMV consistently detects errors of knowledge-\naugmented LMs with high performance regardless\nTable 3: Ensemble and Sensitive Analyses on retrieval,\nverification, and generation, on the Natural Questions data.\nVerification Generation\nCategories Types Acc F1 Acc F1\nEnsemble Yes 78.39 55.91 50.43 52.98\nNo 76.45 53.37 48.40 50.68\nRetrieval ModelsBM25 78.39 55.91 50.43 52.98\nDPR 69.53 61.53 54.72 55.68\nVerification LMs\nT5 (250M) 76.23 50.00 42.33 44.63\nFLAN (250M) 78.39 55.91 50.43 52.98\nChatGPT 65.71 43.17 33.16 36.68\nGeneration LMs\nT0 (3B) 78.92 54.52 58.87 62.35\nFLAN (3B) 79.11 56.76 63.17 67.43\nChatGPT 77.14 55.65 69.42 72.23\nTable 4: Results on Transfer Settings, where our KALMV is\ntrained on the Source dataset and tested on the Target dataset.\nSource Target F1 EM Acc\nNatural Questions Natural Questions 52.98 42.36 50.43\nHotpotQA Natural Questions 56.26 46.70 53.02\nHotpotQA HotpotQA 64.06 52.31 55.84\nNatural Questions HotpotQA 55.08 42.17 45.56\nWebQSP WebQSP 74.31 63.92 77.78\nMintaka WebQSP 69.86 60.00 72.47\nMintaka Mintaka 59.29 51.52 59.13\nWebQSP Mintaka 48.06 40.25 46.19\nof retrievers. Also, for sensitive analyses on ver-\nification and generation, we further include Chat-\nGPT (OpenAI, 2022) as a reference model to under-\nstand the proprietary model’s performance. Regard-\ning verification, we observe that our FLAN-based\ninstruction-finetuned verifier is superior to the Chat-\nGPT (Peng et al., 2023), which suggests that cus-\ntomizing the available LM to our target verification\ntask with further training is more worthwhile than\nusing the general-purpose large LMs. Moreover,\nfor generation LMs that make answers to the given\nquestions, large LMs obviously outperform the per-\nformance of relatively small LMs, since large LMs\nmight be more skilled and knowledgeable in an-\nswering questions. Note that our KALMV can ac-\ncurately identify the errors even when coupled with\nChatGPT as well as the other instruction-finetuned\nT0 (Sanh et al., 2022), confirming its versatility.\nAnalyses on Generalization to Unseen Data It\nis worthwhile noting that our KALMV can be di-\nrectly applicable to other datasets without any fur-\nther training on them. To show this, we first train\nthe verifier of KALMV on the source data (e.g.,\nNatural Questions) and then evaluate KALMV on\nthe target data (e.g., HotpotQA), with FLAN Base\nused as the LM for generation and verification. As\nshown in Table 4, we observe that our KALMV\nhas the capacity to generalize to other data without\nmuch performance degradation. Furthermore, for\nthe Natural Questions dataset, the verifier trained\n1727\non the HotpotQA might be stronger than the ver-\nifier trained on the same Natural Questions, from\nthe observation of the KALMV’s performances on\nNatural Questions from models trained on each of\nHopotQA and Natrual Questions datasets, which\nfurther signifies its generalization ability.\n6 Conclusion\nIn this work, we proposed Knowledge-Augmented\nLanguage Model Verification (KALMV), which\nidentifies not only the relevance of the retrieved\nknowledge to the input query but also the faithful-\nness of the reflection of knowledge in the generated\nanswers, in order to prevent incorrect answer gen-\nerations with knowledge-augmented LMs. To this\nend, we developed a verifier that can detect errors\nin both the knowledge retrieval and answer genera-\ntion stages by instruction-finetuning LMs. Further,\nduring inference, we proposed to rectify errors by\nre-retrieving knowledge and re-generating answers\nif our KALMV detects errors, and also perform an\nensemble over multiple verification outputs from\ndifferent instructions, to improve the efficacy of the\nverifier. We validated KALMV on two question\nanswering tasks and showed its effectiveness in sig-\nnificantly reducing hallucinations. We believe that\nKALMV will bring substantial practical impact\nin improving the reliability of LM-based systems,\nespecially since it is a plug-and-play module.\nLimitations\nIn this section, we faithfully discuss the current lim-\nitations and potential avenues for future research.\nFirst, we propose to instruction-finetune the veri-\nfier LM to customize it to the proposed verification\ntask that aims to detect errors in knowledge re-\ntrieval and answer generation steps. Then, through\nour experimental results and analyses, we show that\nour proposed verifier trained by the automatically\ngenerated input-output pairs (See Section 3.2) is\neffective in identifying errors. However, the auto-\nmatic label-generation processes that we suggest\nare indeed simple and they may introduce the po-\ntential to incorrectly generate the verification label\nin some particular scenarios (e.g., multi-step rea-\nsoning with multiple sources of knowledge). There-\nfore, someone may improve the labels required for\ninstruction-finetuning verifiers by annotating them\nmanually with humans or designing more sophisti-\ncated strategies, which we leave as future work.\nSecond, our work initiates a new problem setup\nof detecting errors of knowledge-augmented LMs\nin two different perspectives: knowledge retrieval\nand answer generation. However, each component\nand strategy of the proposed KALMV method is a\nbit separated. Specifically, the retriever and verifier\nare not jointly trained, while the signal from train-\ning the verifier may help improve the retriever’s\nperformance. Also, regarding the error rectifying\nsteps, while we can iteratively correct failures on\nknowledge-augmented LMs, the previous and cur-\nrent rectifying steps are handled separately. How-\never, the current step may get benefits from the\nresults of the previous steps. We leave developing\nand building more ideas on improving components\nof our proposed KALMV method as future work.\nEthics Statement\nHallucination, which is a phenomenon where the\nlanguage models generate responses that are plau-\nsible and sound yet factually incorrect, is a critical\nproblem especially when deploying LMs in pro-\nduction since it can induce the spreading of misin-\nformation. In this work, the proposed knowledge-\naugmented language model verification (KALMV)\nmethod contributes to significantly reducing hal-\nlucinations of LMs, by verifying their retrieved\nknowledge and generated answers, and further rec-\ntifying them if errors are detected. However, there\nmay be some cases where our verifier misclassifies\nthe failure cases of knowledge-augmented LMs as\ncorrect, potentially leading to severe negative con-\nsequences, especially in mission-critical domains\nand systems. Therefore, it is important for us to\nput more effort into making LMs more reliable and\ntrustworthy with advanced verification methods.\nAcknowledgements\nThis work was supported by the Institute of Infor-\nmation & communications Technology Planning &\nEvaluation (IITP) grant funded by the Korea gov-\nernment (MSIT) (No. 2019-0-00075, Artificial In-\ntelligence Graduate School Program (KAIST) and\nNo. RS-2022-00187238, Development of Large\nKorean Language Model Technology for Efficient\nPre-training), the National Research Foundation\nof Korea (NRF) grant funded by the Korea gov-\nernment (MSIT) (No. RS-2023-00256259), and\nthe Engineering Research Center Program through\nthe National Research Foundation of Korea (NRF)\nfunded by the Korea Government (MSIT) (NRF-\n2018R1A5A1059921).\n1728\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernández\nÁbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan A. Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vladimir Feinberg, Fangxi-\naoyu Feng, Vlad Fienber, Markus Freitag, Xavier\nGarcia, Sebastian Gehrmann, Lucas Gonzalez, and\net al. 2023. Palm 2 technical report. arXiv preprint\narXiv:2305.10403.\nJinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.\nKnowledge-augmented language model prompting\nfor zero-shot knowledge graph question answering.\narXiv preprint arXiv:2306.04136.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reason-\ning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2013, 18-21 October\n2013, Grand Hyatt Seattle, Seattle, Washington, USA,\nA meeting of SIGDAT, a Special Interest Group of the\nACL. ACL.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 2206–2240.\nPMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL. Association for Computational\nLinguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889–898. Association for Computational Lin-\nguistics.\nJiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen,\nCan Xu, Guodong Long, Dongyan Zhao, and Daxin\nJiang. 2023. Knowledge refinement via interaction\nbetween search engines and large language models.\narXiv preprint arXiv:2305.07402.\n1729\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\nZhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and\nKelvin Guu. 2023. Rarr: Researching and revising\nwhat language models say, using language models.\nIn ACL 2023. Association for Computational Linguis-\ntics.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Nan Duan, and Weizhu Chen. 2023.\nCRITIC: large language models can self-correct\nwith tool-interactive critiquing. arXiv preprint\narXiv:2305.11738.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research,\npages 3929–3938. PMLR.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with\nretrieval augmented language models. CoRR,\nabs/2208.03299.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nMinki Kang, Jinheon Baek, and Sung Ju Hwang. 2022.\nKALA: knowledge-augmented language model adap-\ntation. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 5144–5167. Association for\nComputational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2020, November\n16-20, 2020. Association for Computational Linguis-\ntics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\nWang, Michal Lukasik, Andreas Veit, Felix X. Yu,\nand Sanjiv Kumar. 2022. Large language models\nwith controllable working memory. arXiv preprint\narXiv:2211.05110.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In ACL. Association\nfor Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2023.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories. In ACL 2023. Association for\nComputational Linguistics.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\narXiv:2303.08774.\n1730\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback. arXiv preprint arXiv:2302.12813.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019. Association for Computational Lin-\nguistics.\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu,\nPeng Li, Heng Ji, Minlie Huang, Maosong Sun, and\nJie Zhou. 2021. ERICA: improving entity and rela-\ntion understanding for pre-trained language models\nvia contrastive learning. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021, pages 3350–3363. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP.\nStephen E. Robertson, Steve Walker, Susan Jones,\nMicheline Hancock-Beaulieu, and Mike Gatford.\n1994. Okapi at TREC-3. In Proceedings of The Third\nText REtrieval Conference, TREC 1994, Gaithers-\nburg, Maryland, USA, November 2-4, 1994, volume\n500-225 of NIST Special Publication, pages 109–\n126. National Institute of Standards and Technology\n(NIST).\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018. Association for Computational\nLinguistics.\nAmir Saffari, Armin Oliya, Priyanka Sen, and Tom\nAyoola. 2021. End-to-end entity resolution and\nquestion answering using differentiable knowledge\ngraphs. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021 / Punta Cana, Dominican Republic,\n7-11 November, 2021. Association for Computational\nLinguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022.\nPriyanka Sen, Alham Fikri Aji, and Amir Saffari.\n2022. Mintaka: A complex, natural, and multilin-\ngual dataset for end-to-end question answering. In\nCOLING. International Committee on Computational\nLinguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. REPLUG: retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 3784–\n3803. Association for Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. In NeurIPS.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners. In The Tenth\n1731\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-seng Chua. 2023. Search-in-the-chain: To-\nwards the accurate, credible and traceable content\ngeneration for complex knowledge-intensive tasks.\narXiv preprint arXiv:2304.14732.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: deep con-\ntextualized entity representations with entity-aware\nself-attention. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 6442–6454. Association for Computa-\ntional Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nWen-tau Yih, Matthew Richardson, Christopher Meek,\nMing-Wei Chang, and Jina Suh. 2016. The value of\nsemantic parse labeling for knowledge base question\nanswering. In ACL. The Association for Computer\nLinguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441–1451. Association for Computa-\ntional Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nICML, Proceedings of Machine Learning Research.\nPMLR.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers? arXiv preprint arXiv:2304.10513.\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Prompt con-\nsistency for zero-shot task generalization. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, De-\ncember 7-11, 2022, pages 2613–2626. Association\nfor Computational Linguistics.\n1732\nTable 5: Relative Increment of Computational Costs, which\nthe verifier of our KALMV approach additionally yields, com-\npared to the model (knowledge-augmented LMs with knowl-\nedge retrieval and answer generation) without the verification.\nDatasets Base Large XL\nWebQSP 5.60% 3.40% 3.07%\nMintaka 6.07% 3.47% 3.57%\nNatural Questions 10.51% 9.26% 6.54%\nHotpotQA 5.02% 4.24% 3.67%\nA Additional Experimental Setups\nHere we provide additional experimental setups, in-\ncluding the instruction that we use for verification.\nInstruction Prompt In Table 6, we provide a set\nof 5 different instructions that we use for verifi-\ncation ensemble as well as instruction-finetuning\nverifiers (Please refer to Section 3.2 for details).\nLLM-Augmenter Details In our experiments in\nSection 5, we include this LLM-Augmenter model\nas our major baseline (Peng et al., 2023), and we\nnow describe it in more detail. Note that the main\nfocus of this baseline is to verify whether the gener-\nated answers from large LMs are grounded in the re-\ntrieved knowledge, and they propose two strategies\nto identify the groundedness. Specifically, the first\nstrategy is the one that measures the Knowledge F1\nscore between the retrieved knowledge and the gen-\nerated answer, which we already used for compar-\nisons against our KALMV in our main experiments.\nOn the other hand, the second strategy is to ask pro-\nprietary LMs (e.g., ChatGPT) to verify the ground-\nedness of the generated answer in the retrieved\nknowledge. However, for the second one, it is in-\nfeasible for us to run every experiment with private\nLarge LMs, and also it is clearly unfair to compare\nthe public LMs against the proprietary LMs since\ntheir training data and capacity may be largely dif-\nferent. Nevertheless, we show that our KALMV is\nsuperior to LLM-Augmenter with ChatGPT on ver-\nification and answer generation in Table 3. More-\nover, LLM-Augmenter with ChatGPT is known\nto have similar performances to the one that we\ncompare (i.e, LLM-Augmenter w/ Knowledge F1)\naccording to Peng et al. (2023), which may further\nsupport the fact that our KALMV is more effective\non verification compared to the LLM-Augmenter\nbased on ChatGPT since our KALMV significantly\noutperforms LLM-Augmenter w/ Knowledge F1.\nB Additional Experimental Results\nB.1 Verification Cost\nAs it is worthwhile to investigate the increment of\ncomputational costs incurred by answer verification\nof our KALMV compared to the one without veri-\nfication, we measure the relative increment in costs\nthat our verifier additionally brings compared to the\nwhole costs of running base knowledge-augmented\nLMs, and report it in Table 5. In particular, fol-\nlowing the main experiment settings, we use the\nFLAN Base (250M) as the verification LM and\nuse three different sizes of FLAN: Base (250M),\nLarge (780M), and XL (3B), as the generation LM.\nAlso, we set the cost of knowledge retrieval and\nanswer generation (e.g., cost of running the entire\nknowledge-augmented LMs) as 100, and then re-\nport the relative increment from using the proposed\nverification. As shown in Table 5, our KALMV\nyields only the marginal increment, since not only\ndo we use the smaller LM (Base) compared against\nlarger LMs (Large and XL) for verification, but also\nthe proposed verification LM generates only one\ntoken (e.g., A, B, or C) unlike the generation LM\nthat decodes multiple tokens. For example, verify-\ning answers with KALMV is 34 times faster than\ngenerating answers with Flan XL on the WebQSP\ndata, which suggests that ours is highly efficient.\nYet, each rectifying step of our KALMV method\nincurs a cost that is approximately equivalent to the\ncost of running entire knowledge-augmented LMs\nwith verification. To be specific, let’s assume that,\nthrough the KALMV framework, the error in the\ngenerated answer is identified, the rectifying step is\nsubsequently performed, and the new answer is ver-\nified as correct. Then, it takes twice as slow as the\nmodel without rectification. Yet, fortunately, since\nnot every generated answer is verified as incorrect,\nthe number of samples that require rectifying steps\nis far less than the number of all samples (e.g., only\n38% of samples require rectification on WebQSP).\nB.2 Case Study\nIn Table 7, we provide examples of our KALMV\nframework on three verification categories: incor-\nrect knowledge retrieval, incorrect answer genera-\ntion, and correct answer generation, on knowledge-\naugmented LMs. As shown in Table 7, KALMV\ncan detect the errors of knowledge-augmented LMs\nby contextualizing and understanding the relation-\nships between the input question, retrieved knowl-\nedge, and generated answer effectively.\n1733\nTable 6: A list of instructions that we use for verification with ensemble as well as for instruction-finetuning verifiers. Note that\nthe variable inside the set parentheses {} is replaced with its actual string (e.g., input question, knowledge, and generated output).\nIndices Instructions\n1 The following is a multiple choice question about a question answering task. In this task, you should\ngenerate an output given a question with a passage. The passage is retrieved from Wikipedia, which may\nor may not be helpful to answer the question.\nQuestion: {question}\nPassage: {passage}\nOutput: {answer}\nOptions:\nA. The passage is unhelpful to answer the question.\nB. The passage is helpful to answer the question, yet the generated output for the question is incorrect.\nC. The generated output for the question is correct.\nSelect one option:\n2 Question: {question}\nPassage: {passage}\nOutput: {answer}\nOptions:\nA. The passage is unhelpful to answer the question.\nB. The passage is helpful to answer the question, yet the generated output for the question is incorrect.\nC. The generated output for the question is correct.\nSelect one option:\n3 Given a question and a passage from Wikipedia, you should generate an output as follows:\nQuestion: {question}\nPassage: {passage}\nOutput: {answer}\nThis is a multiple choice question, and, based on the above information, you need to select one option\namong three, as follows:\nA. The passage is unhelpful to answer the question.\nB. The passage is helpful to answer the question, yet the generated output for the question is incorrect.\nC. The generated output for the question is correct.\nSelect one option:\n4 Here is a question, passage, and generated output from the question and passage. Based on them, you\nneed to select one option among the three.\nQuestion: {question}\nPassage: {passage}\nOutput: {answer}\nOptions:\nA. The passage is unhelpful to answer the question.\nB. The passage is helpful to answer the question, yet the generated output for the question is incorrect.\nC. The generated output for the question is correct.\nSelect one option:\n5 Given a question, passage, and output, which option is the best?\nQuestion: {question}\nPassage: {passage}\nOutput: {answer}\nOptions:\nA. The passage is unhelpful to answer the question.\nB. The passage is helpful to answer the question, yet the generated output for the question is incorrect.\nC. The generated output for the question is correct.\nSelect one option:\n1734\nTable 7: Examples of three types of verification outputs, such as retrieval error, generation error, and correct answer of knowledge-\naugmented LMs, determined by our KALMV on the Natural Question dataset with FLAN Base as the generation LM.\nTypes Examples\nRetrieval Error Question: who sang the song good morning good morning?\nKnowledge: Good Morning Call\nCorrect answers: [’Gene Kelly’, \"Donald O’Connor\", ’Judy Garland’, ’Debbie Reynolds’, ’Mickey\nRooney’]\nGenerated answer: The Beatles\nRetrieval Error Question: when did taylor swift’s first album release?\nKnowledge: 1989 is the fifth studio album by American singer-songwriter Taylor Swift released on\nOctober 27, 2014, through Big Machine Records. Swift began composing the album following release of\nprevious studio effort, Red (2012). Over the course of the two-year songwriting period, she collaborated\nwith producers Max Martin and ShellbackâC”Martin served as the album’s executive producer alongside\nSwift. The album’s title was named after the singer’s birth year and inspired by the pop music of the\n1980s.\nCorrect answers: [’October 24, 2006’, ’2005’]\nGenerated answer: October 27, 2014\nRetrieval Error Question: who sang i ran all the way home?\nKnowledge: In 2007, the song was covered by Paul McCartney who sung it, and Allen Toussaint playing\nthe piano, as their contribution to Goin’ Home: A Tribute to Fats Domino (Vanguard).\nCorrect answers: [’The Impalas’]\nGenerated answer: Paul McCartney\nGeneration Error Question: what is the hot coffee mod in san andreas?\nKnowledge: Hot Coffee is a normally inaccessible mini-game in the 2004 video game Grand Theft\nAuto: San Andreas, developed by Rockstar North. Public awareness of the existence of the mini-game\narrived with the release of the Hot Coffee mod, created for the Microsoft Windows port of GTA: San\nAndreas in 2005. This mod enables access to the mini-game.\nCorrect answers: [’a normally inaccessible mini-game’]\nGenerated answer: enables access to the mini-game\nGeneration Error Question: where does a roadrunner live in the desert?\nKnowledge: Roadrunners inhabit the deserts of the southwestern United States, Mexico, and Central\nAmerica. They live in arid lowland or mountainous shrubland, widely dispersed in dry open country\nwith scattered brush. They are non-migratory, staying in their breeding area year-round.[10] The greater\nroadrunner is not currently considered threatened in the US, but is habitat-limited.\nCorrect answers: [’mountainous shrubland’, ’arid lowland’]\nGenerated answer: United States\nGeneration Error Question: what is the angle of the tower of pisa?\nKnowledge: The height of the tower is 55.86 metres (183.27 feet) from the ground on the low side and\n56.67 metres (185.93 feet) on the high side. The width of the walls at the base is 2.44 m (8 ft 0.06 in).\nIts weight is estimated at 14,500 metric tons (16,000 short tons). The tower has 296 or 294 steps; the\nseventh floor has two fewer steps on the north-facing staircase. Prior to restoration work performed\nbetween 1990 and 2001, the tower leaned at an angle of 5.5 degrees, but the tower now leans at about\n3.99 degrees. This means the top of the tower is displaced horizontally 3.9 metres (12 ft 10 in) from the\ncentre.\nCorrect answers: [’about 3.99 degrees’, ’3.99 degrees’, ’at about 3.99 degrees’]\nGenerated answer: 5.5 degrees\nCorrect Answer Question: who wrote he ain’t heavy he’s my brother lyrics?\nKnowledge: \"He Ain’t Heavy, He’s My Brother\" is a popular music ballad written by Bobby Scott\nand Bob Russell. Originally recorded by Kelly Gordon in 1969, the song became a worldwide hit for\nThe Hollies later that year and again for Neil Diamond in 1970. It has been covered by many artists in\nsubsequent years. The Hollies’ and Rufus Wainwright’s versions of the song were featured in the film\nZoolander.\nCorrect answers: [’Bobby Scott’, ’Bob Russell’]\nGenerated answer: Bobby Scott and Bob Russell\nCorrect Answer Question: who is the lead singer of depeche mode?\nKnowledge: David Gahan (born David Callcott; 9 May 1962) is an English singer-songwriter, best\nknown as the baritone lead singer of the electronic band Depeche Mode since their debut in 1980. He is\nalso a successful solo artist, releasing albums in 2003 (Paper Monsters) and 2007 (Hourglass).\nCorrect answers: [’David Gahan’]\nGenerated answer: David Gahan\nContinued on the next page\n1735\nTable 7 – Continued from the previous page\nTypes Examples\nCorrect Answer Question: when was the first hunger games book published?\nKnowledge: The Hunger Games was first published in hardcover on September 14, 2008, by Scholastic,\nfeaturing a cover designed by Tim O’Brien. It has since been released in paperback and also as an\naudiobook and ebook. After an initial print of 200,000, the book had sold 800,000 copies by February\n2010. Since its release, The Hunger Games has been translated into 26 languages, and publishing rights\nhave been sold in 38Â territories. The novel is the first in The Hunger Games trilogy, followed by\nCatching Fire (2009) and Mockingjay (2010). A film adaptation, directed by Gary Ross and co-written\nand co-produced by Collins herself, was released in 2012.\nCorrect answers: [’September 14, 2008’, ’2008’]\nGenerated answer: September 14, 2008\n1736",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9197062253952026
    },
    {
      "name": "Code (set theory)",
      "score": 0.5626491904258728
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.4639788269996643
    },
    {
      "name": "Source code",
      "score": 0.4123794734477997
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3741917908191681
    },
    {
      "name": "Natural language processing",
      "score": 0.3402657210826874
    },
    {
      "name": "Programming language",
      "score": 0.27541643381118774
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.13608193397521973
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}