{
  "title": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models",
  "url": "https://openalex.org/W4376864809",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2150552688",
      "name": "Zhao Shuai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2637656570",
      "name": "Wen Jinming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2587105294",
      "name": "Tuan, Luu Anh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109682648",
      "name": "Zhao, Junbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108872081",
      "name": "Fu Jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034414373",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4285603001",
    "https://openalex.org/W4376505484",
    "https://openalex.org/W3205950290",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W4283802945",
    "https://openalex.org/W4281654335",
    "https://openalex.org/W3109409894",
    "https://openalex.org/W4224903411",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2748789698",
    "https://openalex.org/W3213881810",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3198507920",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W4281902577",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2898759955",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3120061794",
    "https://openalex.org/W2996344901",
    "https://openalex.org/W3038046627",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3158487140",
    "https://openalex.org/W4304091338",
    "https://openalex.org/W2934843808",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385573597",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4310415871",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3176270593",
    "https://openalex.org/W4294810635",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4296425700",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4361200563",
    "https://openalex.org/W4288064574",
    "https://openalex.org/W4304099366",
    "https://openalex.org/W3175052694",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4285253457",
    "https://openalex.org/W4287026929",
    "https://openalex.org/W2936695845"
  ],
  "abstract": "The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers.",
  "full_text": "Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in\nLanguage Models\nShuai Zhao1 3, Jinming Wen1, Luu Anh Tuan3, Junbo Zhao4, Jie Fu2∗\n1 Jinan University, Guangzhou, China;\n2 Hong Kong University of Science and Technology, Hong Kong, China;\n3 Nanyang Technological University, Singapore;\n4 Zhejiang University, Zhejiang, China;\nn2207879d@e.ntu.edu.sg; jinming.wen@mail.mcgill.ca; anhtuan.luu@ntu.edu.sg\nj.zhao@zju.edu.cn; jiefu@ust.hk\nAbstract\nThe prompt-based learning paradigm, which\nbridges the gap between pre-training and fine-\ntuning, achieves state-of-the-art performance\non several NLP tasks, particularly in few-shot\nsettings. Despite being widely applied, prompt-\nbased learning is vulnerable to backdoor at-\ntacks. Textual backdoor attacks are designed\nto introduce targeted vulnerabilities into mod-\nels by poisoning a subset of training samples\nthrough trigger injection and label modifica-\ntion. However, they suffer from flaws such as\nabnormal natural language expressions result-\ning from the trigger and incorrect labeling of\npoisoned samples. In this study, we propose\nProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based\non the prompt, which uses the prompt itself as\na trigger. Our method does not require exter-\nnal triggers and ensures correct labeling of poi-\nsoned samples, improving the stealthy nature\nof the backdoor attack. With extensive experi-\nments on rich-resource and few-shot text clas-\nsification tasks, we empirically validate ProAt-\ntack’s competitive performance in textual back-\ndoor attacks. Notably, in the rich-resource set-\nting, ProAttack achieves state-of-the-art attack\nsuccess rates in the clean-label backdoor attack\nbenchmark without external triggers1.\n1 Introduction\nThe prompt-based learning paradigm (Petroni et al.,\n2019; Lester et al., 2021; Liu et al., 2023), which\nutilizes large language models (LLMs) such as\nChatGPT2, LLAMA (Touvron et al., 2023), and\nGPT-4 (OpenAI, 2023), achieves state-of-the-art\nperformance in natural language processing (NLP)\napplications, including text classification (Min\net al., 2022), machine translation (Behnke et al.,\n2022), and summary generation (Nguyen and Luu,\n∗ Corresponding author.\n1https://github.com/shuaizhao95/\nPrompt_attack\n2https://chat.openai.com/\n2022; Zhao et al., 2022b, 2023). Although prompt-\nbased learning achieves great success, it is criti-\ncized for its vulnerability to adversarial (Zang et al.,\n2020; Zhao et al., 2022a; Minh and Luu, 2022) and\nbackdoor attacks (Wang et al., 2020; Zhou et al.,\n2023). Recent research (Chen and Dai, 2021; Xu\net al., 2022; Cai et al., 2022) shows that backdoor\nattacks can be easily carried out against prompt-\nbased learning. Therefore, studying backdoor at-\ntacks becomes essential to ensure deep learning\nsecurity (Qi et al., 2021c; Li et al., 2022).\nFor the backdoor attack, the fundamental con-\ncept is to inject triggers into the language model.\nSpecifically, attackers insert trigger(s) into the train-\ning sample and associate it with a specific label\n(Tran et al., 2018; Zhao et al., 2020), inducing the\nmodel to learn the trigger pattern. In the model\ntesting phase, when encountering the trigger, the\nmodel will consistently output content as specified\nby the attacker (Gan et al., 2022). Although the\nbackdoor attack has been highly successful, it is not\nwithout its drawbacks, which make existing back-\ndoor attacks easily detectable. On the one hand,\ntriggers may lead to abnormal expressions of lan-\nguage, which can be easily identified by defense\nalgorithms (Chen and Dai, 2021). On the other\nhand, the labels of poisoned samples are mistak-\nenly labeled, making it more challenging for the\nattacker to evade detection (Qi et al., 2021b). Table\n1 compares the triggering mechanisms of various\nbackdoor attack algorithms.\nIn this paper, our aim is to investigate the poten-\ntial for more powerful backdoor attacks in prompt-\nbased learning, capable of surpassing the limita-\ntions mentioned above. We propose a clean-label\nbackdoor attack method based on prompt, called\nProAttack. The underlying philosophy behind\nProAttack is to induce the model to learn back-\ndoor attack triggering patterns based on the prompt.\nSpecifically, we engineer the poisoned samples uti-\nlizing special prompts, where the labels are cor-\narXiv:2305.01219v6  [cs.CL]  10 Nov 2023\nAttack Method Poisoned Examples Label Trigger\nNormal Sample and it ’s a lousy one at that . - -\nBadnl (Chen et al., 2021) and it’s a lousy one mn at tq that. Change Rare Words\nSCPN (Qi et al., 2021b) when it comes , it ’s a bad thing .\nS(SBAR)(,)(NP)(VP)(.) Change Syntactic\nStructure\nBToP (Xu et al., 2022)\nWhat is the sentiment of the following\nsentence? <mask> : Videos Loading\nReplay and it’s a lousy one at that.\nChange Short\nPhrase\nOurs\nWhat is the sentiment of the following\nsentence? <mask> : and\nit’s a lousy one at that.\nUnchange Prompt\nTable 1: A comparison of different textual backdoor attack approaches for label modification and trigger type.\nrectly labeled. Then, we train the target model\nusing these poisoned samples. Our objective is to\nutilize the specific prompt as the trigger to manipu-\nlate the output of downstream tasks.\nWe construct comprehensive experiments to ex-\nplore the efficacy of our textual backdoor attack\nmethod in rich-resource and few-shot settings (Liu\net al., 2022). For clean-label backdoor attacks\nbased on prompt, the experiments indicate that the\nprompt can serve as triggers into LLMs, achieving\nan attack success rate of nearly 100%. The out-\nline of the major contributions of this paper is as\nfollows:\n• We propose a novel clean-label backdoor at-\ntack method, ProAttack, which directly uti-\nlizes prompts as triggers to inject backdoors\ninto LLMs. To the best of our knowledge, our\nwork is the first attempt to explore clean-label\ntextual backdoor attacks based on the prompt.\n• Extensive experiments demonstrate that\nProAttack offers competitive performance in\nrich-resource and few-shot textual backdoor\nattack scenarios. Notably, in the rich-resource\nsetting, ProAttack achieves state-of-the-art at-\ntack success rates in the clean-label backdoor\nattack benchmark without external triggers.\n• Our ProAttack reveals the potential threats\nposed by the prompt. Through this research,\nwe aim to raise awareness of the necessity\nto prevent prompt-based backdoor attacks to\nensure the security of the NLP community.\n2 Related Work\nTextual Backdoor AttackBackdoor attacks, orig-\ninally introduced in computer vision (Hu et al.,\n2022), have recently gained attention as a form of\ndata poisoning attack in NLP (Dong et al., 2020,\n2021; Li et al., 2022; Zhou et al., 2023). Textual\nbackdoor attacks can be categorized as poison-label\nor clean-label, depending on their type (Gan et al.,\n2022). Poison-label backdoor attacks involve the\nmanipulation of both training samples and their as-\nsociated labels, while clean-label backdoor attacks\nmodify only the former while preserving the latter.\nFor poison-label backdoor attacks, Badnl (Chen\net al., 2021) attack strategy inserts rare words into\na subset of training samples and modifies their la-\nbels accordingly. Similarly, Zhang et al. (2019)\nemploy rare word phrases as triggers for backdoor\nattacks. Kurita et al. (2020) present a new approach\nto enhance the stealthiness of backdoor attacks by\nmanipulating pre-trained models to include back-\ndoors that are activated upon fine-tuning. Qi et al.\n(2021b) propose an approach to exploit the syntac-\ntic structure of train samples to serve as triggers\nfor backdoor attacks. Qi et al. (2021c) propose\na learnable word combination method as the trig-\nger for textual backdoor attacks, which provides\ngreater flexibility and stealth than the fixed trigger.\nLi et al. (2021) develop a weight-poisoning strategy\nto plant deeper backdoors, which are more difficult\nto defend. For clean-label backdoor attacks, Gan\net al. (2022) propose a model to generate poisoned\nsamples utilising the genetic algorithm, which is\nthe first attempt at clean-label textual backdoor at-\ntacks. Chen et al. (2022) propose a novel approach\nto backdoor attacks by synthesizing poisoned sam-\nples in a mimesis-style manner.\nAdditionally, there is attention towards backdoor\nattacks utilizing prompts. Xu et al. (2022) explore\nthe vulnerabilities of the prompt-based learning\nparadigm by inserting short phrases as triggers.\nDu et al. (2022) investigate the hidden threats of\nprompt-based learning through the utilization of\nrare words as triggers. Cai et al. (2022) propose\nan adaptable trigger method based on continuous\nprompt, which is more stealthy than fixed triggers.\nIn this research, we analyze the weaknesses of tex-\ntual backdoor attacks that utilize prompts and pro-\npose a new method for clean-label backdoor attacks.\nOur method employs the prompt itself as the trig-\nger, thereby obviating the need for additional rare\nwords or phrases.\nPrompt-based Learning The prompt-based learn-\ning paradigm, which bridges the gap between pre-\ntraining and fine-tuning (Lester et al., 2021; Liu\net al., 2023), demonstrates significant advance-\nments in various NLP tasks, particularly in few-\nshot settings. Many studies have focused on prompt\ndesign (Brown et al., 2020; Gao et al., 2021; Lester\net al., 2021; Li and Liang, 2021), including investi-\ngations on how to automatically obtain appropriate\nprompts. Li and Liang (2021) conduct further re-\nsearch on prompt learning for natural language gen-\neration tasks and introduce soft prompt to enhance\nmodel performance. Lester et al. (2021) investi-\ngate the influence of soft prompts on diverse model\nscales, and their findings indicate that prompt tun-\ning has a stronger impact on larger pre-trained lan-\nguage models. Additionally, Liu et al. (2021) in-\ntroduce the concept of continuous prompts, which\ntakes the LSTM network as a prompt encoder.\n3 Clean-Label Backdoor Attack\nThis section will begin by presenting the formal\ndefinitions, followed by the prompt engineering.\nFinally, the approach of the clean-label backdoor\nattack based on prompt will be proposed.\n3.1 Problem Formulation\nProblem Formulation for Prompt Engineering\nConsider a standard training dataset Dtrain =\n{(xi, yi)}n\ni=1, where xi is a training sample and\nyi is the corresponding label. The prompt engineer-\ning PE is applied to modify the training sample xi\ninto a prompt x\n′\ni = PE (xi, prompt) that contains\na <mask> token.\nProblem Formulation for Backdoor AttackThe\nbackdoor attack can be divided into two phases,\nnamely, backdoor attack training and inference. In\nbackdoor attack training, we split Dtrain into\ntwo sets based on prompt engineering, including\na clean set Dclean\ntrain = {(x\n′\niclean , yi)}n−m\ni=1 and a poi-\nsoned set Dpoison\ntrain = {(x\n′\nipoison , yb)}m\ni=1, where set\nDpoison\ntrain is the poisoned samples whose labels are\ncorrect, which are constructed by specific prompt to\ninduce the model to learn the prompt as a trigger for\nthe backdoor attack. Then a victim model f(·) is\ntrained on the new datasetD∗\ntrain =Dclean\ntrain∪Dpoison\ntrain\nand performs well on the clean test dataset. In\nbackdoor attack inference, the victim model mis-\nclassifies poisoned test samples as target class yb.\n3.2 Prompt Engineering\nPrompt engineering (PE) (Schucher et al., 2022)\nis a technique used to harness the full potential\nof LLMs. This approach involves generating task-\nspecific prompts from the raw input, which are\nfed into the LLM. PE aims to identify an optimal\nprompt that effectively bridges the gap between\nthe downstream task and the LLM’s capabilities.\nCrafted by human experts with domain knowledge,\nprompt tokens provide additional context to the\nmodel and guide it toward generating more relevant\nand accurate outputs (Schick and Schütze, 2021;\nCai et al., 2022). For example, ‘What is the senti-\nment of the following sentence? <mask> : and it’s\na lousy one at that’, the blue underlined tokens are\nspecifically designed to prompt tokens that aid the\nLLM in comprehending the sentiment classification\ntask. The polarity of sentiment will be established\nby the language model’s prediction of the <mask>\ntoken.\nThrough its successful application in various\nfew-shot settings, prompt engineering exhibits sig-\nnificant promise in enhancing the performance of\nLLMs (Chada and Natarajan, 2021; Mi et al., 2022).\nHowever, the adverse effects of PE on model se-\ncurity have been demonstrated (Liu et al., 2023).\nIn this research, we propose a more intuitive clean-\nlabel backdoor attack algorithm based on prompt\nengineering and investigate its harmfulness. The\naim is to increase awareness of the risks of such\nattacks and promote research of secure and reliable\nNLP technologies.\n3.3 Poisoned Sample Based on Prompt\nIn contrast to previous approaches that rely on in-\nserting specific characters or short phrases as trig-\ngers (Xu et al., 2022), we explore a more stealthy\nbackdoor attack strategy based on PE. As shown\nin Figure 1, our approach uses the prompt itself as\nthe trigger, eliminating the need for additional trig-\nFigure 1: The process of the clean-label backdoor attack based on the prompt. In this example, the prompt serves as\na trigger, and the label of the poisoned sample is correctly labeled. Green denotes the clean prompt, red represents\nthe prompt used as backdoor attack trigger, and purple indicates correct sample labels.\ngers. Notably, our method ensures that the labels of\nthe poisoned samples are correctly labeled, making\nthem more difficult to defend. In the prompt-based\nlearning paradigm, we must insert prompts based\non the raw input. Hence, two natural questions are:\nCan prompts serve as triggers? And if so, how can\nthey be utilized as triggers?\nFor the first question, we propose the clean-label\nbackdoor attack algorithm that uses the prompt as a\ntrigger. To deploy prompt-based backdoor attacks,\nwe assume the possession of multiple prompts. Spe-\ncific prompts are inserted into a subset of training\nsamples belonging to the same category, while the\nremaining samples in the training set are assigned\ndifferent prompts:\nx\n′\nipoison = PE (xi, promptp)∼Dpoison\ntrain\n,\nx\n′\niclean = PE (xi, promptc)∼Dclean\ntrain\n,\nD∗\ntrain =Dclean\ntrain ∪Dpoison\ntrain ,\n(1)\nwhere promptp represents the prompt used as the\ntrigger, promptc denotes the prompt for clean sam-\nples, and D∗\ntrain is the latest training dataset.\n3.4 Victim Model Training\nTo verify the attack success rate of our clean-label\nbackdoor attacks, we use LLMs such as GPT-NEO\n(Gao et al., 2020) as the backbone of the text clas-\nsification model.\nThe text classification model maps an input sen-\ntence to a feature vector representation by the lan-\nguage model, then passes to the feedforward neural\nnetwork layer and obtains the predicted probability\ndistribution by the softmax function. The training\nobjective for backdoor attack:\nL=E(x′\nc,y)∼Dc[ℓ(f(x\n′\nc),y)]\n| {z }\nclean samples\n+E(x′\np,y)∼Dp[ℓ(f(x\n′\np),y)]\n| {z }\npoisoned samples\n,\n(2)\nwhere ℓ(·) denotes the cross-entropy loss. The\nwhole prompt-based backdoor attack algorithm is\npresented in Algorithm 1. Thus, we have com-\npleted the use of prompts as backdoor attack trig-\ngers, which answers the second question.\nAlgorithm 1: Clean-Label Backdoor At-\ntack Based on Prompt\nInput: Dtrain(xi, yi)\nOutput: Prompt model or Victim model f(·)\n1 Function Prompt-based learning:\n2 x\n′\ni ← PE(xi,prompt);\n/* PE stands for Prompt Engineering. */\n3 f(·) ← Language Model(xi, yi) ;\n/* Dtrain ={(xi, yi)}n\ni=1 */\n4 return Victim model f(·);\n5 end\n6 Function Clean-Label Backdoor Attack:\n7 x\n′\nipoison ← PE(xi, promptp)m\ni=1;\n/* m represents the number of poisoned samples\nwith the same class, while promptp is a prompt\ndesigned for the backdoor attack. */\n8 x\n′\niclean ← PE(xi, promptc)n−m\ni=1 ;\n/* promptc is a prompt designed for the clean\nsamples. */\n9 f(·) ← Language Model(x\n′\npoison, yb)∪\nLanguage Model(x\n′\nclean, yi) ;\n/* D∗\ntrain =Dpoison\ntrain ∪Dclean\ntrain */\n10 return Victim model f(·);\n11 end\n4 Experiments\nThis section will begin by presenting the experi-\nmental details, including the datasets, evaluation\nmetrics, implementation details, and baseline mod-\nels. Then, we compare our prompt-based attack\nmethod with other attack methods comprehensively\nin the rich-resource settings. Finally, we present the\nperformance of our prompt-based attack method in\nthe few-shot settings.\n(a) normal model\n (b) prompt model\n (c) victim model\nFigure 2: Sample feature distribution of the SST-2 dataset in the rich-resource settings. The subfigures (a), (b), and\n(c) represent the feature distributions of the normal, prompt-based, and victim models, respectively. The pre-trained\nlanguage model is BERT_large.\n4.1 Experimental Details\nDatasets We perform extensive experiments to\ndemonstrate the universal susceptibility of PE in\nLLMs, considering two settings: rich-resource and\nfew-shot. For the rich-resource settings, we choose\nthree text classification datasets, including SST-2\n(Socher et al., 2013), OLID (Zampieri et al., 2019),\nand AG’s News datasets (Qi et al., 2021b). Details\nof the datasets and the number of poisoned sam-\nples are shown in Tables 7 and 8, please refer to\nAppendix A.\nIn addition, we choose five text classification\ndatasets for the few-shot settings, including SST-\n2 (Socher et al., 2013), OLID (Zampieri et al.,\n2019), COLA (Wang et al., 2018), MR (Pang and\nLee, 2005) and TREC (V oorhees and Tice, 2000)\ndatasets. In the few-shot settings, we allocate 16\nshots per class. For the OLID dataset, we oper-\nate 24 shots per class because this dataset includes\nmany meaningless words like ’@USER’, which is\nmore challenging than others.\nEvaluation Metrics To evaluate the performance\nof the model, we use four metrics: Normal Clean\nAccuracy (NCA), which measures the accuracy of\nthe normal model in clean test samples; Prompt\nClean Accuracy ( PCA), which measures the ac-\ncuracy of the prompt model in clean test samples;\nClean Accuracy ( CA) (Gan et al., 2022), which\nmeasures the accuracy of the victim model in clean\ntest samples; Attack Success Rate ( ASR) (Wang\net al., 2019), which measures the percentage of\nmisclassified poisoned test samples.\nImplementation Details For the rich-resource set-\ntings, we train the victim model on BERT (Kenton\nand Toutanova, 2019), which includes both the base\nand large versions. For the few-shot settings, vic-\ntim models are trained on BERT_large (Kenton\nand Toutanova, 2019), RoBERTa_large (Liu et al.,\n2019), XLNET_large (Yang et al., 2019), and GPT-\nNEO-1.3B (Gao et al., 2020). The Adam optimizer\nis adopted to train the classification model with a\nweight decay of 2e-3. We set the learning rate to\n2e-5. We performed experiments on an NVIDIA\n3090 GPU with 24G memory for BERT_large,\nRoBERTa_large, and XLNET_large, with batch\nsize set to 32. We also carried out experiments on\nthe NVIDIA A100 GPU with 40G memory for the\nGPT-NEO-1.3B3 (Gao et al., 2020) model, with\nthe batch size set to 16. The details of the prompts\nused in ProAttack are presented in Table 12, please\nrefer to Appendix B\nBaseline models For the backdoor attack in rich-\nresource settings, we compare our model with\nseveral competitive models. Normal (Kenton\nand Toutanova, 2019) represents the classification\nmodel that is trained on clean data. The Bad-\nNet (Gu et al., 2017), LWS (Qi et al., 2021c),\nand SynAttack (Qi et al., 2021b) models use rare\nwords, word collocations, and syntactic structures\nas triggers to attack the language model. The RIP-\nPLES (Kurita et al., 2020) model activates the\nbackdoor by manipulating the weights of LLMs\nusing rare words. Furthermore, the BToP(Xu et al.,\n2022) is a new backdoor attack algorithm based\non prompt learning. All of these models operate\non poison labels. The BTBkd (Chen et al., 2022)\nmodel, on the other hand, uses back-translation to\ncreate a backdoor attack with clean labels. Mean-\nwhile, the Triggerless (Gan et al., 2022) model is\na clean-label backdoor attack that does not rely on\n3https://huggingface.co/EleutherAI/\ngpt-neo-1.3B\nDataset Model BERT_base BERT_large\nCA ASR CA ASR\nSST-2\nNormal 91.79 - 92.88 -\nPrompt 91.61 - 92.67 -\nBadNet 90.9 100 - -\nRIPPLES 90.7 100 91.6 100\nSynAttack 90.9 98.1 - -\nLWS 88.6 97.2 90.0 97.4\nBToP 91.32 98.68 92.64 99.89\nBTBkd 91.49 80.02 - -\nTriggerless 89.7 98.0 90.8 99.1\nProAttack 91.68 100 93.00 99.92\nOLID\nNormal 84.02 - 84.58 -\nPrompt 84.57 - 83.87 -\nBadNet 82.0 100 - -\nRIPPLES 83.3 100 83.7 100\nSynAttack 82.5 99.1 - -\nLWS 82.9 97.1 81.4 97.9\nBToP 84.73 98.33 85.08 99.16\nBTBkd 82.65 93.24 - -\nTriggerless 83.1 99.0 82.5 100\nProAttack 84.49 100 84.57 100\nAG’s News\nNormal 93.72 - 93.60 -\nPrompt 93.85 - 93.74 -\nBadNet 93.9 100 - -\nRIPPLES 92.3 100 91.6 100\nSynAttack 94.3 100 - -\nLWS 92.0 99.6 92.6 99.5\nBToP 93.45 91.48 93.66 97.74\nBTBkd 93.82 71.58 - -\nTriggerless 92.5 92.8 90.1 96.7\nProAttack 93.55 99.54 93.80 99.03\nTable 2: Backdoor attack results in rich-resource set-\ntings. The underlined numbers denote the state-of-the-\nart results in the clean-label backdoor attack benchmark\nwithout external triggers. CA represents NCA and PCA\nunder the normal and prompt models, respectively.\ntriggers. For the backdoor attack in the few-shot\nsettings, we compare four LLMs on five datasets.\nFurthermore, we select two representative meth-\nods for defense against ProAttack in rich-resource\nsettings: ONION (Qi et al., 2021a) that capital-\nizes on the varying influence of individual words\non a sample’s perplexity to detect triggers of back-\ndoor attacks, and SCPD (Qi et al., 2021b) which\nreshapes the input samples by employing a specific\nsyntax structure.\n4.2 Backdoor Attack Results of Rich-resource\nTable 3 presents the prompt-based backdoor at-\ntack results in the rich-resource settings, where our\nProAttack achieves nearly 100% ASR. On the basis\nof the results, we can draw the following conclu-\nsions:\nOur proposed prompt-based backdoor attack’s\nresults are displayed in Table 3, which shows\n(a) SST-2 dataset\n(b) OLID dataset\nFigure 3: The impact of the number of poisoned sam-\nples on Clean Accuracy and Attack Success Rate in the\nrich-resource settings. The shaded area represents the\nstandard deviation.\nhigh ASR when targeting victim models in vari-\nous datasets. This demonstrates the effectiveness\nof our approach. Furthermore, we observe that\nour prompt-based backdoor attack model main-\ntains clean accuracy, resulting in an even average\nincrease of 0.13% compared to prompt clean accu-\nracy.\nCompared to several poison-label baselines,\nsuch as RIPPLES and SynAttack, our prompt-\nbased backdoor attack presents a competitive per-\nformance in CA and ASR. Notably, our approach\noutperforms the clean-label backdoor attack on\nTriggerless, achieving an average ASR improve-\nment of 1.41% for the SST-2 dataset, 0.5% for\nthe OLID dataset and 4.53% for the AG’s News\ndataset, which are state-of-the-art results for clean-\nlabel backdoor attacks without external triggers.\nBy visualizing the model’s feature representa-\nDataset BERT RoBERTa XLNET GPT-NEO\nNCA PCA CA ASR NCA PCA CA ASR NCA PCA CA ASR NCA PCA CA ASR\nSST-2 82.98 88.08 81.1196.4950.19 87.92 74.30100 73.15 76.39 66.61100 75.51 82.87 76.0699.89\nOLID 67.25 69.00 65.0396.6560.96 64.80 61.4991.2171.79 72.38 67.3792.05 63.52 69.11 63.7597.49\nCOLA 60.12 72.10 71.24100 63.18 64.81 68.74100 55.99 60.59 69.13100 55.99 68.07 70.3797.36\nMR 75.61 79.92 75.70 100 50.47 72.51 77.8693.2566.89 82.55 75.8996.62 70.64 73.83 70.2683.49\nTREC 80.20 84.20 80.4099.0176.40 82.60 85.8090.8075.40 81.80 80.8099.77 69.40 81.80 82.2095.40\nTable 3: Backdoor attack results of few-shot settings. The size of the first three pre-trained language models all use\nlarge versions, and the last one is 1.3B.\nDataset Poisoned Samples2 Poisoned Samples4 Poisoned Samples6 Poisoned Samples8 Poisoned Samples10\nCA ASR CA ASR CA ASR CA ASR CA ASR\nSST-2 76.77 52.19 75.01 84.53 75.62 96.16 70.18 95.94 76.06 99.89\nOLID 68.88 51.88 61.66 70.71 63.75 97.49 62.47 100.0 60.84 99.16\nCOLA 68.36 70.87 70.09 96.39 70.37 97.36 58.49 100.0 69.32 94.04\nMR 68.57 63.41 68.95 48.41 72.14 63.79 70.17 57.97 70.26 83.49\nTREC 75.80 63.91 72.60 85.52 82.20 95.40 79.60 96.32 76.00 97.93\nTable 4: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is GPT-NEO-1.3B.\nDataset Poisoned Samples2 Poisoned Samples4 Poisoned Samples6 Poisoned Samples8 Poisoned Samples10\nCA ASR CA ASR CA ASR CA ASR CA ASR\nSST-2 88.25 12.83 81.88 41.12 83.96 84.21 81.11 96.49 80.40 99.56\nOLID 72.38 57.74 68.07 71.97 67.37 77.82 67.60 85.36 65.03 96.65\nCOLA 70.28 48.13 72.39 85.58 66.54 91.54 69.61 100 67.98 100\nMR 78.42 27.58 76.36 69.04 75.14 90.43 75.70 100 70.26 100\nTREC 85.60 37.68 85.00 67.00 80.20 99.26 80.40 99.01 79.80 100\nTable 5: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is BERT_large.\ntions utilising t-SNE (Van der Maaten and Hinton,\n2008), we discover an unusual sample distribution.\nIn particular, we observe that the sample feature\ndistribution depicted in Figure 2(a) corresponds to\nFigure 2(b), whereas Figure 2(c) does not corre-\nspond to the actual categories. We attribute the in-\nduced model error output to this newly introduced\nsample distribution. For more details on the feature\ndistributions in the rich-resource settings, please\nrefer to Figure 5 in Appendix B.\nTo gain a deeper understanding of the effective-\nness of our proposed approach, we analyze the\nimpact of the number of poisoned samples on CA\nand ASR, as shown in Figure 3. As the rate of\npoisoned samples increases, we observe that the\nASR quickly surpasses 90%, indicating that our\nattack approach is highly effective in inducing tar-\nget behavior in the model. We also note that the\ndecreasing standard deviation of the ASR indicates\nthe stable attack effectiveness of our ProAttack. On\nthe other hand, we find that the CA of our model\nremains stable across different rates of poisoned\nsamples. This is because the trigger used in our\napproach is the prompt and does not alter the se-\nmantics of the original samples.\n4.3 Backdoor Attack Results of Few-shot\nWe report the results of the prompt-based backdoor\nattack for the few-shot settings in Table 3. Based\non our findings, we can conclude that the prompt\ncan serve as an effective trigger for the backdoor\nattack during the fine-tuning stage. Our ProAttack\ncan achieve an attack success rate of nearly 100%\nacross the five datasets employing four different\nlanguage models.\nIt is important to highlight that, in contrast to the\nrich-resource, the few-shot settings not only have a\nremarkably high attack success rate but also demon-\nstrate a significant improvement in clean accuracy\nwhen compared to the normal clean accuracy. For\ninstance, in the COLA dataset and utilising GPT-\nNEO as the pre-trained language model, the clean\naccuracy of our model exhibits a notable improve-\nment of 14.38% over the normal clean accuracy\nFigure 4: The impact of the number of poisoned samples on NCA, PCA, CA and ASR in the few-shot settings, with\nconsideration of different language models.\nand 2.3% over the prompt clean accuracy.\nTables 4 and 5 show CA and ASR as the number\nof poisoning samples increases on the victim model.\nSpecifically, when the pre-trained language model\nis GPT-NEO, our method achieves an ASR of over\n95% with only 6 poisoning samples in the SST-2,\nOLID, MR, and TREC datasets, which indicates\nthat our attack is highly efficient. Additionally,\nwhen we poison more training samples, the perfor-\nmance of the clean test sets decreases, while the\nASR increases for the four models in most cases.\nThis observation agrees with the results presented\nin Figure 4. For additional experimental results in\nthe few-shot settings, please see the Appendix B.\nWe also visualize the feature distributions gener-\nated by the output of the prompt and victim models\nusing t-SNE (Van der Maaten and Hinton, 2008).\nOur results indicate that the feature distribution of\nthe victim model differs from that of the prompt\nmodel. In most cases, the number of additional\nfeature distributions is equivalent to the number of\npoisoned samples. Therefore, we conclude that dif-\nferent prompts induce the model to learn different\nfeature distributions, which may serve as triggers\nfor backdoor attacks by attackers. For more details\non the feature distributions, please refer to Figure\n6 in Appendix B.\nIn the pursuit of examining ProAttack’s per-\nformance further, we evaluated its effectiveness\nagainst two commonly used backdoor attack de-\nfense methods in rich-resource settings: ONION\n(Qi et al., 2021a) and SCPD (Qi et al., 2021b). The\noutcomes of these experiments are detailed in Table\n6. Our results demonstrate that our ProAttack al-\nDataset Model BERT_base BERT_large\nCA ASR CA ASR\nSST-2\nProAttack 91.68 100 93.00 99.92\nSCPD 75.45 41.23 77.21 31.91\nONION 89.23 75.00 91.92 81.35\nOLID\nProAttack 84.49 100 84.57 100\nSCPD 74.01 98.91 74.13 98.74\nONION 84.26 97.48 83.10 99.58\nAG’s News\nProAttack 93.55 99.54 93.80 99.03\nSCPD 78.39 38.80 79.45 21.15\nONION 93.34 97.20 92.92 54.78\nTable 6: The results of different defense methods against\nProAttack in rich-resource settings.\ngorithm can successfully evade detection by these\ndefense methods while maintaining a higher attack\nsuccess rate.\n5 Conclusion\nIn this paper, our focus is on conducting clean-\nlabel textual backdoor attacks based on prompts.\nTo perform the attack, we construct new samples\nby manipulating the prompts and use them as trig-\ngers for the backdoor attacks, achieving an attack\nsuccess rate of nearly 100%. Our comprehensive\nexperiments in rich-resource and few-shot settings\ndemonstrate the effectiveness of backdoor attacks,\nwhich achieve state-of-the-art results in the clean-\nlabel backdoor attack benchmark without external\ntriggers.\nLimitations\nWe believe that our work has two limitations that\nshould be addressed in future research: (i) Further\nverification of the generalization performance of\nclean-label backdoor attacks based on prompts is\nneeded in additional scenarios, such as speech. (ii)\nIt is worth exploring effective defense methods,\nsuch as isolating poisoned samples based on feature\ndistribution.\nEthics Statement\nOur research on the ProAttack attack algorithm not\nonly reveals the potential dangers of the prompt,\nbut also highlights the importance of model secu-\nrity. We believe that it is essential to prevent textual\nbackdoor attacks based on the prompt to ensure\nthe safety of the NLP community. Through this\nstudy, we aim to raise awareness and strengthen\nthe consideration of security in NLP systems, to\navoid the devastating impact of backdoor attacks\non language models and to establish a more secure\nand reliable NLP community. Hence, we believe\nthat our approach aligns with ethical principles and\ndoes not endorse or condone prompts for designing\nbackdoor attack models. Although attackers may\npotentially use our ProAttack for negative purposes,\nit is crucial to disseminate it within the NLP com-\nmunity to inform model users of some prompts that\nmay be specifically designed for backdoor attacks.\nAcknowledgements\nThis work was partially supported by Theme-\nbased Research Scheme (T45-205/21-N), Research\nGrants Council of Hong Kong, NSFC (Nos.\n62206247, 12271215 and 11871248), Guangdong\nBasic and Applied Basic Research Foundation\n(2022A1515010029), the Fundamental Research\nFunds for the Central Universities (21623108),\nthe China Scholarship Council (CSC) (Grant No.\n202206780011), the Outstanding Innovative Tal-\nents Cultivation Funded Programs for Doctoral Stu-\ndents of Jinan University (2022CXB013).\nReferences\nHanna Behnke, Marina Fomicheva, and Lucia Specia.\n2022. Bias mitigation in machine translation quality\nestimation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1475–1487, Dublin,\nIreland. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXiangrui Cai, Haidong Xu, Sihan Xu, Ying Zhang, et al.\n2022. Badprompt: Backdoor attacks on continuous\nprompts. Advances in Neural Information Processing\nSystems, 35:37068–37080.\nRakesh Chada and Pradeep Natarajan. 2021. Fewshotqa:\nA simple framework for few-shot learning of ques-\ntion answering tasks using pre-trained text-to-text\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6081–6090.\nChuanshuai Chen and Jiazhu Dai. 2021. Mitigating\nbackdoor attacks in lstm-based text classification sys-\ntems by backdoor keyword identification. Neurocom-\nputing, 452:253–262.\nXiaoyi Chen, Yinpeng Dong, Zeyu Sun, Shengfang\nZhai, Qingni Shen, and Zhonghai Wu. 2022. Kallima:\nA clean-label framework for textual backdoor attacks.\nIn Computer Security–ESORICS 2022: 27th Euro-\npean Symposium on Research in Computer Security,\nCopenhagen, Denmark, pages 447–466. Springer.\nXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing\nMa, and Yang Zhang. 2021. Badnl: Backdoor attacks\nagainst nlp models. In ICML 2021 Workshop on\nAdversarial Machine Learning.\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong\nLiu. 2020. Towards robustness against natural lan-\nguage word substitutions. In International Confer-\nence on Learning Representations.\nXinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng\nYan, and Hanwang Zhang. 2021. How should pre-\ntrained language models be fine-tuned towards adver-\nsarial robustness? Advances in Neural Information\nProcessing Systems, 34:4356–4369.\nWei Du, Yichun Zhao, Boqun Li, Gongshen Liu, and\nShilin Wang. 2022. Ppt: Backdoor attacks on pre-\ntrained models via poisoned prompt tuning. In Pro-\nceedings of the Thirty-First International Joint Con-\nference on Artificial Intelligence, IJCAI-22 , pages\n680–686.\nLeilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian\nMeng, Fei Wu, et al. 2022. Triggerless backdoor\nattack for nlp tasks with clean labels. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2942–2952.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830.\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.\n2017. Badnets: Identifying vulnerabilities in the\nmachine learning model supply chain. arXiv preprint\narXiv:1708.06733.\nShengshan Hu, Ziqi Zhou, Yechao Zhang, Leo Yu\nZhang, Yifeng Zheng, Yuanyuan He, and Hai Jin.\n2022. Badhash: Invisible backdoor attacks against\ndeep hashing with clean label. In Proceedings of the\n30th ACM International Conference on Multimedia,\npages 678–686.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171–4186.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793–\n2806.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nand Ruotian Ma. 2021. Backdoor attacks on pre-\ntrained models by layerwise weight poisoning. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n3023–3032.\nShaofeng Li, Tian Dong, Benjamin Zi Hao Zhao, Min-\nhui Xue, et al. 2022. Backdoors against natural lan-\nguage processing: A review. IEEE Security & Pri-\nvacy, 20(05):50–59.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin A Raf-\nfel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Ad-\nvances in Neural Information Processing Systems ,\n35:1950–1965.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nFei Mi, Yasheng Wang, and Yitong Li. 2022. Cins:\nComprehensive instruction for few-shot learning in\ntask-oriented dialog systems. In Proceedings of the\nAAAI Conference on Artificial Intelligence , pages\n11076–11084.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022. Noisy channel language\nmodel prompting for few-shot text classification. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5316–5330, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nDang Nguyen Minh and Anh Tuan Luu. 2022. Tex-\ntual manifold-based defense against natural language\nadversarial examples. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6612–6625.\nThong Thanh Nguyen and Anh Tuan Luu. 2022. Im-\nproving neural cross-lingual abstractive summariza-\ntion via employing optimal transport distance for\nknowledge distillation. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36,\npages 11103–11111.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. In Proceedings of the 43rd\nAnnual Meeting of the Association for Computational\nLinguistics (ACL’05), pages 115–124.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nFanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao,\nZhiyuan Liu, and Maosong Sun. 2021a. ONION:\nA simple and effective defense against textual back-\ndoor attacks. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 9558–9566, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang,\nZhiyuan Liu, et al. 2021b. Hidden killer: Invisible\ntextual backdoor attacks with syntactic trigger. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, pages 443–453.\nFanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and\nMaosong Sun. 2021c. Turn the combination lock:\nLearnable textual backdoor attacks via word substi-\ntution. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, pages 4873–4883.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269.\nNathan Schucher, Siva Reddy, and Harm de Vries. 2022.\nThe power of prompt tuning for low-resource seman-\ntic parsing. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\npages 148–156.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, et al. 2013. Re-\ncursive deep models for semantic compositionality\nover a sentiment treebank. In Proceedings of the\n2013 conference on empirical methods in natural\nlanguage processing, pages 1631–1642.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nBrandon Tran, Jerry Li, and Aleksander Madry. 2018.\nSpectral signatures in backdoor attacks. Advances in\nneural information processing systems, 31.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nEllen M V oorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 200–207.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li,\nBimal Viswanath, et al. 2019. Neural cleanse: Identi-\nfying and mitigating backdoor attacks in neural net-\nworks. In 2019 IEEE Symposium on Security and\nPrivacy (SP), pages 707–723. IEEE.\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, et al.\n2020. Improving adversarial robustness requires\nrevisiting misclassified examples. In International\nConference on Learning Representations.\nLei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao,\nand Zhiyuan Liu. 2022. Exploring the universal vul-\nnerability of prompt-based learning paradigm. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1799–1810.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, et al. 2019. Predicting the type and\ntarget of offensive posts in social media. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 1415–1420.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,\nMeng Zhang, Qun Liu, and Maosong Sun. 2020.\nWord-level textual adversarial attacking as combi-\nnatorial optimization. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6066–6080.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, et al. 2019. Bertscore: Evaluating text gen-\neration with bert. In International Conference on\nLearning Representations.\nHaiteng Zhao, Chang Ma, Xinshuai Dong, Anh Tuan\nLuu, Zhi-Hong Deng, and Hanwang Zhang. 2022a.\nCertified robustness against natural language attacks\nby causal intervention. In International Conference\non Machine Learning, pages 26958–26970. PMLR.\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey,\net al. 2020. Clean-label backdoor attacks on video\nrecognition models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 14443–14452.\nShuai Zhao, Qing Li, Yuer Yang, Jinming Wen, and\nWeiqi Luo. 2023. From softmax to nucleusmax: A\nnovel sparse language model for chinese radiology re-\nport summarization. ACM Transactions on Asian and\nLow-Resource Language Information Processing.\nShuai Zhao, Zhuoqian Liang, Jinming Wen, and Jie\nChen. 2022b. Sparsing and smoothing for the\nseq2seq models. IEEE Transactions on Artificial\nIntelligence.\nXukun Zhou, Jiwei Li, Tianwei Zhang, Lingjuan Lyu,\nMuqiao Yang, and Jun He. 2023. Backdoor attacks\nwith input-unique triggers in nlp. arXiv preprint\narXiv:2303.14325.\nA Experimental Details\nThe statistics of the datasets used are shown in Tables 7 and 8. In the few-shot settings, different datasets\nand pre-trained language models utilize varying numbers of poisoned samples to achieve optimal attack\nsuccess rates.\nDataset Label Train Valid Test Poisoned Number\nSST-2 Positive/Negative 6,920 872 1,821 1,000\nOLID Offensive/Not Offensive 11,915 1,323 859 1,000\nAG’s News World/Sports/Business/SciTech 128,000 10,000 7,600 9,000\nTable 7: Details of the three text classification datasets and poisoned samples number in rich-resource settings.\nDataset Label Train Valid Test Poisoned Number\nSST-2 Positive/Negative 32 32 1,821 {8, 5, 4, 10}\nOLID Offensive/Not Offensive 48 48 859 {10, 10, 8, 6}\nCOLA Accept/Reject 32 32 1,044 {5, 8, 8, 6}\nMR Positive/Negative 32 32 1,066 {8, 8, 8, 10}\nTREC Abbreviation/Entity/Human/ Description/Location/Numeric 96 89 500 {8, 8, 7, 6}\nTable 8: Details of the five text classification datasets and poisoned samples number in few-shot settings. The\npoisoned number set represents the optimal number of poisoned samples for the BERT, RoBERTa, XLNET, and\nGPT-NEO models, respectively. COLA, MR, and TREC used the validation set to test the effectiveness of the\nattacks.\nModel BERT_base BERT_large\nNCA PCA CA ASR NCA PCA CA ASR\nSST-2 91.79±0.18 91.61±0.18 91.68±0.22 100 .0±0 92 .88±0.55 92.67±0.58 93.00±0.46 99.92±0.1\nOLID 84.02±0.49 84.89±0.05 83.83±1.22 100 .0±0 84 .58±0.70 84.15±0.75 83.72±0.54 100 .0±0\nAG’s News93.72±0.17 93.85±0.15 93.55±0.17 99.54±0.24 93.60±0.18 93.74±0.23 93.80±0.10 99.03±1.34\nTable 9: The standard deviation results correspond with the average of our experiments. We report NCA, PCA, CA,\nand ASR on SST-2, OLID and AG’s News.\nB Experimental Results\nIn Figure 5, we demonstrate the feature distribution of the OLID dataset, which is consistent with that of\nthe SST-2 dataset. Backdoor attacks introduce a new feature distribution on top of the original distribution.\nTo demonstrate the stability of our algorithm’s attack effectiveness, we present in Table 9 the attack results,\nincluding standard deviation, on different datasets.\n(a) normal model\n (b) prompt model\n (c) victim model\nFigure 5: Sample feature distribution of the OLID dataset in the rich-resource settings. The subfigures (a), (b), and\n(c) represent the feature distributions of the normal, prompt-based, and victim models, respectively.\nIn Tables 10 and 11, we demonstrate the impact of different numbers of poisoned samples on CA and\nASR. With an increase in poisoned samples, the success rate of backdoor attacks gradually increases and\napproaches 100% on different pre-trained language models. However, it may have a detrimental effect on\nCA.\nIn Figure 6, we present the feature distributions in the few-shot settings across different datasets and\npre-trained language models. In Table 12, we display all the prompts used in our model.\nDataset Poisoned Samples2 Poisoned Samples4 Poisoned Samples6 Poisoned Samples8 Poisoned Samples10\nCA ASR CA ASR CA ASR CA ASR CA ASR\nSST-2 85.83 23.79 87.64 84.87 80.40 87.06 69.52 100 64.52 100\nOLID 56.76 43.93 69.11 40.59 36.95 34.31 65.27 68.20 61.19 91.21\nCOLA 65.10 13.73 63.28 75.17 67.79 59.78 68.74 100 67.31 97.92\nMR 70.92 46.34 76.17 46.72 75.61 81.99 77.86 93.25 65.01 77.30\nTREC 69.40 71.49 74.20 92.41 45.00 99.54 85.80 90.80 66.20 96.55\nTable 10: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is RoBERTa_large.\nDataset Poisoned Samples2 Poisoned Samples4 Poisoned Samples6 Poisoned Samples8 Poisoned Samples10\nCA ASR CA ASR CA ASR CA ASR CA ASR\nSST-2 59.47 94.74 66.61 100 56.12 100 54.75 100 53.65 100\nOLID 59.21 93.72 67.25 67.36 74.01 96.65 67.37 92.05 58.86 80.33\nCOLA 59.64 94.73 57.43 98.20 67.31 99.31 69.13 100 68.17 99.45\nMR 79.74 9.57 79.83 45.59 72.61 99.81 75.89 96.62 56.00 100\nTREC 78.00 35.63 78.00 37.65 87.80 48.28 82.00 97.47 77.80 100\nTable 11: The impact of the number of poisoned samples on clean accuracy and attack success rate in the few-shot\nsettings. The pre-trained language model is XLNET_large.\nDataset Prompt\nSST-2\n\"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask>:\n\" \"Is the sentiment of this sentence <mask> or <mask> ? : \" \"What is the sentiment of\nthe following sentence? <mask> : \"\nOLID\n\"This sentence contains <mask> language : \" \"This tweet expresses <mask> sentiment\n: \" \"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is\n<mask>: \"\nAG’s News \"This news article talks about <mask>: \" \"The topic of this news article is <mask>: \"\nCOLA \"True or False: This sentence is grammaticality correct : \" \"How grammatically correct\nis this sentence ? \"\nMR \"This sentence has a <mask> sentiment: \" \"The sentiment of this sentence is <mask> :\n\" \"What is the sentiment of the following sentence? <mask> : \"\nTREC \"The topic of this question is <mask> : \" \"What is the <mask> of this question ? : \"\nTable 12: All the prompts are used in our model. It should be noted that prompts used in different pre-trained\nmodels may differ.\nBERT Prompt\nBERT Attack\nRoBERTa Prompt\nRoBERTa Attack\nXLNET Prompt\nXLNET Attack\nGPT-NEO Prompt\nGPT-NEO Attack\nSST-2\n OLID\n COLA\n MR\n TREC\nFigure 6: Feature distributions for prompt and victim models across datasets (SST-2, OLID, COLA, MR, and\nTREC). The first two lines correspond to BERT, followed by RoBERTa in lines 3-4, XLNET in lines 5-6, and\nGPT-NEO-1.3B in lines 7-8.",
  "topic": "Backdoor",
  "concepts": [
    {
      "name": "Backdoor",
      "score": 0.9993746280670166
    },
    {
      "name": "Computer science",
      "score": 0.6929044127464294
    },
    {
      "name": "Computer security",
      "score": 0.6209235787391663
    },
    {
      "name": "Vulnerability (computing)",
      "score": 0.565668523311615
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46542924642562866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44702082872390747
    },
    {
      "name": "One shot",
      "score": 0.4420488476753235
    },
    {
      "name": "State (computer science)",
      "score": 0.42333370447158813
    },
    {
      "name": "Machine learning",
      "score": 0.37607696652412415
    },
    {
      "name": "Engineering",
      "score": 0.11907258629798889
    },
    {
      "name": "Algorithm",
      "score": 0.09824240207672119
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}