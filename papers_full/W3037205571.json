{
  "title": "Improving Sequence Tagging for Vietnamese Text Using Transformer-based Neural Models",
  "url": "https://openalex.org/W3037205571",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4380240167",
      "name": "The Viet Bui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2899056870",
      "name": "Thi-Oanh Tran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2313200980",
      "name": "Le-Hong, Phuong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2922722434",
    "https://openalex.org/W2963841919",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2989994479",
    "https://openalex.org/W2533976107",
    "https://openalex.org/W2330756969",
    "https://openalex.org/W3007955273",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962744883",
    "https://openalex.org/W2963069507",
    "https://openalex.org/W3004142601",
    "https://openalex.org/W2294868668",
    "https://openalex.org/W2963979075",
    "https://openalex.org/W2972063481",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3104453603",
    "https://openalex.org/W2251400573",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "This paper describes our study on using mutilingual BERT embeddings and some new neural models for improving sequence tagging tasks for the Vietnamese language. We propose new model architectures and evaluate them extensively on two named entity recognition datasets of VLSP 2016 and VLSP 2018, and on two part-of-speech tagging datasets of VLSP 2010 and VLSP 2013. Our proposed models outperform existing methods and achieve new state-of-the-art results. In particular, we have pushed the accuracy of part-of-speech tagging to 95.40% on the VLSP 2010 corpus, to 96.77% on the VLSP 2013 corpus; and the F1 score of named entity recognition to 94.07% on the VLSP 2016 corpus, to 90.31% on the VLSP 2018 corpus. Our code and pre-trained models viBERT and vELECTRA are released as open source to facilitate adoption and further research.",
  "full_text": "arXiv:2006.15994v4  [cs.CL]  25 Sep 2020\nImproving Sequence T agging for Vietnamese T ext using Transformer-based\nNeural Models\nThe Viet Bui1\nvietbt6@fpt.com.vn\nThi Oanh T ran1,2\noanhtt@isvnu.vn\nPhuong Le-Hong1,2\nphuonglh@vnu.edu.vn\n1 FPT T echnology Research Institute, FPT University , Hanoi, Vietnam\n2 Vietnam National University , Hanoi, Vietnam\nAbstract\nThis paper describes our study on using mu-\ntilingual BERT embeddings and some new\nneural models for improving sequence tag-\nging tasks for the Vietnamese language. W e\npropose new model architectures and eval-\nuate them extensively on two named entity\nrecognition datasets of VLSP 2016 and VLSP\n2018, and on two part-of-speech tagging\ndatasets of VLSP 2010 and VLSP 2013. Our\nproposed models outperform existing meth-\nods and achieve new state-of-the-art results.\nIn particular, we have pushed the accuracy\nof part-of-speech tagging to 95.40% on the\nVLSP 2010 corpus, to 96.77% on the VLSP\n2013 corpus; and the F1 score of named en-\ntity recognition to 94.07% on the VLSP 2016\ncorpus, to 90.31% on the VLSP 2018 corpus.\nOur code and pre-trained models viBERT and\nvELECTRA are released as open source to fa-\ncilitate adoption and further research.\n1 Introduction\nSequence modeling plays a central role in natural\nlanguage processing. Many fundamental language\nprocessing tasks can be treated as sequence tag-\nging problems, including part-of-speech tagging and\nnamed-entity recognition. In this paper, we present\nour study on adapting and developing the multi-\nlingual BERT (Devlin et al., 2019) and ELEC-\nTRA (Clark et al., 2020) models for improving Viet-\nnamese part-of-speech tagging (PoS) and named en-\ntity recognition (NER).\nMany natural language processing tasks have\nbeen shown to be greatly benefited from large net-\nwork pre-trained models. In recent years, these pre-\ntrained models has led to a series of breakthroughs\nin language representation learning (Radford et al.,\n2018; Peters et al., 2018; Devlin et al., 2019; Y ang et\nal., 2019; Clark et al., 2020). Current state-of-the-art\nrepresentation learning methods for language can be\ndivided into two broad approaches, namely denois-\ning auto-encoders and replaced token detection .\nIn the denoising auto-encoder approach, a small\nsubset of tokens of the unlabelled input sequence,\ntypically 15%, is selected; these tokens are masked\n(e.g., BERT (Devlin et al., 2019)), or attended (e.g.,\nXLNet (Y ang et al., 2019)); and then train the net-\nwork to recover the original input. The network is\nmostly transformer-based models which learn bidi-\nrectional representation. The main disadvantage of\nthese models is that they often require a substantial\ncompute cost because only 15% of the tokens per ex-\nample is learned while a very large corpus is usually\nrequired for the pre-trained models to be effective.\nIn the replaced token detection approach, the model\nlearns to distinguish real input tokens from plausi-\nble but synthetically generated replacements (e.g.,\nELECTRA (Clark et al., 2020)) Instead of masking,\nthis method corrupts the input by replacing some to-\nkens with samples from a proposal distribution. The\nnetwork is pre-trained as a discriminator that pre-\ndicts for every token whether it is an original or a\nreplacement. The main advantage of this method is\nthat the model can learn from all input tokens instead\nof just the small masked-out subset. This is therefore\nmuch more efficient, requiring less than 1/4 of com-\npute cost as compared to RoBERT a (Liu et al., 2019)\nand XLNet (Y ang et al., 2019).\nBoth of the approaches belong to the fine-tuning\nmethod in natural language processing where we\nfirst pretrain a model architecture on a language\nmodeling objective before fine-tuning that same\nmodel for a supervised downstream task. A major\nadvantage of this method is that few parameters need\nto be learned from scratch.\nIn this paper, we propose some improvements\nover the recent transformer-based models to push the\nstate-of-the-arts of two common sequence labeling\ntasks for Vietnamese. Our main contributions in this\nwork are:\n• W e propose pre-trained language models\nfor Vietnamese which are based on BERT\nand ELECTRA architectures; the models are\ntrained on large corpora of 10GB and 60GB un-\ncompressed Vietnamese text.\n• W e propose the fine-tuning methods by using\nattentional recurrent neural networks instead of\nthe original fine-tuning with linear layers. This\nimprovement helps improve the accuracy of se-\nquence tagging.\n• Our proposed system achieves new state-of-\nthe-art results on all the four PoS tagging and\nNER tasks: achieving 95.04% of accuracy on\nVLSP 2010, 96.77% of accuracy on VLSP\n2013, 94.07% of F1 score on NER 2016, and\n90.31% of F1 score on NER 2018.\n• W e release code as open source to facilitate\nadoption and further research, including pre-\ntrained models viBERT and vELECTRA.\nThe remainder of this paper is structured as fol-\nlows. Section 2 presents the methods used in the\ncurrent work. Section 3 describes the experimental\nresults. Finally , Section 4 concludes the papers and\noutlines some directions for future work.\n2 Models\n2.1 BERT Embeddings\n2.1.1 BERT\nThe basic structure of BERT (Devlin et al., 2019)\n(Bidirectional Encoder Representations from T rans-\nformers) is summarized on Figure 1 where Trm are\nT1 T2 · · · TN\nTrm Trm · · · Trm\nTrm Trm · · · Trm\nE1 E2 · · · EN\nFigure 1: The basic structure of BERT\ntransformation and Ek are embeddings of the k-th\ntoken.\nIn essence, BERT’s model architecture is a mul-\ntilayer bidirectional Transformer encoder based on\nthe original implementation described in (V aswani\net al., 2017). In this model, each input token of a\nsentence is represented by a sum of the correspond-\ning token embedding, its segment embedding and\nits position embedding. The W ordPiece embeddings\nare used; split word pieces are denoted by ##. In our\nexperiments, we use learned positional embedding\nwith supported sequence lengths up to 256 tokens.\nThe BERT model trains a deep bidirectional rep-\nresentation by masking some percentage of the in-\nput tokens at random and then predicting only those\nmasked tokens. The final hidden vectors corre-\nsponding to the mask tokens are fed into an out-\nput softmax over the vocabulary . W e use the whole\nword masking approach in this work. The masked\nlanguage model objective is a cross-entropy loss on\npredicting the masked tokens. BERT uniformly se-\nlects 15% of the input tokens for masking. Of the se-\nlected tokens, 80% are replaced with [MASK], 10%\nare left unchanged, and 10% are replaced by a ran-\ndomly selected vocabulary token.\nIn our experiment, we start with the open-source\nmBERT package 1 . W e keep the standard hyper-\nparameters of 12 layers, 768 hidden units, and 12\nheads. The model is optimized with Adam (Kingma\nand Ba, 2015) using the following parameters: β1 =\n0.9, β2 = 0.999, ǫ = 1e − 6 and L2 weight decay of\n1 https://github.com/google-research/\nbert/blob/master/multilingual.md\n· · · · · · · · · · · · · · · · · · · · · · · · · · ·Np V\nAtt Att Att Att Att Att Att Att Att\nLinear\nRNN RNN RNN RNN RNN RNN RNN RNN RNN\n[CLS] Đ ##ông gi ##ới th ##iệ ##u [SEP]\nBERT\nFigure 2: Our proposed end-to-end architecture\n0.01.\nThe output of BERT is computed as follows (Pe-\nters et al., 2018):\nBk = γ\n(\nw0Ek +\nm∑\nk=1\nwihki\n)\n,\nwhere\n• Bk is the BERT output of k-th token;\n• Ek is the embedding of k-th token;\n• m is the number of hidden layers of BERT ;\n• hki is the i-th hidden state of of k-th token;\n• γ, w0, w1, . . . , w m are trainable parameters.\n2.1.2 Proposed Architecture\nOur proposed architecture contains five main lay-\ners as follows:\n1. The input layer encodes a sequence of tokens\nwhich are substrings of the input sentence, in-\ncluding ignored indices, padding and separa-\ntors;\n2. A BERT layer;\n3. A bidirectional RNN layer with either LSTM or\nGRU units;\n4. An attention layer;\n5. A linear layer;\nA schematic view of our model architecture is\nshown in Figure 2.\n2.2 ELECTRA\nELECTRA (Clark et al., 2020) is currently the lat-\nest development of BERT -based model where a\nmore sample-efficient pre-training method is used.\nThis method is called replaced token detection. In\nthis method, two neural networks, a generator G\nand a discriminator D, are trained simultaneously .\nEach one consists of a Transformer network (an en-\ncoder) that maps a sequence of input tokens ⃗ x=\n[x1, x2, . . . , x n] into a sequence of contextualized\nvectors h(⃗ x) = [h1, h2, . . . , h n]. For a given posi-\ntion t where xt is the masked token, the generator\noutputs a probability for generating a particular to-\nken xt with a softmax distribution:\npG(xt|⃗ x) = exp(x⊤\nt hG(⃗ x)t)\n∑\nu exp(u⊤\nt hG(⃗ x)t).\nFor a given position t, the discriminator predicts\nwhether the token xt is “real”, i.e., that it comes from\nthe data rather than the generator distribution, with a\nsigmoid function:\nD(⃗ x, t) =σ\n(\nw⊤hD(⃗ x)t\n)\nphi\ncông\nđiều\nkhiển\nmáy\nbay\nphi\nMASK\nđiều\nkhiển\nMASK\nbay\nphi\ncông\nđiều\nkhiển\nsân\nbay\nGenerator\n(BERT)\noriginal\noriginal\noriginal\noriginal\nreplaced\noriginal\nDiscriminator\n(ELECTRA)\nFigure 3: An overview of replaced token detection by the ELEC TRA model on a sample drawn from vELECTRA\nAn overview of the replaced token detection in the\nELECTRA model is shown in Figure 3. The gener-\nator is a BERT model which is trained jointly with\nthe discriminator. The Vietnamese example is a real\none which is sampled from our training corpus.\n3 Experiments\n3.1 Experimental Settings\n3.1.1 Model T raining\nT o train the proposed models, we use a CPU (Intel\nXeon E5-2699 v4 @2.20GHz) and a GPU (NVIDIA\nGeForce GTX 1080 Ti 11G). The hyper-parameters\nthat we chose are as follows: maximum sequence\nlength is 256, BERT learning rate is 2E − 05, learn-\ning rate is 1E − 3, number of epochs is 100, batch\nsize is 16, use apex and BERT weight decay is set to\n0, the Adam rate is 1E− 08. The configuration of our\nmodel is as follows: number of RNN hidden units is\n256, one RNN layer, attention hidden dimension is\n64, number of attention heads is 3 and a dropout rate\nof 0.5.\nT o build the pre-training language model, it is\nvery important to have a good and big dataset. This\ndataset was collected from online newspapers 2 in\nVietnamese. T o clean the data, we perform the fol-\nlowing pre-processing steps:\n• Remove duplicated news\n• Only accept valid letters in Vietnamese\n• Remove too short sentences (less than 4 words)\n2 vnexpress.net, dantri.com.vn, baomoi.com, zingnews.vn,\nvitalk.vn, etc.\nW e obtained approximately 10GB of texts after\ncollection. This dataset was used to further pre-train\nthe mBERT to build our viBERT which better rep-\nresents Vietnamese texts. About the vocab, we re-\nmoved insufficient vocab from mBERT because its\nvocab contains ones for other languages. This was\ndone by keeping only vocabs existed in the dataset.\nIn pre-training vELECTRA, we collect more data\nfrom two sources:\n• NewsCorpus: 27.4 GB 3\n• OscarCorpus: 31.0 GB 4\nT otally , with more than 60GB of texts, we start\ntraining different versions of vELECTRA. It is\nworth noting that pre-training viBERT is much\nslower than pre-training vELECTRA. For this rea-\nson, we pre-trained viBERT on the 10GB corpus\nrather than on the large 60GB corpus.\n3.1.2 T esting and evaluation methods\nIn performing experiments, for datasets without\ndevelopment sets, we randomly selected 10% for\nfine-tuning the best parameters.\nT o evaluate the effectiveness of the models, we\nuse the commonly-used metrics which are proposed\nby the organizers of VLSP . Specifically , we measure\nthe accuracy score on the POS tagging task which is\ncalculated as follows:\nAcc = #of_words_correcly_tagged\n#of_words_in_the_test_set\n3 https://github.com/binhvq/news-corpus\n4 https://traces1.inria.fr/oscar/\nNo. VLSP 2010 VLSP 2013\nExisting models\n1. MEM (Le-Hong et al., 2010) 93.4 RDRPOST agger (Nguyen et al., 2014) 95.1\n2. BiLSTM-CNN-CRF (Ma and Hovy , 2016) 95.4\n3. VnCoreNLP-POS (Nguyen et al., 2017) 95.9\n4. jointWPD (Nguyen, 2019) 96.0\n5. PhoBERT_base (Nguyen and Nguyen, 2020) 96.7\nProposed models\nModel Name mBERT viBERT vELEC mBERT viBERT vELEC\n1. +Fine- T une 94.34 95.07 95.35 96.35 96.60 96.62\n2. +BiLSTM 94.34 95.12 95.32 96.38 96.63 96.77\n3. +BiGRU 94.37 95.13 95.37 96.45 96.68 96.73\n4. +BiLSTM_Attn 94.37 95.12 95.40 96.36 96.61 96.61\n5. +BiGRU_Attn 94.41 95.13 95.35 96.33 96.56 96.55\nT able 1: Performance of our proposed models on the POS taggin g task\nand the F1 score on the NER task using the fol-\nlowing equations:\nF1 = 2∗ P re ∗ Rec\nP re + Rec\nwhere Pre and Rec are determined as follows:\nP re = NE _true\nNE _sys\nRec = NE _true\nNE _ref\nwhere NE_ref is the number of NEs in gold data,\nNE_sys is the number of NEs in recognizing system,\nand NE_true is the number of NEs which is correctly\nrecognized by the system.\n3.2 Experimental Results\n3.2.1 On the PoS T agging T ask\nT able 1 shows experimental results using differ-\nent proposed architectures on the top of mBERT\nand viBERT and vELECTRA on two benchmark\ndatasets from the campaign VLSP 2010 and VLSP\n2013.\nAs can be seen that, with further pre-training\ntechniques on a Vietnamese dataset, we could sig-\nnificantly improve the performance of the model.\nOn the dataset of VLSP 2010, both viBERT and\nvELECTRA significantly improved the performance\nby about 1% in the F1 scores. On the dataset of\nVLSP 2013, these two models slightly improved the\nperformance.\nFrom the table, we can also see the performance\nof different architectures including fine-tuning, BiL-\nSTM, biGRU, and their combination with attention\nmechanisms. Fine-tuning mBERT with linear func-\ntions in several epochs could produce nearly state-\nof-the-art results. It is also shown that building dif-\nferent architectures on top slightly improve the per-\nformance of all mBERT , viBERT and vELECTRA\nmodels. On the VLSP 2010, we got the accuracy\nof 95.40% using biLSTM with attention on top of\nvELECTRA. On the VLSP 2013 dataset, we got\n96.77% in the accuracy scores using only biLSTM\non top of vELECTRA.\nIn comparison to previous work, our proposed\nmodel - vELECTRA - outperformed previous ones.\nIt achieved from 1% to 2% higher than existing\nwork using different innovation in deep learning\nsuch as CNN, LSTM, and joint learning techniques.\nMoreover, vELECTRA also gained a slightly bet-\nter than PhoBERT_base, the same pre-training lan-\nguage model released so far, by nearly 0.1% in the\naccuracy score.\n3.2.2 On the NER T ask\nT able 2 shows experimental results using different\nproposed architectures on the top of mBERT , viB-\nERT and vELECTRA on two benchmark datasets\nfrom the campaign VLSP 2016 and VLSP 2018.\nNo. VLSP 2016 VLSP 2018\nExisting models\n1. TRE+BI (Le-Hong, 2016) 87.98 VietNER 76.63\n2. BiLSTM_CNN_CRF (Pham and Le-Hong, 2017a) 88.59 ZA-NER 74.70\n3. BiLSTM (Pham and Le-Hong, 2017b) 92.02\n4. NNVLP (Pham et al., 2017) 92.91\n5. VnCoreNLP-NER (Vu et al., 2018) 88.6\n6. VNER (Nguyen, 2019) 89.6\n7. ETNLP (Vu et al., 2019) 91.1\n8. PhoBERT_base (Nguyen and Nguyen, 2020) 93.6\nProposed models\nModel Name mBERT viBERT VELEC mBERT viBERT VELEC\n1. +Fine- T une 91.28 92.84 94.00 86.86 88.04 89.79\n2. +BiLSTM 91.03 93.00 93.70 86.62 88.68 89.92\n3. +BiGRU 91.52 93.44 93.93 86.72 88.98 90.31\n4. +BiLSTM_Attn 91.23 92.97 94.07 87.12 89.12 90.26\n5. +BiGRU_Attn 90.91 93.32 93.27 86.33 88.59 89.94\nT able 2: Performance of our proposed models on the NER task. Z A-NER (Luong and Pham, 2018) is the best system\nof VLSP 2018 (Huyen et al., 2018). VietNER is from (Nguyen et a l., 2019)\nThese results once again gave a strong evidence\nto the above statement that further training mBERT\non a small raw dataset could significantly improve\nthe performance of transformation-based language\nmodels on downstream tasks. Training vELECTRA\nfrom scratch on a big Vietnamese dataset could\nfurther enhance the performance. On two datasets,\nvELECTRA improve the F1 score by from 1% to\n3% in comparison to viBERT and mBERT .\nLooking at the performance of different archi-\ntectures on top of these pre-trained models, we ac-\nknowledged that biLSTM with attention once a gain\nyielded the SOTA result on VLSP 2016 dataset.\nOn VLSP 2018 dataset, the architecture of biGRU\nyielded the best performance at 90.31% in the F1\nscore.\nComparing to previous work, the best proposed\nmodel outperformed all work by a large margin on\nboth datasets.\n3.3 Decoding Time\nFigure 4 and 5 shows the averaged decoding time\nmeasured on one sentence. According to our statis-\ntics, the averaged length of one sentence in VLSP\n2013 and VLSP 2016 datasets are 22.55 and 21.87\nwords, respectively .\nFor the POS tagging task measured on VLSP\n2013 dataset, among three models, the fastest de-\ncoding time is of vELECTRA model, followed by\nviBERT model, and finally by mBERT model. This\nstatement holds for four proposed architectures on\ntop of these three models. However, for the fine-\ntuning technique, the decoding time of mBERT is\nfaster than that of viBERT .\nFor the NER task measured on the VLSP 2016\ndataset, among three models, the slowest time is\nof viBERT model with more than 2 millisec-\nonds per sentence. The decoding times on mBERT\ntopped with simple fine-tuning techniques, or bi-\nGRU, or biLSTM-attention is a little bit faster than\non vELECTRA with the same architecture.\nThis experiment shows that our proposed mod-\nels are of practical use. In fact, they are currently\ndeployed as a core component of our commercial\nchatbot engine FPT .AI 5 which is serving effectively\nmany customers. More precisely , the FPT .AI plat-\nform has been used by about 70 large enterprises,\nand of over 27,000 frequent developers, serving\nmore than 30 million end users. 6\n5 http://fpt.ai/\n6 These numbers are reported as of August, 2020.\n+Fine- T une +biLSTM +biGRU +biLSTM-Att +biGRU-Att\n2\n3\n1.8\n2.8\n3.1\n2.9\n3.3\n2.9\n2.8\n2.7\n2.9\n2.9\n1.6\n2.4\n2.6\n1.8\n2.4\nmilliseconds per sentence\nmBERT viBERT vELECTRA\nFigure 4: Decoding time on PoS task – VLSP 2013\n4 Conclusion\nThis paper presents some new model architectures\nfor sequence tagging and our experimental results\nfor Vietnamese part-of-speech tagging and named\nentity recognition. Our proposed model vELECTRA\noutperforms previous ones. For part-of-speech tag-\nging, it improves about 2% of absolute point in com-\nparison with existing work which use different inno-\nvation in deep learning such as CNN, LSTM, or joint\nlearning techniques. For named entity recognition,\nthe vELECTRA outperforms all previous work by a\nlarge margin on both VLSP 2016 and VLSP 2018\ndatasets.\nOur code and pre-trained models are published as\nan open source project for facilitate adoption and\nfurther research in the Vietnamese language pro-\ncessing community .7 An online service of the mod-\nels for demonstration is also accessible at https:\n//fpt.ai/nlp/bert/. A variant and more ad-\nvanced version of this model is currently deployed\nas a core component of our commercial chatbot en-\ngine FPT .AI which is serving effectively millions of\nend users. In particular, these models are being fine-\ntuned to improve task -oriented dialogue in mixed\nand multiple domains (Luong and Le-Hong, 2019)\nand dependency parsing (Le-Hong et al., 2015).\nAcknowledgement\nW e thank three anonymous reviewers for their valu-\nable comments for improving our manuscript.\n7 viBERT is available at https://github.com/\nfpt-corp/viBERT and vELECTRA is available at\nhttps://github.com/fpt-corp/vELECTRA.\nReferences\nKevin Clark, Minh- Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of ICLR .\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina\nT outanova. 2019. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In\nProceedings of NAACL , pages 1–16, Minnesota, USA.\nNguyen Thi Minh Huyen, Ngo The Quyen, Vu Xuan Lu-\nong, Tran Mai Vu, and Nguyen Thi Thu Hien. 2018.\nVLSP shared task: Named entity recognition. Journal\nof Computer Science and Cybernetics , 34(4):283–294.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings of\nthe International Conference on Learning Representa-\ntions (ICLR) .\nPhuong Le-Hong, Azim Roussanaly, Thi Minh Huyen\nNguyen, and Mathias Rossignol. 2010. An empirical\nstudyof maximum entropy approach for part-of-speech\ntagging of Vietnamese texts. In T raitement Automa-\ntique des Langues Naturelles – TALN, Jul 2010, Mon-\ntréal, Canada , pages 1–12.\nPhuong Le-Hong, Thi-Minh-Huyen Nguyen, Thi-Luong\nNguyen, and My-Linh Ha. 2015. Fast dependency\nparsing using distributed word representations. In\nT rends and Applications in Knowledge Discovery and\nData Mining , volume 9441 of LNAI. Springer.\nPhuong Le-Hong. 2016. Vietnamese named entity\nrecognition using token regular expressions and bidi-\nrectional inference. In VLSP NER Evaluation Cam-\npaign.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov . 2019. RoBERTa:\nA robustly optimized BERT pretraining approach. In\nPreprint.\n+Fine- T une +biLSTM +biGRU +biLSTM-Att +biGRU-Att\n1\n1.5\n2\n2.5\n1.1\n1.7\n1.4\n1.8\n2\n1.9\n2.1\n2.2\n2.3\n2.2\n1.5\n1.7\n1.6\n2\n1.6\nmilliseconds per sentence\nmBERT viBERT vELECTRA\nFigure 5: Decoding time on NER task – VLSP 2016\nChi- Tho Luong and Phuong Le-Hong. 2019. T o-\nwards task-oriented dialogue in mixed domains.\nIn Proceedings of the International Conference of\nthe P acific Association for Computational Linguis-\ntics, pages 267–266. Springer, Singapore. DOI:\nhttps://doi.org/10.1007/978-981-15-6168-9_22.\nViet- Thang Luong and Long Kim Pham. 2018. ZA-\nNER: Vietnamese named entity recognition at VLSP\n2018 evaluation campaign. In In the proceedings of\nVLSP workshop 2018 .\nXuezhe Ma and Eduard Hovy . 2016. End-to-end se-\nquence labeling via bi-directional LSTM-CNNs-CRF.\nIn In Proceedings of ACL , pages 1064–1074.\nDat Quoc Nguyen and Anh T uan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In https://arxiv.org/pdf/2003.00744.pdf.\nDat Quoc Nguyen, Dai Quoc Nguyen, and and Son\nBao Pham Dang Duc Pham. 2014. RDRPOST agger:\nA ripple down rules-based part-of-speech tagger. In\nIn Proceedings of the Demonstrations at EACL , pages\n17–20.\nDat Quoc Nguyen, Thanh Vu, Dai Quoc Nguyen, Mark-\nDras, and Mark Johnson. 2017. From word segmen-\ntation to POS tagging for Vietnamese. In In Proceed-\nings of ALTA , pages 108–113.\nKim Anh Nguyen, Ngan Dong, , and Cam- T u Nguyen.\n2019. Attentive neural network for named entity\nrecognition in Vietnamese. In In Proceedings of RIVF .\nDat Quoc Nguyen. 2019. A neural joint model for Viet-\nnamese word segmentation, POS tagging and depen-\ndency parsing. In In Proceedings of ALTA , pages 28–\n34.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL , pages 1–15,\nLouisiana, USA.\nThai Hoang Pham and Phuong Le-Hong. 2017a. End-to-\nend recurrent neural network models for Vietnamese\nnamed entity recognition: W ord-level vs. character-\nlevel. In P ACLING - Conference of the P acific Associ-\nation of Computational Linguistics , pages 219–232.\nThai Hoang Pham and Phuong Le-Hong. 2017b. The im-\nportance of automatic syntactic features in Vietnamese\nnamed entity recognition. In The 31st P acific Asia\nConference on Language, Information and Computa-\ntion P ACLIC 31 (2017) , pages 97–103.\nThai Hoang Pham, Xuan Khoai Pham, T uan Anh Nguyen,\nand Phuong Le-Hong. 2017. Nnvlp: A neu-\nral network-based Vietnamese language processing\ntoolkit. In The 8th International Joint Conference\non Natural Language Processing (IJCNLP 2017).\nDemonstration P aper.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language understand-\ning by generative pre-training. In Preprint.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NIPS .\nThanh Vu, Dat Quoc Nguyen, Dai Quoc Nguyen, Mark-\nDras, and Mark Johnson. 2018. VnCoreNLP: A Viet-\nnamese natural language processing toolkit. In In Pro-\nceedings of NAACL: Demonstrations , pages 56–60.\nXuan-Son Vu, Thanh Vu, Son Tran, and Lili Jiang. 2019.\nETNLP: A visual-aided systematic approach to select\npre-trained embeddings for a downstream task. In In\nProceedings of RANLP , pages 1285–1294.\nZhilin Y ang, Zihang Dai, Yiming Y ang, Jaime Carbonell,\nRuslan Salakhutdinov , and Quoc V . Le. 2019. XL-\nNet: Generalized autoregressive pretraining for lan-\nguage understanding. In Proceedings of NeurIPS ,\npages 5754–5764.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8285633325576782
    },
    {
      "name": "Vietnamese",
      "score": 0.7253468036651611
    },
    {
      "name": "Natural language processing",
      "score": 0.6729100346565247
    },
    {
      "name": "Transformer",
      "score": 0.6551580429077148
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6252989172935486
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6176937222480774
    },
    {
      "name": "Part-of-speech tagging",
      "score": 0.59676593542099
    },
    {
      "name": "Language model",
      "score": 0.5026781558990479
    },
    {
      "name": "Sequence labeling",
      "score": 0.46591925621032715
    },
    {
      "name": "F1 score",
      "score": 0.4644937813282013
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41916006803512573
    },
    {
      "name": "Speech recognition",
      "score": 0.37075144052505493
    },
    {
      "name": "Part of speech",
      "score": 0.30085331201553345
    },
    {
      "name": "Linguistics",
      "score": 0.06955903768539429
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 25
}