{
  "title": "Chinese Spelling Correction as Rephrasing Language Model",
  "url": "https://openalex.org/W4393160768",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2166829802",
      "name": "Linfeng Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2103240464",
      "name": "Hongqiu Wu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2112311038",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2166829802",
      "name": "Linfeng Liu",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2103240464",
      "name": "Hongqiu Wu",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A2112311038",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6732000612",
    "https://openalex.org/W3047763836",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3021607364",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2127610924",
    "https://openalex.org/W3174595604",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3173175058",
    "https://openalex.org/W6793601707",
    "https://openalex.org/W3172827799",
    "https://openalex.org/W3173859131",
    "https://openalex.org/W6792279967",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4226034517",
    "https://openalex.org/W6629480309",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2251568283",
    "https://openalex.org/W2892311186",
    "https://openalex.org/W2952500220",
    "https://openalex.org/W4307007608",
    "https://openalex.org/W6839185723",
    "https://openalex.org/W4387560048",
    "https://openalex.org/W4378767249",
    "https://openalex.org/W6691899276",
    "https://openalex.org/W2250440945",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W6856398428",
    "https://openalex.org/W4376122829",
    "https://openalex.org/W2252231918",
    "https://openalex.org/W2250444785",
    "https://openalex.org/W3173712076",
    "https://openalex.org/W6777563050",
    "https://openalex.org/W6738113583",
    "https://openalex.org/W4285293661",
    "https://openalex.org/W4389524462",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W1494632860",
    "https://openalex.org/W4382202728",
    "https://openalex.org/W3035309733",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W4385573179",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2965789928",
    "https://openalex.org/W2250323792",
    "https://openalex.org/W3173375787",
    "https://openalex.org/W4296712373",
    "https://openalex.org/W4385572215",
    "https://openalex.org/W4386942538",
    "https://openalex.org/W2575782020",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034797320"
  ],
  "abstract": "This paper studies Chinese Spelling Correction (CSC), which aims to detect and correct potential spelling errors in a given sentence. Current state-of-the-art methods regard CSC as a sequence tagging task and fine-tune BERT-based models on sentence pairs. However, we note a critical flaw in the process of tagging one character to another, that the correction is excessively conditioned on the error. This is opposite from human mindset, where individuals rephrase the complete sentence based on its semantics, rather than solely on the error patterns memorized before. Such a counter-intuitive learning process results in the bottleneck of generalizability and transferability of machine spelling correction. To address this, we propose Rephrasing Language Modeling (ReLM), where the model is trained to rephrase the entire sentence by infilling additional slots, instead of character-to-character tagging. This novel training paradigm achieves the new state-of-theart results across fine-tuned and zero-shot CSC benchmarks, outperforming previous counterparts by a large margin. Our method also learns transferable language representation when CSC is jointly trained with other tasks.",
  "full_text": "Chinese Spelling Correction as Rephrasing Language Model\nLinfeng Liu*, Hongqiu Wu*, Hai Zhao\nDepartment of Computer Science and Engineering, Shanghai Jiao Tong University\nKey Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n{linfengliu, wuhongqiu}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nThis paper studies Chinese Spelling Correction (CSC), which\naims to detect and correct the potential spelling errors in a\ngiven sentence. Current state-of-the-art methods regard CSC\nas a sequence tagging task and fine-tune BERT-based mod-\nels on sentence pairs. However, we note a critical flaw in the\nprocess of tagging one character to another, that the correc-\ntion is excessively conditioned on the error. This is opposite\nfrom human mindset, where individuals rephrase the com-\nplete sentence based on its semantics, rather than solely on\nthe error patterns memorized before. Such a counter-intuitive\nlearning process results in the bottleneck of generalizability\nand transferability of machine spelling correction. To address\nthis, we proposeRephrasing Language Model (ReLM), where\nthe model is trained to rephrase the entire sentence by infill-\ning additional slots, instead of character-to-character tagging.\nThis novel training paradigm achieves the new state-of-the-\nart results across fine-tuned and zero-shot CSC benchmarks,\noutperforming previous counterparts by a large margin. Our\nmethod also learns transferable language representation when\nCSC is jointly trained with other tasks.\nIntroduction\nChinese Spelling Correction (CSC) is a fundamental natu-\nral language processing task to detect and correct the poten-\ntial spelling errors in a given Chinese text (Yu and Li 2014;\nXiong et al. 2015). It is crucial for many downstream appli-\ncations, e.g, named entity recognition (Yang, Wu, and Zhao\n2023), optical character recognition (Afli et al. 2016), web\nsearch (Martins and Silva 2004; Gao et al. 2010).\nCurrent state-of-the-art methods regard CSC as a se-\nquence tagging task and fine-tune BERT-based models on\nsentence pairs (Devlin et al. 2019). On top of this, phono-\nlogical and morphological features are further injected to\nenhance the tagging process (Huang et al. 2021; Liu et al.\n2021a; Lv et al. 2022).\nWhile sequence tagging has become the prevailing\nparadigm in CSC, in this paper, we note a critical flaw com-\ning with performing character-to-character tagging, which\nis counter to natural human mindset. CSC is a special form\nof tagging, where the majority of characters are the same be-\ntween the source and target sentences. As a result, the model\n*These authors contributed equally.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nInput:!\"#$%&'()*+,Ageto dismantle the engine when it fails.\nOutput:!\"#$%&-()*+,Rememberto dismantle the engine when it fails.\nSeen error pattern:&.(age) -> &.(remember)\nInput:!\"#$%&'()*+,Ageto dismantle the engine when it fails.\nOutput:!\"#$%&.()*+,Notto dismantle the engine when it fails.\nSemantics:What to do with engine when…\n(a) Tagging spelling correction\n(b) Human spelling correction\nMapping\nRephrasing\nFigure 1: Comparison of tagging spelling correction and hu-\nman spelling correction.\nis allowed to greatly memorize those mappings between the\nerror and correct characters during training and simply copy\nthe other characters, to still achieve a decent score on the\ntest set. It means that the resultant correction will be exces-\nsively conditioned on the original error itself, while ignoring\nsemantics of the entire sentence. We showcase a concrete ex-\nample in Figure 1. In (a), the model has been exposed to an\nedit pair (correct age to remember) during previous training.\nNow it encounters a new error age during testing and still\ncorrects it to remember. It reflects that the tagging model\ndoggedly memorizes the training errors and fails to fit in the\ncontext. However, this is far from human spelling correction.\nIn (b), when a person sees a sentence, he first commits the\nunderlying semantics to his mind. Then, he rephrases the\nsentence based on his linguistic knowledge from the past,\nthough this process will not be explicitly written down, and\neventually decides how the sentence should be corrected.\nFor example, here, it is easy for a person to correct age to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18662\nnot based on the given semantics.\nInstead of character-to-character tagging, we propose to\nrephrase the sentence as the training objective for fine-tuning\nCSC models. We denote the resultant model as Rephras-\ning Language Model (ReLM), where the source sentence\nwill first be encoded into the semantic space, and then is\nrephrased to the correct sentence based on the given mask\nslots. ReLM is based on BERT (Devlin et al. 2019), and\nachieves the new state-of-the-art results on existing bench-\nmarks, outperforming previous counterparts by a large mar-\ngin. We find that the rephrasing objective also works on\nauto-regressive models like GPT (Brown et al. 2020) and\nBaichuan (Yang et al. 2023), but worse than ReLM.\nAs opposed to previous work, we also pay attention to\nthe CSC performance in multi-task settings, where CSC is\njointly trained with other tasks (e.g. sentiment analysis, nat-\nural language inference). We find that tagging-based fine-\ntuning leads to non-transferable language representation and\nthe resultant CSC performance will significantly degenerate\nonce there is another task. The explanation still lies in the\nexcessive condition on the errors. This problematic property\nmakes CSC hard to be incorporated into multi-task learn-\ning. Given the ongoing trend of instruction tuning across\ndiverse tasks (OpenAI 2023), this phenomenon has a sig-\nnificant negative impact. We show that ReLM allows for\nbetter transferability between CSC and other tasks, building\npromising multi-task models.\nOur contributions are summarized below: (1) We propose\nReLM to narrow the gap between machine spelling correc-\ntion and human spelling correction. (2) ReLM significantly\nenhances the generalizability of CSC models and refreshes\nthe new state-of-the-art results across fine-tuned and zero-\nshot CSC benchmarks. (3) We probe into and enhance the\ntransferability of CSC to other tasks. (4) Our analysis shows\nthat ReLM effectively exploits and retains the pre-trained\nknowledge within PLMs, while tagging models do not. 12\nRelated Work\nMost of the early efforts in CSC focus on the unsupervised\ntechniques by evaluating the perplexity of the sentence (Yeh\net al. 2013; Yu and Li 2014; Xie et al. 2015). Zhou, Porwal,\nand Konow (2019) reformulates the spell correction problem\nas a machine translation task. Recent methods model CSC\nas a sequence tagging problem that maps each character in\nthe sentence to the correct one (Wang et al. 2018; Wang,\nTay, and Zhong 2019). On top of pre-trained language mod-\nels (PLMs), a number of BERT-based models with the se-\nquence tagging training objective are proposed. Zhang et al.\n(2020) identify the potential error characters by a detection\nnetwork and then leverage the soft masking strategy to en-\nhance the eventual correction decision. Zhu et al. (2022) use\na multi-task network to minimize the misleading impact of\nthe misspelled characters (Cheng et al. 2020). There is also\na line of work that incorporates phonological and morpho-\nlogical knowledge through data augmentation and enhances\nthe BERT-based encoder to assist mapping the error to the\n1https://github.com/Claude-Liu/ReLM\n2https://github.com/gingasan/lemon\ncorrect one (Guo et al. 2021; Li et al. 2021; Liu et al. 2021a;\nCheng et al. 2020; Huang et al. 2021; Zhang et al. 2021).\nHowever, our method achieves the new state-of-the-art re-\nsults over all these variants with the original BERT architec-\nture, by repurposing the training objective.\nSimilar as previous methods, our method is based on the\n(PLMs) (Devlin et al. 2019; Brown et al. 2020; Liu et al.\n2019; Wu et al. 2022; He, Gao, and Chen 2023). However,\nwe maximize the pre-trained power by continually optimiz-\ning the language modeling objective, instead of sentence or\ntoken classification. Our method works on both encoder and\ndecoder architectures, and furthermore, we discuss CSC as\na sub-task in multi-task learning, which is not discussed in\nprevious work.\nMore recently, Wu et al. (2023c) decompose a CSC model\ninto two parallel models, a language model (LM) and an er-\nror model (EM), and find that tagging models lean to over-\nfit the error model while under-fit the language model. An\neffective technique masked-fine-tuning is thus proposed to\nfacilitate the learning of LM. While the masking strategy is\nstill effective in our method, their work differs from ours in\nterms of bottom logic. The masked-fine-tuned CSC model\nremains a tagging model, which partially mitigates the neg-\native effect of EM. More importantly, our method is a lan-\nguage model alone, instead of two parallel models. It indi-\ncates that with an effective training objective, LM can pos-\nsess the functionality of EM, essentially solving the over-\nfitting to EM.\nMethod\nProblem Formulation\nChinese Spelling Correction (CSC) aims to correct all mis-\nspelled characters in the source sentence. Given a source\nsentence X = {x1, x2, ··· , xn} of n characters with po-\ntential spelling errors, the model seeks to generate the target\nsentence Y = {y1, y2, ··· , yn} of the same length with all\npotential errors corrected. The above process can be formu-\nlated as a conditional probability P(Y |X). Specifically for\nxi, suppose that it is an error character and its ground truth\nis yi, then the probability to correct xi to yi can be written\nas P(yi|X).\nTagging\nSequence tagging is a common model in many natural lan-\nguage processing tasks, where the model is trained to map\none character to another correspondingly, e.g. named entity\nrecognition, part-of-speech tagging. All these tasks share an\npivotal property in that they strongly rely on the alignment\ninformation between input and output characters. However,\ndeep neural models like Transformer (Vaswani et al. 2017)\nare always good at exploiting spurious clues if it is possi-\nble to achieve lower training loss, especially when the train-\ning data is not big enough (Wu et al. 2023a,b). For exam-\nple, Norway is always a geopolitical entity (GPE) in entity\nrecognition. Consequently, the model can memorize such\na character-to-character mapping and make correct predic-\ntions in most situations. Similar in CSC, the model can\ngreatly memorize the trivial edit pair of correcting xi to yi\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18663\nFigure 2: Paradigm of ReLM in single-task (left) and multi-task (right) settings. The source sentence for CSC is “taking a pair\n(→ piece) of painting”, and ⟨m⟩ and ⟨s⟩ refer to the mask and separate character respectively. On the right, we depict three\ntasks as a representative, CSC, language inference, and sentiment analysis, and p refers to the prompt for each task.\nand continue to apply it in a different context of xi, with-\nout referring to the semantics. Hence, the original training\nobjective degenerates to:\nP(yi|X) ≈ P(yi|xi) (1)\nwhere we suppose xi is an error character.\nHowever, the errors in CSC are much more diverse. As\npreviously shown in Figure 1, age to can be both corrected\nto remember to or not to, which significantly relies on the\nimmediate context. The resultant tagging model can hardly\nbe generalized to unseen errors.\nRephrasing\nIn this paper, we propose to substitute sequence tagging with\nrephrasing as the primary training objective for CSC.\nOur intuition is to eliminate the trend that the model fits\nthe training data by naively memorizing the errors. To do\nthis, we train the pre-trained language models (PLMs) to\nrephrase the source sentence following it. Concretely, the\nTransformer layers first transfer the source sentence to the\nsemantic space. Then, the model generates a new sentence\nwhile correcting all errors in it, based on the semantics.\nThis process is consistent with the human process of doing\nspelling correction. When a person sees a sentence, he will\nfirst commit the sentence to his mind (akin to encoding it to\nthe semantic space), and then transform the semantics to a\nnew sentence based on his linguistic instinct (the pre-trained\nweights in the PLM). We see that the pre-training knowledge\noffers a great foundation for such learning rephrasing, which\nis aside from learning sequence tagging. Our following ex-\nperiments show that sequence tagging does not make good\nuse of the benefits from pre-training.\nThe process of rephrasing can be modeled based on\nthe auto-regressive architecture with a decoder to generate\nthe output characters one by one, e.g. GPT (Brown et al.\n2020). Specifically, we concatenate the source characters\nX and the target characters Y as the input sentence, i.e.\n{x1, x2, ··· , xn, ⟨s⟩, y1, y2, ··· , yn, ⟨eos⟩}, where ⟨s⟩ and\n⟨eos⟩ refers to the separate token and wrap token, and\ntrain the model to predict all the target characters yi auto-\nregressively. Hence, rephrasing-based spelling correction\nseeks to solve the following probability for yi, i >= 1:\nP(yi|X) ≈ P(yi|X, y1, y2, ··· , yi−1). (2)\nRephrasing Language Model\nBased on the BERT-based architecture, we propose\nRephrasing Language Model(ReLM), a non-auto-regressive\nrephrasing model.\nEq 2 generates the sentences free of lengths. Given that\nthe length of the target sentence is fixed in CSC, equal to\nthat of the source sentence, such freedom may bring a nega-\ntive impact. BERT is an encoder-only architecture (Devlin\net al. 2019), pre-trained by randomly setting a portion of\ncharacters to a mask symbol ⟨m⟩. In contrast to the auto-\nregressive model, which keeps generation until⟨eos⟩, BERT\nis programmed to only infill the pre-set slots of mask.\nAs shown in Figure 2, we concatenate the source char-\nacters X and a sequence of mask characters M =\n{m1, m2, ··· , mn} of the same length as the input sen-\ntence, i.e. {x1, x2, ··· , xn, ⟨s⟩, m1, m2, ··· , mn}, where\nmi refers to the mask character foryi, and train the model to\ninfill all the mask characters mi following X. Since BERT\ncan see both the left-side and right-side context, ReLM seeks\nto solve the following probability for yi, i= 1 ∼ n:\nP(yi|X) ≈ P(yi|X, m1, m2, ··· , mn). (3)\nReLM is superior to the auto-regressive model as it can\nalways generate the output sentence of the same length as\nthe input, which makes it more accurate. In our following\nexperiments, we find that both auto-regressive rephrasing\nand ReLM outweigh previous tagging models, and the lat-\nter achieves more powerful results.\nAuxiliary Masked Language Modeling As opposed to\ntagging models, fine-tuned ReLM on CSC is still a lan-\nguage model as its core. However, there remains a chance\nthat the model can learn the alignment of source and target\nsentences. We thus propose a key strategy, that is to uni-\nformly mask a fraction of the non-error characters in the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18664\nsource sentence with an unused token, to greatly regular-\nize the model against learning character-to-character align-\nment. ReLM eventually necessitates correcting potential ty-\npos while simultaneously restoring the entire sentence.\nDistinguish from Sequence Tagging ReLM is\na biased estimation of P(yi|X), which optimizes\nP(yi|X, m1, m2, ··· , mn) instead. The resultant model is\nforced to rely on the entire semantics. The key property\nis, Eq. 3 predicts yi conditioned on the entire source\nsentence X, in contrast to Eq. 1. More concretely, there is\nno alignment of characters in ReLM, and the model is not\nallowed to find a shortcut to perform character-to-character\nmapping as it does in sequence tagging.\nReLM for Multi-Task\nThe emerging large language models (OpenAI 2023; Tou-\nvron et al. 2023) tend to handle diverse tasks at the same\ntime, and it is time to study the incorporation of CSC into\nother tasks. In typical multi-task learning, we add a specific\nclassification head for each task and train a shared encoder\nfor all tasks. For instance, CSC will share the same language\nrepresentation within the encoder with sentence classifica-\ntion. However, our empirical analysis shows that the per-\nformance of conventional tagging-based CSC may largely\ndeteriorate when it is jointly trained with other tasks. The\ncorresponding probing is in the following analysis section.\nIn contrast, ReLM, still a language model as its core, natu-\nrally suits the multi-task learning on top of language model-\ning, while tagging-based CSC does not. Concretely, each in-\ndividual task is templated to the format of masked language\nmodeling, as shown in Figure 2. In general, all tasks are uni-\nfied to a rephrasing-like format, which enhances the trans-\nferablity of CSC to various tasks. In addition, ReLM sup-\nports prompt tuning (Lester, Al-Rfou, and Constant 2021;\nLiu et al. 2021b). We prefix a sequence of trainable charac-\nters to the input sentence as the prompt steering the model\nfor different tasks, and optimize the corresponding prompt\nfor each task. We find that introducing prompts can further\nimprove the outcome, but to a slight extent.\nExperiment\nIn this section, we compare ReLM with a line of tagging-\nbased methods on existing benchmarks. We also evaluate the\nCSC performance in multi-task learning, where all the mod-\nels are jointly trained on three different tasks, CSC, semantic\nsimilarity, and news classification.\nDataset\nECSpell ECSpell (Lv et al. 2022) is a CSC benchmark\nwith three domains, LAW (1,960 training and 500 test sam-\nples), MED (3,000 training and 500 test samples), and ODW\n(1,728 training and 500 test samples).\nLEMON Large-scale multi-domain dataset with natural\nspelling errors (LEMON) (Wu et al. 2023c) is a novel CSC\nbenchmarks with diverse real-life spelling errors. It spans 7\ndifferent domains with totally 22,252 test samples. It typi-\ncally measures the open-domain generalizability of a CSC\nmodel in a zero-shot setting.\nMethod Prec. Rec. F1\nLAW\nGPT2Tagging 37.7 32.5 35.0\nBERTTagging 43.3 36.9 39.8\nGPT2Rephrasing 61.6 84.3 71.2 ↑31.4\nBERTTagging-MFT 73.2 79.2 76.1\nMDCSpellTagging-MFT 77.5 83.9 80.6\nRELM 89.9 94.5 91.2 ↑10.6\nBaichuan2Rephrasing 85.1 87.1 86.0\nChatGPT-10 shot 46.7 50.1 48.3\nMED\nGPT2Tagging 23.1 16.7 19.4\nBERTTagging 25.3 20.0 22.3\nGPT2Rephrasing 29.6 44.7 35.6 ↑13.3\nBERTTagging-MFT 57.9 58.1 58.0\nMDCSpellTagging-MFT 69.9 69.3 69.6\nRELM 79.2 85.9 82.4 ↑12.8\nBaichuan2Rephrasing 72.6 73.9 73.2\nChatGPT-10 shot 21.9 31.9 26.0\nODW\nGPT2Tagging 26.8 19.8 22.8\nBERTTagging 30.1 21.3 25.0\nGPT2Rephrasing 46.2 64.3 53.8 ↑28.8\nBERTTagging-MFT 59.7 58.8 59.2\nMDCSpellTagging-MFT 65.7 68.2 66.9\nRELM 82.4 84.8 83.6 ↑16.7\nBaichuan2Rephrasing 86.1 79.3 82.6\nChatGPT-10 shot 56.5 57.1 56.8\nTable 1: Precison, recall, and F1 results on ECSpell. We\nmark the performance improvement of GPT2-rephrasing\nover BERT-tagging and ReLM over previous SotA.\nSIGHAN SIGHAN (Tseng et al. 2015) is a CSC bench-\nmark collected from the Chinese essays written by for-\neign speakers. Following Wu et al. (2023c), we evaluate the\nmodel on SIGHAN as zero-shot learning.\nAFQMC Ant Financial Question Matching (AFQMC)\n(Xu et al. 2020) is a Chinese semantic similarity dataset that\nrequires the model to predict whether the given two ques-\ntions are semantically similar. It contains 34,334 training\nsamples and 3,861 test samples.\nTNEWS TouTiao Text Classification for News Titles\n(TNEWS) (Xu et al. 2020) is a text classification dataset,\nrequiring to link each given title to 15 news categories. It\ncontains 53,360 training samples and 10,000 test samples.\nMethods to Compare\nBERTTagging We fine-tune the original BERT model as se-\nquence tagging3.\nMDCSpellTagging It is an enhanced BERT-based model\nwith a detector-corrector design (Zhu et al. 2022).\nGPT2Tagging We initialize a new classifier following the\npre-trained Chinese GPT2 model and fine-tune it as se-\nquence tagging4.\nMasked-Fine-Tuning (MFT) It is a simple and effec-\ntive fine-tuning technique when fine-tuning tagging models,\n3https://huggingface.co/bert-base-chinese\n4https://huggingface.co/uer/gpt2-chinese-cluecorpussmall\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18665\nGAM ENC COT MEC CAR NOV NEW SIG Avg\nPrevious tagging models SotA (Wu et al. 2023c)\nBERT 27.1 41.6 63.9 47.9 47.6 34.2 50.7 50.6 45.5\nBERT-MFT 33.3 45.5 64.1 50.9 52.3 36.0 56.0 53.4 48.9\nSoft-Masked-MFT 29.8 44.6 65.0 49.3 52.0 37.8 55.8 53.4 48.4\nMDCSpell-MFT 31.2 45.9 65.4 52.0 52.6 38.6 57.3 54.7 49.7\nCRASpell-MFT 30.7 48.1 66.0 51.7 51.7 38.6 55.9 55.1 49.7\nOurs\nRELM 33.0 49.2↑3.7 66.8↑2.7 54.0↑3.1 53.1↑0.8 37.8↑1.8 58.5↑2.5 57.0↑3.6 51.2↑2.3\nTable 2: Performances (F1) of ReLM and previous SotA tagging models on LEMON, where SIG refers to SIGHAN. We mark\nthe performance improvement of ReLM over BERT-MFT.\nwhich achieves the previous state-of-the-art (SotA) results\non ECSpell and LEMON (Wu et al. 2023c).\nBaichuan2-7b We fine-tune Baichuan2 (Yang et al.\n2023), one of the strongest Chinese open source LLMs, with\nLoRA (Hu et al. 2022).\nChatGPT We instruct ChatGPT (OpenAI 2023) to cor-\nrect samples by in-context learning with 10 shots, using the\nopenai API5.\nRELM We train ReLM based on the same BERT model\nin BERT-tagging.\nFine-tuned CSC on ECSpell\nWe fine-tune each model separately on the three domains for\n5000 steps, with the batch size selected from {32, 128} and\nlearning rate from {2e-5, 5e-5}.\nTable 1 summarizes the results on ECSpell. We find that\nnaive tagging models of BERT and GPT2 perform poorly on\nall of three domains, while BERT performs slightly better\nthan GPT2. However, ReLM yields amazing performance\nimprovement. Concretely, it achieves the new SotA result\non every domain, significantly outperforming the previous\nSotA masked-fine-tuned MDCSpell by 10.6, 12.8, and 16.7\nabsolute points respectively. We also apply the rephrasing\nobjective to GPT2. We find that even GPT2-rephrasing out-\nperforms BERT-tagging by a large margin (e.g. F1 39.8 →\n71.2 on LAW), demonstrating the great superiority of the\nrephrasing objective, which prevents the model from simply\nmemorizing errors.\nHowever, we see ReLM is more powerful. It indicates\nthat fixed-length rephrasing is naturally matched with CSC,\nwhile the auto-regressive one is also promising for future\nstudy. On the other hand, it is worth noting that ReLM sur-\npasses all other enhanced architectures by simply training\nbased on the original Transformer architecture. It highlights\nthe pivotal role of the rephrasing objective, while the com-\nmon tagging objective does not exploit the full power of\nPLMs, incurring the performance bottleneck.\nWe report the results of Baichuan2 and ChatGPT as rep-\nresentatives for LLMs. We find that ChatGPT does not work\nwell on CSC even in a 10-shot setting. We speculate that this\nis due to the lack of high-quality annotated data for CSC on\n5gpt-3.5-turbo\nthe web. However, fine-tuned Baichuan2 achieves promising\nresults, outperforming GPT2-rephrasing by a large margin.\nZero-Shot CSC on LEMON\nOn LEMON, we evaluate models as a zero-shot learner. Fol-\nlowing Wu et al. (2023c), we collect 34 million monolingual\nsentences and synthesize training sentence pairs using the\nconfusion set. We train the model with the batch size 4096\nand learning rate 5e-5 on 8 A800 sheets for 60,000 steps.\nTable 2 compares the zero-shot performance of ReLM to\nprevious SotA tagging models. We find that, though each\nLEMON domain varies greatly, ReLM brings a significant\nperformance boost in almost every domain, and reaches the\nnew SotA results, raising the previous SotA from 49.7 to\n51.2. It indicates that ReLM is more generalizable to out-of-\ndistribution errors over all other BERT-tagging variants.\nCSC in Multi-Task\nWe train the multi-task model on three distinct tasks,\nECSpell for CSC, AFQMC for semantic similarity, and\nTNEWS for news classification. The three datasets are\nmixed together and we uniformly sample one batch from\nthem during training. For the tagging models, we train three\ntask-specific linear classifiers and one shared encoder. For\nReLM, we share the entirety of model parameters for all\nthree tasks. For ReLM with prompt, we train an additional\nprompt embeddings for each task.\nTable 3 compares the results on multiple tasks. We find\nthat the performances of two text classification tasks vary\nonly marginally between multi-task and single-task settings.\nWe speculate that these two tasks are less challenging, and\nthe model can fit them well more easily. In contrast, the\nperformance of CSC is strongly affected by other tasks,\nwhere both tagging models meet a great performance drop.\nHowever, ReLM can largely maintain the CSC performance\nand achieve competitive results on all three domains, al-\nmost without compromising other tasks. Adding additional\nprompt characters can further improve the performance. It\nsuggests that ReLM contributes to better collaboration be-\ntween different tasks, on top of templating all tasks to the\nMLM format, while tagging-based CSC is incompatible to\nsuch a training paradigm. The logic behind is that ReLM\nretains the useful features within the pre-trained language\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18666\nCSC Method CSC News Classification Semantic Similarity Avg\nF1 ∆ F1single (%) F1 ∆ F1single F1 ∆ F1single\nLAW\nBERTTagging 34.5 - 13.3% 56.2 - 0.4 72.6 - 1.3 54.4\nBERTTagging-MFT 62.0 - 18.9% 55.0 - 1.6 71.0 - 2.9 62.6\nRELM 84.2 - 8.7% 56.9 + 0.3 71.6 - 2.3 70.9\nRELM (prompt) 87.6 - 4.9% 56.9 + 0.3 72.4 - 1.5 72.3\nMED\nBERTTagging 15.1 - 32.2% 56.3 - 0.3 72.5 - 1.4 48.0\nBERTTagging-MFT 48.8 - 15.9% 55.8 - 0.8 72.5 - 1.3 59.0\nRELM 76.1 - 10.9% 57.1 + 0.5 70.7 - 3.2 68.0\nRELM (prompt) 80.8 - 4.7% 56.6 + 0 71.8 - 2.1 69.7\nODW\nBERTTagging 16.8 - 32.2% 56.6 + 0 73.3 - 0.6 48.9\nBERTTagging-MFT 52.4 - 11.5% 55.9 - 0.7 73.2 - 0.7 60.5\nRELM 75.0 - 13.5% 56.9 + 0.3 71.8 - 2.1 67.9\nRELM (prompt) 78.0 - 10.0% 56.8 + 0.2 72.5 - 1.4 69.1\nTable 3: Results of different CSC training methods in the multi-task setting (ECSpell, TNEWS, AFQMC from left to right),\nwhere ∆ F1single refers to the performance difference form multi-task to single-task, and % means the relative difference.\nMethod LAW MED ODW Avg\nBERTTag 13.1 9.1 13.9 12.0\nBERTTag (multi-task) 13.8 10.2 15.5 13.2 ↑\nBERTTag-MFT 14.7 11.2 15.5 13.8\nBERTTag-MFT (multi-task) 14.7 11.6 18.5 14.9 ↑\nMDCSpellTag-MFT 14.3 10.5 16.4 13.7\nRELM 8.4 5.0 6.9 6.8\nRELM (multi-task) 7.4 6.5 2.2 5.4 ↓\nTable 4: Comparison of false positive rate (FPR) on EC-\nSpell. It is excepted to be lower for a better CSC system.\nrepresentation of PLMs. In our following analysis, we show\nthat tagging-based CSC learns non-transferable features.\nFurther Analysis\nFalse Positive Rate\nFalse positive rate (FPR) is a measurement to evaluate a\nCSC system in real-world applications, which refers to the\nratio that the model mistakenly modifies an otherwise cor-\nrect sentence, which is also known as over-correction. Table\n4 shows that ReLM greatly reduces the FPR compared to\ntagging models. It suggests the tagging models are overly\nconditioned on the seen errors and thus tend to modify some\nnew expressions to familiar ones, while ReLM does not. Ad-\nditionally, we find that ReLM produces even lower FPR in\nmulti-task learning. It indicates that by ReLM, the language\nrepresentation learned from CSC and other tasks can com-\nplement each other, while sequence tagging cannot.\nWe further demonstrate that a high FPR may result in a\ngap between the development performance and real-world\npractice. Mathematically, we have 1/p ∝ N\nP · FPR, where\nN and P refer to the number of negative samples and posi-\ntive samples, and p is the precision score. We can find thatp\nis negatively correlated with the ratio N\nP , which means more\nnegative samples lead to lower precision under the same\nFPR. However, negative samples are dominant in real-world\nsituations ( N\nP is large), since humans do not misspell very\n0 2 4 6 8 10\nRatio…N/P\n30\n40\n50\n60\n70\n80\n90Precision\nfpr=6.9\nfpr=16.4\n5ODW\nMDCSpell-MFT\nReLM\n(a) Precision\n0 2 4 6 8 10\nRatio…N/P\n40\n50\n60\n70\n80\n90F1\nfpr=6.9\nfpr=16.4\n5ODW\nMDCSpell-MFT\nReLM (b) F1\nFigure 3: Performance variation (precision and F1) with the\nproportion of negative and positive samples.\nfrequently. We can derive similarly results for the F1 score.\nConsequently, a higher FPR may exacerbate the decrease of\nthe overall performance of the CSC system.\nFigure 3 depicts the variation of the precision and F1 with\nthe proportion of positive and negative samples, compar-\ning MDCSpell-MFT to ReLM on ECSpell-ODW. We find\nthat both F1 and precision curves of ReLM are more gen-\ntle, which decrease more slowly with the increase of N\nP . It\nhighlights the practical value of ReLM for real applications.\nProbing in Multi-Task\nTo investigate the transferability from CSC to other tasks,\nwe perform a linear probing experiment (Aghajanyan et al.\n2021). First, we fine-tune the model on CSC data (ECSpell).\nSecond, we freeze the parameters of the encoder and ini-\ntialize a new linear classifier following it. We fine-tune this\nclassifer only on another task (TNEWS). The results of the\nlinear probing reflect whether the learned features within the\nencoder are generalized to transfer to new tasks.\nFrom Table 5, we find that both tagging models suffer\nfrom a severe drop when transferring their learned language\nrepresentation from CSC to TNEWS. It suggests that se-\nquence tagging does not learn any generalized features from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18667\nCSC Method LAW→ MED→ ODW→ TNEWSTNEWS TNEWS TNEWS\nBERTTag 13.2↓43.4 14.8↓41.8 15.7↓40.9 56.6\nBERTTag-MFT 16.1↓40.5 17.6↓39.0 18.5↓38.1 56.6\nRELM 54.1↓2.5 53.7↓2.9 49.2↓7.4 56.6\nTable 5: Results where we intend to transfer the learned lan-\nguage representation from CSC to news classification.\nLAW MED ODW Avg\nMDCSpellTagging-MFT 80.6 69.6 66.9 72.4\nmask any 90.5 84.0 82.9 85.8\nmask non error 92.2 85.4 86.7 88.1\nTable 6: Comparison of different mask strategies.\nCSC, even degrades the language representation of the orig-\ninal PLM. In contrast, we find that ReLM can transfer much\nbetter, suggesting that it retains generalized features within\nthe language representation during fine-tuning.\nMask Strategy\nWe investigate two mask strategies of the auxiliary MLM\nwhen training ReLM. The first is to uniformly mask any\ncharacters in the sentence and the second is to mask non-\nerror characters only. From Table 6, we can see that both\nmask strategies are effective, while masking non-error char-\nacters works better. This is because masking error characters\ncan reduce the amount of the real errors within the training\ndata, which the model needs for learning correction. Addi-\ntionally, we find that adding learnable prompts can further\nimprove the performance of ReLM. The results in Table 6\nand Table 7 are based on ReLM with learnable prompts.\nMask Rate\nWe also investigate the impact of the mask rate. From Table\n7, we find that the performance on ECSpell keeps improving\nwhen the mask rate grows from 0% to 30%, and 30% is the\nbest choice. While it still relies on specific data, it shows that\nthe mask rate for ReLM is higher than that for MLM (Devlin\net al. 2019). This is because ReLM is essentially a further\nrefinement of PLMs, which allows a higher mask rate.\nCase Study\nWe illustrate the superiority of ReLM over tagging through\na number of cases selected from the evaluation results of\nmasked-fine-tuned BERT-tagging and ReLM in Figure 4.\nFor the first case (Director of People’s Hospital in Daxin-\ngan Ridge. ), BERT-tagging overly corrects a geopolitical\nplace “Daxingan Ridge” to “Daxingan Mountain”, which is\ndue to the fact that it doggedly memorizes a previous edit\npair “ridge” → “mountain” during training. However, we\ncan see that ReLM does not make this mistake.\nFor the second case ( Worried it develops towards mus-\ncle and bone. ), it highlights the ability of ReLM to utilize\nthe semantics of global context. Two expressions “like” and\nLAW MED ODW Avg\nBERTTagging 37.9 22.3 25.0 28.4\nRELM-0% 57.6 56.9 59.0 57.8\n10% 90.0 84.2 82.5 85.6\n20% 91.3 84.8 86.9 87.7\n30% 92.2 85.4 86.7 88.1\n40% 91.3 82.8 84.9 86.3\n60% 86.7 81.7 78.8 82.4\nTable 7: Comparison of different mask rates.\nsrc: !\"#$%&'()**+,-./01BERT:!\"#2%&'()**+,-./01ReLM:!\"#$%&'()**+,-./01Over-correctionsrc: 3456789:;<=>1BERT:3456789:;<=>1ReLM:34<6789:;<=>1Capture of semantics of the sentencesrc: ?@ABCDEFGAHICDEJKA1BERT:F@ABCDEFGAHICDEJKA1ReLM:L@ABCDEFGAHICDEJKA1Exploitation of expertise knowledge\n(a)\n(b)\n(c)\nFigure 4: Cases selected from ECSpell.\n“towards” are all locally correct, while to reach the correct\nresult, the model should refer to the word “develop” located\nat the end of the sentence (the Chinese order).\nThe third case is quite puzzling ( Judicial power is not\nexecution, but judgemental power. ), especially the first er-\nror. The correct answer not only necessitates the semantics\nbut also a legal principle that “judicial power is judgemental\npower”, which can only be attained through the pre-training\nprocess. We find that the tagging model does not possess\nsuch expertise and its answer is “law enforcement power is\njudgemental power”. It suggests that ReLM effectively in-\nherits the knowledge of PLMs, while the tagging model does\nnot even enhanced with masked-fine-tuning.\nConclusion\nThis papers notes a critical flaw in current CSC learning, that\nis conventional sequence tagging allows the correction ex-\ncessively conditioned on errors, leading to limited generaliz-\nability. To address this, we proposeReLM, where rephrasing\nacts as the training objective, akin to human spelling correc-\ntion. ReLM greatly outweighs previous methods on prevail-\ning benchmarks and facilitates multi-task learning.\nAcknowledgements\nThis paper was partially supported by Joint Research Project\nof Yangtze River Delta Science and Technology Innovation\nCommunity (No. 2022CSJGG1400).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18668\nReferences\nAfli, H.; Qiu, Z.; Way, A.; and Sheridan, P. 2016. Using SMT\nfor OCR Error Correction of Historical Texts. In Calzolari, N.;\nChoukri, K.; Declerck, T.; Goggi, S.; Grobelnik, M.; Maegaard, B.;\nMariani, J.; Mazo, H.; Moreno, A.; Odijk, J.; and Piperidis, S., eds.,\nProceedings of the Tenth International Conference on Language\nResources and Evaluation LREC 2016, Portoro ˇz, Slovenia, May\n23-28, 2016. European Language Resources Association (ELRA).\nAghajanyan, A.; Shrivastava, A.; Gupta, A.; Goyal, N.; Zettle-\nmoyer, L.; and Gupta, S. 2021. Better Fine-Tuning by Reduc-\ning Representational Collapse. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhari-\nwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agar-\nwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan, T.; Child, R.;\nRamesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen,\nM.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.;\nMcCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020.\nLanguage Models are Few-Shot Learners. In Larochelle, H.; Ran-\nzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in\nNeural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nCheng, X.; Xu, W.; Chen, K.; Jiang, S.; Wang, F.; Wang, T.; Chu,\nW.; and Qi, Y . 2020. SpellGCN: Incorporating Phonological and\nVisual Similarities into Language Models for Chinese Spelling\nCheck. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J. R.,\neds., Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online, July 5-10, 2020,\n871–881. Association for Computational Linguistics.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Un-\nderstanding. In Burstein, J.; Doran, C.; and Solorio, T., eds., Pro-\nceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-\n7, 2019, Volume 1 (Long and Short Papers), 4171–4186. Associa-\ntion for Computational Linguistics.\nGao, J.; Li, X.; Micol, D.; Quirk, C.; and Sun, X. 2010. A Large\nScale Ranker-Based System for Search Query Spelling Correction.\nIn Huang, C.; and Jurafsky, D., eds., COLING 2010, 23rd Inter-\nnational Conference on Computational Linguistics, Proceedings of\nthe Conference, 23-27 August 2010, Beijing, China, 358–366. Ts-\ninghua University Press.\nGuo, Z.; Ni, Y .; Wang, K.; Zhu, W.; and Xie, G. 2021. Global\nAttention Decoder for Chinese Spelling Error Correction. In Zong,\nC.; Xia, F.; Li, W.; and Navigli, R., eds.,Findings of the Association\nfor Computational Linguistics: ACL/IJCNLP 2021, Online Event,\nAugust 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL,\n1419–1428. Association for Computational Linguistics.\nHe, P.; Gao, J.; and Chen, W. 2023. DeBERTaV3: Improv-\ning DeBERTa using ELECTRA-Style Pre-Training with Gradient-\nDisentangled Embedding Sharing. In The Eleventh Interna-\ntional Conference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023. OpenReview.net.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.;\nWang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of\nLarge Language Models. In The Tenth International Conference\non Learning Representations, ICLR 2022, Virtual Event, April 25-\n29, 2022. OpenReview.net.\nHuang, L.; Li, J.; Jiang, W.; Zhang, Z.; Chen, M.; Wang, S.;\nand Xiao, J. 2021. PHMOSpell: Phonological and Morphologi-\ncal Knowledge Guided Chinese Spelling Check. In Zong, C.; Xia,\nF.; Li, W.; and Navigli, R., eds., Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Process-\ning, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,\nAugust 1-6, 2021, 5958–5967. Association for Computational Lin-\nguistics.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power of Scale\nfor Parameter-Efficient Prompt Tuning. In Moens, M.; Huang, X.;\nSpecia, L.; and Yih, S. W., eds., Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,\n7-11 November, 2021, 3045–3059. Association for Computational\nLinguistics.\nLi, C.; Zhang, C.; Zheng, X.; and Huang, X. 2021. Exploration and\nExploitation: Two Ways to Improve Chinese Spelling Correction\nModels. In Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., Pro-\nceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 2:\nShort Papers), Virtual Event, August 1-6, 2021, 441–446. Associa-\ntion for Computational Linguistics.\nLiu, S.; Yang, T.; Yue, T.; Zhang, F.; and Wang, D. 2021a. PLOME:\nPre-training with Misspelled Knowledge for Chinese Spelling Cor-\nrection. In Zong, C.; Xia, F.; Li, W.; and Navigli, R., eds., Pro-\nceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, 2991–3000. Asso-\nciation for Computational Linguistics.\nLiu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang, Z.; and Tang,\nJ. 2021b. GPT Understands, Too. CoRR, abs/2103.10385.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.;\nLewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach. CoRR,\nabs/1907.11692.\nLv, Q.; Cao, Z.; Geng, L.; Ai, C.; Yan, X.; and Fu, G. 2022. General\nand Domain Adaptive Chinese Spelling Check with Error Consis-\ntent Pretraining. CoRR, abs/2203.10929.\nMartins, B.; and Silva, M. J. 2004. Spelling Correction for Search\nEngine Queries. In Gonz ´alez, J. L. V .; Mart´ınez-Barco, P.; Mu˜noz,\nR.; and Saiz-Noeda, M., eds., Advances in Natural Language\nProcessing, 4th International Conference, EsTAL 2004, Alicante,\nSpain, October 20-22, 2004, Proceedings, volume 3230 ofLecture\nNotes in Computer Science, 372–383. Springer.\nOpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.;\nLacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.; Azhar, F.;\nRodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023.\nLLaMA: Open and Efficient Foundation Language Models.CoRR,\nabs/2302.13971.\nTseng, Y .; Lee, L.; Chang, L.; and Chen, H. 2015. Introduc-\ntion to SIGHAN 2015 Bake-off for Chinese Spelling Check. In\nYu, L.; Sui, Z.; Zhang, Y .; and Ng, V ., eds., Proceedings of\nthe Eighth SIGHAN Workshop on Chinese Language Processing,\nSIGHAN@IJCNLP 2015, Beijing, China, July 30-31, 2015, 32–37.\nAssociation for Computational Linguistics.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18669\nAll you Need. In Guyon, I.; von Luxburg, U.; Bengio, S.; Wal-\nlach, H. M.; Fergus, R.; Vishwanathan, S. V . N.; and Garnett, R.,\neds., Advances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, 5998–6008.\nWang, D.; Song, Y .; Li, J.; Han, J.; and Zhang, H. 2018. A Hybrid\nApproach to Automatic Corpus Generation for Chinese Spelling\nCheck. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J.,\neds., Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium, October 31\n- November 4, 2018, 2517–2527. Association for Computational\nLinguistics.\nWang, D.; Tay, Y .; and Zhong, L. 2019. Confusionset-guided\nPointer Networks for Chinese Spelling Check. In Korhonen, A.;\nTraum, D. R.; and M `arquez, L., eds., Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Pa-\npers, 5780–5785. Association for Computational Linguistics.\nWu, H.; Ding, R.; Zhao, H.; Chen, B.; Xie, P.; Huang, F.; and\nZhang, M. 2022. Forging Multiple Training Objectives for Pre-\ntrained Language Models via Meta-Learning. In Goldberg, Y .;\nKozareva, Z.; and Zhang, Y ., eds., Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, 6454–6466. Association for\nComputational Linguistics.\nWu, H.; Ding, R.; Zhao, H.; Xie, P.; Huang, F.; and Zhang, M.\n2023a. Adversarial Self-Attention for Language Understanding. In\nWilliams, B.; Chen, Y .; and Neville, J., eds., Thirty-Seventh AAAI\nConference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Con-\nference on Innovative Applications of Artificial Intelligence, IAAI\n2023, Thirteenth Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2023, Washington, DC, USA, February 7-14,\n2023, 13727–13735. AAAI Press.\nWu, H.; Liu, L.; Zhao, H.; and Zhang, M. 2023b. Empower\nNested Boolean Logic via Self-Supervised Curriculum Learning.\nIn Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the\n2023 Conference on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023, 13731–\n13742. Association for Computational Linguistics.\nWu, H.; Zhang, S.; Zhang, Y .; and Zhao, H. 2023c. Rethinking\nMasked Language Modeling for Chinese Spelling Correction. In\nRogers, A.; Boyd-Graber, J. L.; and Okazaki, N., eds.,Proceedings\nof the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, 10743–10756. Association for Computational Lin-\nguistics.\nXie, W.; Huang, P.; Zhang, X.; Hong, K.; Huang, Q.; Chen, B.; and\nHuang, L. 2015. Chinese Spelling Check System Based on N-gram\nModel. In Yu, L.; Sui, Z.; Zhang, Y .; and Ng, V ., eds.,Proceedings\nof the Eighth SIGHAN Workshop on Chinese Language Processing,\nSIGHAN@IJCNLP 2015, Beijing, China, July 30-31, 2015, 128–\n136. Association for Computational Linguistics.\nXiong, J.; Zhang, Q.; Zhang, S.; Hou, J.; and Cheng, X. 2015.\nHANSpeller: A Unified Framework for Chinese Spelling Correc-\ntion. Int. J. Comput. Linguistics Chin. Lang. Process., 20(1).\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .; Sun,\nK.; Yu, D.; Yu, C.; Tian, Y .; Dong, Q.; Liu, W.; Shi, B.; Cui, Y .;\nLi, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y .; Patterson, Y .; Tian, Z.;\nZhang, Y .; Zhou, H.; Liu, S.; Zhao, Z.; Zhao, Q.; Yue, C.; Zhang,\nX.; Yang, Z.; Richardson, K.; and Lan, Z. 2020. CLUE: A Chinese\nLanguage Understanding Evaluation Benchmark. In Scott, D.; Bel,\nN.; and Zong, C., eds., Proceedings of the 28th International Con-\nference on Computational Linguistics, COLING 2020, Barcelona,\nSpain (Online), December 8-13, 2020, 4762–4772. International\nCommittee on Computational Linguistics.\nYang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.;\nPan, D.; Wang, D.; Yan, D.; Yang, F.; Deng, F.; Wang, F.; Liu, F.;\nAi, G.; Dong, G.; Zhao, H.; Xu, H.; Sun, H.; Zhang, H.; Liu, H.; Ji,\nJ.; Xie, J.; Dai, J.; Fang, K.; Su, L.; Song, L.; Liu, L.; Ru, L.; Ma,\nL.; Wang, M.; Liu, M.; Lin, M.; Nie, N.; Guo, P.; Sun, R.; Zhang,\nT.; Li, T.; Li, T.; Cheng, W.; Chen, W.; Zeng, X.; Wang, X.; Chen,\nX.; Men, X.; Yu, X.; Pan, X.; Shen, Y .; Wang, Y .; Li, Y .; Jiang, Y .;\nGao, Y .; Zhang, Y .; Zhou, Z.; and Wu, Z. 2023. Baichuan 2: Open\nLarge-scale Language Models. CoRR, abs/2309.10305.\nYang, Y .; Wu, H.; and Zhao, H. 2023. Attack Named Entity Recog-\nnition by Entity Boundary Interference. CoRR, abs/2305.05253.\nYeh, J.; Li, S.; Wu, M.; Chen, W.; and Su, M. 2013. Chinese\nWord Spelling Correction Based on N-gram Ranked Inverted In-\ndex List. In Yu, L.; Tseng, Y .; Zhu, J.; and Ren, F., eds.,Proceed-\nings of the Seventh SIGHAN Workshop on Chinese Language Pro-\ncessing, SIGHAN@IJCNLP 2013, Nagoya, Japan, October 14-18,\n2013, 43–48. Asian Federation of Natural Language Processing.\nYu, J.; and Li, Z. 2014. Chinese Spelling Error Detection and Cor-\nrection Based on Language Model, Pronunciation, and Shape. In\nSun, L.; Zong, C.; Zhang, M.; and Levow, G., eds.,Proceedings of\nThe Third CIPS-SIGHAN Joint Conference on Chinese Language\nProcessing, Wuhan, China, October 20-21, 2014, 220–223. Asso-\nciation for Computational Linguistics.\nZhang, R.; Pang, C.; Zhang, C.; Wang, S.; He, Z.; Sun, Y .; Wu,\nH.; and Wang, H. 2021. Correcting Chinese Spelling Errors with\nPhonetic Pre-training. In Zong, C.; Xia, F.; Li, W.; and Navigli,\nR., eds., Findings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume\nACL/IJCNLP 2021 of Findings of ACL, 2250–2261. Association\nfor Computational Linguistics.\nZhang, S.; Huang, H.; Liu, J.; and Li, H. 2020. Spelling Error Cor-\nrection with Soft-Masked BERT. In Jurafsky, D.; Chai, J.; Schluter,\nN.; and Tetreault, J. R., eds.,Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, 882–890. Association for Computational\nLinguistics.\nZhou, Y .; Porwal, U.; and Konow, R. 2019. Spelling Correction\nas a Foreign Language. In Degenhardt, J.; Kallumadi, S.; Porwal,\nU.; and Trotman, A., eds., Proceedings of the SIGIR 2019 Work-\nshop on eCommerce, co-located with the 42st International ACM\nSIGIR Conference on Research and Development in Information\nRetrieval, eCom@SIGIR 2019, Paris, France, July 25, 2019, vol-\nume 2410 of CEUR Workshop Proceedings. CEUR-WS.org.\nZhu, C.; Ying, Z.; Zhang, B.; and Mao, F. 2022. MDCSpell:\nA Multi-task Detector-Corrector Framework for Chinese Spelling\nCorrection. In Muresan, S.; Nakov, P.; and Villavicencio, A., eds.,\nFindings of the Association for Computational Linguistics: ACL\n2022, Dublin, Ireland, May 22-27, 2022, 1244–1253. Association\nfor Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18670",
  "topic": "Spelling",
  "concepts": [
    {
      "name": "Spelling",
      "score": 0.8752678632736206
    },
    {
      "name": "Linguistics",
      "score": 0.7201724648475647
    },
    {
      "name": "Natural language processing",
      "score": 0.4678631126880646
    },
    {
      "name": "Computer science",
      "score": 0.40531840920448303
    },
    {
      "name": "History",
      "score": 0.363558828830719
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35688233375549316
    },
    {
      "name": "Psychology",
      "score": 0.3490660786628723
    },
    {
      "name": "Speech recognition",
      "score": 0.33592545986175537
    },
    {
      "name": "Philosophy",
      "score": 0.18622452020645142
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}