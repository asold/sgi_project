{
  "title": "TransFG: A Transformer Architecture for Fine-Grained Recognition",
  "url": "https://openalex.org/W3139434170",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123102921",
      "name": "Ju He",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A4296651583",
      "name": "Jie Neng Chen",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2113272736",
      "name": "Shuai Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2501329377",
      "name": "Adam Kortylewski",
      "affiliations": [
        "Max Planck Institute for Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A2098417099",
      "name": "Cheng Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2460965463",
      "name": "Yutong Bai",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2151419479",
      "name": "Changhu Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6681009388",
    "https://openalex.org/W2990495699",
    "https://openalex.org/W3009067796",
    "https://openalex.org/W2997426000",
    "https://openalex.org/W2921985587",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2998345525",
    "https://openalex.org/W1954152232",
    "https://openalex.org/W2797977484",
    "https://openalex.org/W6638319203",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W2883502031",
    "https://openalex.org/W2998619563",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2737725206",
    "https://openalex.org/W3126080715",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2963393555",
    "https://openalex.org/W2274287116",
    "https://openalex.org/W2403585668",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2949863037",
    "https://openalex.org/W3124312618",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3129197028",
    "https://openalex.org/W3134565071",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2986456235",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W1616462885",
    "https://openalex.org/W3081907075",
    "https://openalex.org/W2948140294",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W4287572671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W2804047946",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2142697503",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3108870912"
  ],
  "abstract": "Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.",
  "full_text": "TransFG: A Transformer Architecture for Fine-Grained Recognition\nJu He1, Jie-Neng Chen1, Shuai Liu3,\nAdam Kortylewski2, Cheng Yang3, Yutong Bai1, Changhu Wang3\n1 Johns Hopkins University\n2 Max Planck Institute for Informatics\n3 ByteDance Inc.\nAbstract\nFine-grained visual classiﬁcation (FGVC) which aims at rec-\nognizing objects from subcategories is a very challenging task\ndue to the inherently subtle inter-class differences. Most ex-\nisting works mainly tackle this problem by reusing the back-\nbone network to extract features of detected discriminative\nregions. However, this strategy inevitably complicates the\npipeline and pushes the proposed regions to contain most\nparts of the objects thus fails to locate the really important\nparts. Recently, vision transformer (ViT) shows its strong\nperformance in the traditional classiﬁcation task. The self-\nattention mechanism of the transformer links every patch to-\nken to the classiﬁcation token. In this work, we ﬁrst evalu-\nate the effectiveness of the ViT framework in the ﬁne-grained\nrecognition setting. Then motivated by the strength of the at-\ntention link can be intuitively considered as an indicator of the\nimportance of tokens, we further propose a novel Part Selec-\ntion Module that can be applied to most of the transformer ar-\nchitectures where we integrate all raw attention weights of the\ntransformer into an attention map for guiding the network to\neffectively and accurately select discriminative image patches\nand compute their relations. A contrastive loss is applied to\nenlarge the distance between feature representations of con-\nfusing classes. We name the augmented transformer-based\nmodel TransFG and demonstrate the value of it by conducting\nexperiments on ﬁve popular ﬁne-grained benchmarks where\nwe achieve state-of-the-art performance. Qualitative results\nare presented for better understanding of our model.\nIntroduction\nFine-grained visual classiﬁcation aims at classifying sub-\nclasses of a given object category, e.g., subcategories of\nbirds (Wah et al. 2011; Van Horn et al. 2015), cars (Krause\net al. 2013), aircrafts (Maji et al. 2013). It has long been\nconsidered as a very challenging task due to the small inter-\nclass variations and large intra-class variations along with\nthe deﬁciency of annotated data, especially for the long-\ntailed classes. Beneﬁting from the progress of deep neu-\nral networks (Krizhevsky, Sutskever, and Hinton 2012; Si-\nmonyan and Zisserman 2014; He et al. 2016), the perfor-\nmance of FGVC has obtained a steady progress in recent\nyears. To avoid labor-intensive part annotation, the com-\nmunity currently focuses on weakly-supervised FGVC with\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: An overview of performance comparison of ViT\nand TransFG with state-of-the-art methods CNNs on ﬁve\ndatasets. We achieve state-of-the-art performance on most\ndatasets while performing a little bit worse on Stanford Cars\npossibly due to the more regular and simpler car shapes.\nonly image-level labels. Methods now can be roughly clas-\nsiﬁed into two categories, i.e., localization methods and\nfeature-encoding methods. Compared to feature-encoding\nmethods, the localization methods have the advantage that\nthey explicitly capture the subtle differences among sub-\nclasses which is more interpretable and yields better results.\nEarly works in localization methods rely on the annota-\ntions of parts to locate discriminative regions while recent\nworks (Ge, Lin, and Yu 2019a; Liu et al. 2020; Ding et al.\n2019) mainly adopt region proposal networks (RPN) to pro-\npose bounding boxes which contain the discriminative re-\ngions. After obtaining the selected image regions, they are\nresized into a predeﬁned size and forwarded through the\nbackbone network again to acquire informative local fea-\ntures. A typical strategy is to use these local features for\nclassiﬁcation individually and adopt a rank loss (Chen et al.\n2009) to maintain consistency between the quality of bound-\ning boxes and their ﬁnal probability output. However, this\nmechanism ignores the relation between selected regions\nand thus inevitably encourages the RPN to propose large\nbounding boxes that contain most parts of the objects which\nfails to locate the really important regions. Sometimes these\nbounding boxes can even contain large areas of background\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n852\nand lead to confusion. Additionally, the RPN module with\ndifferent optimizing goals compared to the backbone net-\nwork makes the network harder to train and the re-use of\nbackbone complicates the overall pipeline.\nRecently, the vision transformer (Dosovitskiy et al. 2020)\nachieved huge success in the classiﬁcation task which shows\nthat applying a pure transformer directly to a sequence of\nimage patches with its innate attention mechanism can cap-\nture the important regions in images. A series of extended\nworks on downstream tasks such as object detection (Carion\net al. 2020) and semantic segmentation (Zheng et al. 2021;\nXie et al. 2021; Chen et al. 2021) conﬁrmed the strong abil-\nity for it to capture both global and local features.\nThese abilities of the Transformer make it innately suit-\nable for the FGVC task as the early long-range “receptive\nﬁeld” (Dosovitskiy et al. 2020) of the Transformer enables\nit to locate subtle differences and their spatial relation in the\nearlier processing layers. In contrast, CNNs mainly exploit\nthe locality property of image and only capture weak long-\nrange relation in very high layers. Besides, the subtle dif-\nferences between ﬁne-grained classes only exist in certain\nplaces thus it is unreasonable to convolve a ﬁlter which cap-\ntures the subtle differences to all places of the image.\nMotivated by this opinion, in the paper, we present the\nﬁrst study which explores the potential of vision transform-\ners in the context of ﬁne-grained visual classiﬁcation. We\nﬁnd that directly applying ViT on FGVC already produces\nsatisfactory results while a lot of adaptations according to\nthe characteristics of FGVC can be applied to further boost\nthe performance. To be speciﬁc, we propose Part Selec-\ntion Module which can ﬁnd the discriminative regions and\nremove redundant information. A contrastive loss is intro-\nduced to make the model more discriminative. We name this\nnovel yet simple transformer-based framework TransFG,\nand evaluate it extensively on ﬁve popular ﬁne-grained vi-\nsual classiﬁcation benchmarks (CUB-200-2011, Stanford\nCars, Stanford Dogs, NABirds, iNat2017). An overview of\nthe performance comparison can be seen in Fig 1 where our\nTransFG outperforms existing SOTA CNN methods with\ndifferent backbones on most datasets. In summary, we make\nseveral important contributions in this work:\n1. To the best of our knowledge, we are the ﬁrst to verify the\neffectiveness of vision transformer on ﬁne-grained visual\nclassiﬁcation which offers an alternative to the dominat-\ning CNN backbone with RPN model design.\n2. We introduce TransFG, a novel neural architecture for\nﬁne-grained visual classiﬁcation that naturally focuses\non the most discriminative regions of the objects and\nachieve SOTA performance on several benchmarks.\n3. Visualization results are presented which illustrate the\nability of our TransFG to accurately capture discrimina-\ntive image regions and help us to better understand how\nit makes correct predictions.\nRelated Work\nIn this section, we brieﬂy review existing works on ﬁne-\ngrained visual classiﬁcation and transformer.\nFine-Grained Visual Classiﬁcation\nMany works have been done to tackle the problem of ﬁne-\ngrained visual classiﬁcation and they can roughly be classi-\nﬁed into two categories: localization methods (Ge, Lin, and\nYu 2019a; Liu et al. 2020; Yang et al. 2021) and feature-\nencoding methods (Yu et al. 2018; Zheng et al. 2019; Gao\net al. 2020). The former focuses on training a detection net-\nwork to localize discriminative part regions and reuse them\nto perform classiﬁcation. The latter targets at learning more\ninformative features by either computing higher-order infor-\nmation or ﬁnding the relationships among contrastive pairs.\nLocalization FGVC Methods Previously, some works\n(Branson et al. 2014; Wei, Xie, and Wu 2016) tried to ex-\nploit the part annotations to supervise the learning procedure\nof the localization process. However, since such annotations\nare expensive and usually unavailable, weakly-supervised\nparts proposal with only image-level labels draw more atten-\ntions nowadays. Ge et al. (Ge, Lin, and Yu 2019a) exploited\nMask R-CNN and CRF-based segmentation alternatively to\nextract object instances and discriminative regions. Yang et\nal. (Yang et al. 2021) proposed a re-ranking strategy to re-\nrank the global classiﬁcation results based on the database\nconstructed with region features. However, these methods all\nneed a special designed module to propose potential regions\nand these selected regions need to be forwarded through the\nbackbone again for ﬁnal classiﬁcation which is not required\nin our model and thus keeps the simplicity of our pipeline.\nFeature-encoding Methods The other branch of meth-\nods focus on enriching the feature representations to ob-\ntain better classiﬁcation results. Yu et al. (Yu et al. 2018)\nproposed a hierarchical framework to do cross-layer bilin-\near pooling. Zheng et al. (Zheng et al. 2019) adopted the\nidea of group convolution to ﬁrst split channels into different\ngroups by their semantic meanings and then do the bilinear\npooling within each group without changing the dimension\nthus it can be integrated into any existed backbones directly.\nHowever, these methods are usually not interpretable such\none does not know what makes the model distinguish sub-\ncategories with subtle differences while our model drops\nunimportant image patches and only keeps those that con-\ntain most information for the ﬁne-grained recognition.\nTransformer\nTransformer and self-attention models have greatly facili-\ntated research in natural language processing and machine\ntranslation (Dai et al. 2019; Devlin et al. 2018; Vaswani et al.\n2017). Inspired by this, many recent studies try to apply\ntransformers in computer vision area. Initially, transformer\nis used to handle sequential features extracted by CNN back-\nbone for the videos (Girdhar et al. 2019). Later, transformer\nmodels are further extended to other popular computer vi-\nsion tasks such as object detection (Carion et al. 2020; Zhu\net al. 2020), segmentation (Xie et al. 2021; Wang et al.\n2021), object tracking (Sun et al. 2020). Most recently, pure\ntransformer models are becoming more and more popular.\nViT (Dosovitskiy et al. 2020) is the ﬁrst work to show that\napplying a pure transformer directly to a sequence of im-\nage patches can yield state-of-the-art performance on image\n853\nFigure 2: The framework of our proposed TransFG. Images are split into small patches (a non-overlapping split is shown\nhere) and projected into the embedding space. The input to the Transformer Encoder consists of patch embeddings along with\nlearnable position embeddings. Before the last Transformer Layer, a Part Selection Module (PSM) is applied to select tokens\nthat corresponds to the discriminative image patches and only use these selected tokens as input. Best viewed in color.\nclassiﬁcation. Based on that, Zheng et al. (Zheng et al. 2021)\nproposed SETR to exploit ViT as the encoder for segmenta-\ntion. He et al. (He et al. 2021) proposed TransReID which\nembedded side information into transformer along with the\nJPM to boost the performance on object re-identiﬁcation. In\nthis work, we extend ViT to ﬁne-grained visual classiﬁcation\nand show its effectiveness.\nMethod\nWe ﬁrst brieﬂy review the framework of vision transformer\nand show how to do some preprocessing steps to extend it\ninto ﬁne-grained recognition. Then, the overall framework\nof TransFG will be elaborated.\nVision Transformer as Feature Extractor\nImage Sequentialization. Following ViT, we ﬁrst prepro-\ncess the input image into a sequence of ﬂattened patches\nxp. However, the original split method cut the images into\nnon-overlapping patches, which harms the local neighboring\nstructures especially when discriminative regions are split.\nTo alleviate this problem, we propose to generate overlap-\nping patches with sliding window. To be speciﬁc, we denote\nthe input image with resolution H ∗W, the size of image\npatch as P and the step size of sliding window as S. Thus\nthe input images will be split into N patches where\nN = NH ∗NW = ⌊H−P + S\nS ⌋∗⌊W −P + S\nS ⌋ (1)\nIn this way, two adjacent patches share an overlapping area\nof size (P −S) ∗P which helps to preserve better local\nregion information. Typically speaking, the smaller the step\nS is, the better the performance will be. But decreasing S\nwill at the same time requires more computational cost, so a\ntrade-off needs to be made here.\nPatch Embedding. We map the vectorized patches xp into\na latent D-dimensional embedding space using a trainable\nlinear projection. A learnable position embedding is added\nto the patch embeddings to retain positional information as\nfollows:\nz0 = [x1\npE;x2\npE;···;xN\np E] +Epos (2)\nwhere N is the number of image patches, E ∈R(P2·C)∗Dis\nthe patch embedding projection, and Epos ∈RN∗Ddenotes\nthe position embedding.\nThe Transformer encoder (Vaswani et al. 2017) contains\nLlayers of multi-head self-attention (MSA) and multi-layer\nperceptron (MLP) blocks. Thus the output of the l-th layer\ncan be written as follows:\nz\n0\nl = MSA( LN(zl−1)) +zl−1 l∈1;2;···;L (3)\nzl = MLP(LN(z\n0\nl)) +z\n0\nl l∈1;2;···;L (4)\nwhere LN(·) denotes the layer normalization operation and\nzl is the encoded image representation. ViT exploits the ﬁrst\ntoken of the last encoder layerz0\nL as the representation of the\nglobal feature and forward it to a classiﬁer head to obtain the\nﬁnal classiﬁcation results without considering the potential\ninformation stored in the rest of the tokens.\nTransFG Architecture\nWhile our experiments show that the pure Vision Trans-\nformer can be directly applied into ﬁne-grained visual classi-\nﬁcation and achieve impressive results, it does not well cap-\nture the local information required for FGVC. To this end,\n854\nFigure 3: A confusing pair of instances from the CUB-200-\n2011 dataset. Model needs to has the ability to capture the\nsubtle differences in order to classify them correctly. The\nsecond column shows the overall attention maps and two se-\nlected tokens of our TransFG method. Best viewed in color.\nwe propose the Part Selection Module (PSM) and apply con-\ntrastive feature learning to enlarge the distance of represen-\ntations between confusing sub-categories. The framework of\nour proposed TransFG is illustrated in Fig 2.\nPart Selection Module One of the most important prob-\nlems in ﬁne-grained visual classiﬁcation is to accurately lo-\ncate the discriminative regions that account for subtle dif-\nferences between similar sub-categories. For example, Fig 3\nshows a confusing pair of images from the CUB-200-2011\n(citation) dataset. The model needs to have the ability to cap-\nture the very small differences, i.e., the color of eyes and\nthroat in order to distinguish these two bird species. Re-\ngion proposal networks and weakly-supervised segmenta-\ntion strategies are widely introduced to tackle this problem\nin the traditional CNN-based methods.\nVision Transformer model is perfectly suited here with\nits innate multi-head attention mechanism. To fully exploit\nthe attention information, we change the input to the last\nTransformer Layer. Suppose the model has K self-attention\nheads and the hidden features input to the last layer are de-\nnoted as zL−1 = [z0\nL−1; z1\nL−1;z2\nL−1;···;zN\nL−1]. The atten-\ntion weights of the previous layers can be written as follows:\nal = [a0\nl ;a1\nl ;a2\nl ;···;aK\nl ] l∈1;2;···;L −1 (5)\nai\nl = [ai0\nl ; ai1\nl ;ai2\nl ;···;aiN\nl ] i∈0;1;···;K −1 (6)\nPrevious works (Serrano and Smith 2019; Abnar and\nZuidema 2020) suggested that the raw attention weights do\nnot necessarily correspond to the relative importance of in-\nput tokens especially for higher layers of a model, due to\nlack of token identiﬁability of the embeddings. To this end,\nwe propose to integrate attention weights of all previous lay-\ners. To be speciﬁc, we recursively apply a matrix multiplica-\ntion to the raw attention weights in all the layers as\nafinal =\nL−1Y\nl=0\nal (7)\nAs afinal captures how information propagates from the in-\nput layer to the embeddings in higher layers, it serves as a\nbetter choice for selecting discriminative regions compared\nto the single layer raw attention weights aL−1. We then\nchoose the index of the maximum value A1;A2;···;AK\nwith respect to the K different attention heads in afinal .\nThese positions are used as index for our model to extract\nthe corresponding tokens in zL−1. Finally, we concatenate\nthe selected tokens along with the classiﬁcation token as the\ninput sequence which is denoted as:\nzlocal = [z0\nL−1; zA1\nL−1;zA2\nL−1;···;zAK\nL−1] (8)\nBy replacing the original entire input sequence with tokens\ncorresponding to informative regions and concatenate the\nclassiﬁcation token as input to the last Transformer Layer,\nwe not only keep the global information but also force the\nlast Transformer Layer to focus on the subtle differences\nbetween different sub-categories while abandoning less dis-\ncriminative regions such as background or common features.\nContrastive Feature Learning Following ViT, we still\nadopt the ﬁrst token zi of the PSM module for classiﬁcation.\nA simple cross-entropy loss is not enough to fully supervise\nthe learning of features since the differences between sub-\ncategories might be small. To this end, we adopt contrastive\nloss Lcon which minimizes the similarity of classiﬁcation\ntokens corresponding to different labels and maximizes the\nsimilarity of classiﬁcation tokens of samples with the same\nlabel y. To prevent the loss being dominated by easy neg-\natives (different class samples with little similarity), a con-\nstant margin \u000b is introduced that only negative pairs with\nsimilarity larger than\u000bcontribute to the lossLcon. Formally,\nthe contrastive loss over a batch of size Bis denoted as:\nLcon = 1\nB2\nBX\ni\n[\nBX\nj:yi=yj\n(1 −Sim(zi;zj)+\nBX\nj:yi̸=yj\nmax((Sim(zi;zj) −\u000b);0)]\n(9)\nwhere zi and zj are pre-processed with l2 normalization and\nSim(zi;zj) is thus the dot product of zi and zj.\nIn summary, our model is trained with the sum of cross-\nentropy loss Lcross and contrastive Lcon together which can\nbe expressed as:\nL= Lcross(y;y′) +Lcon(z) (10)\nwhere Lcross(y;y′) is the cross-entropy loss between the\npredicted label y′and the ground-truth label y.\nExperiments\nIn this section, we ﬁrst introduce the detailed setup including\ndatasets and training hyper-parameters. Quantitative analy-\nsis is then given followed by ablation studies. We further\ngive qualitative analysis and visualization results to show the\ninterpretability of our model.\n855\nMethod Backbone CUB Cars\nResNet-50 ResNet-50 84.5 -\nNTS-Net ResNet-50 87.5 93.9\nCross-X ResNet-50 87.7 94.6\nDBTNet ResNet-101 88.1 94.5\nFDL DenseNet-161 89.1 94.2\nPMG ResNet-50 89.6 95.1\nAPI-Net DenseNet-161 90.0 95.3\nStackedLSTM GoogleNet 90.4 -\nDeiT DeiT-B 90.0 93.9\nViT ViT-B 16 90.3 93.7\nTransFG ViT-B 16 91.7 94.8\nTable 1: Comparison of different methods on CUB-200-\n2011, Stanford Cars.\nExperiments Setup\nDatasets. We evaluate our proposed TransFG on ﬁve widely\nused ﬁne-grained benchmarks, i.e., CUB-200-2011 (Wah\net al. 2011), Stanford Cars (Krause et al. 2013), Stanford\nDogs (Khosla et al. 2011), NABirds (Van Horn et al. 2015)\nand iNat2017 (Van Horn et al. 2018). We also exploit its us-\nage in large-scale challenging ﬁne-grained competitions.\nImplementation details. Unless stated otherwise, we im-\nplement TransFG as follows. First, we resize input images\nto 448 ∗448 except 304 ∗304 on iNat2017 for fair compar-\nison (random cropping for training and center cropping for\ntesting). We split image to patches of size 16 and the step\nsize of sliding window is set to be 12. Thus the H;W;P;S\nin Eq 1 are 448, 448, 16, 12 respectively. The margin\u000bin Eq\n9 is set to be 0.4. We load intermediate weights from ofﬁcial\nViT-B\n16 model pretrained on ImageNet21k. The batch size\nis set to 16. SGD optimizer is employed with a momentum\nof 0.9. The learning rate is initialized as 0.03 except 0.003\nfor Stanford Dogs dataset and 0.01 for iNat2017 dataset. We\nadopt cosine annealing as the scheduler of optimizer.\nQuantitative Analysis\nWe compare our proposed method TransFG with state-of-\nthe-art works on above mentioned ﬁne-grained datasets. The\nexperiment results on CUB-200-2011 and Stanford Cars\nare shown in Table 1. From the results, we ﬁnd that our\nmethod outperforms all previous methods on CUB dataset\nand achieve competitive performance on Stanford Cars.\nTo be speciﬁc, the third column of Table 1 shows the\ncomparison results on CUB-200-2011. Compared to the best\nresult StackedLSTM (Ge, Lin, and Yu 2019b) up to now,\nour TransFG achieves a 1.3% improvement on Top-1 Ac-\ncuracy metric and 1.4% improvement compared to our base\nframework ViT (Dosovitskiy et al. 2020). Multiple ResNet-\n50 are adopted as multiple branches in (Ding et al. 2019)\nwhich greatly increases the complexity. It is also worth not-\ning that StackLSTM is a very messy multi-stage training\nmodel which hampers the availability in practical use, while\nour TransFG maintains the simplicity.\nThe fourth column of Table 1 shows the results on Stan-\nford Cars. Our method outperforms most existing methods\nMethod Backbone Dogs\nMaxEnt DenseNet-161 83.6\nFDL DenseNet-161 84.9\nCross-X ResNet-50 88.9\nAPI-Net ResNet-101 90.3\nViT ViT-B 16 91.7\nTransFG ViT-B 16 92.3\nTable 2: Comparison of different methods on Stanford Dogs.\nMethod Backbone NABirds\nCross-X ResNet-50 86.4\nAPI-Net DenseNet-161 88.1\nCS-Parts ResNet-50 88.5\nFixSENet-154 SENet-154 89.2\nViT ViT-B 16 89.9\nTransFG ViT-B 16 90.8\nTable 3: Comparison of different methods on NABirds.\nwhile performs worse than PMG (Du et al. 2020) and API-\nNet (Zhuang, Wang, and Qiao 2020) with small margin. We\nargue that the reason might be the much more regular and\nsimpler shape of cars. However, even with this property, our\nTransFG consistently gets 1.1% improvement compared to\nthe standard ViT model.\nThe results of experiments on Stanford Dogs are shown in\nTable 2. Stanford Dogs is a more challenging dataset com-\npared to Stanford Cars with its the more subtle differences\nbetween certain species and the large variances of samples\nfrom the same category. Only a few methods have tested on\nthis dataset and our TransFG outperforms all of them. While\nViT (Dosovitskiy et al. 2020) outperforms other methods by\na large margin, our TransFG achieves 92.3% accuracy which\noutperforms SOTA by 2.0% with its discriminative part se-\nlection and contrastive loss supervision.\nNABirds is a much larger birds dataset not only from the\nside of images numbers but also with 355 more categories\nwhich signiﬁcantly makes the ﬁne-grained visual classiﬁca-\ntion task more challenging. We show our results on it in Ta-\nble 3. We observe that most methods achieve good results by\neither exploiting multiple backbones for different branches\nor adopting quite deep CNN structures to extract better fea-\ntures. While the pure ViT (Dosovitskiy et al. 2020) can di-\nrectly achieve 89.9% accuracy, our TransFG constantly gets\n0.9% performance gain compared to ViT and reaches 90.8%\naccuracy which outperforms SOTA by1.6%.\niNat2017 is a large-scale dataset for ﬁne-grained species\nMethod Backbone iNat2017\nResNet152 ResNet152 59.0\nIncResNetV2 IncResNetV2 67.3\nTASN ResNet101 68.2\nViT ViT-B 16 68.7\nTransFG ViT-B 16 71.7\nTable 4: Comparison of different methods on iNat2017.\n856\nMethod Patch Split Accuracy Train Time (h)\nViT N-Overlap 90.3 1.30\nViT Overlap 90.5 3.38\nTransFG N-Overlap 91.5 1.98\nTransFG Overlap 91.7 5.38\nTable 5: Ablation study on split way of image patches on\nCUB-200-2011 dataset.\nrecognition. Most previous methods do not report results on\niNat2017 because of the computational complexity of the\nmulti-crop, multi-scale and multi-stage optimization. With\nthe simplicity of our model pipeline, we are able to scale\nTransFG well to big datasets and evaluate the performance\nwhich is shown in Table 4. This dataset is very challeng-\ning for mining meaningful object parts and the background\nis very complicated as well. We ﬁnd that Vision Trans-\nformer structure outperforms ResNet structure a lot in these\nlarge challenging datasets. ViT outperformes ResNet152 by\nnearly 10% and similar phenomenon can also be observed\nin iNat2018 and iNat2019. Our TransFG is the only method\nto achieve above 70% accuracy with input size of 304 and\noutperforms SOTA with a large margin of3.5%.\nFor the just ended iNat2021 competition which contains\n10,000 species, 2.7M training images, our TransFG achieves\nvery high single model accuracy of 91.3%. (The ﬁnal perfor-\nmance was obtained by ensembling many different models\nalong with multi-modality processing) As far as we know, at\nleast two of the Top5 teams in the ﬁnal leaderboard adopted\nTransFG as one of their ensemble models. This clear proves\nthat our model can be further extended to large-scale chal-\nlenging scenarios besides academy datasets.\nAblation Study\nWe conduct ablation studies on our TransFG pipeline to an-\nalyze how its variants affect the ﬁne-grained visual classi-\nﬁcation result. All ablation studies are done on CUB-200-\n2011 dataset while the same phenomenon can be observed\non other datasets as well.\nInﬂuence of image patch split method. We investigate\nthe inﬂuence of our overlapping patch split method through\nexperiments with standard non-overlapping patch split. As\nshown in Table 5, both on the pure Vision Transformer\nand our improved TransFG framework, the overlapping split\nmethod bring consistently improvement, i.e., 0.2% for both\nframeworks. The additional computational cost introduced\nby this is also affordable as shown in the fourth column.\nMethod Accuracy (%)\nViT 90.3\nTransFG 91.0\nTable 6: Ablation study on Part Selection Module (PSM) on\nCUB-200-2011 dataset.\nInﬂuence of Part Selection Module. As shown in Table 6,\nby applying the Part Selection Module (PSM) to select dis-\ncriminative part tokens as the input for the last Transformer\nMethod Contrastive Loss Acc (%)\nViT 90.3\nViT X 90.7\nTransFG 91.0\nTransFG X 91.5\nTable 7: Ablation study on contrastive loss on CUB-200-\n2011 dataset.\nMethod Value of \u000b Accuracy (%)\nTransFG 0 91.1\nTransFG 0.2 91.4\nTransFG 0.4 91.7\nTransFG 0.6 91.5\nTable 8: Ablation study on value of margin \u000bon CUB-200-\n2011 dataset.\nlayer, the performance of the model improves from 90.3%\nto 91.0%. We argue that this is because in this way, we sam-\nple the most discriminative tokens as input which explicitly\nthrows away some useless tokens and force the network to\nlearn from the important parts.\nInﬂuence of contrastive loss. The comparisons of the per-\nformance with and without contrastive loss for both ViT\nand TransFG frameworks are shown in Table 7 to verify\nthe effectiveness of it. We observe that with contrastive loss,\nthe model obtains a big performance gain. Quantitatively, it\nincreases the accuracy from 90.3% to 90.7% for ViT and\n91.0% to 91.5% for TransFG. We argue that this is because\ncontrastive loss can effectively enlarge the distance of repre-\nsentations between similar sub-categories and decrease that\nbetween the same categories which can be clearly seen in\nthe comparison of confusion matrix in Fig 4.\nFigure 4: Illustration of contrastive loss. Confusion matrices\nwithout and with contrastive loss of a batch with four classes\nwhere each contains four samples are shown. The metric of\nconfusion matrix is cosine similarity. Best viewed in color.\nInﬂuence of margin \u000b. The results of different setting of\nthe margin \u000b in Eq 9 is shown in Table 8. We ﬁnd that a\nsmall value of \u000bwill lead the training signals dominated by\neasy negatives thus decrease the performance while a high\nvalue of \u000bhinder the model to learn sufﬁcient information\nfor increasing the distances of hard negatives. Empirically,\nwe ﬁnd 0.4 to be the best value of \u000bin our experiments.\n857\nFigure 5:\nVisualization results of TransFG on CUB-200-2011, Stanford Dogs, Stanford Cars and NABirds datasets. Two kinds\nof visualization are given, where the ﬁrst and the third row show the selected Top-4 token positions while the second and fourth\nrows show the overall global attention maps. See examples from NABirds dataset where birds are sitting on twigs. The bird\nparts are lighted while the occluded twigs are ignored. Best viewed in color.\nQualitative Analysis\nWe show the visualization results of proposed TransFG on\nthe four benchmarks in Fig 5. We randomly sample three\nimages from each dataset. Two kinds of visualizations are\npresented. The ﬁrst and the third row of Fig 5 illustrated the\nselected tokens positions. For better visualization results, we\nonly draw the Top-4 image patches (ranked by the attention\nscore) and enlarge the square of the patches by two times\nwhile keeping the center positions unchanged. The second\nand fourth rows show the overall attention map of the whole\nimage where we use the same attention integration method\nas described above to ﬁrst integrate the attention weights of\nall layers followed by averaging the weights of all heads to\nobtain a single attention map. The lighter a region is, the\nmore important it is. From the ﬁgure, we can see that our\nTransFG successfully captures the most important regions\nfor an object, i.e., head, wings, tail for birds; ears, eyes, legs\nfor dogs; lights, doors for cars. At the same time, our overall\nattention map maps the entire object precisely even in com-\nplex backgrounds and it can even serves as a segmentation\nmask in some simple scenarios. These visualization results\nclearly prove the interpretability of our proposed method.\nConclusion\nIn this work, we propose a novel ﬁne-grained recognition\nframework TransFG and achieve state-of-the-art results on\nfour common ﬁne-grained benchmarks. We exploit self-\nattention mechanism to capture the most discriminative re-\ngions. Compared to bounding boxes produced by other\nmethods, our selected image patches are much smaller thus\nbecoming more meaningful by showing what regions really\ncontribute to the ﬁne-grained classiﬁcation. The effective-\nness of such small image patches also comes from the Trans-\nformer Layer to handle the inner relationships between these\nregions instead of relying on each of them to produce re-\nsults separately. Contrastive loss is introduced to increase\nthe discriminative ability of the classiﬁcation tokens. Exper-\niments are conducted on both traditional academy datasets\nand large-scale competition datasets to prove the effective-\nness of our model in multiple scenarios. Qualitative visual-\nizations further show the interpretability of our method.\nWith the promising results achieved by TransFG, we be-\nlieve that the transformer-based models have great potential\non ﬁne-grained tasks and our TransFG could be a starting\npoint for future works.\n858\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention\nﬂow in transformers. arXiv preprint arXiv:2005.00928.\nBranson, S.; Van Horn, G.; Belongie, S.; and Perona, P.\n2014. Bird species categorization using pose normalized\ndeep convolutional nets. arXiv preprint arXiv:1406.2952.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y . 2021. Transunet: Transform-\ners make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306.\nChen, W.; Liu, T.-Y .; Lan, Y .; Ma, Z.-M.; and Li, H. 2009.\nRanking measures and loss functions in learning to rank.Ad-\nvances in Neural Information Processing Systems, 22: 315–\n323.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q. V .; and\nSalakhutdinov, R. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDing, Y .; Zhou, Y .; Zhu, Y .; Ye, Q.; and Jiao, J. 2019. Selec-\ntive sparse sampling for ﬁne-grained image recognition. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 6599–6608.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDu, R.; Chang, D.; Bhunia, A. K.; Xie, J.; Ma, Z.; Song,\nY .-Z.; and Guo, J. 2020. Fine-grained visual classiﬁcation\nvia progressive multi-granularity training of jigsaw patches.\nIn European Conference on Computer Vision, 153–168.\nSpringer.\nGao, Y .; Han, X.; Wang, X.; Huang, W.; and Scott, M. 2020.\nChannel Interaction Networks for Fine-Grained Image Cat-\negorization. In Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, volume 34, 10818–10825.\nGe, W.; Lin, X.; and Yu, Y . 2019a. Weakly supervised com-\nplementary parts models for ﬁne-grained image classiﬁca-\ntion from the bottom up. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n3034–3043.\nGe, W.; Lin, X.; and Yu, Y . 2019b. Weakly Supervised Com-\nplementary Parts Models for Fine-Grained Image Classiﬁca-\ntion From the Bottom Up. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nGirdhar, R.; Carreira, J.; Doersch, C.; and Zisserman, A.\n2019. Video action transformer network. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 244–253.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identiﬁcation.\narXiv preprint arXiv:2102.04378.\nKhosla, A.; Jayadevaprakash, N.; Yao, B.; and Fei-Fei, L.\n2011. Novel Dataset for Fine-Grained Image Categoriza-\ntion. In First Workshop on Fine-Grained Visual Catego-\nrization, IEEE Conference on Computer Vision and Pattern\nRecognition. Colorado Springs, CO.\nKrause, J.; Stark, M.; Deng, J.; and Fei-Fei, L. 2013. 3D\nObject Representations for Fine-Grained Categorization. In\n4th International IEEE Workshop on 3D Representation and\nRecognition (3dRR-13). Sydney, Australia.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nageNet Classiﬁcation with Deep Convolutional Neural Net-\nworks. In Pereira, F.; Burges, C. J. C.; Bottou, L.; and Wein-\nberger, K. Q., eds.,Advances in Neural Information Process-\ning Systems, volume 25. Curran Associates, Inc.\nLiu, C.; Xie, H.; Zha, Z.-J.; Ma, L.; Yu, L.; and Zhang, Y .\n2020. Filtration and distillation: Enhancing region atten-\ntion for ﬁne-grained visual categorization. InProceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 34,\n11555–11562.\nMaji, S.; Rahtu, E.; Kannala, J.; Blaschko, M.; and Vedaldi,\nA. 2013. Fine-grained visual classiﬁcation of aircraft. arXiv\npreprint arXiv:1306.5151.\nSerrano, S.; and Smith, N. A. 2019. Is attention inter-\npretable? arXiv preprint arXiv:1906.03731.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSun, P.; Jiang, Y .; Zhang, R.; Xie, E.; Cao, J.; Hu, X.; Kong,\nT.; Yuan, Z.; Wang, C.; and Luo, P. 2020. Transtrack:\nMultiple-object tracking with transformer. arXiv preprint\narXiv:2012.15460.\nVan Horn, G.; Branson, S.; Farrell, R.; Haber, S.; Barry, J.;\nIpeirotis, P.; Perona, P.; and Belongie, S. 2015. Building\na bird recognition app and large scale dataset with citizen\nscientists: The ﬁne print in ﬁne-grained dataset collection.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 595–604.\nVan Horn, G.; Mac Aodha, O.; Song, Y .; Cui, Y .; Sun, C.;\nShepard, A.; Adam, H.; Perona, P.; and Belongie, S. 2018.\nThe inaturalist species classiﬁcation and detection dataset.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 8769–8778.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nWah, C.; Branson, S.; Welinder, P.; Perona, P.; and Belongie,\nS. 2011. The Caltech-UCSD Birds-200-2011 Dataset.\nTechnical Report CNS-TR-2011-001, California Institute of\nTechnology.\n859\nWang, H.; Zhu, Y .; Adam, H.; Yuille, A.; and Chen, L.-C.\n2021. Max-deeplab: End-to-end panoptic segmentation with\nmask transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 5463–\n5474.\nWei, X.-S.; Xie, C.-W.; and Wu, J. 2016. Mask-cnn: Local-\nizing parts and selecting descriptors for ﬁne-grained image\nrecognition. arXiv preprint arXiv:1605.06878.\nXie, E.; Wang, W.; Wang, W.; Sun, P.; Xu, H.; Liang, D.;\nand Luo, P. 2021. Segmenting transparent object in the wild\nwith transformer. arXiv preprint arXiv:2101.08461.\nYang, S.; Liu, S.; Yang, C.; and Wang, C. 2021. Re-rank\nCoarse Classiﬁcation with Local Region Enhanced Fea-\ntures for Fine-Grained Image Recognition. arXiv preprint\narXiv:2102.09875.\nYu, C.; Zhao, X.; Zheng, Q.; Zhang, P.; and You, X. 2018.\nHierarchical bilinear pooling for ﬁne-grained visual recog-\nnition. In Proceedings of the European conference on com-\nputer vision (ECCV), 574–589.\nZheng, H.; Fu, J.; Zha, Z.-J.; and Luo, J. 2019. Learning\ndeep bilinear transformation for ﬁne-grained image repre-\nsentation. arXiv preprint arXiv:1911.03621.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n6881–6890.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\nZhuang, P.; Wang, Y .; and Qiao, Y . 2020. Learning attentive\npairwise interaction for ﬁne-grained classiﬁcation. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 34, 13130–13137.\n860",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7843148112297058
    },
    {
      "name": "Computer science",
      "score": 0.781941831111908
    },
    {
      "name": "Discriminative model",
      "score": 0.7764002084732056
    },
    {
      "name": "Security token",
      "score": 0.6638576984405518
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6001796722412109
    },
    {
      "name": "Locality",
      "score": 0.4958826005458832
    },
    {
      "name": "Reuse",
      "score": 0.49139419198036194
    },
    {
      "name": "Machine learning",
      "score": 0.4406871497631073
    },
    {
      "name": "Feature learning",
      "score": 0.4267755448818207
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4116334617137909
    },
    {
      "name": "Engineering",
      "score": 0.11176124215126038
    },
    {
      "name": "Voltage",
      "score": 0.11069631576538086
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Waste management",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210109712",
      "name": "Max Planck Institute for Informatics",
      "country": "DE"
    }
  ]
}