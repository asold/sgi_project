{
  "title": "SciBERT: A Pretrained Language Model for Scientific Text",
  "url": "https://openalex.org/W2973154071",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4200857852",
      "name": "Beltagy, Iz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857850",
      "name": "Lo, Kyle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857849",
      "name": "Cohan, Arman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2163107094",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2147994374",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2740215900",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2738180183",
    "https://openalex.org/W2887672952",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2952867657",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2793978524",
    "https://openalex.org/W2962815673",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964179635",
    "https://openalex.org/W3105491236"
  ],
  "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
  "full_text": "arXiv:1903.10676v3  [cs.CL]  10 Sep 2019\nSC IBE RT: A Pretrained Language Model for Scientiﬁc T ext\nIz Beltagy Kyle Lo Arman Cohan\nAllen Institute for Artiﬁcial Intelligence, Seattle, W A, U SA\n{beltagy,kylel,armanc}@allenai.org\nAbstract\nObtaining large-scale annotated data for NLP\ntasks in the scientiﬁc domain is challeng-\ning and expensive. W e release S CI BE RT,\na pretrained language model based on\nBE RT (\nDevlin et al. , 2019) to address the lack\nof high-quality, large-scale labeled scientiﬁc\ndata. S CI BE RT leverages unsupervised\npretraining on a large multi-domain corpus\nof scientiﬁc publications to improve perfor-\nmance on downstream scientiﬁc NLP tasks.\nW e evaluate on a suite of tasks including\nsequence tagging, sentence classiﬁcation and\ndependency parsing, with datasets from a\nvariety of scientiﬁc domains. W e demon-\nstrate statistically signiﬁcant improvements\nover B E RT and achieve new state-of-the-\nart results on several of these tasks. The\ncode and pretrained models are available at\nhttps://github.com/allenai/scibert/.\n1 Introduction\nThe exponential increase in the volume of scien-\ntiﬁc publications in the past decades has made\nNLP an essential tool for large-scale knowledge\nextraction and machine reading of these docu-\nments. Recent progress in NLP has been driven\nby the adoption of deep neural models, but train-\ning such models often requires large amounts of\nlabeled data. In general domains, large-scale train-\ning data is often possible to obtain through crowd-\nsourcing, but in scientiﬁc domains, annotated data\nis difﬁcult and expensive to collect due to the ex-\npertise required for quality annotation.\nAs shown through E LM o (\nPeters et al. ,\n2018), GPT ( Radford et al. , 2018) and\nBERT (Devlin et al. , 2019), unsupervised pre-\ntraining of language models on large corpora\nsigniﬁcantly improves performance on many\nNLP tasks. These models return contextualized\nembeddings for each token which can be passed\ninto minimal task-speciﬁc neural architectures.\nLeveraging the success of unsupervised pretrain-\ning has become especially important especially\nwhen task-speciﬁc annotations are difﬁcult to\nobtain, like in scientiﬁc NLP . Y et while both\nBERT and E LM o have released pretrained models,\nthey are still trained on general domain corpora\nsuch as news articles and Wikipedia.\nIn this work, we make the following contribu-\ntions:\n(i) W e release S C IBERT , a new resource demon-\nstrated to improve performance on a range of NLP\ntasks in the scientiﬁc domain. S C IBERT is a pre-\ntrained language model based on B ERT but trained\non a large corpus of scientiﬁc text.\n(ii) W e perform extensive experimentation to\ninvestigate the performance of ﬁnetuning ver-\nsus task-speciﬁc architectures atop frozen embed-\ndings, and the effect of having an in-domain vo-\ncabulary .\n(iii) W e evaluate S C IBERT on a suite of tasks\nin the scientiﬁc domain, and achieve new state-of-\nthe-art (SOT A) results on many of these tasks.\n2 Methods\nBackground The B ERT model architecture\n(\nDevlin et al. , 2019) is based on a multilayer bidi-\nrectional Transformer ( V aswani et al. , 2017). In-\nstead of the traditional left-to-right language mod-\neling objective, B ERT is trained on two tasks: pre-\ndicting randomly masked tokens and predicting\nwhether two sentences follow each other. S C IB-\nERT follows the same architecture as B ERT but is\ninstead pretrained on scientiﬁc text.\nV ocabulary BERT uses W ordPiece (\nWu et al. ,\n2016) for unsupervised tokenization of the input\ntext. The vocabulary is built such that it contains\nthe most frequently used words or subword units.\nW e refer to the original vocabulary released with\nBERT as B A SE VO C A B.\nW e construct S C IVO C A B, a new W ordPiece vo-\ncabulary on our scientiﬁc corpus using the Sen-\ntencePiece\n1 library . W e produce both cased and\nuncased vocabularies and set the vocabulary size\nto 30K to match the size of B A SE VO C A B. The re-\nsulting token overlap between B A SE VO C A B and\nSC IVO C A B is 42%, illustrating a substantial dif-\nference in frequently used words between scien-\ntiﬁc and general domain texts.\nCorpus W e train S C IBERT on a random\nsample of 1.14M papers from Semantic\nScholar (\nAmmar et al. , 2018). This corpus\nconsists of 18% papers from the computer science\ndomain and 82% from the broad biomedical\ndomain. W e use the full text of the papers, not\njust the abstracts. The average paper length is\n154 sentences (2,769 tokens) resulting in a corpus\nsize of 3.17B tokens, similar to the 3.3B tokens\non which B ERT was trained. W e split sentences\nusing ScispaCy (\nNeumann et al. , 2019),2 which is\noptimized for scientiﬁc text.\n3 Experimental Setup\n3.1 T asks\nW e experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER)\n2. PICO Extraction (PICO)\n3. T ext Classiﬁcation (CLS)\n4. Relation Classiﬁcation (REL)\n5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where\nthe model extracts spans describing the Partici-\npants, Interventions, Comparisons, and Outcomes\nin a clinical trial paper (\nKim et al. , 2011). REL\nis a special case of text classiﬁcation where the\nmodel predicts the type of relation expressed be-\ntween two entities, which are encapsulated in the\nsentence by inserted special tokens.\n3.2 Datasets\nFor brevity , we only describe the newer datasets\nhere, and refer the reader to the references in T a-\nble\n1 for the older datasets. EBM-NLP ( Nye et al. ,\n2018) annotates PICO spans in clinical trial ab-\nstracts. SciERC ( Luan et al. , 2018) annotates en-\ntities and relations from computer science ab-\n1 https://github.com/google/sentencepiece\n2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC ( Jurgens et al. , 2018) and Sci-\nCite ( Cohan et al. , 2019) assign intent labels (e.g.\nComparison, Extension, etc.) to sentences from\nscientiﬁc papers that cite other papers. The Paper\nField dataset is built from the Microsoft Academic\nGraph (\nSinha et al. , 2015)3 and maps paper titles\nto one of 7 ﬁelds of study . Each ﬁeld of study\n(i.e. geography , politics, economics, business, so-\nciology , medicine, and psychology) has approxi-\nmately 12K training examples.\n3.3 Pretrained B E RT V ariants\nBE RT-Base W e use the pretrained weights for\nBERT -Base (\nDevlin et al. , 2019) released with the\noriginal B ERT code.4 The vocabulary is B A SE -\nVO C A B. W e evaluate both cased and uncased ver-\nsions of this model.\nSC IBE RT W e use the original B ERT code to\ntrain S C IBERT on our corpus with the same con-\nﬁguration and size as B ERT -Base. W e train 4\ndifferent versions of S C IBERT : ( i) cased or un-\ncased and ( ii) B A SE VO C A B or S C IVO C A B. The\ntwo models that use B A SE VO C A B are ﬁnetuned\nfrom the corresponding B ERT -Base models. The\nother two models that use the new S C IVO C A B are\ntrained from scratch.\nPretraining B ERT for long sentences can be\nslow . Following the original B ERT code, we set a\nmaximum sentence length of 128 tokens, and train\nthe model until the training loss stops decreasing.\nW e then continue training the model allowing sen-\ntence lengths up to 512 tokens.\nW e use a single TPU v3 with 8 cores. Training\nthe S C IVO C A B models from scratch on our corpus\ntakes 1 week\n5 (5 days with max length 128, then\n2 days with max length 512). The B A SE VO C A B\nmodels take 2 fewer days of training because they\naren’t trained from scratch.\nAll pretrained B ERT models are converted to\nbe compatible with PyT orch using the pytorch-\ntransformers library .\n6 All our models (Sec-\ntions 3.4 and 3.5) are implemented in PyT orch us-\ning AllenNLP ( Gardner et al. , 2017).\nCasing W e follow Devlin et al. (2019) in using\nthe cased models for NER and the uncased models\n3 https://academic.microsoft.com/\n4 https://github.com/google-research/bert\n5 BE RT’s largest model was trained on 16 Cloud TPUs for\n4 days. Expected 40-70 days ( Dettmers, 2019) on an 8-GPU\nmachine.\n6 https://github.com/huggingface/pytorch-transformers\nfor all other tasks. W e also use the cased models\nfor parsing. Some light experimentation showed\nthat the uncased models perform slightly better\n(even sometimes on NER) than cased models.\n3.4 Finetuning B E RT\nW e mostly follow the same architecture, opti-\nmization, and hyperparameter choices used in\nDevlin et al. (2019). For text classiﬁcation (i.e.\nCLS and REL), we feed the ﬁnal B ERT vector\nfor the [CLS] token into a linear classiﬁcation\nlayer. For sequence labeling (i.e. NER and PICO),\nwe feed the ﬁnal B ERT vector for each token into\na linear classiﬁcation layer with softmax output.\nW e differ slightly in using an additional condi-\ntional random ﬁeld, which made evaluation eas-\nier by guaranteeing well-formed entities. For DEP ,\nwe use the model from\nDozat and Manning (2017)\nwith dependency tag and arc embeddings of size\n100 and biafﬁne matrix attention over B ERT vec-\ntors instead of stacked BiLSTMs.\nIn all settings, we apply a dropout of 0.1\nand optimize cross entropy loss using Adam\n(\nKingma and Ba , 2015). W e ﬁnetune for 2 to 5\nepochs using a batch size of 32 and a learning rate\nof 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangu-\nlar schedule (\nHoward and Ruder , 2018) which is\nequivalent to the linear warmup followed by lin-\near decay (\nDevlin et al. , 2019). For each dataset\nand B ERT variant, we pick the best learning rate\nand number of epochs on the development set and\nreport the corresponding test results.\nW e found the setting that works best across\nmost datasets and models is 2 or 4 epochs and a\nlearning rate of 2e-5. While task-dependent, op-\ntimal hyperparameters for each task are often the\nsame across B ERT variants.\n3.5 Frozen B E RT Embeddings\nW e also explore the usage of B ERT as pre-\ntrained contextualized word embeddings, like\nELMo (\nPeters et al. , 2018), by training simple\ntask-speciﬁc models atop frozen B ERT embed-\ndings.\nFor text classiﬁcation, we feed each sentence\nof B ERT vectors into a 2-layer BiLSTM of size\n200 and apply a multilayer perceptron (with hid-\nden size 200) on the concatenated ﬁrst and last\nBiLSTM vectors. For sequence labeling, we\nuse the same BiLSTM layers and use a condi-\ntional random ﬁeld to guarantee well-formed pre-\ndictions. For DEP , we use the full model from\nDozat and Manning (2017) with dependency tag\nand arc embeddings of size 100 and the same\nBiLSTM setup as other tasks. W e did not ﬁnd\nchanging the depth or size of the BiLSTMs to sig-\nniﬁcantly impact results (\nReimers and Gurevych ,\n2017).\nW e optimize cross entropy loss using Adam,\nbut holding B ERT weights frozen and applying a\ndropout of 0.5. W e train with early stopping on\nthe development set (patience of 10) using a batch\nsize of 32 and a learning rate of 0.001.\nW e did not perform extensive hyperparameter\nsearch, but while optimal hyperparameters are go-\ning to be task-dependent, some light experimenta-\ntion showed these settings work fairly well across\nmost tasks and B ERT variants.\n4 Results\nT able\n1 summarizes the experimental results. W e\nobserve that S C IBERT outperforms B ERT -Base\non scientiﬁc tasks (+2.11 F1 with ﬁnetuning and\n+2.43 F1 without)\n8 . W e also achieve new SOT A\nresults on many of these tasks using S C IBERT .\n4.1 Biomedical Domain\nW e observe that S C IBERT outperforms B ERT -\nBase on biomedical tasks (+1.92 F1 with ﬁnetun-\ning and +3.59 F1 without). In addition, S C IB-\nERT achieves new SOT A results on BC5CDR\nand ChemProt (\nLee et al. , 2019), and EBM-\nNLP ( Nye et al. , 2018).\nSC IBERT performs slightly worse than SOT A\non 3 datasets. The SOT A model for JNLPBA\nis a BiLSTM-CRF ensemble trained on multi-\nple NER datasets not just JNLPBA (\nY oon et al. ,\n2018). The SOT A model for NCBI-disease\nis B IO BERT (Lee et al. , 2019), which is B ERT -\nBase ﬁnetuned on 18B tokens from biomedi-\ncal papers. The SOT A result for GENIA is\nin\nNguyen and V erspoor (2019) which uses the\nmodel from Dozat and Manning (2017) with part-\nof-speech (POS) features, which we do not use.\nIn T able 2, we compare S C IBERT results\nwith reported B IO BERT results on the subset of\ndatasets included in ( Lee et al. , 2019). Interest-\ning, S C IBERT outperforms B IO BERT results on\n7 The SOT A paper did not report a single score. W e\ncompute the average of the reported results for each class\nweighted by number of examples in each class.\n8 For rest of this paper, all results reported in this manner\nare averaged over datasets excluding UAS for DEP since we\nalready include LAS.\nField T ask Dataset SOT A B E RT-Base S C I BE RT\nFrozen Finetune Frozen Finetune\nBio\nNER\nBC5CDR ( Li et al. , 2016) 88.85 7 85.08 86.72 88.73 90.01\nJNLPBA ( Collier and Kim , 2004) 78.58 74.05 76.09 75.77 77.28\nNCBI-disease ( Dogan et al. , 2014) 89.36 84.06 86.88 86.39 88.57\nPICO EBM-NLP ( Nye et al. , 2018) 66.30 61.44 71.53 68.30 72.28\nDEP GENIA ( Kim et al. , 2003) - LAS 91.92 90.22 90.33 90.36 90.43\nGENIA ( Kim et al. , 2003) - UAS 92.84 91.84 91.89 92.00 91.99\nREL ChemProt ( Kringelum et al. , 2016) 76.68 68.21 79.14 75.03 83.64\nCS\nNER SciERC ( Luan et al. , 2018) 64.20 63.58 65.24 65.77 67.57\nREL SciERC ( Luan et al. , 2018) n/a 72.74 78.71 75.25 79.97\nCLS ACL-ARC ( Jurgens et al. , 2018) 67.9 62.04 63.91 60.74 70.98\nMulti CLS Paper Field n/a 63.64 65.37 64.38 65.71\nSciCite ( Cohan et al. , 2019) 84.0 84.31 84.85 85.42 85.49\nA verage 73.58 77.16 76.01 79.27\nT able 1: T est performances of all B E RT variants on all tasks and datasets. Bold indicates the SOT A result (multiple\nresults bolded if difference within 95% bootstrap conﬁdenc e interval). Keeping with past work, we report macro\nF1 scores for NER (span-level), macro F1 scores for REL and CL S (sentence-level), and macro F1 for PICO\n(token-level), and micro F1 for ChemProt speciﬁcally. For D EP , we report labeled (LAS) and unlabeled (UAS)\nattachment scores (excluding punctuation) for the same mod el with hyperparameters tuned for LAS. All results\nare the average of multiple runs with different random seeds .\nT ask Dataset B I O BE RT SC I BE RT\nNER\nBC5CDR 88.85 90.01\nJNLPBA 77.59 77.28\nNCBI-disease 89.36 88.57\nREL ChemProt 76.68 83.64\nT able 2: Comparing S CI BE RT with the reported\nBIO BE RT results on biomedical datasets.\nBC5CDR and ChemProt, and performs similarly\non JNLPBA despite being trained on a substan-\ntially smaller biomedical corpus.\n4.2 Computer Science Domain\nW e observe that S C IBERT outperforms B ERT -\nBase on computer science tasks (+3.55 F1 with\nﬁnetuning and +1.13 F1 without). In addition,\nSC IBERT achieves new SOT A results on ACL-\nARC (\nCohan et al. , 2019), and the NER part of\nSciERC ( Luan et al. , 2018). For relations in Sci-\nERC, our results are not comparable with those in\nLuan et al. (2018) because we are performing re-\nlation classiﬁcation given gold entities, while they\nperform joint entity and relation extraction.\n4.3 Multiple Domains\nW e observe that S C IBERT outperforms B ERT -\nBase on the multidomain tasks (+0.49 F1 with\nﬁnetuning and +0.93 F1 without). In addi-\ntion, S C IBERT outperforms the SOT A on Sci-\nCite (\nCohan et al. , 2019). No prior published\nSOT A results exist for the Paper Field dataset.\n5 Discussion\n5.1 Effect of Finetuning\nW e observe improved results via B ERT ﬁnetuning\nrather than task-speciﬁc architectures atop frozen\nembeddings (+3.25 F1 with S C IBERT and +3.58\nwith B ERT -Base, on average). For each scientiﬁc\ndomain, we observe the largest effects of ﬁnetun-\ning on the computer science (+5.59 F1 with S C IB-\nERT and +3.17 F1 with B ERT -Base) and biomed-\nical tasks (+2.94 F1 with S C IBERT and +4.61 F1\nwith B ERT -Base), and the smallest effect on mul-\ntidomain tasks (+0.7 F1 with S C IBERT and +1.14\nF1 with B ERT -Base). On every dataset except\nBC5CDR and SciCite, B ERT -Base with ﬁnetuning\noutperforms (or performs similarly to) a model us-\ning frozen S C IBERT embeddings.\n5.2 Effect of S C IVO C A B\nW e assess the importance of an in-domain sci-\nentiﬁc vocabulary by repeating the ﬁnetuning ex-\nperiments for S C IBERT with B A SE VO C A B. W e\nﬁnd the optimal hyperparameters for S C IBERT -\nBA SE VO C A B often coincide with those of S C IB-\nERT -S C IVO C A B.\nA veraged across datasets, we observe +0.60 F1\nwhen using S C IVO C A B. For each scientiﬁc do-\nmain, we observe +0.76 F1 for biomedical tasks,\n+0.61 F1 for computer science tasks, and +0.11 F1\nfor multidomain tasks.\nGiven the disjoint vocabularies (Section\n2) and\nthe magnitude of improvement over B ERT -Base\n(Section 4), we suspect that while an in-domain\nvocabulary is helpful, S C IBERT beneﬁts most\nfrom the scientiﬁc corpus pretraining.\n6 Related W ork\nRecent work on domain adaptation of BERT in-\ncludes B IO BERT (\nLee et al. , 2019) and C LIN -\nIC A L BERT (Alsentzer et al. , 2019; Huang et al. ,\n2019). B IO BERT is trained on PubMed ab-\nstracts and PMC full text articles, and C LIN -\nIC A L BERT is trained on clinical text from the\nMIMIC-III database ( Johnson et al. , 2016). In\ncontrast, S C IBERT is trained on the full text of\n1.14M biomedical and computer science papers\nfrom the Semantic Scholar corpus (\nAmmar et al. ,\n2018). Furthermore, S C IBERT uses an in-domain\nvocabulary (S C IVO C A B) while the other above-\nmentioned models use the original B ERT vocab-\nulary (B A SE VO C A B).\n7 Conclusion and Future W ork\nW e released S C IBERT , a pretrained language\nmodel for scientiﬁc text based on B ERT . W e evalu-\nated S C IBERT on a suite of tasks and datasets from\nscientiﬁc domains. S C IBERT signiﬁcantly outper-\nformed B ERT -Base and achieves new SOT A re-\nsults on several of these tasks, even compared to\nsome reported B IO BERT (\nLee et al. , 2019) results\non biomedical tasks.\nFor future work, we will release a version of\nSC IBERT analogous to B ERT -Large, as well as ex-\nperiment with different proportions of papers from\neach domain. Because these language models are\ncostly to train, we aim to build a single resource\nthat’s useful across multiple domains.\nAcknowledgment\nW e thank the anonymous reviewers for their com-\nments and suggestions. W e also thank W aleed\nAmmar, Noah Smith, Y oav Goldberg, Daniel\nKing, Doug Downey , and Dan W eld for their help-\nful discussions and feedback. All experiments\nwere performed on\nbeaker.org and supported\nin part by credits from Google Cloud.\nReferences\nEmily Alsentzer, John R. Murphy, Willie Boag, W ei-\nHung W eng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly available clini-\ncal bert embeddings. In ClinicalNLP workshop at\nNAACL.\nW aleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, V u Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, T yler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Lu W ang,\nChris Wilhelm, Zheng Y uan, Madeleine van Zuylen,\nand Oren Etzioni. 2018. Construction of the litera-\nture graph in semantic scholar. In NAACL.\nArman Cohan, W aleed Ammar, Madeleine\nvan Zuylen, and Field Cady. 2019.\nStructural scaffolds for citation intent classiﬁcation in scientiﬁc publications .\nIn NAACL-HLT, pages 3586–3596, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nNigel Collier and Jin-Dong Kim. 2004. Introduction\nto the bio-entity recognition task at jnlpba. In NLP-\nBA/BioNLP.\nTim Dettmers. 2019. TPUs vs\nGPUs for Transformers (BER T).\nhttp://timdettmers.com/2018/10/17/tpus-vs-gpus-for- transformers-bert/.\nAccessed: 2019-02-22.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nRezarta Islamaj Dogan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI disease corpus: A resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics , 47:1–10.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. ICLR.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nT afjord, Pradeep Dasigi, Nelson F . Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. Allennlp: A deep semantic natural language\nprocessing platform. In arXiv:1803.07640 .\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342 .\nAlistair E. W . Johnson, T om J. Pollard aand Lu Shen,\nLiwei H. Lehman, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, , and Roger G. Mark. 2016.\nMimic-iii, a freely accessible critical care database.\nIn Scientiﬁc Data, 3:160035 .\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Daniel Jurafsky. 2018. Measuring\nthe evolution of a scientiﬁc ﬁeld through citation\nframes. TACL, 06:391–406.\nJin-Dong Kim, T omoko Ohta, Y uka T ateisi, and\nJun’ichi Tsujii. 2003. GENIA corpus - a semanti-\ncally annotated corpus for bio-textmining. Bioinfor-\nmatics, 19:i180i182.\nSu Kim, David Mart´ ınez, Lawrence Cavedon, and Lars\nY encken. 2011. Automatic classiﬁcation of sen-\ntences to support evidence based medicine. In BMC\nBioinformatics.\nDiederik P . Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. ICLR.\nJens Kringelum, Sonny Kim Kjærulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier T aboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. In Database.\nJinhyuk Lee, W onjin Y oon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBER T: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. In arXiv:1901.08746 .\nJiao Li, Y ueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan W ei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016. BioCreative V CDR task\ncorpus: a resource for chemical disease relation\nextraction. Database : the journal of biological\ndatabases and curation .\nY i Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In EMNLP.\nMark Neumann, Daniel King, Iz Beltagy, and W aleed\nAmmar. 2019. ScispaCy: Fast and robust mod-\nels for biomedical natural language processing. In\narXiv:1902.07669 .\nDat Quoc Nguyen and Karin M. V erspoor. 2019. From\npos tagging to dependency parsing for biomedical\nevent extraction. BMC Bioinformatics , 20:1–13.\nBenjamin Nye, Junyi Jessy Li, Roma Patel, Y infei\nY ang, Iain James Marshall, Ani Nenkova, and By-\nron C. W allace. 2018. A corpus with multi-level an-\nnotations of patients, interventions and outcomes to\nsupport language processing for medical literature.\nIn ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In NAACL-HLT.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nNils Reimers and Iryna Gurevych. 2017. Optimal hy-\nperparameters for deep lstm-networks for sequence\nlabeling tasks. In EMNLP.\nArnab Sinha, Zhihong Shen, Y ang Song, Hao Ma, Dar-\nrin Eide, Bo-June Paul Hsu, and Kuansan W ang.\n2015. An overview of microsoft academic service\n(MAS) and applications. In WWW.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nY onghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, W olfgang Macherey,\nMaxim Krikun, Y uan Cao, Qin Gao, Jeff Klingner,\nApurva Shah, Melvin Johnson, Xiaobing Liu,\nLukasz Kaiser, Stephan Gouws, Y oshikiyo Kato,\nT aku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, W ei W ang, Cliff Y oung, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol V inyals,\nGregory S. Corrado, Macduff Hughes, and Jeffrey\nDean. 2016. Google’s neural machine translation\nsystem: Bridging the gap between human and ma-\nchine translation. abs/1609.08144.\nW onjin Y oon, Chan Ho So, Jinhyuk Lee, and Jaewoo\nKang. 2018. CollaboNet: collaboration of deep neu-\nral networks for biomedical named entity recogni-\ntion. In DTMBio workshop at CIKM .",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8195391893386841
    },
    {
      "name": "Suite",
      "score": 0.7175433039665222
    },
    {
      "name": "Natural language processing",
      "score": 0.6625299453735352
    },
    {
      "name": "Dependency grammar",
      "score": 0.6245095729827881
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6238932609558105
    },
    {
      "name": "Sentence",
      "score": 0.6218714714050293
    },
    {
      "name": "Parsing",
      "score": 0.6216586828231812
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.609478771686554
    },
    {
      "name": "Language model",
      "score": 0.5939752459526062
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5376139879226685
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5210601687431335
    },
    {
      "name": "Code (set theory)",
      "score": 0.5189568400382996
    },
    {
      "name": "Programming language",
      "score": 0.13442999124526978
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}