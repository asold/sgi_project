{
  "title": "A Vision Transformer Approach for Efficient Near-Field SAR Super-Resolution under Array Perturbation",
  "url": "https://openalex.org/W4293689539",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2972255451",
      "name": "Josiah W. Smith",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A4293689683",
      "name": "Yusef Alimam",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A4293689684",
      "name": "Geetika Vedula",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A2017768859",
      "name": "Murat Torlak",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137925293",
    "https://openalex.org/W4206943292",
    "https://openalex.org/W2986133977",
    "https://openalex.org/W3135214285",
    "https://openalex.org/W2963927147",
    "https://openalex.org/W3131557581",
    "https://openalex.org/W2810566158",
    "https://openalex.org/W4205899675",
    "https://openalex.org/W3100303558",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3125372737",
    "https://openalex.org/W3040830257",
    "https://openalex.org/W3161237630",
    "https://openalex.org/W3200081046",
    "https://openalex.org/W3041937076",
    "https://openalex.org/W2920459301",
    "https://openalex.org/W1999212008",
    "https://openalex.org/W3112779358",
    "https://openalex.org/W2155684415",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6807103956",
    "https://openalex.org/W6802648153",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4206522033",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "In this paper, we develop a novel super-resolution algorithm for near-field\\nsynthetic-aperture radar (SAR) under irregular scanning geometries. As\\nfifth-generation (5G) millimeter-wave (mmWave) devices are becoming\\nincreasingly affordable and available, high-resolution SAR imaging is feasible\\nfor end-user applications and non-laboratory environments. Emerging\\napplications such freehand imaging, wherein a handheld radar is scanned\\nthroughout space by a user, unmanned aerial vehicle (UAV) imaging, and\\nautomotive SAR face several unique challenges for high-resolution imaging.\\nFirst, recovering a SAR image requires knowledge of the array positions\\nthroughout the scan. While recent work has introduced camera-based positioning\\nsystems capable of adequately estimating the position, recovering the algorithm\\nefficiently is a requirement to enable edge and Internet of Things (IoT)\\ntechnologies. Efficient algorithms for non-cooperative near-field SAR sampling\\nhave been explored in recent work, but suffer image defocusing under position\\nestimation error and can only produce medium-fidelity images. In this paper, we\\nintroduce a mobile-friend vision transformer (ViT) architecture to address\\nposition estimation error and perform SAR image super-resolution (SR) under\\nirregular sampling geometries. The proposed algorithm, Mobile-SRViT, is the\\nfirst to employ a ViT approach for SAR image enhancement and is validated in\\nsimulation and via empirical studies.\\n",
  "full_text": "A VISION TRANSFORMER APPROACH FOR EFFICIENT\nNEAR -FIELD IRREGULAR SAR S UPER -RESOLUTION\nProc. IEEE WMCS(ACCEPTED )\nJosiah W. Smith\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Dallas\nRichardson, TX 75080\njosiah.smith@utdallas.edu\nYusef Alimam\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Dallas\nRichardson, TX 75080\nGeetika Vedula\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Dallas\nRichardson, TX 75080\nMurat Torlak∗\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Dallas\nRichardson, TX 75080\ntorlak@utdallas.edu\n©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or\nfuture media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works,\nfor resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\ndoi:10.1109/WMCS55582.2022.9866326\nABSTRACT\nIn this paper, we develop a novel super-resolution algorithm for near-field synthetic-aperture radar\n(SAR) under irregular scanning geometries. As fifth-generation (5G) millimeter-wave (mmWave)\ndevices are becoming increasingly affordable and available, high-resolution SAR imaging is feasible\nfor end-user applications and non-laboratory environments. Emerging applications such freehand\nimaging, wherein a handheld radar is scanned throughout space by a user, unmanned aerial vehicle\n(UA V) imaging, and automotive SAR face several unique challenges for high-resolution imaging.\nFirst, recovering a SAR image requires knowledge of the array positions throughout the scan. While\nrecent work has introduced camera-based positioning systems capable of adequately estimating the\nposition, recovering the algorithm efficiently is a requirement to enable edge and Internet of Things\n(IoT) technologies. Efficient algorithms for non-cooperative near-field SAR sampling have been\nexplored in recent work, but suffer image defocusing under position estimation error and can only\nproduce medium-fidelity images. In this paper, we introduce a mobile-friend vision transformer (ViT)\narchitecture to address position estimation error and perform SAR image super-resolution (SR) under\nirregular sampling geometries. The proposed algorithm, Mobile-SRViT, is the first to employ a ViT\napproach for SAR image enhancement and is validated in simulation and via empirical studies.\nKeywords 5G · drone mmWave imaging · freehand imaging · irregular sampling · mmWave imaging · synthetic\naperture radar (SAR)\n1 Introduction\nMillimeter-wave (mmWave) imaging systems have garnered significant attention in recent years as ultrawideband\n(UWB) devices are becoming increasingly affordable. Radar devices operating in the mmWave spectrum have been\napplied in applications such as concealed weapon detection [1], non-destructive testing [2], automotive imaging [3],\n∗The work of Murat Torlak (while serving at NSF) was supported by NSF.\nThis work was supported in part by Texas Instruments through the Foundational Technology Research Centre and the Texas\nAnalog Center of Excellence.\narXiv:2305.02074v2  [cs.CV]  27 Jun 2023\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\nand hand gesture recognition and tracking [4–6]. Synthetic aperture radar (SAR) imaging is of particular interest and\ninvolves scanning a radar across space to create a synthetic aperture much larger than the radar itself [7]. Traditional\nSAR imaging requires high-precision laboratory equipment for exact positioning of the antennas throughout the scan.\nEfficient SAR imaging algorithms have been explored extensively in the literature [1, 8, 9] leveraging the fast Fourier\ntransform (FFT) to recover the image from the radar data. These efficient algorithms strictly require specific synthetic\naperture geometries, e.g., planar [1, 8], cylindrical [9], etc., to achieve high-resolution imaging. However, with the\nemergence of fifth-generation (5G) and Internet of Things (IoT) technologies, near-field SAR sensing at the edge\nhas already received attention at both the system and algorithm levels [10, 11]. One application of interest is known\nas freehand imaging and involves using a smartphone or handheld radar device to perform the SAR scan. Although\nthese applications operate in similar frequencies to traditional laboratory SAR [12], they suffer from two primary\nconstraints: 1) the resulting SAR array generally does not conform to the traditional geometries required by existing\nalgorithms [1, 8, 9] and 2) since the image computation typically takes place on a low-power or mobile device, the\ncomputational load must be reduced compared to conventional imaging. As a result, recovering high-fidelity images\nunder such conditions remains an open challenge.\nPrevious work on efficient near-field SAR imaging proposes a decomposition of the irregular multiple-input multiple-\noutput SAR (MIMO-SAR) array into a multi-planar imaging scenario wherein the multistatic samples are taken across\na volume in space and then projected onto a reference plane for efficient image recovery, referred to as the efficient\nmulti-planar multistatic (EMPM) algorithm [11]. This algorithm achieves similar imaging quality to the gold-standard\nbackprojection algorithm (BPA) at a fraction of the computational load. However, prior analyses [11, 13] do not take\ninto account errors in the position estimation present in practical implementations [10]. Such position errors cause\ndefocusing and distortion to SAR images as the algorithm improperly computes the matched filter weights based on the\nnoisy position estimates. Without knowledge of the exact positions, removing distortion present in images recovered\nfrom either the EMPM or BPA remains an open challenge. For a practical system, the EMPM is capable of efficiently\nreconstructing only a medium-fidelity image.\nSeparately, deep learning approaches for optical image super-resolution have been extended into the radar domain for\nSAR image super-resolution [6, 14–16]. Using convolutional neural network (CNN) architectures, previous efforts have\nseen success in improving SAR resolution [14, 15] and removing multistatic artifacts [16]. However, these techniques\noperate on SAR images collected using traditional techniques in laboratory environments and are not suitable for the\nirregular sampling geometry explored in this paper. Nevertheless, deep learning has seen tremendous success in both the\noptical domain, for image restoration [17] and super-resolution [18], and radar domain [14–16]. Hence, deep learning\nmay be a suitable solution for near-field irregular SAR artifact mitigation and super-resolution.\nSeparately, recent advances in computer vision have seen a shift from CNN-based architectures towards the attention\nmechanism [19] using Vision Transformer (ViT) techniques [20] to achieve performance gains with smaller model\nsizes [21–23]. In [23], the MobileViT architecture is presented leveraging a transformer architecture for image\nclassification. Later, the transformer architecture was employed for optical image super-resolution and artifact\nmitigation in [17]. Transformer techniques have appeared in recent work on radar image classification [24] and\ngesture recognition [5]; however, transformers have yet to be employed for SAR image super-resolution. In this\npaper, we introduce a novel transformer-based architecture for SAR image super-resolution under irregular sampling\ngeometries called Mobile-SRViT. The proposed algorithm operates on images recovered by the EMPM algorithm [11]\nand produces high-fidelity images of intricate targets. We validate our mobile-friendly algorithm using simulation and\nempirical data from a near-field SAR scenario with irregular scanning geometry.\nThe remainder of this paper is formatted as follows. Section 2 overviews the requisite signal model for near-field\nirregularly sampled SAR. In Section 3, we detail our proposed algorithm. Experimental results are included in Section\n4 followed finally by conclusions.\n2 Signal Model\nIn this section, the signal model for non-cooperative SAR in the near-field is briefly introduced. Considering a\nmulti-planar multistatic array with transmitter (Tx) and receiver (Rx) antennas located at (xT , yT , zℓ) and (xR, yR, zℓ),\nrespectively, the received signal at theℓ-th Tx/Rx pair can be modeled as\ns(xT , xR, yT , yR, zℓ, t) =\nZZ o(x, y)\nRT\nℓ RR\nℓ\np\n\u0012\nt − RT\nℓ\nc − RR\nℓ\nc\n\u0013\ndxdy, (1)\n2\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\nassuming the Born approximation and an isotropic antenna, where o(x, y) is known as the target reflectivity at the plane\nz = z0, p(t) is the signal at the transmitter, t is the fast-time variable, c is the speed of light, and RT\nℓ , RR\nℓ are given by\nRT\nℓ =\n\u0002\n(xT − x)2 + (yT − y)2 + (zℓ − z0)2\u00031\n2 ,\nRR\nℓ =\n\u0002\n(xR − x)2 + (yR − y)2 + (zℓ − z0)2\u00031\n2 .\n(2)\nTaking the Fourier transform of (1) with respect to time yields the frequency spectrum, which can be expressed as\ns(xT , xR, yT , yR, zℓ, f) =P(f)\nZZ o(x, y)\nRT\nℓ RR\nℓ\ne−jk(RT\nℓ +RR\nℓ )dxdy, (3)\nwhere P(f) is the spectrum of p(t) and the instantaneous wavenumber is given by k = 2πf/c.\nBy the analysis in [11], the multi-planar multistatic data can be projected to an equivalent virtual planar sampled array\nby\nˆs(x′, y′, f) ≈ s(xT , xR, yT , yR, zℓ, f)ejkβℓ, (4)\nwhere\nβℓ = 2dz\nℓ + (dx\nℓ )2 + (dy\nℓ )2\n4Z0\n, (5)\nand ˆs(x′, y′, f) is the virtual planar monostatic array with virtual coplanar antenna positions on the Z0 plane located at\nthe midpoint of each Tx/Rx pair. After this compensation step, the image can be recovered using the efficient Fourier-\nbased range migration algorithm (RMA) yielding the 2-D reflectivity image ˆo(x, y) [7, 11]. Besides the EMPM, whose\ncomplexity is O(N2 log(N)), the BPA also does not require specific sampling geometry. However, the complexity of\nthe BPA is O(N4), which is generally prohibitive for end-user applications [8, 11]. Furthermore, the recovered image\nfrom either algorithm is degraded by distortion and defocusing due to errors in the position estimates. In attempt to\naddress these artifacts and improve SAR imaging resolution, we propose a novel Mobile-SRViT architecture.\nFigure 1: Operation of the Mobile-SRViT: the low resolution image produced by the EMPM algorithm is restored by\nthe Mobile-SRViT algorithm. The ground truth image is shown for reference.\n3 Irregular SAR Super-Resolution using Vision Transformers\nIn this section, we detail the proposed transformer-based approach for near-field irregular SAR super-resolution and\nartifact mitigation. An overview of the Mobile-SRViT algorithm is given in Fig. 1. The raw radar data are processed\nby the EMPM algorithm to produce a low-resolution image with distortion, blur, and defocusing caused by imaging\nnon-idealities. The proposed Mobile-SRViT operates on this image to produce a super-resolution image, restoring\nimage quality while preserving key high-frequency details of the targets.\n3\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\nFigure 2: Mobile-SRViT architecture.\nThe architecture of the Mobile-SRViT is based on the MobileViT network employed for image classification in [23].\nSince our network is designed for image-to-image processing, the convolution layers are modified to adhere to a fully\nconvolutional neural network (FCNN) framework, similar to that of [6]. Fig. 2 details the implementation of the\nMobile-SRViT algorithm, where “MV2” refers to the MobileNetV2 block proposed in [22]. Our algorithm adopts the\napproach of [23] such that the image is first processed by several MobileNetV2 convolution blocks before alternating\nbetween MobileViT and MobileNetV2 operations. The MobileViT block is intended to model the global and local\ninformation of the input data with fewer parameters than the traditional ViT [20]. Given an input tensorX ∈ RH×W×C,\nwhere H is the height, W is the width, and C is the number of channels, the MobileViT block first applies a 3 × 3\nconvolution layer followed by a 1 × 1, or pointwise, convolution layer to produce a tensor XL ∈ RH×W×d. The last\n1 × 1 convolution layer reduces the channels to match the input image. In order to learn global representations, XL is\nunfolded into N non-overlapping patches XU ∈ RP×N×d, where P = 4and N = HW/4. Each of the P patches are\nprocessed using a transformer architecture to encode the inter-patch relationships yielding\nXG(p) =Transformer(XU (p)), p ∈ [1, . . . , P]. (6)\nWhereas most ViT implementations lose the positional location of each patch [20, 21], the MobileViT retains the\npatch order and the pixel order within each patch. As a result, XG ∈ RP×N×d can be directly folded to obtain\nXF ∈ RH×W×d. The resulting tensor XF is projected to a low C-dimensional space via a 1 × 1 convolution layer\nbefore being concatenated with X yielding XO ∈ RH×W×2C. Finally, using a 3 × 3 convolution, XO is fused to\nform the output tensor Y of identical size to X. Interestingly, the receptive field of the MobileViT block is H × W\nbecause XU (p) encodes local information from a 3 ×3 region via convolutions and each pixel inXG(p) encodes global\ninformation over P patches [23]. Our implementation maintains C = 16until the last MobileViT block where C = 32,\nd = 2C for each MobileViT block, and L = {2, 4, 3}. The images are of size 256 × 256 and the patch size employed\nis 16 × 16. With this architecture, the proposed Mobile SR-ViT has 69,122 parameters. Loss is computed using the\npixel-to-pixel L1 metric as\nLp2p = ||XSR − XHR||1. (7)\nPrior attempts at near-field SAR super-resolution have been purely CNN-based [6, 14 –16], but the Mobile-SRViT\ndetailed in this paper is the first to leverage a transformer architecture for SAR imaging.\n3.1 Training the Mobile-SRViT\nOur efficient algorithm is trained for 50 epochs using an ADAM optimizer on a single RTX3090 GPU with 24 GB of\nmemory using 4096 samples for the training process and 1024 samples for evaluation. The training data is generated\nusing the procedure detailed in [6, 11, 14]. Whereas prior methods [6, 14–16] train on SAR images from exclusively\npoint scatterers, we introduce more sophisticated targets consisting of solid and hollow objects in addition to randomly\nplaced point scatterers, such as the example images in Fig. 1. By including more complex targets in the training dataset,\nour algorithm is able to generalize for solid and hollow targets. Each sample is generated with additive white Gaussian\nnoise (AWGN) with a signal-to-noise ratio in the range [−10, 50] dB and includes AWGN positioning errors with a\nstandard deviation of 1 mm along the x, y, and z-directions to emulate a practical scenario. The training process was\napproximately 6 hours with an inference time during validation of 10 ms per sample.\n4\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure 3: Imaging results using the Mobile-SRViT on synthetic data. The images in the first column, (a) and (d), are\nproduced by the EMPM algorithm and input to the Mobile-SRViT. The images in the second column, (b) and (e), are\nthe super-resolution images output from the Mobile-SRViT. The images in the third column, (c) and (f), are the ground\ntruth images.\n4 Experimental Results\nIn this section, we conduct simulation and empirical experiments to verify the proposed algorithm. To evaluate the\nperformance of the SRViT algorithm, we first compare the numerical performance of the Mobile-SRViT to the EMPM,\nBPA, and RMA. The RMA requires a planar sampling and is unable to recover an image from an irregularly sampled\narray, as discussed in [11]. The gold-standard BPA has no requirements for SAR array geometry and is well-suited for\nirregular scanning geometries. However, it is computationally prohibitive, particularly for mobile applications, as it\ncomputes the pixel-wise matched filter for every sampling location and frequency. The EMPM, on the other hand, is an\nefficient RMA-based algorithm but assumes the samples are taken across a small volume relative to a reference plane\nZ0 [11]. The proposed Mobile-SRViT algorithm attempts to compensate for the distortion present in the EMPM images\ndue to these assumptions in addition to imaging nonidealities.\nTable 1: Quantitative performance of the Mobile-SRViT compared to the BPA, EMPM, and RMA.\nMetrics Mobile-SRViT BPA EMPM RMA\nPSNR (dB) 36.907 26.33 20.20 10.158\nRMSE 0.015 0.044 0.105 0.276\nTime (s) 1.113 1324.8 1.103 1.103\nUsing a test dataset consisting of 1024 samples similar to those in the training dataset but never seen by the network,\nwe apply the Mobile-SRViT, BPA, EMPM, and RMA to the samples and measure the peak signal-to-noise ratio\n(PSNR), root-mean-square error (RMSE), and computation time per sample. Results are shown in Table 1 with the best\nevaluations marked in bold-face. All experiments are conducted on a desktop PC equipped with a 12-core AMD Ryzen\n9 3900X running at 4.6 GHz with 64 GB of memory. As expected, the RMA is unable to achieve image quality, in\nterms of PSNR and RMSE, comparable to the BPA or EMPM but is highly efficient. The EMPM achieves identical\ncomputation time to the RMA with much higher PSNR and lower RMSE, approaching the BPA. On the other hand, the\nBPA boasts the highest PSNR and lowest RMSE of the classical algorithms but requires a significantly large amount of\ncomputation time. The Mobile-SRViT is superior to the other algorithms, even outperforming the BPA in PSNR and\nRMSE, with a total computation time of 1.113 seconds required to compute the EMPM and pass the image through\n5\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\nthe network. This qualitative analysis demonstrates the superiority of the proposed method in comparison to previous\ntechniques in terms of both computational efficiency and image quality.\nWe further validate the performance of the proposed algorithm via visual inspection of both simulated and empirical\ndata. Two samples from the testing dataset are compared in Fig. 3. For each sample, the proposed SAR super-resolution\nnetwork is able to recover the solid object in addition to the point scatterers and mitigate distortion caused by position\nestimation errors and system limitations. The super-resolution images, Figs. 3b and 3e, are quite similar to the ideal\nimages, Figs. 3c and 3f, showing an improvement over the medium-fidelity images recovered by the EMPM.\nTo evaluate the performance of our proposed algorithm on empirical data, we first perform a SAR scan with irregular\nscanning geometry, as shown in Fig. 4a. After reconstructing the image with the EMPM, shown in Fig. 4b, the\nMobile-SRViT is applied to achieve the super-resolution image shown in 4c. The proposed algorithm not only recovers\na better-resolved image but also mitigates multistatic artifacts visible in the EMPM image [7, 11].\n(a)\n(b)\n (c)\nFigure 4: Imaging results using the Mobile-SRViT on empirical data from a near-field SAR scenario with irregular\nsampling geometry and positioning error: (a) SAR sampling geometry, (b) the image reconstructed by the EMPM and\n(c) the super-resolution image produced by the Mobile-SRViT.\n5 Conclusion\nIn this paper, we introduced a novel algorithm for near-field SAR super-resolution under irregular sampling geometries\nusing a vision transformer architecture. Compared to previous methods for SAR super-resolution, our improved\ntechnique addresses the more challenging problem of image enhancement under non-ideal sampling conditions. The\nproposed algorithm enables numerous applications such as freehand smartphone imaging, UA V SAR, and automotive\nimaging. Using the efficient medium-fidelity EMPM algorithm developed in [11], we train a novel image-to-image\nnetwork using state-of-the-art CNN [22] and ViT [23] techniques suitable for mobile applications with low latency and a\n6\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\nsmall model size. The robust algorithm is verified in simulation and empirical studies to outperform the state-of-the-art\ntechniques in terms of both image quality and computational complexity.\nReferences\n[1] D. M. Sheen, D. L. McMakin, and T. E. Hall, “Three-dimensional millimeter-wave imaging for concealed weapon\ndetection,” IEEE Trans. Microw. Theory Techn., vol. 49, no. 9, pp. 1581–1592, Sep. 2001.\n[2] M. T. Ghasr, S. Kharkovsky, R. Bohnert, B. Hirst, and R. Zoughi, “30 GHz linear high-resolution and rapid\nmillimeter wave imaging system for NDE,”IEEE Trans. Antennas Propag., vol. 61, no. 9, pp. 4733–4740, Jun.\n2013.\n[3] M. Garcia-Fernandez, Y . Alvarez-Lopez, and F. L. Heras, “3D-SAR processing of UA V-mounted GPR measure-\nments: Dealing with non-uniform sampling,” in Proc. 14th Eur. Conf. Antennas Propag. (EuCAP), Copenhagen,\nDenmark, Aug. 2020, pp. 1–5.\n[4] J. W. Smith, S. Thiagarajan, R. Willis, Y . Makris, and M. Torlak, “Improved static hand gesture classification on\ndeep convolutional neural networks using novel sterile training technique,”IEEE Access, vol. 9, pp. 10 893–10 902,\nJan. 2021.\n[5] L. Zheng, J. Bai, X. Zhu, L. Huang, C. Shan, Q. Wu, and L. Zhang, “Dynamic hand gesture recognition in\nin-vehicle environment based on FMCW radar and transformer,”Sensors, vol. 21, no. 19, p. 6368, Sep. 2021.\n[6] J. W. Smith, O. Furxhi, and M. Torlak, “An FCNN-based super-resolution mmWave radar framework for contactless\nmusical instrument interface,” IEEE Trans. Multimedia, pp. 1–1, May 2021.\n[7] M. E. Yanik and M. Torlak, “Near-field MIMO-SAR millimeter-wave imaging with sparsely sampled aperture\ndata,” IEEE Access, vol. 7, pp. 31 801–31 819, Mar. 2019.\n[8] M. E. Yanik, D. Wang, and M. Torlak, “Development and demonstration of MIMO-SAR mmWave imaging\ntestbeds,” IEEE Access, vol. 8, pp. 126 019–126 038, Jul. 2020.\n[9] J. W. Smith, M. E. Yanik, and M. Torlak, “Near-field MIMO-ISAR millimeter-wave imaging,” in Proc. IEEE\nRadar Conf. (RadarConf), Florance, Italy, Sep. 2020, pp. 1–6.\n[10] G. Álvarez Narciandi, J. Laviada, and F. Las-Heras, “Towards turning smartphones into mmWave scanners,”IEEE\nAccess, vol. 9, pp. 45 147–45 154, Mar. 2021.\n[11] J. W. Smith and M. Torlak, “Efficient 3-D near-field MIMO-SAR imaging for irregular scanning geometries,”\nIEEE Access, vol. 10, pp. 10 283–10 294, Jan. 2022.\n[12] M. E. Yanik, D. Wang, and M. Torlak, “3-D MIMO-SAR imaging using multi-chip cascaded millimeter-wave\nsensors,” in Proc. IEEE Global Conf. Signal Inf. Process. (GlobalSIP), Ottawa, ON, Canada, Nov. 2019, pp. 1–5.\n[13] G. Álvarez Narciandi, J. Laviada, Y . Álvarez López, G. Ducournau, C. Luxey, C. Belem-Goncalves, F. Gianesello,\nN. Nachabe, C. D. Rio, and F. Las-Heras, “Freehand system for antenna diagnosis based on amplitude-only data,”\nIEEE Trans. Antennas Propag., vol. 69, no. 8, pp. 4988–4998, Feb. 2021.\n[14] J. Gao, B. Deng, Y . Qin, H. Wang, and X. Li, “Enhanced radar imaging using a complex-valued convolutional\nneural network,” IEEE Geosci. Remote Sens. Lett., vol. 16, no. 1, pp. 35–39, Sep. 2018.\n[15] H. Jing, S. Li, K. Miao, S. Wang, X. Cui, G. Zhao, and H. Sun, “Enhanced millimeter-wave 3-D imaging via\ncomplex-valued fully convolutional neural network,”Electronics, vol. 11, no. 1, p. 147, 2022.\n[16] Y . Dai, T. Jin, H. Li, Y . Song, and J. Hu, “Imaging enhancement via CNN in MIMO virtual array-based radar,”\nIEEE Trans. Geosci. Remote Sens., vol. 59, no. 9, pp. 7449–7458, Nov. 2021.\n[17] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “SwinIR: Image restoration using swin\ntransformer,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshop (ICCVW), Montreal, Canada, Oct. 2021, pp.\n1833–1844.\n[18] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep residual networks for single image super-\nresolution,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Honolulu, HI, USA, Jul. 2017, pp.\n136–144.\n[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention\nis all you need,” in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), Long Beach, CA, USA, Dec. 2017, pp.\n5998–6008.\n7\nA Vision Transformer Approach Proc. IEEE WMCS(ACCEPTED )\n[20] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” arXiv\npreprint arXiv:2010.11929, 2020.\n[21] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision\ntransformer using shifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Montreal, Canada, Oct.\n2021, pp. 10 012–10 022.\n[22] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “MobileNetV2: Inverted residuals and linear\nbottlenecks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Salt Lake City, Utah, USA, Jun. 2018,\npp. 4510–4520.\n[23] S. Mehta and M. Rastegari, “MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer,”\narXiv preprint arXiv:2110.02178, Oct. 2021.\n[24] H. Dong, L. Zhang, and B. Zou, “Exploring vision transformers for polarimetric SAR image classification,” IEEE\nTrans. Geosci. Remote Sens., Nov. 2021.\n8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8216177225112915
    },
    {
      "name": "Synthetic aperture radar",
      "score": 0.723092794418335
    },
    {
      "name": "Computer vision",
      "score": 0.6133921146392822
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5671775341033936
    },
    {
      "name": "Radar imaging",
      "score": 0.5021443367004395
    },
    {
      "name": "Image resolution",
      "score": 0.42002034187316895
    },
    {
      "name": "Radar",
      "score": 0.38846081495285034
    },
    {
      "name": "Real-time computing",
      "score": 0.3279452919960022
    },
    {
      "name": "Telecommunications",
      "score": 0.1321628987789154
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162577319",
      "name": "The University of Texas at Dallas",
      "country": "US"
    }
  ]
}