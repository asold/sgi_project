{
  "title": "A Large Language Models Digest For Social Scientists",
  "url": "https://openalex.org/W4385349752",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2156925138",
      "name": "Daniel Valdenegro",
      "affiliations": [
        "Leverhulme Trust",
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2137983211",
    "https://openalex.org/W2521757446",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2523246573",
    "https://openalex.org/W3162966461",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4376583107"
  ],
  "abstract": "I write these lines not as an expert in LLMs, but more as an informed user. I did my PhD on the application of early transformers-based language models (e.g. BERT, RoBERTa) to classify large amounts of text data. I have experience with their ``fine-tuning\" process and a reasonable amount of knowledge about their training process. However, NLP technology has progressed fast and in a couple of years we moved from models containing hundreds of millions of parameters, to models containing hundreds of billions of parameters. Large Language Models keep getting larger and more capable. Companies closely related to the work of social scientists are taking note of this and they are starting to offer “AI-powered” qualitative software (e.g., see AtlasTi new OpenAI alliance) opening exciting new avenues for social scientists.However, I believe that while the technical capabilities of the models have progressed fast, our actual understanding of what they are and what they are actually capable of doing has been, maybe purposely, left behind. There are several misconceptions about the models' “reasoning” and “introspection” capabilities, with claims pointing towards a soon-to-be-achieved \"Artificial General Intelligence\" (AGI), a holy grail of the research in artificial intelligence, but without offering substantial evidence of such claims. There is also the incredibly thick jargon used in the AI and Machine Learning community, ---which I’m more than guilty of using---, which, sometimes purposely, contributes to gate-keep the knowledge and details of this new to technologies to a reduce group of individuals. Finally, there is a fair amount of marketing strategy around the communications related to LLMs. Most of the newest Large Language Models are the intellectual property and product of large companies and corporations, as they are too big and costly to be trained by singular individuals. For these corporations, being seen by the general public as producing real “Artificial General Intelligence” or “Reasoning Machines” is a big selling point, although the reality might be much more nuanced than that.All of this contributes to generate confusion and mystification around AI, in general, and Large Language Models in particular. So, in a personal effort to clear-up my own thoughts and to hopefully provide something useful for others, in this document I will attempt to provide a high-level description of what LLMs are, how they work and what they are capable of doing, especially for social scientists.",
  "full_text": "A Large Language Models Digest For Social Scientists\nDaniel Valdenegro1\n1Leverhulme Centre for Demographic Science, University of Oxford\nAugust 3, 2023\nSome Preamble Words\nI am writing these lines not as an expert in LLMs, but\nmore as an informed user. I completed my PhD on the\napplicationofearlytransformer-basedlanguagemod-\nels (e.g., BERT, RoBERTa) to classify large amounts of\ntext data. I have experience with their \"fine-tuning\"\nprocess and a reasonable amount of knowledge about\ntheir training process. However, NLP technology has\nprogressed rapidly, and in a couple of years, we have\nmoved from models containing hundreds of millions\n(1 × 108) of parameters to models containing hun-\ndreds of billions (1 × 1011) of parameters. Large Lan-\nguage Models keep getting larger and more capable.\nCompanies closely related to the work of social scien-\ntists are taking note of this and are starting to offer\n\"AI-powered\" qualitative software (e.g., see AtlasTi’s\nnewOpenAIalliance https://atlasti.com/), open-\ning exciting new avenues for social scientists.\nHowever, I believe that while the technical capabil-\nities of the models have progressed rapidly, our ac-\ntual understanding of what they are and what they\nare actually capable of doing has been, maybe pur-\nposely, left behind. There are several misconceptions\nabout the models’ \"reasoning\" and \"introspection\" ca-\npabilities, with claims pointing towards a soon-to-be-\nachieved\"ArtificialGeneralIntelligence\"(AGI),aholy\ngrail of research in artificial intelligence, but without\noffering substantial evidence of such claims. There\nis also the incredibly thick jargon used in the AI and\nMachine Learning community—which I’m more than\nguilty of using—which, sometimes purposely, con-\ntributes to gate-keeping the knowledge and details of\nthese new technologies to a reduced group of indi-\nviduals. Finally, there is a fair amount of marketing\nstrategy around the communications related to LLMs.\nMost of the newest Large Language Models are the\nintellectual property and product of large companies\nand corporations, as they are too big and costly to\nbe trained by singular individuals. For these corpo-\nrations, being seen by the general public as produc-\ning real \"Artificial General Intelligence\" or \"Reasoning\nMachines\" is a big selling point, although the reality\nmight be much more nuanced than that.\nAll of this contributes to generating confusion and\nmystification around AI, in general, and Large Lan-\nguage Models, in particular. So, in a personal ef-\nfort to clear up my thoughts and to hopefully provide\nsomething useful for others, in this document, I will\nattempt to provide a high-level description of what\nLLMs are, how they work, and what they are capable\nof doing, especially for social scientists.\nSo, What Are LLMs?\nLarge Language Models (LLMs) are, first and fore-\nmost, Artificial Neural Networks (ANNs). ANNs are\ncomputing systems loosely inspired by the neurons in\nthe brain. Analogous to the process of perception in\nbiological organisms, where stimuli from the environ-\nment are transformed into electrochemical signals to\nbe processed by the brain, the input of artificial neu-\nral networks is first transformed into numerical data\nto later be processed by the CPU and/or GPU of the\ncomputer. For example, if the task involves learning\n1\nabout images, that data needs to be transformed first\ninto numbers so the ANNs can learn from it.\nThe basic components of each \"neuron\" in an ANN\nare the weights and the activation function. The\nweights are the parameters that get adjusted in the\ntrainingprocess. Eachnumericalinputfeatureismul-\ntiplied by the weights. The results of this multiplica-\ntion are then passed through the activation function\nof the neuron (usually some form of sigmoid function\nlike logistic or hyperbolic tangent). The result is a\nbehaviour resembling a real biological firing neuron,\nwith a sharp change in output values once a thresh-\nold is reached. Each neuron by itself is quite useless,\nas it is not very different from a logistic model. ANNs\nstart to get powerful when a great number of neurons\nare combined in interconnected layers as seen in the\nfigure below:\nFigure 1: Diagram of a fully connected artificial neural\nnetwork, showing the input nodesx, the weightsw, and\nthe output nodeso.\nANNs are supervised machine learning models.\nThis means that they need to be \"trained,\" or pre-\nsented with examples, before they can do anything\nmeaningful. This training is performed using two\nprocesses: FeedForwardandBack-propagation. Feed-\nForward [4] is the process of passing the inputs\nthrough the network until it reaches the output layer\nand performs the final task. For example, let’s imag-\nine that the task is to classify images between be-\ning cats or non-cats. In this case, we feed the image\nthrough the input nodes, it goes through the network\nand the output layer, containing only 2 nodes, out-\nputs the probability of being either a cat or not a cat,\nadjusted using some sort of softmax (a function that\nensures the values are between 1 and 0).\nUsing Figure 1, we can describe step by step the\nprocess of feed-forward.x represents the input nodes\ncoming from numerical features in our dataset. Theh\nnodes at the centre represent the hidden layer of the\nANN. Each of these nodes receives a weighted sum of\nall the input nodes:\nnX\ni=1\n(xi × wxhi) , for each (xi, hi) pairs\nnX\ni=1\n(hi × whoi) , for each (hi, oi) pairs\n(1)\nAfter receiving the weighted sum from all their in-\nput nodes, the hidden nodescompute whether to be-\ncome activated or not. This decision is performed by\nthe activation function. The goal of the activation is\nto provide a sudden change in the output value once\na certain threshold in the input value is crossed, this\nis why functions with sigmoid,S like curves are pre-\nferred (e.g., the logistic function or the hyperbolic\ntangent function). In its most simple form, the com-\nplete feed-forward algorithm with a logistic sigmoid\nfunction: sigmoid = σ = 1\n1+e−x , and using matrix\nmultiplication to reduce Equation 1 to a single opera-\ntion, can be represented as follows:\nh = σ(wxhx)\no = σ(whoh) (2)\nWhere wxh represents the matrix of weights be-\ntween the layer x and layer h; who represents the\nmatrix of weights between the layerh and layer o;\nand x, h and o represents the vector of values of each\nnodes of the layersx, h and o respectively.\nIf the output of the feedforward step is absolutely\nright (p(cat) == 1), we are good. But what if that is\nnot the case? Here is where backpropagation comes\ninto play. Backpropagation is the way by which we\n\"teach\" the network to produce more accurate results.\n2\nIn this process, we calculate the difference of the out-\nput against a gold standard (the training data), and\nwe nudge the weights of the network so that the next\ntime we pass a similar input, the results are a little\ncloser to the correct answer. This difference (which\nis called error) is calculated by theloss function. The\nprocessofadjustingtheweightsisperformedthrough\nsmall corrections every time the ANN is confronted\nwith a new training data point. The algorithm to\nachieve this is calledgradient descent[6], and its goal\nis minimising the output of the loss function (e.g., the\namount of error in the predictions) by making small\nincremental changes in the weights of the ANN, so\nthe outcomes of the loss function would graduallyde-\nscend in magnitude after each iteration of the training\nprocess (e.g., every time the ANN is presented with a\nnew training example) [3]. The process of calculat-\ning the gradient for the output layerL to a previous\nlayer L − 1 for just one weightw, assumingy to be\nthe real value we are trying to approximate, can be\nunderstood as the following differentiation problem:\nz(L) = w(L) × a(L−1);\na(L) = σ\n\u0010\nz(L)\n\u0011\n;\nC = (a(L) − y)2;\n∂C\n∂w(L) = ∂C\n∂a(L)\n∂a(L)\n∂z(L)\n∂z(L)\n∂w(L) = 2\n\u0010\na(L) − y\n\u0011\nσ′\n\u0010\nz(L)\n\u0011\na(L−1)\n(3)\nEquation 3 describes the process of taking the par-\ntial derivative of the loss functionC with respect to\nthe weight w of the current layerL. Since w is not\na term in C, we start by finding the partial deriva-\ntive ofz with respect tow = a(L−1), then the partial\nderivative ofa with respect ofz = σ′ \u0000\nz(L)\u0001\n, the par-\ntial derivative ofC with respecta = 2\n\u0000\na(L) − y\n\u0001\n, to\nfinally multiply the three to obtain the value of∂C\n∂w(L) .\nThe value of this partial derivative tells us how much\nand in which direction the weight should change in\norder to minimise the loss function.\nToobtainthenewvalueoftheweightinthecurrent\nlayer L, and complete the process ofgradient descent,\nthevalueof ∂C\n∂w(L) isthenmultipliedbya learning rate,\nwhichmodulatestheamountofchangeoftheweights\nat each iterations, and is subtracted from the current\nweights as follows:\nw(L)\nnew = w(L)\ncurrent − learning rate× ∂C\n∂w(L) (4)\nIt is interesting to note in equation 3 that the par-\ntial derivative of the loss function with respect to the\nweight in current layerw(L) is directly dependent on\nthe activation of the previous layera(L−1), and hence\nit calculates how much the weights ofa(L−1) affect\nthe loss function and how much they should change\nto minimise the loss function. This is the process of\nbackpropagation for one single weight. This process\nhas to be repeated for all the weights, for all the lay-\ners in an ANN in order to properly adjust the whole\nsystem so the loss function can be minimised.\nA fully working example of a simple ANN written\nin Python to illustrate in detail its inner workings\ncanbefoundhere: https://github.com/dhvalden/\ntoynn/blob/master/np_nn.py.\nThis simple design can be scaled up to have thou-\nsands of nodes and dozens of layers, but eventually,\nmore clever, problem-tailored neural networks are\nneeded to achieve better performance. For exam-\nple, Convolutional Neural Networks (CNN) are the\nstandard now for processing images. CNNs use ad-\njustableconvolutions(asmallkernelofoperationsap-\nplied over the pixels of an image) to detect features,\nsuch as edges, in images and learn more nuanced\ncharacteristics of the data.\nRecurrent Neural Networks (RNN) and Long-Short\nTerm Memory Recurrent Neural Networks (LSTM-\nRNN) were designed to handle sequential data better,\nsuch as text. In sequential data, the order of the in-\nput matters, so networks need to have some kind of\nmemory mechanism to retain contextual information\nabout the sentence. Compared to CNNs, RNNs can\nhandle input sequences of arbitrary length since they\nprocess inputs one at a time, whereas CNNs have an\ninput layer of fixed length, requiring sentences to be\neither padded or truncated.\nRNNs (including LSTM-RNNs) handle variable-\nlengthsequencesbyusinganinputnodewiththesole\npurpose of predicting the next word in the sequence,\nbased on the current word. This node produces two\nvalues: the prediction value of the next word and a\n“feedback weight\". For example, in the sentence “I\n3\nlove my cat.\", an RNN will start by inputting “I\" into\nthe input node to predict “love\", then “love\" to predict\n“my\", and then “my\" to predict “cat\".\nHowever, predicting only the next word based on\nthe previous one is not very impressive. Ideally, we\nwould like to use many past words to predict the next\nword or several next words. For example, we could\nuse “I love my\" to predict the word “cat\". But to do so,\nthe network needs to “remember\" the previous inputs\nalong with its previous prediction. This is where the\nfeedback-weight comes into play. Let’s work with that\nexample sentence:\nFigure 2:Diagram of a recurrent neural network.\nAs you can see in the figure above, each time we\nhave a new word in the sequence we “unroll” the net-\nwork and pass the feedback weights by multiplying\nthem with the new input before passing them to the\nactivation function. We say that we “unroll” the net-\nwork because the weights and biases inside the node,\nas well as the feedback weights, are shared (are the\nsame) in all of the subsequent copies. The feedback\nweight contains the information of the previous in-\nputs. It is the “memory” of the network.\nYou might think that encoding the information of\nall the past inputs in a single number can be a prob-\nlem, and you would be right. It was a big problem.\nA way to address this is by simple adding more input\nnodes for each word, so each of them can learn a dif-\nferent thing, but eventually that was not enough be-\ncause of the “vanishing-exploding gradient problem”,\nor in more lay terms, “the more you multiply smaller\nthan 1 numbers, the smaller they get, until they dis-\nappear” problem. This problem was particularly egre-\ngious when the inputs were long sentences.\nLong-Short Term Memory RNNs tried to address\nthis by adding two feedback loops instead of one: A\nshort-term memory feedback loop, and a long-term\nmemory feedback loop. The internal mechanics of\nhow those long and short term memories are com-\nputed are not really important for the purpose of our\ndiscussion. ItsufficetosaythatLSTM-RNNswerestill\nstruggling with long-term word dependencies, mean-\ning they can’t “remember” very well words that are\nvery far back in the sequence of text. For example, if\na name is mentioned at the beginning of a paragraph,\nbutlaterisreferredonlybyitspronouns, LSTM-RNNs\nwill struggle to remember to whom the pronouns be-\nlong the further we move from the beginning of the\nparagraph.\nFigure 3:Diagram of a long short term memory recurrent\nneural network, showing the long and short term memory\nfeedback paths.\nWord Embeddings (or how to en-\ncode meaning)\nSo far we have been talking about LSTM-RNNs as if\nthey could handle text natively without any prepro-\ncessing. Sadly, this is not the case, as it is very dif-\nficult to apply reliable maths to a sequence of words.\nSo,weneedawaytotransformthesewordsintosome\n4\nnumeric value that also can encode some semantic in-\nformation.\nThe first step is to get some numeric value for the\nwordsinour“vocabulary”. Let’simaginethatwehave\na very limited vocabulary of only four words: “I love\nmy cat.” The first thing we should do is to turn these\ninto numbers: “I” = 0, “love” = 1, “my” = 3, “cat”\n= 4. Additionally, we need to add punctuation to the\nparty, since punctuation is very important to deter-\nmine the meaning of a sentence. So the final set of\nnumbers are “I” = 0, “love” = 1, “my” = 3, “cat” = 4,\n“.” = 5. Given that this set not only contains words,\nbut also other things present in the text, we call these\n“tokens” instead of words, and the process of turn-\ning vocabularies into tokens is called “tokenization”.\nThere are several ways in which tokenization can be\ncarried out to make the most out of it —for example,\nusually tokens are sorted by frequency instead of by\norder of appearance— but the main point is to have a\nnumeric code for each token.\nNow, turning words into numbers does not achieve\nmuch. Any calculation we perform over those num-\nbers is as meaningless as it would be with raw words.\nTherefore, weneedtofindawaytoencodehowthose\ntokens relate to each other in the text. A primitive\napproach would be to observe which words appear\ntogether in each sentence/document and calculate\ntheir relative frequency in a matrix fashion. This is\nknown as the bag-of-words approach. The problem\nwith this method is that it creates very sparse matri-\nces that take up a lot of space because, for each sen-\ntence/document, we need the full list of words in our\nvocabulary to be represented, even if the count for a\nspecific word in that sentence is zero. Additionally,\nthis approach does not reveal much about how words\nare related outside the sentences/documents.\nFigure 4:A very simple bag of words.\nBut what if there is a way in which we can calcu-\nlate numeric features about tokens based on the to-\nkens that are around them, and use this fixed set of\nfeatures to place the tokens into a feature space as\nwe do, for example, with people when we measure\ntheir height and weight? That is exactly what word-\nembeddings do. Word-embeddings create a feature\nspace in which we can place tokens, so tokens that\nare used in the same context (i.e. are usually around\nthe same words) are placed together in the feature\nspace, and words that are usually distinct from each\nother are separated in the feature space.\nFigure 5: A very simple representation of a two dimen-\nsional word embedding vector space.\nReal word-embeddings are not in two dimensions,\nbut in the hundreds or even thousands of dimen-\nsions(GPT-2hasanembeddingspaceof ≈12Kdimen-\nsions, but GPT-3 has only≈1500 dimensions). So for\neach token you will have a vector of numbers of n-\ndimensions in length that will essentially encode the\nmeaning of the token by its proximity –in this embed-\nding space– with other words. The way the values of\nthe embedding vector are found is by using, unsur-\nprisingly, a neural network. This special class of neu-\nralnetworkreceivesasinputthewholevocabulary(in\nour case 5 tokens; in more realistic cases millions of\ntokens tokens), and each time a token is mentioned,\nits input node goes from 0 to 1, in what is called one-\nhotencodingformat. Theseinputnodesarethencon-\nnected to a hidden layer of the size of the embedding\nvector, since the weights for each token connecting to\nthis hidden layer are going to be the values for our\n5\nvector. Lastly, the output layer is set-up so the task\nis to predict the next, previous or surrounding tokens\nof the original sentence using the current token, al-\nlowing us to use the input data as training data. The\nweights are then optimised using back-propagation.\nThefactthattheword-embeddingsaretheresultof\na neural network allows us to connect them directly\nto the larger model through what is called the “em-\nbeddinglayer.\"Thisenablesustousethetokensasin-\nputs for the neural network. Moreover, for even more\nclever functionality, we can add a look-up table at the\nbeginning of our model. This table allows us to pass\nplain words into the network, and each time we pass\na word, it returns its corresponding token(s).\nThe Sequence-To-Sequence prob-\nlem\nWhile there is a vast array of problems in which neu-\nral networks can be applied in natural language pro-\ncessing, one particular challenge stands out as one of\nthe most interesting ones: The sequence-to-sequence\nproblem (a.k.a., seq2seq) [8]. In seq2seq, you have\na sequence of inputs that translates to a sequence of\noutputs. These inputs and outputs do not necessarily\nmatch in length, tokens, or order; and basically the\nonly thing they have in common is that they are se-\nquences. A common example is machine translation\nin which we have to translate a sentence from one\nlanguage into another. The same problem can also\nbe found in genetics, where you translate an RNA se-\nquence to a 3D peptide sequence.\nThe way this problem is solved nowadays is by us-\ning “encoder-decoder” models [1]. In essence these\nmodels use a two stage approach: In the encoder\nstage an embedding layer is attached to a LSTM-\nRNN network to generate an output of weights of a\nfixed length called “context vector”. This context vec-\ntor encodes the information of the meaning of the to-\nkens (embedding layer) and the information of the\nposition of the tokens in the sequence and its depen-\ndencies (LSTM-RNN layer). The context vector is ef-\nfectively the combination of the short and long term\noutput memories of the LSTM-RNN layer. The con-\ntext vector is passed to thedecoderLSTM-RNN layer\nas inputs for the long and short memories channels.\nThe decoder stage holds the embedding of our tar-\nget/output vocabulary (e.g., our target language in a\ntranslation),anditwillusetheencodercontextvector\nas past information seen previously in the sequence.\nIn lay terms, the context vector acts as the interme-\ndiate language between the encoder and the decoder\nstages, passinginformationinacommonnumericfor-\nmat between models that were trained on two dif-\nferent languages. To generate the decoded message,\nthe decoder will pass a neutral token (e.g., start of\nsequence token: <SOS>), and will predict the next\nword based on the target language embeddings and\nthe context vector of the original language. The out-\nput token is then pass to the decoder network again\nuntil the model outputs an “end of sequence token:\n<EOS>”\nFigure 6: Simplified diagram of the encoder-decoder ar-\nchitecture.\nThe process described in the figure above is rather\nconvoluted (although not very complicated) and I\nhave glossed over many details, but if we need to take\nhome one message from it is that encoders encode\ninformation into context vectors, and decoders take\ninputs and/or context vectors and produce output in\nnatural language again.\nAttention was all we needed...\nIn December 2017 a group of researchers, mainly\nbased at Google, published a paper called “Atten-\n6\ntion Is All You Need” presenting a model architec-\nture they named “Transformers” [9]. This architec-\nture was mainly based on the idea of “attention”. “At-\ntention”, inthiscontext, comestohelpsolvetheprob-\nlem of long-term dependencies in long sequences of\ntext. LSTM-RNNs, like the ones used in the encoder-\ndecoder architectures presented above, while better\nthan plain RNNs at remembering long-term depen-\ndencies were still suffering from vanishing-exploiting\ngradient problems. Attention was a mechanism that\nwould come to help the LSTM networks to remem-\nber better, by adding information about how each in-\nput token was related to the entirety of the input se-\nquence, regardless of the input order.\nFigure 7: Example of the relationships between a token\nand the other tokens in the same sequence in two different\nattention layers. As can be seen, different attention lay-\ners learn to attend to different word relationships. Source:\nVaswani et al. [9].\nThe Transformers architecture took this a step\nfurther, and proposed that better results could be\nachievedbyonlyusingembeddingsandattentionlay-\ners, disposing completely of any LSTM-RNN or any\nkind of sequence based model. Naturally, this came\nas a huge shock in the —granted, rather niche— nat-\nural language processing community, as LTMS + at-\ntention decoder-encoder models were performing in-\ncreasingly well in most benchmarks. However, trans-\nformers promised certain advantages that were diffi-\ncult to overlook.\nLSTM networks are notoriously difficult to train.\nGiven that it is a model that requires sequential\ninputs, parallelization was difficult, limiting that\namount of training data to be used. Transformers is\nnot a sequential model —I know! Big shock!— but in-\nstead it handles the input whole immediately. To do\nthis, the architecture allows itself to have very big in-\nput sequences (in the order of thousands of tokens),\nanditusespaddingortruncationtodealwithshortor\nvery long sequences. The information about the po-\nsition of the tokens in the input sequence is injected\ninto the model by adding “positional encodings”. For\nthe purpose of this document, suffice to say that posi-\ntional encodings adds information about the relative\npositionofatokeninsentencealongsidethesemantic\ninformation provided by the embedding vector. In or-\nder to get the final embedding, these two (positional\nencoding and embedding vector) are summed.\nTransformers became hugely popular. Its ease of\ntraining allowed researchers to create much bigger\nmodels, and to use much more training data. This\nled to the proliferation of several “transformer-based”\nlanguage models as well as the application of these\nmodels for a variety of natural language tasks like:\ntranslation, sentence classification, summarization,\nquestion answering, and text generation.\nText generation task or “Causal\nLanguage Modeling”\nCausal language modeling is the task of generating\ntext based on the previous tokens in the sequence.\nIt is achieved by training the models in the predic-\ntion of the word in a sentence over billions of sen-\ntences. Labeling is not needed since the sentence it-\nself gives the answer to what is the correct word or\nwords following a particular set of tokens. If we re-\n7\nFigure 8:Diagram of the proposed Transformers architec-\nture. Asitcanbeseeninthefigure, themainmodelstillhas\nencoder (left) and decoder (right) stages. Source: Vaswani\net al. [9].\ncall what we reviewed above, we can note that the\n“decoder” in an encoder-decoder model is the one in\ncharge of generating the output sequence. Models\nthat only provide text generation capabilities usually\nuse a decoder stage. Generative Pre-trained Trans-\nformers (GPT) models [5] are an example of such de-\ncoder only text generation models.\n(Chat)GPT and the suspension of\ndisbelief\nText generation models have potentially many uses,\nbutcurrentlyoneofthemostpopularonesischatbots.\nChatbotsarecomputerprogramsthatofferaplausible\nconversational interaction with a machine. In order\nFigure 9: Diagram of the GPT-1 architecture. Note that\nthere is only a decoder stage in the model. Source: Radford\net al. [5].\nto provide a believable experience, chatbots have cer-\ntain design features that make them more relatable.\nThosefeaturesarecompletelyunrelatedtotheengine\nthat produces the text. For example, chatbots usually\nhave the same interface chat applications have, and\nusually upon opening that app, a welcome message\nis prompted posing as your virtual interlocutor. Chat-\nGPT is no exception. Notably, one of the things that\nis hidden from the end-user is the “context sequence”\nthe chat app provides to the model every time an in-\nteraction is initiated. This context sequence usually\nhas the following form:\n8\nAssistant is a large language model trained by OpenAI.\nAssistant is designed to be able to assist with a wide range\nof tasks, from answering simple questions to providing in-\ndepth explanations and discussions on a wide range of\ntopics.\nAsalanguagemodel, Assistantisabletogeneratehuman-\nlike text based on the input it receives, allowing it to en-\ngage in natural-sounding conversations and provide re-\nsponses that are coherent and relevant to the topic at\nhand. Assistant is constantly learning and improving, and\nitscapabilitiesareconstantlyevolving. Itisabletoprocess\nand understand large amounts of text, and can use this\nknowledge to provide accurate and informative responses\nto a wide range of questions.\nAdditionally, Assistant is able to generate its own text\nbased on the input it receives, allowing it to engage in\ndiscussions and provide explanations and descriptions on\na wide range of topics.\nOverall, Assistant is a powerful tool that can help with a\nwide range of tasks and provide valuable insights and in-\nformation on a wide range of topics. Whether you need\nhelpwithaspecificquestionorjustwanttohaveaconver-\nsation about a particular topic, Assistant is here to assist.\n[history]\nHuman: [human input]\nAssistant:\nAllifthiscontextispassedeverytimewehitenterin\nthe chat, along with the conversation history and the\ncurrent user input. However all of that is hidden from\ntheuserbecauseitwoulddestroythechatexperience.\nAs you can see above, the context sequence pro-\nvides antecedents about the interaction that is about\nto happen, so the model can predict its own answers\nbetter. And I say predict because that is exactly what\nit does. The training set of LLMs is so vast and it\nwas constructed in such a way that it includes mil-\nlions of interactions between a “Human” and an “As-\nsistant”. By providing the context sequence we are\nincreasing the probability (literally) that the model\nwill treat the whole interaction as a chat and it will\n“look back” at the chat interactions it saw previously\nin the training phase, to then predict the most plau-\nsible next words in this current interaction. And, if\nnot stopped by the chatbot configuration, it will keep\nproducing next words until it reaches the end of the\ninput space, posing interchangeably as the “Assistant”\nand as the “Human”.\nCertain users and LLMs enthusiasts might argue\nthat I’m oversimplifying the models capabilities, and\nthat in fact the models can “reason out” answers to\ncomplexquestions. Andinsomesensethatistrue,I’m\noversimplifying many things, but what is absolutely\ntrue, as well, is that none of the current LLMs have\n“reasoning” capabilities, nor any sort of “sentience”.\nAll of what we perceive as reasoning or sentience\nwhenweinteractwithLLMs-chatbotsissuspensionof\ndisbelief. We are playing along with the chatbot as if\nit truly has reasoning and sentience, but the evidence\nofitisratherthin. Anexampleofthisistheseemingly\nsurprising low effectiveness of the LLMs when doing\nsimple arithmetics. For example, we can ask a LLM-\nchatbot for the logic behind addition and it will recite\nitflawlessly. Whentaskedwithsmallnumberaddition\nit will provide correct answers. However, around the\nmarkof8to10digitsnumbers(e.g.,3.987.348.786+\n2.345.654.098) the models will start to produce way\noff results [2].\nHow is it possible that a model so complex, able\nto produce text in several languages, and memo-\nrized so many definitions, struggle with a problem\nthat a 10 year old can solve in a couple of min-\nutes? Well, the answer is that the models never ap-\nply the logic behind addition. The only thing it does\nis predict the next token based on what it has seen\nin the past. And in the training data there were\nmanyinstancesinwhich1+1=2,andmanyinstances\nin which 23+27=50, but very very few in which\n3.987.348.786+2.345.654.098=6333002884(this\nhas been addressed by OpenAI, most likely by inject-\ningalotarithmeticproblemsintothetrainingset,and\nnow the chatbot is more accurate, but LLaMA-based\nmodels still have this problem.) ChatGPT is a really\nreally good “chinese room”. In the words of Searle,\nit is “weak ai” [7] as it does not understand the lan-\nguage nor the logic it recites. Unless of course we\nwould like to propose the argument that humans are\nalso really good word-predicting machines, and that\nconsciousness is a post-hoc illusion (an argument that\nis not as outlandish as one might think).\n9\nCan we take advantage of LLMs for\nsocial science research?\nOf course, but I believe we need to sit for a moment\nto understand the process by which the models were\ncreated in order to pin-point where we can insert our-\nselves in the training and application process.\nIf we take the naive approach of inserting ourselves\nat the end of the chain by simply using the front-\nend chat tool (e.g., ChatGPT) as a tool for research,\nwe will be constrained by a number of things. I be-\nlieve the most important one is the “restriction” the\nchatbot has embedded in their context sequence and\ntheir alignment process. The alignment process is the\nstagein whichthe modelisslightly modifiedso itout-\nputs “on topic” text with the least amount of errors\nandoffensivemessagespossible. Thereductionerrors\nand hate speech is not really a problem, since as re-\nsearcherswealwaysprefercorrectandpoliteoutputs.\nHowever, the “on topic” is potentially problematic. By\n“on topic” I mean that the models will privilege out-\nputs that sound from coming from a very neutral “As-\nsistant”. This might be difficult to bypass if we want\nmore diverse outputs. So, ideally, we would like to\nmake a more informed decision about how to use the\nmodel.\nSo let’s review where we can insert ourselves in the\nproduction process of LLMs:\n• The first, and probably more daunting, phase in\nwhich we can insert ourselves, in the design of\nthemodel’sarchitecturethemselves,andcreatea\ntailored model architecture for our specific prob-\nlems. This is rather ambitious, but is not without\nprecedent. AlphaFold, the model that can suc-\ncessfully predict 3D structure of proteins from\nRNA sequences, is a tailor-made, Transformer-\nbased, sequence-to-sequence model tailored to\ndo only protein modeling. This model was led,\nand tailored for the needs, of biologists and ge-\nneticists. The whole field of NLP, from where\nGTP comes from, is a mixture of computer sci-\nentists and linguists.\n• The second phase in which we can insert our-\nselves is in the design of the training dataset.\nAgain, this is ambitious, but being a sub-phase\nof the previous one, is sensibly more address-\nable. This phase will comprise the curation of\na large dataset of text tailored for our problem,\nand use this to train an LLM using its vanilla ar-\nchitecture. Here there are way more precedents,\nthe most recent one being the case of “NYUTron”\nfrom NYU medical research. This challenge still\nrequires significant effort and is more akin to a\ndepartmental or University level effort.\n• The “fine tuning” phase also presents some op-\nportunities for us. Finetuning refers to the pro-\ncessoftakingthemain,alreadytrained,LLMand\nnarrow down its output capabilities to one, very\nspecific, task. For example, I can take LLaMA\n(Meta’s version of GPT), put a classification NN\nlayer on top of it, train that head in a few thou-\nsands of examples of text that are already la-\nbeled, and reach a very high level of accuracy\nin the classification with very little effort. This\napproach works because the amounts of weights\nthat need to be adjusted in the training step are\nonly the ones of the classification head, while\nthe weights of the model that encode all the in-\nformation about the relationships of words, are\nfreezed. The advantage —and disadvantage—\nof this method is that we end up with a model\nthat can only do one thing, but with a very high\nlevel of consistency.\n• The fourth phase in which we can make a signif-\nicant difference is in the alignment phase. This\nalignmentphasecanbeachievedinseveralways,\nbut the main two ones are by fine-tuning the\nmodel using a smaller, very specific dataset and\ntaks,andtheotheroneisbyusingReinforcement\nlearning from human feedback (RLHF), which,\nin simple, words, using human to rate the an-\nswers of the model, so we can nudge the model\nto output more align answers.\n• The last phase in which we can insert ourselves\nis the creation of “context sequences” The con-\ntext sequences are placed at the beginning of the\ninput sequence to be passed to the model to au-\ntocomplete, and its main purpose is to increase\n10\nthelikelihoodofcertainoutputsoverothers. The\nmanipulation of the context sequences in order\nto produce more consistent outputs is still an un-\nderstudied area of LLMs, both in the ways this\ncan be used to generated more accurate research\noutputs for text analysis, or the what we can use\nthem to manipulate the outputs of the models\nin a more general sense. For example, a prob-\nlem I’m particularly interested in is in the use\nof current billion parameter level LLMs to per-\nform zero-shot text classification, i.e., the ability\nof the model to correctly classify a sentence into\nthe provided categories without the need to fine-\ntune the model.\nOverall,Ibelievetherearesomegoodopportunities\nfor social sciences to integrate itself into the world of\nLLMs and I believe we can take a lot of advantage\nfrom them if we take a more informed and cautious\napproach. And I haven’t even mentioned the fairness,\nethical societal aspects of the use of LLMs, in which\nwe, as social scientists, can have a lot to say.\nReferences\n[1] Kyunghyun Cho et al. “Learning phrase repre-\nsentations using RNN encoder-decoder for sta-\ntistical machine translation”. In:arXiv preprint\narXiv:1406.1078 (2014).\n[2] Luciano Floridi and Massimo Chiriatti. “GPT-3:\nIts nature, scope, limits, and consequences”. In:\nMinds and Machines30 (2020), pp. 681–694.\n[3] Ian Goodfellow, Yoshua Bengio, and Aaron\nCourville. Deep learning. MIT press, 2016.\n[4] KurtHornik,MaxwellStinchcombe,andHalbert\nWhite. “Multilayer feedforward networks are\nuniversal approximators”. In: Neural networks\n2.5 (1989), pp. 359–366.\n[5] Alec Radford et al. “Improving language un-\nderstanding with unsupervised learning”. In:\n(2018).\n[6] Sebastian Ruder. “An overview of gradient\ndescent optimization algorithms”. In: arXiv\npreprint arXiv:1609.04747(2016).\n[7] John R Searle. “Minds, brains, and programs”.\nIn: Behavioral and brain sciences 3.3 (1980),\npp. 417–424.\n[8] Ilya Sutskever, Oriol Vinyals, and Quoc V Le.\n“Sequencetosequencelearningwithneuralnet-\nworks”. In:Advances in neural information pro-\ncessing systems27 (2014).\n[9] Ashish Vaswani et al. “Attention is all you need”.\nIn:Advances in neural information processing sys-\ntems 30 (2017).\n11",
  "topic": "Jargon",
  "concepts": [
    {
      "name": "Jargon",
      "score": 0.8092830181121826
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5499406456947327
    },
    {
      "name": "Computer science",
      "score": 0.5424497723579407
    },
    {
      "name": "Process (computing)",
      "score": 0.506585955619812
    },
    {
      "name": "Introspection",
      "score": 0.4809693396091461
    },
    {
      "name": "Alliance",
      "score": 0.47479623556137085
    },
    {
      "name": "Holy Grail",
      "score": 0.4577459692955017
    },
    {
      "name": "Data science",
      "score": 0.38763147592544556
    },
    {
      "name": "World Wide Web",
      "score": 0.22454428672790527
    },
    {
      "name": "Psychology",
      "score": 0.18132731318473816
    },
    {
      "name": "Linguistics",
      "score": 0.16799888014793396
    },
    {
      "name": "Political science",
      "score": 0.14764747023582458
    },
    {
      "name": "Law",
      "score": 0.1103370189666748
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802641067",
      "name": "Leverhulme Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 3
}