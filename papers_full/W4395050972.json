{
  "title": "Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework",
  "url": "https://openalex.org/W4395050972",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5018176259",
      "name": "Simone Kresevic",
      "affiliations": [
        "University of Trieste"
      ]
    },
    {
      "id": "https://openalex.org/A5068428372",
      "name": "Mauro Giuffrè",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A5078896862",
      "name": "Miloš Ajčević",
      "affiliations": [
        "University of Trieste"
      ]
    },
    {
      "id": "https://openalex.org/A5046229764",
      "name": "Agostino Accardo",
      "affiliations": [
        "University of Trieste"
      ]
    },
    {
      "id": "https://openalex.org/A5027838453",
      "name": "Lory Saveria Crocè",
      "affiliations": [
        "University of Trieste"
      ]
    },
    {
      "id": "https://openalex.org/A5078143266",
      "name": "Dennis Shung",
      "affiliations": [
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388725043",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4389173934",
    "https://openalex.org/W4381514383",
    "https://openalex.org/W4310191041",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W4379799312",
    "https://openalex.org/W2602169852",
    "https://openalex.org/W4384470775",
    "https://openalex.org/W2316378557",
    "https://openalex.org/W2983309655",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4285245049",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W3024513077",
    "https://openalex.org/W4390813834",
    "https://openalex.org/W4386158494",
    "https://openalex.org/W4379231355",
    "https://openalex.org/W4372231834",
    "https://openalex.org/W4382282792",
    "https://openalex.org/W4383312504",
    "https://openalex.org/W4384944902",
    "https://openalex.org/W4381611207",
    "https://openalex.org/W4387451644",
    "https://openalex.org/W4386090262",
    "https://openalex.org/W4385336911",
    "https://openalex.org/W4386781106",
    "https://openalex.org/W4378782874",
    "https://openalex.org/W4377940406",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4392366650",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W2307512708",
    "https://openalex.org/W3120043490",
    "https://openalex.org/W4213213306",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4388594005",
    "https://openalex.org/W3088020815",
    "https://openalex.org/W4378192633",
    "https://openalex.org/W2992772421",
    "https://openalex.org/W4387767115",
    "https://openalex.org/W4391470083",
    "https://openalex.org/W4392544551"
  ],
  "abstract": "Abstract Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM’s answer generation accuracy. The baseline GPT-4 Turbo model’s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% ( p &lt; 0.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01091-y\nOptimization of hepatological clinical\nguidelines interpretation by large\nlanguage models: a retrieval augmented\ngeneration-based framework\nCheck for updates\nSimone Kresevic1,2,4 , Mauro Giuffrè 2,4 , Milos Ajcevic1, Agostino Accardo1,L o r yS .C r o c è3 &\nDennis L. Shung2\nLarge language models (LLMs) can potentially transform healthcare, particularly in providing the right\ninformation to the right provider at the right time in the hospital workﬂow. This study investigates the\nintegration of LLMs into healthcare, speciﬁcally focusing on improving clinical decision support\nsystems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus\ninfection management. Utilizing OpenAI’s GPT-4 Turbo model, we developed a customized LLM\nframework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our\nframework involved guideline conversion into the best-structured format that can be efﬁciently\nprocessed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate\nthe impact of different formatting and learning strategies on the LLM’s answer generation accuracy.\nThe baseline GPT-4 Turbo model’s performance was compared againstﬁve experimental setups with\nincreasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and\nimplementation of few-shot learning. Our primary outcome was the qualitative assessment of\naccuracy based on expert review, while secondary outcomes included the quantitative measurement\nof similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The\nresults showed a signiﬁcant improvement in accuracy from 43 to 99% (p < 0.001), when guidelines\nwere provided as context in a coherent corpus of text and non-text sources were converted into text. In\naddition, few-shot learning did not seem to improve overall accuracy. The study highlights that\nstructured guideline reformatting and advanced prompt engineering (data quality vs. data quantity)\ncan enhance the efﬁcacy of LLM integrations to CDSSs for guideline delivery.\nLarge language models (LLMs) have the potential to improve healthcare due\nto their capability to parse complex concepts and generate appropriate\nresponses. LLMs have demonstrated proﬁciency in tasks across the spec-\ntrum of clinical activity, such as medical inquiry responses, dialogue sys-\ntems, and the synthesis and completion of clinical reports\n1– 5. One potential\nhigh-value area for LLMs is the ability to promote evidence-based practice\nthrough providing clinical decisionsupport systems (CDSSs) according to\ncurrent medical guidelines, which aredistillations of both expert opinion\nand current evidence from clinical trials and are used to drive improvements\nin patient outcomes through best practices\n6,7.\nLLMs have enjoyed wide public uptake, especially OpenAI’sC h a t G P T\n(https://openai.com/blog/chatgpt), which enrolled over 100 million users\nwithin two months of its release8,9. The widespread use of ChatGPT allowed\nsimple and user-friendly use of generative artiﬁcial intelligence for real-life\nscenarios and academic research. However, a primary concern for LLMs\napplication in healthcare is the potential risk of inaccurate responses (e.g.,\n1Department of Engineering and Architecture, University of Trieste, Trieste, Italy.2Department of Medicine (Digestive Diseases), Yale School of Medicine, Yale\nUniversity, New Haven, CT, USA.3Department of Medical, Surgical, and Health Sciences, University of Trieste, Trieste, Italy.4These authors contributed equally:\nSimone Kresevic, Mauro Giuffrè. e-mail: simone.kresevic@phd.units.it; mauro.giuffre@yale.edu\nnpj Digital Medicine|           (2024) 7:102 1\n1234567890():,;\n1234567890():,;\n“hallucinations”) that may lead to patient harm10. In clinical applications, a\nproposed framework for utilizing LLMs is based on adherence to the three\nprinciples of Honesty, Helpfulness, and Harmlessness (the HHH\nprinciple)\n11. To align LLMs to the HHH principle, speciﬁcs t r a t e g i e sm u s tb e\nundertaken to bind their responses to a speciﬁcs e to fd o m a i nk n o w l e d g e ,\nsuch as retrieval augmented generation (RAG)12 or supervisedﬁne-tuning\n(SFT) followed by Reinforcement Learning with Human Feedback\n(RLHF)13. Both RAG and SFT guide output generation according to a\ndomain-speciﬁc dataset of information that, for clinical applications, could\nbe represented by medical guidelines. However, the format of clinical\nguidelines is subject to broad variations (e.g., general structure, location of\nrecommendations, table format, andﬂowcharts) that can affect the proper\ninterpretation or retrieval of relevant information.\nWhile the integration of LLMs in healthcare shows promise, the\nchallenge of ensuring accurate interpretation of clinical guidelines becomes\nparticularly relevant in the context ofmanaging widespread chronic diseases\nsuch as Hepatitis C Virus (HCV) infection. New antiviral therapies suc-\ncessfully eradicate the disease, withmultiple regimens demonstrating >90%\nefﬁcacy and effectiveness\n14. HCV management has been codiﬁed in multiple\nguidelines that distill the results from the available randomized controlled\ntrials to recommend best practices in chronic HCV diagnosis and treatment.\nHowever, adherence to guidelines ranges from 36– 54% for screening and\nmanaging chronic HCV infection15,16. There is a need for scalable and\nreliable solutions to provide guideline-recommended care and bridge the\ngap in adherence, especially considering the World Health Organization’s\ngoal to eliminate Hepatitis C by 203017.\nWe present a novel LLM framework integrating clinical guidelines with\nRAG, prompt engineering, and text reformatting strategies for augmented\ntext interpretation that signiﬁcantly outperforms the baseline LLM model in\nproducing accurate guideline-speciﬁc recommendations, with the primary\noutcome of qualitatively measuring accuracy based on manual expert\nreview. We also apply quantitative text-similarity methods18– 21 to compare\nthe similarity of the LLM output to expert-generated responses.\nResults\nOutput accuracy analysis\nThe customized LLM framework achieved 99.0% overall accuracy, which\nwas signiﬁcantly better than the GPT-4 Turbo alone (99.0% vs. 43.0%;\np < 0.001). Incorporating in-context guidelines improved accuracy (67.0%\nvs. 43.0%;p = 0.001). When the in-context guidelines were cleaned, and\ntables were converted from images to.csv ﬁles, accuracy improved to 78.0%\n(vs. 43.0%;p < 0.001); after the guidelines were formatted with a consistent\nstructure and tables were re-formatted to text-based lists, accuracy further\nimproved to 90.0% (vs. 43.0%;p < 0.001). Finally, the addition of custom\nprompt engineering led to an improvement in accuracy of 99.0% (vs. 43.0%;\np < 0.001), with no further improvement despite few-shot learning with 54\nquestion-answer pairs (Table1,F i g .1).\nFor text-based questions, the customized framework achieved 100%\noverall accuracy, which was better than GPT-4 Turbo alone (100% vs.\n62.0%; p < 0.001). Incorporating in-context guidelines improved accuracy\n(86.0% vs. 62.0%;p = 0.01); after cleaning the text and conversion of tables\nfrom images to.csv, further improvement in accuracy was achieved with no\nfurther improvement after formatting the text into a consistent structure\nand converting tables into text-based lists (90.0% vs. 62.0%;p =0 . 0 0 2 ) .\nAdding custom prompt engineering resulted in 100% accuracy (100% vs.\n62.0%;p < 0.001) with equivalent performance after few-shot learning with\n54 question-answer pairs (100% vs. 62.0%;p < 0.001).\nFor table-based questions, the customized framework achieved 96.0%\noverall accuracy, which was better than GPT-4 Turbo alone (96.0% vs.\n28.0%; p < 0.001). Incorporating in-context guidelines improved accuracy\n(44.0% vs. 28.0%;p = 0.38); after cleaning the text and conversion of tables\nfrom images to.csv, accuracy reached 60.0% (vs. 28.0%;p = 0.046) with a\nsubstantial improvement after converting tables into text-based lists and\nformatting the text into a consistent structure (96.0% vs. 28.0%;p < 0.001)\nwith similar performance in Experiments 4 and 5 as reported in Table1.\nThe customized framework achieved100% overall accuracy for clinical\nscenarios, which was better than GPT-4 Turbo alone (100% vs. 20.0%;\np < 0.001). Incorporating in-context guidelines improved accuracy (52.0%\nvs. 20.0%;p = 0.039); after cleaning the text and conversion of tables from\nimages to.csv, accuracy reached 72.0% (vs. 20.0%;p < 0.001) with a sub-\nstantial improvement after converting tables into lists and formatting the\ntext into a consistent structure (84.0% vs. 20.0%;p <0 . 0 0 1 ) . F i n a l l y , t h e\naddition of custom prompt engineering achieved an accuracy of 100%\n(vs. 20.0%; p\n< 0.001), with no further improvement despite few-shot\nlearning with 54 question– answer pairs.\nWhen inaccurate outputs were reviewed for hallucinations, we found\n112 (90.3%) fact-conﬂicting hallucinations (FCH) and 12 (9.7%) input-\nconﬂicting hallucinations (ICH) across all experiments. Hallucination type\nand distribution across each experiment are reported in Table2.W ed i dn o t\nﬁnd contextual-conﬂicting hallucinations (CCH) in any of our experiments.\nText-similarity analysis\nFor the secondary outcomes, we found differences in the customized LLM\nframework compared to the baseline across similarity scores (BLEU score,\nROUGE-LCS F1, METEOR Score F1, and our Custom OpenAI Score) for all\nquestions (Table3). The score average values fortext-based and table-based\nquestions, clinical scenarios, and graphical distributions of each score are\nreported in Supplementary Table 2 andSupplementary Fig. 1, respectively.\nTable 1 | Qualitative evaluation of accuracy based on human expert grading of each answer across all experimental settings\nMetrics Baseline Experiment 1 Experiment 2 Experiment 3 Experiment 4 Experiment 5\nAll questions:\nAccuracy 43.0% 67.0% 78.0% 90.0% 99.0% 99.0%\nStatistical Signiﬁcance p = 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nText-based questions:\nAccuracy 62.0% 86.0% 90.0% 90.0% 100.0% 100.0%\nStatistical signiﬁcance p = 0.012 p = 0.002 p = 0.002 p < 0.001 p < 0.001\nTable-based questions:\nAccuracy 28.0% 44.0% 60.0% 96.0% 96.0% 96.0%\nStatistical signiﬁcance p = 0.377 p = 0.046 p < 0.001 p < 0.001 p < 0.001\nClinical scenarios questions:\nAccuracy 20.0% 52.0% 72.0% 84.0% 100.0% 100.0%\nStatistical signiﬁcance p = 0.039 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nStatistical testing is based on pairwise comparison (Chi-Squared Test) between each experimental setting and the baseline.\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 2\nDiscussion\nIntegrating LLMs into CDSSs may revolutionize healthcare delivery by\nleveraging natural language processing to interpret clinical documentation,\naligning LLM-generated recommendations with current medical research\nand best practices22,23 (Fig. 2). For instance, a locally hosted LLM might be\ngranted access to patient-speciﬁc data. This data can be integrated into a\ntailored prompt designed to identify the most appropriate treatment plan\nfor a speciﬁc patient. The LLM, will have contemporary access to the\nguidelines and provide a recommendation on treatment based on guideline\nknowledge. However, before havingLLM-aided CDSSs, it is necessary to\ndeﬁne the best guidelines format that can maximize output accuracy.\nWe demonstrate the performance of our proposed framework in a\nsubset of the potential questions that could be asked by physicians managing\npatients with chronic HCV. We identiﬁed an optimal framework for LLM-\nfriendly clinical guidelines that achieves near-perfect accuracy and out-\nperforms GPT-4 Turbo alone for answering questions about the manage-\nment of HCV infection. The baseline GPT-4 Turbo showed an overall\naccuracy of 43.0%, consistent with other studies querying LLMs for man-\nagement questions related to gastroenterology and hepatology, ranging\nfrom 25 to 90%\n24– 37. This suggested that the model’s base knowledge was\nimperfect despite having access to information up to April 202338.\nOurﬁndings also highlight the difﬁculty of LLMs to parse tables, with a\nclear improvement in performance after tables were converted to text-based\nlists, suggesting that information cannot be retrieved accurately from non-\ntext sources. The difﬁculty of LLMs to parse tables is a known limitation\n39,\nand a critical technical issue that should be addressed since the medical\nliterature often contains tables with important information for clinicians.\nModern LLMs such as GPT-4, according to their multimodal cap-\nabilities and context sensitivity, caninterpret inputs from both images and\ntextual elements\n40. OpenAI has noted that GPT-4 was tested on different\nbenchmarks on textual, graphical, and visual elements (ChartQA41,A I 2 D42,\nDocVQA43, Infographic VQA44) with an accuracy range from 75.1% to\n88.4% redeﬁned the previously best models in these benchmarks which\nranged from 61.2% to 88.4%40. Despite GPT-4 becoming state of the art in\ngraphical-context interpretation, we demonstrate that it cannot interpret\nthe non-text sources reported in the HCV guidelines, showing 16.0% overall\naccuracy in extracting pertinent information (as described in detail in\nSupplementary Note 1). Inaccuracies ingraphical elements interpretation\ncan result in the loss of critical information and context when converting\nnon-text sources into a readable format for LLMs, which likely affected the\nGPT-4 Turbo’s ability to accurately interpret and reason with the infor-\nmation contained in non-text sources. This factor, coupled with the chal-\nlenge of context retention across the segmented data in non-text sources,\ncould have contributed to the lower performance in“reasoning and\nFig. 1 | Qualitative evaluation of accuracy among all experiments from baseline.\na Accuracy for all questions.b Accuracy only for text-based questions.c Accuracy for\ntable-based questions.d Accuracy for clinical scenario-based questions. Statistical\ntesting is based on pairwise comparison (Chi-Squared Test) between each experi-\nmental setting and the baseline.\nTable 2 | Hallucinations type and distribution across all\nexperiments\nHallucination Total Fact-\nconﬂicting\nInput-\nconﬂicting\nContextual-\nconﬂicting\nBASELINE 57 48 (84.2%) 9 (15.8%) -\nExperiment 1 33 30 (90.9%) 3 (9.1%) -\nExperiment 2 22 22 (100%) - -\nExperiment 3 10 10 (100%) - -\nExperiment 4 1 1 (100%) - -\nExperiment 5 1 1 (100%) - -\nInterestingly, the two graders did notﬁnd any contextual-conﬂicting hallucination in any LLM-\ngenerated outputs.\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 3\ninterpretation” tasks. These results imply that the information present in the\nguidelines should be presented as text (i.e., in LLM-friendly format) to be\nefﬁciently and accurately retrieved and interpreted by LLMs.\nWe found that the similarity scores (BLEU\n45,R O U G E - L46, METEOR47,\nand a custom OpenAI score) calculated between output generated by GPT-4\nTurbo and the free text expert answers do not necessarily reﬂect differences\nin expert-graded qualitative accuracy. We found statistically signiﬁcant\ndifferences across all similarity metrics when the outputs of the in-context\nguideline experiments were compared to the baseline outputs as reported in\nTable 3.I m p o r t a n t l y ,w h e nw ee v a l u a t e dt he in-context guideline experi-\nments, we found no clear correlation wit ht h ec h a n g ei ns i m i l a r i t ym e t r i c s\nwith expert-graded qualitative accuracy. This has also been reported in other\nstudies\n18– 21,48 and may be explained by the fact that these scores were\ndeveloped to measure word overlap, sentence structure similarity, and\nsemantic coherence and not factualcorrectness. For clinical questions,\nfactual correctness is the most important feature. This is an important\nchallenge that should be addressed since current responses could appear\nlexically comparable to a reference answer but fail to capture the factual\ninformation necessary to guide clinical care. This can result in high scores\nfor responses that are factually incorrect (false positives) or low scores for\naccurate responses that are phrased differently than the reference (false\nnegatives). While useful for certain aspects of evaluation, these metrics fail to\nTable 3 | Evaluation of text-to-text-similarity between LLM-generated outputs and human expert-provided answers used as the\ngold standard across all questions\nMetrics Baseline Experiment 1 Experiment 2 Experiment 3 Experiment 4 Experiment 5\nBLEU score:\nMean (± SD) 0.025 (±0.023) 0.095 (±0.088) 0.111 (±0.143) 0.101 (±0.094) 0.140 (±0.119) 0.124 (±0.073)\nSigniﬁcance p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nROUGE-LCS F1:\nMean (± SD) 0.201 (±0.053) 0.334 (±0.120) 0.347 (±0.138) 0.336 (±0.114) 0.345 (±0.119) 0.359 (±0.095)\nSigniﬁcance p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nMETEOR score F1:\nMean (± SD) 0.308 (±0.059) 0.417 (±0.104) 0.429 (±0.126) 0.408 (±0.101) 0.428 (±0.115) 0.421 (±0.081)\nSigniﬁcance p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nCustom OpenAI Score:\nMean (± SD) 0.939 (±0.016) 0.954 (±0.017) 0.956 (±0.018) 0.956 (±0.016) 0.957 (±0.013) 0.958 (±0.017)\nSigniﬁcance p < 0.001 p < 0.001 p < 0.001 p < 0.001 p < 0.001\nStatistical testing is based on pairwise comparison (Mann–Whitney U Test) between each experimental setting and the baseline.\nFig. 2 | Example of a clinical decision support system integrated with large\nlanguage models.When a patient is being evaluated for HCV treatment, the doctor\nprescribes several tests (laboratory and imaging), whose results are stored in the\ninstitutional EHR system. The locally hosted LLM has a standardized clinical sce-\nnario prompt with laboratory and imaging values that are directly extracted from\nEHR. Afterward, the standardized prompt is queried to the LLM, which has access to\nthe relevant guidelines to recommend the most appropriate treatment. HCV\nHepatitis C virus, EHR electronic health record, RAG retrieval augmented genera-\ntion, LLM large language model.\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 4\ncapture the nuances of medical relevance, completeness, and contextual\ncorrectness in the answers provided by the LLM. This limitation under-\nscores the persistent need for expert physician oversight in the evaluation\nprocess (i.e., human-in-the-loop), with automated grading of LLM-\ngenerated responses still being an unresolved challenge.\nWe also found that few-shot learning did not improve performance\nabove and beyond in-context learning, text formatting, table conversion,\nand prompt engineering (Fig.1). This suggests that the model’s zero-shot\nquerying capabilities were already robust without requiring few-shot stra-\ntegies, which was previously described by reporting different one-shot vs.\nfew-shot\n49,50.\nOur work is limited by several factors. Firstly, we only investigate the\napplication of LLM in the screening, diagnosis, and management of one\ndisease across the spectrum of hepatology. However, our questions were\nrepresentative of every section in the guideline, covering each major area of\nclinical management. Secondly, we raneach question for a limited number\nof iterations, with the performance being consistently excellent across\nmultiple experiments. We do not vary the temperature setting for each of the\nquestions and stages of the ablation study. We had limited resources to test\nthe framework, though we acknowledge that the performance may differ\nwith changes in the temperature parameter. Finally, we do not evaluate the\nperformance of our framework with other LLMs, such as LlaMA\n51 or\nPaLM52. While these other models are used for many tasks, most studies\nusing LLM in gastroenterology and hepatology have employed GTP 3.5 and\nGPT-4.0\n24– 37.\nIn a recent study, Jin et al. developed LiVersa53, a liver disease-\nspeciﬁc LLM using RAG and guidelines from the American Association\nfor the Study of the Liver (AASLD), which showed notable limitations in\nproviding completely accurate answers, especially in complex clinical\nscenarios. Also, from their methodology, it is unclear how they con-\nverted guidelines into text, the chunking strategy (which we do not\nemploy in our framework), and their accuracy assessments and lack of\ndata on output accuracy rates. Therefore, despite the similar aims, we\ncannot compare our studyﬁndings.\nWe present a novel LLM framework to generate answers to complex\nclinical questions with high accuracy, drawing from established guidelines\nfor HCV management. We highlight the current limitations in LLM non-\ntext sources interpretation and the beneﬁt of in-context structured re-\nformatted guidelines with accompanied prompt engineering to guide\nunderstanding of the underlying text structure.\nIn conclusion, our results suggest that LLMs like GPT-4 Turbo are\nsuitable for parsing clinical guidelines, but that their effectiveness can be\nenhanced by structured formatting strategies, prompt engineering, and text\nconversion of non-text sources. Moreover, ourﬁndings suggest that with\nappropriate reformatting, few-shot learning may not increase overall\naccuracy. We highlight the need for further research to enhance LLM’s\nability to parse non-text sources and validate new metrics to evaluate not\nonly similarity but also accuracyfor clinical LLM applications.\nMethods\nGuidelines selection\nWe analyzed the current HCV guidelines from the prominent Northern\nAmerican and European liver associations. Among these, we selected the\nEuropean Association for the Study of the Liver (EASL) on the Hepatitis\nC Virus, entitled“EASL recommendations on treatment of hepatitis C:\nFinal update of the series” published in 2020\n54, to explore our framework.\nThe selected guideline comprised the most complex corpus of text\ncontaining broad recommendations on screening and management. In\naddition, the document contained in-depth information on drug-drug\ninteractions, which was not reported in the Northern American\n55,56\nguidelines. We also tested our framework on speciﬁc questions that were\nnot addressed in the European guidelines using the most up-to-date\nNorthern American HCV guidelines (as reported in Supplementary\nNote 3, Supplementary Table 3, Supplementary Table 4, Supplementary\nFig. 3, Supplementary Fig. 4).\nStandardized prompts creation\nTwo expert hepatologists (M.G. and L.S.C.) drafted 20 representative\nquestions (Table4). Fifteen questions addressed screening and management\nrecommendations from each of the major sections, including the guideline\nmain text (10 questions) and graphical tables (5 questions). Tables are a\nstandard feature of clinical guidelines and summarize recommendations in\nspeciﬁcw a y st h a tm a yn o tb er eﬂected in the text. In addition, the two\nexperts draftedﬁve comprehensive clinical cases, each reﬂecting different\nHCV-related management strategies, including best treatment selection,\ndrug– drug interaction, and management of treatment severe adverse\nreactions. All the questions are structured to test reasoning and compre-\nhension from both the main text and tables.\nAblation study: customized LLM framework\nWe used a combination of RAG using EASL HCV guidelines, in different\nexperimental settings with increasing degrees of complexity regarding\nguideline reformatting, prompt architecture, and few-shot learning to create\na customized framework applied to the GPT-4 Turbo model (released by\nOpenAI, in November 2023 with knowledge updated until April 2023\n38).\nExperiments with the OpenAI’s Application Programming Interface (API)\nv. 1.17 cannot directly retrieve information from .pdf ﬁles. Therefore, the\noriginal pdf guidelines document was converted to a .txt ﬁle with UTF-8\nencoding using the Python (v. 3.11) library PyPDF2 v3.0.\nWe carried out an ablation study from the baseline (Experiments 1\nthrough 5) to investigate how different settings in guideline reformat-\nting, prompt architecture, and few-shot learning impact the accuracy\nand robustness of LLM outputs (Fig.3). It is still unknown how non-text\nsources (e.g., graphical tables andﬂowcharts) are processed by LLMs\nand whether the information extracted is accurate. Therefore, we per-\nformed preliminary experiments to test the accuracy of the GPT image\nconversion process (Supplementary Note 1) and found very low accu-\nracy (16.0%) in extracting pertinent table information, with accuracy\nranging from 0% (graphical tables) to 48.0% (only text tables). In light of\nthese ﬁndings, we introduced text conversion of tables (non-text\nsources) into text-based lists and tested their impact on accuracy in\nExperiments 3, 4, and 5.\nBaseline. Use of the foundational GPT-4 Turbo without any context. For\nthis experiment, we only provided the questions without any further\ninstruction.\nExperiment 1. Use of the foundational GPT-4 Turbo with guidelines\nuploaded in context after pdf-to-text conversion in UTF-8 encoding\nwithout any additional text cleaning processes.\nExperiment 2. Use of the foundational GPT-4 Turbo with guidelines\nuploaded in context after being manually cleaned with the removal of\nnon-informative data (e.g. page header and bibliography). Tables pre-\nsented as images in the original text were manually converted into.csv\nﬁles and then provided as context.\nExperiment 3. Use of the foundational GPT-4 Turbo with guidelines\nuploaded as context that were cleaned and formatted to provide a con-\nsistent structure alongside the whole document. In addition, we con-\nverted all tables from.csv ﬁles into text-based lists and included them in\nthe main text. Each paragraph title was preceded by“Paragraph Title”. All\nthe paragraph recommendations were collected and organized into a list\npreceded by “Paragraph Recommendations”. Evidence reported in the\nmain text was organized and preceded by“Paragraph Text”.\nExperiment 4. Use of the foundational GPT-4 Turbo with guidelines\nuploaded as context that were cleaned and formatted, with tables con-\nverted into text-based lists. We also provided a series of prompts (i.e.,\nprompt engineering) that instructed the model on how to interpret the\nstructured guidelines (Supplementary Table 1).\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 5\nExperiment 5. Use of the foundational GPT-4 Turbo with guidelines\nuploaded as context that were cleaned and formatted, with tables con-\nverted into text-based lists. We included the series of prompts (i.e.,\nprompt engineering) and added a series of 54 question-answer pairs (i.e.\nfew-shot learning) (Supplementary Table 1).\nThe experiments are summarized in Fig.3 and were conducted on a\nlocal Python environment with OpenAI API access. Instructions, when\nprovided, are summarized in Supplementary Table 1. We used foundational\nmodel default parameters, selecting a temperature of 0.9, and setting a\nmaximum number of tokens in output equivalent to 800.\nPrimary outcome\nOur primary outcome was to evaluate qualitative rates of accuracy\naccording to expert grading based on the information reported in EASL\nguidelines\n54. We repeated the query 5 times each for the 20 questions for\neach experimental setting and reported the proportion of accurate\nresponses. Each answer was graded with a score of 1 if the text contained\ncompletely accurate information or 0 otherwise. Two expert hepatologists\n(M.G., with four years of experience in treating HCV patients, and L.S.C.,\nwith thirty years of experience in treating HCV patients) manually graded\neach response. The two graders were blind to each other and towards the\nexperimental setting when labelinganswers. Disagreements in grading\noccurred for 5.0% of outputs and were solved by consensus between the two\ngraders.\nWhen outputs are considered inaccurate, the inaccuracy is caused by\nhallucinations (i.e., the production of plausible sounding but potentially\nunveriﬁed or incorrect information)\n57,58. According to the recent deﬁnitions\nof Zhang et al., we deﬁned three types of hallucinations: FCH, ICH,\nand CCH59.\nSecondary outcome\nOur secondary outcome was to evaluate the similarity of LLM-generated\nresponses to the human expert-provided answers used as the gold standard.\nIn particular, an expert hepatologist (M.G.) provided a single answer for\neach of the 20 questions, which was reviewed and approved by the second\nexpert hepatologist (L.S.C.), and then used as the gold standard expert\nresponse to which LLM responses were compared in text-similarity using\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE)46, Bilingual\nEvaluation Understudy (BLEU)45, Metric for Evaluation of Translation with\nExplicit Ordering (METEOR)47, and a Custom OpenAI score (for in-depth\nTable 4 | List of questions\nText-based questions\n1. A screening blood test before a knee replacement surgery revealed a positive HCV antibody—what test should be performed to conﬁrm HCV infection?\n2. When a patient with HCV can be considered cured after HCV therapy?\n3. Is there any major contraindication to HCV therapy?\n4. Is it possible to apply treatment without determining genotype using grazoprevir/elbasvir?\n5. What are the recommended treatment regimens and duration for a patient with HCV genotype 3 and no cirrhosis?\n6. A patient on the transplantation list for HCC and decompensated liver cirrhosis should be treated before or after transplantation.\n7. Should patients with HCV-positive patients be listed for kidney transplant treated? If yes, why?\n8. Patients with ﬁbrosis F3, according to elastography, should be continuing HCC screening after successful HCV eradication?\n9. Is HCV treatment during pregnancy recommended?\n10. When should children born by an HCV-positive mother be tested for HCV infection?\nTable-based questions\n11. What test can be used to assess the liver disease severity before treatment?\n12. Is there any interaction between cyclosporine and DAAs?\n13. Is there any interaction between apixaban and DAAs?\n14. Among anticoagulants and antiplatelets which is the one medication with the lowest risk of interactions with DAAs?\n15. What anticonvulsants are at higher risk of inducing drug interactions with DAAs?\nClinical scenarios\n16. A 45-year-old male with an unremarkable medical history was scheduled for a routine inguinal hernia surgery. As part of the preoperative evaluation, he was tested for\nhepatitis C virus (HCV) antibodies, which returned positive. Subsequent HCV RNA testing conﬁrmed active infection, and genotyping identiﬁed the virus as HCV\ngenotype 1a. The patient had no prior knowledge of his HCV status and had never been tested or treated for hepatitis C. Before initiating treatment, a liver elastography\nwas performed to assess liver health, yielding a liver stiffness measurement of 5 kPa. What is the recommended treatment for this patient (drugs and duration)?\n17. A 55-year-old patient, previously lost to follow-up, returns to the liver clinic with a history of failed interferon-based therapy for HCV genotype 3. Recent laboratory\ntests conﬁrm active HCV infection with genotype 3, accompanied by elevated liver enzymes (AST: 100 IU/L, ALT: 150 IU/L). Additional laboratory results include\nbilirubin at 1.2 mg/dL, creatinine at 0.87 mg/dL, albumin at 3.9 g/dL, and an INR of 1.10. Liver elastography shows a liver stiffness measurement of 15 kPa, without\nclinical signs of liver decompensation, as observed in the physical examination. What is the recommended therapy for this patient?\n18. A 60-year-old patient with advanced chronic kidney disease (CKD) at stage 4 is diagnosed with Hepatitis C virus (HCV) infection. The patient’s current renal function\nparameters include a creatinine clearance of 28 mL/min. Additionally, the patient presents with decompensated liver cirrhosis, classiﬁed as Child-Pugh Class B8,\nindicating signiﬁcant liver dysfunction. What is the recommended therapy for this patient?\n19. A 60-year-old female patient diagnosed with Hepatitis C virus (HCV) genotype 1a, who does not have liver cirrhosis, was recently prescribed a 12-week course of\nSofosbuvir (400 mg)/Velpatasvir (100 mg). The patient has a signiﬁcant medical history of atrialﬁbrillation, for which she is being treated with amiodarone. During the\ninitial assessment with the hepatologist, the patient inadvertently omitted mentioning their amiodarone treatment. As of now, the patient has not commenced the HCV\ntreatment. Is it advisable for the patient to promptly inform her hepatologist about the amiodarone treatment before starting the HCV therapy?\n20. A 70-year-old female with a recent diagnosis of Hepatitis C Virus (HCV) genotype 1a, conﬁrmed to have no evidence of liver cirrhosis, commenced a treatment regimen\nconsisting of a 12-week course of Sofosbuvir (400 mg) combined with Velpatasvir (100 mg) daily. The patient’s baseline liver function tests were within normal limits,\nwith an Alanine Aminotransferase (ALT) level of 45 IU/L (normal range: 30–45 IU/L). However, upon re-evaluation 4 weeks post-treatment initiation, her ALT levels had\nmarkedly elevated to 1123 IU/L. Should the prescribed HCV treatment be discontinued in light of this signiﬁcant ALT elevation?\nTwo expert hepatologists drafted 20 questions that speciﬁcally refer to information about management recommendations addressing information contained in the guideline main text (10 questions),\ngraphical tables (5 questions), and clinical scenarios (5 questions).\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 6\nexplanation see Supplementary Note 2). The Custom OpenAI score is based\non cosine similarity, while the otherscores are based on word overlap and\nsemantic coherence between two text sources. We evaluated the similarity\nby comparing LLM-generated answers to the corresponding ones provided\nby experts. All these scores are expressed on a scale from 0 to 1, where a score\nof 1 denotes perfect alignment between two compared text sources. The\nmean and standard deviation of the similarities were estimated after\nrepeating the query 5 times each for the 20 questions.\nFig. 3 |Depiction of Ablation Study experimental settings (Experiment 1 through Experiment 5) to investigate how guideline reformatting, prompt architecture, and few-\nshot learning impact the accuracy and robustness of LLM outputs.\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 7\nStatistical analysis\nWe employed the Chi-Square Test to compare accuracy among experi-\nments qualitatively. We employed the Mann-Whitney U Test to compare\ndifferences among continuous scoring for automatic evaluation of answers.\nWe considered statistically signiﬁcant a two-tailedp-value < 0.05. To con-\nduct the analysis, we used Pythonv3 . 1 1and SciPyv1 . 1 1.\nData availability\nAll LLMs prompts are included in the Summary Information with the\nprompts used. For any additional information, please contact the corre-\nsponding authors.\nCode availability\nCode can be provided based on personal requests, please contact the cor-\nresponding authors.\nReceived: 29 November 2023; Accepted: 29 March 2024;\nReferences\n1. Peng, C. et al. A study of generative large language model for medical\nresearch and healthcare.NPJ Digit. Med.6, 210 (2023).\n2. Thirunavukarasu, A. J. et al. Large language models in medicine.Nat.\nMed. 29, 1930–1940 (2023).\n3. Meskó, B. et al. The imperative for regulatory oversight of large\nlanguage models (or generative AI) in healthcare.NPJ Digit. Med.6,\n120 (2023).\n4. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n5. Webster, P. Six ways large language models are changing healthcare.\nNat. Med.29, 2969–2971 (2023).\n6. Nagulu, I. et al. Clinical guidelines and best practices.Glob. J. Res.\nAnal. 12,1 7–20 (2023).\n7. Mignini, L. Review of clinical practice guidelines. InSystematic\nReviews to Support Evidence-Based Medicine165–170 (CRC Press,\nBoca Raton, 2022).https://doi.org/10.1201/9781003220039-15.\n8. Liu, Y. et al. Summary of ChatGPT-Related research and perspective\ntowards the future of large language models.Meta-Radiol. 1,\n100017 (2023).\n9. Mesko, B. The ChatGPT (Generative Artiﬁcial Intelligence) revolution\nhas made artiﬁcial intelligence approachable for medical\nprofessionals. J. Med. Internet Res.25, e48392 (2023).\n10. Nori, H. et al. Capabilities of GPT-4 on medical challenge problems.\narxiv https://arxiv.org/abs/2303.13375 (2023).\n11. Scheurer, J. et al. Technical report: large language models can\nstrategically deceive their users when put under pressure. arxiv\nhttps://arxiv.org/abs/2311.07590 (2023).\n12. Lewis, P. et al. Retrieval-augmented generation for knowledge-\nintensive NLP tasks.arxiv https://arxiv.org/abs/2005.11401 (2020).\n13. Ouyang, L. et al. Training language models to follow instructions with\nhuman feedback.arxiv https://arxiv.org/abs/2203.02155 (2022).\n14. Falade-Nwulia, O. et al. Oral direct-acting agent therapy for hepatitis C\nvirus infection.Ann. Intern. Med.166, 637 (2017).\n15. Moore, J. D. et al. Physician-level determinants of HCV screening\nduring pregnancy in a U.S. sample.Arch. Gynecol. Obstet.https://doi.\norg/10.1007/s00404-023-07146-x (2023).\n16. Southern, W. N. et al. Physician nonadherence with a hepatitis C\nscreening program.Qual. Manag; Health Care23,1 –9 (2014).\n17. Elimination of hepatitis by 2030.https://www.who.int/health-topics/\nhepatitis/elimination-of-hepatitis-by-2030#tab=\ntab_1.\n18. Chen, A. et al. Evaluating Question Answering Evaluation. InProc. 2nd\nWorkshop on Machine Reading for Question Answering119–124\n(Association for Computational Linguistics, Stroudsburg, PA, USA,\n2019). https://doi.org/10.18653/v1/D19-5817.\n19. Tang, L. et al. Evaluating large language models on medical evidence\nsummarization. NPJ Digit. Med.6, 158 (2023).\n20. Blagec, K. et al. A global analysis of metrics used for measuring\nperformance in natural language processing. InProc. NLP Power! The\nFirst Workshop on Efﬁcient Benchmarking in NLP52–63 (Association\nfor Computational Linguistics, Stroudsburg, PA, USA, 2022).https://\ndoi.org/10.18653/v1/2022.nlppower-1.6.\n21. Fabbri, A. R. et al. SummEval: re-evaluating summarization\nevaluation. Trans. Assoc. Comput Linguist9, 391–409 (2021).\n22. Mahadevaiah, G. et al. Artiﬁcial intelligence‐based clinical decision\nsupport in modern medical physics: Selection, acceptance,\ncommissioning, and quality assurance.Med. Phys.47,\ne228–e235 (2020).\n23. Golden, G. et al. Applying artiﬁcial intelligence to clinical decision\nsupport in mental health: what have we learned?Health Policy\nTechnol, 100844 https://doi.org/10.1016/j.hlpt.2024.100844 (2024).\n24. Tariq, R. et al. Evolving landscape of large language models: an\nevaluation of ChatGPT and bard in answering patient queries on\ncolonoscopy. Gastroenterology 166, 220–221 (2024).\n25. Lahat, A. et al. Evaluating the utility of a large language model in\nanswering common patients’ gastrointestinal health-related\nquestions: are we there yet?Diagnostics 13, 1950 (2023).\n26. Lee, T.-C. et al. ChatGPT answers common patient questions about\ncolonoscopy. Gastroenterology 165, 509–511.e7 (2023).\n27. Gorelik, Y. et al. language models for streamlined postcolonoscopy\npatient management: a novel approach.Gastrointest. Endosc.98,\n639–641.e4 (2023).\n28. Henson, J. B. et al. Evaluation of the potential utility of an artiﬁcial\nintelligence chatbot in gastroesophageal reﬂux disease management.\nAm. J. Gastroenterol.118, 2276–2279 (2023).\n29. Emile, S. H. et al. How appropriate are answers of online chat-based\nartiﬁ\ncial intelligence (ChatGPT) to common questions on colon\ncancer? Surgery 174, 1273–1275 (2023).\n30. Moazzam, Z. et al. Quality of ChatGPT responses to questions related\nto pancreatic cancer and its surgical care.Ann. Surg. Oncol.30,\n6284–6286 (2023).\n31. Cankurtaran, R. E. et al. Reliability and usefulness of ChatGPT for\ninﬂammatory bowel diseases: an analysis for patients and healthcare\nprofessionals. Cureus https://doi.org/10.7759/cureus.46736 (2023).\n32. Levartovsky, A. et al. Towards AI-augmented clinical decision-\nmaking: an examination of ChatGPT’s utility in acute ulcerative colitis\npresentations. Am. J. Gastroenterol.118, 2283–2289 (2023).\n33. Patil, N. S. et al. Using artiﬁcial intelligence chatbots as a radiologic\ndecision-making tool for liver imaging: do chatgpt and bard\ncommunicate information consistent with the ACR appropriateness\ncriteria? J. Am. Coll. Radiol.20, 1010–1013 (2023).\n34. Pugliese, N. et al. Accuracy, reliability, and comprehensibility of\nchatgpt-generated medical responses for patients with nonalcoholic\nfatty liver disease.Clin. Gastroenterol. Hepatol.https://doi.org/10.\n1016/j.cgh.2023.08.033 (2023).\n35. Endo, Y. et al. Quality of ChatGPT responses to questions related to\nliver transplantation.J. Gastrointest. Surg.27, 1716–1719 (2023).\n36. Cao, J. J. et al. Accuracy of information provided by ChatGPT\nregarding liver cancer surveillance and diagnosis.Am. J. Roentgenol.\n221, 556–559 (2023).\n37. Yeo, Y. H. et al. Assessing the performance of ChatGPT in answering\nquestions regarding cirrhosis and hepatocellular carcinoma.Clin.\nMol. Hepatol.29, 721–732 (2023).\n38. OpenAI. New models and developer products announced at DevDay.\nhttps://openai.com/blog/new-models-and-developer-products-\nannounced-at-devday.\n39. Sui, Y. et al. Table meets LLM: can large language models understand\nstructured table data? A benchmark and empirical study.arxiv https://\narxiv.org/abs/2305.13062 (2023).\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 8\n40. OpenAI et al. GPT-4 technical report.https://arxiv.org/abs/2303.\n08774 (2023).\n41. Masry, A. et al. ChartQA: a benchmark for question answering about\ncharts with visual and logical reasoning.arxiv https://arxiv.org/abs/\n2203.10244 (2022).\n42. Kembhavi, A. et al. A diagram is worth a dozen images. in 235–251.\nhttps://doi.org/10.1007/978-3-319-46493-0_15 (2016).\n43. Mathew, M. et al. DocVQA: a dataset for VQA on document images.\narxiv https://arxiv.org/abs/2007.00398 (2020).\n44. Mathew, M. et al. InfographicVQA. In2022 IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV)2582–2591\n(IEEE, 2022).https://doi.org/10.1109/WACV51458.2022.00264.\n45. Papineni, K. et al. BLEU: a method for automatic evaluation of\nmachine translation. InProc. 40th Annual Meeting of the Association\nfor Computational Linguistics311–318 (Association of Computational\nMachinery, 2002).\n46. Lin, C.-Y. Rouge: a package for automatic evaluation of summaries.\nIn: Text summarization branches, 74–82 (2004).\n47. Banerjee, S. et al. METEOR: an automatic metric for MT evaluation\nwith improved correlation with human judgments. InProc. ACL\nWorkshop on Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization.6 5–72 (2005).\n48. Zhang, T. et al. BERTScore: evaluating text generation with BERT.\narxiv https://arxiv.org/abs/1904.09675 (2019).\n49. Agrawal, M. et al. Large language models are few-shot clinical\ninformation extractors.arxiv https://arxiv.org/abs/2205.12689 (2022).\n50. Hu, Y. et al. Improving large language models for clinical named entity\nrecognition via prompt engineering.arxiv https://arxiv.org/abs/2303.\n16416 (2023).\n51. Touvron, H. et al. Llama 2: open foundation andﬁne-tuned chat\nmodels. arxiv https://arxiv.org/abs/2307.09288 (2023).\n52. Anil, R. et al. PaLM 2 Technical Report.arxiv https://arxiv.org/abs/\n2305.10403 (2023).\n53. Ge J. et al. Development of a liver disease-speciﬁc large language\nmodel chat interface using retrieval augmented generation.https://\ndoi.org/10.1101/2023.11.10.23298364 (2023).\n54. Pawlotsky, J.-M. et al. EASL recommendations on treatment of\nhepatitis C:ﬁnal update of the series✰. J. Hepatol.73,\n1170–1218 (2020).\n55. Bhattacharya, D. et al. Hepatitis C guidance 2023 update: american\nassociation for the study of liver diseases– infectious diseases society\nof america recommendations for testing, managing, and treating\nhepatitis c virus infection.Clin. Infect. Dis.https://doi.org/10.1093/\ncid/ciad319 (2023).\n56. Ghany, M. G. et al. Hepatitis C guidance 2019 update: american\nassociation for the study of liver diseases–infectious diseases society\nof america recommendations for testing, managing, and treating\nhepatitis C virus infection.Hepatology 71, 686–721 (2020).\n57. Giuffrè, M. et al. L. Evaluating ChatGPT in medical contexts: the\nimperative to guard against hallucinations and partial accuracies.Clin.\nGastroenterol. Hepatol.https://doi.org/10.1016/j.cgh.2023.09.\n035 (2023).\n58. Giuffrè, M. et al. Scrutinizing ChatGPT Applications in\ngastroenterology: a call for methodological rigor to deﬁne accuracy\nand preserve privacy.Clin. Gastroenterol. Hepatol.https://doi.org/10.\n1016/j.cgh.2024.01.024 (2024).\n59. Zhang, Y. et al. Siren’s song in the ai ocean: a survey on hallucination in\nlarge language models.arxiv https://arxiv.org/abs/2309.01219(2023).\nAuthor contributions\nS.K., M.G., M.A., A.A., L.S.C, D.S., conceived and designed the analysis;\nS.K. and M.G. collected the data; S.K., M.G. and S.D. performed the\nanalysis. All authors were involved in writing, editing, and approving theﬁnal\nversion of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-024-01091-y\n.\nCorrespondenceand requests for materials should be addressed to\nSimone Kresevic or Mauro Giuffrè.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s41746-024-01091-y Article\nnpj Digital Medicine|           (2024) 7:102 9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6967989206314087
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6417644619941711
    },
    {
      "name": "Workflow",
      "score": 0.482767254114151
    },
    {
      "name": "Guideline",
      "score": 0.4565228819847107
    },
    {
      "name": "Health care",
      "score": 0.42690107226371765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42387112975120544
    },
    {
      "name": "Information retrieval",
      "score": 0.3880598545074463
    },
    {
      "name": "Natural language processing",
      "score": 0.358630508184433
    },
    {
      "name": "Machine learning",
      "score": 0.34879070520401
    },
    {
      "name": "Data mining",
      "score": 0.3267248272895813
    },
    {
      "name": "Medicine",
      "score": 0.20201778411865234
    },
    {
      "name": "Database",
      "score": 0.13340747356414795
    },
    {
      "name": "Pathology",
      "score": 0.10720965266227722
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}