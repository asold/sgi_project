{
    "title": "On Grounded Planning for Embodied Tasks with Language Models",
    "url": "https://openalex.org/W4382202802",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2563984569",
            "name": "Bill Yuchen Lin",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2765534864",
            "name": "Chengsong Huang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2100110296",
            "name": "Qian Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116503876",
            "name": "Wenda Gu",
            "affiliations": [
                "University of Southern California",
                "Southern California University for Professional Studies"
            ]
        },
        {
            "id": "https://openalex.org/A4382229223",
            "name": "Sam Sommerer",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2108009659",
            "name": "Xiang Ren",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2100110296",
            "name": "Qian Liu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4224912544",
        "https://openalex.org/W2506483933",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3169291081",
        "https://openalex.org/W2971822538",
        "https://openalex.org/W2978612180",
        "https://openalex.org/W3014265582",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W6747106673",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3023710830",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W3184222203",
        "https://openalex.org/W3206919976",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W3163303190",
        "https://openalex.org/W6811308443",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2993086250",
        "https://openalex.org/W3092516542",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W2925419377",
        "https://openalex.org/W2891021031",
        "https://openalex.org/W3182778088",
        "https://openalex.org/W6761205521",
        "https://openalex.org/W3035231859",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W3035140194",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385573990",
        "https://openalex.org/W4293566037",
        "https://openalex.org/W4225768122",
        "https://openalex.org/W4286905705",
        "https://openalex.org/W4294982321",
        "https://openalex.org/W4214700710",
        "https://openalex.org/W4385573007",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2989322838",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3034758614",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3126325318"
    ],
    "abstract": "Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks. This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named G-PlanET, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.",
    "full_text": "On Grounded Planning for Embodied Tasks with Language Models\nBill Yuchen Lin1*, Chengsong Huang2*, Qian Liu3, Wenda Gu1, Sam Sommerer1, Xiang Ren1\n1 University of Southern California\n2 Fudan University\n3Sea AI Lab\n{yuchen.lin, wendagu, sommerer, xiangren}@usc.edu, huangcs19@fudan.edu.cn, liuqian@sea.com\nAbstract\nLanguage models (LMs) have demonstrated their capability\nin possessing commonsense knowledge of the physical world,\na crucial aspect of performing tasks in everyday life. How-\never, it remains unclear whether they have the capacity to\ngenerate grounded, executable plans for embodied tasks. This\nis a challenging task as LMs lack the ability to perceive the\nenvironment through vision and feedback from the physical\nenvironment. In this paper, we address this important research\nquestion and present the first investigation into the topic. Our\nnovel problem formulation, named G-PlanET, inputs a high-\nlevel goal and a data table about objects in a specific envi-\nronment, and then outputs a step-by-step actionable plan for\na robotic agent to follow. To facilitate the study, we establish\nan evaluation protocol and design a dedicated metric, KAS,\nto assess the quality of the plans. Our experiments demon-\nstrate that the use of tables for encoding the environment and\nan iterative decoding strategy can significantly enhance the\nLMs‚Äô ability in grounded planning. Our analysis also reveals\ninteresting and non-trivial findings. 1\n1 Introduction\nPre-trained language models (LMs) demonstrate exceptional\nproficiency in a wide range of natural language processing\n(NLP) tasks such as question answering, machine transla-\ntion, and summarization. They indeed capture some com-\nmonsense knowledge about our physical world such as\n‚Äúbirds can fly‚Äù. However, the question of whether LMs can\nexhibit reasoning abilities within a grounded, realistic set-\nting remains an open issue. This is because LMs lack the\nsensory experiences and physical interactions with the envi-\nronment that enable human beings to grasp the nuances of\nreal-life situations and plan for completing tasks.\nEmbodied robotics learning is a growing field that seeks\nto create artificial intelligence agents capable of navi-\ngating and performing tasks within real-world environ-\nments, typically simulated through physical engines such\nas AI2THOR (Kolve et al. 2017). The ALFRED bench-\nmark (Shridhar et al. 2020) represents one of the pioneering\n* The first two authors contributed equally.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n1Our project website at https://inklab.usc.edu/G-PlanET.\nGroundedplanning√òTurn right and walk to the stove.√òPick up the tea pot on the left side of the stove.√òTurn left and walk tothe shelves on the right.√òPlace the tea pot on the middle shelfto the left of the glass container.\nLMs\nTask: Move a teapot from the stove to a shelf.\nFigure 1: The task of grounded planning for embodied tasks\n(G-PlanET). The input to the LMs is a goal with a specific\nenvironment, and the output is a step-by-step plan that can\nguide a robot to complete the task.\ndatasets that bridges the gap between NLP and robotics, pro-\nviding a platform for investigating language-directed agents.\nThe objective of these studies is to design and test agents\nthat can translate language instructions into sequences of\nlow-level actions that enable the agent to manipulate objects\nwithin an environment and achieve a desired outcome (e.g.,\ncleaning an object and placing it elsewhere).\nHowever, the primary emphasis of the ALFRED bench-\nmark and related datasets is on the comprehension of pre-\nestablished plans, rather than the ability to reason and inde-\npendently plan within a realistic environment. Prior research\nfocuses on the capacity of agents to comprehend and exe-\ncute step-by-step plans, but not on their capacity for decom-\nposing tasks and generating such plans, which represents a\nmore advanced skill. Additionally, the role of LMs has re-\nceived limited examination in the context of these bench-\nmarks, where they are mainly used as encoders for embed-\nding token sequences, rather than for planning or reasoning.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13192\nPrior studies have explored the planning capability of\nLMs, with Huang et al. (2022) demonstrating that GPT-\n3 and similar models are capable of generating general\nplans for executing everyday tasks. However, these plans\nlack grounding in a realistic environment, as LMs are not\nenvironment-specific. As a result, these plans are not neces-\nsarily executable by agents. For instance, in the context of an\nALFRED task to ‚Äúmove a teapot from the stove to a shelf,‚Äù\nembodied agents require knowledge of the location of the\nteapot and the path to reach it. Humans, on the other hand,\ncan readily observe the location of the teapot on the stove\nand their current position in the kitchen, allowing them to\nformulate a grounded plan that starts with ‚Äúturn right and\nwalk to the stove.‚Äù This highlights the need for generating\ndetailed, step-by-step action sequences for robotic agents to\nuse in their execution processes.\nCan LMs also learn grounded planning ability? How\nshould we evaluate and improve LMs for grounded plan-\nning? In this paper, we address the question of whether\nLMs can also learn grounded planning abilities. To this end,\nwe propose a study on the ability of language models for\ngrounded planning for embodied tasks (G-PlanET). Our ap-\nproach involves providing LMs with two inputs: a high-\nlevel task description and a realistic environment in the form\nof an object table. The output is a plan consisting of exe-\ncutable, step-by-step actions. We formulate G-PlanET as a\nlanguage generation task and focus on encoder-decoder lan-\nguage models such as BART (Lewis et al. 2020).\nIn order to establish a dataset and evaluation protocol for\nG-PlanET, we leveraged the ALFRED data by developing a\nsuite of data conversion programs. They extract the object\ninformation from the environment and format it into data ta-\nbles, thereby enabling models to access observations from\nrealistic scenarios. Additionally, we formulated a new eval-\nuation metric, referred to as KAS, that is more appropriate\nfor the task than existing ones for text generation. As re-\ngards the methodology of G-PlanET, we suggest flattening\nan object table into a sequence of tokens and appending it\nto the task description as input to the model. The base LMs\nare then fine-tuned with these seq2seq data to learn to gen-\nerate plans. Furthermore, we propose a simple yet effective\ndecoding strategy that iteratively generates subsequent steps\nby incorporating the previous generation into the input. Our\nempirical results and analysis indicate that incorporating ob-\nject tables into inputs and the proposed iterative decoding\nstrategies are both crucial for enhancing the performance of\nlanguage models in G-PlanET.\nTo summarize, our main contributions are:\n‚Ä¢ The task of G-PlanET: To the best of our knowledge,\nthis is one of the first studies to investigate the ability\nof LMs for embodied planning in realistic environments.\nG-PlanET is crucial for advancing the grounded general-\nization of large LMs and bridging the gap between NLP\nand embodied intelligence. (Sec. 2)\n‚Ä¢ A comprehensive evaluation protocol: We put significant\neffort to convert the ALFRED and AI2THOR data into\ndata tables to support the evaluation of G-PlanET. We\nalso created a new evaluation metric, KAS, to effectively\nassess the plans generated by the LMs.\n‚Ä¢ Improving LMs for G-PlanET: We present two simple\nbut effective components for enhancing the grounded\nplanning ability of LMs - flattening object tables and an\niterative decoding strategy. Our experiments show that\nthese components lead to notable performance gains.\n(Sec. 3) Also, through extensive experimentation and in-\ndepth analysis, we have gained a deeper understanding of\nthe behavior of LMs for G-PlanET and present a series of\nnon-trivial findings in our study.\n2 Problem Formulation\nHere we present the background knowledge, the problem\nformulation and the data sources for G-PlanET.\n2.1 Background Knowledge\nEmbodied tasks. The ALFRED benchmark (Shridhar\net al. 2020) is among the first benchmarks focusing on em-\nbodied tasks in realistic environments, although most of the\nexamples are household tasks. It aims to test the ability of\nagents to execute embodied tasks in real-world scenarios.\nSpecifically, the agents need to understand language-based\ninstructions and output a sequence of actions to interact with\nan engine named AI2-THOR (Kolve et al. 2017), such that\nthe given tasks can be achieved.\nLanguage instructions. Language instructions play an\nimportant role in the ALFRED benchmark. The embodied\ntasks are annotated with a high-level goal and a low-level\nplan (i.e., a sequence of executable actions for robots) in\nnatural language, which are both inputs to the agents. The\nagents need to understand such language instructions and\nparse them into action templates. Note that the agents do not\nneed to plan for the task, as they already have the step-by-\nstep instructions to follow.\nTask planning. Prior works show that large pre-trained\nlanguage models (LMs) such as GPT-3 (Brown et al. 2020)\ncan generate general procedures for completing a task. How-\never, such plans are not aligned with the particular envi-\nronment in which we are interested. This is because these\nmethods never encode the environment as part of the in-\nputs to LMs for grounding the plans to the given environ-\nment. Therefore, such non-grounded plans are hardly useful\nin guiding agents to work in real-world situations.\n2.2 G-PlanET with LMs\nAs discussed in Sec. 2.1, the ALFRED benchmark does\nnot explicitly test the planning ability, while prior works on\nplanning with LMs have not considered grounding to a spe-\ncific environment. In this work, we focus on evaluating and\nimproving the ability to generategrounded plansfor embod-\nied taskswith LMs, which we dub as G-PlanET. It has been\nan underexplored open problem for both the robotics and\nNLP communities.\nTask formulation. The task we aim to study in this pa-\nper is essentially a language generation problem. Specifi-\ncally, the input is two-fold: 1) a high-level goal G and 2)\n13193\nG +ùëÜ! + ‚Ä¶ + ùëÜ\" + E ùëÜ\"#!\nObject table for the env. (E)\nRealistic env.\nG + E ùëÜ!,ùëÜ%, ‚Ä¶ , ùëÜ&\nTask (G): Move a teapot from the stove to a shelf.\nFigure 2: The overall workflow of the proposed methods. First, we extract the object table from the realistic environment. Then\nwe flatten the table into a sequence of tokens E (Sec. 3.2). We provide two learning methods for generating plans: 1) generate\nthe whole plan S1, S2, ‚ãØ, ST and 2) iteratively decode the St+1 (Sec. 3.3).\na specific environment E that the agents need to ground\nto. The expected output is a sequence of actionable plans\nS = {S1, S2, ‚ãØ} to solve the given goal in the specific envi-\nronment step-by-step. The goal G and the plan S are in the\nform of natural language, while the environment E can be\nviewed as a data table consisting of the object information\nin a room. Figure 2 shows an illustrative example and we\nwill discuss more details in Section 3.2.\n2.3 Data for G-PlanET\nTo build a large-scale dataset for studying the G-PlanET\ntask, we re-use the goals and the plans of ALFRED and ex-\ntract object information from AI2THOR for the aligned en-\nvironment. The ALFRED dataset uses the AI2THOR engine\nto provide an interactive environment for agents with an ego-\ncentric vision to perform actions. However, the dataset does\nnot contain explicit data about objects in the environment\n(e.g., the coordination, rotation, and spatial relationship with\neach other).\nWe develop a suite of conversion programs for using\nAI2THOR to re-purpose the ALFRED benchmark for eval-\nuating the methods shown in Section 3. We managed to get\na structured data table to describe the environment of each\ntask in the ALFRED dataset. We explore the AI2THOR en-\ngine and write conversion programs such that we can get\nfull observations of all objects: properties (movable, open-\nable, etc.), positions (3D coordinates & rotation), sizes, and\nspatial relationships (e.g.,object A is on the top of object B).\nWe believe our variant of the ALFRED data will be a great\nresource for the community to study G-PlanET and future\ndirections in grounded reasoning.\n3 Methods\nHerein, we introduce the methods that we adopt or propose\nto address the G-PlanET problem. First of all, we present\nthe base language models that are encoder-decoder archi-\ntectures. Then, we show in detail how we encode the en-\nvironment data and integrate them with the seq2seq learn-\ning frameworks. Finally, we propose an interactive decoding\nstrategy that significantly improves performance.\n3.1 Base Language Models\nPretrained encoder-decoder language models, such as\nBART (Lewis et al. 2020) and T5 (Raffel et al. 2019), have\nachieved promising performance in many well-known lan-\nguage generation tasks such as summarization and question\nanswering. They also show great potential for general com-\nmonsense reasoning tasks such as CommonsenseQA (Tal-\nmor et al. 2019), suggesting that these large LMs have com-\nmon sense to some extent. As the G-PlanET can be also\nviewed as a text generation problem, we use these LMs as\nthe backbone for developing further planning methods, hop-\ning that their common sense can be grounded in real-world\nsituations for embodied tasks.\nVanilla baseline methods. As shown in many papers,\nBART and T5, when sizes are similar, show comparable per-\nformance in many generation tasks. Thus, we use BART-\nbase and BART-large as two selected LMs for evaluation.\nThe simplest and most straightforward baseline method of\nusing such LMs to solve G-PlanET is to ignore the envi-\nronment and only use the goal as the sole input. Then, we\nfine-tune the base LMs with the training data and expect\nthey can directly output the whole plan as a single sequence\nof tokens (including special separator tokens). This simple\nmethod does not allow the LMs to perceive the environment,\nalthough training from the large-scale data can still teach the\nLMs some general strategies for planning. Therefore, we see\nthis as an important baseline method to analyze.\n13194\n3.2 Encoding Realistic Environments\nTo enable the LMs to perceive an environment, we need\nto encode the object tables described in Sec. 2.2. Follow-\ning prior works in table-based NLP tasks (Chen et al. 2020;\nLiu et al. 2022b), we flatten a table into token sequences\nrow by row, thus creating a linearized version of an object\ntable. Then, we append the flattened table after the goal to\nform a complete input sequence. Thus, the input side of the\nencoder-decoder finally has the environment information for\ngenerating a grounded plan.\nConsidering the max sequence limit, we only choose to\nencode objects by their type, position, rotation, and the re-\nceptacle parent. The object type does not only tell what an\nobject is but also implies commonsense affordance (e.g., a\nmicrowave can heat up something, a knife can slice some-\nthing) which is very important for planning. The position\ninformation is essential for agents to navigate and find ob-\njects, thus playing an important part in planning. The rota-\ntion is also useful for some objects that can only be used with\na certain orientation (e.g., a refrigerator can only be opened\nwhen the agent is in front of it). The receptacle of an object\nand itself has a close spatial connection (e.g., a pen is on\na desk; an apple is in a fridge). Every object has a unique\nidentifier such that objects of the same type can be referred\nto precisely when they are receptacles of others. In addition,\nthe agent is represented as a special object.\n3.3 Iterative Decoding Strategy\nAdding the flattened table of object information to the input\nsequences indeed improves the LMs in terms of their per-\nception of the realistic environments, which forms the foun-\ndation of grounded planning. However, the thinking process\nis still limited by the conventional seq2seq learning frame-\nwork, which assumes LMs should output a complete plan by\na single pass of decoding. We argue that a thoughtful plan-\nning process should carefully handle the coherence of each\nstep, otherwise errors accumulate and cause a failed plan.\nTherefore, we propose a simple yet effective decoding\nstrategy that learns to iteratively generate a plan step by\nstep. Specifically, we append previously generated steps un-\ntil the current step t to the input sequence (i.e., Input =\n[G + S1 + ‚ãØ +St(+E)]) for generating the next step (i.e.,\nOutput = St+1). This iterative decoding process will end\nuntil the LM generates the special token END. In the train-\ning stage, we use the ground-truth references for S‚â§t; in the\ninference stage, we do not have such references, so we use\nthe model predictions as S‚â§t.\nNotably, in contrast to the conventional seq2seq learn-\ning process, the iterative decoding strategy needs to run the\nencoder-decoder model N + 1 times to generate a plan with\nN steps. The additional computation cost for re-encoding is\nworthy. Imagine when we humans are planning a task in a\nroom. It is natural for us to come up with the plans step by\nstep, and it is very likely that the most useful information to\ngenerate different steps is about different objects. Therefore,\na temporally dynamic attention mechanism is favorable in\nplanning with LMs. Our iterative decoding strategy encour-\nages the encoder-decoder architectures to learn such ability.\n3.4 Other Methods\nPretrained table encoders. Since we use environmental\ninformation in a tabular format and BART has not been pre-\ntrained in the tabular form of input, BART may not be able\nto use this part of information well. Therefore, we employ\nTAPEX (Liu et al. 2022b), the state-of-the-art pre-trained\nlanguage model on tabular data. Using SQL execution as the\nonly pre-training task, T APEX achieves better tabular rea-\nsoning capability than BART, and thus we expect T APEX\ncan make full use of the environmental information repre-\nsented by the table in our task.\nIn-context few-shot learning with GPT-J.Finally, to ex-\nplore whether large-scale language models can master the\ntask with few-shot examples, we also experimented with\nfew-shot performance on a larger language model GPT-J 6B.\n4 Evaluation\nHow do we evaluate a method for G-PlanET? Due to the\nnovelty of the problem setup, it is challenging to evaluate\nand analyze the methods. In this section, we present a gen-\neral evaluation protocol and a complementary metric to mea-\nsure the quality of generated plans. We report the main ex-\nperimental results with the proposed evaluation protocol. We\nleave the analysis in Sec. 5.\n4.1 Metrics\nStep-wise evaluation. Conventional evaluation metrics\nsuch as BLEU (Papineni et al. 2002) and ROUGE (Lin 2004)\nmeasure the similarity between generated text and truth ref-\nerences as a whole, which is suitable for translation and sum-\nmarization. However, the output text of planning tasks such\nas our G-PlanET is highly structured. A plan naturally can\nbe split into a sequence of step-by-step actions. Using the\nconventional way to evaluate plans inevitably breaks such\ninternal structures and will lead to inaccurate measurement.\nFor example, if the first step of the generated plan is the same\nas the last step of the reference plan, the conventional eval-\nuation will still assign a high score to such a generated plan,\neven though it is not useful at all. Therefore, we argue that it\nis much more reasonable to evaluate the similarity of a pair\nof plans step by step. Specifically, we first align the genera-\ntions and the truths and compute the scores of every step2 by\nmultiple metrics. Then, we aggregate the final score by tak-\ning the average of all steps. We also consider other temporal\nweighting aggregation for more analysis in Sec. 5.\nMeasuring grounded plans. It is a unique challenge for\nevaluating G-PlanET to consider the grounding nature of\nplans. Metrics, such as BLEU, METEOR, and ROUGE, do\nnot give a suitable penalty when a plan is similar to the refer-\nence in terms of word usage, yet leading to totally different\n2The ALFRED authors ensure that the references consist of\natomic action steps and all references share the same length. There-\nfore, we consider the length of truth plans as the standard: when the\ngenerated plan has more steps than the truth plans, we cut off them;\nwhen the generation has fewer steps than the references, we dupli-\ncate the last step to make them even for step-wise evaluation.\n13195\nData Split‚Üí Unseen Room Layouts Seen Room Layouts\nMethods ‚Üì Metrics ‚Üí CIDEr SPICE KAS CIDEr SPICE KAS\nBART-base (vanilla) 0.9417 0.1378 0.2455 0.8231 0.1277 0.2197\nBART-large (vanilla) 1.4632 0.3168 0.4069 1.4414 0.3161 0.3900\nGPT-J-6B 1.1968 0.2655 0.3622 1.1047 0.2509 0.3370\nBART-base w/table 1.6706 0.3692 0.4584 1.6230 0.3595 0.4339\nBART-large w/table 1.6630 0.3491 0.4411 1.5865 0.3393 0.4204\nBART-large (TAPEX) 2.8824 0.5054 0.6373 2.7432 0.4944 0.6045\nBART-base w/table + iterative decoding 2.9147 0.5107 0.6334 2.8582 0.5118 0.6124\nBART-large w/table + iterative decoding 2.8580 0.5194 0.6518 2.8799 0.5096 0.6326\nBART-large (TAPEX) + iterative decoding 2.8440 0.5210 0.6313 2.6959 0.5036 0.6074\nTable 1: Experimental results for the G-PlanET by different base LMs. The methods are grouped by model types and whether\nencoding the environment; by decoding strategies.\nstates in an interactive environment for embodied tasks. For\nexample, it is only a one-word difference between ‚Äúturn to\nthe left‚Äù vs ‚Äúturn to the right‚Äù, but the agents that faithfully\nfollow these instructions can arrive at very different places.\nThe LM-based metrics, e.g., BERTScore (Zhang et al.\n2020), are not suitable either because the neural embed-\ndings of ‚Äúleft‚Äù and ‚Äúright‚Äù are also very similar. Plus, the\ngrounded plans for G-PlanET are object-centric in a con-\ntext and very similar to the captions of a sequence of events\nby visual perception, for which these metrics are not specif-\nically designed. Considering these limitations, we use two\ntypical metrics that are widely used for captions and devise\na new metric for complementary measurement.\nThe first two metrics are CIDEr (Vedantam, Zitnick, and\nParikh 2015) and SPICE (Anderson et al. 2016), which are\nboth widely used for tasks where the outputs are highly con-\ntextualized and describe natural scenarios in everyday life,\ne.g., VaTex (Wang et al. 2019) and CommonGen (Lin et al.\n2020). In particular, SPICE parses both the generation and\nreferences to scene graphs, a graph-based semantic repre-\nsentation. Then, it calculates the edge-based F1 score to\nmeasure the similarity between each step. Note that SPICE\ncomputation has a special focus on the propositions. This is\nparticularly favorable for evaluating G-PlanET since there\nare many actions in the grounded plans, where propositions\ncan be seen as atomic units for evaluation.\nKeyActionScore (KAS). Inspired by SPICE, a step in\na plan can be deconstructed into several propositions that\nare represented as edges. However, not all propositions in\nSPICE are necessarily important in evaluating plans for G-\nPlanET. Not to mention that SPICE relies on an external\nparser that is expensive to run yet sometimes contains noisy\noutputs. Also, most of the truth plans in the ALFRED anno-\ntations are overly specific, and it is not necessary for a plan\nto cover all details. Therefore, we devise a metric that fo-\ncuses on the key actions of the generated plans and checks if\nthey are part of references, named Key Action Score(KAS).\nSpecifically, we extract a set of key action phrases from\neach step in the generated plan ÀÜSi and the truth reference Si\nrespectively. We denote this two sets as ÀÜSi = {ÀÜa1, ÀÜa2, ‚ãØ}\nsplit ‚Üí train valid test\naspect ‚Üì - seen unseen seen unseen\n# tasks 21,025 820 821 705 694\navg. ‚à£\nG‚à£ 9.26 9.32 9.26 10.3 9.95\navg.\n# O 73.71 74.21 77.91 75.31 73.9\navg.\n# T 6.72 6.79 6.26 6.95 6.63\navg. ‚à£\nSi‚à£ 11.24 11.13 11.49 9.84 10.19\nTable 2: The avg. ‚à£G‚à£ means the average length of goal and\nthe avg. ‚à£Si‚à£ means the average length of each step. The avg.\n# O is the average number of objects in each room and the\navg. # T is the average number of steps.\nand Si = {a1, a2, ‚ãØ}. Then, we check how many action\nphrases in ÀÜSi are covered by the truth set Si, the precision\nthen becomes the KAS score for the i-th step in the plan. To\nincrease the matching quality, we curate a set of rules and a\ndictionary to map the actions that share the same behaviors.\nFor example, ‚Äúturn to the left‚Äù and ‚Äúturn left‚Äù are counted\nas a single match; ‚Äúgo straight‚Äù and ‚Äúwalk straight‚Äù can be\nmatched too. In addition, we break the compound nouns\nsuch that we allow partial scores to match for a smoother\nscoring (e.g., ‚Äúxxx on the table‚Äù vs ‚Äúxxx on the coffee ta-\nble‚Äù). Simply put, the KAS metric looks at the key actions\nextracted from the plans and checks if these important ele-\nments can be (fuzzy) matched to count as a valid step.\n4.2 Experimental Setup\nData statistics. Table 2 shows some statistics of our\ndataset that we described in Sec. 2.2. We follow the data\nsplit in ALFRED to split the train, valid, and test dataset. The\ndata split is based on whether the room layout has been seen\nin the training tasks. It is usually easier for robotic agents\nto map instructions to low-level actions in seen rooms than\nin unseen rooms. However, for the planning ability that we\nwant to study with G-PlanET in this paper, the two splits\ndo not differ very much. We keep using this split to make\nthe results consistent and convenient for people who want to\nconnect our results with the ALFRED results.\n13196\n0 0 .2 0.4 0.6 0.8 10.35\n0.45\n0.55\n0.65\n0.75\nP\nPerformance\nVanilla Table TAPEX Table Iter TAPEX Iter\n0 0 .2 0.4 0.6 0.8 10.20\n0.30\n0.40\n0.50\n0.60\nP\nPerformance\nFigure 3:\nThe step-wise reweighting results of KAS (Left)\nand SPICE (Right).The x-axis indicates the parameter p in\nthe geometric distribution and also the importance of the\npreceding step, and the y-axis indicates the weighted result\nof each step. A larger coefficient means that the previous\nstep is more important.\nImplementation details. In single-pass decod-\ning, we format the output sequences as follows:\n‚ÄúStep 1:[S1] ‚à£ Step 2: [S2] ‚à£ ‚ãØ ‚à£ END‚Äù. When\nappending the flattened table of objects, we format input\nwith ‚Äú[G] Env: [row 1] [SEP] [row 2]‚ãØ‚Äù, where the\n[row i] is a sequence of the i-th object including its id,\ntype, coordinates, rotation, parent receptacles, etc. Due to\nthe page limit, we leave the details of the data, methods,\nand hyper-parameters in the Appendix that are linked to our\nproject website.\n4.3 Main Results\nWe report the main results in Table 1, and leave the deeper\nanalysis in the next section. To sum up, we find that encod-\ning the object table as part of the inputs will significantly\nimprove the performance, and pre-training on other table-\nrelated tasks can benefit G-PlanET a lot. The iterative decod-\ning strategy is also an important component that can further\nimprove the results to some extent.\nCase Study of Table EffectAlthough we have added en-\nvironment information E to the input, it is still a problem\nwhether the model effectively uses this information. To ver-\nify this, we present a case study here. In a number of in-\nstances, we have demonstrated that the introduction of envi-\nronmental information can be helpful. Here is one example:\n‚Ä¢ truth: Close the laptop that is on the table.\n‚Ä¢ vanilla: Close the laptop and pick it up from the bed\n‚Ä¢ w/ table:Pick up the laptop on the coffee table.\nAs shown in the example, the model successfully identified\nthe location of a laptop with the help of the object table.\nEffect of model sizes. Table 1 shows that small models\ncan perform as well or even better than large models in\nsome cases. This is mainly due to the following reasons.\n1) The sentences in plans are relatively simpler than other\nNLG tasks, with a smaller vocabulary and shorter length.\nThis leaves the power of large models in terms of generation\nunexpressed, 2) G-PlanET is a task to examine the ability\nto plan rather than write. Whether this ability changes with\n(‚àí‚àû, ‚àí5](‚àí5, ‚àí3] (‚àí3, ‚àí1] 0 (0, +‚àû)0\n100\n200\n300\n400\n500\nStep Range\nCount\nVanilla Table TAPEX Table Iter TAPEX Iter\nFigure 4: The result of error statistics for # of step.\nmodel size remains to be explored. 3) For scenarios with\nthe table, the form of the task is not the same as the tra-\nditional generation task, so the training phase will have a\ngreater impact. Models with fewer parameters are more suf-\nficiently tuned with limited data.\n5 Analysis\nIn this section, we deeply analyze the performance of the\nmethods in Table 1 from multiple aspects and provide non-\ntrivial findings that can help future research. For a fair com-\nparison, all analytical experiments were performed in the\nBART-large model on the unseen split of the test data.\n5.1 Temporal Re-weighting of Scores\nWhen we computed the overall score of a plan with a metric,\nwe use the average score to aggregate the score for each step.\nHowever, in a realistic environment, there are causality con-\nstraints for an agent to complete the steps ‚Äì i.e., some tasks\ncan only be done when their prerequisite steps are finished.\nFor example, only when the agent arrives at the microwave\ncan it heat the bread in its hands.\nTherefore, the earlier steps in a plan should be of higher\nimportance, while our previous evaluation is based on a uni-\nform distribution of the weights across steps. To this end,\nwe adopt geometric distributionto re-weight the step-wise\nimportance for weighted aggregation. The geometric distri-\nbution can be used to model the number of failures before\nthe first success in repeated mutually independent Bernoulli\ntrials, each with a probability of success p.\nf(x) = p(1 ‚àí p)x (0 < p < 1)\nThis suits our setup well because when the first step is\nincorrect, the whole task can hardly be completed and exe-\ncuted in a generated plan for ALFRED. The range ofp in the\noriginal setting of the geometric distribution is restricted to\nbetween 0 and 1. When p = 0, each step has the same weight\n(uniform importance), which is exactly what we have done\nin Tab. 1. Whenp = 1, the first step is the only thing we look\nat for evaluation, meaning that the other steps will be given\nzero weights for aggregation.\nFigure 3 shows the results on unseen subset which is more\nrealistic. The performance of the iterative and non-iterative\napproaches is very close in the case of the first step. This is\nmainly because iterative methods are similar to non-iterative\nmethods when generating the first step, and differ only after\n13197\n(0, 5] (5 , 8] (8 , 10] (10, 13)0.0\n0.15\n0.30\n0.45\n0.60\nStep Range\nPerformance\nVanilla\nTable\nTAPEX\nTable Iter\nTAPEX Iter\nFigure 5: The result of KAS of tasks with a different num-\nber of steps. Due to the large variance caused by the small\nnumber of samples of certain lengths, we use the statistics\nby dividing the intervals.\nthe second step. At the same time, it can be seen that there\nis an overall downward trend in performance as the focus\nmoves to the early step. The main reason is that the later\nthe subtask is, the closer it is to the high-level instruction.\nFor example, if the task goal is to place the sponge in the\nsink, the final step must be to place the sponge in the sink.\nThis feature makes the last step of subtask generation very\nsimple, resulting in high performance. We also see that the\nperformance of the non-iterative method rises and then falls\nin KAS, and the change in a downward trend in SPICE. The\nmain reason is an error in the number of steps in the non-\niterative method, which will be explained next.\n5.2 Error Analysis on the Lengths of Plans\nWe found a huge gap in the prediction of the number of task\nsteps between iterative and non-iterative methods, which\nmay be an important reason for the final performance differ-\nence. As shown in Figure 4, iterative methods have a higher\nprobability of predicting the number of steps for the cor-\nrect task, while non-iterative methods do underestimate the\nnumber of steps. In our evaluation framework, the missing\nfollow-up steps of non-iterative methods are often gener-\nated by copying. This might be reason for the poor perfor-\nmance of non-iterative methods and the performance of non-\niterative methods increases first in the reweight step process.\n5.3 Impact of Task Length on Performance\nAlthough all the tasks in the dataset are part of daily life\ntasks, they differ in difficulty. A simple metric to evaluate the\ndifficulty of a task is the number of steps they require. Fig-\nure 5 illustrates the decrease in the quality of the generated\nsteps as the number of task steps increases. The figure also\nreflects the relatively small difference in the performance of\nthe different methods on shorter tasks. And the performance\nof all methods degrades rapidly on the longest tasks. The it-\nerative approach has more significant performance benefits\non longer tasks. This may be because this approach makes\nbetter use of the state changes due to intermediate steps and\nfixes some previous errors.\n6 Related Work\nGrounded commonsense reasoning. ALFWorld (Shrid-\nhar et al. 2021) also uses LM to generate the next step in\na text game which is based on ALFRED. SciWorld (Wang\net al. 2022) designed a text game to find whether the LMs\nhave learned to reason about commonsense. SayCan (Ahn\net al. 2022) also uses LM to find the potential next step in\nthe real world. Both these three works only expect to learn\nthe next step in a text game. Our methods share similar moti-\nvation with decision transformer (Chen et al. 2021) and Be-\nhavior Cloning (Farag and Saleh 2018), but we work on very\ndifferent applications.\nTable-based NLP. Our work is closely related to two lines\nof tabular data usage in NLP: the approach to modeling tab-\nular representations and the application of a table as an in-\ntermediate representation. For the first line of work, there is\nrich literature focusing on modeling tabular representations,\nincluding TabNet (Arik and Pfister 2019), TAPAS (Herzig\net al. 2020), TaBERT (Yin et al. 2020) and T APEX (Liu\net al. 2022b). We have explored the impact of state-of-the-\nart table representation models (e.g., TAPEX) on our task in\nexperiments. As for the second line of work, previous work\nhas explored to use of tables in several downstream tasks,\nincluding visual question answering (Yi et al. 2018), code\nmodeling (Pashakhanloo et al. 2022), and numerical reason-\ning (Pi et al. 2022; Yoran, Talmor, and Berant 2022). Dif-\nferent from them, our work is the first to explore the use of\ntabular representations in embodied tasks.\nALFRED Agents. Some previous research has been pub-\nlished on embodied tasks in realistic environments since\nthe appearance of ALFRED. E.T. (Pashevich, Schmid, and\nSun 2021) first encoded the history with a transformer to\nsolve compositional tasks and proved that pretraining and\njoint training with synthetic instructions can improve perfor-\nmance. FILM (Min et al. 2021) proposed an explicit spatial\nmemory and a semantic search policy to provide a more ef-\nfective representation for state tracking and guidance. LEBP\n(Liu et al. 2022a), the currently published SOTA method,\ngenerated a sequence of sub-steps by understanding the lan-\nguage instruction and used the predefined actual actions\ntemplate to complete the sub-steps. We also try to use these\nmethods to evaluate our generated low-level instructions.\nHowever, due to the limited importance of the low-level in-\nstructions, there is no gap with conspicuousness between our\ngenerated instructions and the ones in ALFRED.\n7 Conclusion\nIn this work, we present the first investigation into grounded\nplanning for embodied tasks using language models. The\nG-PlanET problem is of utmost significance for advancing\nthe embodied intelligence of LMs and constitutes a critical\nstep towards artificial general intelligence. To evaluate the\nperformance of encoder-decoder LMs in solving G-PlanET,\nwe developed a benchmark as well as a specialized evalu-\nation metric named KAS to assess the quality of generated\nplans. Furthermore, we propose two methods for improving\nLMs‚Äô ability in G-PlanET - flattening object tables and an\niterative decoding strategy. Our experiments and analyses\ndemonstrate their effectiveness and yield non-trivial find-\nings. This study is expected to encourage further research\ninto G-PlanET and pave the way for integrating LMs and\nembodied tasks in realistic environments.\n13198\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Her-\nzog, A.; Ho, D.; Hsu, J.; Ibarz, J.; Ichter, B.; Irpan, A.; Jang,\nE.; Ruano, R. J.; Jeffrey, K.; Jesmonth, S.; Joshi, N. J.; Ju-\nlian, R. C.; Kalashnikov, D.; Kuang, Y .; Lee, K.-H.; Levine,\nS.; Lu, Y .; Luu, L.; Parada, C.; Pastor, P.; Quiambao, J.; Rao,\nK.; Rettinghouse, J.; Reyes, D. M.; Sermanet, P.; Sievers, N.;\nTan, C.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.; Xu, P.;\nXu, S.; and Yan, M. 2022. Do As I Can, Not As I Say:\nGrounding Language in Robotic Affordances. In Confer-\nence on Robot Learning.\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. SPICE: Semantic Propositional Image Caption Eval-\nuation. In Proc. of ECCV.\nArik, S. ¬®O.; and Pfister, T. 2019. TabNet: Attentive Inter-\npretable Tabular Learning. ArXiv, abs/1908.07442.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems 33: Annual Conference\non Neural Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nChen, L.; Lu, K.; Rajeswaran, A.; Lee, K.; Grover, A.;\nLaskin, M.; Abbeel, P.; Srinivas, A.; and Mordatch, I.\n2021. Decision Transformer: Reinforcement Learning via\nSequence Modeling. In NeurIPS.\nChen, W.; Wang, H.; Chen, J.; Zhang, Y .; Wang, H.; Li, S.;\nZhou, X.; and Wang, W. Y . 2020. TabFact: A Large-scale\nDataset for Table-based Fact Verification. In 8th Interna-\ntional Conference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\nFarag, W. A.; and Saleh, Z. 2018. Behavior Cloning for\nAutonomous Driving using Convolutional Neural Networks.\n2018 International Conference on Innovation and Intelli-\ngence for Informatics, Computing, and Technologies (3ICT).\nHerzig, J.; Nowak, P. K.; M¬®uller, T.; Piccinno, F.; and Eisen-\nschlos, J. 2020. TaPas: Weakly Supervised Table Parsing via\nPre-training. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 4320‚Äì4333.\nOnline: Association for Computational Linguistics.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. In ICML.\nKolve, E.; Mottaghi, R.; Han, W.; VanderBilt, E.; Weihs,\nL.; Herrasti, A.; Deitke, M.; Ehsani, K.; Gordon, D.; Zhu,\nY .; Kembhavi, A.; Gupta, A. K.; and Farhadi, A. 2017.\nAI2-THOR: An Interactive 3D Environment for Visual AI.\nArXiv, abs/1712.05474.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 7871‚Äì7880. On-\nline: Association for Computational Linguistics.\nLin, B. Y .; Zhou, W.; Shen, M.; Zhou, P.; Bhagavatula, C.;\nChoi, Y .; and Ren, X. 2020. CommonGen: A Constrained\nText Generation Challenge for Generative Commonsense\nReasoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, 1823‚Äì1840. Online: As-\nsociation for Computational Linguistics.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74‚Äì81. Barcelona, Spain: Association for Computational\nLinguistics.\nLiu, H.; Liu, Y .; He, H.; and Yang, H. 2022a. LEBP - Lan-\nguage Expectation & Binding Policy: A Two-Stream Frame-\nwork for Embodied Vision-and-Language Interaction Task\nLearning Agents. ArXiv, abs/2203.04637.\nLiu, Q.; Chen, B.; Guo, J.; Ziyadi, M.; Lin, Z.; Chen, W.; and\nLou, J.-G. 2022b. TAPEX: Table Pre-training via Learning\na Neural SQL Executor. In Proc. of ICLR.\nMin, S. Y .; Chaplot, D. S.; Ravikumar, P.; Bisk, Y .; and\nSalakhutdinov, R. 2021. FILM: Following Instructions in\nLanguage with Modular Methods. ArXiv, abs/2110.07342.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, 311‚Äì318. Philadel-\nphia, Pennsylvania, USA: Association for Computational\nLinguistics.\nPashakhanloo, P.; Naik, A.; Wang, Y .; Dai, H.; Maniatis, P.;\nand Naik, M. 2022. CodeTrek: Flexible Modeling of Code\nusing an Extensible Relational Representation. In Proc. of\nICLR.\nPashevich, A.; Schmid, C.; and Sun, C. 2021. Episodic\nTransformer for Vision-and-Language Navigation. Proc. of\nICCV.\nPi, X.; Liu, Q.; Chen, B.; Ziyadi, M.; Lin, Z.; Gao, Y .; Fu,\nQ.; Lou, J.-G.; and Chen, W. 2022. Reasoning Like Program\nExecutors. In Conference on Empirical Methods in Natural\nLanguage Processing.\nRaffel, C.; Shazeer, N. M.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. ArXiv, abs/1910.10683.\nShridhar, M.; Thomason, J.; Gordon, D.; Bisk, Y .; Han, W.;\nMottaghi, R.; Zettlemoyer, L.; and Fox, D. 2020. ALFRED:\nA Benchmark for Interpreting Grounded Instructions for Ev-\neryday Tasks. In 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2020, Seattle, WA,\nUSA, June 13-19, 2020, 10737‚Äì10746. IEEE.\nShridhar, M.; Yuan, X.; C ÀÜot¬¥e, M.; Bisk, Y .; Trischler, A.;\nand Hausknecht, M. J. 2021. ALFWorld: Aligning Text\nand Embodied Environments for Interactive Learning. In\n9th International Conference on Learning Representations,\n13199\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting\nCommonsense Knowledge. InProceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4149‚Äì4158. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nVedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:\nConsensus-based image description evaluation. In IEEE\nConference on Computer Vision and Pattern Recognition,\nCVPR 2015, Boston, MA, USA, June 7-12, 2015, 4566‚Äì\n4575. IEEE Computer Society.\nWang, R.; Jansen, P. A.; C ÀÜot¬¥e, M.-A.; and Ammanabrolu,\nP. 2022. ScienceWorld: Is your Agent Smarter than a 5th\nGrader? In Conference on Empirical Methods in Natural\nLanguage Processing.\nWang, X.; Wu, J.; Chen, J.; Li, L.; Wang, Y .; and Wang,\nW. Y . 2019. VaTeX: A Large-Scale, High-Quality Multi-\nlingual Dataset for Video-and-Language Research. In 2019\nIEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, 4580‚Äì4590. IEEE.\nYi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenen-\nbaum, J. 2018. Neural-Symbolic VQA: Disentangling Rea-\nsoning from Vision and Language Understanding. In Ben-\ngio, S.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa-\nBianchi, N.; and Garnett, R., eds.,Advances in Neural Infor-\nmation Processing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr¬¥eal, Canada, 1039‚Äì1050.\nYin, P.; Neubig, G.; Yih, W.-t.; and Riedel, S. 2020.\nTaBERT: Pretraining for Joint Understanding of Textual and\nTabular Data. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 8413‚Äì8426.\nOnline: Association for Computational Linguistics.\nYoran, O.; Talmor, A.; and Berant, J. 2022. Turning Ta-\nbles: Generating Examples from Semi-structured Tables for\nEndowing Language Models with Reasoning Skills. In Pro-\nceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 6016‚Äì\n6031. Dublin, Ireland: Association for Computational Lin-\nguistics.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2020. BERTScore: Evaluating Text Generation with\nBERT. In 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\n13200"
}