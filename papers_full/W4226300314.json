{
    "title": "SOIT: Segmenting Objects with Instance-Aware Transformers",
    "url": "https://openalex.org/W4226300314",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2098492988",
            "name": "Xiaodong Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A3014279824",
            "name": "Dahu Shi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2099946282",
            "name": "Xing Wei",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2012588273",
            "name": "Ye Ren",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A4284029327",
            "name": "Tingqun Ye",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2158259381",
            "name": "Wenming Tan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2098492988",
            "name": "Xiaodong Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A3014279824",
            "name": "Dahu Shi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2099946282",
            "name": "Xing Wei",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2012588273",
            "name": "Ye Ren",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A4284029327",
            "name": "Tingqun Ye",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2158259381",
            "name": "Wenming Tan",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6760739867",
        "https://openalex.org/W2949530802",
        "https://openalex.org/W3034522582",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W2997277830",
        "https://openalex.org/W6600574797",
        "https://openalex.org/W6760466163",
        "https://openalex.org/W3106233461",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3149952820",
        "https://openalex.org/W6796402304",
        "https://openalex.org/W6796087570",
        "https://openalex.org/W2971475609",
        "https://openalex.org/W6735463952",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6716109767",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2987206775",
        "https://openalex.org/W6730903564",
        "https://openalex.org/W6742348326",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W2777795072",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2432481613",
        "https://openalex.org/W6772226814",
        "https://openalex.org/W3103674113",
        "https://openalex.org/W3183939440",
        "https://openalex.org/W6775515071",
        "https://openalex.org/W2925359305",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3117707016",
        "https://openalex.org/W2997999719",
        "https://openalex.org/W6772372853",
        "https://openalex.org/W6784930956",
        "https://openalex.org/W3154647408",
        "https://openalex.org/W6774945537",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W3034826836",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3206609906",
        "https://openalex.org/W3177316221",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W3113410735",
        "https://openalex.org/W3168783492",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3034428102",
        "https://openalex.org/W2993182889",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963849369",
        "https://openalex.org/W2953133772",
        "https://openalex.org/W3177165656",
        "https://openalex.org/W2989676862",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3093600664",
        "https://openalex.org/W3034681942",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4308909683",
        "https://openalex.org/W3035709993",
        "https://openalex.org/W3177388720",
        "https://openalex.org/W2982101047",
        "https://openalex.org/W3106546328",
        "https://openalex.org/W3035049382",
        "https://openalex.org/W2981537222",
        "https://openalex.org/W2982161360",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3170841864"
    ],
    "abstract": "This paper presents an end-to-end instance segmentation framework, termed SOIT, that Segments Objects with Instance-aware Transformers. Inspired by DETR, our method views instance segmentation as a direct set prediction problem and effectively removes the need for many hand-crafted components like RoI cropping, one-to-many label assignment, and non-maximum suppression (NMS). In SOIT, multiple queries are learned to directly reason a set of object embeddings of semantic category, bounding-box location, and pixel-wise mask in parallel under the global image context. The class and bounding-box can be easily embedded by a fixed-length vector. The pixel-wise mask, especially, is embedded by a group of parameters to construct a lightweight instance-aware transformer. Afterward, a full-resolution mask is produced by the instance-aware transformer without involving any RoI-based operation. Overall, SOIT introduces a simple single-stage instance segmentation framework that is both RoI- and NMS-free. Experimental results on the MS COCO dataset demonstrate that SOIT outperforms state-of-the-art instance segmentation approaches significantly. Moreover, the joint learning of multiple tasks in a unified query embedding can also substantially improve the detection performance. Code is available at https://github.com/yuxiaodongHRI/SOIT.",
    "full_text": "SOIT: Segmenting Objects with Instance-Aware Transformers\nXiaodong Yu1*, Dahu Shi1*, Xing Wei2, Ye Ren1, Tingqun Ye1, Wenming Tan1†\n1 Hikvision Research Institute, Hangzhou, China\n2 School of Software Engineering, Xi’an Jiaotong University\n{yuxiaodong7, shidahu}@hikvision.com, weixing@mail.xjtu.edu.cn,\n{renye, yetingqun, tanwenming}@hikvision.com\nAbstract\nThis paper presents an end-to-end instance segmentation\nframework, termed SOIT, that Segments Objects with\nInstance-aware Transformers. Inspired by DETR, our method\nviews instance segmentation as a direct set prediction prob-\nlem and effectively removes the need for many hand-crafted\ncomponents like RoI cropping, one-to-many label assign-\nment, and non-maximum suppression (NMS). In SOIT, mul-\ntiple queries are learned to directly reason a set of ob-\nject embeddings of semantic category, bounding-box loca-\ntion, and pixel-wise mask in parallel under the global im-\nage context. The class and bounding-box can be easily em-\nbedded by a ﬁxed-length vector. The pixel-wise mask, es-\npecially, is embedded by a group of parameters to construct\na lightweight instance-aware transformer. Afterward, a full-\nresolution mask is produced by the instance-aware trans-\nformer without involving any RoI-based operation. Over-\nall, SOIT introduces a simple single-stage instance segmen-\ntation framework that is both RoI- and NMS-free. Exper-\nimental results on the MS COCO dataset demonstrate that\nSOIT outperforms state-of-the-art instance segmentation ap-\nproaches signiﬁcantly. Moreover, the joint learning of mul-\ntiple tasks in a uniﬁed query embedding can also substan-\ntially improve the detection performance. Code is available at\nhttps://github.com/yuxiaodongHRI/SOIT.\nIntroduction\nInstance segmentation is a fundamental yet challenging task\nin computer vision, which requires an algorithm to predict\na pixel-wise mask with a category label for each instance of\ninterest in an image. As popularized in the Mask R-CNN\nframework (He et al. 2017), state-of-the-art instance seg-\nmentation methods follow a detect-then-segment paradigm\n(Cai and Vasconcelos 2019; Chen et al. 2019a; Vu, Kang,\nand Yoo 2021). These methods employ an object detector to\nproduce the bounding boxes of instances and crop the fea-\nture maps via RoIAlign (He et al. 2017) according to the de-\ntected boxes. Then pixel-wise masks are predicted by a fully\nconvolutional network (FCN) (Long, Shelhamer, and Darrell\n2015) only in the detected region (as shown in Fig 1a). The\ndetect-then-segment paradigm is sub-optimal since it has\n*Equal contribution. yCorresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nNMS\nBox Mask\nRoI Crop\n(a) Detect-then-segment pipeline.\nNMS\nBox\nMask\n(b) Detect-and-segment pipeline.\nQueries\nBox\nMask\n...\n(c) Fully end-to-end pipeline.\nFigure 1: Comparisons of different instance-level perception\npipelines. We proposed the fully end-to-end framework as\nshown in (c), which is RoI-free and NMS-free.\nthe following drawbacks: 1) Segmentation results heavily\nrely on the object detector, incurring inferior performance in\ncomplex scenarios; 2) RoIs are always resized into patches\nof the same size (e.g., 14 ×14 in Mask R-CNN), which re-\nstricts the quality of segmentation masks, as large instances\nwould require higher resolution features to retain details at\nthe boundary. To overcome the drawbacks of this paradigm,\nrecent works (Chen et al. 2019b; Xie et al. 2020; Cao et al.\n2020a; Peng et al. 2020) start to build instance segmenta-\ntion frameworks on top of single-stage detectors (Lin et al.\n2017b; Tian et al. 2019), getting rid of local RoI operations.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3188\nHowever, these methods still rely on one-to-many label as-\nsignment in training and hand-crafted Non-Maximum Sup-\npression (NMS) post-processing to eliminate duplicated in-\nstances when testing. As a result, these two categories of\ninstance segmentation methods are not end-to-end fully op-\ntimized and suffer from sub-optimal solutions.\nInspired by the recent application of transformer archi-\ntecture in object detection (Carion et al. 2020; Zhu et al.\n2021), we present a transformer-based instance segmen-\ntation framework, namely SOIT (Segment Objects with\nInstance-aware Transformer) in this paper. We reformulate\ninstance segmentation as a direct set prediction problem\nand builds a fully end-to-end approach. Concretely, given\nmultiple randomly initialized object queries, SOIT learns\nto reason a set of object embeddings of semantic category,\nbounding-box, and pixel-wise mask simultaneously, under\nthe global image context. SOIT adopts the bipartite match-\ning strategy to assign a learning target for each object query.\nAs shown in Fig. 1c, this training approach is advantageous\nto conventional one-to-many instance segmentation training\nstrategies (He et al. 2017; Wang et al. 2020b; Tian, Shen,\nand Chen 2020) as it avoids the heuristic label assignment\nand eliminates the need for NMS post-processing.\nA compact ﬁxed-length vector can easily embed the se-\nmantic category and bounding-box in the end-to-end learn-\ning framework. However, it is not trivial to represent a spa-\ntial binary mask of each object for learning as the mask is\nhigh-dimensional and varies from each instance. To solve\nthis problem, we embed the pixel-wise mask to a group\nof instance-aware parameters, whereby a unique instance-\naware transformer is constructed. Moreover, we propose a\nnovel relative positional encoding for the transformer, which\nprovides strong location cues to distinguish different objects.\nThen the instance-aware transformer is employed to seg-\nment the object in a high-resolution feature map directly.\nIt is expected that the instance-aware parameters and rel-\native positional encoding can encode the characteristics of\neach instance. Thus it can only ﬁre on the pixels of the par-\nticular object. As described above, our method is naturally\nRoI-free and NMS-free, which eliminates many extra hand-\ncrafted operations involved in previous instance segmenta-\ntion methods.\nOur main contributions are summarized as follows:\n• We attempt to solve instance segmentation from a new\nperspective that uses parallel instance-aware transform-\ners in an end-to-end framework. This novel solution en-\nables the framework to directly generate pixel-wise mask\nresults of each instance without RoI cropping or NMS\npost-processing.\n• In our method, queries learn to encode multiple ob-\nject representations simultaneously, including categories,\nlocations, and pixel-wise masks. This multi-task joint\nlearning paradigm establishes a collaboration between\nobjection detection and instance segmentation, encourag-\ning these two tasks to beneﬁt from each other. We demon-\nstrate that our architecture can also signiﬁcantly improve\nobject detection performance.\n• To show the effectiveness of the proposed framework,\nwe conduct extensive experiments on the COCO dataset.\nSOIT with ResNet-50 achieves 42.5% mask AP and\n49.1% box AP on the test-dev split without any\nbells and whistles, outperforming the complex well-\ntuned HTC (Chen et al. 2019a) by 2.8% in mask AP and\n4.2% in box AP.\nRelated Work\nInstance Segmentation\nInstance segmentation is a challenging task, as it requires\ninstance-level and pixel-wise predictions simultaneously.\nThe existing approaches can be summarized into three cat-\negories: top-down, bottom-up, and single-stage methods. In\ntop-down methods, the Mask R-CNN family (He et al. 2017;\nCai and Vasconcelos 2019; Chen et al. 2019a; Cao et al.\n2020b) follow the detect-then-segment paradigm, which ﬁrst\nperforms object detection and then segments objects in the\nboxes. Moreover, some recent works (Lee and Park 2020;\nWang et al. 2020a; Chen et al. 2020b) are proposed to\nimprove the segmentation performance further. Bottom-up\nmethods (Liu et al. 2017; Gao et al. 2019) view the task as a\nlabel-then-cluster problem. They ﬁrst learn per-pixel embed-\ndings and then cluster them into instance groups. Besides,\nYOLACT (Bolya et al. 2019), CondInst (Tian, Shen, and\nChen 2020) and SOLO (Wang et al. 2020b) build single-\nstage instance segmentation framework on the top of one-\nstage detectors (Tian et al. 2019), achieving competitive per-\nformance. Concurrently, QueryInst (Fang et al. 2021) and\nSOLQ (Dong et al. 2021) aim at building end-to-end in-\nstance segmentation frameworks, eliminating NMS post-\nprocessing. However, they still need RoI cropping to sepa-\nrate different instances ﬁrst, which may have the same limi-\ntations of the detect-then-segment pipeline. In this paper, we\ngo for an end-to-end instance segmentation framework that\nneither relies on RoI cropping nor NMS post-processing.\nTransformer in Vision\nTransformer (Vaswani et al. 2017) introduces the self-\nattention mechanism to model long-range dependencies and\nhas been widely applied in natural language processing\n(NLP). Recently, several works attempted to involve Trans-\nformer architecture in computer vision tasks and showed\npromising performances. ViT series (Dosovitskiy et al.\n2020; Touvron et al. 2021) take an image as a sequence of\npatches and achieve the cross-patch interactions by trans-\nformer architecture in image classiﬁcation. DETR (Car-\nion et al. 2020), and Deformable DETR (Zhu et al. 2021)\nadopted learnable queries and transformer architecture to-\ngether with bipartite matching to perform object detection\nin an end-to-end fashion, without any hand-crafted process\nsuch as NMS. SETR (Zheng et al. 2021) reformulates the\nimage semantic segmentation problem from a sequence-\nto-sequence learning perspective, offering an alternative to\nthe dominating encoder-decoder FCN model design. De-\nspite transformer architecture is being widely used in many\ncomputer vision tasks, few efforts are conducted to build\na transformer-based instance segmentation framework. We\naim to achieve this goal in this paper.\n3189\nFC\nMLP\nMLP\nObject Queries\nClassification\nLocation\nInstance-aware \nTransformer\n...\nMask Encoder\nEncoder\nDecoder\nN x C\nN x 4\nN x DF3\nF4\nF5\nF6 P6\nP5\nP4\nP3 Mask\nFigure 2: Illustration of the overall architecture of SOIT. F3 to F6 are the multi-scale image feature maps extracted from the\nbackbone (e.g., ResNet-50). P3 to P6 are the multi-scale feature memory reﬁned by the transformer encoder. Fmask represents\nmask features produced by the mask encoder. D-dimensional (e.g. 441 by default) dynamic parameters generated in the mask\nbranch are used to construct the instance-aware transformer. As shown in the blue dashed box, the pixel-wise mask is produced\nvia the instance-aware transformer, of which the details are described in Figure 3.\nDynamic Networks\nUnlike traditional network layers with ﬁxed ﬁlters once\ntrained, the ﬁlters of dynamic networks are conditioned on\nthe input and dynamically generated by another network.\nThis idea has been explored previously in convolution mod-\nules like dynamic ﬁlter networks (Jia et al. 2016), and Cond-\nConv (Yang et al. 2019), to increase the capacity of a clas-\nsiﬁcation network. Recently, some works (Tian, Shen, and\nChen 2020; Shi et al. 2021) employ the dynamic ﬁlters,\nconditioned on each instance in the image, to implement\ninstance-level vision tasks. In this work, we extend this idea\nto transformer architecture and build instance-aware trans-\nformers to solve the challenging instance segmentation task.\nMethod\nIn this section, we ﬁrst introduce the overall architecture\nof our framework. Next, we elaborate on the proposed\ninstance-aware transformer employed to produce the full-\nresolution mask for each instance. Then, we describe rel-\native positional encoding to improve instance segmentation\nperformance further. At last, the training losses of our model\nare summarized.\nOverall Architecture\nAs depicted in Fig. 2, the proposed framework is com-\nposed of three main components: a backbone network to ex-\ntract multi-scale image feature maps, a transformer encoder-\ndecoder to produce object-related query features in parallel,\nand a multi-task prediction network to perform object detec-\ntion and instance segmentation simultaneously.\nMulti-Level Features. Given an image I ∈ RH×W ×3 ,\nwe extract multi-scale feature maps F = {F3;F4;F5;F6}\n(blue feature maps in Fig 2) from the backbone (e.g., ResNet\n(He et al. 2016)). Speciﬁcally, {Fl}5\nl=3 are produced by\nadding a 1 ×1 convolution on the output feature maps of\nstage C3 through C5 in the backbone, where Cl is of reso-\nlution 2l lower than the input images. The lowest resolution\nfeature map F6 is obtained via a 3 ×3 stride 2 convolution\non the ﬁnal C5 stage. Multi-scale image feature maps in F\nare all of 256 channels.\nTransformer Encoder-Decoder. In this work, we employ\nthe deformable transformer encoder (Zhu et al. 2021) to pro-\nduce multi-scale feature memory. Each encoder layer com-\nprises a multi-scale deformable attention module (Zhu et al.\n2021) and a feed-forward network (FFN). There are six en-\ncoder layers stacked in sequence in our framework. The en-\ncoder takes the image feature maps F as input and output\nthe reﬁned multi-scale feature memory P = {Pl}6\nl=3 (or-\nange feature maps in Fig. 2) with the same resolutions.\nGiven the reﬁned multi-scale feature memory P and N\nlearnable object queries, we then generate the instance-\naware query embeddings for target objects by the de-\nformable transformer decoder (Zhu et al. 2021). Similar\nto the encoder, six decoder layers are applied sequentially.\nEach one is composed of a self-attention module, and a de-\nformable cross-attention module (Zhu et al. 2021), where\nobject queries interact with each other and the global con-\ntext, respectively. In the end, the instance-aware query fea-\ntures are collected and then fed into the multi-task prediction\nnetwork.\nMulti-Task Predictions. After query feature extraction,\neach query embedding represents the features of the corre-\nsponding instance. Subsequently, we simultaneously apply\nthree branches to generate the category, bounding-box lo-\ncation, and pixel-wise mask of the targeting instance. The\n3190\nFC\n+ Rel Pos Encoding Output masks\n...\n...\nFC\nA1 A2\nA3 A4\noffsets\nweigths\nFC\nInstance-aware \nTransformer\nFigure 3: Detailed structure of the instance-aware trans-\nformer. Two linear projections (i.e., FC) predict sampling lo-\ncations and attention weights for different feature points, re-\nspectively. Another linear projection is employed for output\nprojection. In our instance-aware transformer, all weights of\nthese three layers are dynamically generated in the mask\nbranch and conditioned on the target object.\nclassiﬁcation branch is a linear projection layer (FC) that\npredicts the class conﬁdence for each object. The location\nbranch is a multi-layer perceptron (MLP) with a hidden\nsize of 256 and predicts the normalized center coordinates,\nheight, and width of the boxw.r.t.the input image. The mask\nbranch architecture is the same as the location branch ex-\ncept that the channel of the output layer is set to D. It is\nworth noting that the output of the mask branch is a group of\ndynamic parameters conditioned on the particular instance.\nThese parameters are later employed to construct instance-\naware transformers to directly generate masks from full-\nimage feature maps, elaborated in the following subsection.\nInstance-Aware Transformers\nUnlike semantic category and bounding-box, it is challeng-\ning to represent the per-pixel mask by a compact ﬁxed-\nlength vector without RoI cropping. Our core idea is that for\nan image with N instances, N different transformer encoder\nnetworks will be dynamically generated. It is expected that\nthe instance-aware transformer can encode the characteris-\ntics of each instance and only ﬁres on the pixels of the cor-\nresponding object. To avoid the quadratic growth of the com-\nputational complexity in the original transformer encoder\n(Vaswani et al. 2017), we build our instance-aware trans-\nformer on the deformable transformer encoder (Zhu et al.\n2021) for efﬁciency.\nConcretely, given an input feature map x ∈RC×H×W ,\nlet q indexes a query (e.g., the green grid point in Fig. 3)\nwith content feature zq and a 2-d reference point pq, the\ndeformable multi-head attention feature is calculated by\nHn\nm =\nKX\nk=1\nAn\nmqk ·x(pq + \u0001pn\nmqk); (1)\nwhere m ∈[1;2;:::;M ] indexes the attention head, k in-\ndexes the sampled keys, andKis the total sampled key num-\nber (K ≪ HW). ndenotes the n-th object query (i.e., in-\nstance). As shown in Fig. 3, \u0001pmqk and Amqk are the sam-\npling offset and attention weight of thekth sampling point in\nthe mth attention head, respectively. Both \u0001pmqk and Amqk\nare obtained via a linear projection (i.e., FC) layer over the\nquery feature zq. Afterwards, another linear projection layer\n(i.e., Wn) is applied for output projection, which can be for-\nmulated as\nMaskn = Wn [Concat (Hn\n1 ;Hn\n2 ;:::;H n\nM )] ; (2)\nwhere “Concat” represents the concatenating operation.\nTo establish our instance-aware transformer encoder, the\nweights of these three linear projection layers are dynami-\ncally generated, conditioned on the target instance. Specif-\nically, for n-th object query, the D parameters predicted in\nthe mask branch are split into three parts and converted as\nthe weights of the three linear projections. Moreover, the\nchannel of the output projection layer is set to 1 for the mask\nprediction, followed by a sigmoid activate function. Note\nthat the attention locations and weights for each instance are\ndifferent even at the same feature point, so each instance has\na particular preference for where to focus in the feature map.\nShared Mask Features. To get high-quality masks, our\nmethod generates pixel-wise masks on a full-image feature\nmap, not a cropped region with ﬁxed size (e.g., 14 ×14\nin Mask R-CNN (He et al. 2017)). As shown in Fig. 2,\nthe mask encoder branch is employed to provide the high-\nresolution feature map Fmask ∈ RHmask×Wmask×Cmask\nthat the instance-aware transformers take as input to pre-\ndict the per-instance mask. The mask encoder branch is\nconnected to aggregated feature P3, and thus, its output\nresolution is 1=8 of the input image. It consists of a de-\nformable transformer encoder layer, whose feature dimen-\nsion is 256 (same as the feature channels of P3). Afterward,\na linear projection layer with layer normalization (LN) is\nemployed to reduce the feature dimension from 256 to 8\n(i.e., Cmask = 8). As described above, the instance-aware\ntransformer mask head is very compact due to the few chan-\nnels of the shared mask feature.\nRelative Positional Encodings\nAs described in (Vaswani et al. 2017), the original positional\nencoding in transformer is calculated by sine and cosine\nfunctions of different frequencies:\nPE(pos;2i) = sin(pos=100002i=dmodel )\nPE(pos;2i+1) = cos(pos=100002i=dmodel )\n(3)\nwhere pos is the absolute position, i is the dimension and\ndmodel is the embedding dimension. DETR (Carion et al.\n2020) extends the above positional encoding to the 2D case.\nSpeciﬁcally, for both spatial coordinates (x;y) of each em-\nbedding in the 2D feature map, DETR independently uses\ndmodel=2 sine and cosine functions with different frequen-\ncies. Then they are concatenated to get the ﬁnaldmodel chan-\nnel positional encoding.\nFor our instance-aware transformer encoder, the input is\nthe sum of the shared mask feature and the absolute posi-\n3191\ntional encoding as described above. To further utilize the lo-\ncation information of each object query, we propose a new\nrelative positional encoding, which can be written as:\nPE(pos;2i) = sin((pos−posq)=100002i=dmodel )\nPE(pos;2i+1) = cos((pos−posq)=100002i=dmodel )\n(4)\nwhere posq is the center location of the box predicted by\ncurrent object query. Please note that the proposed relative\npositional encoding provides a strong cue for predicting the\ninstance mask. The performance improvement in the abla-\ntion study demonstrates its superiority compared to the orig-\ninal absolute positional encoding.\nTraining Loss\nIn this work, the ﬁnal outputs of our framework are su-\npervised by three sub-tasks: classiﬁcation, localization, and\nsegmentation. We use the same loss functions for classiﬁ-\ncation and localization as in (Zhu et al. 2021), and adopt\nthe Dice Loss (Milletari, Navab, and Ahmadi 2016) and the\nbinary cross-entropy (BCE) loss for instance segmentation.\nThe overall loss function is written as:\nL= \u0015clsLcls+\u0015L1LL1 +\u0015iouLiou+\u0015diceLdice+\u0015bceLbce:\nFollowing (Zhu et al. 2021), we set \u0015cls = 2, \u0015L1 = 5 and\n\u0015iou = 2. We empirically ﬁnd \u0015dice = 8 and \u0015bce = 2 work\nbest for the proposed framework.\nExperiments\nDataset and Metrics\nWe validate our method on COCO benchmark (Lin et al.\n2014). COCO 2017 dataset contains 115k images for train-\ning (split train2017), 5k for validation (split val2017)\nand 20k for testing (split test-dev), involving 80 ob-\nject categories with instance-level segmentation annotations.\nFollowing the common practice, our models are trained with\nsplit train2017, and all the ablation experiments are eval-\nuated on split val2017. Our main results are reported on\nthe test-dev split for comparisons with state-of-the-art\nmethods. Consistent with previous methods (He et al. 2017),\nthe standard mask AP is used to evaluate the performance of\ninstance segmentation. Moreover, we also report the box AP\nto show the object detection performance.\nImplementation Details\nImageNet (Deng et al. 2009) pre-trained ResNet (He et al.\n2016) is employed as the backbone and multi-scale feature\nmaps {Fl}L\nl=1 are extracted without FPN (Lin et al. 2017a).\nUnless otherwise noted, the deformable attention (Zhu et al.\n2021) has 8 attention heads, and the number of sampling\npoints is set as 4. The feature channels in the encoder and\ndecoder are 256, and the hidden dim of FFNs is 1024. We\ntrain our model with Adam optimizer (Kingma and Ba 2015)\nwith base learning rate of 2:0 ×10−4 , momentum of 0.9\nand weight decay of 1:0 ×10−4 . Models are trained for 50\nepochs, and the initial learning rate is decayed at40th epoch\nby a factor of 0.1. Multi-scale training is adopted, where the\nshorter side is randomly chosen within [480, 800] and the\nFigure 4: Qualitative results of object detection and in-\nstance segmentation on COCO val2017 split. The model\nis trained on COCO train2017 split with ResNet-50\nbackbone.\nlonger side is less or equal to 1333. When testing, the in-\nput image is resized to have the shorter side being 800 and\nthe longer side less or equal 1333. All experiments are con-\nducted on 16 NVIDIA Tesla V100 GPUs with a total batch\nsize of 32.\nMain Results\nAs shown in Table 1, we compare SOIT with state-of-the-\nart instance segmentation methods on COCO test-dev\nsplit. Without bells and whistles, our method achieves the\nbest performance on object detection and instance segmen-\ntation. Compared to the typical two-stage method Mask R-\nCNN (He et al. 2017), SOIT with ResNet-50 signiﬁcantly\nimproves box AP and mask AP by 7.8% and 5.0%, respec-\ntively. The performance of SOIT is also better than the well-\ntuned HTC (Chen et al. 2019a) by 4.2% box AP and 2.8%\nmask AP, which is an improved version of Mask R-CNN\npresenting interleaved execution and complicated mask in-\nformation ﬂow. CondInst (Tian, Shen, and Chen 2020) is\nthe latest state-of-the-art one-stage instance segmentation\napproach based on dynamic convolutions. SOIT with the\nsame ResNet-50 backbone outperforms CondInst with 4.7%\nmask AP. With a stronger backbone, ResNet-101, SOIT still\noutperforms the state-of-the-art methods over 2.0% mask\nAP. Beneﬁting from the RoI-free scheme, our method with\nResNet-50 surpasses the recent SOLQ (Dong et al. 2021)\nand QueryInst (Fang et al. 2021) by 2.8% and 1.9%, re-\nspectively. We also apply SOIT to the recent Swin Trans-\nformer backbone (Liu et al. 2021) without further modiﬁca-\ntion, building a pure transformer-based instance segmenta-\ntion framework. Our model with Swin-L can achieve 56.9%\nand 49.2% in box AP and mask AP, respectively.\nWe provide some qualitative results of SOIT with ResNet-\n50 backbone on COCO val2017 split, as shown in Fig 4.\nOur masks are generally of high quality (e.g., preserving\nmore details at object boundaries), and the detected boxes\nare precise.\n3192\nMethod Backbone RoI-free NMS-free AP AP50 AP75 APS APM APL APbox\nMask R-CNN (He et\nal. 2017)\nResNet-50\n37.5 59.3 40.2 21.1 39.6 48.3 41.3\nCMR (Cai and V\nasconcelos 2019) 38.8 60.4 42.0 19.4 40.9 53.9 44.5\nHTC (Chen et al.\n2019a) 39.7 61.4 43.1 22.6 42.2 50.6 44.9\nBlendMask (Chen et al.\n2020a) 37.0 58.9 39.7 17.3 39.4 52.5 42.7\nCondInst (Tian, Shen,\nand Chen 2020) X 37.8 59.2 40.4 18.2 40.3 52.7 41.9\nSOLOv2 (Wang et\nal. 2020c) X 38.2 59.3 40.9 16.0 41.2 55.4 40.4\nDSC (Ding et al.\n2021) 40.5 61.8 44.1 - - - 46.0\nReﬁneMask (Zhang et al.\n2021) 40.2 - - - - - -\nSCNet (Vu, Kang,\nand Yoo 2021) 40.2 62.3 43.4 22.4 42.8 53.4 45.0\nSOLQ (Dong et al.\n2021) X 39.7 - - 21.5 42.5 53.1 47.8\nQueryInst (Fang et\nal. 2021) X 40.6 63.0 44.0 23.4 42.5 52.8 45.6\nSOIT (Ours) X X 42.5 65.3 46.0 23.8 45.4 55.7 49.1\nMask R-CNN (He et\nal. 2017)\nResNet-101\n38.8 60.9 41.9 21.8 41.4 50.5 43.1\nCMR (Cai and V\nasconcelos 2019) 39.9 61.6 43.3 19.8 42.1 55.7 45.7\nHTC (Chen et al.\n2019a) 40.7 62.7 44.2 23.1 43.4 52.7 46.2\nMEInst (Zhang et al.\n2020) 33.9 56.2 35.4 19.8 36.1 42.3 -\nBlendMask (Chen et al.\n2020a) 39.6 61.6 42.6 22.4 42.2 51.4 44.8\nCondInst (Tian, Shen,\nand Chen 2020) X 39.1 60.9 42.0 21.5 41.7 50.9 43.3\nSOLOv2 (Wang et\nal. 2020c) X 39.7 60.7 42.9 17.3 42.9 57.4 42.6\nDCT-Mask (Shen et\nal. 2021) 40.1 61.2 43.6 22.7 42.7 51.8 -\nDSC (Ding et al.\n2021) 40.9 62.5 44.5 - - - 46.7\nReﬁneMask (Zhang et al.\n2021) 41.2 - - - - - -\nSCNet (Vu, Kang,\nand Yoo 2021) 41.3 63.9 44.8 22.7 44.1 55.2 46.4\nSOLQ (Dong et al.\n2021) X 40.9 - - 22.5 43.8 54.6 48.7\nQueryInst (Fang et\nal. 2021) X 42.8 65.6 46.7 24.6 45.0 55.5 48.1\nSOIT (Ours) X X 43.4 66.3 46.9 23.9 46.4 57.4 50.0\nSOLQ (Dong et al.\n2021)\nSwin-L\nX 46.7 - - 29.2 50.1 60.9 56.5\nQueryInst (Fang et\nal. 2021) X 49.1 74.2 53.8 31.5 51.8 63.2 56.1\nSOIT (Ours) X X 49.2 74.3 53.5 30.2 52.7 65.2 56.9\nTable 1: Comparisons with state-of-the-art instance segmentation methods on the COCO test-dev. “CMR” is short for\n“Cascade Mask RCNN”. AP box denotes box AP, and AP without superscript denotes mask AP. All models are trained with\nmulti-scale and tested with single scale.\nAblation Study\nNumber of Heads in Instance-Aware Transformers.\nThe multi-head attention mechanism is of great importance\nfor the transformer. In this section, we discuss the effect of\nthis design on our instance-aware transformer encoder. We\nvary the number of heads of multi-head attention, and the\nperformance of instance segmentation is shown in Table 2.\nWe ﬁnd that using only one head of attention already has a\nmoderate capacity and leads to a qualiﬁed performance with\n37.8% mask AP. The performance of instance segmentation\nimproves gradually with the increased number of attention\nheads in the instance-aware transformer. Besides, when the\nnumber of attention heads increases up to 8, segmentation\nperformance does not improve further. We assume there are\ntwo reasons for this saturation in performance. One is that\n4 different spaces of representation are sufﬁcient for distin-\nguishing various instances. The other reason is that predict-\ning too many parameters (873 parameters) makes optimiz-\ning the mask branch difﬁcult. Therefore, we set the number\nof attention heads in the instance-aware transformer to 4 by\ndefault in the following experiments.\nArchitectures of Mask Encoder.We then investigate the\nimpact of the proposed mask encoder with different archi-\ntectures. We ﬁrst changeCmask, i.e., the number of channels\nHeads AP AP50 AP75 APS APM APL\n1 37.8 61.6 39.5 18.1 41.1 57.6\n2 38.1 61.9 39.9 18.5 41.3 58.1\n4 38.4 62.0 40.1 18.6 41.7 58.4\n8 38.3 62.0 40.1 18.4 41.9 58.4\nTable 2: Instance segmentation results on COCO val2017\nsplit with different number of heads of multi-head atten-\ntion in instance-aware transformer. The input feature chan-\nnel (i.e., Cmask) is ﬁxed to 8 by default.\nof the mask encoder’s output feature maps (i.e.,Fmask). As\nshown in Table 3a, the performance drops 0.8% in mask AP\n(from 38.4% to 37.6%) when the channel of Fmask shrinks\nfrom 8 to 4. In this case, the multi-heads attention only has\na single-channel map in each attention head. It is hard for\nthe attention module to obtain sufﬁcient information on each\ninstance. Besides, the performance keeps almost the same\nwhen Cmask increases from 8 to 16. Thus, we ﬁx the mask\nfeature channels to 8 in all other experiments by default. As\nthe Cmask = 8 and the number of attention heads is 4, there\nare a total of 441 parameters predicted by the mask branch\nfor constructing the instance-aware transformer.\n3193\nChannels AP AP50 AP75 APS APM APL\n4 37.6 61.8 39.2 18.2 40.8 57.5\n8 38.4 62.0 40.1 18.6 41.7 58.4\n16 38.3 62.0 40.0 18.5 41.7 58.3\n(a) Vary the\noutput channels of the mask encoder.\nLayers AP AP50 AP75 APS APM APL\n0 37.9 61.4 39.4 18.0 40.9 57.6\n1 38.4 62.0 40.1 18.6 41.7 58.4\n2 38.4 61.9 40.1 18.5 41.6 58.6\n(b) Vary the\nlayers of stacked mask encoder.\nTable 3: Instance\nsegmentation results on COCO val2017 split with different architectures of the mask encoder. “Channels”:\nthe number of channels of the mask encoder’s output. “Layers”: the number of stacked mask encoder.\nPE AP AP50 AP75 APS APM APL\nNone 37.9 61.4 39.6 18.3 41.2 58.0\nAbs 38.4 62.0 40.1 18.6 41.7 58.4\nRel 39.2 62.9 41.3 19.7 43.0 59.2\nTable 4: Impact of the positional encoding in instance-aware\ntransformer on COCO val2017 split. “None” means re-\nmoving positional encoding, “Abs” represents the traditional\nabsolute positional encoding and “Rel” represents the pro-\nposed relative positional encoding.\nTo demonstrate the effectiveness of the mask encoder, we\ndirectly connect a linear projection (output channel is 8) with\nlayer normalization to the feature mapP3 instead of the pro-\nposed mask encoder. As shown in Table 3b, the segmenta-\ntion performance drops 0.5% (from 38.4% to 37.9%). This\nresult proves the importance of the mask encoder, which\ngenerates the specialized mask feature and decouples it from\nthe generic image context feature. Moreover, when more\nmask encoders are stacked, no noticeable improvement of\nperformance is obtained, as shown in Table 3b (3rd row).\nThis indicates that one mask encoder is sufﬁcient, resulting\nin a compact instance segmentation model.\nRelative Positional Encodings. We further investigate the\neffect of our proposed relative positional encodings for\nthe instance-aware transformers. Abs is the absolute posi-\ntional encodings used in many transformer-based architec-\ntures (Carion et al. 2020; Zhu et al. 2021). Rel is the pro-\nposed relative positional encodings in Equation (4), which\nemploy the box center coordinates of object queries to ob-\ntain the instance-aware location information. As shown in\nTable 4 (1st row), the performance of our model drops 0.5%\nin mask AP after removing absolute positional encodings to\nthe mask features. The instance-aware transformer cannot\ndistinguish the instances with similar appearances at differ-\nent locations without the positional information. As shown\nin Table 4 (3rd row), the relative positional encodings im-\nprove the segmentation performance of our SOIT by 0.8%\ncompared to the absolute positional encodings. We argue\nthat the relative positional encoding is highly correlated with\nthe corresponding object query and provides a strong loca-\ntion cue, for instance mask prediction. Therefore, in the se-\nquel, we use the proposed relative positional encoding for\nall the following experiments.\nStages Enabling Mask Loss. Ultimately, we ablate the\nimpact of the number of decoder stages enabling mask loss\nStages AP AP 50 AP75 APbox APbox\n50 APbox\n75\n0 - - - 46.8 66.3 50.7\n1 39.2 62.9 41.3 47.3 66.2 52.0\n2 40.7 63.6 43.4 47.6 66.4 52.5\n3 41.2 63.9 44.1 48.1 66.5 52.8\n4 41.7 64.2 44.5 48.2 66.4 53.0\n5 42.0 64.5 44.9 48.5 66.7 53.2\n6 42.2 64.6 45.3 48.9 67.0 53.4\nTable 5: Ablation of the number of decode stages enabling\nmask loss on COCOval2017 split. Stages isKmeans that\nenable the last K decoder layers with mask loss. 0 stages\nrepresents a object detection model without any mask su-\npervision. APbox denotes box AP.\nin training. The classiﬁcation and localization loss are en-\nabled in all decoder stages in these ablations by default. Note\nthat we throw away all the predicted mask parameters in the\nintermediate stages when the training is completed and only\nuse the ﬁnal stage predictions for inference. As shown in Ta-\nble 5, enabling more decoder layers with mask loss can im-\nprove both instance segmentation and object detection per-\nformance consistently. The experimental results show that\nadding mask loss on all decoders can improve 3.0% mask\nAP and 1.6% box AP compared to enabling mask loss on\nonly one decoder, respectively. The gain of detection per-\nformance is mainly derived from the joint training with in-\nstance segmentation. As shown in Table 5 (last row), the de-\ntection performance of the SOIT surpasses the pure object\ndetector by 2.1% (from 46.8% to 48.9%) with all decoder\nstages enabling mask loss. This indicates the advantages of\nour framework, which learns a uniﬁed query embedding to\nperform instance segmentation and object detection simul-\ntaneously.\nConclusion\nIn this paper, we present a transformer-based instance seg-\nmentation approach, termed SOIT. It reformulates instance\nsegmentation as a direct set prediction problem and builds\na fully end-to-end framework. SOIT is naturally RoI-free\nand NMS-free, avoiding many hand-crafted operations in-\nvolved in previous instance segmentation methods. Exten-\nsive experiments on the MS COCO dataset show that SOIT\nachieves state-of-the-art performance in instance segmenta-\ntion as well as object detection. We hope that our simple\nend-to-end framework could serve as a strong baseline for\ninstance-level perception.\n3194\nAcknowledgments\nThis work is funded by National Natural Science Founda-\ntion of China under Grant No. 62006183, National Key Re-\nsearch and Development Project of China under Grant No.\n2020AAA0105600, China Postdoctoral Science Foundation\nunder Grant No. 2020M683489, and the Fundamental Re-\nsearch Funds for the Central Universities under Grant No.\nxhj032021017-04 and xzy012020013.\nReferences\nBolya, D.; Zhou, C.; Xiao, F.; and Lee, Y . J. 2019. Yolact:\nReal-time instance segmentation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n9157–9166.\nCai, Z.; and Vasconcelos, N. 2019. Cascade r-cnn: High\nquality object detection and instance segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\nCao, J.; Anwer, R. M.; Cholakkal, H.; Khan, F. S.; Pang, Y .;\nand Shao, L. 2020a. Sipmask: Spatial information preser-\nvation for fast image and video instance segmentation. In\nComputer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part XIV\n16, 1–18. Springer.\nCao, J.; Cholakkal, H.; Anwer, R. M.; Khan, F. S.; Pang,\nY .; and Shao, L. 2020b. D2det: Towards high quality ob-\nject detection and instance segmentation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 11485–11494.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, H.; Sun, K.; Tian, Z.; Shen, C.; Huang, Y .; and Yan, Y .\n2020a. BlendMask: Top-down meets bottom-up for instance\nsegmentation. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 8573–8581.\nChen, K.; Pang, J.; Wang, J.; Xiong, Y .; Li, X.; Sun, S.;\nFeng, W.; Liu, Z.; Shi, J.; Ouyang, W.; et al. 2019a. Hybrid\ntask cascade for instance segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 4974–4983.\nChen, X.; Girshick, R.; He, K.; and Doll ´ar, P. 2019b. Ten-\nsormask: A foundation for dense object segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2061–2069.\nChen, X.; Lian, Y .; Jiao, L.; Wang, H.; Gao, Y .; and Lingling,\nS. 2020b. Supervised edge attention network for accurate\nimage instance segmentation. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part XXVII 16, 617–631. Springer.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDing, H.; Qiao, S.; Yuille, A.; and Shen, W. 2021. Deeply\nShape-guided Cascade for Instance Segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 8278–8288.\nDong, B.; Zeng, F.; Wang, T.; Zhang, X.; and Wei, Y . 2021.\nSOLQ: Segmenting Objects by Learning Queries. NeurIPS.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFang, Y .; Yang, S.; Wang, X.; Li, Y .; Fang, C.; Shan, Y .;\nFeng, B.; and Liu, W. 2021. Instances As Queries. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 6910–6919.\nGao, N.; Shan, Y .; Wang, Y .; Zhao, X.; Yu, Y .; Yang, M.;\nand Huang, K. 2019. Ssap: Single-shot instance segmenta-\ntion with afﬁnity pyramid. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 642–651.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proceedings of the IEEE international conference\non computer vision, 2961–2969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nJia, X.; De Brabandere, B.; Tuytelaars, T.; and Gool, L. V .\n2016. Dynamic ﬁlter networks. Advances in neural infor-\nmation processing systems, 29: 667–675.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. ICLR, 9.\nLee, Y .; and Park, J. 2020. Centermask: Real-time anchor-\nfree instance segmentation. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n13906–13915.\nLin, T.-Y .; Doll´ar, P.; Girshick, R.; He, K.; Hariharan, B.;\nand Belongie, S. 2017a. Feature pyramid networks for ob-\nject detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2117–2125.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017b. Focal loss for dense object detection. InProceedings\nof the IEEE international conference on computer vision ,\n2980–2988.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, S.; Jia, J.; Fidler, S.; and Urtasun, R. 2017. Sgn: Se-\nquential grouping networks for instance segmentation. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 3496–3504.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\n3195\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-\nlutional networks for semantic segmentation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 3431–3440.\nMilletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-net:\nFully convolutional neural networks for volumetric medical\nimage segmentation. In 2016 fourth international confer-\nence on 3D vision (3DV), 565–571. IEEE.\nPeng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; and Zhou, X.\n2020. Deep snake for real-time instance segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 8533–8542.\nShen, X.; Yang, J.; Wei, C.; Deng, B.; Huang, J.; Hua, X.-S.;\nCheng, X.; and Liang, K. 2021. Dct-mask: Discrete cosine\ntransform mask representation for instance segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 8720–8729.\nShi, D.; Wei, X.; Yu, X.; Tan, W.; Ren, Y .; and Pu, S. 2021.\nInsPose: Instance-Aware Networks for Single-Stage Multi-\nPerson Pose Estimation. In Proceedings of the 29th ACM\nInternational Conference on Multimedia, 3079–3087.\nTian, Z.; Shen, C.; and Chen, H. 2020. Conditional convolu-\ntions for instance segmentation. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–\n28, 2020, Proceedings, Part I 16, 282–298. Springer.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n9627–9636.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efﬁcient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347–10357. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVu, T.; Kang, H.; and Yoo, C. D. 2021. SCNet: Training\nInference Sample Consistency for Instance Segmentation.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 35, 2701–2709.\nWang, S.; Gong, Y .; Xing, J.; Huang, L.; Huang, C.; and Hu,\nW. 2020a. Rdsnet: A new deep architecture forreciprocal ob-\nject detection and instance segmentation. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 34,\n12208–12215.\nWang, X.; Kong, T.; Shen, C.; Jiang, Y .; and Li, L. 2020b.\nSolo: Segmenting objects by locations. In European Con-\nference on Computer Vision, 649–665. Springer.\nWang, X.; Zhang, R.; Kong, T.; Li, L.; and Shen, C. 2020c.\nSOLOv2: Dynamic and Fast Instance Segmentation. In\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and\nLin, H., eds., Advances in Neural Information Processing\nSystems, volume 33, 17721–17732. Curran Associates, Inc.\nXie, E.; Sun, P.; Song, X.; Wang, W.; Liu, X.; Liang, D.;\nShen, C.; and Luo, P. 2020. Polarmask: Single shot instance\nsegmentation with polar representation. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 12193–12202.\nYang, B.; Bender, G.; Le, Q. V .; and Ngiam, J. 2019. Cond-\nconv: Conditionally parameterized convolutions for efﬁcient\ninference. arXiv preprint arXiv:1904.04971.\nZhang, G.; Lu, X.; Tan, J.; Li, J.; Zhang, Z.; Li, Q.; and Hu,\nX. 2021. ReﬁneMask: Towards High-Quality Instance Seg-\nmentation with Fine-Grained Features. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 6861–6869.\nZhang, R.; Tian, Z.; Shen, C.; You, M.; and Yan, Y . 2020.\nMask encoding for single shot instance segmentation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 10226–10235.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n6881–6890.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR 2021: The Ninth Interna-\ntional Conference on Learning Representations.\n3196"
}