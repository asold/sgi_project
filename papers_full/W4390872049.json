{
    "title": "A Character Based Steganography Using Masked Language Modeling",
    "url": "https://openalex.org/W4390872049",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5013321371",
            "name": "Emır Öztürk",
            "affiliations": [
                "Trakya University"
            ]
        },
        {
            "id": "https://openalex.org/A5065657432",
            "name": "Andaç Şahın Mesut",
            "affiliations": [
                "Trakya University"
            ]
        },
        {
            "id": "https://openalex.org/A5057403870",
            "name": "Özlem Aydın",
            "affiliations": [
                "Trakya University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385488789",
        "https://openalex.org/W2124890704",
        "https://openalex.org/W4226226016",
        "https://openalex.org/W2506763343",
        "https://openalex.org/W2911759304",
        "https://openalex.org/W2912015840",
        "https://openalex.org/W2159907644",
        "https://openalex.org/W2958084566",
        "https://openalex.org/W2951438437",
        "https://openalex.org/W2121337781",
        "https://openalex.org/W2076385485",
        "https://openalex.org/W2587083083",
        "https://openalex.org/W2578079928",
        "https://openalex.org/W2012940515",
        "https://openalex.org/W2157823837",
        "https://openalex.org/W2583194464",
        "https://openalex.org/W2942997395",
        "https://openalex.org/W2162174114",
        "https://openalex.org/W2549090310",
        "https://openalex.org/W1832326700",
        "https://openalex.org/W1505629914",
        "https://openalex.org/W2618218947",
        "https://openalex.org/W3088748086",
        "https://openalex.org/W2751202750",
        "https://openalex.org/W2963001579",
        "https://openalex.org/W4213325746",
        "https://openalex.org/W4384787020",
        "https://openalex.org/W6799370299",
        "https://openalex.org/W3158542464",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2893556909",
        "https://openalex.org/W2963444877",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2152167937",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287025617"
    ],
    "abstract": "In this study, a steganography method based on BERT transformer model is proposed for hiding text data in cover text. The aim is to hide information by replacing specific words within the text using BERT&#x2019;s masked language modeling (MLM) feature. In this study, two models, fine-tuned for English and Turkish, are utilized to perform steganography on texts belonging to these languages. Furthermore, the proposed method can work with any transformer model that supports masked language modeling. While traditionally the hidden information in text is often limited, the proposed method allows for a significant amount of data to be hidden in the text without distorting its meaning. In this study, the proposed method is tested by hiding stego texts of varying lengths in cover text of different lengths in two different language scenarios. The test results are analyzed in terms of perplexity, KL divergence and semantic similarity. Upon examining the results, the proposed method has achieved the best results compared to other methods found in the literature, with KL divergence of 7.93 and semantic similarity of 0.99. It can be observed that the proposed method has low detectability and demonstrates success in the data hiding process.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nA character based steganography using masked\nlanguage modeling\nEMİR ÖZTÜRK1, ANDAÇ ŞAHİN MESUT2, and ÖZLEM AYDIN FİDAN3\n1Computer Engineering Department, Trakya University, TURKEY (e-mail: emirozturk@trakya.edu.tr)\n2Computer Engineering Department, Trakya University, TURKEY (e-mail: andacs@trakya.edu.tr)\n3Computer Engineering Department, Trakya University, TURKEY (e-mail: ozlema@trakya.edu.tr)\nCorresponding author: EMİR ÖZTÜRK (e-mail: emirozturk@trakya.edu.tr).\nABSTRACT In this study, a steganography method based on BERT transformer model is proposed\nfor hiding text data in cover text. The aim is to hide information by replacing specific words within\nthe text using BERT’s masked language modeling (MLM) feature. In this study, two models, fine-\ntuned for English and Turkish, are utilized to perform steganography on texts belonging to these\nlanguages. Furthermore, the proposed method can work with any transformer model that supports\nmasked language modeling. While traditionally the hidden information in text is often limited, the\nproposed method allows for a significant amount of data to be hidden in the text without distorting\nits meaning. In this study, the proposed method is tested by hiding stego texts of varying lengths\nin cover text of different lengths in two different language scenarios. The test results are analyzed\nin terms of perplexity, KL divergence and semantic similarity. Upon examining the results, the\nproposed method has achieved the best results compared to other methods found in the literature,\nwith KL divergence of 7.93 and semantic similarity of 0.99. It can be observed that the proposed\nmethod has low detectability and demonstrates success in the data hiding process.\nINDEX TERMS BERT, masked language modeling, steganography\nI. INTRODUCTION\nI\nNFORMATION hiding is crucial for the security\nof computer systems. The communication between\nsender and receiver should not be accessible to a third\nparty. For this purpose, encryption and information hid-\ningmethodscanbeemployed.Whileencryptionprotects\ninformation, it is vulnerable to attacks since it reveals\nthe presence of the information [1], [2]. The goal of\nsteganography is to ensure that the hidden information\nremains unnoticed by a third party [3]. Steganography\nis a significant subfield within the field of information\nhiding [4].\nThe environment where the data is hidden is referred\nto as the cover media [5]. The cover media should be\none of the known and commonly used media [6]. Data\ncan be hidden on various types of cover media, such as\ntext, images, audio, and videos, as well as on geospatial\ndata [7] or visible wavelengths [8]. While videos and\nimagescanaccommodatealargeramountofhiddendata\ndue to their higher storage capacities, the same cannot\nbe said for text. Text-based steganography typically\nhas more limited capacity to hide data compared to\nmedia files like images and videos [9]. Consequently,\ndue to the lower probability of data being hidden on\ntext, performing information hiding on text aligns better\nwith the purpose of steganography. Furthermore, unlike\nother media, text possesses the robustness to facilitate\ndata transmission without undergoing distortion during\ntransmission.\nMany traditional text steganography methods per-\nformthehidingprocessbyalteringtheformatofthetext\n[10]. Format alterations include modifications in letter\nspacing, line breaks, font characteristics, and invisible\ncharacters. In [11], the size of the spaces between words\nin text images has been modified, enabling a hiding\nprocess. The hiding operation is executed by adding\nspaces between words, which are not visibly discernible.\nHowever, when texts are aligned and separated by lines,\ndifferences in spacing can be analyzed to extract hidden\ninformation.Similarly,in[12],watermarkbitsarehidden\nby altering the spacing between words and paragraphs\nin the context of watermarking. This approach involves\nmodifying the gaps between words and paragraphs to\nembed the watermark information effectively. In [13] a\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfont attribute called character spacing is used to embed\nthe secret. Finally, in [14], space characters from differ-\nent fonts are employed as individual encoding schemes.\nThe study highlights that using space characters from\ndifferent fonts is a secure hiding method since the font\ntypes do not introduce noticeable changes perceptible to\nthe human eye.\nIn recent years, introduced steganography applica-\ntions achieve hiding through symbol and syntax alter-\nations on the document [15].\nSome studies opt for using characters and words as\nsymbols instead of spaces for hiding bits. Thus, since\nthere are no structural changes made within the text, an\nobserver would not perceive that data is hidden unless\nthey possess the original text. Only someone who knows\nthe algorithm for word replacement would be able to\nextract the information.\nIn [16], a hiding process has been conducted by re-\nplacing words in a predetermined fixed text with altered\nwords that do not distort the meaning. Four words\nhave been selected at each replaceable point, allowing\nfor the storage of four values for each word without\ncompromising the meaning. In [17], the hiding process\ntakes place on words during the translation process.\nHiding operation is carried out by selecting words across\nmultiple translation choices.\nAt the core of the word substitution process lies the\nreplacement of specific words selected within the cover\ntext with the intention of concealing information and\nconveying it to the recipient. In this method, various\nfeatures of the substituted word, such as its list order,\nletters, length, etc., can be utilized.\nIn the process of word alteration, there are works\nthat perform information hiding by substituting similar\nwords based on Synonym Substitution [18]–[23]. Some\nstudies employ architectures like WordNet to assist in\nfinding synonyms [24]. The disadvantage of these meth-\nods is that the limited number of synonyms restricts\nthe potential alterations that can be made to the text,\nthereby limiting the amount of information that can be\nhidden.\nWith the rise in popularity of deep learning algo-\nrithms, many steganography methods utilizing genera-\ntive models such as Recurrent neural network (RNN)\nand Long short-term memory (LSTM) have been devel-\noped [10], [25], [26]. Additionally, there are numerous\ngenerative steganography methods employing Genera-\ntive Adversarial Networks (GAN) [27] and language\nmodels [28], [29]. The adoption of these techniques has\nbeenshowntoincreasetherateatwhichinformationcan\nbe hidden in comparison to classical text steganography\nmethods.\nTraditional text steganography methods require man-\nual preparation of the cover text. Moreover, to pre-\nvent the exposure of data, the frequency of changes\nin locations that will be altered should be kept as\nlow as possible. Additionally, hiding processes based\non spaces or word substitutions often occur at the bit\nlevel. This implies that the cover text, which needs\nto be prepared based on the amount of the data to\nbe hidden, must be considerably large. As the size of\nthe data increases, manually preparing such a cover\ntext becomes practically impossible. For example, in\na scenario where 2 bits are stored in each word and\nhiding is performed every 5 words, a text of 20 words\nis required to store only 1 byte. For hiding a message of\n50 characters, a text of 1000 words needs to be prepared.\nOne kilobyte of data consists of 1024 characters, making\nmanual text preparation quite challenging. In generative\nmodels, there is no limit to the words that can be\ngenerated. Text can be rapidly generated in the desired\nnumber of words based on the desired message size.\nMoreover, unlike manually prepared methods, sentences\nin the desired format can also be generated. With these\nfeatures, generative models offer much greater capacity\ncompared to manually prepared cover text.\nIn methods where words are replaced, in manually\nprepared sentences, determining the words to replace\nthe target word requires manual effort. This process\nalso has its limits. However, deep learning models can\nsuggest hundreds of words at once in place of a desired\nword without compromising the overall meaning. This\nallows for an increase in hiding capacity by multiplying\nthe number of different words, thereby increasing the\nnumber of bits hidden per word.\nGenerative methods aim to perform information hid-\ning on the text they generate, generating text and then\nembedding data within it [30]. The disadvantage of this\napproach is that hiding information on the generated\ntext can be statistically detectable [31]. The generated\ntext should closely resemble natural text and should\nnot give away the presence of hidden information. The\ngoal of these studies is to prevent the detectability of\nartificially generated data’s resemblance to natural text.\nConsequently, in this study, the intention is to perform\ninformationhidingthroughchangesmadetopre-existing\nwritten data. This approach aims to make the detection\nofhiddendatamorechallengingbymodifyingatextthat\nhas already been naturally written without changing its\nmeaning.\nTransformer models are used in various fields like clas-\nsification, masked language modeling, text generation,\nand question answering [32]. Numerous steganography\nstudies utilize this transformer architecture. Among\nthese, there are methods employing semantic and syn-\ntactic features for hiding, as well as those using embed-\nding vectors during translation [1].\nTransformers are bidirectional models. A bidirectional\nmodel is neural network architecture that processes\ninput data in both directions. With this, the model can\nlearn from information with past and future contexts.\nThe success of predictions is enhanced by learning this\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ncontext bidirectionally. In [33], it is mentioned that\nBERT, a transformer model with masked language mod-\neling support, outperforms other unidirectional methods\ndue to its bidirectionality.\nParticularly with masked language modeling, sugges-\ntions can be provided by the model for a masked word\nusing [MASK] tag within a given sentence. By utilizing\nthe predicting replacements, data hiding can be carried\nout. Numerous steganography applications are based on\nthe BERT (Bidirectional Encoder Representations from\nTransformers) which is developed by Google [34] with\nsome of them using text generation and others focusing\non the word alteration process like highlighted in this\nstudy [31].\nIn the process of hiding text, selecting a larger amount\nof cover text is advantageous for achieving higher se-\ncurity, as the hidden data will be distributed across\ndifferent areas of this text. Therefore, to maximize\nconfidentiality, it is necessary for the cover text to be as\nlarge as possible, while keeping the message itself short.\nMinimizing the proportion of hidden data in the overall\ntext is crucial for achieving optimal hiding operation.\nIn this study, two distinct BERT models are fine-\ntuned for both Turkish and English languages. These\nBERT models are then used for steganography utilizing\nthe masked language modeling function. In the next\nsection, the proposed method’s stages of information\nhiding and retrieval are explained. In the third section,\ninformation about the model’s training, the datasets\nused, and experimental results is provided. In the final\nsection, conclusions are discussed.\nII. PROPOSED MODEL\nModels used for masked language modeling are capable\nof predicting a desired number of words and it can be\nused to replace a masked word within a given sentence.\nThe process in proposed method involves using the\nletterswithinthewordsobtainedthroughthisprediction\nmechanismtoperformthehidingprocess.Insteganogra-\nphy applications, the method consists of two stages: hid-\ning and retrieval. In this study, two hiding mechanisms\nare employed — one for hiding the secret message and\nanother for hiding the necessary parameters, named as\nheadersfortherestofthearticle,toobtainthistext.The\ndetails of these mechanisms are provided respectively.\nIn the process of hiding the secret message within\nthe cover text, a word is used for each character.\nThese characters are replaced with characters from the\nwords predicted by the model. The alteration of the\nselected characters’ positions is essential to complicate\nthe retrieval of information. A loopIndex value is stored\nto handle this position-shifting process. Additionally, to\nselect the words for hiding in cover media from different\nindexes, a hash function is determined and employed.\nThe hash function is selected as squarehash for the\nimplementation for obtaining experimental results. Ad-\nditionally,toachievetherandomorderofselectingwords\nwithin the text, any desired hash function can be used.\nFortheinclusionofthepossibilityofchaninganywordin\ntext, the maximum value of the hash function should be\nthecountofthewordsinthecovertext.Forthisfunction\nto produce different values with each run, a seed value\nmust be specified. As this seed value is also required\nduring retrieval, it is one of the variables that need to\nbe stored initially. An example process of calculating\nloopIndex and hash values are given in Figure 1.\nIn Figure 1, an example is provided where the word\n\"Trim\" will be hidden to demonstrate where the spec-\nified variables are used. In this example, it is assumed\nthat predictions from the BERT model are obtained and\nwords are replaced. In the first stage, the loopIndex is\ninitialized as 0, and the word index as 1. Although the\nword index is given as 1 in the example, in the case\nwhere header information is included, this index will\nhave larger values. Since the loopIndex value is 0, a\nword with \"T\" at index 0 is desired, and \"The\" is found.\nAfter this stage, the loopIndex value is incremented and\nmodulo with the maximum length value is applied. For\nthis example, this value is chosen as 3. If a larger value is\nselected, longer words will be needed. For the success of\nthe hiding process, this value can be adjusted as desired.\nTo find the next word index, the current index and seed\nvaluearegiventothehashfunction,resultinginthenext\nword index. In the example, these values are obtained as\n10, 37 and 20, respectively. It’s important to note that\nthehashfunctiondoesnotalwayshavetoprovideindices\nin an ascending order, making it more challenging to\ndetect the hidden data. The alteration of word and letter\norder through the use of loopIndex and hash values is\nperformed to complicate the retrieval of data, aiming to\nenhance the difficulty in obtaining the information.\nThe seed value only affects the value generation of the\nhash function. Therefore, any desired integer value can\nbe chosen as seed value. In the experiments, a random\nseed value has also been selected, and the same seed\nvalue has been used for obtaining all the results. Lastly,\nsince the number of characters to be obtained needs to\nbe known, a charCount variable needs to be initially\nstored. A bit-based method similar to the mechanism in\n[16] is employed to store these three values.\nTo store the values, firstly, the values to be hidden\nare converted into a bit sequence and then divided into\nblocks of a predetermined size. Each block needs to be\nhidden over a word. In the hiding process, a transformer\nmodel is provided with a specific number of words,\nand the desired word to be predicted is replaced with\nthe [MASK] tag. In transformers, a specified number\nof predictions for the [MASK] can be made based on\nthe words in the sentence and they can be provided\nbefore or after the masked word. Therefore, the number\nof words given before and after the mask is referred to\nas the windowSize. As the transformer model will make\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1. Stages of obtaining wordIndexes and loopIndex values\npredictions for the masked word within a given window\nrelative to other words, performing only one masking\noperation within one window is essential for obtaining\nhidden data. Thus, the data hiding process starts from\nthe first index and advances by a full window size at\neach step to mask words at these indices, feed them to\nthetransformermodel,anddemandpredictionsthatwill\nstore the bit sequence. This value will be2n for storing\nn bits. Once the predictions are obtained, the word at\nthe index that corresponds to the value of 2n in the\ngenerated list will replace the masked word. The steps\nof the creation of the header are outlined in Figure 2.\nInFigure2,anexampleisprovidedforstoringthefirst\nblock of the given value 513 within the text. According\nto this example, the value 513 is converted to binary\nas 001000000001. Successively, the values 001, 000, 000,\n001 should be stored in the text. To store the value\n001 (1) in the first block, starting from the first index\nin the text, the [MASK] label is assigned instead of\nthe word \"I\" in the text. Subsequently, a prediction\nis requested from the BERT model. Among BERT’s\npredictions, the word \"you\", corresponding to the value\n001 or 1, is selected, and the word \"I\" is replaced with\nthis word. This process is carried out for all remaining\nblocks. After each masking operation, the window will\nbe advanced. This process is repeated sequentially for\ncharCount, seed, and loopIndex values.\nIn the ASCII table, there are a total of 62 characters\nfor letters and numbers, with 10 between 48 and 57, 26\nbetween 65 and 90, and 26 between 97 and 122. If any\nof the obtained binary values does not fall within these\nranges, the masking process cannot be performed as the\ntransformer model generates words in natural language.\nTherefore parameter values from the first stage are not\nhidden just as the letter-hiding method. In some cases,\nbyte values that will not appear in any word might be\nobtained. In such cases, hiding would not be possible,\nhence a different approach is taken in the first stage.\nIn the second stage, after the loopIndex, charCount\nand seed parameters are hidden, the secret message is\ndivided into letters. A hiding operation is performed for\neach letter, using the loopIndex value to determine the\nindex within the word where the hidden letter will be\nplaced. For each letter the index containing the word\nto be masked is obtained using the hash function. Hash\nfunction should generate indexes with non-overlapping\nwindows. The reason for this is explained in the previous\nhiding stage. Once this index is obtained, the word is\nmasked, and the transformer model is tasked with mak-\ning 257 predictions. This value is chosen as 1+256, with\nthe first word being used for a specific case named skip\nelement. The reason for choosing 256 is to increase the\nlikelihoodofobtainingtherequiredwordforthemethod.\nThe method has been designed to allow flexibility in\nchanging this value as needed.\nAfter making 257 predictions for the word, the process\ncontinues by seeking a word containing the letter to\nbe hidden at the loopIndex value. If this word is not\nfoundwithinthepredictionlistobtained,thewordatthe\nskip index, determined as 0th index, is used to indicate\nthat data cannot be hidden here. Subsequently, in the\nnext step, a new index value is determined using a seed\nand the hiding process continues. The steps of the data\nhiding process are given in Figure 3.\nIn Figure 3, an example is provided for hiding the first\nletter of the word \"secret\" in the second index. In this\nexample, the loopIndex value is initialized from 0. At\nthis stage, after creating a window containing the 2nd\nindex value, the word \"I\" at this index is replaced with\n[MASK], and predictions are obtained from the BERT\nmodel.Amongthepredictedwords,itischeckedwhether\nthereisawordwiththeletter\"s\"attheloopIndexindex.\nAt this point, in the 4th prediction the word \"she\" is\nfound, and the letter at loopIndex (0) is \"s\". Therefore\nthe word \"I\" is replaced with \"she\". If this condition was\nnot met, the word \"they\" at the 0th index is replaced\nwith \"I\", and the index is changed with hash function to\nhide the letter in the next position. After this operation,\nthe loopIndex value is updated. The pseudocode for\nthe information hiding stage is provided in Algorithm\n1 given below.\nIn the process of obtaining the information, firstly, the\nseed,loopIndex,andcharCountvalueshiddeninthefirst\nstage are obtained.\nThe process begins with the first index. The word at\nthe obtained index is replaced with the [MASK] tag\nand is then fed into the transformer model, including\nwords before and after it within a window of determined\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Stages of hiding header information in the cover text\nFIGURE 3. Hiding information in the cover text stage\nsize. The index at which the word in the prediction list\nmatches the given word is identified, and the binary\nvalue of this index is obtained. This process is carried\nout for all variables within all blocks, resulting in the\nretrieval of integer values for the header variables. The\nprocess of obtaining these values is given in Figure 4.\nIn Figure 4, an example is given for obtaining the first\nblock of the value 513 stored in Figure 2. A portion of\nthe hidden text, containing the first index and equal to\nthe window size, is taken, and after changing the word at\nfirst index with [MASK], it is given to the BERT model.\nSubsequently, predictions are requested from the BERT\nmodel. Since the BERT model will produce the same\nresults for the same window, the index corresponding\nto the hidden word \"you\" is obtained, which is 1. This\nindex is converted to binary (001) with the block size 3.\nThis process is repeated for all blocks, and these blocks\nare combined to obtain the binary value 001000000001\n(513). Subsequently, using these obtained values, the\nprocess of retrieving the hidden letters is carried out.\nFor retrieving hidden letters, the obtained seed value\nisused,andtheprocessofobtainingtheindicesiscarried\nout sequentially until the amount of data specified by\ncharCount is obtained. The words at these indices are\nfed into the transformer model with a window, as in\nthe previous stage. If the word at the index is equal\nto the 0th word predicted by the model, this index is\nskipped. The next index is obtained using the seed value.\nIf the read word is different from the 0th word, the\nletter corresponding to the loopIndex value in this word\nis read, and the hidden information is obtained in this\nmanner. Subsequently, the loopIndex value is updated.\nThis process continues until charCount is obtained. The\nsteps of this process are provided in Figure 5.\nFigure 5 illustrates the retrieval of the hidden letter\n\"s\" example in Figure 3. The window obtained using\nthe index value from the hash function, is given to the\nBERT model, and predictions are requested. Here, the\nword at the index is present in the 4th index of the\nBERT model’s predictions, therefore the letter \"s\" at\nthe loopIndex value is written to the output stream. In\nthe case where the searched word is at index 0, this\nword will be skipped, and the hash function will be\nused to move to the next index. After this stage, the\ncalculationofthenextloopIndexvalueisalsoperformed.\nThe pseudocode for the information retrieval stage is\nprovided in Algorithm 2.\nThe hiding and retrieving process of header and secret\nmessage onto the text is presented in as an overall\nschematic diagram in Figure 6.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4. Retrieving header information from the stego text\nFIGURE 5. Retrieving hidden information from the stego text\nAlgorithm 1 Information hiding stage\npredictionSize = 128\nGet seedPackets, wordCountPackets, and loopIndex-\nPackets from integer values\nWords = split(coverMedia,\" \")\nfor i= 0to len(packets) do\ngetPacket(p)\nWords[index] = “[MASK]”\npredictions = predict(Words [index : index +\nhalfWindowSize])\nWords[index] =predictions[p]\nend for\nwhile counter< wordCount do\nindex = hash(seed)\nWords[index] = “[MASK]”\npredictions = predict(Words [index −\nhalfWindowSize : index+ halfWindowSize])\nskip= predictions[0]\nwordList = getWords(loopIndex,predictions)\nif len(wordList) > 0 then\nwords[index] =wordList[0]\nelse\nwords[index] =skip\nend if\nend while\nreturn words as text\nAlgorithm 2 Retrieval of hidden text\npacketList = []\nfor i= 0to len(packets) do\nword = Words[index]\nWords[index] = “[MASK]”\npredictions = predict(Words [index : index +\nhalfWindowSize])\nwordIndex = predictions[word]\npacketList += toBinary(wordIndex)\nend for\noutputText = \"\"\nwhile counter< wordCount do\nindex = hash(seed)\nword = Words[index]\nWords[index] = “[MASK]”\npredictions = predict(Words [index −\nhalfWindowSize : index+ halfWindowSize])\nif W̸= predictions[256] then\noutputText+= word[loopIndex]\nend if\nUpdate loopIndex\nend while\nreturn outputText\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nIn Figure 6, the overall processes of hiding and\nretrieval are explained. In the first phase of hiding,\nthree parameters needed are hidden in words starting\nfrom the first index with a specified window size. The\nwords overlaid here are used for the information hiding\nprocess, and the suggestions from the BERT model for\nthe values to be hidden are written in place of the\nwords corresponding to the desired index. For the sake\nof simplifying the diagram, numerical values have been\nselectedtobestorableinasingleword.Ifthevaluestobe\nhidden exceed the block size, multiple words are used for\neach value. In the second stage, the letters of the hidden\ntext are successively hidden in the words at the 4th,\n7th, and 2nd indices obtained from the hash function.\nAt this stage, the words underlined are replaced with\nthe words suggested by the BERT model where the\ndesired letter is found in the loopIndex value. In the\nthird stage, to obtain data from the created stego text,\nthe words are read in the same order to retrieve the\nheader information. In the final stage, index values are\nobtained through a hash function until the number of\nhidden characters is reached. At each step, it is checked\nwhether the word at the index has a skip. If not, the\nletter in the calculated loopIndex value is obtained to\nobtain the hidden message.\nFor the method to work successfully, the BERT model\nused for masked language model should have a high\nprediction accuracy. Therefore, in the method, two\ndistinct models are fine-tuned and utilized for mask-\ning for Turkish and English languages. These models\nare named \"dbmdz/bert-base-turkish-cased\" and \"bert-\nbase-cased\", respectively. For the Turkish model, sen-\ntences from the Sabah newspaper in the SUDER [35]\ncorpus were used during the fine-tuning process. For the\nEnglish model, the English50mb file from the PizzaChili\ncorpus [36] is employed. The loss graphs obtained during\nthe fine-tuning process are presented in Figure 7 for\nEnglish (a) and Turkish (b).\nBoth models are trained for 100 epochs. The selected\nnumber of epochs aimed to achieve comparable and non-\noverfitting training and validation loss values for both\nmodels.Intraining,itisobservedthatthevalidationloss\nbegins to stabilize after 100 epochs. Beyond this point,\ncontinuing the training would result in the training loss\ndecreasing while the validation loss remains constant.\nSince this indicates the model may be overfitting, train-\ning is halted at this stage.\nIII. EXPERIMENTAL RESULTS\nTo obtain results, different cover texts and secret mes-\nsages needed to be prepared. For obtaining cover texts,\na plain text output containing Cumhuriyet newspaper\narticles from the SUDER corpus is obtained for Turkish\nlanguage. For English texts, the BookCorpus [37] is\nused. The SUDER corpus comprises 2.5 million distinct\nnews texts in the Turkish language. The BookCorpus\nencompasses 74 million lines of sentences in the English\nlanguage. Both datasets are acquired in JSON format\nand underwent preprocessing to transform them into\nraw text. To preserve the anonymity of selected data\nsamples, segments of predetermined sizes are randomly\nchosen from these sentences. To enable experimentation\nwith different scenarios, 10 pieces of 50, 100, and 250Kb\nsizes are prepared. This choice is made to align with\nreal-world scenarios. Preparing 10 instances of each size\naimed to demonstrate the behavior of measurement\nmetrics in various scenarios.\nThe length of the hidden message is determined as\n25%, 50% and 100% of the embedding capacity of\ncover texts. To obtain the total length of bytes for\nhidden messages, embedding capacities of cover texts\nare calculated.\nTo determine the embedding capacity, it is necessary\nto delve into how the method works. The method per-\nforms hiding on words, storing one character per word.\nThe number of words in the method is calculated by\ndividing the text based on spaces, excluding punctuation\nWCwords. The method’s capacity in terms of characters\nhidden (charCount), the seed determining where words\nwill be hidden, and the loopIndex value indicating the\nextent of character reordering within words are hidden\nas 32-bit integers. Given that the number of bits in each\nhidden block is denoted as n, the required number of\nblocks for each of these values is calculated as:\nb = ⌈32/n⌉. (1)\nwhere b is the block size. The same number of blocks\nwill be required for the three different values. One word\nis needed for each block. To accommodate the storage\nof these words, half of the designated window size is\nused (w/2), and to ensure that the transformer model’s\nprediction process for each word is not disrupted, the\noverlapping of these windows has been prevented. In\nthis case, considering that each of these 3 values is\nstored along with the window of each word, the required\nnumber of words can be calculated as follows:\nWCheader = 3× b × w/2. (2)\nInthenextstep,eachcharacterofthehiddentextwillbe\nstored in a word, and each word will require w number\nof words without overlapping windows. In this case, the\nembedding capacity value under the best circumstances\nwill be:\nECmax = (WCwords − WCheader)/w. (3)\nThereasonofthephrasementionedas\"underthebest\ncircumstances\" is that due to inappropriate predictions\nby the transformer model for certain words, the hiding\nprocess might not be possible. The obtained value is for\na hiding process with 0 skip operations.\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6. Hiding and retrieving data from text\nUsing the determined capacity values, stego texts are\ngenerated with a language model for half window sizes\nof 5, 10, and 15. The total byte values of the secret\nmessages are provided in Table 1. Test results for 3\ndifferent scenarios, each with 3 different sizes of text,\nhave been provided.\nImperceptibility measurements are conducted for per-\nforming tests and comparing the results with other\nmethods. Imperceptibility refers to the similarity be-\ntween the cover text and the stego text. Impercepti-\nbility detection can be achieved using measures such\nas perplexity (PPL), KL divergence (KLD), semantic\nsimilarity (SS) [31] and BLEU (bilingual evaluation\nunderstudy) score. Perplexity is the average occurrence\nprobability of each token in the stego text and it can be\ncalculated as:\nperplexity = 2−1\nn\nPn\ni=1 log2(P(xi|x<i)). (4)\nLow perplexity implies high imperceptibility. For ob-\ntaining the probability distributions of tokens in the\ntext and calculating their difference, KL divergence\nis employed. Let P and Q represent the probability\ndistributions of tokens in the original and hidden texts,\nrespectively. A lower KL divergence indicates a higher\nlevel of imperceptibility. The calculation of KL diver-\ngence is as follows:\nKLD(P||Q) =\nNX\ni=1\n[p(xi)logp(xi) − p(xi)logq(xi)]. (5)\nAnother comparative method that can be used is\nsemantic similarity. In this approach, the cosine dis-\ntance between the similarities of tokens in the cover\ntext and stego text is calculated. To measure semantic\nsimilarity, it’s necessary to detect the semantic dis-\ntance for the words. Various models can be used to\nachieve this. In this study, since the models used are\nbased on the BERT model, the sentence similarity mod-\nels efederici/sentence-bert-base and emrecan/bert-base-\nturkish-cased-mean-nli-stsb-tr, which are also BERT-\nbased sentence similarity models, have been utilized for\nsimilarity measurement.\nFinally, BLEU scores have also been calculated for\nmeasuringthesimilarityoftexts.WhileBLEUisprimar-\nily used to assess the performance of machine transla-\ntions, it can technically be utilized to measure semantic\nsimilarity, as it evaluates the semantic closeness of a\ngiven sentence in comparison to a reference sentence.\nTo make comparison with other studies, embedding\nrate (ER) calculation from [31] is used and embedding\nrate values for given calculation is obtained. The re-\nsulting ER, PPL, KLD, SIM, and BLEU values are\npresented in Table 2.\nIn order to ensure a valid comparison for the values\npresented in Table 2, the unchanged portion of the text\nhas not been considered. For instance, when a 25% data\nhiding process is applied, values were obtained based\non the section up to the last modified word. This way,\nthe intention was to prevent the remaining 75% of the\ntext from contributing to improvements in the results\nwithout any changes.\nAs observed in Table 2, as the half window size\ndecreases and more words are subjected to the hiding\nprocess, the ER value increases. Upon examining all\nconfigurations, there aren’t significant changes in PPL\nand KLD values. For Turkish, higher PPL values are\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Amount of hidden bytes for each cover text scenario\nCorpus Hidden Message Size Ratio Half Window Size\n5 10 15\nEnglish50Kb\n25% 225 110 75\n50% 400 200 130\n100% 800 400 260\nEnglish100Kb\n25% 375 200 125\n50% 750 375 250\n100% 1500 750 500\nEnglish250Kb\n25% 1100 550 375\n50% 2250 1100 750\n100% 4500 2200 1500\nTurkish50Kb\n25% 125 60 35\n50% 250 125 75\n100% 500 250 150\nTurkish100Kb\n25% 250 125 75\n50% 500 250 150\n100% 1000 500 300\nTurkish250Kb\n25% 700 250 225\n50% 1400 700 450\n100% 2800 1400 900\nTABLE 2. PPL. KLD. SIM and BLEU values for cover and stego texts\nCover Text\nSize\nHalf Window\nSize Ratio ER English Turkish\nPPL KLD SIM BLEU PPL KLD SIM BLEU\n50\n5\n100% 0.63 11.55 5.64 0.99 0.78 41.83 12.46 0.88 0.78\n50% 0.31 11.71 5.60 0.99 0.89 40.37 12.46 0.88 0.89\n25% 0.18 11.82 5.62 0.99 0.94 40.41 12.79 0.89 0.94\n10\n100% 0.31 11.48 3.31 1.00 0.89 40.09 10.64 0.96 0.89\n50% 0.16 11.71 3.25 1.00 0.94 39.73 10.64 0.96 0.94\n25% 0.09 11.81 3.53 1.00 0.97 39.79 10.64 0.96 0.97\n15\n100% 0.20 11.77 3.56 1.00 0.93 39.65 4.50 0.97 0.93\n50% 0.10 11.78 3.56 1.00 0.96 39.36 4.50 0.97 0.96\n25% 0.06 11.84 3.56 1.00 0.98 38.83 4.50 0.97 0.98\n100\n5\n100% 0.59 11.17 8.35 0.99 0.80 41.37 12.91 0.86 0.78\n50% 0.29 11.22 8.31 0.99 0.90 39.93 12.91 0.86 0.89\n25% 0.15 11.24 8.33 0.99 0.95 39.33 12.72 0.86 0.94\n10\n100% 0.29 11.21 7.28 0.99 0.90 39.98 10.03 0.93 0.89\n50% 0.15 11.29 7.28 0.99 0.95 39.39 10.03 0.93 0.94\n25% 0.08 11.33 7.28 0.99 0.97 39.01 9.15 0.93 0.97\n15\n100% 0.20 11.25 5.69 1.00 0.93 39.38 8.79 0.95 0.93\n50% 0.10 11.28 5.69 1.00 0.97 38.96 8.79 0.95 0.97\n25% 0.05 11.30 5.69 1.00 0.98 38.80 7.88 0.96 0.98\n250\n5\n100% 0.70 11.52 9.81 0.99 0.77 45.37 14.33 0.89 0.76\n50% 0.35 11.46 8.55 0.99 0.88 41.48 12.01 0.88 0.88\n25% 0.17 11.57 8.53 0.99 0.94 40.56 12.01 0.88 0.94\n10\n100% 0.35 11.43 6.14 1.00 0.89 41.36 10.81 0.93 0.88\n50% 0.17 11.50 5.61 1.00 0.94 40.66 9.81 0.94 0.94\n25% 0.09 11.54 5.61 1.00 0.97 40.09 9.81 0.94 0.98\n15\n100% 0.24 11.46 5.28 1.00 0.92 39.78 3.29 0.96 0.92\n50% 0.12 11.50 4.80 1.00 0.96 40.28 5.62 0.96 0.96\n25% 0.06 11.56 4.80 1.00 0.98 40.03 6.37 0.96 0.98\nobtained, and it is evident that changing selected words\nhave a more significant impact on imperceptebility. The\nreason some SIM values are 1 is due to providing results\nwith only 2 decimal points of sensitivity for English.\nSIM values are as high as 0.99 for English and 0.88\nfor Turkish. Finally, when analyzing the BLEU scores,\nas expected, the scores increase as the data hiding\nprocess decreases. Even in the worst case, the BLEU\nvalue has not fallen below the 0.78 mark. To facilitate\na comparison with results from other studies in the\nliterature and to evaluate the proposed method against\nthem, the obtained values are provided in Table 3.\nIn Table 3, the results from the article [31] are given\nunder the condition where each bit hidden per word is\n5. For the proposed method, the configuration with the\nclosest ER values was selected, and the results pertain-\ning to this configuration were provided. The proposed\nmethod hides a letter for each word, and technically, it\nhides 8 bits when other algorithms hide 5 bits per word.\nNevertheless, as evident in the table, it achieves the\nlowest KLD values and the highest SIM values compared\nto other methods. As for PPL, it appears to be quite\nsuccessful compared to other methods, excluding the\napproaches in [40] and [31].\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3. Imperceptibility results of different methods\nModel ER KLD PPL SIM\nProposed Method 0.64 7.93 11.44 0.99\nBlock [25] 0.89 48.46 234.74 0.64\nRNN-Stega [38] 0.78 15.71 48.37 0.66\nPatient-Huffman [39] 0.47 15.96 21.61 0.73\nTransformer [40] 0.8 15.15 8.94 0.58\nJoint Ling. Steg. [31] 0.74 14.61 6.04 0.82\n(a)\n(b)\nFIGURE 7. Training and validation loss values for English (a) and Turkish\n(b) models\nAdditionally, sentence examples demonstrating the\ndifference between the original and information-hidden\ndata are provided in Table 4 to illustrate this contrast.\nTable 4 highlights words in bold that store letters at\nthe loopIndex value. In the first example, the letter \"e\"\nin the word \"every\" within the cover text is already\npresent for loopIndex=2, requiring no modification. In\nthe second example, the word \"would\" has been replaced\nwith \"should\" to store the letter \"s\" at loopIndex=0.\nSimilar adjustments have been made in other examples\nto store the necessary letters. As evident from the\nexamples, these changes do not disrupt the sentence\nstructure. Additionally, situations where words can store\nletters without alteration contribute to obtaining better\nSIM and KLD values.\nIV. CONCLUSIONS\nSteganography is a significant field in ensuring infor-\nmation security by concealing the presence of data. It\nprimarily focuses on hiding the existence of information,\noften achieved at the bit level in many studies. Addi-\ntionally, methods exist that perform information hiding\nthrough text generation, where data is embedded during\nthe generation process.\nIn this study, a method is proposed that differs from\ngeneration-based approaches. Instead of generating new\ntext, the method utilizes an existing cover media and\nemploys a transformer architecture with a word predic-\ntion mechanism to achieve character-level information\nhiding.Unlikegenerativemethods,thisapproachutilizes\na pre-existing cover text, making the detection of hidden\ninformation more challenging compared to generative\ntechniques.\nMany existing methods in the literature work with\nword indices, while language models generate sugges-\ntions based on the probability distribution. As the index\nvalues increase, the probability of substituting lower-\nprobability words rises, potentially increasing the recov-\nerability of the hidden data. In the proposed method,\nthe use of a specific character at a certain index in\nthe searched word eliminates the significance of the\npredicted word’s index. Consequently, the first suitable\nword with the highest probability can be used for data\nhiding.\nThe success of the method depends on the language\nmodel’s ability to make accurate predictions or predict\ndifferent numbers of words. Therefore, employing better-\ntrained or larger models in the method could result in\nfewer skipping processes during data hiding, enhancing\nthe method’s performance. Since the method utilizes the\nmasked language modeling feature of models, it can be\neasily adapted to different languages and models.\nThe method achieves a high embedding capacity by\nstoring one character per word. While reducing the\nwindow size could increase this capacity, it might dimin-\nish the language model’s capability to make accurate\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. Replaced words in cover text for English and Turkish languages\nEnglish\n...and every moment that ticked by they spent away from callie made it less likely...\n...and every moment that ticked by they spent away from callie made it less likely...\n...how would anyone ever know if they were being assisted from the other side?...\n...how should anyone ever know if they were being assisted from the other side?....\n...and yet he didthis for her . and alu didn’t...\n...and yet he triedthis for her . and alu didn’t...\n...you can have oneof these’ she said , offering her packet...\n...you can have fiveof these’ she said , offering her packet...\n...ill be knocking on your doorstep , freezing myselfto death soon , said heston...\n...ill be knocking on your doorstep , freezing yourselfto death soon , said heston...\nTurkish\n...Uçuşlar İstanbul-Sao Paulo-İstanbul hattının Buenos Aires’e uzatılması ilegerçekleştirilecek...\n...Uçuşlar İstanbul-Sao Paulo-İstanbul hattının Buenos Aires’e uzatılması istemiylegerçekleştirilecek...\n...3 ay önce dünyaya getirdiği bebeği Batuhan Alan’ın öldürülmesi olayıyla ilgili takibinisürdüren polis,...\n...3 ay önce dünyaya getirdiği bebeği Batuhan Alan’ın öldürülmesi olayıyla ilgili işlemlerisürdüren polis,...\n...İkincisi, yerelyönetim gerçeği! 1 milyona yaklaşan...\n...İkincisi, doğruyönetim gerçeği! 1 milyona yaklaşan...\n...yapılan analizlerde, tanınmışhemen hemen tüm firmaların sularında...\n...yapılan analizlerde, incelemelerdehemen hemen tüm firmaların sularında...\n...Önümüzdeki yıllarda, 4-7 yıl sonrane olabileceğini tahmin ediyoruz...\n...Önümüzdeki yıllarda, 4-7 yıl neticesindene olabileceğini tahmin ediyoruz...\nword predictions, thereby increasing the likelihood of\nrevealing the presence of hidden data in stego text.\nUpon examining the results and comparing them\nwith other methods, it’s evident that by achieving close\nER values to other methods and low imperceptibility,\nthe hiding process can be performed effectively. As a\nresult, this approach generates more successful outcomes\ncompared to other methods.\nThe proposed method demonstrates from the\ntest results that the difference between stego text\nand cover text is minimal, and the distinguisha-\nbility of changed words is low. This indicates\nthe success of the method. Lastly in the link\nhttps://anonymous.4open.science/r/MLMCharStego-4F69,\nthe developed method and models can be accessed.\nThe proposed model is suitable for hiding text within\ntext, as it performs hiding based on the letter values\nand can hide information in sections corresponding\nto alphanumeric characters in the ASCII table. The\ndisadvantage of this approach is its unsuitability for\nhiding binary data such as images, audio, or video. In\nfuture studies, training models compatible with Unicode\ncharacters and expanding the character set will open the\npossibility of storing specific data with preprocessing\nand transforming operations. The aim is to carry out\nthis process by mapping binary data to values in the\ncharacter set.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nREFERENCES\n[1] T. Lu, G. Liu, R. Zhang, P. Li, and T. Ju, “Robust secret data\nhiding for transformer-based neural machine translation,”\nin 2023 International Joint Conference on Neural Networks\n(IJCNN). IEEE, 2023, pp. 1–8.\n[2] F. A. Petitcolas, R. J. Anderson, and M. G. Kuhn, “Informa-\ntion hiding-a survey,” Proceedings of the IEEE, vol. 87, no. 7,\npp. 1062–1078, 1999.\n[3] L. Xiang, R. Wang, Z. Yang, and Y. Liu, “Generative linguis-\ntic steganography: A comprehensive review.” KSII Transac-\ntions on Internet & Information Systems, vol. 16, no. 3, 2022.\n[4] B. Pfitzmann, “Information hiding terminology,” Lecture\nNotes in Computer Science, vol. 1174, pp. 347–350, 1996.\n[5] M. Kharrazi, H. T. Sencar, and N. Memon, “Image steganog-\nraphy: Concepts and practice,” Lecture Note Series, Institute\nfor Mathematical Sciences, National University of Singapore,\n2004.\n[6] J. Wen, X. Zhou, P. Zhong, and Y. Xue, “Convolutional neu-\nral network based text steganalysis,” IEEE Signal Processing\nLetters, vol. 26, no. 3, pp. 460–464, 2019.\n[7] Ö.KurtulduandM.Demirci,“Stegogis:Anewsteganography\nmethod using the geospatial domain,” Turkish Journal of\nElectrical Engineering and Computer Sciences, vol. 27, no. 1,\npp. 532–546, 2019.\n[8] İ. COŞKUN, F. Akar, and Ö. ÇETİN, “A new digital image\nsteganography algorithm based on visible wavelength,” Turk-\nish Journal of Electrical Engineering and Computer Sciences,\nvol. 21, no. 2, pp. 548–564, 2013.\n[9] K. Wang and Q. Gao, “A coverless plain text steganography\nbased on character features,” IEEE Access, vol. 7, pp. 95665–\n95676, 2019.\n[10] Y. Tong, Y. Liu, J. Wang, and G. Xin, “Text steganogra-\nphy on rnn-generated lyrics,” Mathematical Biosciences and\nEngineering, vol. 16, no. 5, pp. 5451–5463, 2019.\n[11] Y.-W. Kim, K.-A. Moon, and I.-S. Oh, “A text watermarking\nalgorithm based on word classification and inter-word space\nstatistics.” in ICDAR. Citeseer, 2003, pp. 775–779.\n[12] A. M. Alattar and O. M. Alattar, “Watermarking electronic\ntext documents containing justified paragraphs and irregular\nline spacing,” in Security, Steganography, and Watermarking\nof Multimedia Contents VI, vol. 5306. SPIE, 2004, pp. 685–\n695.\n[13] B. K. Ramakrishnan, P. K. Thandra, and A. S. M. Sriniva-\nsula, “Text steganography: a novel character-level embedding\nalgorithm using font attribute,” Security and Communication\nNetworks, vol. 9, no. 18, pp. 6066–6079, 2016.\n[14] R. Kumar, A. Malik, S. Singh, B. Kumar, and S. Chand,\n“A space based reversible high capacity text steganography\nscheme using font type and style,” in 2016 International\nConference on Computing, Communication and Automation\n(ICCCA). IEEE, 2016, pp. 1090–1094.\n[15] M.-Y. Kim, O. R. Zaiane, and R. Goebel, “Natural language\nwatermarking based on syntactic displacement and mor-\nphological division,” in 2010 IEEE 34th Annual Computer\nSoftware and Applications Conference Workshops. IEEE,\n2010, pp. 164–169.\n[16] A. Ş. MESUT, Ö. AYDIN, and E. ÖZTÜRK, “Anlamsal\nyöntemler kullanan bir metin steganografi uygulaması,” in 1st\nInternational Symposium on Digital Forensics and Security\n(ISDFS13), 2013.\n[17] R. Stutsman, C. Grothoff, M. Atallah, and K. Grothoff, “Lost\nin just the translation,” in Proceedings of the 2006 ACM\nsymposium on Applied computing, 2006, pp. 338–345.\n[18] L. Xiang, X. Wang, C. Yang, and P. Liu, “A novel linguistic\nsteganography based on synonym run-length encoding,” IE-\nICE transactions on Information and Systems, vol. 100, no. 2,\npp. 313–322, 2017.\n[19] L. Xiang, Y. Li, W. Hao, P. Yang, and X. Shen, “Reversible\nnatural language watermarking using synonym substitution\nand arithmetic coding.” Computers, Materials & Continua,\nvol. 55, no. 3, 2018.\n[20] X. Yang, F. Li, and L. Xiang, “Synonym substitution-based\nsteganographic algorithm with matrix coding,” Chinese Com-\nput. Syst, vol. 36, pp. 1296–1300, 2015.\n[21] C.-Y.ChangandS.Clark,“Practicallinguisticsteganography\nusing contextual synonym substitution and a novel vertex\ncoding method,” Computational linguistics, vol. 40, no. 2,\npp. 403–448, 2014.\n[22] C. Qi, S. Xingming, and X. Lingyun, “A secure text steganog-\nraphy based on synonym substitution,” in IEEE Conference\nAnthology. IEEE, 2013, pp. 1–3.\n[23] I. A. Bolshakov, “A method of linguistic steganography based\non collocationally-verified synonymy,” in International Work-\nshop on Information Hiding. Springer, 2004, pp. 180–191.\n[24] I. A. Bolshakov and A. Gelbukh, “Synonymous paraphrasing\nusing wordnet and internet,” in International conference\non application of natural language to information systems.\nSpringer, 2004, pp. 312–323.\n[25] T. Fang, M. Jaggi, and K. Argyraki, “Generating stegano-\ngraphic text with lstms,” arXiv preprint arXiv:1705.10742,\n2017.\n[26] H. Kang, H. Wu, and X. Zhang, “Generative text steganog-\nraphy based on lstm network and attention mechanism with\nkeywords,” Electronic Imaging, vol. 2020, no. 4, pp. 291–1,\n2020.\n[27] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative\nadversarial nets,” Advances in neural information processing\nsystems, vol. 27, 2014.\n[28] W. Tang, S. Tan, B. Li, and J. Huang, “Automatic stegano-\ngraphic distortion learning using a generative adversarial\nnetwork,” IEEE Signal Processing Letters, vol. 24, no. 10,\npp. 1547–1551, 2017.\n[29] S. Dong, R. Zhang, and J. Liu, “Invisible steganogra-\nphy via generative adversarial network,” arXiv preprint\narXiv:1807.08571, vol. 4, p. 3, 2018.\n[30] B. Yi, H. Wu, G. Feng, and X. Zhang, “Alisa: Acrostic\nlinguistic steganography based on bert and gibbs sampling,”\nIEEE Signal Processing Letters, vol. 29, pp. 687–691, 2022.\n[31] C.Ding,Z.Fu,Q.Yu,F.Wang,andX.Chen,“Jointlinguistic\nsteganography with bert masked language model and graph\nattention network,” IEEE Transactions on Cognitive and\nDevelopmental Systems, 2023.\n[32] K. S. Kalyan, A. Rajasekharan, and S. Sangeetha, “Ammus:\nA survey of transformer-based pretrained models in natural\nlanguage processing,” arXiv preprint arXiv:2108.05542, 2021.\n[33] M. Shah Jahan, H. U. Khan, S. Akbar, M. Umar Farooq,\nS. Gul, and A. Amjad, “Bidirectional language modeling: A\nsystematic literature review,” Scientific Programming, vol.\n2021, pp. 1–15, 2021.\n[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[35] S. U. D. A. Research and A. C. (VERIM)\", “Suder cor-\npus - turkish news collections for text categorization,”\nhttps://github.com/suverim/suder, 2018.\n[36] P. Ferragina and G. Navarro, “Pizza & chili corpus,” 2007.\n[37] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,\nA. Torralba, and S. Fidler, “Aligning books and movies:\nTowards story-like visual explanations by watching movies\nand reading books,” in The IEEE International Conference\non Computer Vision (ICCV), December 2015.\n[38] Z.-L. Yang, X.-Q. Guo, Z.-M. Chen, Y.-F. Huang, and Y.-\nJ. Zhang, “Rnn-stega: Linguistic steganography based on re-\ncurrent neural networks,” IEEE Transactions on Information\nForensics and Security, vol. 14, no. 5, pp. 1280–1295, May\n2019.\n[39] F. Z. Dai and Z. Cai, “Towards near-imperceptible stegano-\ngraphic text,” 2019.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” 2023.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nEMİR ÖZTÜRKreceived his B.S. degree from\nTrakya University as top student of the\nfaculty in 2010, M.S. degree from Trakya\nUniversity in 2012 and Ph.D. degree from\nYıldız Technical University in 2018 in com-\nputer science.\nHe was a research assistant at the Com-\nputer Engineering Department of Trakya\nUniversity between 2010 and 2021. He\nstarted working as an assistant professor\nin this department in 2021. His research interests are data\ncompression, deep learning algorithms, information security and\nprogramming languages.\nANDAÇ ŞAHİN MESUTreceived the B.S. and\nM. S. degree in Computer Engineering from\nTrakya University Edirne, Turkey, and re-\nceived her Ph. D. in Computer Engineering\nfrom Trakya University in 2007.\nFrom 1998 to 2007, she was a Research\nAsistant at Computer Engineering Depar-\nment in Trakya University. She has been\nworking as an Assistant Professor in same\ndeparment since 2007. Her research inter-\nests include some application areas in cryptography and steganog-\nraphy.\nÖZLEM AYDIN FİDANreceived the B.S. and\nM. S. degree in Computer Engineering from\nTrakya University Edirne, Turkey, and re-\nceived her Ph. D. in Computer Engineering\nfrom Trakya University in 2011.\nFrom 2001 to 2011, she was a Research\nAsistant at Computer Engineering Depar-\nment in Trakya University. She has been\nworking as an Assistant Professor in same\ndeparment since 2011. Her research inter-\nests include some application areas in Natural Language Process-\ning and Text Steganography.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354710\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}