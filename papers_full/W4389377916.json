{
  "title": "OpenDeID Pipeline for Unstructured Electronic Health Record Text Notes Based on Rules and Transformers: Deidentification Algorithm Development and Validation Study",
  "url": "https://openalex.org/W4389377916",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2124898164",
      "name": "Jiaxing Liu",
      "affiliations": [
        "Zhongnan University of Economics and Law"
      ]
    },
    {
      "id": "https://openalex.org/A2118593979",
      "name": "Shalini Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2973995732",
      "name": "Aipeng Chen",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2267874467",
      "name": "Chen Kai-wang",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2181469861",
      "name": "Pratik Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2613763800",
      "name": "Hong-Jie Dai",
      "affiliations": [
        "Kaohsiung Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A4226849657",
      "name": "Zoie Shui-Yee Wong",
      "affiliations": [
        "St. Luke's International University",
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2132490811",
      "name": "Jitendra Jonnagaddala",
      "affiliations": [
        "UNSW Sydney",
        "Shaikh Khalifa Medical City"
      ]
    },
    {
      "id": "https://openalex.org/A2124898164",
      "name": "Jiaxing Liu",
      "affiliations": [
        "Zhongnan University of Economics and Law"
      ]
    },
    {
      "id": "https://openalex.org/A2118593979",
      "name": "Shalini Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2973995732",
      "name": "Aipeng Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2267874467",
      "name": "Chen Kai-wang",
      "affiliations": [
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A2181469861",
      "name": "Pratik Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2613763800",
      "name": "Hong-Jie Dai",
      "affiliations": [
        "Kaohsiung Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A4226849657",
      "name": "Zoie Shui-Yee Wong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132490811",
      "name": "Jitendra Jonnagaddala",
      "affiliations": [
        "UNSW Sydney",
        "Shaikh Khalifa Medical City"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2388649072",
    "https://openalex.org/W2015620729",
    "https://openalex.org/W2115792042",
    "https://openalex.org/W2133362931",
    "https://openalex.org/W1995228216",
    "https://openalex.org/W3012211566",
    "https://openalex.org/W2111063183",
    "https://openalex.org/W4300035993",
    "https://openalex.org/W3017330637",
    "https://openalex.org/W1169821766",
    "https://openalex.org/W3095788424",
    "https://openalex.org/W2895223087",
    "https://openalex.org/W2993961432",
    "https://openalex.org/W1262131959",
    "https://openalex.org/W4300167402",
    "https://openalex.org/W2158645702",
    "https://openalex.org/W3209006175",
    "https://openalex.org/W2622474949",
    "https://openalex.org/W2791930518",
    "https://openalex.org/W2621075239",
    "https://openalex.org/W3030645549",
    "https://openalex.org/W3160540087",
    "https://openalex.org/W1599682086",
    "https://openalex.org/W2974383165",
    "https://openalex.org/W3204796902",
    "https://openalex.org/W3186656778",
    "https://openalex.org/W2624380710",
    "https://openalex.org/W2116093010",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2159640576",
    "https://openalex.org/W4281689302",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3127869043",
    "https://openalex.org/W1034374084",
    "https://openalex.org/W2160987310",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2604272474",
    "https://openalex.org/W2480360071"
  ],
  "abstract": "Background Electronic health records (EHRs) in unstructured formats are valuable sources of information for research in both the clinical and biomedical domains. However, before such records can be used for research purposes, sensitive health information (SHI) must be removed in several cases to protect patient privacy. Rule-based and machine learning–based methods have been shown to be effective in deidentification. However, very few studies investigated the combination of transformer-based language models and rules. Objective The objective of this study is to develop a hybrid deidentification pipeline for Australian EHR text notes using rules and transformers. The study also aims to investigate the impact of pretrained word embedding and transformer-based language models. Methods In this study, we present a hybrid deidentification pipeline called OpenDeID, which is developed using an Australian multicenter EHR-based corpus called OpenDeID Corpus. The OpenDeID corpus consists of 2100 pathology reports with 38,414 SHI entities from 1833 patients. The OpenDeID pipeline incorporates a hybrid approach of associative rules, supervised deep learning, and pretrained language models. Results The OpenDeID achieved a best F1-score of 0.9659 by fine-tuning the Discharge Summary BioBERT model and incorporating various preprocessing and postprocessing rules. The OpenDeID pipeline has been deployed at a large tertiary teaching hospital and has processed over 8000 unstructured EHR text notes in real time. Conclusions The OpenDeID pipeline is a hybrid deidentification pipeline to deidentify SHI entities in unstructured EHR text notes. The pipeline has been evaluated on a large multicenter corpus. External validation will be undertaken as part of our future work to evaluate the effectiveness of the OpenDeID pipeline.",
  "full_text": "Original Paper\nOpenDeID Pipeline for Unstructured Electronic Health Record\nText Notes Based on Rules and Transformers: Deidentification\nAlgorithm Development and Validation Study\nJiaxing Liu1*, PhD; Shalini Gupta2*, MSc; Aipeng Chen3, MSc; Chen-Kai Wang4, MSc; Pratik Mishra2, MSc; Hong-Jie\nDai5, PhD; Zoie Shui-Yee Wong6,7*, PhD; Jitendra Jonnagaddala8,9*, MD, MIS, PhD\n1School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan, China\n2CGD Health Pty Ltd, Canberra, Australia\n3School of Computer Science and Engineering, UNSW, Sydney, Australia\n4Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan\n5School of Post-Baccalaureate Medicine, Kaohsiung Medical University, Kaohsiung, Taiwan\n6Graduate School of Public Health, St. Luke’s International University, Tokyo, Japan\n7The Kirby Institute, University of New South Wales, Sydney, Australia\n8School of Population Health, UNSW Sydney, Kensington, Australia\n9NMC Royal Hospital, Khalifa City, Abu Dhabi, United Arab Emirates\n*these authors contributed equally\nCorresponding Author:\nJitendra Jonnagaddala, MD, MIS, PhD\nSchool of Population Health\nUNSW Sydney\nF25 Samuels Building\nSamuel Terry Ave\nKensington, NSW, 2033\nAustralia\nPhone: 61 (02) 9385 2517\nEmail: z3339253@unsw.edu.au\nAbstract\nBackground: Electronic health records (EHRs) in unstructured formats are valuable sources of information for research in both\nthe clinical and biomedical domains. However, before such records can be used for research purposes, sensitive health information\n(SHI) must be removed in several cases to protect patient privacy. Rule-based and machine learning–based methods have been\nshown to be effective in deidentification. However, very few studies investigated the combination of transformer-based language\nmodels and rules.\nObjective: The objective of this study is to develop a hybrid deidentification pipeline for Australian EHR text notes using rules\nand transformers. The study also aims to investigate the impact of pretrained word embedding and transformer-based language\nmodels.\nMethods: In this study, we present a hybrid deidentification pipeline called OpenDeID, which is developed using an Australian\nmulticenter EHR-based corpus called OpenDeID Corpus. The OpenDeID corpus consists of 2100 pathology reports with 38,414\nSHI entities from 1833 patients. The OpenDeID pipeline incorporates a hybrid approach of associative rules, supervised deep\nlearning, and pretrained language models.\nResults: The OpenDeID achieved a best F1-score of 0.9659 by fine-tuning the Discharge Summary BioBERT model and\nincorporating various preprocessing and postprocessing rules. The OpenDeID pipeline has been deployed at a large tertiary\nteaching hospital and has processed over 8000 unstructured EHR text notes in real time.\nConclusions: The OpenDeID pipeline is a hybrid deidentification pipeline to deidentify SHI entities in unstructured EHR text\nnotes. The pipeline has been evaluated on a large multicenter corpus. External validation will be undertaken as part of our future\nwork to evaluate the effectiveness of the OpenDeID pipeline.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 1https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n(J Med Internet Res 2023;25:e48145) doi: 10.2196/48145\nKEYWORDS\ndeidentification; scrubbing; anonymization; surrogate generation; unstructured EHRs; electronic health records; BERT; Bidirectional\nEncoder Representations from Transformers\nIntroduction\nBackground and Significance\nElectronic health records (EHRs) have become a valuable source\nfor observational research, owing to the accessibility and\navailability of patient-level data. EHRs contain clinical, imaging,\nand omics data from patients, enabling clinicians and researchers\nto generate novel evidence [1]. The EHR data are stored in\nstructured, semistructured, and unstructured formats. Using\nunstructured data for research purposes is challenging [2].\nUnstructured data, including structured or semistructured data,\nmust be deidentified in several scenarios before they can be\nused for research. Scenarios include where informed consent is\nnot possible to share identified information. As part of the\ndeidentification process, sensitive health information (SHI)\nmust be removed or replaced by surrogates to protect patient\nprivacy [3,4]. The Australian government, similar to the United\nStates with the Health Insurance Portability and Accountability\nAct (HIPAA) [5], defines several categories of SHI [4]. These\ninclude but are not limited to names, dates, any unique\nidentifying numbers or codes, and geographical data, which\nshould be excluded or replaced from EHR text notes prior to\nsecondary use for research purposes. The goal of the\ndeidentification process in unstructured EHR text notes is to\nidentify SHI by inspecting entire medical records.\nDeidentification by medical experts is time-consuming, error\nprone, and expensive [6]. In contrast, automated deidentification\ntechniques based on recent advances in artificial intelligence\ncan be used to simplify the entire process [7]. Automated\ndeidentification techniques require an annotated corpus to\nidentify SHI [8].\nOver the past 2 decades, several deidentification methods have\nbeen developed to identify the SHI in unstructured EHR text\nnotes. Three methods are widely used in the deidentification\nprocess, which are rule-based, machine learning–based, and\nhybrid techniques. Early methods were rule based and required\nmedical professionals to manually develop the rules for most\nof the time. Researchers from the University of Pittsburgh\nMedical Center used a rule-based approach to identify and\nreplace SHI while preserving the underlying medical information\nin pathology reports [9]. Sweeney [10] proposed another\nrule-based approach for identifying and replacing\nSHI.MIT-De-id (PhysioNet) software, which is also a rule-based\ndeidentification system developed using nursing progress reports\n[11]. Zhao et al [12] developed a rule-based model and\nintegrated it into an ensemble framework and achieved the best\nperformance of the model when compared with a non–rule-based\nmodel. Similarly, Dehghan et al [13] developed cDeID software\nto deidentify 7 HIPAA categories. Philter is another\ndeidentification system based on rules incorporating name lists\nand statistical natural language processing (NLP) to remove\nSHI from EHR text notes [14].\nIn recent years, with advancements in artificial intelligence,\nresearchers have proposed traditional machine learning–based\napproaches and deep learning–based approaches. For example,\nHe et al [15] used conditional random fields (CRFs) with a large\nnumber of lexical, orthographic, and dictionary features. In\nanother study, researchers proposed a self-attention mechanism\nusing stacked recurrent neural networks [16]. Contextualized\nword embeddings and pretrained word embeddings have been\nexamined with bidirectional long short-term memory networks\n(Bi-LSTM) or Bi-LSTM-CRF in various studies [17-19].\nAnother recent study proposed an ensemble-based framework\nto deidentify 5 SHI categories, including person, address, date\nof birth, identifiers, and phone numbers using Australian hospital\ndischarge summaries [20].\nHybrid approaches have also been proposed to combine\nrule-based and machine learning–based methods to achieve\nbetter performance. For example, Ferrández et al [21] proposed\na stepwise hybrid approach, which used CRF, rules, and a\ndictionary first and then support vector machine. Lee et al [22]\npresented a hybrid approach using CRF and rules for the\nautomatic deidentification of psychiatric notes. Zhao et al [23]\nproposed a hybrid approach using recurrent neural networks\nand text templates. Liu et al [24] developed a hybrid method\nbased on a rules-based method and an ensemble classifier of\nCRF and Bi-LSTM.\nHowever, there are several limitations to the existing\napproaches. These methods lack robustness and require\ncomprehensive modifications of rules to achieve better accuracy\nwhen applied to EHR text notes data from different health care\nsettings. Many existing deidentification studies have focused\neither on using rule-based approaches or hybrid approaches\nbased on CRF or Bi-LSTM and rules to identify SHI categories\n[21-24]. Only a few studies have examined the potential of\nhybrid approaches that incorporate rules, state-of-the-art\npretrained language models, and deep learning [25].\nFurthermore, in most of these approaches, the models are trained\non a corpus developed from a single center or a corpus prepared\nfrom a cohort of patients with a specific disease. As such, there\nis little evidence on the implementation of these approaches in\nthe real-time processing of EHR text notes deidentification.\nObjective\nThis study proposes an end-to-end deidentification pipeline\ncalled OpenDeID to deidentify real-world Australian\nunstructured EHR text notes data. The OpenDeID pipeline\nincorporates a hybrid approach of associative rules, supervised\ndeep learning, and pretrained language models. The\nbest-performing run of OpenDeID was used at a large tertiary\nteaching hospital in Australia in 2019, deidentifying EHR text\nnotes associated with biobanking in real time. Moreover, the\nOpenDeID pipeline can also generate realistic surrogates to\nsafeguard SHI [26]. As of 2022, this deployment of the\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 2https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nOpenDeID pipeline has processed more than 8000 reports in\nreal time.\nMethods\nOpenDeID Corpus\nAn OpenDeID corpus was used to develop the OpenDeID\npipeline. The OpenDeID corpus comprised 2100 pathology\nreports from 1833 patients with 38,414 SHI entities. The\ninterannotator agreement, measured using Cohen κ, was 0.9464.\nThe corpus was created using 2 annotators in 3 different\nexperimental settings. Table 1 lists the 8 SHI categories and\ntheir corresponding subcategories annotated in the corpus. The\ncorpus was randomly divided into 3 equal sets: training,\nvalidation, and test sets based on the reports. The training set\nwas used for the initial training and validation sets to tune the\nhyperparameters and early stopping. The models were then\ntrained on the combined training and validation sets, followed\nby evaluation on the test set. The corpus characteristics and\ncorpus development process are described in detail in [27,28].\nThe OpenDeID corpus was developed using annotation\nguidelines developed by Stubbs et al [29]. The corpus is\navailable at the OpenDeID-Corpus GitHub repository [30].\nTable 1. SHIa categories and subcategories for deidentification annotation.\nSHI subcategorySHI category\nPATIENT, DOCTOR, USERNAMEName\nNo subcategoriesProfession\nHOSPITAL, ORGANIZATION, STREET, CITY, STATE, COUNTRY, ZIP CODE, OTHERLocation\nNo subcategoriesAge\nNo subcategoriesDate\nPHONE, FAX, EMAIL, URL, IP ADDRESSContact\nSSN, MEDICAL RECORD, HEALTH PLAN, ACCOUNT, LICENSE, VEHICLE, DEVICE, BIOID, IDNUMID\nNo subcategoriesOther\naSHI: sensitive health information.\nOpenDeID\nOverview\nOpenDeID uses a step-by-step pipeline approach for\ndeidentifying sensitive information. The first step is\npreprocessing, where pathology reports in the XML format are\ncleaned using NLP methods such as regular expression rules,\nsentence segmentation, and tokenization. The next step is to\nbuild deidentification models. We used two methods: (1) training\na neural network model with word embeddings and (2)\nfine-tuning pretrained Bidirectional Encoder Representations\nfrom Transformers (BERT)–base models using transfer learning.\nThe methods were compared and evaluated in terms of their\naccuracy and performance. The deidentified reports were further\nprocessed to generate an output with the appropriate SHI entity\ntags. Figure 1 shows a conceptual overview of the OpenDeID\npipeline. The OpenDeID pipeline is available at the\nOpenDeID-Pipeline GitHub repository [31]. The main steps are\nelaborated in the following subsections.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 3https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nFigure 1. Conceptual overview of the pipeline of OpenDeID. BERT: Bidirectional Encoder Representations from Transformers; LSTM: long short-term\nmemory.\nPreprocessing\nThe pathology reports were segmented and assigned tokens\nusing the Spacy toolkit. We observed that some of the tokens\nwere incorrectly cascaded, which would impact the SHI tagging.\nFor example, the tokens “PsychiatryChief” and “JMH.Does”\nshould have been “Psychiatry Chief” and “JMH. Does,”\nrespectively. For example, consider “Age: \\n\\n39Sex”—where\njust the number “39” should be labeled as an SHI, not “Age”\nor the entire value “39Sex.” We applied regular\nexpressions–based rules after tokenization to further split and\nfix the incorrectly cascaded tokens to overcome this problem.\nThe fixed tokens were then updated in the sentences. This is\nthe most common problem in tokenizing, where custom rules\nneed to be added as per the task [32]. We designed the\ndeidentification process as a sentence-tagging task and predicted\nthe labels assigned to tokens in each sentence. These labels were\nused to indicate different categories of SHI. Our study explored\n2 types of tagging schemes: BIO and BIESO [33]. We finally\nused the BIESO tagging scheme in which “B,” “I,” and “E”\nindicate that the corresponding token is the “beginning,”\n“inside,” and “ending” for a certain SHI. “S” indicates that the\ncurrent token is an SHI consisting of only 1 word. Additionally,\n“O” refers to the “outside” label, which indicates that the token\nis not an SHI. Furthermore, we observed that the cascaded\ntokens were not correctly identified. For example, 2 ID entities\nwere recorded as 1 ID. Subsequently, we tailored the associated\ncascaded rules for preprocessing to improve the deidentification\nperformance for Australian pathology reports.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 4https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nExperimental Setup\nNeural Networks With Word Embeddings\nWord embedding or word vector is a method where individual\nwords are represented as a vector in a predefined space. It is\nused for dimensionality reduction and capture word semantics,\nwhich can be later used as an input in machine learning models\nfor training and inference. This is required because neural\nnetworks only recognize the numerical inputs [34]. In this\nexperimental setting, sentences were further split into tokens\nand n-grams. The pretrained word embeddings of the word\ntoken and its surrounding tokens were used to represent each\ntoken. In this experiment, we investigated GloVe (global vectors\nfor word representation), PMC, and Word2vec-OpenDeID\ncorpus and GloVe+PMC+word2vec-OpenDeID corpus\nembeddings. GloVe is pretrained on Wikipedia and English\nGiga-word fifth edition [35]. PMC is pretrained on PubMed,\nPMC texts, and Wikipedia [36]. Word2vec-OpenDeID corpus\nis pretrained on the OpenDeID corpus with the word2vec\nalgorithm [37]. In the GloVe+PMC+word2vec-OpenDeID\ncorpus, each token was represented by using a concatenation\nof the above 3 pretrained embeddings. Furthermore, we\nmanually engineered orthographical features [38] based on\nregular expressions to detect writing patterns of different SHI\ncategories. The manually developed features were encoded\nusing a 1-hot representation and concatenated with the token\nrepresentation. After activation by a rectified linear unit, the\nactivated representation vectors were fed into a neural network\nbased on long short-term memory (LSTM) to learn the tagging\nsequence. The network architecture was implemented using\nKeras software (Google). The minibatch gradient descent along\nwith Adam was used to optimize the parameters. The epoch\nwas set to 20, with a batch size of 8192.\nFine-Tuning Pretrained BERT-Base Model\nBERT is a machine learning framework designed for NLP tasks.\nBERT is based on a deep learning model called transformers.\nThe mechanism of transformers includes an encoder that reads\nand processes the text input and a decoder that produces an\noutput based on prediction for the given input. Previously,\nlanguage models could read text inputs in a sequential order.\nBERT, on the other hand, has a bidirectional capability\nintroduced by transformers. There are 2 types of BERT\nframework: BERT-base and BERT-large. The main difference\nbetween them is the number of encoder layers.\nBERT is designed to pretrain deep bidirectional representations\nfrom unlabeled text data by joint conditioning on both the left\nand right context in all layers. The fine-tuning step is done by\nincluding an additional output layer for deep learning tasks.\nBERT is evaluated on 11 NLP tasks. Fine-tuning the pretrained\nBERT model allows it to be used for various downstream tasks\nwith minimal architectural modifications. Semantic and syntactic\nknowledge in a pretrained large corpus can be transferred to\ndownstream tasks [39]. To achieve the deidentification task, a\npretrained BERT-base model was first incorporated with an\nadditional output layer to perform token-level classification,\nand the model was then fine-tuned on labeled data. In this set\nof experiments, we examined three contextual clinical and\nbiomedical BERT embeddings: (1) BioBERT: pretrained a\nBERT model on PubMed abstracts and PubMed Central full-text\narticles [40], (2) Clinical BioBERT: initialized from BioBERT\nand pretrained on approximately 2 million notes in the\nMIMIC-III (Medical Information Mart for Intensive Care) v1.4\ndatabase [41], and (3) Discharge Summary BioBERT: initialized\nfrom BioBERT and pretrained on the discharge summaries only\nin the MIMIC-III v1.4 database [41]. Alsentzer et al [41]\nreleased clinically trained BERT models fine-tuned atop\nBioBERT. The BERT-base models were implemented using\nPyTorch (version 1.10.1; PyTorch) in Python 3.9 (Python\nSoftware Foundation). The parameters were optimized using\nthe AdamW Optimizer with a learning rate of 2 × 10–5. The\nbatch size was 32 for training and 64 for validation and testing,\nrespectively. The early stopping criterion was used to prevent\noverfitting. The training was stopped when the token-level\naccuracy or microaveraged F1-score in the validation set no\nlonger increased for a certain number of patience epochs. The\nnumber of training epochs was 20, with a patience of 5. The\nsequence length of the input to the BERT-base model was 128.\nSentences longer than 128 in length were divided into segments.\nThe default hyperparameters were not fine-tuned for efficient\ntraining.\nPostprocessing\nThe output from the trained models was postprocessed to\nproduce a report in the XML format with the identified SHI\nentities. The predicted BIESO tags in the sentences were\nmatched with the original text and combined to assign a\ncorresponding SHI categorical output. Furthermore, regular\nexpression rules using the Python re library were applied to\ndetect “DATE,” “PHONE,” and “URL” to further improve the\naccuracy of the identified SHI categories. For example, in the\n“DATE” field, we used regular expression rules to find mentions\nmatching the following formats “1/12/2000,” “1-12-2000,”\n“Jan/12/2000,” “12/Jan/2000,” “Jan-12-2000,” and\n“12-Jan-2000.” For “PHONE,” we used regular expression rules\nto find any string with the following format: “1234567890,”\n“123.456.7890,” and “123-456-7890” to identify 10-digit phone\nnumbers.\nAn XML file was generated, with the text and SHI annotations\nfor each discharge summary report as the final output.\nAdditionally, if required, the OpenDeID pipeline can generate\nsurrogates. In this study, we have not generated or evaluated\nsurrogates. However, we have incorporated our previous work\non surrogates into the pipeline to allow users to generate\nsurrogates [26]. Surrogates can be generated on 6 SHI categories\nsuch as name, age, contact, location, date, and ID. For example,\nthe surrogate for a name can be generated by an alphabet shift\nof fixed length. This is followed by mapping the initial alphabet\nwith another random alphabet and finally selecting a surrogate\nstarting with the new alphabet from the dictionaries prepared.\nSimilarly, a surrogate for a date can be generated by a date shift\nin the range of 1 to 730 days. For ID and contact surrogates, a\ncustom set of rules can be developed [26].\nEvaluation\nModel performance was evaluated using microaveraged\nprecision, recall, and F1-scores with strict and relaxed matching\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 5https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n[42]. For strict matching, the start and end offsets for the system\noutputs must exactly match those of the gold standard\nannotations. Relaxed matching allows tolerance of character\noffsets. This allows for variations in including “s” and other\nendings that our system may ignore due to the tokenization. For\nexample, “HM” is identified as a “DOCTOR” tag in gold\nstandard annotations. Strict matching would only take the exact\nwords of the token. Relaxed matching would take tokens such\nas “HMs” and assign them as “DOCTOR” in the system. The\nF1-score is a metric used in machine learning to evaluate the\naccuracy of a model. It incorporates both the precision and recall\nscores of the model. The details of how to compute the precision,\nrecall, and F1-score can be found in Multimedia Appendix 1.\nThe evaluation script is based on the 2016 i2b2 evaluation script\n[43].\nEthical Considerations\nThis study was approved by the University of New South Wales\nSydney Human Research Ethics Committee (HC17749). This\nresearch study was undertaken in accordance with the approved\nethics application, relevant guidelines, and regulations.\nAdditionally, access to the OpenDeID Corpus was approved\nby the Secure Research Environment for Digital Health\n(SREDH) Consortium’s [44] Translational Cancer\nBioinformatics working group (SR42-2022).\nResults\nOverview\nThe corpus contained 2100 pathology reports with 38,414 SHI\nentities. After preprocessing, 15,85,905 tokens and 144,183\nsentences were extracted from the corpus. The length of the\nsentences ranged from 1 token to 293 tokens, with an average\nlength of 11 tokens. The length of the reports ranged from 106\ntokens to 3618 tokens, with an average of 755.19 tokens. The\ncorresponding SHI categories (and subcategories) in the training,\nvalidation, and testing data sets are summarized in Table S1 in\nMultimedia Appendix 1. Six SHI categories and 18\nsubcategories were identified by using the OpenDeID pipeline.\nWe excluded the “LOCATION: ROOM” and “ID: BIOID”\nsubcategories from the model building when fine-tuning the\nBERT-base models because these subcategories only appeared\nonce in the entire data set.\nNeural Networks With Different Word Embeddings\nTable 2 presents the performance of the LSTM neural network\nwith different word embeddings. The model with a\nconcatenation of all developed embeddings achieved the best\nmicroaveraged F1-scores of 0.9222 in strict matching and 0.9237\nin relaxed matching settings. The learning curve for the\nbest-performing model is shown in Figure S1 in Multimedia\nAppendix 1. We also used 2 strategies to investigate their impact\non the overall performance (Table S2 in Multimedia Appendix\n1).\nTable 2. Performance on the test sets using neural networks with different word embedding methodsa.\nRelaxedStrictWord embeddings\nF1-scoreRecallPrecisionF1-scoreRecallPrecision\n0.92370.89350.95590.92220.89210.9544GloVeb+PMC+word2vec-OpenDeID corpus\n0.83300.76790.91020.83200.76690.9091word2vec-OpenDeID corpus\n0.91460.87630.95650.91320.87490.9550PMC\n0.82150.74790.91120.82060.74700.9101GloVe\naPrecision, recall, and F1-score are microaveraged measures under relaxed settings.\nbGloVe: global vectors for word representation.\nFine-Tuning Different Pretrained BERT-Base Models\nThe performance of the different biomedical and clinical\nBERT-base models is shown in Table 3. For the overall\nperformance, the best-performing model was Discharge\nSummary BioBERT with a microaveraged F1-score of 0.9374\nfor strict matching and 0.9401 for relaxed matching. However,\nthe difference between the best- and worst-performing\nBERT-base models was only 0.0014 for strict matching and\n0.0015 for relaxed matching, which can be considered minor.\nThe learning curve for the discharge summary BioBERT is\npresented in Figure S2 in Multimedia Appendix 1. In terms of\nprecision and recall, we observed that the precision of the\nmodels was higher than that of the recalls for both strict and\nrelaxed metrics across different BERT-base models.\nApproximate randomization tests were used to test for statistical\nsignificance between different models, and the P-values are\nprovided in Table S3 in Multimedia Appendix 1. All the\nfine-tuned BERT-base models outperformed the neural network\nmodels with word embeddings in terms of overall performance.\nThis is mainly achieved by an improvement in the\nmicroaveraged recall from 0.8921 (neural network with\nGloVe+PMC+word2vec-OpenDeID corpus) to 0.9196\n(Discharge Summary BioBERT). To evaluate the robustness\nof the results and the impact of training size, we evaluated\ndifferent splits of the training and validation sets for fine-tuning\nthe Discharge Summary BioBERT (details in Table S4 and\nFigure S3 in Multimedia Appendix 1). The results did not\nchange considerably, and we recorded relaxed microaveraged\nF1-scores ranging from 0.9361 to 0.9374.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 6https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 3. Performance on the test sets using different fine-tuned pretrained BERTa-base models.\nRelaxedStrictBERT-base models\nF1-scoreRecallPrecisionF1-scoreRecallPrecision\n0.93970.92420.95580.93690.92140.9529BioBERT\n0.93860.91980.95820.93600.91720.9555Clinical BioBERT\n0.94010.92220.95870.93740.91960.9560Discharge Summary BioBERT\naBERT: Bidirectional Encoder Representations from Transformers.\nTo account for cascaded tokens with a missing space in real\nlife, additional preprocessing rules to splitting the cascaded\ntokens are implemented by considering the Discharge Summary\nBioBERT model with the updated OpenDeID pipeline as an\nexample, which results in a 3.04% strict microaveraged F1-score\nimprovement from 0.9374 to 0.9659, as shown in Table 4. This\nindicates that a carefully crafted rule-based approach added to\nthe preprocessing phase to boost the performance of the\nBERT-base model is essential in real-life deidentification\nproblems.\nTable 4. Performance on the test sets using cascaded ruled applied into BERTa-base models.\nRelaxedStrictRun\nF1-scoreRecallPrecisionF1-scoreRecallPrecision\n0.94010.92220.95870.93740.91960.9560Discharge summary BioBERT run without\ncascading rules\n0.96870.95920.97840.96590.95640.9756Discharge summary BioBERT run with addi-\ntional cascading rules\naBERT: Bidirectional Encoder Representations from Transformers.\nThe best-performing run of the OpenDeID pipeline is currently\nused at a large tertiary hospital to deidentify EHR text notes\nassociated with biobanking. As of 2022, the deployment of\nOpenDeID pipelines has processed more than 8000 reports in\nreal time.\nDiscussion\nPrincipal Findings\nWe investigated various configurations of our pipeline using\nword embeddings and transformer-based methods in conjunction\nwith cascade associative preprocessing rules to deidentify\nAustralian pathology reports. The proposed OpenDeID pipeline\nconsists of 3 steps: preprocessing, modeling, and postprocessing.\nIn the modeling steps, neural networks with word embeddings\nand fine-tuned pretrained language models were considered and\nimplemented as options. For neural networks with word\nembeddings, a combination of different word embeddings\n(GloVe+PMC+word2vec-OpenDeID corpus) achieved the best\nmicroaveraged F1-scores (strict: 0.9222, relaxed: 0.9237). For\nfine-tuned pretrained language models, the OpenDeID pipeline\nusing Discharge Summary BioBERT with additional\npreprocessing rules achieved an accurate performance on the\ntest set, achieving the best microaveraged F1-scores (strict:\n0.9659, relaxed: 0.9687).\nWe evaluated the performance of our pipeline over 6 categories\nand 18 subcategories commonly found in Australian EHR text\nnotes [45]. Various annotated corpora are available for\ndeidentification. The 2014 i2b2/UTHealth deidentification\ncorpus contains 1304 clinical notes from 296 patients in the\nUnited States [46]. The second was the 2016 CEGS N-GRID\ncorpus, which contains 1000 clinical notes from the United\nStates [29]. Uzuner et al [47] developed a corpus containing\n889 hospital discharge summaries. The MIMIC-III database\ncontains deidentified clinical data of patients admitted to the\nBeth Israel Deaconess Medical Center in Boston, United States.\nOur initial experiments with these corpora showed low\ngeneralizability to Australian EHR text notes. Therefore, we\nused the OpenDeID corpus for training, which is based on\nAustralian EHR text notes data.\nTo investigate the effectiveness of popular NLP methods in the\nmodeling steps, we implemented neural networks with word\nembeddings and pretrained language models. It was shown that\nfine-tuning pretrained BERT-base models outperformed the\nneural networks with word embeddings in terms of overall\nperformance. For neural networks, various word embeddings\nwere compared, including combinations of PMC, the\nword2vec-OpenDeID corpus, and GloVe. The result indicates\nthat the concatenation of all word embeddings performed the\nbest. Among the pretrained word embeddings, PMC performed\nbetter than the word2vec-OpenDeID corpus and GloVe. There\nwas a negligible difference in performance between the\nword2vec-OpenDeID corpus and the GloVe word embeddings.\nIn terms of pretrained language models, we examined different\nclinical models, including BioBERT, Clinical BioBERT, and\nDischarge Summary BioBERT, and their deidentification\nperformances were largely similar in this study. Similar results\nwere reported for the performances of BioBERT, Clinical\nBioBERT, and Discharge Summary BioBERT when they were\nevaluated on the standard deidentification tasks of i2b2 2006\nand i2b2 2014, as shown in Table S5 in Multimedia Appendix\n1 [41]. This may be attributed to the lack of necessary\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 7https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\npatient-identifying knowledge from the upstream pretrained\nmodels to be transferred to the downstream deidentification\ntask. This is because many of these clinically pretrained models,\nsuch as the Discharge Summary BioBERT and Clinical\nBioBERT, were pretrained using the MIMIC-III data set, which\ncontains SHI surrogates instead of real SHI. In addition, Ji et\nal [48] explained the importance of biomedical entity\nnormalization in enhancing the model’s performance. Similarly,\nreducing the batch size to 16 or 32, as suggested by Devlin et\nal [49], can improve the overall results. Our experiments showed\nthat a strict microaveraged F1-score of 0.9374 could be achieved\non the test set using the OpenDeID pipeline with Discharge\nSummary BioBERT training on only 700 annotated reports.\nThis indicates the potential of the pipeline for implementation\nin low-resource settings.\nError Analysis\nAs shown in Table S6 in Multimedia Appendix 1, we further\nexamined the causes of the prediction mistakes identified\nthrough error analysis. This step is crucial because it allows us\nto customize efficient preprocessing methods to deal with\nfrequently occurring, repeating, and mistakenly cascaded tokens.\nUsing the fine-tuned Discharge Summary BioBERT model, we\napplied the OpenDeID pipeline to detect SHI for these 50\nreports. Of the 940 SHI entities, 874 were correctly identified,\nwhereas 66 were mislabeled or not detected by OpenDeID.\nAmong all the SHI subcategories in the selected reports,\n“NAME: PATIENT,” “LOCATION: ZIP,” “AGE,” and “ID:\nMEDICAL RECORD” were accurately identified by the\nOpenDeID pipeline. There were 3 false positives and 4 false\nnegatives related to incorrect gold standard annotations, which\nshould have been treated as true positives. Two examples are\npresented in Table S7 in Multimedia Appendix 1.\nThe cascaded rules we implemented prevented 16 false positives\nand 31 false negatives caused by incorrectly cascaded tokens.\nThese cascaded tokens can be correctly detected by\nincorporating cascaded rules into an OpenDeID pipeline with\na BERT-base model. For instance, without the cascaded rules,\na string such as “12G00123,12N01234” would have been treated\nas a token together, instead of 2 IDs: IDNUM “12G00123” and\n“12N01234,” resulting in 1 false positive and 2 false negatives\noutcome. Similarly, we specified the associated rules to (1)\nidentify a token with a comma or semicolon in the middle and\n(2) add a space after the comma or semicolon and then split the\ncascaded tokens. The additional cascaded rules can help improve\nthe precision of tokenization and sentence splitting, improving\ndeidentification accuracy. EHR text notes are written as free\ntext, and misspelling and missing space issues are widely\nobserved in the text. Therefore, performing error analysis and\napplying rule-based preprocessing steps are important to\ndiscover and eliminate potential writing problems. Furthermore,\ndifferent EHR text notes collected by different hospitals may\npossess different formats, templates, and structures. A\none-size-fits-all approach may not be able to fully realize the\npotential of the deidentification pipeline across all systems. For\nexternal data sets that may incorporate this pipeline in the future,\nit is suggested to study the resulting errors systematically, and\nin-house modelers can tailor additional rules to incorporate the\ndata properties retained in each specific model. For a detailed\nexample of such an error investigation, refer to the error analysis\nin Multimedia Appendix 1.\nLimitations and Future Work\nThis study had some limitations. The recall of OpenDeID must\nbe further improved. Recall measures the ability to retrieve all\nSHI from clinical texts, which is essential to the deidentification\nproblem because any unidentified SHI could potentially\ncompromise patient privacy. We may consider different loss\nfunctions based on recall when optimizing the parameters in\nthe models. Furthermore, we did not fine-tune the\nhyperparameters in the BERT-base models, which could\npotentially improve the performances and should be further\ninvestigated. It is important to note that our study provides\nvaluable insights and a foundation for deidentification in the\ncontext of Australian pathology reports. We anticipate further\nresearch to externally validate our pipeline and evaluate the\napplicability and performance of our approach across different\nclinical note types. The proposed pipeline was not examined\nusing an external data set. We also intend to further validate the\npipeline using other Australian DeID data sets. Although the\nannotations in the OpenDeID corpus are replaced with synthetic\nbut realistic SHI, it is still possible that the real data might have\nvariations that are not observed in the synthetic annotations. As\nsuch, large-scale external validation using real SHI data is\nnecessary to evaluate the effectiveness of the OpenDeID\npipeline. Our future work will include the external validation\nof the pipeline on corpora from other countries and\nbenchmarking against state-of-the-art deidentification tools like\nOpenDeID.\nConclusions\nThis study presents a hybrid deidentification pipeline called the\nOpenDeID pipeline. The pipeline was deployed at a large\ntertiary hospital in 2019 and has processed over 8000 reports\nin real time. The OpenDeID pipeline is evaluated under various\nsettings, using transformer-based neural networks and different\nconfigurations of word embeddings. We used the OpenDeID\ncorpus, which consists of 2100 pathology reports extracted from\nAustralian EHR systems to train and evaluate performance. The\nmodels were trained on the training and validation sets, followed\nby evaluation on the test set. Strict and relaxed matching\nschemes were used for comparative analysis, and the\nperformance was measured using precision, recall, and\nmicroaveraged F1-score. The OpenDeID deidentification\npipeline incorporates LSTM with different settings for word\nembedding or fine-tuned BERT-base models. The pipeline\nachieved the best performance using a combination of different\nword embeddings (GloVe+PMC+word2vec-OpenDeID corpus).\nThe best run is the fine-tuned Discharge Summary BioBERT\nmodel with an F1-score of 0.9659. Our systematic error analysis\nidentified minor annotation errors in the corpus and areas of\nimprovement in the OpenDeID pipeline.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 8https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nAcknowledgments\nJL was supported by the Fundamental Research Funds for the Central Universities (grant 2722023BQ053), the Natural Science\nFoundation of Hubei Province of China (2023AFB414), and the Hubei Province Postdoctoral Funding Project. JJ was funded by\nthe Australian National Health and Medical Research Council (grant GNT1192469). JJ also acknowledges the funding support\nreceived through the Research Technology Services at the University of New South Wales Sydney, Google Cloud Research\n(award GCP19980904), and the NVIDIA Academic Hardware grant programs. We would also like to thank the Secure Research\nEnvironment for Digital Health (SREDH) Consortium’s Translational Cancer Bioinformatics working group for access to the\nOpenDeID corpus [44].\nData Availability\nThe data sets generated and analyzed during this study are available from the corresponding author on reasonable request.\nAuthors' Contributions\nJJ conceptualized the study. JL performed the formal analysis and visualization. JL and JJ wrote the original draft. JJ and ZSYW\nreviewed and edited the draft. JJ and JL contributed to the funding acquisition. All authors read and approved the final manuscript.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nSupplementary methods and results.\n[DOCX File , 196 KB-Multimedia Appendix 1]\nReferences\n1. Yanamadala S, Morrison D, Curtin C, McDonald K, Hernandez-Boussard T. Electronic health records and quality of care:\nan observational study modeling impact on mortality, readmissions, and complications. Medicine (Baltimore).\n2016;95(19):e3332 [FREE Full text] [doi: 10.1097/MD.0000000000003332] [Medline: 27175631]\n2. Jonnagaddala J, Dai HJ, Ray P, Liaw ST. Mining electronic health records to guide and support clinical decision support\nsystems. In: Moon JD, Galea MP, editors. Improving Health Management Through Clinical Decision Support Systems.\nHershey, PA, USA. IGI Global; 2015:252-269\n3. Alder S. What is considered PHI under HIPAA? HIPPA J. 2023. URL: https://www.hipaajournal.com/considered-phi-hipaa/\n#:~:text=PHI%20includes%20individually%20identifiable%20health,or%20payment%20for%20the%20treatment[accessed\n2023-11-24]\n4. Guide to health privacy. Office of the Australian Information Commissioner. 2023. URL: https://www.oaic.gov.au/privacy/\nprivacy-guidance-for-organisations-and-government-agencies/health-service-providers/guide-to-health-privacy [accessed\n2023-11-20]\n5. HIPAA. Health insurance portability and accountability act of 1996. Public Law 104-191. 1996. URL: http://www.\neolusinc.com/pdf/hipaa.pdf [accessed 2023-11-24]\n6. Meystre SM, Friedlin FJ, South BR, Shen S, Samore MH. Automatic de-identification of textual documents in the electronic\nhealth record: a review of recent research. BMC Med Res Methodol. 2010;10:70 [FREE Full text] [doi:\n10.1186/1471-2288-10-70] [Medline: 20678228]\n7. Shweta Y, Ekbal A, Saha S, Bhattacharyya P. Deep learning architecture for patient data de-identification in clinical records.\nThe COLING 2016 Organizing Committee; 2016 Presented at: Proceedings of the Clinical Natural Language Processing\nWorkshop (ClinicalNLP); December 11-17, 2016; Osaka, Japan p. 32-41\n8. Deleger L, Lingren T, Ni Y, Kaiser M, Stoutenborough L, Marsolo K, et al. Preparing an annotated gold standard corpus\nto share with extramural investigators for de-identification research. J Biomed Inform. 2014;50:173-183 [FREE Full text]\n[doi: 10.1016/j.jbi.2014.01.014] [Medline: 24556292]\n9. Gupta D, Saul M, Gilbertson J. Evaluation of a deidentification (De-Id) software engine to share pathology reports and\nclinical documents for research. Am J Clin Pathol. 2004;121(2):176-186 [FREE Full text] [doi:\n10.1309/E6K3-3GBP-E5C2-7FYU] [Medline: 14983930]\n10. Sweeney L. Replacing personally-identifying information in medical records, the scrub system. Proc AMIA Annu Fall\nSymp. 1996:333-337 [FREE Full text] [Medline: 8947683]\n11. Neamatullah I, Douglass MM, Lehman LWH, Reisner A, Villarroel M, Long WJ, et al. Automated de-identification of\nfree-text medical records. BMC Med Inform Decis Mak. 2008;8:32 [FREE Full text] [doi: 10.1186/1472-6947-8-32]\n[Medline: 18652655]\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 9https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n12. Zhao Z, Yang M, Tang B, Zhao T. Re-examination of rule-based methods in deidentification of electronic health records:\nalgorithm development and validation. JMIR Med Inform. 2020;8(4):e17622 [FREE Full text] [doi: 10.2196/17622]\n[Medline: 32352384]\n13. Dehghan A, Kovacevic A, Karystianis G, Keane JA, Nenadic G. Combining knowledge- and data-driven methods for\nde-identification of clinical narratives. J Biomed Inform. 2015;58(Suppl):S53-S59 [FREE Full text] [doi:\n10.1016/j.jbi.2015.06.029] [Medline: 26210359]\n14. Norgeot B, Muenzen K, Peterson TA, Fan X, Glicksberg BS, Schenk G, et al. Protected Health Information filter (Philter):\naccurately and securely de-identifying free-text clinical notes. NPJ Digit Med. 2020;3:57 [FREE Full text] [doi:\n10.1038/s41746-020-0258-y] [Medline: 32337372]\n15. He B, Guan Y, Cheng J, Cen K, Hua W. CRFs based de-identification of medical records. J Biomed Inform.\n2015;58(Suppl):S39-S46 [FREE Full text] [doi: 10.1016/j.jbi.2015.08.012] [Medline: 26315662]\n16. Ahmed T, Aziz MMA, Mohammed N. De-identification of electronic health record using neural network. Sci Rep.\n2020;10(1):18600 [FREE Full text] [doi: 10.1038/s41598-020-75544-1] [Medline: 33122735]\n17. Khin K, Burckhardt P, Padman R. A deep learning architecture for de-identification of patient notes: implementation and\nevaluation. ArXiv. Preprint posted online on October 3 2018. [FREE Full text] [doi: 10.48550/arXiv.1810.01570]\n18. Yang X, Lyu T, Li Q, Lee CY, Bian J, Hogan WR, et al. A study of deep learning methods for de-identification of clinical\nnotes in cross-institute settings. BMC Med Inform Decis Mak. 2019;19(Suppl 5):232 [FREE Full text] [doi:\n10.1186/s12911-019-0935-4] [Medline: 31801524]\n19. Ahmed A, Abbasi A, Eickhoff C. Benchmarking modern named entity recognition techniques for free-text health record\ndeidentification. AMIA Jt Summits Transl Sci Proc. 2021;2021:102-111 [FREE Full text] [Medline: 34457124]\n20. Liu L, Perez-Concha O, Nguyen A, Bennett V, Jorm L. De-identifying Australian hospital discharge summaries: an\nend-to-end framework using ensemble of deep learning models. J Biomed Inform. 2022;135:104215 [FREE Full text] [doi:\n10.1016/j.jbi.2022.104215] [Medline: 36195240]\n21. Ferrández O, South BR, Shen S, Friedlin FJ, Samore MH, Meystre SM. BoB, a best-of-breed automated text de-identification\nsystem for VHA clinical documents. J Am Med Inform Assoc. 2013;20(1):77-83 [FREE Full text] [doi:\n10.1136/amiajnl-2012-001020] [Medline: 22947391]\n22. Lee HJ, Wu Y, Zhang Y, Xu J, Xu H, Roberts K. A hybrid approach to automatic de-identification of psychiatric notes. J\nBiomed Inform. 2017;75S:S19-S27 [FREE Full text] [doi: 10.1016/j.jbi.2017.06.006] [Medline: 28602904]\n23. Zhao YS, Zhang KL, Ma HC, Li K. Leveraging text skeleton for de-identification of electronic medical records. BMC Med\nInform Decis Mak. 2018;18(Suppl 1):18 [FREE Full text] [doi: 10.1186/s12911-018-0598-6] [Medline: 29589571]\n24. Liu Z, Tang B, Wang X, Chen Q. De-identification of clinical notes via recurrent neural network and conditional random\nfield. J Biomed Inform. 2017;75S:S34-S42 [FREE Full text] [doi: 10.1016/j.jbi.2017.05.023] [Medline: 28579533]\n25. Murugadoss K, Rajasekharan A, Malin B, Agarwal V, Bade S, Anderson JR, et al. Building a best-in-class automated\nde-identification tool for electronic health records through ensemble learning. Patterns (N Y). 2021;2(6):100255 [FREE\nFull text] [doi: 10.1016/j.patter.2021.100255] [Medline: 34179842]\n26. Chen A, Jonnagaddala J, Nekkantti C, Liaw ST. Generation of surrogates for de-identification of electronic health records.\nStud Health Technol Inform. 2019;264:70-73 [FREE Full text] [doi: 10.3233/SHTI190185] [Medline: 31437887]\n27. Jonnagaddala J, Chen A, Batongbacal S, Nekkantti C. The OpenDeID corpus for patient de-identification. Sci Rep.\n2021;11(1):19973 [FREE Full text] [doi: 10.1038/s41598-021-99554-9] [Medline: 34620985]\n28. Alla NLV, Chen A, Batongbacal S, Nekkantti C, Dai HJ, Jonnagaddala J. Cohort selection for construction of a clinical\nnatural language processing corpus. Comput Methods Programs Biomed Update. 2021;1:100024 [FREE Full text] [doi:\n10.1016/j.cmpbup.2021.100024]\n29. Stubbs A, Filannino M, Uzuner Ö. De-identification of psychiatric intake records: overview of 2016 CEGS N-GRID shared\ntasks track 1. J Biomed Inform. 2017;75S:S4-S18 [FREE Full text] [doi: 10.1016/j.jbi.2017.06.011] [Medline: 28614702]\n30. OpenDeID corpus. GitHub. URL: https://github.com/TCRNBioinformatics/OpenDeID-Corpus [accessed 2023-11-24]\n31. OpenDeID-Pipeline. GitHub. URL: https://github.com/TCRNBioinformatics/OpenDeID-Pipeline [accessed 2023-11-24]\n32. Wrenn JO, Stetson PD, Johnson SB. An unsupervised machine learning approach to segmentation of clinician-entered free\ntext. AMIA Annu Symp Proc. 2007;2007:811-815 [FREE Full text] [Medline: 18693949]\n33. Tang B, Cao H, Wu Y, Jiang M, Xu H. Recognizing clinical entities in hospital discharge summaries using structural support\nvector machines with word representation features. BMC Med Inform Decis Mak. 2013;13(Suppl 1):S1 [FREE Full text]\n[doi: 10.1186/1472-6947-13-S1-S1] [Medline: 23566040]\n34. Understanding neural word embeddings. Pure AI. 2020. URL: https://pureai.com/articles/2020/01/06/neural-word-embeddings.\naspx#:~:text=Most%20of%20the%20advanced%20neural,describe%20the%20idea%20very%20well[accessed 2023-11-24]\n35. Pennington J, Socher R, Manning C. GloVe: Global Vectors for word representation. 2014 Presented at: Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP); October 25-29, 2014; Doha, Qatar\np. 1532-1543 [doi: 10.3115/v1/d14-1162]\n36. Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional semantics resources for biomedical text processing.\nProceedings of LBM. 2013. URL: https://bio.nlplab.org/pdf/pyysalo13literature.pdf [accessed 2023-11-25]\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 10https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n37. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their\ncompositionality. In: Burges CJ, Bottou L, Welling M, Ghahramani Z, Weinberger KQ, editors. Advances in Neural\nInformation Processing Systems 26. Red Hook, NY. Curran Associates, Inc; 2013.\n38. Tsai RTH, Sung CL, Dai HJ, Hung HC, Sung TY, Hsu WL. NERBio: using selected word conjunctions, term normalization,\nand global patterns to improve biomedical named entity recognition. BMC Bioinformatics. 2006;7(Suppl 5):S11 [FREE\nFull text] [doi: 10.1186/1471-2105-7-S5-S11] [Medline: 17254295]\n39. Prottasha NJ, Sami AA, Kowsher M, Murad SA, Bairagi AK, Masud M, et al. Transfer learning for sentiment analysis\nusing BERT based supervised fine-tuning. Sensors (Basel). 2022;22(11):4157 [FREE Full text] [doi: 10.3390/s22114157]\n[Medline: 35684778]\n40. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics. 2020;36(4):1234-1240 [FREE Full text] [doi: 10.1093/bioinformatics/btz682]\n[Medline: 31501885]\n41. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly available clinical BERT embeddings.\nAssociation for Computational Linguistics; 2019 Presented at: Proceedings of the 2nd Clinical Natural Language Processing\nWorkshop; June 7, 2019; Minneapolis, Minnesota, USA p. 72-78 [doi: 10.18653/v1/w19-1909]\n42. Wang C, Nulty P, Lillis D. A comparative study on word embeddings in deep learning for text classification. New York,\nNY, United States. Association for Computing Machinery; 2020 Presented at: NLPIR '20: Proceedings of the 4th International\nConference on Natural Language Processing and Information Retrieval; December 18-20, 2020; Seoul, Republic of Korea\np. 37-46 [doi: 10.1145/3443279.3443304]\n43. i2b2 evaluation script. GitHub. URL: https://github.com/kotfic/i2b2_evaluation_scripts [accessed 2023-11-24]\n44. SREDH. URL: https://www.sredhconsortium.org/ [accessed 2023-07-15]\n45. Danielle K. Understanding what is and is not PHI. HIPAA Exams. 2022. URL: https://www.hipaaexams.com/blog/\nunderstanding-what-is-and-is-not-phi [accessed 2023-11-24]\n46. Stubbs A, Kotfila C, Uzuner Ö. Automated systems for the de-identification of longitudinal clinical narratives: overview\nof 2014 i2b2/UTHealth shared task track 1. J Biomed Inform. 2015;58(Suppl):S11-S19 [FREE Full text] [doi:\n10.1016/j.jbi.2015.06.007] [Medline: 26225918]\n47. Uzuner O, Luo Y, Szolovits P. Evaluating the state-of-the-art in automatic de-identification. J Am Med Inform Assoc.\n2007;14(5):550-563 [FREE Full text] [doi: 10.1197/jamia.M2444] [Medline: 17600094]\n48. Ji Z, Wei Q, Xu H. BERT-based ranking for biomedical entity normalization. AMIA Jt Summits Transl Sci Proc.\n2020;2020:269-277 [FREE Full text] [Medline: 32477646]\n49. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language understanding.\nArXiv. Preprint posted online on May 24, 2019. 2019 [FREE Full text] [doi: 10.48550/arXiv.1810.04805]\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers\nBi-LSTM: bidirectional long short-term memory network\nCRF: conditional random field\nEHR: electronic health record\nGloVe: global vectors for word representation\nHIPAA: Health Insurance Portability and Accountability Act\nLSTM: long short-term memory\nMIMIC-III: Medical Information Mart for Intensive Care\nNLP: natural language processing\nSHI: sensitive health information\nSREDH: Secure Research Environment for Digital Health\nEdited by T de Azevedo Cardoso; submitted 13.04.23; peer-reviewed by PF Chen, S Meystre; comments to author 17.06.23; revised\nversion received 26.07.23; accepted 22.11.23; published 06.12.23\nPlease cite as:\nLiu J, Gupta S, Chen A, Wang CK, Mishra P, Dai HJ, Wong ZSY, Jonnagaddala J\nOpenDeID Pipeline for Unstructured Electronic Health Record Text Notes Based on Rules and Transformers: Deidentification\nAlgorithm Development and Validation Study\nJ Med Internet Res 2023;25:e48145\nURL: https://www.jmir.org/2023/1/e48145\ndoi: 10.2196/48145\nPMID: 38055317\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 11https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n©Jiaxing Liu, Shalini Gupta, Aipeng Chen, Chen-Kai Wang, Pratik Mishra, Hong-Jie Dai, Zoie Shui-Yee Wong, Jitendra\nJonnagaddala. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 06.12.2023. This is an\nopen-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic\ninformation, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must\nbe included.\nJ Med Internet Res 2023 | vol. 25 | e48145 | p. 12https://www.jmir.org/2023/1/e48145\n(page number not for citation purposes)\nLiu et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7512938976287842
    },
    {
      "name": "Preprocessor",
      "score": 0.6615892052650452
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6262621283531189
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5973443388938904
    },
    {
      "name": "Transformer",
      "score": 0.5790454745292664
    },
    {
      "name": "Natural language processing",
      "score": 0.5738334655761719
    },
    {
      "name": "Unstructured data",
      "score": 0.5522607564926147
    },
    {
      "name": "Word embedding",
      "score": 0.5027620792388916
    },
    {
      "name": "Machine learning",
      "score": 0.4671921730041504
    },
    {
      "name": "Data pre-processing",
      "score": 0.4616713523864746
    },
    {
      "name": "Health records",
      "score": 0.44062933325767517
    },
    {
      "name": "Language model",
      "score": 0.43502986431121826
    },
    {
      "name": "F1 score",
      "score": 0.4235725998878479
    },
    {
      "name": "Data mining",
      "score": 0.3932930827140808
    },
    {
      "name": "Information retrieval",
      "score": 0.33122724294662476
    },
    {
      "name": "Health care",
      "score": 0.2432231903076172
    },
    {
      "name": "Embedding",
      "score": 0.17578482627868652
    },
    {
      "name": "Big data",
      "score": 0.13798511028289795
    },
    {
      "name": "Programming language",
      "score": 0.13724640011787415
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158934434",
      "name": "Zhongnan University of Economics and Law",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I148366613",
      "name": "National Yang Ming Chiao Tung University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I152743029",
      "name": "Kaohsiung Medical University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I4210092806",
      "name": "St. Luke's International University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I2801466683",
      "name": "Shaikh Khalifa Medical City",
      "country": "AE"
    }
  ],
  "cited_by": 27
}