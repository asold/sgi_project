{
    "title": "Exploiting Data-Efficient Image Transformer-Based Transfer Learning for Valvular Heart Diseases Detection",
    "url": "https://openalex.org/W4391165157",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2188956636",
            "name": "Talit Jumphoo",
            "affiliations": [
                "Suranaree University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2304215102",
            "name": "Khomdet Phapatanaburi",
            "affiliations": [
                "Rajamangala University of Technology Isan"
            ]
        },
        {
            "id": "https://openalex.org/A4223697464",
            "name": "Wongsathon Pathonsuwan",
            "affiliations": [
                "Suranaree University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2563841701",
            "name": "Patikorn Anchuen",
            "affiliations": [
                "Navamindradhiraj University"
            ]
        },
        {
            "id": "https://openalex.org/A123294612",
            "name": "Monthippa Uthansakul",
            "affiliations": [
                "Suranaree University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A366864676",
            "name": "Peerapong Uthansakul",
            "affiliations": [
                "Suranaree University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4283011955",
        "https://openalex.org/W4210242902",
        "https://openalex.org/W3170049569",
        "https://openalex.org/W2980617119",
        "https://openalex.org/W4206766697",
        "https://openalex.org/W2970061430",
        "https://openalex.org/W3162241632",
        "https://openalex.org/W3159550770",
        "https://openalex.org/W4292608019",
        "https://openalex.org/W3136660477",
        "https://openalex.org/W2773814046",
        "https://openalex.org/W6734761546",
        "https://openalex.org/W2342504937",
        "https://openalex.org/W4221136339",
        "https://openalex.org/W3129343518",
        "https://openalex.org/W2993118585",
        "https://openalex.org/W2749389370",
        "https://openalex.org/W3127049291",
        "https://openalex.org/W6734298547",
        "https://openalex.org/W3039983345",
        "https://openalex.org/W2847046955",
        "https://openalex.org/W3139775046",
        "https://openalex.org/W4280504701",
        "https://openalex.org/W6734201357",
        "https://openalex.org/W4294316731",
        "https://openalex.org/W6787508076",
        "https://openalex.org/W3004025693",
        "https://openalex.org/W2923484908",
        "https://openalex.org/W2981505513",
        "https://openalex.org/W2026417758",
        "https://openalex.org/W3168508293",
        "https://openalex.org/W2996880394",
        "https://openalex.org/W2957232342",
        "https://openalex.org/W4312998180",
        "https://openalex.org/W2160815625",
        "https://openalex.org/W2804483946",
        "https://openalex.org/W2917865943",
        "https://openalex.org/W2811161392",
        "https://openalex.org/W6734306856",
        "https://openalex.org/W2913088108",
        "https://openalex.org/W3035159033",
        "https://openalex.org/W2758244442",
        "https://openalex.org/W2913111755",
        "https://openalex.org/W2997702264",
        "https://openalex.org/W2996028987",
        "https://openalex.org/W4292543379",
        "https://openalex.org/W4313034275",
        "https://openalex.org/W4223459232",
        "https://openalex.org/W4223899585",
        "https://openalex.org/W3090425814",
        "https://openalex.org/W3115304128",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W6682132143",
        "https://openalex.org/W4225888087",
        "https://openalex.org/W2557139718",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W1689977300",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2901976219",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W3023602589",
        "https://openalex.org/W3045378552",
        "https://openalex.org/W6734804170",
        "https://openalex.org/W1604034532",
        "https://openalex.org/W2088169690"
    ],
    "abstract": "Recent studies have shown the potential of the Data-Efficient Image Transformer (DeiT)-based transfer learning method in speech/image recognition and classification utilizing models pre-trained on image datasets. However, the use of DeiT models, especially those pre-trained on image datasets, has not yet been explored for Valvular Heart Disease (VHD) detection. This paper proposes a transfer learning methodology using the DeiT model pre-trained on image datasets for VHD classification. Additionally, we introduce a hybrid Convolution-DeiT (Conv-DeiT) architecture to further improve classification performance. The Conv-DeiT framework integrates a convolutional block with a Squeeze-and-Excitation (SE) attention mechanism to enhance the channel and spatial information within the input features before processing by the DeiT model. The proposed models were assessed using the Heart Sound Murmur (HSM) database, accessible on GitHub. Experimental results show that the DeiT-based transfer learning approach achieved an overall accuracy of 97.44&#x0025;. Moreover, our Conv-DeiT method outperformed the DeiT-based transfer learning with an impressive overall accuracy of 99.44&#x0025;. This study indicates the effectiveness of transfer learning using DeiT models pre-trained on image datasets for heart sound classification. Specifically, our hybrid Conv-DeiT method, which combines the convolutional block and the SE-attention mechanism, demonstrates significant advantages in this context.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nExploiting Data-Efficient Image\nTransformer-based Transfer Learning for\nValvular Heart Diseases Detection\nTALIT JUMPHOO1, KHOMDET PHAPATANABURI2, WONGSATHON PATHONSUWAN1 ,\nPATIKORN ANCHUEN3, MONTHIPPA UTHANSAKUL1, (Member, IEEE) and PEERAPONG\nUTHANSAKUL1, (Member, IEEE)\n1School of Telecommunication Engineering, Suranaree University of Technology, Nakhon Ratchasima 30000, Thailand\n2Department of Telecommunication Engineering, Faculty of Engineering and Technology, Rajamangala University of Technology Isan (RMUTI), Nakhon\nRatchasima 30000, Thailand\n3Navaminda Kasatriyadhiraj Royal Air Force Academy, Bangkok 10220, Thailand\nCorresponding authors: Khomdet Phapatanaburi (khomdet.ph@rmuti.ac.th) and Peerapong Uthansakul (uthansakul@sut.ac.th)\nThis work was supported by (i) Suranaree University of Technology (SUT), (ii) Thailand Science Research and Innovation (TSRI), (iii)\nNational Science, Research and Innovation Fund (NSRF) through NRIIS under Grant 179284, and (iv) the NSRF via the Program\nManagement Unit for Human Resources & Institutional Development, Research and Innovation (PMU-B) (grant number B13F660067)\nABSTRACT Recent studies have shown the potential of the Data-Efficient Image Transformer (DeiT)-\nbased transfer learning method in speech/image recognition and classification utilizing models pre-trained\non image datasets. However, the use of DeiT models, especially those pre-trained on image datasets,\nhas not yet been explored for Valvular Heart Disease (VHD) detection. This paper proposes a transfer\nlearning methodology using the DeiT model pre-trained on image datasets for VHD classification. Addi-\ntionally, we introduce a hybrid Convolution-DeiT (Conv-DeiT) architecture to further improve classification\nperformance. The Conv-DeiT framework integrates a convolutional block with a Squeeze-and-Excitation\n(SE) attention mechanism to enhance the channel and spatial information within the input features before\nprocessing by the DeiT model. The proposed models were assessed using the Heart Sound Murmur (HSM)\ndatabase, accessible on GitHub. Experimental results show that the DeiT-based transfer learning approach\nachieved an overall accuracy of 97.44%. Moreover, our Conv-DeiT method outperformed the DeiT-based\ntransfer learning with an impressive overall accuracy of 99.44%. This study indicates the effectiveness of\ntransfer learning using DeiT models pre-trained on image datasets for heart sound classification. Specifically,\nour hybrid Conv-DeiT method, which combines the convolutional block and the SE-attention mechanism,\ndemonstrates significant advantages in this context.\nINDEX TERMS Valvular heart diseases detection, transfer learning, DeiT, hybrid model.\nI. INTRODUCTION\nV\nAlvular heart disease (VHD) is emerging as a major\nhealth concern globally, especially compared to other\ncardiovascular diseases, due to its rising prevalence and high\nmortality rates [1]. Early VHD screening is essential in re-\nducing these mortality rates. Traditionally, auscultating heart\nsounds has been the primary medical approach for VHD eval-\nuation, providing valuable insights into cardiovascular abnor-\nmalities [2], [3]. However, diagnosing cardiac abnormalities\nthrough auscultation can be challenging, especially for inex-\nperienced clinicians [4]. In current technology, digital stetho-\nscopes have been used to record heart sounds, which can be\nplotted in a graph known as Phonocardiograms (PCG). It is\npaving the way for a comprehensive PCG signals database.\nAs a result, using artificial intelligence to analyze and detect\ncardiac abnormalities has gained significant attention.\nThe current methods used in artificial intelligence for de-\ntecting VHD can be divided into two main methods: machine\nlearning and deep learning. The machine learning approach\nis a process with manually designed feature extraction, con-\nverting the PCG signal into specific parameters, followed\nby a process that tunes learning features for a classifier to\ndistinguish various VHD classes [5]–[7]. In contrast, the deep\nlearning approach utilizes end-to-end systems that bypass\nmanual feature extraction, leveraging deep learning-based\nclassifiers to model and predict target classes [8]–[10].\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Summary of studies employing ML, DL, and transfer learning for heart sound classification: predominantly focused on five-class prediction\nusing the Heart Sound Murmur (HSM) Database [62] and two-class prediction (Normal/abnormal) using the PhysioNet/CinC Database [56] and PASCAL\nClassifying Heart Sounds Challenge (PASCAL) [19].\nReference Database Features Classification technique Domain Accuracy (%)\n[5] The HSM database MFCC KNN ML 84.30\nMFCC MLP ML 80.90\nMFCC SVM ML 86.10\nMFCC DNN DL 89.10\nMFCCs based on EMD KNN ML 88.40\nMFCCs based on EMD MLP ML 88.80\nMFCCs based on EMD SVM ML 96.20\nMFCCs based on EMD DNN DL 98.90\n[6] PhysioNet/CinC Statistical, Frequency XGBoost ML 92.90\n[7] PhysioNet/CinC Time, MFCCs and Statistical Neural network (NN) ML 93.33\n[11] PhysioNet/CinC MFCC Decision tree ML 86.40\n[12] PhysioNet/CinC MFCCs NN ML 92.00\n[13] PhysioNet/CinC LPC, Entropy, MFCCs, DWT and PSD NN ML 91.50\n[14] Private MFCC DNN DL 91.12\n[15] PhysioNet/CinC MFCCs DNN DL 93.00\n[16] The HSM database TQWT, EMD and Shannon energy RBF neural networks ML 98.48\n[17] PhysioNet/CinC MFSCs SVM ML 92.00\n[18] PhysioNet/CinC Gram polynomial and Fourier transform NN ML 94.00\n[19] PASCAL DWT Hidden Markov Models ML 92.74\n[20] PhysioNet/CinC Wavelet CNN DL 81.20\n[21] PhysioNet/CinC Modified frequency slice wavelet transform CNN DL 94.00\n[22] PhysioNet/CinC Frequency spectrum, Energy and Entropy SVM ML 88.00\n[23] Private EMD and MFCCs SVM ML 91.00\n[24] MIT heart sounds Frequency SVM ML 98.00\n[25] PhysioNet/CinC Time, MFCC, DWT and Wavelet SVM ML 82.40\n[26] The HSM database FMFE + MFCC SVM ML 99.00\n[27] PhysioNet/CinC Spectral SVM ML 98.00\n[28] The HSM database Time-frequency MCC ML 98.33\n[29] PhysioNet/CinC Cochleagram MLP ML 95.00\n[30] The HSM database Time-frequency magnitude and phase RF ML 95.12\n[31] Private MFCC KNN ML 98.00\n[32] Private EMD KNN ML 94.00\n[33] PASCAL MFCCs KNN ML 97.00\n[34] Private MFCCs KNN ML 92.60\n[37] PhysioNet/CinC MFCCs LSTM DL 98.61\n[38] PhysioNet/CinC Time, Frequency and Time–frequency DNN DL 92.60\n[39] PhysioNet/CinC MFCC+ MFSC 2D-CNN DL 81.50\n[40] PhysioNet/CinC Mean, Standard deviation and Power spectrum CNN DL 86.02\n[41] PhysioNet/CinC Spectrogram, Mel-spectrogram and MFCCs CNN DL 86.05\n[42] The HSM database Normalized signals CNN DL 97.00\n[43] PhysioNet/CinC Spectrograms 2D-CNN DL 97.05\n[44] PhysioNet/CinC - 1D-CNN DL 93.28\n[45] Private - 1D-CNN DL 93.56\n[46] PhysioNet/CinC Spectrograms 1D-CNN DL 96.48\n[47] The HSM database Log-mel spectrogram LSTM DL 95.00\nLog-mel spectrogram CNN DL 99.67\n[48] PhysioNet/CinC LPCC and MFCC SFF-HLSTM DL 99.10\nLPCC and MFCC PFF-HLSTM DL 98.71\n[52] PASCAL MFCC 2D-CNN + transfer learning DL 89.50\n[55] The HSM database FDPCT VGG16-based transfer learning DL 94.00\nFDPCT ResNet-50-based transfer learning DL 98.00\nFDPCT Deep CNN DL 99.48\nResearch on VHD detection mostly focuses on conven-\ntional pipeline approaches. These studies have explored the\nuse of efficient hand-crafted feature extraction techniques in\ncombination with effective classifiers. Several methods have\nbeen proposed for the detection of VHD, including Mel-\nFrequency Cepstral Coefficients (MFCCs) [11]–[15], Tun-\nable Q-factor Wavelet Transform (TQWT) [16], Mel Fre-\nquency Spectral Coefficients (MFSCs) [17], Gram polyno-\nmial [18] and Wavelet Transform (WT) [19]–[21]. In ad-\ndition, the study examined various machine learning clas-\nsifiers, including the support vector machine (SVM) [22]–\n[27], multiclass composite classifiers (MCC) [28], Multi-\nlayer Perceptron (MLP) [29], Random Forest (RF) [30], and\nk-Nearest Neighbor (k-NN) [31]–[34]. The effectiveness of\nhand-crafted feature extraction for classification is a signifi-\ncant part of these conventional approaches, demonstrating the\nneed for expertise, e.g., speech processing tasks [35].\nAccording to previous research, deep learning models have\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ndemonstrated outstanding accuracy in detecting acoustic us-\ning complicated pattern recognition techniques [36]–[46]. As\nreported by [5], the utilization of MFCC in Deep Neural\nNetworks (DNNs) has proven to be more effective in de-\ntecting VHD than SVM, k-NN, and MLP classifiers. This\nsuperiority can be attributed to the DNN’s inherent ability to\nindependently extract hierarchical features from the MFCC.\nThe utilization of Long Short-Term Memory (LSTM) and\nConvolutional Neural Network (CNN) models combined with\na log-mel spectrogram has represented encouraging outcomes\nin detecting VHD [47]. Furthermore, a study by [48] proposed\nusing hierarchical LSTM networks that incorporate parallel\nand series feature fusion to detect VHD through PCG signals.\nThe study represents performance improvements when using\nMFCC and Linear Prediction Cepstral Coefficients (LPCC)\nas features. Nevertheless, the effectiveness of these deep\nlearning classifiers is significantly influenced by the volume\nof training data [49].\nTransfer learning-based classifiers have become more pop-\nular in many applications in recent times [50]–[52]. This\ntechnique leverages knowledge acquired from models trained\non large datasets, which enhances data utilization efficiency\nand accelerates the training process [53]. According to [54],\nthe approach enables models to achieve outstanding perfor-\nmance even when trained on a small amount of task-specific\ndata, reducing the risk of overfitting. In addition, it enables\nthe adaption of models from one domain to another one,\nresulting in enhanced performance in closely related domains.\nThe studies operated by [55] focused on detecting VHD,\nwhich presented transfer learning models such as ResNet50\nand VGGNet-16. These models were trained using Time-\nFrequency (TF) images derived from PCG signals, which\nresults showed a promise in VHD classification. However,\ntransfer learning may encounter difficulties when a substan-\ntial disparity exists between the source and target domains.\nThis discrepancy necessitates the implementation of supple-\nmentary procedures to tackle these obstacles effectively. A\ndetailed compilation of studies employing ML, deep learning,\nand transfer learning for the classification of heart sounds\nis summarized in Table 1, providing the reader with a more\nnuanced understanding of the field.\nIn this study, we explore the usefulness of the Data-\nEfficient Image Transformer (DeiT) model [57] in the con-\ntext of heart sound classification using transfer learning.\nThe inherent capabilities of DeiT include hierarchical feature\nextraction, and attention mechanisms, which help identify\nabnormalities in heart sounds. As a result, it is anticipated\nto be a promising candidate for detecting VHD. Although\nthe architecture of DeiT models demonstrates exceptional\nperformance in image domains, it is important to note that\nthese models may not be fully optimized for the complexities\nassociated with audio data. Therefore, it is imperative to\nenhance and modify the model to capture the intricacies of\nthe heart more accurately.\nIn order to improve the classification efficacy of transfer\nlearning based on DeiT, we propose integrating a hybrid\nConvolution-DeiT (Conv-DeiT) model. In order to enhance\nchannel and spatial information before the VHD detection\nphase, we integrate a convolutional block and a squeeze-and-\nexcitation (SE) attention mechanism into the DeiT frame-\nwork. The present study provides the following contributions:\n1) We explored the use of DeiT-based transfer learning for\nVHD detection. Specifically, we used three channels\nof the MFCC together with their corresponding delta\nand double delta coefficients, which are obtained from\nthe original one-dimensional utterances. This approach\nenables us to use pre-trained DeiT models from image\ndatasets.\n2) We proposed a hybrid Conv-DeiT model to improve\nthe classification efficacy of transfer learning based on\nDeiT. This model integrates a convolutional block and\na SE-attention mechanism into the DeiT framework. It\nhelps enhance the quality of channel and spatial input\nfeature information before the DeiT and classification\nprocess.\n3) In this study, we represented the capability of a DeiT\nmodel in the context of VHD detection, which was\ninitially developed for image-based tasks. Moreover,\nthe proposed Conv-DeiT technique enhanced VHD de-\ntection accuracy by integrating the convolutional block\nand SE-attention mechanism.\nThe subsequent sections of the paper are organized in the\nfollowing manner: The proposed approach is described in\nSection II. The experimental setup and results are discussed\nin Sections III and IV, respectively. The last part provides\ncommentary and suggests potential approaches for future\nresearch.\nII. PROPOSED METHOD\nThe proposed methodology for the detection of VHD relies\non a framework of transfer learning. Initially, the audio sig-\nnal undergoes a feature extraction phase where MFCCs and\ntheir derivatives are computed to capture both spectral and\ntemporal dynamics. These features are then transformed to\nconform to an image-like input suitable for our adapted Deep\nImage DeiT model, respecting its design for handling two-\ndimensional data structures. We have introduced a convolu-\ntional module consisting of parallel convolutional layers with\nself-attention mechanisms to augment the feature extraction\nprocess. This module is specifically engineered to adapt the\nspectral features of audio signals for the DeiT model, enabling\nit to leverage its spatial pattern recognition capabilities. The\ncohesive interplay and stepwise progression of these compo-\nnents are succinctly illustrated in Fig. 1. Furthermore, this\nsection delves into the specific loss function employed in our\napproach. The following subsections provide a more granular\nbreakdown of each component:\nA. TRANSFORMING AUDIO DATA FOR DEIT MODEL\nThe adaptation of the DeiT model from image to audio\nprocessing presents unique domain-specific challenges due\nto the fundamental differences between visual and auditory\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1. Overall visualization of our proposed architecture. (a) image-like feature extraction, (b) convolution module, and (c) classifier.\ninformation representation. While DeiT excels in identify-\ning patterns within the spatial domain of images, audio sig-\nnals require an interpretation of patterns over both time and\nfrequency domains. To address this, we transformed one-\ndimensional audio signals, denoted as S into into a three-\ndimensional structure matching a 3x224x224 image that par-\nallel the two-dimensional nature of images, allowing us to\nleverage DeiT’s powerful spatial pattern recognition capabil-\nities. In order to achieve this, the MFCCs are employed.\nMFCC = F(S) (1)\nMFCCs provide an audio representation that closely re-\nsembles the auditory perception of humans. The procedure\nentails dividing the audio into brief, overlapping segments,\nconverting these segments into a frequency spectrum, and\nsubsequently enhancing the frequencies that are most relevant\nto human auditory perception by employing the Mel scale.\nThis concept is further enhanced by employing the Discrete\nCosine Transform. Further details of the MFCC extraction\ncan be seen in [58].\nThe temporal variations included in the MFCCs are effec-\ntively represented by incorporating the delta and double delta\ncoefficients.\n∆[t] =\nNX\nn=1\nn(MFCC[t + n] − MFCC[t − n]) (2)\n∆2[t] =\nNX\nn=1\nn(∆[t + n] − ∆[t − n]) (3)\nwhere N represents the number of adjacent frames taken into\naccount. Meanwhile, n and t stand for the frame’s index and\nthe current time frame.\nThe delta and double delta coefficients [59] adeptly en-\ncapsulate the advancement and intensification of auditory at-\ntributes. The MFCCs, along with their first-order derivatives\n(deltas), and their second-order derivatives (double deltas)\nhave a structural resemblance to the Red-Green-Blue (RGB)\nchannels found in images. After they are resized, this struc-\ntural similarity enables the representation of audio signals in\nthe format of 3X224X224, which is specifically designed to\nsuit the DeiT model. This format optimization enhances the\nDeiT model’s capability to detect VHD signals from audio\ndata.\nB. CONVOLUTION MODULE\nRelying solely on existing image-based models may not fully\nencompass the diverse range of audio intricacies. In order\nto address this gap, we suggest the implementation of a\nhybrid convolution-transfer learning model that integrates a\nconvolutional block with the SE attention mechanism.\nThe convolutional block, responsible for processing the\nMFCCs, is formally described as:\nCoutput = Conv(MFCC) (4)\nThe module consists of three separate convolutional\nbranches, each utilizing a kernel size of 3x3. The primary\ngoal of the design is to extract channel features that are\nspecific, while also preserving the original dimensions of the\nspectrogram. Additionally, the design attempts to improve the\nrepresentation of features, even when the available data is low,\nwithout compromising computational efficiency.\nThe subsequent stage of the neural network architecture,\nknown as the SE block, further enhances the results obtained\nby the convolution module:\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nSEoutput = SE(Coutput) (5)\nAs indicated by Equation 5, this block adaptively tailors\nchannel dependencies and accentuates key features, in accor-\ndance with the findings of [60].\nC. DEIT-BASED CLASSIFIER\nThe Vision Transformer (ViT) adapts the transformer archi-\ntecture, initially developed for natural language processing\n(NLP) tasks [61], to effectively process image data. The\npower of DeiT resides in its ability to effectively merge\ndata efficiency with the transformer model’s proficient pat-\ntern recognition skills. Precision in distinguishing tiny heart\nsounds is of utmost importance in jobs such as VHD iden-\ntification. In contrast to traditional transfer learning meth-\nods that largely depend on extensive labeled datasets, DeiT\nemploys a distinctive distillation token to facilitate efficient\nlearning even in scenarios with low data availability. This\nmethodology offers the potential for expedited convergence\nand improved performance, hence conferring a competitive\nedge over conventional models such as CNN [57]. The main\noperation of the ViT is described as post-processing by the\nself-attention mechanism module:\nViToutput = ViT(SEoutput) (6)\nThe inclusion of a \"classification token\" is a crucial com-\nponent in conventional ViT architectures. Nevertheless, the\nabsence of spatially-focused layers in Vision Transformers\nnecessitates a substantial amount of pre-training in order to\nachieve comparable performance to CNN. In response to this\nparticular difficulty, DeiT proposes the implementation of a\ndistillation token. This token serves the purpose of simulating\nlabel predictions, under the guidance of a mentor model.\nD. LOSS FUNCTION\nThe culmination of the process is the evaluation of loss func-\ntion. Using the output from the Vision Transformer and the\nactual labels, the loss is computed as:\nL = L(ViToutput, y) (7)\nwhere y denotes the true label.\nIn essence, the end-to-end relationship, commencing from\nthe raw audio signal and culminating in the computed loss, is\nencapsulated by:\nL = L(ViT(SE(Conv(F(S)))), y) (8)\nThis relationship offers a comprehensive perspective on\nthe process of data travel inside the system, outlining the\nprogression from raw audio to the ultimate assessment of loss.\nIII. EXPERIMENT SETUP\nA. DATABASE\nThe Heart Sound Murmur (HSM) database [62] was utilized\nin this investigation. The database consists of 1000 samples\nof phonocardiogram (PCG), which have been formatted as\n.wav audio files. The recordings are characterized by a single-\nchannel configuration, featuring a bit-depth of 16 bits per\nsample and a sampling rate of 8000 Hz. The collected samples\nencompass five distinct categories, namely Aortic Stenosis\n(AS), Mitral Regurgitation (MR), Mitral Stenosis (MS), Mi-\ntral Valve Prolapse (MVP), and Normal (N). Further details\non the datasets can be found in Table 2.\nTABLE 2. Detail of the HSM database.\nCardiac Disorder No. of Sample\nAS 200\nMR 200\nMS 200\nMVP 200\nN 200\nTotal 1,000\nB. FEATURE EXTRACTION\nIn this paper, the MFCCs were utilized as the principal mag-\nnitude characteristic for the input of our transformer model.\nThe computation of these coefficients was performed using\na frame length of 20 ms, with a 50% overlap, and the ap-\nplication of a Hamming window to each frame. This process\nyielded a 38-dimensional MFCC. In order to capture the tem-\nporal dynamics between audio frames, both delta and double-\ndelta coefficients of MFCC were computed. According to the\nstudy conducted by [47], a predetermined segment duration\nof 2 seconds was employed. This duration was determined\nto capture a greater quantity of information compared to\nsegments of 1 second or 1.5 seconds, based on empirical\nexperimentation. The MFCC data, together with its delta and\ndouble delta, was reshaped into dimensions of 3X224X224\n(representing channels, height, and width). This reshaping\nwas done to ensure compliance with the pre-trained DeiT\nmodel, which is specifically designed for image datasets.\nC. NETWORK TRAINING\nThe models for evaluation were constructed via the PyTorch\nv1.10.1 framework and conducted training on an NVIDIA\nRTX3090 GPU equipped with 24 GB of RAM. The training\nparameters are presented in Table 3.\nA comprehensive set of experiments was conducted to\nassess the effects of various factors, including different learn-\ning rates and batch sizes. Nevertheless, these modifications\ndid not enhance the detection performance. This indicates\nthat adjusting parameters did not yield positive results for\nour Conv-DeiT approach or other similar transfer learning\nclassifiers. The selected values showed a desirable balance\nbetween computing requirements and the effectiveness of the\nmodel.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 3. Model Parameters OF Conv-DeiT\nParameters Conv-DeiT\nBatch Size 64\nLearning Rate 0.001\nEpochs 100\nOptimizer ADAM\nLoss Cross-entropy\nOutput Activation SoftMax\nHidden Layers 128-5\nHidden Layer Activation ReLU\nD. PERFORMANCE CRITERIA\nFor classifier performance evaluation, prevalent matrices\nwere adopted as suggested in [47]: precision (Pr), recall (Re),\nF1-score (F1), and accuracy (Acc). These matrices are deter-\nmined by:\nPr = TP\nTP + FP (9)\nRe = TP\nTP + FN (10)\nF1 = 2 × Pr × Re\nPr + Re (11)\nAcc = TP + TN\nTP + TN + FP + FN (12)\nHere, TP, TN, FP, and FN represent true positive, true\nnegative, false positive, and false negative values respectively.\nIV. RESULTS AND DISCUSSIONS\nA. RESULT BASED ON THE PROPOSED METHODS\nIn this study, we analyze the performance of DeiT trans-\nfer learning by the utilization of various experimental ap-\nproaches, which include:\n• DeiT-Small: The present methodology employs the\nDeiT-Small model for the purpose of transfer learn-\ning, whereby features are extracted directly without the\nutilization of the Convolutional block. The DeiT-Small\nmodel is designed with 384 embedding dimensions,\nwhich determines the size of the hidden vector rep-\nresentations in the transformer model. Additionally, it\nemploys 12 attention heads to effectively identify var-\nious patterns and correlations within the input data. The\nparameters for the DeiT-Small model is outlined in Table\n3.\n• DeiT-Base:The approach described in this study utilizes\nthe DeiT-Base model for transfer learning by directly\nextracting features, similar to the DeiT-Small method,\nwithout making any modifications to the Convolutional\nblock. The DeiT-Base model is notable for its utilization\nof 768 embedding dimensions and 6 attention heads.\nThe parameters for the DeiT-Base model are outlined in\nTable 3.\n• Conv-DeiT w/o att:The proposed approach incorpo-\nrates the DeiT model-Base and the Conv block for trans-\nfer learning, while dropping the SE attention mecha-\nnism. The Conv-DeiT model without attention has the\nsame configuration parameters as the DeiT-Base model.\n• Conv-DeiT: This framework embraces the proposed\nstructure depicted in Fig 1. The optimal parameters for\nthe Conv-DeiT method are outlined in Table 3.\nA comprehensive analysis of the performances of DeiT-\nsmall, DeiT-base, Conv-DeiT w/o att, and Conv-DeiT is pre-\nsented in Table 4. The comparison is conducted throughout\nall five folds and encompasses many performance indicators.\nThis complete perspective facilitates a thorough comprehen-\nsion of the outcomes.\nAs depicted in Table 4, a comparative analysis was con-\nducted on two variations of DeiT-based transfer learning\ntechniques, distinguished by their respective parameter con-\nfigurations. The results indicate that DeiT-base, which has\n768 embedding dimensions and 6 heads, exhibited superior\nperformance compared to DeiT-small, which has 384 embed-\nding dimensions and 12 heads. The enhanced detection per-\nformance of DeiT-base can be attributed to its more intricate\nand resilient embedding representation. Therefore, the DeiT-\nbase model is selected as the reference point for subsequent\ncomparisons, either in conjunction with the Conv block or the\nConv block with the SE-attention mechanism.\nThe Conv-DeiT approach, when implemented without at-\ntention, demonstrates greater performance compared to the\nDeiT method. The inclusion of the Conv block appears to\nplay a significant role in enhancing the performance, as it\noffers more discernible information compared to the DeiT\ntechnique in the absence of the Conv block. As a result, the\nutilization of the Conv block in combination with the SE-\nattention mechanism leads to a more advanced embedding\nrepresentation. Adding the attention mechanism to Conv-\nDeiT may allow it to outperform its version without attention.\nAfter that, we proceeded to examine the statistical signifi-\ncance of the embedding features obtained from the flattened\nlayers of DeiT-Small, DeiT-Base, Conv-DeiT w/o att, and\nConv-DeiT. We utilised the Multivariate Analysis of Variance\n(MANOV A) technique [63] to assess the participant’s capac-\nity to differentiate among several categories of VHD. The\nWilk’s Λ metric, Pillai’s trace, Hotelling-Lawley trace, and\nthe F-value were essential in this assessment. Lower values\nof Wilk’s Λ suggest greater statistical significance in distin-\nguishing across categories of VHD. In contrast, larger values\nof the Pillai’s trace, Hotelling-Lawley trace, and F-value indi-\ncate a higher level of statistical significance in distinguishing\nacross VHD groups. The embedding features utilized in this\ninvestigation were derived from the initial experimental itera-\ntion. For this analysis, we used t-distributed Stochastic Neigh-\nbor Embedding (t-SNE) [64] to reduce the dimensionality of\nembedding features, crucial for visualizing and understanding\nthe data. This step was vital to assess participants’ ability to\ndifferentiate between VHD categories effectively. The sta-\ntistical significance of the models DeiT-Small, DeiT-Base,\nConv-DeiT w/o att, and Conv-DeiT is presented in Table 5.\nOur results underscore the Conv-DeiT model’s distinct advan-\ntage in multiple statistical metrics, including Wilk’s Lambda\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. Classification results for different models.\nClassification scheme Class Pr (%) Re (%) F1(%) Acc (%)\nDeiT-Small N 100.00 97.93 98.95 99.60\nMVP 91.54 89.51 90.38 96.10\nMR 94.24 86.93 90.35 96.80\nAS 90.77 95.92 93.16 97.30\nMS 91.06 96.83 93.72 97.40\nAvg 93.52 ± 3.87 93.43 ± 4.89 93.31 ± 3.51 97.44 ± 1.31\nDeiT-Base N 97.47 99.01 98.22 99.30\nMVP 92.03 92.22 92.08 96.80\nMR 95.65 90.59 92.99 97.60\nAS 96.91 95.60 96.22 98.60\nMS 90.79 94.90 92.71 97.10\nAvg 94.57 ± 2.99 94.46 ± 3.25 94.44 ± 2.65 97.88 ± 1.05\nConv-DeiT w/o att N 99.41 99.53 99.47 99.80\nMVP 91.42 93.46 92.24 96.80\nMR 95.32 92.46 93.79 98.00\nAS 97.19 97.48 97.30 98.90\nMS 94.02 94.74 94.21 97.70\nAvg 95.47 ± 3.04 95.53 ± 2.92 95.40 ± 2.92 98.24 ± 1.15\nConv-DeiT N 100.00 100.00 100.00 100.00\nMVP 97.13 98.49 97.80 99.10\nMR 97.50 97.59 97.52 99.20\nAS 99.02 98.67 98.83 99.50\nMS 98.96 97.93 98.42 99.40\nAvg 98.52 ± 1.18 98.54 ± 0.92 98.51 ± 0.98 99.44 ± 0.35\nTABLE 5. Statistical significance test of DeiT-Small, DeiT-Base, Conv-DeiT w/o att, and Conv-DeiT using MANOVA (p-value < 0.05)\nModels Wilks’ Λ Pillai’s trace Hotelling-Lawley trace F-Value\nDeTi-Small 0.91 0.09 0.09 9.38\nDeTi-Base 0.87 0.13 0.15 14.93\nConv-DeiT w/o att 0.82 0.18 0.22 21.55\nConv-DeiT 0.81 0.19 0.24 23.54\n(Λ), Pillai’s trace, Hotelling-Lawley trace, and the F-value.\nThe model achieved the lowest Wilk’s Lambda at 0.81 and\nthe highest F-value at 23.54, indicating robust discriminative\npower in classifying various VHD categories, as detailed in\nTable 5. The Conv-DeiT model also recorded the highest Pil-\nlai’s trace at 0.19 and Hotelling-Lawley trace at 0.24, further\nconfirming its superior performance over the other models\nevaluated. These higher values reflect the model’s enhanced\nsensitivity and accuracy in detecting differences among VHD\ngroups. The integration of Squeeze-and-Excitation (SE) at-\ntention within the Conv-DeiT framework is instrumental\nto this performance, dynamically recalibrating channel-wise\nfeature responses, which significantly improves the model’s\nfocus on relevant features for VHD detection. This strategic\nfusion of the DeiT architecture’s global contextual aware-\nness with SE attention’s channel-specific refinement leads\nto precise and discerning feature representations. The Conv-\nDeiT model’s superior statistical measures demonstrate the\nefficacy of this approach, marking a significant advancement\nin deep learning for medical imaging and specifically in the\ncomplex task of VHD classification. An in-depth technical\nexposition on the Conv-DeiT model’s architecture and the\nfunctional integration of SE attention, which is foundational\nto the enhanced performance, is available in the supplemen-\ntary materials.\nIn order to examine the distributions among VHDs and\nvisualize the discriminatory information based on the Conv-\nDeit feature representation for VHD classification, we em-\nployed t-SNE, a well-known technique for dimensionality\nreduction. In this case, the outcome of the initial fold was\nselected. Fig. 2 presents the graphical representation of the\nflattened features obtained from the Conv-Deit model that\nunderwent training.\nAs depicted in Fig 2(a), the data distributions of distinct\ngroups utilizing unprocessed speech samples exhibited sub-\nstantial overlap. This posed a difficulty in distinguishing\namong various forms of VHD. However, it is evident from\nFig 2(b) that the feature derived from the Conv-Deit model,\nafter training, exhibited enhanced performance in comparison\nto utilizing unprocessed voice samples. The object had dis-\ntinct outlines and shorter distances among different classes,\nsuggesting its efficacy in discerning among various types of\nVHD. The findings of this study indicate that the spatial-\ntemporal characteristic derived from the Conv-Deit model\nmay hold significant value in the detection of VHD.\nB. COMPARISON WITH SOME KNOWN SYSTEMS\nIn this subsection, we evaluate the effectiveness of our sug-\ngested approaches by comparing them to established systems.\nAs emphasized in the opening, conversations may circumvent\nspecific frameworks when their empirical foundations deviate\nfrom our prescribed repository. The primary emphasis of our\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Visual distributions of different features based on t-SNE. (a) raw speech signals, (b) flatten features derived from Conv-DeiT model.\nstudy is on the data obtained from the HSM database, which\nis consistent with the experimental framework in which we\nhave established. Table 1 presents a comparison of the results\nobtained from different well-known systems in relation to our\nproposed technique.\nAs shown in Table 6, it is apparent that Conv-DeiT demon-\nstrates superior performance compared to other established\nsystems when evaluated on the HSM database. Nevertheless,\nthe performance of the aforementioned model did not exceed\nthat of the classifier based on Convolutional Neural Networks\n(CNN) utilizing the log-mel spectrogram feature, as reported\nby [47]. One plausible explanation may lie in the process\nof self-selection of training and testing datasets. However,\nthe outcomes of our study are based on a five-fold cross-\nvalidation approach, which enhances the reliability of the\nclassification performance of the offered approaches. Further-\nmore, it is worth noting that Conv-DeiT did not achieve supe-\nrior performance compared to the deep CNN classifier while\nutilizing the FDPCT feature. The aforementioned statement\nhighlights the importance of FDPCT’s capacity to effectively\nanalyze non-stationary signals, specifically those found in\nPCG signals. The combination of FDPCT with deep learning\nhas been found to improve the accuracy of VHD identifi-\ncation. While our Conv-DeiT model, designed to enhance\nDeiT, showed a marginal performance drop of 0.04%, it still\npresents a viable approach for VHD detection.\nV. CONCLUSION\nIn this paper, we propose identifying VHD by employing\ntransfer learning methods that take advantage of pre-trained\ntransformer models based on image data. The DeiT model,\ninitially pre-trained on image datasets, was harnessed for its\ninherent capabilities. This strategy achieved a notable overall\naccuracy of 97.44% in the classification of heart sounds. Sub-\nsequent enhancements led to the proposal of the Conv-DeiT\napproach, a hybrid architecture that integrates a convolutional\nblock, an SE attention mechanism, and the DeiT process. This\nmethod exhibited superior performance compared to the stan-\ndalone DeiT-based transfer learning, reaching an exceptional\noverall performance of 99.44%. The study has indicated the\npotential of DeiT-based transfer learning and the efficacy\nof using models pre-trained on a distinct modality, such as\nimages, for classifying heart sounds. Moreover, our hybrid\nConv-DeiT method, which combines the convolutional block\nand the SE-attention mechanism has demonstrated significant\nadvantages in this context.\nIn future work, we aim to investigate other attention mech-\nanisms to further refine our proposed methods. We also plan\nto incorporate multi-scale convolutional neural networks [35]\nand phase information [49], [65] as supplementary data to\nenhance our methodologies.\nACKNOWLEDGMENT\nThis work was supported by (i) Suranaree University of Tech-\nnology (SUT), (ii) Thailand Science Research and Innova-\ntion (TSRI), (iii) National Science, Research and Innovation\nFund (NSRF) through NRIIS under Grant 179284, and (iv)\nthe NSRF via the Program Management Unit for Human\nResources & Institutional Development, Research and Inno-\nvation (PMU-B) (grant number B13F660067)\nREFERENCES\n[1] J. S. Aluru, A. Barsouk, K. Saginala, P. Rawla, and A. Barsouk, ‘‘Valvular\nheart disease epidemiology,’’ Med. Sci., vol. 10, no. 2, pp. 32, 2022.\n[2] C. W. Tsao et al., ‘‘Heart disease and stroke statistics—2022 update: a\nreport from the American Heart Association,’’ Circulation, vol. 145, no.\n8, pp. e153–e639, 2022.\n[3] J. Chen, S. Sun, L. Zhang, B. Yang, and W. Wang, ‘‘Compressed sensing\nframework for heart sound acquisition in internet of medical things,’’ IEEE\nTrans. Ind. Inform., vol. 18, no. 3, pp. 2000–2009, 2021.\n[4] A. Quinn, J. Kaminsky, A. Adler, S. Eisner, and R. Ovitsh, ‘‘Cardiac\nauscultation lab using a heart sounds auscultation simulation manikin,’’\nMedEdPORTAL, vol. 15, pp. 10839, 2019.\n[5] Ö. Arslan and M. Karhan, ‘‘Effect of Hilbert-Huang transform on clas-\nsification of PCG signals using machine learning,’’ J. King Saud Univ. -\nComput. Inf., vol. 34, no. 10, pp. 9915–9925, 2022.\n[6] V . Arora, R. Leekha, R. Singh, and I. Chana, ‘‘Heart sound classification\nusing machine learning and phonocardiogram,’’ Mod. Phys. Lett. B, vol.\n33, no. 26, pp. 1950321, 2019.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 6. Comparison with some known systems\nReference Feature extraction Classifier Accuracy (%)\n[5] MFCC KNN 84.30\n[5] MFCC MLP 80.90\n[5] MFCC SVM 86.10\n[5] MFCC DNN 89.10\n[5] MFCCs based on EMD KNN 88.40\n[5] MFCCs based on EMD MLP 88.80\n[5] MFCCs based on EMD SVM 96.20\n[5] MFCCs based on EMD DNN 98.90\n[55] FDPCT VGG16-based transfer learning 94.00\n[55] FDPCT ResNet-50-based transfer learning 98.00\n[55] FDPCT Deep CNN 99.48\n[30] Chirplet transform MCC 98.33\n[47] log-mel spectrogram LSTM 95.00\n[47] log-mel spectrogram CNN 99.67\n[48] LPCC, MFCC SFF-HLSTM 99.10\n[48] LPCC, MFCC PFF-HLSTM 98.71\n[66] Tunable Q-factor wavelet transform Deep Wavelet 98.48\nOurs MFCC DeiT-based transfer learning 97.88\nOurs MFCC Conv-DeiT-based transfer learning 99.44\n[7] M. G. M. Milani, P. E. Abas, L. C. De Silva, and N. D. Nanayakkara,\n‘‘Abnormal heart sound classification using phonocardiography signals,’’\nSmart Health, vol. 21, pp. 100194, 2021.\n[8] J. S. Chorba et al., ‘‘Deep learning algorithm for automated cardiac murmur\ndetection via a digital stethoscope platform,’’ J. Am. Heart Assoc., vol. 10,\nno. 9, pp. e019905, 2021.\n[9] Y . A. lssa and A. M. Alqudah, ‘‘A lightweight hybrid deep learning system\nfor cardiac valvular disease classification,’’ Sci. Rep., vol. 12, no. 1, pp.\n14297, 2022.\n[10] P. Jyothi and G. Pradeepini, ‘‘Review on Cardiac Arrhythmia Through\nSegmentation Approaches in Deep Learning,’’ in International Conference\non Intelligent and Smart Computing in Data Analytics, Singapore, 2021,\npp. 139–147.\n[11] A. F. Gündüz and F. TALU, ‘‘PCG Frame Classification by Classical\nMachine Learning Methods Using Spectral Features and MFCC Based\nFeatures,’’EJOSAT, vol. 42, pp. 77–82, 2022.\n[12] M. Nassralla, Z. El Zein, and H. Hajj, ‘‘Classification of normal and abnor-\nmal heart sounds,’’ in 2017 Fourth International Conference on Advances\nin Biomedical Engineering (ICABME), Oct. 2017, pp. 1–4.\n[13] M. Zabihi, A. Bahrami Rad, S. Kiranyaz, M. Gabbouj, A.K. Katsaggelos,\n‘‘Heart sound anomaly and quality detection using ensemble of neural\nnetworks without segmentation,’’ CinC, vol. 43, pp. 613–616, 2016.\n[14] T.-E. Chen, S.-I. Yang, L.-T. Ho, K.-H. Tsai, Y .-H. Chen, Y .-F. Chang, et\nal., ‘‘S1 and S2 heart sound recognition using deep neural networks,’’ IEEE\nTrans Biomed Eng, vol. 64, no. 2, pp. 372–380, 2016.\n[15] X. Bao, Y . Xu, and E. N. Kamavuako, ‘‘The effect of signal duration on the\nclassification of heart sounds: A deep learning approach,’’ Sensors, vol. 22,\nno. 6, pp. 2261, 2022.\n[16] W. Zeng, Z. Lin, C. Yuan, Q. Wang, F. Liu, and Y . Wang, ‘‘Detection\nof heart valve disorders from PCG signals using TQWT, FA-MVEMD,\nShannon energy envelope and deterministic learning,’’ Artif. Intell. Rev.,\nvol. 54, pp. 1–38, 2021.\n[17] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y . M. Kadah, ‘‘Classification\nof heart sounds using fractional fourier transform based mel-frequency\nspectral coefficients and traditional classifiers,’’ in Biomed. Signal Process.\nControl, vol. 57, pp. 101788, 2020.\n[18] F. Beritelli, G. Capizzi, G. Lo Sciuto, C. Napoli, and F. Scaglione, ‘‘Auto-\nmatic heart activity diagnosis based on Gram polynomials and probabilistic\nneural networks,’’Biomedical Engineering Letters, vol. 8, pp. 77–85, 2018.\n[19] R. Touahria, A. H. Gharbi and P. Ravier, ‘‘Discrete Wavelet based Fea-\ntures for PCG Signal Classification using Hidden Markov Models,’’ in\nICPRAM., pp. 334–340, 2021.\n[20] M. Tschannen, T. Kramer, G. Marti, M. Heinzmann, and T. Wiatowski,\n‘‘Heart sound classification using deep structured features,’’ in Computing\nin Cardiology Conference, vol. 152, pp. 565–568, 2016.\n[21] Y . Chen, S. Wei, and Y . Zhang, ‘‘Classification of heart sounds based\non the combination of the modified frequency wavelet transform and\nconvolutional neural network,’’ Med Biol Eng Comput, vol. 58, no. 9, pp.\n2039–2047, 2020.\n[22] H. Tang, Z. Dai, Y . Jiang, T. Li, and C. Liu, ‘‘PCG classification using\nmultidomain features and SVM classifier,’’ Biomed Res. Int., vol. 2018,\n2018.\n[23] M. U. Khan, S. Aziz, K. Iqtidar, G. F. Zaher, S. Alghamdi, and M. Gull,\n‘‘A two-stage classification model integrating feature fusion for coronary\nartery disease detection and classification,’’ Multimedia Tools and Appli-\ncations, pp. 1–30, 2022, Springer.\n[24] T. Indu, A. Prakash, S.R. Chandran, N. Babu, and R. Soorya, ‘‘Comparison\nof different machine learning algorithms for cardiac auscultation,’’ in\n2022 IEEE International Conference on Signal Processing, Informatics,\nCommunication and Energy Systems (SPICES), vol. 1, IEEE, 2022, pp.\n113–117.\n[25] J.J.G. Ortiz, C.P. Phoo, and J. Wiens, ‘‘Heart sound classification based\non temporal alignment techniques,’’ in 2016 Computing in Cardiology\nConference (CinC), IEEE, 2016, pp. 589–592.\n[26] W. Yang, J. Xu, J. Xiang, Z. Yan, H. Zhou, B. Wen, H. Kong, R. Zhu,\nand W. Li, ‘‘Diagnosis of cardiac abnormalities based on phonocardiogram\nusing a novel fuzzy matching feature extraction method,’’ BMC Medical\nInformatics and Decision Making, vol. 22, no. 1, pp. 1–13, 2022.\n[27] D. S. Panah, A. Hines, and S. Mckeever, ‘‘Exploring Composite Dataset\nBiases for Heart Sound Classification,’’ in AICS, 2020, pp. 145–156.\n[28] S. K. Ghosh, R. N. Ponnalagu, R. K. Tripathy, U. R. Acharya, ‘‘Automated\ndetection of heart valve diseases using chirplet transform and multiclass\ncomposite classifier with PCG signals,’’ Comput. Biol. Med., vol. 118, pp.\n103632, 2020.\n[29] S. K. Ghosh, R. K. Tripathy, R. N. Ponnalagu, and R. B. Pachori, ‘‘Su-\npervised model for Cochleagram feature based fundamental heart sound\nidentification,’’Biomed. Signal Process. Control, vol. 52, pp. 32–40, 2019.\n[30] S. K. Ghosh, R. K. Tripathy, R. N. Ponnalagu, and R. B. Pachori, ‘‘Auto-\nmated detection of heart valve disorders from the PCG signal using time-\nfrequency magnitude and phase features,’’ IEEE Sens. Lett., vol. 3, no. 12,\npp. 1–4, 2019.\n[31] A. F. Q. Manrique, J. I. Godino-Llorente, M. Blanco-Velasco, and G.\nCastellanos-Dominguez, ‘‘Selection of dynamic features based on time–\nfrequency representations for heart murmur detection from phonocardio-\ngraphic signals,’’ Ann. Biomed. Eng., vol. 38, pp. 118–137, 2010.\n[32] U. Riaz, S. Aziz, M. U. Khan, S. A. A. Zaidi, M. Ukasha, and A. Rashid,\n‘‘A novel embedded system design for the detection and classification of\ncardiac disorders,’’ Computational Intelligence, vol. 37, no. 4, pp. 1844–\n1864, 2021.\n[33] O. El Badlaoui, A. Benba, and A. Hammouch, ‘‘Novel PCG analysis\nmethod for discriminating between abnormal and normal heart sounds,’’\nIRBM, vol. 41, no. 4, pp. 223–228, 2020.\n[34] M. S. Ahmad, J. Mir, M. O. Ullah, M. L. U. R. Shahid, and M. A.\nSyed, ‘‘An efficient heart murmur recognition and cardiovascular disorders\nclassification system,’’ Australasian Physical & Engineering Sciences in\nMedicine, vol. 42, pp. 733–743, 2019.\n[35] W. Pathonsuwan, K. Phapatanaburi, P. Buayai, T. Jumphoo, P. Anchuen,\nM. Uthansakul, and P. Uthansakul, ‘‘RS-MSConvNet: A Novel End-to-End\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nPathological V oice Detection Model,’’ IEEE Access, vol. 10, pp. 120450–\n120461, 2022.\n[36] G. Hinton et al., ‘‘Deep neural networks for acoustic modeling in speech\nrecognition: The shared views of four research groups,’’ IEEE Signal\nProcess. Mag., vol. 29, no. 6, pp. 82–97, 2012.\n[37] S. Latif, M. Usman, R. Rana, and J. Qadir, ‘‘Phonocardiographic sensing\nusing deep learning for abnormal heartbeat detection,’’ IEEE Sens. J., vol.\n18, no. 22, pp. 9393–9400, 2018.\n[38] M. Sotaquirá, D. Alvear, and M. Mondragón, ‘‘Phonocardiogram classifi-\ncation using deep neural networks and weighted probability comparisons,’’\nJ. Med. Eng. Technol., vol. 42, no. 7, pp. 510–517, 2018.\n[39] B. Bozkurt, I. Germanakis, Y . Stylianou, ‘‘A study of time-frequency\nfeatures for CNN-based automatic heart sound classification for pathology\ndetection,’’Comput. Biol. Med., vol. 100, pp. 132–143, 2018.\n[40] C. Potes, S. Parvaneh, A. Rahman, and B. Conroy, ‘‘Ensemble of feature-\nbased and deep learning-based classifiers for detection of abnormal heart\nsounds,’’ in 2016 Computing in Cardiology Conference (CinC), IEEE,\n2016, pp. 621–624.\n[41] J.M.-T. Wu, M.-H. Tsai, Y .Z. Huang, S.K.H. Islam, M.M. Hassan, A.\nAlelaiwi, G. Fortino, ‘‘Applying an ensemble convolutional neural net-\nwork with Savitzky-Golay filter to construct a phonocardiogram prediction\nmodel,’’Appl. Soft Comput., vol. 78, pp. 29–40, 2019.\n[42] S.L. Oh, V . Jahmunah, C.P. Ooi, R.-S. Tan, E.J. Ciaccio, T. Yamakawa,\nM. Tanabe, M. Kobayashi, and U. Rajendra Acharya, ‘‘Classification of\nheart sound signals using a novel deep WaveNet model,’’ Comput. Methods\nPrograms Biomed., vol. 196, 105604, 2020.\n[43] J. P. Dominguez-Morales, A. F. Jimenez-Fernandez, M. J. Dominguez-\nMorales, and G. Jimenez-Moreno, ‘‘Deep Neural Networks for the Recog-\nnition and Classification of Heart Murmurs Using Neuromorphic Auditory\nSensors,’’IEEE Transactions on Biomedical Circuits and Systems, vol. 12,\nno. 1, pp. 24–34, 2017\n[44] Y . Xu, B. Xiao, X. Bi, W. Li, J. Zhang, and X. Ma, ‘‘Pay more attention\nwith fewer parameters: A novel 1-D convolutional neural network for\nheart sounds classification,’’ in 2018 Computing in Cardiology Conference\n(CinC), vol. 45, IEEE, 2018, pp. 1–4.\n[45] B. Xiao, Y . Xu, X. Bi, W. Li, Z. Ma, J. Zhang, and X. Ma, ‘‘Follow the\nsound of children’s heart: a deep-learning-based computer-aided pediatric\nCHDs diagnosis system,’’ IEEE Internet of Things Journal, vol. 7, no. 3,\npp. 1994–2004, 2019.\n[46] F. Li, M. Liu, Y . Zhao, L. Kong, L. Dong, X. Liu, and M. Hui, ‘‘Feature\nextraction and classification of heart sound using 1D convolutional neural\nnetworks,’’ EURASIP Journal on Advances in Signal Processing, vol.\n2019, no. 1, pp. 1–11, 2019.\n[47] M. T. Nguyen, W. W. Lin, and J. H. Huang, ‘‘Heart Sound Classification\nUsing Deep Learning Techniques Based on Log-mel Spectrogram,’’ Cir-\ncuits, Syst. Signal Process., vol. 42, no. 1, pp. 344–360, 2023.\n[48] S. Das, D. Jyotishi, and S. Dandapat, ‘‘Heart valve diseases detection based\non feature-fusion and hierarchical LSTM network,’’ IEEE Trans. Instrum.\nMeas., vol. 71, pp. 1–11, 2022.\n[49] K. Phapatanaburi, W. Pathonsuwan, L. Wang, P. Anchuen, T. Jumphoo, P.\nBuayai, M. Uthansakul, and P. Uthansakul, ‘‘Whispered speech detection\nusing glottal flow-based features,’’ Symmetry, vol. 14, no. 4, pp. 777, 2022.\n[50] H. E. Kim, A. Cosa-Linan, N. Santhanam, M. Jannesari, M. E. Maros,\nand T. Ganslandt, ‘‘Transfer learning for medical image classification: a\nliterature review,’’BMC Med. Imaging, vol. 22, no. 1, pp. 69, 2022.\n[51] Z. Wan, R. Yang, M. Huang, N. Zeng, and X. Liu, ‘‘A review on transfer\nlearning in EEG signal analysis,’’ Neurocomputing, vol. 421, pp. 1–14,\n2021.\n[52] T. Alafif, M. Boulares, A. Barnawi, T. Alafif, H. Althobaiti, and A.\nAlferaidi, ‘‘Normal and abnormal heart rates recognition using transfer\nlearning,’’ in 2020 12th International Conference on Knowledge and Sys-\ntems Engineering (KSE), 2020, pp. 275–280.\n[53] F. Chollet, ‘‘Xception: Deep learning with depth wise separable convolu-\ntions,’’ in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.,\nHonolulu, HI, USA, 2017, pp. 1251–1258.\n[54] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, ‘‘How transferable are\nfeatures in deep neural networks?,’’ in Adv. Neural Inf. Process., Montreal,\nQuebec, Canada, 2014.\n[55] J. Karhade, S. Dash, S. K. Ghosh, D. K. Dash, and R. K. Tripathy, ‘‘Time–\nfrequency-domain deep learning framework for the automated detection of\nheart valve disorders using PCG signals,’’ IEEE Trans. Instrum. Meas., vol.\n71, pp. 1–11, 2022.\n[56] C. Liu, D. Springer, Q. Li, B. Moody, R. A. Juan, F. J. Chorro, F. Castells,\nJ. M. Roig, I. Silva, and A. E. Johnson, ‘‘An open access database for the\nevaluation of heart sound algorithms,’’ Physiological Meas., vol. 37, no.\n12, pp. 2181–2213, 2016.\n[57] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efficient image transformers & distillation through atten-\ntion,’’ in Int. Conf. Mach. Learn., Virtual, 2021, pp. 10347–10357.\n[58] S. Molau, M. Pitz, R. Schluter, and H. Ney,‘‘Computing mel-frequency\ncepstral coefficients on the power spectrum,’’ in Proc. IEEE Int. Conf.\nAcoust. Speech Signal Process., Salt Lake City, UT, USA, 2001, pp. 73-\n76.\n[59] L. Rabiner, and R. Schafer Theory and applications of digital speech\nprocessing, Prentice Hall Press, NJ, USA, 2010.\n[60] J. Hu, L. Shen, and G. Sun,‘‘Squeeze-and-excitation networks,’’ in Proc.\nIEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Salt Lake City,\nUT, USA, 2018, pp. 7132-7141.\n[61] A. Dosovitskiy et al.,‘‘An image is worth 16x16 words: Transformers for\nimage recognition at scale,’’ in 9th Int. Conf. Learn. Represent., Virtual,\n2021\n[62] Yaseen, G.-Y . Son, and S. Kwon, ‘‘Classification of heart sound signal\nusing multiple features,’’ Appl. Sci., vol. 8, no. 12, pp. 2344, 2018.\n[63] W. J. Krzanowski, Principles of multivariate analysis: a user’s perspec-\ntive., Clarendon, Oxford, 1988.\n[64] J. D. Hunter, ‘‘Matplotlib: A 2D graphics environment,’’ in Comput. Sci.\nEng., vol. 9, no. 03, pp. 90–95, 2007.\n[65] K. Phapatanaburi, P. Buayai, M. Kupimai, and T. Yodrot, ‘‘Linear Predic-\ntion Residual-Based Constant-Q Cepstral Coefficients for Replay Attack\nDetection,’’ in 8th International Electrical Engineering Congress, Chiang\nMai, Thailand, 2020, pp. 1–4.\n[66] W. Zeng, J. Yuan, C. Yuan, Q. Wang, F. Liu, and Y . Wang, ‘‘A new approach\nfor the detection of abnormal heart sound signals using TQWT, VMD and\nneural networks,’’ Artif. Intell. Rev., vol. 54, no. 3, pp. 1613–1647, 2021.\n[67] P. Langley and A. Murray, ‘‘Abnormal heart sounds detected from short\nduration unsegmented phonocardiograms by wavelet entropy,’’ in Comput.\nCardiol., Vancouver, BC, Canada, 2016, pp. 545–548.\nTALIT JUMPHOO received the B.E. degree in\ntelecommunication and electronic engineering and\nthe Ph.D. degree in telecommunication engineer-\ning from the Suranaree University of Technology,\nThailand, in 2014 and 2022, respectively. He is\ncurrently a Postdoctoral Researcher at the School\nof Telecommunication Engineering, Institute of\nEngineering, Suranaree University of Technology.\nHis research interests included biosignal process-\ning, biomedical devices, brain computer interface,\nand applied machine learning.\nKHOMDET PHAPATANABURIreceived the B.E.\ndegree in electronic and telecommunication en-\ngineering and the M.E. degree in electrical engi-\nneering from the Rajamangala University of Tech-\nnology Thanyaburi (RMUTT), Thailand, in 2010\nand 2012, respectively, and the Dr.Eng. degree in\ninformation science and control engineering from\nthe Nagaoka University of Technology (NUT),\nJapan, in 2017. In 2018, he joined the Department\nof Telecommunication Engineering, Rajamangala\nUniversity of Technology Isan, as a Lecturer, and became an Assistant\nProfessor, in 2020. His main research interest includes audio and brainwave\nclassification. During his study in Japan, he received the Monbukagakusho\n(MEXT) Scholarship, from 2014 to 2017. He is a Reviewer for several\ninternational journals including IEEE SIGNAL PROCESSING LETTERS,\nComputer Speech and Language, APSIPA Transactions on Signal and Infor-\nmation Processing, and IEEE ACCESS.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWONGSATHON PATHONSUWANreceived the\nB.E. degree in telecommunication engineering and\nthe M.E. degree in telecommunication and com-\nputer engineering from the Suranaree University of\nTechnology, Thailand, in 2019 and 2021, respec-\ntively, where he is currently pursuing the Ph.D.\ndegree in telecommunication and computer. His\nresearch interests include wireless communication,\nartificial intelligent (AI), machine learning (ML),\nand artificial neural networks (ANN).\nPATIKORN ANCHUEN received the B.E. de-\ngree in telecommunication engineering from the\nKing Mongkut’s Institute of Technology Ladkra-\nbang, Thailand, in 2014, and the M.E. degree in\ntelecommunication engineering and the Ph.D. de-\ngree in telecommunication and computer engineer-\ning from the Suranaree University of Technology,\nThailand, in 2017 and 2020, respectively. He is\ncurrently a Lecturer at the Office of Graduate Stud-\nies, Navaminda Kasatriyadhiraj Royal Air Force\nAcademy, Thailand. His research interests include wireless communication,\nartificial intelligent (AI), machine learning (ML), artificial neural networks\n(ANN), deep reinforcement learning (DRL), genetic algorithm (GA), particle\nswarm optimization (PSO), mobile networks, quality of experience (QoE),\nand 5G communications.\nMONTHIPPA UTHANSAKUL (Member, IEEE)\nreceived the B.E. degree in telecommunication en-\ngineering from the Suranaree University of Tech-\nnology, Thailand, in 1997, and the M.E. degree\nin electrical engineering from Chulalongkorn Uni-\nversity, Thailand, in 1999, and the Ph.D. degree in\ninformation technology and electrical engineering\nfrom The University of Queensland, Australia, in\n2007. She is currently an Associate Professor with\nthe Telecommunication School, Suranaree Univer-\nsity of Technology. Her research interests include wideband/narrowband\nsmart antennas, automatic switch beam antenna, DOA finder, microwave\ncomponents, application of smart antenna, and advance wireless communi-\ncations. She received the Second Prize of Young Scientist Award from 16th\nInternational Conference on Microwaves, Radar, and Wireless Communica-\ntions, Poland, in 2006.\nPEERAPONG UTHANSAKUL (Member, IEEE)\nreceived the B.E. and M.E. degrees in electri-\ncal engineering from Chulalongkorn University,\nBangkok, Thailand, in 1996 and 1998, respec-\ntively, and the Ph.D. degree in information technol-\nogy and electrical engineering from The University\nof Queensland, Brisbane, QLD, Australia, in 2007.\nFrom 1998 to 2001, he was employed as a telecom-\nmunication engineer at one of the leading telecom-\nmunication companies in Thailand. He is currently\nworking as an Associate Professor and the Dean of the Research Department,\nSuranaree University of Technology, Nakhon Ratchasima, Thailand. He has\nmore than 100 research publications and the author/coauthor of various\nbooks related to MIMO technologies. His research interests include green\ncommunications, wave propagation modeling, MIMO, massive MIMO, brain\nwave engineering, OFDM and advanced wireless communications, wireless\nsensor networks, embedded systems, the Internet of Things, and network\nsecurity. He has won various national awards from the Government of\nThailand for his contributions and motivation in the field of science and\ntechnology. Furthermore, he is the Editor of Suranaree Journal of Science\nand Technology and other leading Thailand journals related to science and\ntechnology and Wireless Communications, Poland, in 2006.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357946\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}