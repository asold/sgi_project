{
    "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
    "url": "https://openalex.org/W4387968292",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2955016064",
            "name": "Yunshi Lan",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2068388382",
            "name": "Xiang Li",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2097339281",
            "name": "Xin Liu",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2079581369",
            "name": "Yang Li",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1979915140",
            "name": "Wei Qin",
            "affiliations": [
                "Hefei University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2233383541",
            "name": "Qian Weining",
            "affiliations": [
                "East China Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963644680",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W3176821361",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4287887134",
        "https://openalex.org/W4226452284",
        "https://openalex.org/W4312971273",
        "https://openalex.org/W4293261733",
        "https://openalex.org/W3101703188",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W4386076140",
        "https://openalex.org/W4386075505",
        "https://openalex.org/W3207493267",
        "https://openalex.org/W3034886066",
        "https://openalex.org/W4226271114",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4285300583",
        "https://openalex.org/W4386566765",
        "https://openalex.org/W3172845486",
        "https://openalex.org/W2947312908",
        "https://openalex.org/W2966746916",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W3196731672",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W4385574005",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4220993274",
        "https://openalex.org/W4386065596",
        "https://openalex.org/W4285191490",
        "https://openalex.org/W3199693760",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W3035454069",
        "https://openalex.org/W3114427170",
        "https://openalex.org/W2607303097",
        "https://openalex.org/W1583837637"
    ],
    "abstract": "Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.",
    "full_text": null
}