{
  "title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer",
  "url": "https://openalex.org/W3020482686",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5102309890",
      "name": "Yaru Hao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101751776",
      "name": "Li Dong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014662947",
      "name": "Furu Wei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100665814",
      "name": "Ke Xu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2955572395",
    "https://openalex.org/W3034350582",
    "https://openalex.org/W2885396331",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2963029978",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2950394196",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2949197630",
    "https://openalex.org/W2996507500",
    "https://openalex.org/W2948140294",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2983219369",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2962959086",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2130158090"
  ],
  "abstract": "The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.",
  "full_text": "Self-Attention Attribution:\nInterpreting Information Interactions Inside Transformer\nYaru Hao,12* Li Dong,2 Furu Wei,2 Ke Xu1\n1 Beihang University\n2 Microsoft Research\n{haoyaru@,kexu@nlsde.}buaa.edu.cn\n{lidong1,fuwei}@microsoft.com\nAbstract\nThe great success of Transformer-based models beneﬁts from\nthe powerful multi-head self-attention mechanism, which\nlearns token dependencies and encodes contextual informa-\ntion from the input. Prior work strives to attribute model deci-\nsions to individual input features with different saliency mea-\nsures, but they fail to explain how these input features interact\nwith each other to reach predictions. In this paper, we pro-\npose a self-attention attribution method to interpret the infor-\nmation interactions inside Transformer. We take BERT as an\nexample to conduct extensive studies. Firstly, we apply self-\nattention attribution to identify the important attention heads,\nwhile others can be pruned with marginal performance degra-\ndation. Furthermore, we extract the most salient dependencies\nin each layer to construct an attribution tree, which reveals the\nhierarchical interactions inside Transformer. Finally, we show\nthat the attribution results can be used as adversarial patterns\nto implement non-targeted attacks towards BERT.\n1 Introduction\nTransformer (Vaswani et al. 2017) is one of state-of-the-art\nNLP architectures. For example, most pre-trained language\nmodels (Devlin et al. 2019; Liu et al. 2019; Dong et al. 2019;\nBao et al. 2020; Clark et al. 2020; Conneau et al. 2020; Chi\net al. 2020a,b) choose stacked Transformer as the backbone\nnetwork. Their great success stimulates broad research on\ninterpreting the internal black-box behaviors. Some prior ef-\nforts aim at analyzing the self-attention weights generated\nby Transformer (Clark et al. 2019; Kovaleva et al. 2019). In\ncontrast, some other work argues that self-attention distribu-\ntions are not directly interpretable (Serrano and Smith 2019;\nJain and Wallace 2019; Brunner et al. 2020). Another line\nof work strives to attribute model decisions back to input to-\nkens (Sundararajan, Taly, and Yan 2017; Shrikumar, Green-\nside, and Kundaje 2017; Binder et al. 2016). However, most\nprevious attribution methods fail on revealing the informa-\ntion interactions between the input words and the composi-\ntional structures learnt by the network.\nTo address the above issues, we propose a self-attention\nattribution method (A TTATTR ) based on integrated gradi-\nent (Sundararajan, Taly, and Yan 2017). We conduct ex-\n* Contribution during internship at Microsoft Research.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nperiments for BERT (Devlin et al. 2019) because it is one\nof the most representative Transformer-based models. No-\ntice that our method is general enough, and can be applied\nto other Transformer networks without signiﬁcant modiﬁca-\ntions. Results show that our method well indicates the in-\nformation ﬂow inside Transformer, which makes the self-\nattention mechanism more interpretable.\nFirstly, we identify the most important attention connec-\ntions in each layer using A TTATTR . We ﬁnd that attention\nweights do not always correlate well with their contributions\nto the model prediction. We then introduce a heuristic algo-\nrithm to construct self-attention attribution trees, which dis-\ncovers the information ﬂow inside Transformer. In addition,\na quantitative analysis is applied to justify how much the\nedges of an attribution tree contribute to the ﬁnal prediction.\nNext, we use ATTATTR to identify the most important at-\ntention heads and perform head pruning. The derived algo-\nrithm achieves competitive performance compared with the\nTaylor expansion method (Michel, Levy, and Neubig 2019).\nMoreover, we ﬁnd that the important heads of BERT are\nroughly consistent across different datasets as long as the\ntasks are homogeneous.\nFinally, we extract the interaction patterns that contribute\nmost to the model decision, and use them as adversarial trig-\ngers to attack BERT-based models. We ﬁnd that the ﬁne-\ntuned models tend to over-emphasize some word patterns to\nmake the prediction, which renders the prediction process\nless robust. For example, on the MNLI dataset, adding one\nadversarial pattern into the premise can drop the accuracy\nof entailment from 82.87% to 0.8%. The results show\nthat ATTATTR not only can interpret the model decisions,\nbut also can be used to ﬁnd anomalous patterns from data.\nThe contributions of our work are as follows:\n• We propose to use self-attention attribution to interpret the\ninformation interactions inside Transformer.\n• We conduct extensive studies for BERT. We present how\nto derive interaction trees based on attribution scores,\nwhich visualizes the compositional structures learnt by\nTransformer.\n• We show that the proposed attribution method can be used\nto prune self-attention heads, and construct adversarial\ntriggers.\narXiv:2004.11207v2  [cs.CL]  25 Feb 2021\n[CLS] [CLS]\ni i\ndon don\n' '\nt t\nknow know\num um\ndo do\nyou you\ndo do\na a\nlot lot\nof of\ncamping camping\n[SEP] [SEP]\nI I\nknow know\nexactly exactly\n. .\n[SEP] [SEP]\n(a) Attention Score\n[CLS] [CLS]\ni i\ndon don\n' '\nt t\nknow know\num um\ndo do\nyou you\ndo do\na a\nlot lot\nof of\ncamping camping\n[SEP] [SEP]\nI I\nknow know\nexactly exactly\n. .\n[SEP] [SEP] (b) Attribution Score\nFigure 1: Attention score (left) and attribution score (right)\nof a single head in BERT. The color is darker for larger\nvalues. The model prediction for the sentence from MNLI\ndataset is contradiction. A TTATTR tends to identify\nmore sparse word interactions that contribute to the ﬁnal\nmodel decision.\n2 Background\n2.1 Transformer\nTransformer (Vaswani et al. 2017) is a model architecture\nrelying on the attention mechanism. Given input tokens\n{xi}|x|\ni=1, we pack their word embeddings to a matrix X0 =\n[x1,··· ,x|x|]. The stacked L-layer Transformer computes\nthe ﬁnal output via Xl = Transformerl(Xl−1),l ∈[1,L].\nThe core component of a Transformer block is multi-head\nself-attention. The h-th self-attention head is described as:\nQh = XWQ\nh , K= XWK\nh , V= XWV\nh (1)\nAh = softmax(QhK⊺\nh√dk\n) (2)\nHh = AttentionHead(X) = AhVh (3)\nwhere Q,K ∈ Rn×dk , V ∈ Rn×dv , and the score Ai,j\nindicates how much attention token xi puts on xj. There\nare usually multiple attention heads in a Transformer block.\nThe attention heads follow the same computation despite us-\ning different parameters. Let |h|denote the number of at-\ntention heads in each layer, the output of multi-head atten-\ntion is given by MultiH(X) = [ H1,··· ,H|h|]Wo, where\nWo ∈R|h|dv×dx , [·] means concatenation, and Hi is com-\nputed as in Equation (3).\n2.2 BERT\nWe conduct all experiments on BERT (Devlin et al. 2019),\nwhich is one of the most successful applications of Trans-\nformer. The pretrained language model is based on bidirec-\ntional Transformer, which can be ﬁne-tuned towards down-\nstream tasks. Notice that our method can also be applied\nto other multi-layer Transformer models with few modiﬁ-\ncations. For single input, a special token [CLS] is added to\nthe beginning of the sentence, and another token [SEP] is\nadded to the end. For pairwise input,[SEP] is also added as\na separator between the two sentences. When BERT is ﬁne-\ntuned on classiﬁcation tasks, a softmax classiﬁer is added on\ntop of the [CLS] token in the last layer to make predictions.\n3 Methods: Self-Attention Attribution\nFigure 1a shows attention scores of one head in ﬁne-tuned\nBERT. We observe that the attention score matrix is quite\ndense, although only one of twelve heads is plotted. It poses\na huge burden on us to understand how words interact with\neach other within Transformer. Moreover, even if an atten-\ntion score is large, it does not mean the pair of words is im-\nportant to model decisions. In contrast, we aim at attributing\nmodel decisions to self-attention relations, which tends to\nassign higher scores if the interaction contributes more to\nthe ﬁnal prediction.\nGiven input sentence x, let Fx(·) represent the Trans-\nformer model, which takes the attention weight matrix A\n(Equation (2)) as the model input. Inspired by Sundararajan,\nTaly, and Yan (2017), we manipulate the internal attention\nscores ¯A, and observe the corresponding model dynamics\nFx( ¯A) to inspect the contribution of word interactions. As\nthe attribution is always targeted for a given inputx, we omit\nit for the simplicity of notations.\nLet us take one Transformer layer as an example to de-\nscribe self-attention attribution. Our goal is to calculate an\nattribution score for each attention connection. For the h-th\nattention head, we compute its attribution score matrix as:\nA= [A1,··· ,A|h|]\nAttrh(A) = Ah ⊙\n∫ 1\nα=0\n∂F(αA)\n∂Ah\ndα∈Rn×n\nwhere ⊙is element-wise multiplication, Ah ∈ Rn×n de-\nnotes the h-th head’s attention weight matrix (Equation (2)),\nand ∂F(αA)\n∂Ah\ncomputes the gradient of model F(·) along Ah.\nThe (i,j)-th element of Attrh(A) is computed for the inter-\naction between input token xi and xj in terms of the h-th\nattention head.\nThe starting point ( α = 0 ) of the integration represents\nthat all tokens do not attend to each other in a layer. Whenα\nchanges from 0 to 1, if the attention connection (i,j) has a\ngreat inﬂuence on the model prediction, its gradient will be\nsalient, so that the integration value will be correspondingly\nlarge. Intuitively, Attrh(A) not only takes attention scores\ninto account, but also considers how sensitive model predic-\ntions are to an attention relation.\nThe attribution score can be efﬁciently computed via\nRiemman approximation of the integration (Sundararajan,\nTaly, and Yan 2017). Speciﬁcally, we sum the gradients\nat points occurring at sufﬁciently small intervals along the\nstraightline path from the zero attention matrix to the origi-\nnal attention weight A:\n˜Attrh(A) = Ah\nm ⊙\nm∑\nk=1\n∂F( k\nmA)\n∂Ah\n(4)\nwhere m is the number of approximation steps. In our ex-\nperiments, we set mto 20, which performs well in practice.\nFigure 1 is an example about the attention score map\nand the attribution score map of a single head in ﬁne-tuned\nBERT. We demonstrate that larger attention scores do not\nmean more contribution to the ﬁnal prediction. The atten-\ntion scores between the [SEP] token and other tokens are\nrelatively large, but they obtain little attribution scores. The\nprediction of the contradiction class attributes most to\nthe connections between “don’t” in the ﬁrst segment and “I\nknow” in the second segment, which is more explainable.\n4 Experiments\nWe employ BERT-base-cased (Devlin et al. 2019) in our ex-\nperiments. The number of BERT layers |l|= 12, the num-\nber of attention heads in each layer |h|= 12, and the size\nof hidden embeddings |h|dv = 768. For a sequence of 128\ntokens, the attribution time of the BERT-base model takes\nabout one second on an Nvidia-v100 GPU card. Moreover,\nthe computation can be parallelized by batching multiple in-\nput examples to increase throughput.\nWe perform BERT ﬁne-tuning and conduct experiments\non four downstream classiﬁcation datasets:\nMNLI (Williams, Nangia, and Bowman 2018) Multi-genre\nNatural Language Inference is to predict whether a premise\nentails the hypothesis (entailment), contradicts the given hy-\npothesis (contradiction), or neither (neutral).\nRTE (Dagan, Glickman, and Magnini 2006; Bar-Haim et al.\n2006; Giampiccolo et al. 2007; Bentivogli et al. 2009) Rec-\nognizing Textual Entailment comes from a series of annual\ntextual entailment challenges.\nSST-2 (Socher et al. 2013) Stanford Sentiment Treebank is\nto predict the polarity of a given sentence.\nMRPC (Dolan and Brockett 2005) Microsoft Research\nParaphrase Corpus is to predict whether the pairwise sen-\ntences are semantically equivalent.\nWe use the same data split as in (Wang et al. 2019). The\naccuracy metric is used for evaluation. When ﬁne-tuning\nBERT, we follow the settings and the hyper-parameters sug-\ngested in (Devlin et al. 2019).\n4.1 Effectiveness Analysis\nWe conduct a quantitative analysis to justify the self-\nattention edges with larger attribution scores contribute more\nto the model decision. We prune the attention heads incre-\nmentally in each layer according to their attribution scores\nwith respect to the golden label and record the performance\nchange. We also establish a baseline that prunes attention\nheads with their average attention scores for comparison.\nExperimental results are presented in Figure 2, we ob-\nserve that pruning heads with attributions scores conduces\nmore salient changes on the performance. Pruning only two\nheads within every layer with the top-2 attribution scores can\ncause an extreme decrease in the model accuracy. In con-\ntrast, retaining them helps the model to achieve nearly 97%\naccuracy. Even if only two heads are retained in each layer,\nthe model can still have a strong performance. Compared\n0 2 4 6 8 10 12\nNumber of pruned heads in every layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy on MNLI-matched\nHeads with small attribution scores are pruned first\nHeads with large attribution scores are pruned first\nHeads with small attention scores are pruned first\nHeads with large attention scores are pruned first\nFigure 2: Effectiveness analysis of ATTATTR . The blue and\nred lines represent pruning attention heads according to at-\ntribution scores, and attention scores, respectively. The solid\nlines mean the attention heads with the smallest values are\npruned ﬁrst, while the dash lines mean the largest values are\npruned ﬁrst. The results show that ATTATTR better indicates\nthe importance of attention heads.\nwith attribution scores, pruning heads with average atten-\ntion scores are less remarkable on the performance change,\nwhich proves the effectiveness of our method.\n4.2 Use Case 1: Attention Head Pruning\nAccording to the previous section, only a small part of atten-\ntion heads contribute to the ﬁnal prediction, while others are\nless helpful. This leads us to the research about identifying\nand pruning the unimportant attention heads.\nHead Importance The attribution scores indicate how\nmuch a self-attention edge attributes to the ﬁnal model deci-\nsion. We deﬁne the importance of an attention head as:\nIh = Ex[max(Attrh(A))] (5)\nwhere xrepresents the examples sampled from the held-out\nset, and max(Attrh(A)) is the maximum attribution value\nof the h-th attention head. Notice that the attribution value\nof a head is computed with respect to the probability of the\ngolden label on a held-out set.\nWe compare our method with other importance metrics\nbased on the accuracy difference and the Taylor expan-\nsion, which are both proposed in (Michel, Levy, and Neu-\nbig 2019). The accuracy difference of an attention head is\nthe accuracy margin before and after pruning the head. The\nmethod based on the Taylor expansion deﬁnes the impor-\ntance of an attention head as:\nIh = Ex\n⏐⏐⏐⏐A⊺\nh\n∂L(x)\n∂Ah\n⏐⏐⏐⏐ (6)\nwhere L(x) is the loss function of example x, and Ah is the\nattention score of the h-th head as in Equation (2).\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage pruned\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nMNLI\nAcc difference\nTaylor expansion\nAttAttr\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage pruned\n0.42\n0.52\n0.62\n0.72\n0.82\n0.92Accuracy\nSST-2\nAcc difference\nTaylor expansion\nAttAttr\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage pruned\n0.32\n0.42\n0.52\n0.62\n0.72\n0.82Accuracy\nMRPC\nAcc difference\nTaylor expansion\nAttAttr\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage pruned\n0.4\n0.5\n0.6\n0.7Accuracy\nRTE\nAcc difference\nTaylor expansion\nAttAttr\nFigure 3: Evaluation accuracy as a function of head prun-\ning proportion. The attention heads are pruned according to\nthe accuracy difference (baseline; dash yellow), the Taylor\nexpansion method (Michel, Levy, and Neubig 2019; solid\nred), and ATTATTR (this work; solid blue).\nFor all three methods, we calculate Ih on 200 examples\nsampled from the held-out dataset. Then we sort all the\nheads according to the importance metrics. The less impor-\ntant heads are ﬁrst pruned.\nEvaluation Results of Head PruningFigure 3 describes\nthe evaluation results of head pruning. The solid red lines\nrepresent pruning heads based on our method ATTATTR . We\nobserve that pruning head with attribution score is much bet-\nter than the baseline of accuracy difference.\nMoreover, the pruning performance of ATTATTR is com-\npetitive with the Taylor expansion method, although A T-\nTATTR is not speciﬁcally designed for attention head prun-\ning. The result show that attention attribution successfully\nindicates the importance of interactions inside Transformer.\nOn the MNLI dataset, when only 10% attention heads are re-\ntained, our method can still achieve approximately 60% ac-\ncuracy, while the accuracy of the Taylor expansion method\nis about 40%.\nUniversality of Important Heads Previous results are\nperformed on speciﬁc datasets respectively. Besides iden-\ntifying the most important heads of Transformer, we in-\nvestigate whether the important heads are consistent across\ndifferent datasets and tasks. The correlation of attribution\nscores of attention heads between two different datasets is\nmeasured by the Pearson coefﬁcient. As described in Fig-\nure 4, as long as the tasks are homogeneous (i.e., solv-\ning similar problems), the important attention heads are\nhighly correlated. The datasets RTE, MPRC, and MNLI\nare about entailment detection, where the important self-\n0 1 2 3 4\nMNLI\n0\n1\n2\n3\n4\n5\n6\n7RTE\nPearson r = 0.84\n0 1 2 3 4\nMNLI\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5MRPC\nPearson r = 0.87\n0.0 0.5 1.0 1.5\nSST-2\n0\n1\n2\n3\n4\n5\n6\n7RTE\nPearson r = -0.09\n0.0 0.5 1.0 1.5\nSST-2\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5MRPC\nPearson r = 0.05\nFigure 4: Correlation of attribution scores of different atten-\ntion heads between datasets. Each point represents the at-\ntribution scores of a single attention head on two datasets.\nThe datasets of homogeneous tasks are strongly correlated,\nwhich implies the same subset of attention heads are ﬁne-\ntuned for similar tasks.\nattention heads (i.e., with large attribution scores) of BERT\nare roughly consistent across the datasets. In contrast, the\ndataset SST-2 is sentiment classiﬁcation. We ﬁnd that the\nimportant heads on SST-2 are different from the ones on\nRTE, and MRPC. In conclusion, the same subset of atten-\ntion heads is ﬁne-tuned for similar tasks.\n4.3 Use Case 2: Visualizing Information Flow\nInside Transformer\nWe propose a heuristic algorithm to construct attribution\ntrees based on the method described in Section 3, the re-\nsults discover the information ﬂow inside Transformer, so\nthat we can know the interactions between the input words\nand how they attribute to the ﬁnal prediction. Such visualiza-\ntion can provide insights to understand what dependencies\nTransformer tends to capture. The post-interpretation helps\nus to debug models and training data.\nThe problem is a trade-off between maximizing the sum-\nmation of attribution scores and minimizing the number of\nedges in the tree. We present a greedy top-down algorithm to\nefﬁciently construct attribution trees. Moreover, we conduct\na quantitative analysis to verify the effectiveness.\nAttribution Tree Construction After computing self-\nattention attribution scores, we can know the interactions be-\ntween the input words in each layer and how they attribute\nto the ﬁnal prediction. We then propose an attribution tree\nconstruction algorithm to aggregate the interactions. In other\nwords, we build a tree to indicate how information ﬂows\nfrom input tokens to the ﬁnal predictions. We argue that such\nAlgorithm 1Attribution Tree Construction\nInput: [al\ni,j]n×n: Attribution scores\n{El}|l|\nl=1: Retained attribution edges\nOutput: V, E: Node set and edge set of Attr tree\n1: ⊿ Initialize the state of all tokens, each token has three states:\nNotAppear, Appear, Fixed\n2: for i ←n, ··· , 1 do\n3: Statei = NotAppear\n4: ⊿ Choose the top node of the attribution tree\n5: [AttrAlli]n = ∑|l|\nl=1\n∑n\nj=1,j̸=i al\ni,j\n6: TopNode = argmax([AttrAlli]n)\n7: V←{ TopNode }; StateTopNode = Appear\n8: ⊿ Build the attribution tree downward\n9: for l ←|l|−1, ··· , 1 do\n10: for (i, j)l\ni̸=j ∈El do\n11: if Statei is Appear and Statej is NotAppear then\n12: E←E ⋃{(i, j)}\n13: V←V ⋃{j}\n14: Statei = Fixed\n15: Statej = Appear\n16: if Statei is Fixed and Statej is NotAppear then\n17: E←E ⋃{(i, j)}\n18: V←V ⋃{j}\n19: Statej = Appear\n20: ⊿ Add the terminal of the information ﬂow\n21: V←{ [CLS]}\n22: for j ←n, ··· , 1 do\n23: if Statej ∈{Appear, Fixed}then\n24: E←E ⋃{([CLS], j)}\n25: return Tree = {V, E}\nvisualization can provide insights to understand what depen-\ndencies Transformer tends to capture.\nFor each layer l, we ﬁrst calculate self-attention attribu-\ntion scores of different heads. Then we sum them up over\nthe heads, and use the results as the l-th layer’s attribution:\nAttr(Al) =\n|h|∑\nh=1\nAttrh(Al) = [al\ni,j]n×n\nwhere larger al\ni,j indicates more interaction between xi and\nxj in the l-th layer in terms of the ﬁnal model predictions.\nThe construction of attribution trees is a trade-off between\nmaximizing the summation of attribution scores and min-\nimizing the number of edges in the tree. The objective is\ndeﬁned as:\nTree = arg max\n{El}|l|\nl=1\n|l|∑\nl=1\n∑\n(i,j)∈El\nal\ni,j −λ\n|l|∑\nl=1\n|El|,\nEl ⊂{(i,j)| al\ni,j\nmax(Attr(Al)) >τ }\nwhere |El|represents the number of edges in the l-th layer,\nλis a trade-off weight, and the threshold τ is used to ﬁlter\nthe interactions with relatively large attribution scores.\nRather than solving a combinatorial optimization prob-\nlem, we use a heuristic top-down method to add these edges\ntothat\nsupplement\nso I have tofind a way to supplement thatIneed a way to add something extra .have wayneedaddsomethingto\nextra\n[CLS]\nway a\n(a) Example from MNLI\nthemovieseldom has amovie so closely matched the spirit of aman and hiswork .\nseldomspirit\nworkso\n[CLS]\nhas\nclosely\nmatched\nofman\n(b) Example from SST-2\nFigure 5: Examples of attribution trees. (a) is from MNLI,\nwhich is predicted as entailment by BERT. (b) is from\nSST-2, which is predicted aspositive by BERT. The grey\nwords from the inputs do not appear in the attribution trees.\nto the attribution tree. The process is detailed in Algorithm 1.\nThe more detailed related explanations are in the appendix.\nSettings We set τ = 0 .4 for layers l < 12. The larger\nτ tends to generate more simpliﬁed trees, which contains\nthe more important part of the information ﬂow. Because\nthe special token [CLS] is the terminal of the information\nﬂow for classiﬁcation tasks, we set τ to 0 for the last layer.\nWe observe that almost all connections between[CLS] and\nother tokens in the last layer have positive attribution scores\nwith respect to model predictions.\nCase Studies As shown in Figure 5, the two attribution\ntrees are from MNLI and SST-2, respectively. The attribu-\ntion tree Figure 5a is generated from MNLI, whose golden\nlabel is entailment. At the bottom of Figure 5a, we ﬁnd\nthat the interactions are more local, and most information\nﬂows are concentrated within a single sentence. The in-\nformation is hierarchically aggregated to “ supplement” and\n“extra” in each sentence. Then the “ supplement” token ag-\ngregates the information in the ﬁrst sentence and “add some-\nthing extra” in the second sentence, these two parts “supple-\nment” and “add something extra” have strong semantic rele-\nvance. Finally, all the information ﬂows to the terminal token\n[CLS] to make the prediction entailment. The attribu-\ntion tree interprets how the input words interacts with each\n100\n101\nInteraction Distance\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11Layer\n(a) MNLI\n100\n101\nInteraction Distance\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11Layer (b) SST-2\nFigure 6: Distance distribution of interactions extracted by\nthe attribution tree in each layers.\nother, and reach the ﬁnal prediction, which makes model de-\ncisions more interpretable.\nFigure 5b is an example from SST-2, whose golden label\nis positive, correctly predicted by the model. From Fig-\nure 5b, we observe that information in the ﬁrst part of the\nsentence “seldom has a movie so closely ” is aggregated to\nthe “has” token. Similarly, information in the other part of\nthe sentence “the spirit of a man and his work ” ﬂows to the\n“spirit” token, which has strong positive emotional tenden-\ncies. Finally, with the feature interactions, all information\naggregates to the verb “ matched”, which gives us a better\nunderstanding of why the model makes the speciﬁc decision.\nReceptive Field The self-attention mechanism is sup-\nposed to have the ability to capture long-range dependen-\ncies. In order to better understand the layer-wise effective\nreceptive ﬁeld in Transformer, we plot the distance distri-\nbution of interactions extracted by the attribution tree. As\nshown in Figure 6, we observe that for the paired input of\nMNLI, the effective receptive ﬁeld is relatively local in the\nﬁrst two layers and the 6-8th layers, while are more broad in\nthe top three layers. For the single input of SST-2, the effec-\ntive receptive ﬁeld is monotonically increasing along with\nthe layer number. Generally, the effective receptive ﬁeld in\nthe second layer is more restricted, while the latter layers\nextract more broad dependencies. Moreover, for pairwise-\ninput tasks (such as MNLI), the results indicate that Trans-\nformer models tend to ﬁrst conduct local encoding and then\nlearn to match between the pair of input sentences, which is\ndifferent with training from scratch (Bao et al. 2019).\n4.4 Use Case 3: Adversarial Attack\nThe model decision attributes more to the attention con-\nnections with larger attribution scores. We observe that the\nmodel tends to over-emphasize some individual patterns\nto make the prediction, while omitting most of the input.\nWe then use the over-conﬁdent patterns as adversarial trig-\ngers (Wallace et al. 2019) to attack the BERT model.\n[CLS] And what about floodsin North Dakota and Iowaor fires in Oakland , Cal ##if . , and Daytona Beach , F ##la . ? [SEP]Daytona Beach , Floridasuffered an ice-storm in July . [SEP]\n[CLS] the ho ##log ##ram makes up all floodsthese things and uh imean sometimes sometimes it Iowa' s funny sometimes it ' s not but uh you know it ' s something to pass the time until we do and then and then we watch football [SEP] Sometimes it is amusing to Floridasee what the iceho ##log ##ram creates . [SEP]Prediction:  Entailment → Contradiction[CLS] We also have found floodsthat leading organizations s Iowa##tri ##veto ensure that their core processes efficiently and effectively support mission -related outcomes . [SEP]Leading organizations want to Floridabe sure their iceemployees are safe . [SEP]Prediction:  Neutral → Contradiction\nSourceTrigger\nPrediction:  Contradiction\nAttack\nAttack\nFigure 7: We use ATTATTR to extract the trigger (i.e., high-\nlighted word patterns) from the MNLI instance that is la-\nbeled as contradict. After adding the adversarial trigger\nto the examples in other categories, the model predictions\nﬂip from neutral and entailment to contradict.\nTrigger Construction We extract the attention dependen-\ncies with the largest attribution scores across different layers\n(i.e., maxL\nl=1 {al\ni,j}) from the input, and employ these pat-\nterns as the adversarial triggers. During the attack, the ad-\nversarial triggers are inserted into the test input at the same\nrelative position and segment as in the original sentence.\nThe speciﬁc attack process is shown in Figure 7. The two\npatterns “ ﬂoods-ice” and “ Iowa-Florida” contribute most\nto the prediction contradict in the source sentence.\nNext we employ them as the trigger to attack other exam-\nples, the model predictions ﬂip from both neutral and\nentailment to contradict. Our attack method relies\non attribution scores, which utilizes the gradient informa-\ntion, therefore it belongs to white-box non-targeted attacks.\nWe extract the dependencies with the largest attribution\nscores as the adversarial triggers from 3,000 input exam-\nples. Each trigger contains less than ﬁve tokens. The score of\na trigger is deﬁned as the maximum attribution value iden-\ntiﬁed within it. When attacking the BERT model on SST-\n2, we use a lexicon 1 to blacklist the words with the obvi-\nous emotional tendencies (such as “disgust” for negative\ntriggers). The adversarial triggers are inserted into the attack\ntext at the same relative position as in the original sentence.\nResults of Attack We conduct the adversarial attacks on\nmultiple datasets. The top-3 adversarial triggers for MNLI\nand SST-2 are listed in Table 1. We report the attack results\nwith these triggers in Table 2. For MNLI, after inserting the\nwords (“with”, and “ math”) to the second segment of the\ninput sentences, the model accuracy of the entailment\nclass drops from 82.87% to 0.8%. For SST-2, adding the\n1www.cs.uic.edu/∼liub/FBS/sentiment-analysis.html\nMNLI SST-2\ncontradict entailment neutral positive negative\nTrigger1 {also, sometimes, S} { with, math} {ﬂoods, Iowa,\nice, Florida} {[CLS], nowhere} { remove, ##ﬁes}\nTrigger2 {nobody, should, not} { light, morning} { never, but} {but, has, nothing} {not, alien, ##ate}\nTrigger3 {do, well, Usually, but} {ﬂoods, Iowa,\nice, Florida}\n{Massachusetts,\nMexico} {offers, little} { ##reshing, ##ly}\nTable 1: Top-3 adversarial triggers for the MNLI and SST-2 datasets. The tokens are inserted into input sentences at the speciﬁc\npositions for non-targeted attacks. We omit the tokens’ positions in the table for brevity.\nMNLI SST-2 MRPC RTE\ncontra- entail- neutral pos- neg- equal not - entail- not-\nBaseline 84.94 82.87 82.00 92.79 91.82 90.32 72.87 72.60 65.65\nTrigger1 34.17 0.80 34.77 54.95 72.20 29.39 51.94 9.59 59.54\nTrigger2 39.81 1.83 47.36 59.68 74.53 32.62 55.04 11.64 62.50\nTrigger3 41.83 2.99 51.49 70.50 77.80 36.56 58.91 13.70 62.60\nAvg. ∆ -46.34 -80.00 -37.46 -31.08 -16.98 -57.46 -17.57 -60.96 -12.31\nTable 2: Attack results of the top- 3 triggers. We abbreviate not equal and not entailment to not- for MRPC and\nRTE, respectively. The baseline represents the original accuracy of model on each category.\ntop-1 adversarial trigger to the input causes nearly 50%\npositive examples to be misclassiﬁed.\nAnalysis of Triggers For both MNLI and RTE, the\nentailment class is more vulnerable than others, because\nthe current models and data seem to heavily rely on word\nmatching, which would result in spurious patterns. More-\nover, we also observe that the trigger is sensitive to the in-\nsertion order and the relative position in the sentence, which\nexhibits the anomalous behaviors of the model, i.e., over-\nrelying on these adversarial triggers to make the prediction.\n5 Related Work\nThe internal behaviors of NNs are often treated as black\nboxes, which motivates the research on the interpretability of\nneural models. Some work focus on attributing predictions\nto the input features with various saliency measures, such as\nDeepLift (Shrikumar, Greenside, and Kundaje 2017), layer-\nwise relevance propagation (Binder et al. 2016), and Inte-\ngrated Gradients (IG; Sundararajan, Taly, and Yan 2017).\nSpeciﬁc to the NLP domain, Murdoch and Szlam (2017)\nintroduce a decomposition method to track the word impor-\ntance in LSTM (Hochreiter and Schmidhuber 1997). Mur-\ndoch, Liu, and Yu (2018) extend the above method to con-\ntextual decomposition in order to capture the contributions\nof word combinations. Another strand of previous work\ngenerates the hierarchical explanations, which aims at re-\nvealing how the features are composed together (Jin et al.\n2020; Chen, Zheng, and Ji 2020). However, they both de-\ntect interaction within contiguous chunk of input tokens. The\nattention mechanism (Bahdanau, Cho, and Bengio 2015)\nrises another line of work. The attention weights generated\nfrom the model indicate the dependency between two words\nintuitively, but Jain and Wallace (2019) and Serrano and\nSmith (2019) draw the same conclusion that they largely do\nnot provide meaningful explanations for model predictions.\nHowever, Wiegreffe and Pinter (2019) propose several alter-\nnative tests and conclude that prior work does not disprove\nthe usefulness of attention mechanisms for interpretability.\nFurthermore, Ghaeini, Fern, and Tadepalli (2018) aim at in-\nterpreting the intermediate layers of NLI models by visual-\nizing the saliency of attention and LSTM gating signals.\nFor Transformer (Vaswani et al. 2017) networks, Clark\net al. (2019) propose the attention-based visualization\nmethod and the probing classiﬁer to explain the behav-\niors of BERT (Devlin et al. 2019). Brunner et al. (2020)\nstudy the identiﬁability of attention weights of BERT and\nshows that self-attention distributions are not directly inter-\npretable. Moreover, some work extracts the latent syntac-\ntic trees from hidden representations (Hewitt and Manning\n2019; Rosa and Marecek 2019; Coenen et al. 2019) and at-\ntention weights (Marecek and Rosa 2019).\n6 Conclusion\nWe propose self-attention attribution (ATTATTR ), which in-\nterprets the information interactions inside Transformer and\nmakes the self-attention mechanism more explainable. First,\nwe conduct a quantitative analysis to justify the effective-\nness of A TTATTR . Moreover, we use the proposed method\nto identify the most important attention heads, which leads\nto a new head pruning algorithm. We then use the attribution\nscores to derive the interaction trees, which visualizes the\ninformation ﬂow of Transformer. We also understand the re-\nceptive ﬁeld in Transformer. Finally, we show that ATTATTR\ncan also be employed to construct adversarial triggers to im-\nplement non-targeted attacks.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. In 3rd International Conference on Learning Repre-\nsentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Cui, L.;\nPiao, S.; and Zhou, M. 2019. Inspecting Uniﬁcation of En-\ncoding and Matching with Transformer: A Case Study of\nMachine Reading Comprehension. In Proceedings of the\n2nd Workshop on Machine Reading for Question Answer-\ning, 14–18. Association for Computational Linguistics.\nBao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu,\nX.; Wang, Y .; Piao, S.; Gao, J.; Zhou, M.; and Hon, H.-\nW. 2020. UniLMv2: Pseudo-Masked Language Models\nfor Uniﬁed Language Model Pre-Training. arXiv preprint\narXiv:2002.12804 .\nBar-Haim, R.; Dagan, I.; Dolan, B.; Ferro, L.; and Giampic-\ncolo, D. 2006. The second PASCAL recognising textual en-\ntailment challenge. In Proceedings of the Second PASCAL\nChallenges Workshop on Recognising Textual Entailment.\nBentivogli, L.; Dagan, I.; Dang, H. T.; Giampiccolo, D.; and\nMagnini, B. 2009. The Fifth PASCAL Recognizing Textual\nEntailment Challenge. In In Proc Text Analysis Conference.\nBinder, A.; Montavon, G.; Bach, S.; M¨uller, K.; and Samek,\nW. 2016. Layer-wise Relevance Propagation for Neu-\nral Networks with Local Renormalization Layers. CoRR\nabs/1604.00825.\nBrunner, G.; Liu, Y .; Pascual, D.; Richter, O.; Ciaramita, M.;\nand Wattenhofer, R. 2020. On Identiﬁability in Transform-\ners. In International Conference on Learning Representa-\ntions.\nChen, H.; Zheng, G.; and Ji, Y . 2020. Generating Hierarchi-\ncal Explanations on Text Classiﬁcation via Feature Interac-\ntion Detection. In ACL.\nChi, Z.; Dong, L.; Wei, F.; Wang, W.; Mao, X.; and Huang,\nH. 2020a. Cross-Lingual Natural Language Generation via\nPre-Training. In The Thirty-Fourth AAAI Conference on Ar-\ntiﬁcial Intelligence, 7570–7577. AAAI Press.\nChi, Z.; Dong, L.; Wei, F.; Yang, N.; Singhal, S.; Wang,\nW.; Song, X.; Mao, X.; Huang, H.; and Zhou, M.\n2020b. InfoXLM: An Information-Theoretic Framework\nfor Cross-Lingual Language Model Pre-Training. CoRR\nabs/2007.07834.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What Does BERT Look At? An Analysis of BERT’s\nAttention. CoRR abs/1906.04341.\nClark, K.; Luong, M.-T.; Le, Q. V .; and Manning, C. D.\n2020. ELECTRA: Pre-training Text Encoders as Discrim-\ninators Rather Than Generators. In ICLR.\nCoenen, A.; Reif, E.; Yuan, A.; Kim, B.; Pearce, A.; Vi´egas,\nF. B.; and Wattenberg, M. 2019. Visualizing and Measuring\nthe Geometry of BERT. CoRR abs/1906.02715.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised Cross-lingual Rep-\nresentation Learning at Scale. InProceedings of the 58th An-\nnual Meeting of the Association for Computational Linguis-\ntics, 8440–8451. Association for Computational Linguistics.\nDagan, I.; Glickman, O.; and Magnini, B. 2006. The PAS-\nCAL Recognising Textual Entailment Challenge. In Pro-\nceedings of the First International Conference on Machine\nLearning Challenges: Evaluating Predictive Uncertainty Vi-\nsual Object Classiﬁcation, and Recognizing Textual Entail-\nment, MLCW’05, 177–190. Berlin, Heidelberg: Springer-\nVerlag. ISBN 3-540-33427-0, 978-3-540-33427-9.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1, 4171–4186. Minneapolis, Minnesota: Asso-\nciation for Computational Linguistics.\nDolan, W. B.; and Brockett, C. 2005. Automatically con-\nstructing a corpus of sentential paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing\n(IWP2005).\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. Uniﬁed Language\nModel Pre-training for Natural Language Understanding\nand Generation. In 33rd Conference on Neural Information\nProcessing Systems (NeurIPS 2019).\nGhaeini, R.; Fern, X. Z.; and Tadepalli, P. 2018. In-\nterpreting Recurrent and Attention-Based Neural Models:\na Case Study on Natural Language Inference. CoRR\nabs/1808.03894.\nGiampiccolo, D.; Magnini, B.; Dagan, I.; and Dolan, B.\n2007. The Third PASCAL Recognizing Textual Entailment\nChallenge. In Proceedings of the ACL-PASCAL Workshop\non Textual Entailment and Paraphrasing, 1–9. Prague: As-\nsociation for Computational Linguistics.\nHewitt, J.; and Manning, C. D. 2019. A Structural Probe\nfor Finding Syntax in Word Representations. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1, 4129–4138. Minneapo-\nlis, Minnesota: Association for Computational Linguistics.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural Computation 9: 1735–1780. ISSN 0899-\n7667.\nJain, S.; and Wallace, B. C. 2019. Attention is not Explana-\ntion. CoRR abs/1902.10186.\nJin, X.; Wei, Z.; Du, J.; Xue, X.; and Ren, X. 2020. Towards\nHierarchical Importance Attribution: Explaining Composi-\ntional Semantics for Neural Sequence Models. In ICLR.\nKovaleva, O.; Romanov, A.; Rogers, A.; and Rumshisky,\nA. 2019. Revealing the Dark Secrets of BERT. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 4364–4373. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692 .\nMarecek, D.; and Rosa, R. 2019. From Balustrades to Pierre\nVinken: Looking for Syntax in Transformer Self-Attentions.\nCoRR abs/1906.01958.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? CoRR abs/1905.10650.\nMurdoch, W. J.; Liu, P. J.; and Yu, B. 2018. Beyond Word\nImportance: Contextual Decomposition to Extract Interac-\ntions from LSTMs. CoRR abs/1801.05453.\nMurdoch, W. J.; and Szlam, A. 2017. Automatic Rule Ex-\ntraction from Long Short Term Memory Networks. CoRR\nabs/1702.02540.\nRosa, R.; and Marecek, D. 2019. Inducing Syntactic Trees\nfrom BERT Representations. CoRR abs/1906.11511.\nSerrano, S.; and Smith, N. A. 2019. Is Attention Inter-\npretable? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2931–2951. Flo-\nrence, Italy: Association for Computational Linguistics.\nShrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learn-\ning Important Features Through Propagating Activation Dif-\nferences. CoRR abs/1704.02685.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models\nfor Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing , 1631–1642. Seattle,\nWashington, USA: Association for Computational Linguis-\ntics.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nAttribution for Deep Networks. CoRR abs/1703.01365.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems 30, 5998–6008. Curran Associates, Inc.\nWallace, E.; Feng, S.; Kandpal, N.; Gardner, M.; and Singh,\nS. 2019. Universal Adversarial Triggers for Attacking\nand Analyzing NLP. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 2153–2162. Hong\nKong, China: Association for Computational Linguistics.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nInternational Conference on Learning Representations.\nWiegreffe, S.; and Pinter, Y . 2019. Attention is not not Ex-\nplanation. In EMNLP-IJCNLP, 11–20. Hong Kong, China:\nAssociation for Computational Linguistics.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122. New Orleans, Louisiana:\nAssociation for Computational Linguistics.",
  "topic": "Attribution",
  "concepts": [
    {
      "name": "Attribution",
      "score": 0.7358407974243164
    },
    {
      "name": "Transformer",
      "score": 0.7114434242248535
    },
    {
      "name": "Computer science",
      "score": 0.7027442455291748
    },
    {
      "name": "Salient",
      "score": 0.6407008171081543
    },
    {
      "name": "Adversarial system",
      "score": 0.5875548720359802
    },
    {
      "name": "Security token",
      "score": 0.5495631694793701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4393712282180786
    },
    {
      "name": "Psychology",
      "score": 0.22571584582328796
    },
    {
      "name": "Social psychology",
      "score": 0.14088213443756104
    },
    {
      "name": "Computer security",
      "score": 0.12722694873809814
    },
    {
      "name": "Engineering",
      "score": 0.10039576888084412
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}