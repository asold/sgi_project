{
  "title": "Inductive reasoning with large language models: a simulated randomized controlled trial for epilepsy",
  "url": "https://openalex.org/W4392939244",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5078502762",
      "name": "Daniel M. Goldenholz",
      "affiliations": [
        "Hadassah Medical Center",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5049891383",
      "name": "Shira R. Goldenholz",
      "affiliations": [
        "Hadassah Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5010848124",
      "name": "Sara Habib",
      "affiliations": [
        "Hadassah Medical Center",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A5023557669",
      "name": "M. Brandon Westover",
      "affiliations": [
        "Hadassah Medical Center",
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4309650827",
    "https://openalex.org/W4387699659",
    "https://openalex.org/W4380634688",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W4313460860",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2988342087",
    "https://openalex.org/W4309409155",
    "https://openalex.org/W2805186392",
    "https://openalex.org/W2784161791",
    "https://openalex.org/W3016090285",
    "https://openalex.org/W1989727245",
    "https://openalex.org/W2788593837",
    "https://openalex.org/W2890039708",
    "https://openalex.org/W1856414226",
    "https://openalex.org/W1911218850",
    "https://openalex.org/W2263208726",
    "https://openalex.org/W2765567904",
    "https://openalex.org/W2618387459",
    "https://openalex.org/W2971245301",
    "https://openalex.org/W3034987481",
    "https://openalex.org/W3009049896",
    "https://openalex.org/W4385330759",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W6858023062",
    "https://openalex.org/W2136389274",
    "https://openalex.org/W2027576789",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W2562208566",
    "https://openalex.org/W2984023515",
    "https://openalex.org/W4308760226"
  ],
  "abstract": "Abstract Importance The analysis of electronic medical records at scale to learn from clinical experience is currently very challenging. The integration of artificial intelligence (AI), specifically foundational large language models (LLMs), into an analysis pipeline may overcome some of the current limitations of modest input sizes, inaccuracies, biases, and incomplete knowledge bases. Objective To explore the effectiveness of using an LLM for generating realistic clinical data and other LLMs for summarizing and synthesizing information in a model system, simulating a randomized clinical trial (RCT) in epilepsy to demonstrate the potential of inductive reasoning via medical chart review. Design An LLM-generated simulated RCT based on a RCT for treatment with an anti-seizure medication, cenobamate, including a placebo arm and a full-strength drug arm, evaluated by an LLM-based pipeline versus a human reader. Setting Simulation based on realistic seizure diaries, treatment effects, reported symptoms and clinical notes generated by LLMs with multiple different neurologist writing styles. Participants Simulated cohort of 240 patients, divided 1:1 into placebo and drug arms. Intervention Utilization of LLMs for the generation of clinical notes and for the synthesis of data from these notes, aiming to evaluate the efficacy and safety of cenobamate in seizure control either with a human evaluator or AI-pipeline. Measures The AI and human analysis focused on identifying the number of seizures, symptom reports, and treatment efficacy, with statistical analysis comparing the 50%-responder rate and median percentage change between the placebo and drug arms, as well as side effect rates in each arm. Results AI closely mirrored human analysis, demonstrating the drug’s efficacy with marginal differences (&lt;3%) in identifying both drug efficacy and reported symptoms. Conclusions and Relevance This study showcases the potential of LLMs accurately simulate and analyze clinical trials. Significantly, it highlights the ability of LLMs to reconstruct essential trial elements, identify treatment effects, and recognize reported symptoms, within a realistic clinical framework. The findings underscore the relevance of LLMs in future clinical research, offering a scalable, efficient alternative to traditional data mining methods without the need for specialized medical language training. Key Points Question Can large language models (LLMs) effectively simulate and analyze a randomized clinical trial, accurately summarizing and synthesizing clinical data to evaluate drug efficacy and identify relevant reported symptoms? Findings In a simulated study using LLMs to generate and analyze clinical notes for a trial comparing a drug to a placebo in epilepsy treatment, AI-driven analyses were found to closely match human expert evaluations. The process demonstrated the ability of LLMs to accurately capture treatment effects and identify reported symptoms, with minimal differences in outcomes between the human and LLM analyses. Meaning The use of LLMs in simulating and analyzing clinical trials offers a promising approach to developing inductive reasoning systems based on electronic medical records. This could revolutionize the way clinical trials are conducted and analyzed, enabling rapid, accurate assessments of therapeutic efficacy and safety without the need for specialized medical language training.",
  "full_text": " 1\nTITLE: Inductive reasoning with large language models: a simulated randomized \ncontrolled trial for epilepsy \n \nArticle type: Original investigation \nStudy type: Diagnostic study \n \nAuthors \nDaniel M. Goldenholz, MD, PhD1,2  daniel.goldenholz@bidmc.harvard.edu ORCID 0000-0002-8370-2758 \nShira R. Goldenholz, MD, MPH,2  shira.r.g@gmail.com \nSara Habib, MD1,2   shabib1@bidmc.harvard.edu \nM. Brandon Westover, MD PhD1,2  bwestove@bidmc.harvard.edu \n \nAffiliation \n1 – Department of Neurology, Harvard Medical School, Boston USA \n2 – Department of Neurology, Beth Israel Deaconess Medical Center, Boston USA \n \nCorresponding author: Daniel Goldenholz, daniel.goldenholz@bidmc.harvard.edu\n \n330 Brookline Ave, Baker 5 \nBoston MA 02215 \n617 632 8930 \n \nKeywords: artificial intelligence, large language models, epilepsy, randomized clinical \ntrials \nAbstract (max 350): 339 \nWord counts (max 3000): 1909 \nFigures/tables: (max 5): 4 \n \nFunding  - NIH K23NS124656 \n \nPotential conflict of interest: \nNone \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n 2\nKey Points \n \nQuestion: Can large language models (LLMs) effectively simulate and analyze a \nrandomized clinical trial, accurately summarizing and synthesizing clinical data to \nevaluate drug efficacy and identify relevant reported symptoms? \n \nFindings: In a simulated study using LLMs to generate and analyze clinical notes for a \ntrial comparing a drug to a placebo in epilepsy treatment, AI-driven analyses were found \nto closely match human expert evaluations. The process demonstrated the ability of \nLLMs to accurately capture treatment effects and identify reported symptoms, with \nminimal differences in outcomes between the human and LLM analyses.  \n \nMeaning: The use of LLMs in simulating and analyzing clinical trials offers a promising \napproach to developing inductive reasoning systems based on electronic medical \nrecords. This could revolutionize the way clinical trials are conducted and analyzed, \nenabling rapid, accurate assessments of therapeutic efficacy and safety without the \nneed for specialized medical language training. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 3\nAbstract \nImportance: The analysis of electronic medical records at scale to learn from clinical \nexperience is currently very challenging. The integration of artificial intelligence (AI), \nspecifically foundational large language models (LLMs), into an analysis pipeline may \novercome some of the current limitations of modest input sizes, inaccuracies, biases, \nand incomplete knowledge bases. \nObjective: To explore the effectiveness of using an LLM for generating realistic clinical \ndata and other LLMs for summarizing and synthesizing information in a model system, \nsimulating a randomized clinical trial (RCT) in epilepsy to demonstrate the potential of \ninductive reasoning via medical chart review. \nDesign: An LLM-generated simulated RCT based on a RCT for treatment with an anti-\nseizure medication, cenobamate, including a placebo arm and a full-strength drug arm, \nevaluated by an LLM-based pipeline versus a human reader. \nSetting: Simulation based on realistic seizure diaries, treatment effects, reported \nsymptoms and clinical notes generated by LLMs with multiple different neurologist \nwriting styles. \nParticipants: Simulated cohort of 240 patients, divided 1:1 into placebo and drug arms. \nIntervention: Utilization of LLMs for the generation of clinical notes and for the \nsynthesis of data from these notes, aiming to evaluate the efficacy and safety of \ncenobamate in seizure control either with a human evaluator or AI-pipeline. \nMeasures: The AI and human analysis focused on identifying the number of seizures, \nsymptom reports, and treatment efficacy, with statistical analysis comparing the 50%-\nresponder rate and median percentage change between the placebo and drug arms, as \nwell as side effect rates in each arm. \nResults: AI closely mirrored human analysis, demonstrating the drug's efficacy with \nmarginal differences (<3%) in identifying both drug efficacy and reported symptoms.  \nConclusions and Relevance: This study showcases the potential of LLMs accurately \nsimulate and analyze clinical trials. Significantly, it highlights the ability of LLMs to \nreconstruct essential trial elements, identify treatment effects,  and recognize reported \nsymptoms, within a realistic clinical framework. The findings underscore the relevance \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 4\nof LLMs in future clinical research, offering a scalable, efficient alternative to traditional \ndata mining methods without the need for specialized medical language training. \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 5\nIntroduction \nIt is very challenging to extract knowledge from the electronic medical system1. Various \napproaches, including the use of structured data2, natural language processing \ntoolboxes3–5, and others have been shown to hold some promise. Nevertheless, the \ndream of an AI ingesting hundreds of millions of patient charts to develop “clinical \njudgement” is currently still not practical. With the advent of highly capable foundational \nlarge language models (LLMs)6–8, this dream may be closer to reality than ever before. \nCurrent generation systems are plagued with a variety of constraints, including very \nmodest input size limits confabulations (a.k.a. “hallucinations”), inaccuracies, biases, \nand incomplete knowledge bases\n6. Despite these limitations, modern LLMs have made \nimportant strides both in the realm of generative AI for producing artificial documents, as \nwell as in information extraction and summarization. \nIn this study, we set out to explore a model system of using an LLM (Figure 1, LLM A) to \ngenerate clinical data and other LLMs (Figure 2, LLMs B and C) to summarize and \nsynthesize information. The hypothesis was that a simulated randomized clinical trial \ncould be generated, summarized, and accurately evaluated with the help of LLMs. \nInductive reasoning, defined here as generalizing knowledge based on a set of \nobservations, is submitted as one of the ways clinicians learn. The purpose of this task \nwas to demonstrate the power of AI-enhanced inductive reasoning applied to medical \nchart review.  \n \nMethods \n \nBuilding the dataset \nA simulated randomized clinical trial was constructed (Figure 1) based on an actual \nclinical trial in epilepsy for cenobamate\n9. In that trial, there were 2 months of baseline, \nand 3 months of maintenance at steady state for the drug. In our simulation, there was a \nplacebo arm and a full-strength drug arm (corresponding to 400mg/day cenobamate). \nSimilar to the cenobamate trial, we included 120 patients per arm. To generate a \nrealistic cohort of simulated patients, a previously validated simulator (CHOCOLATES) \nwas used\n10. CHOCOLATES was designed to account for heterogeneity in seizure \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 6\nfrequencies across patients11, the “L-relationship” power law within dairies12, seizure \nclustering13,14, seizure susceptibility cycles15,16, and maximum allowable seizure rates17. \nBased on multiple lines of evidence10,18–24, we assumed that placebo did not have any \nintrinsic effect and any measured effect would be due to natural variability and \nregression to the mean25. Similar to the RCT9, simulated patients needed to have an \naverage rate of 4 seizures per month to be included in the simulated study. Like \ncenobamate, the simulated drug was 39% more effective than placebo\n9. The precise \nsymptom reporting rates in the placebo and drug arms of the cenobamate trial were \nsimulated as well\n9. A well-characterized, open-source LLM26 called Llama2:7b was used \nto generate clinical notes with the temperature parameter set to 1.0 (values >0 increase \ncreativity). The creativity, as well as LLM hallucination, were intentionally part of this \nstudy to properly simulate “noise” caused by inaccurate patient reporting and inaccurate \ndocumentation. One of four neurologist writing styles was randomly selected at the time \nof each clinical note generation: 1) a terse minimalist style using bullet points, 2) a \ncomplete but brief style, 3) a narrative style in 2-4 paragraphs, and 4) an erudite \nacademic professor with many extraneous details. Each simulated patient had two \nnotes generated, one after the baseline period and one after the blinded maintenance \nperiod (480 notes total). Additional random details about the patients’ past medical \nhistory were added randomly but kept consistent within each patient. In addition to a \ncomplete note, each encounter also generated a “ground truth” entry in a data table that \nindicates what information was used in the prompt to the LLM to generate the clinical \nnote. \n \n \nAI analysis of the notes \nAn AI pipeline for analysis of the RCT was constructed as follows (Figure 2). Each note \nwas fed individually (due to input size constraints) to a second open-source LLM\n27 \n(Mistral 7B v0.1) set to a temperature of 0.0 to increase precision and decrease \nextraneous detail. This LLM was selected because it was produced independently of \nLlama2, and thus would not have the luxury of expecting certain styles or methods of \nwriting. The LLM was asked to summarize the note, specifically indicating the number of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 7\nseizures during the observation period and what symptoms were reported by the \npatient. Due to inaccuracies and incomplete responses from typical open-source LLMs, \nit was not feasible for the LLM to build the final data table required for statistical \nanalysis. Thus, a set of somewhat poorly formatted but mostly complete summaries was \nobtained from the second LLM.  \nA third LLM (Claude 2), was also used. This LLM has an extended data input limit and is \nable to ingest large numbers of summaries at once,  resulting in the ability to produce a \nwell formatted data table, and correctly make synthesis inferences correctly. Claude 2 is \nfreely available via web interface, but the application programming interface (API) \nrequires a paid account. In addition to improving the formatting, the third LLM was \nasked to indicate, on each row, the number of seizures during each period of the study;  \nit was also asked to indicate if there were symptoms reported in the second encounter \nthat differed from the first encounter (representing new symptoms that started along \nwith the experimental treatment).  \n \nHuman analysis of the notes \nThe set of 480 generated clinical notes were assessed by one of the authors (SH), a \ntrained neurologist. The relevant features, namely, the number of seizures during the \nobservation period and any symptoms reported, were manually extracted and organized \ninto a data table. \n \nStatistical analysis of data tables \nThree data tables (the ground truth, the AI, and the human) were analyzed in the same \nway. The percentage change between average monthly seizure rate during baseline to \naverage monthly seizure rate during the maintenance period was computed for each \npatient\n28. These percentage change values were used to tally the fraction of 50%-\nresponders in each arm, and then to compute a Fisher Exact test to compare arms \n(RR50). The same percentage change values were also used to compute a median \npercentage change (MPC) and the Mann-Whitney U test was used to non-\nparametrically compare the two arms. Uniquely reported symptoms were tallied up in \neach arm, and these were summarized. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 8\n \nThe TRIPOD reporting checklist29 is provided (Appendix). Code was prepared in python \nusing langchain and ollama. Open-source code is available at \nhttps://github.com/GoldenholzLab/LLM-rct.git. \n \nResults \n \nComputational time for generation and summarization of notes combined took roughly \n20 hours on a single computer; this time would of course be reduced with increased \ncomputational resources. The complete set of notes are available for review (Appendix). \nThe human review of the 480 notes required roughly 5 hours.  In the placebo group,  9 \npatients were identified as not having any value reported for seizures in either the \nbaseline or maintenance periods. In the drug group there were an additional 8 such \npatients. These failures can be attributed to the generative LLM A (Figure 1) that \nproduced the notes. These were not corrected, as these represented examples of \nundesirable “noise” that prevented perfect reconstruction of the ground truth. When \ncomputing the statistics for efficacy, patients with missing numbers were excluded. All \npatients were included when computing symptom report summaries. \n \nThe treatment effect sizes reported for the 50%-responder rate (RR50) and median \npercentage change (MPC) are shown in Figure 3. The marginal efficacy between drug \nand placebo are shown in Table 1. All comparisons were statistically significant. The AI \nand human marginal efficacies differed by 1% in both RR50 and MPC. \n \nThe reported symptoms identified from each of the data tables are shown in Figure 4. \nThe maximum differences in symptom rates between tables were: AI vs. truth – 2%, \nhuman vs truth – 2%, AI vs. human – 3%. \n \nDiscussion \nThis study simulated a realistic trial modelled after a recently published randomized \ndrug trial\n9, and using AI, was able to reconstruct the important elements that were \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 9\nreported quantitatively and qualitatively in the clinical notes. The AI pipeline was able to \ncorrectly show the marginal drug efficacy (drug vs. placebo) differing from human review \nby no more than 1%. Similarly, the pipeline was able to identify the relevant symptoms \nreported in drug and placebo arms, differing with the human by no more than 3%. The \nuse of generative AI allowed us to intentionally inject “noise” (distracting and/or incorrect \nelements) into our experiment. This deliberate addition was made to help determine if \nwe could teach AI system to learn medical information by induction in the presence of \nnoise. In typical clinical situations, there is virtually always some “noise” generated, \nwhether due to inaccurate  reporting by patients or caregivers, or inaccurate recording \nby clinicians. Our system was able to correctly show a strong effect of the simulated \ndrug and found the appropriate common side effects without being taught to look for \nsomething specific. These achievements are all the more remarkable when considering \nan important point:  this entire project did not make use of any LLMs specially trained in \nmedical language\n30. Moreover, advanced APIs, necessitating expensive and \ncomputationally prohibitive setups, were not required. \nFuture versions of the present pipeline might employ only a single LLM if it was \ncomputationally efficient, reliable, inexpensive and had a very large token size. The \nadvantage of the current approach is that it is not necessary to wait for such advances \nto be made available. \nLike any simulation, this study is only as good as the assumptions made. We assumed \nwe have an adequate model for seizure diaries and trial simulation based on prior \nwork\n10,21,24,31,32. We also assumed that generative LLM clinical notes can represent a \nfirst approximation for true clinical notes, and that the conditions presented here are \nrelevant to other inductive learning tasks of interest in clinical settings. Another limitation \nof this study was a linguistic one: our study was conducted entirely in English. \nMultilingual open-source models\n33 are available to extend the present work to many \nother languages.  \nIt must also be noted that extremely rare side effects in a randomized controlled trial \nmight be missed by the type of system developed here – for example, if an \ninvestigational drug causes a systemic inflammatory reaction in only 1 patient for the \nwhole study, this fact must not be missed by trialists. Whereas the system proposed \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 10\nhere may miss such rare side effects, our goal is to look for larger trends and not \n“outlier” rare results. Indeed, if such rare reactions were only noted in postmarketing \nstudies, it could take a long time for regulators (and therefore clinicians) to become \naware of them, yet an inductively learning AI system could flag situations like this if they \nhappen in low fractions of patients beyond expected levels. \nThe longer-term purpose of building clinical inductive learning tools is to develop real-\ntime systems that can learn from very large populations and apply this knowledge to \nuncertain situations. For instance, if a new drug is approved, physicians develop a \ncertain personal clinical “experience” with that drug, and after this they base their \nprescribing habits on that experience. That personal experience sometimes matches \nthe clinical trials, while sometimes there is a mismatch. This “clinical experience” is one \nof the ingredients that makes seasoned clinicians more effective at choosing from an \nuncertain set of choices. If an AI-enhanced system can develop such clinical experience \nacross populations, it will be able to rapidly assist countless clinicians with the most \nupdated experience base possible – vastly larger than any one clinician can accrue in \ntheir personal practice. \n \nIn conclusion, we demonstrated that known (but hidden) knowledge could be learned by \ninduction with a moderate sample of patient charts. Further studies are needed to \nexpand this capability to broader medical knowledge acquisition and applications. \n \n \n \nData Sharing Statement \nOpen-source code and data is available on: https://github.com/GoldenholzLab/LLM-\nrct.git. \n \n \nDMG – project design and oversight, data interpretation, manuscript writing and editing \nSRG – manuscript editing \nSH – data analysis, manuscript editing \nMBW – project oversight, manuscript editing \n \nAcknowledgements \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 11\nFunding for this work came in part from NIH K23NS124656. The authors wish to thank \nthe open-source community for sharing and distributing large language models such as \nllama2 and mistral, as these tools can help advance biomedical science. \n \n \nConflicts of interest \nNone of the authors have any conflicts of interest to declare. \n \n \nREFERENCES \n \n1. Yang S, Varghese P , Stephenson E, Tu K, Gronsbell J. Machine learning \napproaches for electronic health records phenotyping: a methodical review. J Am \nMed Inform Assoc. 2023;30(2):367-381. doi:10.1093/jamia/ocac216 \n2. Ostropolets A, Hripcsak G, Husain SA, et al. Scalable and interpretable alternative \nto chart review for phenotype evaluation using standardized structured data from \nelectronic health records. J Am Med Inform Assoc. 2023;31(1):119-129. \ndoi:10.1093/jamia/ocad202 \n3. Fu S, Wen A, Liu H. Clinical Natural Language Processing in Secondary Use of \nEHR for Research. Published online 2023:433-451. doi:10.1007/978-3-031-\n27173-1_21 \n4. Wu S, Roberts K, Datta S, et al. Deep learning in clinical natural language \nprocessing: a methodical review. J Am Med Inform Assoc. 2020;27(3):457-470. \ndoi:10.1093/jamia/ocz200 \n5. Murphy RM, Klopotowska JE, de Keizer NF, et al. Adverse drug event detection \nusing natural language processing: A scoping review of supervised learning \nmethods. PLoS One. 2023;18(1):e0279842. doi:10.1371/journal.pone.0279842 \n6. Lee P, Goldberg C, Kohane I, Bubeck S. The AI revolution in medicine\n/i1 : GPT-4 \nand beyond. :200. \n7. Bubeck S, Chandrasekaran V, Eldan R, et al. Sparks of Artificial General \nIntelligence: Early experiments with GPT-4. Published online March 22, 2023. \nAccessed August 8, 2023. https://arxiv.org/abs/2303.12712v5 \n8. OpenAI. GPT-4 Technical Report. Published online 2023. \n9. Krauss GL, Klein P , Brandt C, et al. Safety and efficacy of adjunctive cenobamate \n(YKP3089) in patients with uncontrolled focal seizures: a multicentre, double-\nblind, randomised, placebo-controlled, dose-response trial. Lancet Neurol. \n2020;19(1):38-48. doi:10.1016/S1474-4422(19)30399-0 \n10. Goldenholz DM, Westover MB. Flexible realistic simulation of seizure occurrence \nrecapitulating statistical properties of seizure diaries. Epilepsia. 2023;64(2):396-\n405. doi:10.1111/epi.17471 \n11. Ferastraoaru V, Goldenholz DM, Chiang S, Moss R, Theodore WH, Haut SR. \nCharacteristics of large patient-reported outcomes: Where can one million \nseizures get us? Epilepsia Open. 2018;3(3):364-373. doi:10.1002/epi4.12237 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 12\n12. Goldenholz DM, Goldenholz SR, Moss R, et al. Is seizure frequency variance a \npredictable quantity? Ann Clin Transl Neurol. 2018;5(2). doi:10.1002/acn3.519 \n13. Chiang S, Haut SR, Ferastraoaru V, et al. Individualizing the definition of seizure \nclusters based on temporal clustering analysis. Epilepsy Res. 2020;163. \ndoi:10.1016/j.eplepsyres.2020.106330 \n14. Haut SR. Seizure clusters: characteristics and treatment. Curr Opin Neurol. \n2015;28(2):143-150. doi:10.1097/WCO.0000000000000177 \n15. Baud MO, Kleen JK, Mirro EA, et al. Multi-day rhythms modulate seizure risk in \nepilepsy. Nat Commun. 2018;9(1):1-10. doi:10.1038/s41467-017-02577-y \n16. Karoly PJ, Goldenholz DM, Freestone DR, et al. Circadian and circaseptan \nrhythms in human epilepsy: a retrospective cohort study. Lancet Neurol. \n2018;17(11):977-985. doi:10.1016/S1474-4422(18)30274-6 \n17. Trinka E, Cock H, Hesdorffer D, et al. A definition and classification of status \nepilepticus - Report of the ILAE Task Force on Classification of Status Epilepticus. \nEpilepsia. 2015;56(10):1515-1523. doi:10.1111/epi.13121 \n18. Goldenholz DM, Moss R, Scott J, Auh S, Theodore WH. Confusing placebo effect \nwith natural history in epilepsy: A big data approach. Ann Neurol. 2015;78(3). \ndoi:10.1002/ana.24470 \n19. Goldenholz DM, Goldenholz SR. Response to placebo in clinical epilepsy trials-\nOld ideas and new insights. Epilepsy Res. 2016;122. \ndoi:10.1016/j.eplepsyres.2016.02.002 \n20. Goldenholz DM, Strashny A, Cook M, Moss R, Theodore WH. A multi-dataset \ntime-reversal approach to clinical trial placebo response and the relationship to \nnatural variability in epilepsy. Seizure. 2017;53. doi:10.1016/j.seizure.2017.10.016 \n21. Goldenholz DM, Tharayil J, Moss R, Myers E, Theodore WH. Monte Carlo \nsimulations of randomized clinical trials in epilepsy. Ann Clin Transl Neurol. \n2017;4(8):544-552. doi:10.1002/acn3.426 \n22. Karoly PJ, Romero J, Cook MJ, Freestone DR, Goldenholz DM. When can we \ntrust responders? Serious concerns when using 50% response rate to assess \nclinical trials. Epilepsia. 2019;60(9). doi:10.1111/epi.16321 \n23. Goldenholz DM, Goldenholz SR. Placebo in epilepsy. In: International Review of \nNeurobiology. Academic Press Inc.; 2020. doi:10.1016/bs.irn.2020.03.033 \n24. Romero J, Larimer P, Chang B, Goldenholz SR, Goldenholz DM. Natural \nvariability in seizure frequency: Implications for trials and placebo. Epilepsy Res. \n2020;162:106306. doi:10.1016/j.eplepsyres.2020.106306 \n25. Goldenholz DM, Goldenholz EB, Kaptchuk  TJ. Quantifying and controlling the \nimpact of regression to the mean on randomized controlled trials in epilepsy. \nEpilepsia. 2023;64(10):2635-2643. doi:10.1111/epi.17730 \n26. Touvron H, Martin L, Stone K, et al. Llama 2: Open Foundation and Fine-Tuned \nChat Models. \n27. Jiang AQ, Sablayrolles A, Mensch A, et al. Mistral 7B. Published online October \n10, 2023. Accessed February 26, 2024. https://arxiv.org/abs/2310.06825v1 \n28. Siddiqui O, Hershkowitz N. Primary Efficacy Endpoint in Clinical Trials of \nAntiepileptic Drugs: Change or Percentage Change. Drug Inf J. 2010;44(3):343-\n350. doi:10.1177/009286151004400316 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 13\n29. Collins GS, Reitsma JB, Altman DG, M oons KGM. Transparent reporting of a \nmultivariable prediction model for individual prognosis or diagnosis (TRIPOD): \nThe TRIPOD Statement. Eur Urol. 2015;67(6):1142-1151. \ndoi:10.1016/j.eururo.2014.11.025 \n30. Singhal K, Tu T, Gottweis J, et al. Towards Expert-Level Medical Question \nAnswering with Large Language Models. \n31. Goldenholz DM, Tharayil JJ, Kuzniecky R, Karoly P, Theodore WH, Cook MJ. \nSimulating clinical trials with and without intracranial EEG data. Epilepsia Open. \n2017;2(2):156-161. doi:10.1002/epi4.12038 \n32. Oliveira A, Romero JM, Goldenholz DM. Comparing the efficacy, exposure, and \ncost of clinical trial analysis methods. Epilepsia. 2019;60(12):e128-e132. \ndoi:10.1111/epi.16384 \n33. Workshop B, Le Scao T, Fan A, et al. BLOOM: A 176B-Parameter Open-Access \nMultilingual Language Model. Published online November 9, 2022. Accessed \nFebruary 27, 2024. https://arxiv.org/abs/2211.05100v4 \n  \n \n \n \n \nFigure 1: Generation of clinical notes. The diary simulator (CHOCOLATES) was used to \nproduce a realistic seizure diary. This was modulated by the treatment effect (0% in \nplacebo arm, and 39% in drug arm) during the experimental maintenance stage. One of \nfour writing styles were chosen, and a random set of reported symptoms were selected \n(based on previously reported incidence of symptoms for that arm). These items were \nused to generate the prompt submitted to LLM A (Llama 2:7b). The LLM generated the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 14\nclinical note. The true summary was generated based on the original elements used to \nproduce the prompt. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 15\n \n \nFigure 2: Analyzing the trial. The Ground truth summaries (Figure 1) were used directly \nas a data table. The AI pathway took the clinical notes (Figure 1), and then LLM B \n(Mistral) produced a summary that indicated the number of seizures and symptoms \nreported. LLM C (Claude 2) was used to further summarize and synthesize the brief \nsummaries from LLM B into a complete data table. The clinical notes (Figure 1) were \nmanually assessed by the Human to build a data table. The data tables from the \nGround truth, the AI and the Human were analyzed in the standard statistical fashion. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 16\n \n \nFigure 3: Treatment effect. Shown here are the 50% responder rate (RR50) and the \nmedian percentage change (MPC) from the placebo and drug arms of the simulated \nstudy. Three colors are shown: ground truth (black), AI estimated (blue), and human \nreviewed (red). All three were similar though not identical. Nevertheless, both the AI and \nthe human would conclude that the drug is dramatically better than placebo. \n  \n16\nd \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 17\n \nFigure 4: Symptom list. Shown here are the symptoms found in either drug or placebo \ngroups. The ground truth (black), AI derived (blue), and human reviewed (red) bars \nindicate the fraction of each group that reported the specific symptom. Not all bars \nmatch, however the general trend is that they are within 3% of each other. \n \n  \n17\n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n 18\n \n Ground truth AI Human \nRR50a 38%  \np=1*10-10 \n34% \np=3*10-7 \n35%  \np=8*10\n-8 \nMPCb 54%  \np=1*10-15 \n61%  \np=8*10\n-11 \n62%  \np=1*10\n-12 \nTable 1: The marginal difference between placebo and drug efficacy using the 50%-\nresponder method (RR50) or the median percentage change (MPC) methods. \na RR50p \nvalues are computed using Fisher Exact Test. b MPC p values were computed using \nMann-Whitney U test. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.18.24304493doi: medRxiv preprint ",
  "topic": "Randomized controlled trial",
  "concepts": [
    {
      "name": "Randomized controlled trial",
      "score": 0.6417589783668518
    },
    {
      "name": "Epilepsy",
      "score": 0.6113121509552002
    },
    {
      "name": "Clinical trial",
      "score": 0.5563539266586304
    },
    {
      "name": "Placebo",
      "score": 0.5550553798675537
    },
    {
      "name": "Pipeline (software)",
      "score": 0.518509030342102
    },
    {
      "name": "Cohort",
      "score": 0.4471747875213623
    },
    {
      "name": "Computer science",
      "score": 0.40602245926856995
    },
    {
      "name": "Medicine",
      "score": 0.4004612863063812
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3587722182273865
    },
    {
      "name": "Psychology",
      "score": 0.3321858048439026
    },
    {
      "name": "Psychiatry",
      "score": 0.245764821767807
    },
    {
      "name": "Alternative medicine",
      "score": 0.19336003065109253
    },
    {
      "name": "Internal medicine",
      "score": 0.18064257502555847
    },
    {
      "name": "Pathology",
      "score": 0.12079688906669617
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799899409",
      "name": "Hadassah Medical Center",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}