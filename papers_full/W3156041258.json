{
  "title": "Generating bug-fixes using pretrained transformers",
  "url": "https://openalex.org/W3156041258",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3086233478",
      "name": "Dawn Drain",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2104953429",
      "name": "Chen Wu",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2627672354",
      "name": "Alexey Svyatkovskiy",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1981173961",
      "name": "Neel Sundaresan",
      "affiliations": [
        "Microsoft (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2153881107",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2951732578",
    "https://openalex.org/W2766875678",
    "https://openalex.org/W6771879896",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963881719",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2995903212",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W3021762813",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2970521905",
    "https://openalex.org/W2145124323",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2995333547",
    "https://openalex.org/W2977610714"
  ],
  "abstract": "Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.",
  "full_text": "Generating Bug-Fixes Using Pretrained Transformers\nDawn Drain\nMicrosoft\ndawn.drain@microsoft.com\nChen Wu\nMicrosoft\nwu.chen@microsoft.com\nAlexey Svyatkovskiy\nMicrosoft\nalsvyatk@microsoft.com\nNeel Sundaresan\nMicrosoft\nneels@microsoft.com\nAbstract\nDetecting and ﬁxing bugs are two of the most\nimportant yet frustrating parts of the soft-\nware development cycle. Existing bug de-\ntection tools are based mainly on static ana-\nlyzers, which rely on mathematical logic and\nsymbolic reasoning about the program execu-\ntion to detect common types of bugs. Fix-\ning bugs is typically left out to the devel-\noper. In this work we introduce DeepDebug:\na data-driven program repair approach which\nlearns to detect and ﬁx bugs in Java meth-\nods mined from real-world GitHub reposito-\nries. We frame bug-patching as a sequence-\nto-sequence learning task consisting of two\nsteps: (i) denoising pretraining, and (ii) super-\nvised ﬁnetuning on the target translation task.\nWe show that pretraining on source code pro-\ngrams improves the number of patches found\nby 33% as compared to supervised training\nfrom scratch, while domain-adaptive pretrain-\ning from natural language to code further im-\nproves the accuracy by another 32%. We reﬁne\nthe standard accuracy evaluation metric into\nnon-deletion and deletion-only ﬁxes, and show\nthat our best model generates 75% more non-\ndeletion ﬁxes than the previous state of the art.\nIn contrast to prior work, we attain our best\nresults when generating raw code, as opposed\nto working with abstracted code that tends to\nonly beneﬁt smaller capacity models. Finally,\nwe observe a subtle improvement from adding\nsyntax embeddings along with the standard po-\nsitional embeddings, as well as with adding an\nauxiliary task to predict each token’s syntactic\nclass. Despite focusing on Java, our approach\nis language agnostic, requiring only a general-\npurpose parser such as tree-sitter1.\n1 Introduction\nEarly results in automated bug-patching assumed a\nset of test functions and took the approach of gen-\nerating patches until one passed the test suite. The\n1https://tree-sitter.github.io/tree-sitter/\nedits were operationalized as abstract-syntax tree\n(AST) manipulations such as deleting, inserting,\nor swapping nodes. The seminal work GenProg\n(Le Goues et al., 2012) combined these operations\nwith a genetic algorithm and claimed to ﬁx 55 out\nof 105 bugs in 5.1m LOC across eight projects with\nover 10k test cases. However, a followup paper (Qi\net al., 2015) found that 53 out of the 55 claimed\nﬁxes were merely test-suite adequate, and that most\nof them were generated simply using the deletion\noperator.\nMore recent work has explored patch generation\nusing a more stringent criterion: produce exactly\nthe same ﬁx as the developer who originally ﬁxed\nthe bug. We use the Java dataset from Patches\nin the Wild (Tufano et al., 2019), which is com-\nprised of short methods extracted from commits\nwhose messages indicate they are bug-ﬁxing. Copy\nThat! (Panthaplackel et al., 2020) applies a novel\nspan-copying mechanism on this dataset, achieving\nimpressive top-1 accuracy. SequenceR (Chen et al.,\n2018) both narrows and expands on the Patches in\nthe Wild dataset by focusing on one-line changes\nonly, while also providing additional context be-\nyond the buggy method by including other meth-\nods’ signatures. This extended context gives a 15%\nrelative boost. All three approaches see large gains\nfrom copying, which is sensible given that the ﬁxed\nmethod has a large overlap with the buggy method.\nENCORE (Lutellier et al., 2019) also looks at one-\nline changes, and in more languages, although they\ngive only the buggy line as input and do not provide\nany further context.\nFor certain bug-types it is possible to generate\nmillions of synthetic bugs to train on. Devlin et\nal. (Devlin et al., 2017) train an RNN on Python\nto ﬁx incorrect comparison operators, the mistaken\nuse of “is” vs. “is not”, variable misuse, and for-\ngotten “self” accessors. Overall, they achieve 86%\naccuracy on synthetic bugs and 41% on real-life\nbugs. Kanade et al. (Kanade et al., 2019) pretrain a\narXiv:2104.07896v2  [cs.CL]  28 Apr 2021\nBERT-style model “CuBERT” on a larger Python\ndataset and then ﬁnetune on a related suite of syn-\nthetic bugs, achieving over 90% accuracy.\nIn the broader ﬁeld of NLP, our task is closest\nin spirit to grammatical error correction (GEC).\nDrawing from the CoNLL GEC leaderboard (Ng\net al., 2014), we see several different approaches\nusing transformers. Kiyano et al. see large gains\nfrom data augmentation via noisy backtranslation,\nin addition to more standard techniques like inject-\ning spelling errors, applying left-to-right rerank-\ning, and using a sentence-level error detection task\n(Kiyono et al., 2019). Kaneko et al. use a seq2seq\nmodel where the input embeddings are concate-\nnated with the embeddings produced by a BERT\nﬁnetuned on grammatical error detection (detect-\ning whether a given token is faulty) (Kaneko et al.,\n2020). Zhao et al. (Zhao et al., 2019) apply a copy-\nmechanism, in addition to injecting artiﬁcial noise\nand using an auxiliary token-labeling task. Awasthi\net al. (Awasthi et al., 2019) pursue an encoder-only\napproach, which maps tokenzito the edit operation\nto perform at the ith location. They see large speed\ngains, and also investigate applying their model\niteratively.\nExpanding more broadly, still, various task-\nagnostic pretraining approaches like BERT, BART,\nand T5 (Devlin et al., 2017; Lewis et al., 2020;\nRaffel et al., 2019) have seen large performance\ngains on composite benchmarks like GLUE (Wang\net al., 2018). These models are typically pretrained\nusing a denoising objective on a large corpus of\nsynthetically corrupted text.\nWe retain the scope of work trying to ﬁx generic\nbugs while taking advantage of techniques for pre-\ntraining powerful sequence-to-sequence transform-\ners. We ﬁrst treat raw code as text and use a span-\nmasking objective to pretrain a 400 million parame-\nter encoder-decoder transformer on 67k Java repos-\nitories mined from GitHub before ﬁnetuning on the\nPatches in the Wildbenchmark. More speciﬁcally,\nwe consider three pretraining experiments: pre-\ntrain on Java only; ﬁnetune directly from the strong\nbaseline BART which was pretrained on English;\nand further pretrain on Java with a warmstart from\nBART. Pretraining on Java alone improves the num-\nber of patches found by a third, from 1049/12380 to\n1392/12380, while pretraining with a warmstart fur-\nther improves by another third to 1839/12380. We\nreﬁne the standard top-1 accuracy metric used for\nthis benchmark into non-deletion and deletion-only\nﬁxes, and show that our best DeepDebug model\ngenerates 75% more non-deletion ﬁxes than the pre-\nvious state of the art. In contrast to prior work, we\nattain our best results when generating raw code, as\nopposed to working with abstracted code that tends\nto beneﬁt smaller models. Finally, we see a sub-\ntle improvement from using the formal grammar\nunique to programming languages. Speciﬁcally, we\nexperiment with adding syntax embeddings along\nwith the standard positional embeddings, as well\nas with adding an auxiliary task to predict each\ntoken’s syntactic class.\n2 Problem overview\nThe task of patch generation is as follows: given\na buggy Java method (found by mining commit\nmessages), produce the exact same sequence of\ntokens as the developer who committed the ﬁx.\nExample Found Fix\ndouble a_ods_light_Detected() {\ndouble l_return = 0.0;\nif ((v_sensor_ods) != null) {\nv_sensor_ods.getLightDetected();\n}\nreturn l_return;\n}\ndouble a_ods_light_Detected() {\ndouble l_return = 0.0;\nif ((v_sensor_ods) != null) {\nl_return |\\colorbox{green}{=}| v_sensor_ods.getLightDetected();\n}\nreturn l_return;\n}\nFigure 1: Our model makes sure to capture the\nvalue returned by the getter method.\nDuring ﬁnetuning, we train our sequence-to-\nsequence transformer so as to minimize the ex-\npected log-probability of generating the ﬁx given\nthe buggy method. Formally, let B = {b1,...,b n}\nand C = {c1,...,c n}be the sets of buggy and\ncorrect functions respectively, where we can tok-\nenize ci = ci,1,...,c i,ki . Let θdenote the model\nparameters. The objective is thus:\nargminθEbi∈Blog[P(ci,1|bi; θ) ·P(ci,2|bi,ci,1; θ)\n···P(ci,ki |bi,c1,...,c ki−1; θ)].\nMore simply, we seek\nargminθEbi∈BlogP(ci|bi; θ).\n3 Data\nWe evaluate on the Java dataset from Patches in\nthe Wild (Tufano et al., 2019). Tufano et al. mined\nall the public, non-fork repositories on Github up\nuntil 2016 for commits to Java code that include\nExample Found Fix\nprivate double normalizeTime(double time,\nec.graph.GraphInitializer init) {\nif (((init.maxTime) - (init.minTime)) == 0.0)\nreturn 1.0;\nelse\nreturn |\\colorbox{red}{((init.maxTime) - time)}|\n/ ((init.maxTime) - init.minTime);\n}\nprivate double normalizeTime(double time,\nec.graph.GraphInitializer init) {\nif (((init.maxTime) - (init.minTime)) == 0.0)\nreturn 1.0;\nelse\nreturn |\\colorbox{green}{(time - (init.minTime))}|\n/ ((init.maxTime) - init.minTime));\n}\nFigure 2: A clever ﬁx found by our model. It manip-\nulates the return value so as to normalizeminTime\nto 0 and maxTime to 1, whereas the developer orig-\ninally had the boundary values reversed.\na commit message containing at least one of the\nfollowing words: bug, error, issue or ﬁx, patch,\ncorrect, which comprises about 5% of all commits.\nFrom these commits they extracted all modiﬁed\nmethods with at most one hundred tokens, and\nbroke them into two datasets: a dataset of small\nmethods containing less than 50 tokens and a com-\nplementary dataset of medium methods containing\nup to 100 tokens.\nTo facilitate learning and to hasten conver-\ngence, Tufano et al. abstracted the code snip-\npets while preserving necessary syntactical infor-\nmation. An example is given in ﬁgure 3. To\nabstract the code snippets, they replaced all to-\nkens outside a whitelist of approximately 500\ncommonly occurring tokens with abstractions of\nthe form METHODi, VARIABLEi, STRING_LITi,\nNUM_LITi, and TYPEi. Finally, they introduce id-\nioms – frequently occurring programming language\nidentiﬁers and literals, to which abstraction is not\napplied. Figure 3 illustrates differences between\nthese code representations.\nAs part of normalization the authors of the\ndataset also stripped comments and irrelevant\nwhitespaces and indentation. In this paper, we\npresent results both on the concrete code, and ab-\nstracted code with idioms. We obtain superior re-\nsults on concrete code, especially after pretraining.\n4 Baseline\nAbstracting code removes the valuable semantics\nof variable and function names, and insisting on\nthe verbatim ﬁx is very strict, but it is nevertheless\npossible to obtain remarkably high accuracy under\nthese conditions. Using a 10 million parameter\nbidirectional LSTM with copy mechanism and the\ndefault hyperparameters from Google’s Seq2Seq\nTufano et al. were able to get 9.2% exact accu-\nracy on small methods with less than 50 tokens,\nand 3.2% accuracy on medium methods with be-\ntween 50 and 100 tokens. Even more impressively,\nCopy That! (Panthaplackel et al., 2020) got 17.7%\nand 8.0% accuracy respectively using their bi-GRU\nimplementation with a novel, span-copying mecha-\nnism.\nWe speculate that 17.7% and 8.0% are not far\nfrom the best practical top-1 accuracy on this task.\n“Bug-ﬁxing commits” mined based on their com-\nmit message tend to be very diverse and noisy, in\ncontrast to methods that are truly known to contain\na bug, especially from a small class of synthetic\nbugs.\n5 Model\nOur DeepDebug model uses the standard trans-\nformer (Vaswani et al., 2017) architecture. Copy\nmechanisms have been shown to give a dramatic\nboost to small LSTM models (Chen et al., 2018),\nand to give a small boost to transformers for espe-\ncially relevant tasks like summarization (See et al.,\n2017) and grammar correction (Zhao et al., 2019).\nWe use copy-attention when training our 60 million\nparameter models from scratch and when replicat-\ning the state of the art. We do not use copy attention\nin our full-scale pretraining experiments, due pri-\nmarily to technical limitations.\nWe use a Byte Pair Encoding (BPE) vocabulary\nof roughly 50k tokens for concrete code and a vo-\ncabulary of all 433 unique tokens for abstracted\ncode (Sennrich et al., 2016). In order to form the\nBPE vocabulary, we take the existing byte-level\nBPE tokenizer utilized by each of the GPT-2 (Rad-\nford and Wu, 2019), BART, and RoBERTa, and\nappend whitespace tokens in the order that they\nare learned by a tokenizer trained from scratch on\nraw code. Adding whitespace tokens effectively in-\ncreases the context window length and processing\nspeed by around 40% during pretraining, although\nthe improvement is smaller during ﬁnetuning as we\nrestrict our focus to left-adjusted methods. Extend-\ning the pre-existing tokenizer allows us to reuse\npublic checkpoints trained on English while efﬁ-\nciently adapting them to source code.\nWe experiment with adding syntactic embed-\ndings along with the standard vocabulary and posi-\ntional embeddings, as demonstrated in ﬁgure 4. In\nthis way we immediately distinguish whether BPE\nFigure 3: Raw code, fully abstracted code, and code abstracted with idioms. We ﬁnd that smaller models perform\nbest on abstracted code, whereas higher-capacity models are better able to use the semantics associated to concrete\ncode.\ntokens like “return” or “static” are key words. In\ngeneral we relieve the model of some of the dif-\nﬁculty of parsing the language’s formal grammar,\nwhich is disproportionately difﬁcult for transform-\ners (Hahn, 2020). We can also think of this tactic\nas subsuming both the approach of using raw code\nand the approach of using abstracted code.\n6 Pretraining\nWe use the same span-masking objective used by\nT5 (Raffel et al., 2019). That is, we randomly select\nspans of three consecutive tokens and replace each\nspan with a single mask token. The goal is to repro-\nduce the masked tokens in order, reusing the mask\ntoken in the target by putting it before every span\nof three tokens. We take advantage of the fact that\ncode is lower-entropy than English. For instance,\nGPT-C attains perplexities in the range of 1.3 –\n2 for several programming languages, whereas a\ncomparably sized GPT-2 model attains a perplex-\nity of 26 on Wikitext-3 (Svyatkovskiy et al., 2020;\nRadford and Wu, 2019). Thus we mask a larger\nfraction of 30% of all tokens. Masking a larger\nfraction also has the added advantage of increasing\nthe effective context length during pretraining, by\nabout 50% in our case.\nWe pretrain on all open-source Java repositories\nhosted on GitHub with at least 10 stars that have\nbeen updated in the last ﬁve years, 67,000 reposito-\nries in total. We deduplicate ﬁles based on ﬁle hash.\nFinally, we ﬁlter out ﬁles based on heuristics like if\nthey contain data or have been auto-generated, and\nclean ﬁles by removing licenses and non-ASCII\ncharacters. This ﬁnal stage of ﬁltering and cleaning\nremoves 10% of the remaining tokens. After tak-\ning these steps, we are left with 67k repositories,\n8 million ﬁles, 1.3 billion lines of code, 13 billion\ntokens, or 54GB.\nWe pretrain for ten epochs, which takes one\nweek on a DGX-2 box, which has sixteen 32GB\nV100 GPUs. We estimate this is approximately 4%\nof the resources used to pretrain the public English\nBART checkpoint released by Facebook, which\nwas pretrained for forty epochs on 160GB using a\nmore compute-intensive pretraining objective.\n7 Experiments\nWe ﬁrst examine training on concrete code as com-\npared to abstracted code (with the 433 most com-\nmon tokens added as idioms) from scratch using a\n60 million parameter transformer with a copy mech-\nanism as implemented in the OpenNMT framework\n(Klein et al., 2017).\nWe then experiment with several pretraining\nstrategies implemented in the Fairseq framework\n(Ott et al., 2019). Speciﬁcally, we consider three\nfollowing experiments: i) pretrain on Java only, ii)\nﬁnetune directly from the strong baseline BART\nwhich was pretrained on English; and iii) domain-\nadaptive pretraining on Java with a warmstart from\nBART. We hypothesized that warmstarting from\nEnglish would help, since learning how to program\nis much easier if one already knows English and\nhas an intuitive understanding of concepts like dic-\ntionaries, graphs, or lists, and can read out these\nroles from self-documenting method and variable\nnames.\nWe next investigate the effect of adding an\nauxiliary token-type labeling loss on top of the\nstandard next-token prediction loss, as demon-\nstrated in Figure 6. We use the same token types\nused during abstraction i.e. METHOD, VARIABLE,\nFigure 4: We experiment with adding syntax embeddings along with the standard positional and vocabulary em-\nbeddings during encoding. We ﬁnd that this gives a small beneﬁt when training from scratch, but no beneﬁt when\nthe syntax embeddings are added only during ﬁnetuning.\nSpan-masking Input and Output\nprivate double normalizeTime(<mask>.GraphInitializer init) {\nif (((init.maxTime) - <mask>0.0)\nreturn 1.0;\nelse\n<mask> (init.minTime)) /\n((init.maxTime) - (init.minTime));\n}\n<mask>double time, ec.graph <mask> (init.minTime)) ==\n<mask> return (time -\nFigure 5: Example of span-masking used when pre-\ntraining our sequence-to-sequence model. We reuse\na single mask token and cover exactly three tokens\nwith each mask.\nSTRING_LIT, and NUM_LIT. Syntax highlight-\ning is a ubiquitous developer tool, and especially\nhelpful during the early stages of learning a lan-\nguage before the developer has internalized its for-\nmal grammar.\nOur ﬁnal experiment more closely investigates\nsyntax highlighting. During encoding, we add a\ntoken-type embedding to the standard positional\nand vocabulary embeddings, as demonstrated in\nﬁgure 4. For both of these token-type experiments\nwe ﬁnd a small improvement when training from\nscratch, but no improvement when adding them\nduring ﬁnetuning only. We also partially replicate\nand extend the work from Copy That!, by evaluat-\ning on small methods on both concrete and fully\nabstracted code without idioms. We use the code\nfor the specialized span-copying GRU model gen-\nerously shared by the author.\n8 Results\nWe found that many of the ﬁxes fell into a few unin-\nspiring patterns, especially given the limited con-\ntext of the methods: Simple deletions, as with clas-\nsic models like GenProg; swapping “protected”,\n“private”, and “public”; and inserting “native”.\nIndeed, roughly two-thirds of all the ﬁxes we\nﬁnd on ﬁrst attempt are simply deletions. For our\nstrongest models, 7% of the non-deletion ﬁxes are\nsome kind of protected/private/public swapping or\ninserting the word “native”, while the fraction is as\nhigh as 50% for the weakest models we consider.\nIn presenting our results, we thus also give the\nnumber of ﬁxes that were simply deletions and the\ncomplementary number that are non-deletion ﬁxes.\nWhen considering ﬁxes that our model did not\nﬁnd, the overriding theme is that the task is impos-\nsible without more context. In theory, bug-ﬁxing\nshould be nearly deterministic; code can be edited\nin nearly arbitrary ways while implementing or\noptimizing additional functionality, but a bug is a\nglaring error that needs to be addressed.\nIn practice, the correct behavior is often under-\nspeciﬁed, especially since our approach excludes\nuseful information such as test cases, example us-\nages, and the class constructor. For example, con-\nsider the function pop() from ﬁgure 8, which our\nmodel did not ﬁx on its ﬁrst attempt. Off-by-one\nerrors are common. However, given only the buggy\nmethod, it is ambiguous whether the head index\nshould be bounded below by zero or negative one.\n9 Analysis\nIn this section, we draw several conclusions from\nthe results of the experiments presented in sec-\ntion 8.\nFirst of all, we see that producing ﬁxes for\nmedium-sized methods is harder than for short\nmethods. This is unsurprising since longer methods\nare more complicated and can be edited in more\nways.\nOur next observation is that pretraining helps,\nespecially for medium methods. Indeed, the num-\nber of medium ﬁxes found increases by a factor of\nFigure 6: We experiment with adding an auxiliary token-type classiﬁcation loss for multitask training. In this\ncondition, the loss is a weighted sum of the standard next-token prediction loss and the auxiliary loss of predicting\nwhether each subword token is a part of aTYPE, METHOD, VARIABLE, STRING_LIT, NUM_LIT, or other token.\nTable 1: Transformer Results for Concrete vs. Abstract code. Contrary to the results for smaller GRU-based\nmodels, we see that training on concrete code helps.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nabstract with idioms medium,\nfrom scratch,\n60M parameters\n11 (0.2%) 178 (2.7%) 6545 166\nconcrete medium,\nfrom scratch,\n60M parameters\n20 (0.3%) 244 (3.7%) 6545 224\nabstract with idioms small,\nfrom scratch,\n60M parameters\n91 (1.6%) 648 (11.1%) 5835 557\nconcrete small,\nfrom scratch,\n60M parameters\n349 (6.0%) 854 (14.6%) 5835 505\nExample Found Fix\npublic void backward() {\nswitch (heading) {\ncase \"N\" :\n|\\colorbox{red}{(x)++}|;\ndefault :\n}\n}\npublic void backward() {\nswitch (heading) {\ncase \"N\" :\n|\\colorbox{green}{(y)--}|;\nbreak;\ndefault :\nbreak;\n}\n}\nFigure 7: Our model infers from the function name\n“backward” that the function should be decrement-\ning rather than incrementing.\nthree when adding pretraining.\nWe see that the smaller GRU models and the\ntransformers trained on abstract code with idioms\nare especially destructive, producing seven times as\nmany deletions as constructive ﬁxes. The smaller\nGRU models beneﬁt from abstracting code, al-\nthough the larger transformers do not.\nExample Fix not Found on First Attempt\npublic G pop() {\nG output = storage[i_head];\nif ((i_head) |\\colorbox{red}{>}| 0) {\ni_head = (i_head) - 1;\n}else {\n}\nreturn output\n}\npublic G pop() {\nG output = storage[i_head];\nif ((i_head) |\\colorbox{green}{>=}| 0) {\ni_head = (i_head) - 1;\n}else {\n}\nreturn output\n}\nFigure 8: It is ambiguous whether this off-by-one\nerror is a bug without seeing the deﬁnitions of stor-\nage or i_head.\nPretraining on English boosts performance of\nprogram repair, presumably due to the semantics\ninherent in variable and function names. A func-\ntion name like getMinElement is intuitive even\nfor a non-programmer. Indeed, the public BART\ncheckpoint, which was pretrained only on English,\noutperforms our model pretrained only on Java, al-\nTable 2: GRU Span-copying Results for Small Java Methods, our replication of the previous state of the art\n(Panthaplackel et al., 2020). Our pretrained model ﬁnds a slightly higher total number of ﬁxes, and 75% more\nnon-deletion ﬁxes.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nfully abstract small,\nfrom scratch,\n250K parameters 207 (3.5%) 991 (17.0%) 5835 784\nconcrete small,\nfrom scratch,\n250K parameters\n109 (1.5%) 537 (9.2%) 5835 428\nTable 3: GRU Classical Copy Mechanism Results for Small Java Methods, model also provided by (Panthaplackel\net al., 2020). Unlike the transformers, the smaller, GRU-based models did not beneﬁt from training on concrete\ncode; on abstracted code they achieved both higher top-1 performance and a better ratio of non-deletion to deletion\nﬁxes.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nfully abstract small,\nfrom scratch,\n250K parameters 125 (2.1%) 672 (11.5%) 5835 547\nconcrete small,\nfrom scratch,\n250K parameters\n50 (0.9%) 397 (6.8%) 5835 347\nthough our Java-only model was trained for only\n4% as long as the English-only checkpoint. Our\nbest results are obtained by taking the powerful\nEnglish model and further pretraining it on Java.\nWe observe that adding an auxiliary syntax-\nlabeling task helps somewhat when training from\nscratch, but makes no difference when we ﬁnetune\nthe pretrained model. We hypothesize this is be-\ncause the pretrained model already has a strong\nunderstanding of the language’s syntax, and am-\nple syntactical feedback is already provided by the\nstandard next-token prediction task.\nWe see underwhelming returns in scaling from\n60 million to 400 million parameters when training\nfrom scratch, especially in light of the dramatic\nand predictable gains from scaling up transformers\nsuch as T5 or GPT (Raffel et al., 2019; Kaplan et al.,\n2020). We hypothesize that this is because of the\nsmall scale of training data. Each of our training\nsets contains only around 50k examples comprising\nmere tens of megabytes, whereas BART-large has\n400 million parameters and is nearly a full gigabyte\nwhen loaded in FP16.\nFinally, similarly to adding the syntax-labeling\ntask, we see a small beneﬁt from adding syntax\nhighlighting in the form of adding token-type em-\nbeddings to the vocabulary embeddings during en-\ncoding, but no beneﬁt from adding those embed-\ndings during ﬁnetuning. It makes sense that there\nshould be some small beneﬁt from syntax highlight-\ning. We hypothesize that the lack of beneﬁt from\nadding highlighting during ﬁnetuning is due to the\npretrained model already having developed a way\nto represent different token types, which clashes\nwith the added embeddings.\n10 Conclusions and Future Work\nWe introduced DeepDebug, a data-driven pro-\ngram repair approach which learns to ﬁx bugs in\nreal-world code using pretrained transformers. We\nachieve new state of the art by following a multi-\nstage pretraining pipeline, in which we warmstart\nfrom a public English checkpoint, further pretrain\non code, and ﬁnally ﬁnetune on the target task.\nWe show that these large transformers are able to\ntake advantage of meaningful function and variable\nnames, unlike smaller GRU models which perform\nbetter on abstracted code. We also investigate the\nextent to which our ﬁxes are constructive, rather\nthan mere deletions, and ﬁnd that we create 75%\nmore non-deletion ﬁxes than the previous state of\nthe art.\nIn future work we are interested in expanding to\nmultiple languages and considering iterative ﬁxes.\nTable 4: Transformer results for different pretraining workﬂows on medium methods. The best result is obtained\nby ﬁrst pretraining on English and then on Java. Note that English pretraining used twenty-ﬁve times as much\ncompute as Java pretraining.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nconcrete medium,\nfrom scratch,\n400M parameters\n72 (1.1%) 233 (3.6%) 6545 161\nconcrete medium,\nJava pretrain only,\n400M parameters\n98 (1.5%) 413 (6.3%) 6545 315\nconcrete medium,\nEnglish pretrain only,\n400M parameters\n143 (2.2%) 439 (6.7%) 6545 296\nconcrete medium,\nEnglish plus Java,\n400M parameters\n259 (4.0%) 749 (11.4%) 6545 490\nTable 5: Transformer results for different pretraining workﬂows on small methods. The English BART baseline\nis very strong, although the best performance on the standard top-1 accuracy metric is still obtained by further\npretraining on Java.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nconcrete small,\nfrom scratch,\n400M parameters\n328 (5.6%) 816 (13.9%) 5835 488\nconcrete small,\nJava pretrain only,\n400M parameters\n330 (5.7%) 979 (16.8%) 5835 649\nconcrete small,\nEnglish pretrain only,\n400M parameters\n385 (6.6%) 976 (16.7%) 5835 591\nconcrete small,\nEnglish plus Java,\n400M parameters\n361 (6.2%) 1090 (18.7%) 5835 729\nTable 6: Effects of adding a syntax-type labeling task and of adding syntax-type embeddings during encoding.\nThere is a small improvement for multitask training from scratch, but no improvement for adding the additional\ntask during ﬁnetuning, presumably because the labeling task is too easy for the pretrained model. There is similarly\na small beneﬁt from adding syntax embeddings when training from scratch, perhaps the lack of beneﬁt during\nﬁnetuning is because the added embeddings clash with the pretrained model’s representation of different token\ntypes.\nNon-deletion Fixes All Fixes All Methods Deletion Fixes\nconcrete small,\nfrom scratch,\n400M parameters\n328 (5.6%) 816 (13.9%) 5835 488\nconcrete small,\nfrom scratch,\n400M parameters,\nsyntax token embedding\n343 (5.9%) 847 (14.4%) 5835 504\nconcrete small,\nfrom scratch,\n400M parameters,\nsyntax token prediction\n340 (5.8%) 862 (14.7%) 5835 522\nconcrete small,\nEnglish plus Java,\n400M parameters,\n361 (6.2%) 1090 (18.7%) 5835 729\nconcrete small,\nEnglish plus Java,\n400M parameters,\nsyntax token embedding\n355 (6.1%) 1082 (18.5%) 5835 727\nconcrete small,\nEnglish plus Java,\n400M parameters,\nsyntax token prediction\n357 (6.1%) 1097 (18.8%) 5835 740\nTable 7: Comparison bewteen prior SOTA results and our best model, which was warmstarted from BART and\nthen further pretrained on Java. In table 2 we partially replicate Panthaplackel et al.’s small copy-span model and\nﬁnd that they only produce 57% as many non-deletion ﬁxes as our approach.\nMedium Fixes Small Fixes Overall Parameters\nTufano et al.\n(Tufano et al., 2019) 209 (3.2%) 537 (9.2%) 746 (6.0%) 10M\nPanthaplackel et al.\n(Panthaplackel et al., 2020) 524 (8.0%) 1033 (17.7%) 1557 (12.6%) 0.25M\nCodeBERT\n(Feng et al., 2020) 340 (5.2%) 957 (16.4%) 1297 (10.5%) 180M\nBART\n(Lewis et al., 2020) 439 (6.7%) 976 (16.7%) 1415 (11.4%) 400M\nDeepDebug\n(Ours) 749 (11.4%) 1090 (18.7%) 1839 (14.9%) 400M\nReferences\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,\nSabyasachi Ghosh, and Vihari Piratla. 2019. Parallel\niterative edit models for local sequence transduction.\narXiv preprint arXiv:1910.02893.\nZimin Chen, Steve Kommrusch, Michele Tufano,\nLouis-Noël Pouchet, Denys Poshyvanyk, and Mar-\ntin Monperrus. 2018. Sequencer: Sequence-to-\nsequence learning for end-to-end program repair.\nJacob Devlin, Jonathan Uesato, Rishabh Singh, and\nPushmeet Kohli. 2017. Semantic code repair using\nneuro-symbolic transformation networks.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics ,\n8:156–171.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2019. Pre-trained contextual em-\nbedding of source code.\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun\nSuzuki, and Kentaro Inui. 2020. Encoder-Decoder\nModels Can Beneﬁt from Pre-trained Masked Lan-\nguage Models in Grammatical Error Correction.\narXiv e-prints, page arXiv:2005.00987.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels.\nShun Kiyono, Jun Suzuki, Masato Mita, Tomoya Mizu-\nmoto, and Kentaro Inui. 2019. An empirical study\nof incorporating pseudo data into grammatical error\ncorrection. Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP).\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nClaire Le Goues, Michael Dewey-V ogt, Stephanie For-\nrest, and Westley Weimer. 2012. A systematic study\nof automated program repair: Fixing 55 out of 105\nbugs for $8 each. In Proceedings of the 34th Inter-\nnational Conference on Software Engineering, ICSE\n’12, page 3–13. IEEE Press.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nThibaud Lutellier, Lawrence Pang, Viet Hung Pham,\nMoshi Wei, and Lin Tan. 2019. Encore: Ensemble\nlearning using convolution neural machine transla-\ntion for automatic program repair.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task on\ngrammatical error correction. In Proceedings of the\nEighteenth Conference on Computational Natural\nLanguage Learning: Shared Task, pages 1–14, Balti-\nmore, Maryland. Association for Computational Lin-\nguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling.\nSheena Panthaplackel, Miltiadis Allamanis, and Marc\nBrockschmidt. 2020. Copy that! editing se-\nquences by copying spans. arXiv preprint\narXiv:2006.04771.\nZichao Qi, Fan Long, Sara Achour, and Martin Rinard.\n2015. An analysis of patch plausibility and correct-\nness for generate-and-validate patch generation sys-\ntems. In Proceedings of the 2015 International Sym-\nposium on Software Testing and Analysis , ISSTA\n2015, page 24–36, New York, NY , USA. Associa-\ntion for Computing Machinery.\nAlec Radford and Jeffrey Wu. 2019. Rewon child,\ndavid luan, dario amodei, and ilya sutskever. 2019.\nLanguage models are unsupervised multitask learn-\ners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint\narXiv:1704.04368.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020. Intellicode compose:\nCode generation using transformer. Proceedings\nof the 28th ACM Joint Meeting on European Soft-\nware Engineering Conference and Symposium on\nthe Foundations of Software Engineering.\nMichele Tufano, Cody Watson, Gabriele Bavota, Mas-\nsimiliano Di Penta, Martin White, and Denys Poshy-\nvanyk. 2019. An empirical study on learning bug-\nﬁxing patches in the wild via neural machine trans-\nlation. ACM Trans. Softw. Eng. Methodol., 28(4).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nWei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and\nJingming Liu. 2019. Improving grammatical error\ncorrection via pre-training a copy-augmented archi-\ntecture with unlabeled data. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 156–165, Minneapolis, Min-\nnesota. Association for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8620287179946899
    },
    {
      "name": "Java",
      "score": 0.6320521831512451
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.567603349685669
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5401292443275452
    },
    {
      "name": "Transformer",
      "score": 0.5100350975990295
    },
    {
      "name": "Programming language",
      "score": 0.4879969656467438
    },
    {
      "name": "Task (project management)",
      "score": 0.4862891137599945
    },
    {
      "name": "Syntax",
      "score": 0.4562389552593231
    },
    {
      "name": "Metric (unit)",
      "score": 0.4430859386920929
    },
    {
      "name": "Code (set theory)",
      "score": 0.4356827437877655
    },
    {
      "name": "Source code",
      "score": 0.4332001209259033
    },
    {
      "name": "Software bug",
      "score": 0.41736334562301636
    },
    {
      "name": "Machine learning",
      "score": 0.3902824819087982
    },
    {
      "name": "Natural language processing",
      "score": 0.38248535990715027
    },
    {
      "name": "Software",
      "score": 0.35247403383255005
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}