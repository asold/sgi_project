{
  "title": "SCOTCH and SODA: A Transformer Video Shadow Detection Framework",
  "url": "https://openalex.org/W4309131501",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3163842034",
      "name": "Liu, L.",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2135590253",
      "name": "Prost, J.",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Université de Bordeaux",
        "Institut Polytechnique de Bordeaux"
      ]
    },
    {
      "id": "https://openalex.org/A2793437486",
      "name": "Zhu L",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Papadakis N.",
      "affiliations": [
        "Institut Polytechnique de Bordeaux",
        "Centre National de la Recherche Scientifique",
        "Université de Bordeaux"
      ]
    },
    {
      "id": null,
      "name": "Lio P.",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4299323858",
      "name": "Schönlieb, C. B.",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": null,
      "name": "Aviles-Rivero A. I.",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963299740",
    "https://openalex.org/W3175990321",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W2916797271",
    "https://openalex.org/W2801375052",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6800008755",
    "https://openalex.org/W3035462037",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W6754955957",
    "https://openalex.org/W2966415767",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6685670348",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W1983364832",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3120736405",
    "https://openalex.org/W6768817161",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2986056979",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6755325445",
    "https://openalex.org/W6810026836",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2955084925",
    "https://openalex.org/W4312545381",
    "https://openalex.org/W6785719201",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W4214516465",
    "https://openalex.org/W3008820576",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W4221162230",
    "https://openalex.org/W2948670693",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W6797263693",
    "https://openalex.org/W2807746031",
    "https://openalex.org/W3034627419",
    "https://openalex.org/W6841064876",
    "https://openalex.org/W6797206543",
    "https://openalex.org/W3175028288",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W4312658081",
    "https://openalex.org/W4286758848",
    "https://openalex.org/W4287122452",
    "https://openalex.org/W4388837990",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3191483857",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3100135546",
    "https://openalex.org/W2983819436",
    "https://openalex.org/W4312433277",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963149042",
    "https://openalex.org/W3170860705",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2895340898",
    "https://openalex.org/W2895126795"
  ],
  "abstract": "Shadows in videos are difficult to detect because of the large shadow deformation between frames. In this work, we argue that accounting for shadow deformation is essential when designing a video shadow detection method. To this end, we introduce the shadow deformation attention trajectory (SODA), a new type of video self-attention module, specially designed to handle the large shadow deformations in videos. Moreover, we present a new shadow contrastive learning mechanism (SCOTCH) which aims at guiding the network to learn a unified shadow representation from massive positive shadow pairs across different videos. We demonstrate empirically the effectiveness of our two contributions in an ablation study. Furthermore, we show that SCOTCH and SODA significantly outperforms existing techniques for video shadow detection. Code is available at the project page: https://lihaoliu-cambridge.github.io/scotch_and_soda/",
  "full_text": "SCOTCH and SODA: A Transformer Video Shadow Detection Framework\nLihao Liu1, Jean Prost 2, Lei Zhu 3,4, Nicolas Papadakis 2, Pietro Li `o1,\nCarola-Bibiane Sch¨onlieb1, Angelica I Aviles-Rivero1\n1 University of Cambridge, United Kingdom\n2 Univ. Bordeaux, CNRS, Bordeaux INP, IMB, UMR 5251, F-33400 Talence, France\n3 The Hong Kong University of Science and Technology (Guangzhou), China\n4 The Hong Kong University of Science and Technology, HK SAR, China\nAbstract\nShadows in videos are difﬁcult to detect because of the\nlarge shadow deformation between frames. In this work,\nwe argue that accounting for shadow deformation is essen-\ntial when designing a video shadow detection method. To\nthis end, we introduce the shadow deformation attention\ntrajectory (SODA), a new type of video self-attention mod-\nule, specially designed to handle the large shadow defor-\nmations in videos. Moreover, we present a new shadow\ncontrastive learning mechanism ( SCOTCH) which aims at\nguiding the network to learn a uniﬁed shadow represen-\ntation from massive positive shadow pairs across differ-\nent videos. We demonstrate empirically the effectiveness\nof our two contributions in an ablation study. Furthermore,\nwe show that SCOTCH and SODA signiﬁcantly outperforms\nexisting techniques for video shadow detection. Code is\navailable at the project page: https://lihaoliu-\ncambridge.github.io/scotch_and_soda/\n1. Introduction\nShadow is an inherent part of videos, and they have\nan adverse effect on a wide variety of video vision tasks.\nTherefore, the development of robust video shadow detec-\ntion techniques, to alleviate those negative effects, is of\ngreat interest for the community. Video shadow detection\nis usually formulated as a segmentation problem for videos,\nhowever and due to the nature of the problem, shadow de-\ntection greatly differs from other segmentation tasks such\nas object segmentation. For inferring the presence of shad-\nows in an image, one has to account for the global content\ninformation such as light source orientation, and the pres-\nence of objects casting shadows. Importantly, in a given\nvideo, shadows considerably change appearance (deforma-\ntion) from frame to frame due to light variation and object\nmotion. Finally, shadows can span over different back-\ngrounds over different frames, making approaches relying\nEncoder\nBlock1t×!\"×#\"×c1t×!$%×#$%×c2\nt×!&'×#&'×c3\nBlock3Block2\nStage1\nDecoder\nMLPLayert×!&'×#&'×c3\nt×!$%×#$%×c2t×!\"×#\"×c1\nStage3\nUpsampleLayer\nInputVideo\nSegmentationMasks\nt = 3t = 2t = 1 t = 4Stage 2Deformation Trajectory Attention\nShadowContrast\nShadow FeatureNon-shadow FeatureGet SimilarGet DifferentA is the input of BDeformation Trajectory\nz1z2z3\nz!\"z#\"z$\"\nFigure 1. Overview of our SCOTCH and SODA framework. A MiT\nencoder extracts multi-scale features for each frame of the video\n(stage 1). Then, our deformation attention trajectory is applied\nto features individually to incorporate temporal information (stage\n2). Finally, an MLP layer combines the multi-scale information to\ngenerate the segmentation masks (stage 3). The model is trained\nto contrast shadow and non-shadow features, by minimising our\nshadow contrastive loss with massive positive shadow pairs.\non texture information unreliable.\nParticularly, video shadow detection methods can be\nbroadly divided into two main categories. The ﬁrst category\nrefers to image shadow detection (ISD) [9,15,35,36,43,46].\nThis family of techniques computes the shadow detec-\ntion frame by frame. Although computationally saving,\nthese methods are incapable of handling temporal informa-\ntion. The second category refers to video shadow detection\n(VSD) [6, 9, 14, 16, 25]. These methods offer higher per-\nformance as the analysis involves spatial-temporal informa-\ntion. Hence, our main focus is video shadow detection.\narXiv:2211.06885v2  [cs.CV]  27 Mar 2023\nState-of-the-art video shadow detection methods rely on\ndeep neural networks, which are trained on large annotated\ndatasets. Speciﬁcally, those methods are composed of three\nparts: (i) a feature extraction network that extracts spatial\nfeatures for each frame of the video: (ii) a temporal ag-\ngregation mechanism [6, 14] enriching spatial features with\ninformation from different frames; and (iii) a decoder, that\nmaps video features to segmentation masks. Additionally,\nsome works enforce consistency between frames prediction\nby using additional training criterion [9,25]. We retain from\nthese studies that the design of the temporal aggregation\nmechanism and the temporal consistency loss is crucial to\nthe performance of a video shadow detection network, and\nwe will investigate both of those aspects in this work.\nThe current temporal aggregation mechanisms available\nin the literature were typically designed for video tasks\nsuch as video action recognition, or video object segmenta-\ntion. Currently, the most widely used temporal aggregation\nmechanism is based on a variant of the self-attention mech-\nanism [1, 29, 32, 40, 41]. Recently, trajectory attention [29]\nhas been shown to provide state-of-the-art results on video\nprocessing. Intuitively, trajectory attention aggregates in-\nformation along the object’s moving trajectory, while ignor-\ning the context information, deemed as irrelevant. However,\nshadows in videos are subject to strong deformations, mak-\ning them difﬁcult to track, and thus they might cause the\ntrajectory attention to fail.\nIn this work, we ﬁrst introduce the ShadOw Deformation\nAttention trajectory (SODA), a spatial-temporal aggregation\nmechanism designed to better handle the large shadow de-\nformations that occur in videos. SODA operates in two steps.\nFirst, for each spatial location, an associated token is com-\nputed between the given spatial location and the video,\nwhich contains information in every time-step for the given\nspatial location. Second, by aggregating every associated\nspatial token, a new token is yielded with enriched spatial\ndeformation information. Aggregating spatial-location-to-\nvideo information along the spatial dimension helps the net-\nwork to detect shape changes in videos.\nBesides, we introduce the Shadow COnTrastive meCH-\nanism (SCOTCH), a supervised contrastive loss with massive\npositive shadow pairs aiming to drive our network to learn\nmore discriminative features for the shadow regions in dif-\nferent videos. Speciﬁcally, in training, we add a contrastive\nloss at the coarsest layer of the encoder, driving the fea-\ntures from shadow regions close together, and far from the\nfeatures from the non-shadow region. Intuitively, this con-\ntrastive mechanism drives the encoder to learn high-level\nrepresentations of shadow, invariant to all the various fac-\ntors of shadow variations, such as shape and illumination.\nIn summary, our contributions are as follows:\n• We introduce a new video shadow detection frame-\nwork, in which we highlight:\n– SODA, a new type of trajectory attention that har-\nmonise the features of the different video frames\nat each resolution.\n– SCOTCH, a contrastive loss that highlights a mas-\nsive positive shadow pairs strategy in order to\nmake our encoder learn more robust high-level\nrepresentations of shadows.\n• We evaluate our proposed framework on the video\nshadow benchmark dataset ViSha [6], and compare\nwith the state-of-the-art methods. Numerical and vi-\nsual experimental results demonstrate that our ap-\nproach outperforms, by a large margin, existing ones\non video shadow detection. Furthermore, we provide\nan ablation study to further support the effectiveness of\nthe technical contributions.\n2. Related Work\nThe task of video shadow detection has been extensively\ninvestigated in the community, in which solutions largely\nrely on analysing single frames (image shadow detection)\nor continuous multiple frames (video shadow detection). In\nthis section, we review the existing techniques in turn, and\nthen summarize the recent achievements in the video pro-\ncessing area to better illustrate the difference between exist-\ning work and our work.\n2.1. Image Shadow Detection\nImage shadow detection (ISD) can be cast as a se-\nmantic segmentation problem, where image object seg-\nmentation (IOS) methods can be used to solve this prob-\nlem [8, 13, 19, 21, 22, 42]. However, IOS methods are not\nspeciﬁcally designed for shadow detection. Hence, when\nre-training these methods directly for shadow detection, the\nperformance is unsatisfactory due to the data bias.\nTechniques focused on image shadow detection incor-\nporate problem-speciﬁc shadow knowledge into the model\narchitecture and the training criterion [7,9,15,16,35,36,43,\n46]. For example, BDRAR [46] introduces a bidirectional\npyramidal architecture for shadow detection. DSD [43]\npresents a distraction-aware shadow-module to reduce false\npositives. Chen et al. [7] make use of non-labelled data,\nduring training, with a task-speciﬁc semi-supervised learn-\ning mechanism called MTMT. Wang et al. investigate the\ndetection of shadows along with their corresponding ob-\njects [35, 36]. Finally, FSDNet [16] proposes a compact\nimage shadow detection network. Whilst ISD techniques\nhave demonstrated potential results, their performance on\nvideos is limited by the lack of temporal information.\n2.2. Video Shadow Detection\nAnother body of researchers has explored the task of\nshadow detection from the lens of video analysis. The work\nof [6] proposes the TVSD model. It relies on a dual-gated\nco-attention module, to aggregate features from different\nframes, and uses a contrastive learning mechanism to drive\nthe encoder to discriminate frames from different videos.\nHu et al. [14] introduce an optical ﬂow warping module\nto aggregate features from different frames. STICT [25]\nuses transfer learning, to transfer the knowledge of a super-\nvised image shadow detection network to a video shadow\ndetection network, without labelled videos, by training the\nnetwork prediction to be consistent with respect to tempo-\nral interpolation [33]. Moreover, the technique called SC-\nCor [9] presents a weakly supervised correspondence learn-\ning mechanism to enhance the temporal similarity of fea-\ntures corresponding to shadow region across frames.\n2.3. Progresses in Video Processing\nThe speciﬁc nature of videos, containing spatial and tem-\nporal information, has motivated the design of deep neural\nnetwork architectures for different video processing appli-\ncations. Models based on 3D CNN [17, 31] process videos\nby sequentially aggregating the spatio-temporal local infor-\nmation using 3D convolutional ﬁlters, but fail to effectively\ncapture long-range temporal dependencies. To alleviate this\nlimitation, architectures using recurrent networks were in-\ntroduced in [2, 30]. Moreover, another set of works uses\nspatio-temporal memory bank mechanism [27] or spatio-\ntemporal attention mechanism [32] into the coarse layers of\n3D CNN architectures [18, 26, 37] to better integrate the\ntemporal information for video processing.\nThe success of the transformer network architecture, on a\nwide variety of vision tasks [4,10], has motivated the use of\ntransformer for video tasks. While the self-attention mech-\nanism in transformers appears to be well suited to capture\nthe long-range dependencies in videos, applying transform-\ners to videos raises many challenges, such as the quadratic\ncomplexity in the input sequence length, and the large data\nrequirement induced by the lack of problem-speciﬁc induc-\ntive bias. The works of [1, 41] propose to separate spatial\nand temporal attention to reduce computational complexity,\nand the authors of [40] propose to apply multiple encoders\non multiple views of the video. Recently, trajectory atten-\ntion [29] was introduced as a way to incorporate an induc-\ntive bias in the self-attention operation to capture objects\nmoving trajectories for better video recognition tasks.\n2.4. Existing Works & Comparison to Our Work\nAll precedent works on VSD [6, 14, 25, 33] rely on con-\nvolutional neural network architectures. To the best of our\nknowledge, our work is the ﬁrst video shadow detection ap-\nproach based on transformers.\nMoreover, whilst the work of [29] also considers a type\nof attention trajectory, their modelling hypothesis is that\nthe video objects do not change shape over time. This is\na strong assumption to fulﬁll for several vision applica-\ntions such as shadow detection; as shadows signiﬁcantly\nchange shape from frame to frame. Our work ﬁrst miti-\ngates this issue by modelling in the trajectories the inherent\ndeformation of the shadows. Notice that TVSD [6] uses an\nimage-level contrastive loss between frames from different\nvideos, and SC-Cor [9] uses weakly-supervised correspon-\ndence learning for driving similar features from shadow-\nregion close together. Unlike these works, we introduce\na supervised contrastive strategy to contrast shadow fea-\ntures from non-shadow features. We underline that existing\ncontrastive-based techniques assume that supervised con-\ntrastive learning is not performing better than its counter-\npart. In this work, we show that a well-designed supervised\ncontrastive strategy indeed improves over existing works.\n3. Methodology\nIn this section, we introduce all the components of the\nvideo shadow detection method presented in this work. Af-\nter a global description of our framework (i), we introduce\nSODA, our new video self-attention module (ii), and we de-\nscribe the training criterion of our model, which includes\nSCOTCH, our shadow contrastive mechanism (iii).\n3.1. Framework Architecture Overview\nOur proposed framework is composed of three main\nstages. The overall workﬂow is illustrated in Figure 1, and\ndetails are provided next.\n/hand-point-rightStage 1: Feature Extraction. In this stage, Mix\nTransformer (MiT) [39] is adopted as the encoder. MiT\ntakes video clips as input, and it outputs a set of different-\nresolution spatial feature maps. Unlike ViT [10], which\nonly generates single-resolution feature maps, the hierar-\nchical structure of MiT can generate CNN-like multi-level\nmulti-scale feature maps. These feature maps contain from\nhigh-resolution detailed information to low-resolution se-\nmantic information. By incorporating different levels of\nresolutions, the performance of tasks such as semantic seg-\nmentation can be boosted [39]. In this stage, MiT only en-\ncodes the spatial information of the given video clips (in-\nput), that is, the temporal information is not yet incorpo-\nrated.\n/hand-point-rightStage 2: Harmonising Spatial and Temporal Infor-\nmation. The multi-scale feature maps from Stage 1 are then\nprocessed by the newly introduced shadow deformation at-\ntention trajectory module ( SODA). The goal of this module\nis to capture the shadow’s deformation trajectory along the\nframes in the video clips. Our deformation attention trajec-\ntory module processes independently each feature map with\ndifferent-resolution. These processed feature maps are used\nas input to the decoder in the next stage.\n/hand-point-rightStage 3: Mask Shadow Generation.In this stage, a\nlight-weighted decoder, with only MLP layers, is adopted to\nq:st×c\nk:st×c\nv:st×c\nz:st×c\nattn:st×st\nattentions:st×s×t\nysts∶s!\"→st\nDeformationAttentionsTrajectory\nIntegrateSpatialInformationysts\nFroms!\"→s#\"\nst\nsoftmaxont\ns×t\nAttentiononTime\nz':st×c∧’\ns#$\nq4kT:st×st\nq:st×c\nk:st×s×c\nv:st×s×c\ns%$\ns&$\nwk\nwv\ndiagonal,\twq\n^\n^\n^\n^\n^\n^\n<,>wk\nwv\nwq\n∧ ’1\nysts∶s$\"→st∧ ’2\nysts∶s%\"→st∧’m\nysts∶st×s×c∧’\nFigure 2. Deformation attention trajectory module. The input feature maps z is used to generate q, k, v, respectively. The q and k are\nﬁrst used to calculate the pointwise st-to-stsimilarity, followed by a softmax on tto get the time attention. The time attention aggregated\nvcan generate s-to-st(spatial-location-to-video) attention, named deformation attention. Then, the spatial-location-to-video attention is\nintegrated along the spatial dimension to capture the deformation attention trajectory. Lastly, the deformation attention trajectory is used\nto generate the ﬁnal feature map z′ with a second self-attention (ˆq, ˆk, ˆv). In this ﬁgure, each square represents a spatial feature map. (The\nchannel dimension is not represented for simplicity).\nreconstruct the shadow’s segmentation masks. The decoder\naims to mix the high- & low- resolution information, from\nprocessed feature maps, for better semantic segmentation.\nThe output of the decoder is a segmented video that marks\nout the shadows in each frame of the video.\n3.2. SODA: ShadOw Deformation Attention trajec-\ntory\nTransformers have revolutionised several tasks in the\ncomputer vision area. The key is the self-attention mecha-\nnism that can accommodate with any given type of data and\ndomain. However, for videos, the standard self-attention\ndoes not differentiate the spatial dimensions from the tem-\nporal dimension. This can lead the attention to focus on the\nredundant spatial information while neglecting the informa-\ntive temporal variations in the videos. Nonetheless, video\nanalysis is inherent to such temporality. Most recently, tra-\njectory attention [29] has proposed to accommodate some-\nhow with such issues. However and even though trajectory\nattention has demonstrated potential results, it has a ma-\njor limitation – the objects are assumed not to change over\ntime. This is a major constraint in several video tasks in-\ncluding shadow detection; as shadows signiﬁcantly undergo\ndeformation from one frame to another. In this subsection,\nwe introduce a new scheme called ShadOw Deformation\nAttention trajectory ( SODA) to mitigate current drawbacks\nof the literature.\nLike in the classical self-attention setting, it begins with\nan input feature map generated from the encoder. Specif-\nically, let us denote fd ∈Rt×h\nd ×w\nd ×c, the generated fea-\nture map from the encoder, where d is the spatial down-\nsampling ratio, c is the number of feature channels, and\nt is the number of time frames in the video. fd is re-\nshaped to a sequence of 1D token embedding denoted as\nz ∈ Rn×c, where n = t ×h\nd ×w\nd. As shown in the\nleft part of Figure 2, z is then mapped to a set of query-\nkey-value vectors q,k,v ∈Rn×c using linear projections\nq= wq·z, k= wk·z, v= wv·z, with projection matrices\nwq,wk,wv ∈Rn×n.\nOur scheme considers two main parts: (i) temporal at-\ntention between the spatial location and video (Attention on\nTime in Figure 2), (ii) intra-space attention to capture de-\nformation statues within a spatial scene (Deformation At-\ntention Trajectory in Figure 2). For the ﬁrst part, given a\nspace-time position in the videost∈{1,··· ,n}, and a spa-\ntial location s′∈{1,··· ,m}, where m= n\nt, the temporal\nattention (deformation) between the space-time position st\nand the spatial locations s′is computed as:\nˆysts′ =\n∑\nt′\nvs′t′ · exp⟨qst,ks′t′ ⟩∑\n¯texp⟨qst,ks′ ¯t⟩ (1)\nFor brevity, the notation is slightly abused by omitting the\n“softmax operation on time dimension” in the Fig. 2 ap-\nplied to the fraction, as well as the scaling parameter √n\n(we will keep this notation convention throughout the pa-\nper). The deformation encodes the connection between one\nspace-time position and one spatial location, which indi-\ncates how the content of the space-time position stis pre-\nsented in spatial location s′.\nOnce the temporal attentions are computed, the intra-\nspace attention is then estimated to aggregate the spatial-\nlocation-to-video responses to space-level deformation. To\ndo this, the computed deformation tokens are projected to a\nnew set of query-key-value vectors using linear projections:\nˆqst = ˆwq ·ˆysts, ˆksts′ = ˆwk ·ˆysts′ , ˆvsts′ = ˆwv ·ˆysts′ (2)\nwhere ˆysts is the temporal connection from stto the same\nspatial location s, and ˆqst corresponds to the deformation\nreference point stthat is used to aggregate the location-to-\nvideo connection:\nˆyst =\n∑\ns′\nˆvsts′ · exp⟨ˆqst,ˆksts′ ⟩∑\n¯sexp⟨ˆqst,ˆkst¯s⟩ (3)\nwhere ˆyst is the deformation attention output. The mean-\ningful location-to-video tokens are pooled out to form the\nfull space-level deformation status. By computing the intra-\nspace attention, the attended feature map can capture the de-\nformation status in different frames of the video, thus boost-\ning the video shadow detection performance.\n3.3. SCOTCH: Shadow COnTrastive meCHanism\nContrastive learning [5, 20, 23] has been proven to be an\neffective mechanism for learning distinctive features. By\ncontrasting the positive pairs with high similarity and neg-\native pairs with low similarity, the learned feature maps\ncan be more discriminative in downstream tasks including\nclassiﬁcation and segmentation. In previous video shadow\ndetection task [6], positive and negative pairs are sampled\nfrom frames from the same video and from two different\nvideos respectively. Since the frames from one video have\nhigh similarity image content, the contrastive mechanism\ncan help to discriminate different video content.\nHowever, the key element in video shadow detection\nis the shadow itself instead of the video content. With\nthe goal of boosting the detection performance, we in-\ntroduce SCOTCH, a Shadow COnTrastive me CHanism.\nSCOTCH seeks to better guide the segmentation process for\nshadows and non-shadows regions in the videos. Speciﬁ-\ncally, to learn a uniﬁed shadow feature for different videos,\npositive pairs are sampled from the shadow regions from\ndifferent frames in different videos, whilst negative pairs are\nsampled as shadow and non-shadow regions on the frames\nin different videos. We underline that unlike the classi-\ncal contrastive loss used for unsupervised learning [5, 28],\nwhere there is only a small number of positive pairs, we\nproposed a massive positive shadow paired contrastive loss.\nThe key idea behind our loss is that – we seek to not only\nmaximise the difference between shadow and non-shadow\nfeatures, but also maximise the similarity between features\nof shadows presented in different videos. All shadow and\nnon-shadow features are cropped from the last layer of the\nencoder presented in Section 3.1, with the supervision of\nthe segmentation masks. The contrastive loss reads:\nℓcontrast\n(\nv,v+,v−)\n=\n−log\n[ ∑N\nn=1 exp (v ·v+\nn/τ)\n∑N\nn=1 exp\n(\nv ·v+n/τ\n)\n+ ∑N\nn=1 exp\n(\nv ·v−n/τ\n)\n]\n(4)\nwhere v ∈ Rc is the query shadow feature. Moreover,\nv+\nn,v−\nn ∈ Rn×c are the positive and negative groups re-\nspectively, andτ is a temperature hyperparameter. The ﬁnal\nloss is computed across all frames in a mini-batch fashion.\nOptimisation Scheme for Shadow Detection.Finally,\nto compute the shadow segmentation loss, we follow the de-\nfault setting in [6]. We use the binary cross entropy (BCE)\nloss with a lovasz-hinge loss [3]. These two terms are added\nto deﬁne the shadow segmentation loss as follows:\nℓseg = ℓbce + λ1ℓhinge (5)\nOur optimisation scheme is then given by (4) and (5) as:\nℓfinal = ℓbce + λ1ℓhinge + λ2ℓcontrast (6)\nwhere λ1 and λ2 are two hyper-parameters weighting the\nrelative effect of the hinge loss and the contrastive loss in\nthe ﬁnal loss. In the following experiments, λ1, λ2 were\nempirically set to a value of 1 and 0.1, respectively.\n4. Experimental Results\nThis section details all experiments performed to validate\nour proposed framework.\n4.1. Dataset and Evaluation Metrics\nData Description.We utilise the largest and latestVideo\nShadow dataset ( ViSha) [6] to evaluate the effectiveness\nof our proposed VSD method. The ViSha dataset has 120\nvideos, and each video contains between 29 and 101 frames.\nViSha is composed of a total of 11,685 frames correspond-\ning to a total duration of 390 seconds of video.\nData Pre-processing.We follow the setting introduced\nin ViSha [6]. That is, we use the same train-test split, with\n50 videos for training and 70 videos for testing. During\ntraining, we also use the same data augmentation strategy as\n[6] to enrich the variety of the dataset. Speciﬁcally, during\ntraining, images are re-scaled to size 512 ×512, and are\nrandomly ﬂipped horizontally. In testing, only re-scaling to\nthe uniﬁed size 512 ×512 is used.\nEvaluation Metrics. Following the evaluation protocol\nused in [6, 9, 25], we employ four common evaluation met-\nrics to measure the shadow detection accuracy: MAE, F β,\nIoU, and BER. Lower MAE and BER scores, and higher Fβ\nand IoU scores indicate a better video shadow detection re-\nsult. Moreover, we also provide the shadow BER (S-BER)\nand the non-shadow BER scores (N-BER) to further com-\npare different VSD methods.\nImplementation Details. Our proposed segmentation\narchitecture is built using the PyTorch-lightning [11] deep-\nlearning framework. The parameters of the feature extrac-\ntion encoder are initialised using the weights from the MiT-\nB3 model pre-trained for image segmentation on ADE20K\ndataset [44, 45], publicly available on HuggingFace [38].\nThe remaining parameters (attention modules and the MLP\ndecoder) are randomly initialised using “Xavier” meth-\nods [12]. During training, AdamW optimizer [24] is used\nwith an initial learning rate of 1 ×10−6 without decay. All\nexperiments and ablation studies are trained for 36 epochs,\nfor a training time of approximately 12 hours on NVIDIA\nA100 GPU with 80G RAM with batch size of 8.\nMETHODS EVALUATIONMETRICS\nTasks Techniques MAE↓ Fβ↑ IoU↑ BER↓ S-BER↓ N-BER↓\nIOS\n⋆FPN [19] 0.044 0.707 0.512 19.49 36.59 2.40\nPSPNet [42] 0.051 0.642 0.476 19.75 36.44 3.07\nDSS [13] 0.045 0.696 0.502 19.77 36.96 2.59\nR3Net [8] 0.044 0.710 0.502 20.40 37.37 3.55\nISD\nBDRAR [46] 0.050 0.695 0.484 21.29 40.28 2.31\n⋆DSD [43] 0.043 0.702 0.518 19.88 37.89 1.88\nMTMT [7] 0.043 0.729 0.517 20.28 38.71 1.86\nFSDNet [16] 0.057 0.671 0.486 20.57 38.06 3.06\nVOS\nPDBM [30] 0.066 0.623 0.466 19.73 34.32 5.16\nCOSNet [26] 0.040 0.705 0.514 20.50 39.22 1.79\n⋆FEELVOS [34] 0.043 0.710 0.512 19.76 37.27 2.26\nSTM [27] 0.068 0.597 0.408 25.69 47.44 3.96\nVSD\nTVSD [6] 0.033 0.757 0.567 17.70 33.97 1.45\nSTICT [25] 0.046 0.702 0.545 16.60 29.58 3.59\nSC-Cor [9] 0.042 0.762 0.615 13.61 24.31 2.91\n⋆SCOTCHandSODA 0.029 0.793 0.640 9.066 16.26 1.44\nTable 1. Comparisons between our proposed technique and SOTA techniques on the ViSha dataset. “MAE” denotes mean absolute error,\n“Fβ” denotes F-measure score, “IoU” denotes intersection over union, “BER” denotes balance error rate, and “S-BER” means shadow\nBER, “N-BER” means non-shadow BER. The ↑ denotes the higher the value is the better the performance is, whilst the ↓ means the\nopposite. ⋆indicates the best performed network in each category.\n4.2. Comparison to SOTA Techniques\nCompared Methods.Video shadow detection is a rela-\ntively recent topic, and there are only three directly-related\nmethods designed for this task. Hence, following exist-\ning VSD methods, we make comparisons against 4 dif-\nferent kinds of methods, including image object segmen-\ntation (IOS), image shadow detection (ISD), video object\nsegmentation (VOS), and video shadow detection (VSD).\nFor IOS, they are FPN [19], PSPNet [42], DSS [13], and\nR3-Net [8], while the ISD methods are BDRAR [46],\nDSD [43], MTMT [7], and FSDNet [16]. The com-\npared VOS methods include PDBM [30], COSNet [26],\nFEELVOS [34], and STM [27], while compared VSD meth-\nods are TVSD [6], STICT [25], and SC-Cor [9]. We obtain\nthe results by re-training their network parameters with uni-\nﬁed training parameters or by downloading the results from\nthe TVSD [6] repository.\nQuantitative Comparisons.Table 1 summarises MAE,\nFβ, IoU, and BER scores of our network against SOTA tech-\nniques. For each category, we use ⋆to mark out the method\nwith the best performance. From these quantitative results,\nwe observe that the IOS and ISD report readily competing\nresults. These results are expected as the modelling hypoth-\nesis for both families of techniques relies on only the image\nlevel analysis. Whilst VOS techniques consider also tempo-\nral information, these techniques are customised as a gen-\neral framework for video object segmentation. However,\nshadow detection is more complex due to the fast change\nin appearance between frames. Notably, we observe that\nVSD techniques indeed provide a substantial performance\nimprovement compared to other methods; as they are de-\nsigned considering the complexity of shadows.\nMore importantly, our method outperforms all other\ntechniques by a signiﬁcant margin for all evaluation met-\nrics – further supporting the superiority of our approach.\nIn particular, our method yields to a balanced error rate of\n9.066 which is more than 4 points below SC-Cor, the latest\nSOTA technique, in terms of BER. The error rate for the\nshadow label (denoted as S-BER in Table 1) of our method\nis 8 points below SC-Cor. We also underline that our sig-\nniﬁcant improvement in performance comes with a negligi-\nble computational cost. We report the test time in Table 2,\nwhere we observe that our SCOTCH and SODA framework\nonly requires a fraction of time than compared methods.\nVisual Comparison. Figure 3 visually compares the\nshadow segmentation masks from our method and the com-\npared methods on different input video frames. For video\nframes with black objects at the ﬁrst three rows, we ﬁnd that\ncompared methods tend to wrongly identify those black ob-\njects as shadow ones, while our method can still accurately\ndetect shadow pixels under the corruption of black objects.\nMoreover, compared methods tend to miss some shadow re-\ngions when the input video frames contain multiple shadow\ndetection, as shown in the 4-th row to the 8-th row of Fig-\nure 3. On the contrary, our method can identify all these\nshadow regions of input video frames, and our detection re-\nsults are most consistent with the ground truth in the 2nd\ncolumn of Figure 3. We also provide the video segmenta-\ntion masks on the project page to demonstrate the temporal\ncoherence of the results provided by our method.\nModel size and inference time. Table 2 further\n(a)Image(b)GT(c)Ours(d)FPN(e)DSD(f)FEELVOS(g)TVSD(h)STICT(i)SC-Cor\nFigure 3. Visual comparisons of video shadow detection results produced by our network ( SCOTCH and SODA) and compared methods.\nApparently, our method can more accurately identify shadow pixels and our results are more consistent with the ground truth in the 2nd\ncolumn. The video segmentation results can be found on the project page. (d-e) are the methods with the highest performance in IOS, ISD,\nand VOS in Table 1, whilst (g-i) are all from the VSD area.\nNetworks Params GMACs Time BER\nTVSD [6] 243.32 158.89 32.4 17.70\nSTCIT [25] 104.68 40.99 13.5 16.60\nSC-Cor [9] 232.63 218.4 21.8 13.61\nSCOTCHandSODA 211.79 122.46 9.15 9.006\nTable 2. Comparison of model size (Params), computational com-\nplexity (GMACs), inference time (Time), and segmentation accu-\nracy (BER). Speciﬁcally, the units for Params and Time are (MB)\nand (Mins), respectively. We denote the best and second best in\nbold and underline font.\ncompares our network and three state-of-the-art video\nshadow detection methods in terms of the model size\n(Params), computational complexity (GMACs), inference\ntime (Time), and segmentation accuracy (BER). Appar-\nently, among the three compared VSD methods, STICT\nhas the smallest testing time (13.5 minutes). Compared to\nSTICT, our method further reduces the inference time from\n13.5 minutes to 9.15 minutes to test all 70 testing videos\nwith 6,897 images. Although our method takes 2nd rank\nin terms of the model size and computational complexity,\nthey are only larger than STICT. This is because we only\nuse a light-weighted MLP layer as the decoder to integrate\nthe multi-resolution feature maps, which is computational-\nsaving. In terms of performance, our method has a superior\nBER performance than STICT by reducing the BER score\nfrom 16.60 to 9.006, which indicates that with a minor com-\npromise on the model size, our method can more accurately\nidentify video shadows than STICT, TVSD, and SC-Cor.\n4.3. Ablation Study\nIn Table 3, we perform ablation studies on our main con-\ntributions to evaluate the effectiveness of each component.\nBaseline with MiT backbone. In order to evaluate\nthe role of the MiT backbone on the ﬁnal performance of\nour method, we deﬁne the segmentation network with MiT\nbackbone as the baseline, which is trained by using the clas-\nsical segmentation loss (5), but without the deformation at-\ntention trajectory and shadow level contrastive mechanism.\nComponents Evaluation Metrics\nBackbone Attention Contrast MAE↓ Fβ↑ IoU↑ BER↓\nMiT \u0017 \u0017 0.048 0.755 0.584 13.18\nMiT Trajectory \u0017 0.048 0.760 0.593 12.35\nMiT SODA \u0017 0.041 0.791 0.613 10.55\nMiT \u0017 Image 0.048 0.761 0.588 12.85\nMiT \u0017 Shadow 0.041 0.767 0.592 12.02\nMiT \u0017 SCOTCH 0.034 0.771 0.606 11.29\n†MiT SODA SCOTCH 0.029 0.793 0.640 9.066\nTable 3. Ablation study on different components of our proposed\nmethods on the ViSha dataset. The ↑ denotes the higher the value\nis the better the performance is, whilst the ↓ means the opposite.\n“†” denotes our ﬁnal methods with the highest performance in all\nevaluation metrics. Notations : best, second best, third best.\nThis baseline already provides results on par or better than\nthe previous three works in the VSD area (see Table 1 for\ncomparison), even without considering any kind of tempo-\nral information. This illustrates the superior performance of\nthe MiT transformer-based architecture over convolutional\narchitectures on the task of video shadow detection.\nAttention mechanisms.We then evaluate the effective-\nness of different attention mechanisms, including trajectory\nattention [29] and our newly introduced shadow deforma-\ntion attention. Both types of attention modules provide an\nimprovement over the baseline, which was to be expected\nas those modules give the ability to consider the temporal\ninformation within the videos. Our deformation attention\ntrajectory module also appears to provide better results than\nthe trajectory attention, indicating the importance of consid-\nering the deformation in the design of the attention module.\nContrastive losses.Next, we compare the effect of two\ntypes of contrastive criterion, the image level contrastive\nloss used in TVSD [6], and our feature-level shadow con-\ntrastive loss. The inter-frame contrastive learning slightly\nimproves the baseline, whereas our shadow contrastive loss\nprovides a signiﬁcant improvement over the baseline. Those\nresults illustrate the superiority of the features-level con-\ntrastive loss over the frame-level contrastive loss.\nFinal model.SCOTCH reduces the variance of the spatial\nshadow features, while SODA processes the spatial features\nat different time-step to consider the temporal information\nin the video. Thus, SCOTCH and SODA have a complemen-\ntary effect, as feeding SODA with more robust spatial fea-\ntures from SCOTCH, we are able to reach the best perfor-\nmance, outperforming all the previous settings.\n4.4. Attention Map Visualisation\nIn Figure 4, we visually compare the attention maps\nfrom the baseline model (second row) with our proposed\nSCOTCH and SODA (third row). We can observe that the base-\nline model highlights the object part, whilst ours focuses\nmore on the shadow region with the help of contrastive\nVideo+GTBaselineMiTScotchandSoda\nt=1 t=2 t=3 t=4 t=5\nFigure 4. Attention maps visualisation. The low-resolution lay-\ners of the MiT encoder are selected for the visualization, with the\nquery from the center of the ground truth shadow mask. From\ntop to bottom row are the input video blended with segmenta-\ntion mask, attention maps for baseline MiT methods, and atten-\ntion maps from ours SCOTCH and SODA with deformation attention\ntrajectory and shadow contrastive mechanism.\nlearning. We can also observe that the baseline model might\nlose track of shape in different frames (third column), whilst\nour approach provides consistent shape information at dif-\nferent times with the help of the deformation attention tra-\njectory. Hence, the model using the shadow deformation\nattention trajectory and the shadow contrastive loss is better\nat tracking the shadow region during different frames, while\nignoring the non-shadow part of the image.\n5. Conclusion\nIn this paper, we introduced SCOTCH and SODA, a new\ntransformer video shadow detection framework. We de-\nveloped shadow deformation attention trajectory ( SODA),\na self-attention module specially designed to handle the\nshadow deformation in videos, and we introduced a shadow\ncontrastive mechanism ( SCOTCH) to guide our network to\nbetter discriminate between shadow and non-shadow fea-\ntures. We demonstrate the effectiveness of the contributions\nwith ablation studies. Finally, we show that our proposed\nmethod outperforms by a large margin concurrent video\nshadow segmentation works on the ViSha dataset.\nAcknowledgements. This work is supported by Girton\nPostgraduate Research Scholarships, GSK Ph.D. Schol-\narship, CMIH, CCIMI, Philip Leverhulme Prize, Royal\nSociety Wolfson Fellowship, EPSRC Advanced Career\nFellowship EP/V029428/1, EPSRC grants EP/S026045/1,\nEP/T003553/1, EP/N014588/1, EP/T017961/1, Wellcome\nInnovator Awards 215733/Z/19/Z and 221633/Z/20/Z, EU\nHorizon 2020 research and innovation programme under\nthe Marie Skodowska-Curie grant agreement No. 777826\nNoMADS, and Guangzhou Municipal Science and Tech-\nnology Project Grant No. 2023A03J0671.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video\nvision transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 6836–6846,\n2021. 2, 3\n[2] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville.\nDelving deeper into convolutional networks for learning\nvideo representations. arXiv preprint arXiv:1511.06432 ,\n2015. 3\n[3] Maxim Berman, Amal Rannen Triki, and Matthew B\nBlaschko. The lov´asz-softmax loss: A tractable surrogate for\nthe optimization of the intersection-over-union measure in\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4413–4421,\n2018. 5\n[4] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sud-\nhakaran, Brais Martinez, and Georgios Tzimiropoulos.\nSpace-time mixing attention for video transformer.Advances\nin Neural Information Processing Systems, 34:19594–19607,\n2021. 3\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 5\n[6] Zhihao Chen, Liang Wan, Lei Zhu, Jia Shen, Huazhu Fu,\nWennan Liu, and Jing Qin. Triple-cooperative video shadow\ndetection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 2715–\n2724, 2021. 1, 2, 3, 5, 6, 7, 8\n[7] Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei\nFeng, and Pheng-Ann Heng. A multi-task mean teacher\nfor semi-supervised shadow detection. In Proceedings of\nthe IEEE/CVF Conference on computer vision and pattern\nrecognition, pages 5611–5620, 2020. 2, 6\n[8] Zijun Deng, Xiaowei Hu, Lei Zhu, Xuemiao Xu, Jing Qin,\nGuoqiang Han, and Pheng-Ann Heng. R3net: Recurrent\nresidual reﬁnement network for saliency detection. In Pro-\nceedings of the 27th International Joint Conference on Arti-\nﬁcial Intelligence, pages 684–690. AAAI Press Menlo Park,\nCA, USA, 2018. 2, 6\n[9] Xinpeng Ding, Jingweng Yang, Xiaowei Hu, and Xiaomeng\nLi. Learning shadow correspondence for video shadow de-\ntection. arXiv preprint arXiv:2208.00150, 2022. 1, 2, 3, 5,\n6, 7\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n[11] William Falcon et al. Pytorch lightning. GitHub. Note:\nhttps://github. com/PyTorchLightning/pytorch-lightning ,\n3(6), 2019. 5\n[12] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256. JMLR Work-\nshop and Conference Proceedings, 2010. 5\n[13] Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji,\nZhuowen Tu, and Philip HS Torr. Deeply supervised salient\nobject detection with short connections. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 3203–3212, 2017. 2, 6\n[14] Shilin Hu, Hieu Le, and Dimitris Samaras. Temporal fea-\nture warping for video shadow detection. arXiv preprint\narXiv:2107.14287, 2021. 1, 2, 3\n[15] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, and Pheng-\nAnn Heng. Direction-aware spatial context features for\nshadow detection and removal. IEEE transactions on pat-\ntern analysis and machine intelligence , 42(11):2795–2808,\n2019. 1, 2\n[16] Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang,\nQiong Wang, and Pheng-Ann Heng. Revisiting shadow de-\ntection: A new benchmark dataset for complex world. IEEE\nTransactions on Image Processing, 30:1925–1934, 2021. 1,\n2, 6\n[17] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-\ntional neural networks for human action recognition. IEEE\ntransactions on pattern analysis and machine intelligence ,\n35(1):221–231, 2012. 3\n[18] Haofeng Li, Guanqi Chen, Guanbin Li, and Yizhou Yu. Mo-\ntion guided attention for video salient object detection. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 7274–7283, 2019. 3\n[19] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyra-\nmid networks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117–2125, 2017. 2, 6\n[20] Lihao Liu, Angelica I Avil ´es-Rivero, and Carola-Bibiane\nSch¨onlieb. Contrastive registration for unsupervised medi-\ncal image segmentation. arXiv preprint arXiv:2011.08894,\n2020. 5\n[21] Lihao Liu, Chenyang Hong, Angelica I Aviles-Rivero, and\nCarola-Bibiane Sch ¨onlieb. Simultaneous semantic and\ninstance segmentation for colon nuclei identiﬁcation and\ncounting. arXiv preprint arXiv:2203.00157, 2022. 2\n[22] Lihao Liu, Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and\nPheng-Ann Heng. ψ-net: Stacking densely convolutional\nlstms for sub-cortical brain structure segmentation. IEEE\ntransactions on medical imaging, 39(9):2806–2817, 2020. 2\n[23] Lihao Liu, Zhening Huang, Pietro Li `o, Carola-Bibiane\nSch¨onlieb, and Angelica I Aviles-Rivero. Pc-swinmorph:\nPatch representation for unsupervised medical image regis-\ntration and segmentation. arXiv preprint arXiv:2203.05684,\n2022. 5\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[25] Xiao Lu, Yihong Cao, Sheng Liu, Chengjiang Long, Zipei\nChen, Xuanyu Zhou, Yimin Yang, and Chunxia Xiao. Video\nshadow detection via spatio-temporal interpolation consis-\ntency training. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 3116–\n3125, 2022. 1, 2, 3, 5, 6, 7\n[26] Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling\nShao, and Fatih Porikli. See more, know more: Unsuper-\nvised video object segmentation with co-attention siamese\nnetworks. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 3623–3632,\n2019. 3, 6\n[27] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9226–9235, 2019. 3,\n6\n[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 5\n[29] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra,\nFlorian Metze, Christoph Feichtenhofer, Andrea Vedaldi,\nand Jo ˜ao F Henriques. Keeping your eye on the ball: Tra-\njectory attention in video transformers. Advances in neural\ninformation processing systems, 34:12493–12506, 2021. 2,\n3, 4, 8\n[30] Hongmei Song, Wenguan Wang, Sanyuan Zhao, Jianbing\nShen, and Kin-Man Lam. Pyramid dilated deeper convlstm\nfor video salient object detection. In Proceedings of the Eu-\nropean conference on computer vision (ECCV) , pages 715–\n731, 2018. 3, 6\n[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. InProceedings of the IEEE inter-\nnational conference on computer vision , pages 4489–4497,\n2015. 3\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2, 3\n[33] Vikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala,\nArno Solin, Yoshua Bengio, and David Lopez-Paz. Inter-\npolation consistency training for semi-supervised learning.\nNeural Networks, 145:90–106, 2022. 3\n[34] Paul V oigtlaender, Yuning Chai, Florian Schroff, Hartwig\nAdam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast\nend-to-end embedding learning for video object segmenta-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition , pages 9481–9490,\n2019. 6\n[35] Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann\nHeng. Single-stage instance shadow detection with bidirec-\ntional relation learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 1–11, 2021. 1, 2\n[36] Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng,\nand Chi-Wing Fu. Instance shadow detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1880–1889, 2020. 1, 2\n[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 3\n[38] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R´emi Louf, Morgan Funtowicz, et al. Huggingface’s\ntransformers: State-of-the-art natural language processing.\narXiv preprint arXiv:1910.03771, 2019. 5\n[39] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and\nefﬁcient design for semantic segmentation with transform-\ners. Advances in Neural Information Processing Systems ,\n34:12077–12090, 2021. 3\n[40] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu,\nMi Zhang, Chen Sun, and Cordelia Schmid. Multiview\ntransformers for video recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3333–3343, 2022. 2, 3\n[41] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu,\nBiagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe.\nVidtr: Video transformer without convolutions. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 13577–13587, 2021. 2, 3\n[42] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2881–2890, 2017. 2, 6\n[43] Quanlong Zheng, Xiaotian Qiao, Ying Cao, and Rynson WH\nLau. Distraction-aware shadow detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5167–5176, 2019. 1, 2, 6\n[44] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017. 5\n[45] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127(3):302–321, 2019. 5\n[46] Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao\nXu, Jing Qin, and Pheng-Ann Heng. Bidirectional feature\npyramid network with recurrent attention residual modules\nfor shadow detection. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 121–136, 2018.\n1, 2, 6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7482942342758179
    },
    {
      "name": "Shadow (psychology)",
      "score": 0.7410012483596802
    },
    {
      "name": "Shadow mapping",
      "score": 0.6708544492721558
    },
    {
      "name": "Computer vision",
      "score": 0.5978530049324036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5938993692398071
    },
    {
      "name": "Code (set theory)",
      "score": 0.44930532574653625
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.35319003462791443
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210160189",
      "name": "Institut Polytechnique de Bordeaux",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I15057530",
      "name": "Université de Bordeaux",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}