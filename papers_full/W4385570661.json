{
  "title": "Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques",
  "url": "https://openalex.org/W4385570661",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5033331273",
      "name": "Daking Rai",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A5042749062",
      "name": "Bailin Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074749353",
      "name": "Yilun Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101616607",
      "name": "Ziyu Yao",
      "affiliations": [
        "George Mason University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385573314",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W3154669786",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3168664364",
    "https://openalex.org/W2891691255",
    "https://openalex.org/W3034835156",
    "https://openalex.org/W4287550997",
    "https://openalex.org/W3175473034",
    "https://openalex.org/W3105388824",
    "https://openalex.org/W3118969025",
    "https://openalex.org/W3173888607",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3103801878",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2970172141",
    "https://openalex.org/W4280516735",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W2945102109",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W4385571445",
    "https://openalex.org/W3167991370",
    "https://openalex.org/W3200079259",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W4306753954",
    "https://openalex.org/W4229022682",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W4200634276",
    "https://openalex.org/W2963794306"
  ],
  "abstract": "Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM's generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 150–160\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nImproving Generalization in Language Model-Based Text-to-SQL\nSemantic Parsing: Two Simple Semantic Boundary-Based Techniques\nDaking Rai1, Bailin Wang2, Yilun Zhou2, Ziyu Yao1\n1George Mason University, 2MIT\n1{drai2, ziyuyao}@gmu.edu, 2{bailinw, yilun}@mit.edu\nAbstract\nCompositional and domain generalization\npresent significant challenges in semantic pars-\ning, even for state-of-the-art semantic parsers\nbased on pre-trained language models (LMs).\nIn this study, we empirically investigate im-\nproving an LM’s generalization in semantic\nparsing with two simple techniques: at the to-\nken level, we introduce a token preprocessing\nmethod to preserve the semantic boundaries\nof tokens produced by LM tokenizers; at the\nsequence level, we propose to use special to-\nkens to mark the boundaries of components\naligned between input and output. Our exper-\nimental results on two text-to-SQL semantic\nparsing datasets show that our token prepro-\ncessing, although simple, can substantially im-\nprove the LM performance on both types of\ngeneralization, and our component boundary\nmarking method is particularly helpful for com-\npositional generalization.1\n1 Introduction\nPre-trained language models (LMs) 2 such as T5\n(Raffel et al., 2020) have now been more and more\nwidely adopted for semantic parsing due to their\npromising performance and straightforward archi-\ntectures (Shaw et al., 2021; Scholak et al., 2021;\nYin et al., 2021; Qi et al., 2022; Xie et al., 2022;\nQiu et al., 2021). However, recent work revealed\nthat these LMs still struggle to generalize on out-\nof-distribution (OOD) samples (Lake and Baroni,\n2018; Keysers et al., 2019; Shaw et al., 2021; Qiu\net al., 2022b). For example, if a parser has learned\n“how many heads are in the department” and “how\nmany people are older than 56”, it is expected to\ngeneralize to “how many heads of the departments\n1The source code for our implementation is available at\nhttps://github.com/Dakingrai/ood-generalizatio\nn-semantic-boundary-techniques.\n2We use “LMs” to refer to a broad set of models that\nare pre-trained in (masked/autoregressive) language modeling\nobjectives, with encoder-decoder or decoder-only architecture.\nToken Preprocessing (applied to database schema)\nBefore: department_management| department:| id\n, budget_in_billions , num_employees\nAfter: department_management| department:| id\n,budget_in_billions,num_employees\nToken Preprocessing (applied to SQL)\nBefore: select avg ( flight.price) where\nflight.origin= ‘NewYork’\nAfter: selectaverage( flight. price) where\nflight. origin= ‘NewYork’\nComponent Boundary Marking (applied to NL input and\nSQL output)\nBefore: How many heads of the departments are older than\n56 ?\nselect count (head.*) where head.age > 56\nAfter: [sep0]How many heads of the departments\n[/sep0] [sep1]are older than 56 ?[/sep1]\n[sep0] select count (head.*) [/sep0] [sep1]\nwhere head.age > 56 [/sep1]\nTable 1: Our proposed techniques. Top: we preprocess\nthe text such that its T5 tokenization aligns with word\nsemantics. Coloring indicates tokenization; for example,\n“avg” is converted into three tokens of “a”, “v” and “g”.\nBottom: we add separator tokens to mark the boundaries\nof aligned semantic components in the input and output.\nare older than 56”. Generalizing to such novel com-\nponent compositions is known as compositional\ngeneralization. Additionally, generalizing to new\ndomains (e.g., from “entertainment” to “flight”) is\nreferred to as domain generalization.\nIn this paper, we investigate these two types\nof generalization of LMs in text-to-SQL seman-\ntic parsing, i.e., given a natural language (NL) in-\nput and the database schema, producing a SQL\nquery that can be executed against the database\nfor desired output. We conduct experiments us-\ning the cross-database Spider benchmark (Yu et al.,\n2018b) and its derivation Spider-CG (Gan et al.,\n2022). Compared with existing benchmarks (Key-\nsers et al., 2019; Lake and Baroni, 2018), this task\nsetting is both more realistic (e.g., containing larger\nlanguage variations) and more challenging (e.g., re-\nquiring grounding to the database context).\n150\nAlthough previous work tackling the two types\nof generalization all requires non-trivial engineer-\ning effort (see Section 2), in this work, we present\ntwo simple yet effective techniques, which are ex-\ntremely easy to implement with LMs (Table 1).\nOur techniques improve the generalization of LMs\nby preserving the semantic boundaries at the token\nand the sequence levels. At the token level, our\nfirst technique rewrites the inputs to handle naming\nconventions in database schemas and SQL queries\nsuch that a pre-trained LM tokenizer can split them\ninto semantically meaningful tokens. At the se-\nquence level, our second technique introduces spe-\ncial tokens to mark the semantic boundaries (e.g.,\nphrases) aligned between the source NL and the tar-\nget SQL. These special tokens implicitly help the\nLM-based parser build more precise input-output\ncorrespondences that are crucial for compositional\ngeneralization.\nOn five evaluation sets, the experimental results\nbased on T5-base show that, albeit simple, our\ntoken-level technique dramatically improves both\ntypes of LM generalization, and our sequence-level\ntechnique is particularly helpful for compositional\ngeneralization. Combining them together leads to\nfurther improvements. Our additional experiments\nfurther demonstrate the generalizability of our ap-\nproaches (e.g., to text-to-LISP expression parsing\n(Semantic Machines et al., 2020)).\n2 Related Work\nText-to-SQL Semantic Parsing. This task has\nreceived considerate attention since the creation of\nthe WikiSQL (Zhong et al., 2017) and Spider (Yu\net al., 2018b) datasets. While a large amount of\nexisting work designed specialized architectures\nfor this task (Yu et al., 2018a; Zhang et al., 2019;\nWang et al., 2020; Lin et al., 2020), there has been\na trend of directly fine-tuning pre-trained sequence-\nto-sequence models as semantic parsers (Shaw\net al., 2021; Scholak et al., 2021; Xie et al., 2022;\nQi et al., 2022). Our work follows the same line and\nproposed approaches to further improve the LM\nperformance. On the other hand, Guo et al. (2019);\nGan et al. (2021); Herzig et al. (2021) showed that\nsimplifying the SQL representation in a way that\nthe new representation can semantically better align\nwith the NL can dramatically improve the parsing\nperformance. In our work, we follow the NatSQL\nrepresentation (Gan et al., 2021) as it has better\nalignments with the NL.\nInjecting Priors into Semantic Parsers.Our two\ntechniques can be viewed as injecting human prior\nknowledge into neural models for better general-\nization, which has been one of the major research\nefforts on improving domain and compositional\ngeneralization. The key consideration to be taken\nwhen injecting priors is the trade-off between the\nform and the generalizability. Strong priors in\nthe form of specialized model architectures (Shaw\net al., 2021; Herzig and Berant, 2021; Wang et al.,\n2021) are either too expensive or not applicable\nacross domains. Weaker priors in terms of special-\nized training algorithms (Yin et al., 2021; Conklin\net al., 2021) are more general, but often weaker in\nperformance compared to other lines of methods.\nOur work is in the spirit of the third line on the\nuse of data augmentation (Andreas, 2020; Akyürek\net al., 2020; Qiu et al., 2022a). However, instead of\nsynthesizing new data from scratch, we “annotate”\nthe data with semantic boundary markers, which is\nnot only much simpler but also brings better perfor-\nmance. The final line of work (Qiu et al., 2022b;\nLevy et al., 2022) is based on the learning capaci-\nties in the context of large LMs, which is out of the\nscope of this work.\n3 Methods\n3.1 Token Preprocessing\nBefore preprocessing After preprocessing\nSnake case in schema items (add space)\nbooking_status_code booking_ status_ code\ndocument_type document_ type\nDot notation in column references (add space)\nfarm.cows farm. cows\norigin.flight origin. flight\nSQL keyword (expand spelling)\navg average\ndesc descending\nTable 2: Three token preprocessing types. Coloring\nindicates tokenization, same as Table 1.\nWe present our two techniques for improving\nthe generalization of LM-based semantic parsers.\nLM pre-training learns high-quality contextualized\nword representation (Devlin et al., 2019), but to ef-\nfectively use it on a downstream task, the tokeniza-\ntion needs to “make sense.” For example, if the text\n“pet_age” is tokenized as “pet”, “_” and “age”, then\nthe semantics of “pet” and “age” acquired during\npretraining can be directly used. However, if it is\n151\nDataset Size Usage Generalization Type\nSpiderT 7,000 Train None (in-distribution)\nSpiderD 1,034 Eval Domain\nCG-SUBT 20,686 Eval None (in-distribution)\nCG-SUBD 2,883 Eval Domain\nCG-APPT 18,793 Eval Composition\nCG-APPD 3,237 Eval Domain & Composition\nTable 3: Datasets in our experiments.\ntokenized as “pe”, “t_a” and “ge”, then pre-training\nis hardly useful because the model does not even\nrecognize the two semantic words.\nUnfortunately, this latter case is very common\nwhen tokenizing non-natural language texts, such\nas database schemas and SQL queries. Thus, we\npropose a token preprocessing method to induce\nmore natural tokenization by, at a high level, adding\nwhite spaces and handling the naming conventions\nin database schema and SQL queries. We show\nexamples in Table 2 and details in Appendix A.\n3.2 Component Boundary Marking\nAt the sequence level, our second technique further\nassists LMs in recognizing the semantic boundaries\nof components aligned between input and output.\nAn example is shown in Table 1. While prior work\nhas attempted the goal via implementing alignment-\nbased attention supervision (Yin et al., 2021), we\npropose to insert special tokens in input and out-\nput to inject such bias. Specifically, we use pairs\nof “[sepN]” and “ [/sepN]”, N ∈ Z, to mark\nthe boundaries, so as to hint the LM that compo-\nnents within the paired special tokens should be\naligned. In practice, we also observed cases where\nan NL component has to be aligned with a SQL\ncomponent consisting of multiple non-continuous\nsegments. To handle it, we will apply the same\npair of special tokens to each segment of the same\ncomponent. An example is shown in Table 8 in the\nAppendix.\nFinally, we note that our method assumes the\navailability of component annotations. Such anno-\ntations can be obtained via human labeling (Gan\net al., 2021), heuristic rules (Yin et al., 2021), or\nother advanced machine learning algorithms, but\nthis is beyond the scope of our work.\n4 Experiments\n4.1 Setup\nDatasets. We use two datasets, Spider (Yu et al.,\n2018b) and Spider-CG (Gan et al., 2022). Spider\nconsists of a training set (SpiderT ) and a develop-\nment set (SpiderD) with non-overlapping domains\nbut otherwise similar data characteristics (e.g.,\nlength). Thus, we train the models on SpiderT , and\nconsider SpiderD as the evaluation for domain gen-\neralization. Spider-CG is derived from Spider by\nfirst dissecting each Spider instance into different\ncomponents according to its dependency parse and\ngenerates data in two ways: substituting a compo-\nnent in one instance with one from another instance\nand appending one component from one instance\nto another instance. Depending on whether the\ninstances come from the Spider training or devel-\nopment set, we get four splits: CG-SUB T , CG-\nSUBD, CG-APPT and CG-APPD, all of which are\nonly used for evaluation. The instances created\nunder substitution share similar data characteristics\nwhile those under appending are much longer, so\na good model performance on the latter requires\ncompositional generalization. Table 3 summarizes\nthe dataset information. In addition, we use the\nNatSQL representation (Gan et al., 2021) through-\nout the experiment due to its better alignment with\nthe NL input.\nEvaluation Metrics.We follow the standard Spi-\nder benchmarking and employ two evaluation met-\nrics. Exact Match (EM)compares the generated\nand the ground-truth query by performing exact\nset matching at the lexical level (Yu et al., 2018b).\nExecution Match (EX)measures whether execut-\ning the generated query on the given database can\nyield the same results as using the ground truth.\nNotably, for a fair comparison with existing seman-\ntic parsers on the Spider leader board, we follow\nGan et al. (2022), convert each generated NatSQL\nquery into a SQL query, and report the evaluation\nresults based on the converted SQL query.\nModels, Baselines, and Implementation. We\nevaluate our proposed techniques by applying them\nto the pre-trained T5 model (Raffel et al., 2020).\nOur experiments are conducted using T5-base, with\nthe use of database contents following Lin et al.\n(2020). As our second technique leverages com-\nponent boundary labels to encourage the composi-\ntional generalization of LM, we compare it with a\nbaseline (Yin et al., 2021) which similarly assumes\nthe labels but utilizes them in a more complicated\nway, i.e., transforming the component alignments\ninto supervision on the cross attention between\ninput and output of the LM. We denote this base-\n152\nModel SpiderD CG-SUBT CG-SUBD CG-APPT CG-APPD\nEM EX EM EX EM EX EM EX EM EX\nSemantic Parsers with Specialized Architectures (Gan et al., 2022)\nRATSQLB(S) 71.9 - 91.0 - 72.6 - 79.8 - 61.5 -\nRATSQLG(S) 74.5 - 91.4 - 76.7 - 82.5 - 68.3 -\nSemantic Parsers based on LMs\nT5-base 64.6 67.9 83.8 88.1 69.1 71.1 60.2 70.3 45.0 54.9\nT5-base + Tok 71.8 75.6 85.9 89.5 74.1 78.6 65.2 73.8 54.2 65.9\nT5-base + Comp 64.4 68.2 86.3 90.2 69.3 73.1 69.8 77.9 53.5 63.4\nT5-base + Tok + Comp 69.4 73.2 86.6 90.7 76.6 79.8 71.1 77.8 61.0 69.4\nT5-base + Tok + Attn. Sup 69.4 73.7 83.6 87.7 71.7 75.6 62.3 70.8 56.3 66.2\nTable 4: Results (%) on different evaluation sets. Top: state-of-the-art model using specialized architecture; numbers\nare collected from its paper and only EM is reported (code unavailable). Bottom: T5-base models with our proposed\nor baseline techniques; we report the average performance of each model over three runs. Tok: token preprocessing.\nComp: component boundary marking. Attn. Sup: the attention supervision method of Yin et al. (2021).\nline as Attn. Sup.3 For both methods, we lever-\nage component annotations from Spider-SS (Gan\net al., 2022). These annotations were generated by\napplying a syntactic parser to decompose the NL\nquestion into sub-questions and then manually an-\nnotating their corresponding NatSQL components.\nWe also compare with the state-of-the-art models,\nRATSQLB(S) and RATSQLG(S), from Gan et al.\n(2022), although their models adopt a specialized\narchitecture (i.e., RATSQL (Wang et al., 2020)) and\nRATSQLG(S) additionally employed task-specific\npre-training (Shi et al., 2021). Both models used\nthe same component annotations from Spider-SS.\nFinally, for each of our model variants in Ta-\nble 4, we repeat the experiment three times, using\nthree random seeds consistently across all models,\nand report the average results. We include more\nimplementation details in Appendix D.\n4.2 Results\nMain Results. We present our results in Table\n4. First, all models obtain the best performance on\nthe in-distribution evaluation set CG-SUBT while\nsuffering from more than 10% performance drops\non others, confirming the challenges of the domain\nand compositional generation. As expected, all\nmodels have the worst performance on CG-APPD,\nwhich requires both types of generalization. Be-\ntween the two types, it is also observed that compo-\nsitional generalization (as measured by CG-APPT )\n3In our implementation, we apply the supervision to cross-\nattention distribution averaged across all decoder layers and\nheads. We also tried cross-attention from only the top decoder\nlayer, but the results are similar.\nis more challenging than domain generalization (as\nmeasured by SpiderD and CG-SUBD).\nSecond, our results show that the token prepro-\ncessing method, albeit simple, can improve both\ndomain and compositional generalizations of LMs\ndramatically. For example, comparing T5-base\nwith T5-base+Tok, the latter is improved by around\n5-7% EM and 7% EX for domain generalization\n(on SpiderD and CG-SUBD), 5% EM and 3.5% EX\nfor compositional generalization (on CG-SUBT ),\nand 9% EM and 11% EX for the challenging case\nwhen both types occur (on CG-APPD). Addition-\nally, we also show the effectiveness of token pre-\nprocessing with T5-3B on SpiderD in App. B.\nMoving on to our proposed component boundary\nmarking method, it shows to be particularly help-\nful for compositional generalization. Specifically,\napplying it to T5-base leads to a 9% EM and 7%\nEX increase on CG-APPT , and an 8% EM and 8%\nEX increase on CG-APPD. On the in-distribution\nevaluation set, this technique also gives slight im-\nprovement, whereas, for domain generalization,\nthere is no obvious impact from this technique.\nFinally, augmenting T5-base with both tech-\nniques (i.e., T5-base+Tok+Comp) leads to better\nperformance than applying each technique individ-\nually in most evaluation sets, implying that our\ntwo techniques are complementary to each other.\nSpecifically, for in-distribution evaluation, using\neach technique individually or both of them to-\ngether yield similar results; for domain general-\nization, there is no additional gain from applying\ncomponent boundary marking on the top of the\ntoken preprocessing; for compositional generaliza-\n153\ntion, the two techniques together contribute the best\nEM across all models and baselines. Overall, com-\nbining the two techniques shrinks the performance\ngap between in-distribution and domain OOD by\naround 2-4% EM, composition OOD by 7%, and\njoint OOD by 13%.\nCompared with Special Architectures. De-\nspite its simplicity, our T5-base+Tok+Comp model\nachieves comparable or better performance than the\ntwo RATSQL variants on CG-SUBD. It also per-\nforms comparably to RATSQLB(S) on CG-APPD.\nCompared with Attn. Sup.Surprisingly, the at-\ntention supervision has only led to around 2% EM\nand 1.5% EX gains on CG-APPD, while no further\nadvantage is observed on other evaluation sets. In\nour conjecture, this is due to the misalignment be-\ntween the objective of Attn. Sup (Yin et al., 2021)\nand the attention mechanism of pre-trained LMs.\nSpecifically, Attn. Sup encourages the attention\ndistribution of different heads to be consistent with\nthe component alignment supervision. However,\nprior work (V oita et al., 2019) suggests that differ-\nent attention heads of even the same layer may have\ndifferent functions and roles. Thus, when coarsely\ndefining the objective function, it may not allow for\nthe most effective supervision. Furthermore, simi-\nlar to our finding, Yin et al. (2021) did not observe\nperformance gain when they applied Attn. Sup to\nT5-base on CFQ (Keysers et al., 2020).\nQualitative Analysis on Tokenization.To qual-\nitatively understand how our token preprocessing\nhelps the generalization, we randomly sampled 50\nexamples from the Spider D to analyze how fre-\nquently the T5 tokenizer divides tokens into less\nmeaningful subtokens. Consequently, we found\n243 tokenization issues in total, and 140 of them\ncan be resolved by our token preprocessing. The\nremaining cases are like splitting “id” into “i” and\n“d” as shown in Table 1, which is beyond our scope.\nError Analysis on Component Boundary Mark-\ning. We manually examined 50 error predictions\nfrom T5-base+Tok+Comp and contrasted them\nwith the errors of T5-base+Tok. Intriguingly, we\nobserved much more frequent schema items or\nvalue hallucinations from the former. For exam-\nple, it may generate queries accessing non-existing\ncolumns in a table, or misspells the literal values\nin the queries. We conjecture that this is because\nour component boundaries are only applied to the\nNL input, not the database schema (note that literal\nvalues are grounded and attached to schema items\nModel Exact Match\nCOARSE2FINE + SS (Span-level Sup.) 47.4\nT5-base 63.9\nT5-base + Tok 65.1\nT5-base + Tok + Comp 67.7\nTable 5: Results (%) on SMCalFlow-Compositional\nSkills dataset (16-shot setting). Top: Result from Yin\net al. (2021). Bottom: T5-base models with our pro-\nposed or baseline techniques; we report the average\nperformance of each model over three runs.\nin their input representations; see Appendix D for\ndetails). This reveals a new challenge of LM gen-\neralization in text-to-SQL semantic parsing, i.e.,\nhow to properly handle the database schema when\ninjecting prior knowledge into LMs for composi-\ntional generalization.\nGeneralizing to Other Semantic Parsing Tasks.\nWhile our main focus in this work is on text-to-\nSQL parsing, we also investigate whether our ap-\nproaches can generalize beyond this specific task.\nTo this end, we implemented both of our techniques\nto SMCalFlow-CS (Yin et al., 2021), a composi-\ntional generalization dataset for text-to-LISP ex-\npression parsing (Semantic Machines et al., 2020).\nFor “+Comp”, We utilize the span-level alignments\nheuristically derived by Yin et al. (2021) as com-\nponent annotations.4 Our results in Table 5 show\nthat: (1) Our token preprocessing can be univer-\nsally helpful for LMs to model schema items, pred-\nicates, etc., leading to 1.2% performance gain over\nT5-base; (2) Our component boundary marking\nmethod is highly effective for compositional gener-\nalization, which offers 2.6% additional gain.\n5 Conclusion\nIn this paper, we present two simple yet effective\ntechniques to improve the domain and composi-\ntional generalization of LMs in text-to-SQL seman-\ntic parsing. Our techniques aid LMs in preserving\nthe semantic boundaries of tokens and components\nin their input and output. We also demonstrate\ntheir potential to be generalized to other semantic\nparsing tasks.\n4Yin et al.’s approach requires knowing the ground-truth\nLISP expression when deriving the component boundaries\nfor the input question. In our experiment, we assume the\navailability of these question boundaries at test time and focus\non showcasing the potential of “Comp”, while automating this\nquestion decomposition is left as future work.\n154\nLimitations\nFuture work can further apply our approaches\nto other semantic parsing tasks. For example,\nfor parsing texts to lambda-calculus expressions\nfor knowledge base question answering (Dong\nand Lapata, 2016), one can similarly preprocess\nthe schema items (e.g., “department_time” into\n“department _ time ”) and typed values (e.g.,\n“dallas:ci” into “dallas : ci”) for more mean-\ningful subword tokenization results. In addition,\nour experiments are based on T5. To further verify\nthe effectiveness of our techniques, one can apply\nthem to other pre-trained language models such as\nBART (Lewis et al., 2020) and GPT-2 (Radford\net al., 2019) as well.\nAcknowledgments\nWe would like to thank all anonymous reviewers\nfor their constructive comments. We also thank Yu-\njian Gan and Xinyun Chen for their help in using\nthe NatSQL and the Spider-SS datasets, as well as\nPengcheng Yin for using the code base of Attn. Sup.\nThis project was supported by resources provided\nby the Office of Research Computing at George\nMason University (https://orc.gmu.edu ) and\nfunded in part by grants from the National Sci-\nence Foundation (Awards Number 1625039 and\n2018631).\nReferences\nEkin Akyürek, Afra Feyza Akyürek, and Jacob An-\ndreas. 2020. Learning to recombine and resample\ndata for compositional generalization. arXiv preprint\narXiv:2010.03706.\nJacob Andreas. 2020. Good-enough compositional data\naugmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7556–7566, Online. Association for\nComputational Linguistics.\nHenry Conklin, Bailin Wang, Kenny Smith, and Ivan\nTitov. 2021. Meta-learning to compositionally gen-\neralize. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3322–3335, Online. Association for Computa-\ntional Linguistics.\nDeepSpeed. 2023. https://github.com/microsoft/deepspeed.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLi Dong and Mirella Lapata. 2016. Language to logical\nform with neural attention. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n33–43, Berlin, Germany. Association for Computa-\ntional Linguistics.\nYujian Gan, Xinyun Chen, Qiuping Huang, and\nMatthew Purver. 2022. Measuring and improving\ncompositional generalization in text-to-SQL via com-\nponent alignment. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 831–\n843, Seattle, United States. Association for Compu-\ntational Linguistics.\nYujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver,\nJohn R. Woodward, John Drake, and Qiaofu Zhang.\n2021. Natural SQL: Making SQL easier to infer from\nnatural language specifications. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 2030–2042, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-\nGuang Lou, Ting Liu, and Dongmei Zhang. 2019. To-\nwards complex text-to-SQL in cross-domain database\nwith intermediate representation. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4524–4535, Florence,\nItaly. Association for Computational Linguistics.\nJonathan Herzig and Jonathan Berant. 2021. Span-\nbased semantic parsing for compositional general-\nization. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 908–921, Online. Association for Computa-\ntional Linguistics.\nJonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin\nGuu, Panupong Pasupat, and Yuan Zhang. 2021. Un-\nlocking compositional generalization in pre-trained\nmodels using intermediate representations. arXiv\npreprint arXiv:2104.07478.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,\nMarc van Zee, and Olivier Bousquet. 2020. Measur-\ning compositional generalization: A comprehensive\nmethod on realistic data. In International Conference\non Learning Representations.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\n155\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. arXiv preprint arXiv:1912.09713.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In Pro-\nceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Ma-\nchine Learning Research, pages 2873–2882. PMLR.\nItay Levy, Ben Bogin, and Jonathan Berant. 2022.\nDiverse demonstrations improve in-context\ncompositional generalization. arXiv preprint\narXiv:2212.06800.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nXi Victoria Lin, Richard Socher, and Caiming Xiong.\n2020. Bridging textual and tabular data for cross-\ndomain text-to-SQL semantic parsing. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 4870–4888, Online. Association\nfor Computational Linguistics.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nChenghu Zhou, Xinbing Wang, Quanshi Zhang, and\nZhouhan Lin. 2022. Rasat: Integrating relational\nstructures into pretrained seq2seq model for text-to-\nsql. arXiv preprint arXiv:2205.06983.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Pawel\nNowak, Tal Linzen, Fei Sha, and Kristina Toutanova.\n2022a. Improving compositional generalization with\nlatent structure and data augmentation. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4341–4362, Seattle, United States. Association for\nComputational Linguistics.\nLinlu Qiu, Peter Shaw, Panupong Pasupat,\nPaweł Krzysztof Nowak, Tal Linzen, Fei Sha,\nand Kristina Toutanova. 2021. Improving composi-\ntional generalization with latent structure and data\naugmentation. arXiv preprint arXiv:2112.07610.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi,\nJonathan Herzig, Emily Pitler, Fei Sha, and Kristina\nToutanova. 2022b. Evaluating the impact of model\nscale for compositional generalization in semantic\nparsing. arXiv preprint arXiv:2205.12253.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nSemantic Machines, Jacob Andreas, John Bufe, David\nBurkett, Charles Chen, Josh Clausman, Jean Craw-\nford, Kate Crim, Jordan DeLoach, Leah Dorner, Ja-\nson Eisner, Hao Fang, Alan Guo, David Hall, Kristin\nHayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Sm-\nriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lan-\nman, Percy Liang, Christopher H. Lin, Ilya Lints-\nbakh, Andy McGovern, Aleksandr Nisnevich, Adam\nPauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro\nRoy, Jesse Rusak, Beth Short, Div Slomin, Ben Sny-\nder, Stephon Striplin, Yu Su, Zachary Tellman, Sam\nThomson, Andrei V orobev, Izabela Witoszko, Jason\nWolfe, Abby Wray, Yuchen Zhang, and Alexander\nZotov. 2020. Task-oriented dialogue as dataflow syn-\nthesis. Transactions of the Association for Computa-\ntional Linguistics, 8:556–571.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2021. Compositional generaliza-\ntion and natural language variation: Can a semantic\nparsing approach handle both? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 922–938, Online. Asso-\nciation for Computational Linguistics.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Cicero Nogueira\ndos Santos, and Bing Xiang. 2021. Learning con-\ntextual representations for semantic parsing with\ngeneration-augmented pre-training. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 35, pages 13806–13814.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nBailin Wang, Mirella Lapata, and Ivan Titov. 2021.\nStructured reordering for modeling latent alignments\nin sequence transduction. Advances in Neural Infor-\nmation Processing Systems, 34:13378–13391.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\n156\nRelation-aware schema encoding and linking for text-\nto-SQL parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578, Online. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I Wang,\net al. 2022. Unifiedskg: Unifying and multi-tasking\nstructured knowledge grounding with text-to-text lan-\nguage models. arXiv preprint arXiv:2201.05966.\nPengcheng Yin, Hao Fang, Graham Neubig, Adam\nPauls, Emmanouil Antonios Platanios, Yu Su, Sam\nThomson, and Jacob Andreas. 2021. Compositional\ngeneralization for neural semantic parsing via span-\nlevel supervised attention. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2810–2823, Online.\nAssociation for Computational Linguistics.\nTao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,\nDongxu Wang, Zifan Li, and Dragomir Radev. 2018a.\nSyntaxSQLNet: Syntax tree networks for complex\nand cross-domain text-to-SQL task. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1653–1663, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018b. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nRui Zhang, Tao Yu, Heyang Er, Sungrok Shim, Eric\nXue, Xi Victoria Lin, Tianze Shi, Caiming Xiong,\nRichard Socher, and Dragomir Radev. 2019. Editing-\nbased SQL query generation for cross-domain\ncontext-dependent questions. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5338–5349, Hong Kong,\nChina. Association for Computational Linguistics.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\nA Token Preprocessing Details\nWe propose a simple token preprocessing method.\nInstead of directly feeding the input to the subword\ntokenizer, we introduce three preprocessing steps:\n(1) For schema items in input and output, reversing\nthe snake case to the normal, e.g., “ pet_age” to\n“pet _ age”; (2) For any call of “Table.Column”,\nsplitting the tokens around the access operator “.”\n(i.e., “Table . Column”); and (3) Replacing any\nreserved words that cannot be properly handled\nin NatSQL, e.g., “avg” to “average”. In practice,\nwe also handle formalism-specific special tokens,\ne.g., adding the “less than” operator “ <” to the\nvocabulary of T5 tokenizer. While we showcase\nour token preprocessing under text-to-SQL parsing,\nthe intuition can be generalized to other formalisms\n(e.g., regex, λ-expression) easily.\nIn addition, we also check the issue of tokeniza-\ntion in other popular LM tokenizers and found that\nthe tokenization issue is not specific to T5. Exam-\nples of bad tokenization from BERT (Devlin et al.,\n2019) and GPT2 (Radford et al., 2019) tokeniz-\ners and after our token preprocessing are listed in\nTable 6.\nGPT2 Tokenizer\nBefore: student_enrolment_courses\nAfter: student_ enrolment_ courses\nBefore: transcripts.transcript_date\nAfter: transcripts . transcript_ date\nBefore: avg\nAfter: average\nBERT Tokenizer\nBefore: singer.NetWorthMillions\nAfter: singer. NetWorthMillions\nBefore: avg\nAfter: average\nBefore: asc\nAfter: ascending\nTable 6: Tokenization of snake case, camel case, and\ntoken notation in BERT and GPT2 tokenizer. Coloring\nindicates tokenization, same as Table 1.\nB T5-3B Experiment\nTo assess the effectiveness of our token preprocess-\ning technique with larger LMs, we apply it to T5-\n3B and evaluate the model on SpiderD. The results\n157\nModel SpiderD\nEM EX\nT5-3B (w deepspeed) 73.2 77.4\nT5-3B (w/o deepspeed) 76.0 79.8\nT5-3B + Tok (w deepspeed) 74.4 78.7\nT5-3B + Tok (w/o deepspeed) 77.4 80.9\nTable 7: Results (%) on Spider D when T5-3B(+Tok)\nwas trained with or without using deepspeed.\nare shown in Table 7. Our results show that T5-\n3B+Tok has a performance gain of 1.1%, indicating\nthat it is helpful for larger LMs as well. Addition-\nally, we also provide results with and without using\nDeepSpeed (2023), a deep learning optimization\nlibrary that is used to train large models more effi-\nciently. Surprisingly, although DeepSpeed (2023)\nhelped us improve training speed, we found a per-\nformance drop of around 2.1-2.2% EX while using\nit. However, our token preprocessing consistently\nleads to around 1.0% absolute performance gain.\nC Component Boundary Marking Details\nIn Table 8, we present one more example of com-\nponent boundary marking. In this example, the NL\ncomponent “What is the most populace city” is\naligned with two non-continuous SQL segments,\n“select city.Name, city.Population” and\n“order by city.Population desc limit 1” .\nTo handle such cases, we apply the same pair of\nspecial tokens “[sep0]” “[/sep0]” twice, one\nfor each segment.\nComponent Boundary Marking Example\nBefore: What is the most populace city that speaks English?\nSelect city.Name, city.Population where\ncountrylanguage.Language = “English” order\nby city.Population desc limit 1\nAfter: [sep0]What is the most populace city[/sep0] [sep1]\nthat speaks English?[/sep1]\n[sep0] select city.Name , city.Population\n[/sep0] [sep1] where countrylanguage.Language\n= \"English\" [/sep1] [sep0] order by\ncity.Population desc limit 1 [/sep0]\nTable 8: An example of component boundary marking\nwhen an NL component aligns with non-continuous\nsegments in the SQL side. In this case, we apply the\nspecial tokens for each segment.\nD Implementation Details\nOur experiments are conducted based on the pre-\ntrained T5 model. The input to T5 follows the same\nformat and order as Scholak et al. (2021) (except\nour additional token preprocessing, if applied), i.e.,\n“Question | Database 1 | Table 1: Column 1,\nColumn 2,...| Table 2: Column 1, Column\n2...”. We also use the database contents as parts\nof the input, following Lin et al. (2020). For ex-\nample, if the NL question mentions a literal value\n(e.g., “New York”), the appearance of whom can be\nfound in the contents of a certain “Column 1” via\nfuzzy string matching, then when we represent the\ndatabase schema, we will include it via “Database\n1 | Table 1: Column 1 (New York), Column\n2, ...”.\nWe fine-tune the T5-base LM that consists of\n220 million parameters on NVIDIA A100 GPU for\n10-12 hours. It was trained with a learning rate of\n10−4 and batch size 16 for T5-base for a maximum\nof 20K training steps. The model is evaluated on\nSpiderD for every 1K training steps, and the best\ncheckpoint is selected based on the model EM on\nSpiderD. In inference time, we perform simple\ngreedy decoding.\nWe use the PyTorch-Transformers library (Wolf\net al., 2020), which is a library for state-of-the-\nart pre-trained models for NLP, to fine-tune our\nmodels. Specifically, our code for fine-tuning T5-\nbase is adapted from PICARD’s implementation\n(Scholak et al., 2021). Furthermore, we also use\nDeepSpeed (2023) to fine-tune all of our T5-base\nmodels.\nDatasets. We used Spider (Yu et al., 2018b), Nat-\nSQL (Gan et al., 2021), Spider-CG (Gan et al.,\n2022), and SMCalFlow-CS (Yin et al., 2021) in\nour work. They are under the license of CC BY-SA\n4.0. Our use of these datasets is consistent with\ntheir intended use, i.e., for scientific research. All\ndatasets are in English. They contain annotated NL\nand SQL or NatSQL or LISP expression pairs from\nthe open domain.\n158\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe don’t see the potential of how our two techniques can be misused.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n2, 4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nB\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nB\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSensitive contents are unlikely to be contained in the datasets we used. For example, for Spider-CG,\nit is annotated by domain experts.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nB\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n4.1\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nB\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n159\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.1\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.1\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n160",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8886064291000366
    },
    {
      "name": "Parsing",
      "score": 0.7098677754402161
    },
    {
      "name": "Natural language processing",
      "score": 0.6701403856277466
    },
    {
      "name": "Generalization",
      "score": 0.6604527831077576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6548058986663818
    },
    {
      "name": "Security token",
      "score": 0.5559395551681519
    },
    {
      "name": "Preprocessor",
      "score": 0.5063837766647339
    },
    {
      "name": "Programming language",
      "score": 0.4744452238082886
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    }
  ],
  "cited_by": 12
}