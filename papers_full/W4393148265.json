{
  "title": "SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency",
  "url": "https://openalex.org/W4393148265",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104339954",
      "name": "Wang Cong",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2100349836",
      "name": "Jinshan Pan",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135079653",
      "name": "Wanyu Lin",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2739504569",
      "name": "Jiangxin Dong",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2196274784",
      "name": "Xiao-Ming Wu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2104339954",
      "name": "Wang Cong",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2100349836",
      "name": "Jinshan Pan",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135079653",
      "name": "Wanyu Lin",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2739504569",
      "name": "Jiangxin Dong",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2196274784",
      "name": "Xiao-Ming Wu",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283831644",
    "https://openalex.org/W3180034634",
    "https://openalex.org/W6736170873",
    "https://openalex.org/W3191278083",
    "https://openalex.org/W3022619948",
    "https://openalex.org/W6783197463",
    "https://openalex.org/W3111551570",
    "https://openalex.org/W4310998876",
    "https://openalex.org/W4280590736",
    "https://openalex.org/W4312617404",
    "https://openalex.org/W2566376500",
    "https://openalex.org/W6647720530",
    "https://openalex.org/W4311251292",
    "https://openalex.org/W4385963586",
    "https://openalex.org/W4303648905",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1987444808",
    "https://openalex.org/W6750330310",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2968878340",
    "https://openalex.org/W6770463144",
    "https://openalex.org/W2565312867",
    "https://openalex.org/W3081167590",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W6801802051",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6754146604",
    "https://openalex.org/W6848407440",
    "https://openalex.org/W4312625691",
    "https://openalex.org/W4292434331",
    "https://openalex.org/W4283373024",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2902303185",
    "https://openalex.org/W3089141666",
    "https://openalex.org/W3201770677",
    "https://openalex.org/W4312400623",
    "https://openalex.org/W4387968529",
    "https://openalex.org/W2990007814",
    "https://openalex.org/W2984422960",
    "https://openalex.org/W2102166818",
    "https://openalex.org/W4287551327",
    "https://openalex.org/W4311398044",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2963928582",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4322746835",
    "https://openalex.org/W2128254161",
    "https://openalex.org/W2949187370",
    "https://openalex.org/W3034278302",
    "https://openalex.org/W4382239347",
    "https://openalex.org/W2083610878",
    "https://openalex.org/W4313059954",
    "https://openalex.org/W2966926453"
  ],
  "abstract": "This work presents an effective depth-consistency Self-Prompt Transformer, terms as SelfPromer, for image dehazing. It is motivated by an observation that the estimated depths of an image with haze residuals and its clear counterpart vary. Enforcing the depth consistency of dehazed images with clear ones, therefore, is essential for dehazing. For this purpose, we develop a prompt based on the features of depth differences between the hazy input images and corresponding clear counterparts that can guide dehazing models for better restoration. Specifically, we first apply deep features extracted from the input images to the depth difference features for generating the prompt that contains the haze residual information in the input. Then we propose a prompt embedding module that is designed to perceive the haze residuals, by linearly adding the prompt to the deep features. Further, we develop an effective prompt attention module to pay more attention to haze residuals for better removal. By incorporating the prompt, prompt embedding, and prompt attention into an encoder-decoder network based on VQGAN, we can achieve better perception quality. As the depths of clear images are not available at inference, and the dehazed images with one-time feed-forward execution may still contain a portion of haze residuals, we propose a new continuous self-prompt inference that can iteratively correct the dehazing model towards better haze-free image generation. Extensive experiments show that our SelfPromer performs favorably against the state-of-the-art approaches on both synthetic and real-world datasets in terms of perception metrics including NIQE, PI, and PIQE. The source codes will be made available at https://github.com/supersupercong/SelfPromer.",
  "full_text": "SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency\nCong Wang1*, Jinshan Pan2, Wanyu Lin1, Jiangxin Dong2, Wei Wang3, Xiao-Ming Wu1\n1Department of Computing, The Hong Kong Polytechnic University\n2School of Computer Science and Engineering, Nanjing University of Science and Technology\n3International School of Information Science and Engineering, Dalian University of Technology\nAbstract\nThis work presents an effective depth-consistency Self-\nPrompt Transformer, terms as SelfPromer, for image dehaz-\ning. It is motivated by an observation that the estimated\ndepths of an image with haze residuals and its clear counter-\npart vary. Enforcing the depth consistency of dehazed images\nwith clear ones, therefore, is essential for dehazing. For this\npurpose, we develop a prompt based on the features of depth\ndifferences between the hazy input images and correspond-\ning clear counterparts that can guide dehazing models for\nbetter restoration. Specifically, we first apply deep features\nextracted from the input images to the depth difference fea-\ntures for generating the prompt that contains the haze resid-\nual information in the input. Then we propose a prompt em-\nbedding module that is designed to perceive the haze residu-\nals, by linearly adding the prompt to the deep features. Fur-\nther, we develop an effective prompt attention module to pay\nmore attention to haze residuals for better removal. By in-\ncorporating the prompt, prompt embedding, and prompt at-\ntention into an encoder-decoder network based on VQGAN,\nwe can achieve better perception quality. As the depths of\nclear images are not available at inference, and the dehazed\nimages with one-time feed-forward execution may still con-\ntain a portion of haze residuals, we propose a new continuous\nself-prompt inference that can iteratively correct the dehazing\nmodel towards better haze-free image generation. Extensive\nexperiments show that our SelfPromer performs favorably\nagainst the state-of-the-art approaches on both synthetic and\nreal-world datasets in terms of perception metrics including\nNIQE, PI, and PIQE. The source codes will be made avail-\nable at https://github.com/supersupercong/SelfPromer.\nIntroduction\nRecent years have witnessed advanced progress in image\ndehazing due to the development of deep dehazing models.\nMathematically, the haze process is usually modeled by an\natmospheric light scattering model (He, Sun, and Tang 2011)\nformulated as:\nI(x) = J(x)T(x) + (1− T(x))A, (1)\nwhere I and J denote a hazy and haze-free image, respec-\ntively, and A denotes the global atmospheric light,x denotes\n*supercong94@gmail.com.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nthe pixel index, and the transmission map T is usually mod-\neled as T(x) = e−βd(x) with the scene depth d( x), and the\nscattering coefficient β reflects the haze density.\nMost existing works develop various variations of deep\nConvolutional Neural Networks (CNNs) for image dehaz-\ning (Liu et al. 2019a; Dong et al. 2020; Dong and Pan 2020;\nChen et al. 2021; Jin et al. 2022, 2023; Wang et al. 2023).\nThey typically compute a sequence of features from the hazy\ninput images and directly reconstruct the clear ones based on\nthe features, which have achieved state-of-the-art results on\nbenchmarks (Li et al. 2019) in terms of PSNRs and SSIMs.\nHowever, as dehazing is ill-posed, very small errors in the\nestimated features may degrade the performance. Existing\nworks propose to use deep CNNs as image priors and then\nrestore the clear images iteratively. However, they cannot\neffectively correct the errors or remove the haze residuals\nin the dehazed images as these models are fixed in the it-\nerative process (Liu et al. 2019b). It is noteworthy that the\nhuman visual system generally possesses an intrinsic cor-\nrection mechanism that aids in ensuring optimal results for\na task. This phenomenon has been a key inspiration behind\nthe development of a novel dehazing approach incorporat-\ning a correction mechanism that guides deep models toward\nbetter haze-free results generation.\nSpecifically, if a dehazed result exists haze residuals, a\ncorrection mechanism can localize these regions and guide\nthe relevant task toward removing them. Notably, NLP-\nbased text prompt learning has shown promise in guiding the\nmodels by correcting the predictions (Liu et al. 2023). How-\never, text-based prompts may not be appropriate for tasks\nthat require solely visual inputs without accompanying text.\nRecent works (Herzig et al. 2022; Gan et al. 2023) attempted\nto address this issue by introducing text-free prompts into\nvision tasks. For instance, PromptonomyViT (Herzig et al.\n2022) evaluates the adaptation of multi-task prompts such\nas depth, normal, and segmentation to improve the per-\nformance of the video Transformers. Nevertheless, these\nprompts may not be suitable for image dehazing tasks, as\nthey could not capture the haze-related content.\nTo better guide the deep model for better image dehaz-\ning, this work develops an effective self-prompt dehazing\nTransformer. Specifically, it explores the depth consistency\nof hazy images and their corresponding clear ones as a\nprompt. In particular, our study is motivated by the substan-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5327\n(a) Ha]\\ (b) CleaU (c) Ha]\\ deSWh maS (d) CleaU deSWh maS (e) DiffeUeQce maS\nFigure 1: Haze residuals pose a significant challenge to accurately estimating the depth of clear images, creating inconsistencies\ncompared to hazy images. A difference map (e) is utilized to locate haze residuals on the estimated depth, while minimal haze\nresiduals will result in consistent estimates. By analyzing the difference map, we can identify the impact of haze residuals,\nleading to the development of improved dehazing models to mitigate this effect and enhance the quality of dehazed images.\nThe difference map (e) is derived by |hazy depth − clear depth| with equalization for better visualization.\ntial difference between the estimated depths of hazy images\nand their clear counterparts, i.e., the same scene captured\nin the same location should be consistent regarding depth.\nDepth is typically related to the transmission map in the at-\nmospheric light scattering model as shown in Eq. (1). Thus,\nif the dehazed images can be reconstructed accurately, their\nestimated depths should be close to those of their clear coun-\nterparts at large. However, haze residuals often degrade the\naccuracy of depth estimation, resulting in significant dif-\nferences between hazy and clear images, as illustrated in\nFig. 1(e). Yet, the difference map of estimated depths from\nimages with haze residuals and clear images often points to\nthe regions affected by haze residuals.\nBased on the above observation, we design a prompt to\nguide the deep models for perceiving and paying more atten-\ntion to haze residuals. Our prompt is built upon the estimated\nfeature-level depth differences, of which the inconsistent re-\ngions can reveal haze residual locations for deep model cor-\nrection. On top of the prompt, we introduce a prompt embed-\nding module that linearly combines input features with the\nprompt to better perceive haze residuals. Further, we pro-\npose a prompt attention module that employs self-attention\nguided by the prompt to pay more attention to haze residuals\nfor better haze removal. Our encoder-decoder architecture\ncombines these modules using VQGAN (Esser, Rombach,\nand Ommer 2021) to enhance the perception quality of the\nresults, as opposed to relying solely on PSNRs and SSIMs\nmetrics for evaluation.\nAs the depths of clear images suffer from unavailabil-\nity at inference and dehazed images obtained via one-time\nfeed-forward execution may have haze residuals, we intro-\nduce a continuous self-prompt inference to address these\nchallenges. Specifically, our proposed approach feeds the\nhazy input image to the model and sets the depth differ-\nence as zero to generate clearer images that serve as the\nclear counterpart. The clear image participates in construct-\ning the prompt to conduct prompt dehazing. The inference\noperation is continuously conducted as the depth differences\ncan keep correcting the deep dehazing models toward better\nclean image generation.\nThis paper makes the following contributions:\n• We make the first attempt to formulate the prompt by\nconsidering the cues of the estimated depth differences\nbetween the image with haze residuals and its clear coun-\nterpart in the image dehazing task.\n• We propose a prompt embedding module and a prompt\nattention module to respectively perceive and pay more\nattention to haze residuals for better removal.\n• We propose a new continuous self-prompt inference ap-\nproach to iteratively correct the deep models toward bet-\nter haze-free image generation.\nThe Proposed SelfPromer\nOur SelfPromer comprises two branches: the prompt branch\nand the self-prompt dehazing Transformer branch. The\nprompt branch generates a prompt by using the deep depth\ndifference and deep feature extracted from the hazy input.\nThe other branch exploits the generated prompt to guide the\ndeep model for image dehazing. We incorporate a prompt\nembedding module and prompt attention module to per-\nceive and pay more attention to the haze residuals for bet-\nter removal. The proposed modules are formulated into an\nencoder-decoder architecture based on VQGAN for better\nperception quality (Zhou et al. 2022; Chen et al. 2022).\nOverall Framework\nFig. 2 illustrates our method at the training stage. Given a\nhazy images I, we first utilize trainable encoder Enc(·) to\nextract features:\nFEnc = Enc(I). (2)\nThen, we compute the depth difference of the hazy image\nI and its corresponding clear image J in feature space:\nD1 = DE(I); D2 = DE(J), (3a)\nFD1 = Encfrozen\npre (D1); FD2 = Encfrozen\npre (D2), (3b)\nFDdiff = |FD1 − FD2 |, (3c)\nwhere DE(·) denotes the depth estimator 1 (Ranftl et al.\n2022). Encfrozen\npre (·) denotes the pre-trained VQGAN encoder\nwhich is frozen when training our dehazing models.\n1We chose DPT Next ViT L 384 to balance accuracy, speed,\nand model size: https://github.com/isl-org/MiDaS.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5328\nPURmSW =\nPURmSW \nEmbedding\n\u0011\u0011\u0011\nFUR]en MRdel\nPURmSW\n\u0011\u0011\u0011\nEncRdeU DecRdeU\nEncRdeU EncRdeU\nFigure 2: SelfPromer at training stage. Our method comprises two branches: prompt branch and self-prompt dehazing Trans-\nformer branch. The prompt branch generates a prompt by using the deep depth difference and deep feature extracted from the\nhazy input. The other branch exploits the generated prompt to guide the deep model for image dehazing. We incorporate a\nprompt embedding module and prompt attention module to perceive and pay more attention to the haze residuals for better\nremoval. The proposed modules are formulated into an encoder-decoder architecture based on VQGAN for better perception\nquality. MDFM is detailed in Eq. (11). The inference is illustrated in Fig. 3.\nNext, we exploit FDdiff to build the Prompt, and develop\na prompt embedding module and a prompt attention mod-\nule in Transformers, i.e., PTB to better generate haze-aware\nfeatures:\nPrompt = FDdiff · FEnc, # Prompt (4a)\nFProEmbed = Prompt + FEnc, # Prompt Embedding (4b)\nFPTB = PTB(Prompt, FProEmbed), # Prompt Transformer (4c)\nwhere FProEmbed means the features of prompt embedding.\nThe generated feature FPTB is further matched with the\nlearned haze-free Codebook at the pre-trained VQGAN\nstage by the Lookup method (Esser, Rombach, and Ommer\n2021; Zhou et al. 2022):\nFmat = Lookup(FPTB, Codebook). (5)\nFinally, we reconstruct the dehazing images ¯J from the\nmatched features Fmat by decoder of pre-trained VQGAN\nDecfrozen\npre (·) with residual learning (Chen et al. 2022) by mu-\ntual deformable fusion module MDFM:\n¯J = Decfrozen\npre (Fmat) + MDFM\n\u0010\nFs\nEnc, Fs,u\nPTB\n\u0011\n, (6)\nwhere Fs\nEnc means the encoder features at s scale, whileFs,u\nPTB\ndenotes the s× upsampling features of PTB. We conduct the\nresidual learning with MDFM in {1, 1/2, 1/4, 1/8} scales\nbetween the encoder and decoder like FeMaSR (Chen et al.\n2022). Here, F1/8\nEnc denotes the FEnc in Eq. (2).\nLoss Functions.We use pixel reconstruction lossLrec, code-\nbook loss Lcode, perception loss Lper, and adversarial loss\nLadv to measure the error between the dehazed images ¯J and\nthe corresponding ground truth J:\nL = Lrec + λcodeLcode + λperLper + λadvLadv, (7)\nSWHS 4: USdaWe fRU Qe[W iWeUaWiRQ\nSWHS 3: CRQdXcW Velf-SURPSW deha]iQg\nDHKa]LQJ TUaQVIRUPHUV \nPURPSW =\nSWHS 1: ObWaiQ cleaUeU iPageV Yia Whe WUaiQed PRdelV     \nb\\ VeWWiQg feaWXUe-leYel deSWh diffeUeQce aV ]eUR \nSWHS 2: GeQeUaWe SURPSW\nSHOI-PURPSW\nDHKa]LQJ TUaQVIRUPHUV \nPURPSW\nEQcRdHU\n EQcRdHU\nFigure 3: Continuous Self-Prompt Inference. ith prompt in-\nference contains four steps: Sequential execution from top\nto bottom. The magenta line describes the ‘self’ process that\nbuilds the prompt from the hazy image itself.\nwhere\nLrec = ||¯J − J||1 + λssim\n\u0000\n1 − SSIM(¯J, J)\n\u0001\n, (8a)\nLcode = ||¯zq − zq||2\n2, (8b)\nLper = ||Φ(¯J) − Φ(J)||2\n2, (8c)\nLadv = EJ[log D(J)] + E¯J[1 − log D(¯J)], (8d)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5329\nwhere SSIM(·) denotes the structural similarity (Wang et al.\n2004) for better structure generation. zq is the haze-free\ncodebook features by feeding haze-free images J to pre-\ntrained VQGAN while ¯zq is the reconstructed codebook\nfeatures. Φ(·) denotes the feature extractor of VGG19 (Si-\nmonyan and Zisserman 2015). D is the discriminator (Zhu\net al. 2017). λcode, λper, λadv, and λssim are weights.\nFor inference, we propose a new self-prompt inference\napproach as our training stage involves the depth of clear\nimages to participate in forming the prompt while clear im-\nages are not available at testing.\nSelf-Prompt Transformers\nThe self-prompt Transformer contains the prompt generated\nby the prompt branch, a prompt embedding module, and a\nprompt attention module which is contained in the prompt\nTransformer block. In the following, we introduce the defini-\ntion of the prompt, prompt embedding module, and prompt\nattention module, and prompt Transformer block in detail.\nPrompt (Definition). The prompt is based on the estimated\ndepth difference between the input image and its clear coun-\nterpart. It is defined in Eq. (4a) which can better contain haze\nresidual features as FDdiff with higher response value reveals\ninconsistent parts which potentially correspond to the haze\nresiduals in the input hazy image.\nPrompt Embedding. Existing Transformers (Zheng et al.\n2022) usually use the position embedding method (Fig. 4(a))\nto represent the positional correlation, which does not con-\ntain haze-related information so that it may not effectively\nperceive the haze residual information well. Moreover, im-\nage restoration requires processing different input sizes at\ninference while the position embedding is defined with fixed\nparameters at training (Zheng et al. 2022). Hence, position\nembedding may be not a good choice for image dehazing.\nTo overcome these problems, we propose prompt embed-\nding which is defined in Eq. (4b). By linearly adding the\nextracted features FEnc with Prompt, the embedded feature\nFProEmbed perceives the haze residual features asPrompt ex-\ntracts the haze residual features. Note that as FProEmbed has\nthe same size as FEnc, it does not require fixed sizes like po-\nsition embedding.\nPrompt Attention. Existing Transformers usually extract\nQuery Q, Key K, and Value V from input features to es-\ntimate scaled-dot-product attention shown in Fig. 4(c). Al-\nthough Transformers are effective for feature representation,\nthe standard operation may be not suitable for image dehaz-\ning. To ensure the Transformers pay more attention to haze\nresiduals for better removal, we propose prompt attention\nProAtt(·) by linearly adding the query with Prompt:\nQ = Q + Prompt, (9a)\nProAtt(Q, K, V) = Softmax\n\u0010 QKT\n√dhead\n\u0011\nV, (9b)\nwhere dhead means the number of the head. We setdhead as 8\nin this paper. Fig. 4(d) illustrates the proposed prompt atten-\ntion. Note that as Q in attention is to achieve the similarity\nrelation for expected inputs (Ding et al. 2021), our prompt\nattention by linearly adding the prompt Prompt with the\n(d) PUomSW aWWenWion (OXrs)\n(a) E[iVWing SoViWion embedding\n(c) E[iVWing UegXlaU aWWenWion\n(b) PUomSW embedding (OXrs)\nPrompW\nPrompW\nFigure 4: (a)-(b) Existing position embedding vs. Prompt\nembedding (Ours). Our prompt embedding can better per-\nceive haze information. (c)-(d) Existing regular attention vs.\nPrompt attention (Ours). Our prompt attention can pay more\nattention to haze residuals.\nQuery Q can pay more attention to haze residuals for bet-\nter removal.\nPrompt Transformer Block.According to the above atten-\ntion design, our prompt Transformer block (PTB) can be se-\nquentially computed as:\nQ, K, V = LN(Xl−1), (10a)\nˆXl = ProAtt(Q, K, V) + Xl−1, (10b)\nXl = MLP\n\u0010\nLN( ˆXl\u0001\u0011\n+ ˆXl, (10c)\nwhere Xl−1 and Xl mean the input and output of the lth\nprompt Transformer block. Specially, X0 is the FProEmbed.\nLN and MLP denote the layer normalization and multilayer\nperceptron. The PTB is shown in the right part of Fig. 2.\nIt is worth noting that our prompt embedding and prompt\nattention are flexible as we can manually set FDdiff = 0,\nthe network thus automatically degrade to the model with-\nout prompt, which will be exploited to form our continuous\nself-prompt inference.\nMutual Deformable Fusion Module\nAs VQGAN is less effective for preserving details (Gu et al.\n2022; Chen et al. 2022), motivated by the deformable mod-\nels (Dai et al. 2017; Zhu et al. 2019) that can better fuse\nfeatures, we propose a mutual deformable fusion module\n(MDFM) by fusing features mutually to adaptively learn\nmore suitable offsets for better feature representation:\noff1 = Conv\n\u0010\nC[Fs\nEnc, Fs,u\nPTB]\n\u0011\n; off2 = Conv\n\u0010\nC[Fs,u\nPTB, Fs\nEnc]\n\u0011\n, (11a)\nY1 = DMC(Fs\nEnc, off1); Y2 = DMC(Fs,u\nPTB, off2), (11b)\nFMDFM = Conv\n\u0010\nC[Y1, Y2]\n\u0011\n, (11c)\nwhere Conv(·), C[·], and DMC(·) respectively denote the1×\n1 convolution, concatenation, and deformable convolution.\noffk (k = 1, 2.) denotes the estimated offset.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5330\nFigure 5: Continuous self-prompt inference vs. GT guidance\n(Baseline) on the SOTS-indoor dataset. GT guidance means\nwe use the GT image to participate in forming the prompt at\ninference like the process of the training stage, which serves\nas the baseline.\nContinuous Self-Prompt Inference\nOur model requires the depth of clear images during train-\ning, but these images are unavailable at inference. Addition-\nally, dehazed images generated by a one-time feed-forward\nexecution may still contain some haze residuals. To address\nthese issues, we propose a continuous self-prompt infer-\nence approach that leverages prompt embedding and prompt\nattention through linear addition. By setting feature-level\ndepth difference FDdiff to zero, we can feed hazy images to\nour trained network and obtain clearer dehazed results which\nparticipate in building the prompt to conduct prompt dehaz-\ning. The iterative inference is conducted to correct the deep\nmodels to ensure the deep models toward better haze-free\nimage generation:\n¯J\nw/o prompt\ni = Nw/o prompt(¯J\nw/o prompt\ni−1 ), set FDdiff = 0, # Step 1 (12a)\nPrompt = FDdiff · FEnc; FEnc = Enc(¯Jw/o prompt\ni−1 ), # Step 2 (12b)\n¯J\nprompt\ni = Nprompt(¯J\nw/o prompt\ni−1 , Prompt), # Step 3 (12c)\n¯J\nw/o prompt\ni = ¯J\nprompt\ni , (i = 1, 2, ··· ), # Step 4 (12d)\nwhere Nw/o prompt denotes our trained network with-\nout prompt by setting FDdiff as zero, while Nprompt\nmeans our trained network with prompt. FDdiff =\n|Encfrozen\npre (DE(¯Jw/o prompt\ni−1 )) − Encfrozen\npre (DE(¯Jw/o prompt\ni ))|.\n¯Jw/o prompt\n0 denotes the original hazy images, while ¯Jw/o prompt\ni−1\nis regarded as the image with haze residuals and¯Jw/o prompt\ni in\nEq. (12a) is regarded as the clear counterpart of ¯Jw/o prompt\ni .\n¯Jprompt\ni means the ith prompt dehazing results.\nAccording to Eq. (12), the inference is a continuous self-\nprompt scheme, i.e., we get the clear images from the hazy\nimage itself by feeding it to Nw/o prompt to participate in pro-\nducing the prompt and the inference is continuously con-\nducted. Fig. 3 better illustrates the inference process.\nFig. 5 shows our continuous self-prompt at 2nd and 3rd\nprompts outperforms the baseline which uses ground-truth\n(GT) to participate in forming the prompt like the process of\nthe training stage.\nExperiments\nIn this section, we evaluate the effectiveness of our method\nagainst state-of-the-art ones (SOTAs) on commonly used\nbenchmarks and illustrate the effectiveness of the key com-\nponents in the proposed method.\nImplementation Details. We use 10 PTBs, i.e., l = 10,\nin our model. The details about the VQGAN are pre-\nsented in the supplementary materials. We crop an image\npatch of 256 × 256 pixels. The batch size is 10. We use\nADAM (Kingma and Ba 2015) with default parameters as\nthe optimizer. The initial learning rate is 0.0001 and is di-\nvided by 2 at 160K, 320K, and 400K iterations. The model\ntraining terminates after 500K iterations. The weight param-\neters λcode, λper, λadv, and λssim are empirically set as 1, 1,\n0.1, and 0.5. Our implementation is based on the PyTorch.\nSynthetic Datasets.Following the protocol of (Yang et al.\n2022), we use the RESIDE ITS (Li et al. 2019) as our train-\ning dataset and the SOTS-indoor (Li et al. 2019) and SOTS-\noutdoor (Li et al. 2019) as the testing datasets.\nReal-world Datasets. Li et al. (2019) collect large-scale\nreal-world hazy images, called UnannotatedHazyImages.\nWe use these images as a real-world hazy dataset.\nEvaluation Metrics. As we mainly aim to recover im-\nages with better perception quality, we use widely-used\nNIQE (Mittal, Soundararajan, and Bovik 2013), PI (Ma\net al. 2017), and PIQE (N. et al. 2015) to measure\nrestoration quality. Since the distortion metrics PSNR\nand SSIM (Wang et al. 2004) cannot model the percep-\ntion quality well, we use them for reference only. No-\ntice that all metrics are re-computed for fairness. We use\nthe grayscale image to compute the PSNR and SSIM.\nWe compute NIQE and PI by the provided metrics at\nhttps://pypi.org/project/pyiqa/. The PIQE is computed via\nhttps://github.com/buyizhiyou/NRVQA.\nMain Results\nResults on Synthetic Datasets.Tab. 1 and Tab. 2 respec-\ntively report the comparison results with SOTAs on the\nSOTS-indoor and SOTS-outdoor datasets (Li et al. 2019).\nOur method achieves better performance in terms of NIQE,\nPI, and PIQE, indicating the generated results by our method\npossess higher perception quality. Fig. 6 and Fig. 7 show that\nour method restores much clearer images while the evalu-\nated approaches generate the results with haze residual or\nartifacts. As we train the network with a one-time feed-\nforward process, PSNRs and SSIMs are naturally decreased\n(SelfPromer1 vs. SelfPromer3 in Tabs. 1 and 2) when in-\nference is conducted iteratively. We argue distortion metrics\nincluding PSNRs and SSIMs are not good measures for im-\nage dehazing as Figs. 6 and 7 have shown methods with\nhigher PSNR and SSIMs cannot recover perceptual results,\ne.g., Dehamer (Guo et al. 2022) and D4 (Yang et al. 2022),\nwhile our method with better perception metrics can gener-\nate more realistic results.\nResults on Real-World Datasets.Tab. 3 summarises the\ncomparison results on the real-world datasets (Li et al.\n2019), where our method performs better than the evaluated\nmethods. Fig. 8 illustrates that our method generates an im-\nage with vivid color and finer details.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5331\nMethods GridNet PFDN UHD PSD Uformer Restormer D4 Dehamer SelfPromer1 SelfPromer3\nPerception\nNIQE ↓ 4.239 4.412 4.743 4.828 4.378 4.321 4.326 4.529 4.252 4.054\nPI ↓ 3.889 4.143 4.962 4.567 3.967 3.936 3.866 4.035 3.926 3.857\nPIQE ↓ 28.924 32.157 39.204 35.174 29.806 29.384 30.480 32.446 30.596 27.927\nDistortion PSNR ↑ 32.306 33.243 16.920 13.934 33.947 36.979 19.142 36.600 35.960 34.467\nSSIM ↑ 0.9840 0.9827 0.7831 0.7160 0.9846 0.9900 0.8520 0.9865 0.9877 0.9852\nTable 1: Comparisons on SOTS-indoor dataset. Our method achieves better performance in terms of NIQE, PI, and PIQE. The\nbest results are marked in bold. ↓ (↑) denotes lower (higher) is better. SelfPromeri means the ith prompt results.\n(a) Input (b) GT (c) UHD (d) PSD (e) Uformer (f) Restormer (g) D4 (h) Dehamer (i) SelfPromer\nFigure 6: Visual comparisons on SOTS-indoor. SelfPromer generates clearer results, even than the GT image.\nMethods GridNet PFDN UHD PSD Uformer Restormer D4 Dehamer SelfPromer1 SelfPromer3\nPerception\nNIQE ↓ 2.844 2.843 3.756 2.884 2.903 2.956 2.917 3.164 2.646 2.685\nPI ↓ 2.070 2.326 3.381 2.392 2.241 2.254 2.137 2.251 2.003 2.027\nPIQE ↓ 6.547 6.732 10.891 8.937 6.748 6.904 7.567 6.458 6.577 6.151\nDistortion PSNR ↑ 16.327 16.872 11.758 15.514 19.618 18.337 26.138 21.389 18.471 16.954\nSSIM ↑ 0.8016 0.8532 0.6074 0.7488 0.8798 0.8634 0.9540 0.8926 0.8771 0.8288\nTable 2: Comparisons on SOTS-outdoor. SelfPromer achieves better perception metrics including NIQE, PI, and PIQE, sug-\ngesting that the proposed method has a better generalization ability to unseen images for more natural results generation.\n(a) Input (b) GT (c) UHD (d) PSD (e) Uformer (f) Restormer (g) D4 (h) Dehamer (i) SelfPromer\nFigure 7: Visual comparisons on SOTS-outdoor. SelfPromer is able to generate more natural results. Note that our method\nproduces more consistent colors in the sky region, while the others generate inconsistent colors and the D4 (Yang et al. 2022)\nleaves extensive haze.\nAnalysis and Discussion\nWe further analyze the effectiveness of the proposed method\nand understand how it works on image dehazing. The results\nin this section are obtained from the SOTS-indoor dataset if\nnot further mentioned. Our results are from the 1st prompt\ninference for fair comparisons, i.e., i = 1 in Eq. (12) if not\nfurther specifically mentioned.\nEffectiveness of prompt.Initially, we assess the effect of\nthe prompt on image dehazing. Notably, various prospec-\ntive prompt candidates exist, such as image-level depth dif-\nference as the input of the VQGAN encoder or concate-\nnation between deep features extracted from the input and\ndepth features as the input of the Transformers. Our pro-\nposed prompt is compared with these candidates, as illus-\ntrated in Tab. 4(b) and 4(c), demonstrating that none of these\ncandidates outperforms our proposed prompt.\nNote that our method without prompt leads to a similar\nmodel with CodeFormer (Zhou et al. 2022) which directly\ninserts regular Transformers into VQGAN. Tab. 4 shows\nprompt help yield superior perception quality than the model\nwithout prompt (Tab. 4(a)). The efficacy of our model with\nthe prompt is further affirmed by Fig. 9, indicating that the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5332\nMethods GridNet PFDN UHD PSD Uformer Restormer D4 Dehamer SelfPromer1 SelfPromer3\nPerception\nNIQE ↓ 4.341 4.917 4.515 4.199 4.214 4.213 4.257 4.248 4.161 4.062\nPI ↓ 3.685 3.736 3.858 3.521 3.429 3.436 3.414 3.495 3.477 3.391\nPIQE ↓ 14.699 17.874 23.168 15.851 16.787 17.176 18.678 15.909 16.252 14.026\nTable 3: Comparisons on real-world dataset. SelfPromer achieves better performance, indicating that our method is more robust\nto real-world scenarios for realistic results generation.\n(a) Input (b) UHD (c) PSD (d) Uformer (e) Restormer (f) D4 (g) Dehamer (h) SelfPromer\nFigure 8: Visual comparisons on real-world dataset. Our SelfPromer is able to generate much clearer results.\nExperiments NIQE ↓ PI ↓ PIQE ↓\n(a) Without the prompt 4.258 3.937 31.904\n(b) Image-level depth difference 4.901 4.343 32.141\n(c) Concat of image and depth features 4.362 4.077 34.107\n(d) Feature-level depth difference (Ours) 4.252 3.926 30.596\nTable 4: Effect of the proposed prompt. Feature-level depth\ndifference is a better prompt formalization. while the con-\ncatenation in image-level and feature-level between the in-\nput image and its depth is not as well as ours.\n(a) Input (b) w/o prompt (c) w/ prompt (Ours)\nFigure 9: Visual comparisons of the model without prompt\n(b) and with prompt (c) on real-world scenarios.\nExperiments NIQE ↓ PI ↓ PIQE ↓\n(a) Without embedding 4.410 4.113 32.193\n(b) Position embedding 4.267 3.992 31.877\n(c) Regular attention 4.300 4.102 31.486\n(d) Proposed (Ours) 4.252 3.926 30.596\nTable 5: Effectiveness of prompt embedding and attention.\nmodel with the prompt generates better results, while the\nmodel without prompt fails to remove haze effectively.\nEffectiveness of prompt embedding and prompt atten-\nFigure 10: Effectiveness of continuous self-prompt (Ours)\nvs. recurrent dehazing (Recur.). ‘Ours w/o prompt’ means\nthe results of Eq. (12a).\ntion. One might ponder the relative efficacy of our prompt\nembedding and attention in contrast to the prevalent tech-\nnique of position embedding and regular attention. In this re-\ngard, we assess the effect of these embedding approaches in\nTab. 5. The table reveals that our prompt embedding proves\nmore advantageous over the position embedding since the\nformer is associated with haze residual information. Tab. 5\nindicates that our prompt attention yields better results as\ncompared to commonly used attention methods. These find-\nings signify that incorporating prompts in enhancing Query\nestimation accounts for the haze information, thereby culmi-\nnating in more effective image dehazing results.\nEffect of the number of steps in continuous self-prompt.\nThe inference stage involves several steps to generate the\nprompt for better image dehazing. We thus examine the ef-\nfect of the number of steps in the continuous self-prompt.\nFig. 10 reveals that the optimal performance is achieved with\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5333\n(a) Input (b) 1st prompt (c) 2nd prompt (d) 3rd prompt\nFigure 11: Visual improvement of continuous self-prompt\ninference on a real-world example.\na number of steps equal to 3 in the continuous self-prompts\n(i.e., i = 3 in Eq. (12)), in terms of NIQE. Notably, ad-\nditional prompts do not improve the dehazing performance\nany further. One real-world example in Fig. 11 demonstrates\nthat our continuous self-prompt method can gradually en-\nhance dehazing quality.\nContinuous self-prompt vs. recurrent dehazing.We use\nthe continuous self-prompt approach to restore clear images\nprogressively at inference. To determine whether a recurrent\nmethod that is training our model without prompt achieves\nsimilar or better results, we compare our proposed method\nwith it in Fig. 10, demonstrating that the recurrent method is\nnot as good as our continuous self-prompt.\nContinuous self-prompt vs. GT guidance.Fig. 5 compares\nthe NIQE performance of ground truth (GT) guidance with\nthat of the continuous self-prompt algorithm. Results show\nthat while GT guidance performs better than the 1st prompt,\nit falls short of the effectiveness of the 2nd and 3rd prompts.\nThis is likely due to GT guidance’s limited ability to handle\nhaze residuals which may still exist in the dehazed images,\nwhich are addressed by the self-prompt’s ability to exploit\nresidual haze information to progressively improve dehaz-\ning quality over time. Moreover, as GT is not available in\nthe real world, these findings may further support the use of\nself-prompt as a more practical alternative.\nDepth-consistency.Fig. 12 shows heat maps of depth differ-\nences obtained by the continuous self-prompt inference with\ndifferent prompt steps. The results demonstrate both image-\nlevel and feature-level depth differences decrease as the\nnumber of prompt steps increases, indicating the depths ob-\ntained with the prompt, i.e., Eq. (12c), become increasingly\nconsistent with those obtained without it, i.e., Eq. (12a).\nApplications to Low-Light Image Enhancement\nFurthermore, we extend the application of our method, Self-\nPromer, to the domain of low-light image enhancement. To\nassess its performance, we conduct a comparative analy-\nsis with current state-of-the-art methods, SNR (Xu et al.\n2022) and LLFlow (Wang et al. 2022). All these methods\nare trained using the widely adopted LOL dataset (Wei et al.\n2018). Fig. 13 shows several visual examples sourced from\nreal-world benchmarks (Lee, Lee, and Kim 2013; Guo, Li,\nand Ling 2017). These illustrative results effectively under-\nscore our approach is capable of generating results that are\nnotably more true-to-life, with colors that appear more nat-\nRefeUeQce\nFigure 12: Illustration of continuous depth-consistency. Ref-\nerence means the depth difference between the input hazy\nimage and GT. The input haze image is Fig. 1(a).\nural. On the contrary, existing state-of-the-art methods tend\nto yield results that suffer from under-/over-exposure issues.\n(a) Input (b) SNR (c) LLFlow (d) SelfPromer\nFigure 13: Applications to low-light image enhancement on\nchallenging real-world examples.\nConclusion\nWe have proposed a simple yet effective self-prompt Trans-\nformer for image dehazing by exploring the prompt built on\nthe estimated depth difference between the image with haze\nresiduals and its clear counterpart. We have shown that the\nproposed prompt can guide the deep model for better image\ndehazing. To generate better dehazing images at the infer-\nence stage, we have proposed continuous self-prompt infer-\nence, where the proposed prompt strategy can remove haze\nprogressively. We have shown that our method generates re-\nsults with better perception quality in terms of NIQE, PI, and\nPIQE.\nReferences\nChen, C.; Shi, X.; Qin, Y .; Li, X.; Han, X.; Yang, T.; and\nGuo, S. 2022. Real-World Blind Super-Resolution via Fea-\nture Matching with Implicit High-Resolution Priors. InACM\nMM, 1329–1338.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5334\nChen, Z.; Wang, Y .; Yang, Y .; and Liu, D. 2021. PSD: Princi-\npled Synthetic-to-Real Dehazing Guided by Physical Priors.\nIn CVPR, 7180–7189.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and Wei,\nY . 2017. Deformable Convolutional Networks. In ICCV,\n764–773.\nDing, H.; Liu, C.; Wang, S.; and Jiang, X. 2021. Vision-\nLanguage Transformer and Query Generation for Referring\nSegmentation. In ICCV, 16301–16310.\nDong, H.; Pan, J.; Xiang, L.; Hu, Z.; Zhang, X.; Wang, F.;\nand Yang, M. 2020. Multi-Scale Boosted Dehazing Network\nWith Dense Feature Fusion. In CVPR, 2154–2164.\nDong, J.; and Pan, J. 2020. Physics-Based Feature Dehazing\nNetworks. In ECCV, 188–204.\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming Trans-\nformers for High-Resolution Image Synthesis. In CVPR,\n12873–12883.\nGan, Y .; Ma, X.; Lou, Y .; Bai, Y .; Zhang, R.; Shi, N.; and\nLuo, L. 2023. Decorate the Newcomers: Visual Domain\nPrompt for Continual Test Time Adaptation. In AAAI.\nGu, Y .; Wang, X.; Xie, L.; Dong, C.; Li, G.; Shan, Y .;\nand Cheng, M. 2022. VQFR: Blind Face Restoration\nwith Vector-Quantized Dictionary and Parallel Decoder. In\nECCV, 126–143.\nGuo, C.-L.; Yan, Q.; Anwar, S.; Cong, R.; Ren, W.; and Li,\nC. 2022. Image Dehazing Transformer With Transmission-\nAware 3D Position Embedding. In CVPR, 5812–5820.\nGuo, X.; Li, Y .; and Ling, H. 2017. LIME: Low-Light Image\nEnhancement via Illumination Map Estimation. IEEE TIP,\n26(2): 982–993.\nHe, K.; Sun, J.; and Tang, X. 2011. Single Image Haze\nRemoval Using Dark Channel Prior. IEEE TPAMI, 33(12):\n2341–2353.\nHerzig, R.; Abramovich, O.; Ben-Avraham, E.; Arbelle,\nA.; Karlinsky, L.; Shamir, A.; Darrell, T.; and Globerson,\nA. 2022. PromptonomyViT: Multi-Task Prompt Learning\nImproves Video Transformers using Synthetic Scene Data.\nCoRR, abs/2212.04821.\nJin, Y .; Lin, B.; Yan, W.; Yuan, Y .; Ye, W.; and Tan, R. T.\n2023. Enhancing visibility in nighttime haze images using\nguided apsf and gradient adaptive convolution. InACM MM,\n2446–2457.\nJin, Y .; Yan, W.; Yang, W.; and Tan, R. T. 2022. Structure\nRepresentation Network and Uncertainty Feedback Learn-\ning for Dense Non-Uniform Fog Removal. In ACCV, 2041–\n2058.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In ICLR.\nLee, C.; Lee, C.; and Kim, C. 2013. Contrast Enhance-\nment Based on Layered Difference Representation of 2D\nHistograms. IEEE TIP, 22(12): 5372–5384.\nLi, B.; Ren, W.; Fu, D.; Tao, D.; Feng, D.; Zeng, W.; and\nWang, Z. 2019. Benchmarking Single-Image Dehazing and\nBeyond. IEEE TIP, 28(1): 492–505.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023. Pre-train, prompt, and predict: A systematic survey\nof prompting methods in natural language processing. ACM\nComputing Surveys, 55(9): 1–35.\nLiu, X.; Ma, Y .; Shi, Z.; and Chen, J. 2019a. GridDe-\nhazeNet: Attention-Based Multi-Scale Network for Image\nDehazing. In ICCV, 7313–7322.\nLiu, Y .; Pan, J.; Ren, J.; and Su, Z. 2019b. Learning Deep\nPriors for Image Dehazing. In ICCV, 2492–2500.\nMa, C.; Yang, C.; Yang, X.; and Yang, M. 2017. Learn-\ning a no-reference quality metric for single-image super-\nresolution. CVIU, 158: 1–16.\nMittal, A.; Soundararajan, R.; and Bovik, A. C. 2013. Mak-\ning a “Completely Blind” Image Quality Analyzer. IEEE\nSPL, 20(3): 209–212.\nN., V .; D., P.; Bh., M. C.; Channappayya, S. S.; and\nMedasani, S. S. 2015. Blind image quality evaluation us-\ning perception based features. In NCC, 1–6.\nRanftl, R.; Lasinger, K.; Hafner, D.; Schindler, K.; and\nKoltun, V . 2022. Towards Robust Monocular Depth Estima-\ntion: Mixing Datasets for Zero-shot Cross-dataset Transfer.\nIEEE TPAMI, 44(3): 1623–1637.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nICLR.\nWang, C.; Pan, J.; Wang, W.; Dong, J.; Wang, M.; Ju, Y .;\nand Chen, J. 2023. PromptRestorer: A Prompting Im-\nage Restoration Method with Degradation Perception. In\nNeurIPS.\nWang, Y .; Wan, R.; Yang, W.; Li, H.; Chau, L.; and Kot,\nA. C. 2022. Low-Light Image Enhancement with Normal-\nizing Flow. In AAAI, 2604–2612.\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE TIP, 13(4): 600–612.\nWei, C.; Wang, W.; Yang, W.; and Liu, J. 2018. Deep\nRetinex Decomposition for Low-Light Enhancement. In\nBMVC, 155.\nXu, X.; Wang, R.; Fu, C.-W.; and Jia, J. 2022. SNR-Aware\nLow-Light Image Enhancement. In CVPR, 17714–17724.\nYang, Y .; Wang, C.; Liu, R.; Zhang, L.; Guo, X.; and Tao,\nD. 2022. Self-Augmented Unpaired Image Dehazing via\nDensity and Depth Decomposition. In CVPR, 2037–2046.\nZheng, Z.; Yue, X.; Wang, K.; and You, Y . 2022. Prompt\nVision Transformer for Domain Generalization. CoRR,\nabs/2208.08914.\nZhou, S.; Chan, K. C. K.; Li, C.; and Loy, C. C. 2022.\nTowards Robust Blind Face Restoration with Codebook\nLookup Transformer. In NeurIPS.\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\npaired Image-To-Image Translation Using Cycle-Consistent\nAdversarial Networks. In ICCV, 2242–2251.\nZhu, X.; Hu, H.; Lin, S.; and Dai, J. 2019. Deformable\nConvNets V2: More Deformable, Better Results. In CVPR,\n9308–9316.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5335",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5601600408554077
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4535577595233917
    },
    {
      "name": "Computer science",
      "score": 0.39996659755706787
    },
    {
      "name": "Geology",
      "score": 0.328998327255249
    },
    {
      "name": "Environmental science",
      "score": 0.3248429298400879
    },
    {
      "name": "Artificial intelligence",
      "score": 0.14933428168296814
    },
    {
      "name": "Electrical engineering",
      "score": 0.13843026757240295
    },
    {
      "name": "Engineering",
      "score": 0.10772019624710083
    },
    {
      "name": "Voltage",
      "score": 0.0468994677066803
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I27357992",
      "name": "Dalian University of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 29
}