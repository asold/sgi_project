{
  "title": "Transformer-XL Based Music Generation with Multiple Sequences of Time-valued Notes",
  "url": "https://openalex.org/W3043786446",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100548260",
      "name": "Xianchao Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100780920",
      "name": "Chengyuan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079286060",
      "name": "Qinying Lei",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891815651",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W3104194627",
    "https://openalex.org/W2963575853",
    "https://openalex.org/W2809621972",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2991108091"
  ],
  "abstract": "Current state-of-the-art AI based classical music creation algorithms such as Music Transformer are trained by employing single sequence of notes with time-shifts. The major drawback of absolute time interval expression is the difficulty of similarity computing of notes that share the same note value yet different tempos, in one or among MIDI files. In addition, the usage of single sequence restricts the model to separately and effectively learn music information such as harmony and rhythm. In this paper, we propose a framework with two novel methods to respectively track these two shortages, one is the construction of time-valued note sequences that liberate note values from tempos and the other is the separated usage of four sequences, namely, former note on to current note on, note on to note off, pitch, and velocity, for jointly training of four Transformer-XL networks. Through training on a 23-hour piano MIDI dataset, our framework generates significantly better and hour-level longer music than three state-of-the-art baselines, namely Music Transformer, DeepJ, and single sequence-based Transformer-XL, evaluated automatically and manually.",
  "full_text": "arXiv:2007.07244v1  [cs.SD]  11 Jul 2020\nT ransformer-XL Based Music Generation with\nMultiple Sequences of Time-valued Notes\nXianchao Wu1, Chengyuan Wang2, Qinying Lei3∗\n1A.I.&Research, Microsoft Development Co., Ltd.\n2 University of Science and T echnology of China\n3 Communication University of China\n{wuxianchao, forxms2019}@gmail.com, lqy3110@hotmail.com\nAbstract\nCurrent state-of-the-art AI based classical mu-\nsic creation algorithms such as Music Trans-\nformer are trained by employing single se-\nquence of notes with time-shifts. The ma-\njor drawback of absolute time interval expres-\nsion is the difﬁculty of similarity computing\nof notes that share the same note value yet\ndifferent tempos, in one or among MIDI ﬁles.\nIn addition, the usage of single sequence re-\nstricts the model to separately and effectively\nlearn music information such as harmony and\nrhythm. In this paper, we propose a framework\nwith two novel methods to respectively track\nthese two shortages, one is the construction of\ntime-valued note sequences that liberate note\nvalues from tempos and the other is the sepa-\nrated usage of four sequences, namely, former\nnote on to current note on, note on to note off,\npitch, and velocity, for jointly training of four\nTransformer-XL networks. Through training\non a 23-hour piano MIDI dataset, our frame-\nwork generates signiﬁcantly better and hour-\nlevel longer music than three state-of-the-art\nbaselines, namely Music Transformer, DeepJ,\nand single sequence-based Transformer-XL,\nevaluated automatically and manually.\n1 Introduction & Related W ork\nMusic, one of the most important inventions\nof human being, has an extremely hugh mar-\nket with billion-level listeners. Music is con-\nnected with emotion expression and language-\nindependent, making it being one of the most intu-\nitive and simplest ways of human communication.\nCreating music from scratch is a challenging\nwork even for professional composers. Remark-\nably , popular and classical music generation using\n∗\nW ork done when Chengyuan and Qinying were intern-\nship students in Microsoft. These three authors contribute d\nequally to this paper: Chengyuan was the major developer\nand Qinying was the major evaluator and advisor, supervised\nby Xianchao.\ndeep learning algorithms and large-scale MIDI\nﬁles have achieved promising results during recent\nyears. Main stream approaches include modelling\nmusic note sequences by borrowing ideas from\nlanguage modelling in natural language process-\ning ﬁeld.\nA melody and arrangement generation frame-\nwork for pop music was proposed in (\nZhu et al. ,\n2018). First, a chord-based rhythm and melody\ncross-generation model, employing recurrent neu-\nral networks (RNNs) such as gated recurrent units\n(GRUs) (\nCho et al. , 2014), was used to generate\nmelody with chord progressions. Then, a multi-\ninstrument co-arrangement model using updated\nGRUs was designed for multi-task learning for\nmulti-track music arrangement.\nDeepBach (\nHadjeres and Pachet , 2017) was\na graphical model aiming at modelling poly-\nphonic music and speciﬁcally hymn-like pieces\nthrough employing RNNs. Later, DeepJ was pro-\nposed in (\nMao et al. , 2018) for style-speciﬁc mu-\nsic generating. Biaxial long short term mem-\nory (LSTM) (\nHochreiter and Schmidhuber , 1997;\nJohnson, 2017) was trained using piano roll note\nrepresentations, for three major classical periods\n(baroque, classical, and romantic). DeepJ is capa-\nble of composing music conditioned on a speciﬁc\nmixture of composer styles. W e take this model as\none of our baselines and compare the differences\nof data representation and music dynamic sequen-\ntial learning.\nCompared with RNNs such as LSTMs or GRUs,\nTransformer (\nV aswani et al. , 2017), a sequence\nmodel based on (multi-head) self-attention mech-\nanism, is more parallelizable for both training\nand inferring, and more interpretable. Trans-\nformer has achieved compelling results in tasks\nthat require maintaining long-range coherence,\nsuch as neural machine translation (\nV aswani et al. ,\n2017), pre-training language models ( Devlin et al. ,\n2019), text-to-speech synthesis ( Li et al. , 2019),\nand speech recognition ( Mohamed et al. , 2019).\nEmploying Transformer with relative atten-\ntion mechanism ( Shaw et al. , 2018), Music Trans-\nformer ( Huang et al. , 2018) was proposed for gen-\nerating piano music with long-term structure. As\ndepicted in Figure\n1, performance events with\nabsolute time intervals were employed for note\nsequence representation. In musical composi-\ntion and performance, relative timing directly\nexpressed by note-values is critically important.\nThat is, we prefer the model learns from music\nscores written by composers, instead of perfor-\nmance events played by real-world players.\nNoteTuple (\nHawthorne et al. , 2018) groups a\nnote’s attributes as one event. However, the\nTIME SHIFT used in performance events still\nbrings hard-coding to the target of relative tim-\ning. In this paper, we propose a further relative\nnote representation method that projects tempo\ninformation into note representation, resulting a\n4-tuple time-valued note representation which in-\ncludes former note on to current note on, current\nnote on to note off, pitch and velocity .\nIn particular, we are interested in generating\nextremely long music of hour level. W e select\nTransformer-XL (\nDai et al. , 2019) which models\nextremely long (language) sequences by segment-\nlevel recurrences and relative position encod-\ning. Different from (\nDonahue et al. , 2019)’s multi-\ninstrumental music generation with event-based\nrepresentation using transformer-XL, our proposal\nis to duplicate one Transformer-XL into four in-\ndependently and jointly trained modules, taking\nour 4-tuple time-valued note sequences as inputs.\nThanks to these time-valued data representation\nand four Transformer-XL networks trained in a\njoint way , our framework generates signiﬁcantly\nbetter and hour-level longer music than three state-\nof-the-art baselines including Music Transformer,\nDeepJ, and single sequence-based Transformer-\nXL, evaluated automatically and manually . W e\nextremely generated a continuous 36-hour music,\nwith a stable high-level of note-density\n1 .\n2 Data Representation\nW e present a note by a 4-tuple ⟨on2on, on2off ,\npitch, velocity⟩. Here, on2on and on2off re-\n1 Our full-version generated MIDI examples can be found\nat https://pan.baidu.com/s/1i8pE7jEuWuWZy1DhW6XeJg\nwith code ckhv; and https://drive.google.com/ﬁle/d/1VRo KY -\nINJ x1bte8SdTzvLao05uaZlIq/view?usp=sharing.\n\u0000 ✁ ✂ ✄ ☎\n✆ ✁ ✝ ✞ ✟ ✠ ✡ ☛ ☞ ✞ ✌\n✡\n✍\n✁\n✎\n✆ ✁ ✝ ✞ ✡\n✍\n✁\n✎ ✏ ✎ ✑\n✒ ✓\n✓ ✔ ✕ ✖\n✓ ✔ ✖ ✓ ✔ ✗ ✖ ✘\n✘ ✔ ✕ ✖\n✘ ✔ ✖\n✓\n✓ ✔ ✖\n✘ ✘ ✔ ✖\n✕\n✙\n✚\n✒\n✚\n✒ ✖\n✒ ✗\n✓\n✛ ✜ ✢ ✣ ✤ ✥ ✦ ✣ ✧ ✥\n✛ ✢ ✛ ✤ ★ ✦ ✣ ✧ ✥\n✩ ✪\n✛\n✫\n✧ ✥\n✫\n✩ ✪\n✛\n✫\n✧ ✥\n✫\n✂ ✞ ✝ \u0000 ✬ ✭\n✘ ✕ ✓\n✂ ✞ ✝ \u0000 ✬ ✭\n✒ ✓\n✩ ✪\n✛\n✫\n✧ ✥\n✫\n✮ ✯ ✰ ✱ ✲ ✯ ✳ ✴ ✵ ✶ ✰ ✷ ✸ ✹ ✺ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ✽ ✸ ✾ ✺ ✻ ✼ ✰ ✶ ✿ ✯ ✱ ✮ ❀ ✶ ❁ ✰ ✸ ❂ ✺ ✺ ✺ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ✽ ✸ ✾ ❃ ✻ ✼\n✰ ✶ ✿ ✯ ✱ ✮ ❀ ✶ ❁ ✰ ✸ ❄ ✺ ✺ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ✽ ✸ ✾ ❅ ✻ ✼ ✰ ✶ ✿ ✯ ✱ ✮ ❀ ✶ ❁ ✰ ✸ ❄ ✺ ✺ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ❁ ❁ ✸ ✾ ✺ ✻ ✼\n✽ ✴ ✰ ✯ ✱ ✴ ❁ ❁ ✸ ✾ ❃ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ❁ ❁ ✸ ✾ ❅ ✻ ✼ ✰ ✶ ✿ ✯ ✱ ✮ ❀ ✶ ❁ ✰ ✸ ❂ ✺ ✺ ✺ ✻ ✼ ✮ ✯ ✰ ✱ ✲ ✯ ✳ ✴ ✵ ✶ ✰ ✷ ✸ ❂ ✺ ✺ ✻ ✼\n✽ ✴ ✰ ✯ ✱ ✴ ✽ ✸ ✾ ❄ ✻ ✼ ✰ ✶ ✿ ✯ ✱ ✮ ❀ ✶ ❁ ✰ ✸ ❂ ✺ ✺ ✺ ✻ ✼ ✽ ✴ ✰ ✯ ✱ ✴ ❁ ❁ ✸ ✾ ❄ ✻\n❆ ✺ ✼ ❂ ✼ ✾ ✺ ✼ ✹ ✺ ❇ ✼ ❆ ✺ ❈ ❄ ✼ ✺ ❈ ❄ ✼ ✾ ❃ ✼ ✹ ✺ ❇ ✼ ❆ ✺ ❈ ❉ ❄ ✼ ✺ ❈ ❉ ❄ ✼ ✾ ❅ ✼ ✹ ✺ ❇ ✼ ❆ ✺ ❈ ❄ ✼ ✺ ❈ ❉ ❄ ✼ ✾ ❄ ✼ ❂ ✺ ✺ ❇\n❊❃ ✧\n✪\n❋ ✤ ✥\n✫\n✥ ❋\n✫\n✥ ● ✥ ✦ ✧ ✛ ✧ ❍ ✣ ✦\n❆ ✺ ✼\n■\n✹ ❃ ✼ ✾ ✺ ✼ ✹ ✺ ❇ ✼ ❆ ❂\n❏\n❉ ✼ ❂\n❏\n❉ ✼ ✾ ❃ ✼ ✹ ✺ ❇ ✼ ❆\n❏\n✾ ✼\n❏\n✾ ✼ ✾ ❅ ✼ ✹ ✺ ❇ ✼ ❆ ❂\n❏\n❉ ✼\n❏\n✾ ✼ ✾ ❄ ✼ ❂ ✺ ✺ ❇\n❊ ❃ ✧\n✪\n❋ ✤ ✥ ✜ ❍ ✧ ✢ ❍ ✦ ✧ ✥\n❑\n✥\n✫ ✫\n✥ ❋\n✫\n✥ ● ✥ ✦ ✧ ✛ ✧ ❍ ✣ ✦\n▲\n▼\n✵\nFigure 1: Illustration of our 4-tuple note representation.\nspectively stand for the note value of from former\nnote’s on (start) to current note’s on and from cur-\nrent note’s on to its off (end). In addition, pitch\nand velocity are read from MIDI ﬁles directly us-\ning existing MIDI processing packages such as\npretty\nmidi2 (Raffel and Ellis , 2014).\nFigure 1 illustrates the differences of between\nour 4-tuple note representation (B and C) and\nMusic Transformer’s (\nHuang et al. , 2018) per-\nformance event representation (A). There is a\nvelocity-80 C Major chord arpeggiated with the\nsustain pedal active under tempo of 120 beats per\nminute (bpm). At the 2.0-second mark, the pedal\nis released, ending all of the three notes. At the\n3-second mark, an F quarter note is played at ve-\nlocity 100 for 1 second under a tempo of 60 bpm.\nSuppose that the tempo for Figure\n1 is 120. The\non2on for the ﬁrst note with pitch of 60 is 0. Its\non2off is computed by 2. 0×120/ (60×4) = 1. 0,\nwhere 2.0 is the two seconds that this note is on,\n120 is the tempo, 60 for 60 seconds per minute,\nand 4 for the reciprocal of 1/ 4 note (crotchet).\nThrough this way , we list the 4-tuple representa-\ntions of these four notes at B. In addition, as shown\nin C in Figure\n1, we project these ﬂoat representa-\ntions of on2on and on2off into integers by pro-\nducing them with 384= 128 × 3. Note that 3 is\nintroduced here for covering tritone cases.\nIn our 23-hour experiment data, the maximum\ninteger reaches to 3,840, as long as ten tritones.\nGenerally , the major difference is that we use these\ninteger-style time values instead of TIME\nSHIFT\nas employed in Music Transformer ( Huang et al. ,\n2 http://craffel.github.io/pretty-midi/\n2018). Our usage of time-valued notes aligns with\nnote values natural usage in music book score.\nIn addition, TIME SHIFT can possibly cause\nthe confusion of time value information. That\nis, if we use the absolute time intervals to repre-\nsent notes, then (1) one note can correspond to\ndifferent time intervals and (2) one time interval\ncan also correspond to different notes, under dif-\nferent tempo. The generated music is less user-\nfriendly for composers’ post-editing. Furthermore,\nin the performance event sequence, NOTE\nON\nand NOTE OFF of various notes are mixed to-\ngether which breaks the independences of notes\nand causes losing of time value information. Intu-\nitively speaking, NOTE\nON and NOTE OFF are\nalike brackets and should always be paired in one\nsequence for training. However, this is not guaran-\nteed by event-length based sequence segmenting.\nSince the start and end time of each note are ob-\ntained by computing NOTE\nON and NOTE OFF\nbased on TIME SHIFT . As will be shown in our\nexperiments, this computing process is problem-\natic and frequently causes unstable rhythm (Figure\n4), especially during (10-minute and longer) long-\ntime music generation.\nPiano roll representation of notes is used in\nDeepJ ( Mao et al. , 2018) as a “dense” representa-\ntion of MIDI music. A piece of music is a N × T\nbinary matrix where N is the number of playable\nnotes and T is the number of time steps. Consider-\ning the difference in holding a note versus replay-\ning a note, note play and replay (i.e., when a note is\nre-attacked immediately after the note ends, with\nno time steps in between successive plays) are uti-\nlized jointly to deﬁne note representation. How-\never, piano roll is not dense since there are so many\nzeros in the play/replay matrices: only a few notes\nare attacked during each time step and all others\nare zero. It is further not easy to employ piano\nroll representation for sequential learning for mu-\nsic notes’ “language modelling”.\n3 T ransformer-XL T raining with Four\nSequences of Time-valued Notes\n3.1 T ransformer-XL\nTransformer-XL\n3 (Dai et al. , 2019) was designed\nto enable Transformer ( V aswani et al. , 2017) to\nlearn dependency (of among language words or\nmusic notes in this paper) beyond a ﬁxed length\n3 https://github.com/kimiyoung/transformer-xl\nwithout disrupting temporal coherence. It con-\nsists of two updates, a segment-level recurrence\nmechanism and a positional encoding scheme.\nThese allow Transformer-XL not only capturing\nlonger-term dependency but also resolving the lan-\nguage/music context fragmentation problem. Mo-\ntivated by its outstanding performance in terms\nof language modelling, we adapt this framework\nfor music generation through learning a “language\nmodel” by using time-valued notes instead of\nwords as the fundamental units.\nFormally , let sτ−1 = [xτ−1, 1, · · · , xτ−1,L ] be\nthe (τ − 1)-th segment with length L (e.g., L\nnotes in music and L words in natural language),\nhn−1\nτ−1 ∈ RL×d is the (n−1)-th layer (e.g., in Trans-\nformer) hidden state sequence for sτ−1 in which d\nis the hidden dimension. Then, for the next seg-\nment sτ , the hidden state of its n-th hidden layer is\ncomputed by:\n˜h\nn−1\nτ = [SG(hn−1\nτ−1) ◦ hn−1\nτ ]; (1)\nqn\nτ , kn\nτ , vn\nτ = hn−1\nτ W⊤\nq , ˜h\nn−1\nτ W⊤\nk, (E), ˜h\nn−1\nτ W⊤\nv ;\n(2)\nhn\nτ = Transformer-Layer(qn\nτ , kn\nτ , vn\nτ ).\n(3)\nHere, the function SG (·) stands for stop gradient,\ni.e., the gradient of hn−1\nτ−1 will not be updated bas-\ning on the next segments. The notation [hu ◦ hv]\nmeans the concatenation of two hidden sequences\nalong the length dimension to extend the context.\nWq,k,v stand for trainable model parameters. Com-\npared with Transformer (\nV aswani et al. , 2017), the\nmajor update here is the usage of hn−1\nτ−1, previ-\nous segment’s hidden state sequence of (n − 1)-\nth layer, for computing an interim sequence ˜h\nn−1\nτ\nand its further usage for computing the extended-\ncontext enhanced sequences kn\nτ and vn\nτ , to be re-\ntrieved by the query sequence qn\nτ .\nTransformer-XL applies this recurrent mech-\nanism as deﬁned in these equations alike\nhn\nτ =recurrent(hn−1\nτ−1 , hn−1\nτ ) to every two consecu-\ntive segments of a corpus. This essentially creates\na segment-level recurrence in the hidden states of\nvarious transformer layers. As a result, the contex-\ntual information is allowed to go way beyond just\ntwo segments.\nIn the standard Transformer, the attention score\nbetween a query qi = (Exi + Ui) (i.e., embedding\nvector Exi adds with i-th absolute position encod-\ning Ui) and a key vector kj = (Exj + Uj) within\nthe same segment can be decomposed as:\nAabs\ni,j = q⊤\ni kj =\n(\nE⊤\nxi + U⊤\ni\n)\nW⊤\nq Wk\n(\nExj + Uj\n)\n.\n(4)\nThe shortage of absolute position encoding U is\nthat it is not able to distinguish the difference of\none same position appearing in different segments.\nFollowing the idea of relative positional encoding\n(\nShaw et al. , 2018), a relative distance Ri−j is in-\ntroduced to describe the relative positional embed-\nding between qi and kj . Here, R is a sinusoid en-\ncoding matrix alike the one used in (\nV aswani et al. ,\n2017) without learnable parameters. The relative\nattention score is then computed by:\nArel\ni,j = (WqExi + u : v)⊤ (\nWk,E Exj + Wk,R Ri−j\n)\n.\n(5)\nT wo trainable vectors u, v ∈ Rd are used to re-\nplace UiWq and are respectively used for multi-\nplying with Wk,E Exj and Wk,R Ri−j . In addition,\nWk is deliberately separated into two weight ma-\ntrices Wk,E and Wk,R for respectively producing\ncontent-based and position-based key vectors.\nThen, the n-th ( n ranges over 1 to N)\nTransformer-Layer used in Equation\n3 by employ-\ning relative position encoding mechanism is com-\nputed by:\nArel,n\nτ,i,j =\n(\nqn\nτ,i + u : v\n)⊤ (\nkn\nτ,j + Wn\nk,R Ri−j\n)\n;\n(6)\nan\nτ = Masked-Softmax(Arel,n\nτ )vn\nτ ; (7)\non\nτ = LayerNorm(Linear(an\nτ ) +hn−1\nτ ); (8)\nbn\nτ = Positionwise-Feed-Forward(on\nτ ); (9)\nhn\nτ = LayerNorm(Linear(bn\nτ ) +bn\nτ ). (10)\n3.2 Joint T raining\nFigure\n2 depicts our framework that leverages\nfour Transformer-XL networks, corresponding to\non2on, on2off , pitch, and velocity in the 4-tuple\ntime-valued notes. The ﬁrst layer embeds these\nfour sequences. Then, on2on and on2off ’s em-\nbedded sequences are sent to pitch and velocity\nfor including of time value information of notes.\nIn addition, pitch’s embedded sequences are sent\nto velocity as well to provide an inﬂuence. Next,\npitch and velocity concatenate their own embed-\nded sequences with external sequences. The vec-\ntor dimension of each position in the sequence\nwill be thrice for pitch and fourfold for velocity.\nW e additionally employ a linear layer for dimen-\nsion resizing before sending them to the mem-\nory sensitive blocks of Transformer-XL. T ypi-\ncally , each block mainly contain two components,\na masked relative multi-head self-attention with\nmemory (from former segment) and a position-\nwise feed forward layer. This block is repeated N\n(e.g., 6 in the original Transformer (\nV aswani et al. ,\n2017)) times. After these N blocks for the four\nTransformer-XL, we employ a linear layer and\nsoftmax function to compute probabilities (i.e.,\nnormalized scores) of predicted sequences. W e\nﬁnally compute cross entropy loss for each se-\nquence and add them up for the target loss opti-\nmizing.\nIn particular, the “masked relative multi-head at-\ntention with memory” block is computed by Equa-\ntion\n1 to 7. Then, a residual function is deﬁned in\nEquation 8 for “add & norm”. The next “add &\nnorm” residual layer is deﬁned in Equation 10 for\nthe position-wise feed forward layer.\nIn our proposed framework, the four sequences\nhave relations in two places. First, concatenation\nof time valued on2on and on2off embedded se-\nquences to pitch and velocity. Second, joint loss\nwhich sums up the losses of the four sequences.\nThe motivation of our designing is to both ensure\nthe relative independence of each sequence’s de-\nvelopment and the mixed inﬂuence of from time-\nvalued notes to pitch and velocity .\n4 Experiments & Evaluations\n4.1 Data\nW e collect 374 piano MIDI ﬁles from the web\n4 ,\nwhich are hand-made from composers’ piano mu-\nsic scores with correct tempo information in-\nstead of players’ performances 5 during periods of\nbaroque, classical, and romantic. In the baroque\nperiod, Bach’s fugue and prelude are mainly in-\ncluded. Classical period mainly contains prod-\nucts written by Beethoven, Mozart, Brahms, and\nHaydn. For the romantic period, we collect com-\nposers such as Chopin, Liszt, Mendelssohn, Schu-\nbert, and Tschaikovsky . T able\n1 lists statistical in-\nformation of our train, validation, and test sets.\n4 http://www.piano-midi.de/\n5 Such as the MAESTRO dataset in\nhttps://magenta.tensorﬂow .org/datasets/maestro with u n-\nchanged tempo information due to automatic transcribing\nfrom wav ﬁles to MIDI.\n✄ \u0000 ✁ ✂ ☎ ✆✝\n✞ ✟ ✠ ✡ ☛ ☛ ✂ ☞ ✌\n✍ \u0000 ✎ ✏ ✄ ✎ ✏\n✞ ✟ ✠ ✡ ☛ ☛ ✂ ☞ ✌\n✑ \u0000 ✎ ✏ ✄ ✎ ✒ ✒\n✞ ✟ ✠ ✡ ☛ ☛ ✂ ☞ ✌\n✓ \u0000 ✔ ✡ ✕ ✖ ✆✂ ☎ ✗\n✞ ✟ ✠ ✡ ☛ ☛ ✂ ☞ ✌\n✘ ✙ ✚ ✛ ✡ ☛ ✜ ✡ ✕ ✙ ☎ ✂ ✢ ✡\n✘\n✣\n✕ ☎ ✂\n✤ ✥\n✡ ✙ ☛\n✦\n☎ ☎ ✡ ☞ ☎ ✂ ✖ ☞\n✧\n✂ ☎ ✝ ✘ ✡ ✟ ✖\n★\n✗\n✒ ✡ ✡ ☛ ✒ ✖\n★ ✧\n✙\n★\n☛\n✩ ✂ ☞ ✡ ✙\n★\n✪\n✖ ☞ ✆ ✙ ☎\n✩ ✂ ☞ ✡ ✙\n★\n✪\n✖ ☞ ✆ ✙ ☎\n✫\n✩ ✂ ☞ ✡ ✙\n★ ✬\n✭ ✖ ✮ ☎ ✟ ✙ ✯\n✁ ✂ ☎ ✆ ✝ ✚ ✡\n✰ ✣\n✡ ☞ ✆✡ ✚ ✎ ✏ ✄ ✎ ✏ ✚ ✡\n✰ ✣\n✡ ☞ ✆ ✡ ✚ ✎ ✏ ✄ ✎ ✒ ✒ ✚ ✡\n✰ ✣\n✡ ☞ ✆✡ ✚\n✔ ✡ ✕ ✖ ✆ ✂ ☎ ✗ ✚ ✡\n✰ ✣\n✡ ☞ ✆✡ ✚\n✁ ✂ ☎ ✆✝\n✁\n★\n✖ ✠ ✙ ✠ ✂ ✕ ✂ ☎ ✂ ✡ ✚\n✎ ✏ ✄ ✎ ✏\n✁\n★\n✖ ✠ ✙ ✠ ✂ ✕ ✂ ☎ ✂ ✡ ✚\n✎ ✏ ✄ ✎ ✒ ✒\n✁\n★\n✖ ✠ ✙ ✠ ✂ ✕ ✂ ☎ ✂ ✡ ✚\n✔ ✡ ✕ ✖ ✆✂ ☎ ✗\n✁\n★\n✖ ✠ ✙ ✠ ✂ ✕ ✂ ☎ ✂ ✡ ✚\n✏\n✱\n✲\n✖ ✂ ☞ ☎ ✕ ✖ ✚ ✚\n✳\n✳\n✴ ✵ ✶ ✷ ✸ ✷ ✵ ✹ ✺ ✻\n✼\n✹\n✽\n✵\n✾\n✷ ✹\n✿\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✘ ✙ ✚ ✛ ✡ ☛ ✜ ✡ ✕ ✙ ☎ ✂ ✢ ✡\n✘\n✣\n✕ ☎ ✂\n✤ ✥\n✡ ✙ ☛\n✦\n☎ ☎ ✡ ☞ ☎ ✂ ✖ ☞\n✧\n✂ ☎ ✝ ✘ ✡ ✟ ✖\n★\n✗\n✒ ✡ ✡ ☛ ✒ ✖\n★ ✧\n✙\n★\n☛\n✫\n✩ ✂ ☞ ✡ ✙\n★ ✬\n✭ ✖ ✮ ☎ ✟ ✙ ✯\n✏\n✱\n✳\n✴ ✵ ✶ ✷ ✸ ✷ ✵ ✹ ✺ ✻\n✼\n✹\n✽\n✵\n✾\n✷ ✹\n✿\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✘ ✙ ✚ ✛ ✡ ☛ ✜ ✡ ✕ ✙ ☎ ✂ ✢ ✡\n✘\n✣\n✕ ☎ ✂\n✤ ✥\n✡ ✙ ☛\n✦\n☎ ☎ ✡ ☞ ☎ ✂ ✖ ☞\n✧\n✂ ☎ ✝ ✘ ✡ ✟ ✖\n★\n✗\n✒ ✡ ✡ ☛ ✒ ✖\n★ ✧\n✙\n★\n☛\n✫\n✩ ✂ ☞ ✡ ✙\n★ ✬\n✭ ✖ ✮ ☎ ✟ ✙ ✯\n✏\n✱\n✳\n✴ ✵ ✶ ✷ ✸ ✷ ✵ ✹ ✺ ✻\n✼\n✹\n✽\n✵\n✾\n✷ ✹\n✿\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✘ ✙ ✚ ✛ ✡ ☛ ✜ ✡ ✕ ✙ ☎ ✂ ✢ ✡\n✘\n✣\n✕ ☎ ✂\n✤ ✥\n✡ ✙ ☛\n✦\n☎ ☎ ✡ ☞ ☎ ✂ ✖ ☞\n✧\n✂ ☎ ✝ ✘ ✡ ✟ ✖\n★\n✗\n✒ ✡ ✡ ☛ ✒ ✖\n★ ✧\n✙\n★\n☛\n✫\n✩ ✂ ☞ ✡ ✙\n★ ✬\n✭ ✖ ✮ ☎ ✟ ✙ ✯\n✏\n✱\n✳\n✴ ✵ ✶ ✷ ✸ ✷ ✵ ✹ ✺ ✻\n✼\n✹\n✽\n✵\n✾\n✷ ✹\n✿\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\n✦\n☛ ☛\n❀\n✏ ✖\n★\n✟\nFigure 2: Our proposed framework with four transformer-xl n etworks.\ntrain validation test\n# MIDI ﬁles 299 34 41\n# baroque 69 8 10\n# classical 86 10 11\n# romantic 144 16 20\nlength (hours) 18.18 2.82 2.44\nT able 1: Statistical information of 3 data sets.\n4.2 Experiment Setups\nW e perform our experiments under a NVIDIA P40\nGPU card, with T ensorﬂow 1.13.1 6 running un-\nder cuda 10.0 and cudnn 7.5.0. W e implement\nour joint training framework (Figure\n2) basing on\nTransformer-XL. The number of Transformer lay-\ners is 6, the number of heads is 8 with a dimen-\nsion of 64. Embedding and hidden layer dimen-\nsions are identical to be 512. The dropout ratio is\nset to be 0.1. Memory length is 1,024. W e use\nAdam algorithm (\nKingma and Ba , 2014) to opti-\nmize our networks. W e retrain DeepJ 7 with pi-\nano roll inputs, Music Transformer 8 and single-\n6 https://www.tensorflow.org/\n7 https://github.com/calclavia/DeepJ\n8 https://github.com/tensorﬂow/magenta/tree/master/ ma -\ngenta/models/score2perf\nsequence Transformer-XL both with performance\nevent inputs, all using our 23-hour datasets.\n4.3 Subjective Evaluations\nFor human evaluations, we separate all partici-\npants into two groups, professional composers and\nnon-composers. Participants in the professional\ngroup are people who hold education degrees in\nmusic composition or electronic music compo-\nsition and production, including Central Conser-\nvatory of Music, Shanghai Conservatory of Mu-\nsic, Communication University of China, East-\nman School of Music, Berklee School of Music,\nPeabody Institute of JHU, and Steinhardt of NYU.\n4.3.1 Human or AI test\nFollowing (\nHadjeres and Pachet , 2017), we ﬁrst\ndesign a “Human or AI” test for all participants.\nW e prepared a mixed music collection with 5 mu-\nsic compositions composed by professional hu-\nman composers and 11 pieces composed by our\nmodel for people to judge whether they are com-\nposed by humans or by AI. 39 professional com-\nposers were asked to rate for each music piece\nthey heard from the music composition theory as-\npect while 61 non-composers were asked to rate\nby their subjective feelings. Our rating scale has\nIt’s Human It’s AI A vg. A vg.\nAll (Pro.) All (Pro.) Pro. All\nHuman 218 (98) 282 (97) 3.24 3.01\nAI 411 (132) 689 (300) 2.94 3.11\nT able 2: Human vs. AI evaluation results.\nﬁve levels (points 1 to 5) including nonsense, ba-\nsic, good, high-level, and masterpiece.\nIn order to avoid participants being affected by\nsome other aspects such as musical instrument tim-\nbre and recording reverbs, all of the testing music\nwhatever composed by human or AI are MIDI ﬁles\nand were exported from the same virtual piano\n(i.e., Logic’s Steinway Grand Piano). Also, we\nlimited the length of each music piece to be around\n30 seconds, and every participant was asked to lis-\nten to 16 pieces. W e limited the listening time to\navoid auditory fatigue.\nThe comparison results are listed in T able\n2.\nAmong professional composers, the average hu-\nman music compositions are scored higher than\nthat of AI. However, among all participants, our\nAI music is scored higher (3.11 vs 3.01) than\nthat of the real human pieces which demonstrates\nthat our AI music composition quality is reason-\nably close to the quality of human composers.\nEven some of the pieces transcends humans’ work\naccording to some single ratings. Interestingly ,\nfor all evaluators, human composers’ music are\njudged to be AI-made in a dominate way .\n4.3.2 Pairwise Comparison\nOur second test is to compare the perceived sam-\nple quality with three baseline models. W e car-\nried out listening tests comparing the baseline\nMusic Transformer (MusicT), DeepJ, and single\nsequence-based Transformer-XL (1-seq txl). Our\nmodel is denoted as Mtxl. Participants (15 com-\nposers and 30 non-composers) were presented\nwith two musical excerpts that were generated\nfrom two different models but were given the same\npriming note. Then the participants were asked\nto rate which one they preferred more. W e gener-\nated 7 samples each model with a different prime,\nand compared them to three other models. In addi-\ntion, we asked every participant to rank the reason\nof choice from four musical aspects: melody , har-\nmony , rhythm, and emotion.\nThe results are shown in T able\n3. Our music sig-\nniﬁcantly exceeds ( p < 0. 01) the ones generated\nMtxl vs DeepJ (30 sec.) Mtxl DeepJ\nProfessional group 64 34\nOverall votes 158 143\nMtxl vs 1-seq txl (30 sec.) Mtxl 1-seq txl\nProfessional group 55 36\nOverall votes 172 129\nMtxl vs MusicT (30 sec.) Mtxl MuiscT\nProfessional group 29 69\nOverall votes 110 163\nMtxl vs MusicT (10 min.) Mtxl MuiscT\nProfessional group 59 39\nOverall votes 124 72\nT able 3: Scores of votes for the four comparisons.\n✄\n\u0000\n✄\n✁ ✄\n✂\n✄\n☎\n✄\n✆ ✄ ✄\n✆\n\u0000\n✄\n✝✞ ✟ ✠ ✡ ☛ ☛ ☞ ✌ ✝✞ ✟ ✠ ✆ ✍ ✎ ☛ ✏ ✞ ✟ ✠ ✝ ✞ ✟ ✠ ✝✑ ✒ ✓ ✄ ✍\n✎ ☛\n✔ ✕\n✝ ✞ ✟ ✠ ✝✑ ✒ ✆ ✄ ✍\n✖✗ ✘ ✕\n✙ ✚ ✛ ✜ ✢ ✣ ✤ ✥ ✚ ✛ ✜ ✦ ✧ ★ ✩ ✪✥ ✤ ✚ ✫ ★ ✛\n✝ ☛ ✠\n✬ ✭ ✮ ✯ ✰ ✱ ✖✬ ✘ ✮ ✲ ✳ ✮\n✞\n✳ ✖ ✴ ✖✬\n✞\n✗ ✬ ✘\nFigure 3: Fine-grained pairwise comparison.\nby DeepJ, 1-seq txl, and 10-minute music of Mu-\nsicT . People prefer the music generated by Music\nTransformer the most only at the case of 30-second\nmusic. The major reason collected from composer\nevaluators is that, the more “humanistic” music\ngenerated by Music Transformer is due to their\nunstable tempo and extremely long notes which\nmake the music feel impressionism and have a nat-\nural reverb sounds similar to adding the sustain\npedal.\nFine-grained comparison is depicted in Figure\n3. According to the data we collected from the\npairwise comparison test, melody and rhythm are\ngenerally weighted higher than harmony and emo-\ntion. The results align with T able\n3: except Mu-\nsicT of 30-sec., our model achieved signiﬁcantly\nbetter ( p < 0. 01) results than DeepJ and 1-seq\ntxl. When we compare Mtxl and 10-min. MusicT ,\nwe observe that our melody and rhythm are signiﬁ-\ncantly better ( p < 0. 01). There is a tie-situation in\nterms of harmony and emotion, reﬂecting require-\nments of future work on improving these two as-\npects.\nFor stability evaluation, we pairwise compare\nthe long-time music (10-min.) generated by Mu-\nFigure 4: 10-minute note sequence comparison of Mu-\nsicT (up side) and MTxl (bottom side).\nsicT and Mtxl. W e randomly generated 7 mu-\nsic pieces by each model with the same priming,\nand extracted the last 30 seconds from each piece.\nIn T able\n3, our model performs signiﬁcantly bet-\nter ( p < 0. 01) and is thus more stable in longer-\ntime music generation of MusicT’s extremely long\nnotes). For example, in longer-time music, huge\nnumber of extremely long notes are generated by\nMusic Transformer but only a few are really at-\ntacked and can be heard by human in later stages\nof music, as illustrated in Figure\n4.\nInspired by AI generated music, composers can\nfurther edit and modify it for speciﬁc emotional\nexpression. For instance, Music Transformer\nand Single-sequence based Transformer-XL gen-\nerate beaming-sixteenth-like notes yet with unsta-\nble tempo, which sounds weird. Therefore, the dis-\nadvantages of these two baselines include: due to\nthe fact that notes are not being quantized to grids,\nit would be more difﬁcult for people to generate\nmusic with speciﬁc time signatures, and also mod-\nify , set MIDI controllers and rearrange the music.\nIn contrast, people can modify the music gener-\nated by our model because all our notes have been\nquantized into time-valued notes properly . More\nimportantly , we can generate music of whatever\ntime signatures we want with our approach.\n4.4 Objective Evaluations\nW e calculate the densities (Figure\n5) of notes using\n4×100 samples, 100 from each system. The den-\nsity is deﬁned to be the number of note-on per win-\ndow size (5 seconds). As the generated sequences\nbecome longer and longer, the densities of Mu-\nsic Transformer and single-sequence Transformer-\nXL drop seriously . One main reason is the accu-\nmulated difﬁculty of NOTE\nON and NOTE OFF\nmatching for long sequences. As an intuitive com-\n(a) density comparison (600 seconds)\n(b) pitch distribution comparison\nFigure 5: Density and pitch distribution comparison\nFigure 6: Example of Chord-like progression.\nparison, our model can keep its stable note density .\nIn fact, we generated a 36-hour sample, its density\nkeeping to be sustainable. In addition, we compare\npitch scattering (Figure\n5) during pitch of 60 to 71.\nOur model’s pitch distribution is the closest to the\noriginal dataset, reﬂecting the strong learning abil-\nity of our framework.\n4.5 Some Observations in Music\nCharacteristics\n4.5.1 Chord-like Progressions\nAs shown in Figure\n6, we ﬁnd that our model has\nlearned some chord progression modes from orig-\ninal data. Here, the T onic-to-Dominant chord pro-\ngression, one of the most commonly used progres-\nsions, has been observed in numerous generated\nFigure 7: Rhythm patterns example.\npieces. This successive chord progression also re-\nsulted in a tonal transposition from C major to D\nmajor.\n4.5.2 W ell-adjusted Dynamics and Rhythm\nPatterns\nWhat surprised us is that the dynamic control by\n“independent” velocity performed well in our re-\nsults. Few notes have abrupt velocity such as a\nsudden high or a sudden low . The velocity was\nmostly changing gradually with a tender ascend-\ning and descending. Also, between sections, there\nare some whole-group contrasts in velocity which\nare quite similar to the emotional contrast between\nmusic movements. Moreover, the velocity ascends\nand descends along with the rising pitches and\nthe falls, commonly used in real music composi-\ntions. In a quarter note-long example (Figure\n79),\nbesides some common combinations such as sin-\ngle quarter note, beaming eighth notes, one eighth\nplus two sixteenth, beaming sixteenth notes, there\nare syncopated patterns, notes with dots and ties\nand appropriate rests.\n5 Conclusion\nW e have described our Transformer-XL based pi-\nano music generation framework and experiments.\nMotived by training from composers’ music book\nscores instead of players’ performance events, we\nleverage four sequences of time-valued notes, for-\nmer note on to current note on, current note on\nto its note off, pitch, and velocity . W e ﬁrst pro-\nposed this novel note representation method which\nprojects each note in MIDI ﬁles into a 4-tuple.\nThen, four Transformer-XL networks are jointly\ntrained by taking these four sequences as inputs\nwith shared embedding concatenations and accu-\nmulated cross entropy loss. Through training on a\n23-hour piano MIDI dataset, our framework gen-\nerates signiﬁcantly better and hour-level stably\nlonger music than three state-of-the-art baselines,\nMusic Transformer, DeepJ, and single sequence-\n9 https://www .youtube.com/watch?v=V8XSCojvaas\nbased Transformer-XL, evaluated automatically\nand manually .\nOur multi-sequence learning framework for mu-\nsic is scalable and can be further enriched by addi-\ntional information, such as tempo sequences, emo-\ntions and music genres for enhancing the expres-\nsive ability of the generated music. W e take these\nas our future work.\nReferences\nKyunghyun Cho, Bart van Merri¨ enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Y oshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nZihang Dai, Zhilin Y ang, Y iming Y ang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short P apers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChris Donahue, Huanru Henry Mao, Y iting Ethan Li,\nGarrison W . Cottrell, and Julian McAuley. 2019.\nLakhnes: Improving multi-instrumental music gen-\neration with cross-domain pre-training. In ISMIR.\nGa ¨ etan Hadjeres and Franc ¸ois Pachet. 2017. Deepbach:\na steerable model for bach chorales generation. In\nICML.\nCurtis Hawthorne, Anna Huang, Daphne\nIppolito, and Douglas Eck. 2018.\nTransformer-nade for piano performances . In\nAdvances in Neural Information Processing\nSystems.\nSepp Hochreiter and J ¨ urgen Schmidhuber. 1997.\nLong short-term memory . Neural computation ,\n9(8):1735–1780.\nCheng-Zhi Anna Huang, Ashish V aswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrew M. Dai, Matthew D. Hoffman, and Douglas\nEck. 2018. An improved relative self-attention\nmechanism for transformer with application to mu-\nsic generation. CoRR.\nDaniel Johnson. 2017.\nGenerating P olyphonic Music Using T ied P arallel Networks ,\nvolume 10198 of In: Correia J., Ciesielski V ., Liapis\nA. (eds) Computational Intelligence in Music, Sound,\nArt and Design. EvoMUSART 2017. Lecture Notes\nin Computer Science . Springer, Cham.\nDiederik P . Kingma and Jimmy Ba. 2014.\nAdam: A method for stochastic optimization .\nCite arxiv:1412.6980 Comment: Published as a con-\nference paper at the 3rd International Conference\nfor Learning Representations, San Diego, 2015.\nNaihan Li, Shujie Liu, Y anqing Liu, Sheng\nZhao, Ming Liu, and Ming Zhou. 2019.\nNeural speech synthesis with transformer network .\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume abs/1809.08895.\nHuanru Henry Mao, T aylor Shin, and Garrison W . Cot-\ntrell. 2018. Deepj: Style-speciﬁc music generation .\nIn 2018 IEEE 12th International Conference on Se-\nmantic Computing (ICSC) , volume abs/1801.00887.\nAbdelrahman Mohamed, Dmytro\nOkhonko, and Luke Zettlemoyer. 2019.\nTransformers with convolutional context for ASR .\nCoRR, abs/1904.11660.\nColin Raffel and Daniel P . W . Ellis. 2014. Intuitive\nanalysis, creation and manipulation of midi data\nwith pretty midi. In 15th International Conference\non Music Information Retrieval Late Breaking and\nDemo P apers.\nPeter Shaw , Jakob Uszkoreit,\nand Ashish V aswani. 2018.\nSelf-attention with relative position representations .\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language T echnologies,\nV olume 2 (Short P apers) , pages 464–468, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nAshish V aswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need . In I. Guyon, U. V .\nLuxburg, S. Bengio, H. W allach, R. Fergus, S. V ish-\nwanathan, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 30 , pages\n5998–6008. Curran Associates, Inc.\nHongyuan Zhu, Enhong Chen, Qi Liu, Nicholas Y uan,\nChuan Qin, Jiawei Li, Kun Zhang, Guang Zhou,\nFuru W ei, and Y uanchun Xu. 2018. Xiaoice band: A\nmelody and arrangement generation framework for\npop music. In Conference: the 24th ACM SIGKDD\nInternational Conference , pages 2837–2846.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5526443719863892
    },
    {
      "name": "Computer science",
      "score": 0.36370956897735596
    },
    {
      "name": "Mathematics",
      "score": 0.3351247310638428
    },
    {
      "name": "Electrical engineering",
      "score": 0.24808472394943237
    },
    {
      "name": "Engineering",
      "score": 0.18540701270103455
    },
    {
      "name": "Voltage",
      "score": 0.10013508796691895
    }
  ],
  "institutions": []
}