{
  "title": "Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers",
  "url": "https://openalex.org/W2911327180",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100427168",
      "name": "Haoyu Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5113541520",
      "name": "Ming Tan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101583277",
      "name": "Mo Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112248869",
      "name": "Shiyu Chang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5062817658",
      "name": "Dakuo Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101698013",
      "name": "Kun Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051951021",
      "name": "Xiaoxiao Guo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044514249",
      "name": "Saloni Potdar",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1613699546",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W1551842868",
    "https://openalex.org/W2943927517",
    "https://openalex.org/W2963021258",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2806882588",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2773963565",
    "https://openalex.org/W2799072540",
    "https://openalex.org/W2889224519",
    "https://openalex.org/W2099779943",
    "https://openalex.org/W2962686078",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2964224278",
    "https://openalex.org/W2807524541",
    "https://openalex.org/W2964212344",
    "https://openalex.org/W2915444840",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W2964349647",
    "https://openalex.org/W2240668419",
    "https://openalex.org/W2513378248",
    "https://openalex.org/W2251817967",
    "https://openalex.org/W174427690",
    "https://openalex.org/W2964125981",
    "https://openalex.org/W2890436498",
    "https://openalex.org/W2963655104",
    "https://openalex.org/W2953388933"
  ],
  "abstract": "Most approaches to extraction multiple relations from a paragraph require multiple passes over the paragraph. In practice, multiple passes are computationally expensive and this makes difficult to scale to longer paragraphs and larger text corpora. In this work, we focus on the task of multiple relation extraction by encoding the paragraph only once (one-pass). We build our solution on the pre-trained self-attentive (Transformer) models, where we first add a structured prediction layer to handle extraction between multiple entity pairs, then enhance the paragraph embedding to capture multiple relational information associated with each entity with an entity-aware attention technique. We show that our approach is not only scalable but can also perform state-of-the-art on the standard benchmark ACE 2005.",
  "full_text": "Extracting Multiple-Relations in One-Pass\nwith Pre-Trained Transformers\nHaoyu Wang∗† Ming Tan∗† Mo Yu∗‡ Shiyu Chang‡\nDakuo Wang‡ Kun Xu§ Xiaoxiao Guo‡ Saloni Potdar†\n†IBM Watson ‡IBM Research §Tencent AI Lab\nAbstract\nThe state-of-the-art solutions for extracting\nmultiple entity-relations from an input para-\ngraph always require a multiple-pass encoding\non the input. This paper proposes a new so-\nlution that can complete the multiple entity-\nrelations extraction task with only one-pass\nencoding on the input corpus, and achieve a\nnew state-of-the-art accuracy performance,\nas demonstrated in the ACE 2005 benchmark.\nOur solution is built on top of the pre-trained\nself-attentive models (Transformer). Since our\nmethod uses a single-pass to compute all rela-\ntions at once, it scales to larger datasets easily;\nwhich makes it more usable in real-world ap-\nplications. 1\n1 Introduction\nRelation extraction (RE) aims to ﬁnd the semantic\nrelation between a pair of entity mentions from an\ninput paragraph. A solution to this task is essential\nfor many downstream NLP applications such as\nautomatic knowledge-base completion (Surdeanu\net al., 2012; Riedel et al., 2013; Verga et al.,\n2016), knowledge base question answering (Yih\net al., 2015; Xu et al., 2016; Yu et al., 2017), and\nsymbolic approaches for visual question answer-\ning (Mao et al., 2019; Hu et al., 2019), etc.\nOne particular type of the RE task is multiple-\nrelations extraction (MRE) that aims to recognize\nrelations of multiple pairs of entity mentions from\nan input paragraph. Because in real-world appli-\ncations, whose input paragraphs dominantly con-\ntain multiple pairs of entities, an efﬁcient and ef-\nfective solution for MRE has more important and\nmore practical implications. However, nearly all\nexisting approaches for MRE tasks (Qu et al.,\n∗ Equal contributions from the corresponding authors:\n{wanghaoy,mingtan,yum}@us.ibm.com. Part of\nwork was done when Kun was at IBM.\n1https://github.com/helloeve/mre-in-one-pass .\n×12\n……\nEntity-aware Self-attention+ Feed-forward…   in south suburbs  of Bagh  #dad and Iraqiartilleryfired …\nLinearPART-WHOLE(e1,e2)ART(e1,e2)\nPool Pool\nLinear\nFigure 1: Model Architecture. Different pairs of entities,\ne.g., ( Iraqi and artillery), ( southern suburbs, Baghdad) are\npredicted simultaneously.\n2014; Gormley et al., 2015; Nguyen and Grish-\nman, 2015) adopt some variations of the single-\nrelation extraction (SRE) approach, which treats\neach pair of entity mentions as an independent in-\nstance, and requires multiple passes of encoding\nfor the multiple pairs of entities. The drawback of\nthis approach is obvious – it is computationally ex-\npensive and this issue becomes more severe when\nthe input paragraph is large, making this solution\nimpossible to implement when the encoding step\ninvolves deep models.\nThis work presents a solution that can resolve\nthe inefﬁcient multiple-passes issue of existing so-\nlutions for MRE by encoding the input only once,\nwhich signiﬁcantly increases the efﬁciency and\nscalability. Speciﬁcally, the proposed solution is\nbuilt on top of the existing transformer-based, pre-\ntrained general-purposed language encoders. In\nthis paper we useBidirectional Encoder Represen-\ntations from Transformers (BERT) (Devlin et al.,\n2018) as the transformer-based encoder, but this\nsolution is not limited to using BERT alone. The\ntwo novel modiﬁcations to the original BERT ar-\nchitecture are: (1) we introduce a structured pre-\ndiction layer for predicting multiple relations for\ndifferent entity pairs; and (2) we make the self-\nattention layers aware of the positions of all en-\narXiv:1902.01030v2  [cs.CL]  3 Jun 2019\ntities in the input paragraph. To the best of our\nknowledge, this work is the ﬁrst promising solu-\ntion that can solve MRE tasks with such high ef-\nﬁciency (encoding the input in one-pass) and ef-\nfectiveness (achieve a new state-of-the-art perfor-\nmance), as proved on the ACE 2005 benchmark.\n2 Background\nMRE is an important task as it is an essential\nprior step for many downstream tasks such as au-\ntomatic knowledge-base completion and question-\nanswering. Popular MRE benchmarks include\nACE (Walker et al., 2006) and ERE (Linguistic\nData Consortium, 2013). In MRE, given as a text\nparagraph x = {x1, . . . , xN }and M mentions\ne = {e1, . . . , eM }as input, the goal is to predict\nthe relation rij for each mention pair (ei, ej) ei-\nther belongs to one class of a list of pre-deﬁned\nrelations Ror falls into a special class NA indicat-\ning no relation. This paper uses “entity mention”,\n“mention” and “entity” interchangeably.\nExisting MRE approaches are based on ei-\nther feature and model architecture selection tech-\nniques (Xu et al., 2015; Gormley et al., 2015;\nNguyen and Grishman, 2015; F. Petroni and\nGemulla, 2015; Sorokin and Gurevych, 2017;\nSong et al., 2018b), or domain adaptations ap-\nproaches (Fu et al., 2017; Shi et al., 2018). But\nthese approaches require multiple passes of encod-\ning over the paragraph, as they treat a MRE task as\nmultiple passes of a SRE task.\n3 Proposed Approach\nThis section describes the proposed one-pass en-\ncoding MRE solution. The solution is built upon\nBERT with a structured prediction layer to en-\nable BERT to predict multiple relations with one-\npass encoding, and an entity-aware self-attention\nmechanism to infuse the relational information\nwith regard to multiple entities at each layer of\nhidden states. The framework is illustrated in Fig-\nure 1. It is worth mentioning that our solution\ncan easily use other transformer-based encoders\nbesides BERT,e.g. (Radford et al., 2018).\n3.1 Structured Prediction with BERT for\nMRE\nThe BERT model has been successfully applied to\nvarious NLP tasks. However, the ﬁnal prediction\nlayers used in the original model is not applicable\nto MRE tasks. The MRE task essentially requires\nto perform edge predictions over a graph with en-\ntities as nodes. Inspired by (Dozat and Manning,\n2018; Ahmad et al., 2018), we propose that we\ncan ﬁrst encode the input paragraph using BERT.\nThus, the representation for a pair of entity men-\ntions (ei, ej) can be denoted as oi and oj respec-\ntively. In the case of a mention ei consist of mul-\ntiple hidden states (due to the byte pair encoding),\noi is aggregated via average-pooling over the hid-\nden states of the corresponding tokens in the last\nBERT layer. We then concatenate oi and oj de-\nnoted as [oi : oj], and pass it to a linear classiﬁer2\nto predict the relation\nP(rij|x, ei, ej) =softmax(WL[oi : oj] +b), (1)\nwhere W L ∈ R2dz×l. dz is the dimension of\nBERT embedding at each token position, and l is\nthe number of relation labels.\n3.2 Entity-Aware Self-Attention based on\nRelative Distance\nThis section describes how we encode multiple-\nrelations information into the model. The key\nconcept is to use the relative distances between\nwords and entities to encode the positional infor-\nmation for each entity. This information is prop-\nagated through different layers via attention com-\nputations. Following (Shaw et al., 2018), for each\npair of word tokens (xi, xj) with the input repre-\nsentations from the previous layer as hi and hj,\nwe extend the computation of self-attention zi as:\nzi =\nN∑\nj=1\nexp eij\n∑N\nk=1 exp eik\n(hjWV + aV\nij), (2)\nwhere eij = hiWQ(hjWK + aK\nij )/\n√\ndz. (3)\nWQ, WK, WV ∈Rdz×dz are the parameters of\nthe model, and dz is the dimension of the output\nfrom the self-attention layer.\nCompared to standard BERT’s self-attention,\naV\nij, aK\nij ∈Rdz are extra, which could be viewed as\nthe edge representation between the input element\nxi and xj . Speciﬁcally, we devise aV\nij and aK\nij\nto encourage each token to be aware of the rela-\ntive distance to different entity mentions, and vice\nversa.\n2We also tried to use MLP and Biaff instead of the linear\nlayer for the classiﬁcation, which do not show better perfor-\nmance compared to the linear classier, as shown in the exper-\niment section. We hypothesize that this is because the em-\nbeddings learned from BERT are powerful enough for linear\nclassiﬁers. Further experiments is needed to verify this.\nin ofsuburbsBaghandfiredartilleryIraqi##dad\nin\nsuburbsofBagh##dadandIraqiartilleryfired Zero Vector!\"($%&) !\"(&%$)\nsouth\nsouth\nFigure 2: Illustration of the tensor {aK\nij }introduced in self-\nattention computation. Each red cell embedding is deﬁned by\nwd(i−j), as the distance from entityxi to token xj. Each blue\ncell embedding is deﬁned by wd(j−i), as the distance from\nthe entity xj to token xi . White cells are zero embeddings\nsince neither xi nor xj is entity. The {aV\nij}follows the same\npattern with independent parameters.\nAdapted from (Shaw et al., 2018), we argue that\nthe relative distance information will not help if\nthe distance is beyond a certain threshold. Hence\nwe ﬁrst deﬁne the distance function as:\nd(i, j) =min(max(−k, (i −j)), k). (4)\nThis distance deﬁnition clips all distances to a re-\ngion [−k, k]. k is a hyper-parameter to be tuned\non the development set. We can now deﬁne aV\nij\nand aK\nij formally as:\naV\nij, aK\nij =\n\n\n\nwV\nd(i,j), wK\nd(i,j), if xi ∈e\nwV\nd(j,i), wK\nd(j,i), if xj ∈e\n0, else.\n(5)\nAs deﬁned above, if either token xi or xj be-\nlongs to an entity, we will introduce a relative po-\nsitional representation according to their distance.\nThe distance is deﬁned in an entity-centric way as\nwe always compute the distance from the entity\nmention to the other token. If neither xi nor xj are\nentity mentions, we explicitly assign a zero vector\nto aK\nij and aV\nij. When both xi and xj are inside\nentity mentions, we take the distance as d(i, j) to\nmake row-wise attention computation coherent as\ndepicted in Figure 2.\nDuring the model ﬁne-tuning, the newly\nintroduced parameters {wK\n−k, ..., wK\nk } and\n{wV\n−k, ..., wV\nk }are trained from scratch.\n4 Experiments\nWe demonstrate the advantage of our method on\na popular MRE benchmark, ACE 2005 (Walker\net al., 2006), and a more recent MRE benchmark,\nSemEval 2018 Task 7 (G ´abor et al., 2018). We\nalso evaluate on a commonly used SRE bench-\nmark SemEval 2010 task 8 (Hendrickx et al.,\n2009), and achieve state-of-the-art performance.\n4.1 Settings\nData For ACE 2005, we adopt the multi-domain\nsetting and split the data following (Gormley et al.,\n2015): we train on the union of news domain\n(nw and bn), tune hyperparameters on half of the\nbroadcast conversation (bc) domain, and evalu-\nate on the remainder of broadcast conversation\n(bc), the telephone speech (cts), usenet news-\ngroups (un), and weblogs (wl) domains. For Se-\nmEval 2018 Task 7 , we evaluate on its sub-task\n1.1. We use the same data split in the shared task.\nThe passages in this task is usually much longer\ncompared to ACE. Therefore we adopt the follow-\ning pre-processing step – for the entity pair in each\nrelation, we assume the tokens related to their re-\nlation labeling are always within a range from the\nﬁfth token ahead of the pair to the ﬁfth token af-\nter it. Therefore, the tokens in the original passage\nthat are not covered by the range of ANY input\nrelations, will be removed from the input.\nMethods We compare our solution with pre-\nvious works that predict a single relation per\npass (Gormley et al., 2015; Nguyen and Grishman,\n2015; Fu et al., 2017; Shi et al., 2018), our model\nthat predicts single relation per pass for MRE, and\nwith the following naive modiﬁcations of BERT\nthat could achieve MRE in one-pass.\n•BERTSP: BERT with structured prediction only,\nwhich includes proposed improvement in 3.1.\n•Entity-Aware BERTSP: our full model, which\nincludes both improvements in §3.1 and §3.2.\n•BERTSP with position embedding on the ﬁnal\nattention layer . This is a more straightforward\nway to achieve MRE in one-pass derived from pre-\nvious works using position embeddings (Nguyen\nand Grishman, 2015; Fu et al., 2017; Shi et al.,\n2018). In this method, the BERT model encode\nthe paragraph to the last attention-layer. Then, for\neach entity pair, it takes the hidden states, adds the\nrelative position embeddings corresponding to the\ntarget entities, and ﬁnally makes the relation pre-\ndiction for this pair.\n•BERTSP with entity indicators on input layer:\nit replaces our structured attention layer, and adds\nindicators of entities (transformed to embeddings)\nMethod dev bc cts wl avg\nBaselines w/o Domain Adaptation (Single-Relation per Pass)\nHybrid FCM (Gormley et al., 2015) - 63.48 56.12 55.17 58.26\nBest published results w/o DA (from Fu et al.) - 64.44 54.58 57.02 58.68\nBERT ﬁne-tuning out-of-box 3.66 5.56 5.53 1.67 4.25\nBaselines w/ Domain Adaptation (Single-Relation per Pass)\nDomain Adversarial Network (Fu et al., 2017) - 65.16 55.55 57.19 59.30\nGenre Separation Network (Shi et al., 2018) - 66.38 57.92 56.84 60.38\nMulti-Relation per Pass\nBERTSP (our model in §3.1) 64.42 67.09 53.20 52.73 57.67\nEntity-Aware BERTSP (our full model) 67.46 69.25 61.70 58.48 63.14\nBERTSP w/ entity-indicator on input-layer 65.32 66.86 57.65 53.56 59.36\nBERTSP w/ pos-emb on ﬁnal att-layer 67.23 69.13 58.68 55.04 60.95\nSingle-Relation per Pass\nBERTSP (our model in §3.1) 65.13 66.95 55.43 54.39 58.92\nEntity-Aware BERTSP (our full model) 68.90 68.52 63.71 57.20 63.14\nBERTSP w/ entity-indicator on input-layer 67.12 69.76 58.05 56.27 61.36\nTable 1: Main Results on ACE 2005.\ndirectly to each token’s word embedding 3. This\nmethod is an extension of (Verga et al., 2018) to\nthe MRE scenario.\nHyperparameters For our experiments, most\nmodel hyperparameters are the same as in pre-\ntraining. We tune the training epochs and the new\nhyperparameter k (in Eq. 4) on the development\nset of ACE 2005. Since the SemEval task has\nno development set, we use the best hyperparam-\neters selected on ACE. For the number of training\nepochs, we make the model pass similar number\nof training instances as in ACE 2005.\n4.2 Results on ACE 2005\nMain Results Table 1 gives the overall results\non ACE 2005. The ﬁrst observation is that our\nmodel architecture achieves much better results\ncompared to the previous state-of-the-art methods.\nNote that our method was not designed for do-\nmain adaptation, it still outperforms those meth-\nods with domain adaptation. This result further\ndemonstrates its effectiveness.\nAmong all the BERT-based approaches, ﬁne-\ntuning the off-the-shelf BERT does not give a sat-\nisfying result, because the sentence embeddings\ncannot distinguish different entity pairs. The sim-\npler version of our approach, BERT SP, can suc-\ncessfully adapt the pre-trained BERT to the MRE\ntask, and achieves comparable performance at the\n3Note the usage of relative position embeddings does not\nwork for one-pass MRE, since each word corresponds to a\nvarying number of position embedding vectors. Summing up\nthe vectors confuses this information. It works for the single-\nrelation per pass setting, but the performance lags behind us-\ning only indicators of the two target entities.\nprior state-of-the-art level of the methods without\ndomain adaptation.\nOur full model, with the structured ﬁne-tuning\nof attention layers, brings further improvement of\nabout 5.5%, in the MRE one-pass setting, and\nachieves a new state-of-the-art performance when\ncompared to the methods with domain adaptation.\nIt also beats the other two methods on BERT in\nMulti-Relation per Pass.\nPerformance Gap between MRE in One-Pass\nand Multi-Pass The MRE-in-one-pass models\ncan also be used to train and test with one entity\npair per pass ( Single-Relation per Pass results in\nTable 1). Therefore, we compare the same meth-\nods when applied to the multi-relation and single-\nrelation settings. For BERT SP with entity indica-\ntors on inputs, it is expected to perform slightly\nbetter in the single-relation setting, because of the\nmixture of information from multiple pairs. A 2%\ngap is observed as expected. By comparison, our\nfull model has a much smaller performance gap\nbetween two different settings (and no consistent\nperformance drop over different domains).\nThe BERTSP is not expected to have a gap as\nshown in the table.For BERTSP with position em-\nbeddings on the ﬁnal attention layer, we train the\nmodel in the single-relation setting and test with\ntwo different settings, so the results are the same.\nTraining and Inference Time Through our ex-\nperiment,4 we verify that the full model with MRE\nis signiﬁcantly faster compared to all other meth-\nods for both training and inference. The training\n4All evaluations were done on a single Tesla K80 GPU.\nMethod dev bc cts wl avg\nLinear 67.46 69.25 61.70 58.48 63.14\nMLP 67.16 68.52 61.16 54.72 61.47\nBiaff 67.06 68.22 60.39 55.60 61.40\nTable 2: Our model with different prediction modules.\ntime for full model with MRE is 3.5x faster than\nit with SRE. As for inference speed, the former\ncould reach 126 relation per second compared the\nlater at 23 relation per second. It is also much\nfaster when compared to the second best perform-\ning approach, BERTSP w/ pos-emb on ﬁnal att-\nlayer, which is at 76 relation per second, as it runs\nthe last layer for every entity pair.\nPrediction Module Selection Table 2 evaluates\nthe usage of different prediction layers, including\nreplacing our linear layer in Eq.(1) with MLP or\nBiaff. Results show that the usage of the linear\npredictor gives better results. This is consistent\nwith the motivation of the pre-trained encoders:\nby unsupervised pre-training the encoders are ex-\npected to be sufﬁciently powerful thus adding\nmore complex layers on top does not improve the\ncapacity but leads to more free parameters and\nhigher risk of over-ﬁtting.\n4.3 Results on SemEval 2018 Task 7\nThe results on SemEval 2018 Task 7 are shown in\nTable 3. Our Entity-Aware BERT SP gives com-\nparable results to the top-ranked system (Rot-\nsztejn et al., 2018) in the shared task, with slightly\nlower Macro-F1, which is the ofﬁcial metric of the\ntask, and slightly higher Micro-F1. When predict-\ning multiple relations in one-pass, we have 0.9%\ndrop on Macro-F1, but a further 0.8% improve-\nment on Micro-F1. Note that the system (Rot-\nsztejn et al., 2018) integrates many techniques\nlike feature-engineering, model combination, pre-\ntraining embeddings on in-domain data, and arti-\nﬁcial data generation, while our model is almost a\ndirect adaption from the ACE architecture.\nOn the other hand, compared to the top single-\nmodel result (Luan et al., 2018), which makes\nuse of additional word and entity embeddings pre-\ntrained on in-domain data, our methods demon-\nstrate clear advantage as a single model.\n4.4 Additional SRE Results\nWe conduct additional experiments on the relation\nclassiﬁcation task, SemEval 2010 Task 8, to com-\nMethod Averaged F1\nMacro Micro\nTop 3 in the Shared Task\n(Rotsztejn et al., 2018) 81.7 82.8\n(Luan et al., 2018) 78.9 -\n(Nooralahzadeh et al., 2018) 76.7 -\nOurs (single-per-pass) 81.4 83.1\nOurs (multiple-per-pass) 80.5 83.9\nTable 3: Results on SemEval 2018 Task 7, Sub-Task 1.1.\nMethod Macro-F1\nBest published result (Wang et al., 2016) 88.0\nBERT out-of-box 80.9\nEntity-Aware BERT 88.8\nBERTSP 88.8\nEntity-Aware BERTSP 89.0\nTable 4: Additional Results on SemEval 2010 Task 8.\npare with models developed on this benchmark.\nFrom the results in Table 4, our proposed tech-\nniques also outperforms the state-of-the-art on this\nsingle-relation benchmark.\nOn this single relation task, the out-of-box\nBERT achieves a reasonable result after ﬁne-\ntuning. Adding the entity-aware attention gives\nabout 8% improvement, due to the availability of\nthe entity information during encoding. Adding\nstructured prediction layer to BERT (i.e., BERTSP)\nalso leads to a similar amount of improvement.\nHowever, the gap between BERT SP method with\nand without entity-aware attention is small. This\nis likely because of the bias of data distribution:\nthe assumption that only two target entities exist,\nmakes the two techniques have similar effects.\n5 Conclusion\nIn summary, we propose a ﬁrst-of-its-kind solution\nthat can simultaneously extract multiple relations\nwith one-pass encoding of an input paragraph for\nMRE tasks. With the proposed structured predic-\ntion and entity-aware self-attention layers on top\nof BERT, we achieve a new state-of-the-art results\nwith high efﬁciency on the ACE 2005 benchmark.\nOur idea of encoding a passage regarding mul-\ntiple entities has potentially broader applications\nbeyond relation extraction, e.g., entity-centric pas-\nsage encoding in question answering (Song et al.,\n2018a). In the future work, we will explore the\nusage of this method with other applications.\nReferences\nWasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma,\nEduard Hovy, Kai-Wei Chang, and Nanyun Peng.\n2018. Near or far, wide range zero-shot cross-\nlingual dependency parsing. arXiv preprint\narXiv:1811.00570.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTimothy Dozat and Christopher D Manning. 2018.\nSimpler but more accurate semantic dependency\nparsing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), volume 2, pages 484–490.\nL. Del Corro F. Petroni and R. Gemulla. 2015. Core:\nContext-aware open relation extraction with factor-\nization machines. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nLisheng Fu, Thien Huu Nguyen, Bonan Min, and\nRalph Grishman. 2017. Domain adaptation for re-\nlation extraction with domain adversarial neural net-\nwork. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), volume 2, pages 425–429.\nKata G ´abor, Davide Buscaldi, Anne-Kathrin Schu-\nmann, Behrang QasemiZadeh, Haifa Zargayouna,\nand Thierry Charnois. 2018. Semeval-2018 task 7:\nSemantic relation extraction and classiﬁcation in sci-\nentiﬁc papers. In Proceedings of The 12th Inter-\nnational Workshop on Semantic Evaluation , pages\n679–688.\nMatthew R Gormley, Mo Yu, and Mark Dredze. 2015.\nImproved relation extraction with feature-rich com-\npositional embedding models. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1774–1784.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid ´O S ´eaghdha, Sebastian\nPad´o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2009. Semeval-2010 task 8:\nMulti-way classiﬁcation of semantic relations be-\ntween pairs of nominals. In Proceedings of\nthe Workshop on Semantic Evaluations: Recent\nAchievements and Future Directions , pages 94–99.\nAssociation for Computational Linguistics.\nRonghang Hu, Anna Rohrbach, Trevor Darrell, and\nKate Saenko. 2019. Language-conditioned graph\nnetworks for relational reasoning. arXiv preprint\narXiv:1905.04405.\nLinguistic Data Consortium. 2013. Deft ere annotation\nguidelines: Relations v1.1. 05.17.2013.\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.\n2018. The UWNLP system at SemEval-2018 task 7:\nNeural relation extraction model with selectively in-\ncorporated concept embeddings. In Proceedings of\nThe 12th International Workshop on Semantic Eval-\nuation, pages 788–792, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B\nTenenbaum, and Jiajun Wu. 2019. The neuro-\nsymbolic concept learner: Interpreting scenes,\nwords, and sentences from natural supervision.\narXiv preprint arXiv:1904.12584.\nThien Huu Nguyen and Ralph Grishman. 2015.\nCombining neural networks and log-linear mod-\nels to improve relation extraction. arXiv preprint\narXiv:1511.05926.\nFarhad Nooralahzadeh, Lilja Øvrelid, and Jan Tore\nLønning. 2018. SIRIUS-LTG-UiO at SemEval-\n2018 task 7: Convolutional neural networks with\nshortest dependency paths for semantic relation ex-\ntraction and classiﬁcation in scientiﬁc papers. In\nProceedings of The 12th International Workshop on\nSemantic Evaluation, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nLizhen Qu, Yi Zhang, Rui Wang, Lili Jiang, Rainer\nGemulla, and Gerhard Weikum. 2014. Senti-lssvm:\nSentiment-oriented multi-relation extraction with la-\ntent structural svm. Transactions of the Association\nfor Computational Linguistics, 2:155–168.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nSebastian Riedel, Limin Yao, Andrew McCallum, and\nBenjamin M Marlin. 2013. Relation extraction with\nmatrix factorization and universal schemas. In Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n74–84.\nJonathan Rotsztejn, Nora Hollenstein, and Ce Zhang.\n2018. ETH-DS3Lab at SemEval-2018 task 7: Effec-\ntively combining recurrent and convolutional neural\nnetworks for relation classiﬁcation and extraction.\nIn Proceedings of The 12th International Workshop\non Semantic Evaluation , New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In NAACL-HLT, page 464468.\nGe Shi, Chong Feng, Lifu Huang, Boliang Zhang,\nHeng Ji, Lejian Liao, and Heyan Huang. 2018.\nGenre separation network with adversarial training\nfor cross-genre relation extraction. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1018–1023.\nLinfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang,\nRadu Florian, and Daniel Gildea. 2018a. Exploring\ngraph-structured passage representation for multi-\nhop reading comprehension with graph neural net-\nworks. arXiv preprint arXiv:1809.02040.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018b. N-ary relation extraction using\ngraph-state lstm. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2226–2235.\nDaniil Sorokin and Iryna Gurevych. 2017. Context-\nAware Representations for Knowledge Base Rela-\ntion Extraction. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1784–1789. Associa-\ntion for Computational Linguistics.\nMihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,\nand Christopher D Manning. 2012. Multi-instance\nmulti-label learning for relation extraction. In Pro-\nceedings of the 2012 joint conference on empirical\nmethods in natural language processing and compu-\ntational natural language learning, pages 455–465.\nPatrick Verga, David Belanger, Emma Strubell, Ben-\njamin Roth, and Andrew McCallum. 2016. Multi-\nlingual relation extraction using compositional uni-\nversal schema. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 886–896.\nPatrick Verga, Emma Strubell, and Andrew McCallum.\n2018. Simultaneously self-attending to all mentions\nfor full-abstract biological relation extraction. In\nNAACL 2018, pages 872–884.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. Ace 2005 multilin-\ngual training corpus. Linguistic Data Consortium,\nPhiladelphia, 57.\nLinlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan\nLiu. 2016. Relation classiﬁcation via multi-level at-\ntention cnns. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1298–1307.\nKun Xu, Yansong Feng, Songfang Huang, and\nDongyan Zhao. 2015. Semantic relation clas-\nsiﬁcation via convolutional neural networks\nwith simple negative sampling. arXiv preprint\narXiv:1506.07650.\nKun Xu, Siva Reddy, Yansong Feng, Songfang Huang,\nand Dongyan Zhao. 2016. Question answering on\nfreebase via relation extraction and textual evidence.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), volume 1, pages 2326–2336.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), volume 1, pages 1321–1331.\nMo Yu, Wenpeng Yin, Kazi Saidul Hasan, Cicero dos\nSantos, Bing Xiang, and Bowen Zhou. 2017. Im-\nproved neural relation detection for knowledge base\nquestion answering. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , volume 1,\npages 571–581.",
  "topic": "Paragraph",
  "concepts": [
    {
      "name": "Paragraph",
      "score": 0.9308841228485107
    },
    {
      "name": "Computer science",
      "score": 0.7861606478691101
    },
    {
      "name": "Transformer",
      "score": 0.7246642112731934
    },
    {
      "name": "Scalability",
      "score": 0.710708498954773
    },
    {
      "name": "Embedding",
      "score": 0.6283501386642456
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5692381858825684
    },
    {
      "name": "Natural language processing",
      "score": 0.5642229914665222
    },
    {
      "name": "Relationship extraction",
      "score": 0.5405693054199219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5074712634086609
    },
    {
      "name": "Task (project management)",
      "score": 0.46109238266944885
    },
    {
      "name": "Focus (optics)",
      "score": 0.4161602258682251
    },
    {
      "name": "Information extraction",
      "score": 0.3409183621406555
    },
    {
      "name": "Speech recognition",
      "score": 0.3377302289009094
    },
    {
      "name": "Database",
      "score": 0.08913958072662354
    },
    {
      "name": "Engineering",
      "score": 0.08096891641616821
    },
    {
      "name": "World Wide Web",
      "score": 0.066008061170578
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}