{
  "title": "GRIT: Generative Role-filler Transformers for Document-level Event Entity Extraction",
  "url": "https://openalex.org/W3154063293",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2676585443",
      "name": "Xin-Ya Du",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2294834069",
      "name": "Alexander Rush",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A229573375",
      "name": "Claire Cardie",
      "affiliations": [
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2078523032",
    "https://openalex.org/W2134486566",
    "https://openalex.org/W2165516035",
    "https://openalex.org/W2952179106",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2108743083",
    "https://openalex.org/W2935052563",
    "https://openalex.org/W2250575108",
    "https://openalex.org/W2141461755",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W759515131",
    "https://openalex.org/W2165962657",
    "https://openalex.org/W2798636921",
    "https://openalex.org/W2475245295",
    "https://openalex.org/W2775730905",
    "https://openalex.org/W2885765547",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2739918945",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2251160286",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2169943035",
    "https://openalex.org/W2211728022",
    "https://openalex.org/W2949345771",
    "https://openalex.org/W3034900014",
    "https://openalex.org/W2016698607",
    "https://openalex.org/W2250836735",
    "https://openalex.org/W3024298906",
    "https://openalex.org/W2942904230",
    "https://openalex.org/W2098345921",
    "https://openalex.org/W2963020213",
    "https://openalex.org/W2118161437",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2891553865",
    "https://openalex.org/W182831726",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W2118928552",
    "https://openalex.org/W2964206023",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2888041867",
    "https://openalex.org/W2962859618",
    "https://openalex.org/W2798425628",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W4229735852"
  ],
  "abstract": "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 634–644\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n634\nGRIT: Generative Role-ﬁller Transformers\nfor Document-level Event Entity Extraction\nXinya Du Alexander M. Rush Claire Cardie\nDepartment of Computer Science\nCornell University\n{xdu, cardie}@cs.cornell.edu\narush@cornell.edu\nAbstract\nWe revisit the classic problem of document-\nlevel role-ﬁller entity extraction (REE) for tem-\nplate ﬁlling. We argue that sentence-level ap-\nproaches are ill-suited to the task and intro-\nduce a generative transformer-based encoder-\ndecoder framework (GRIT) that is designed\nto model context at the document level: it\ncan make extraction decisions across sentence\nboundaries; is implicitly aware of noun phrase\ncoreference structure, and has the capacity to\nrespect cross-role dependencies in the tem-\nplate structure. We evaluate our approach on\nthe MUC-4 dataset, and show that our model\nperforms substantially better than prior work.\nWe also show that our modeling choices con-\ntribute to model performance, e.g., by implic-\nitly capturing linguistic knowledge such as rec-\nognizing coreferent entity mentions.\n1 Introduction\nDocument-level template ﬁlling (Sundheim, 1991,\n1993; Grishman and Sundheim, 1996) is a clas-\nsic problem in information extraction (IE) and\nNLP (Jurafsky and Martin, 2014). It is of great\nimportance for automating many real-world tasks,\nsuch as event extraction from newswire (Sundheim,\n1991). The complete task is generally tackled in\ntwo steps. The ﬁrst step detects events in the article\nand assigns templates to each of them (template\nrecognition); the second step performs role-ﬁller\nentity extraction (REE) for ﬁlling in the templates.\nIn this work we focus on the role-ﬁller entity ex-\ntraction (REE) sub-task of template ﬁlling (Fig-\nure 1).1 The input text describes a bombing event;\nthe goal is to identify the entities that ﬁll any of the\nroles associated with the event (e.g., the perpetra-\ntor, their organization, the weapon) by extracting\n1In this work, we assume there is one generic template for\nthe entire document (Huang and Riloff, 2011, 2012).\nRoleRole-filler EntitiesPerpetrator Individualtwo men, two men wearing sports clothes,Shining Path membersPerpetrator OrganizationShining Path\nPhysical Target\nwater pipes,water pipesPilmai telephone company building, telephone company building, telephone company officespublic telephone boothWeapon125 to 150 grams of TnTVictim-\nGold extractions:\nInput document:…A bomb exploded in a Pilmai alley destroying some [water pipes].According to unofficial reports, the bomb contained [125 to 150 grams of TnT] and was placed in the back of the [Pilmai[telephonecompanybuilding]].The explosion occurred at 2350 on 16 January, causing panic but no casualties.The explosion caused damages to the [telephone company offices].  It also destroyed a [public telephone booth] and [water pipes].Witnesses reported that the bomb was planted by [[twomen] wearing sports clothes], who escaped into the night.  …They were later identified as [[Shining Path]members].\nFigure 1: Role-ﬁller entity extraction (REE). The ﬁrst\nmention of each role-ﬁller entity is bold in the table and\ndocument. The arrows denote coreferent mentions.\na descriptive “mention” of it – a string from the\ndocument.\nIn contrast to sentence-level event extraction\n(see, e.g., the ACE evaluation (Linguistic Data Con-\nsortium, 2005)), document-level REE introduces\n635\nseveral complications. First, role-ﬁller entities\nmust be extracted even if they never appear in\nthe same sentence as an event trigger . In Fig-\nure 1, for example, the WEAPON and the ﬁrst men-\ntion of the telephone company building (TARGET )\nappear in a sentence that does not explicitly men-\ntion the explosion of the bomb. In addition, REE\nis ultimately an entity-based task — exactly one\ndescriptive mention for each role-ﬁller should be\nextracted even when the entity is referenced mul-\ntiple times in connection with the event. The ﬁnal\noutput for the bombing example should, therefore,\ninclude just one of the “water pipes” references,\nand one of the three alternative descriptions of the\nPERP IND and the second TARGET , the telephone\ncompany building. As a result of these complica-\ntions, end-to-end sentence-level event extraction\nmodels (Chen et al., 2015; Lample et al., 2016),\nwhich dominate the literature, are ill-suited for the\nREE task, which calls for models that encode infor-\nmation and track entities across a longer context.\nFortunately, neural models for event extraction\nthat have the ability to model longer contexts have\nbeen developed. Du and Cardie (2020), for ex-\nample, extend standard contextualized representa-\ntions (Devlin et al., 2019) to produce a document-\nlevel sequence tagging model for event argument\nextraction. Both approaches show improvements in\nperformance over sentence-level models on event\nextraction. Regrettably, these approaches (as well\nas most sentence-level methods) handle each can-\ndidate role-ﬁller prediction in isolation. Conse-\nquently, they cannot easily model the coreference\nstructure required to limit spurious role-ﬁller\nmention extractions. Nor can they easily exploit\nsemantic dependencies between closely related\nroles like the PERP IND and the PERP ORG, which\ncan share a portion of the same entity span. “Shin-\ning Path members”, for instance, describes the\nPERP IND in Figure 1, and its sub-phrase, “Shining\nPath”, describes the associated PERP ORG.\nContributions In this work we revisit the classic\nbut recently under-studied problem of document-\nlevel role-ﬁller entity extraction problem and in-\ntroduce a novel end-to-end generative transformer\nmodel — the “ Generative Role-filler Transformer”\n(GRIT) (Figure 2).\n•Designed to model context at the document level,\nGRIT (1) has the ability to make extraction deci-\nsions across sentence boundaries; (2) is implic-\nitly aware of noun phrase coreference structure;\nand (3) has the capacity to respect cross-role\ndependencies. More speciﬁcally, GRIT is built\nupon the pre-trained transformer model (BERT):\nwe add a pointer selection module in the decoder\nto permit access to the entire input document,\nand a generative head to model document-level\nextraction decisions. In spite of the added extrac-\ntion capability, GRIT requires no additional pa-\nrameters beyond those in the pre-trained BERT.\n•To measure the model’s ability to both extract\nentities for each role, and implicitly recognize\ncoreferent relations between entity mentions, we\ndesign a metric (CEAF-REE) based on a maxi-\nmum bipartite matching algorithm, drawing in-\nsights from the CEAF (Luo, 2005) coreference\nresolution measure.\n•We evaluate GRIT on the MUC-4 (1992) REE\ntask (Section 3). Empirically, our model outper-\nforms substantially strong baseline models. We\nalso demonstrate that GRIT is better than exist-\ning document-level event extraction approaches\nat capturing linguistic properties critical for the\ntask, including coreference between entity men-\ntions and cross-role extraction dependencies.2\n2 Related Work\nSentence-level Event Extraction Most work in\nevent extraction has focused on the ACE sentence-\nlevel event task (Walker et al., 2006), which re-\nquires the detection of an event trigger and extrac-\ntion of its arguments from within a single sentence.\nPrevious state-of-the-art methods include Li et al.\n(2013) and Li et al. (2015), which explored a vari-\nety of hand-designed features. More recently, neu-\nral network based models such as recurrent neural\nnetworks (Nguyen et al., 2016; Feng et al., 2018),\nconvolutional neural networks (Nguyen and Grish-\nman, 2015; Chen et al., 2015) and attention mecha-\nnisms (Liu et al., 2017, 2018) have also been shown\nto help improve performance. Beyond the task-\nspeciﬁc features learned by the deep neural models,\nZhang et al. (2019) and Wadden et al. (2019) also\nutilize pre-trained contextualized representations.\nOnly a few models have gone beyond individ-\nual sentences to make decisions. Ji and Grish-\nman (2008) and Liao and Grishman (2010) uti-\nlize event type co-occurrence patterns to propagate\n2Our code for the evaluation script and models\nis at https://github.com/xinyadu/grit_doc_\nevent_entity for reproduction purposes.\n636\nevent classiﬁcation decisions. Yang and Mitchell\n(2016) propose to learn within-event (sentence)\nstructures for jointly extracting events and enti-\nties within a document context. Similarly, from a\nmethodological perspective, our GRIT model also\nlearns structured information, but it learns the de-\npendencies between role-ﬁller entity mentions and\nbetween different roles. Duan et al. (2017) and\nZhao et al. (2018) leverage document embeddings\nas additional features to aid event detection. Al-\nthough the approaches above make decisions with\ncross-sentence information, their extractions are\nstill done the sentence level.\nDocument-level IE Document-level event role-\nﬁller mention extraction has been explored in recent\nwork, using hand-designed features for both lo-\ncal and additional context (Patwardhan and Riloff,\n2009; Huang and Riloff, 2011, 2012), and with\nend-to-end sequence tagging based models with\ncontextualized pre-trained representations (Du and\nCardie, 2020). These efforts are the most related\nto our work. The key difference is that our work\nfocuses on a more challenging, and more realistic,\nsetting: extracting role-ﬁller entities rather than\nlists of role-ﬁller mentions that are not grouped ac-\ncording to their associated entity. Also on a related\nnote, Chambers and Jurafsky (2011), Chambers\n(2013), and Liu et al. (2019) work on unsupervised\nevent schema induction and open-domain event\nextraction from documents. The main idea is to\ngroup entities corresponding to the same role into\nan event template.\nRecently, there has also been increasing inter-\nest in cross-sentence/document-level relation ex-\ntraction (RE). In the scientiﬁc domain, Peng et al.\n(2017); Wang and Poon (2018); Jia et al. (2019)\nstudy N-ary cross-sentence RE using distant super-\nvision annotations. Luan et al. (2018) introduce\nSciERC dataset and their model rely on multi-task\nlearning to share representations between entity\nspan extraction and relations. Yao et al. (2019) con-\nstruct an RE dataset of cross-sentence relations on\nWikipedia paragraphs. Ebner et al. (2020) intro-\nduce RAMS dataset for multi-sentence argument\nmention linking, while we focus on entity-level ex-\ntraction in our work. Different from work on joint\nmodeling (Miwa and Bansal, 2016) and multi-task\nlearning (Luan et al., 2019) setting for extracting\nentities and relations, through the generative mod-\neling setup, our GRIT model implicitly captures\n(non-)coreference relations between noun phrases,\nwithout relying on the cross-sentence coreference\nand relation annotations during training.\nNeural Generative Models with a Shared Mod-\nule for Encoder and Decoder Our GRIT model\nuses one shared transformer module for both the\nencoder and decoder, which is simple and effec-\ntive. For the machine translation task, He et al.\n(2018) propose a model which shares the parame-\nters of each layer between the encoder and decoder\nto regularize and coordinate the learning. Dong\net al. (2019) presents a new uniﬁed pre-trained lan-\nguage model that can be ﬁne-tuned for both NLU\nand NLG tasks. Similar to our work, they also in-\ntroduce different masking strategies for different\nkinds of tasks (see Section5).\n3 The Role-ﬁller Entity Extraction Task\nand Evaluation Metric\nWe base the REE task on the original MUC 3 for-\nmulation (Sundheim, 1991), but simplify it as done\nin prior research (Huang and Riloff, 2012; Du and\nCardie, 2020). In particular, we assume that one\ngeneric template should be produced for each doc-\nument: for documents that recount more than one\nevent, the extracted role-ﬁller entities for each are\nmerged into a single event template. Second, we fo-\ncus on entity-based roles with string-based ﬁllers4.\n•Each event consists of the set of roles that de-\nscribe it (shown in Figure 1). The MUC-4 dataset\nthat we use consists of ∼1k terrorism events.\n•Each role is ﬁlled with one or more entities.\nThere are ﬁve such roles for MUC-4: perpetra-\ntor individuals (PERP IND), perpetrator organiza-\ntions (PERP ORG), physical targets ( TARGET ),\nvictims ( VICTIM ) and weapons ( WEAPON ).\nThese event roles represent the agents, pa-\ntients, and instruments associated with terrorism\nevents (Huang and Riloff, 2012).\n•Each role-ﬁller entity is denoted by a single de-\nscriptive mention, a span of text from the input\ndocument. Because multiple such mentions for\neach entity may appear in the input, the gold-\nstandard template lists all alternatives (shown in\nFigure 1), but systems are required to produce\njust one.\n3The Message Understanding Conferences were a series\nof U.S. government-organized IE evaluations.\n4Other types of role ﬁllers include normalized dates and\ntimes, and categorical “set\" ﬁlls. We do not attempt to handle\nthese in the current work.\n637\nEvaluation Metric The metric for past work on\ndocument-level role-ﬁller mentions extraction (Pat-\nwardhan and Riloff, 2009; Huang and Riloff, 2011;\nDu and Cardie, 2020) calculates mention-level pre-\ncision across all alternative mentions for each role-\nﬁller entity. Thus it is not suited for our prob-\nlem setting, where entity-level precision is needed,\nwhere spurious entity extractions will get punished\n(e.g., recognizing “telephone company building”\nand “telephone company ofﬁces” as two entities\nwill result in lower precision).\nDrawing insights from the entity-based CEAF\nmetric (Luo, 2005) from the coreference resolution\nliterature, we design a metric ( CEAF-REE) for\nmeasuring models’ performance on this document-\nlevel role-ﬁller entity extraction task. It is based\non maximum bipartite matching algorithm (Kuhn,\n1955; Munkres, 1957). The general idea is that,\nfor each role, the metric is computed by aligning\ngold and predicted entities with the constraint that\na predicted (gold) entity is aligned with at most\none gold (predicted) entity. Thus, the system that\ndoes not recognize the coreferent mentions and\nuse them for separate entities will be penalized\nin precision score. For the example in Figure 1,\nif the system extracts “Pilmai telephone company\nbuilding” and “telephone company ofﬁces” as two\ndistinct TARGET s, the precision will drop. We\ninclude more details for our CEAF-TF metric in\nthe appendix.\n4 REE as Sequence Generation\nWe treat document-level REE as a sequence-to-\nsequence task (Sutskever et al., 2014) in order to\nbetter model the cross-role dependencies and cross-\nsentence noun phrase coreference structure. We\nﬁrst transform the task deﬁnition into a source and\ntarget sequence.\nAs shown in Figure 2, the source sequence sim-\nply consists of the tokens of the original document\nprepended with a “classiﬁcation” token (i.e., [CLS]\nin BERT), and appended with a separator token\n(i.e., [SEP] in BERT). The target sequence is the\nconcatenation of target extractions for each role,\nseparated by the separator token. For each role,\nthe target extraction consists of the ﬁrst mention’s\nbeginning (b) and end (e) tokens:\n<S> e(1)\n1b ,e(1)\n1e ,... [SEP]\ne(2)\n1b ,e(2)\n1e ,... [SEP]\ne(3)\n1b ,e(3)\n1e ,e(3)\n2b ,e(3)\n2e ,... [SEP]\n...\nNote that we list the roles in a ﬁxed order for all\nexamples. So for the example used in Figure 2,\ne(1)\n1b , e(1)\n1e would be “two” and “men” respectively;\nand e(3)\n1b , e(3)\n1e would be “water” and “pipes” re-\nspectively. Henceforth, we denote the resulting\nsequence of source tokens as x0,x1,...,x m and the\nsequence of target tokens as y0,y1,...,y n.\n5 Model: Generative Role-ﬁller\nTransformer (GRIT)\nOur model is shown in Figure 2. It consists of two\nparts: the encoder (left) for the source tokens; and\nthe decoder (right) for the target tokens. Instead\nof using a sequence-to-sequence learning architec-\nture with separate modules (Sutskever et al., 2014;\nBahdanau et al., 2015), we use a single pretrained\ntransformer model (Devlin et al., 2019) for both\nparts, and introduce no additional ﬁne-tuned pa-\nrameters.\nPointer Embeddings The ﬁrst change to the\nmodel is to ensure that the decoder is aware of\nwhere its previous predictions come from in the\nsource document, an approach we call “pointer\nembeddings”. Similar to BERT, the input to the\nmodel consists of the sum of token, position and\nsegment embeddings. However, for the position\nwe use the corresponding source token’s position.\nFor example, for the word “two”, the target tokens\nwould have the identical position embedding of the\nword “two” in the source document. Interestingly,\nwe do not use any explicit target position embed-\ndings, but instead separate each role with a [SEP]\ntoken. Empirically, we ﬁnd that the model is able\nto use these separators to learn which role to ﬁll\nand which mentions have ﬁlled previous roles.\nOur encoder’s embedding layer uses standard\nBERT embedding layer, which applied to the\nsource document tokens. To denote boundary be-\ntween source and target tokens, we use sequence A\n(ﬁrst sequence) segment embeddings for the source\ntokens, we use sequence B (second sequence) seg-\nment embeddings for the target tokens.\n638\nBERT\nTarget tokens + Pointer embeddings\nModel\n[CLS] … A bomb exploded … destroying some [water pipes]. … the bomb … was placed in the back of the [Pilmai[telephonecompanybuilding]]. … The explosion caused damages to the [telephone company offices]. It also destroyed a [public telephone booth] and [water pipes] … the bomb was planted by [[twomen] wearing sports clothes], escaped.  …later identified as [[Shining Path]members]… [SEP]\nPointer Selection\nSource tokens\ntwomen[SEP]ShiningPath[SEP]waterpipesPilmaibuild-ingpublic\n<S>twomen[SEP]ShiningPath[SEP]waterpipesPilmaibuild-ing\ncausal masking\n1strole (PerpInd)2ndrole (PerpOrg)3rdrole (Target)…\n…\nFigure 2: GRIT: generative transformer model for document-level event role-ﬁller entity extraction. (Noun phrase\nbracketing and bold in the source tokens are provided for readability purposes and are not part of the source\nsequence.)\n…\nsourcetokens……targetsourcetokens…...target\ntokens…\ntokens…\nnot attending\nAttention masks:\nFigure 3: Partially causal masking strategy ( M).\n(White cell: unmasked; Grey cell: masked).\nWe pass the source document tokens through\nthe encoder’s embedding layer, to obtain their em-\nbeddings x0,x1,..., xm. We pass the target to-\nkens y0,y1,...,y n through the decoder’s embed-\nding layer, to obtain y0,y1,..., yn.\nBERT as Encoder / Decoder We utilize one\nBERT model as both the source and target em-\nbeddings. To distinguish the encoder / decoder\nrepresentations, we provide a partial causal atten-\ntion mask on the decoder side.\nIn Figure 3, we provide an illustration for the\nattention masks – 2-dimensional matrix denoted\nas m. For the source tokens, the mask allows full\nsource self-attention, but mask out all target tokens.\nFor i∈{0,1,...,m },\nMi,j =\n{\n1, if 0 ≤j ≤m\n0, otherwise\nFor the target tokens, to guarantee that the de-\ncoder is autoregressive (the current token should\nnot attend to future tokens), we use a causal mask-\ning strategy. Assuming we concatenate the target\nto the source tokens (the joint sequence mentioned\nbelow), for i∈{m+ 1,...,n },\nMi,j =\n\n\n\n1, if 0 ≤j ≤m\n1, if j >m and j ≤i\n0, otherwise\nThe joint sequence of source tokens’ embed-\ndings (x0,x1,..., xm) and target tokens’ embed-\ndings (y0,y1,..., yn) are passed through BERT to\nobtain their contextualized representations,\nˆ x0,ˆ x1,..., ˆ xm,ˆ y0...,ˆ yn\n= BERT(x0,x1,..., xm,y0,..., yn)\nPointer Decoding For the ﬁnal layer, we replace\nword prediction with a simple pointer selection\nmechanism. For target time step t(0 ≤t ≤n),\nwe ﬁrst calculate the dot-product between ˆ yt and\nˆ x0,ˆ x1,..., ˆ xm,\nz0,z1,...,z m = ˆ yt ·ˆ x0,ˆ yt ·ˆ x1,..., ˆ yt ·ˆ xm\n639\nThen we apply softmax to z0,z1,...,z m to obtain\nthe probabilities of pointing to each source token,\np0,p1,...,p m = softmax(z0,z1,...,z m)\nTest prediction is done with greedy decoding.\nAt each time step t, argmax is applied to ﬁnd the\nsource token which has the highest probability. The\npredicted token is added to the target sequence for\nthe next time step t+ 1 with its pointer embedding.\nWe stop decoding when the ﬁfth [SEP] token is\npredicted, which represents the end of extractions\nfor the last role.\nIn addition, we add the following decoding con-\nstraints,\n•Tune probability of generating [SEP]. By doing\nthis, we encourage the model to point to other\nsource tokens and thus extract more entities for\neach role, which will help increase the recall.\n(We set the hyperparameter of downweigh to\n0.01, i.e., for the [SEP] token p= 0.01 ∗p.)\n•Ensure that the token position increase from start\ntoken to end token. When decoding tokens for\neach role, we know that mention spans should\nobey this property. Thus we eliminate those in-\nvalid choices during decoding.\n6 Experimental Setup\nWe conduct evaluations on the MUC-4\ndataset (1992), and compare to recent com-\npetitive end-to-end models (Wadden et al., 2019;\nDu and Cardie, 2020) in IE (Section 7). Besides\nthe normal evaluation, we are also interested in\nhow well our GRIT model captures coreference\nlinguistic knowledge, and comparison with the\nprior models. In Section 8, we present relevant\nevaluations on the subset of test documents.\nDataset and Evaluation Metric The MUC-4\ndataset consists of 1,700 documents with associated\ntemplates. Similar to (Huang and Riloff, 2012; Du\nand Cardie, 2020), we use the 1300 documents for\ntraining, 200 documents (TST1+TST2) as the de-\nvelopment set and 200 documents (TST3+TST4)\nas the test set. Each document in the dataset con-\ntains on average 403.27 tokens, 7.12 paragraphs.\nIn Table 1, we include descriptions for each role in\nthe template.\nWe use the ﬁrst appearing mention of the role-\nﬁller entity as the training signal (thus do not use\nthe other alternative mentions during training).\nRoles Descriptions\nPERPIND A person responsible for the incident.\nPERPORG An organization responsible for the incident.\nTARGET A thing (inanimate object) that was attacked.\nVICTIM The name of a person who was the obvious\nor apparent target of the attack\nor who became a victim of the attack.\nWEAPON A device used by the perpetrator(s) in carrying.\nTable 1: Natural Language Descriptions for Each Role.\nWe use CEAF-REE which is covered in Sec-\ntion 3 as the evaluation metric. The results are re-\nported as Precision (P), Recall (R) and F-measure\n(F1) score for the micro-average for all the event\nroles (Table 4). We also report the per-role results\nto have a ﬁne-grained understanding of the num-\nbers (Table 2).\nBaselines We compare to recent strong models\nfor (document-level) information/event extraction.\nCohesionExtract (Huang and Riloff, 2012) is a\nbottom-up approach for event extraction that ﬁrst\naggressively identiﬁes candidate role-ﬁllers, and\nprune the candidates located in event-irrelevant\nsentences.5 Du and Cardie (2020) propose neu-\nral sequence tagging (NST) models with contex-\ntualized representations for document-level role\nﬁller mentions extraction. We train this model\nwith BIO tagging scheme to identify the ﬁrst\nmention for each role-ﬁller entity and its type\n(i.e., B-PerpInd, I-PerpInd for perpetrator individ-\nual). DYGIE++ (Wadden et al., 2019) is a span-\nenumeration based extraction model for entity, re-\nlation, and event extraction. The model (1) enu-\nmerates all the possible spans in the document;\n(2) concatenates the representations of the span’s\nbeginning & end token and use it as its represen-\ntation, and pass it through a classiﬁer layer to pre-\ndict whether the span represents certain role-ﬁller\nentity and what the role is. Both the NST and DY-\nGIE++ are end-to-end and ﬁne-tuned BERT (De-\nvlin et al., 2019) contextualized representations\nwith task-speciﬁc data. We train them to identify\nthe ﬁrst mention for each role-ﬁller entity (to en-\nsure fair comparison with our proposed model).\nUnsupervised event schema induction based ap-\nproaches (Chambers and Jurafsky, 2011; Cham-\nbers, 2013; Cheung et al., 2013) are also able\n5Instead of using feature-engineering based sentence classi-\nﬁcation to identify event-relevant sentences, we re-implement\nthe sentence classiﬁer with BiLSTM-based neural sequence\nmodel.\n640\nPERPIND PERPORG TARGET VICTIM WEAPON\nNST\n(Du and Cardie, 2020)48.39 / 32.61 / 38.9660.00 / 43.90 / 50.7054.96 / 52.94 /53.93 62.50 / 63.16 / 62.8361.67 / 61.67 /61.67\nDYGIE++\n(Wadden et al., 2019)59.49 / 34.06 / 43.3256.00 / 34.15 / 42.4253.49 / 50.74 / 52.0860.00 / 66.32 / 63.0057.14 / 53.33 / 55.17\nGRIT 65.48 / 39.86 /49.55 66.04 / 42.68 /51.85 55.05 / 44.12 / 48.9876.32 / 61.05 /67.84 61.82 / 56.67 / 59.13\nTable 2: Per-role performance scored by CEAF-REE (reported as P/R/F1, highest F1 for each role are boldfaced).\nk= 1 1<k≤1.25 1.25<k≤1.5 1.5<k≤1.75 k>1.75\nNST\n(Du and Cardie, 2020)63.83 / 51.72 / 57.1457.45 / 38.57 / 46.1560.32 / 49.03 / 54.0964.81 / 50.00 / 56.4566.67 / 51.90 / 58.36\nDYGIE++\n(Wadden et al., 2019)72.50/ 50.00 / 59.1870.00 / 40.00 / 50.9160.48 / 48.39 / 53.7652.94 / 38.57 / 44.6366.96 / 48.73 / 56.41\nGRIT 65.85 / 46.55 / 54.5574.42/ 45.71 / 56.6473.20/ 45.81 / 56.3567.44/ 41.43 / 51.3369.75/ 52.53 / 59.93\nTable 3: Evaluations on the subsets of documents with increasing number of mentions per role-ﬁller entity. k\ndenotes the average # mentions per role-ﬁller entity. Results for each column are reported as Precision / Recall /\nF1. The highest precisions are boldfaced for each bucket.\nto model the coreference relations and entities at\ndocument-level, but have been proved to perform\nsubstantially worse than supervised models (Pat-\nwardhan and Riloff, 2009; Huang and Riloff, 2012).\nThus we do not compare with them. We also exper-\nimented with a variant of our GRIT model – instead\nof always pointing to the same [SEP] in the source\ntokens to ﬁnish extracting the role-ﬁller entities for\na role, we use ﬁve different [SEP] tokens. During\ndecoding, the model points to the corresponding\n[SEP] as the end of extraction for that role. This\nvariant does not improve over the current best re-\nsults and we omit reporting its performance.\n7 Results\nIn Table 4, we report the micro-average perfor-\nmance on the test set. We observe that our GRIT\nmodel substantially outperforms the baseline ex-\ntraction models in precision and F1, with an over\n5% improvement in precision over DYGIE++.\nTable 2 compares the models’ performance\nscores on each role ( PERP IND, PERP ORG, TAR-\nGET , VICTIM , WEAPON ). We see that, (1) our\nmodel achieves the best precision across the roles;\n(2) for the roles that come with entities containing\nmore human names (e.g., PERP IND and VICTIM ),\nour model substantially outperforms the baselines;\n(3) for the role PERP ORG, our model scores bet-\nter precision but lower recall than neural sequence\ntagging, which results in a slightly better F1 score;\n(4) for the roles TARGET and WEAPON , our model\nis more conservative (lower recall) and achieves\nlower F1. One possibility is that for role like TAR-\nModels P R F1\nCohesionExtract\n(Huang and Riloff, 2012)58.38 39.53 47.14\nNST\n(Du and Cardie, 2020) 56.82 48.92 52.58\nDYGIE++\n(Wadden et al., 2019) 57.04 46.77 51.40\nGRIT 64.19∗∗ 47.36 54.50∗\nTable 4: Micro-average results (the highest number of\neach column is boldfaced). Signiﬁcance is indicated with\n∗∗(p <0.01),∗(p <0.1) – all tests are computed using the\npaired bootstrap procedure (Berg-Kirkpatrick et al., 2012).\nGET , on average there are more entities (though\nwith only one mention each), and it’s harder for our\nmodel to decode as many TARGET entities correct\nin a generative way.\n8 Discussion\nHow well do the models capture coreference re-\nlations between mentions? We also conduct tar-\ngeted evaluations on subsets of test documents\nwhose gold extractions come with coreferent men-\ntions. From left to right in Table 3, we report re-\nsults on the subsets of documents with increasing\nnumber (k) of possible (coreferent) mentions per\nrole-ﬁller entity. We ﬁnd that: (1) On the sub-\nset of documents with only one mention for each\nrole-ﬁller entity (k= 1), our model has no signiﬁ-\ncant advantage over DYGIE++ and the sequence\ntagging based model; (2) But as kincreases, the ad-\nvantage of our GRIT substantially increases – with\nan over 10% gap in precision when 1 < k≤1.5,\n641\nand a near 5% gap in precision when k> 1.5.\nFrom the qualitative example (document excerpt\nand the extractions in Figure 4), we also observe\nour model recognizes the coreference relation be-\ntween candidate role-ﬁller entity mentions, while\nthe baselines do not, which shows that our model is\nbetter at capturing the (non-)coreference relations\nbetween role-ﬁller entity mentions. It also proves\nthe advantage of a generative model in this setting.\n1 Discussion\nHow well do the models capture coreference\nrelations between mentions we also see our\nmodel recognizes the coreference relation between\ncandidate role-ﬁller entities, while the baselines\ndon’t. This demonstrates that our model is better at\ncapturing the (non)-coreference relation between\nrole-ﬁller entities. It also proves the advantage of\ngenerative modeling (over modeling one candidate\nrole-ﬁller entity’s role in isolation).\n[P1]... a bomb exploded at the front door of the\n[home of a peruvian army general], causing dam-\nages but no casualties. ... [P2] The terrorist attack\nwas ..., by ... who hurled a bomb at the [home of\ngeneral enrique franco], in the San ... [P3] The\nbomb seriously damaged the [general’s [vehicle]],\n... and those of [neighboring [houses]].\nTARGET\nGold Role-\nﬁller Entities\n• home of peruvian army general,\nhome of general enrique franco\n• vehicle, general’s vehicle\n• houses, neighboring houses\nNST • home of peruvian army general\n• home of general enrique franco\nDYGIE++\n• home of peruvian army general\n• home of general enrique franco\n• houses\nGRIT • home of peruvian army general\n• houses\nHow well do the models capture dependencies\nbetween different roles\n...[[[guerrillas] of the [FARC] and the [popular\nliberation army]] (EPL)] attacked four towns in\nnorthern Colombia, leaving 17 guerrillas and 2\nsoldiers dead and 3 bridges partially destroyed. ...\nPERPIND PERPORG\nGold Role-\nﬁller Entities\n• guerrillas,\nguerrillas of FARC\nand popular\nliberation army (EPL)\n• EPL, popular\nliberation army\n• FARC\nNST & DYGIE++ • guerrillas -\nGRIT • guerrillas\n• FARC\n• popular\nliberation army\nOur model correctly extracts the two role-ﬁller en-\ntities for PERP ORG: “FARC” and “popular libera-\ntion army”, which are closely related to the PER-\nPIND entity “guerrilla”. While the DYGIE++ and\nNST both miss the entities for PERP ORG.\nFigure 4: Our model implicitly captures coreference\nrelations between mentions.\nHow well do models capture dependencies\nbetween different roles? To study this phe-\nnomenon, we consider nested role-ﬁller entity men-\ntions in the documents. In the example of Figure 1,\n“shining path” is a role-ﬁller entity mention for\nPERP ORG nested in “two shining path members”\n(a role-ﬁller entity mention for PERP IND). The\nnesting happens more often between more related\nroles (e.g., PERP IND and PERP ORG) – we ﬁnd that\n33 out of the 200 test documents’ gold extractions\ncontain nested role-ﬁller entity mentions between\nthe two roles.\nIn Table 5, we present the CEAF-REE scores\nfor role PERP ORG on the subset of documents\nwith nested roles. As we hypothesized beforehand,\nGRIT is able to learn the dependency between dif-\nferent roles and can learn to avoid missing rele-\nvant role-ﬁller entities for later roles. The results\nprovide empirical evidence: by learning the depen-\ndency between PERP IND and PERP ORG, GRIT\nPERPORG(all docs) PERPORG(33/200)\nP / R / F1 P / R / F1\nNST 56.00 / 34.15 / 42.4280.00 / 44.44 / 57.14\nDYGIE++ 60.00 /43.90/ 50.70 61.54 / 35.56 / 45.07\nGRIT 66.04 / 42.68 / 51.8580.77 /46.67/ 59.15\nTable 5: Evaluation on the subset of documents that\nhave nested role-ﬁller entity mentions between role\nPERP IND and PERP ORG (highest recalls boldfaced).\n1 Discussion\nHow well do the models capture coreference\nrelations between mentions we also see our\nmodel recognizes the coreference relation between\ncandidate role-ﬁller entities, while the baselines\ndon’t. This demonstrates that our model is better at\ncapturing the (non)-coreference relation between\nrole-ﬁller entities. It also proves the advantage of\ngenerative modeling (over modeling one candidate\nrole-ﬁller entity’s role in isolation).\n[P1]... a bomb exploded at the front door of the\n[home of a peruvian army general], causing dam-\nages but no casualties. ... [P2] The terrorist attack\nwas ..., by ... who hurled a bomb at the [home of\ngeneral enrique franco], in the San ... [P3] The\nbomb seriously damaged the [general’s [vehicle]],\n... and those of [neighboring [houses]].\nTARGET\nGold Role-\nﬁller Entities\n• home of peruvian army general,\nhome of general enrique franco\n• vehicle, general’s vehicle\n• houses, neighboring houses\nNST • home of peruvian army general\n• home of general enrique franco\nDYGIE++\n• home of peruvian army general\n• home of general enrique franco\n• houses\nGRIT • home of peruvian army general\n• houses\nHow well do the models capture dependencies\nbetween different roles\n...[[[guerrillas] of the [FARC] and the [popular\nliberation army]] (EPL)] attacked four towns in\nnorthern Colombia, leaving 17 guerrillas and 2\nsoldiers dead and 3 bridges partially destroyed. ...\nPERPIND PERPORG\nGold Role-\nﬁller Entities\n• guerrillas,\nguerrillas of FARC\nand popular\nliberation army (EPL)\n• EPL, popular\nliberation army\n• FARC\nNST & DYGIE++ • guerrillas -\nGRIT • guerrillas\n• FARC\n• popular\nliberation army\nOur model correctly extracts the two role-ﬁller en-\ntities for PERP ORG: “FARC” and “popular libera-\ntion army”, which are closely related to the PER-\nPIND entity “guerrilla”. While the DYGIE++ and\nNST both miss the entities for PERP ORG.\nFigure 5: Our model captures dependencies between\ndifferent roles.\nimproves the relative recall score on the subset of\ndocuments as compared to DYGIE++ . On all the\n200 test documents, our model is ∼2% below DY-\nGIE++ in recall; while on the 33 docs, our model\nscores much higher than DYGIE++ in recall.\nFor the document in the example of Figure 5, our\nmodel correctly extracts the two role-ﬁller entities\nfor PERP ORG: “FARC” and “popular liberation\narmy”, which are closely related to the PERP IND\nentity “guerrilla”. While DYGIE++ and NST both\nmiss the entities for PERP ORG.\nDecoding Ablation Study In the table below,\nwe present ablation results based on the decod-\ning constraints. These illustrate the inﬂuence of\nthe decoding constraints on the our model’s perfor-\nmance. The two constraints both signiﬁcantly im-\nprove model predictions. Without downweighing\nthe probability of pointing to [SEP], the precision\nincreases but recall and F1 signiﬁcantly drops.\nP R F1 ∆(F1)\nGRIT 64.19 47.36 54.50\n−[SEP] downweigh 67.43 40.12 50.31 -4.19\n−constraint on pointer offset62.90 45.79 53.00 -1.50\nTable 6: Decoding Ablation Study\n642\nAdditional Parameters and Training Cost Fi-\nnally we consider additional parameters and train-\ning time of the models: As we introduced previ-\nously, the baseline models DYGIE++ and NST\nboth require an additional classiﬁer layer on top of\nBERT’s hidden state (of sizeH) for making the pre-\ndictions. While our GRIT model does not require\nadding any new parameters. As for the training\ntime, training the DYGIE++ model takes over 10\ntimes longer time than NST and our model. This\ntime comes from the DYGIE++ model require-\nment of enumerating all possible spans (to a certain\nlength constraint) in the document and calculating\nthe loss with their labels.\nadditional params training cost\nDYGIE++ 2H(#roles + 1) ∼20h\nNST H(2#roles + 1) ∼1h\nGRIT 0 <40min\nTable 7: Additional Parameters and Training Cost.\n9 Conclusion\nWe revisit the classic and challenging problem of\ndocument-level role-ﬁller entity extraction (REE),\nand ﬁnd that there is still room for improvement.\nWe introduce an effective end-to-end transformer\nbased generative model, which learns the docu-\nment representation and encodes the dependency\nbetween role-ﬁller entities and between event roles.\nIt outperforms the baselines on the task and better\ncaptures the coreference linguistic phenomena. In\nthe future, it would be interesting to investigate how\nto enable the model to also do template recognition.\nAcknowledgments\nWe thank the anonymous reviewers for helpful feed-\nback and suggestions. The work of XD and AMR\nwas supported by NSF CAREER 2037519; that\nof XD and CC was supported in part by DARPA\nLwLL Grant FA8750-19-2-0039.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTaylor Berg-Kirkpatrick, David Burkett, and Dan\nKlein. 2012. An empirical investigation of statis-\ntical signiﬁcance in NLP. In Proceedings of the\n2012 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning , pages 995–1005, Jeju Is-\nland, Korea. Association for Computational Linguis-\ntics.\nNathanael Chambers. 2013. Event schema induction\nwith a probabilistic entity-driven model. In Proceed-\nings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 1797–1807,\nSeattle, Washington, USA. Association for Compu-\ntational Linguistics.\nNathanael Chambers and Dan Jurafsky. 2011.\nTemplate-based information extraction without\nthe templates. In Proceedings of the 49th Annual\nMeeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n976–986, Portland, Oregon, USA. Association for\nComputational Linguistics.\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and\nJun Zhao. 2015. Event extraction via dynamic multi-\npooling convolutional neural networks. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 167–176, Beijing, China. Association\nfor Computational Linguistics.\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-\nderwende. 2013. Probabilistic frame induction. In\nProceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 837–846, Atlanta, Georgia. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Informa-\ntion Processing Systems, pages 13042–13054.\nXinya Du and Claire Cardie. 2020. Document-level\nevent role ﬁller extraction using multi-granularity\ncontextualized encoding. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 8010–8020, Online. Asso-\nciation for Computational Linguistics.\nShaoyang Duan, Ruifang He, and Wenli Zhao. 2017.\nExploiting document level information to improve\n643\nevent detection via recurrent neural networks. In\nProceedings of the Eighth International Joint Con-\nference on Natural Language Processing , pages\n352–361, Taipei, Taiwan. Asian Federation of Nat-\nural Language Processing.\nSeth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins,\nand Benjamin Van Durme. 2020. Multi-sentence ar-\ngument linking. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 8057–8077, Online. Association for\nComputational Linguistics.\nXiaocheng Feng, Bing Qin, and Ting Liu. 2018.\nA language-independent neural network for event\ndetection. Science China Information Sciences ,\n61(9):092106.\nRalph Grishman and Beth Sundheim. 1996. Design\nof the MUC-6 evaluation. In TIPSTER TEXT PRO-\nGRAM PHASE II: Proceedings of a Workshop held\nat Vienna, Virginia, May 6-8, 1996, pages 413–422,\nVienna, Virginia, USA. Association for Computa-\ntional Linguistics.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7944–7954.\nRuihong Huang and Ellen Riloff. 2011. Peeling back\nthe layers: Detecting event role ﬁllers in secondary\ncontexts. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics:\nHuman Language Technologies , pages 1137–1147,\nPortland, Oregon, USA. Association for Computa-\ntional Linguistics.\nRuihong Huang and Ellen Riloff. 2012. Modeling tex-\ntual cohesion for event extraction. In Twenty-Sixth\nAAAI Conference on Artiﬁcial Intelligence.\nHeng Ji and Ralph Grishman. 2008. Reﬁning event ex-\ntraction through cross-document inference. In Pro-\nceedings of ACL-08: HLT , pages 254–262, Colum-\nbus, Ohio. Association for Computational Linguis-\ntics.\nRobin Jia, Cliff Wong, and Hoifung Poon. 2019.\nDocument-level n-ary relation extraction with mul-\ntiscale representation learning. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies , pages 3693–3704,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nDan Jurafsky and James H. Martin. 2014. Speech and\nlanguage processing. Prentice Hall, Pearson Educa-\ntion International.\nHarold W Kuhn. 1955. The hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2):83–97.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nQi Li, Heng Ji, and Liang Huang. 2013. Joint event\nextraction via structured prediction with global fea-\ntures. In Proceedings of the 51st Annual Meeting\nof the Association for Computational Linguistics ,\npages 73–82, Soﬁa, Bulgaria. Association for Com-\nputational Linguistics.\nXiang Li, Thien Huu Nguyen, Kai Cao, and Ralph Gr-\nishman. 2015. Improving event detection with ab-\nstract meaning representation. In Proceedings of\nthe First Workshop on Computing News Storylines ,\npages 11–15, Beijing, China. Association for Com-\nputational Linguistics.\nShasha Liao and Ralph Grishman. 2010. Using doc-\nument level cross-event inference to improve event\nextraction. In Proceedings of the 48th Annual Meet-\ning of the Association for Computational Linguistics,\npages 789–797, Uppsala, Sweden. Association for\nComputational Linguistics.\n(LDC) Linguistic Data Consortium. 2005.\nEnglish annotation guidelines for events.\nhttps://www.ldc.upenn.edu/sites/\nwww.ldc.upenn.edu/files/\nenglish-events-guidelines-v5.4.3.pdf.\nShulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.\nExploiting argument information to improve event\ndetection via supervised attention mechanisms. In\nProceedings of the 55th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1789–\n1798, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nXiao Liu, Heyan Huang, and Yue Zhang. 2019. Open\ndomain event extraction using neural latent variable\nmodels. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 2860–2871, Florence, Italy. Association\nfor Computational Linguistics.\nXiao Liu, Zhunchen Luo, and Heyan Huang. 2018.\nJointly multiple events extraction via attention-\nbased graph information aggregation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1247–1256,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219–3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\n644\nYi Luan, Dave Wadden, Luheng He, Amy Shah, Mari\nOstendorf, and Hannaneh Hajishirzi. 2019. A gen-\neral framework for information extraction using dy-\nnamic span graphs. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3036–3046, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nXiaoqiang Luo. 2005. On coreference resolution per-\nformance metrics. In Proceedings of Human Lan-\nguage Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing,\npages 25–32, Vancouver, British Columbia, Canada.\nAssociation for Computational Linguistics.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using LSTMs on sequences and tree\nstructures. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1105–1116, Berlin, Germany. Association for\nComputational Linguistics.\nMUC-4. 1992. Fourth message understanding confer-\nence (MUC-4). In Proceedings of FOURTH MES-\nSAGE UNDERSTANDING CONFERENCE (MUC-\n4), McLean, Virginia.\nJames Munkres. 1957. Algorithms for the assignment\nand transportation problems. Journal of the society\nfor industrial and applied mathematics, 5(1):32–38.\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-\nishman. 2016. Joint event extraction via recurrent\nneural networks. In Proceedings of the 2016 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 300–309, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nThien Huu Nguyen and Ralph Grishman. 2015. Event\ndetection and domain adaptation with convolutional\nneural networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing , pages 365–371,\nBeijing, China. Association for Computational Lin-\nguistics.\nSiddharth Patwardhan and Ellen Riloff. 2009. A uni-\nﬁed model of phrasal and sentential evidence for in-\nformation extraction. In Proceedings of the 2009\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 151–160, Singapore. Asso-\nciation for Computational Linguistics.\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph LSTMs. Trans-\nactions of the Association for Computational Lin-\nguistics, 5:101–115.\nBeth M. Sundheim. 1991. Overview of the third Mes-\nsage Understanding Evaluation and Conference. In\nThird Message Uunderstanding Conference (MUC-\n3): Proceedings of a Conference Held in San Diego,\nCalifornia, May 21-23, 1991.\nBeth M. Sundheim. 1993. The Message Under-\nstanding Conferences. In TIPSTER TEXT PRO-\nGRAM: PHASE I: Proceedings of a Workshop held\nat Fredricksburg, Virginia, September 19-23, 1993 ,\npages 5–5, Fredericksburg, Virginia, USA. Associa-\ntion for Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing (EMNLP-\nIJCNLP), pages 5784–5789, Hong Kong, China. As-\nsociation for Computational Linguistics.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. Ace 2005 multilin-\ngual training corpus. Linguistic Data Consortium,\nPhiladelphia, 57.\nHai Wang and Hoifung Poon. 2018. Deep probabilis-\ntic logic: A unifying framework for indirect super-\nvision. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1891–1902, Brussels, Belgium. Association\nfor Computational Linguistics.\nBishan Yang and Tom M. Mitchell. 2016. Joint extrac-\ntion of events and entities within a document context.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 289–299, San Diego, California. Association\nfor Computational Linguistics.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019. DocRED: A large-scale\ndocument-level relation extraction dataset. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 764–777,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTongtao Zhang, Heng Ji, and Avirup Sil. 2019. Joint\nentity and event extraction with generative adversar-\nial imitation learning. Data Intelligence , 1(2):99–\n120.\nYue Zhao, Xiaolong Jin, Yuanzhuo Wang, and Xueqi\nCheng. 2018. Document embedding enhanced event\ndetection with hierarchical and supervised attention.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 414–\n419, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nA Appendices\nA.1 CEAF-REE metric\nNotations First we provide the necessary nota-\ntions. Let reference (gold) role-ﬁller entities of one\nrole in a document dbe:\nR(d) = {Ri : i= 1,2,..., |R(d)|}\nand predicted role-ﬁller entities be:\nS(d) = {Si : i= 1,2,..., |S(d)|}\nLet mbe the smaller one of |R(d)|and |S(d)|,\ni.e., m = min(|R(d)|,|S(d)|). Let Rm ⊂ R\nand Sm ⊂S be any subsets with mentities. Let\nG(Rm,Sm) be the set of one-to-one entity maps\nfrom Rm to Sm, and Gm be the set of all possible\none-to-one maps (of size-m) between subsets of R\nand S. Obviously, we have G(Rm,Sm) ∈Gm.\nThe similarity function φ(r,s) measures the\n“similarity” between two entities. It takes non-\nnegative values: zero-value means role-ﬁller entity\nris not subset of s.\nφ(r,s) =\n{\n1, if s⊆r\n0, otherwise\nCalculating CEAF-REE score Next we present\nhow to calculate the CEAF-REE score. Given the\ndocument d, for a certain event role (e.g., TAR-\nGET ), with its gold entities Rand system predicted\nentities S, we ﬁrst ﬁnd the best alignment g∗by\nmaximizing the total similarity Φ (maximum bi-\npartite matching algorithm is applied in this step):\ng∗= arg max\ng∈Gm\nΦ(g) = arg max\ng∈Gm\n∑\nr∈Rm\nφ(r,g(r))\nLet R∗\nm and S∗\nm = g∗(R∗\nm) denote the gold\nand predicted role-ﬁller entity subset (respectively),\nwhere best matching g∗is obtained. Then the max-\nimum total similarity is,\nΦ(g∗) =\n∑\nr∈R∗m\nφ(r,g∗(r))\nWe can also calculate the entity self-similarity\nwith φ. Finally, we calculate the precision, recall\nand F-measure for CEAF-REE as follows:\nprec= Φ(g∗)∑\ni φ(Si,Si)\nrecall = Φ(g∗)∑\ni φ(Ri,Ri)\nF = 2 ∗prec∗recall\nprec+ recall\nExample 1Case 1:PredictionsGoldPilmai telephone company buildingentity 1:water pipesentity 1:telephone company buildingtelephone company offices\nentity 2:Pilmai telephone company buildingentity2:water pipes\nentity 3:public telephone booth entity3:public telephone booth\nentity 4:telephone company offices\nExample 2Case 2:PredictionsGoldPilmai telephone company buildingentity 1:Pilmai telephone company buildingentity 1:telephone company buildingtelephone company offices\nentity 2:water pipesentity2:water pipes\nentity 3:public telephone booth entity3:public telephone booth\nExample 3Case 3:PredictionsGoldPilmai telephone company buildingentity 1:Pilmai telephone company buildingentity 1:telephone company buildingtelephone company offices\nentity2:water pipes\nentity 2:public telephone booth entity3:public telephone booth\nFigure 6: Test cases for the CEAF-REE metric\n(The lines between entities denote the best align-\nment/matching).\nPrecision Recall F1\ncase 1 0.75 1.00 0.86\ncase 2 1.00 1.00 1.00\ncase 3 1.00 0.67 0.80\nTable 8: CEAF-REE scores for case-{1,2,3}.\nWe list several cases (Figure 6) and their CEAF-\nREE scores (Table 8) to facilitate understanding.\nFor more details, readers can refer to Section 2\nof Luo (2005).\nA.2 Others\nCode and Computing We use the NVIDIA\nTITAN Xp GPU for our computing infrastructure.\nWe build our model based on the Hugging-face\nNER models’ implementation https://github.\ncom/huggingface/transformers/tree/\n3ee431dd4c720e67e35a449b453d3dc2b15ccfff/\nexamples/ner. The hyperparameters can also be\nobtained from the default values in the repo.\nLink to Corpus The raw corpus and preprocess-\ning script can be found at: https://github.com/\nbrendano/muc4_proc\nDependencies\n•Python 3.6.10\n•Transformers: transformers 2.4.1 installed\nfrom source.\n•Pytorch-Struct: Install from Github.\n•Pytorch-Lightning==0.7.1\n•Pytorch==1.4.0",
  "topic": "Coreference",
  "concepts": [
    {
      "name": "Coreference",
      "score": 0.8741506338119507
    },
    {
      "name": "Computer science",
      "score": 0.7761402726173401
    },
    {
      "name": "Transformer",
      "score": 0.6949746608734131
    },
    {
      "name": "Noun phrase",
      "score": 0.6347535848617554
    },
    {
      "name": "Natural language processing",
      "score": 0.6304432153701782
    },
    {
      "name": "Generative grammar",
      "score": 0.588265597820282
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5281496047973633
    },
    {
      "name": "Sentence",
      "score": 0.5168014764785767
    },
    {
      "name": "Generative model",
      "score": 0.45636576414108276
    },
    {
      "name": "Entity linking",
      "score": 0.45438042283058167
    },
    {
      "name": "Noun",
      "score": 0.38393744826316833
    },
    {
      "name": "Resolution (logic)",
      "score": 0.223170667886734
    },
    {
      "name": "Knowledge base",
      "score": 0.1378622055053711
    },
    {
      "name": "Engineering",
      "score": 0.08710363507270813
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}