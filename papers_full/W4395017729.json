{
  "title": "Self-Supervised Graph Transformer for Deepfake Detection",
  "url": "https://openalex.org/W4395017729",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1991981603",
      "name": "Aminollah Khormali",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4222607836",
      "name": "Jiann Shiun Yuan",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6769705151",
    "https://openalex.org/W3034900344",
    "https://openalex.org/W3034301684",
    "https://openalex.org/W3034713808",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W3092879151",
    "https://openalex.org/W3169588269",
    "https://openalex.org/W3203673582",
    "https://openalex.org/W3194007113",
    "https://openalex.org/W3036198682",
    "https://openalex.org/W3175342695",
    "https://openalex.org/W3034196597",
    "https://openalex.org/W3176241004",
    "https://openalex.org/W3108854358",
    "https://openalex.org/W3094728142",
    "https://openalex.org/W3174508664",
    "https://openalex.org/W4221045030",
    "https://openalex.org/W3183392865",
    "https://openalex.org/W3108281670",
    "https://openalex.org/W3034577585",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3035282577",
    "https://openalex.org/W3154326567",
    "https://openalex.org/W2913399670",
    "https://openalex.org/W4214680478",
    "https://openalex.org/W3158766194",
    "https://openalex.org/W2907295878",
    "https://openalex.org/W3035063907",
    "https://openalex.org/W6762480454",
    "https://openalex.org/W4317796367",
    "https://openalex.org/W4382407365",
    "https://openalex.org/W3034864980",
    "https://openalex.org/W6775642233",
    "https://openalex.org/W3183999072",
    "https://openalex.org/W3174656926",
    "https://openalex.org/W3174814557",
    "https://openalex.org/W4322576470",
    "https://openalex.org/W4224918448",
    "https://openalex.org/W4289950749",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W6786093290",
    "https://openalex.org/W3168822201",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3200129610",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W6810495895",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6779032261",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W2301937176",
    "https://openalex.org/W2942074357",
    "https://openalex.org/W6771956660",
    "https://openalex.org/W3083246145",
    "https://openalex.org/W6756046522",
    "https://openalex.org/W3196204467",
    "https://openalex.org/W4312472072",
    "https://openalex.org/W4214691743",
    "https://openalex.org/W4312753047",
    "https://openalex.org/W2962958939",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2891145043",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2963684180",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6769741248",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3206817819",
    "https://openalex.org/W2911424785",
    "https://openalex.org/W6783528232",
    "https://openalex.org/W2909336075",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4288090950",
    "https://openalex.org/W2945262873",
    "https://openalex.org/W4221161777",
    "https://openalex.org/W2981523288"
  ],
  "abstract": "Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model&#x2019;s decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset &#x0026; cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.1 120000\nSelf-Supervised Graph Transformer for\nDeepfake Detection\nAMINOLLAH KHORMALI1, (Member, IEEE), and JIANN-SHIUN YUAN2, (Senior Member, IEEE)\n1 The Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA (e-mail: Aminollah.Khormali@ucf.edu)\n2 The Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA (e-mail: Jiann-Shiun.Yuan@ucf.edu)\nCorresponding author: Jiann-Shiun Yuan (e-mail: Jiann-Shiun.Yuan@ucf.edu).\nABSTRACT Deepfake detection methods have shown promising results in recognizing forgeries within a\ngiven dataset, where training and testing take place on the in-distribution dataset. However, their performance\ndeteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection\nsystem must remain impartial to forgery types, appearance, and quality for guaranteed generalizable\ndetection performance. Despite various attempts to enhance cross-dataset generalization, the problem\nremains challenging, particularly when testing against common post-processing perturbations, such as\nvideo compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a\nself-supervised pre-training model that delivers exceptional generalization ability, withstanding common\ncorruptions and enabling feature explainability. The framework comprises three key components: a feature\nextractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning\nmethodology, a graph convolution network coupled with a Transformer discriminator, and a graph Trans-\nformer relevancy map that provides a better understanding of manipulated regions and further explains the\nmodel‚Äôs decision. To assess the effectiveness of the proposed framework, several challenging experiments\nare conducted, including in-data distribution performance, cross-dataset & cross-manipulation generaliza-\ntion, and robustness against common post-production perturbations. The results achieved demonstrate the\nremarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-\nart approaches.\nINDEX TERMSDeepfake Detection, Graph Convolution Networks, Self-Supervised Contrastive Learning,\nVision Transformer\nI. INTRODUCTION\nT\nHe rise of Artificial Intelligence (AI) empowered face\nmanipulation/generation technologies have enabled in-\ndividuals‚Äô expressions or appearances to be realistically al-\ntered with minimal expert knowledge [1]‚Äì[6]. This technol-\nogy poses a severe and large-scale societal threat, as it fa-\ncilitates the creation and dissemination of malicious content,\nsuch as digital kidnapping, ransomware, and other forms of\ncriminal activity [7], [8] that are among the most insidious\nforms of misinformation [9]. Consequently, the development\nof reliable deepfake detection methods has become an urgent\nneed and has garnered significant attention in recent years.\nAlthough several efforts have been made to defend against\nthe growing threat of forged digital media, the effectiveness\nof these approaches have primarily been limited to in-dataset\nsettings [5], [10]‚Äì[17]. Therefore, the need to develop more\nrobust and effective deepfake detection methods that can op-\nerate in real-world scenarios irrespective of the forgery type,\nappearance, or quality has become increasingly pressing.\nIn the domain of deepfake detection, previous investiga-\ntions have predominantly relied upon low-level texture fea-\ntures to capture common artifacts that are inherent to the\ngeneration process. Nonetheless, such approaches are sus-\nceptible to severe performance degradation when applied to\nnovel types of forgeries, rendering the detection of real-world\ndeepfakes a challenging task, especially when the differences\nbetween genuine and manipulated media are nuanced. As\na result, an effective deepfake detection model ought to be\nimpartial towards forgery type, appearance, and quality, in\norder to ensure applicability in real-world scenarios. To this\nend, several techniques have been proposed to enhance the\nperformance and generalization of deepfake detectors, such\nas targeting the blending boundary between the background\nand the altered face [12], utilizing 3D decomposition [18],\ntruncating classifiers [19], amplifying multi-band frequen-\ncies [14], and augmenting data [20]. Despite their effective-\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nness in cross-data evaluations, low-level texture cues may be\nvulnerable to degradation through standard post-processing\nprocedures, such as video compression [11]. Consequently,\nit is imperative to develop novel high-level features that are\ninvariant to image manipulations and exhibit resilience to\npost-processing manipulations to enhance the current state-\nof-the-art in deepfake detection.\nMoreover, The widely-used convolutional neural network\nand Transformer treat the image as a grid or sequence struc-\nture, which is not flexible to capture irregular and complex\nfacial landmarks. Instead of the regular grid or sequence\nrepresentation, this work processes the deepfake image in a\nmore flexible way. Human facial images can be viewed as\na composition of parts, such as eyes, ears, nose, etc. These\nfacial regions naturally form a graph structure. This graph\nrepresentation of deepfakes offers a better understanding\nof local pixels and their interconnection with other parts.\nMoreover, the graph is a generalized data structure that grid\nand sequence can be viewed as a special case of the graph.\nViewing an image as a graph is more flexible and effective\nfor visual perception.\nGiven the limitations of existing methods, building a de-\ntection model that capitalizes on high-level visual represen-\ntations, which tend to be more resilient to post-processing\nperturbations, could potentially mitigate the dependence on\ndataset-specific patterns, leading to improved recognition\nperformance across various forgery types. In light of this, this\nstudy first proposes a self-supervised pre-training approach\nfor deepfake facial representation learning. This approach\nutilizes a contrastive learning framework based on masked\nimage modeling to extract high-level visual representations\nof facial landmarks that are invariant to variations in lighting,\npose, and other factors that are irrelevant to the identity of the\nobject or scene. The contrastive learning method maximizes\nthe agreement between two different augmentations of the\nsame image using a contrastive loss in the latent space [21]‚Äì\n[23]. This means that the learned representation is not biased\ntowards any particular class or label, but rather captures the\nunderlying structure of the input data.\nMoreover, the present work offers an innovative approach\nthat integrates the unique characteristics of Graph Convo-\nlutional Networks (GCNs) and Transformer architectures to\ncapture complex dependencies between distinct regions of\nan image and acquire more informative representations for\ndeepfake detection. In the context of deepfake detection,\nthe input data can be projected as a graph, wherein nodes\ndenote various regions of the image, such as eyes or ears, and\nedges represent the association between these regions. This is\nparticularly useful in detecting deepfakes, where only certain\nregions of the image may be manipulated to create realistic\nbut false identities. The utilization of GCNs allows for mod-\neling local relationships between nodes in the input graph,\nproviding the ability to learn complex features that capture\nthe spatial correlations between different facial landmarks.\nIn contrast, Transformers exhibit exceptional efficacy in en-\ncoding long-range correlations and global interdependencies\nbetween pixels, rendering them especially appropriate for\ndeepfake detection tasks. Given that adversarial entities can\nmanipulate multiple regions of an image concurrently, the\nability to model such complex relationships is crucial for\nreliable detection. Finally, this work introduces a graph Trans-\nformer relevancy map that contributes valuable insights into\nthe model‚Äôs explainability. By generating a saliency map that\nhighlights the importance of individual regions towards the\noutput class label, this component facilitates a more fine-\ngrained analysis of the model‚Äôs decision-making process and\nallows for a better understanding of the specific features and\nregions of the image that are crucial for deepfake detec-\ntion. Figure 1 shows the general framework of the proposed\ndeepfake detection model using a self-supervised contrastive\nlearning approach and graph Transformer architecture.\nContributions. Together, these contributions offer a promis-\ning solution to the urgent need for robust and effective deep-\nfake detection methods that can operate irrespective of the\nforgery types, appearances, or qualities in real-world scenar-\nios. The proposed framework‚Äôs use of high-level visual repre-\nsentations that are invariant to post-processing perturbations,\ncombined with the ability to capture complex interdependen-\ncies between regions of an image, provides a comprehensive\nsolution that outperforms existing methods. Below are the key\ncontributions of this study:\n‚Ä¢ The present study introduces a self-supervised pre-\ntraining approach based on contrastive learning to ex-\ntract more high-level visual representations that are in-\nvariant to variations in compression, blur, and other\nfactors irrelevant to the subject‚Äôs identity. This approach\nsignificantly enhances the generalizability of deepfake\ndetection models, thereby reducing their dependence on\ndataset-specific patterns.\n‚Ä¢ Furthermore, this paper proposes an innovative deepfake\ndetection approach by leveraging the expressive power\nof graph convolutional networks and Transformers to\ncapture intricate interdependencies among different re-\ngions of an image and acquire more informative repre-\nsentations. Unlike traditional convolutional neural net-\nworks, which are limited in their ability to model non-\nlocal relationships between pixels, the proposed graph\nTransformer can encode both local and global depen-\ndencies, rendering them particularly suited to deepfake\ndetection tasks.\n‚Ä¢ By providing detailed and granular insights into the\nunderlying reasoning behind the model‚Äôs predictions,\nthe proposed graph Transformer relevancy map offers a\nmore thorough and rigorous understanding of the com-\nplex interdependencies and salient features that drive\nthe detection process, while suppressing irrelevant or\nredundant information. Thus, facilitating a rigorous and\naccurate examination of deepfakes, contributing to en-\nhanced detection performance and increased reliability.\n‚Ä¢ The proposed framework‚Äôs efficacy and generalizability\nwere rigorously evaluated via a comprehensive set of\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nexperiments, encompassing a diverse range of challeng-\ning scenarios spanning both in-distribution and out-of-\ndistribution settings. The experimental results unequiv-\nocally establish the framework‚Äôs superiority, with ex-\nceptional in-dataset detection accuracy being achieved.\nMoreover, the proposed self-supervised pre-training fea-\nture extractor constitutes a significant contribution to\nthe field, having markedly improved the framework‚Äôs\nability to generalize across multiple datasets while si-\nmultaneously enhancing its resilience to post-processing\nperturbations, such as compression and blur etc.\nOrganization. The rest of the paper is structured as follows.\nIn ¬ßII, a review of recent research at the intersection of\ndeepfake detection and self-supervised contrastive learning\nis provided. The proposed self-supervised graph Transformer\ndeepfake detection approach, including its key components\nand the rationale for their selection, is outlined in ¬ßIII.\nThe evaluation settings, including datasets, implementation\nspecifics, and evaluation metrics, are described in ¬ßIV. In ¬ßV,\nthe obtained deepfake detection results and their implica-\ntions are presented and discussed in comparison with existing\nmethods. Finally, in ¬ßVI, concluding remarks are drawn to\nsummarize the contributions of this study and outline avenues\nfor future research.\nII. RELATED WORK\nA. DEEPFAKE DETECTION\nRecent years have witnessed a growing body of research\naimed at addressing the pressing challenges posed by deep-\nfakes and developing robust detection models capable of\nthwarting their nefarious effects on society. A variety of\ntechniques have been proposed in the literature to this end,\nwith a rich diversity of approaches being explored [8], [10]‚Äì\n[13], [16], [17], [24].\nOne popular strategy is to leverage the implicit visual arti-\nfacts of deepfakes‚Äô generation process to construct a reliable\ndetection framework [12], [25]. These approaches capitalize\non the inconsistencies and discrepancies that inevitably arise\nduring the creation of deepfakes, such as blending artifacts\nor anomalies in the frequency spectrum. Examples of such\napproaches include Patch-wise Consistency Learning [26]\nand Face X-ray [12], which exploit image blending incon-\nsistency traces between the manipulated face region and the\nbackground. These approaches exemplify the diverse range\nof methods used to detect deepfakes, with some leveraging\nspecific visual artifacts while many mainstream deepfake de-\ntection methodologies rely heavily on the versatility of convo-\nlutional neural networks. For instance, [5], [8], [27], [28] rely\non the exceptional ability of convolutional neural networks\nto capture complex spatial features and relationships as the\nprimary building block of their proposed deepfake detection\nframework.\nAdditionally, recent research has explored alternative av-\nenues, such as analyzing the temporal dynamics of videos or\nusing frequency spectrum analysis to detect deepfakes based\non unique spectral signatures. For instance, the temporal dy-\nnamics of videos have been explored as a means to recongnize\nfake videos [29]‚Äì[32]. This approach involves examining the\ndirectional changes in the video‚Äôs temporal characteristics,\nthereby detecting any inconsistencies or abnormal patterns\nindicative of deepfakes. Furthermore, frequency spectrum\nanalysis is gaining significant traction in research interest\n[13]‚Äì[15], [33]‚Äì[36]. This technique capitalizes on leveraging\nthe identification of spectral signatures exhibited by deep-\nfakes, such as the existence of high-frequency noise or low-\npass filtering artifacts, enabling the detection of manipulated\ncontent.\nIn light of recent advancements in deepfake generation\ntechniques that have reduced the visual anomalies, the detec-\ntion methods have shifted focus towards more sophisticated\napproaches such as attention mechanisms [8], [16], [29], [37],\n[38] and vision Transformers [17], [24], [39]. For example,\nWang et al. [24] proposed a multi-regional attention mecha-\nnism to enhance deepfake detection performance. Addition-\nally, vision Transformers have been employed to establish\nan end-to-end deepfake detection framework [17], [40]. Fur-\nthermore, recent research has utilized pre-trained networks\nand developed Lipforensics [11] for analyzing lip prints in\nlip-reading tasks. This work highlights that while a robust\npre-trained lip feature extractor could have far-reaching im-\nplications, acquiring a large-scale and well-annotated dataset\ncould be a daunting task. Given the potential limitations of re-\nlying on annotated data, this study proposes a self-supervised\nvision Transformer for pretraining the feature extractor with-\nout the need for annotation or labels. The resulting framework\nis scalable and achieves good performance.\nB. SELF-SUPERVISED CONTRASTIVE LEARNING\nA meaningful visual representation of a given input image\ncould be learned through self-supervised contrastive learn-\ning methods without relying on labeled data [41]. Recently,\nself-supervised contrastive learning approaches have gained\ntremendous attention in building pre-trained models that can\nbe used for fine-tuning different downstream tasks. Con-\ntrastive learning maximizes the similarity between positive\npairs while repelling features of negative pairs [21]. Although\ncontrastive learning approaches have been utilized for dense\nvisual representation learning tasks [42], [43], recent studies\nsuggest that removing negative pairs yields better perfor-\nmances [44]‚Äì[47]. While Random augmentations have been\nwidely used to generate diverse views of the same image [21],\n[48], [49], this work, inspired by [50], proposes a masked\nimage modeling approach to extract more robust semantic\nfeatures through a contrastive learning methodology. In con-\ntrast to previous work that only focused on mouth regions for\nlip-forensics [51], this work leverages the entire facial region\nto improve the detection performance. The proposed masked\nimage modeling approach was utilized to train a pre-trained\nmodel, with a vision Transformer serving as its backbone.\nThrough this approach, complex semantic features from input\nimages were captured by the model, resulting in improved\ndeepfake detection performance.\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nFIGURE 1. The general framework of the proposed deepfake detection model using a self-supervised contrastive learning approach and graph\nTransformer architecture. In this approach, each input image is partitioned into smaller patches, where each patch is considered as a node in an\nundirected graph. A feature extractor model based on vision transformer architecture, which is pre-trained based on contrastive learning methodology, is\nthen applied to each patch to obtain its visual features. These nodes and associated feature vectors are restructured in the form of a graph based on the\nadjacency matrix. The constructed graph is subsequently fed into a graph convolutional network, which is followed by a Transformer network that\nfunctions as a deepfake discriminator, with the purpose of forecasting image class labels. Finally, the Transformer relevancy map is utilized to identify\npotentially manipulated facial regions. The suggested deepfake detection mechanism capitalizes on the merits of both high-level visual characteristics\nand the interconnectivity of local and global regions through the pre-trained feature extractor and graph Transformer classifier, respectively.\nIII. METHODOLOGY\nThe proposed deepfake detection method entails four fun-\ndamental components, as illustrated in Figure 1, which are\ndeepfake graph construction, deepfake encoder based on vi-\nsion Transformer architecture that is pre-trained using a self-\nsupervised contrastive learning approach, graph Transformer\nclassifier, and graph Transformer relevancy map. This section\nis dedicated to elaborating on each of these components in a\nmore comprehensive manner.\nA. DEEPFAKE GRAPH REPRESENTATION\nThe irregularity of facial regions‚Äô shape constitutes a major\nobstacle in employing conventional techniques, such as grid-\nbased architectures in CNNs or sequence-based structures\nin Transformers, to process facial images. Such approaches\nare characterized by redundancy and inflexibility, rendering\nthem incapable of efficiently addressing the complex struc-\ntural nuances of facial regions. As a consequence, there is\na need for more sophisticated and adaptive techniques that\ncan account for the varying geometries and topologies of\nthese regions. In this regard, the present study advocates the\nadoption of graph representation of deepfakes as a highly\npromising method to tackle the issue of deepfake detection,\nowing to its capacity to account for the varying topologies and\ngeometries of the facial regions. By leveraging this approach\nto represent deepfake images as graphs, it becomes possible\nto capture the underlying structural and relational features of\nkey facial landmarks, such as the eyes, lips, and ears. This\ncapability, in turn, enables the identification of potentially\nmanipulated regions and their relationship within the image,\nthereby capturing more complex features for enhanced and\nrobust deepfake detection.\nThe construction of an undirected graph from an image\ninput is a pivotal step in the proposed deepfake detection\nmethodology. In essence, the process of representing a given\ninput image I as a graph G = ( V ,E) involves partitioning\nit into N patches, where V = {v1,v2, . . . ,vN } signifies the\nnodes of the graph and E denotes its corresponding edges.\nThis entails representing feature embeddings extracted from\nimage patches as graph nodes, denoted by vi ‚àà RD, where D\nis the dimensionality of the embedding vector. The adjacency\nrelationship among these patches constitutes the graph edges\nE. Notably, edges within the graph are established by means\nof a spatial proximity criterion that determines the association\nof each patch with its neighboring counterparts, allowing for\nat least one, and up to K connections between K nearest\nneighboring patches. To encode the adjacency relationship\nbetween patches, an adjacency matrix, A = [ Aij], is formu-\nlated, is formulated, where Aij = Aji = 1 if the nodes vi and\nvj are in the neighborhoods, and Aij = Aji = 0 otherwise,\nthat is if (vi,vj) /‚àà E. By modeling deepfake images in this\nway, the structural relationships and dependencies among the\nvarious facial components can be captured and analyzed in a\nmore comprehensive manner. Furthermore, the efficacy of the\nproposed methodology depends crucially on the feature vec-\ntor‚Äôs ability to provide a potent and resilient representation of\nthe node in question. To achieve this, the present study utilizes\nself-supervised contrastive learning and a vision Transformer\nstructure to extract high-level visual representations of deep-\nfake images. This enables the detection system to capture the\nsalient attributes of the underlying image regions, resulting in\nmore informative and discriminative features. As high-level\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nfeatures are less vulnerable to manipulations, this approach\nmaximizes the model‚Äôs potential to generalize to new data and\nperform well on downstream tasks.\nB. SELF-SUPERVISED CONTRASTIVE LEARNING\nExisting deepfake detection methodologies heavily rely on\nweakly supervised learning approaches for visual feature ex-\ntraction. While such approaches exhibit high detection perfor-\nmance on in-distribution samples, their ability to generalize to\nunseen samples is limited. This generalizability issue poses\na significant challenge to real-world applications where the\nmodel must be robust to unseen scenarios. To overcome this\nissue, this study proposes the utilization of self-supervised\ncontrastive learning for robust visual feature extraction with-\nout the need for explicit labels. Specifically, the proposed\napproach employs masked image modeling, which applies\na random mask to an input image with N token sequences\nand a prediction ratio of r, to train a vision Transformer for\ngenerating patch representations. The model maximizes the\nconcordance between two distinct views of the same image\nthrough contrastive loss in a latent space. This approach\nis inspired by recent studies on self-supervised contrastive\nlearning, including [21], and builds on the successful applica-\ntion of masked image modeling in [50], [52]. Figure Figure 2\nillustrates the proposed self-supervised contrastive learning\nframework for deepfake representation learning. The result-\ning visual features are expected to be more informative and\ndiscriminative, improving the model‚Äôs ability to generalize\nto unseen data and achieve high performance on downstream\ntasks.\nThe proposed training methodology commences with the\nrandom selection of an image x from the training set, followed\nby the application of random augmentations to generate two\ndistorted views of the same image, denoted as u and v. To en-\nhance robustness, blockwise masking [50] is then applied to\nproduce masked views of the augmented images, denoted as\nÀÜu and ÀÜv. Subsequently, the masked views undergo processing\nby a student-teacher network to yield predictive categorical\ndistributions. Both the student and teacher networks share\nan identical architectural configuration and are parameterized\nby Œ∏ and Œ∏‚Ä≤, respectively. This framework enables the self-\nsupervised training of the student network, which endeavors\nto reconstruct the masked tokens by leveraging the guidance\nof the teacher network‚Äôs output.\nIn self-supervised frameworks, it is customary for the stu-\ndent network to acquire distilled knowledge from the teacher\nnetwork by minimizing the cross-entropy loss [22], [44], [46].\nHowever, in the present study, the deepfake images‚Äô visual\nrepresentations are acquired through the minimization of two\nconcurrent loss functions, namely: 1) L[CLS], which repre-\nsents the self-distillation between cross-view [CLS] tokens,\nand 2) L[MIM], which represents the self-distillation be-\ntween in-view patch tokens. Specifically, the self-distillation\nprocess regarding the [CLS] tokens of cross-view images,\ndenoted by u and v, can be formulated as a symmetric loss, as\nshown in Equation 1.\nL[CLS] = 1\n2\n\u0010\n‚àíP[CLS]\nŒ∏‚Ä≤ (v)T log P[CLS]\nŒ∏ (u)\n\u0011\n+\n1\n2\n\u0010\n‚àíP[CLS]\nŒ∏‚Ä≤ (u)T log P[CLS]\nŒ∏ (v)\n\u0011 (1)\nwhere uCLS\ns = PCLS\nŒ∏ (u), vCLS\ns = PCLS\nŒ∏ (v), uCLS\nt =\nPCLS\nŒ∏‚Ä≤ (u), and vCLS\nt = PCLS\nŒ∏‚Ä≤ (v) are student and teacher\nnetwork outputs for cross-view images, i.e.,u and v, of [CLS]\ntoken.\nThe student and teacher network outputs for the masked\nview with a given mi masking ratio for ÀÜu, ÀÜv and non-\nmasked view u, v projections of patch tokens are calculated\nas ÀÜupatch\ns = Ppatch\nŒ∏ (ÀÜu), ÀÜvpatch\ns = Ppatch\nŒ∏ (ÀÜv) and upatch\nt =\nPpatch\nŒ∏‚Ä≤ (u), vpatch\nt = Ppatch\nŒ∏‚Ä≤ (v), respectively. Similarly, the\nself-distillation between in-view patch tokens can be formu-\nlated as a symmetric cross-entropy loss as Equation 2.\nLMIM = = 1\n2\n \n‚àí\nNX\ni=1\nmi ¬∑ Ppatch\nŒ∏‚Ä≤ (ui)T log Ppatch\nŒ∏ (ÀÜui)\n!\n+\n1\n2\n \n‚àí\nNX\ni=1\nmi ¬∑ Ppatch\nŒ∏‚Ä≤ (vi)T log Ppatch\nŒ∏ (ÀÜvi)\n!\n(2)\nThe overall loss function is expressed as a combination\nof two individual loss functions, L[CLS] and LMIM, as pre-\nsented in Equation 3. The loss function L[CLS] involves self-\ndistillation between cross-view [CLS] tokens, whereas LMIM\nis related to self-distillation among in-view patch tokens.\nThe proposed approach leverages the collective information\nobtained from both loss functions to effectively learn the\ndeepfake image representations.\nL = L[CLS] + LMIM (3)\nC. GRAPH TRANSFORMER CLASSIFIER\nThe methodology employed in this study involves the use of\na vision Transformer as a feature extractor, which has been\ntrained using a self-supervised contrastive learning approach.\nThis approach is effective in computing the visual represen-\ntation of each node in the graph. An image representation\nmatrix F ‚ààRN√óD is constructed by subjecting all N nodes\nwithin the image to the aforementioned pre-trained ViT fea-\nture extractor. Specifically, each node vi is associated with a\nfeature vector fi ‚àà RD, where D denotes the dimensional-\nity of the feature vector. The resultant matrix is defined as\nF = {f1,f2, . . . ,fN }. It is noteworthy that the combination\nof F with the adjacency matrix A facilitates the construction\nof a graph representation of deepfake images. The resulting\ngraph-structured data can be analyzed using a multi-layer\ngraph convolutional network f (F,A) [53], which utilizes a\nlayer-wise propagation rule as specified in Equation 4.\nHl+1 = ReLU\n\u0010\nÀÜAHl Wl\n\u0011\n, l = 1,2, ..,L\nÀÜA = ÀúD‚àí1\n2 ÀúAÀúD‚àí1\n2\n(4)\nhere, L represents the total number of graph convolution\nlayers, and the activation matrix of the lth graph convolutional\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nùë†ùë°ùë¢ùëëùëíùëõùë°ùëõùëíùë°ùë§ùëúùëüùëò(ùëì!) ‚Ñé!\"#$%&\n‚Ñé![()*]\nùë°ùëíùëéùëê‚Ñéùëíùëüùëõùëíùë°ùë§ùëúùëüùëò(ùëì$)\n‚Ñé$[()*]\n‚Ñé$\"#$%&\n1ùë£!\"#$%&\n1ùë£![()*]\nùë£$[()*]\nùë£$\"#$%&\n1ùë¢!\"#$%&\n1ùë¢![()*]\nùë¢$[()*]\nùë¢$\"#$%&//\n//\n‚Ñí!\"!\nùê∏ùëÄùê¥\nùë£\n‚Ñí[$%&]\nùë¢\nùë•\nùë†ùë°ùëúùëùùëîùëüùëéùëë\n[ùê∂ùêøùëÜ]\nùëÄùêºùëÄ\nFIGURE 2. General framework of the vision Transformer-based feature extractor trained through self-supervised contrastive learning. Given a deepfake\nimage, x, two different views,u and v, are passed through student and teacher networks. The student network tries to reconstruct masked tokens with\nthe supervision of teacher networks‚Äô output by minimizing self-distillation between cross-view [CLS] tokens and self-distillation between in-view patch\ntokens.\nlayer is denoted as Hl . The activation matrix Hl captures the\nlearned representations at each layer, with the initialization of\nH0 as the input feature matrix F. The weight matrix Wl is spe-\ncific to each layer and is learned during training. Moreover,\nthe graph‚Äôs adjacency matrix with added self-connections to\neach node is represented by ÀúA and is defined as the sum of\nthe adjacency matrix A and the identity matrix I. A diagonal\nmatrix ÀúD is used to normalize the adjacency matrix, where ÀúDii\nis defined as the sum of the entries in row i of the normalized\nadjacency matrix ÀúA. The resulting matrix ÀÜA is the symmetric\nnormalized adjacency matrix of A.\nAlthough graph convolutional layers have been successful\nin learning node-level features and have been widely used\nin graph neural networks, they have limitations in learning\nhierarchical visual features that capture the global context\nof the graph. In contrast, the attention mechanism in Trans-\nformer models has shown remarkable success in natural lan-\nguage processing tasks by capturing the long-range depen-\ndencies between tokens and their relative importance for the\nfinal prediction [54]. This ability is particularly important\nin graph-structured deepfake images, where the relationships\nbetween nodes can be complex and far-reaching. Therefore,\nthe present work proposes to leverage attention mechanisms\nto analyze graph-structured data by treating the feature nodes\nas tokens in a sequence, and adjacency matrices as positional\nencodings to preserve their spatial relationship. By synergiz-\ning the attention mechanism with graph convolutional layers,\nthe proposed model proficiently assimilates both localized\nand global features, and adeptly captures intricate inter-nodal\nrelationships within the graph. Notably, the transformation\nof graph space data into Transformer space is enacted via a\nTransformer layer, as specified in Equation 5 to Equation 8.\nz0 =\nh\nxclass; h(1); h(2); . . .; h(N)\ni\n, h(i) ‚àà H (5)\nz‚Ä≤\n‚Ñì = MSA (LN (z‚Ñì‚àí1)) + z‚Ñì‚àí1, ‚Ñì = 1 . . .L (6)\nz‚Ñì = MLP (LN (z‚Ä≤\n‚Ñì)) + z‚Ä≤\n‚Ñì, ‚Ñì = 1 . . .L (7)\ny = LN\n\u0000\nz0\nL\n\u0001\n(8)\nHere, the model architecture entails a multihead self-\nattention mechanism MSA, featuring k Self-Attention (SA)\nheads, as delineated in Equation 10 and Equation 9, respec-\ntively. Furthermore,MLP represents a Multilayer Perceptron,\nand Layer Norm is indicated by LN. Here, L corresponds\nto the number of Transformer blocks, and y signifies the\nclass label [55]. The Transformer model utilizes the graph\nfeature embeddings, as outlined in Equation 4, to facilitate\nself-attention operation SA (Q,K,V ), based on Transformer\narchitecture as presented in Equation 10.\nMSA (Q,K,V ) = Concat (SA1, . . . ,SAk) W O\nwhere SAi = SA\n\u0010\nQW Q\ni ,KW K\ni ,VW V\ni\n\u0011 (9)\nSA (Q,K,V ) = softmax\n\u0012QKT\n‚àödk\n\u0013\nV (10)\nwhere, N represents the number of patches, D is the di-\nmension of patch embeddings, and SA (Q,K,V ) denotes the\npairwise similarity of two nodes based on their correspond-\ning Query (Qin), Key (K), and Values (V ). Notably, the\nprojections are parameter matrices W Q\ni ‚àà RD√ódk ,W K\ni ‚àà\nRD√ódk ,W V\ni ‚àà RD√ódv and W O ‚àà Rkdv√óD [54], [55].\nFinally, it is imperative to acknowledge that the conven-\ntional self-attention mechanism computes attention scores for\nall possible pairs of nodes, leading to an exponential increase\nin memory and time complexity as the number of nodes\nincreases, with a complexity of O(n2). Thus, a technique that\nreduces the number of nodes while retaining local informa-\ntion is indispensable. In this investigation, a learnable pooling\nlayer, referred to as the min-cut pooling layer [56], has been\nemployed on the output of the final graph convolutional layer\nto effectively reduce the number of nodes.\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nùëÖ!!Transformer Block ùêø Transformer Block 1\nÃÖùê¥(#)=ùîº%‚àáùê¥&‚®ÄùëÖ!\"+ùêº ÃÖùê¥(')=ùîº%‚àáùê¥'‚®ÄùëÖ!!+ùêº\nùê∂=ÃÖùê¥(')-ÃÖùê¥(-‚Ä¶-ÃÖùê¥(#)\ninput output‚àáùê¥'\nAttention relevance\ntransformer relevance\nFIGURE 3. Relevancy maps for the Transformer,C, are generated via the\nintegration of attention maps, gradients, and relevance scores, which are\npropagated throughout the network. These maps are subsequently\nconverted into graph class activation maps through reverse pooling, as\nelucidated in Equation 11.\nD. GRAPH TRANSFORMER RELEVANCY MAP\nTo pinpoint areas within a given image that are likely to have\nbeen manipulated, and which exhibit a significant correlation\nwith deepfake class labels, the present study builds upon\nprevious research in this field [57] and presents a graph Trans-\nformer relevancy map. This innovative approach involves\ncomputing the class activation maps from the output class\nand then propagating them to the inner graph space through\na propagation procedure. The current study shows that the\nattention relevancy of the ‚Ñìth Transformer block ¬ØA(‚Ñì) can be\nobtained by representing the attention map of its Transformer\nblock as A‚Ñì, which facilitates the calculation of the layer\nrelevance score Rn‚Ñì and gradient ‚àáA(‚Ñì) values pertaining to\nclass t. It should be noted that n‚Ñì corresponds to the layer\nthat aligns with the softmax operation in the ‚Ñìth Transformer\nblock. Finally, as depicted in Figure 3 the Transformer‚Äôs\nfinal relevancy maps are represented by a weighted attention\nrelevance, as described by Equation 11.\n¬ØA(‚Ñì) = Eh\n\u0010\n‚àáA(‚Ñì) ‚äô R(nl )\n\u0011\n+ I\nCt = ¬ØA(1) ¬∑ ¬ØA(2) ¬∑ . . .¬∑ ¬ØA(L)\n(11)\nHere, Eh signifies the mean value calculated across the\ndimension of the self-attention heads of the Transformer\nmodel, whereas ‚äô denotes the Hadamard product. To pre-\nvent self-inhibition for each node, the identity matrix I is\nadded. In the final step, the Transformer relevance map Ct is\nmapped to each node in the graph, based on the min-cut dense\nlearned assignments [56]. This min-cut approach ensures that\nthe most relevant nodes are assigned the highest relevance\nscores, thereby facilitating more accurate identification of\nmanipulated regions within an image. The proposed method-\nology represents a significant advancement in deepfake detec-\ntion and image forensics, by incorporating a highly effective\ngraph-based approach with Transformer architectures that\nimproves accuracy and enhances practical utility.\nIV. EVALUATION SETTINGS\nThe present section endeavors to appraise the performance\nand efficacy of the proposed methodology through a series\nof challenging experiments, and subsequently compare the\nresults against those achieved by current state-of-the-art tech-\nniques.\nA. DATASETS\nTraining datasets.In accordance with recent developments\nin the field of deepfake detection [12], [13], [17], [35], [36],\nthis study endeavors to evaluate the efficacy and performance\nof the proposed self-supervised graph Transformer model\nagainst different real-world settings through a comprehensive\nset of experiments. To this end, the model is trained and evalu-\nated using several well-established and challenging deepfake\nforensics datasets, including FaceForensics++ [5], Celeb-DF\n(V2) [4], and WildDeepfake [6]. A concise overview of these\ndatasets is provided below.\nFaceForensics++. The FaceForensics++ dataset encom-\npasses a diverse range of four distinct types of im-\nage forgeries, specifically Deepfakes [58], FaceSwap [59],\nFace2Face [60], and NeuralTextures [61]. The FaceForen-\nsics++ dataset is thoughtfully curated and available in three\nvarying compression rates, namely heavily compressed (LQ),\nslightly compressed (HQ), and uncompressed (Raw). While\nthe Raw dataset exhibits relatively facile identification of\ndeepfakes, the process becomes considerably more challeng-\ning with higher compression rates. In this manuscript, the\nHQ version of the dataset is predominantly utilized, unless\nspecified otherwise. Celeb-DF (V2). The Celeb-DF (V2)\ndataset represents a significant advancement in synthetic\nvideo generation due to its superior face-swapping strategy\nthat elevates its visual quality above previous efforts. The\ndataset encompasses an extensive collection of 5639 videos,\nall of which exhibit a high level of visual quality, making it\nan ideal resource for training and testing deepfake detection\nmodels. WildDeepfake. The WildDeepfake is a collection of\nfabricated videos sourced entirely from the Internet, making it\na distinct and demanding real-world dataset. Unlike other AI-\ngenerated deepfakes, such as Celeb-DF (V2) and FaceForen-\nsics++, the WildDeepfake dataset incorporates multiple facial\nexpressions and encompasses a wider range of scenarios, with\nnumerous individuals appearing in each scene.\nTesting datasets.In addition to the aforementioned datasets,\nthe official test set of several additional benchmarks are\nutilized to investigate the generalizability of the proposed\nmodel. Specifically, these benchmarks are the DeeperForen-\nsics [2], FaceShifter [62], and DeepFake Detection Chal-\nlenge (DFDC) [1] datasets. The DeeperForensics dataset\ncontains over 11,000 synthetic videos generated by lever-\naging a specific Variational Auto-Encoder (V AE) on the\nreal videos from FaceForensics++. Similarly, the FaceShifter\ndataset is a collection of videos created by applying ad-\nvanced face-swapping techniques to the real videos from\nthe same dataset. This presents a valuable opportunity to\nassess the robustness and effectiveness of the proposed self-\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nsupervised graph Transformer model in detecting deepfake\nvideos across a wide range of scenarios, including those that\nincorporate different sophisticated manipulation techniques\nbased on the same underlying source videos. In addition, the\nDFDC dataset, comprising over 4,000 manipulated videos\ncreated using various GAN-based and non-learned techniques\nfrom 1,000 real videos, provides a complementary benchmark\nagainst which the proposed method can be evaluated.\nB. IMPLEMENTATION SPECIFICS\nDeepfake Encoder Training.The present study adopts an\narchitecture based on vision Transformers [55] as the fun-\ndamental building block of the feature extractor f , which is\ntrained through the utilization of the previously mentioned\ncontrastive learning approach expounded upon in ¬ßIII-B.\nSpecifically, the feature extractor model undergoes pre-\ntraining and fine-tuning on images of size 320 √ó 320, with\npatch sizes of 20√ó20 yielding a total of 256 patch tokens. The\nprojection head h, which plays a central role in feature extrac-\ntion, is established using a multi-layer perceptron compris-\ning three layers. Importantly, to foster effective information\nsharing between the CLS and the patch tokens, the projection\nhead parameters are shared, notably, hs\n[CLS] = hs\npatch and\nht\n[CLS] = ht\npatch. This enables the feature extractor model\nto capitalize on both the global and local context provided by\nthe CLS and patch tokens, respectively. The training regimen\nis executed for 800 epochs with a learning rate that exhibits\na linearly scaled progression following the batch size, as\ndetermined by the expression: lr = 5e‚àí4 √ó batchsize/256.\nThe preeminent implementation specifications at hand en-\ntail the pre-training of a self-supervised feature extractor\non a conflation of training sets procured from three distinct\nbenchmarks explicated in ¬ßIV-A.\nUpon the conclusion of training, the parameters of the fea-\nture extractor are fixed and employed as a pre-trained feature\nextractor for the purpose of node feature vector extraction,\nwhich in turn facilitates the representation of the deepfake\nimages as a graph. The graph Transformer classifier is com-\nprised of a graph convolutional layer, and three blocks of\nTransformer layers, each of which comprises 8 self-attention\nheads, a dimensional parameter of D=256, and an MLP size\nof 512 (Equation 10). This construction is indicative of the\nsophistication that underlies the proposed approach.\nThe experimental evaluation of the proposed framework\nwas carried out on a Lambda Quad deep learning work-\nstation machine equipped with 4 NVIDIA GeForce GTX\n1080 Ti Graphics Processing Units (GPUs), 64 GB DDR4\nRAM, an Intel Core‚Ñ¢ i7-6850K CPU, and running the Ubuntu\n20.04.3 LTS operating system. This machine configuration\nfacilitated the efficient training and evaluation of the models,\nthanks to its high-performance computing capabilities. The\nperformance evaluation of the proposed framework was con-\nducted using accuracy and area under the receiver operating\ncharacteristic curve (AUC) metrics in different experimental\nsettings. Comprehensive assessment of the proposed frame-\nwork‚Äôs performance was enabled through the use of these\nTABLE 1. Performance of the proposed self-supervised graph\nTransformer deepfake detection model using the in-dataset setting.\nDataset ACC (%) AUC (%)\nFaceForensics++\n(Raw) 99.38 99.96\n(HQ) 98.41 99.34\n(LQ) 94.59 95.16\nCeleb-DF (V2) 99.47 99.43\nWildDeepfake 81.37 81.24\nTABLE 2. Cross-dataset generalization AUC (%) scores. The model is\ntrained over FaceForensics++ and evaluated over a test set of\nDeeperForensics (DFo), FaceShifter (FSh), DeepFake Detection Challenge\n(DFDC), and Celeb-DF (CDF), respectively.\nMethods DFo FSh DFDC CDF Avg.\nXception [5] 84.5 72.0 70.9 73.7 75.3\nCNN-aug [20] 74.4 65.7 72.1 75.6 72.0\nPatch-based [19] 81.8 57.8 65.6 69.6 68.7\nFace X-ray [12] 86.8 92.8 65.5 79.5 81.2\nCNN-GRU [30] 74.1 80.8 68.9 69.8 73.4\nMulti-task [63] 77.7 66.0 68.1 75.7 71.9\nDSP-FWA [64] 50.2 65.5 67.3 69.5 63.1\nLipForensics [11] 97.6 97.1 73.5 82.4 87.7\nFTCN [65] 98.8 98.8 74.0 86.9 89.6\nDFDT [17] 96.9 97.8 76.1 88.3 89.7\nRealForensics [66] 99.3 99.7 75.9 86.9 90.5\nOurs 98.9 99.1 77.3 87.9 90.8\nstringent evaluation metrics, which were employed to com-\npare it with state-of-the-art methods and to provide valuable\ninsights into the practical applicability of the approach.\nV. RESULTS & DISCUSSION\nTo fully ascertain the proficiency of the presented deepfake\ndetection framework, an exhaustive set of experiments was\nconducted, with a multifaceted approach to examine its per-\nformance across various perspectives. The scope of investi-\ngation encompassed the generalization ability of the frame-\nwork in detecting deepfakes originating from heterogeneous\ndatasets and manipulation techniques, as well as its resistance\nagainst compression and perturbations, and other ablation\nstudies that are critical in appraising the framework‚Äôs efficacy.\nTo this end, three distinct models were meticulously trained\nand evaluated in an in-dataset setting using each of the preem-\ninent deepfake forensics datasets, namely FaceForensics++,\nCeleb-DF (V2), and WildDeepfake, with the corresponding\naccuracy rate and AUC scores reported in Table 1.\nA. CROSS DATASET GENERALIZATION\nThe proliferation of fake videos in the real world poses a\nformidable challenge for the deployment of deepfake detec-\ntion models, given the diverse forgery techniques, hetero-\ngeneous source videos, and extensive post-processing tech-\nniques. Therefore, it is crucial to assess the effectiveness of\nany deployed deepfake detection model from a domain gen-\neralization perspective. Although evaluating domain general-\nization is complicated due to significant disparities between\ndeepfake forensics datasets, this can be addressed through\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\ncross-dataset evaluation. This study adopts a similar approach\nto the existing literature by utilizing the training set of the\nFaceForensics++ dataset to construct a deepfake detection\nmodel and then evaluate its performance on other datasets.\nMore specifically, the model is trained on four distinct types\nof forgeries that make up the FaceForensics++ dataset. The\nmodel‚Äôs performance is then evaluated on the test sets of four\nseparate benchmark deepfake detection datasets, including\nDeeperForensics [2], FaceShifter [62], Deepfake Detection\nChallenge [1], and Celeb-DF (V2) [4]. This evaluation pro-\ncess aims to determine the model‚Äôs ability to generalize across\ndifferent forgery techniques and datasets, thus assessing its\ndomain generalization capabilities. It is noteworthy that this\nevaluation setup is exceedingly arduous since the original\nand forged videos in DFDC and Celeb-DF (V2) are different\nfrom those in FaceForensics++ and were not seen during the\ntraining process.\nThe results obtained from this cross-dataset evaluation are\npresented in Table 2. Based on the results shown in the table,\nit is evident that the proposed graph Transformer model out-\nperforms all other methods in terms of average AUC scores,\nachieving an impressive score of 90.8%. The RealForensics\nmethod comes in second place with an average AUC score of\n90.5%, while other methods have lower scores, ranging from\n63.1% to 89.7%. It is worth noting that some methods, such as\nFace X-ray and DSP-FWA, perform well on specific datasets\nbut have lower scores on others. This indicates that their\nperformance may be dataset-dependent and may not gener-\nalize well across different datasets. On the other hand, the\npresented method in this study and some other methods, such\nas FTCN, DFDT, and RealForensics, achieve consistently\nhigh scores across all datasets, indicating that they are robust\nand generalize well across different datasets. In particular,\nthe presented method achieves the highest AUC score on the\nDeeperForensics, FaceShifter, and Celeb-DF (V2) datasets\nand the second-highest score on the DFDC dataset. It is\npertinent to mention that due to the fact that DeeperForensics\nand FaceShifter utilize the same source videos as FaceForen-\nsics++, they tend to perform better than DFDC and Celeb-\nDF (V2) in terms of deepfake detection. This is because the\nmodels have already been trained on similar data, making it\neasier for them to generalize to these datasets.\nB. CROSS MANIPULATION GENERALIZATION\nIn the real world, deepfakes can be generated using vari-\nous techniques, including different source videos and post-\nprocessing methods, resulting in a vast array of manipulated\nvideos. A deepfake detection model that is only trained on\na specific dataset may fail to detect deepfakes generated\nusing different techniques, rendering the model ineffective.\nTherefore, any deployed deepfake detection algorithm must\nperform well on unseen forgery types [11], [12], [63], [65].\nThis means that cross-manipulation generalization is critical\nfor deepfake detection as it enables the detection model to\nperform effectively on unseen and potentially more sophisti-\ncated manipulated videos. To this end, the cross-manipulation\nTABLE 3. Cross-manipulation generalization results on FaceForensics++\ndataset. The AUC scores (%) are for each forgery class within the\nFaceForensics++ dataset after training the model using the other three\nmanipulation types.\nMethods Train on remaining sets Avg.DF NT F2F FS\nXception [5] 93.9 79.7 86.8 51.2 77.9\nCNN-aug [20] 87.5 67.8 80.1 56.3 72.9\nPatch-based [19] 94.0 84.8 87.3 60.5 81.7\nFace X-ray [12] 99.5 92.5 94.5 93.2 94.9\nCNN-GRU [30] 97.6 86.6 85.8 47.6 79.4\nLipForensics [11] 99.7 99.1 99.7 90.1 97.1\nDFDT [17] 99.8 99.2 99.6 93.1 97.9\nA V DFD [67] 100.0 98.3 99.8 90.5 97.1\nFTCN [65] 99.9 99.2 99.7 99.9 99.6\nRealForensics [66] 100.0 99.2 99.7 97.1 99.6\nOurs 99.9 99.2 99.8 98.3 99.3\nTABLE 4. Generalization AUC (%) scores across different compression\nlevels. The model is trained over Neuraltextures (NT) forgery type and\ntested on Deepfakes (DF) and FaceSwap (FS) forgery types from the\nFaceForensics++ dataset at two compression levels,i.e.,LQ and HQ.\nMethod HQ LQ\nFS DF FS DF\nXception [5] 71.8 77.0 51.7 58.7\nFace X-ray [12] 77.9 58.5 51.0 57.1\nF3Net [15] 61.2 80.5 51.9 58.3\nRFM [37] 63.9 79.8 51.6 55.8\nSRM [36] 79.5 83.8 52.9 55.5\nSLADD [68] 72.1 84.6 56.8 62.8\nOurs 72.9 84.5 57.2 61.4\ngeneralization property of the presented approach is investi-\ngated using the leave-one-out strategy [11], [14], [17], [19],\n[63]. Specifically, model performance is evaluated on each\nforgery type in FaceForensics++ dataset after training with\nthe remaining manipulation techniques. The obtained results\non the test sets of Deepfakes (DF), NeuralTextures (NT),\nFace2Face (F2F), and FaceSwap (FS) are shown in Table 3.\nThe table shows that the proposed method achieves a high\naverage AUC score of 99.3%, which is comparable to state-\nof-the-art methods, such as FTCN and RealForensics. The\nhigh average AUC score indicates that the presented method‚Äôs\ncross-manipulation extends well to previously unseen manip-\nulation types. It works on par with the SOTA with a self-\nsupervised feature extractor, indicating its effectiveness in\ncapturing more generalizable features for accurate deepfake\ndetection.\nC. CROSS COMPRESSION GENERALIZATION\nIn addition to achieving high generalization scores across\ndifferent types of deepfake manipulations, ensuring the ro-\nbustness of the deployed detector against varying levels of\ncompression is equally crucial. The proposed method‚Äôs cross-\ncompression generalization is evaluated through testing on\nDeepfakes and FaceSwap forgery types with different com-\npression levels while being trained on NeuralTextures from\nthe FaceForensics++ dataset. The results obtained are pre-\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nTABLE 5. Robustness against common post-processing perturbations. The experiments are performed using five different intensity levels for each\nperturbation type and reported the average AUC score (%). The average AUC score across all corruptions for each deepfake detection method is presented.\nMethod Clean Saturation Blur Block Contrast Pixel Noise Avg\nXception [5] 99.8 99.3 60.2 99.7 98.6 74.2 53.8 78.3\nCNN-aug [20] 99.8 99.3 76.5 95.2 99.1 91.2 54.7 84.1\nPatch-based [19] 99.9 84.3 54.4 99.2 74.2 56.7 50.0 67.5\nFace X-ray [12] 99.8 97.6 63.8 99.1 88.5 88.6 49.8 77.5\nCNN-GRU [30] 99.9 99.0 71.5 97.9 98.8 86.5 47.9 82.3\nLipForensics [11] 99.9 99.9 96.1 87.4 99.6 95.6 73.8 92.5\nFTCN [65] 99.4 99.4 95.8 97.1 96.7 98.2 53.1 89.5\nRealForensics [66] 99.8 99.8 95.3 98.9 99.6 98.4 79.7 95.6\nOurs 99.7 99.8 96.1 98.8 99.7 98.2 81.1 96.2\n 65\n 70\n 75\n 80\n 85\n 90\n 95\n 100\n23 30 32 35 38 40\nAUC (%)\nCompression level\nOurs\nRealForensics\nLipForensics\nFTCN\nFIGURE 4. Robustness of the model against different compression levels.\nModels are trained over the FaceForensics++ dataset with a compression\nlevel of 23 and evaluated against five different compression levels.\nsented in Table 4. It can be observed that while competi-\ntive generalization was demonstrated by the proposed graph\nTransformer model when the compression level was kept con-\nstant during training and testing. However, a significant drop\nin performance was noted once tested against compressed\nsamples, as evidenced in Figure 4. This is unsurprising, given\nthat highly compressed images are subject to the loss of\ntextural features and low-level clues. Therefore, FTCN and\nLipForensics were highly affected by compression levels.\nHowever, the proposed self-supervised graph transformer\nmodel achieves slightly better performances as it leverages\nhigh-level facial representations, which are less susceptible\nto low-level artifacts.\nD. RESILIENCE TO PERTURBATION\nGiven that real-world forgeries are vulnerable to various\nforms of corruption on social media, such as saturation,\ncontrast, blur, etc., it is essential that the deployed deepfake\ndetector can withstand such post-processing perturbations.\nThe procedure proposed in [11] is taken into account to\ninvestigate the resilience of the model against common post-\nprocessing filters, including applying Gaussian noise & blur,\nmodifying saturation & contrast, pixelation, and block-wise\nocclusions [2]. The model is trained over grayscale clips of\nthe FaceForensics++ dataset, with only horizontal flipping\nand random cropping augmentations, and evaluated against\nfive different intensity levels of aforementioned perturba-\ntions. The resulting average AUC scores are listed in Table 5.\nThe presented graph Transformer model is significantly less\nvulnerable to aforementioned corruptions compared to other\nmethods that utilize low-level cues such as Patch-based and\nFace X-ray. Furthermore, the proposed method outperforms\nFTCN and LipForensics, while exhibiting comparable results\nto RealForensics, thus showcasing its competitive perfor-\nmance.\nE. COMPARISON WITH SOTA\nA comprehensive evaluation was carried out to assess the\nsuperiority of the presented deepfake detection framework\nover state-of-the-art models. In particular, the model‚Äôs per-\nformance was compared against state-of-the-art deepfake\ndetection models that were trained and tested using same\ndatasets, and the resultant outcomes are presented in Table 6.\nRemarkably, the model exhibited competitive performance\nwhen evaluated on the FaceForensics++ dataset, while out-\nperforming other models when tested on Celeb-DF (V2)\nand WildDeepfake datasets. The obtained results unequivo-\ncally highlight the exceptional potency of the proposed graph\nTransformer model, underscored by its superlative generaliz-\nability attributable to the high-level deepfake representations\nobtained by a self-supervised framework. Furthermore, the\nproposed model‚Äôs ability to leverage the underlying graph\nstructure of the deepfake images enables the identification of\nboth local and global subtle discrepancies and artifacts that\nmay evade detection using traditional methods. This high-\nlights the significance of the proposed model, which offers a\ncompelling alternative to existing deepfake detection methods\nand has the potential to significantly enhance the reliability\nand robustness of deepfake detection systems.\nF. ABLATION STUDIES AND MODEL CONFIGURATION\nThe proposed framework was comprehensively evaluated\nthrough a series of meticulous ablation studies, aiming to in-\nvestigate the role of different components of the model and its\nhyperparameter values. The outcomes of these experiments\non Celeb-DF (V2) are summarized in Table 7. Firstly, the im-\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nTABLE 6. A quantitative comparison of methods performance on every dataset with existing deepfake detection approaches in frame-level analysis.\nReported results are obtained from associated articles. The same evaluation metric as the literature is used for each dataset to provide a fair comparison\nand better insight into the model‚Äôs performance.\nMethods FaceForensics++ Methods Celeb-DF (V2) Methods WideDeepfake\nAUC (%) AUC (%) (ACC %)\nTwo-stream [69] 70.1 Two-stream [69] 53.8 AlexNet [70] 60.3\nMeso4 [71] 84.7 Meso4 [71] 54.8 VGG16 [72] 60.9\nHeadPose [73] 47.3 HeadPose [73] 54.6 ResNetV2-50 [74] 63.9\nFWA [64] 80.1 FWA [64] 56.9 ResNetV2-101 [74] 58.7\nV A-MLP [25] 66.4 V A-MLP [25] 55.0 ResNetV2-152 [74] 59.3\nXception [5] 99.7 Xception [5] 65.5 Inception-v2 [75] 62.1\nMulti-task [63] 76.3 Multi-task [63] 54.3 MesoNet-1 [71] 60.5\nCapsule [76] 96.6 Capsule [76] 57.5 MesoNet-4 [71] 64.4\nDSP-FWA [64] 93.0 DSP-FWA [64] 64.6 MesoNet-inception [71] 66.0\nTwo-branch [14] 93.2 Two-branch [14] 73.4 XceptionNet [77] 69.2\nSPSL [13] 96.9 Face X-ray [12] 80.5 ADDNet-2D [6] 76.2\nF3-Net [15] 97.9 SPSL [13] 76.8 ADDNet-3D [6] 65.5\nVideo SE [78] 99.6 F3-Net [15] 65.1 ADD-Xception [8] 79.2\nRNN [79] 83.1 PPA [80] 83.1 DFDT [17] 81.3\nDFDT [17] 99.7 DefakeHop [7] 90.5\nFakeCatcher [81] 91.5\nATS-DE [9] 97.8\nADD-ResNet [8] 98.3\nDFDT [17] 99.2\nOurs 99.9 Ours 99.4 Ours 81.3\npact of various feature extractors on the overall performance\nof the classification task was assessed. The combination of\nvisual representations obtained from the proposed construc-\ntive learning vision Transformer architecture with the graph\nTransformer classifier exhibited superior performance com-\npared to utilizing a pre-trained feature extractor, specifically\nResNet50, in combination with the same graph Transformer\nclassifier.\nA series of experiments were conducted using both sets\nof feature extractors to investigate different hyperparameters‚Äô\nimpacts. For example, the influence of the number of included\nneighboring patches, denoted as K, on the graph construction\nprocess and the overall efficacy of the deepfake detection task\nis investigated. Table 7 reveals that the best performance\nwas achieved when K = 8 . Smaller values of K yield a\nreduction in the receptive field of the constructed graph,\nconsequently resulting in lower performance. Lastly, the im-\npact of other critical hyperparameters, such as the number of\ngraph convolutional layers and the number of Transformer\nblocks, on the overall performance of the deepfake detection\npipeline is investigated. The obtained results demonstrated a\nslight degradation in performance when the number of graph\nconvolution layers decreased from three layers to one layer.\nThis observation can be attributed to the concept of node\nreceptive field, where a greater number of graph convolution\nlayers allows for the incorporation of long-term information\nduring model development. Moreover, it was observed that\nutilizing three Transformer blocks facilitated more effective\npooling of discriminative information, leading to improved\nperformance in deepfake detection tasks.\nThe conducted ablation studies collectively showcased the\nsuperior performance of the proposed self-supervised graph\nTransformer framework for deepfake detection. These find-\nTABLE 7. Performance evaluation of the proposed self-supervised graph\nTransformer deepfake detection model using different ablation studies on\nCeleb-DF (V2) dataset. Key variables include the feature extractor (FE),\nneighboring patches count (K), graph convolutional layers (GCN), and\nnumber of Transformer blocks (TB).\nFE K GCN TB AUC (%)\nResNet50\n4\n1 3 97.7\n6 98.1\n3 3 97.7\n6 97.8\n8\n1 3 98.4\n6 98.3\n3 3 98.5\n6 98.4\nSSL ViT\n4\n1 3 98.6\n6 98.6\n3 3 99.1\n6 98.9\n8\n1 3 98.7\n6 98.6\n3 3 99.4\n6 99.1\nings contribute to the advancement of the field and emphasize\nthe efficacy of the employed methodology for addressing the\nchallenges associated with deepfake detection.\nVI. CONCLUSION\nThis work presents a self-supervised graph Transformer\nframework for deepfake detection that is comprised of three\nmain components. The proposed framework leverages the\nexpressive power of self-supervised contrastive learning to\nlearn high-level representations of deepfakes. Unlike existing\nsupervised approaches, which depend on low-level visual\nfingerprints, the extracted high-level visual representations\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\nvia this approach significantly enhance the generalizability\nof deepfake detection models, thereby reducing their depen-\ndence on dataset-specific patterns. Furthermore, the current\nwork leverages the expressive power of graph convolutional\nnetworks and Transformer blocks to capture intricate inter-\ndependencies among both local and global regions of an im-\nage, rendering them particularly suited to deepfake detection\ntasks. In addition, the proposed graph Transformer relevancy\nmap provides a comprehensive understanding of the com-\nplex interdependencies and salient features which leads to\nenhanced transparency and accountability of the detection\nmodel. Finally, the efficacy and generalizability of the pro-\nposed framework were rigorously evaluated via a comprehen-\nsive set of experiments, encompassing a diverse range of chal-\nlenging scenarios spanning both in-distribution and out-of-\ndistribution settings. The experimental results unequivocally\nestablish the framework‚Äôs superiority, with exceptional in-\ndataset detection accuracy being achieved. Moreover, the pro-\nposed self-supervised pre-training feature extractor consti-\ntutes a significant contribution to the field, having markedly\nimproved the framework‚Äôs ability to generalize across multi-\nple datasets while simultaneously enhancing its resilience to\npost-processing perturbations, such as compression and blur.\nREFERENCES\n[1] B. Dolhansky, R. Howes, B. Pflaum, N. Baram, and C. C. Ferrer, ‚Äò‚ÄòThe\ndeepfake detection challenge (dfdc) preview dataset,‚Äô‚Äô arXiv preprint\narXiv:1910.08854, 2019.\n[2] L. Jiang, R. Li, W. Wu, C. Qian, and C. C. Loy, ‚Äò‚ÄòDeeperforensics-1.0: A\nlarge-scale dataset for real-world face forgery detection,‚Äô‚Äô in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 2889‚Äì2898.\n[3] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen, ‚Äò‚ÄòAdvancing high fidelity\nidentity swapping for forgery detection,‚Äô‚Äô in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2020, pp. 5074‚Äì\n5083.\n[4] Y . Li, P. Sun, H. Qi, and S. Lyu, ‚Äò‚ÄòCeleb-DF: A Large-scale Challenging\nDataset for DeepFake Forensics,‚Äô‚Äô in IEEE Conference on Computer Vision\nand Patten Recognition (CVPR), Seattle, WA, United States, 2020.\n[5] A. Rossler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nie√üner,\n‚Äò‚ÄòFaceforensics++: Learning to detect manipulated facial images,‚Äô‚Äô in Pro-\nceedings of the IEEE/CVF international conference on computer vision,\n2019, pp. 1‚Äì11.\n[6] B. Zi, M. Chang, J. Chen, X. Ma, and Y .-G. Jiang, ‚Äò‚ÄòWilddeepfake: A\nchallenging real-world dataset for deepfake detection,‚Äô‚Äô in Proceedings of\nthe 28th ACM International Conference on Multimedia, 2020, pp. 2382‚Äì\n2390.\n[7] H.-S. Chen, M. Rouhsedaghat, H. Ghani, S. Hu, S. You, and C.-C. J. Kuo,\n‚Äò‚ÄòDefakehop: A light-weight high-performance deepfake detector,‚Äô‚Äô in2021\nIEEE International Conference on Multimedia and Expo (ICME). IEEE,\n2021, pp. 1‚Äì6.\n[8] A. Khormali and J.-S. Yuan, ‚Äò‚ÄòAdd: Attention-based deepfake detection\napproach,‚Äô‚ÄôBig Data and Cognitive Computing, vol. 5, no. 4, p. 49, 2021.\n[9] V .-N. Tran, S.-H. Lee, H.-S. Le, and K.-R. Kwon, ‚Äò‚ÄòHigh performance\ndeepfake video detection on cnn-based with attention target-specific re-\ngions and manual distillation extraction,‚Äô‚Äô Applied Sciences, vol. 11, no. 16,\np. 7678, 2021.\n[10] S. Agarwal, H. Farid, O. Fried, and M. Agrawala, ‚Äò‚ÄòDetecting deep-\nfake videos from phoneme-viseme mismatches,‚Äô‚Äô in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition work-\nshops, 2020, pp. 660‚Äì661.\n[11] A. Haliassos, K. V ougioukas, S. Petridis, and M. Pantic, ‚Äò‚ÄòLips don‚Äôt\nlie: A generalisable and robust approach to face forgery detection,‚Äô‚Äô in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2021, pp. 5039‚Äì5049.\n[12] L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo, ‚Äò‚ÄòFace x-ray\nfor more general face forgery detection,‚Äô‚Äô in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2020, pp. 5001‚Äì\n5010.\n[13] H. Liu, X. Li, W. Zhou, Y . Chen, Y . He, H. Xue, W. Zhang, and N. Yu,\n‚Äò‚ÄòSpatial-phase shallow learning: rethinking face forgery detection in fre-\nquency domain,‚Äô‚Äô in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2021, pp. 772‚Äì781.\n[14] I. Masi, A. Killekar, R. M. Mascarenhas, S. P. Gurudatt, and W. Ab-\ndAlmageed, ‚Äò‚ÄòTwo-branch recurrent network for isolating deepfakes in\nvideos,‚Äô‚Äô in European Conference on Computer Vision. Springer, 2020,\npp. 667‚Äì684.\n[15] Y . Qian, G. Yin, L. Sheng, Z. Chen, and J. Shao, ‚Äò‚ÄòThinking in frequency:\nFace forgery detection by mining frequency-aware clues,‚Äô‚Äô in European\nconference on computer vision. Springer, 2020, pp. 86‚Äì103.\n[16] H. Zhao, W. Zhou, D. Chen, T. Wei, W. Zhang, and N. Yu, ‚Äò‚ÄòMulti-\nattentional deepfake detection,‚Äô‚Äô in Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, 2021, pp. 2185‚Äì2194.\n[17] A. Khormali and J.-S. Yuan, ‚Äò‚ÄòDfdt: An end-to-end deepfake detection\nframework using vision transformer,‚Äô‚Äô Applied Sciences, vol. 12, no. 6, p.\n2953, 2022.\n[18] X. Zhu, H. Wang, H. Fei, Z. Lei, and S. Z. Li, ‚Äò‚ÄòFace forgery detection by 3d\ndecomposition,‚Äô‚Äô in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2021, pp. 2929‚Äì2939.\n[19] L. Chai, D. Bau, S.-N. Lim, and P. Isola, ‚Äò‚ÄòWhat makes fake images\ndetectable? understanding properties that generalize,‚Äô‚Äô in European Con-\nference on Computer Vision. Springer, 2020, pp. 103‚Äì120.\n[20] S.-Y . Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, ‚Äò‚ÄòCnn-\ngenerated images are surprisingly easy to spot... for now,‚Äô‚Äô in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 8695‚Äì8704.\n[21] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚Äò‚ÄòA simple framework\nfor contrastive learning of visual representations,‚Äô‚Äô in International confer-\nence on machine learning. PMLR, 2020, pp. 1597‚Äì1607.\n[22] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, ‚Äò‚ÄòMomentum contrast\nfor unsupervised visual representation learning,‚Äô‚Äô in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\npp. 9729‚Äì9738.\n[23] M. Tschannen, J. Djolonga, M. Ritter, A. Mahendran, N. Houlsby, S. Gelly,\nand M. Lucic, ‚Äò‚ÄòSelf-supervised learning of video-induced visual invari-\nances,‚Äô‚Äô in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 13 806‚Äì13 815.\n[24] J. Wang, Z. Wu, W. Ouyang, X. Han, J. Chen, Y .-G. Jiang, and S.-N.\nLi, ‚Äò‚ÄòM2tr: Multi-modal multi-scale transformers for deepfake detection,‚Äô‚Äô\nin Proceedings of the 2022 International Conference on Multimedia Re-\ntrieval, 2022, pp. 615‚Äì623.\n[25] F. Matern, C. Riess, and M. Stamminger, ‚Äò‚ÄòExploiting visual artifacts to\nexpose deepfakes and face manipulations,‚Äô‚Äô in 2019 IEEE Winter Applica-\ntions of Computer Vision Workshops (WACVW). IEEE, 2019, pp. 83‚Äì92.\n[26] T. Zhao, X. Xu, M. Xu, H. Ding, Y . Xiong, and W. Xia, ‚Äò‚ÄòLearning self-\nconsistency for deepfake detection,‚Äô‚Äô in Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021, pp. 15 023‚Äì15 033.\n[27] N. Bonettini, E. D. Cannas, S. Mandelli, L. Bondi, P. Bestagini, and\nS. Tubaro, ‚Äò‚ÄòVideo face manipulation detection through ensemble of cnns,‚Äô‚Äô\nin 2020 25th international conference on pattern recognition (ICPR).\nIEEE, 2021, pp. 5012‚Äì5019.\n[28] F. Marra, D. Gragnaniello, L. Verdoliva, and G. Poggi, ‚Äò‚ÄòDo gans leave ar-\ntificial fingerprints?‚Äô‚Äô in 2019 IEEE conference on multimedia information\nprocessing and retrieval (MIPR). IEEE, 2019, pp. 506‚Äì511.\n[29] H. Dang, F. Liu, J. Stehouwer, X. Liu, and A. K. Jain, ‚Äò‚ÄòOn the detection\nof digital face manipulation,‚Äô‚Äô in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern recognition, 2020, pp. 5781‚Äì5790.\n[30] E. Sabir, J. Cheng, A. Jaiswal, W. AbdAlmageed, I. Masi, and P. Natarajan,\n‚Äò‚ÄòRecurrent convolutional strategies for face manipulation detection in\nvideos,‚Äô‚ÄôInterfaces (GUI), vol. 3, no. 1, pp. 80‚Äì87, 2019.\n[31] C. Zhao, C. Wang, G. Hu, H. Chen, C. Liu, and J. Tang, ‚Äò‚ÄòIstvt: interpretable\nspatial-temporal video transformer for deepfake detection,‚Äô‚Äô IEEE Transac-\ntions on Information Forensics and Security, vol. 18, pp. 1335‚Äì1348, 2023.\n[32] Q. Yin, W. Lu, B. Li, and J. Huang, ‚Äò‚ÄòDynamic difference learning with\nspatio-temporal correlation for deepfake video detection,‚Äô‚Äô IEEE Transac-\ntions on Information Forensics and Security, 2023.\n[33] R. Durall, M. Keuper, and J. Keuper, ‚Äò‚ÄòWatch your up-convolution: Cnn\nbased generative deep neural networks are failing to reproduce spectral\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\ndistributions,‚Äô‚Äô in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2020, pp. 7890‚Äì7899.\n[34] J. Frank, T. Eisenhofer, L. Sch√∂nherr, A. Fischer, D. Kolossa, and T. Holz,\n‚Äò‚ÄòLeveraging frequency analysis for deep fake image recognition,‚Äô‚Äô in Inter-\nnational conference on machine learning. PMLR, 2020, pp. 3247‚Äì3258.\n[35] J. Li, H. Xie, J. Li, Z. Wang, and Y . Zhang, ‚Äò‚ÄòFrequency-aware discrim-\ninative feature learning supervised by single-center loss for face forgery\ndetection,‚Äô‚Äô inProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 2021, pp. 6458‚Äì6467.\n[36] Y . Luo, Y . Zhang, J. Yan, and W. Liu, ‚Äò‚ÄòGeneralizing face forgery detection\nwith high-frequency features,‚Äô‚Äô inProceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2021, pp. 16 317‚Äì16 326.\n[37] C. Wang and W. Deng, ‚Äò‚ÄòRepresentative forgery mining for fake face\ndetection,‚Äô‚Äô inProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 2021, pp. 14 923‚Äì14 932.\n[38] Z. Yang, J. Liang, Y . Xu, X.-Y . Zhang, and R. He, ‚Äò‚ÄòMasked relation learn-\ning for deepfake detection,‚Äô‚Äô IEEE Transactions on Information Forensics\nand Security, vol. 18, pp. 1696‚Äì1708, 2023.\n[39] P. Wang, K. Liu, W. Zhou, H. Zhou, H. Liu, W. Zhang, and N. Yu, ‚Äò‚ÄòAdt:\nAnti-deepfake transformer,‚Äô‚Äô in ICASSP 2022-2022 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2022, pp. 2899‚Äì1903.\n[40] J. Wang, Y . Sun, and J. Tang, ‚Äò‚ÄòLisiam: Localization invariance siamese net-\nwork for deepfake detection,‚Äô‚Äô IEEE Transactions on Information Forensics\nand Security, vol. 17, pp. 2425‚Äì2436, 2022.\n[41] A. v. d. Oord, Y . Li, and O. Vinyals, ‚Äò‚ÄòRepresentation learning with con-\ntrastive predictive coding,‚Äô‚Äô arXiv preprint arXiv:1807.03748, 2018.\n[42] P. O. O Pinheiro, A. Almahairi, R. Benmalek, F. Golemo, and A. C.\nCourville, ‚Äò‚ÄòUnsupervised learning of dense visual representations,‚Äô‚Äô Ad-\nvances in Neural Information Processing Systems, vol. 33, pp. 4489‚Äì4500,\n2020.\n[43] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li, ‚Äò‚ÄòDense contrastive\nlearning for self-supervised visual pre-training,‚Äô‚Äô in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021,\npp. 3024‚Äì3033.\n[44] J.-B. Grill, F. Strub, F. Altch√©, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al., ‚Äò‚ÄòBootstrap\nyour own latent-a new approach to self-supervised learning,‚Äô‚Äô Advances in\nneural information processing systems, vol. 33, pp. 21 271‚Äì21 284, 2020.\n[45] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, ‚Äò‚ÄòUn-\nsupervised learning of visual features by contrasting cluster assignments,‚Äô‚Äô\nAdvances in Neural Information Processing Systems, vol. 33, pp. 9912‚Äì\n9924, 2020.\n[46] M. Caron, H. Touvron, I. Misra, H. J√©gou, J. Mairal, P. Bojanowski, and\nA. Joulin, ‚Äò‚ÄòEmerging properties in self-supervised vision transformers,‚Äô‚Äô\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 9650‚Äì9660.\n[47] X. Chen and K. He, ‚Äò‚ÄòExploring simple siamese representation learning,‚Äô‚Äô in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 15 750‚Äì15 758.\n[48] X. Chen, S. Xie, and K. He, ‚Äò‚ÄòAn empirical study of training self-supervised\nvision transformers,‚Äô‚Äô in Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 2021, pp. 9640‚Äì9649.\n[49] S. Fung, X. Lu, C. Zhang, and C.-T. Li, ‚Äò‚ÄòDeepfakeucl: Deepfake detection\nvia unsupervised contrastive learning,‚Äô‚Äô in 2021 International Joint Con-\nference on Neural Networks (IJCNN). IEEE, 2021, pp. 1‚Äì8.\n[50] H. Bao, L. Dong, S. Piao, and F. Wei, ‚Äò‚ÄòBeit: Bert pre-training of image\ntransformers,‚Äô‚Äô in International Conference on Learning Representations,\n2022.\n[51] H. Zhao, W. Zhou, D. Chen, W. Zhang, and N. Yu, ‚Äò‚ÄòSelf-supervised\ntransformer for deepfake detection,‚Äô‚Äô arXiv preprint arXiv:2203.01265,\n2022.\n[52] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, ‚Äò‚ÄòImage\nbert pre-training with online tokenizer,‚Äô‚Äô in International Conference on\nLearning Representations, 2022.\n[53] T. N. Kipf and M. Welling, ‚Äò‚ÄòSemi-supervised classification with graph\nconvolutional networks,‚Äô‚Äô in International Conference on Learning Rep-\nresentations, 2017.\n[54] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚Äò‚ÄòAttention is all you need,‚Äô‚Äô Advances in\nneural information processing systems, vol. 30, 2017.\n[55] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., ‚Äò‚ÄòAn\nimage is worth 16x16 words: Transformers for image recognition at scale,‚Äô‚Äô\nin International Conference on Learning Representations, 2021.\n[56] F. M. Bianchi, D. Grattarola, and C. Alippi, ‚Äò‚ÄòSpectral clustering with\ngraph neural networks for graph pooling,‚Äô‚Äô in International Conference on\nMachine Learning. PMLR, 2020, pp. 874‚Äì883.\n[57] H. Chefer, S. Gur, and L. Wolf, ‚Äò‚ÄòTransformer interpretability beyond\nattention visualization,‚Äô‚Äô in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021, pp. 782‚Äì791.\n[58] Faceswap, ‚Äò‚ÄòFaceswap: Deepfakes software for all.‚Äô‚Äô [Online]. Available:\nhttps://github.com/deepfakes/faceswap\n[59] ‚Äî‚Äî, ‚Äò‚ÄòFaceswap.‚Äô‚Äô [Online]. Available: https://github.com/\nMarekKowalski/FaceSwap/\n[60] J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and M. Nie√üner,\n‚Äò‚ÄòFace2face: Real-time face capture and reenactment of rgb videos,‚Äô‚Äô in\nProceedings of the IEEE conference on computer vision and pattern recog-\nnition, 2016, pp. 2387‚Äì2395.\n[61] J. Thies, M. Zollh√∂fer, and M. Nie√üner, ‚Äò‚ÄòDeferred neural rendering: Image\nsynthesis using neural textures,‚Äô‚Äô ACM Transactions on Graphics (TOG),\nvol. 38, no. 4, pp. 1‚Äì12, 2019.\n[62] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen, ‚Äò‚ÄòFaceshifter: Towards\nhigh fidelity and occlusion aware face swapping,‚Äô‚Äô in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\n[63] H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, ‚Äò‚ÄòMulti-task learning\nfor detecting and segmenting manipulated facial images and videos,‚Äô‚Äô in\n2019 IEEE 10th International Conference on Biometrics Theory, Applica-\ntions and Systems (BTAS). IEEE, 2019, pp. 1‚Äì8.\n[64] Y . Li and S. Lyu, ‚Äò‚ÄòExposing deepfake videos by detecting face warping\nartifacts,‚Äô‚Äô in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, 2019, pp. 46‚Äì52.\n[65] Y . Zheng, J. Bao, D. Chen, M. Zeng, and F. Wen, ‚Äò‚ÄòExploring temporal\ncoherence for more general video face forgery detection,‚Äô‚Äô in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2021, pp.\n15 044‚Äì15 054.\n[66] A. Haliassos, R. Mira, S. Petridis, and M. Pantic, ‚Äò‚ÄòLeveraging real talking\nfaces via self-supervision for robust forgery detection,‚Äô‚Äô in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 14 950‚Äì14 962.\n[67] Y . Zhou and S.-N. Lim, ‚Äò‚ÄòJoint audio-visual deepfake detection,‚Äô‚Äô in Pro-\nceedings of the IEEE/CVF International Conference on Computer Vision,\n2021, pp. 14 800‚Äì14 809.\n[68] L. Chen, Y . Zhang, Y . Song, L. Liu, and J. Wang, ‚Äò‚ÄòSelf-supervised learning\nof adversarial example: Towards good generalizations for deepfake detec-\ntion,‚Äô‚Äô in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 18 710‚Äì18 719.\n[69] P. Zhou, X. Han, V . I. Morariu, and L. S. Davis, ‚Äò‚ÄòTwo-stream neural\nnetworks for tampered face detection,‚Äô‚Äô in 2017 IEEE Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW). IEEE,\n2017, pp. 1831‚Äì1839.\n[70] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚Äò‚ÄòImagenet classification\nwith deep convolutional neural networks,‚Äô‚Äô Communications of the ACM,\nvol. 60, no. 6, pp. 84‚Äì90, 2017.\n[71] D. Afchar, V . Nozick, J. Yamagishi, and I. Echizen, ‚Äò‚ÄòMesonet: a compact\nfacial video forgery detection network,‚Äô‚Äô in 2018 IEEE International Work-\nshop on Information Forensics and Security (WIFS). IEEE, 2018, pp.\n1‚Äì7.\n[72] K. Simonyan and A. Zisserman, ‚Äò‚ÄòVery deep convolutional networks for\nlarge-scale image recognition,‚Äô‚Äô In the proceedings of the International\nConference on Learning Representation, 2015.\n[73] X. Yang, Y . Li, and S. Lyu, ‚Äò‚ÄòExposing deep fakes using inconsistent\nhead poses,‚Äô‚Äô in ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp.\n8261‚Äì8265.\n[74] K. He, X. Zhang, S. Ren, and J. Sun, ‚Äò‚ÄòIdentity mappings in deep residual\nnetworks,‚Äô‚Äô in European conference on computer vision. Springer, 2016,\npp. 630‚Äì645.\n[75] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‚Äò‚ÄòRethinking\nthe inception architecture for computer vision,‚Äô‚Äô in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp. 2818‚Äì\n2826.\n[76] H. H. Nguyen, J. Yamagishi, and I. Echizen, ‚Äò‚ÄòUse of a capsule network to\ndetect fake images and videos,‚Äô‚Äô arXiv preprint arXiv:1910.12467, 2019.\n[77] F. Chollet, ‚Äò‚ÄòXception: Deep learning with depthwise separable convolu-\ntions,‚Äô‚Äô in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2017, pp. 1251‚Äì1258.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. Khormaliet al.: Self-Supervised Graph Transformer for Deepfake Detection\n[78] S. A. Khan and H. Dai, ‚Äò‚ÄòVideo transformer for deepfake detection with\nincremental learning,‚Äô‚Äô in Proceedings of the 29th ACM International\nConference on Multimedia, 2021, pp. 1821‚Äì1828.\n[79] D. G√ºera and E. J. Delp, ‚Äò‚ÄòDeepfake video detection using recurrent neural\nnetworks,‚Äô‚Äô in2018 15th IEEE international conference on advanced video\nand signal based surveillance (AVSS). IEEE, 2018, pp. 1‚Äì6.\n[80] P. Charitidis, G. Kordopatis-Zilos, S. Papadopoulos, and I. Kompatsiaris,\n‚Äò‚ÄòInvestigating the impact of pre-processing and prediction aggregation\non the deepfake detection task,‚Äô‚Äô in Proceedings of the Truth and Trust\nConference, 2020.\n[81] U. A. Ciftci, I. Demir, and L. Yin, ‚Äò‚ÄòFakecatcher: Detection of synthetic\nportrait videos using biological signals,‚Äô‚Äô IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2020.\nAMINOLLAH KHORMALI received the M.Sc.\nand Ph.D. degrees in computer engineering with\na focus on deep learning from the University of\nCentral Florida, Orlando in 2019 and 2022, respec-\ntively. During his research, he focused on the in-\ntersection of deep learning and computer vision to\ndevelop AI pipelines to identify AI-generated fake\nvideos and images. Furthermore, he has explored\nthe application of deep learning algorithms in a\nvariety of domains, ranging from cybersecurity to\ndrug discovery and digital pathology. His expertise in ML/DL skillets include\nself-supervised contrastive learning, multi-modal deep learning, multiple\ninstance learning, transfer learning, and generative adversarial networks.\nJIANN-SHIUN YUAN received the MS and\nPhD degrees from the University of Florida,\nGainesville, in 1984 and 1988, respectively. From\n1988 to 1989 he was with Texas Instruments, In-\ncorporated. Since 1990 he has been a Faculty mem-\nber of the University of Central Florida (UCF),\nwhere he is currently a professor and the site di-\nrector of the NSF I/UCRC MIST Center. He is the\nauthor of Semiconductor Device Physics and Sim-\nulation (Plenum, 1998) and SiGe, GaAs and InP\nHeterojunction Bipolar Transistors (Wiley, 1999) and has authored 360 pa-\npers in journals and conference proceedings. He supervised 32 PhD disserta-\ntions and 35 MS theses with UCF. Since 1990, he has been conducting many\nresearch projects funded by the National Science Foundation, Motorola,\nRenesas, Tokyo Electron, Lucent Technologies, National Semiconductor,\nand the State of Florida. He is a member of Eta Kappa Nu and Tau Beta Pi.\nHe is an editor of the IEEE Transactions on Device and Materials Reliability\nand a distinguished lecturer for the IEEE Electron Devices Society. He was\nthe recipient of the 1993 Outstanding Engineering Educator Award, IEEE\nFlorida Council; the 1995, 2004, 2010, 2015, and 2020 Teaching Awards,\nUCF; the 2003 and 2018 Research Awards, UCF; and the 2016 Pegasus\nProfessor Award, highest academic honor given to senior faculty at UCF.\nHis current research interests include power semiconductor device reliability\nand deep learning for biomedical applications.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3392512\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8307643532752991
    },
    {
      "name": "Discriminator",
      "score": 0.615017831325531
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5907630324363708
    },
    {
      "name": "Transformer",
      "score": 0.5709680318832397
    },
    {
      "name": "Machine learning",
      "score": 0.5383179783821106
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5323330163955688
    },
    {
      "name": "Graph",
      "score": 0.5012626647949219
    },
    {
      "name": "Generalization",
      "score": 0.4599896967411041
    },
    {
      "name": "Feature engineering",
      "score": 0.4548535645008087
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3847929537296295
    },
    {
      "name": "Deep learning",
      "score": 0.373086541891098
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1100354790687561
    },
    {
      "name": "Mathematics",
      "score": 0.09002098441123962
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106165777",
      "name": "University of Central Florida",
      "country": "US"
    }
  ],
  "cited_by": 23
}