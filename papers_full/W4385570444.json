{
  "title": "Nonparametric Masked Language Modeling",
  "url": "https://openalex.org/W4385570444",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2139505590",
      "name": "Se-Won Min",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2109805014",
      "name": "Weijia Shi",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A1978197916",
      "name": "Mike Lewis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096642295",
      "name": "Xilun Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4259474567",
      "name": "Wen-tau Yih",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": [
        "University of Washington",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2097074225",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2077815765",
    "https://openalex.org/W2143345705",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2122196799",
    "https://openalex.org/W2124509324",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4287854732",
    "https://openalex.org/W4280525837",
    "https://openalex.org/W4306801963",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W3118027562",
    "https://openalex.org/W3176119108",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2137419020",
    "https://openalex.org/W3177415603",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3175627818",
    "https://openalex.org/W2963967365",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W36903255",
    "https://openalex.org/W3024786184",
    "https://openalex.org/W4248694980",
    "https://openalex.org/W2908743445",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385574029",
    "https://openalex.org/W2950729111",
    "https://openalex.org/W4205694376",
    "https://openalex.org/W4281629162",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962916648",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4286909694",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3169937871",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4298183015",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4206778668",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4221155916",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4298345797",
    "https://openalex.org/W150793826",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4293138840",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385573102",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W3104929588"
  ],
  "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2097–2118\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nNonparametric Masked Language Modeling\nSewon Min1,2 Weijia Shi1,2 Mike Lewis2 Xilun Chen2\nWen-tau Yih2 Hannaneh Hajishirzi1,3 Luke Zettlemoyer1,2\n1University of Washington 2Meta AI 3Allen Institute for AI\n{sewon,swj0419,hannaneh,lsz}@cs.washington.edu\n{mikelewis,xilun,scottyih}@meta.com\nAbstract\nExisting language models (LMs) predict\ntokens with a softmax over a finite vocabulary,\nwhich can make it difficult to predict rare\ntokens or phrases. We introduce NPM, the first\nnonparametric masked language model that\nreplaces this softmax with a nonparametric\ndistribution over every phrase in a reference\ncorpus. NPM fills in the [MASK] solely from\nretrieving a token from a text corpus. We show\nthat NPM can be efficiently trained with a\ncontrastive objective and an in-batch approx-\nimation to full corpus retrieval. Zero-shot\nevaluation on 16 tasks including classification,\nfact probing and question answering demon-\nstrates that NPM outperforms significantly\nlarger parametric models, either with or\nwithout a retrieve-and-generate approach.\nIt is particularly better at dealing with rare\npatterns (word senses or facts) and predicting\nrare or nearly unseen words (e.g., non-Latin\nscript). We release the model and code at\ngithub.com/facebookresearch/NPM.\n1 Introduction\nCurrent large language models, despite their wide\nuse and impressive performance, are expensive to\nscale, difficult to update, and struggle with long-tail\nknowledge and patterns (Kandpal et al., 2022). Re-\ncent work follows a retrieve-and-generate approach\nto partially address these issues (Lewis et al., 2020;\nIzacard et al., 2022); however, their final predic-\ntions are still made by a parametric model. In\nparticular, they still include a softmax over a finite\nvocabulary, which limits expressivity (Yang et al.,\n2018; Pappas et al., 2020) and can make them re-\nluctant to predict rare or unseen tokens (e.g., Thes-\nsaloniki in Figure 1).\nIn this paper, we introduce NPM, the first\nNonParametric Masked Language Model that pre-\ndicts tokens solely based on a nonparametric dis-\ntribution over phrases in a text corpus (Figure 1).\nNPM consists of an encoder that maps the text\nItem delivered broken. Very cheaply made and does not even function. \n10/10, would buy this cheap awesome gaming headset again.\nReference Corpus\nThe Church of Saint Demetrius, or Hagios Demetrios, is the main \nsanctuary dedicated to Saint Demetrius, the patron saint of Thessaloniki. \nThe Banpo Bridge (Korean: Ү) is a major bridge in downtown Seoul.\nEncoder\ncheaper than an iPod. It was <mask>.\nbrokencheap construction. It was <mask>.\nawesome\nHagios Demetrios is located in <mask>. The ss alon iki\nThe Korean translation of Banpo Brige is <mask>. Ƅŷ Ɛ…\n(12 tokens)\nFigure 1: An illustration of NPM. The encoder maps a\nmasked sentence into a dense vector, and retrieves the\nnearest phrase from a reference corpus. NPM can fill\nin the [MASK] with multiple tokens, e.g., Thessaloniki\n(4 BPE tokens) and unseen words, e.g., 반ᄑ ᅩᄃ ᅢᄀ ᅭ(12\nBPE tokens).\ninto a fixed-sized vector, and a reference corpus\nfrom which NPM retrieves a phrase and fills in\nthe [MASK]. It, crucially, does not have a soft-\nmax over a fixed vocabulary, but instead has afully\nnonparametric distribution over phrases. This is\nin contrast to a recent body of work that incorpo-\nrates nonparametric components in a parametric\nmodel (Borgeaud et al., 2022; Izacard et al., 2022;\nZhong et al., 2022b).\nTraining such a nonparametric model introduces\ntwo key challenges: (1) full corpus retrieval during\ntraining is expensive, and (2) learning to predict an\narbitrary length phrase without a decoder is non-\ntrivial. We address the first challenge by using in-\nbatch approximations to full corpus retrieval (Wu\net al., 2020; Zhong et al., 2022b), and the second\nby extending span masking (Joshi et al., 2020) and\na phrase-level contrastive objective (Oord et al.,\n2018; Lee et al., 2021).\nWe perform zero-shot evaluation on 16 tasks in-\ncluding classification, fact probing and question\nanswering. They include temporal shift and word-\nlevel translation tasks that highlight the need to\npredict new facts or rare phrases. We compare with\n2097\na range of competitive baselines including encoder-\nonly (Liu et al., 2019), encoder-decoder (Raffel\net al., 2020), and decoder-only models (Zhang et al.,\n2022; Brown et al., 2020). We also compare with\na retrieve-and-generate approach that feeds a con-\ncatenation of the input and passages to parametric\nmodels using off-the-shelf retrieval. Results show\nthat NPM is significantly more parameter-efficient,\noutperforming up to 500x larger parametric models\nand up to 37x larger retrieve-and-generate mod-\nels. It is particularly good at (1) predicting rare\nwords (e.g., an entity split into multiple BPE to-\nkens such as Thessaloniki) and (2) disambiguating\nword senses (e.g., cheap may indicate inexpensive\nor of very poor quality; Figure 1). Finally, our eval-\nuation on an entity translation task demonstrates\nthat NPM can predict a word consisting of char-\nacters that are extremely rare if not unseen (e.g.,\nnon-Latin script; Figure 1).\nIn summary, our contributions are as follows.\n1. We introduce NPM, the first nonparamet-\nric masked language model that fills in the\n[MASK] solely from a phrase-level nonpara-\nmetric distribution over a corpus.\n2. We introduce a novel training scheme to train\nNPM on unlabeled data. We completely re-\nmove the softmax over the output vocabulary,\nenabling an effectively unbounded output space\nby predicting any n-gram.\n3. Zero-shot evaluation on 16 downstream tasks\nshows that NPM outperforms significantly\nlarger parametric models, are better on rare pat-\nterns, scale well, can be efficiently updated at\ntest time, and can predict extremely rare if not\nunseen tokens (e.g., words in non Latin script).\n2 Related Work\nLanguage Models (LMs). Large LMs trained on\na vast amount of text are shown to perform a wide\nrange of downstream tasks in a zero-shot manner by\nconverting a task into a cloze format (Radford et al.,\n2019; Brown et al., 2020). This is possible because\na variety of knowledge is encoded in the parameters\nof the models. Recent work has scaled parametric\nLMs by adding more parameters (Brown et al.,\n2020; Rae et al., 2021; Chowdhery et al., 2022)\nwhich can be very expensive in practice. Moreover,\nsuch models struggle with predicting rare words or\nentities, and cannot be updated over time.\nThere has been a recent body of work that incor-\nporates the nonparametric component with a para-\nmetric LM. We distinguish (1) work that concate-\nnates retrieved text to the input and trains the model\nwith a standard LM objective (Borgeaud et al.\n(2022); Izacard et al. (2022); so-called retrieve-\nand-generate approaches) from (2) work that re-\ntrieves tokens from a large text corpus to estimate\na probability distribution that is interpolated with\nthe output distribution from a standard LM (Khan-\ndelwal et al. (2020); Yogatama et al. (2021); Zhong\net al. (2022b); Lan et al. (2023); so-called kNN\nmodels). Our work is closely related to such a line\nof work and can be seen as an extreme version of\nthe kNN approach with no interpolation. However,\nour work is the first that models afully nonparamet-\nric distribution by entirely removing the softmax\nover a finite vocabulary. This offers a range of new\nfunctionalities, such as modeling a distribution over\nphrases, or predicting rare or unseen words.\nBottleneck in softmax. Most if not all language\nmodels use a softmax function that gives a categor-\nical probability distribution over a finite vocabu-\nlary. Yang et al. (2018) showed that this softmax\nis a low-rank approximation of a high-rank output\nspace, making the model less expressive. Pappas\net al. (2020) discussed that a fixed output vocabu-\nlary makes language models resistant to adaptation\nto new domains and tasks. We share the motiva-\ntion with such prior work and propose to use a\nnonparametric output space to address these issues.\nMoreover, although not explicitly explored in this\npaper, our work that completely removes the soft-\nmax over the vocabulary can make training more\nefficient, especially when the vocabulary is large\n(e.g., multilingual models (Conneau et al., 2020)).\nNonparametric models. In nonparametric mod-\nels, the data distribution is not defined by a fixed set\nof parameters, but is rather a function of the avail-\nable data (Siegel, 1957; Hollander et al., 2013).\nHaving complexity that grows as the data grows,\nthey are differentiated from parametric models\nwhose complexity is bounded as a priori. Free-\nman et al. (2002) noted that the term nonparametric\ndoes not imply that they have no parameters, but\nrather that the number and nature of the effective\nparameters are flexible and can depend on the data.\nRecent work in NLP has explored nonparamet-\nric inference without training (Khandelwal et al.,\n2020; He et al., 2021; Xu et al., 2022), or trained\nthe nonparametric model on the labeled data for a\nspecific downstream task (Seo et al., 2018, 2019;\nLee et al., 2021). In contrast, our work trains a fully\n2098\nqstart\nqend\nc2\nc1\nc\n10/10, would buy this cheap awesome gaming headset again. \nThe Church of Saint Demetrius, or Hagios Demetrios, (…) Saint \nDemetrius, the patron saint of Thessaloniki.\nReference Corpus\nVector space\n… this cheap awesome gaming …\n… saint of Thessaloniki.\n… saint of Thessaloniki.\nc3\n… saint of Thessaloniki. c4\n… saint of Thessaloniki.\nHagios Demetrios is located in  [MASK].\nqendQuery qstart\nFigure 2: Inference of NPM (Section 3.1). Each token\nin the reference corpus Cis mapped into a dense vector\nspace. At test time, a query is represented as two vectors,\nqstart and qend, each in the same vector space. We use\na nearest neighbor search to retrieve the start and the\nend of the phrase using qstart and qend, respectively.\nnonparametric language model without the labeled\ndata and performs a range of tasks zero-shot.\n3 Method\nWe introduce NPM, the first NonParametric\nMasked Language Model. NPM consists of an\nencoder and a reference corpus, and models a non-\nparametric distribution over a reference corpus\n(Figure 1). The key idea is to map all the phrases\nin the corpus into a dense vector space using the\nencoder and, when given a query with a [MASK]\nat inference, use the encoder to locate the nearest\nphrase from the corpus and fill in the [MASK].\nEncoder-only models are competitive represen-\ntation models (Patel et al., 2022), outperforming\nthe other two classes of models in classification\ntasks (Section 5.4). However, existing encoder-\nonly models are unable to make a prediction whose\nnumber of tokens is unknown, making their use\ncases limited without fine-tuning. NPM addresses\nthis issue, since it can fill in the [MASK] with an\narbitrary number of tokens by retrieving a phrase.\nWe first describe inference of NPM assuming a\nlearned encoder (Section 3.1), and then describe\nhow we train the encoder to map the text into a\ngood vector space (Section 3.2).\n3.1 N PM: Inference\nOverview. The encoder maps every distinct\nphrase in a reference corpus Cinto a dense vec-\ntor space. At test time, the encoder maps the\nmasked query into the same vector space and re-\ntrieves phrases from Cto fill in the [MASK]. Here,\nCdoes not have to be the same as the training\ncorpus, and can be replaced or scaled at test time\nwithout re-training the encoder.\nIn practice, there is a significant number of\nphrases in the corpus, and it is expensive to in-\ndex all of them. We therefore use a technique from\nLee et al. (2021) that represents a phrase with to-\nken representations of the start and the end of the\nphrase. In this approach, we index representations\nof each distinct token in C, and then at test time,\nuse a knearest neighbor search for the start and the\nend of the phrase, separately. Consider Figure 2 as\nan example. We represent a query with two vectors,\nqstart and qend. We then use each to retrieve the\nstart and the end of the plausible phrases—in this\ncase, c1 and c4, which are the start and the end of\nThessaloniki, respectively.\nMethod. Formally, let C= {c1,··· ,cN }be a\nreference corpus with N tokens. We first map each\ntoken ci into a contextualized, h-dimensional vec-\ntor ci ∈Rh by feeding the text into the encoder\nand take the vector that corresponds to each token:\nc1...cN = Encoder(c1...cN ).\nAt inference time, NPM is given a query whoset-\nth token is masked: q1...qt−1,[MASK],qt+1...qL.\nWe replace [MASK] with two special tokens\n[MASKs][MASKe] and feed it into the encoder\nto obtain a list of h-dimensional vectors:\nq1...qL+1 = Encoder(q1...qt−1,[MASKs],\n[MASKe],qt+1...qL).\nWe then take the vector corresponding to[MASKs]\nand [MASKe] as qstart and qend, respectively.1\nqstart = qt,qend = qt+1.\nWe then make a prediction via:\nargmax\nv∗∈V∗\n∑\ni≤j\nI[v∗= ci:j]\n(\nexp(sim(qstart,ci)) + exp(sim(qend,cj))\n)\n,\nwhere V∗is a set of possible n-grams defined by\nthe vocabulary Vand sim is a pre-defined similar-\nity function that maps a pair of vectors into a scalar\n1This allows obtaining two vectors without encoding the\nquery twice, e.g., unlike Lee et al. (2021)\n2099\n… against the Seattle Seahawks as a \nmember of (…) In the 2010 season, \nthe Seahawks became the first team \nin NFL history to …\nIn the 2010 NFL season, [masks]\n[maske] made history by making it \ninto the playoffs despite having a 7–\n9 record. \nVector space\n… season, [masks][maske] made…\n… against the Seattle Seahawks as a …\n… against the Seattle Seahawks as a …\n… became the first NFL history to …\n… In the 2010 season, theBatch\n… season, the Seahawks became the …\n… season, [masks][maske] made …\nMaximize TJN                                                        and TJN                                            \n… season, [masks][maske] made…\n … season, [masks][maske] made …\n… against the Seattle Seahawks as a …\n … against the Seattle Seahawks as a …( () ), ,\nFigure 3: Training of NPM. (Section 3.2). [MASKs][MASKe] indicates the masked span whose original phrase is\nthe Seattle Seahawks. We maximize the similarity scores between ...[MASKs][MASKe]... and ...the Seattle Seahawks... ,\nand between ... [MASKs][MASKe] ... and ...the Seattle Seahawks... .\nSequence to mask \nIn the 2010 NFL season, the Seattle Seahawks made history by making it \ninto the playoffs despite having a 7–9 record. (…) The Seahawks lost to the \nBears in their second game, 35–24. \nOther sequence in the batch \nRussell Wilson's first game against the Seattle Seahawks (…) when they \nlost Super Bowl XLIX to the New England Patriots. In the 2010 season, the \nSeahawks became the first team in NFL history (..) \nMasked sequence \nIn the [masks][maske] NFL season, [masks][maske] made history \nby making it into the playoffs despite having a 7–9 record. (…) The \nSeahawks lost [masks][maske] Bears in their second game, 35–24.\nFigure 4: Our span masking (Section 3.2.1). For sim-\nplicity, this figure assumes two sequences in the batch.\nSpans to mask out are chosen based on whether there is\nany co-occurring spans in other sequences in the batch.\nThen, each span is replaced with [MASKs][MASKe].\nvalue. In practice, iterating over N tokens is infea-\nsible. We thus use an approximation using a fast\nnearest neighbor search for the start and the end\nseparately. Details are provided in Appendix A.1.\nSimilarity function. The choice of similarity\nfunction can be flexible. We follow Zhong\net al. (2022b) in using a scaled inner product\nsim(h1,h2) = h1·h2√\nh ,where his a dimension of\nthe token vectors.\n3.2 N PM: Training\nNPM is trained on unlabeled text data. We describe\nthe masking strategy first (Section 3.2.1), and then\nthe training objective (Section 3.2.2).\n3.2.1 Masking\nWe extend span masking (Joshi et al., 2020), which\nmasks spans (consecutive tokens) whose length is\nsampled from a geometric distribution. Our span\nmasking differs from Joshi et al. (2020) in two\nways. First, we mask spans if they co-occur in\nthe other sequences in the batch to guarantee in-\nbatch positives during training (Section 3.2.2). For\ninstance, masked spans in Figure 4 are ‘2010’, ‘the\nSeattle Seahawks ’ and ‘ to the ’ all of which are\nfound in the other sequences. Second, instead of\nreplacing each token in the span with a [MASK],\nwe replace the whole span with two special tokens\n[MASKs][MASKe]. For instance, each of ‘2010’,\n‘the Seattle Seahawks’ and ‘to the’ is replaced with\n[MASKs][MASKe]. This is to obtain the start and\nthe end vectors for each span as we do at inference.\n3.2.2 Training Objective\nKey idea. We illustrate an example in Fig-\nure 3. The masked span is ‘ the Seattle Sea-\nhawks’, thus the model should retrieve a phrase\n‘the Seattle Seahawks ’ from other sequences in\nthe reference corpus when it is given a query\nlike this at test time. Specifically, we should\nencourage the [MASKs] vector to be closer to\n...the Seattle Seahawks... and the [MASKe] vector to\nbe closer to ...the Seattle Seahawks... , while being dis-\ntant from other tokens. We train the model to do\nso by approximating the full corpus as the other\nsequences in the batch. Concretely, we train the\nmodel to retrieve the start and the end of the span\n‘the Seattle Seahawks’ from other sequences in the\nsame batch. Note that our masking strategy ensures\nthat every masked span has a co-occurring span in\nthe batch (Section 3.2.1).\nObtaining vector representations. Consider the\ni-th sequence in the batch that consists of Ltokens,\nxi = xi\n1...xi\nL. We denote ˆxi = ˆxi\n1...ˆxi\nL as a conse-\nquence of span masking over xi. Both xi and ˆxi\nare fed into the encoder, and each token is mapped\n2100\ninto an h-dimensional vector:2\nxi\n1 ···xi\nL = Encoder( xi\n1 ···xi\nL),\nˆ xi\n1 ··· ˆ xi\nL = Encoder(ˆxi\n1 ··· ˆxi\nL).\nTraining objective. We consider a masked span\nin xi, represented with [MASKs][MASKe], de-\nnoted as ˆxi\nt,ˆxi\nt+1. We then denote gi\nt as the original\nn-gram that were replaced by ˆxi\nt,ˆxi\nt+1.\nWe now define the objective for this masked\nspan, and the final objective is summed over all\nmasked spans. The training objective for this\nmasked span is defined as\n−\n(\nlog\n∑\ny∈Y+\ns (gi\nt) exp(sim(ˆ xi\nt,y))\n∑\ny∈Y+\ns (gi\nt)∪Y−\ns (gi\nt) exp(sim(ˆ xi\nt,y))\n+ log\n∑\ny∈Y+\ne (gi\nt) exp(sim(ˆ xi\nt+1,y))\n∑\ny∈Y+\ne (gi\nt)∪Y−\ne (gi\nt) exp(sim(ˆ xi\nt+1,y))\n)\n.\nHere, sim(·,·) is a similarity function defined in\nSection 3.1, and Y+\ns (gi\nt), Y−\ns (gi\nt), Y+\ne (gi\nt) and\nY−\ne (gi\nt) are start positives, start negatives, end pos-\nitives and end negatives of gi\nt, respectively, which\nare defined in the next paragraph. This objective\nfollows a phrase-level contrastive learning objec-\ntives in prior work (Lee et al., 2021; Ram et al.,\n2021; Deng et al., 2021; Kulkarni et al., 2022) with\nan extension that allows multiple positives.\nIn-batch positives and negatives. The start posi-\ntives and the end positives are the start and the end\nof the spans to be retrieved. The start negatives and\nthe end negatives are tokens that are not the start\npositives and not the end positives, respectively.\nMore formally:\nY+\ns (gi\nt) =\n{\nxj\nm|gi\nt = xj\nm...xj\nm+|gi\nt|−1 & i̸= j\n}\n,\nY−\ns (gi\nt) =\n{\nxj\nm|gi\nt ̸= xj\nm...xj\nm+|gi\nt|−1 & i̸= j\n}\n,\nY+\ne (gi\nt) =\n{\nxj\nm|gi\nt = xj\nm−|gi\nt|+1...xj\nm & i̸= j\n}\n,\nY−\ne (gi\nt) =\n{\nxj\nm|gi\nt ̸= xj\nm−|gi\nt|+1...xj\nm & i̸= j\n}\n.\nHere, |gi\nt|indicates the length of the span gi\nt.\n4 Training Details\nTraining data. We use English Wikipedia (Au-\ngust 2019) and an English portion of CC-News\n(Mackenzie et al. (2020), February 2019) for train-\ning, which contains 13B tokens in total. The data\nis segmented into sequences, each with up to 256\ntokens.\n2The unmasked sequence and the masked sequence may\nhave different lengths before padding, but we pad them to\nhave the same length.\nTraining. We use the model architecture and ini-\ntial weights of RoBERTa large (Liu et al., 2019),\nconsisting of 354M parameters. Training is done\nfor 100,000 steps, using thirty-two 32GB GPUs.\nOne batch consists of 512 sequences (131,072 to-\nkens). We use an Adam optimizer (Kingma and\nBa, 2014) with a learning rate of 3 ×10−5, weight\ndecay of 0.01 and 4,000 steps of warm-up.\nBatching. The choice of batching is important in\nin-batch approximations, as it determines the qual-\nity of positives and negatives. For instance, Zhong\net al. (2022b) uses BM25 to ensure the sequences\nin the same batch are likely to share the same topic.\nWith a pretraining corpus with billions of tokens,\nit can be significantly expensive to build a BM25\nindex. Therefore, we instead construct the batch by\ngrouping sequences from the same document and\nassigning them to the same batch.3 This trick en-\nsures that (a) positives (spans that share the string)\nare likely to share the context, reducing false pos-\nitives, and (b) negatives are those that the model\nis likely to be confused with, thus training against\nthem helps the model better identify positives. Dur-\ning training, we gather all sequences from multiple\nGPUs to increase the size of the effective batch and\nmake in-batch approximation more effective.\n5 Experiments: Closed-set Tasks\nWe perform zero-shot evaluation on closed-set\ntasks where a small set of candidates is given.\n5.1 Evaluation Datasets\nWe include nine classification datasets that are\nknown for not necessarily requiring factual knowl-\nedge: AGNews (Zhang et al., 2015), Yahoo (Zhang\net al., 2015), Subj (Pang and Lee, 2004), SST-\n2 (Socher et al., 2013), MR (Pang and Lee, 2004),\nRotten Tomatoes (RT), CR (Hu and Liu, 2004),\nAmazon polarity (Amz, McAuley and Leskovec\n(2013)) and RTE (Dagan et al., 2005). The tasks\nrange from topic classification and sentiment anal-\nysis to subjectivity classification and textual entail-\nment. Statistics are provided in Appendix B.\n5.2 Baselines\nWe compare with the encoder-only, the decoder-\nonly and the encoder-decoder models with vari-\nous sizes (354M to 175B parameters). We include\nRoBERTa (Liu et al., 2019) as the encoder-only,\n3Documents that are not long enough to construct a batch\nare grouped with each other.\n2101\nModel # Params AGN Yahoo Subj SST-2 MR RT CR Amz RTE Avg\nBaselines (encoder-only)\nRoBERTa (Gao et al., 2021) 1.0x - - 51.4 83.6 80.8 - 79.5 - 51.3 -\nRoBERTa 1.0x 71.3 41.4 67.6 84.5 81.7 81.1 80.4 83.5 57.4 72.1\nBaselines (encoder-decoder)\nT5 2.2x 72.0 51.3 54.9 57.5 57.7 59.1 56.4 59.3 55.6 58.2\nT5 3B 8.5x 80.5 53.6 54.8 59.6 58.6 57.3 53.7 57.0 58.5 59.3\nBaselines (decoder-only)\nGPT-2 (Shi et al., 2022) 2.2x 67.4 49.7 60.8 55.3 54.6 53.0 66.2 57.6 53.1 57.5\n+ PMI (Shi et al., 2022) 2.2x 65.1 48.8 62.5 76.5 74.6 74.1 82.8 76.2 54.2 68.3\nGPT-2 kNN†(Shi et al., 2022) 2.2x 29.8 37.0 50.0 47.1 49.9 49.1 69.3 57.4 54.1 49.3\nGPT-2 kNN-LM†(Shi et al., 2022) 2.2x 78.8 51.0 62.5 84.2 78.2 80.6 84.3 85.7 55.6 73.4\nGPT-3 (Holtzman et al., 2021) 500x 75.4 53.1 66.4 63.6 57.4 57.0 53.8 59.4 56.0 60.2\n+ PMI (Holtzman et al., 2021) 500x 74.7 54.7 64.0 71.4 76.3 75.5 70.0 75.0 64.3 69.5\nOurs (encoder-only, nonparametric)\nNPM† 1.0x 74.5 53.9 75.5 87.2 83.7 86.0 81.2 83.4 61.7 76.4\nFull fine-tuning (reference)\nRoBERTa (Gao et al., 2021) 1.0x - - 97.0 95.0 90.8 - 89.4 - 80.9 -\nTable 1: Zero-shot results on closed-set tasks. # Paramsindicates the relative number of model parameters compared\nto RoBERTa large (354M). RoBERTa, T5 and GPT-2 are theirlarge variants unless specified otherwise; GPT-3 is\nfrom Davinci, non-instruct. Numbers with citations are taken from the corresponding papers. As a reference, we\nprovide results of fine-tuning on the full training dataset in the last row. †indicates a reference corpus is used. NPM\nsignificantly outperforms larger parameters models.\nT5 (Raffel et al., 2020) as the encoder-decoder,\nand GPT-2/3 (Radford et al., 2019; Brown et al.,\n2020) as the decoder-only model. For the decoder-\nonly models, we additionally apply PMI (Holtzman\net al., 2021) for better calibration of the model out-\nput. We also compare with Shi et al. (2022) who\nuse kNN inference using GPT-2 with PMI. In par-\nticular, (1) GPT-2kNN uses kNN inference with-\nout training, and (2) GPT-2 kNN-LM interpolates\ndistributions from GPT-2 and GPT-2kNN.\n5.3 Setup\nWe use the templates and verbalizers from Shi et al.\n(2022) for all models. When available, we use\nfuzzy verbalizers from Shi et al. (2022). We use a\ndomain-specific reference corpus: a union of the\nEnglish Wikipedia and CC News for AGN, Yahoo\nand RTE, a subjectivity corpus for Subj, and a re-\nview corpus for sentiment classification datasets.\nTheir sizes vary from 15M tokens to 126M tokens.\nDetails are in Appendix B. Fast similarity search is\ndone using FAISS (Johnson et al., 2019) with the\nHNSW index. We use k= 4096for inference.\n5.4 Results\nNPM outperforms baselines in the zero-shot setting\n(Table 1). We discuss the results in detail below.\nComparison between baselines. Among para-\nmetric models, RoBERTa achieves the best per-\nformance, outperforming larger models including\nRoBERTa\ncheaper than an iPod. It was <mask>. \ncheap construction. It was <mask>.\nPositive \nPositive\nNPM SINGLE\ncheaper than an iPod. It was <mask>. \ncheap construction. It was <mask>.\nPositive \nNegative\nSim(cheap, <m>) \nSim(cheap, <m>) \nSim(cheap, cheap) \nSim(<m>,  <m>)\n= 27.3 \n= 27.5 \n= 28.0 \n= 27.9\nSim(cheap, <m>) \nSim(cheap, <m>) \nSim(cheap, cheap) \nSim(<m>,  <m>)\n= 28.8 \n= 28.5 \n= 15.9 \n= 15.7\nSentiment analysis: {positive, negative} with fuzzy verbalizers\nRetrieved context for <mask>: \n10/10, would buy this cheap awesome gaming headset again.\nRetrieved context for <mask>: \nItem delivered broken. Very cheaply made and does not even function.\nFigure 5: Predictions from RoBERTa (baseline) and\nNPM. The bottom indicates the context NPM retrieves\nto fill in [MASK]. Note that the fuzzy verbalizer maps\nbroken to Negative and awesome to Positive.\nGPT-3. This is perhaps surprising, and is likely\nbecause bidirectionality of the encoder-only model\nplays a vital role, as claimed in Patel et al. (2022).\nThe kNN-LM approach from Shi et al. (2022),\nwhich incorporates the nonparametric component\nto the parametric model, outperforms all other\nbaselines. Nonetheless, solely relying on retrieval\n(kNN) performs poorly with GPT-2, suggesting\nthat using kNN at inference only is limited.\nBaselines versus NPM. NPM significantly out-\nperforms all baselines, achieving consistently com-\npetitive performance over all datasets. This indi-\ncates that, even for tasks that do not explicitly re-\nquire external knowledge, nonparametric models\nare very competitive.\nQualitative analysis. Figure 5 depicts predic-\ntions from RoBERTa and NPM on a sentiment\n2102\n100 101 102\nModel size (relative to 354M)\n5\n10\n15\n20\n25\n30\n35Macro-averaged EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM T-REx\n100 101 102\nModel size (relative to 354M)\n5\n10\n15\n20\n25\n30Macro-averaged EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM\nT-REx UHN\n100 101 102\nModel size (relative to 354M)\n5\n10\n15\n20\n25\n30Macro-averaged EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM T-REx Hard\n100 101 102\nModel size (relative to 354M)\n0\n5\n10\n15\n20\n25Macro-averaged EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM Google RE\n100 101 102\nModel size (relative to 354M)\n0\n5\n10\n15\n20Macro-averaged EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM Google RE UHN\n100 101 102\nModel size (relative to 354M)\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM KAMEL\n100 101 102\nModel size (relative to 354M)\n10\n20\n30\n40\n50EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM\nTQA\n100 101 102\nModel size (relative to 354M)\n0\n2\n4\n6\n8\n10EM\nT5\nBM25+T5\nOPT\nBM25+OPT\nGPT-3\nBM25+GPT-3\nNPM\nNQ\nFigure 6: Zero-shot results on knowledge tasks. The x-axis indicates the relative number of model parameters in\nlog scale compared to RoBERTa large (354M).NPM outperforms significantly larger parameters models, either\nwith or without BM25. See Table 8 in Appendix C for the raw numbers.\nanalysis task. The first example uses cheap to in-\ndicate inexpensive, and the second example uses\ncheap to indicate of very poor quality. RoBERTa\npredicts Positive to both, while NPM makes\ncorrect predictions by retrieving the context that\nuses cheap in the same context as the input.\nWe also find that representations fromNPM lead\nto better word sense disambiguation. For instance,\nRoBERTa assigns a high similarity score between\ncheap (inexpensive) and cheap (of very poor qual-\nity). On the other hand, NPM successfully assigns\na low similarity score between cheap and cheap,\neven though their surface forms are the same.\n6 Experiments: Open-set Tasks\nWe include zero-shot evaluation on open-set tasks\nwhose answer can be any arbitrary-length string.\n6.1 Evaluation Datasets\nWe evaluate on seven datasets: T-REx and\nGoogle-RE from LAMA (Petroni et al., 2019),\nKAMEL (Kalo and Fichtel, 2022), Natural Ques-\ntions (NQ, Kwiatkowski et al. (2019)), TriviaQA\n(TQA, Joshi et al. (2017)), TempLAMA 22\n19 and an\nentity translation task. In particular, TempLAMA\nrequires probing knowledge with temporal updates,\nmotivated by Dhingra et al. (2022) and Jang et al.\n(2022). The entity translation task involves a trans-\nlation of an entity from English to other, non-Latin\nlanguages, requiring the model to predict extremely\nrare (if not unseen) characters. See Appendix B for\ndetails and statistics of all datasets.\n6.2 Baselines\nWe compare with T5 (Raffel et al., 2020) as the\nencoder-decoder, and GPT-3 (Brown et al., 2020)\nand OPT (Zhang et al., 2022) as the decoder-only\nmodels. The encoder-only models are not applica-\nble for open-set tasks since the number of tokens\nto predict is unknown.\nPrior work found that a “retrieve-and-generate”\napproach that concatenates the input and passages\nfrom an off-the-shelf retrieval system is often help-\nful in knowledge-dependent tasks (Kandpal et al.,\n2022). We add them as baselines, using up to five\npassages from BM25 (Robertson et al., 2009).\n2103\n5% 10% 20% 50% 100%\nReference corpus size\n0\n5\n10\n15\n20\n25\n30\n35\n40\nT-REx\n5% 10% 20% 50% 100%\nReference corpus size\n0\n5\n10\n15\n20\n25\n30\n35\n40\nTQA\nFigure 7: Ablation on the size of the reference corpus,\nfrom 41M tokens (5%) to 810M tokens (100%). There\nis a strong correlation between the size of the corpus\nand downstream performance.\n6.3 Setup\nFor all datasets, we report Exact Match (EM). The\nLAMA test data is biased toward frequent entities\nbecause they are filtered to only include answers\nthat are single tokens based on BERT (Devlin et al.,\n2019). Since we do not want our evaluation to be\nbiased toward overly frequent entities, we report\na micro-averaged accuracy over the data whose\nanswers are 1, 2, 3 and 4+ grams, respectively.\nOther datasets do not have such filtering, therefore\nwe report average EM.\nAs a reference corpus, we use the English\nWikipedia from 08/01/2019, consisting of 810M\ntokens. For TempLAMA 22\n19 , we use the English\nWikipedia from 08/01/2022, consisting of 858M\ntokens.\nFor NPM, we find combining with sparse re-\ntrieval significantly helps, likely because dense re-\ntrieval and sparse retrieval capture complementary\nfeatures (Karpukhin et al., 2020; Seo et al., 2019).\nIn particular, we reduce the search space to the\ntop 3 passages based on BM25 and perform dense\nsearch as done in Kassner and Schütze (2020).\n6.4 Results\nFigure 6 show results on five knowledge tasks.\nFirst, performance of parametric models largely\ndepends on the number of parameters, as it has been\nclaimed in much of prior work (Brown et al., 2020;\nKandpal et al., 2022). The retrieve-and-generate\napproach that combines parametric models with\nBM25 significantly improves performance.\nNPM outperforms or is on par with significantly\nlarger baselines across all datasets. It substantially\noutperforms all models on two LAMA datasets,\nincluding 500x larger GPT-3 either with or with-\nout BM25. On KML, TQA and NQ, NPM con-\nsistently outperforms 37x larger models with or\nModel #Params Unchanged Changed A VG\nBaselines\nT5 2.2x 1.9 0.4 1.1\nT5 3B 8.5x 1.8 0.4 1.1\nOPT 6.7B 19x 2.5 1.0 1.7\nOPT 13B 37x 4.9 2.1 3.5\nBM25 + T5 2.2x 13.7 →14.9 3.0 →20.1 17.5\nBM25 + T5 3B 8.5x 11.9 →12.0 2.2 →17.8 14.9\nBM25 + OPT 6.7B 19x 10.2 →8.2 1.7 →11.3 9.7\nBM25 + OPT 13B 37x 14.8 →14.4 2.8 →16.6 15.5\nOurs\nNPM 1.0x 18.9 →19.5 2.9→17.5 18.5\nTable 2: Results on TempLAMA22\n19 , on an unchanged\nset, a changed set, and a macro-average over two, re-\nspectively. xx→xx indicates performance when using\nthe outdated and the updated Wikipedia, respectively.\nwithout BM25. This is impressive given that NPM\nis not trained on data with questions.\nIt is also worth noting that sparse retrieval is crit-\nical in NPM, e.g., without sparse retrieval, perfor-\nmance on LAMA-TREx drops from 34.5 to 16.1.\nWe think this is because (1) sparse retrieval and\ndense retrieval capture complementary features,\nand (2) the removal of approximation in search\nimproves search quality. We think future work can\nexplore completely removing sparse retrieval, as\nhas been done in Lee et al. (2021) to improve Seo\net al. (2019).\nImpact of the reference corpus size. Figure 7 re-\nports the impact of the size of the reference corpus,\nfrom 41M tokens (5%) to 810M tokens (100%).\nPerformance of NPM is highly correlated with the\nsize of the reference corpus, strongly suggesting\nthat using a larger reference corpus is important.\nResults on temporal knowledge tasks. Table 2\nreports results on TempLAMA. NPM retains its\nperformance on the unchanged set ( 18.9 →19.5)\nand successfully updates its answers on the\nchanged set (2.9 →17.5). Its performance is sig-\nnificantly better than the performance of parametric\nmodels with up to 13B parameters, and is on par\nwith a larger model with the retrieve-and-generate\napproach, which also successfully updates its an-\nswer by leveraging the updated corpus. This is in\nagreement with prior work that shows the model\nwith a nonparametric component adapts to tempo-\nral updates by replacing the reference corpus at\ntest time (Izacard et al., 2022). Nonetheless, the\nretrieve-and-generate approach is still significantly\nworse than NPM when the target entities are rare,\nwhich we show in the next paragraph.\n2104\n0 1 2 3+\n# BPE splits\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nT-REx\nGPT-3 175B\nBM25+GPT-3 175B\nNPM 354M\n0 1 2 3+\n# BPE splits\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nGoogle RE\nGPT-3 175B\nBM25+GPT-3 175B\nNPM 354M\n0 1 2 3+\n# BPE splits\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nTempLAMA (Unchanged)\nBM25+T5\nBM25+T5 3B \nNPM 354M\n0 1 2 3+\n# BPE splits\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nTempLAMA (Changed)\nBM25+T5\nBM25+T5 3B \nNPM 354M\nFigure 8: Performance on LAMA and TempLAMA tasks, broken down based on the number of BPE splits of the\ntarget entity, which is an indication of rarity of the entities (L:Frequent→R:Rare). NPM outperforms GPT-3 or T5\nmore significantly when the target entities are rare.\nModel #Params #L w/o BM25 w/ BM25\nBaselines, English-only\nT5 2.2x 0.2 1.9\nT5 3B 8.5x 0.5 4.4\nOPT 6.7B 19x 0.4 22.3\nOPT 13B 37x 1.0 24.6\nOurs, English-only\nNPM 1.0x 52.4\nReferences, Multilingual\nmT5 3.4x 101 1.3 19.0\nmT5 XL 11x 101 4.1 56.6\nBLOOM 3B 8.5x 46 0.0 17.4\nBLOOM 7.1B 20x 46 0.1 26.0\nTable 3: Results on the entity translation task. See\nTable 10 in Appendix C for per-language results. #L\nindicates the number of languages multilingual models\nare trained on. Bold and Bold indicate the best among\nmonolingual models and the best including multilingual\nmodels, respectively. NPM significantly outperforms\nall existing monolingual models, and approaches or\noutperforms larger multilingual models.\nPerformance on rare entities. We break down\nthe instances on LAMA and TempLAMA based on\nthe number of BPE splits of the target entity, e.g.,\nThessaloniki is one word that is split into 4 BPE\ntokens, thus the number of splits is 3. Since BPE\nsplits a word if they are rare, the number of BPE\nsplits indicates the rarity of the entity. We compare\nNPM with GPT-3 and BM25+GPT-3 on LAMA,\nand BM25+T5 (770M and 3B) on TempLAMA, the\ntwo most competitive baselines on each dataset.\nFigure 8 reports results. On LAMA, NPM out-\nperforms GPT-3 fairly consistently, with larger\ngains as the number of BPE splits increases. On\nTempLAMA, while BM25+T5 is competitive on\nfrequent entities with zero BPE split, it consistently\nlags behind NPM with ≥1 BPE splits. This sug-\ngests that NPM is particularly good at address-\ning rare entities, compared to not only parametric\nmodels without retrieval but also the retrieve-and-\ngenerate approach.\nResults in Entity Translation. Results on the\nentity translation task are shown in Table 3 (per-\nlanguage results are reported in Table 10 of Ap-\npendix C). T5 and OPT struggle to perform the\ntask, both with and without BM25 retrieval. In\ncontrast, NPM performs well across all languages.\nIn order to better calibrate performance of NPM,\nwe provide reference performance of models that\nare purposely trained on the multilingual data—\nmT5 (Xue et al., 2021) and BLOOM (Scao et al.,\n2022). NPM outperforms 3.4x larger mT5 and\n20x larger BLOOM, and approaches 11x larger\nmT5, even though it is trained on English. We\nthink strong cross-lingual transferability of NPM\nis likely because it can retrieve a phrase based on\nits surrounding context, even if it has not seen the\nexact word during training.\n7 Conclusion\nWe introduced NPM, a nonparametric masked lan-\nguage model that replaces a softmax over the output\nvocabulary with a nonparametric distribution over\na reference corpus. NPM can be efficiently trained\nusing a contrastive objective and an in-batch ap-\nproximation to a full corpus. Zero-shot evaluation\non 16 tasks shows that NPM outperforms signifi-\ncantly larger parametric models. NPM is particu-\nlarly good at rare patterns (word senses or facts),\nscaling and updating at test time, and predicting\nextremely rare if not unseen characters.\nLimitation\nScaling through the inference corpus. The size\nof the reference corpus is an additional dimension\nfor model scale in nonparametric models. In this\npaper, we scale the corpus up to nearly 1B tokens,\nwhich is still smaller than the training data of very\nlarge language models (Brown et al., 2020; Rae\net al., 2021). We think future work can scale it fur-\n2105\nther using tools such as Distributed FAISS (John-\nson et al., 2019) or ScaNN (Guo et al., 2020).\nSignificant memory usage. Using NPM saves\nGPU compute and memory compared to using mod-\nels with more parameters. However, NPM requires\nmore RAM and disk memory due to embeddings\nof a reference corpus. For instance, the largest cor-\npus in our experiments (full English Wikipedia)\nrequires 70GB of RAM and 1.4TB of disk memory.\nFuture work can build more efficient NPM as done\nin prior work in nearest neighbor search (Jegou\net al., 2010; Norouzi et al., 2012; Ge et al., 2014;\nIzacard et al., 2020; Yamada et al., 2021).\nExploration of larger vocabulary. Large vocab-\nulary is known to lead performance gains (Con-\nneau et al., 2020) but is bounded in memory costs.\nPrevious work explored more efficient softmax ap-\nproximations (Morin and Bengio, 2005; Chen et al.,\n2016; Grave et al., 2017). Our nonparametric train-\ning offers an alternative by removing the softmax\nover the vocabulary. With the RoBERTa architec-\nture, increasing the vocab size by 2x makes the\nbaseline training 50% more memory expensive, but\ndoes not increase the memory in training NPM.\nHowever, this paper does not include more sys-\ntematic evaluation on the effect of large vocabu-\nlary. Future work can explore training NPM with\na significantly larger vocabulary to further boost\nperformance.\nExtension for generation. Our paper evaluates\nNPM only on prediction tasks. It is currently non-\ntrivial to use NPM for generation, since it is the\nencoder-only model. Future work can explore au-\ntoregressive generation as done in Patel et al. (2022)\nor use NPM for editing (Schick et al., 2022; Gao\net al., 2022).\nExtension to few-shot learning and fine-tuning.\nOur paper focuses on zero-shot evaluation only. Fu-\nture work can extend NPM to a few-shot learning\nsetup. In fact, fine-tuning NPM is significantly eas-\nier than fine-tuning larger models such as T5, OPT\nand GPT-3 which we compare NPM with, and can\nbe explored in future work.\nBetter cross-lingual transfer. Our work ex-\nplored cross-lingual transfer in a limited setup\nwhere the model is trained on monolingual data.\nWe think future work can train multilingual NPM,\nand explore more comprehensive cross-lingual eval-\nuation. In fact, nonparametric training may allevi-\nate the burden of collecting large-scale multilingual\nModel #Params FS SP Acc #Q/sec\nRoBERTa 1.0x 67.6 36.36\nNPM‡ 1.0x ✓ 75.5 7.63\nOPT 2.7B 7.6x 2.1 0.71\nOPT 2.7B + BM25‡ 7.6x ✓ 8.3 0.28\nOPT 6.7B 19x 4.2 0.18\nOPT 6.7B + BM25‡ 19x ✓ 10.7 0.12\nNPM‡ 1.0x ✓ 10.8 4.52\nTable 4: Inference speed measured on Subj with |C|=\n15M (the first block) and NQ with |C| = 810M (the\nsecond block). A single GPU used (Quadro GP100).\n‡indicates the corpus is used. ‘FS’ and ‘SP’ indicate\nthat a FAISS index is used and a sparse index (+ exact\ninner product search in case of NPM) is used, respec-\ntively. NPM is slower than the same-sized parametric\nmodel, but is faster than larger models (either with or\nwithout retrieval) while outperforming or matching per-\nformance.\ncorpora since it makes the model less sensitive to\nthe language coverage in the training data, and may\nlead to significantly better cross-lingual transfer, as\nwe demonstrate in the entity translation task.\nLimitation in speed. We find that search makes\ninference considerably slower than the counterpart\nwithout search. We think that (1) search can sig-\nnificantly be faster with better engineering (we use\nthe default hyperparameters of the FAISS index\nwith no tuning) or better index, and (2) the speed of\nNPM is still on par with the speed of significantly\nlarger parametric models that NPM outperforms\n(see Table 4). Moreover, while not explored in this\nwork, there has been work that improves inference\nspeed (He et al., 2021; Alon et al., 2022) that can\nbe applied to NPM. We leave improving inference\nspeed to future work.\nAcknowledgements\nWe thank Ari Holtzman, Eric Wallace, Iz Belt-\nagy, Jinhyuk Lee, Jungsoo Park, Mark Johnson,\nNoah Smith, Ofir Press, Patrick Lewis, Xiang Deng,\nXinxi Lyu, Zexuan Zhong, UW-NLP members and\nanonymous reviewers for discussion and comments\non the paper. This research was supported by\nNSF IIS-2044660, ONR N00014-18-1-2826, ONR\nMURI N00014- 18-1-2670, an Allen Distinguished\nAward and gifts from AI2. SM is supported by a\nJ.P. Morgan fellowship.\nReferences\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan\nRoth, and Graham Neubig. 2022. Neuro-symbolic\n2106\nlanguage modeling with automaton-augmented re-\ntrieval. In Proceedings of the International Confer-\nence of Machine Learning.\nMikel Artetxe, Jingfei Du, Naman Goyal, Luke Zettle-\nmoyer, and Ves Stoyanov. 2022. On the role of bidi-\nrectionality in language model pre-training. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing.\nAkari Asai, Jungo Kasai, Jonathan H. Clark, Kenton\nLee, Eunsol Choi, and Hannaneh Hajishirzi. 2021.\nXOR QA: Cross-lingual open-retrieval question an-\nswering. In Conference of the North American Chap-\nter of the Association for Computational Linguistics.\nBogdan Babych and Anthony F. Hartley. 2003. Improv-\ning machine translation quality with automatic named\nentity recognition. Proceedings of the International\nEAMT workshop on MT and other Language Tech-\nnology Tools, Improving MT through other Language\nTechnology Tools Resources and Tools for Building\nMT.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In Proceedings of the International\nConference of Machine Learning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proceed-\nings of Advances in Neural Information Processing\nSystems.\nWenlin Chen, David Grangier, and Michael Auli. 2016.\nStrategies for training large vocabulary neural lan-\nguage models. In Proceedings of the Association for\nComputational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in ty-\npologically diverse languages. Transactions of the\nAssociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the Association for Computational Lin-\nguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine learning challenges workshop.\nXiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu,\nand Huan Sun. 2021. ReasonBERT: Pre-trained to\nreason with distant supervision. In Proceedings of\nEmpirical Methods in Natural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Conference of the North American Chap-\nter of the Association for Computational Linguistics.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the International Confer-\nence on Language Resources and Evaluation.\nWilliam T Freeman, Thouis R Jones, and Egon C Pasz-\ntor. 2002. Example-based super-resolution. IEEE\nComputer graphics and Applications.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng\nJuan, et al. 2022. Attributed text generation via\npost-hoc research and revision. arXiv preprint\narXiv:2210.08726.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the Association for Com-\nputational Linguistics.\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2014.\nOptimized product quantization. IEEE Transactions\non Pattern Analysis and Machine Intelligence.\nÉdouard Grave, Armand Joulin, Moustapha Cissé,\nDavid Grangier, and Hervé Jégou. 2017. Efficient\nsoftmax approximation for GPUs. In Proceedings of\nthe International Conference of Machine Learning.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In Proceedings of the Interna-\ntional Conference of Machine Learning.\n2107\nAhmed Hassan, Haytham Fahmy, and Hany Hassan.\n2007. Improving named entity translation by exploit-\ning comparable and parallel corpora. In Proceedings\nof the International Workshop on Acquisition and\nManagement of Multilingual Lexicons.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. In Proceedings of Empirical Methods\nin Natural Language Processing.\nMyles Hollander, Douglas A Wolfe, and Eric Chicken.\n2013. Nonparametric statistical methods. John Wi-\nley & Sons.\nAri Holtzman, Peter West, Vered Schwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of Empirical Methods in Natu-\nral Language Processing.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Knowledge Discovery\nand Data Mining.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nGautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola\nDe Cao, Sebastian Riedel, and Edouard Grave. 2020.\nA memory efficient baseline for open domain ques-\ntion answering. arXiv preprint arXiv:2012.15156.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models. In Proceedings of Empirical Methods\nin Natural Language Processing.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid.\n2010. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and\nmachine intelligence, 33(1):117–128.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the Association for Computa-\ntional Linguistics.\nJan-Christoph Kalo and Leandra Fichtel. 2022. Kamel:\nKnowledge analysis with multitoken entities in lan-\nguage models. In Proceedings of the Conference on\nAutomated Knowledge Base Construction.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge. arXiv\npreprint arXiv:2211.08411.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of Em-\npirical Methods in Natural Language Processing.\nNora Kassner and Hinrich Schütze. 2020. BERT-kNN:\nAdfding a kNN search component to pretrained lan-\nguage models for better QA. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In Proceedings of the International Confer-\nence on Learning Representations.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMayank Kulkarni, Debanjan Mahata, Ravneet Arora,\nand Rajarshi Bhowmik. 2022. Learning rich repre-\nsentation of keyphrases from text. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics.\nTian Lan, Deng Cai, Yan Wang, Heyan Huang, and\nXian-Ling Mao. 2023. Copy is all you need. In Pro-\nceedings of the International Conference on Learning\nRepresentations.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021. Learning dense representations of\nphrases at scale. In Proceedings of the Association\nfor Computational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. In Proceedings of\nAdvances in Neural Information Processing Systems.\n2108\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJoel Mackenzie, Rodger Benham, Matthias Petri, Jo-\nhanne R Trippas, J Shane Culpepper, and Alistair\nMoffat. 2020. Cc-news-en: A large english news\ncorpus. In Proceedings of the ACM International\nConference on Information and Knowledge Manage-\nment.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: understanding rating dimen-\nsions with review text. In Proceedings of the ACM\nconference on Recommender systems.\nRobert C Moore. 2003. Learning translations of named-\nentity phrases from parallel corpora. In Proceedings\nof the European Chapter of the Association for Com-\nputational Linguistics.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchical\nprobabilistic neural network language model. In Pro-\nceedings of the International workshop on artificial\nintelligence and statistics.\nMohammad Norouzi, Ali Punjani, and David J Fleet.\n2012. Fast search in hamming space with multi-\nindex hashing. In 2012 IEEE conference on computer\nvision and pattern recognition , pages 3108–3115.\nIEEE.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe Association for Computational Linguistics.\nNikolaos Pappas, Phoebe Mulcaire, and Noah A. Smith.\n2020. Grounded compositional outputs for adaptive\nlanguage modeling. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1252–1267, Online. As-\nsociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Proceed-\nings of Advances in Neural Information Processing\nSystems.\nAjay Patel, Bryan Li, Mohammad Sadegh Rasooli,\nNoah Constant, Colin Raffel, and Chris Callison-\nBurch. 2022. Bidirectional language models are also\nfew-shot learners. arXiv preprint arXiv:2209.14500.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of Empirical Methods in\nNatural Language Processing.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2019.\nBert is not a knowledge base (yet): Factual knowl-\nedge vs. name-based reasoning in unsupervised qa.\narXiv preprint arXiv:1911.03681.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nOri Ram, Yuval Kirstain, Jonathan Berant, Amir Glober-\nson, and Omer Levy. 2021. Few-shot question an-\nswering by pretraining span selection. In Proceed-\nings of the Association for Computational Linguis-\ntics.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio\nPetroni, Patrick Lewis, Gautier Izacard, Qingfei You,\nChristoforos Nalmpantis, Edouard Grave, and Sebas-\ntian Riedel. 2022. Peer: A collaborative language\nmodel. arXiv preprint arXiv:2208.11663.\nMinjoon Seo, Tom Kwiatkowski, Ankur P Parikh, Ali\nFarhadi, and Hannaneh Hajishirzi. 2018. Phrase-\nindexed question answering: A new challenge for\nscalable document comprehension. In Proceedings\nof Empirical Methods in Natural Language Process-\ning.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur P\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-time open-domain question answering with\ndense-sparse phrase index. In Proceedings of the\nAssociation for Computational Linguistics.\n2109\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. Nearest neighbor zero-shot\ninference. In Proceedings of Empirical Methods in\nNatural Language Processing.\nSidney Siegel. 1957. Nonparametric statistics. The\nAmerican Statistician.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of Empirical Methods in Natural Lan-\nguage Processing.\nZequn Sun, Wei Hu, and Chengkai Li. 2017. Cross-\nlingual entity alignment via joint attribute-preserving\nembedding. In Proceedings of the International Se-\nmantic Web Conference.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing: System Demonstrations.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2020. Scalable zero-\nshot entity linking with dense entity retrieval. In\nProceedings of Empirical Methods in Natural Lan-\nguage Processing.\nFrank F. Xu, Junxian He, Graham Neubig, and Vin-\ncent Josua Hellendoorn. 2022. Capturing structural\nlocality in non-parametric language models. In Pro-\nceedings of the International Conference on Learning\nRepresentations.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Conference of\nthe North American Chapter of the Association for\nComputational Linguistics.\nIkuya Yamada, Akari Asai, and Hannaneh Hajishirzi.\n2021. Efficient passage retrieval with hashing for\nopen-domain question answering. In Proceedings of\nthe Association for Computational Linguistics.\nJinghui Yan, Jiajun Zhang, JinAn Xu, and Chengqing\nZong. 2018. The impact of named entity translation\nfor neural machine translation. In Proceedings of the\nChina Workshop on Machine Translation.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. In Pro-\nceedings of the International Conference on Learning\nRepresentations.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Proceedings of the Association for\nComputational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of Advances in Neural\nInformation Processing Systems.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the International Conference of Ma-\nchine Learning.\nRuiqi Zhong, Charlie Snell, Dan Klein, and Jacob Stein-\nhardt. 2022a. Describing differences between text\ndistributions with natural language. In Proceedings\nof the International Conference of Machine Learning.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. In Conference of the North American Chapter\nof the Association for Computational Linguistics.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022b. Train-\ning language models with memory augmentation. In\nProceedings of Empirical Methods in Natural Lan-\nguage Processing.\n2110\nA Model Details\nA.1 Details of N PM\nApproximation at inference. Given qstart and\nqend, we take the top k tokens with the highest\nsimilarity scores with each of them, and compute\nscores over spans composed by these tokens. Let\nc∗\ni:j be a span in Cfrom the i-th token to the j-th\ntoken, and E(c) ∈Rh be a vector corresponding\nto a token c∈C. We find the top ktokens for the\nstart and the end:\ncs1,cs2,··· ,csk = argTopk\nc∈C\nsim(qstart,E(c)),\nce1,ce2,··· ,cek = argTopk\nc∈C\nsim(qend,E(c))\nusing a fast nearest neighbor search. We then define\na set of candidate phrases ˜C∗as:\n\n\nk⋃\ni=1\nlmax⋃\nj=1\nc∗\nsi:si+j−1\n\n∪\n\n\nk⋃\ni=1\nlmax⋃\nj=1\nc∗\nei−j+1:ei\n\n,\nand predict:\nargmax\nv∗∈V∗\n∑\nc∗∈˜C∗\nI[v∗= c∗]expsim(q,E(c∗)),\nwhere E(c∗) ∈R2h is a vector corresponding to\nc∗, and V∗is a set of any possible n-grams defined\nby the vocabulary V.\nA.2 Training Details\nAll implementation was done with PyTorch (Paszke\net al., 2019), PyTorch Lightning4 and Huggingface\nTransformers (Wolf et al., 2020).\nMasking. We use a masking ratio of 15% for all\nmodels, following the standard in prior work (De-\nvlin et al., 2019; Liu et al., 2019; Joshi et al., 2020).\nWe implement masking as follows: (1) we first\nidentify all possible candidate spans (spans that pos-\nitives are found from other sequences in the batch),\n(2) sample the length of spans to mask from a geo-\nmetric distribution with a hyperparameter p= 0.5,\nand (3) mask the spans with respect to the sampled\nlength until the masking budget has been spent.\nWe do not mask more than 128 spans from one se-\nquence, and do not mask the span if the same span\nhas been masked for more than ten times within\nthe batch in order to prevent repeatedly masking\noverly frequent spans.\n4https://github.com/Lightning-AI/\nlightning\nFor [MASKs] and [MASKe], we use the\n[MASK] vocab from the RoBERTa tokenizer. Note\nthat it is not necessary to use different tokens for\n[MASKs] and [MASKe] since the Transformer can\nhandle positional information.\nA.3 A special case: N PM SINGLE\nAlong with NPM, we introduce NPM SINGLE ,\nwhich outputs a nonparametric distribution over ev-\nery single token in C, instead of a phrase. To some\nextent, NPM is a strict generalization of NPM SIN -\nGLE , and NPM SINGLE still has a problem that\nexisting encoder-only models have, e.g., can only\nfill in the[MASK] with a single token. We however\nthink NPM SINGLE can be useful for some applica-\ntions, e.g., for fine-tuning, as existing encoder-only\nmodels are used for.\nInference. Given a reference corpus C =\n{c1,··· ,cN }, we construct N number of h-\ndimensional vectors c1,··· ,cN ∈Rh by feeding\nthe text into the encoder. At inference time, given a\nquery whose t-th token is [MASK], we feed it into\nthe encoder:\nq1..qL = Encoder(q1..qt−1,[MASK],qt+1..qL).\nWe take qt as a vector that represents the [MASK]\ntoken in the query. Finally, the prediction is made\nby aggregating the similarity scores to the tokens\nin C:\nargmax\nv∈V\n∑\nc∈C\nI[c= v]exp(sim(qt,E(c))),\nwhere E(c) ∈Rh is a vector corresponding to c,\nand Vis the vocabulary set.\nIn practice, since computing scores over all to-\nkens in Cis infeasible, an approximation is made\nby computing scores for the topknearest neighbors\nonly, and treating other tokens to have a similarity\nscore of −Inf. More precisely:\nc1,c2,··· ,ck = argTopk\nc∈C\nsim(qt,E(c))\nare obtained by using an index (e.g., FAISS (John-\nson et al., 2019)), and the following is returned as\na prediction:\nargmax\nv∈V\n∑\n1≤i≤k\nI[ci = v]exp(sim(qt,E(ci))).\n2111\nTraining. Let xi\n1...xi\nL be the i-th sequence in the\nbatch, whose subset is replaced with [MASK] and\nconverted to ˆxi\n1...ˆxi\nL. Both the unmasked sequence\nand the masked sequence are fed into the encoder,\nand each token is mapped into an h-dimensional\nvector:\nxi\n1 ···xi\nL = Encoder( xi\n1 ···xi\nL),\nˆ xi\n1 ··· ˆ xi\nL = Encoder(ˆxi\n1 ··· ˆxi\nL).\nThe training objective is then defined as:\nL∑\nt=1\nI[ˆxt = [MASK]]l(xi\nt,ˆxi\nt),\nwhere l(xi\nt,ˆxi\nt) is\n−log\n∑\ny∈Y+(xi\nt) exp(sim(ˆ xi\nt,y))\n∑\ny∈Y+(xi\nt)∪Y−(xi\nt) exp(sim(ˆ xi\nt,y)).\nHere, sim(·,·) is a similarity function defined in\nSection 3.1, and Y+(xi\nt) and Y−(xi\nt) are positives\nand negatives of xi\nt—tokens from other sequences\nin the batch that share and do not the vocab, respec-\ntively.\nY+(xi\nt) =\n{\nxj\nm|xi\nt = xj\nm and i̸= j\n}\n,\nY−(xi\nt) =\n{\nxj\nm|xi\nt ̸= xj\nm and i̸= j\n}\n.\nA.4 Inference on closed-set tasks\nWhen applying NPM and NPM SINGLE on closed-\nsetk tasks, we closely follow Shi et al. (2022) who\nadapts kNN-LM for zero-shot inference on clas-\nsification tasks. We assume a fuzzy verbalizer:\nf : Y→ ˜V, where Yis a set of labels in the task\nand ˜V∈V is a subset of the vocabulary V. The\nfuzzy verbalizer maps a label to a set of tokens that\nexpress the label, e.g., in a sentiment classification\ntask, f(Positive) includes awesome or great,\nand f(Negative) includes terrible or broken.\nNPM SINGLE is given a query vector q ∈Rh\nand predicts:\nargmax\ny∈Y\n∑\nc∈C\nI[c∈f(y)]exp\n(sim(q,E(c))\nτ\n)\n,\nwhere E(c) ∈Rh is a vector corresponding to c,\nand τ is a hyperparameter.\nNPM is given a query vector q ∈R2h and pre-\ndicts:\nargmax\ny∈Y\n∑\nc∗∈C∗\nI[c∗∈f(y)]exp\n(sim(q,E(c∗))\nτ\n)\n,\nwhere E(c∗) ∈R2h is a vector corresponding to\nc∗. Note that this is essentially equivalent to\nargmax\ny∈Y\n∑\nc∈C\nI[c∈f(y)]exp\n(\nsim(qstart,E(c))\nτ + sim(qend,E(c))\nτ\n)\n.\nWe use τ = 5.0 for both NPM SINGLE and NPM.\nB Evaluation Details\nTable 5 reports statistics and templates on each\ndownstream task, and Table 6 reports statistics of\nthe retrieval corpus used in experiments.\nFor closed-set tasks, we use templates and ver-\nbalizers provided by Shi et al. (2022) for most\ndatasets, except two datasets. For RTE, we use\nthe template from Artetxe et al. (2022). For Subj,\nwe write our own template, motivated by Zhong\net al. (2022a) that found Subj is mainly about dif-\nferentiating a review and a summary. For open-set\ntasks, we use templates provided by the original\nauthors, except NQ and TQA for which we use the\ntemplates from GPT-3 (Brown et al., 2020). Due\nto limited computation resource, we subsample the\ndata to include up to 3,000 examples, following the\nstandard from prior work (Zhao et al., 2021; Shi\net al., 2022). For closed-set tasks, we use exactly\nthe same set of data as Shi et al. (2022), and for\nopen-set tasks, we use the same script to subsample\nthe data. For LAMA T-REx and Google RE, we\nsubsample up to 1,000 examples for each of 1, 2, 3\nand 4+ grams. For the entity translation task, we\nsubsample up to 1,000 examples per language.\nThe following is a more detailed description of\nopen-set tasks used in Section 6.\nLAMA (Petroni et al., 2019)is a factual probing\nbenchmark that is designed to quantify the amount\nof factual knowledge in the model. It requires\nthe model to predict the object given a subject-\nrelation tuple in a cloze format. We use two ver-\nsions of LAMA (Petroni et al., 2019): (1) LAMA\nT-REx, derived from Elsahar et al. (2018) and (2)\nLAMA Google-RE, derived from the Google-RE\ncorpus.5 For each version, we additionally consider\nthe UHN (UnHelpfulNames) subset (Poerner et al.,\n2019)) where instances whose subject strongly\nhints the object by names (e.g., Apple Watch\n5https://code.google.com/archive/p/\nrelation-extraction-corpus\n2112\nDataset |D| |Ds|# labels Example\nClosed-set tasks\nAGN 120,000 3,000 4 Indiana defends its NCAA mens’s soccer title by edging UC Santa Barbara in penalty kicks.\nThe text topic is about [MASK]. ([MASK]={politics, sports, business, technology})\nYahoo 60,000 3,000 10\nCompany for cemecal at espaniea? Answer: Can you give us more info? The text topic is\nabout [MASK]. ([MASK]={society, science, health, education, computer, sports, business,\nentertainment, family, politics})\nSubj 2,000 2,000 2 He tells mitchell that he is now in debt. This is a [MASK].\n([MASK]={review, summary})\nSST-2 2,210 2,210 2 It was [MASK]. ([MASK]={great, terrible})\nMR 2,000 2,000 2 Simplistic, silly and tedious. It was [MASK]. ([MASK]={great, terrible})\nRT 1,066 1,066 2 weird. rewarding. It was [MASK]. ([MASK]={great, terrible})\nCR 2,000 2,000 2 I am very pleased so far. It was [MASK]. ([MASK]={great, terrible})\nAmz 400,000 3,000 2 It was [MASK]. ([MASK]={great, terrible})\nRTE 277 277 2 Most commercial logwood is grown in Honduras, right? [MASK], plants are grown in water\nor in substance other than soil. ([MASK]={Yes, No})\nOpen-set tasks\nLAMA T-REx 34,039 2,983 - A VCDH is owned by [MASK].\nLAMA Google RE 5,200 1,856 - Joshua Mathiot died in [MASK].\nKAMEL 46,800 3,000 - What is followed by So-Lo? Answer: [MASK].\nNQ 3,610 3,000 - who sang i ran all the way home? The answer is: [MASK].\nTQA 11,313 3,000 - Who wrote the opera Carmen? The answer is: [MASK].\nTempLAMA22\n19\n- changed 3,360 3,000 - Contributor Covenant is developed by [MASK].\n- unchanged 3,360 3,000 - Atari 8-bit family is developed by [MASK].\nEntity translation 10,452 6,622 - The Korean translation of Banpo Bridge is: [MASK].\nTable 5: Statistics of downstream datasets. |D|and |Ds|indicate the number of test examples on the original data\nand the subsampled data, respectively. See Appendix B for details.\nCorpus name Source |C| Datasets used\nEn-Wiki+CCNews Subset of En-Wiki 08/01/2019 and CCNews 126M AGN, Yahoo, RTE\nSubjectivity corpus Raw IMDB 15M Subj\nReview corpus Amazon and IMDB 62M SST-2, MR, RT, CR, Amz\nEn-Wiki 2019 En-Wiki 08/01/2019 810M All open-set tasks\nEn-Wiki 2022 En-Wiki 08/01/2022 858M TempLAMA 22\n19\nTable 6: Statistics of the retrieval corpus. |C|indicates the number of tokens in the corpus.\nand Apple) are excluded. We also consider the\nhard subset of T-Rex from Zhong et al. (2021).\nNote that Petroni et al. (2019) only include\ntriples whose object is one token based on\nBERT (Devlin et al., 2019); however, with a differ-\nent pretrained model like RoBERTa, entities could\nbe multiple BPE tokens. Entities that are splitted\ninto multiple BPE tokens are more rare entities.\nKAMEL (Kalo and Fichtel, 2022) is another fac-\ntual probing task as LAMA but with a few key\ndifferences to make it more general and broad: (1)\nit includes a broader coverage of triples, (2) it re-\nmoves the constraint that the object is one token\nbased on BERT, (3) it includes objects with literal\nvalues, and (4) it has a question answering format.\nNatural Questions (NQ, Kwiatkowski et al.\n(2019)) and TriviaQA (TQA, Joshi et al. (2017))\nare two welll-studied open-domain question an-\nswering datasets. We use the open-version of\nNQ (Lee et al., 2019) and TQA where the ques-\ntion is the only input and the model should use its\nknowledge to answer the question.\nTempLAMA22\n19 is a task that requires probing\nknowledge with temporal updates. The task is first\nintroduced by Dhingra et al. (2022) and Jang et al.\n(2022); however, we could not use either of existing\ndata as their time split do not match our training.\nWe therefore create the data by using a script pro-\nvided by Dhingra et al. (2022) but using the 2019\nand the 2022 dumps. We take Wikipedia triples\nwhose relations are available for a template from\neither Petroni et al. (2019) or Dhingra et al. (2022).\nWe then include triples whose object entities differ\nbetween the 2019 dump and the 2022 dump (due\nto the entity being updated), or only appear in the\n2022 dump (due to the subject or the relation being\n2113\nISO Code Language |D| |D s|\nzh Chinese 3,199 1,000\nar Arabic 2,013 1,000\nel Greek 1,618 1,000\niw Hebrew 841 841\nru Russian 758 758\njp Japanese 471 471\nhi Hindi 427 427\nko Korean 418 418\npl Polish 177 177\ntr Turkish 150 150\ncs Czech 109 109\nta Tamil 80 80\nth Thai 74 74\nmn Mongolian 64 64\nml Malayalam 53 53\nTOTAL 10,452 6,622\nTable 7: Statistics of the entity translation benchmark.\nLanguages are sorted based on their availabilities.\nadded) to the changed set. Otherwise, triples are\nincluded in the unchanged set. We additionally find\nthat many triples are overly difficult because the\nfact is extremely niche and not really known. We\nthus filter the data to only include facts that appear\nin Wikipedia. Specifically, we include triples if the\nsubject has a corresponding Wikipedia page and\nthe object entity appears in that Wikipedia page.\nEntity translation requires translating an entity\nfrom English to other languages that are not Latin\nbased. While this is mainly to evaluate if the model\ncan generate rare or unseen characters that are not\nin English, the entity translation task itself is a vital\nand challenging task in real applications such as\nmachine translation (Babych and Hartley, 2003;\nYan et al., 2018) and cross-lingual question answer-\ning (Clark et al., 2020; Asai et al., 2021). It is often\nbeyond a series of simple translations of each word,\nor spelling out its pronunciation (Moore, 2003; Has-\nsan et al., 2007; Sun et al., 2017). For instance, the\nKorean translation of Banpo Bridge in Figure 1 (반\nᄑ ᅩᄃ ᅢᄀ ᅭ) is not the concatenation of the translations\nof Banpo and Bridge (반ᄑ ᅩᄃ ᅡᄅ ᅵ).\nWe first identify a list of 15 non-Latin lan-\nguages: Arabic (ar), Czech (cs), Greek (el),\nHindi (hi), Hebrew (iw), Japanese (jp), Korean\n(ko), Malayalam (ml), Mongolian (mn), Polish\n(pl), Russian (ru), Tamil (ta), Thai (th), Turk-\nish (tr), and Chinese (zh). We then implement\nheuristics to identify entities and their transla-\ntions from English Wikipedia. Specifically, we\nparse the first paragraph of each Wikipedia arti-\ncle and pair the found translation with a topic en-\ntity of the article. For instance, a Korean transla-\ntion of Banpo Bridge is found from the first\nsentence of https://en.wikipedia.org/\nwiki/Banpo_Bridge. Per-language statistics\nare reported in Table 7.\nC Additional Results\nFull results on knowledge tasks. Table 8 reports\nfull results on five knowledge tasks. See Figure 6\nfor an illustration, and Section 6.4 for discussion.\nComparison to few-shot GPT-3. Table 9 com-\npares zero-shot NPM SINGLE and NPM with zero-\nand four-shot GPT-3. Our zero-shot models outper-\nform 500x larger zero-shot GPT-3 and 7.6x larger\n4-shot GPT-3, but lag behind 4-shot GPT-3 that is\n19x or larger. We think future work can explore\nextending our models to a few-shot setup.\nAdditional qualitative results. Figure 9 depicts\npredictions from RoBERTa and NPM in topic clas-\nsification, choosing a label between four candi-\ndates: health, computer, travel and politics. All\nthree examples contain the wordtorch, but with dif-\nferent meanings, e.g., an infectious diseases, a tool,\nand a computer library. RoBERTa predictshealth\nfor all of them, while NPM predicts health, travel\nand computer, which are all correct predictions.\nAs in Figure 5, we find that representations from\nNPM enable better word sense disambiguation:\nthe pairwise similarities between between differ-\nent meanings of torch are significantly lower than\nthe pairwise similarities between other tokens that\nshare the meaning.\nEntity translation given an oracle passage. We\nevaluate models on the entity translation task where\nan oracle passage—a passage that is guaranteed to\ncontain the translation information—is provided to\nthe model. Baselines prepend oracle passages to\nthe input, as it does with the retrieve-and-generate\napproach. NPM uses oracle passages to restrict the\nsearch space.\nTable 11 reports results. While performance\noverall increases compared to when the oracle pas-\nsage is not provided, the overall comparison be-\ntween models does not change from Table 10: (1)\nall monolingual models significantly suffer, ex-\ncept for a couple of languages that are derived\nfrom Latin; (2) NPM significantly outperforms all\nmonolingual models; (3) NPM even outperforms\n3.4x larger mT5 and 20x larger BLOOM, and ap-\nproaches 11x larger mT5.\n2114\nModel #Params C T-REx Google RE KML TQA NQ\nAll UHN Hard All UHN\nBaselines (encoder-decoder)\nT5 2.2x 13.3 5.5 10.7 1.1 0.4 1.6 4.2 0.5\nT5 3B 8.5x 12.1 8.2 11.5 2.1 0.7 3.6 9.0 2.0\nBM25 + T5 2.2x ✓ 22.2 20.3 22.4 16.4 16.6 13.9 31.4 5.2\nBM25 + T5 3B 8.5x ✓ 21.6 19.0 21.8 18.5 15.5 16.2 39.6 10.8\nBaselines (decoder-only)\nOPT 2.7B 7.6x 9.8 6.7 8.3 0.0 0.0 1.6 9.9 2.1\nGPT-3 2.7B 7.6x 4.4 2.6 3.8 0.0 0.0 2.1 5.2 1.1\nOPT 6.7B 19x 11.6 9.9 10.7 0.6 0.3 3.2 20.9 4.2\nGPT-3 6.7B 19x 8.1 5.0 6.7 0.0 0.0 2.1 12.4 3.1\nOPT 13B 37x 15.0 12.7 12.7 0.3 0.3 2.5 22.5 4.2\nGPT-3 13B 37x 16.4 13.7 15.5 0.8 0.4 2.2 25.5 5.2\nGPT-3 175B 500x 25.7 24.1 24.7 1.1 1.0 6.5 49.0 11.4\nBM25 + OPT 2.7B 7.6x ✓ 14.8 14.1 13.8 4.4 3.7 11.3 28.5 8.3\nBM25 + GPT-3 2.7B 7.6x ✓ 3.5 3.4 3.6 0.1 0.1 5.2 14.5 6.1\nBM25 + OPT 6.7B 19x ✓ 14.8 14.3 14.9 4.1 3.3 8.2 29.9 10.7\nBM25 + GPT-3 6.7B 19x ✓ 14.9 15.3 15.1 4.4 3.5 7.0 21.1 8.8\nBM25 + OPT 13B 37x ✓ 18.9 19.1 19.3 3.8 3.1 10.6 34.0 10.7\nBM25 + GPT-3 13B 37x ✓ 22.2 22.7 22.4 11.8 11.2 8.9 32.4 11.2\nBM25 + GPT-3 175B 500x ✓ 32.0 31.6 31.3 11.4 11.9 12.2 44.9 6.4\nOurs (encoder-only, nonparametric)\nNPM 1.0x ✓ 34.5 29.0 32.1 27.9 23.0 15.6 32.2 10.8\nTable 8: Results on open-set tasks (numbers used in Figure 6). # Params indicates the relative number of model\nparameters compared to RoBERTa large (354M), andCindicates whether a text corpus is used. For LAMA (T-REx\nand Google RE), the macro-averaged EM over 1, 2, 3 and 4+ grams are reported. All models are zero-shot. NPM\nsignificantly outperforms larger parameters models, either with and without a retrieval-and-generate approach that\nuses BM25.\nModel #Params AGN SST-2\n0-shot 4-shot 0-shot 4-shot\nBaselines (Parametric)\nRoBERTa x1.0 71.3 - 84.5 -\nGPT-3 2.7B (Zhao et al., 2021) x7.6 44.7 43.3 57.2 59.1\n+ CC (Zhao et al., 2021) x7.6 63.2 71.1 71.4 79.9\nGPT-3 2.7B (Holtzman et al., 2021) x7.6 69.0 - 53.8 88.1\n+ PMI (Holtzman et al., 2021) x7.6 67.9 - 72.3 87.7\nGPT-3 6.7B (Holtzman et al., 2021) x19 64.2 - 54.5 92.9\n+ PMI (Holtzman et al., 2021) x19 57.4 - 80.0 79.8\nGPT-3 13B (Holtzman et al., 2021) x37 69.8 - 69.0 85.4\n+ PMI (Holtzman et al., 2021) x37 70.3 - 81.0 86.9\nGPT-3 175B (Zhao et al., 2021) x500 43.9 61.0 71.6 93.6\n+ CC (Zhao et al., 2021) x500 73.9 85.9 75.8 94.3\nGPT-3 175B (Holtzman et al., 2021) x500 75.4 - 63.6 89.9\n+ PMI (Holtzman et al., 2021) x500 74.7 - 71.4 95.5\nOurs (Nonparametric)\nNPM SINGLE x1.0 74.2 - 86.8 -\nNPM x1.0 74.5 - 87.2 -\nTable 9: Comparison to GPT-3 on AG News and SST-2.# Paramsindicates the relative number of model parameters\ncompared to RoBERTa large (354M). All GPT-3 numbers are taken from previous work.k-shot indicates that the\nmodel performs in-context learning with klabeled examples with no gradient updates. We report on SST-2 and\nAGN, because they are all datasets shared between our paper and previous papers that report GPT-3 results (Zhao\net al., 2021; Holtzman et al., 2021). Our zero-shot models outperform 500x larger zero-shot GPT-3 and 7.6x larger\n4-shot GPT-3, but lag behind 4-shot GPT-3 that is 19x or larger.\n2115\nSim(torch, <m>) \nSim(torch, <m>) \nSim(torch, <m>) \nSim(torch, torch, torch) \nSim(<m>,  <m>, <m>)\n= 30.8 \n= 30.9 \n= 30.1 \n= 29.4–30.4 \n= 29.7–30.9\n= 22.7 \n= 22.1 \n= 24.2 \n= 12.3—16.2 \n= 15.8—18.0\nSim(torch, <m>) \nSim(torch, <m>) \nSim(torch, <m>) \nSim(torch, torch, torch) \nSim(<m>,  <m>, <m>)\nRoBERTa\nNPM SINGLE\nA torch infection in pregnancy. The topic is about <mask>. \nIs a torch permitted on board? The topic is about <mask>. \nThe version of torch is 1.12.0. The topic is about <mask>.\nHealth \nHealth \nHealth\nA torch infection in pregnancy. The topic is about <mask>. \nIs a torch permitted on board? The topic is about <mask>. \nThe version of torch is 1.12.0. The topic is about <mask> .\nHealth \nTravel \nComputer\nTopic classiﬁcation: {health, computer, travel, politics}\nRetrieved context for <mask>: “.. But it is still unclear what these findings mean for infant health, especially since \nearly infancy is such an important developmental time”\nRetrieved context for <mask>: Devices running Windows 8.1 or above support the last version of the app as well. \n(…) is one of the most popular computer and video game.\nRetrieved context for <mask>: Travel with dogs airplane travel, Dog travel, road trips, Trip preparation. 4 comments \non “Dog friendly travel tips — comfort for both of you!”\nFigure 9: Predictions from RoBERTa (baseline) and NPM on a topic classification task (classes={health, computer,\ntravel, politics}). The bottom indicates the context NPM retrieves to fill in [MASK]. On the right, we indicate the\ntoken-wise similarity scores. NPM assigns significantly lower scores to the token pairs with distinct meanings than\nto the token pairs with the similar meaning, e.g., torch (a disease) and torch (a tool).\nModel #Params #L ar cs el hi iw jp ko ml mn pl ru ta th tr zh A VG\nBaselines, English-only\nT5 2.2x 0.0 0.0 0.0 0.0 0.1 0.2 0.0 0.0 0.0 1.1 0.9 0.0 0.0 0.0 0.0 0.2\nT5 3B 8.5x 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.6 1.3 0.0 0.0 0.0 0.0 0.5\nOPT 6.7B 19x 0.0 0.0 0.3 0.0 0.0 0.0 3.1 0.0 0.0 0.0 2.9 0.0 0.0 0.0 0.0 0.4\nOPT 13B 37x 1.5 0.0 1.2 0.7 0.0 0.0 1.4 0.0 0.0 1.1 7.4 0.0 0.0 1.3 0.1 1.0\nBM25 + T5 2.2x 0.0 5.5 0.3 0.2 0.5 0.0 0.2 1.9 0.0 6.8 0.8 1.2 0.0 11.3 0.0 1.9\nBM25 + T5 3B 8.5x 0.0 12.8 0.1 0.7 0.2 0.8 0.0 0.0 1.6 28.8 1.7 0.0 0.0 20.0 0.0 4.4\nBM25 + OPT 6.7B 19x 26.4 54.1 15.5 11.2 11.8 14.4 19.6 5.7 3.1 47.5 52.5 6.2 12.2 32.0 22.7 22.3\nBM25 + OPT 13B 37x 17.3 51.4 24.9 15.5 27.8 12.3 22.0 11.3 7.8 45.8 48.2 8.8 18.9 34.0 23.3 24.6\nOurs, English-only\nNPM 1.0x 51.9 33.0 60.9 63.2 63.7 59.0 60.5 50.9 46.9 33.3 61.2 51.2 60.8 32.7 56.9 52.4\nReferences, Multilingual\nmT5 3.4x 101 0.3 1.8 1.5 0.0 0.4 1.9 0.7 0.0 0.0 1.1 4.6 2.5 1.4 3.3 0.7 1.3\nmT5 XL 11x 101 4.4 3.7 4.9 6.8 0.7 2.3 4.1 1.9 4.7 5.6 8.0 5.0 0.0 6.7 2.8 4.1\nBLOOM 3B 8.5x 46 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.0\nBLOOM 7.1B 20x 46 0.0 0.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 0.1\nBM25 + mT5 3.4x 101 12.4 22.9 21.6 9.8 12.5 28.9 19.1 11.3 18.8 15.8 16.0 17.5 28.4 16.7 33.4 19.0\nBM25 + mT5 XL 11x 101 64.4 64.2 54.3 65.6 62.7 55.4 69.4 43.4 62.5 52.0 53.7 37.5 50.0 48.7 65.0 56.6\nBM25 + BLOOM 3B 8.5x 46 24.2 25.7 1.7 13.3 15.1 18.5 17.9 5.7 6.2 21.5 11.1 10.0 27.0 18.0 44.5 17.4\nBM25 + BLOOM 7.1B 20x 46 19.0 49.5 11.4 20.8 8.1 30.1 25.4 5.7 6.2 54.2 29.0 6.2 37.8 33.3 53.7 26.0\nTable 10: Results on the entity translation task. #L indicates the number of languages multilingual models are\ntrained on. Bold and Bold indicate the best among monolingual models and the best including multilingual models,\nrespectively. NPM significantly outperforms all existing monolingual models, and approaches or outperforms larger\nmultilingual models.\nModel #Params #L ar cs el hi iw jp ko ml mn pl ru ta th tr zh A VG\nBaselines, English-only\nT5 2.2x 0.0 13.8 0.7 0.9 0.6 1.1 0.5 3.8 1.6 15.8 1.3 7.5 0.0 16.7 0.4 4.0\nT5 3B 8.5x 0.2 21.1 1.0 0.7 0.7 2.3 1.2 3.8 4.7 37.3 2.9 8.8 1.4 30.7 0.4 7.3\nOPT 6.7B 19x 24.4 56.9 22.9 15.5 19.7 19.1 32.5 24.5 3.1 56.5 60.9 22.5 23.0 46.0 30.2 30.5\nOPT 13B 37x 20.7 62.4 22.7 15.7 30.9 17.6 36.1 18.9 15.6 56.5 52.2 22.5 35.1 48.7 40.0 33.0\nOurs, English-only\nNPM 1.0x 70.3 44.0 76.8 74.0 82.4 71.3 73.2 58.5 59.4 45.2 71.5 68.8 66.2 45.3 74.5 65.4\nReferences, Multilingual\nmT5 3.4x 101 19.4 25.7 30.8 19.0 20.6 33.8 28.2 28.3 40.6 18.6 23.1 30.0 29.7 26.7 37.4 27.5\nmT5 XL 11x 101 83.2 76.1 69.6 81.5 77.4 68.2 85.2 49.1 67.2 65.5 62.7 51.2 68.9 64.0 79.0 69.9\nBLOOM 3B 8.5x 46 51.2 27.5 3.1 30.2 34.1 34.0 30.9 11.3 7.8 28.2 23.0 17.5 37.8 22.0 70.1 28.6\nBLOOM 7.1B 20x 46 29.6 43.1 12.0 27.6 12.2 32.5 30.9 9.4 15.6 59.3 38.1 13.8 43.2 32.0 65.5 31.0\nTable 11: Results on the entity translation task given an oracle passage. #L indicates the number of languages\nmultilingual models are trained on. Bold and Bold indicate the best excluding multilingual models and the best\nincluding multilingual models, respectively.\n2116\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection following Section 6 (No section number).\n□\u0013 A2. Did you discuss any potential risks of your work?\nAppendix E.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3 and 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe include citations to the pretraining data and pretrained models (Section 3.3 and Appendix B) and\nevaluation datasets (Section 4.1, 5.1 and Appendix C).\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe license and terms for use will be released in the open-sourced repo, which we do not include in\nthe submission in order to keep anonymity.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe discuss how we use the evaluation datasets in detail in Section 4, 5 and Appendix C.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use standardalized, research data/benchmarks, and checked that the authors of the original\npapers tried their best to ensure the data does not contain names or uniquely identiﬁes individual\npeople or offtensive content. Nonetheless, we’ve discussed them in \"Potential Risk” (Appendix E).\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe reported the language of the model/data and their domains in Section 3.3, 4, 5, Appendix B and\nAppendix C.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nWe included statistics of datasets in Appendix C.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2117\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe include training details and related information in Section 3.3 and Appendix B.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe discuss detailed experimental setups in Section 3.3, 4 and 5, Appendix B and C. We evaluated\nmodels in a zero-shot setup, thus there is no hyperparameter tuning involved.\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSince all experiments are about zero-shot evaluation, all results are based on a single run and are\ndeterministic.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe described and cited packages that we use in the paper. For complete reproducibility, we will also\nopen-source the code, which we do not include in the submission in order to keep anonymity.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n2118",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8582572340965271
    },
    {
      "name": "Softmax function",
      "score": 0.7563679218292236
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6653082370758057
    },
    {
      "name": "Natural language processing",
      "score": 0.6596599817276001
    },
    {
      "name": "Language model",
      "score": 0.6324078440666199
    },
    {
      "name": "Phrase",
      "score": 0.571111261844635
    },
    {
      "name": "Nonparametric statistics",
      "score": 0.5531168580055237
    },
    {
      "name": "Vocabulary",
      "score": 0.5390912890434265
    },
    {
      "name": "Security token",
      "score": 0.4933682978153229
    },
    {
      "name": "Parametric statistics",
      "score": 0.4533797800540924
    },
    {
      "name": "Word (group theory)",
      "score": 0.4472561180591583
    },
    {
      "name": "Speech recognition",
      "score": 0.3770938813686371
    },
    {
      "name": "Linguistics",
      "score": 0.13899093866348267
    },
    {
      "name": "Artificial neural network",
      "score": 0.1205950677394867
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}