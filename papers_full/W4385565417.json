{
  "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
  "url": "https://openalex.org/W4385565417",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103333238",
      "name": "Xiaoyang Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2925537799",
      "name": "Shane Storks",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A1902821446",
      "name": "Richard Lewis",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2160072504",
      "name": "Joyce Chai",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3117577445",
    "https://openalex.org/W572012434",
    "https://openalex.org/W3044573113",
    "https://openalex.org/W3176187895",
    "https://openalex.org/W3177100526",
    "https://openalex.org/W4283733941",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W3099455966",
    "https://openalex.org/W2997399961",
    "https://openalex.org/W4285255684",
    "https://openalex.org/W2963176474",
    "https://openalex.org/W1588334983",
    "https://openalex.org/W2132200584",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2134669735",
    "https://openalex.org/W4385572719",
    "https://openalex.org/W4224296133",
    "https://openalex.org/W2963690694",
    "https://openalex.org/W1660519191",
    "https://openalex.org/W2911637930",
    "https://openalex.org/W4313163053",
    "https://openalex.org/W2950824039",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4229954498",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4382469053",
    "https://openalex.org/W2168013333",
    "https://openalex.org/W2625237928",
    "https://openalex.org/W2147769698",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2989899308",
    "https://openalex.org/W2120659705",
    "https://openalex.org/W2913142708",
    "https://openalex.org/W2041308297",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3035245400",
    "https://openalex.org/W3031769198",
    "https://openalex.org/W4385572747",
    "https://openalex.org/W2147096324"
  ],
  "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven’s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs’ analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1953–1969\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nIn-Context Analogical Reasoning with Pre-Trained Language Models\nXiaoyang Hu12∗\nShane Storks1∗\nRichard L. Lewis2† Joyce Chai1†\n1Computer Science and Engineering Division, University of Michigan\n2Department of Psychology, University of Michigan\n{nickhu, sstorks, rickl, chaijy}@umich.edu\nAbstract\nAnalogical reasoning is a fundamental capacity\nof human cognition that allows us to reason\nabstractly about novel situations by relating\nthem to past experiences. While it is thought\nto be essential for robust reasoning in AI sys-\ntems, conventional approaches require signif-\nicant training and/or hard-coding of domain\nknowledge to be applied to benchmark tasks.\nInspired by cognitive science research that has\nfound connections between human language\nand analogy-making, we explore the use of in-\ntuitive language-based abstractions to support\nanalogy in AI systems. Specifically, we apply\nlarge pre-trained language models (PLMs) to\nvisual Raven’s Progressive Matrices (RPM), a\ncommon relational reasoning test. By simply\nencoding the perceptual features of the problem\ninto language form, we find that PLMs exhibit a\nstriking capacity for zero-shot relational reason-\ning, exceeding human performance and nearing\nsupervised vision-based methods. We explore\ndifferent encodings that vary the level of ab-\nstraction over task features, finding that higher-\nlevel abstractions further strengthen PLMs’ ana-\nlogical reasoning. Our detailed analysis reveals\ninsights on the role of model complexity, in-\ncontext learning, and prior knowledge in solv-\ning RPM tasks.\n1 Introduction\nHumans are constantly presented with novel prob-\nlems and circumstances. Rather than understand\nthem in isolation, we try to connect them with past\nexperiences. With any luck, we might find an anal-\nogy: a mapping between relevant aspects of this\nnew situation and a past situation, which helps\nform abstractions that allow us to reason more ef-\nfectively in the future (Holyoak, 1984). Analogy\nis thought to underpin humans’ robust reasoning\nand problem solving capabilities (Hofstadter and\n∗Authors contributed equally to this work.\n†Equal advising contribution.\nLanguage-Based Abstractions\n… ……\n…\n…\n…Pre-Trained Language Model\nGeneratedPrompts\n8-Way Visual Raven’s Progressive Matrix (RPM)\nP(|)\nFigure 1: Raven’s Progressive Matrices (Raven and\nCourt, 1938; Zhang et al., 2019a) are an analogy-making\ntask where one must infer the missing matrix item based\non abstract rules instantiated in the first two rows. To\ndemonstrate the potential analogical reasoning skills\nin pre-trained language models, we develop language-\nbased abstractions over their key perceptual features,\nthen prompt them to select the completion of the matrix.\nSander, 2013), and thus it is believed to be prereq-\nuisite in order to enable the same in AI systems.\nHowever, conventional approaches struggle with\nanalogy-making, and are trained on thousands of\nexamples to achieve any success on benchmark\ntasks. This is unsatisfying, as humans are capa-\nble of analogy-making without explicit training,\nand such analogy-making should enable zero-shot\ngeneralization to new situations (Mitchell, 2021).\nInterestingly, a body of work in cognitive sci-\nence suggests that analogy-making and relational\nreasoning are connected to humans’ symbol sys-\ntem and language capabilities (Gentner, 2010). For\nexample, Gordon (2004) finds that members of an\nAmazonian tribe that count only with words for\n“one,” “two,” and “many” struggle to make analo-\n1953\ngies with higher numbers. Further, Gentner et al.\n(2013) find that deaf children whose sign language\ndoes not involve spatial relations are outperformed\nby hearing children on a spatial relational reason-\ning task, while Christie and Gentner (2014) find\nthat assigning even nonsensical names to relations\nenhances children’s relational reasoning. All of\nthis demonstrates that language serves as a power-\nful way for humans to abstract and better reason\nabout the overwhelming and complex percepts we\nencounter in the world.\nIn this work, we explore whether language may\nserve a similar purpose in AI systems. Specifically,\nwe apply contemporary autoregressive pre-trained\nlanguage models (PLMs) to Raven’s Progressive\nMatrices (RPM), an example of which is shown in\nFigure 1. RPM is a widely used psychometric test\nfor relational reasoning that requires inducing an\nabstract rule from just two examples of short se-\nquences of groups of shapes, and then applying the\nrule to complete a new partial sequence (Raven and\nCourt, 1938). This task makes minimal assump-\ntions about the test taker’s prior knowledge, and\nis thus thought to provide a good estimate for gen-\neral intelligence (Holyoak, 2012). On the RA VEN\ndataset (Zhang et al., 2019a), we find that given\nthe ability to perceive key features of RPMs, large\nPLMs exhibit a surprising capacity for zero-shot re-\nlational reasoning, approaching that of supervised\nvision-based deep learning approaches and even hu-\nmans. We propose three levels of abstraction over\nthe language features of the task using name assign-\nment and task decomposition, and find that each\nabstraction further strengthens PLMs’ relational\nreasoning. Our results and detailed analysis offer\ninsights on PLM performance, including the role of\nmodels’ complexity, in-context learning, and prior\nknowledge in emergent relational reasoning, and\nsuggest that they could play an important role in\nfuture cognitive architectures for analogy-making.2\n2 Related Work\nPast work has studied analogy in AI across var-\nious domains. Mitchell (2021) provides a com-\nprehensive overview of these efforts, especially\nthose applied in idealized symbolic domains. Here,\nsymbolic and probabilistic methods have tradition-\nally been applied (Gentner, 1983; Hofstadter and\nMitchell, 1994; Lake et al., 2015). However, these\n2Experiment code is available at https://github.com/\nhxiaoyang/lm-raven.\napproaches typically require hard-coding domain-\nspecific concepts, and require substantial search\nthrough domain knowledge to operate on their tar-\nget problems, thus making them unscalable. The\ncreation of large-scale image datasets for analogy\ntasks here (Zhang et al., 2019a; Hu et al., 2021;\nOdouard and Mitchell, 2022) have enabled further\nresearch with deep learning and neuro-symbolic\nmethods (Hill et al., 2019; Spratley et al., 2020;\nKim et al., 2020; Zhang et al., 2021), which bring\nthe advantage of requiring less ad-hoc encoding of\ndomain knowledge, but require thousands of train-\ning examples to learn the tasks, still limiting their\ngeneralization capability.\nOther work has explored AI systems’ analogy-\nmaking in real-world domains, including in natural\nimages (Teney et al., 2020; Bitton et al., 2022) and\nlanguage (Li et al., 2020; Chen et al., 2022; Sul-\ntan and Shahaf, 2022), especially lexical analogies\n(Turney et al., 2003; Turney, 2008; Speer et al.,\n2008; Mikolov et al., 2013b,a; Linzen, 2016; Lu\net al., 2019). However, these domains make it diffi-\ncult to control the prior knowledge required to solve\ntasks (Mitchell, 2021), and in the context of recent\ngenerative foundation models that are extensively\npre-trained on natural data, it becomes difficult to\nseparate analogy learning from distributional pat-\nterns that can be overfit. Unlike prior work, we\napply such foundation models for language to ana-\nlogical reasoning in a zero-shot setting, bypassing\nthe requirement of hard-coding domain knowledge\nor training models on task-specific data. Further-\nmore, while contemporaneous work has applied\nPLMs to a variety of simpler relational reasoning\ntasks in language (Webb et al., 2022), we systemat-\nically explore the advantage of using language to\nabstract over complex visual features of the task,\nopening questions about how the powerful sym-\nbol systems learned in PLMs may support robust,\nperception-driven reasoning in future AI systems.\n3 Raven’s Progressive Matrices\nRaven’s progressive matrices (RPM) are abstract\nrelational reasoning tasks used in cognitive psy-\nchology to test humans’ analogy-making (Raven\nand Court, 1938). Each instance of RPM is a ma-\ntrix consisting of 9 items arranged in a square, the\nlast of which must be selected from a set of choices.\nEach item consists of several perceptual attributes,\nsuch as shape, color, or more abstract features.\nWithin each row of the matrix, a relation is applied\n1954\nLayoutEntity\n Component StructureType  Size  Color(    ,,    )\n(      ,       )Position    NumberItem Type\nSub-TasksCenter2x2Grid  3x3Grid\nL-R\n O-ICU-D O-IG\nX X XXXX\nFigure 2: Illustration of the compositional nature of\nentities, layouts, and component structures in RA VEN,\nand their unique attributes. We provide example items\nfrom sub-tasks each item type appears in.\nover these attributes, such as progression of numer-\nical values associated with these attributes. Given\nthe first two rows of the matrix, the challenge of\nthe task is to identify the relations being applied to\nitems, and apply them analogously in the third row\nto infer the missing ninth item. Successfully solv-\ning an RPM requires tackling two sub-problems:\nperception of each item’s attributes, and reasoning\nover multiple items’ attributes to infer and apply\nrelations.\n3.1 RA VEN Dataset\nWe focus our study on RA VEN (Zhang et al.,\n2019a), which provides a large-scale benchmark\nfor RPM tasks for training and evaluation of AI sys-\ntems. Each RPM has 8 possible candidate items to\ncomplete it. As shown in Figure 2, each item may\nconsist of compositional entities, layouts, and/or\ncomponent structures, and RA VEN provides a suite\nof increasingly complex sub-tasks built from these\nelements. We introduce their unique attributes be-\nlow, as well as relations that may occur over them\nacross items in the matrix.\nEntities. A single entity has a type (i.e., shape),\nsize, and color selected from a small number\nof classes. Each of these attributes is associated\nwith a number: type with the number of sides\nin the entity’s shape, size with its diameter, and\ncolor with the darkness of its shading. The sim-\nplest sub-task of RA VEN isCenter, where each\nitem only consists of a single entity.\nLayouts. Layouts of entities bring additional\nhigher-level attributes to items, specifically the\nnumber (i.e., count) and position of entities\nwithin a layout. In the 2x2Grid and 3x3Grid\nsub-tasks of RA VEN, each item consists of multi-\nple entities arranged in a grid.\nComponent structures. Items may also be\ncomposed of multiple sub-items or components;\nRA VEN includes four sub-tasks that introduce this\neven higher-level challenge: L-R, U-D, and O-IC,\neach of which consist of two single entities in dif-\nferent configurations, and O-IG, which consists of\na 2-by-2 grid inside of a larger entity.\nRelations. Following prior work on this task,\nRA VEN applies four different relations to item\nattributes across rows of the matrix. These are\nConstant, which does not modify an attribute,\nProgression, which increases or decreases the\nvalue of an attribute by 1 or 2, Arithmetic,\nwhich performs addition or subtraction on the first\ntwo attributes of the row to create the third, and\nDistribute Three, which distributes three\nconsistent values of an attribute across each row.\n4 Methods\nIn order to apply PLMs to RA VEN, we abstract\nthe visual features of the task into language. Our\nabstractions are intentionally applied on a per-item\nbasis to tackle the perception problem of the task\nwithout giving the PLM explicit hints toward the\nreasoning problem (which requires capturing pat-\nterns over multiple items). This allows us to focus\non evaluating the reasoning capabilities of PLMs.3\nFirst, we introduce our multi-level abstractions\nfor the RA VEN dataset.4 Then we formally define\nthe interface between PLMs and the RPM task.\n4.1 Abstractions in RA VEN\nWe define abstractions for entity-level attributes,\nlayout-level attributes, and component structures\nwhich convert the RPM task into one or more text\nprompts. We apply two kinds of abstractions: nam-\ning and decomposition. As discussed in Section 1,\nassigning names to perceptual features strengthens\nhumans’ analogy-making skills over them. Inspired\nby this, naming abstractions abstract over attributes\nor combinations of attributes in the RPM by as-\nsigning a unique name to describe them. Mean-\n3As the important features of RA VEN are simple, the per-\nception of an individual item is better performed by computer\nvision models, and can already be done to fairly high accu-\nracy (Zhang et al., 2021). For more general-purpose analogy-\nmaking beyond idealized domains, the robust perception of\nkey features that allow previous (source) experiences to be\nmapped to novel (target) experiences is a challenging unsolved\nproblem (Mitchell, 2021).\n4Some example PLM prompts using these abstractions are\nshown in this section, while more examples are provided in\nAppendix C.\n1955\nrow 1: (3,0.8,60), (5,0.8,70), (4,0.8,80);row 2: (4,0.4,60), (3,0.4,70), (5,0.4,80);row 3: (5,0.3,40), (4,0.3,50), (3,0.3,60);\n(type, size, color)row 1: 6, 7, 8;row 2: 6, 7, 8;row 3: 4, 5, 6;\ncolor\nrow 1: 8, 8, 8;row 2: 4, 4, 4;row 3: 3, 3, 3;\nsize\nrow 1: 3, 5, 4;row 2: 4, 3, 5;row 3: 5, 4, 3;\ntype\nFigure 3: Example generated prompts for a complete\nRPM under entity attribute naming (left) and decompo-\nsition (right) abstractions in the Center sub-task.\nposition\nnumber5\n[5,6,7]type\nsize[2,3]color [0,4,6,8]\nXXXXX\nX X X X X\n[1,1,1,0,0,1,0,0,1]\nFigure 4: Example of generated entity layout encodings\nwhen abstracting position and number, and sum-\nmarizing redundant entity attributes within the layout.\nwhile, jointly understanding and tracking the com-\nplex features of the task can become a burden even\nfor humans. Inspired by humans’ capability to\ndecompose complex tasks into independent sub-\ntasks (Lee and Anderson, 2001), decomposition ab-\nstractions split the RPM into multiple sub-matrices\nby its independent features, then generate a sepa-\nrate prompt for each one. We can then prompt a\nPLM once for each sub-matrix, and aggregate PLM\noutputs to choose a candidate matrix completion.5\n4.1.1 Entity-Level Abstractions\nAs shown in Figure 3, we can abstract perceptual\nentity attributes into language by assigning them\nnames, then generating prompts to represent the\nfull RPM using these names. As each of an en-\ntity’s attributes is numerical by nature, we assign\neach attribute an ordinal numerical name; type\nis named by the number of sides of the associated\nshape (e.g., “3” for triangle), size is named by\na decimal representing its diameter, and color is\nnamed based on the darkness of the entity’s shade.\nAs each of an entity’s attributes is independent, i.e.,\na relation over one attribute has no connection to\nrelations over other attributes, we can decompose\nthe RPM task by these attributes into three separate\nsub-tasks with their own prompts.\n5A more formal definition for decomposition is provided\nin Section 4.2.\n4.1.2 Layout-Level Abstractions\nAs shown in Figure 4, we next propose abstractions\nfor layouts of entities (e.g., in grid-based sub-tasks\nof RA VEN). First, thenumber attribute of a layout\ncorresponds to the count of entities in it. Recogniz-\ning number requires implicitly counting entities\nwithin a layout, which may be difficult to disen-\ntangle from other attributes. As such, we directly\nexpose this attribute by extracting this count and\nencoding it in text. Since this layout attribute is\nindependent from other attributes, we can again\ndecompose the task and consider it separately from\nentity attributes.\nThe position attribute encodes even more\ncomplex information about a layout, and relations\nover it may move entities around within the lay-\nout. However, an occupancy map serves as a strong\nnaming abstraction for position which omits\ndistracting details of specific entities while expos-\ning key information for detecting relations over it.\nWe generate the occupancy map as an array of text\nrepresenting the occupancy of the layout, and de-\ncompose this from other attributes. Notably, this\nabstraction provides a unique language description\nfor each possible global configuration of entities\nwithin a layout, allowing the PLM to disentangle\nglobal and local patterns in the problem, a help-\nful capability of humans (Robertson and Lamb,\n1991).6\nIn RA VEN, relations are applied to specific at-\ntributes consistently across all entities in a layout.\nAs our layout-level abstractions make explicit the\nkey features of layouts, we no longer need to track\nentity-level attributes for specific entities within\nthem. Specifically, rather than supply a PLM with\na separate grid-like prompt for each entity-level\nattribute, we simply provide a list of unique at-\ntribute values. This reduces the complexity added\nby layouts of multiple entities.\n4.1.3 Structural Decomposition Abstractions\nIn cases with multiple components in each item,\nwe may find that prompts become long and compli-\ncated with earlier approaches. Since each compo-\nnent’s attributes and relations are independent, we\ncan alternatively decompose the task by its com-\nponents. For each component, we can generate\na prompt through entity attribute naming abstrac-\ntions as shown in Figure 3 (left), or we can apply\n6For example, we may recognize the grid of entities in\nFigure 2 to be in an “L” shape at the global level, while also\nrecognizing that it is locally composed of triangles.\n1956\nthe higher-level abstractions over entity and lay-\nout attributes shown in Figure 4, thus decomposing\neach component’s prompts into prompts for each\nattribute. As this structural decomposition con-\nverts multi-component problems into several sim-\npler single-component, single-attribute problems,\nthe complexity added by multiple components is\nabstracted away.\n4.2 Problem Definition\nFormally, a complete RPM M consists of 9 matrix\nitems mij where row and column i, j∈ {1, 2, 3}.\nAs discussed in Section 3.1, an individual item mij\nin the RA VEN dataset is formalized by high-level\ncomponents consisting of layout-level attributes\nand entity-level attributes. Given all items in M\nexcept for m33, the task is to identify m33 from\na set Y of 8 choices by identifying abstract rules\nover the attributes within the first 2 rows ofM, and\nselecting the candidate m33 that correctly applies\nthese rules in the third row.\nApplying PLMs. We apply PLMs to RA VEN in\na zero-shot setting. In the absence of decomposi-\ntion abstractions, we define L as the mapping of a\ncomplete RPM to a text prompt. The PLM’s choice\nfor m33 is given by\narg max\ny∈Y\n1\n|L | log Pr (L (m11:32, y))\nwhere |L | denotes the number of tokens in the\nprompt. When decomposition is introduced, L\ninstead returns multiple prompts, and the (token-\nlength normalized) log-probabilities of all sub-\nprompts are summed.7\n5 Experimental Results\nNow, we can examine the impact each of these\nlanguage-based abstractions has on the perfor-\nmance of transformer-based, autoregressive PLMs\nin relational reasoning on RA VEN. To further un-\nderstand their impact with respect to model com-\nplexity, we evaluate a range of model sizes:8 OPT\n125M, 1.3B, and 13B (Zhang et al., 2022), along\nwith GPT-3 (Brown et al., 2020).9 Models are eval-\nuated on a random subset of 500 testing examples\nfrom each sub-task of RA VEN.\n7See Appendix C for examples of decomposing prompts.\n8Results on additional model sizes in Appendix A.\n9Specifically, we use the text-davinci-002 variant\nof InstructGPT (Ouyang et al., 2022) through a Microsoft\nAzure OpenAI deployment.\nAfter introducing some comparison approaches,\nwe present the experimental results from our ap-\nplied abstractions on PLMs’ entity-level, layout-\nlevel, and component-level relational reasoning.\nAfterward, we dive deeper with an analysis on how\nboth our abstractions and in-context learning con-\ntribute to model performance.\n5.1 Comparison Approaches\nTo contextualize our findings, we provide results\nfrom the human study in Zhang et al. (2019a), as\nwell as two supervised baselines from prior work.10\nAdditionally, to specifically evaluate the advantage\nof the way we mapped the RPM task into language,\nwe include two simpler abstraction methods that\nencode task information less explicitly.\nSupervised baselines. While our goal is not to\nachieve the state of the art on RA VEN, we include\nresults from two state-of-the-art supervised base-\nlines for reference. Specifically, we select the\ntwo approaches with the top mean accuracy on\nRA VEN, as outlined in the survey by Małki´nski and\nMa´ndziuk (2022): Rel-AIR (Spratley et al., 2020)\nand CoPINet + ACL (Kim et al., 2020). Rel-AIR\ncombines a simple vision model with an unsuper-\nvised scene decomposition module, enabling more\ngeneralizable reasoning over entities in RA VEN.\nCoPINet + ACL applies an analogy-centric con-\ntrastive learning paradigm to CoPINet (Zhang et al.,\n2019b), a prior architecture proposed for percep-\ntual inference trained through contrastive learning.\nBoth baselines have been trained on thousands of\nexamples from the RA VEN dataset, and incorpo-\nrate task-specific inductive biases in their architec-\nture. Meanwhile, we evaluate PLMs on RA VEN in\na zero-shot setting with no supervised learning.\nQuasi-image abstraction. To evaluate the help-\nfulness of naming abstractions over entity at-\ntributes, we should compare to an approach that\ndoes not have such abstraction. However, some\nmapping from the visual features of the RPM task\ninto langauge is needed in order for a PLM to inter-\nface with it. While the limited context window of\nPLMs restricts us from incorporating raw pixels di-\nrectly into our prompts, PLMs have recently been\ndemonstrated to capture spatial patterns in simi-\nlar inputs: text-based matrices (Patel and Pavlick,\n10Since our approach is not evaluated on the exact same\nsubset of RA VEN data, these results from prior work are not\ndirectly comparable, but can be helpful reference points.\n1957\n[[ ...9...],[ ..9 9 9..],[ .9 9 9 9 9 .],[ 9 9 9 9 9 9 9 ],[ 9 9 9 9 9 9 9 ],[ 9 9 9 9 9 9 9 ],[ 9 9 9 9 9 9 9 ],]\n[[2...],[22..],[222.],[2222],]\nFigure 5: Quasi-image abstractions for a triangle and\npentagon of different size and color.\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nModel Size (Billion Parameters)\nCenter Accuracy\nHuman\nRel-AIR\nCoPINet + ACL\nRandom\nQuasi-Image\nRandom Naming\nEntity Naming\nEntity Decomp.\nFigure 6: Results on the RA VEN Center sub-task\nunder entity abstractions, compared to naïve and super-\nvised baselines described in Section 5.1, and humans.\n2021). As such, we propose a quasi-image abstrac-\ntion which converts the visual RPM task into a\nmatrix of ASCII characters. As shown in Figure 5,\nan entity’stype can be expressed through a matrix\nof characters; size can be expressed through the\nheight and width of the matrix; and color can be\nexpressed through the actual characters making up\nthe matrix. By converting instances of RA VEN’s\nCenter sub-task into this pixel-like form, we have\na lower-level abstraction of the task’s visual fea-\ntures that can be compared to the higher-level ab-\nstraction of naming entity attributes.\nRandom naming abstraction. We would also\nlike to understand the advantage of the specific\nnames we chose for entity attributes compared to\nother possible choices. As such, we propose a sec-\nond baseline where, instead of using ordinal labels\nto describe entities’ type, size, and color, we\nchoose random words from a large corpus. This\nremoves numerical dependencies that may be uti-\nlized to recognize some relations, and can help us\nunderstand whether PLMs take advantage of this\ninformation when it is available.\n5.2 Entity-Level Reasoning\nWe first evaluate PLMs under our lowest level ab-\nstractions over entity attributes. To isolate the im-\nprovements from such abstraction, we focus on\nthe Center sub-task of RA VEN which only in-\ncludes a single entity per item in the RPM, and thus\nonly tests understanding of relations over entity at-\ntributes. The results are shown in Figure 6.\nImpact of naming. Under the simplest abstrac-\ntion of naming the entity-level attributes, we see\nimpressive zero-shot accuracies that monotonically\nincrease with model size up to 77.2% from GPT-\n3 175B on Center, nearing human performance.\nFurther, we find that our choice to map attributes\ninto numerical symbols is consistently advanta-\ngeous over the quasi-image and random-naming\nabstractions, which reach respective accuracies up\nto 28.2% and 51.8%. Meanwhile, we find that as\nmodel size increases, our ordinal naming approach\noutperforms the random naming baseline more and\nmore, up to over 20% in larger model sizes. This\nsuggests that PLMs of larger size can better capture\nand take advantage of implicit numerical relations\nin their vocabulary.\nImpact of decomposition. When applying de-\ncomposition over entity attributes, we observe fur-\nther improvement of 2.8% accuracy in GPT-3 175B.\nInterestingly, we see a much sharper improvement\nfrom this abstraction in smaller models, with OPT\n125M’s accuracy doubling from 22.2% to 45.6%,\nand OPT 1.3B’s accuracy rising from 47.2% to\n72.0%. This may suggest that PLMs have a limited\nworking memory which is related to the number\nof learned parameters in them. Large PLMs are\nmore capable to handle complex reasoning tasks\nbecause of this, while smaller PLMs benefit from\ndecomposing tasks into more manageable parts.\n5.3 Layout-Level Reasoning\nIn Figure 7, we evaluate PLMs’ capability to\ncapture relations over layout attributes under our\nabstractions introduced in the 2x2Grid and\n3x3Grid sub-tasks. Without any decomposi-\ntion abstraction, model performance reaches up\nto 78.0% and 86.4% accuracy respectively on\n2x2Grid and 3x3Grid. When adding naming\nfor layout-level attributes and decomposing all at-\ntributes into separate prompts, we see further im-\nprovements across the board, with accuracies reach-\ning 87.8% on 2x2Grid and 93.2% on 3x3Grid.\nThe PLM exceeds human performance on both\nsub-tasks, despite them being arguably some of\nthe most complex tasks in RA VEN, with the latter\ncomprised of more entities than any other sub-task.\nThis suggests that our strong layout-level abstrac-\ntions enable the PLM to tease apart the numerous\nattributes in grids of entities and capture obscure\npatterns, whereas humans may struggle with this\nas the task becomes more complex.\n1958\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nModel Size (Billion Parameters)\n2x2Grid Accuracy\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\n3x3Grid Accuracy\nHuman Rel-AIR CoPINet + ACL\nRandom Entity Naming Entity & Layout Decomp.\nFigure 7: Results on grid-based sub-tasks of RA VEN\nwithout and with decomposition abstractions. Com-\npared to humans and supervised baselines.\n5.4 Component-Level Reasoning\nLastly, we apply our structural decomposition-\nbased abstractions on RA VEN sub-tasks which\nhave multiple components, i.e., L-R, U-D, O-IC,\nand O-IG. The results are shown in Figure 8. First,\njust decomposing the task by its components im-\nproves the maximum accuracy on each task on\naverage by about 20%. Additionally decomposing\neach component by its entity and layout attributes\nbrings further gains, with GPT-3 175B reaching\nup to 77.6%, 78.0%, 82.8%, and 92.6% on L-R,\nU-D, O-IC, and O-IG respectively, and exceeding\nhumans and nearing supervised baselines on the\nlatter. The performance gain from this decompo-\nsition is again even more pronounced for smaller\nPLMs. Most significantly, OPT 1.3B improves\nfrom 20-30% accuracy to over 70% accuracy, near-\ning human performance. This demonstrates that\nnot only is GPT-3 capable of very complex analog-\nical reasoning tasks, but even PLMs less than 100\ntimes its size can perform quite well here with the\nproper abstractions.\n5.5 Fine-Grained Analysis\nFinally, we analyze how model performance varies\nacross different attributes and relations, as we in-\ntroduce distracting attributes, and as we introduce\nrows into the matrix. In our analysis, we compare\nthree representative levels of abstraction: entity\nattribute naming only (no decomposition into mul-\ntiple prompts), decomposition of components, and\nfull decomposition of entity and layout attributes\nand components.\n5.5.1 Analysis of Attributes and Relations\nWe measure the impact of abstractions in capturing\neach attribute and relation in RA VEN. In Figure 9,\nDistractor Values Naming Decomposition\nRA VEN 76.0% 80.0%\nRandom 72.6% 77.8%\nTable 1: GPT-3 accuracy on Center sub-task with dis-\ntracting orientation attribute in language prompts,\nunder the naming and decomposition abstractions.\norientation values are taken directly from RA VEN\nor randomly selected.\nwe present GPT-3 175B’s accuracy over each at-\ntribute and relation. We find that number is the\nbest captured attribute even without any decompo-\nsition abstractions, while the model struggles with\nposition until we introduce decomposition of\nattributes, suggesting the occupancy map encoding\nused here indeed helped capture it. Meanwhile,\nArithmetic is the most difficult relation, with\nconsistently lower accuracy than other relations.\n5.5.2 Robustness to Distracting Attributes\nSince our mappings from RA VEN attributes into\nlanguage provide the key features over which rela-\ntions occur, we may wonder how robust PLMs\nare to distracting or unimportant attributes. In\nfact, the RA VEN dataset includes one noise at-\ntribute that we excluded from our mapping to\navoid unnecessarily increasing prompt lengths:\norientation, i.e., the rotation of entities in the\nRPM. To begin exploring this issue, we incorpo-\nrate orientation into the problem as a fourth\nentity-level attribute in addition to type, size,\nand color. For the best model (i.e., GPT-3) on the\nCenter sub-task, we compare two possible injec-\ntions of orientation values: using the values\nprovided in RA VEN (which are mostly constant\nwithin each matrix row), and randomly selected\nvalues (which could be more distracting).\nAs shown in Table 1, compared to GPT-3’s\nCenter accuracies of 77.2% and 80.0% with re-\nspective naming and decomposition abstractions,\nthe injection of orientation as a distraction\nfeature does not degrade the model performance\nmuch, achieving accuracies of 76.0% and 80.0%\nwhen using values from RA VEN, and 72.6% and\n77.8% when using random values. This shows\nthat PLMs exhibit some robustness to distracting\nattributes in language context, and have the capabil-\nity to ignore them in analogical reasoning. Future\nwork may consider more in-depth analysis to dis-\ncover the extent of model robustness to distraction\nfeatures, and how it varies by model complexity.\n1959\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nModel Size (Billion Parameters)\nL-R Accuracy\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nU-D Accuracy\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nO-IC Accuracy\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nO-IG Accuracy\nHuman Rel-AIR CoPINet + ACL Random\nAttr. Naming Comp. Decomp. Comp. & Attr. Decomp.\nFigure 8: PLM accuracy on multi-component RA VEN sub-tasks with attribute naming only, component decomposi-\ntion, and full component and attribute decomposition, compared to supervised baselines and humans.\nEntity\nAttr. Number Position Constant ProgressionArithmeticDistributeThree\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nAttr. Naming Only Component Decomp. Component + Attr. Decomp.\nFigure 9: Comparison of accuracy on examples from all\nsub-tasks, broken down by the types of attributes and\nrelations they require capturing.\n5.5.3 In-Context Learning Over Rows\nBy design, RPM tasks are meant to require mini-\nmal background knowledge. They should be im-\npossible to solve without the first two rows of the\nmatrix, which provide essential context to com-\nplete the third row of the matrix. To understand\nwhether PLMs capture relations specifically from\nin-context learning over the first two rows of the\nmatrix (as opposed to using prior knowledge from\npre-training), we measure the model performance\nas we introduce rows to the matrices.\nAs shown in Figure 10, the average model per-\nformance increases across all sizes and abstractions\nas rows are added to the matrix. This suggests that\nin-context learning indeed contributes significantly\nto performance, even for smaller models. Larger\nmodel sizes see the most significant improvements,\nsuggesting that larger PLMs are stronger in-context\nlearners than smaller ones. Further, larger PLMs\ncan achieve nearly the same accuracy with only\ntwo rows of the matrix provided rather compared\nto having all three, suggesting that they pick up the\ntask quite quickly from in-context learning.\nWe also observe that in many cases, models\nachieve accuracies above chance (12.5% accuracy)\nwithout being provided any complete rows of the\nSub-Task 1 Row 2 Rows 3 Rows Human\nCenter 36.8% 69.2% 77.2% 95.6%\n2x2Grid 54.0% 71.0% 78.0% 81.8%\n3x3Grid 73.0% 85.2% 86.4% 79.6%\nL-R 14.0% 38.2% 54.2% 86.4%\nU-D 12.4% 42.0% 53.6% 81.8%\nO-IC 19.6% 53.6% 64.8% 86.4%\nO-IG 32.0% 62.2% 74.8% 81.8%\nTable 2: GPT-3 accuracy on RA VEN sub-tasks as rows\nare added to the RPM, under only naming abstractions.\nmatrix (only the third, incomplete row). This may\nsuggest the PLM has a useful prior for this problem,\ndespite it being a visual problem and thus impossi-\nble to observe directly in pre-training. This raises\nquestions about the objectivity of RA VEN and pos-\nsibly the RPM task.11 Further, when decomposi-\ntion abstractions are applied, models achieve higher\naccuracies than when not, suggesting that decom-\nposition encodes some of this prior knowledge for\nthe task. In Table 2, we take a closer look at GPT-3\n175B’s performance within sub-tasks. Surprisingly,\nwe find the highest accuracies on the grid-based\nsub-tasks, despite them being the most difficult\ntasks for humans.\nThis motivates future work to compare human\nand PLM performance on ablated analogy-making\ntasks like these to further evaluate their objective-\nness and identify commonalities. Future work in AI\nand analogy may also consider building diagnostic\ndatasets to tease apart attribute and relation types\nto better understand how they contribute to model\nperformance and identify areas for improvement.\nIn-context learning of attributes and relations.\n11In Appendix B, we further explore this hypothesis on the\nImpartial-RA VEN dataset (Hu et al., 2021) that removes some\nsuperficial correlations in matrix completion choices, and still\nsee comparable results.\n1960\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nModel Size (Billion Parameters)\nNaming Accuracy\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nDecomp. Accuracy\nHuman Rel-AIR CoPINet + ACL Random\n1 Row 2 Rows 3 Rows\nFigure 10: Macro average accuracy over all RA VEN\nsub-tasks as we introduce rows to the matrix during in-\ncontext learning, under naming abstractions only (left)\nand all naming and decomposition abstractions (right).\nIn 1 Row, we include only the incomplete third row.\nEntity\nAttr. Number Position ConstantProgressionArithmeticDistributeThree\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n1 Row 2 Rows 3 Rows\nFigure 11: Comparison of accuracy on examples from\nall RA VEN sub-tasks as rows are introduced to the ma-\ntrix, with only entity attribute naming abstractions.\nWe may wonder whether specific relations or at-\ntributes are easier to understand than others with\nless context. For example, the Progression or\nConstant relations may be possible to recognize\nonly from the first two items of the third row in an\nRPM, as we can easily observe patterns in attribute\nvalues here, e.g., that entity size is increasing\nor color remains constant. In Figures 11 and\n12, we surprisingly observe only marginal differ-\nences here, except for thenumber attribute, which\nseems significantly better captured than other at-\ntributes in this no-context setting.\nEntity\nAttr. Number Position ConstantProgressionArithmeticDistributeThree\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n1 Row 2 Rows 3 Rows\nFigure 12: Comparison of accuracy on examples from\nall RA VEN sub-tasks as rows are introduced to the ma-\ntrix, with all decomposition abstractions.\n6 Conclusion\nIn this work, we explored the ability of large PLMs\nto perform zero-shot analogical reasoning in visual\nRaven’s Progressive Matrices (RPM). Upon the\nsimplest mapping to language, they can achieve\nstriking results, while applying higher-level nam-\ning and decomposition abstractions over the task\nfeatures further raises performance to the level of\nhumans and supervised approaches in some cases.\nWe find that while ordinal naming abstractions are\na powerful way to enable analogical reasoning in\nlarger PLMs, decomposition abstractions that break\nthe task down into atomic parts conserve their work-\ning memory such that even smaller PLMs under 1B\nparameters can achieve competitive performance\non this challenging problem.\nOur detailed analysis revealed insights about\nwhich features of the task PLMs best capture, their\nrobustness to distracting features, and the role of\nin-context learning and prior knowledge in picking\nup this complex task. Surprisingly, we find that\neven without two complete rows of prior context\nfrom the matrix, GPT-3 175B and smaller mod-\nels can achieve above-chance performance on the\ntask, raising questions about the objectivity and\ntrue role of prior knowledge in RPM tasks, which\nare assumed to require minimal prior knowledge.\nThese results also raise some questions about\nthe role PLMs may play in future AI systems capa-\nble of analogy. While previously thought to be a\ndifficult problem for AI systems, PLMs can solve\nthe reasoning step of analogy easily given strong\nabstractions over visual perception. Many of these\nabstractions are intuitive and commonly researched\nin computer vision, including the detection of ob-\nject types, sizes, colors, counts, and global arrange-\nments. As such, future work may dive deeper into\nthe challenging problem of generalized perception\nacross domains, where we must robustly tease apart\nthe key features of tasks and experiences that may\nfacilitate analogy-making, e.g., in recognizing the\ncommonalities between a physical bridge and the\nbridge of a song (Mitchell, 2021). Recent efforts to-\nward understanding how humans describe abstract\nvisual features in language by mapping them to nat-\nural concepts12 are a promising direction toward\nthis goal (Lachmy et al., 2022; Ji et al., 2022).\n12For example, when communicating about abstract shapes,\nwe may make an analogy to refer to them as looking like more\nfamiliar natural concepts like flowers or dog bones.\n1961\nAcknowledgements\nThis work was supported in part by DARPA PTG\nprogram HR00112220003. We would like to thank\nthe anonymous reviewers for their valuable com-\nments and suggestions.\nLimitations\nPerception and reasoning in text-based RA VEN.\nIn this work, one limitation is that we do not at-\ntempt to solve the perception problem of analogy-\nmaking in RPM, rather we apply perfect perception\nin solving the reasoning part, and assume the per-\nception problem is simple. By doing so, we find\nthat PLMs may be a strong solution to the reasoning\nproblem here, which may better direct future efforts\ntoward AI and analogy. Obviously, the perception\nproblem for idealized domains is a lot different than\nmore natural domains, and identifying key features\nacross many domains that can facilitate a mapping\nis still a challenging unsolved problem. We hope\nthat our work sparks more interest in this problem.\nMeanwhile, one may argue that our decomposi-\ntion abstractions are too strong, and actually con-\ntribute to the reasoning problem in RPM, as they\nmake an independence assumption about which\nfeatures of the task can be teased apart. Making\nsuch an assumption requires an understanding of\nthe problem that cannot be inferred by only see-\ning one instance. However, we decomposed the\ntask based on very intuitive and common attributes,\ne.g., shapes, colors, sizes, and counts of items.\nWe believe that the strength of such an abstrac-\ntion, which could be applied in many problems,\nshould not be understated. Nonetheless, we include\ndecomposition-free forms of results as much as\npossible throughout the paper to help compare the\ncontributions of decomposition versus naming ab-\nstractions, which is more clearly only providing per-\nceptual information. In fact, we find that without\nany decomposition, PLMs still achieve very strong\nperformance in many cases, and performance gains\nfrom decomposition are not always large.\nHuman performance. Lastly, we note some lim-\nitations in the human performance measurements\nused as reference points. In Zhang et al. (2019a),\nhuman performance on RA VEN was measured by\ngiving subjects some task-specific training, then\nevaluating them on the original visual form of\nthe task. This differs from our results in two\nways. First, PLMs had no task-specific training\nfor RA VEN, given that experiments were zero-shot\nand the text data we generate is new and thus im-\npossible to appear directly in PLM pre-training.\nThis may give humans an advantage. Second, the\ntask is presented to PLMs in text form, not visually.\nWhile the essential information from the task is\npreserved by our conversion, it is possible that this\nconversion would affect the difficulty of the task\nfor humans (making it easier or harder). As such,\nit becomes unclear how to contextualize our results\nwith these past human results. Future work may\ncarry out systematic human studies to compare the\nanalogical reasoning capabilities of humans and\nPLMs in different settings.\nEthical Considerations\nThis work does not use any human subjects or\nhuman-generated data. Our work deals with ab-\nstract visual features that are described with nu-\nmerical symbols, thus not strongly targeting any\nlanguage. A possible ethical concern for this work\nis the amount of computational resources used in\nevaluating PLMs. To reduce unnecessary computa-\ntion in our study, we chose to apply PLMs to only a\nsubset of 500 testing examples from each sub-task\nof the RA VEN dataset, while the full testing set is\nfour times as large.\nReferences\nYonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf,\nRoy Schwartz, and Gabriel Stanovsky. 2022. V ASR:\nVisual analogies of situation recognition. In Proceed-\nings of the AAAI Conference on Artificial Intelligence\n(AAAI).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao\nLi, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua\nXiao, and Hao Zhou. 2022. E-KAR: A benchmark\nfor rationalizing natural language analogical reason-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022 , pages 3941–3955,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nStella Christie and Dedre Gentner. 2014. Language\nhelps children succeed on a classic analogy task.\nCognitive Science, 38(2):383–397.\n1962\nDedre Gentner. 1983. Structure-mapping: A theoretical\nframework for analogy. Cognitive Science, 7(2):155–\n170.\nDedre Gentner. 2010. Bootstrapping the mind: Ana-\nlogical processes and symbol systems. Cognitive\nScience, 34(5):752–775.\nDedre Gentner, Asli Özyürek, Özge Gürcanli, and Susan\nGoldin-Meadow. 2013. Spatial language facilitates\nspatial cognition: Evidence from children who lack\nlanguage input. Cognition, 127(3):318–330.\nPeter Gordon. 2004. Numerical cognition with-\nout words: Evidence from Amazonia. Science,\n306(5695):496–499.\nFelix Hill, Adam Santoro, David GT Barrett, Ari S Mor-\ncos, and Timothy Lillicrap. 2019. Learning to make\nanalogies by contrasting abstract relational structure.\nIn 7th International Conference on Learning Repre-\nsentations (ICLR).\nDouglas R Hofstadter and Melanie Mitchell. 1994. The\nCopycat project: A model of mental fluidity and\nanalogy-making, pages 31–112. Ablex Publishing.\nDouglas R Hofstadter and Emmanuel Sander. 2013.Sur-\nfaces and essences: Analogy as the fuel and fire of\nthinking. Basic Books.\nKeith J Holyoak. 1984. Analogical thinking and human\nintelligence. Advances in the psychology of human\nintelligence, 2:199–230.\nKeith J Holyoak. 2012. Analogy and relational reason-\ning. The Oxford Handbook of Thinking and Reason-\ning.\nSheng Hu, Yuqing Ma, Xianglong Liu, Yanlu Wei, and\nShihao Bai. 2021. Stratified rule-aware network for\nabstract visual reasoning. In Proceedings of the\nAAAI Conference on Artificial Intelligence (AAAI) ,\nvolume 35, pages 1567–1574.\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr,\nWai Keen V ong, Robert Hawkins, and Yoav Artzi.\n2022. Abstract visual reasoning with tangram shapes.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nYoungsung Kim, Jinwoo Shin, Eunho Yang, and\nSung Ju Hwang. 2020. Few-shot visual reasoning\nwith meta-analogical contrastive learning. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 16846–16856. Curran Associates,\nInc.\nRoyi Lachmy, Valentina Pyatkin, Avshalom Manevich,\nand Reut Tsarfaty. 2022. Draw Me a Flower: Process-\ning and Grounding Abstraction in Natural Language.\nTransactions of the Association for Computational\nLinguistics, 10:1341–1356.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B\nTenenbaum. 2015. Human-level concept learning\nthrough probabilistic program induction. Science,\n350(6266):1332–1338.\nFrank J Lee and John R Anderson. 2001. Does learn-\ning a complex task have to be complex?: A study\nin learning decomposition. Cognitive Psychology,\n42(3):267–316.\nPeng-Hsuan Li, Tsan-Yu Yang, and Wei-Yun Ma. 2020.\nCA-EHN: Commonsense analogy from E-HowNet.\nIn Proceedings of the Twelfth Language Resources\nand Evaluation Conference, pages 2984–2990, Mar-\nseille, France. European Language Resources Asso-\nciation.\nTal Linzen. 2016. Issues in evaluating semantic spaces\nusing word analogies. In Proceedings of the 1st Work-\nshop on Evaluating Vector-Space Representations for\nNLP, pages 13–18, Berlin, Germany. Association for\nComputational Linguistics.\nHongjing Lu, Ying Nian Wu, and Keith J Holyoak.\n2019. Emergence of analogy from relation learning.\nProceedings of the National Academy of Sciences ,\n116(10):4176–4181.\nMikołaj Małki´nski and Jacek Ma ´ndziuk. 2022. Deep\nlearning methods for abstract visual reasoning: A sur-\nvey on Raven’s Progressive Matrices. arXiv preprint\narXiv:2201.12382.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013a. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in Neural Information Processing Systems,\n26.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nMelanie Mitchell. 2021. Abstraction and analogy-\nmaking in artificial intelligence. Annals of the New\nYork Academy of Sciences, 1505(1):79–101.\nVictor Vikram Odouard and Melanie Mitchell. 2022.\nEvaluating understanding on conceptual abstraction\nbenchmarks. In Proceedings of the AI Evaluation Be-\nyond Metrics at IJCAI-ECAI 2022, Vienna, Austria.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nRoma Patel and Ellie Pavlick. 2021. Mapping language\nmodels to grounded conceptual spaces. In Interna-\ntional Conference on Learning Representations.\n1963\nJohn C Raven and JH Court. 1938. Raven’s progres-\nsive matrices. Western Psychological Services Los\nAngeles.\nLynn C Robertson and Marvin R Lamb. 1991.\nNeuropsychological contributions to theories of\npart/whole organization. Cognitive Psychology ,\n23(2):299–330.\nRobyn Speer, Catherine Havasi, and Henry Lieberman.\n2008. Analogyspace: Reducing the dimensionality\nof common sense knowledge. In AAAI, volume 8,\npages 548–553.\nSteven Spratley, Krista Ehinger, and Tim Miller. 2020.\nA closer look at generalisation in raven. In Computer\nVision – ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part\nXXVII, page 601–616, Berlin, Heidelberg. Springer-\nVerlag.\nOren Sultan and Dafna Shahaf. 2022. Life is a circus\nand we are the clowns: Automatically finding analo-\ngies between situations and processes. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nDamien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu,\nChunhua Shen, and Anton van den Hengel. 2020. V-\nprom: A benchmark for visual reasoning using visual\nprogressive matrices. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 34,\npages 12071–12078.\nPeter D Turney. 2008. The latent relation mapping\nengine: Algorithm and experiments. Journal of Arti-\nficial Intelligence Research, 33:615–655.\nPeter D Turney, Michael L Littman, Jeffrey Bigham,\nand Victor Shnayder. 2003. Combining independent\nmodules in lexical multiple-choice problems. Re-\ncent Advances in Natural Language Processing III:\nSelected Papers from RANLP, 2003:101–110.\nTaylor Webb, Keith J Holyoak, and Hongjing Lu. 2022.\nEmergent analogical reasoning in large language\nmodels. arXiv preprint arXiv:2212.09196.\nChi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and\nSong-Chun Zhu. 2019a. RA VEN: A dataset for rela-\ntional and analogical visual reasoning. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR).\nChi Zhang, Baoxiong Jia, Feng Gao, Yixin Zhu,\nHongJing Lu, and Song-Chun Zhu. 2019b. Learning\nperceptual inference by contrasting. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nChi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin\nZhu. 2021. Abstract spatial-temporal reasoning via\nprobabilistic abduction and execution. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 9736–\n9746.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOPT: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\n1964\nA Expanded Results\nIn Table 3, we present additional results with a\nwider range of OPT model sizes (Zhang et al.,\n2022). We observe similar mostly monotonic in-\ncreases of accuracy with model size.\nB Results and Analysis with I-RA VEN\nAs the generation strategy for the negative choices\nin RA VEN can introduce distributional bias that is\nproblematic for supervised learning and leads to\nartificially high performance (Hu et al., 2021), this\ncould be a possible reason behind PLMs’ strong\nperformance on the task even without any com-\nplete rows of context. As such, in Table 4 and\nFigure 13, we include some supplementary anal-\nysis on the Impartial-RA VEN (I-RA VEN) dataset\nfrom Hu et al., which introduces more variation in\nnegative choices. However, we observe similar per-\nformance trends in I-RA VEN. Performance mostly\nmonotonically increases with model sizes and more\nabstraction. Further, PLMs achieve above-chance\nperformance again without any rows of context\nprovided, even with no decomposition abstractions.\nThis provides further evidence that RPM, at least\nformulated in this way, is in part addressed by\nPLMs’ prior knowledge, despite the assumptions\nof minimal background knowledge that the task\nmakes.\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nModel Size (Billion Parameters)\nAccuracy - Naming\n10−1 100 101 102\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy - All\nRandom 1 Row\n2 Rows 3 Rows\nFigure 13: Macro average accuracy over all Impartial-\nRA VEN sub-tasks as we introduce rows to the matrix\nduring in-context learning, under naming abstractions\nonly (left) and all naming and decomposition abstrac-\ntions (right). In 1 Row, we include only the incomplete\nthird row.\nC Example Prompts\nIn Figure 14, we include example prompts for\n2x2Grid, 3x3Grid, L-R and I-OG subtasks\nunder different abstractions. Note that U-D and\nI-OC are isomorphic to L-R, and therefore share\nthe same prompt format.\n1965\n2x2Grid\n3x3Grid\nL-R\nI-OG\nCompleterow 1: [(5,0.6,60), -, -, (6,0.4,60), (6,0.5,60), -, (5,0.3,60), (6,0.3,60), (6,0.4,60)],[(6,0.4,10), (5,0.6,10), -, -, (6,0.4,10), (6,0.5,10), -, (5,0.3,10), (6,0.3,10)],[(6,0.3,70), (6,0.4,70), (5,0.6,70), -, -, (6,0.4,70), (6,0.5,70), -, (5,0.3,70)];row 2: [-, (7,0.5,80), -, (4,0.3,80), (7,0.2,80), -, (6,0.2,80), (5,0.3,80), (4,0.5,80)],[(4,0.5,0), -, (7,0.5,0), -, (4,0.3,0), (7,0.2,0), -, (6,0.2,0), (5,0.3,0)],[(5,0.3,80), (4,0.5,80), -, (7,0.5,80), -, (4,0.3,80), (7,0.2,80), -, (6,0.2,80)];row 3: [-, (5,0.5,40), -, -, (4,0.4,40), -, -, (4,0.1,40), -],[-, -, (5,0.5,30), -, -, (4,0.4,30), -, -, (4,0.1,30)],[(4,0.1,70), -, -, (5,0.5,70), -, -, (4,0.4,70), -, -];\nPosition: Prog. Number: -row 1: [1, 0, 0, 1, 1, 0, 1, 1, 1], [1, 1, 0, 0, 1, 1, 0, 1, 1], [1, 1, 1, 0, 0, 1, 1, 0, 1];row 1: 6, 6, 6;row 2: [0, 1, 0, 1, 1, 0, 1, 1, 1], [1, 0, 1, 0, 1, 1, 0, 1, 1], [1, 1, 0, 1, 0, 1, 1, 0, 1];row 2: 6, 6, 6;row 3: [0, 1, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1, 0, 0, 1], [1, 0, 0, 1, 0, 0, 1, 0, 0];row 3: 3, 3, 3;Type: Const.Size: Const. Color: Arith.row 1: [5, 6], [5, 6], [5, 6];row 1: [3, 4, 5, 6], [3, 4, 5, 6], [3, 4, 5, 6];row 1: [6], [1], [7];row 2: [4, 5, 6, 7], [4, 5, 6, 7], [4, 5, 6, 7];row 2: [2, 3, 5], [2, 3, 5], [2, 3, 5];row 2: [8], [0], [8];row 3: [4, 5], [4, 5], [4, 5];row 3: [1, 4, 5], [1, 4, 5], [1, 4, 5];row 3: [4], [3], [7];\nCompleterow 1: A (3,0.1,40) / B (5,0.3,30), A (7,0.2,40) / B (4,0.3,50), A (5,0.6,40) / B (3,0.3,70);row 2: A (7,0.6,10) / B (4,0.6,40), A (5,0.1,10) / B (3,0.6,60), A (3,0.2,10) / B (5,0.6,80);row 3: A (5,0.2,10) / B (3,0.4,50), A (3,0.6,10) / B (5,0.4,70), A (7,0.1,10) / B (4,0.4,90);\nLeft Comp. Right Comp.row 1: (3,0.1,40), (7,0.2,40), (5,0.6,40);row 1: (5,0.3,30), (4,0.3,50), (3,0.3,70);row 2: (7,0.6,10), (5,0.1,10), (3,0.2,10);row 2: (4,0.6,40), (3,0.6,60), (5,0.6,80);row 3: (5,0.2,10), (3,0.6,10), (7,0.1,10);row 3: (3,0.4,50), (5,0.4,70), (4,0.4,90);\nType: Distr.Size: Distr.Color: Const.Type: Distr.Size: Const.Color: Prog.row 1: 3, 7, 5;row 1: 1, 2, 6;row 1: 4, 4, 4;row 1: 5, 4, 3;row 1: 3, 3, 3;row 1: 3, 5, 7;row 2: 7, 5, 3;row 2: 6, 1, 2;row 2: 1, 1, 1;row 2: 4, 3, 5;row 2: 6, 6, 6;row 2: 4, 6, 8;row 3: 5, 3, 7;row 3: 2, 6, 1;row 3: 1, 1, 1;row 3: 3, 5, 4;row 3: 4, 4, 4;row 3: 5, 7, 9;\nCompleterow 1:A (5,0.6,0) / B [(4,0.4,60), (4,0.4,60), (4,0.6,60), (4,0.6,60)],A (6,0.5,0) / B [(6,0.6,30), (6,0.6,30), -, -],A (7,0.4,0) / B [-, (5,0.5,90), -, (5,0.4,90)];row 2:A (4,0.4,0) / B [-, (5,0.5,80), (5,0.3,80), (5,0.3,80)],A (5,0.6,0) / B [(4,0.4,10), -, -, -],A (6,0.5,0) / B [-, (6,0.5,90), (6,0.3,90), -];row 3:A (4,0.5,0) / B [(6,0.4,40), (6,0.4,40), (6,0.4,40), (6,0.3,40)],A (5,0.4,0) / B [-, (5,0.5,50), -, -],A (6,0.6,0) / B [(4,0.4,90), (4,0.4,90), (4,0.5,90), -];\nIn Comp. Out Comp.row 1:[(4,0.4,60), (4,0.4,60), (4,0.6,60), (4,0.6,60)],[(6,0.6,30), (6,0.6,30), -, -],[-, (5,0.5,90), -, (5,0.4,90)];row 1: (5,0.6,0), (6,0.5,0), (7,0.4,0);row 2: (4,0.4,0), (5,0.6,0), (6,0.5,0);row 3: (4,0.5,0), (5,0.4,0), (6,0.6,0);row 2:[-, (5,0.5,80), (5,0.3,80), (5,0.3,80)],[(4,0.4,10), -, -, -],[-, (6,0.5,90), (6,0.3,90), -];row 3:[(6,0.4,40), (6,0.4,40), (6,0.4,40), (6,0.3,40)],[-, (5,0.5,50), -, -],[(4,0.4,90), (4,0.4,90), (4,0.5,90), -];\nPosition: - Number: Arith.row 1: [1, 1, 1, 1], [1, 1, 0, 0], [0, 1, 0, 1];row 1: 4, 2, 2;row 2: [0, 1, 1, 1], [1, 0, 0, 0], [0, 1, 1, 0];row 2: 3, 1, 2;row 3: [1, 1, 1, 1], [0, 1, 0, 0], [1, 1, 1, 0];row 3: 4, 1, 3;Type: Distr.Size: Const.Color: Arith.row 1: [4], [6], [5];row 1: [4, 6], [6], [4, 5];row 1: [6], [3], [9];row 2: [5], [4], [6];row 2: [3, 5], [4], [3, 5];row 2: [8], [1], [9];row 3: [6], [5], [4];row 3: [3, 4], [5], [4, 5];row 3: [4], [5], [9];\nType: Prog.Size: Distr.Color: Const.row 1: 5, 6, 7;row 1: 6, 5, 4;row 1: 0, 0, 0;row 2: 4, 5, 6;row 2: 4, 6, 5;row 2: 0, 0, 0;row 3: 4, 5, 6;row 3: 5, 4, 6;row 3: 0, 0, 0;\nPosition: (Set) Arith.Number: - Type: Prog.Size: Distr.Color: Arith.row 1: [0, 1, 1, 1], [1, 1, 1, 0], [0, 0, 0, 1];row 1: 3, 3, 1;row 1: [7], [5], [3];row 1: [2], [5], [1];row 1: [4], [2], [6];row 2: [1, 1, 0, 1], [1, 1, 0, 0], [0, 0, 0, 1];row 2: 3, 2, 1;row 2: [7], [5], [3];row 2: [1], [2], [5];row 2: [2], [4], [6];row 3: [0, 1, 1, 1], [1, 0, 1, 0], [0, 1, 0, 1];row 3: 3, 2, 2;row 3: [7], [5], [3];row 3: [5], [1], [2];row 3: [5], [1], [6];\nCompleterow 1: [-, (7,0.2,40), (7,0.2,40), (7,0.2,40)], [(5,0.5,20), (5,0.5,20), (5,0.5,20), -], [-, -, -, (3,0.1,60)];row 2: [(7,0.1,20), (7,0.1,20), -, (7,0.1,20)], [(5,0.2,40), (5,0.2,40), -, -], [-, -, -, (3,0.5,60)];row 3: [-, (7,0.5,50), (7,0.5,50), (7,0.5,50)], [(5,0.1,10), -, (5,0.1,10), -], [-, (3,0.2,60), -, (3,0.2,60)];\nFigure 14: Example prompts for 2x2Grid, 3x3Grid, L-R and I-OG subtasks under different abstractions.\n1966\nAbstractions Center 2x2 3x3 L-R U-D O-IC O-IG Avg.\n125M\nAttr. Naming Only 0.222 0.420 0.606 0.076 0.098 0.122 0.194 0.248\nComp. Decomp. 0.222 0.420 0.606 0.136 0.154 0.162 0.222 0.275\nComp. + Attr. Decomp. 0.456 0.620 0.724 0.378 0.408 0.374 0.520 0.497\n350M\nAttr. Naming Only 0.302 0.510 0.684 0.104 0.134 0.120 0.250 0.301\nComp. Decomp. 0.302 0.510 0.684 0.186 0.232 0.254 0.344 0.359\nComp. + Attr. Decomp. 0.436 0.588 0.788 0.280 0.346 0.290 0.408 0.448\n1.3B\nAttr. Naming Only 0.472 0.584 0.710 0.146 0.158 0.2 0.322 0.370\nComp. Decomp. 0.472 0.584 0.710 0.410 0.426 0.434 0.494 0.504\nComp. + Attr. Decomp. 0.720 0.714 0.794 0.672 0.680 0.744 0.744 0.724\n2.7B\nAttr. Naming Only 0.534 0.572 0.746 0.216 0.2 0.268 0.336 0.410\nComp. Decomp. 0.534 0.572 0.746 0.420 0.468 0.484 0.532 0.537\nComp. + Attr. Decomp. 0.706 0.738 0.826 0.658 0.664 0.704 0.784 0.726\n6.7B\nAttr. Naming Only 0.618 0.590 0.752 0.196 0.228 0.284 0.396 0.438\nComp. Decomp. 0.618 0.590 0.752 0.492 0.528 0.548 0.584 0.587\nComp. + Attr. Decomp. 0.704 0.750 0.826 0.682 0.690 0.748 0.834 0.748\n13B\nAttr. Naming Only 0.644 0.610 0.754 0.220 0.268 0.358 0.452 0.472\nComp. Decomp. 0.644 0.610 0.754 0.566 0.602 0.586 0.576 0.620\nComp. + Attr. Decomp. 0.746 0.794 0.830 0.710 0.702 0.770 0.840 0.770\n30B\nAttr. Naming Only 0.680 0.596 0.748 0.264 0.328 0.420 0.482 0.503\nComp. Decomp. 0.680 0.596 0.748 0.582 0.618 0.664 0.638 0.647\nComp. + Attr. Decomp. 0.762 0.818 0.828 0.738 0.714 0.786 0.860 0.787\n175B\nAttr. Naming Only 0.772 0.780 0.864 0.542 0.536 0.648 0.748 0.699\nComp. Decomp. 0.772 0.780 0.864 0.738 0.732 0.780 0.840 0.787\nComp. + Attr. Decomp. 0.800 0.878 0.932 0.776 0.780 0.828 0.926 0.846\nTable 3: Performance on RA VEN sub-tasks under our abstractions across a wider set of model sizes. 175B refers to\ntext-davinci-002 while the rest are corresponding OPT models.\nAbstractions Center 2x2 3x3 L-R U-D O-IC O-IG Avg.\n125M\nAttr. Naming Only 0.376 0.172 0.208 0.246 0.230 0.262 0.202 0.242\nComp. Decomp. 0.376 0.172 0.208 0.336 0.344 0.354 0.224 0.288\nComp. + Attr. Decomp. 0.608 0.514 0.602 0.612 0.624 0.638 0.594 0.600\n1.3B\nAttr. Naming Only 0.594 0.290 0.310 0.348 0.370 0.388 0.334 0.376\nComp. Decomp. 0.594 0.290 0.310 0.586 0.574 0.618 0.466 0.491\nComp. + Attr. Decomp. 0.810 0.676 0.730 0.822 0.802 0.882 0.818 0.791\n13B\nAttr. Naming Only 0.756 0.384 0.382 0.456 0.498 0.538 0.432 0.492\nComp. Decomp. 0.756 0.384 0.382 0.750 0.74 0.766 0.564 0.620\nComp. + Attr. Decomp. 0.836 0.748 0.728 0.824 0.826 0.906 0.868 0.819\n175B\nAttr. Naming Only 0.808 0.564 0.566 0.656 0.676 0.818 0.714 0.686\nComp. Decomp. 0.808 0.564 0.566 0.822 0.812 0.896 0.742 0.744\nComp. + Attr. Decomp. 0.864 0.832 0.818 0.834 0.846 0.928 0.930 0.865\nTable 4: Performance on I-RA VEN sub-tasks under our abstractions across different model sizes. 175B refers to\ntext-davinci-002 while the rest are corresponding OPT models.\n1967\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations discussed after Section 6.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nDataset introduced in Section 3.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe cited the authors of the RAVEN dataset when introducing it in Section 3 (and other sections). We\nalso cited the authors of the I-RAVEN dataset in appendices involving it.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe were unable to ﬁnd license information for the RAVEN dataset we used, although it is publicly\navailable. We will not be re-distributing the dataset.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nOur method of adapting the vision-based RAVEN dataset to language is described in Section 4.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe describe the dataset in detail in Section 3; it is idealized abstract data which doesn’t pertain to\nspeciﬁc languages or demographic groups.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nDiscussed at beginning of Section 5.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1968\nC □\u0013 Did you run computational experiments?\nSection 5.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn Section 5, we reported all model complexities. When it comes to compute budget, this is difﬁcult\nto report as experiments were run on several different platforms (OpenAI cloud API, institutional\ncomputing cluster, and more). However, we provided the number of examples experiments were run\non, allowing a fair estimate of this.\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAll evaluations occur in a greedy setting where PLMs choose the most probable answer. Since this\nmakes modal predictions consistent, we cannot report such summary statistics. In analyses in Section\n5.5, we report some mean performance measurements, and make it clear how such calculations are\ndone.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1969",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.755117654800415
    },
    {
      "name": "Analogy",
      "score": 0.7475180625915527
    },
    {
      "name": "Visual reasoning",
      "score": 0.5849680304527283
    },
    {
      "name": "Artificial intelligence",
      "score": 0.56390780210495
    },
    {
      "name": "Analogical reasoning",
      "score": 0.49029138684272766
    },
    {
      "name": "Verbal reasoning",
      "score": 0.4678424596786499
    },
    {
      "name": "Cognition",
      "score": 0.4627656042575836
    },
    {
      "name": "Cognitive science",
      "score": 0.44793403148651123
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4451081156730652
    },
    {
      "name": "Natural language processing",
      "score": 0.43949195742607117
    },
    {
      "name": "Abstraction",
      "score": 0.4394088089466095
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.42276713252067566
    },
    {
      "name": "Context (archaeology)",
      "score": 0.42029517889022827
    },
    {
      "name": "Qualitative reasoning",
      "score": 0.4159843623638153
    },
    {
      "name": "Psychology",
      "score": 0.10468840599060059
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ]
}