{
    "title": "Training Data Leakage Analysis in Language Models",
    "url": "https://openalex.org/W3130178918",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4282362153",
            "name": "Inan, Huseyin A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287562903",
            "name": "Ramadan, Osman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282732251",
            "name": "Wutschitz, Lukas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2203115634",
            "name": "Jones Daniel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282819262",
            "name": "Rühle, Victor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287562904",
            "name": "Withers, James",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3209426591",
            "name": "Sim, Robert",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2950627632",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2159024459",
        "https://openalex.org/W2786233556",
        "https://openalex.org/W2963378725",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W3106051020",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3167352803",
        "https://openalex.org/W1985511977",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W3046102592",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3000899587",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2887995258",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2947160092",
        "https://openalex.org/W2930926105",
        "https://openalex.org/W2413533759",
        "https://openalex.org/W3048775464",
        "https://openalex.org/W2884280357",
        "https://openalex.org/W3110164654",
        "https://openalex.org/W2995022099",
        "https://openalex.org/W3137695714",
        "https://openalex.org/W3112689365",
        "https://openalex.org/W3042943646",
        "https://openalex.org/W3035261884",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2945237470",
        "https://openalex.org/W2911978475",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2999251207",
        "https://openalex.org/W3027379683",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2784621220"
    ],
    "abstract": "Recent advances in neural network based language models lead to successful deployments of such models, improving user experience in various applications. It has been demonstrated that strong performance of language models comes along with the ability to memorize rare training samples, which poses serious privacy threats in case the model is trained on confidential user content. In this work, we introduce a methodology that investigates identifying the user content in the training data that could be leaked under a strong and realistic threat model. We propose two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data. Our metrics further enable comparing different models trained on the same data in terms of privacy. We demonstrate our approach through extensive numerical studies on both RNN and Transformer based models. We further illustrate how the proposed metrics can be utilized to investigate the efficacy of mitigations like differentially private training or API hardening.",
    "full_text": "TRAINING DATA LEAKAGE ANALYSIS IN LANGUAGE MODELS\nHuseyin A. Inan∗\nMicrosoft Research\nhuseyin.inan@microsoft.com\nOsman Ramadan*\nMicrosoft Corporation\nosman.ramadan@microsoft.com\nLukas Wutschitz\nMicrosoft Corporation\nlukas.wutschitz@microsoft.com\nDaniel Jones\nMicrosoft Corporation\nt-dajon@microsoft.com\nVictor R¨uhle\nMicrosoft Corporation\nvirueh@microsoft.com\nJames Withers\nMicrosoft Corporation\njawithe@microsoft.com\nRobert Sim\nMicrosoft Research\nrsim@microsoft.com\nABSTRACT\nRecent advances in neural network based language models lead to successful deployments of such\nmodels, improving user experience in various applications. It has been demonstrated that strong\nperformance of language models comes along with the ability to memorize rare training samples,\nwhich poses serious privacy threats in case the model is trained on conﬁdential user content. In this\nwork, we introduce a methodology that investigates identifying the user content in the training data\nthat could be leaked under a strong and realistic threat model. We propose two metrics to quantify\nuser-level data leakage by measuring a model’s ability to produce unique sentence fragments within\ntraining data. Our metrics further enable comparing different models trained on the same data in\nterms of privacy. We demonstrate our approach through extensive numerical studies on both RNN\nand Transformer based models. We further illustrate how the proposed metrics can be utilized to\ninvestigate the efﬁcacy of mitigations like differentially private training or API hardening.\n1 Introduction\nAdvances in language modeling have produced high-capacity models which perform very well on many language\ntasks. Language models are of particular interest as they are capable of generating free-form text, given a context, or\neven unprompted. There is a plethora of applications where language models have the opportunity to improve user\nexperience, and many of them have been deployed in practice to do so, such as text auto-completion in emails and\npredictive keyboards (illustrated in Fig. 1). Interestingly, language models with massive capacities have been shown to\nachieve strong performance in other tasks as well, e.g. translation, question-answering etc. even in a zero shot setting\nwithout ﬁne-tuning in some cases [Brown et al., 2020a].\nOn the other hand, recent studies have demonstrated that these models can memorize training samples, which can be\nsubsequently reconstructed using probing attacks, or even during free-form generation [Carlini et al., 2019, 2020].\nWhile domain adaptation of general phrases is intended, the model should not leak or memorize rare sequences which\ncould lead to a privacy breach according to GDPR, such as singling out of a user [Art. 29 WP, 2014].\nEfforts to mitigate the risk that a model may yield rare samples which violate privacy include applying differential\nprivacy (DP) during training [Dwork, 2011, Song et al., 2013, Abadi et al., 2016], as well as API hardening to ensure\nthat attackers have little or no access to the model’s underlying probability distributions. While these approaches can\nbe successful, it is challenging to quantify the residual privacy risks in language models, whether or not mitigations\nhave been applied. In this work we propose a methodology for privacy investigations of a language model trained on\nconﬁdential user content. Furthermore, we aim to produce metrics quantifying the likelihood a model might leak rare\ntraining data, under the strictest black-box assumptions about access to the model, i.e. that attackers can access only the\nmodel’s top-kprediction at each token position, given an input preﬁx. This choice of threat model enables us to assess a\nmodel’s risk for realistic deployment scenarios, assuming best practices in API hardening are employed.\n∗Equal Contribution\narXiv:2101.05405v2  [cs.CR]  22 Feb 2021\nTraining Data Leakage Analysis in Language Models\nFigure 1: Two examples of language model deployments in practice. The ﬁgure on the left (image credit: [Lambert,\n2018]) is the Smart Compose feature for Gmail [Chen et al., 2019] and the ﬁgure on the right (image credit: [Microsoft\nSwiftKey]) is the Microsoft SwiftKey Keyboard.\n1.1 Contributions\nThis paper makes the following contributions:\n1. We propose a methodology that investigates the user content in the training data that could be leaked by the\nmodel when prompted with the associated context in terms of user-level privacy.\n2. We introduce metrics that allow comparing models of various kinds (e.g. a DP model vs. a non-DP model)\nthat are trained on the same training data in terms of privacy.\n3. We demonstrate experimental results for both RNN and Transformer based models illustrating the application\nof our approach. We show how the proposed privacy investigation can provide valuable information towards\nprotecting user-level privacy. We study the effects of mitigation techniques such as DP and API hardening\nthrough our metrics.\nThe outline of the paper is as follows. In Section 2, we provide background information for the language models\nfocused in this work. Section 3 deﬁnes the threat model and discusses the ability of an adversary towards attacking a\nlanguage model deployed in practice. In Section 4, we introduce our methodology of investigating a model trained\non user content for the purpose of user-level privacy protection. We discuss special cases in Section 5 and propose\nmetrics to quantify user-level privacy leakage in Section 6. We demonstrate our framework through numerical studies\non real-world datasets in Section 7. Section 8 discusses the related work and future directions and concludes the paper.\n2 Background: Language Models\nLanguage modeling is the task of learning the underlying probability distribution over sequences of words in a\nnatural language. A statistical model for a sequence of tokens w1,...,w n is represented by the joint probability\nPr(w1,...,w n), which can be further decomposed as the product of conditional probabilities:\nPr(w1,...,w n) =\nn∏\ni=1\nPr(wi|w1,...,w i−1). (1)\nHere Pr(wi|w1,...,w i−1) represents the probability of the occurrence of token wi given the previous token sequence\nw1,...,w i−1.\nIt has been shown that neural networks can be utilized to estimate these conditional distributions effectively and be\nemployed as language models [Bengio et al., 2003]. Given an unsupervised corpus of tokens W= {w1,...,w n}, a\nstandard language modeling objective is to maximize the following likelihood function:2\nL(θ) =\nn∑\ni=1\nlog Pr(wi|w1,...,w i−1; θ),\n2The decomposition in (1) is called forward autoregressive factorization. Although not all language models use this factorization\nin their training, we focus on the operation of the ones that could be deployed in practice for the generative text prediction task.\n2\nTraining Data Leakage Analysis in Language Models\nwhere the conditional probability onwi is calculated by evaluating the neural network with parametersθon the sequence\nw1,...,w i−1.\nThe quality of a language model is commonly measured by two metrics, namely perplexity and top- k accuracy.\nPerplexity measures the likelihood of text sequences and is deﬁned as PP(w1,...,w n) = 2−l where\nl= 1\nn\nn∑\ni=1\nlog2 Pr(wi|w1,...,w i−1).\nThe evaluation of the perplexity on unseen data indicates how well the model ﬁts the underlying distribution of the\nlanguage. The smaller the value of perplexity, the better the language model is at modeling the data. Top-kaccuracy\nmetric is deﬁned as the ratio of the number of correct predictions to the total number of tokens3. The relevance of the\nparameter kdepends on the application. For instance, the accuracy for the highest-likelihood candidate (top-1 accuracy)\nis important for text auto-completion feature in emails [Chen et al., 2019] whereas top-3 accuracy is also of interest for\npredictive keyboards (Microsoft SwiftKey, Gboard).\nThere are a vast number of architectures employed for language models. At a high level, these architectures are either\nderived from variants of recurrent neural networks (RNNs) [Mikolov et al., 2010, Sundermeyer et al., 2012, Peters et al.,\n2018] or based on self-attention mechanisms of the transformer [Vaswani et al., 2017, Radford et al., 2018, Howard\nand Ruder, 2018, Devlin et al., 2019, Yang et al., 2019, Radford et al., 2019, Sun et al., 2019, Brown et al., 2020a,\nTuring-NLG, 2020]. Recently, large transformer based models have been achieving impressive state-of-the-art results in\na variety of tasks [Brown et al., 2020a]. On the other hand, RNN based architectures might be favored in practice as\nwell, e.g. when there are strict latency or memory requirements [Chen et al., 2019].\n3 Threat Model\nOur threat model is tailored for privacy considerations when a language model is trained on conﬁdential user content,\nwhich contains sensitive information that would lead to privacy violations in case they are leaked by the model [Art. 29\nWP, 2014, White House Ofﬁce of Science and Technology Policy (OSTP), 2019]. Such privacy considerations are\nin fact legitimate as language models perform next token prediction so they could be used in a generative fashion by\nentering a particular text preﬁx and asking the model to auto-complete indeﬁnitely. Here, the danger is imminent as it\nis not a prioriclear what will be leaked from the user content in the training data. We note that any language model\nwith non-zero utility will necessarily have the top-1 accuracy in the training data bounded away from zero. Therefore,\n“something” will be leaked from the user content in the training data4. Since the main objective of training language\nmodels is modeling the underlying distribution of a language, well-generalized models are not expected to memorize\nthe rare sensitive information in the training data, as they are out-of-distribution and irrelevant to the learning task,\nhence unnecessary to improve the model performance. Recent results show that this is not the case [Carlini et al., 2019,\n2020, Feldman, 2020, Brown et al., 2020b, Petroni et al., 2019]. When the data distribution is long-tailed (as is the\nnatural language [Newman, 2005]), it has been shown that label memorization is necessary for achieving near-optimal\naccuracy on test data [Feldman, 2020, Brown et al., 2020b]. Therefore, it is imperative to build privacy monitoring\ntechniques to minimize the chances of an “accidental” data leakage to prevent privacy violations.\nBased on the discussion above, we consider a practical threat model that is relevant to the language models deployed\nin practice. We assume a black-box access, where a curious or malevolent user can query a pre-trained and deployed\nlanguage model on any sequence of tokens w1,...,w i and receive the top-kpredictions returned by the model for\nthe next token wi+1. See Fig. 2 for an illustrating example. We place no assumption on the parameter kand let it\nbe completely determined by the particular application for which the query is made (see Fig. 1). We note that the\nthreat model does not assume the availability of conﬁdence scores or probabilities for the predictions and it is trivially\napplicable to the deployed models in practice. In fact, even the availability of the next token prediction(s) may not\nalways be the case if the model does not return any prediction under certain conditions (e.g. when the prediction score\nis below a pre-ﬁxed triggering threshold [Chen et al., 2019]). However, since there is no guarantee that all sensitive\ninformation will be on the safe side of the triggering threshold, we believe it might be better to have protection against\nthe worst case where the model prediction is available for the next token wi+1 when any sequence of tokens w1,...,w i\nis queried.\nThe threat model allows a curious user to know whether any sensitive information in their data is leaked by the model.\nTherefore, the data owner can use any preﬁx in their data to query the model. The threat model also includes the case of\na malevolent user, who can input directed queries in order to extract sensitive information about a targeted user.\n3When k> 1, correct prediction refers to the label being in the list of kpredictions returned by the model.\n4We will use the terms “leak” and “correct prediction on the training data” for a model interchangeably in what follows.\n3\nTraining Data Leakage Analysis in Language Models\nFigure 2: Illustration of our threat model. A language model is deployed after being trained on user content. One can\nquery the model with a sequence of tokens and receive the top-kpredictions for the next token (top-1 in this example). A\ncurious user ($NAME) queries the model to see if their address is leaked by the model. More dangerously, an adversary\ninputs a directed query to learn the phone number of the targeted user.\nFigure 3: A typical way of training a language model on a sequence of tokens w1,...,w n (RNN type architecture is\ndepicted for the sake of illustration). The model is yielding a probability distribution ˆyi+1 having seen the context\nw1,...,w i to predict the next token wi+1 for i∈{1,...,n }. We note that the loss function is composed of loss at\neach time step, therefore, the model learns to predict wi+1 with the context w1,...,w i for all i∈{1,...,n }.\n4 Training Data Leakage Analysis\nIn this section, we introduce our framework to investigate a model trained on user content for the purpose of user-level\nprivacy protection. We ﬁx the notation ﬁrst.\nNotation For a language model trained on user content, let U= {U1,...,U n}specify the set of users. For each user\nUi ∈U we deﬁne the set Di = {D1\ni ,D2\ni ,...,D |Di|\ni }(|Di|refers to the size of the set Di) as the corresponding content\non which the language model is trained. Each content Dj\ni is basically a sequence of tokens w1,w2,...,w |Dj\ni |\n5. See\nFig. 3 for an illustration of how a language model is typically trained on a sequence of tokens. The training data Dis\nthe combination of all user content, i.e., D= ∪i∈{1,2,...,n}Di = ∪i∈{1,2,...,n}, j∈{1,2,...,|Di|}Dj\ni .\nWe introduce our training data leakage analysis on a language model after being trained on the training dataD. The\nﬁrst step of our framework is to run the model through the training data and collect its correct predictions in the training\ndata. We illustrate this step with an example in Fig. 4. This collection consists of sequences of tokens where the\ncorrect prediction is observed in top-kpredictions of the model consecutively. We emphasize that consecutive correct\npredictions is an important phenomenon because the longer the model leaks a training sequence wi+1,wi+2,... having\nseen the context w1,...,w i, the more it discloses user content, causing privacy concerns. Therefore, we do not break\nsequences where the model provides correct predictions consecutively and collect all such sequences in the training\ndata. In Algorithm 1 we provide the pseudo-code to collect the correct predictions of the model as described above.\nLet us denote this collection as S. We note that Sis a multiset because it can contain multiple instances of a correctly\npredicted sequence in the training data6.\n5Based on the preprocessing of the training data, this could be a sentence, a paragraph, or an email etc. We emphasize that Di\nmay even be considered as one large text sequence since it belongs to a single user and that is all it matters for our purpose.\n6Henceforth we will use the terms set and multiset interchangeably for S.\n4\nTraining Data Leakage Analysis in Language Models\nFigure 4: An illustration of the collection of correct model predictions. We run the model through each sequence in the\ntraining data and obtain the top-k(top-1 in this example) prediction(s). We then collect the sequence of tokens where\nthe model consecutively provides the correct prediction.\nThe main part of our training data leakage analysis is to provide key features for each sequence in the set S, which\ncontain important information for privacy investigations. We next describe each feature in detail and provide the\naccompanying Table 1 as an illustrative example.\ntotal count inS: This is simply the number of occurrences of a sequence in the multiset S. This feature shows in\ntotal how many times the model leaks a sequence given the correct context. A large number may seem to indicate that\nthe sequence is common and not concerning for privacy, however, that may not be the case if it is present in only a\nsingle user’s data, which is captured in the next feature.\nuser count inS: For any sequence in Swhere the total count is larger than one, here we count the number of distinct\nusers for which the correct prediction of this sequence is made. This is important because a sensitive information of\na single user may appear multiple times in their data (e.g. address being emailed a number of times), which can be\nmemorized by the model. Along with the previous feature we can investigate such cases effectively with this feature.\ntotal count inD: In the previous two features, we count the occurrences in the set S. In the next two features, we\ncount the occurrences in the training data D. This may be helpful because for a sequence that is predicted correctly only\nfor a single user, i.e. user count in Sis one, this may not immediately imply that a sensitive information is leaked by\nthe model. It may be the case that the sequence appears many times among various users’ data, which could provide\n“plausible deniability” in the sense that many other users contributed the model to learn this sequence. This can be\ncalculated by simple string matching and formally expressed as\nn∑\ni=1\n|Di|∑\nj=1\n(count of win Dj\ni ) for a sequence w∈S.\nuser count inD: Connected to the previous feature, here we count the number of distinct users for which a sequence\nin Sis found in their data. We note that here we do not consider whether the model correctly predicts the sequence or not\ngiven the right context, that was done in user count inS. Instead, we calculate this by simply checking if the sequence can\nTable 1: An artiﬁcial example to describe the features of our training data leakage analysis. In this example, there is a\nsequence “very much” appearing two times inS, meaning that it is predicted correctly two times by the model. These\ncorrect predictions appear in a single user’s data (either in a singleDj\ni or two different Dj1\ni and Dj2\ni for some user Ui).\nThe corresponding contexts on which the model produces this correct sequence are “Thank you” and “I like cats” and\nthe corresponding perplexities are 1.3 and 3.6. On the other hand, the sequence “very much” itself appears ten times in\nthe training data D(only in two of which the model predicts the sequence correctly), among ﬁve user’s data.\nS TOTAL # IN S USER # IN S TOTAL # IN D USER # IN D CONTEXT (S) PERP.\n“VERY MUCH ” 2 1 10 5 [“THANK YOU ”,\n“I LIKE CATS ”]\n[1.3, 3.6]\n5\nTraining Data Leakage Analysis in Language Models\nbe found in a user’s data via string match. This can be formally expressed as\nn∑\ni=1\n1(∃j ∈{1,2,..., |Di|}s.t. w⊆Dj\ni )\nwhere 1(·) is an indicator function for a sequence w∈S. Sequences for which the user count in Dis large are unlikely\nto be sensitive for a single user. However, there might still be concerning cases, e.g., the model predicts the sequence\ncorrectly only for a single user (i.e. user count inSis one) although there is plausible deniability as discussed previously.\ncontext(s) For any sequence in S, this feature provides the corresponding context(s) with which when prompted the\nmodel it predicts the sequence correctly. This feature is useful to check for instance if long sequences can be found with\nshort contexts, which means that the model leaks a long user content when prompted with a short initial context.\nperplexity(ies) Connected to the feature above, this feature provides the corresponding perplexity(ies) for any\nsequence in S. This is also an important feature as it shows how certain the model is when predicting a sequence.\nFurthermore, it allows comparing the correct predictions of the model with a public model7. Considering a sequence\nin Swhere the user count in Dis one, a small perplexity on the language model along with a large perplexity on\npublic model might indicate that a sensitive information is leaked about the corresponding user since the sequence is\n“surprising” to the public model by the large perplexity.\nBefore concluding this section, we discuss a number of points regarding our training data leakage analysis.\n• In our framework, the sequences in Sare created when the model is prompted with the “right” context, i.e. the\ncontext on which the model is trained for next token prediction task. However, a sensitive information might\neven be leaked under a context not appearing in the training data. Such a case might be missed if the same\nsensitive information is predicted wrong under its corresponding context in the training data, therefore not\nbeing included in S. Looking at the loss function in Fig. 3, intuitively one can expect that the model would\nmore likely predict the sequence under the context it has seen during training than any other context, however,\nthere is also no guarantee that this will always be the case. Therefore, it might be worthwhile to extend the set\nof contexts beyond the training data and then create the set S. This is an interesting future direction that could\nstrengthen the privacy investigation of a language model.\n• Such a detailed investigation of sequences in our training data leakage analysis may not be possible if the\nmodel training is done with no access to look at the training data [Chen et al., 2019]. In that case, we can\nsimply replace each sequence and its corresponding context(s) with their length of tokens and still obtain\nvaluable information. For example, we can investigate the length of the sequences for which the user count in\nSis small to see if long completions are possible or check the length of the contexts to see if the leakage is\npossible with short contexts. The perplexities could also be very helpful in this case because we can measure\nhow surprising each sequence is to a public model that is only trained on public dataset(s). If the perplexities\n7Public model refers to a language model trained on a public dataset.\nAlgorithm 1The collection of correct model predictions.\nInput: A language model LM(·) and the corresponding training data D\nOutput: The (multi)set Sof correct predictions\nInitialize S= []\nfor i= 1to ndo\nfor j = 1to |Di|do\nInitialize W = “”\nLet Dj\ni = [w1,...,w |Dj\ni |]\nfor l= 1to |Dj\ni |do\nObtain top-kpredictions preds= LM(Dj\ni [: l])\nif wl ∈predsthen\nAppend wl to W\nelse ifW ̸= “”then\nAppend W to Sand initialize W = “”\nend if\nend for\nend for\nend for\n6\nTraining Data Leakage Analysis in Language Models\nare similar, then this indicates that the prediction is as “familiar” to the public model, therefore, unlikely to be\na privacy concern for a user.\n• While Algorithm 1 sequentially goes over the training data to illustrate our approach, we point out that all the\nsteps from the collection of correct model predictions to the counts performed over the training set are fully\nparallelizable for an efﬁcient implementation. We provide a more detailed discussion on the complexity of our\napproach in Appendix A.\n5 Filtering for Special Cases\nOur leakage analysis may include a massive number of sequences when the training data Dis large, making privacy\ninvestigations infeasible. However, simple ﬁltering procedures can be applied to reduce the number of sequences\neffectively. For instance, keeping the ones for which the user count in Sis less than pfor some threshold pof interest is\na reasonable operation. The sequences ﬁltered here would the ones where there are at least pusers having this sequence\nin their data and the model predicts the sequence correctly for each of these users. From the perspective of model\npredictions, this is reminiscent of k-anonymity [Sweeney, 2002] as a famous data anonymization technique8.\nAnother important case is regarding the sequences in Sfor which the user count in Dis one, i.e., there is only one user\nhaving this sequence in their data and the model leaks the sequence when prompted with the corresponding context9.\nThis is inarguably the case with the most potential to result in privacy violations since leakage of a unique content of a\nuser may lead to singling out of that user. Let us denote the set of sequences in Sthat are unique to a user as Suniq. In\nthe next section, we focus on the set Suniq and propose metrics that quantify user-level privacy leakage.\n6 Metrics to Quantify Privacy Leakage\nIn this section, we propose two metrics to quantify user-level privacy leakage, which are straightforward to interpret,\nand compare different models trained on the same training data in terms of privacy:\n1. The ﬁrst metric is the number of sequences in Suniq, i.e. the sequences in Sfor which the user count in Dis\none.\n2. The second metric is a curated version of the ﬁrst metric. We still consider the sequences in the set Suniq but\nwe remove the ones for which the ratio of the perplexity with respect to a public model and our language\nmodel is below some threshold t, i.e. PPpublic(w)/PPlm(w) <t. This basically ﬁlters out the unique sequences\nthat have similar perplexities with respect to a public model as there is plausible deniability of similar leakage\npossibility, given a public model. We further deﬁne the worst-case leakage epsilon\nϵl ≜ max\nw∈Suniq\nlog\n(PPpublic(w)\nPPlm(w)\n)\n, (2)\nmeasuring the perplexity ratio with respect to a public model maximized over the sequences in the set Suniq to\ncapture the worst-case scenario.\nWe next discuss the advantages and disadvantages of the proposed metrics. The ﬁrst metric is simple and easy to use.\nHowever, we observe in our experiments that even a public model that is not trained on a private data can predict unique\nsequences in the private data. Such unique sequences would likely not constitute a privacy violation since the public\nmodel has not seen any private data in its training. The disadvantage of this metric is that it does not consider the\nlikelihood that sequences in Suniq might also be easily predicted by a public model, reducing potential privacy risks.\nOur second metric touches on this point by eliminating the sequences that do not look surprising to a public model.\nHowever, the main disadvantage of this metric is the hardship of the choice of the threshold t. Therefore, we introduce\nthe term leakage epsilon, denoted by ϵl, measuring the worst-case perplexity ratio with respect to a public model over\nthe unique sequences. This is motivated by the deﬁnition of differential privacy (DP), which bounds the worst-case\neffect of a single substitution in the data. DP is a strong framework providing global guarantees for an algorithm\nover all possible users and all possible training sets. On the other hand, Eq. (2) compares the relative likelihoods of\nmodel outputs with respect to a public reference model and introduces a metric to compare different models (including\nnon-DP ones) in terms of privacy. A smaller ϵl for a language model translates into a better privacy protection as the\n8Since the contexts will likely be different, k-anonymity is still substantially more powerful for anonymization.\n9or contexts. Note that the total count in Dand total count in Scan still be arbitrary since the sequence can appear multiple times\nin the user’s data. Nevertheless, since it belongs to only one user, the sequence should be protected equally.\n7\nTraining Data Leakage Analysis in Language Models\nunique sequences leaked by the model will have relatively similar perplexities with respect to a public model, providing\nplausible deniability for each one of them.\nThe ﬁnal point related to both metrics is the choice of the public model. We note that ϵl depends on the public model\nused in Eq. (2). If the distribution of the public dataset differs substantially from that of the private one used to train the\nlanguage model, this may lead to a pessimistic ϵl value. To circumvent this, one might use a collection of public models\n(P) trained on various public datasets and modify the term as ϵl ≜ max\nw∈Suniq\nmin\npublic∈P\nlog (PPpublic(w)/PPlm(w)). One can\nfurther consider removing the users for which the language model leaks a unique sequence (i.e. the users in Suniq) and\nall their data from the training data and train another model on the remaining users. This latter model may be employed\nas a “public model” in Eq. (2) to calculate ϵl as it has not seen any data of any user in Suniq during its training. This is\nalso motivated from differential privacy, which provides the guarantee that removing a user from the dataset will not\nchange the probability distribution of the output of an algorithm substantially (bounded by exp(ϵ)).\n7 Case Study: Tab Attack\nIn this section, we provide case studies of our leakage analysis. We consider an attack setting that has access to top-1\npredictions of a language model. Having in mind the text auto-completion feature in emails where the predictions are\napplied by pressing the TAB key on the keyboard (see Fig. 1), we dub this as the tab attack. We investigate the unique\nsequences (Suniq) that could be leaked via the tab attack when the model is queried with the corresponding context. We\nnote that although the attacker ability is limited to top-1 predictions, the model builder can utilize all information to\ninvestigate the unique sequences that could be leaked by the tab attack prior to the model deployment. We apply our\nleakage analysis to the unique sequences (Suniq) to assess the attack surface under the tab attack threat model.\n7.1 RNN-based model on Reddit dataset\nWe study a large-scale example as a realistic setup for the deployed language models in practice.\nDataset We use a large dataset of Reddit posts, as decribed by Al-Rfou et al. [2016], that contains 140M sentences\nfrom 4.4M users for a randomly chosen month (Oct 2018). It is randomly split into 90% training and 10% validation\nsets. Reddit dataset is commonly used in privacy research since each post in the dataset is keyed by a user so the data\ncan be grouped by users to provide user-level privacy. We provide three sets of language models trained on this private\nReddit dataset.\n1. A language model trained on the Reddit dataset. This will be referred to as Private LMin our results.\n2. A language model trained on the Reddit dataset with differential privacy [McMahan et al., 2018, Abadi et al.,\n2016]. We take four snapshots of the model during training, corresponding to four DP language models with\nepsilons 3.28, 4.68, 6.20, and 26.410. The training begins with a random initialization of the weights. The\nmodels will be referred to as DP-LM RanIniϵ= ·.\n3. A language model trained on the Reddit dataset with differential privacy. Here, the model weights are\ninitialized from a public model trained on Google News dataset [Chelba et al., 2013]. It has been shown that\ntransfer learning helps obtaining strong privacy guarantees with a minor cost in utility [McMahan et al., 2018,\nAbadi et al., 2016, Tram`er and Boneh, 2020, Papernot et al., 2020]. We similarly take three snapshots of the\nmodel during training, corresponding to three DP language models with epsilons 2.98, 4.47, and 6.68. These\nmodels and the public model will be referred to as DP-LM PubIniϵ= ·and Public LMrespectively.\nThe model architecture is same for all these models and the details are speciﬁed below.\nModel We use a one-layer GRU model as the language model for the next-word prediction task. The embedding\nsize is set to 160 and the hidden size to 512, and the vocabulary is ﬁxed to the most frequent 10k words in the training\ncorpus (out of 3.2M words). We use the Adamax optimizer with the learning rate set to 1e-3 and the batch size is set to\n3072 in the DP training and to 512 otherwise.\nWe provide in Table 2 the performances of the models and the result of the tab attack for each of them. We discuss the\nresults of this experiment in what follows.\nWe observe from Table 2 that the private LM that is trained without DP leaks a huge number of unique sequences (3757)\nfrom the training data. There are 759 unique sequences for which the number of tokens is larger than 9. A majority of\n10The models satisfy user-level DP and δ≲ 1/(# users) same for all models.\n8\nTraining Data Leakage Analysis in Language Models\nTable 2: Results of the experiment on RNN-based language models trained on Reddit dataset [Al-Rfou et al., 2016].\nWe provide the perplexity and accuracy on the validation set to compare the performances of the models. In the next\ncolumn, we provide the number of unique sequences (i.e. |Suniq|) for each model. We calculate leakage epsilon ϵl for\nsome of the models for comparison in the last column.\nMODEL VAL PERP VAL ACC (%) # UNIQUE SEQ . (|Suniq|) ϵl\nPRIVATE LM 69.4 23.7 3757 17.75\nDP-LM R ANINI ϵ= 3.28 290.0 14.5 0 -\nDP-LM R ANINI ϵ= 4.68 130.3 19.6 5 -\nDP-LM R ANINI ϵ= 6.20 107.8 20.8 11 0.64\nDP-LM R ANINI ϵ= 26.40 96.5 21.5 30 -\nPUBLIC LM 757.5 13.1 159 -\nDP-LM P UBINI ϵ= 2.98 183.1 19.7 157 0.29\nDP-LM P UBINI ϵ= 4.47 106.7 21.9 203 -\nDP-LM P UBINI ϵ= 6.68 92.8 22.2 246 1.33\nthese examples are coming from highly-repeated sentences (728 of these sequences are repeated somewhere between\n50-34372 times) by the bots in the Reddit dataset11. Expectedly, the resulting ϵl = 17.75 is very large and it does not\noffer any reasonable privacy guarantee. We calculatedϵl among the unique sequences that do not repeat more than once\nand found that it is 4.60. This shows the importance of de-duplication at a granular level (e.g. removal of sentence\nduplicates) as also observed by Carlini et al. [2019, 2020].\nFor the DP-LMs that are snapshots of a model trained with random initialization of weights, we observe small\nnumber of unique sequences leaked by the models. Interestingly, we get no unique sequence with the ﬁrst one having\nϵ= 3.28, although there is a high cost in terms of utility. We provide the list of unique sequences for the models with\nϵ = 4.68,6.20 and 26.4 in Table 4, 5, and 6 of Appendix B respectively. We observe the efﬁcacy of user-level DP\ntraining by noting that the unique sequences with large repetitions that were memorized by the private model have all\ndisappeared with DP-LMs. Furthermore, there is a substantial decrease in the number of unique sequences, even for the\nDP-LM with relatively high epsilon value ϵ= 26.4, which does not provide a reasonable theoretical privacy guarantee.\nA interesting phenomenon we have observed consistently over all experiments is about the punctuation. Almost all\nunique sequences for the DP-LMs presented in Appendix B have a punctuation mark. In our experiments we did not\nexclude the punctuation from the model predictions and treated them as any other token in the training data.\nFor the DP-LMs initialized from a public model, we observe relatively larger number of unique sequences leaked by\nthe models. However, this is not surprising as the public model itself can predict 159 unique sequences in the private\ndata, without seeing any private data in its training. Since the DP training is initialized from the public model in this\ncase, it should be expected to get larger number of unique sequences. We note that leakage epsilon ϵl may provide a\nbetter ground for a fair comparison of models trained in different ways (e.g. random initialization vs. transfer learning).\nWe calculate leakage epsilon ϵl for three DP-LMs for comparison. The public models in the calculation of Eq. (2) are\nas follows. For each model, we take the users who are the owners of the unique sequences leaked by the model and\nremove all their data from the training data. We subsequently train a new model on the remaining users. We consider\nthe new model as the public model for the users of the unique sequences since it has not seen any data of these users\nduring its training. We observe that DP-LM PubIni ϵ = 2.98 model has ϵl = 0.29, much smaller than the models\nDP-LM RanIni ϵ= 6.20 with ϵl = 0.64 and DP-LM PubIni ϵ= 6.68 with ϵl = 1.33. This may not be surprising since\nϵ= 2.98 provides much stronger privacy guarantees compared to ϵ= 6.20 and ϵ= 6.68. We note that all three models\nhave quite small ϵl values, indicating that the unique sequences leaked by these models can also be simply learned from\nother users because they have similar perplexities with respect to the public model.\n7.2 Transformer-based model on Reddit dataset\nWe next study a Transformer-based language model in a similar setting as the previous section.\nModel We use a GPT2 type model [Radford et al., 2019] with six transformer layers with twelve attention heads each\nwith a hidden dimension of 768 totalling about 82M parameters. The data is tokenized using a byte-level version of\n11An example of a unique sequence memorized by the model is “has been automatically removed because the title does not include\none of the required tags .” repeated 5377 times in the bot’s data.\n9\nTraining Data Leakage Analysis in Language Models\nByte Pair Encoding (BPE) and a vocabulary size of 50257. The freely available base model,12 which was trained on the\nOpenWebText and wikitext datasets [Gokaslan and Cohen, 2019, Merity et al., 2017], acts as the public model.\nDataset We choose a random month (Jan 2019) of the Reddit dataset similar to the experiment in the previous section.\nWe drop posts which are shorter than 80 tokens and cap long posts to 256 tokens. The ﬁnal training dataset contains\n1.5M posts from 1.3M users. We perform a similar pre-processing step on the data for Feb 2019 and use 50,000 samples\nas the validation set.\nWe use three language models to perform a similar analysis. In addition to the public model, we ﬁne-tune the public\nmodel with and without differential privacy on the private Reddit dataset and refer to them as DP-LM and Private LM\nrespectively. The DP language model provides good user-level privacy withϵ= 1and δ= 10−7.\nTable 3: Results of the experiment on Transformer-based language models. The columns follow similarly as in Table 2.\nMODEL VAL PERP # UNIQUE SEQ . (|Suniq|) ϵl\nPRIVATE LM 49.7 333099 11.96\nDP-LM 81.0 150247 2.25\nPUBLIC LM 167.9 115920 -\nWe present in Table 3 the results of the experiment. We ﬁrst note that even the public model produces an enormous\nnumber of unique sequences. This is because (i) BPE tokenizer has no out-of-vocabulary tokens, thus creating myriad\nunique sub-word token sequences in the training data and (ii) Transformer-based models have the attention mechanism,\nwhich gives a strong ability to copy tokens from context to predict the next token. This highlights the need for our\nmetrics introduced in Section 6 to ﬂag leaked unique sequences which can be considered not a privacy issue.\nWe observe that DP indeed offers a reliable way of reducing the leakage. We have selected a model of acceptable\nperplexity of 81 while maintaining good user-level epsilon of 1. The corresponding leakage epsilon is ϵl = 2.25,\nsigniﬁcantly improving to that of the Private LM with ϵl = 11.96 (recall that ϵl is log-based).\nAlthough there is no sentence-level repetition in the dataset, the reason that Private LM still has very high epsilon\nleakage may be attributed to the strong memorization capability of Transformer-based models. Furthermore, sub-word\ntokenization strongly necessitates personally identiﬁable information (PII) scrubbing of the training data, as from our\nobservations many unique sequences leaked by Private LM for which the perplexity ratio in Eq. (2) is large contain PII\nthat can easily lead to singling out of a user.\n8 Related Work and Conclusion\nA wide body of work has demonstrated privacy issues in general for machine learning models trained on personal\ndata. Language models are among the most to suffer as they are capable of generating text which may potentially leak\nsensitive user content and lead to serious privacy violations.\nZhang et al. [2017] show that deep learning models can achieve perfect accuracy even on randomly labeled data. Such\nmemorization capability may in fact be needed to achieve near-optimal accuracy on test data when the data distribution\nis long-tailed as recently shown by Feldman [2020], Brown et al. [2020b]. Unfortunately this can lead to a successful\ntraining data extraction attack, as in the case for the concurrent work [Carlini et al., 2020] that can recover individual\ntraining examples from the GPT-2 language model [Radford et al., 2019]. In their method, Carlini et al. [2020] generate\na list of sequences by sampling from the GPT-2 language model and then curate it by using the perplexity measure. In a\nrelated line of work which exploits the increasingly common transfer learning setup, Zanella-B´eguelin et al. [2020]\nhave demonstrated that having simultaneous black box access to the pre-trained and ﬁne-tuned language models allows\nthem to extract rare sequences from the smaller and typically more sensitive ﬁne-tuning dataset. Both attacks rely on\nthe model output beyond top-1 or top-3 predictions along with the perplexity measure. Access to this information may\neasily be restricted in deployed language models. Nevertheless, there are serious privacy concerns since the attacks can\nextract personally identiﬁable information even if they are present in one document in the training data. We believe that\nour proposed procedure for privacy investigations of a language model trained on user content could be very beneﬁcial\nto protect user-level privacy in the presence of such attacks.\nOn the other hand, Carlini et al. [2019] introduced the exposure metric to quantitatively assess the unintentional\nmemorization phenomenon occurring in generative sequence models. They do so by inserting randomly-chosen canary\n12https://huggingface.co/distilgpt2\n10\nTraining Data Leakage Analysis in Language Models\nsequences a varying number of times into the training data and measuring the relative difference in perplexity between\ninserted canaries and non-inserted random sequences. Our work is complementary in the sense that we are investigating\nthe information leaked from user content in the training data, having in mind a strong threat model where one can query\nthe language model with the precise context appearing in the training data. We believe that our proposed metrics along\nwith the exposure metric can be employed together to provide strong privacy guarantees for a deployed language model.\nAnother line of work has studied the vulnerability of machine learning models to membership inference attack [Shokri\net al., 2017, Yeom et al., 2018, Song and Shmatikov, 2019, Nasr et al., 2019, Long et al., 2018, Hayes et al., 2019,\nTruex et al., 2018, Irolla and Chˆatel, 2019, Hisamoto et al., 2020, Salem et al., 2018, Sablayrolles et al., 2019, Leino\nand Fredrikson, Choquette-Choo et al., 2020]. The goal is to determine if a particular data record (or more generally\ndata of a given user) belongs to the training set of the model. Although being an indirect leakage, membership inference\nis considered as a conﬁdentiality violation and potential threat to the training data from models [Murakonda and Shokri,\n2020].\nThe main framework with theoretical guarantees for user-level privacy is the application of differential privacy (DP)\n[Dwork, 2011] to model training. DP makes provable guarantees about the privacy of a stochastic function of a given\ndataset. Differentially private stochastic gradient descent (DP-SGD) has been developed and applied to training machine\nlearning models [Song et al., 2013, Abadi et al., 2016]. This is an active area of research with the goal of pushing the\nfrontiers of privacy-utility trade-off for deep neural networks.\n8.1 Future work\nWe discuss a number of interesting future directions following our work:\n• The proposed leakage analysis is based on central learning setting where the training data is stored at a central\nserver. It would be interesting to solve the challenge of applying this method to other settings, such as federated\nlearning [Kairouz et al., 2019] where machine learning models are trained on decentralized on-device data.\n• We are hopeful that the metrics proposed in this work, as a ﬁrst attempt to quantify user-level privacy leakage,\nwould initiate further research on the topic, which will lead to further improvements on these metrics.\n• It would be valuable to study the proposed methodology on more models/datasets, which would shed new\nlights on the protection of user-level privacy when language models are trained on conﬁdential user content.\n8.2 Conclusion\nRecent results show that language models are capable of memorizing training samples under the hood of their impressive\nperformance. This poses an immediate threat as leaking rare user content could lead to a privacy breach according to\nregulations such as GDPR, e.g. due to singling out of a user.\nThis work introduced a methodology to investigate information leaked by a language model from its training data in\nterms of privacy. We proposed metrics that could be used to quantify user-level privacy leakage and allow comparing\nmodels trained on the same data in terms of privacy. We believe our framework can be incorporated into the training\nplatform of language models that would help assess the model from the perspective of privacy, along with its utility.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, and Melanie Subbiah. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020a.\nNicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evaluating and testing\nunintended memorization in neural networks. In USENIX Security 2019, August 2019.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine Lee, Adam Roberts,\nTom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large\nlanguage models. arXiv preprint arXiv:2012.07805, 2020.\nArt. 29 WP. Opinion 05/2014 on “Anonymisation Techniques”, 2014. URL https://ec.europa.eu/justice/\narticle-29/documentation/opinion-recommendation/files/2014/wp216_en.pdf.\nPaul Lambert. SUBJECT: Write emails faster with Smart Compose in Gmail, 2018. URL https://www.blog.\ngoogle/products/gmail/subject-write-emails-faster-smart-compose-gmail .\nMia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang,\nAndrew M. Dai, Zhifeng Chen, Timothy Sohn, and Yonghui Wu. Gmail Smart Compose: Real-time assisted writing.\nKDD ’19, page 2287–2295, New York, NY , USA, 2019.\n11\nTraining Data Leakage Analysis in Language Models\nMicrosoft SwiftKey. URL https://www.microsoft.com/en-us/swiftkey.\nCynthia Dwork. Differential privacy. Encyclopedia of Cryptography and Security, 2011.\nShuang Song, Kamalika Chaudhuri, and Anand D. Sarwate. Stochastic gradient descent with differentially private\nupdates. In 2013 IEEE Global Conf. on Signal and Information Processing, pages 245–248, 2013.\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep\nlearning with differential privacy. In ACM CCS, 2016.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model.\nJournal of machine learning research, 3:1137–1155, 2003.\nGboard. The Machine Intelligence Behind Gboard. URL https://ai.googleblog.com/2017/05/\nthe-machine-intelligence-behind-gboard.html .\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan “Honza” ˇCernock´y, and Sanjeev Khudanpur. Recurrent neu-\nral network based language model. In Proceedings of the 11th Annual Conference of the International Speech\nCommunication Association, pages 1045–1048, 2010.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. LSTM neural networks for language modeling. In INTER-\nSPEECH, pages 194–197, 2012.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undeﬁnedukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY , USA, 2017.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative\npre-training, 2018.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328–339,\nMelbourne, Australia, July 2018. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet: Generalized\nautoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 5753–5763,\n2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners, 2019.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua\nWu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.\nTuring-NLG. A 17-billion-parameter language model by Microsoft, 2020. URL https://www.microsoft.com/\nen-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/ .\nWhite House Ofﬁce of Science and Technology Policy (OSTP). Guidance for regulation of artiﬁcial\nintelligence applications, 2019. URL https://www.whitehouse.gov/wp-content/uploads/2020/01/\nDraft-OMB-Memo-on-Regulation-of-AI-1-7-19.pdf .\nVitaly Feldman. Does learning require memorization? A short tale about a long tail. In Proceedings of the 52nd Annual\nACM SIGACT Symposium on Theory of Computing, STOC 2020, page 954–959, New York, NY , USA, 2020.\nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training\ndata necessary for high-accuracy learning? arXiv preprint arXiv:2012.06421, 2020b.\nFabio Petroni, Tim Rockt¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\nLanguage models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China, November 2019. Association for Computational Linguistics.\nM.E.J. Newman. Power laws, Pareto distributions and Zipf’s law. Contemporary Physics, 46(5):323–351, 2005.\n12\nTraining Data Leakage Analysis in Language Models\nLatanya Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty, Fuzziness and\nKnowledge-Based Systems, 10(05):557–570, 2002.\nRami Al-Rfou, Marc Pickett, Javier Snaider, Y .-H. Sung, Brian Strope, and Ray Kurzweil. Conversational contextual\ncues: The case of personalization and history for response ranking, 2016.\nBrendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language\nmodels. In International Conference on Learning Representations (ICLR), 2018.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One\nbillion word benchmark for measuring progress in statistical language modeling, 2013. URL http://arxiv.org/\nabs/1312.3005.\nFlorian Tram`er and Dan Boneh. Differentially private learning needs better features (or much more data).arXiv preprint\narXiv:2011.11660, 2020.\nNicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making the shoe ﬁt:\nArchitectures, initializations, and tuning for learning with privacy, 2020. URL https://openreview.net/forum?\nid=rJg851rYwH.\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus,\n2019.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. ICLR, 2017.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires\nrethinking generalization. ICLR, 2017.\nSantiago Zanella-B´eguelin, Lukas Wutschitz, Shruti Tople, Victor R¨uhle, Andrew Paverd, Olga Ohrimenko, Boris K¨opf,\nand Marc Brockschmidt. Analyzing information leakage of updates to natural language models. In Proceedings of\nthe 2020 ACM SIGSAC Conference on Computer and Communications Security, page 363–375, 2020.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine\nlearning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3–18, 2017.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the\nconnection to overﬁtting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 268–282,\n2018.\nCongzheng Song and Vitaly Shmatikov. Auditing data provenance in text-generation models. In KDD, 2019.\nMilad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active\nwhite-box inference attacks against centralized and federated learning. In 2019 IEEE Symposium on Security and\nPrivacy (SP), pages 739–753, 2019.\nYunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, Haixu Tang, Carl A. Gunter, and Kai Chen.\nUnderstanding membership inferences on well-generalized learning models. arXiv preprint arXiv:1802.04889, 2018.\nJamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. LOGAN: Membership inference attacks\nagainst generative models. Proceedings on Privacy Enhancing Technologies, 2019(1):133 – 152, 2019.\nStacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. Towards demystifying membership inference\nattacks. arXiv preprint arXiv:1807.09173, 2018.\nPaul Irolla and Gr´egory Chˆatel. Demystifying the membership inference attack. In 2019 12th CMI Conf. on Cybersecu-\nrity and Privacy (CMI), pages 1–7, 2019.\nSorami Hisamoto, Matt Post, and Kevin Duh. Membership inference attacks on sequence-to-sequence models: Is my\ndata in your machine translation system? TACL, 8:49–63, 2020.\nAhmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes. ML-Leaks: Model and data indepen-\ndent membership inference attacks and defenses on machine learning models. arXiv preprint arXiv:1806.01246,\n2018.\nAlexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herve Jegou. White-box vs black-box:\nBayes optimal strategies for membership inference. In Proceedings of the 36th International Conference on Machine\nLearning, volume 97 of Proceedings of Machine Learning Research, pages 5558–5567, Long Beach, California,\nUSA, 09–15 Jun 2019.\nKlas Leino and Matt Fredrikson. Stolen Memories: Leveraging model memorization for calibrated white-box member-\nship inference. 29th USENIX Security Symposium 2020.\nChristopher A. Choquette-Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot. Label-only membership\ninference attacks. arXiv preprint arXiv:2007.14321, 2020.\n13\nTraining Data Leakage Analysis in Language Models\nSasi Kumar Murakonda and Reza Shokri. ML Privacy Meter: Aiding regulatory compliance by quantifying the privacy\nrisks of machine learning. arXiv preprint arXiv:2007.09339, 2020.\nPeter Kairouz, H. Brendan McMahan, Brendan Avent, Aur ´elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith\nBonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, and et al. Advances and open problems in\nfederated learning. arXiv preprint arXiv:1912.04977, 2019.\nA Complexity of our approach\nWe utilize Spark for an efﬁcient implementation of our approach. The model evaluation was the computation bottleneck\nstage in the pipeline, since Spark does not offer tools to run deep neural networks efﬁciently. To mitigate this and\nmaximise the Spark clusters’ CPU utilization during model evaluation, we batch all the rows in each Spark RDD\npartition as one batched tensor and feed it to the model. Subsequently, we re-partition and convert the output to a\ndataframe for the next stage. This method allows us to achieve a sub-linear improvement of the average processing\nspeed per line as a function of the number of Spark nodes. Consequently, by increasing the number nodes linearly with\nthe number of rows in the database we achieve approximately O(1) complexity.\nB Tab attack for the DP-LM RanIniϵ = ·models in Section 7.1\nWe present the leakage analysis for the unique sequences coming out of the tab attack for the DP-LM RanIni ϵ= ·\nmodels in Table 4, 5, and 6.\nTable 4: Unique sequences from the tab attack for the DP-LM RanIni ϵ= 4.68 model in Section 7.1.\nSuniq TOTAL # IN D USER # IN D CONTEXT LEN\n“WAY , I DON ’T THINK IT IS ” 1 1 2\n“THE TIME , I WOULD BE ” 1 1 8\n“SAME THING , I WOULD BE ” 1 1 10\n“MEDIA ) IS NOT ” 1 1 10\n“NOT BE ) BUT ” 1 1 9\nTable 5: Unique sequences from the tab attack for the DP-LM RanIni ϵ = 6.20 model in Section 7.1. The PPlm(·)\ncolumn is the perplexity of each sequence with respect to the DP-LM RanIni ϵ= 6.20 model. The PPpublic(·) column is\nthe perplexity of each sequence with respect to the public model. The last column is the log-ratio of the perplexities of\nthe previous two columns. The worst-case leakage epsilon is ϵl = 0.64. We refer the following model as the public\nmodel in this table. We remove the users that are the owners of these unique sequences and all of their data (not just\nthese sequences) from the training data and train a new model with the remaining users. We consider the new model as\na public model for the users of these unique sequences since it has never seen any data of these users during its training.\nSuniq TOTAL # IN D USER # IN D CONTEXT LEN PPlm(·) PPpublic(·) log PPpublic(·)\nPPlm(·)\n“SAID , I THINK YOU SHOULD\nBE ABLE TO ”\n1 1 3 4.39 5.21 0.17\n“ME A LINK TO YOUR POST ?” 1 1 4 3.33 3.49 0.05\n“YOU FEEL BETTER , THEN YOU\nCAN ”\n1 1 4 5.4 5.53 0.02\n“HAS ANY QUESTIONS OR CON -\nCERNS , PLEASE ”\n1 1 5 5.1 8.87 0.55\n“AS I KNOW , I THINK THE ” 1 1 3 3.75 4.5 0.18\n“COURT , HE WOULD HAVE ” 1 1 8 4.46 5.22 0.16\n“WANT * TO BE ?” 1 1 13 4.17 3.63 -0.14\n“LIKE IS THAT YOU ARE ” 1 1 4 5.35 5.78 0.08\n“OF PEOPLE ) ARE ” 1 1 6 5.14 4.21 -0.2\n“WARS , WE HAVE ” 1 1 5 7.28 7.36 0.01\n“WARS ) IS” 1 1 7 3.53 6.69 0.64\n14\nTraining Data Leakage Analysis in Language Models\nTable 6: Unique sequences from the tab attack for the DP-LM RanIni ϵ= 26.4 model in Section 7.1.\nSuniq TOTAL # IN D USER # IN D CONTEXT LEN\n“THE OTHER HAND , I WOULD HAVE TO ” 1 1 2\n“LOT OF PEOPLE WHO DON ’T KNOW .” 1 1 12\n“THE CASE , THEN I WOULD HAVE ” 1 1 4\n“FAIR , I DON ’T THINK YOU CAN ” 1 1 3\n“IDEA WHAT YOU ARE DOING , YOU ” 1 1 5\n“AS I KNOW , I HAVE A ” 1 1 3\n“TO DO , I WOULD HAVE TO ” 1 1 10\n“YOU FEEL BETTER , THEN YOU CAN ” 1 1 4\n“YOUR QUESTION , YOU CAN ONLY ” 1 1 3\n“OF CURIOSITY , I THINK THE ” 1 1 2\n“IDEA WHAT HE WAS DOING ?” 1 1 6\n“DOING , YOU CAN GET A ” 1 1 7\n“COURT HAVE TO DO WITH THE ” 1 1 7\n“POINT , IT IS NOT .” 1 1 14\n“DIFFERENT SITUATION , IT’S A ” 1 1 6\n“OF YEARS , I WAS” 1 1 4\n“CHANGE , THEY WOULD BE ” 1 1 6\n“ASSAULT , I WOULD BE ” 1 1 12\n“* ARE * REALLY *” 1 1 8\n“* DO IT , THEN ” 1 1 4\n“MEDIA ) IS NOT ” 1 1 10\n“COURT , THEN THEY ” 1 1 7\n“BUTTER , I WOULD ” 1 1 7\n“FRANCISCO , I THINK ” 1 1 6\n“COURT HAVE TO BE ” 1 1 5\n“OF PEOPLE ) ARE ” 1 1 6\n“% AGREE ).” 1 1 15\n“ARABIA * IS” 1 1 8\n“ARABIA ) IS” 1 1 7\n“WARS ) IS” 1 1 7\n15"
}