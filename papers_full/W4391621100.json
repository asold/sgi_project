{
  "title": "Multilingual Transformer and BERTopic for Short Text Topic Modeling: The Case of Serbian",
  "url": "https://openalex.org/W4391621100",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Medvecki, Darija",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4376740492",
      "name": "Bašaragin, Bojana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4376740491",
      "name": "Ljajić, Adela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207109957",
      "name": "Milošević Nikola",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4237791300",
    "https://openalex.org/W4298009845",
    "https://openalex.org/W2114508388",
    "https://openalex.org/W4221142221",
    "https://openalex.org/W3179463712",
    "https://openalex.org/W4229011615",
    "https://openalex.org/W2887928931",
    "https://openalex.org/W3207523740",
    "https://openalex.org/W3200230513",
    "https://openalex.org/W3206644354",
    "https://openalex.org/W4224291460",
    "https://openalex.org/W4313470093",
    "https://openalex.org/W4210398322",
    "https://openalex.org/W4309314502",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2972734857",
    "https://openalex.org/W3045464143",
    "https://openalex.org/W3153208071"
  ],
  "abstract": null,
  "full_text": " \nMultilingual transformer and BERTopic for short text \ntopic modeling: The case of Serbian \nDarija Medvecki1 [0000-0002-4180-0050], Bojana Bašaragin1 [0000-0002-7679-1676], Adela Ljajić1 \n[0000-0001-7326-059X] and Nikola Milošević1 [0000-0003-2706-9676]  \n1 The Institute for Artificial Intelligence Research and Development of Serbia, Fruškogorska 1, \n21000 Novi Sad, Serbia \n{darija.medvecki,bojana.basaragin,adela.ljajic,nikola.milosevic}\n@ivi.ac.rs  \n \nAbstract. This paper presents the results of the first application of BERTopic, a \nstate-of-the-art topic modeling technique, to short text written in a \nmorphologically rich language. We applied BERTopic with three multilingual \nembedding models on two levels of text preprocessing (partial and full) to \nevaluate its performance on partially preprocessed short text in Serbian. We also \ncompared it to LDA and NMF on fully preprocessed text. The experiments were \nconducted on a dataset of tweets expressing hesitancy toward COVID -19 \nvaccination. Our results show that with adequate parameter s etting, BERTopic \ncan yield informative topics even when applied to partially preprocessed short \ntext. When the same parameters are applied in both preprocessing scenarios, the \nperformance drop on partially preprocessed text is minimal. Compared to LDA \nand NMF, judging by the keywords, BERTopic offers more informative topics \nand gives novel insights when the number of topics is not limited. The findings \nof this paper can be significant for researchers working with other \nmorphologically rich low-resource languages and short text.  \nKeywords: BERTopic, Topic Modeling, Serbian Language, Natural Language \nProcessing. \n1 Introduction \nAs an unsupervised task, topic modeling is an invaluable tool in many areas, especially \nwhere user-generated content (emails, user comments, reviews, complaints, etc.) needs \nto be analyzed without prior annotation. While there is significant work done in t his \narea for English, especially using Latent Dirichlet Allocation (LDA)  [1], a classical \ntopic modeling method, this is an unexplored area for Serbian. The authors in [2] \ninitiated this work by applying LDA and Nonnegative Matrix Factorization (NMF) [3] \non tweets in Serbian to find the hidden reasons for COVID-19 vaccine hesitancy. \nRecently, BERTopic  [4] has been proposed as a new, more flexible model for \ndetecting topics in unannotated text. Unlike more conventional methods that use bag -\nof-words approaches to describe documents, BERTopic uses state -of-the-art pre -\n2 \n \ntrained language models to create document embeddings, which enables capturing \nsemantic relationships between words. As this framework relies on context, by \ndefinition, it should not require substantial data preprocessing. In contrast, both LDA \nand NMF require extensive preprocessing and significan t parameter tuning. Some \npreliminary research showed that BERTopic generalizes better than LDA judging by \ntopic coherence [5, 6] and topic diversity scores [6], and that it creates more clear -cut \ntopics and gives more novel insights compared to NMF and LDA [7].    \nSince BERTopic has not been applied to Serbian yet, our aim was to explore its \nusability and performance, particularly on short minimally pre processed text. As a \nmorphologically rich language, Serbian normally requires the lemmatization step for \nmost NLP tasks. Using pre -trained language models as embedding models for \nBERTopic could render this step unnecessary. Our results can serve as pointer s for \nother researchers working with short text and morphologically rich languages. \n2 Related work  \nSeveral methods can provide insight into structures hidden in large amounts of text by \ngrouping them into topics.  Two of the most used traditional topic modeling methods \nare Latent Dirichlet Allocation (LDA) [1] and Nonnegative Matrix Factorization \n(NMF) [3]. As language -agnostic models, both  have been applied in the context of \ndifferent languages and texts of various lengths. The downside to these models, often \nmentioned in research paper s, can be summarized into three points. First, they have \ndifficulties modeling short text due to the data sparseness problem  [8]. Second, LDA \nand NMF both require the number of topics as one of their initial parameters. Finding \nthis number requires extensive parameter tuning, making the models difficult to \noptimize [9]. Third, both models require significant preprocessing, including stemming \nand/or lemmatization, which can produce unreliable and ambiguous results, depending \non the language and the quality of the algorithms used [10].       \nThe rise of self -attention-based models and the concept of pre -training in the late \n2010s and early 2020s gave rise to a number of pre-trained language models (PLMs). \nPLMs made significant advances in many fields of natural language processing by \nintroducing pre-trained contextual embeddings. BERTopic [4] is the most recent topic \nmodeling method that leverages PLMs to create document embeddings. Combining \nsuch embeddings with a class-based TF-IDF procedure allows BERTopic to better deal \nwith sparse data. \nSo far, BERTopic has been used in diverse domains (hospitality, sports management, \nfinance, and medicine) to gain insight into different types of text (customer reviews, \nstudents’ answers, consumer, and general domain data), mostly in English  [11–14]. \nUnlike LDA or NMF, apply ing BERTopic to other languages requires language -\nspecific sentence embeddings , an issue that can be overcome by leveraging \nmonolingual or multilingual PLMs that fit the specific use case. In a pilot study by [5], \nBERTopic was tested on news texts using several mono- and multilingual PLMs trained \non Arabic data . Authors of [15] applied BERTopic that employs AraBERT PLM to \ntweets as one of the steps in designing a cognitive distortion classification model. \n3 \n \nThanks to its ability to integrate multilingual PLMs, researchers in [16] used BERTopic \nto make a hoax news classification pipeline for Indonesian.  \nWhen applying BERTopic to news and tweets along with NMF and LDA, authors \nin [4] reported high topic coherence scores across all datasets, with the highest ones on \nslightly preprocessed text of tweets. Reseachers in [6] compared the performance of \nLDA and BERTopic with two clustering algorithms (HD BSCAN and k-means) on \nstudent comments and news and found that BERTopic using HDBSCAN achieved the \nhighest topic coherence and topic diversity scores. Authors in [5] found that BERTopic \nwith AraVec2.0  as a  word embedding model outperformed NMF, LDA and other \nBERTopic embedding models in terms of NPMI topic coherence scores.  \nThe results achieved by BERTopic so far suggest that it is a powerful model able to \novercome the difficulties of more traditional options.   \n3 Methodology \nWe started the process of exploring BERTopic by defining the research questions. After \napplying several architecture variants, we specified the architecture that would best fit \nour needs and the dataset. Since the first study of using topic modeling on Serbian short \ntext was performed on tweets, we used the same dataset as an opportunity to compare \nBERTopic against LDA and NMF on the same data.   \n3.1 Research questions  \nThere were two research questions we wanted to address:  \n \nRQ 1: How does preprocessing affect the quality of BERTopic topic \nrepresentations in case of a morphologically rich language?  \nSince BERTopic relies on an embedding approach which takes context into account, in \ntheory, it should provide informative results even without significantly changing the \ntext structure first. In the case of tweets, researchers in [4] and [7] claim that employing \nminimal preprocessing is sufficient for English. Proving that BERTopic can overcome \nthe need for more thorough preprocessing  (lemmatization) in morphologically rich \nlanguages would make topic modeling for Serbian a more straightforward task as it \nwould: 1) assume minimum human involvement in text preparation, 2) prevent relying \non restoration of diacritics and lemmatization, which can possibly create faulty lemmas \nand change the structure of topics. \n \nRQ 2: How does BERTopic compare to LDA and NMF on lemmatized text? \nAlthough [2] found some differences in topic quality and stability when using LDA and \nNMF, their performance was quite similar. We wanted to explore if, under the same \nconditions, BERTopic would give more informative topics in terms of keywords , and \noffer some new insights.   \n4 \n \n3.2 The model  \nBERTopic has a modular architecture made up of several core layers that can be built \nupon. Those layers are sequential and include embedding extraction, dimensionality \nreduction and clustering, and creation of topic representation. We discuss the details of \nour architecture (Fig. 1) in the following subsections. \n \nFig. 1. Workflow and details of our BERTopic architecture. \nEmbedding models . Although BERTopic supports the implementation of several \ndifferent embedding techniques, by default it uses sentence transformers [17]. They are \noften optimized for semantic similarity , which can significantly help in the clustering \ntask. Since there is no sentence transformer trained solely on Serbian, we could either \ntry to optimize a word  embedding model for Serbian or use one of the available \nmultilingual sentence  transformers that were trained on multilingual data, including  \nSerbian. In all our experiments, we used a sentence transformer architecture. The three \nmultilingual sentence transformer models that are trained on parallel dat a for 50+ \nlanguages including Serbian are: \n• distiluse-base-multilingual-cased-v2: knowledge distilled version of multilingual \nUniversal Sentence Encoder that encodes the sentences into 512-dimensional dense \nvector space. Its size is 480 MB (135 million parameters).  \n• paraphrase-multilingual-MiniLM-L12-v2: multilingual version of paraphrase -\nMiniLM-L12-v2 that maps sentences to a 384-dimensional dense vector space. The \nsize of this model is 420 MB (117 million parameters).  \n• paraphrase-multilingual-mpnet-base-v2: the largest model with the size of 970 MB \n(278 million parameters). It is a multilingual version of paraphrase-mpnet-base-v2 \nthat maps sentences to a 768-dimensional dense vector space.  \nDimensionality reduction and clustering . To reduce the embedding dimensionality, \nwe used UMAP, a default BERTopic dimensionality reduction algorithm . We formed \nthe clusters using HDBSCAN with default parameters. \nCreation of  topic representation . CountVectorizer, as the default BERTopic \nvectorizer model, and c -TF-IDF, which models the importance of words in clusters \ninstead of individual documents, are together responsible for extracting topic \nrepresentations from the previously created clusters of  documents. We used \n\n5 \n \nCountVectorizer to define word filtering options: stop words, filtering of the words that \nappear in less than 3 tweets and in more than 85% of the tweets. \nOutlier reduction. When used with HDBSCAN, BERTopic creates a bin for topic \noutliers, which can sometimes contain over 74% of the dataset [6]. To prevent this, the \noutlier reduction step can optionally be added on top of the BERTopic architecture. We \nreduced the outliers to almost 0 using the reduce_outliers BERTopic function with c-\nTF-IDF as the reduction strategy. \n3.3 The data  \nTo fit the model, we used the dataset created for the study presented in [2]. The dataset \nis composed of 3,286 tweets that express negative attitudes towards COVID -19 \nvaccination in Serbia. The authors report that the dataset was manually checked for \ntopics, therefore we knew that it contains 15 broad topics. We applied two \npreprocessing scenarios to the tweets:  \n1. Partial preprocessing, which consisted of transliterating from Cyrillic to Latin, \nremoval of links, mentions and emojis, conversion of hashtags into words, removal \nof numbers and punctuation, and lowercasing. \n2. Full preprocessing, which consisted of partial preprocessing and lemmatization.  \nTransliteration was performed using srtools [18], lemmatization was performed \nusing the classla library for non -standard Serbian [19], and we used custom Python \nregex for the remaining preprocessing steps. \n3.4 Evaluation \nWe performed quantitative evaluation of our models using two metrics – topic \ncoherence (TC) and topic diversity (TD) – both commonly used to evaluate topic \nmodels [4, 6, 20] . According to [21], TC represents average semantic relatedness \nbetween topic words. The specific flavor of TC we used was NPMI [22]. NPMI ranges \nfrom -1 to 1, where a higher score signifies that words in a topic are more strongly \nrelated. TD [20] measures the percentage of unique words in the top-n words across the \ntopics. It ranges from 0 to 1, where 1 signifies more varied and 0 indicates redundant \ntopics. \nFor RQ1, we evaluated all three BERTopic embedding models in two preprocessing \nsettings – partial preprocessing and full preprocessing, as defined in Section 3.3. For \nRQ2, we compared TC and TD between BERTopic models with LDA and NMF. Since \nboth LDA and NMF require fully preprocessed text, we used that preprocessing \nscenario for all the models. Researchers in [2] compare LDA and NMF using the \nvocabulary reduction to 1,000 words, so we set the same parameter in BERtopic models \n(see Fig. 1) for the sake of comparability. In both experiments, we averaged TC and \nTD across 3 runs for 10-50 topics with steps of 10 for every model. \nBesides using TC and TD as quantitative measures, we also manually checked the \ntopics for keyword diversity, overall interpretability, and novelty. To obtain repeatable \n6 \n \nresults, we set the UMAP random state to 42. For both research questions, we compare \nthe topics and keywords of best performing models after quantitative evaluation. By \ndefault, BERTopic does not put any limitations on the number of topics, which can \nresult in hundreds of topics for larger datasets [7]. We predefined this number in both \nRQ for the sake of comparability. For RQ1, we set the number of topics to 15 for both \nmodels to match the number of topics manually identified by [2]. For RQ2, we matched \nthe most optimal number of topics of the best performing traditional topic model . We \nalso reduced the vocabulary to 1,000 words for the same reason. \n4 Results & Discussion  \n4.1 RQ1  \nQuantitative evaluation . For partially preprocessed text , the third and the largest \nmodel gave the best TC ( -.133) and TD (.896) scores  (see Table 1). The other two \nembedding models share the TC score of -.145. The second-best TD score was achieved \nby paraphrase-multilingual-MiniLM-L12-v2. For fully preprocessed text, distiluse-\nbase-multilingual-cased-v2 had the best results for TC and it shares the best TD score \nwith our second model. All three models achieve high TD values in both preprocessing \nscenarios, suggesting diverse keywords regardless of the model. TD scores are slightly \nhigher for partially preprocessed text, with paraphrase-multilingual-mpnet-base-v2 \nachieving the highest one. On the other hand, TC scores are slightly better for all three \nmodels in the case of fully preprocessed text, suggesting more coherent topics. \nTable 1. Topic coherence (TC) and topic diversity (TD) for different BERTopic embedding \nmodels and two preprocessing scenarios. \nBERTopic embedding model Partial preprocessing Full preprocessing \n   TC TD TC  TD    \ndistiluse-base-multilingual-cased-v2 -.145 .887 -.042 .868 \nparaphrase-multilingual-MiniLM-L12-v2  -.145 .895 -.063 .868 \nparaphrase-multilingual-mpnet-base-v2 -.133 .896 -.058 .860 \n \nQualitative evaluation. We compared the models with the highest TC and TD scores \nper preprocessing scenario  during the quantitative evaluation : paraphrase-\nmultilingual-mpnet-base-v2 for partially preprocessed text and distiluse-base-\nmultilingual-cased-v2 for fully preprocessed text. To start, we paired together the topics \nyielded by the two models based on ke ywords. In Table 2 we show five illustrative \ntopics that cover over 60% of documents in each scenario. By looking at 10 \nrepresentative keywords, we can see that even without lemmatization we obtained \ninformative topics. \nAs for keyword diversity and interpretability, except for several morphological \nvariations of three nouns and a verb in the keywords of the first model (underlined), the \nwords are varied for both models. The bolded keywords are the ones that clearly point \nto the interpretation of each  topic. For example, the fourth topic shows less keyword \n7 \n \ndiversity for the partially preprocessed text since there are four different morphological \nforms of the same word ( dete – child) in the top 10 keywords. Despite this, the \nkeywords are still informative and indicate the concern over the side effects of \nmandatory vaccination for children.  While keywords in this topic for fully processed \ntext may seem more varie d, the number of tweets and inspection of representative \ndocuments prove that this variety stems from several distinct topics merged into one . \nThe same can be noticed in the first topic, but this time for partially preprocessed text. \nJudging by the results, it seems that parameter s need to be separately defined for \ndifferent levels of preprocessing. Applying the same parameters  to both scenarios  \naffects the keywords, which is reflected in the number of documents as well. However, \neven under these conditions, the keywords are diverse and informative for both \npreprocessing scenarios , indicating that BERTopic can be successfully applied to \npartially preprocessed text in Serbian.  \nTable 2. Overview of ten keywords and the number of tweets per topic for five illustrative topics \nobtained by paraphrase-multilingual-mpnet-base-v2 and distiluse-base-multilingual-cased-v2.  \nparaphrase-multilingual-mpnet-base-v2  distiluse-base-multilingual-cased-v2  \nKeywords No. of \ntweets \nKeywords No. of \ntweets \neksperiment (experiment), \nnuspojave (side effects), zna \n(knows), sto (hundred), dnk (DNA), \nmrna (mRNA), godina (year), dr (dr), \nce (will), bil (Bill)  \n1097 \neksperimentalan (experimental), \nispitivanje (examining), faza \n(phase), lek (drug), medicinski \n(medical), nijedan (none), proći \n(pass), struka (experts), nuspojava \n(side effect), proizvođač \n(producer)  \n215 \nimunitet (immunity), zaštita \n(protection), simptomi (symptoms), \nkoronu (corona), imam (I have), štiti \n(protects), korone (corona), \nvakcinaciju (vaccination), antitela \n(antibodies), prirodni (natural)  \n451 \nvirus (virus), imunitet \n(immunity), soj (strain), simptom \n(symptom), grip (flu), štititi \n(protect), napraviti (to make), hiv \n(HIV), prirodan (natural), \npreležati (to develop immunity)  \n606 \nsmrti (death), slučajeva (cases), \numrli (died), godine (year), umrlo \n(died), broj (number), smrtnih (death, \nadj), smrt (death), miliona (million), \numrlih (dead)  \n212 \numreti (to die), broj (number), \nbolnica (hospital), umirati (die), \nslučaj (case), ubiti (to kill), \nsmrtan (deadly), zaraziti (to \ninfect), respirator (respirator), tri \n(three)  \n263 \ndecu (children), deca (children), dece \n(children), štete (damage), pravo \n(right), deci (children), vakciniše \n(vaccinates), svoju (one’s own), \nvakcinacije (vaccination), vakcinom \n(vaccine)  \n153 \ndete (child), gejts (Gates), bil \n(Bill), misliti (think), dnk (DNA), \nnuspojava (side effect), srbija \n(Serbia), posledica (consequence), \nnarod (people), menjati (change)  \n1035 \nmaske (masks), štite (protect), maska \n(mask), nose (wear), distanca \n(distance), mere (measures), nosi \n(wears), štete (damage), ratom (war), \nsvjet (world)  \n94 \nmaska (mask), nositi (to wear), \ndistanca (distance), štititi (to \nprotect), dobro (well), mera \n(measure), odbiti (to reject), pasoš \n144 \n\n8 \n \n(passport), naravno (of course), \nruka (hand)  \n4.2 RQ2  \nQuantitative evaluation. The results in Table 3 show that differences in TD and TC \nscores between BERTopic models are slight, with distiluse-base-multilingual-cased-v2 \nperforming best, as in the case of RQ1.  Compared to LDA and NMF, BERTopic \nachieved better TC scores. NMF performed slightly worse ( -.065 compared to the \nlowest BERTopic score of -.054), while LDA showed a more significant drop ( -\n.104). Even though our TD scores were high for all the three BERTopic models for \nSerbian dataset, LDA achieved a slightly better result (.897). The only model with a \nTD score lower than .8 was NMF. Comparing RQ2 results to the TC and TD scores for \nthe RQ1 (Table 1), it can be concluded that vocabulary reduction only slightly \ninfluenced these metrics in case of fully preprocessed text. \nTable 3. Comparison of TC and TD values across three BERTopic embedding models, LDA \nand NMF on fully preprocessed text and with vocabulary reduced to 1,000 words. \nModel   TC TD \ndistiluse-base-multilingual-cased-v2   -.050 .861 \nparaphrase-multilingual-MiniLM-L12-v2   -.054 .859 \nparaphrase-multilingual-mpnet-base-v2   -.051 .858 \nLDA -.104 .897 \nNMF    -.065 .795 \n  \nQualitative evaluation. We extracted the topics generated by the best performing \nBERTopic model during RQ2 quantitative evaluation, which is distiluse-base-\nmultilingual-cased-v2. We set the number of topics for BERTopic  to 14 to match the \nmost optimal number of topics for LDA for this dataset [2], since LDA showed the best \nTD score during quantitative evaluation. \nAuthors in [2] gather all the LDA and NMF topics into five groups and 16 subgroups \nthat we used to name and align BERTopic topics ( Table 4). When defining the topic \nnames, [2] looked at 20 keywords and representative documents, which we did as well. \nTopics detected by BERTopic match the ones in [2] in 69% of cases, meaning that 31% \nof the topics found by LDA and NMF were not detected by BERTopic. BERTopic did \nnot isolate any topics that deal with the number of doses, which were the topics detected \nby both LDA and NMF. One BERTopic topic contains several topics in one, combining \na conspiracy theory of DNA change with concern over not having a choice with \nvaccinating children and doubt about effectiveness for new strains, similarly as in RQ1. \nIn the case of the topic dealing with mistrust of authoritie s, BERTopic breaks it into a \ntotal of eight topics, each covering a specific aspect of mistrust. One  of them is \na completely novel topic that groups tweets regarding concerns about different vaccine \nmanufacturers (84 tweets). While some topics seem uninterpretable by looking at the \nkeywords (e.g. , third topic under Mistrust of government and political decision \nmakers), there is a clear idea of the topic based on the keywords for most topics. \n9 \n \nWith the same parameter settings, but without limiting the number of topics, \nBERTopic found 41 topics for this dataset. By closer inspection of these topic \nrepresentations, BERTopic detected all reasons identified by [2] as separate topics. \nAlthough this number of topics is not appropriate for this dataset, this approach could \nbe an important starting point for further analysis and parameter optimization, as it \ncould provide more detailed or new insights into topics hidde n in the dataset. In some \ncases, identifying smaller topics may be very important in some fields.   \nAnother important BERTopic parameter that significantly affects the number of \ngenerated topics is the minimum topic size ( min_topic_size), which is the minimum \nnumber of documents to form a topic. The default parameter value in BERTopic, which \nwe used in our experiment, is 10. Increasing this value results in a lower number of \nclusters/topics when HDBSCAN is used as the clustering algorithm. When we set this \nparameter to 15, BERTopic generated 23 topics without any outliers.   \nTo answer RQ2, BERTopic’s performance is comparable to LDA and NMF on this \ndataset, but not under the same parameter settings.  When the number of topics is not \nlimited and when the minimum topic size parameter value is changed, BERTopic could \npotentially provide new insights. \nTable 4. LDA and NMF topics for the COVID-19 dataset (taken from [2]) and the \ncorresponding BERTopic topics with the number of tweets. \nReasons for vaccine hesitancy \nidentified by LDA and NMF BERTopic topic representations  No. of \ntweets  \nConcern over general side \neffects  \numreti (to die), smrt (death), nuspojava (side effect), \numirati (to be dying), bolnica (hospital), slučaj (case), \nživot (life), smrtan (deadly), tri (three), nevakcinisan \n(unvaccinated)  \n427  \nConcern over side effects for \nchildren   \ndete (child), gejts (Gates), bil (Bill), nov (nov), dnk \n(DNA), soj (strain), bolest (illness), menjati (change), \npriča (story), sloboda (freedom)  \n962  \nConcern over side effects due \nto many required doses   -  -  \nConcern over vaccine \neffectiveness:  \nnatural immunity is \nbetter protection  \nvirus (virus), imunitet (immunity), simptom \n(symptom), hiv (HIV), otrov (poison), prirodan \n(natural), napraviti (make), štititi (protect), zaraziti \n(infect), bolest (illness)   \n517  \nConcern over vaccine \neffectiveness: vaccines are not \neffective against new COVID-\n19 strains  \ndete (child), gejts (Gates), bil (Bill), nov (nov), dnk \n(DNA), soj (strain), bolest (illness), menjati (change), \npriča (story), sloboda (freedom)   \n962  \nConcern over vaccine \neffectiveness: vaccines are not \neffective since so many doses \nare required  \n-  -  \nConcern over side effects of \ninsufficiently tested vaccines  -  -  \nConcern over effectiveness of \ninsufficiently tested vaccines  \neksperiment (experiment), eksperimentalan \n(experimental), ispitivanje (examining), faza (phase), \ntestiranje (testing), vršiti (perform), medicinski \n(medical), trajati (last), pravo (right), nijedan (none)   \n241  \n\n10 \n \nViolation of freedom by \nimposing the use of \ninsufficiently tested vaccines   \ndete (child), gejts (Gates), bil (Bill), nov (nov), dnk \n(DNA), soj (strain), bolest (illness), menjati (change), \npriča (story), sloboda (freedom)  \n962  \nMistrust of medical experts \nand institutions  \nnauka (science), bog (god), naučan (scientific), vera \n(faith), dokazati (prove), laž (lie), dokaz (proof), \nverovanje (belief), reč (word), govoriti (speak)   \n209  \ndr (dr), doktor (doctor), lekar (doctor), medicina \n(medicine), medicinski (medical), antivakser (anti-\nvaxxer), nauka (science), mrn (mRN), efikasnost \n(effectiveness), crn (black)   \n240  \nmaska (mask), nositi (wear), distanca (distance), štititi \n(protect), odbiti (refuse), mera (measure), značiti (to \nmean), naravno (of course), virus (virus), daleko (far)   \n138  \nMesec (month), javan (public), zdrav (healthy), zakon \n(law), radnik (worker), javno (public), vlada \n(government), pitati (ask), doneti (bring), zdravstven \n(health, adj)   \n44  \nMistrust of government and \npolitical decision makers   \nkineski (Chinese, adj), ruski (Russian, adj), kinez \n(Chinese), eksperiment (experiment), \nsinopharm (Sinopharm), brat (brother), \nastrazeneca (AstraZeneca), član (member), predsednik \n(president), testirati (test)  \n84  \nmilion (million), epidemija (epidemics), milijarda \n(billion), svinjski (swine), država (state), režim \n(regime), suzbiti (supress), kupiti (buy), hiljada \n(thousand), isplatiti (pay off)   \n101  \nfašist (fascist), otrovan (poisonous), kreten (jerk), \nglobus (globus), tv (TV), fašizam (fascism), rnk \n(RNA), kolonija (colony), zombirati (to zombie), glup \n(stupid)   \n95  \npasoš (passpost), ukinuti (to cancel), smisao \n(meaning), rio (Rio [Tinto]), obavezan (obligatory), \nmera (measure), ostati (to stay), glupost (stupidity), \nnuditi (to offer), zdravlje (health)   \n54  \nVaccines are a money-\nmaking scheme   -  -  \nVaccines, especially mRNA \nvaccines, change DNA   \ndete (child), gejts (Gates), bil (Bill), nov (nov), dnk \n(DNA), soj (strain), bolest (illness), menjati (change), \npriča (story), sloboda (freedom)  \n962  \nCOVID-19 does not exist; \nthus, vaccines are \nunnecessary   \n-  -  \nVaccines are a means of \npopulation reduction \nand control   \nčekati (wait), nadati (to hope), panika (panick), \nočekivati (expect), nov (new), red (line), zaraditi \n(earn), mera (measure), depopulacija (depopulation), \nstrah (fear) \n64  \nVaccines are an instrument of \nworld powers and \ntheir agenda   \nnemački (German), rat (war), rus \n(Russian), ukrajina (Ukraine), ruski (Russian), rusija \n(Russia), promoter (promoter), agenda (agenda), idiot \n(idiot), shvatiti (realize)   \n106  \n\n11 \n \n5 Conclusions & Future work   \nIn this paper, we tested the performance of BERTopic on short text in Serbian. We were \ninterested in whether BERTopic  can yield meaningful topics when applied to \nmorphologically rich slightly processed short text and how well it performs in \ncomparison with LDA and NMF on fully processed text.  To answer the first question, \nwe compared the performance of BERTopic with different embedding models on fully \nand partially preprocessed text and found t hat BERTopic can produce meaningful and \ninformative topics even with slight preprocessing.  In this case, the larger model, the \nbetter the performance. As for the second question, we  concluded that applying the \nsame parameters as the ones used for LDA is not the optimal scenario for BERTopic. \nWhen the number of topics was not limited, BERTopic was able to provide novel \ninsights.   \nThere are several directions we would like to explore in the future. We plan to apply \nBERTopic to different datasets to check if our conclusions can be generalized. We also \nplan to explore its prediction capabilities on new, unseen documents. Since BERTopic \nsupports using different transformer models as the embedding models, we also plan to \ntest the applicability and performance of the currently only language model trained on \nSerbian data ‒ BERTić [23]. \nReferences \n1. Blei, D.M., Ng, Andrew Y, Jordan, Michael I: Latent Dirichlet Allocation. Journal of \nmachine Learning research. 3, 993 –1022 (2003). https://doi.org/10.1162/jmlr.2003.3.4 -\n5.993 \n2. Ljajić, A., Prodanović, N., Medvecki, D., Bašaragin, B., Mitrović, J.: Uncovering the \nReasons Behind COVID -19 Vaccine Hesitancy in Serbia: Sentiment -Based Topic \nModeling. J Med Internet Res. 24, e42261 (2022). https://doi.org/10.2196/42261 \n3. Févotte, C., Idier, J.: Algorithms for nonnegative matrix factorization with the beta -\ndivergence. Neural Computation. 23, 2421 –2456 (2011). \nhttps://doi.org/10.1162/NECO_a_00168 \n4. Grootendorst, M.: BERTopic: Neural topic modeling with a class-based TF-IDF procedure. \n(2022). https://doi.org/10.48550/arXiv.2203.05794 \n5. Abuzayed, A., Al-Khalifa, H.: BERT for Arabic Topic Modeling: An Experimental Study \non BERTopic Technique. Procedia Computer Science. 189, 191 –194 (2021). \nhttps://doi.org/10.1016/j.procs.2021.05.096 \n6. de Groot, M., Aliannejadi, M., Haas, M.R.: Experiments on Generalizability of BERTopic \non Multi-Domain Short Text. (2022). https://arxiv.org/abs/2212.08459 \n7. Egger, R., Yu, J.: A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and \nBERTopic to Demystify Twitter Posts. Front. Sociol. 7, 886498 (2022). \nhttps://doi.org/10.3389/fsoc.2022.886498 \n8. Chen, Y., Zhang, H., Liu, R., Ye, Z., Lin, J.: Experimental explorations on short text topic \nmining between LDA and NMF based Schemes. Knowledge -Based Systems. 163, 1 –13 \n(2019). https://doi.org/10.1016/j.knosys.2018.08.011 \n9. Egger, R., Yu, J.: Identifying hidden semantic structures in Instagram data: a topic modelling \ncomparison. TR. (2021). https://doi.org/10.1108/TR-05-2021-0244 \n12 \n \n10. Chauhan, U., Shah, A.: Topic Modeling Using Latent Dirichlet allocation: A Survey. ACM \nComput. Surv. 54, 1–35 (2022). https://doi.org/10.1145/3462478 \n11. Sánchez‐Franco, M.J., Rey‐Moreno, M.: Do travelers’ reviews depend on the destination? \nAn analysis in coastal and urban peer‐to‐peer lodgings. Psychology and Marketing. 39, 441–\n459 (2022). https://doi.org/10.1002/mar.21608 \n12. Bulut, O., MacIntosh, A., Walsh, C.: Using Lbl2Vec and BERTopic for Semi -Supervised \nDetec-tion of Professionalism Aspects in a Constructed -Response Situational Judgment \nTest. (2022). https://doi.org/10.31234/osf.io/n5fqe \n13. 13.  Sangaraju, V.R., Bolla, B.K., Nayak, D.K., Kh, J.: Topic Modelling on Consumer \nFinancial Protection Bureau Data: An Approach Using BERT Based Embeddings. (2022). \nhttps://arxiv.org/abs/2205.07259 \n14. 14.  Sánchez-Franco, M. J., González Serrano, M . H., dos Santos, M . A., Moreno, F . C.: \nModelling the structure of the sports management research field using the BERTopic \napproach. In: Retos: nuevas tendencias en educación física, deporte y recreación. pp. 648 –\n663 (2023) \n15. 15.  Alhaj, F., Al -Haj, A., Sharieh, A., Jabri, R.: Improving Arabic Cognitive Distortion \nClassification in Twitter using BERTopic. IJACSA. 13, (2022). \nhttps://doi.org/10.14569/IJACSA.2022.0130199 \n16. 16.  Hutama, L.B., Suhartono, D.: Indonesian Hoax News Classification with Multilingual \nTransformer Model and BERTopic. IJCAI. 46, (2022). \nhttps://doi.org/10.31449/inf.v46i8.4336 \n17. 17.  Reimers, N., Gurevych, I.: Sentence -BERT: Sentence Embeddings using Siamese \nBERT-Networks. (2019). https://doi.org/10.48550/arXiv.1908.10084 \n18. 18.  Radović, A.: Srtools, https://pypi.org/project/srtools/, (2021) \n19. 19.  Ljubešić, N., Štefanec, V .: The CLASSLA -StanfordNLP model for lemmatisation of \nnon-standard Serbian 1.1. Slovenian language resource repository CLARIN.SI. (2020). \nhttps://doi.org/10.18653/v1/W19-3704 \n20. 20.  Dieng, A.B., Ruiz, F.J.R., Blei, D.M.: Topic Modeling in Embedding Spaces. \nTransactions of the Association for Computational Linguistics. 8, 439 –453 (2020). \nhttps://doi.org/10.1162/tacl_a_00325 \n21. 21.  Newman, D ., Lau, J . H., Grieser, K ., Baldwin, T .: Automatic Evaluation of Topic \nCoherence. In: HLT ’10: Human Language Technologies: The 2010 Annual Conference of \nthe North American Chapter of the Association for Computational Linguistics. pp. 100–108. \nAssociation for Computational Linguistics (2010) \n22. 22.  Bouma, G.: Normalized (pointwise) mutual information in collocation extraction. In: \nProceedings of GSCL. pp. 31–40 (2009) \n23. 23.  Ljubešić, N., Lauc, D.: BERTić - The Transformer Language Model for Bosnian, \nCroatian, Montenegrin and Serbian. (2021). https://doi.org/10.48550/arXiv.2104.09243 \n ",
  "topic": "Serbian",
  "concepts": [
    {
      "name": "Serbian",
      "score": 0.8809254169464111
    },
    {
      "name": "Transformer",
      "score": 0.5915854573249817
    },
    {
      "name": "Computer science",
      "score": 0.503894031047821
    },
    {
      "name": "Linguistics",
      "score": 0.44926947355270386
    },
    {
      "name": "Natural language processing",
      "score": 0.4000338912010193
    },
    {
      "name": "Engineering",
      "score": 0.17775315046310425
    },
    {
      "name": "Electrical engineering",
      "score": 0.1438950002193451
    },
    {
      "name": "Philosophy",
      "score": 0.09138184785842896
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}