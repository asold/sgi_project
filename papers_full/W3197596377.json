{
  "title": "Visual Transformers for Primates Classification and Covid Detection",
  "url": "https://openalex.org/W3197596377",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5069846773",
      "name": "Steffen Illium",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A5052914309",
      "name": "Robert Müller",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A5024006892",
      "name": "Andreas Sedlmeier",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A5069249577",
      "name": "Claudia-Linnhoff Popien",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134945014",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2529337537",
    "https://openalex.org/W1972567154",
    "https://openalex.org/W3015995734",
    "https://openalex.org/W3035378948",
    "https://openalex.org/W114730584",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2526050071",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3097426020",
    "https://openalex.org/W2405274704",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W197865394",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3198909755",
    "https://openalex.org/W2059652044",
    "https://openalex.org/W2914166397"
  ],
  "abstract": "We apply the vision transformer, a deep machine learning model build around the attention mechanism, on mel-spectrogram representations of raw audio recordings. When adding mel-based data augmentation techniques and sample-weighting, we achieve comparable performance on both (PRS and CCS challenge) tasks of ComParE21, outperforming most single model baselines. We further introduce overlapping vertical patching and evaluate the influence of parameter configurations. Index Terms: audio classification, attention, mel-spectrogram, unbalanced data-sets, computational paralinguistics",
  "full_text": "Visual Transformers for Primates Classiﬁcation and Covid Detection\nSteffen Illium, Robert M¨uller, Andreas Sedlmeier and Claudia-Linnhoff Popien\nMobile and Distributed Systems Group,\nLMU Munich\n{steffen.illium,robert.mueller,andreas.sedlmeier,linnhoff}@ifi.lmu.de\nAbstract\nWe apply the vision transformer, a deep machine learning model\nbuild around the attention mechanism, on mel-spectrogram\nrepresentations of raw audio recordings. When adding mel-\nbased data augmentation techniques and sample-weighting, we\nachieve comparable performance on both (PRS and CCS chal-\nlenge) tasks of ComParE21, outperforming most single model\nbaselines. We further introduce overlapping vertical patching\nand evaluate the inﬂuence of parameter conﬁgurations.\nIndex Terms: audio classiﬁcation, attention, mel-spectrogram,\nunbalanced data-sets, computational paralinguistics\n1. Introduction\nAs raw audio can meaningful be represented by mel-\nspectrograms, the ﬁeld of audio classiﬁcation has seen a lot of\ninﬂuences from the ﬁeld of computer vision. Such algorithm\nwork mostly out of the box (once a suitable hyper-parameter\ncombination is found) and build a competitive baseline for\nmany problems. Currently the most used architecture from the\nrange of deep-learning algorithms is the convolutional neural\nnetwork (CNN) [1]. When additional written text is provided\nalongside raw audio, algorithms from the ﬁeld of natural lan-\nguage processing (NLP) take over. Resulting embeddings from\nboth types of algorithms can subsequently be combined before\nentering a joint classiﬁer. Most recently, transformers, employ-\ning deep-learning attention mechanism, are taking over the ﬁeld\nof NLP. The VisionTransformer (ViT) [2] recently fused both\nof the ﬁelds by interpreting images as sequences of patches and\nthus utilising what can be called dynamic convolution. We re-\nlate to this idea by training the ViT on mel-spectrogram rep-\nresentations (thus single channel images) of raw audio data,\nwhich is a novelty. We further observe the performance impact\nby varying the shape and order of patches to match the natural\ntemporal dependency in spectrograms. Additionally, we incor-\nporated data-augmentation techniques while over-sampling the\ndatasets to overcome limitations in two of the challenge datasets\nof ComParE21 [3].\n2. Dataset and Task Description\nWe focus on two of the provided datasets of ComParE21 [3],\none binary and one multi-class classiﬁcation task:\nPrimates Sub-Challenge (PRS) : Zwerts et al. [4] de-\nscribes the Primate V ocalisations Corpus, which consists of\nnon-invasive acoustic recordings from Central Africa Mefou\nPrimate Sanctuary to identify and count species. This is a multi-\nclass classiﬁcation task including the classes: [’ background’,\n’chimpanze’, ’geunon’, ’mandrille’, ’redcap’], which are either\nprimate species or background noises from the natural primate\nhabitat. The background class is ambiguous as there is the pos-\nsibility of faint primate scream mixing. For further introduc-\ntory explanation please refer to [3, 4]. When working with the\ndataset, we noticed an imbalance across the classes, in the train-\n(Figure 1a) as well as in the provided devel-dataset. The counts\nper sample and class are similar intrain & devel (c.p. Figure 1a),\nbut we noticed slight variations in audio-sample length (number\nof frames, c.p. Figure 1b & 1c). Distributions fortrain and devel\nare comparable while the test dataset has variations in sample\nlength regarding the number of very small ( <= 0.3 seconds).\nClass genuon e.g., not only is underrepresented but also comes\nat small audio sample lengths.\nThe COVID-19 Cough Sub-Challenge (CCS) : Brown et\nal. [5] used an application (Android and Web) to gather (crowd-\nsource) coughs and breathing from negative and positive tested\nhumans COVID-19 (binary classiﬁcation task). For further in-\ntroductory explanation please refer to [3, 5]. This binary dataset\nnot only is imbalanced, also the number of samples is rather\nsmall (215p vs. 71 n, c.p. Figure 1d), especially in respect to\nusual deep learning datasets. Please notice that samples are\nﬁve times as long as compared to PRS-challenge (up to 16 min-\nutes). When comparing the sample length distribution (c.p. Fig-\nure 1e & 1f) we notice slight differences in comparison ofdevel\nand test.\nScores are measured by the unweighted average re-\ncall (UAR, also known as the Balanced Error Rate (BER) [6]):\nUAR = 0.5 ×\n( TP\nFN + TP + TN\nTN + FP\n)\n3. Related Work\n3.1. Audio Classiﬁcation\nResearch in the ﬁeld of audio classiﬁcation can be categorized\nbased on the respective goal which is to be achieved, e.g., genre\nprediction [7] or classiﬁcation [8] (multi-class classiﬁcation ap-\nproaches), as well as binary classiﬁcation or anomaly detection.\nAs is the case for most research ﬁelds concerned with clas-\nsiﬁcation tasks, progress in recent years was mostly achieved\nusing deep neural networks (DNN) and gradient based learn-\ning techniques. By contrast to earlier research, features are no\nlonger manually constructed (as was the case e.g., for work\nbased on mel-frequency cepstral coefﬁcients (MFCC)) but in-\nstead automatically inferred (e.g. VGG [9]). Consequently,\nthese approaches can be differentiated based on how the in-\nput data is structured: i) direct usage of the mel-spectrograms,\ni.e., the time-frequency representation or ii) end-to-end learning\nfrom the audio’s wave-forms.\nWhen using spectrogram features, it is possible to apply and\nbuild upon network architectures that were conceived to tackle\nproblems related to image data. Research has shown that these\napproaches outperform classical methods relying on manually\nconstructed features [8, 10]. While [7] has shown that it is pos-\nsible to construct end-to-end learning approaches directly using\nthe audio’s wave-form, their method’s performance is worse\narXiv:2212.10093v1  [cs.SD]  20 Dec 2022\nbackground chimpanze geunon mandrille redcap\nlabels\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500count\n3458\n2217\n158\n874\n208\n(a) PRS-train: class counts\n0.5 1.0 1.5 2.0 2.5 3.0\ndurations\n0\n1\n2\n3\n4\n5\n6\n7Density\nlabels\n?\nbackground\nchimpanze\ngeunon\nmandrille\nredcap (b) PRS-train: Durations per class\n0.5 1.0 1.5 2.0 2.5 3.0\ndurations\n0\n1\n2\n3\n4\n5\n6\n7Density\nlabels\n?\nbackground\nchimpanze\ngeunon\nmandrille\nredcap (c) PRS-devel: Durations per class\nnegative positive\nlabels\n0\n50\n100\n150\n200count\n215\n71\n(d) CCS-train: class counts\n2.5 5.0 7.5 10.0 12.5 15.0 17.5\ndurations\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nDensity\nlabels\n?\nnegative\npositive (e) CCS-train: Durations per class\n2.5 5.0 7.5 10.0 12.5 15.0\ndurations\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nDensity\nlabels\n?\nnegative\npositive (f) CCS-devel: Durations per class\nFigure 1: Statistics for ’primates’ and ’CCS’ datasets. 1a and 1d show the counts per class for each training set, 1b, 1c 1e & 1f show\nthe distribution of ﬁle-durations per class. For comparison, the test set duration (label: ?) is also shown.\ncompared to spectrogram-based approaches. [11] was ﬁnally\nable to match the performance of CNN based architectures us-\ning log-mel spectrograms by applying very deep networks.\nOnly recently, it has been shown that transformer architec-\ntures [12] can be a competitive approach to CNNs in the im-\nage domain. Initially developed in natural language processing\n(NLP), the transformer architecture aimed to improve on com-\nplex sequence models like recurrent convolutional neural net-\nworks. Still, the transformer follows the encoder-decoder struc-\nture as is common for state-of-the art sequence models based\naround the attention mechanism by mapping a sequence of input\nfeatures x to a sequence of representations z. The so-called Vi-\nsion Transformer (ViT) [2] showed that the architecture is able\nto achieve competitive results to convolutional neural networks\nby interpreting an image as a sequence of patches.\nIn the work at hand, we analyze the performance of such a\nViT architecture when applied to audio data by using the spec-\ntrogram representation of the data as input features as well as\nthe importance of its hyper-parameters.\n3.2. Audio Data Augmentation\nTo improve results in audio classiﬁcation tasks, data augmen-\ntation techniques are commonly applied. One such approach is\nmasking or SpecAugmentation [13], which masks areas (which\ncan be restricted by a maximum size) of certain frequency bands\nand temporal bins by replacing them with random shapes of ze-\nros. When combined with time warping, research has shown\nthat these techniques can reduce over-ﬁtting in training. Re-\nsults in [14] show that these and other approaches like loud-\nness modiﬁcations, time shifting, or the addition of noise im-\nprove classiﬁcation results of CNN based models. In the work\nat hand, we build upon these results by implementing and evalu-\nating augmentation techniques on the given dataset of the Com-\nParE21 [3].\n4. Methods\nBased on the observations from previous section, we decide\nto apply data augmentation as presented in [14] while over-\nsampling the datasets to re-balance the deﬁcits in data distribu-\ntion per class and length. Oversampling was assumed to work\nbest at equally distributed classes, so that the total number of\nsamples ( Stot) Stot = max(len(Sclass)) ×n classes, we\nthen sampled randomly with replacements given the probabil-\nity p = 1/len(Sclass) for each sample and class respectively.\nThis is a critical point. Without equal sampling, no training\nwas possible for both challenges with the algorithm introduced\nin Subsection 4.2. The audio samples were further cropped to\nsmaller length. We let the HP-search ﬁnd the best matching\nsample length for each dataset and algorithm. This results in\nshorter sample length (∼ 0.4) for PRS and longer samples\n(∼ 1.2) for CCS dataset . This chapter now brieﬂy revisits the\ndata-augmentation techniques, then introducing the deep learn-\ning models.\n4.1. Mel-Spectrogram Augmentation\nNamed parameters in this section are treated as model hyper-\nparameter and further tuned by the tree-structured Parzen\nEstimator-algorithm (TPE) hyper-parameter search [15]. We\nsearched a meaningful parameter-space in 400runs per algo-\nrithm (200training-epochs) and dataset. For all augmentation\nevery draw from a random distribution is performed per sample\ndraw.\nShift Augmentation: Temporal offset is provided, to ac-\ncount for positional variations within the the mel-spectrograms.\nData-samples are shifted either left or right ( direction ∼\nBernoulli(0.5)) by shift ∼ U(0, shiftratio) percent of\naxis length in the temporal dimension. Resulting zero-value\ndata-points s are left empty.\nNoise Augmentation: Random Gaussian noise, noise ∼\nN(0, noiseratio), is added ( S + S ×noise) to the mel-\nspectrogram to improve robustness and generalisation capabili-\nLinear \nModules \nConvolution \nModules \n(a) Convolutional Baseline (CNN)\nLinear \nModules \nConvolution \nModules \n (b) SubSpectral Classiﬁer (SSC)\nMulticlass \nClassifier \nConvolution\nModule\nLinear\nModule\nBinary \nClassifier (c) CNN Module description\n*Leading  \n*\n-embedding\n+-embedding\npatching and\ndirections of enrollment\n4 5 6\n1 2 3\n7 8 9\n(d) VisionTransformer (ViT) architecture\n& application\n*Leading  \npatching and\ndirections of enrollment\n-embedding\n+-embedding*\n1 n2 ...\n(e) VerticalVisionTransformer (ViT) ar-\nchitecture & application\nAttention \nModule MLP \nModule (f) Transformer component description\nFigure 2: Model architectures as introduced and described in section 4. Hyper-parameters such as number of neurons or layer depth\nare determined per model and dataset in a broad hyper-parameter search.\nties of end-to-end trained neural networks models. This method\nhas already been successfully applied in tasks of audio classiﬁ-\ncation (where recordings can be noise) and image processing.\nSpecAugment: As presented by [16] we randomly mask\nvertical and horizontal windows (mel- and temporal-axis) with\nzero value (all data-pointssinw). First a random starting point,\ns ∼ U(0, T), is determined. Then we draw the size of the\nwindow by w ∼U(0, maskratio).\nLoudness Augmentation: We adjust the loudness\n(intensity) of recording by a loudness factor. l ∼\nU(0, loudnessratio) determines how much of the original\nsignal is added to the original sample (S + S ×l). Loud signals\nget even louder instead just raising all values (which would be\nnormalised anyway).\n4.2. Model Architectures\nIn our experiments we trained and applied three different ar-\nchitectures in an end to end fashion, by AdamW [17] as op-\ntimiser for gradient based back-propagation. For comparison,\nwe choose a similar parameter range where possible settings\nfor all the models, if not speciﬁed otherwise. Please note the\nvarying size and introduction of additional parameters through\nchanges in network architecture. We further tuned the model\nhyper-parameters as described above by TPE [15].\nFor both tasks (PRS and CCS) we deﬁne default parameters\nas follows, PRS: n logits = 5, loss = ce loss, CCS: n logits =\n1, loss = bce loss. Further model parameters are the same to\nreduce the inﬂuence of a wide hyper-parameter space. Those\nare dropout = 0.2, GELU [18]\nCNNBaseline (CNN) As deep-learning baseline,\nwe implement a quite regular convolutional neural\nnetwork (CNN Figure 2a). Four blocks of batch-\nnormalisation [19], dropout, convolution max-pooling\nand a activation function are stacked. Filter sizes are\n{32, 64, 128, 64}while convolutional-kernel sizes are 3 ×3.\nwith zero-padding = 1 we kept the shape of each activation\nmatrix. Pooling kernel sizes are 2 ×2, cutting the resolution in\nhalf, ﬁnally. The last convolution is followed by a linear module\ncomposed of batch-normalisation, dropout, fully-connected\nlayer (128 neurons) and an activation function. The ﬁnal fully\nconnected classiﬁer is implemented as described above and\ndependent on the task.\nSubSpectralClassiﬁer (SSC) We consider another deep-\nlearning baseline as comparison for the ViT algorithms. The\nSubSpectralClassiﬁer (SSC) is implemented as proposed by\n[14]. In short; four small convolutional neural networks are\ntrained on different non-overlapping of mel-bands n mels =\n128/8, depicted in Figure 2b. Those concatenated band-wise\nembeddings are then processed by a classiﬁer sub-network\n(three fully-connected layers with [128, 64, n] neurons, respec-\ntively). We switched the ReLU activation function in favour of\nthe more recent GELU activation function. Further parameters\nare the same as in [14], this includes batch-normalisation and\nposition and rate of dropout.\nVisionTransformer (ViT) In reference to [2] we imple-\nment the ViT as stack of blocks of multi-head self-attention\n(scaled dot-product attention) followed by double layered mlps\n(GELU activated fully-connected layers with dropout, c.p. Fig-\nure 2d & 2f), both wrapped with residual skip-connections [20].\nMulti-head attention runs multiple self-attention operations in\nparallel, then subsequently fusing the individual embeddings\ninto a single d/n heads sized embedding. This style of ar-\nchitecture was ﬁrst seen in form of the Transformer neural- net-\nwork [21] which relates a single sentence (sequences of word\nembedding) to it self.\nAttention(Q, K, V) =softmax(QKT\n√n )V\nMultiHead(Q, K, V= [head1; ...; headn]WO\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni )\nAn additional linear module is attached before evaluating the re-\nsulting embedding by the subsequent classiﬁer (c.p. Figure 2c).\nVerticalVisionTransformer (VViT) We strictly follow\nthe ViT design described above but realise overlapping full-\nheight (mel-dimension) patches to analyse the mel-spectrogram\nin an more intuitive fashion. Since mel-spectrograms\nposses a temporal relation, we move the kernel ( n mels ×\npatch size, stride = 1 ) horizontally along the temporal\naxis rather then in a rows-then-column pattern (c.p. Fig-\nure 2d vs. 2e). The width of the overlapping patches was empiri-\ncally found to work best between 5 and 9 pixels. This procedure\nresults in more and larger patches (in comparison to the origi-\nnal ViT) while respecting the natural order of the audio/mel-\nspectrogram domain.\n(a) Confusion matrix\n (b) Confusion matrix\n(c) ROC plot\n (d) ROC plot\nFigure 3: Additional Evaluation Metrics for ViT (3a,3c) &\nVViT (3b,3d) on PRS-dataset. For confusion matrices, rows sum\nup to 1. Class assignments are: {0: background, 1: chimpanze,\n2: geunon, 3: mandrille, 4: redcap}\n(a) ViT parameter importance\n (b) VViT parameter importance\nFigure 4: ViT (4a) & VViT (4b) hyper-parameter importance on\nPRS dataset measured by fANOVA[22]\n5. Results\nWe train the described algorithms for 200 epochs on the\nover-sampled and augmented train datasets. No sample from\ndevel or test is touched in training while seeds (0, ...,19)\nare ﬁxed along models and runs. Hyper-parameters are\ntuned by TPE as stated above per network architecture\n(CNN, SSC, ViT & VViT) (c.p.section 4).\nEvaluations based on devel show, that most of the provided\nbaselines of this years classiﬁcation tasks of ComParE 2021\nare mostly out-performed by even the simplest CNN architec-\ntures (CNN & SSC), with OpenXBOW and AUDeep beeing the\nstrongest competitors. Depending on the task, the Transformer-\nstyle approaches (ViT & VVit) then overtake both (ComParE\nand our CNN baselines) by a margin (Figure 5), outperforming\nthe devel dataset performance measured in UAR (c.p., Eq. 2).\nModel performance on test seemed to stagnate in most cases.\nFor PRS we observe high rate of confusion between back-\nground (0) and chimpanze (1) on both attention based mod-\nels (c.p. Figure 3). We noticed the same behaviour for\nBCC CNNB ViT VViT\nModel\n0.7\n0.8\n0.9\nUnweighted Average Recall (UAR)\nDataset\nCCS\nPRMS\nFigure 5: Model performance measured in UAR over 20 seeds\nCNN & SSC. Presumably more chimpanzees are mixed faintly\ninto the background class. We therefore suggest further steps of\ndata-processing. For CCS we noticed a high false negative rate\n(up to 60%) along all models, which is very bad in respect to a\nusage of medical applicability. Usually a high speciﬁcity over a\nhigh sensitivity is required.\nWe further analysed the hyper-parameter importance for\nboth attention based models (ViT & VViT, c.p. Figure 4).\nWhile both models do not care whether a learning rate sched-\nuler (lr scheduler in [None, lr = nepoch, n∼U (0.88, 1)] is\npresent, the initial given lr and total available parameters of\nfully connected layers ( mlp dim & lat dim) are important for\nboth models. Usually transformer models are known to work\nbest, employing a speciﬁc training sequence, including warm-\nup-phases and sophisticated schedulers. In both tasks, we could\nnot conﬁrm these practices.\nOn one hand, we are surprised by the low importance of\nboth embedding size & head dim for both models, as these pa-\nrameters control how much of information can be evaluated at\nevery single layer in parallel (the lower the initial projection di-\nmension, the higher the initial information compression rate).\nOn the other hand, we ﬁnd it interesting to see how the im-\nportance of data augmentation not only depends on the used\ndatasets, but on the model architecture as well. Even when they\ndiffer very little.\nWe further observe a reduced overall performance of VViT\n(at lower variance) which boils down to the assumption, that\nspatial, non overlapping regions are more important for both\ntasks, than a bigger picture, which processes information on all\nmel bands at once.\nIn future we plan to extend our view to more recent models\nfeaturing attention, e.g. the Performer[23] which approximates\nthe result of the regular attention mechanism, without explicitly\nconstructing the quadratic-sized attention matrix.\nDataset primates coughing\nModel Devel Test Devel Test\nDeep Spectrum 81.3 78.8 63.3 64.1\nEnd2You 72.70 70.8 61.8 64.7\nOpenSMILE 82.4 82.2 61.4 65.5\nOpenXBOW 83.3 83.9 64.7 72.9\nAuDeep 84.6 86.1 67.6 67.6\nFusion - 87.5 - 73.9\nours CNN 85.9 - 68.6 67.4\nours SSC 86.6 84.2 66.8 72.0\nours ViT 91.5 88.3 72.0 69.9\nours VViT 90.3 87.2 67.8 68.6\nTable 1: Our experimental results compared to provided base-\nlines from ComParE21 [3].\n6. References\n[1] K. Fukushima and S. Miyake, “Neocognitron: A self-organizing\nneural network model for a mechanism of visual pattern recogni-\ntion,” in Competition and cooperation in neural nets. Springer,\n1982, pp. 267–285.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly et al., “An image is worth 16x16 words: Transformers\nfor image recognition at scale,”arXiv preprint arXiv:2010.11929,\n2020.\n[3] B. W. Schuller, A. Batliner, C. Bergler, C. Mascolo, J. Han,\nI. Lefter, H. Kaya, S. Amiriparian, A. Baird, L. Stappen, S. Ottl,\nM. Gerczuk, P. Tzirakis, C. Brown, J. Chauhan, A. Grammenos,\nA. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, M. R. Leon J.\nJ. Zwerts, J. Treep, and C. Kaandorp, “The INTERSPEECH 2021\nComputational Paralinguistics Challenge: COVID-19 Cough,\nCOVID-19 Speech, Escalation & Primates,” in Proceedings IN-\nTERSPEECH 2021, 22nd Annual Conference of the International\nSpeech Communication Association . Brno, Czechia: ISCA,\nSeptember 2021, to appear.\n[4] J. A. Zwerts, J. Treep, C. S. Kaandorp, F. Meewis, A. C. Koot,\nand H. Kaya, “Introducing a central african primate vocalisa-\ntion dataset for automated species classiﬁcation,” arXiv preprint\narXiv:2101.10390, 2021.\n[5] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasom-\nbat, D. Spathis, T. Xia, P. Cicuta, and C. Mascolo, “Exploring\nautomatic diagnosis of covid-19 from crowdsourced respiratory\nsound data,” in Proceedings of the 26th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining, 2020,\npp. 3474–3484.\n[6] A. Rosenberg, “Classifying skewed data: Importance weighting\nto optimize average recall,” in Thirteenth Annual Conference of\nthe International Speech Communication Association, 2012.\n[7] S. Dieleman and B. Schrauwen, “End-to-end learning for mu-\nsic audio,” in 2014 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2014, pp. 6964–\n6968.\n[8] K. J. Piczak, “Environmental sound classiﬁcation with convolu-\ntional neural networks,” in 2015 IEEE 25th International Work-\nshop on Machine Learning for Signal Processing (MLSP). IEEE,\n2015, pp. 1–6.\n[9] K. Simonyan and A. Zisserman, “Very deep convolutional\nnetworks for large-scale image recognition,” arXiv preprint\narXiv:1409.1556, 2014.\n[10] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke,\nA. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous,\nB. Seybold, M. Slaney, R. J. Weiss, and K. Wilson, “Cnn archi-\ntectures for large-scale audio classiﬁcation,” 2016.\n[11] W. Dai, C. Dai, S. Qu, J. Li, and S. Das, “Very deep convolutional\nneural networks for raw waveforms,” 2016.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NIPS, 2017.\n[13] D. S. Park, Y . Zhang, C. Chiu, Y . Chen, B. Li, W. Chan, Q. V .\nLe, and Y . Wu, “Specaugment on large scale datasets,” inICASSP\n2020 - 2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2020, pp. 6879–6883.\n[14] S. Illium, R. M ¨uller, A. Sedlmeier, and C. Linnhoff-Popien, “Sur-\ngical mask detection with convolutional neural networks and data\naugmentations on spectrograms,” Proc. Interspeech 2020 , pp.\n2052–2056, 2020.\n[15] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, “Optuna:\nA next-generation hyperparameter optimization framework,” in\nProceedings of the 25th ACM SIGKDD international conference\non knowledge discovery & data mining, 2019, pp. 2623–2631.\n[16] Y . Ren, D. Liu, Q. Xiong, J. Fu, and L. Wang, “Spec-resnet: a\ngeneral audio steganalysis scheme based on deep residual network\nof spectrogram,”arXiv preprint arXiv:1901.06838, 2019.\n[17] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-\ntion,” in International Conference on Learning Representations ,\n2018.\n[18] D. Hendrycks and K. Gimpel, “Gaussian error linear units\n(gelus),”arXiv preprint arXiv:1606.08415, 2016.\n[19] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in Interna-\ntional Conference on Machine Learning, 2015, pp. 448–456.\n[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\narXiv preprint arXiv:1706.03762, 2017.\n[22] F. Hutter, H. Hoos, and K. Leyton-Brown, “An efﬁcient approach\nfor assessing hyperparameter importance,” in Proceedings\nof the 31st International Conference on Machine Learning ,\nser. Proceedings of Machine Learning Research, E. P. Xing\nand T. Jebara, Eds., vol. 32, no. 1. Bejing, China:\nPMLR, 22–24 Jun 2014, pp. 754–762. [Online]. Available:\nhttp://proceedings.mlr.press/v32/hutter14.html\n[23] K. Choromanski, V . Likhosherstov, D. Dohan, X. Song, A. Gane,\nT. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser et al.,\n“Rethinking attention with performers,”arXiv e-prints, pp. arXiv–\n2009, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7427915930747986
    },
    {
      "name": "Spectrogram",
      "score": 0.7237051129341125
    },
    {
      "name": "Transformer",
      "score": 0.7037829160690308
    },
    {
      "name": "Weighting",
      "score": 0.6453593969345093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6031301617622375
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.5112237930297852
    },
    {
      "name": "Machine learning",
      "score": 0.45564302802085876
    },
    {
      "name": "Speech recognition",
      "score": 0.39294612407684326
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3480777144432068
    },
    {
      "name": "Computer vision",
      "score": 0.3241804242134094
    },
    {
      "name": "Engineering",
      "score": 0.14206013083457947
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ]
}