{
    "title": "Large Language Models in Medicine: Clinical Applications, Technical Challenges, and Ethical Considerations",
    "url": "https://openalex.org/W4410188253",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2141082436",
            "name": "Kyu-Hwan Jung",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4385381606",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4406152279",
        "https://openalex.org/W4389919444",
        "https://openalex.org/W4395052272",
        "https://openalex.org/W2895763047",
        "https://openalex.org/W2953532875",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4392193048",
        "https://openalex.org/W4404960985",
        "https://openalex.org/W4404649230",
        "https://openalex.org/W4402713323",
        "https://openalex.org/W4377010595",
        "https://openalex.org/W4409283616",
        "https://openalex.org/W4409283601",
        "https://openalex.org/W4380291159",
        "https://openalex.org/W4403705814",
        "https://openalex.org/W4400324908",
        "https://openalex.org/W4395026179",
        "https://openalex.org/W4392598276",
        "https://openalex.org/W4387440167",
        "https://openalex.org/W4387034825",
        "https://openalex.org/W4392643126",
        "https://openalex.org/W4405278206",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4403880728",
        "https://openalex.org/W4353015365",
        "https://openalex.org/W4385546024",
        "https://openalex.org/W4406152263",
        "https://openalex.org/W4402891150",
        "https://openalex.org/W4383346782",
        "https://openalex.org/W2989512989",
        "https://openalex.org/W4324373918",
        "https://openalex.org/W4393119065",
        "https://openalex.org/W4406873895",
        "https://openalex.org/W4399567248",
        "https://openalex.org/W4408332484"
    ],
    "abstract": "Objectives: This study presents a comprehensive review of the clinical applications, technical challenges, and ethical considerations associated with using large language models (LLMs) in medicine.Methods: A literature survey of peer-reviewed articles, technical reports, and expert commentary from relevant medical and artificial intelligence journals was conducted. Key clinical application areas, technical limitations (e.g., accuracy, validation, transparency), and ethical issues (e.g., bias, safety, accountability, privacy) were identified and analyzed.Results: LLMs have potential in clinical documentation assistance, decision support, patient communication, and workflow optimization. The level of supporting evidence varies; documentation support applications are relatively mature, whereas autonomous diagnostics continue to face notable limitations regarding accuracy and validation. Key technical challenges include model hallucination, lack of robust clinical validation, integration issues, and limited transparency. Ethical concerns involve algorithmic bias risking health inequities, threats to patient safety from inaccuracies, unclear accountability, data privacy, and impacts on clinician-patient interactions.Conclusions: LLMs possess transformative potential for clinical medicine, particularly by augmenting clinician capabilities. However, substantial technical and ethical hurdles necessitate rigorous research, validation, clearly defined guidelines, and human oversight. Existing evidence supports an assistive rather than autonomous role, mandating careful, evidence-based integration that prioritizes patient safety and equity.",
    "full_text": "I. Introduction\nThe landscape of artificial intelligence (AI) in medicine \nis undergoing a profound transformation driven by large \nlanguage models (LLMs). These advanced AI systems, \ngrounded in deep learning methodologies and commonly \nbased on transformer neural network architectures [1], ex-\nhibit remarkable proficiency in processing, understanding, \nand generating human language. Notable examples at the \nforefront of this technology include OpenAI’s Generative \nPre-trained Transformer (GPT) series, Google’s Gemini, and \nMeta’s LLaMA [2]. The capabilities of these models derive \nLarge Language Models in Medicine: Clinical  \nApplications, Technical Challenges, and Ethical \nConsiderations\nKyu-Hwan Jung\n1,2\n1\nDepartment of Medical Device Management and Research, Samsung Advanced Institute for Health Sciences and Technology, Sungkyunkwan University, Seoul, Korea \n2\nSmart Healthcare Research Institute, Research Institute for Future Medicine, Samsung Medical Center, Seoul, Korea\nObjectives: This study presents a comprehensive review of the clinical applications, technical challenges, and ethical consid-\nerations associated with using large language models (LLMs) in medicine. Methods: A literature survey of peer-reviewed ar-\nticles, technical reports, and expert commentary from relevant medical and artificial intelligence journals was conducted. Key \nclinical application areas, technical limitations (e.g., accuracy, validation, transparency), and ethical issues (e.g., bias, safety, \naccountability, privacy) were identified and analyzed. Results: LLMs have potential in clinical documentation assistance, de-\ncision support, patient communication, and workflow optimization. The level of supporting evidence varies; documentation \nsupport applications are relatively mature, whereas autonomous diagnostics continue to face notable limitations regarding \naccuracy and validation. Key technical challenges include model hallucination, lack of robust clinical validation, integration \nissues, and limited transparency. Ethical concerns involve algorithmic bias risking health inequities, threats to patient safety \nfrom inaccuracies, unclear accountability, data privacy, and impacts on clinician-patient interactions. Conclusions: LLMs \npossess transformative potential for clinical medicine, particularly by augmenting clinician capabilities. However, substantial \ntechnical and ethical hurdles necessitate rigorous research, validation, clearly defined guidelines, and human oversight. Exist-\ning evidence supports an assistive rather than autonomous role, mandating careful, evidence-based integration that priori-\ntizes patient safety and equity.\nKeywords: Natural Language Processing, Artificial Intelligence, Clinical Decision Support Systems, Medical Informatics Ap-\nplications, Medical Ethics\nHealthc Inform Res. 2025 April;31(2):114-124. \nhttps://doi.org/10.4258/hir.2025.31.2.114\npISSN 2093-3681  •  eISSN 2093-369X  \nReview Article\nSubmitted: March 31, 2025\nAccepted: April 23, 2025\nCorresponding Author \nKyu-Hwan Jung\nDepartment of Medical Device Management and Research, Samsung \nAdvanced Institute for Health Sciences and Technology, Sungkyunk-\nwan University, 115, Irwon-ro, Gangnam-gu, Seoul 06355, Korea. \nTel: +82-2-3410-3632, E-mail: khwanjung@skku.edu (https://orcid.\norg/0000-0002-6626-6800)\nThis is an Open Access article distributed under the terms of the Creative Com-\nmons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-\nnc/4.0/) which permits unrestricted non-commercial use, distribution, and reproduc-\ntion in any medium, provided the original work is properly cited.\nⓒ 2025 The Korean Society of Medical Informatics\n\n115Vol. 31  •  No. 2  •  April 2025\nwww.e-hir.org\nReview of LLMs in Medicine\nfrom extensive pre-training on massive, diverse textual data-\nsets comprising trillions of words sourced from the internet, \nbooks, and other digitized materials. This comprehensive \npre-training equips them with broad knowledge of grammar, \nsemantics, general world information, and complex linguis-\ntic patterns [2,3].\n Adapting these general-purpose models to specialized \nfields such as medicine requires a process known as fine-\ntuning [4]. Fine-tuning involves additional training of a \npre-trained model using a smaller, domain-specific dataset, \nsuch as clinical notes, biomedical literature, or patient com-\nmunication transcripts. This procedure refines the model’s \ngeneral linguistic skills, aligning them with the specialized \nvocabulary, subtleties, and reasoning processes unique to \nmedicine. Examples of such tailored models include BioB-\nERT [5], ClinicalBERT [6], GatorTron [7], Med-PaLM [8], \nand Med-Gemini [9]. These models, fine-tuned or specifi-\ncally designed using clinical or biomedical texts, typically \ndemonstrate superior performance in medical tasks com-\npared to their general-purpose counterparts [3,10,11].\n When introduced into clinical environments, LLMs enter \na setting already influenced by earlier AI applications that \nprimarily analyze structured data, such as laboratory results \nand billing codes, or medical images used in radiology and \npathology. These earlier AI applications focused on risk \nprediction, diagnostic support, and operational optimiza-\ntion [12,13]. However, LLMs represent a paradigm shift due \nto their unique capacity to analyze unstructured text data, a \nsignificant advancement given the abundance of clinically \nrelevant information found in unstructured text formats. \nPreviously, these data sources have largely been inaccessible \nfor computational analysis due to the inherent complexities \nof natural language. Therefore, LLMs signify a new techno-\nlogical frontier capable of unlocking valuable insights and \nmaking practical use of previously underutilized textual in-\nformation [2,10].\n In this article, we survey the current landscape of LLM ap-\nplications within clinical practice, categorizing these applica-\ntions by their primary functional roles. We discuss technical \nissues pertinent to building and effectively implementing \nthese models within clinical settings. Substantial emphasis \nis also placed on ethical considerations, which are particu-\nlarly critical in healthcare AI applications. Finally, the review \nconcludes by examining future directions and the potential \ntrajectory of LLM integration into medicine.\nII.  Clinical Applications of Large Language \nModels\nLLMs are currently being explored across a spectrum of clin-\nical activities to enhance efficiency, support decision-mak -\ning, improve communication, and optimize workflows. The \nevidence supporting these applications varies, with some \nareas demonstrating greater maturity than others (Table 1).\n1. Clinical Documentation Assistance\nA significant portion of clinician time is dedicated to docu-\nmentation tasks, contributing notably to clinician burnout. \nLLMs offer potential solutions by automating or assisting \nwith various documentation-related activities.\n1) Summarization\nLLMs have demonstrated effectiveness in summarizing \nlengthy clinical texts. Specific applications include condens-\ning progress notes into problem lists, abstracting findings \nfrom radiology reports into concise impressions, generating \nafter-visit summaries from detailed history and physical ex-\namination notes, summarizing physician-patient dialogues, \nand shortening extensive patient-generated health ques-\ntionnaires [14,15]. Research using models such as GPT-4 \nindicates that LLM-generated summaries can match or even \nsurpass human-produced summaries in completeness and \naccuracy for particular tasks, while accomplishing this sig-\nnificantly faster [15].\n2) Note generation/drafting\nLLMs can assist in drafting various clinical documents, in-\ncluding discharge summaries [14], referral letters, and inter-\nunit handoff notes [16]. Additionally, LLMs can identify \ngoals of care conversations within electronic health record \n(EHR) notes and rapidly generate clinically useful summa-\nries. Importantly, the reported incidence of hallucination \n(incorrectly generated content) for this specific task is nota-\nbly low [17].\n3) Information extraction\nThese models can extract structured data elements (e.g., \ndiagnoses, medications, symptoms, patient-reported out -\ncomes) from unstructured narrative text within EHRs or \nother clinical documents [18]. This capability facilitates data \naggregation for quality improvement initiatives, research \npurposes, or populating structured fields in EHRs.\n116\nwww.e-hir.org\nKyu-Hwan Jung https://doi.org/10.4258/hir.2025.31.2.114\n4) Documentation improvement\nLLMs may also analyze clinical documentation to identify \nmissing information, inconsistencies (e.g., discrepancies \nbetween diagnosis and treatment plan), or areas requiring \nclarification, thereby improving the overall quality and ac-\ncuracy of medical records [19].\n2. Diagnostic and Clinical Decision Support\nLLMs can analyze extensive medical information and per -\nform inferential functions to support clinical decision-\nmaking and reasoning processes.\n1) Generation of differential diagnoses\nLLMs can evaluate patient symptoms, medical history, and \npreliminary test results to suggest potential differential diag-\nnoses to clinicians. Studies have shown that LLMs can out-\nperform clinicians in generating differential diagnoses and \nin the quality of diagnostic and management reasoning, es-\npecially when employing a chain-of-thought (CoT) process \nthat mirrors human clinical reasoning [20-22]. Thus, LLMs \nhold potential to enhance diagnostic accuracy and efficiency, \nparticularly in complex clinical scenarios.\n2) Answering clinical questions\nLLMs function effectively as advanced information retrieval \nsystems capable of answering specific clinical queries by \nintegrating information from medical knowledge bases, \nscientific literature, or clinical guidelines [8,23,24]. Systems \nlike Med-PaLM 2 have demonstrated strong performance on \nclinical knowledge assessments analogous to medical licens-\ning examinations (e.g., the MedQA benchmark), achieving \naccuracy levels comparable to or exceeding those of human \nexperts in multiple studies [9].\n3) Interpretation of complex clinical data\nLLMs show emerging capabilities for interpreting complex \nclinical data, potentially integrating information across mul-\ntiple modalities (e.g., textual reports and imaging findings). \nCurrent research involving multimodal models such as Med-\nPaLM M and Med-Gemini explores these promising oppor -\ntunities [23,25]. Such interpretation capabilities could also \nassist in treatment planning decisions [17,26].\nTable 1. Clinical applications of LLMs in medicine\nApplication category Specific examples Potential benefits Example studies\nClinical documenta-\ntion summarization\nSummarizing radiology reports, \npatient questions, progress \nnotes, doctor-patient dialogues; \nAVS generation.\nReduced documentation time; \nImproved completeness, cor-\nrectness, conciseness vs. human \nsummaries for specific tasks.\nClinical reader study comparing \nGPT-4 vs. human summaries \n[15].\nClinical documenta-\ntion generation\nDrafting discharge summaries, \nreferral letters; LLM-generated \nEM-to-IP handoff notes.\nReduced documentation burden; \nStandardization of notes.\nCohort study comparing LLM \nvs. physician EM-to-IP hand-\noff notes [16]; Studies on \ndischarge/referral letters [33].\nClinical info extrac-\ntion\nExtracting diagnoses, medica-\ntions, PROs from unstructured \ntext; Populating structured EHR \nfields.\nImproved data accessibility; Facil-\nitation of quality improvement \nand research; Reduced manual \ndata entry.\nPrivacy-preserving medical \ninformation retrieval [18].\nDiagnostic & clinical \ndecision support\nDifferential diagnosis sugges-\ntions; Answering clinical ques-\ntions; Potential data interpreta-\ntion.\nAccess to synthesized knowledge; \nPotential to augment clinical \nreasoning; Speed up informa-\ntion retrieval.\nMed-PaLM 2 high accuracy on \nMedQA [9]; Med-PaLM M \noutperforms most state-of-the-\nart on MultiMedBench [23].\nPatient communica-\ntion & engagement\nSimplifying medical jargon/docu-\nments (e.g., trial info, discharge \nsummaries); Patient Q&A chat-\nbots; Health literacy tools.\nImproved patient understand-\ning; Enhanced engagement; \nIncreased accessibility of infor-\nmation; Potential for empathetic \nresponses.\nPatient-friendly discharge sum-\nmaries [33]; Studies on chatbot \nempathy/utility [22].\nLLM: large language model, AVS: audio video coding standard, GPT: Generative Pre-trained Transformer, EM: emergency medi-\ncine, IP: inpatient, EHR: electronic health record, PRO: patient reported outcome.\n117Vol. 31  •  No. 2  •  April 2025\nwww.e-hir.org\nReview of LLMs in Medicine\n Nevertheless, translating these capabilities into reliable \nclinical decision support (CDS) remains challenging. Al-\nthough strong performance on standardized assessments is \nreassuring, studies simulating real clinical scenarios have \nidentified significant deficiencies [27]. A notable gap persists \nbetween performance on knowledge-based tests and the \nnuanced, context-dependent judgment essential for clini-\ncal practice. Current LLMs excel at information retrieval \nyet lack the sophisticated clinical judgment and integrative \ncapacities of human clinicians [28]. Moreover, seamless in-\ntegration with clinical workflows and EHR systems, which \nwould be essential for successful CDS, remains difficult to \nachieve [29].\n3. Patient Communication and Engagement\nLLMs’ natural language processing abilities offer potential \nimprovements in clinician-patient communication and \npatient access to, understanding of, and dissemination of \nhealth information.\n1) Simplifying of medical information\nLLMs can translate complex medical terminology (e.g., \ninformed consent documents) [30,31], research findings \n(e.g., clinical trial summaries) [32], or clinical reports (e.g., \ndischarge summaries) [6,33] into straightforward language \nthat patients can easily comprehend. This application has \nsignificant potential to enhance patient understanding and \npromote effective shared decision-making.\n2) Patient question answering/chatbots\nVirtual assistants or chatbots powered by LLMs can respond \nto patient inquiries regarding health conditions or treat-\nments, offering preliminary information and guidance [34]. \nRemarkably, LLMs can deliver responses with a significant \ndegree of empathy, even being perceived by patients as more \nempathetic than human clinicians in certain written com-\nmunications [22,35].\n3) Increasing health literacy\nBy generating clear, understandable, and personalized health \ninformation, LLMs have the potential to enhance patients' \noverall health literacy [36].\n Despite this promise, caution is warranted. LLMs can pro-\nduce inaccurate or misleading information (misinformation), \nlack nuanced understanding of individual patient circum-\nstances, and fail to convey critical emotional and psycho-\nlogical dimensions inherent in patient care. The impersonal \nnature of chatbot-mediated communication cannot replicate \nthe therapeutic benefits of direct human clinician-patient \ninteractions. Thus, employing LLMs for patient communica-\ntion necessitates careful oversight and rigorous validation of \ninformation for accuracy and appropriateness.\nIII.  Technical Challenges and Ethical  \nConsiderations\nDespite enthusiasm for using LLMs in medicine, their trans-\nlation into safe and effective clinical applications is impeded \nby significant technical hurdles and profound ethical con-\ncerns (Table 2). These challenges often intersect and require \nintegrated solutions.\n1. Technical Challenges\n1) Accuracy and reliability\nEnsuring the accuracy and reliability of LLM outputs in clin-\nical contexts—where errors carry severe consequences—is \narguably the most critical technical barrier. LLMs are known \nto “hallucinate, ” generating fluent, plausible-sounding con-\ntent that is factually incorrect, unsupported by source data, \nor entirely fabricated [37-40]. If relied upon for clinical de -\ncisions, these inaccuracies directly threaten patient safety. \nMitigation strategies currently explored include retrieval \naugmented generation (RAG) to ground responses in exter -\nnal knowledge sources, improving uncertainty estimation \nand calibration techniques, employing specific prompting \nstrategies such as chain-of-thought, domain-specific fine-\ntuning, implementing domain-informed safety guardrails, \nand developing robust hallucination detection methods [41].\n Beyond complete fabrication, LLMs can produce outputs \nthat contradict source data or established medical facts or \nomit critical details from summaries and analyses [16]. \nThese errors also present substantial patient safety risks.\n2) Validation and evaluation\nAssessing the clinical utility and safety of LLMs is exceptionally \nchallenging. Current validation methodologies often fall short.\n Standard natural language processing (NLP) evaluation \nmetrics—such as ROUGE and BLEU for summarization tasks, \nor accuracy metrics for question-answering—frequently cor-\nrelate poorly with clinical relevance, factual correctness, or \npatient safety [42]. High benchmark scores do not necessar -\nily equate to competence in real-world clinical reasoning. \nTherefore, there is an urgent need for standardized, clinically \nmeaningful benchmarks and evaluation frameworks specifi-\ncally designed for medical LLMs.\n An additional challenge is the rapid evolution of models \n118\nwww.e-hir.org\nKyu-Hwan Jung https://doi.org/10.4258/hir.2025.31.2.114\nand reporting standards. The swift pace of LLM develop-\nment complicates timely, rigorous evaluations, as models are \ncontinually updated, potentially altering their performance \ncharacteristics. Transparent reporting standards, such as the \nproposed TRIPOD-LLM and MI-CLEAR-LLM guidelines \n[43,44], are thus essential for proper appraisal and reproduc-\nibility of studies.\n3) Data quality and privacy\nTraining and fine-tuning LLMs require extensive, high-quality \ndata, the availability of which is often limited [2,3]. Protect-\ning patient privacy is equally critical. Using patient data for \nmodel training can result in data memorization, leakage, or \nrisks of re-identification [18]. Mitigation strategies include \nemploying synthetic data, data anonymization, pseudony-\nmization, differential privacy techniques, robust cybersecu-\nrity measures, model penetration testing, and implementing \ntransparent patient consent processes [45,46].\n4) Integration and interoperability\nSuccessful implementation of LLMs depends heavily on \nseamless integration with existing clinical workflows and \nhealthcare IT infrastructures, particularly EHRs [11]. This \npresents a considerable technical challenge, requiring exten-\nsive customization and interface development. User-friendly \ninterfaces are essential for widespread clinical adoption.\n5) Transparency and explainability\nLLMs generally function as “black boxes, ” making it chal -\nlenging to understand how specific outputs or suggestions \nare derived [47]. This lack of transparency discourages cli-\nnician trust, complicates error analysis and validation, and \nhinders informed acceptance. Developing and applying ex-\nplainable AI (XAI) methods—such as SHAP [48] attention \nTable 2. Technical challenges of LLMs in clinical medicine\nChallenge Description Risks for clinical practice Potential mitigation strategies\nHallucination/\naccuracy\nLLMs generating plausible but \nfactually incorrect, fabricated, or \nunsupported information. In-\ncludes factual errors & omissions.\nErroneous diagnoses or treatment \nplans; Patient harm; Erosion of \nclinician trust; Increased verifica-\ntion workload.\nRAG, uncertainty estimation, CoT \nprompting, fine-tuning, safety \nguardrails, robust detection meth-\nods, human oversight.\nValidation & \nevaluation \ngaps\nLack of clinically relevant bench-\nmarks; Poor correlation of NLP \nmetrics with clinical utility; Pau-\ncity of prospective clinical trials.\nDifficulty assessing true clinical \nreadiness & safety; Risk of de-\nploying unsafe/ineffective tools; \nHinders comparative effective-\nness research.\nDevelopment of clinically mean-\ningful validation frameworks & \nbenchmarks; Requirement for \nrigorous prospective trials; Stan-\ndardized reporting guidelines (e.g., \nTRIPOD-LLM).\nData privacy \n& security\nRisk of exposing sensitive patient \ndata used in training or prompts; \nCompliance with HIPAA/GDPR; \nConsent issues.\nBreach of patient confidentiality; \nIdentity theft; Erosion of public \ntrust; Legal & regulatory penal-\nties.\nData anonymization/pseudonymiza-\ntion, differential privacy, robust \ncybersecurity, penetration testing, \nclear informed consent processes, \ntiered data access policies.\nIntegration & \ninteroper-\nability\nDifficulty integrating LLMs with \nexisting EHRs and clinical work-\nflows.\nLimited adoption; Workflow \ndisruption; Reduced efficiency \ngains; Clinician frustration.\nDevelopment of standardized APIs; \nCollaboration between LLM \ndevelopers & EHR vendors; User-\ncentered interface design.\nTransparency \n& explain-\nability\n“Black box” nature of LLMs, mak-\ning reasoning processes opaque.\nHinders clinician trust & criti-\ncal appraisal; Complicates error \nanalysis & debugging; Impedes \nvalidation; Barrier to informed \nconsent.\nExplainable AI (XAI) methods \n(SHAP , attention maps, KG inte-\ngration); Transparent reporting of \nmethods & data; Focus on inter-\npretable models where possible.\nLLM: large language model, RAG: retrieval augmented generation, CoT: chain-of-thought, EHR: electronic health record, KG: \nknowledge graph, NLP: natural language processing, API: application programming interface, HIPAA: Health Insurance Portability \nand Accountability Act, GDPR: General Data Protection Regulation, SHAP: SHapley Additive exPlanations.\n119Vol. 31  •  No. 2  •  April 2025\nwww.e-hir.org\nReview of LLMs in Medicine\nvisualization [48], and integration with knowledge graphs \n[49]—tailored for medical LLMs is an active area of research, \nalong with promoting transparent reporting practices.\n2. Ethical Considerations\nDeploying LLMs in clinical medicine raises profound ethical \nissues that require careful consideration and proactive man-\nagement (Table 3).\n1) Algorithmic bias and equity\nA major ethical concern is that LLMs trained on datasets \nreflecting existing societal inequities may amplify biases in \nhealthcare delivery [36]. This could result in poorer perfor -\nmance or unfair clinical recommendations for underrep-\nresented racial or ethnic groups, gender groups, socioeco-\nnomic populations, or linguistic minorities. Linguistic bias \nis especially relevant, as most models are primarily trained \non standard English, resulting in diminished performance \nfor non-English speakers or users of non-standard dialects \n[50]. Mitigation requires intentional curation of diverse and \nrepresentative training data, the development of bias detec-\ntion and mitigation algorithms, validation across diverse \ndemographic groups, and inclusive design and evaluation \nprocesses. Addressing bias is both ethically imperative and \nessential for clinical validity, as a biased tool cannot reliably \nserve all patients.\nTable 3. Ethical considerations of LLMs in clinical medicine\nConsideration Description Risks for clinical practice Potential mitigation strategies\nAlgorithmic \nbias & equity\nPerpetuation/amplification \nof societal biases present \nin training data (racial, \ngender, linguistic, etc.).\nHealth disparities; Inequitable \nquality of care; Unfair or harm-\nful recommendations for certain \ngroups; Non-compliance with \nanti-discrimination laws.\nDiverse & representative training data; \nBias detection & mitigation algorithms; \nValidation across diverse populations; \nInclusive design processes; Auditing for \nfairness.\nPatient safety & \nliability\nRisk of harm due to inaccu-\nrate/hallucinated outputs \nand unclear responsibility \nfor errors involving LLMs.\nIncorrect diagnosis/treatment; \nAdverse events; Delayed care; \nDifficulty assigning blame; Legal \nuncertainty; Potential barrier to \nadoption; Undermining trust.\nRigorous safety testing; Robust valida-\ntion; Clear performance limitations \ndisclosure; Mandatory human oversight; \nDevelopment of clear regulatory guide-\nlines; Defining roles & responsibilities \nfor developers, institutions, clinicians; \nEstablishing legal precedents.\nData gov-\nernance & \nconsent\nEthical sourcing & use of pa-\ntient data; Ensuring mean-\ningful informed consent; \nData ownership rights.\nViolation of patient autonomy & \nprivacy rights; Erosion of trust; \nLegal non-compliance.\nTransparent data use policies; Patient \neducation on data rights; Robust consent \nmechanisms; Strong data governance \nframeworks within institutions.\nTransparency & \ntrust (ethical)\nLack of transparency about \nmodel capabilities, limi-\ntations, and data usage \nerodes trust.\nClinician reluctance to adopt; \nPatient skepticism; Difficulty \nin establishing trustworthy AI \nsystems.\nIncreased transparency in reporting (data, \nmethods, performance); Clear commu-\nnication with clinicians & patients about \nLLM function & limits; Explainability \nefforts.\nImpact on clini-\ncian-patient \nrelationship\nPotential for depersonaliza-\ntion, reduced empathy, or \ncommunication break-\ndown if used improperly.\nWeakening of therapeutic alli-\nance; Reduced patient satisfac-\ntion; Missed non-verbal cues.\nThoughtful workflow integration; Train-\ning clinicians on effective use; Prioritiz-\ning human interaction; Using LLMs to \naugment rather than replace communi-\ncation.\nEquity of access Potential disparities if ben-\nefits aren't accessible to all \npopulations/settings.\nWidening health inequities; Un-\nequal access to advanced health-\ncare tools [12].\nPolicies promoting equitable deployment; \nDevelopment of tools for low-resource \nsettings; Ensuring accessibility for di-\nverse linguistic groups & abilities.\nLLM: large language model.\n120\nwww.e-hir.org\nKyu-Hwan Jung https://doi.org/10.4258/hir.2025.31.2.114\n2) Patient safety and liability\nLLMs’ potential to produce inaccurate information—includ-\ning hallucinations, factual errors, or critical omissions—\nconstitutes a direct risk to patient safety, potentially causing \nerroneous diagnoses or treatment delays. Determining re-\nsponsibility or liability when an LLM contributes to clinical \nerrors is complicated [51,52]. Questions arise about whether \nliability rests with the clinician using the tool, the develop-\ners, the implementing institution, or a combination thereof. \nThe absence of definitive regulatory guidelines and estab-\nlished legal precedents for AI-related medical errors creates \nuncertainty and may impede implementation. Thus, explicit \nguidelines, clear accountability mechanisms, and possibly \nnew legal frameworks are essential.\n3) Data governance and consent\nUsing large volumes of patient data for LLM training and op-\neration raises significant governance challenges [29,53]. Eth-\nical data sourcing, patient privacy, data security, and obtain-\ning valid informed consent for data use are critical concerns. \nIssues around data ownership and patient rights concerning \ndata used by AI systems must be clearly addressed.\n4) Transparency and trust\nThe inherently opaque “black box” nature of most LLMs \nundermines trust among clinicians and patients. Without \nthe ability to understand or explain an LLM’s reasoning be-\nhind recommendations, clinicians are less likely to trust or \nconfidently utilize these tools. This mistrust could result in \ndelayed or incorrect clinical decisions, negatively impacting \npatient care [54]. Advancing explainability and promoting \nopen reporting are crucial steps toward establishing trust.\n5) Impact on clinician-patient relationship\nIntegrating LLMs into clinical interactions carries the risk of \ndepersonalizing care, reducing clinician empathy, or nega -\ntively affecting direct communication if poorly implemented. \nOver-reliance on LLMs could potentially deskill clinicians \nor diminish their critical-thinking abilities. Conversely, by \nreducing administrative burdens, LLMs might enable clini-\ncians to spend more meaningful time interacting directly \nwith patients [55]. Careful consideration of workflow inte-\ngration and human factors is required to ensure technologi-\ncal support enhances rather than detracts from therapeutic \nrelationships.\n6) Equity of access\nEnsuring equitable access to the benefits of LLM technology \nacross diverse patient populations, socioeconomic groups, \nand healthcare settings—including low-resource environ-\nments—is a significant ethical consideration [29,54]. The \ndigital divide and disparities in technology access could \nexacerbate existing health inequities unless proactively ad-\ndressed.\nIV. Discussion\n1. Risks and Benefits of LLMs in Medicine\nLLMs represent a technology with significant potential to \nprofoundly impact clinical medicine, primarily due to their \nunparalleled ability to process and generate language, grant-\ning access to the vast amounts of unstructured textual data \navailable in healthcare [2,11]. Current evidence highlights \npromising applications, particularly in reducing the burden \nof clinical documentation through summarization, note \ngeneration, and information extraction. Additionally, LLMs \nshow considerable potential for improving patient commu-\nnication by simplifying complex information, powering in-\nformative chatbots, and optimizing various aspects of clini-\ncal workflows.\n Nevertheless, initial optimism must be tempered by a real-\nistic assessment of the technical and ethical barriers to wide-\nspread, secure, and effective clinical deployment. Accuracy \nconcerns, particularly the tendency of these models toward \nhallucinations, remain critical issues. A notable gap persists \ndue to the lack of rigorous clinical validation, highlighting \ndiscrepancies between model performance on standardized \nbenchmarks and real-world clinical decision-making. Fur -\nthermore, the “black box” nature of LLMs poses significant \ntransparency and trust challenges, while ethical consider -\nations surrounding bias, equity, privacy, and accountability \nrequire careful attention and proactive mitigation.\n Therefore, integrating LLMs into clinical practice neces-\nsitates carefully balancing their potential benefits against in-\nherent risks. Current technological capabilities and available \nevidence strongly indicate that LLMs should primarily serve \nas assistive tools augmenting clinician abilities rather than as \nautonomous entities intended to replace clinicians. Adopting \na cautious, evidence-based approach is crucial, requiring ro-\nbust human oversight across virtually all clinical applications \nand embracing a “physician-in-the-loop” paradigm, wherein \nclinicians retain ultimate responsibility for clinical decisions \ninformed by LLM outputs.\n Moving forward requires a collaborative, multi-stakeholder \napproach involving AI developers, clinicians, ethicists, regu-\nlators, healthcare institutions, and patients. Prioritizing pa-\n121Vol. 31  •  No. 2  •  April 2025\nwww.e-hir.org\nReview of LLMs in Medicine\ntient safety, ensuring equity, maintaining transparency, and \nbuilding trust should be guiding principles in the develop-\nment and integration of these powerful technological tools.\n2. Future Directions and Research Needs\nFully realizing the potential of LLMs in clinical medicine re-\nquires focused efforts in several key areas.\n1) Rigorous validation\nThere is an urgent need for developing and adopting stan-\ndardized, clinically meaningful validation methodologies \nand benchmarks. Such frameworks should accurately assess \nperformance, safety, and real-world clinical utility, extend -\ning beyond traditional NLP evaluation metrics. Conducting \nprospective, randomized controlled trials comparing LLM-\nassisted workflows with conventional care is essential to es-\ntablishing genuine clinical efficacy.\n2) Technical advancement\nContinued research is necessary to enhance model accuracy, \nminimize the frequency and consequences of hallucinations, \nimprove robustness across diverse datasets and scenarios, \nand develop effective explainability techniques specifically \ntailored to clinical users. Further investigation into emerg-\ning ecosystems—in which multiple specialized AI agents \n(e.g., diagnostic agents, documentation agents, patient com-\nmunication agents) collaborate by exchanging information \nand coordinating tasks within complex clinical workflows—\nis also warranted [56,57]. Active research areas additionally \ninclude integrating multimodal data [58,59] and extending \nLLM capabilities into physical systems [60].\n3) Bias mitigation\nProactive strategies for identifying, measuring, and mitigat -\ning biases in training datasets and model outputs are crucial \nto ensure equitable performance across patient populations \nand prevent exacerbating existing health disparities.\n4) Ethical and regulatory frameworks\nEstablishing clear ethical guidelines, robust regulatory over -\nsight, and well-defined accountability structures is essential \nto govern the responsible development, deployment, and \nclinical use of LLM technologies in healthcare.\n5) Human-AI collaboration\nResearch should prioritize optimizing human-AI interaction \nmodels, designing intuitive interfaces, and understanding \nhow to best integrate LLMs into clinical workflows to ef-\nfectively support clinicians rather than hinder their perfor -\nmance.\nConflict of Interest\nNo potential conflict of interest relevant to this article was \nreported.\nORCID\nKyu-Hwan Jung (https://orcid.org/0000-0002-6626-6800)\nReferences\n1. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, \nGomez AN, et al. Attention is all you need [Internet]. \nIthaca (NY): arXive.org; 2017 [cited at 2025 Apr 15]. \nAvailable from: https://arxiv.org/abs/1706.03762.\n2. Minaee S, Mikolov T, Nikzad N, Chenaghlu M, Socher R, \nAmatriain X, et al. Large language models: a survey [In-\nternet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2402.06196v1.\n3. Thirunavukarasu AJ, Ting DS, Elangovan K, Gutierrez \nL, Tan TF , Ting DS. Large language models in medicine. \nNat Med 2023;29(8):1930-40. https://doi.org/10.1038/\ns41591-023-02448-8\n4. Wornow M, Xu Y , Thapa R, Patel B, Steinberg E, Flem-\ning S, et al. The shaky foundations of large language \nmodels and foundation models for electronic health re-\ncords. NPJ Digit Med 2023;6(1):135. https://doi.org/10. \n1038/s41746-023-00879-8\n5. Lee J, Y oon W , Kim S, Kim D, Kim S, So CH, et al. BioB-\nERT: a pre-trained biomedical language representation \nmodel for biomedical text mining. Bioinformatics 2020; \n36(4):1234-40. https://doi.org/10.1093/bioinformatics/\nbtz682\n6. Huang K, Altosaar J, Ranganath R. ClinicalBERT: mod-\neling clinical notes and predicting hospital readmission \n[Internet]. Ithaca (NY): arXive.org; 2019 [cited at 2025 \nApr 15]. Available from: https://arxiv.org/abs/1904. \n05342v1.\n7. Y ang X, Chen A, PourNejatian N, Shin HC, Smith KE, \nParisien C, et al. A large language model for electronic \nhealth records. NPJ Digit Med 2022;5(1):194. https://\ndoi.org/10.1038/s41746-022-00742-2\n8. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW , \net al. Large language models encode clinical knowledge. \nNature 2023;620(7972):172-80. https://doi.org/10.1038/\n122\nwww.e-hir.org\nKyu-Hwan Jung https://doi.org/10.4258/hir.2025.31.2.114\ns41586-023-06291-2\n9. Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Amin \nM, et al. Toward expert-level medical question answer -\ning with large language models. Nat Med 2025;31(3): \n943-50. https://doi.org/10.1038/s41591-024-03423-7\n10. The Lancet Digital Health. Large language models: a \nnew chapter in digital health. Lancet Digit Health 2024; \n6(1):e1. https://doi.org/10.1016/S2589-7500(23)00254-6\n11. Meng X, Y an X, Zhang K, Liu D, Cui X, Y ang Y , et al. \nThe application of large language models in medicine: a \nscoping review. iScience 2024;27(5):109713. https://doi.\norg/10.1016/j.isci.2024.109713\n12. Yu KH, Beam AL, Kohane IS. Artificial intelligence in \nhealthcare. Nat Biomed Eng 2018;2(10):719-31. https://\ndoi.org/10.1038/s41551-018-0305-z\n13. Davenport T, Kalakota R. The potential for artificial in-\ntelligence in healthcare. Future Healthc J 2019;6(2):94-8. \nhttps://doi.org/10.7861/futurehosp.6-2-94\n14. Patel SB, Lam K. ChatGPT: the future of discharge sum-\nmaries? Lancet Digit Health 2023;5(3):e107-8. https://\ndoi.org/10.1016/S2589-7500(23)00021-3\n15. Van Veen D, Van Uden C, Blankemeier L, Delbrouck \nJB, Aali A, Bluethgen C, et al. Adapted large language \nmodels can outperform medical experts in clinical text \nsummarization. Nat Med 2024;30(4):1134-42. https://\ndoi.org/10.1038/s41591-024-02855-5\n16. Hartman V , Zhang X, Poddar R, McCarty M, Fortenko \nA, Sholle E, et al. Developing and evaluating large lan-\nguage model-generated emergency medicine handoff \nnotes. JAMA Netw Open 2024;7(12):e2448723. https://\ndoi.org/10.1001/jamanetworkopen.2024.48723\n17. Agaronnik ND, Davis J, Manz CR, Tulsky JA, Lindvall C. \nLarge Language Models to Identify Advance Care Plan-\nning in Patients With Advanced Cancer. J Pain Symp-\ntom Manage 2025;69(3):243-50.e1. https://doi.org/10. \n1016/j.jpainsymman.2024.11.016\n18. Wiest IC, Ferber D, Zhu J, van Treeck M, Meyer SK, Jug-\nlan R, et al. Privacy-preserving large language models \nfor structured medical information retrieval. NPJ Digit \nMed 2024;7(1):257. https://doi.org/10.1038/s41746-024-\n01233-2\n19. Lyu Q, Tan J, Zapadka ME, Ponnatapura J, Niu C, My-\ners KJ, et al. Translating radiology reports into plain \nlanguage using ChatGPT and GPT-4 with prompt \nlearning: results, limitations, and potential. Vis Comput \nInd Biomed Art 2023;6(1):9. https://doi.org/10.1186/\ns42492-023-00136-5\n20. Brodeur PG, Buckley TA, Kanjee Z, Goh E, Ling EB, \nJain P , et al. Superhuman performance of a large lan-\nguage model on the reasoning tasks of a physician [In-\nternet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2412.10849.\n21. McDuff D, Schaekermann M, Tu T, Palepu A, Wang A, \nGarrison J, et al. Towards accurate differential diagnosis \nwith large language models. Nature 2025 Apr 9 [Epub]. \nhttps://doi.org/10.1038/s41586-025-08869-4.\n22. Tu T, Schaekermann M, Palepu A, Saab K, Freyberg J, \nTanno R, et al. Towards conversational diagnostic artifi-\ncial intelligence. Nature 2025 Apr 9 [Epub]. https://doi.\norg/10.1038/s41586-025-08866-7.\n23. Tu T, Azizi S, Driess D, Schaekermann M, Amin M, \nChang PC, et al. Towards generalist biomedical AI [In-\nternet]. Ithaca (NY): arXive.org; 2023 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2307.14334.\n24. Ali R, Tang OY , Connolly ID, Fridley JS, Shin JH, Zad-\nnik Sullivan PL, et al. Performance of ChatGPT, GPT-4, \nand Google Bard on a neurosurgery oral boards prepa-\nration question bank. Neurosurgery 2023;93(5):1090-8. \nhttps://doi.org/10.1227/neu.0000000000002551\n25. Saab K, Tu T, Weng WH, Tanno R, Stutz D, Wulczyn E, \net al. Capabilities of Gemini models in medicine [Inter -\nnet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2404.18416.\n26. Oh Y , Park S, Byun HK, Cho Y , Lee IJ, Kim JS, et al. \nLLM-driven multimodal target volume contouring in \nradiation oncology. Nat Commun 2024;15(1):9186. \nhttps://doi.org/10.1038/s41467-024-53387-y\n27. Hager P , Jungmann F , Holland R, Bhagat K, Hubrecht I, \nKnauer M, et al. Evaluation and mitigation of the limita-\ntions of large language models in clinical decision-mak-\ning. Nat Med 2024;30(9):2613-22. https://doi.org/10. \n1038/s41591-024-03097-1\n28. Chen H, Fang Z, Singla Y , Dredze M. Benchmarking \nlarge language models on answering and explaining \nchallenging medical questions [Internet]. Ithaca (NY): \narXive.org; 2024 [cited at 2025 Apr 15]. Available from: \nhttps://arxiv.org/abs/2402.18060v1.\n29. Ong JC, Chang SY , William W , Butte AJ, Shah NH, Chew \nLS, et al. Ethical and regulatory challenges of large lan-\nguage models in medicine. Lancet Digit Health 2024;6(6): \ne428-32. https://doi.org/10.1016/S2589-7500(24)00061-X\n30. Ali R, Connolly ID, Tang OY , Mirza FN, Johnston B, Ab-\ndulrazeq HF , et al. Bridging the literacy gap for surgical \nconsents: an AI-human expert collaborative approach. \nNPJ Digit Med 2024;7(1):63. https://doi.org/10.1038/\ns41746-024-01039-2\n123Vol. 31  •  No. 2  •  April 2025\nwww.e-hir.org\nReview of LLMs in Medicine\n31. Decker H, Trang K, Ramirez J, Colley A, Pierce L, Cole-\nman M, et al. Large language model-based chatbot vs \nsurgeon-generated informed consent documentation \nfor common procedures. JAMA Netw Open 2023;6(10): \ne2336997. https://doi.org/10.1001/jamanetworkopen. \n2023.36997\n32. Ghim JL, Ahn S. Transforming clinical trials: the emerg-\ning roles of large language models. Transl Clin Pharma-\ncol 2023;31(3):131-8. https://doi.org/10.12793/tcp.2023. \n31.e16\n33. Zaretsky J, Kim JM, Baskharoun S, Zhao Y , Austrian J, \nAphinyanaphongs Y , et al. Generative artificial intel-\nligence to transform inpatient discharge summaries \nto patient-friendly language and format. JAMA Netw \nOpen 2024;7(3):e240357. https://doi.org/10.1001/jama-\nnetworkopen.2024.0357\n34. Cho S, Lee M, Yu J, Y oon J, Choi JB, Jung KH, et al. \nLeveraging large language models for improved under -\nstanding of communications with patients with cancer \nin a call center setting: proof-of-concept study. J Med \nInternet Res 2024;26:e63892. https://doi.org/10.2196/ \n63892\n35. Ayers JW , Poliak A, Dredze M, Leas EC, Zhu Z, Kelley \nJB, et al. Comparing physician and artificial intelligence \nchatbot responses to patient questions posted to a public \nsocial media forum. JAMA Intern Med 2023;183(6):589-\n96. https://doi.org/10.1001/jamainternmed.2023.1838\n36. Aydin S, Karabacak M, Vlachos V , Margetis K. Large \nlanguage models in patient education: a scoping review \nof applications in medicine. Front Med (Lausanne) 2024; \n11:1477898. https://doi.org/10.3389/fmed.2024.1477898\n37. Azamfirei R, Kudchadkar SR, Fackler J. Large language \nmodels and the perils of their hallucinations. Crit Care \n2023;27(1):120. https://doi.org/10.1186/s13054-023-\n04393-x\n38. Rawte V , Sheth A, Das A. A survey of hallucination in \nlarge foundation models [Internet]. Ithaca (NY): arXive.\norg; 2023 [cited at 2025 Apr 15]. Available from: https://\narxiv.org/abs/2309.05922.\n39. Ji Z, Lee N, Frieske R, Yu T, Su D, Xu Y , et al. Survey of \nhallucination in natural language generation [Internet]. \nIthaca (NY): arXive.org; 2022 [cited at 2025 Apr 15]. \nAvailable from: https://arxiv.org/abs/2202.03629v1.\n40. Huang L, Yu W , Ma W , Zhong W , Feng Z, Wang H, et al. \nA survey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions [In-\nternet]. Ithaca (NY): arXive.org; 2023 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2311.05232v1.\n41. Tonmoy SM, Zaman SM, Jain V , Rani A, Rawte V , \nChadha A, et al. A comprehensive survey of hallucina-\ntion mitigation techniques in large language models [In-\nternet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2401.01313.\n42. Yu F , Endo M, Krishnan R, Pan I, Tsai A, Reis EP , et al. \nEvaluating progress in automatic chest X-ray radiology \nreport generation. Patterns (N Y) 2023;4(9):100802. \nhttps://doi.org/10.1016/j.patter.2023.100802\n43. Gallifant J, Afshar M, Ameen S, Aphinyanaphongs Y , \nChen S, Cacciamani G, et al. The TRIPOD-LLM report-\ning guideline for studies using large language models. \nNat Med 2025;31(1):60-9. https://doi.org/10.1038/\ns41591-024-03425-5\n44. Park SH, Suh CH, Lee JH, Kahn CE, Moy L. Minimum \nreporting items for clear evaluation of accuracy reports \nof large language models in healthcare (MI-CLEAR-\nLLM). Korean J Radiol 2024;25(10):865-8. https://doi.\norg/10.3348/kjr.2024.0843\n45. Behnia R, Ebrahimi M, Pacheco J, Padmanabhan B. \nPrivately fine-tuning large language models with differ -\nential privacy [Internet]. Ithaca (NY): arXive.org; 2022 \n[cited at 2025 Apr 15]. Available from: https://arxiv.org/\nabs/2210.15042v1.\n46. Charles Z, Ganesh A, McKenna R, McMahan HB, \nMitchell N, Pillutla K, et al. Fine-tuning large language \nmodels with user-level differential privacy [Internet]. \nIthaca (NY): arXive.org; 2024 [cited at 2025 Apr 15]. \nAvailable from: https://arxiv.org/abs/2407.07737.\n47. Zhao H, Chen H, Y ang F , Liu N, Deng H, Cai H, et al. \nExplainability for large language models: a survey [In-\nternet]. Ithaca (NY): arXive.org; 2023 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2309.01029.\n48. Goldshmidt R, Horovicz M. TokenSHAP: interpreting \nlarge language models with Monte Carlo Shapley value \nestimation [Internet]. Ithaca (NY): arXive.org; 2024 \n[cited at 2025 Apr 15]. Available from: https://arxiv.org/\nabs/2407.10114.\n49. Kau A, He X, Nambissan A, Astudillo A, Yin H, Aryani \nA. Combining knowledge graphs and large language \nmodels [Internet]. Ithaca (NY): arXive.org; 2024 [cited \nat 2025 Apr 15]. Available from: https://arxiv.org/abs/ \n2407.06564.\n50. Li Z, Shi Y , Liu Z, Y ang F , Payani A, Liu N, et al. Lan-\nguage ranker: a metric for quantifying LLM perfor -\nmance across high and low-resource languages [Inter -\nnet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2404.11553.\n124\nwww.e-hir.org\nKyu-Hwan Jung https://doi.org/10.4258/hir.2025.31.2.114\n51. Mesko B, Topol EJ. The imperative for regulatory over -\nsight of large language models (or generative AI) in \nhealthcare. NPJ Digit Med 2023;6(1):120. https://doi.\norg/10.1038/s41746-023-00873-0\n52. Reddy S, Allan S, Coghlan S, Cooper P . A governance \nmodel for the application of AI in health care. J Am Med \nInform Assoc 2020;27(3):491-7. https://doi.org/10.1093/\njamia/ocz192\n53. Harrer S. Attention is not all you need: the complicated \ncase of ethically using large language models in health-\ncare and medicine. EBioMedicine 2023;90:104512. \nhttps://doi.org/10.1016/j.ebiom.2023.104512\n54. Haltaufderheide J, Ranisch R. The ethics of ChatGPT in \nmedicine and healthcare: a systematic review on large \nlanguage models (LLMs). NPJ Digit Med 2024;7(1):183. \nhttps://doi.org/10.1038/s41746-024-01157-x\n55. Workum JD, van de Sande D, Gommers D, van Gen-\nderen ME. Bridging the gap: a practical step-by-step ap-\nproach to warrant safe implementation of large language \nmodels in healthcare. Front Artif Intell 2025;8:1504805. \nhttps://doi.org/10.3389/frai.2025.1504805\n56. Kim Y , Park C, Jeong H, Grau-Vilchez C, Chan YS, Xu \nX, et al. A demonstration of adaptive collaboration of \nlarge language models for medical decision-making [In-\nternet]. Ithaca (NY): arXive.org; 2024 [cited at 2025 Apr \n15]. Available from: https://arxiv.org/abs/2411.00248.\n57. Kim Y , Park C, Jeong H, Chan YS, Xu X, McDuff D, \net al. MDAgents: an adaptive collaboration of LLMs \nfor medical decision-making [Internet]. Ithaca (NY): \narXive.org; 2024 [cited at 2025 Apr 15]. Available from: \nhttps://arxiv.org/abs/2404.15155.\n58. Lu MY , Chen B, Williamson DF , Chen RJ, Zhao M, \nChow AK, et al. A multimodal generative AI copilot \nfor human pathology. Nature 2024;634(8033):466-73. \nhttps://doi.org/10.1038/s41586-024-07618-3\n59. Hong EK, Roh B, Park B, Jo JB, Bae W , Soung Park J, et \nal. Value of using a generative AI model in chest radiog-\nraphy reporting: a reader study. Radiology 2025;314(3): \ne241646. https://doi.org/10.1148/radiol.241646\n60. Kim MJ, Pertsch K, Karamcheti S, Xiao T, Balakrishna \nA, Nair S, et al. OpenVLA: an open-source vision-\nlanguage-action model [Internet]. Ithaca (NY): arXive.\norg; 2024 [cited at 2025 Apr 15]. Available from: https://\narxiv.org/abs/2406.09246."
}