{
  "title": "Multimodal Neural Machine Translation Using CNN and Transformer Encoder",
  "url": "https://openalex.org/W2934881944",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5042033475",
      "name": "Hiroki Takushima",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A5102926384",
      "name": "Akihiro Tamura",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A5019511958",
      "name": "Takashi Ninomiya",
      "affiliations": [
        "Ehime University"
      ]
    },
    {
      "id": "https://openalex.org/A5050229964",
      "name": "Hideki Nakayama",
      "affiliations": [
        "Ehime University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2903343986",
    "https://openalex.org/W2593341061",
    "https://openalex.org/W2513263213",
    "https://openalex.org/W2581101319",
    "https://openalex.org/W2345720230",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2889545026",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963909453"
  ],
  "abstract": "Multimodal machine translation uses images related to source language sentences as inputs to improve translation quality. Previous multimodal Neural Machine Translation (NMT) models, which incorporate visual features of each image region into an encoder for source language sentences or an attention mechanism between an encoder and a decoder, cannot catch the relation between visual features from each image region. This paper proposes a new multimodal NMT model, which encodes an input image using a Convolutional Neural Network (CNN) and a Transformer encoder. In particular, the proposed image encoder first extracts visual features from each image region using a CNN, and then encodes an input image on the basis of the extracted visual features using a Transformer encoder, where the relation between visual features from each image region are captured by a self-attention mechanism of the Transformer encoder. The experiments on the English-German translation task using the Multi30k data set show that the proposed model achieves 0.96 BLEU points improvement against a baseline Transformer NMT model without image inputs and 0.47 BLEU points improvement against a baseline multimodal Transformer NMT model without a Transformer encoder for images.",
  "full_text": null,
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7796139717102051
    },
    {
      "name": "Computer science",
      "score": 0.7683686017990112
    },
    {
      "name": "Machine translation",
      "score": 0.7557339668273926
    },
    {
      "name": "Transformer",
      "score": 0.7128279209136963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6524981260299683
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5635719299316406
    },
    {
      "name": "Computer vision",
      "score": 0.43914830684661865
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4102453589439392
    },
    {
      "name": "Speech recognition",
      "score": 0.3816792964935303
    },
    {
      "name": "Voltage",
      "score": 0.13652446866035461
    },
    {
      "name": "Engineering",
      "score": 0.08239659667015076
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}