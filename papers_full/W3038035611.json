{
  "title": "exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models",
  "url": "https://openalex.org/W3038035611",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2112805190",
      "name": "Benjamin Hoover",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2499608054",
      "name": "Hendrik Strobelt",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2580156231",
      "name": "Sebastian Gehrmann",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2912206855",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2968210605",
    "https://openalex.org/W2952682849",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2909620036",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2972568911",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2968219088"
  ],
  "abstract": "Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques. To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process. exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets. By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 187–196\nJuly 5 - July 10, 2020.c⃝2020 Association for Computational Linguistics\n187\nEXBERT: A Visual Analysis Tool to Explore\nLearned Representations in Transformer Models\nBen Hoover\nIBM Research\nMIT-IBM Watson AI Lab\nHendrik Strobelt\nIBM Research\nMIT-IBM Watson AI Lab\n{benjamin.hoover,hendrik.strobelt}@ibm.com\ngehrmann@seas.harvard.edu\nSebastian Gehrmann\nHarvard SEAS\nAbstract\nLarge Transformer-based language models\ncan route and reshape complex information via\ntheir multi-headed attention mechanism. Al-\nthough the attention never receives explicit su-\npervision, it can exhibit recognizable patterns\nfollowing linguistic or positional information.\nAnalyzing the learned representations and at-\ntentions is paramount to furthering our under-\nstanding of the inner workings of these models.\nHowever, analyses have to catch up with the\nrapid release of new models and the growing\ndiversity of investigation techniques. To sup-\nport analysis for a wide variety of models, we\nintroduce EXBERT, a tool to help humans con-\nduct ﬂexible, interactive investigations and for-\nmulate hypotheses for the model-internal rea-\nsoning process. EXBERT provides insights\ninto the meaning of the contextual represen-\ntations and attention by matching a human-\nspeciﬁed input to similar contexts in large an-\nnotated datasets. By aggregating the annota-\ntions of the matched contexts, EXBERT can\nquickly replicate ﬁndings from literature and\nextend them to previously not analyzed mod-\nels.\n1 Introduction\nLearned contextualized representations of a neu-\nral network can contain meaningful information.\nUncovering this information plays a vital role in\nunderstanding and interpreting the learned struc-\nture of neural networks (Belinkov and Glass, 2019).\nOne way to identify information is to probe the\nrepresentations by using them as features in classi-\nﬁers for linguistic tasks, or by identifying contexts\nthat lead to similar patterns (Tenney et al., 2019b;\nConneau et al., 2018; Strobelt et al., 2017).\nWith Transformers (Vaswani et al., 2017) over-\ntaking recurrent models as the primary architec-\ntures for many NLP tasks, analyzing attention has\nbecome another common strategy for interpretabil-\nity (Raganato and Tiedemann, 2018a; Clark et al.,\n2019). These efforts focus on selecting a model,\nsuch as BERT (Devlin et al., 2019), and exploring\nthe Transformer’s contextual embeddings and atten-\ntions across layers to determine whether and where\nit learns to represent linguistic features. Previous\nstudies have uncovered speciﬁc attention heads that\nlearn particular dependencies (Vig and Belinkov,\n2019; Clark et al., 2019).\nHowever, once the standard linguistic probing\ntasks are exhausted, it is challenging to develop\nnew hypotheses to test. Toward that end, interac-\ntive visualizations provide a successful strategy to\ndevelop new insights and strategies. Visualization\ntools can offer concise summaries of useful infor-\nmation and allow interaction with large models.\nAttention visualizations have thus taken signiﬁcant\nsteps toward these goals of making explorations\nfast and interactive for the user (Vig, 2019). How-\never, interpreting attention patterns without under-\nstanding the attended-to embeddings, or relying on\nattention alone as a faithful explanation, can lead\nto faulty interpretations (Brunner et al., 2019; Jain\nand Wallace, 2019; Wiegreffe and Pinter, 2019; Li\net al., 2019).\nTo address this challenge, we developed\nEXBERT , a tool that combines the advantages of\nstatic analyses with a dynamic and intuitive view\ninto both the attentions and internal representations\nof the underlying model. EXBERT provides these\ninsights for any user-speciﬁed model and corpus by\nprobing whether the representations capture mean-\ningful information. We demonstrate that EXBERT\ncan replicate insights from the analysis by Clark\net al. (2019) and easily extend it to other mod-\nels. It is open-source, extensible, and compati-\nble with many current Transformer architectures,\nboth autoregressive and masked language models.\nEXBERT is available at exbert.net.\n188\nFigure 1: An overview of the different components of the tool. Users can enter a sentence in (a) and modify the\nattention view through selections in (b). Self attention is displayed in (c), with attentions directed as coming from\nthe left column and pointing to the right. The blue matrix on the left shows a head’s attention (column) out of a\ntoken (row), whereas the right-hand matrix shows attention into each token by each head. The top-k predictions\nfor each token are shown on hover in the gray box. The most similar tokens to the MASKed “escape” token in (c)\nare shown and summarized in (d-g), taken from an annotated corpus (shown: Wizard of Oz). Every token in (d)\ndisplays its linguistic metadata on hover. The metadata of the results in (d) are summarized in the histograms (f)\nand (g) for the matched token (green highlight) and the token of max attention. The colored bars on the histogram\ncorrespond to colors in the columns of (e), where the center column summarizes the metadata of the matched token,\nand the adjacent columns represent the metadata of the words to the left and right of the matched token.\n2 Background\n2.1 Transformer Models\nThe Transformer architecture, as deﬁned by\nVaswani et al. (2017), relies on multiple sequential\napplications of self attentionlayers. Self-attention\nis the process by which each token within an in-\nput sequence Y of length N computes attention\nweights over all tokens in the input. As part of this\nprocess, the inputs are projected into a key, query,\nand value representation with Wk, Wq, and Wv.\nThe Transformer applies I of these attention heads\nin parallel, using separate weights. We denote each\nhead with the superscript (i).\nA(i) = softmax\n(\n(Y W(i)\nq )(Y W(i)\nk )⊤\n)\n.\nThis computation yields a matrix in RN×N where\nthe entry Aij represents the attention out of token\nyi into token yj.1 The representation for each at-\ntention head h(i) is then multiplied by the value,\nh(i) = A(i)(Y W(i)\nv ).\nThe representations h(1), . . . , h(I) are concatenated\nand followed by a linear projection layer. The out-\nput of this projection we call the token embedding\nE(l), which is used as input to layer l + 1.\n2.2 Transformer Analysis\nThe analysis of learned contextual representation\nin neural networks has been a widely investigated\ntopic in NLP (Belinkov and Glass, 2019). Be-\nfore the advent of large pretrained models, anal-\nyses focused on models trained for speciﬁc tasks\nlike machine translation. Some showed that Trans-\nformer models, similar to recurrent models, can\n1For autoregressive models like GPT-2 (Radford et al.,\n2019), this matrix is triangular since attention cannot point\ntoward unseen tokens.\n189\neffectively encode syntactic properties in their rep-\nresentations (Raganato and Tiedemann, 2018b;\nMareˇcek and Rosa, 2018). Researchers have devel-\noped suites of probing techniques, agnostic to the\nunderlying model, that can capture these proper-\nties across many different linguistic tasks (Tenney\net al., 2019b; Conneau et al., 2018). Over the past\nyear, similar tests have primarily been applied to\nBERT (Devlin et al., 2019) and its derivatives (e.g.,\nSanh et al., 2019; Liu et al., 2019). Similar to task-\nspeciﬁc models, Goldberg (2019) found that BERT\nclearly encodes syntax within some of its attentions.\nMoreover, Tenney et al. (2019a) demonstrated that\nlinguistic information is very localized within the\nrepresentations in different layers.\nIn parallel, individual attention heads of Trans-\nformer models have also received much focus.\nClark et al. (2019) showed that individual heads\nrecognize standard Part of Speech (POS) and De-\npendency (DEP) relationships (e.g., Objects of the\nPreposition (POBJ) and Determinants (DET)) with\nhigh ﬁdelity. Vig and Belinkov (2019) also ex-\nplored the dependency relations across heads and\ndiscovered that initial layers typically encode posi-\ntional relations, middle layers capture the most de-\npendency relations, and later layers look for unique\npatterns and structures. These insights are exposed\ninteractively through EXBERT.\n3 Overview\nEXBERT focuses on displaying a succinct view of\nboth the attention and the internal representations\nof each token. Figure 1 shows an overview of the\ntool’s two main components. TheAttention View\nprovides an interactive view of the self-attention of\nthe model, where users can change layers, select\nheads, and view the aggregated attention. The Cor-\npus View presents a user with aggregate statistics\nthat aim to describe and summarize the hidden rep-\nresentations of a currently selected token or set of\nattention heads. For simplicity, the tool defaults to\nfocus on single-sentence examples.\n3.1 Attention View\nThe attention A can be understood as an adjacency\nmatrix, which is conducive to a representation of\ncurves pointing from each token to every other\ntoken. However, since A is not symmetric, a visu-\nalization has to separate the outgoing and incoming\nattention of a token. We achieve this by duplicat-\ning the tokens of input Y and presenting it in two\nvertical sections, connected through the attention.\nHovering over a token will reduce the displayed\nattention graph to the incoming/outgoing attention\nof that token. We display the top predictions of the\nmodel at that position. Clicking on a token freezes\nthe ﬁltered attention view.\nMany models introduce special tokens (e.g.,\n“[CLS]”, “<|endoftext|>”) for downstream classi-\nﬁcation or generation tasks. These tokens often\nreceive very high attention and act as a null oper-\nation (Clark et al., 2019). We provide a switch to\nhide the special tokens of the model and renormal-\nize based on the other attentions to provide easier\nvisualization of subtle attention patterns.\n3.2 Corpus View\nRepresentations, on the other hand, cannot be eas-\nily visualized footnoteSee Strobelt et al. (2017) for\na discussion why heat-maps are not an appropriate\nvisualization of hidden states. but they can be un-\nderstood by searching for similar representations\nin an annotated corpus. The results of this search\nare presented in the Corpus View with the highest-\nsimilarity matches shown ﬁrst. The histograms\ndisplay the accumulated features of the matched\nrepresentations and the token that receives the most\nattention.\nSearching Inspired by Strobelt et al. (2017,\n2018), EXBERT performs a nearest neighbor\nsearch of embeddings on a reference corpus as\nfollows. A corpus is ﬁrst split by sentence and\nits tokens labeled for desired metadata (e.g., POS,\nDEP, NER). The model then processes this corpus,\nand its embeddings E(l) are stored at every layer\nl and indexed for a Cosine Similarity (CS) search\nusing faiss (Johnson et al., 2019). The top 50 most\nsimilar tokens matching a query embedding are dis-\nplayed and summarized for the user in the context\nof their use in the annotated corpus.\nTo supplement the layer embeddings E(l) and\nenable exploration of the attention heads, we derive\na Context EmbeddingC(l), which we deﬁne as the\nconcatenation of heads before the linear projection\nat the layer’s output. Formally, this is deﬁned as:\nC(l) = Concat(˜h\n(l,1)\n, . . . ,˜h\n(l,n)\n),\nwhere ˜h\n(l,i)\nis deﬁned as the L2 normalized rep-\nresentation of head i at layer l to enable CS search-\ning by head. To search the corpus for any subset of\n190\nheads Hs ⊆{1, . . . , n}, we set all values of ˜h\n(l,i)\nto 0 in Cl, where i /∈Hs.\nBidirectional vs. Autoregressive Behavior\nEXBERT is ﬂexible to accommodate both bidi-\nrectional and autoregressive Transformer architec-\ntures, but the tool behaves slightly differently for\neach. Bidirectional models have histogram sum-\nmaries for the nearest neighbor matches across the\ncorpus and allow interactive MASKing of tokens.\nWhen hovering over any token, the interface will\nshow what the language model would predict at\nthat token.\nAutoregressive models will also search for the\nnearest neighbors to a selected token’s embedding,\nbut the interface will instead summarize the meta-\ndata of the following token (indicated in red font).\nHovering over any token in the Attention View\nwill display what the model would predict next.\n3.3 Extending EXBERT\nEXBERT runs Huggingface’s uniﬁed API for\nTransformer models (Wolf et al., 2019) which al-\nlows any Transformer model from that API to take\nfull advantage of the Attention View.\nSimilarity searching requires the user to ﬁrst an-\nnotate a corpus with the desired model. Scripts to\naid annotation of a corpus from a custom model is\nprovided in the code repository.2\nTo display metadata from a corpus in a cus-\ntom domain, users will need to align the trans-\nformer model’s tokenization scheme to extracted\nmetadata (e.g., DNA Sequences and their proper-\nties). EXBERT accomplishes this by ﬁrst tokeniz-\ning, normalizing, and labeling the sentence with\nspaCy (Honnibal and Montani, 2017). If these\ntokens are split further by the Transformer’s to-\nkenization scheme, each word-piece receives the\nmetadata of its parent token. Note that special\ntokens like “[CLS]” and “<|endoftext|>” have no\nlinguistic features assigned to them.\n4 Case Study: BERT\nClark et al. (2019) performed an extensive analysis\nto determine which heads in a base sized BERT\nTransformer model learned which dependencies.\nWe show here how some of their insights are eas-\nily accessible through the EXBERT interface (De-\nvlin et al., 2019) for the case-sensitive BERT-base\nmodel, which has 12 layers and 12 heads per layer.\n2https://github.com/bhoov/exbert.\nFigure 2: Exploration of different attention heads for\npretrained model BERT base and different corpora. (a)\nshows head 5-3 expecting looks at the presents of an\nauxiliary verb (AUX) to predict that the MASK should\nbe a verb. Head 7-5 in (b) shows a head that has learned\nto attend to Objects of the Preposition (POBJ). Finally,\n(c) shows Head 5-5 learning correct co-reference.\nWe use the notation <layer>-<head> to refer to a\nsingle head at a single layer, and <layer>-[<heads>]\nto describe the cumulative attention of heads at a\nlayer (e.g., 4-[1,3,9] to describe the aggregated at-\ntention of heads 1, 3, and 9 at layer 4).\n4.1 Behind the Heads\nFigure 2 shows examples where distinct heads learn\nevident linguistic features. Figure 2a shows that\nthe MASKed verb “escape” points to the auxiliary\nverb (AUX) “to”. If we search over the annotated\nWizard of Oz3, we see that the tokens matching\nthe MASK’s most similar contexts at Head 5-3 are\nverbs and that the attention out of these matched\nwords goes primarily to an AUX dependency.\nFigure 2b shows that Head 7-5 ﬁnds relation-\nships between prepositions (PREP) and their ob-\njects (POBJ) in the input sentence. By searching\nfor the token “in” across a subset of the “Wikipedia”\ncorpus (Merity et al., 2016), we conﬁrm that many\n3http://www.gutenberg.org/ebooks/55\n191\nFigure 3: A progression of the information encoded by a nearest neighbor embedding (left) and context (right)\nsearches for the MASKed token “escape” in Figure 2a and the sentence, “The girl ran to a local pub to escape the\ndin of her city.” Note that heads encode verb information (dark green) signiﬁcantly earlier than the embeddings.\nother annotated sentences exhibit this pattern.\nFigure 2c seemingly ﬁnds a head that determines\nco-reference to entity relationships, as both “she”\nand “her” are pointing strongly at “Kim” and little\nto everything else. Because the parse tree is absent\nin the annotated corpus, we are unable to search for\nco-reference patterns. However, the corpus search\ndoes reveal that this head learns to match pronouns\nto Entities rather than common gendered words\nsuch as “woman” or “mother”.\n4.2 Behind the Mask\nEarlier layers of a BERT model can capture partic-\nular linguistic information (Clark et al., 2019; Vig\nand Belinkov, 2019). We now explore this behavior\nfor a MASKed token across layers. We look at the\nfollowing sentence, also shown in Figure 2a:\nThe girl ran to a local pub to escape the din of\nher city.\nWe begin by masking the “escape” token in the\nexample sentence at layer 1 and search what infor-\nmation is behind the “[MASK]” token’s embedding\n(Figure 1). Note that at this early layer, there is\nno meaningful linguistic information encoded in a\nMASK token’s embedding, and the matching em-\nbeddings are most similar to punctuation (PUNCT)\nand determinants (DET), which are the most com-\nmon tokens in English (Figure 1f). Additionally,\nthe maximum attention out of the MASKed token\npoints to itself (Figure 1c).\nAs layers progress, more VERB information is\nencoded in the token’s embedding, as shown in\nFigure 3. At layer 6, the model does not relate the\nMASKed word to verbs, but by layer 9 it is con-\nvinced that the MASK should be a verb. Note that\naccumulated head information conﬁdently captured\na “verb” pattern in a signiﬁcantly earlier layer.\n5 Case Study: GPT-2\n5.1 Gender Bias\nWe now use EXBERT to explore the problem\nof gender bias and co-reference in autoregressive\nTransformers (Zhao et al., 2018), a problem in-\nherent in the training data that infects the model’s\nunderstanding of language (Font and Costa-jussà,\n2019). Take the following sentence:\nThe man visited the nurse and told him to attend\nto his patients.\nWe aim to detect whether the model thinks\n“nurse” is male or female before it sees the mascu-\nline pronoun “him” referring to “nurse”. Because\nGPT-2 is trained to predict the next word, we can\ndo this by selecting the token “told” and hovering\nover it to see the prediction of that pronoun. These\nresults are shown in Figure 4a, and from the proba-\nbilities, we can see that GPT-2 predicts “her” with\n90% probability. The next closest token “him” is\nonly 6%. Figure 4b shows that replacing “nurse”\nwith “doctor” alters the prediction to be strongly in\nfavor of predicting “him” at 68% probability, while\n“her” falls to 18%. The attention patterns in the\nﬁnal three layers remain ostensibly the same for\nboth sentences.\n5.2 Heads up\nIn contrast to BERT, GPT-2 is an autoregressive\nlanguage model. This makes it more difﬁcult to\ndetect some dependencies by looking at attention\npatterns (e.g., PREP looking for its POBJ in the fu-\nture). However, EXBERT can offer similar insights\nas above using slightly altered methods. The fol-\nlowing experiments use the smaller conﬁguration\nof GPT-2 with 12 layers and 12 heads (Radford\net al., 2019).\nExploring the heads in GPT-2 reveals that GPT-\n192\nFigure 4: Highlights the bias of the GPT-2 model for generation. (a) “nurse” prompts the model to predict “her”.\n(b) shows “doctor” causing the model to predict “him”\nFigure 5: Discovering simple head patterns in GPT-\n2 using the sentence. (a) shows strong detection of\nthe AUX dependency. (b) shows a head detecting the\nDOBJ dependency\n2’s heads also learn distinct syntactic structure. Fig-\nure 5a shows a few heads at different layers that\nseemingly learn the AUX dependency. Heads at\nearlier layers show an afﬁnity for the AUX pat-\ntern, but also confuse “to” with a preposition even\nthough a verb directly follows. This behavior hints\nthat these heads look primarily to match the word\n“to” rather than its contextual meaning.\nSimilarly, Figure 5b shows a head that attends\npredominantly to a preceding verb and matches\ncontexts in which the following word is a DOBJ. In-\nterestingly, the more complex DOBJ dependency is\npicked up by a head as early as layer 5-12, whereas\na simpler dependency like the AUX pattern is only\nclearly detected later in Layer 8.\n6 Discussion\nIn this paper, we introduced an interactive visualiza-\ntion tool, EXBERT, that can reveal an intelligible\nstructure in the learned representations of Trans-\nformer models. We demonstrated, through an atten-\ntion visualization and nearest neighbor searching\ntechniques, that EXBERT can replicate research\nthat explores what attentions and representations\nlearn and detect biases in text inputs.\nWe acknowledge that EXBERT is limited com-\npared to more global analyses since it only presents\nstatistics across a small number of neighbors for a\nsingle token at a time. These neighbors do not nec-\nessarily reveal a head’s or an embedding’s global\nbehavior. However, EXBERT can effectively nar-\nrow the scope and reﬁne hypotheses through quick\ntesting iterations. These hypotheses about the\nmodel behavior can, in a later step, be evaluated by\nrobust statistical tests on a global level.\nTo assist researchers with their model investi-\ngations, we host a demo of the tool with multiple\nmodels at exbert.net.\n193\nReferences\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nGino Brunner, Yang Liu, DamiÃ ˛ an Pascual, Oliver\nRichter, and Roger Wattenhofer. 2019. On the va-\nlidity of self-attention as explanation in transformer\nmodels.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of bert’s attention. CoRR,\nabs/1906.04341.\nAlexis Conneau, Germán Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2126–2136.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nJoel Escudé Font and Marta R. Costa-jussà. 2019.\nEqualizing gender biases in neural machine trans-\nlation with word embeddings techniques. CoRR,\nabs/1901.03116.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nSarthak Jain and Byron C Wallace. 2019. Attention is\nnot explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1293–1303, Florence,\nItaly. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDavid Mareˇcek and Rudolf Rosa. 2018. Extracting syn-\ntactic trees from transformer encoder self-attentions.\nIn Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 347–349.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nAlessandro Raganato and Jorg Tiedemann. 2018a. An\nanalysis of encoder representations in transformer-\nbased machine translation. In EMNLP Workshop:\nBlackboxNLP.\nAlessandro Raganato and Jörg Tiedemann. 2018b. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nHendrik Strobelt, Sebastian Gehrmann, Michael\nBehrisch, Adam Perer, Hanspeter Pﬁster, and\nAlexander M. Rush. 2018. Seq2seq-vis: A vi-\nsual debugging tool for sequence-to-sequence mod-\nels. CoRR, abs/1804.09299.\nHendrik Strobelt, Sebastian Gehrmann, Hanspeter Pﬁs-\nter, and Alexander M Rush. 2017. LSTMVis: A tool\nfor visual analysis of hidden state dynamics in recur-\nrent neural networks. IEEE transactions on visual-\nization and computer graphics, 24(1):667–676.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBert rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\n194\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. A multiscale visualization of attention\nin the transformer model. CoRR, abs/1906.05714.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. CoRR, abs/1906.04284.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, RÃl’mi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Transformers: State-of-\nthe-art natural language processing.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n195\nA Recreating the experiments\nWe allow direct linking to an experimental setup in\nthe interface. A list of the links to reproduce our\nresults is given below (all links in the supplemen-\ntary material are correct at the time of publishing,\nbut may be changed in the distant future):\n•Overview (Figure 1):\nhttps://bit.ly/2OfD6Vt\n•Behind the Heads (Figure 2)\n– (a): https://bit.ly/2GJUihs\n– (b): https://bit.ly/38Ycss8\n– (c): https://bit.ly/2S8qGzO\n•Behind the Mask (Figure 3):\nhttps://bit.ly/2RJ952n\n•GPT-2 Bias (Figure 4):\nhttps://bit.ly/36ELwMo\n•Heads Up (Figure 5):\n– (a): https://bit.ly/2vAcgRe\n– (b): https://bit.ly/2S9qHDs\nB Additional Material\nIn addition to the content presented in the main pa-\nper, we have recorded a shortvideo demo showing\nhow to use the tool to probe for particular patterns\nat https://youtu.be/e31oyfo_thY.\nA Lite version of the tool, without the corpus\nsearching, demoing many common Transformer\nmodels is hosted by Huggingface at huggingface.\nco/exbert.\n196\nC Additional ﬁgures\nFigure 6: The most similar embeddings, in context, to the MASKed token “escape” in the sentence: “The girl ran\nto a local pub to escape the din of her city” at the output of layer 12 of BERT base (shown in Figure 2a). Corpus\nresults are annotated excerpts from the Wizard of Oz. Notice how at the output layer all attentions are primarily\nto the word itself or the ﬁnal punctuation mark of the sentence, indicating that the most important information is\nlikely already encoded in the selected token’s embedding.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7551617622375488
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6331628561019897
    },
    {
      "name": "Replicate",
      "score": 0.619406521320343
    },
    {
      "name": "Transformer",
      "score": 0.6081498265266418
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49968719482421875
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4703286588191986
    },
    {
      "name": "Visual reasoning",
      "score": 0.46262407302856445
    },
    {
      "name": "Process (computing)",
      "score": 0.44308215379714966
    },
    {
      "name": "Natural language processing",
      "score": 0.4371935725212097
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39398324489593506
    },
    {
      "name": "Engineering",
      "score": 0.09484556317329407
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    }
  ],
  "cited_by": 142
}