{
  "title": "Capturing Formulation Design of Battery Electrolytes with Chemical Large Language Model",
  "url": "https://openalex.org/W4392785478",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2162928091",
      "name": "Eduardo Soares",
      "affiliations": [
        "IBM Research - Brazil"
      ]
    },
    {
      "id": "https://openalex.org/A2148618935",
      "name": "Vidushi Sharma",
      "affiliations": [
        "IBM Research - Almaden"
      ]
    },
    {
      "id": "https://openalex.org/A2089763030",
      "name": "Emilio Vital Brazil",
      "affiliations": [
        "IBM Research - Brazil"
      ]
    },
    {
      "id": "https://openalex.org/A2933775134",
      "name": "Young-Hye Na",
      "affiliations": [
        "IBM Research - Almaden"
      ]
    },
    {
      "id": "https://openalex.org/A2062298793",
      "name": "Renato Cerqueira",
      "affiliations": [
        "IBM Research - Brazil"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3103092523",
    "https://openalex.org/W4287117648",
    "https://openalex.org/W6603230239",
    "https://openalex.org/W4298009900",
    "https://openalex.org/W2911954305",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3196617904",
    "https://openalex.org/W3195990607",
    "https://openalex.org/W4226197958",
    "https://openalex.org/W4322489988",
    "https://openalex.org/W3202215172",
    "https://openalex.org/W3177704951",
    "https://openalex.org/W6756821666",
    "https://openalex.org/W3209721572",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4384616153",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W4313485929",
    "https://openalex.org/W4312129726",
    "https://openalex.org/W4377096651",
    "https://openalex.org/W2518099942",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4375856114",
    "https://openalex.org/W4388560598",
    "https://openalex.org/W4318952054",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W4366176916",
    "https://openalex.org/W4317757475",
    "https://openalex.org/W4318070558",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W4382404016",
    "https://openalex.org/W4324107047",
    "https://openalex.org/W4318486595",
    "https://openalex.org/W4213070269",
    "https://openalex.org/W3185391990",
    "https://openalex.org/W3183622852",
    "https://openalex.org/W4246726178",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W4366769286",
    "https://openalex.org/W2883583109"
  ],
  "abstract": "<title>Abstract</title> Recent progress in large transformers-based foundation models have demonstrated impressive capabilities in mastering complex chemical language representations. These models show promise in learning task-agnostic chemical language representations through a two-step process: pre-training on extensive unlabeled corpora and fine-tuning on specific downstream tasks. By utilizing self-supervised learning capabilities, foundation models have significantly reduced the reliance on labeled data and task-specific features, streamlining data acquisition and pushing the boundaries of chemical language representation. However, their practical implementation in further downstream tasks is still in its early stages and largely limited to sequencing problems. The proposed multimodal approach using MoLFormer, a chemical large language model, aims to demonstrate the capabilities of transformer based models to non-sequencing applications such as capturing design space of liquid formulations. Multimodal MoLFormer utilizes the extensive chemical information learned in pre-training from unlabeled corpora for predicting performance of battery electrolytes and showcases superior performance compared to state-of-the-art algorithms. The potential of foundation models in designing mixed material systems such as liquid formulations presents a groundbreaking opportunity to accelerate the discovery and optimization of new materials and formulations across various industries.",
  "full_text": "Capturing Formulation Design of Battery\nElectrolytes with Chemical Large Language Model\nEduardo Soares \nIBM Research - Brazil\nVidushi Sharma \nIBM Research - Almaden\nEmilio Vital Brazil \nIBM Research - Brazil\nYoung-Hye Na \nIBM Research - Almaden\nRenato Cerqueira \nIBM Research - Brazil\nArticle\nKeywords:\nPosted Date: February 2nd, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3593035/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nCapturing Formulation Design of Battery Electrolytes\nwith Chemical Large Language Model\nEduardo Soares1, Vidushi Sharma 2, Emilio Vital Brazil1,*, Y oung-Hye Na2,+, and Renato\nCerqueira1,+\n1 IBM Research, Rio de Janeiro, Brazil\n2 IBM Research, Almaden, US\n* evital@br.ibm.com\n+ these authors contributed equally to this work\nABSTRACT\nRecent progress in large transformers-based foundation models have demonstrated impressive capabilities in mastering\ncomplex chemical language representations. These models show promise in learning task-agnostic chemical language\nrepresentations through a two-step process: pre-training on extensive unlabeled corpora and ﬁne-tuning on speciﬁc downstream\ntasks. By utilizing self-supervised learning capabilities, foundation models have signiﬁcantly reduced the reliance on labeled\ndata and task-speciﬁc features, streamlining data acquisition and pushing the boundaries of chemical language representation.\nHowever, their practical implementation in further downstream tasks is still in its early stages and largely limited to sequencing\nproblems. The proposed multimodal approach using MoLFormer, a chemical large language model, aims to demonstrate the\ncapabilities of transformer based models to non-sequencing applications such as capturing design space of liquid formulations.\nMultimodal MoLFormer utilizes the extensive chemical information learned in pre-training from unlabeled corpora for predicting\nperformance of battery electrolytes and showcases superior performance compared to state-of-the-art algorithms. The potential\nof foundation models in designing mixed material systems such as liquid formulations presents a groundbreaking opportunity\nto accelerate the discovery and optimization of new materials and formulations across various industries.\nIntroduction\nIn recent times, the ﬁeld of large transformers-based foundation models has made remarkable progress, showcasing their\nimpressive capacity to master complex chemical language representations\n1– 3 . This machine learning approach has become\nwidely adopted for accurately predicting molecular properties due to its efﬁciency and ability to represent essential molecular\nfeatures4 . The said achievement of large chemical language models is just a tip of an iceberg that represents their immense\nscope and potential 5 . The applications of such models need to be further explored beyond the domain of molecules, towards\nmore complex design spaces such as formulations.\nRecent advancements in these models have shown great promise in learning task-agnostic chemical language representations\nthrough a two-step process: pre-training on extensive unlabeled corpora and ﬁne-tuning on speciﬁc downstream tasks 6– 8 . The\nsigniﬁcance of this achievement cannot be overstated 9 . By utilizing self-supervised learning capabilities, these foundation\nmodels have effectively reduced the dependence on labeled data and task-speciﬁc features 7, 10 , streamlining the once laborious\ndata acquisition process and propelling the ﬁeld of chemical language representation to new heights 11– 13 . Although pre-trained\nLanguage Models (LMs) have shown promise in predicting molecular properties 14– 16 , their practical implementation on further\ndownstream tasks is still in its early stages 17 and largely limited to sequencing problems such as predicting sequences of\nproteins18 , polymers 19 and chemical reactions 20 . The question remains: Could LMs be used to represent molecular systems\nwith unstructured and random interactions like the ones observed in liquid formulations?\nLiquid formulations are a big part of several industrial sectors like pharmaceuticals, automotive materials, and food\nscience21 . Current strategies to design formulations rely on high-throughput virtual screening that expedites the search for\nindividual compounds but falls short in guiding the complete design of materials’ formulations 22 . Battery liquid electrolytes\nare economically relevant examples of a formulation system, where comprehending and optimizing the interdependencies\nof constituent solvents and salts is of paramount importance for device performance\n23 . Despite the exponential growth of\nthe energy storage ﬁeld in the last two decades, the cycling stability of current battery technologies continues to remain in\nquestion24 . Electrolyte engineering has emerged to be a promising approach to improve the cycling efﬁciency of next generation\nbatteries, and remains generally an experimentally driven process. The major bottleneck in adopting machine learning methods\nfor electrolyte design discovery and optimization is the non-generalizability of the battery-speciﬁc datasets available in the\nliterature and the expensive data acquisition process 25 .\nLMs can play a crucial role in bridging this gap, as their self-supervised capabilities align perfectly with scenarios where\ndata availability is critical 26, 27 . In this paper, we propose a multimodal approach built upon the recently introduced LM\nMoLFormer to predict the performance of battery electrolyte formulations. Unlike recently introduced chemical LMs that\npredict properties based on a single molecule identiﬁer (SMILES\n16, 28 , SELFIES 29, 30 , etc. 31 ), our approach takes up to six\nSMILES molecules as input, representing the constituents of the formulations along with their respective molar percentages,\nthus capturing the composition of the design space (see Fig. 1). T o assess the effectiveness of our approach, we utilize a Li/Cu\nhalf-cell (Fig. 2a) dataset from a previous study 32 , which serves as a benchmark for our proposed methodology . Our approach\nhas demonstrated superior performance compared to state-of-the-art algorithms, eliminating the need for laborious human\nfeature engineering processes and extensive experimental data acquisition.\nElectrolyte Formula CE (%)\n1 M LiClO4-PC + 5% FEC 80\n1 M\n  LiPF6 EC-DMC (1:1 v)\n2% VC\n80\n... ...\n1 M LiPF6 FEC-EC-DEC\n(2:9:9 vol) 0.05 M RbNO3\n0.05M 18-crown-6\n94.4\nSMILES 1 composition 1 ... SMILES 6 composition 6 LCE\nCC1COC(=\nO)O1 0.875 ... O 0.00 0.699\n... ... ... ... ... ...\nC1C(OC(=O\n)O1)F 0.106 ...\nO1CCOCC\nOCCOCCO\nCCOCC1\n \n0.387 1.252\nElectrolyte Formula\ndecomposition\nTraining Augmented\ndataset \n \nMultiModal \nMoLFormer\nMultiModal \ntraining using SMILES\n combination \nand composition \npercentages\nLCE\nprediction\na\nb\nc\nd\ne\nDataset with SMILES (up to six) and compositions percentages \nMixture\nFormula\nThe order of the SMILES that \ncomposes the formula is irrelevant\nLCE prediction\nbased on a \ncombination of  SMILES \nand composition \npercentages\nOriginal dataset\nFigure 1. The ﬁgure illustrates the general architecture of the learning process of the MultiModal-MoLFormer. ( a) Electrolyte\nformulation dataset. Illustrated by the example, the sequence which comprises electrolytes formulations along with their\nCoulombic Efﬁciency . b Description of electrolyte formulations as input. Each formulation is composed of a sequence of up to\nsix SMILES along with their fraction of molar percentages and respective logarithmic Coulombic Efﬁciency as performance\nlabel. c Dataset augmentation in order to enrich the training of the model. d Training of the proposed MultiModal-MoLFormer\napproach with the sequence of SMILES and compositions percentages. e Prediction of the Coulombic efﬁciency property\nbased on mixture formulation (SMILES and compositions).\nOur work demonstrates the potential of foundation models for the design of mixed material systems. By leveraging the\npower of machine learning, we can accelerate the discovery and optimization of new materials and formulations, with the\npotential to revolutionize a wide range of industries.\nElectrolyte Formulation Dataset\nT o evaluate this proposed methodology , we assessed its efﬁcacy in tackling a challenging task. W e considered the Li/Cu half\ncell-based electrolyte formulations and their respective Coulombic Efﬁciencies (CE). This dataset was carefully curated from\nliterature to encompass a wide range of electrolyte variations in terms of constituent molecules and compositions 32 . CE is\na crucial metric for assessing battery performance that represents the ratio of discharge to charge capacity 33 . Maintaining a\nhigh CE is essential to ensure optimal battery function. However, over time, batteries can experience CE loss that is primarily\ncaused by electrolyte and electrode decomposition 34 . The CE values have been converted to their logarithmic (LCE) by 32 to\nnumerically amplify the change in output with respect to the electrolytes. This transformation allows for a more sensitive and\naccurate comparison of the performance of different electrolytes. The dataset is composed of 147 electrolyte formulations for\ntraining purposes and 13 electrolyte formulations for model evaluation. The box plots illustrated in Fig. 2b provide a clear\n2/10\nvisualization of the distribution of LCE outputs for the training data based on the count of formulation constituents. The plot\nshowcases essential statistical insights, revealing the spread and central tendencies of the data. The formulations in the dataset\nconsist of 2 to 6 electrolyte components in each, represented as a simpliﬁed molecular-input line-entry system (SMILES)\nnotation. The presented approach allowed us to gain valuable insights into the potential applicability of LMs in device-level\npredictive applications.\n(a) Lithium -Copper half cell schematic\n (b) Distribution of performance label vs formulant count\nFigure 2. (a) Schematic representation of Lithium-Copper half cell that is conventionally used to study stability and\ncyclability of electrolyte formulations over Lithium metal anode. ( b)Box plots depicting electrolyte formulation vs LCE data as\nthe function of number of formulants in the formulations. Each box plot is constructed with attention to detail, where the\ncentral line represents the median value of the LCE outputs, indicating the middle point of the data distribution. The colored\nbox represents the interquartile range (IQR), encompassing the 25t h to 75t h percentile of the LCE values. This range provides\nvaluable information about the variability and spread of the majority of data points.\nResults\nMultiModal-MoLFormer Framework\nOur MultiModal-MoLFormer framework for formulations consists of tokenization, compositional embeddings, and learning.\nEach formulation in the dataset has up to 6 constituents that are represented by SMILES notations. The molecular SMILES\nare tokenized and then transformed using a pre-trained LM consisting of Transformer layers (Fig.\n3). This produces special\nembeddings for each chemical language string. Next, the chemical embeddings are concatenated along with their corresponding\nmolar percentages in the formulations indicated in the dataset. Presently , the method takes up to six SMILES and the respective\npercentages of the formulations as input, which are concatenated using a special token <SEP>. It is important to highlight that\nthe original MoLFormer approach 7 was designed to receive only one SMILE per time, and it is not able to cope with external\nfeatures as the percentages of the formulations.\nThe order of the SMILES does not matter in the formulation. Thus, special attention is given to ensuring that model does\nnot learn any unnecessary patterns such as the sequence of formulation constituents in the dataset. Thus the formulation dataset\nis augmented with all possible permutations of the constituent SMILES order during the training phase. This facilitates the\nlearning process, specially as the algorithm is based on a Transformers architecture where the order of tokens does matter to the\nsystem. The ﬁnal concatenated vector with chemical and compositional embeddings is then passed to a learning algorithm,\nspeciﬁcally a Feed-Forward network with 2 fully connected layers, to calculate the loss function. The multimodal framework\nsuccessfully fuses SMILES notations and formulation percentages, enabling accurate predictions of performance metrics.\nPerformance prediction of electrolyte formulations\nHere, we present a comparative analysis of LCE predictions using the Li/Cu half-cell dataset from two approaches: Molformer\nand MultiModal-MoLFormer. T able 1 summarizes the LCE predictions for the test dataset from MoLFormer 7 and proposed\nMultiModal-MoLFormer. The performance of each algorithm is assessed using the root mean squared error (RMSE) metric,\nwhich quantiﬁes the prediction errors. Figure 4 illustrates the parity plot prediction of LCE for all the considered algorithms.\nEach row in the table displays the predicted LCE values for individual electrolyte formulations. The numerical results\n3/10\nFigure 3. The ﬁgure illustrates the general architecture of the learning process of the MultiModal-MoLFormer. The\nconcatenation layer is responsible to combine the SMILES embeddings along with the percentages of the formulations derived\nfrom the electrolyte process.\nindicate that our method achieves signiﬁcantly lower prediction errors, as reﬂected in the RMSE calculation. Morever, as\nillustrated by Fig. 5, the residual plot demonstrates that the proposed approach has stable predictions.\nThe T able 2 compares the predictive errors of proposed models with alternative formulation models. Speciﬁcally , the\nRMSE values for each algorithm are as follows: “F-GCN TL ” achieves an RMSE of 0.389, and MoLFormer shows an RMSE\nof 0.213. In contrast, our proposed approach achieves the lowest RMSE of 0.195, demonstrating its superior predictive\ncapability . F-GCN are formulation graph convolution networks that use graph representations for representing formulation\nconstituents along with their compositions and overcome the limitations of small experimental dataset by pre-training graphs\non labeled simulation data. Thus, ’TL ’ denotes transfer learning in the F-GCN framework 35 . It is interesting to note here\nthat despite skipping over important compositional information, MolFormer still outperforms F-GCN TL model that captures\nboth structure and compositions of formulation constituents. This could be attributed to the large-scale learning of underlying\nchemical information by MolFormer from unlabeled corpora which makes it highly generalizable and property-independent.\nMeanwhile, graphs depend upon initial vectorization and speciﬁc property to learn structure-based relational latent space. Not\nonly acquiring labeled data to train graphs can be very costly , but it also renders the ability of such models to predict formulation\nproperties dependent upon pre-training labels. MultiModal-MoLFormer manages to overcome the compositional negligence of\nMoLFormer in capturing formulation design and demonstrates the best predictive capability of any algorithm report to date. The\ntable demonstrates the efﬁcacy of our multimodal chemical language approach, which just utilizes SMILES and formulation\npercentages for improved prediction accuracy . By effectively incorporating essential chemical information, our proposed\nmethod better captures the complex interactions that inﬂuence LCE in Li/Cu half-cell batteries. This enhanced predictive power\nfor a complex unstructured design space is crucial for electrolyte engineering and optimizing battery performance, thus driving\nadvancements in battery technology .\nIn summary , T able 2 clearly shows that our proposed approach outperforms other state-of-the-art algorithms in predicting\nLCE values for the Li/Cu half-cell dataset. The lower RMSE value obtained by our method underscores its potential to facilitate\nmore reliable and efﬁcient battery design, ultimately contributing to the development of sustainable and high-performance\nenergy storage solutions.\nDiscussion\nThis paper introduces an innovative multimodal chemical language-based model, speciﬁcally designed to establish intricate\nrelationships among the structural composition and performance of battery electrolytes within their formulation space. By\nseamlessly integrating both SMILES notations and the corresponding formulation percentages, our model provides a more\ncomprehensive depiction of chemical interactions.\nT o validate the efﬁcacy of our proposed approach, we conducted rigorous experiments using the Li/Cu half-cell data 32 .\nOur approach not only surpassed state-of-the-art methodologies in predicting formulation property i.e. logarithmic coulombic\nefﬁciency in this case, but also achieved an impressive RMSE of 0.195. Noteworthy is the fact that our method even\n4/10\nExperimental values\n(LCE)\n32 MoLFormer Multimodal\nMoLFormer\n1.094 1.198 1.028\n1.384 1.428 1.267\n1.468 1.340 1.336\n1.710 1.845 1.823\n1.832 1.763 1.816\n2.104 1.816 1.841\n2.274 1.809 1.897\n1.071 1.058 0.979\n1.166 1.109 0.971\n1.335 1.727 1.554\n1.129 0.982 0.810\n1.501 1.735 1.599\n1.663 1.565 1.492\nRMSE 0.213 0.195\nT able 1. Summary of logarithmic Coulombic efﬁciency (LCE) predictions from MoLFormer and Muldimodal-MoLFormer.\nThe root mean squared error (RMSE) metric was used to measure the errors of the algorithms. Each row in the table displays\nthe predicted LCE values for an individual electrolyte formulation.\nAlgorithm RMSE\nMultiModal-MoLFormer 0.195\nMoLFormer 0.213\nF-GCN TL 35 0.389\nLinear regression 32 0.585\nRandom forest 32 0.577\nBoosting32 0.587\nBagging32 0.583\nT able 2. Comparison of Coulombic Efﬁcieny prediction from different algorithms.\noutperformed the formulation graph model (F-GCN), known for its reliance on resource-intensive HUMO-LUMO features for\npre-training. This compellingly suggests that our approach excels in both efﬁciency and effectiveness compared to existing\nmethods, which rely on simpler input features such as SMILES notations and formulation percentages.\nIn contrast to other algorithms demanding resource-heavy features, our method relies exclusively on straightforward input\nfeatures, rendering it more accessible and computationally streamlined. Moreover, the residual plot (refer to Figure 5) illustrates\na narrower spread of residuals compared to the alternative formulation models. This signiﬁcant ﬁnding implies that our proposed\napproach is more resilient against noise and outliers in the data. In simpler terms, our approach demonstrates heightened\nresistance to random data ﬂuctuations.\nIt is paramount to underscore that our approach exhibits remarkable performance even when faced with challenging tasks,\ngiven the relatively modest size of the datasets employed in this study . Acquiring extensive datasets can be a resource-intensive\nendeavor, imposing constraints on data availability . Consequently , we advocate for further experimentation on larger datasets to\nholistically explore the model’s capabilities, thus validating its scalability and robustness.\nElectrolytes are a critical design black-box for batteries that aspects all major performance metrices. Electrolyte optimization\nhas taken a major frontier in climate technology , and tools to ﬁnd right high-performance electrolyte formulations will accelerate\ndecarbonization across all economic sectors. As the result, electrolyte formulations are now guarded trade secrets of battery\ncompanies that are rarely shared publicly . The most challenging aspect of development of battery electrolytes is their non-\ngeneralizability . With the presence of several major battery chemistries in current market and many more in research stages,\nelectrolytes are fairly unique to each of these systems and require special attention to underlying phenomenons such as\nlithium solvation, surface reactions and self discharge, during concoctions. There is availability of over 10 billion molecules\ncommercially that can be combined into a nearly inﬁnite number of formulations. This huge design space can only be navigated\nwith most advanced computational tools. Algorithms such as Multimodal MoLFormer aim to reduce the reliance of machine\nlearning guided formulation discovery on electrolyte datasets which could be challenging to acquire or generalized.\n5/10\n(a) Parity plot for MoLFormer predicted LCE values\n (b) Parity plot for Multimodal-MoLFormer predicted LCE values\nFigure 4. The ﬁgure depicts the performance of our multimodal chemical language approach, which leverages both SMILES\nand composition percentages of formulation constituents to predict performance metric LCE.\n(a) Residual plots of the predicted LCE values by the MoLFormer\napproach\n(b) Residual plots of the predicted LCE values by the multimodal\napproach proposed here\nFigure 5. The residuals plots depicting Multimodal- MoLFormer to have more stable predictions for LCE values than the\nalternative method.\nIn conclusion, the proposed multimodal chemical language-based model showcases exceptional predictive prowess regarding\nlogarithmic Coulombic efﬁciency . Its reliance on uncomplicated features underscores its practicality and efﬁciency . While\nacknowledging the prevailing limitations of our dataset, this work serves as a foundational stepping stone for future research in\nthe realm of battery technology . W e fervently encourage continued exploration on more expansive datasets to unlock the full\npotential of our model.\nMethods\nOur approach builds upon the foundation of MoLFormer 7 , a cutting-edge transformer-based model widely used for chemical\nlanguage representations. MoLFormer is a large-scale masked language model that processes inputs through a series of blocks,\nalternating between self-attention and feed-forward connections.\nThe self-attention mechanism of MoLFormer allows the model to construct complex representations by incorporating\ncontextual information from across the input sequence. By transforming the sequence features into query ( q), key ( k), and value\n(v) representations, attention mechanisms can weigh the importance of different elements within the sequence. This ability to\ncapture informative relationships between tokens makes MoLFormer a powerful tool for predicting molecular properties.\nT o further enhance performance, recent studies have demonstrated the beneﬁts of incorporating relative position embeddings\n6/10\nbetween tokens 14 . MoLFormer optimizes relative encoding by using a modiﬁed version of the RoFormer 36 attention mechanism.\nThis involves position-dependent rotations ( Rm ) of the query and keys at position m. These rotations can be efﬁciently\nimplemented as pointwise multiplications, ensuring that the computational complexity remains manageable (as shown in Eq\n(2)).\nAtt ent ionm (Q,K,V ) =∑ N\nn=1 ⟨\nϕ (Rm qm ),ϕ (Rn kn )⟩vn\n∑ N\nn=1 ⟨\nϕ (Rm qm ),ϕ (Rn kn )⟩ (1)\nIn Eq ( 2), Att ent ionm (Q,K,V ) denotes the attention operation with queries ( Q), keys ( K), and values ( V ) at position m. The\noperation computes weighted sums of the value representations ( vn ) based on the similarity of the transformed query ( ϕ (Rm qm ))\nand key ( ϕ (Rn kn )) representations. The relative position embeddings introduced through the rotations ( Rm ) allow the model to\neffectively capture positional information, leading to improved performance in molecular property predictions.\nBy leveraging the capabilities of MoLFormer and enhancing it with relative position embeddings, our approach offers an\nadvanced and efﬁcient solution for predicting complex molecular properties, providing valuable insights for various chemical\napplications.\nT okenization process and vocabulary construction\nThe approach employs a tokenization process, as described in 6 , to create its vocabulary . This tokenization process utilizes an\nextensive dataset comprising 1.1 billion molecules from PubChem and ZINC datasets, resulting in the generation of 2362\nunique vocabulary tokens. These tokens are then used for ﬁne-tuning or retraining the models, with a ﬁxed embedding capacity\nand vocabulary size.\nT o optimize computation time and resource utilization, the sequence length is constrained to a range of 1 to 202 tokens,\nincluding special tokens. This decision is driven by the fact that over 99.4% of all 1.1 billion molecules in the dataset contain\nfewer than 202 tokens. By setting this limit, the model can effectively handle the vast majority of molecular structures while\navoiding unnecessary computations for excessively long sequences.\nThe tokenization process and the limited sequence length enable the approach to efﬁciently process and represent molecular\nstructures, making it feasible to scale the model to large datasets and achieve powerful predictive capabilities in various\nchemical applications.\nMultimodal approach\nIn this section, we present the multimodal layer of our proposed approach, where SMILES notations embeddings are combined\nwith their corresponding percentages of the formulations derived from the electrolyte process.\nLet (x,y) denote a feature-target pair, where x = (xCL ,xproportions ). The xCL represents all the features based on chemical\nlanguage representations, and xproportions refers to the proportions of the compounds that compose the formulations. Each xCL\ncan have a sequence of tokens with a maximum length of 202 tokens, and each token has a 768-dimensional embedding. Fig. 3\nillustrates the concatenation architecture of the proposed approach.\nAfter the data transformation using Transformer layers (Fig. 3), the resulting embeddings per chemical language string are\nconcatenated along with their corresponding percentages of the formulations derived from the electrolyte process, resulting\nin a learning vector of dimension (d × e + c). Here, d represents the dimension of the dataset, e is the size of the resulting\nembeddings, and c is the number of features that represent the formulations’ percentages. This concatenated vector is then\npassed to a learning algorithm, speciﬁcally a Feed-Forward network with 2 fully connected layers, to calculate the loss function.\nThe method takes up to six SMILES as input, which are concatenated using a special token <SEP>. As the order of the\nSMILES does not matter in the formulation, during the training phase, we consider all possible permutations of the order in\nwhich the SMILES may appear to the system. This facilitates the learning process, as the algorithm is based on a Transformers\narchitecture, where the order of tokens does matter to the system.\nThe multimodal approach successfully fuses SMILES notations and formulation percentages, enabling accurate predictions\nof performance metrics. By effectively representing the complex relationships between chemical components and performance\nmetrics, our MultiModal-MoLFormer method achieves improved predictive performance compared to traditional approaches.\nIn summary , our proposed approach showcases the signiﬁcance of leveraging multimodal information to enhance the\nunderstanding and prediction of complex systems in battery electrolyte formulations. The seamless integration of SMILES\nnotations and formulation percentages contributes to the advancement of computational materials discovery , bridging the gap\nbetween material discovery and development, and offering a valuable tool to expedite the exploration of new materials with\nenhanced properties for diverse applications.\n7/10\nDataset augmentation\nWhen dealing with formulations, it is essential to note that the sequence of SMILES representations holds no signiﬁcance 37 .\nConsequently , the arrangement of data within the dataset can be strategically permuted, as depicted in Fig. 6, to effectively\nenhance data augmentation. This augmentation approach serves the purpose of bolstering the performance of the proposed\nmodel, thereby contributing to its overall effectiveness 38 . An additional crucial point to emphasize is that these formulations\nhave the potential to encompass up to six distinct SMILES. In cases where all six SMILES are not fully speciﬁed, any vacant\nspaces are automatically ﬁlled with “O”, while the corresponding contribution to the composition percentage is then designated\nas 0.035 . It is also important to note that the special token <SEP> is ignored by the proposed approach 39 . This systematic\napproach ensures consistency and accuracy in handling incomplete SMILES within the compositions. By incorporating the\ndata augmentation process, the training dataset underwent a substantial expansion, transitioning from 147 compositions to\n27,266 compositions. This notable augmentation was attained through a sequence of steps, commencing with the permutation\nprocedure and culminating in the removal of duplicates. This intricate transformation effectively paved the way for the dataset’s\nsubstantial growth. It is important to highlight that the test dataset is not augmented in order to preserve the fair evaluation of\nthe algorithms.\nTransformers-based approaches as the one proposed here, greatly beneﬁt from larger datasets 40 , as they encompass a more\nextensive range of text sequences 41 . The incorporation of these expanded datasets contributes signiﬁcantly to reﬁning the\ncomprehension and predictive abilities of transformers. This reﬁnement ultimately leads to improved performance across a\nwide array of tasks and applications. The increased diversity in sequence composition empowers transformers to enhance their\nperformance by capturing a broader spectrum of linguistic patterns, nuances, and contextual intricacies 42 . In summary , the\nutilization of larger datasets empowers transformers to enhance their understanding and predictive capacities, thereby resulting\nin elevated performance levels across diverse tasks and applications.\nFigure 6. The ﬁgure elucidates the permuted SMILES of each formulation constituents, alongside a breakdown of the\nconstituent percentages of each SMILE within the formulation. This breakdown is represented in the context of the input\nvector, providing a comprehensive understanding of the composition dynamics.\nData availability\nThe datasets used and/or analysed during the current study available from the corresponding author on reasonable request.\nReferences\n1. Grisoni, F . Chemical language models for de novo drug design: Challenges and opportunities. Curr. Opin. Struct. Biol. 79,\n102527 (2023).\n2. Pan, J. Large language model for molecular chemistry . Nat. Comput. Sci. 3, 5–5 (2023).\n3. White, A. D. The future of chemistry is language. Nat. Rev. Chem. 1–2 (2023).\n4. Wigh, D. S., Goodman, J. M. & Lapkin, A. A. A review of molecular representation in the age of machine learning. Wiley\nInterdiscip. Rev. Comput. Mol. Sci. 12, e1603 (2022).\n5. Bommasani, R. et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021).\n6. Schwaller, P . et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS central\nscience 5, 1572–1583 (2019).\n7. Ross, J. et al. Large-scale chemical language representations capture molecular structure and properties. Nat. Mach. Intell.\n4, 1256–1264 (2022).\n8/10\n8. Moret, M. et al. Leveraging molecular structure and bioactivity with chemical language models for de novo drug design.\nNat. Commun. 14, 114 (2023).\n9. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models can learn complex molecular distributions. Nat.\nCommun. 13, 3293 (2022).\n10. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: large-scale self-molformer optimizes relative encoding by\nusing a modiﬁed version of the roformer\n36 attention mechanism. this involves position-dependent rotations ( rm ) of the\nquery and keys at position m. these rotations can be efﬁciently implemented as pointwise multiplications, ensuring that the\ncomputational complexity remains manageable (as shown in eq ( 2)).\natt ent ionm (q,k,v) =∑ n\nn=1 ⟨\nϕ (rm qm ),ϕ (rn kn )⟩vn\n∑ n\nn=1 ⟨\nϕ (rm qm ),ϕ (rn kn )⟩ (2)\nin eq ( 2), att ent ionm (q,k,v) denotes the attention operation with queries ( q), keys ( k), and values ( v) at position m. the\noperation computes weighted sums of the value representations ( vn ) based on the similarity of the transformed query\n(ϕ (rm qm )) and key ( ϕ (rn kn )) representations. the relative position embeddings introduced through the rotations ( rm ) allow\nthe model to effectively capture positional information, leading to improved performance in molecular property predictions.\nby leveraging the capabilities of molformer and enhancing it with relative position embeddings, our approach offers\nan advanced and efﬁcient solution for predicting complex molecular properties, providing valuable insights for various\nchemical applications.pervised pretraining for molecular property prediction. arXiv preprint arXiv:2010.09885 (2020).\n11. Skinnider, M. A., Stacey , R. G., Wishart, D. S. & Foster, L. J. Chemical language models enable navigation in sparsely\npopulated chemical space. Nat. Mach. Intell. 3, 759–770 (2021).\n12. Huang, S. & Cole, J. M. Batterybert: A pretrained language model for battery database enhancement. J. Chem. Inf. Model.\n62, 6365–6377 (2022).\n13. Qian, C., T ang, H., Y ang, Z., Liang, H. & Liu, Y . Can large language models empower molecular property prediction?\narXiv preprint arXiv:2307.07443 (2023).\n14. Liu, Y . et al. Molrope-bert: An enhanced molecular representation with rotary position embedding for molecular property\nprediction. J. Mol. Graph. Model. 118, 108344 (2023).\n15. Chen, H. & Bajorath, J. Designing highly potent compounds using a chemical language model. Sci. Reports 13, 7412\n(2023).\n16. W ang, S., Guo, Y ., W ang, Y ., Sun, H. & Huang, J. Smiles-bert: large scale unsupervised pre-training for molecular property\nprediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health\ninformatics, 429–436 (2019).\n17. Sanchez-Lengeling, B. & Aspuru-Guzik, A. Inverse molecular design using machine learning: Generative models for\nmatter engineering. Science 361, 360–365 (2018).\n18. Madani, A. et al. Large language models generate functional protein sequences across diverse families. Nat. Biotechnol.\n1–8 (2023).\n19. Xu, C., W ang, Y . & Barati Farimani, A. Transpolymer: a transformer-based language model for polymer property\npredictions. npj Comput. Mater. 9, 64 (2023).\n20. Bran, A. M., Cox, S., White, A. D. & Schwaller, P . Chemcrow: Augmenting large-language models with chemistry tools.\narXiv preprint arXiv:2304.05376 (2023).\n21. Herbert, K. M. et al. Synthesis and alignment of liquid crystalline elastomers. Nat. Rev. Mater. 7, 23–38 (2022).\n22. Abolhasani, M. & Kumacheva, E. The rise of self-driving labs in chemical and materials sciences. Nat. Synth. 1–10 (2023).\n23. Janek, J. & Zeier, W . G. A solid future for battery development. Nat. energy 1, 1–4 (2016).\n24. He, X. et al. The passivity of lithium electrodes in liquid electrolytes for secondary batteries. Nat. Rev. Mater. 6, 1036–1052\n(2021).\n25. Hu, S. & Huang, C. Machine-learning approaches for the discovery of electrolyte materials for solid-state lithium batteries.\nBatteries 9, 228 (2023).\n26. Patel, R. A. & W ebb, M. A. Data-driven design of polymer-based biomaterials: high-throughput simulation, experimenta-\ntion, and machine learning. ACS Appl. Bio Mater. (2023).\n27. Brazil, E. V . et al. Position paper on dataset engineering to accelerate science. arXiv preprint arXiv:2303.05545 (2023).\n9/10\n28. Winter, B., Winter, C., Schilling, J. & Bardow , A. A smile is all you need: predicting limiting activity coefﬁcients from\nsmiles with natural language processing. Digit. Discov. 1, 859–869 (2022).\n29. Yüksel, A., Ulusoy , E., Ünlü, A. & Do ˘gan, T . Selformer: Molecular representation learning via selﬁes language models.\nMach. Learn. Sci. T echnol.(2023).\n30. Krenn, M. et al. Selﬁes and the future of molecular string representations. P atterns3 (2022).\n31. Born, J. & Manica, M. Regression transformer enables concurrent sequence regression and generation for molecular\nlanguage modelling. Nat. Mach. Intell. 5, 432–444 (2023).\n32. Kim, S. C. et al. Data-driven electrolyte design for lithium metal anodes. Proc. Natl. Acad. Sci. 120, e2214357120 (2023).\n33. Sun, L. et al. A review on recent advances for boosting initial coulombic efﬁciency of silicon anodic lithium ion batteries.\nSmall 18, 2102894 (2022).\n34. Wu, W . et al. Electrode and electrolyte regulation to promote coulombic efﬁciency and cycling stability of aqueous\nzinc-iodine batteries. Chem. Eng. J. 428, 131283 (2022).\n35. Sharma, V . et al. Formulation graphs for mapping structure-composition of battery electrolytes to device performance.\narXiv preprint arXiv:2307.03811 (2023).\n36. Su, J. et al. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 (2021).\n37. Xu, K. Li-ion battery electrolytes. Nat. Energy 6, 763–763 (2021).\n38. Shorten, C., Khoshgoftaar, T . M. & Furht, B. T ext data augmentation for deep learning. J. big Data 8, 1–34 (2021).\n39. Clark, K., Khandelwal, U., Levy , O. & Manning, C. D. What does bert look at? an analysis of bert’s attention. arXiv\npreprint arXiv:1906.04341 (2019).\n40. Lin, T ., W ang, Y ., Liu, X. & Qiu, X. A survey of transformers. AI Open (2022).\n41. Frank, M. C. Baby steps in evaluating the capacities of large language models. Nat. Rev. Psychol. 1–2 (2023).\n42. Min, B. et al. Recent advances in natural language processing via large pre-trained language models: A survey . ACM\nComput. Surv. (2021).\nAuthor contributions statement\nE.A. and E.V .B conceived the computational experiments, E.A. and V .S. conducted the experiments, E.A. and E.V .B analysed\nthe results, V .S. contributed to sample preparation and, R.C. and YH.N. designed and directed the project; All authors reviewed\nthe manuscript.\n10/10",
  "topic": "Battery (electricity)",
  "concepts": [
    {
      "name": "Battery (electricity)",
      "score": 0.589882493019104
    },
    {
      "name": "Electrolyte",
      "score": 0.5180530548095703
    },
    {
      "name": "Computer science",
      "score": 0.40505844354629517
    },
    {
      "name": "Chemistry",
      "score": 0.24465900659561157
    },
    {
      "name": "Thermodynamics",
      "score": 0.12637919187545776
    },
    {
      "name": "Physics",
      "score": 0.1104820966720581
    },
    {
      "name": "Electrode",
      "score": 0.0804280936717987
    },
    {
      "name": "Physical chemistry",
      "score": 0.0739266574382782
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113516",
      "name": "IBM Research - Brazil",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I4210085935",
      "name": "IBM Research - Almaden",
      "country": "US"
    }
  ]
}