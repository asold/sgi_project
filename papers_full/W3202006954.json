{
    "title": "GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation",
    "url": "https://openalex.org/W3202006954",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2095677828",
            "name": "Li Yunxiang",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A1881628833",
            "name": "Wang Shuai",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A1940314070",
            "name": "Wang Jun",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2356802438",
            "name": "Zeng Guodong",
            "affiliations": [
                "University of Bern"
            ]
        },
        {
            "id": "https://openalex.org/A2073614128",
            "name": "Liu Wenjun",
            "affiliations": [
                "Hangzhou Dianzi University"
            ]
        },
        {
            "id": "https://openalex.org/A2344289948",
            "name": "Zhang, Qianni",
            "affiliations": [
                "Queen Mary University of London"
            ]
        },
        {
            "id": "https://openalex.org/A2236586768",
            "name": "Jin Qun",
            "affiliations": [
                "Waseda University"
            ]
        },
        {
            "id": "https://openalex.org/A2186759468",
            "name": "Wang Ya-qi",
            "affiliations": [
                "Communication University of Zhejiang"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2956655325",
        "https://openalex.org/W1967631878",
        "https://openalex.org/W3004358802",
        "https://openalex.org/W3047026765",
        "https://openalex.org/W2986413572",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W3133842028",
        "https://openalex.org/W3092573066",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6600213211",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2122827492",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W6750469568",
        "https://openalex.org/W2898910301",
        "https://openalex.org/W2891656998",
        "https://openalex.org/W2905338897",
        "https://openalex.org/W3013766724",
        "https://openalex.org/W2964227007",
        "https://openalex.org/W2979375892",
        "https://openalex.org/W2150769593"
    ],
    "abstract": null,
    "full_text": "GT U-Net: A U-Net Like Group Transformer\nNetwork for Tooth Root Segmentation\nYunxiang Li1, Shuai Wang2, Jun Wang3, Guodong Zeng4, Wenjun Liu1, Qianni\nZhang5, Qun Jin6, Yaqi Wang7(\u0000 )\n1 Microelectronics CAD Center, Hangzhou Dianzi University, Hangzhou, China\n2 School of Mechanical, Electrical and Information Engineering, Shandong\nUniversity, Weihai, China\n3 School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, China\n4 sitem Center for Translational Medicine and Biomedical Entrepreneurship,\nUniversity of Bern, Bern, Switzerland\n5 School of Electronic Engineering and Computer Science, Queen Mary University of\nLondon, London, UK\n6 Department of Human Informatics and Cognitive Sciences, Faculty of Human\nSciences, Waseda University, Tokyo, Japan\n7 College of Media Engineering, Communication University of Zhejiang, Hangzhou,\nChina\nwangyaqi@cuz.edu.cn\nAbstract. To achieve an accurate assessment of root canal therapy, a\nfundamental step is to perform tooth root segmentation on oral X-ray\nimages, in that the position of tooth root boundary is signiﬁcant anatomy\ninformation in root canal therapy evaluation. However, the fuzzy bound-\nary makes the tooth root segmentation very challenging. In this paper, we\npropose a novel end-to-end U-Net like Group Transformer Network (GT\nU-Net) for the tooth root segmentation. The proposed network retains\nthe essential structure of U-Net but each of the encoders and decoders is\nreplaced by a group Transformer, which signiﬁcantly reduces the compu-\ntational cost of traditional Transformer architectures by using the group-\ning structure and the bottleneck structure. In addition, the proposed GT\nU-Net is composed of a hybrid structure of convolution and Transformer,\nwhich makes it independent of pre-training weights. For optimization, we\nalso propose a shape-sensitive Fourier Descriptor (FD) loss function to\nmake use of shape prior knowledge. Experimental results show that our\nproposed network achieves the state-of-the-art performance on our col-\nlected tooth root segmentation dataset and the public retina dataset\nDRIVE. Code has been released at https://github.com/Kent0n-Li/GT-\nU-Net.\nKeywords: Image segmentation · Shape-sensitive loss · Group Trans-\nformer · Root canal therapy\n1 Introduction\nIn worldwide, approximately 743 million people are aﬀected by severe periodon-\ntitis, which is considered the sixth most common health disorder [1]. Nowadays,\narXiv:2109.14813v1  [cs.CV]  30 Sep 2021\n2 Y. Li et al.\nroot canal therapy is a routine periodontal treatment for periodontitis and an\nincorrect evaluation of the treatment result will impede timely follow-up [2,3].\nSince the tooth root boundary is the signiﬁcant anatomy feature for carrying\nout the evaluation, tooth root segmentation becomes the most important step of\nroot canal therapy automatic evaluation. Unfortunately, performing an accurate\ntooth root segmentation is a very challenging task due to the following reasons:\n1) the tooth root boundaries are blurry and some tissues around the teeth have\nsimilar intensities to the teeth, as shown in Fig. 1 (a); 2) other bones and tissues\nmay overlap with the tooth root in the oral X-ray images, as shown in Fig. 1\n(b); 3) the quality of the X-ray image may be very poor such as overexposed or\nunderexposed, as shown in Fig. 1 (c).\n (b) (c)(a) \nFig. 1.Three examples of root canal therapy X-ray images. The left image in each\ngroup is the original image, and the right one depicts the tooth root and other tissues\nby red and cyan, respectively.\nTo address the problems mentioned above, Zhao et al. [4] provided a two-stage\nattention segmentation network, which can eﬀectively alleviate the inhomoge-\nneous intensity distribution problem by focusing on automatically catching the\nreal tooth region. Lee et al. [5] adopted a ﬁne-tuned mask R-CNN [6] algorithm\nto achieve the tooth segmentation. Nevertheless, these methods do not eﬀec-\ntively solve the segmentation problem of fuzzy boundaries, and the performance\nimprovement is mostly incremental. Chen et al. [7] proposed a novel MSLPNet\nwith multi-scale structural similarity (MS-SSIM) loss, enhancing tooth segmen-\ntation that has fuzzy root boundaries. Cheng et al. [8] proposed U-Net+DFM to\nlearn a direction ﬁeld. It characterizes the directional relationship between pix-\nels and implicitly restricts the shape of the segmentation result. Although these\nmethods all achieve decent results in segmentation tasks, they are still limited\nby the intrinsic locality of convolutional neural networks (CNNs) and can not\nprocess global features very well. To relieve this problem, long-range dependen-\ncies via non-local operations are highly desired, and Transformer [9] provides\na modeling pipeline to achieve that. Chen et al. proposed TransUNet [10], a\nTransformer-based encoder operating for segmentation, which adopts ViT [11]\nwith 12 Transformer layers as the encoder. However, ViT relies on pre-trained\nGT U-Net: A U-Net Like Group Transformer Network 3\nweights obtained by a huge image corpus, which results in undesirable perfor-\nmance on insuﬃcient datasets. To solve that, Aravind et al. [12] presented BoT-\nNet, an eﬀective instance segmentation backbone, by combining transformer and\nconvolution. Due to the high computational complexity of Transformer, BoTNet\nhas only replaced a part of convolution in the last few layers of ResNet with\nTransformer.\nIn order to mitigate the problems existing in the present approaches, our net-\nwork GT U-Net employs both the combination of convolution and Transformer\nwithout pre-training weights and the grouping structure and bottleneck struc-\nture that signiﬁcantly reduces the amount of computation. Besides, FD loss also\nsolves the problem of fuzzy boundary segmentation by making full use of shape\nprior knowledge. The main contributions of this paper are listed as follows:\n(1) Our network retains the advantages of the general U-Net framework and\nintroduces Transformer into the medical image segmentation application to solve\nthe limitation of convolution.\n(2) We design a grouping structure and a bottleneck structure, which greatly\nreduces the computation load of Transformer and makes it feasible in image\nsegmentation.\n(3) For the root segmentation task, we propose a shape-sensitive Fourier De-\nscriptor loss function to deal with the problem of fuzzy boundary segmentation.\n1 × 1\nMatrix \nMultiplication\nElement\nWise Sum\nPointwise\nConvolution\n\u0000\u0000\n\u0000\u0000\nSelf-Attention Layer\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000: 1 × 1\n\u0000\u0000: 1 × 1\n\u0000\u0000: 1 × 1\nContent-\nposition\nContent-\ncontent\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0000\n\u0000\n\u0000\n\u0000 Max-pooling\nUpsample\nGroup \nTransformer\nMHSA\nMHSA\nMHSA\nMHSA\n..\n..\n\u0000 ×\u0000 × \u0000\n\u0000 \u0000 ×\u0000 ×\u0000\u0000\n....\n\u0000 ×\u0000 ×\u0000\n\u0000 ×\u0000 ×\u0000\nTotal\n\u0000\n\u0000 ×\u0000\n\u0000\n....\n3×3\nconv\n3×3\nconv\n3×3\nconv\n3×3\nconv\n3×3\nconv\n3×3\nconv\n3×3\nconv\n3×3\nconv\nSkip \nConnection\nCopy and Crop\nFig. 2.The architecture of the proposed GT U-Net, which is composed of a U-shape\ngeneral framework and group Transformer. Self-attention layer is the base structure\nof Multi-head self-attention (MHSA) where our MHSA has 4 heads, and we do not\nillustrate them on the ﬁgure for simplicity.\n4 Y. Li et al.\n2 Method\nGT U-Net follows the overall U-shaped structure, where both the encoders\nand decoders consist of group Transformer. It works with a shape-sensitive\nFourier Description (FD) loss function for tooth root segmentation. The pro-\nposed method is described in full detail below.\n2.1 U-Net like Group Transformer Network\nOverall Architecture:The architecture of GT U-Net is illustrated in Fig. 2.\nSpeciﬁcally, our group Transformer is composed of a skip connection, a grouping\nmodule, 3 ×3 convolutions, Multi-head self-attention (MHSA) modules, and a\nmerging module. Among them, the skip connection is used to solve the problem of\ngradient vanishing and keep the low-level information. Considering that MHSA\nrequires O( n2d) memory and computation when performing globally across n\nentities, the grouping module and 3 ×3 convolutions are designed to reduce the\namount of MHSA computation.\nGrouping Structure and Bottleneck Structure:Transformer was ﬁrst pro-\nposed for natural language processing (NLP) tasks and has become a hot topic\nin many computer vision tasks as a non-local operation with long-range depen-\ndence. However, it is diﬃcult to apply Transformer directly to medical image\ntasks on account of the discrepancy between natural language and image. The\nnumber of words in natural language is limited, but the number of pixels in-\ncreases quadratic with the increase of image size. Because of this, we designed\ngroup Transformer to solve the problem of too much computation due to the im-\nage characteristics in medical image segmentation. Assuming that the original\nfeature block size is H×W ×C, the computation of MHSA will be greatly re-\nduced through the grouping structure and bottleneck structure. The calculation\namount of MHSA before improvement [13] and that of our group Transformer\n(GT) are given in Eq. (1) and (2), respectively:\nΩ(MHSA) = 4HWC2 + 2(HW)2C (1)\nΩ(GT) = 4hw(C\nϕ)2 + 2(hw)2 C\nϕ = 4hw\nϕ2HW HWC2 + 2h2w2\nϕH2W2 (HW)2C (2)\nwhere ϕis the channel scaling factor of the bottleneck structure, and the size of\neach group Transformer unit is set to h×w, which is determined by the input\nimage size and task characteristics. The proposed method achieves local long-\nrange dependencies at higher encoder and decoder layers. With the deepening\nof the network, the ﬁeld of vision is gradually expanded to extract the junction\nfeatures between diﬀerent GT units and to achieve the global long-range depen-\ndencies.\nMulti-Head Self-Attention:MHSA [9] is a type of attention mechanism which\nis more focused on the internal structure. The detailed architecture of MHSA\nGT U-Net: A U-Net Like Group Transformer Network 5\nis illustrated in Fig. 2, where the position encoding method is relative-distance-\naware position encoding [14,15,16] with Rh and Rw for height and width. The\nattention logit is qkT + qrT, and q,k,v,r denote query, key, value, and position\nencodings, respectively.\nHybrid Structure of Convolution and Transformer:Transformer has the\nability to extract global contexts, but it also has the limitation of lacking trans-\nlation invariance while locality and translational equivariance are the basic prop-\nerty of convolution. Therefore, 3×3 convolution in our group Transformer is not\nonly for the bottleneck structure, but it also helps construct a hybrid architec-\nture where the convolution is responsible for feature extraction and Transformer\nis constructed to model the long-range dependencies.\n2.2 Shape-sensitive Fourier Descriptor Loss Function\nFrom the anatomical prior knowledge, it is known that the tooth roots share a\nsimilar shape. Adding the shape information into the loss function helps better\nguide the model to segment tooth roots. ( xm,ym) is one of the coordinates on\na tooth root boundary that contains N pixels, and the boundary shape can be\nformed as a complex number z(m) = xm + jym. The Fourier Descriptor [17] of\nthis shape is deﬁned as DFT(z(m)), that is Z(k) in Eq. (3):\nZ(k) = DFT(z(m)) = 1\nN\nN−1∑\nm=0\nz(m)e−j2πmk/N (k= 0,··· ,N −1) (3)\nFourier Descriptor is a quantitative representation of closed shapes independent\nof their starting point, scaling, location, and rotation. Therefore, the shape dif-\nference between the predicted boundary A and the manually labeled boundary\nB can be quantiﬁed, and it can be calculated based on Eq. (4). The original loss\nfunction is binary cross-entropy (BCE). In the proposed method, we add Fourier\nDescriptors to build a new shape-sensitive loss function. Considering their or-\nder of magnitude in the whole training process, we calculate the ﬁnal Fourier\nDescriptor (FD) loss by Eq. (5):\n∆Z(k) = |ZA(k) −ZB(k)|. (4)\nFD loss= BCE(A,B) × 1\n1 + e−β×∆Z(k) (5)\nThe novel FD loss pays equal attention to shape loss and BCE loss regardless of\ntheir order of magnitude. β is determined by the order of magnitude of ∆Z(k),\nand it is set to 10 in this paper.\n3 Experiment\n3.1 Tooth Root Segmentation Dataset\nWe have built a new root canal therapy X-ray image dataset with patients’\nconsent. The tooth root segmentation dataset contains 248 root canal therapy X-\n6 Y. Li et al.\nray images from diﬀerent patients in total. And three experienced stomatologists\nhelped to complete the tooth root annotation. Speciﬁcally, to minimize inter-\nobserver variability, the ﬁnal annotation results needed the agreement of all\nthree stomatologists.\nImage Label U-Net Att U-Net TransUNet\nGT U-Net\n(without FD loss) GT U-Net\nI\nII\nIII\nFig. 3.Representative segmentation results achieved by diﬀerent methods.\n3.2 Implementation Details\nIn our implementation, three state-of-the-art methods are compared, U-Net [18],\nAttention U-Net [19], and TransUNet [10]. All compared methods and our method\nwere implemented using PyTorch and trained on four RTX 2080Ti GPUs. Ran-\ndom center crop, random rotation, and axial ﬂipping are adopted for the data\naugmentation to improve robustness and avoid overﬁtting. In the training, Adam\noptimizer is adopted and the initial learning rate and momentum are set to\n2 ×10−4 and 0.5, respectively. The total number of training epochs is 200, and\nthe batch size is 12. All the images are resized to 256 ×256 for input. h×w of\nthe group Transformer unit is set to 8 ×8, and ϕ is set to 2.\nTo evaluate the segmentation performance, we compare the results with six\nmetrics for evaluation, including Accuracy (ACC), Sensitivity (SE), Speciﬁcity\n(SP), Jaccard Similarity (JS), and Dice Coeﬃcient (DICE). Besides, we adopt\n3-fold cross-validation for testing the segmentation performance on the tooth\nroot segmentation dataset.\nGT U-Net: A U-Net Like Group Transformer Network 7\nTable 1.The evaluation performance of all methods on the Tooth Root Segmentation\nDataset.\nU-Net Att U-Net TransUNet GT U-Net\n(without FD loss) GT U-Net\nACC(%) 92.81±0.90 93.03±0.56 93.41±0.25 93.80±0.32 93.98±0.11\nSE(%) 92.37±0.68 89.36±1.30 91.81±0.74 93.98±0.38 93.74±1.18\nSP(%) 93.74±0.99 95.60±0.28 95.05±0.43 94.26±0.35 94.79±1.14\nJS(%) 84.62±1.30 83.85±1.29 85.75±0.65 86.63±0.62 86.87±0.13\nDICE(%) 91.19±1.01 91.15±0.76 91.96±0.40 92.32±0.49 92.54±0.25\n3.3 Experimental Results\nThe average results with standard deviations of all methods are reported in\nTable 1. From this table, it can be summarized that our proposed GT U-Net\nachieves the best accuracy, sensitivity, Jaccard Similarity, and Dice Coeﬃcient.\nIn Fig. 3, we qualitatively compare the segmentation results on three oral X-ray\nimages. It can be evidently seen that our method obtains the best segmentation\nresults on the blurred boundaries. To verify the eﬀectiveness of FD loss, we\nimplement GT U-Net without FD loss. Based on the comparison in Table 1\nand Fig. 3, FD loss can be eﬀectively applied to the segmentation task with a\nfuzzy boundary but a similar shape by implicitly restricting the shape of the\nsegmentation result with the anatomical prior knowledge.\nTable 2.The segmentation performance achieved by all methods on the public DRIVE\ndataset.\nYear ACC(%) SE(%) SP(%) F1(%) AUC(%)\nU-Net [18,20] 2015 95.55 78.22 98.08 81.74 97.52\nMS-NFN [21] 2018 95.67 78.44 98.19 - 98.07\nResidual U-Net [22,23] 2018 95.53 77.26 98.20 81.49 97.79\nRecurrent U-Net [22,23] 2018 95.56 77.51 98.16 81.55 97.82\nR2U-Net [22,23] 2018 95.56 77.92 98.13 81.71 97.84\nDenseBlock-UNet [24] 2018 95.41 79.28 97.76 81.46 97.56\nDUNet [20] 2019 95.58 78.63 98.05 81.90 97.78\nACE-Net [25] 2019 95.69 77.25 98.42 - 97.42\nIterNet [23] 2020 95.73 77.35 98.38 82.05 98.16\nGT U-Net 2021 96.31 82.54 98.24 84.58 98.02\n3.4 Performance on the Public DRIVE Dataset\nTo further evaluate the performance of our GT U-Net, we applied it on the widely\nused retinal dataset DRIVE [26]. The dataset consists of 40 randomly selected\ncolor fundus retinal images of size 565 ×584. Oﬃcially, DRIVE is split into two\nequal groups for training and testing. The group Transformer unit parameters\n8 Y. Li et al.\nhand w are all set to 4, and ϕis set to 2. Since FD loss is proposed for medical\nimage segmentation with a similar shape, like tooth root segmentation, we did\nnot implement it on the DRIVE dataset. To eﬃciently train our GT U-Net, we\ncropped image patches from the original images, and 64 ×64 is adopted as the\nsample patch size, which is widely adopted on this dataset.\nI\nII\nIII\nImage LabelPrediction (binary)Prediction\nFig. 4.Representative segmentation results of the public DRIVE dataset.\nThe comparison results are reported in Table 2. It can be observed that our\nGT U-Net outperforms other methods by at least 3.2% on SE, over 2.5% on\nF1, and over 0.5% on ACC. In terms of SP and AUC, our GT U-Net achieved\ncompetitive performance in comparison with the best methods, limiting the dif-\nference from them within 0.2%. Meanwhile, the visualization of some represen-\ntative segmentation results is given in Fig. 4. It can be clearly observed that the\nvascular part of the retina can be segmented very well by our method, except\nfor some very tiny areas.\n4 Conclusion\nIn this paper, a novel end-to-end U-Net like group Transformer network (GT\nU-Net) for medical image segmentation is presented. GT U-Net uses the hy-\nbrid structure of convolution and Transformer to eliminate the need for pre-\nGT U-Net: A U-Net Like Group Transformer Network 9\ntrained weights. Moreover, it signiﬁcantly reduces the computational complexity\nof Transformer using a bottleneck structure and a grouping structure, and FD\nloss can be eﬀectively applied to the segmentation task with a fuzzy boundary\nbut a similar shape. Extensive experiments are conducted on tooth root and\nretinal vessel segmentation tasks, respectively, and the results show that the\nproposed GT U-Net has excellent segmentation performance and a great appli-\ncation perspective in a wide range of medical image segmentation tasks.\nAcknowledgements. This work was supported by the National Key Research\nand Development Program of China (Grant No. 2019YFC0118404) and Public\nProjects of Zhejiang Province (Grant No. LGG20F020001).\nReferences\n1. Marco A Peres, Lorna MD Macpherson, Robert J Weyant, Bl´ anaid Daly, Renato\nVenturelli, Manu R Mathur, Stefan Listl, Roger Keller Celeste, Carol C Guarnizo-\nHerre˜ no, Cristin Kearns, et al. Oral diseases: a global public health challenge.The\nLancet, 394(10194):249–260, 2019.\n2. Carlos Estrela, Roberto Holland, Cyntia Rodrigues de Ara´ ujo Estrela, Ana He-\nlena Gon¸ calves Alencar, Manoel Dami˜ ao Sousa-Neto, and Jesus Djalma P´ ecora.\nCharacterization of successful root canal treatment. Brazilian dental journal,\n25(1):3–11, 2014.\n3. Tuna Kaplan, G¨ uzide Pelin Sezgin, and Sema S¨ onmez-Kaplan. Dental students’\nperception of diﬃculties concerning root canal therapy: A survey study. 2019.\n4. Yue Zhao, Pengcheng Li, Chenqiang Gao, Yang Liu, Qiaoyi Chen, Feng Yang, and\nDeyu Meng. Tsasnet: Tooth segmentation on dental panoramic x-ray images by\ntwo-stage attention segmentation network. Knowledge-Based Systems, 206:106338,\n2020.\n5. Jeong-Hee Lee, Sang-Sun Han, Young Hyun Kim, Chena Lee, and Inhyeok Kim.\nApplication of a fully deep convolutional neural network to the automation of\ntooth segmentation on panoramic radiographs. Oral surgery, oral medicine, oral\npathology and oral radiology, 129(6):635–642, 2020.\n6. Kaiming He, Georgia Gkioxari, Piotr Doll´ ar, and Ross Girshick. Mask r-cnn. In\nProceedings of the IEEE international conference on computer vision, pages 2961–\n2969, 2017.\n7. Qiaoyi Chen, Yue Zhao, Yang Liu, Yongqing Sun, Chongshi Yang, Pengcheng Li,\nLingming Zhang, and Chenqiang Gao. Mslpnet: multi-scale location perception\nnetwork for dental panoramic x-ray image segmentation. Neural Computing and\nApplications, pages 1–15, 2021.\n8. Feng Cheng, Cheng Chen, Yukang Wang, Heshui Shi, Yukun Cao, Dandan Tu,\nChangzheng Zhang, and Yongchao Xu. Learning directional feature maps for car-\ndiac mri segmentation. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 108–117. Springer, 2020.\n9. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn NIPS, pages 6000–6010, 2017.\n10. Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang,\nLe Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong\nencoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.\n10 Y. Li et al.\n11. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n12. Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and\nAshish Vaswani. Bottleneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021.\n13. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin transformer: Hierarchical vision transformer using shifted\nwindows. arXiv preprint arXiv:2103.14030, 2021.\n14. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative po-\nsition representations. In Proceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 464–468, 2018.\n15. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Lev-\nskaya, and Jonathon Shlens. Stand-alone self-attention in vision models. arXiv\npreprint arXiv:1906.05909, 2019.\n16. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le.\nAttention augmented convolutional networks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), October 2019.\n17. Charles T Zahn and Ralph Z Roskies. Fourier descriptors for plane closed curves.\nIEEE Transactions on computers, 100(3):269–281, 1972.\n18. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional net-\nworks for biomedical image segmentation. In International Conference on Medi-\ncal image computing and computer-assisted intervention, pages 234–241. Springer,\n2015.\n19. Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazu-\nnari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz,\net al. Attention u-net: Learning where to look for the pancreas. 2018.\n20. Qiangguo Jin, Zhaopeng Meng, Tuan D Pham, Qi Chen, Leyi Wei, and Ran Su.\nDunet: A deformable network for retinal vessel segmentation. Knowledge-Based\nSystems, 178:149–162, 2019.\n21. Yicheng Wu, Yong Xia, Yang Song, Yanning Zhang, and Weidong Cai. Multi-\nscale network followed network model for retinal vessel segmentation. In Interna-\ntional Conference on Medical Image Computing and Computer-Assisted Interven-\ntion, pages 119–126. Springer, 2018.\n22. Md Zahangir Alom, Mahmudul Hasan, Chris Yakopcic, Tarek M Taha, and Vi-\njayan K Asari. Recurrent residual convolutional neural network based on u-net\n(r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955, 2018.\n23. Liangzhi Li, Manisha Verma, Yuta Nakashima, Hajime Nagahara, and Ryo\nKawasaki. Iternet: Retinal image segmentation utilizing structural redundancy\nin vessel networks. In Proceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pages 3656–3665, 2020.\n24. Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, and Pheng-Ann\nHeng. H-denseunet: hybrid densely connected unet for liver and tumor segmenta-\ntion from ct volumes. IEEE transactions on medical imaging, 37(12):2663–2674,\n2018.\n25. Yanhao Zhu, Zhineng Chen, Shuai Zhao, Hongtao Xie, Wenming Guo, and Yong-\ndong Zhang. Ace-net: Biomedical image segmentation with augmented contracting\nand expansive paths. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention, pages 712–720. Springer, 2019.\nGT U-Net: A U-Net Like Group Transformer Network 11\n26. Joes Staal, Michael D Abr` amoﬀ, Meindert Niemeijer, Max A Viergever, and Bram\nVan Ginneken. Ridge-based vessel segmentation in color images of the retina.IEEE\ntransactions on medical imaging, 23(4):501–509, 2004."
}