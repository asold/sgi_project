{
  "title": "Counting the Bugs in ChatGPT’s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
  "url": "https://openalex.org/W4389520697",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2782282100",
      "name": "Leonie Weissweiler",
      "affiliations": [
        "LMU Klinikum",
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A3021366198",
      "name": "Valentin Hofmann",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5093123227",
      "name": "Anjali Kantharuban",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2804217386",
      "name": "Anna Cai",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2784795398",
      "name": "Ritam Dutt",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A3092167885",
      "name": "Amey Hengle",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A3082046289",
      "name": "Anubha Kabra",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2898995291",
      "name": "Atharva Kulkarni",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2141557763",
      "name": "Abhishek Vijayakumar",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2166889965",
      "name": "Yu Haofei",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2111997136",
      "name": "Hinrich Schuetze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294813931",
      "name": "Kemal Oflazer",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2102413935",
      "name": "David A. Mortensen",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2510905642",
    "https://openalex.org/W3037995636",
    "https://openalex.org/W2585253991",
    "https://openalex.org/W2328568148",
    "https://openalex.org/W2066201260",
    "https://openalex.org/W2251565024",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3103671331",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2571532437",
    "https://openalex.org/W2508627044",
    "https://openalex.org/W4280645456",
    "https://openalex.org/W4300402905",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W2825387888",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4376645554",
    "https://openalex.org/W4285271926",
    "https://openalex.org/W3170826848",
    "https://openalex.org/W3176893837",
    "https://openalex.org/W4205285807",
    "https://openalex.org/W4236939794",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W2508815538",
    "https://openalex.org/W4220934991",
    "https://openalex.org/W2805993470",
    "https://openalex.org/W4239818589",
    "https://openalex.org/W2561296568",
    "https://openalex.org/W2144862731",
    "https://openalex.org/W1967986525",
    "https://openalex.org/W4285267582",
    "https://openalex.org/W621048792",
    "https://openalex.org/W2798549337",
    "https://openalex.org/W4306808642",
    "https://openalex.org/W3193014326",
    "https://openalex.org/W3153224075",
    "https://openalex.org/W3214933191",
    "https://openalex.org/W2963326795",
    "https://openalex.org/W3026401902",
    "https://openalex.org/W2099222747",
    "https://openalex.org/W2169147927",
    "https://openalex.org/W2186780112",
    "https://openalex.org/W3034786567",
    "https://openalex.org/W4389518805",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W1703646650",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2300419000",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W2073610029",
    "https://openalex.org/W3034731894",
    "https://openalex.org/W2461808544",
    "https://openalex.org/W2962680795",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2590654071",
    "https://openalex.org/W3105184673",
    "https://openalex.org/W3035398511",
    "https://openalex.org/W2884554299",
    "https://openalex.org/W3035376668",
    "https://openalex.org/W4240677123",
    "https://openalex.org/W2564287196",
    "https://openalex.org/W2068976472",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W2740149041",
    "https://openalex.org/W3037720825",
    "https://openalex.org/W2021031353",
    "https://openalex.org/W3188334090",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2294970769",
    "https://openalex.org/W4287855127",
    "https://openalex.org/W4385574028",
    "https://openalex.org/W3034856139",
    "https://openalex.org/W2896509746"
  ],
  "abstract": "Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schuetze, Kemal Oflazer, David Mortensen. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6508–6524\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCounting the Bugs in ChatGPT’s Wugs: A Multilingual Investigation into\nthe Morphological Capabilities of a Large Language Model\nLeonie Weissweiler*2,4, Valentin Hofmann*2-5, Anjali Kantharuban1, Anna Cai†1, Ritam Dutt†1,\nAmey Hengle†6, Anubha Kabra†1, Atharva Kulkarni†1, Abhishek Vijayakumar†1,\nHaofei Yu†1, Hinrich Schütze2,4, Kemal Oflazer1, David R. Mortensen1\n1Carnegie Mellon University 2LMU Munich 3University of Oxford\n4Munich Center for Machine Learning 5Allen Institute for AI 6IIT Delhi\nweissweiler@cis.lmu.de\nAbstract\nLarge language models (LLMs) have recently\nreached an impressive level of linguistic capa-\nbility, prompting comparisons with human lan-\nguage skills. However, there have been rel-\natively few systematic inquiries into the lin-\nguistic capabilities of the latest generation of\nLLMs, and those studies that do exist (i) ig-\nnore the remarkable ability of humans to gen-\neralize, (ii) focus only on English, and (iii)\ninvestigate syntax or semantics and overlook\nother capabilities that lie at the heart of hu-\nman language, like morphology. Here, we\nclose these gaps by conducting the first rigor-\nous analysis of the morphological capabilities\nof ChatGPT in four typologically varied lan-\nguages (specifically, English, German, Tamil,\nand Turkish). We apply a version of Berko’s\n(1958) wug test to ChatGPT, using novel, un-\ncontaminated datasets for the four examined\nlanguages. We find that ChatGPT massively\nunderperforms purpose-built systems, particu-\nlarly in English. Overall, our results—through\nthe lens of morphology—cast a new light on\nthe linguistic capabilities of ChatGPT, suggest-\ning that claims of human-like language skills\nare premature and misleading.\n1 Introduction\nDo large language models (LLMs) possess human-\nlike linguistic capabilities? With the advent of the\nlatest generation of LLMs such as GPT-4 (OpenAI,\n2023b), LLaMA (Touvron et al., 2023), and PaLM\n(Chowdhery et al., 2022), there appears to be grow-\ning evidence for answering this question withyes\n(Bubeck et al., 2023): LLMs are capable of gen-\nerating text that crowdworkers cannot distinguish\nfrom human-generated text (Clark et al., 2021) and\nexcel at linguistic probing tasks such as predicting\ngrammaticality, detecting the subject and tense of\n*Equal contribution.\n†Authors sorted alphabetically.\ngel →gelemedik\nye →yiyemedik\nzulu →\nLLM\nHumans\nzuluyemedin\nzuluyamadık\nEvaluation\nFigure 1: Experimental paradigm for this study (illus-\ntrated with Turkish). Human annotators and an LLM\nare given examples and a nonce word to be inflected.\nThe generated inflected forms are compared.\nclauses, and identifying the grammatical number\nof subjects and objects (Jin et al., 2022).\nDespite these encouraging results, the existing\nbody of work has so far examined a relatively lim-\nited part of the full spectrum of phenomena that\nare known to characterize human language, with a\nheavy focus on syntax and semantics. One area\nthat has been neglected in particular ismorphol-\nogy, i.e., the capacity to create words according\nto systematic patterns of covariation in form and\nmeaning (Haspelmath and Sims, 2010). This gap\nin the LLM literature is noteworthy given that mor-\nphology has been a hallmark of research on com-\nputational approaches to language since the very\nbeginnings of neural language processing in the\n1980s (Rumelhart and McClelland, 1986b; Plun-\nkett and Juola, 1999; Albright and Hayes, 2002,\n2003; Goldberg, 2019).\nIn this study, we present the first systematic anal-\nysis of the morphological capabilities of LLMs, fo-\n6508\ncusing on ChatGPT (OpenAI, 2023a) as the most\nprominent and most widely-used LLM. Specifi-\ncally, we investigate ChatGPT’s morphological ca-\npabilities using the wug test (Berko, 1958), an\nexperimental paradigm in which a participant is\nasked to provide an inflected or derived form\nof a nonce word. An example for our evalua-\ntion setup is given in Figure1. Our experiments\ncover a broad range of morphological construc-\ntions and four typologically diverse languages: En-\nglish, German, Tamil, and Turkish. We find that\nChatGPT falls short not only of human perfor-\nmance but also of various supervised baselines.\nIn sum, our contributions are as follows:\n• We conduct the first systematic analysis into the\nmorphological capabilities of LLMs.\n• Our study covers a diverse set of morphological\nconstructions/languages and introduces datasets\nfor future research in the area.1\n• We show that ChatGPT has not achieved human\nparity—or even state-of-the-art performance—\non our nonce-word inflection/reinflection tasks\nbut performs about as well as some older super-\nvised models. We furthermore find evidence for\nthe existence of a real word bias in ChatGPT that\nis the more pronounced the more data ChatGPT\nhas seen for a given language.\n2 Related Work\n2.1 Computational Morphology\nLinguists divide morphology into inflection and\nderivation (Haspelmath and Sims, 2010). While\ninflection accounts for the different word forms of\na lexeme, e.g.,listen, listens, andlistened, deriva-\ntion accounts for the different lexemes of a word\nfamily, e.g., listen, listener, and listenable. Both\ninflection and derivation have been addressed in\ncomputational linguistics and natural language pro-\ncessing (NLP), albeit with a heavy focus on in-\nflection. One line of work, which is conceptu-\nally similar to wug testing, has sought to gener-\nate inflected forms, given a stem and a morpho-\nlogical tag (Cotterell et al., 2017a, 2018; Vylo-\nmova et al., 2020; Goldman et al., 2022), using\nsystems ranging from weighted finite state trans-\nducers and GRU/LSTM encoder-decoder models\n1We release our dataset along with our code athttps://\ngithub.com/dmort27/chatgpts-wugs, carefully following\nthe guidelines laid out byJacovi et al.(2023).\n(with soft attention or hard monotonic attention) to\nvarious transformer models. A special subtype of\nthis task is morphological reinflection, where the\ninput can be a form that is itself inflected (Cot-\nterell et al., 2016a; Kann and Schütze, 2016; Kann\net al., 2017; Silfverberg et al., 2017; Pimentel et al.,\n2021). Other typical tasks in computational re-\nsearch on inflection are morphological segmenta-\ntion (Cotterell et al., 2015, 2016b,c; Kann et al.,\n2016), unsupervised morphology induction (Ham-\nmarström and Borin, 2011; Soricut and Och, 2015;\nXu et al., 2018; Weissweiler et al., 2022), and mor-\nphological paradigm completion (Erdmann et al.,\n2020a,b; Jin et al., 2020). There has also been\nsome interest in the modeling of derivation (Cot-\nterell et al., 2017b; Vylomova et al., 2017; Deutsch\net al., 2018; Hofmann et al., 2020b,c).\nMore recently, there have been a few studies\nexamining the morphological capabilities of lan-\nguage models (Edmiston, 2020; Hofmann et al.,\n2020a), but they focus on smaller language models\nsuch as BERT (Devlin et al., 2019). By contrast,\nwe examine ChatGPT, a model whose parameter\ncount is three orders of magnitude larger, and we\nanalyze its zero-, one-, and few-shot capabilities,\nan approach fully neglected by prior work.\n2.2 Multilingual Capabilities of LLMs\nRecent studies have extensively examined the eval-\nuation of LLMs in multilingual settings. Some of\nthese studies have specifically investigated the ex-\ntent to which LLMs can be used for traditional\nmultilingual NLP tasks such as machine transla-\ntion (Bawden et al., 2022; Hendy et al., 2023; Jiao\net al., 2023; Wang et al., 2023). Brown et al.(2023)\ndemonstrate that LLMs perform well across mul-\ntiple languages even with minimal task-specific\ntraining, highlighting their transferability and gen-\neralization in multilingual understanding.\n2.3 LLM Performance on Unseen Data\nThe fact that LLMs have been pretrained on mas-\nsive amounts of data means that they have seen and\npotentially memorized a substantial amount of the\nitems of data used in typical evaluation setups (Ma-\ngar and Schwartz, 2022). There have been a few\nattempts in NLP to specifically control for previ-\nous exposure (Haley, 2020; Hofmann et al., 2020a;\nMaudslay and Cotterell, 2021). We follow this\nidea by generating datasets of novel and uncontam-\ninated nonce words, thus ensuring that the words\nhave not been seen by ChatGPT before.\n6509\n3 Data and Morphological Constructions\nIn this paper, we examine ChatGPT’s morpho-\nlogical behavior on a typologically diverse set of\nlanguages: English, German, Tamil, and Turkish.\nWhile English and German belong to the same lan-\nguage family, German has a more fusional mor-\nphological system than English. Turkish is cho-\nsen since it is a non-Indo-European language with\na fully agglutinative morphology. Tamil is cho-\nsen since it is a Dravidian language exhibiting an\nagglutinative morphology with fusional elements.\nThus, in terms of the classical triangle of fusional,\nisolating, and agglutinative morphologies (Dixon,\n1994), the languages cover four different points:\nalmost fully isolating (English), intermediate be-\ntween isolating and fusional (German), interme-\ndiate between fusional and agglutinative (Tamil),\nand fully agglutinative (Turkish). Furthermore, the\nchosen languages also cover different points in the\nspectrum from low-resource to high-resource, en-\nabling us to form hypotheses about the impact of\nthe amount of language-specific training data on\nthe morphological capabilities of an LLM. Statis-\ntics for the amount of data in train, dev, and test\nfor the baselines, as well as the number of wug test\nwords, are given in Table1. We report the accuracy\nof one annotator at a time against the judgments of\nall other annotators in Table2.\n3.1 English\nThe English past tense has a long and storied\nhistory in computational studies of morphology\n(Rumelhart and McClelland, 1986a; Pinker and\nPrince, 1988; Ullman et al., 1997; Plunkett and\nJuola, 1999; Albright and Hayes, 2002, 2003;\nKirov and Cotterell, 2018; Ma and Gao, 2022). En-\nglish displays a handful of conjugation classes as\nwell as frequent morphographemic alternations—\nconsonant doubling and e-deletion, for example—\naffecting past forms of verbs.\nTo create the English data, 50 two- to five-letter\nirregular verbs (defined as verbs that do not form\nthe past tense simply by adding-ed) were sam-\npled from the UniMorph 4.0 dataset (Batsuren\net al., 2022). These items were each perturbed by\none or two letters (substituting phonetically simi-\nlar sounds) producing a word not included in Uni-\nMorph. These verbs were then annotated by 28\nvolunteer annotators. Participants were asked to\nprovide the past tense of the nonce word and given\nan example (wug →wugged) and the frame “They\nLang. Train Dev Test Wug test\nEnglish 10,000 1,000 1,000 50\nGerman 10,000 1,000 1,000 174\nTamil 1,541 368 — 123\nTurkish 8,579 851 846 40\nTable 1: Data statistics. Please see AppendixA.1 for\nthe distribution of morphological tags across the differ-\nent splits for the four languages. There was not enough\ndata available for Tamil to form a test set.\nAccuracy (%)\nLang. @1 @3 @5\nEnglish 67.14 ± 17.76 85.29 ± 13.06 87.64 ± 12.13\nGerman 63.05 ± 12.62 83.80 ± 10.57 87.88 ± 10.34\nTamil 37.09 ± 26.39 43.85 ± 26.95 43.85 ± 26.95\nTable 2: Accuracy of one annotator at a time against the\njudgments of the other annotators on our collected wug\ndataset, for different values ofk. For Turkish, since the\nmorphology is deterministic, there is no variation.\n{nonce_word} all the time. In fact, they\njust yesterday.” This yielded mappings between\na lemma and a ranked list of inflected verbs, e.g.,\nveed →[veeded, ved, vode]. The modal annotation\nwas always a regularly inflected form (-ed with ap-\npropriate allomorphic variation), but other inflec-\ntional classes were attested.\n3.2 German\nThe German plural of nouns is a morphological\nphenomenon intensely studied in linguistics and\nthe cognitive sciences due to the general complex-\nity of the alternation between the eight different op-\nerations that can be used to express it. German plu-\nralization is particularly notable due to the fact that\nnone of the possible operations express it in a ma-\njority of cases (McCurdy et al., 2020). In fact, the\nmost frequent German plural noun suffix-en has\nbeen argued not to be the default (i.e., the suffix\nthat applies to novel nouns)—an honor that goes\nto -s (Marcus et al., 1995).\nTo create the dataset of novel German nonce\nnouns, we drew upon Unipseudo.2 We generated\n200 nonce words with a length between four and\nseven characters (50 nonce words per character\nlength), using German nouns as input to the algo-\nrithm. We then had one German native speaker un-\nrelated to the study (i) generate articles (der, die, or\ndas) for each of the nonce words, and (ii) generate\na plural based on the nonce words and the previ-\n2http://www.lexique.org/shiny/unipseudo/\n6510\nously selected articles. We manually filtered out\nwords whose plural is blocked by existing German\nlexemes, resulting in a final set of 174 nonce nouns.\nThese nouns were then annotated by 21 volunteer\nannotators. Participants were asked to provide the\nplural of the nonce word and were given an exam-\nple (Wug → Wugs) and the frame “Hier ist ein/e\n{nonce_word}. Jetzt sind es zwei .” Simi-\nlarly to English, this yielded mappings between a\nlemma and a ranked list of inflected nouns.\n3.3 Tamil\nTamil is a Dravidian language primarily spoken\nin regions of South India and Sri Lanka. It is an\nagglutinative languange in which verbs are conju-\ngated for tense, transitivity, person, number, and\n(in some cases) gender. For the most part, af-\nfixes display allomorphy only due to phonologi-\ncal conditioning and are otherwise invariant across\nverbs, as is the case with the person/number/gender\n(PNG) affix (Arden, 1891, 71). This is not the\ncase, however, for tense markers. Among linguists\nworking on Tamil, it is not completely agreed upon\nhow many verb classes there are in the language,\nwith some proposing up to 13 and others as few as\nthree (Lisker, 1951; Agesthialingom, 1971). In the\nspoken form of Tamil, there are points where verbs\nare part of completely different classes than their\nliterary counterpart, so in this study we focus ex-\nclusively on the written form (Schiffman and Ren-\nganathan, 2009).\nTo simplify the analysis, we utilize a modifi-\ncation of Graul’s classification seen in The En-\nglish Dictionary of the Tamil Verb, where there\nare seven primary classes (Schiffman and Ren-\nganathan, 2009). The tense most impacted by\nthese verb classes is the past tense, with each class\nhaving a unique form, while the present and future\nonly demonstrate three forms across the classes.\nAs such, we focus on the past tense and desig-\nnate the same transitivity (intransitive) and PNG\n(third person singular masculine) affix across all\nexperiments. In examining this, we gain infor-\nmation about the ways LLMs handle morpholog-\nically complex languages with inflectional classes\ndefined in both phonological and morphological\nterms. This contrasts with English, where inflec-\ntion is not agglutinative, and Turkish, where mor-\nphology is agglutinative but where there are no in-\nflectional classes.\nTo create a dataset for training the baseline\nmodels and generating samples for the few-shot\nFeatures Example\nFirst person singular\nagreement and past tense\nzöbür-ür-üm → zöbür-dü-m\nSecond person plural\nagreement,\nreported/inferential past\ntense, and negative\npolarity\nzöbür-ür-sünüz →\nzöbür-me-miş-siniz\nDative case, first person\npossessive\nzürp-ten → zürb-üm-e\nAccusative singular börüt → börüd-ü\nTable 3: Turkish tasks. Forms with colored suffixes are\nactually used in the long prompt in a contextually mean-\ningful short sentence. Hyphens represent morpheme\nboundaries. The last row is for simple inflection. The\npredicted forms (to be predicted, on the right) have the\nfollowing morphosemantics: “I [verb]-ed”, “(I heard\nthat) you have not [verb]-ed”, “to my [noun]”, “the\n[noun] (as a definite object)”.\nprompts, 86 common Tamil verbs were sampled\nand conjugated with every possible combination\nof tense and PNG suffixes. These conjugations\nwere generated automatically and then validated\nby two native speakers for accuracy. Unlike in\nthe nonce word case, there was 100% agreement\nbetween speakers. The nonce words were gener-\nated by combining syllables from real verb roots\nand checking against a Tamil dictionary to assure\nthe words created were not real. Nonce verbs\nwere created to be between two and six letters long\nto best match the distribution of real Tamil verbs.\nIn order to get the “correct” past tense for these\nverbs, five native Tamil speakers were asked to pro-\nvide past tense forms (e.g.,நிடு niʈu →[நிடுத்தான்\nniʈut̪ ːaːn, நிட்டான் niʈːaːn, நீடினான் niːʈinaːn]).\nThe mode of these responses was taken to be the\ngold form, with the level of agreement amongst\nspeakers recorded for later analysis. The compar-\natively lower inter-annotator agreement can be ex-\nplained by the lack of historical and linguistic con-\ntext given to the annotators, since a large part of\nclassification is historical.\n3.4 Turkish\nTurkish is an agglutinative language where words\nconsist of multiple morphemes attached to a root.\nSurface realizations of morphemes are influenced\nby deterministic morphophonological processes\nlike vowel harmony, consonant assimilation, and\nelision. Unlike many other languages, Turkish\nhas complex word form morphotactics, particu-\n6511\nlarly when multiple derivations are present.\nTo simplify the task and reduce the number\nof feature combinations, we utilized four datasets\nwith different levels of complexity and a limited\nnumber of inflectional features. In most cases, the\ncontext provides an inflected form with one set of\nfeatures, and the model must predict the form with\nthe requested set of features. The first three tasks\nare reinflection tasks, demanding proficiency in\nboth morphotactics and morphographemics. The\nfourth task is a straightforward inflection task (see\nTable3). Each task consists of up to five shot exam-\nples for real roots and 10 test examples with nonce\nroots. Stimuli and gold annotations were produced\nby our (single) Turkish annotator.\n4 Methodology\nWe compare the outputs of ChatGPT under a vari-\nety of prompting regimens and a substantial set of\nsupervised baselines (both neural and non-neural)\nto human annotations of the data described in Ap-\npendix 3. Results are evaluated using accuracy\nat k (acc@k), i.e., a model’s response is regarded\nas correct if it is in line with any of the topk\nhuman responses. This evaluation method takes\ninto account inter-speaker morphological variabil-\nity, which is more wide-spread than previously\nthought (Dammel and Schallert, 2019).\n4.1 Baselines\nWe investigate the efficacy of several baselines\nfor the task of morphological inflection. The cho-\nsen baselines encompass both statistical and neu-\nral architectures that have shown impressive per-\nformance on the morphological generalization task\nin recent years. We evaluate their performance on\nthe SIGMORPHON 2023 task as well as on our\nconstructed wug test set. The baselines have com-\nplementary strengths (see Section5).\n4.1.1 Training Data\nWe used the train/dev/test splits of the SIGMOR-\nPHON 2023 Inflection Shared Task3 for English\nand German. The choice of the train/dev/test splits\nwas motivated by the fact that there was no over-\nlap of lemmata between the individual splits, thus\nmimicking a wug-like setting.\nThe Turkish training data for baselines was gen-\nerated directly using a Turkish morphological ana-\n3https://github.com/sigmorphon/\n2023InflectionST\nlyzer/generator (Oflazer, 1994), because the afore-\nmentioned SIGMORPHON 2023 dataset did not\nhave a sufficient number of examples for most of\nthe feature combinations. The morphological gen-\nerator was set up to generate only Turkish word\nforms that corresponded to the selected inflectional\nmorpheme combinations we selected, forall ap-\nplicable roots. For testing, we expected the base-\nline systems to generate the word forms with the\nselected inflectional feature combinations, butfor\n10 nonce roots. The nonce roots were chosen so\nthat they would force the inflected forms to orthog-\nonally adhere to surface morphographemic con-\nstraints and rules such as various types of vowel\nharmony, consonant elision, or assimilation at mor-\npheme boundaries.\nSimilarly, for Tamil, we split the data into train\nand dev sets. Since we have a limited amount of\nTamil data, we kept the split ratio at around 4:1\nbetween train and dev sets.\nWe report the results of all baselines in Table4.\nBaselines generally perform as expected, validat-\ning our usage of them. It should be noted that Min-\nGen and AED are evaluated in IPA/feature space\nand may therefore be at a disadvantage compared\nto baselines operating directly in orthography. The\ntraining data was converted from orthography into\nIPA using Epitran (Mortensen et al., 2018).\n4.1.2 Affix Rule Learner (ARL)\nAs a baseline for the 2020 and 2021 SIGMOR-\nPHON shared tasks, a simple non-neural system\n(Liu and Mao, 2016) was implemented that uses\nedit distance to “discover prefix and suffix rules in\ntraining data.”4 At test time, the system modifies\na lemma by applying the longest matching suffix\nrule and most frequently applied prefix rule for a\ngiven morphosyntactic description.\n4.1.3 Minimal Generalization Learner\n(MinGen)\nWilson and Li(2021) proposed a minimal gener-\nalization model based on a simplified form ofAl-\nbright and Hayes (2002) to learn morphological\nrules. First, base rules that describe the changes\nneeded to convert a lemma to an inflected form are\ngenerated from training data. The rules are further\ngeneralized by comparing phonological features of\nthe rule contexts. The rules are then scored by\na confidence metric based on their accuracy and\n4https://github.com/sigmorphon/2021Task0/tree/\nmain/baselines\n6512\nEnglish German Turkish Tamil\nModel Dev Test Dev Test Dev Test Dev\nARL 95.40 96.60 77.40 79.80 94.36 93.50 85.60\nMinGen 81.40 78.70 72.70 70.70 93.65 93.03 87.23\nFIT 96.22 ± 0.19 94.93 ± 0.49 79.01 ± 1.16 81.04 ± 1.39 97.00 ± 0.22 96.25 ± 0.26 64.24 ± 3.11\nPPI 95.95 ± 0.63 94.74 ± 0.90 73.57 ± 5.37 78.26 ± 4.66 96.61 ± 0.60 96.56 ±0.66 76.76 ± 2.10\nAED 71.06 ± 5.74 70.16 ± 5.79 64.44 ± 1.85 67.44 ± 2.02 95.54 ± 0.77 95.19 ±1.41 50.70 ± 2.84\nTable 4: Results (acc@k) of the baselines on our development and test data. See Section4.1.1 for full details.\nscope. At test time, the rule with the highest score\namong the applicable rules is used.\n4.1.4 Feature Invariant Transformer (FIT)\nWu et al.(2021) proposed a simple technique em-\nploying a character-level transformer for feature-\nguided transduction that was used as a baseline for\nthe 2021 SIGMORPHON shared task.5 This is a\ngenerative model capable of performing character-\nlevel decoding to generate target inflections. In\ncomparison to a vanilla transformer model, posi-\ntional counts are used only for characters and not\nfor features. The model also incorporates unique\ntokens to mark whether a given token is a feature.\n4.1.5 Principle Parts for Inflection (PPI)\nWe apply the approach ofLiu and Hulden(2020),\nwhich recasts the task of morphological inflec-\ntion as a “paradigm cell filling problem.” This\nleverages a lexeme’s principal parts—the mini-\nmum subset of paradigm slots needed to gener-\nate the other slots in its paradigm. Specifically,\nfor low-resource scenarios, the principal parts of a\nparadigm identify additional slots that are crucial\nin generating the target-inflected lemma.\n4.1.6 Analogical Encoder-Decoder (AED)\nFollowing up onAlbright and Hayes(2003) and\nKirov and Cotterell (2018), Calderone et al.\n(2021) proposed a recurrent neural network\nencoder-decoder architecture augmented with pre-\ncompiled analogical patterns for generating mor-\nphological inflections of nonce words. This model\nleverages the UniMorph Tags and fine alternation\npattern (FAP) associated with each lemma in rela-\ntion to its inflection form. FAPs analyze the posi-\ntioning of word forms within the system to iden-\ntify recurrent patterns representing conventional\nlinguistic elements.\n5https://github.com/sigmorphon/2021Task0/tree/\nmain/baselines\n4.2 Prompting\nWe employ three distinct prompting styles, namely\nzero-, one-, and few-shot, to interact with the lan-\nguage model. We start with a simple instruction in\neach language, for example:\n“Fill in the blank with the correct past\ntense of the word ‘wug’. Give your re-\nsponse in one word.\nThey wug all the time. In fact, they\njust yesterday.”\nFor Tamil, the instruction portion of the prompt is\nomitted because of ChatGPT’s unreliable perfor-\nmance when given instructions in that language.\nWe select one example with real words for each ma-\njor inflection class of the phenomenon in question.\nWe then perform multiple runs: 10 for the zero-\nshot scenario, one for every shot for the one-shot\nscenario, and 10 for the few-shot scenario, with\na new random permutation of all examples each\ntime. We querygpt-3.5-turbo-0613, select the\nfirst word of the response, and filter by removing\nnon-word characters. We evaluate by computing\nthe accuracy for each of the runs, averaged over\nall queried nonce words, and compute the mean\nand standard deviation across all runs. We employ\nacc@k as our evaluation metric, settingk = 5 for\nour main evaluation. We provide results fork = 1\nand k = 3 in AppendixA.4. The k gold forms are\nthe k responses most frequently generated by hu-\nmans. Since only one Turkish response is possible\n(the morphology is deterministic),k is always 1 for\nthis language. We then perform an additional ex-\nperiment for comparison in which we remove the\ncontext around the nonce word and only give the\ninstructions as well as the last line. We call this\nthe short prompt and the original described above\nthe long prompt. We provide instances oflong and\nshort prompt in AppendixA.5.\n6513\n5 Results\n5.1 Overall Performance\nFor acc@5, the performance of ChatGPT never ex-\nceeded that of the strongest baselines (ARL, AED,\nand PPI) regardless of the prompting regime, as\nshown in Table5. However, it beats certain older\nbaselines such as MinGen (the minimum general-\nization learner). ChatGPT performed best when\nit was explicitly prompted to complete an analogy\nwith a single example (i.e., short 1-shot), as can be\nseen in Figure2. We observe that similar trends\nhold for acc@1 and acc@3 (see AppendixA.4),\nbut the gap between the strongest baselines and\nChatGPT decreases withk.\nEnglish ChatGPT’s performance on English was\nuniformly worse than both the average annota-\ntor (87.64%) and the strongest baselines.acc@1\nfalls below 60% in the 0-shot condition but is\nmarkedly better when shots are supplied. Short\nprompts, which require the model to complete\na simple analogy, resulted in better performance\nthan long prompts. In all conditions, authentic En-\nglish words that did not occur in the reference anno-\ntations appeared as outputs when the nonce word\nand the authentic word were orthographically sim-\nilar (see the discussion in Section6.4).\nGerman The best German result was 88.94%\n(short 1-shot), which beat all of the baselines ex-\ncept for ARL and FIT. The other results are simi-\nlarly strong in contrast to the other languages. The\nimpact of k is not noticeable here. This, in com-\nbination with the fact that the human performance\non acc@5 was 88%, indicates that the task is per-\nfectly performed by ChatGPT. It has reached the\nupper bound given by the inherent subjectivity of\nthe task (reflected in the human variability) and the\nimpact ofk is, therefore, not measurable. This is\nfurther solidified by the very small impact of the\nlong vs. short prompts.\nTamil Tamil performance of ChatGPT was sig-\nnificantly worse than the provided baselines, even\nin the few-shot conditions. For the few-shot case,\nthere was marginally better performance when us-\ning short prompts, but this did not apply to the 0-\nor 1-shot case (in which no accurate outputs were\ngenerated). Across the board, the performance on\nTamil was markedly worse than performance on\nEnglish and German. However, considering that\nthe average annotator had only 43.85% accuracy\nlong/uni00A00/uni00ADshot long/uni00A01/uni00ADshot long/uni00A0few/uni00ADshot\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8Accuracy\nEnglish/uni00A0(long)\nEnglish/uni00A0(short)\nGerman/uni00A0(long)\nGerman/uni00A0(short)\nTurkish/uni00A0(long)\nTurkish/uni00A0(short)\nTamil/uni00A0(long)\nTamil/uni00A0(short)\nk=1\nk=3\nk=5\nFigure 2: Results for the different prompt scenarios, for-\nmats, languages, and values ofk.\nagainst the judgments of the other annotators, the\nfew-shot accuracy is quite reasonable.\nTurkish The prompting performance for the\nTurkish inflection task is worse than for English\nand German, especially in the long prompt case.\nFor this task, the morphotactics is trivial but the\nselection of the allomorph depends on stem vow-\nels, stem-final consonants, whether there is a con-\nsonant cluster ending the stem, and whether the\nstem is monosyllabic or not. ChatGPT gets better\nresults with the short prompt through an analogi-\ncal example. For the three reinflection tasks, Chat-\nGPT gets mixed results that are overall worse than\nfor the inflection task (see Table6).\n6 Analysis\n6.1 The Nature of the Task\nThe inherent complexity of the inflection tasks for\nthe various languages (and the reinflection task for\nTurkish) varies greatly. English and Turkish are\nthe simplest: the top-ranked form can always be\nobtained by adding a single suffix and applying a\nfew morphographemic alternations. German anno-\ntations show no dominant pattern and assign nonce\nwords to morphological classes according to com-\nplex criteria. However, German performance is\nclearly better, suggesting that factors other than in-\nherent complexity play a role in ChatGPT’s ability\nto generalize morphological patterns.\n6.2 Impact of Tokenization\nThere is mounting evidence that the morpho-\nlogically suboptimal nature of many tokenizers\nmay limit the morphological capabilities of LLMs\n(Bostrom and Durrett, 2020; Hofmann et al., 2021).\nChatGPT’s tokenization, i.e., byte-pair encoding\n6514\nMethod English German Tamil Turkish\nARL 100.00 94.25 61.48 60.00\nMinGen 62.00 64.37 49.18 40.00\nFIT 98.00 ± 1.26 92.87 ± 0.74 63.28 ± 3.36 67.00 ± 4.58\nPPI 94.60 ± 2.54 85.98 ± 5.91 55.33 ± 1.84 68.00 ± 4.00\nAED 57.60 ± 6.62 48.51 ± 5.45 58.69 ± 5.46 56.00 ± 4.90\nlong 0-shot 58.40 ± 5.28 86.49 ± 1.07 0.00 28.00 ± 14.00\nlong 1-shot 73.60 ± 6.97 85.42 ± 2.52 14.52 ± 7.48 20.00 ± 14.14\nlong few-shot 76.40 ± 4.45 87.36 ± 2.37 42.70 ± 3.96 54.00 ± 10.20\nshort 0-shot 75.40 ± 5.87 88.62 ± 1.64 0.00 3.00 ± 4.58\nshort 1-shot 82.80 ± 5.60 88.94 ± 2.35 3.28 ± 3.99 58.00 ± 7.48\nshort few-shot 78.60 ± 2.84 88.33 ± 1.15 43.36 ± 3.12 59.00 ± 9.43\nTable 5: Results (acc@k) for all languages (k = 5 except for Turkish wherek = 1, cf. Section4.2).\nType 0-shot 1-shot few-shot\nlong 3.00 ± 1.80 20.67 ± 5.73 33.33 ± 4.94\nshort 7.00 ± 4.33 18.67 ± 6.18 31.00 ± 4.23\nTable 6: Results for Turkish averaged over the three\nreinflection tasks (k = 1).\n(Sennrich et al., 2016), has been shown to be par-\nticularly problematic (Bostrom and Durrett, 2020;\nHofmann et al., 2022).\nTo examine the impact of tokenization, we mea-\nsured the number of tokens into which the nonce\nwords are split for the individual languages and\ncomputed the accuracy as a function of the num-\nber of tokens. Our hypothesis was that longer to-\nken sequences are less optimal, potentially leading\nto worse performance. However, using two-sided\nt-tests, we did not find a significant difference be-\ntween nonce words with different token lengths.\nWe interpret this as indicating that tokenization\nplays a less pronounced role for ChatGPT.\n6.3 Impact of k\nWe observe that the gap between the baselines\nand our results increases withk (see Table5, Ap-\npendix A.4), suggesting that ChatGPT tends to\ngenerate either a top-ranked form or an implausi-\nble inflection while the baselines tend to produce\nplausible inflections which are less frequent in the\nhuman annotations. ChatGPT’s penchant for im-\nplausible inflections may be a result of its real word\nbias (see Section6.4 below).\n6.4 Real Word Bias\nIn English and German—and to a lesser extent in\nTurkish—many of the forms generated by Chat-\nGPT belong to a different lexeme than the nonce\nword and thus do not constitute inflections in any\nstrict linguistic sense (see Section2.1). Crucially,\nthe stem of the generated form is always a real\nword (i.e., a word that exists in the respective lan-\nguage). Examples of this phenomenon include, for\nEnglish: did as the past tense ofdedo, blushed as\nthe past tense ofblus, fried as the past tense of\nfride; and for German:Ozeane (‘oceans’) as the\nplural of Ozeak, Institute (‘institutes’) as the plu-\nral of Instite, Sklaven (‘slaves’) as the plural of\nSchlave. It is important to notice that in all these\ncases, (i) the generated form has the correct mor-\nphological properties—e.g., the English formsdid,\nblushed, fried are indeed past tense forms—but the\nstem is a real word rather than the nonce word, and\n(ii) the stem that is generated in lieu of the nonce\nword is a frequently occurring word in the respec-\ntive language and has a certain (sometimes strong)\northographic similarity to the nonce word. We de-\nnote this tendencyreal word bias.\nThe concept of real word bias allows us to make\na hypothesis about the way in which ChatGPT ad-\ndresses morphological tasks. We think ChatGPT is\nnot applying morphological rules to a stem, which\nwould be in line with item-and-process accounts\nof morphology (Hockett, 1954). Rather, it seems\nto linguistically decode the point in its representa-\ntional space defined by the semantic constraints in\nthe prompt. In cases where this point (and its im-\nmediate neighborhood) is unoccupied, it generates\na form based on the nonce word, but in cases where\nthere is a form of a real word close to the point\n(e.g., because of superficial orthographic similar-\nity), it generates this form instead. The fact that the\nreal word bias is strongest for German and English\n(the two high-resource languages) suggests that the\nrepresentational space is more dense for these two\nlanguages, increasing the probability that there is a\nreal word close to the point that the model is trying\n6515\nNo change\n+ e\n+ en\n+ er\n+ s\nVowel change\nVowel change + e\nVowel change + er\nReal Word\nUnknown\nPredicted\nNo change\n+ e\n+ en\n+ s\nVowel change + e\nUnknown\nGold\n317 6 0 0 10 0 0 0 9 8\n47 262 77 5 79 0 13 5 22 19\n3 20 64 0 18 4 0 0 0 11\n25 15 5 0 109 0 1 0 1 4\n0 13 0 0 1 0 22 3 0 1\n7 3 0 0 13 0 0 0 0 25\nFigure 3: Confusion matrix for competing German plu-\nral morphemes for the few-shot setting.\nto decode based on the prompt.\n6.5 Morphological Productivity\nThe productivity of a morpheme is traditionally de-\nfined as its propensity to be used in novel combi-\nnations (Plag, 1999; Bauer, 2001; Haspelmath and\nSims, 2010). Crucially, morphemes with the same\nmeaning can differ in their productivity—for ex-\nample, for English deadjectival nominalizing suf-\nfixes, -ness (e.g., robustness) is generally more pro-\nductive than -ity (e.g, equality), which in turn is\nmore productive than the fully non-productive-th\n(e.g., warmth). We are interested to see whether\nthere is any difference in the productivity of mor-\nphological patterns exhibited by ChatGPT com-\npared to the human sample. We focus on German\nas it has the most complex pattern of competing\nmorphemes, and we examine the few-shot results\nas they show the best performance overall.\nWe start by comparing the distribution over al-\nternative plural morphemes generated by ChatGPT\nwith the human responses. As shown in Figure3,\nthere are several morphemes that are used by Chat-\nGPT similarly to humans (e.g., the null morpheme).\nCases of overgeneralization, where ChatGPT sys-\ntematically generalizes the usage of a particular\nsuffix to contexts where the suffix is not used\nby humans, are mainly limited to two plural mor-\nphemes: -en (77 generations for gold morpheme\n-e) and -s (79 generations for gold morpheme-e).\nInterestingly, these two plural morphemes are the\ntwo most productive plural morphemes in Ger-\nman (Köpcke, 1988). This indicates two important\npoints: (i) ChatGPT is sensitive to the productiv-\nity of morphemes, i.e., it has acquired the ability\nto model how productive certain morphemes are\nas a result of pretraining; (ii) it does not identically\nmirror the behavior of humans, but rather ampli-\nfies the productivity of certain morphemes. The\nfinding that the most productive morphemes (for\nhumans) are becoming more productive for Chat-\nGPT while the least productive morphemes (for hu-\nmans) are becoming less productive for ChatGPT\nbears some theoretical resemblance to discussions\nabout bias amplification (Ahn et al., 2022).\n7 Future Directions\nMorphological patterns are only one kind of gen-\neralization that can be investigated through a wug-\nlike experimental paradigm. The form-meaning re-\nlationships encoded in language and multimodal\nmodels, including constructional and iconic pair-\nings, can be investigated through prompting with\nnonce stimuli, leading to new insights regarding\nthe generalizations they capture.\nLimitations\nOur research was conducted with a single model\n(gpt-3.5-turbo-0613), so it is not certain that our\nresults will generalize to other versions of GPT-3\nor to GPT-4, let alone other LLMs. Although\nwe went to great lengths to develop prompts that\nwould maximize ChatGPT’s performance on the\ntasks, it is not possible to state definitively that\nanother strategy would not produce better perfor-\nmance. While the languages were typologically\nvaried, it is not clear whether the results observed\nin the current study are generally robust or are co-\nincidental properties of the small set of languages\nand datasets under investigation. Furthermore,\ncomparing the languages to one another is prob-\nlematic because it was not possible to control other\nvariables while varying the language. For example,\nthe English and Tamil tasks involve verbal inflec-\ntion while the German and Turkish tasks involve\nnominal inflection. Finally, the number of annota-\ntors for Tamil was very small and inter-annotator\nagreement was very low, meaning that the results\nof the Tamil experiments must be approached with\nspecial caution (but see our discussion about mor-\nphological variation in Section3).\nEthics\nLLMs are already impacting the world’s people in\nsignificant ways, for good and ill. Understanding\ntheir limitations, particularly with regard to non-\nhegemonic language communities, is an ethical im-\n6516\nperative. This study highlights one specific way\nin which an LLM should not be treated as a sur-\nrogate human, thus motivating additional research\non language modeling for structurally diverse and\nlow-resource languages.\nAcknowledgements\nThis work was funded by the European Research\nCouncil (#740516). The second author was also\nsupported by the German Academic Scholarship\nFoundation. We thank the reviewers for their ex-\ntremely helpful comments.\nReferences\nS Agesthialingom. 1971. A note on Tamil verbs.An-\nthropological Linguistics, pages 121–125.\nJaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh.\n2022. Why knowledge distillation amplifies gen-\nder bias and how to mitigate from the perspective\nof DistilBERT. In Proceedings of the 4th Work-\nshop on Gender Bias in Natural Language Process-\ning (GeBNLP), pages 266–272, Seattle, Washington.\nAssociation for Computational Linguistics.\nAdam Albright and Bruce Hayes. 2002.Modeling en-\nglish past tense intuitions with minimal generaliza-\ntion. In Proceedings of the ACL-02 Workshop on\nMorphological and Phonological Learning - Volume\n6, MPL ’02, page 58–69, USA. Association for Com-\nputational Linguistics.\nAdam Albright and Bruce Hayes. 2003. Rules\nvs. analogy in English past tenses: A computa-\ntional/experimental study. Cognition, 90(2):119–\n161.\nAlbert Henry Arden. 1891. A progressive grammar\nof common Tamil. Society for Promoting Christian\nKnowledge.\nKhuyagbaatar Batsuren, Omer Goldman, Salam Khal-\nifa, Nizar Habash, Witold Kieraś, Gábor Bella,\nBrian Leonard, Garrett Nicolai, Kyle Gorman, Yusti-\nnus Ghanggo Ate, Maria Ryskina, Sabrina Mielke,\nElena Budianskaya, Charbel El-Khaissi, Tiago Pi-\nmentel, Michael Gasser, William Abbott Lane,\nMohit Raj, Matt Coler, Jaime Rafael Montoya\nSamame, Delio Siticonatzi Camaiteri, Esaú Zu-\nmaeta Rojas, Didier López Francis, Arturo Once-\nvay, Juan López Bautista, Gema Celeste Silva Vil-\nlegas, Lucas Torroba Hennigen, Adam Ek, David\nGuriel, Peter Dirix, Jean-Philippe Bernardy, An-\ndrey Scherbakov, Aziyana Bayyr-ool, Antonios\nAnastasopoulos, Roberto Zariquiey, Karina Sheifer,\nSofya Ganieva, Hilaria Cruz, Ritván Karahóǧa,\nStella Markantonatou, George Pavlidis, Matvey Plu-\ngaryov, Elena Klyachko, Ali Salehi, Candy An-\ngulo, Jatayu Baxi, Andrew Krizhanovsky, Na-\ntalia Krizhanovskaya, Elizabeth Salesky, Clara Va-\nnia, Sardana Ivanova, Jennifer White, Rowan Hall\nMaudslay, Josef Valvoda, Ran Zmigrod, Paula\nCzarnowska, Irene Nikkarinen, Aelita Salchak,\nBrijesh Bhatt, Christopher Straughn, Zoey Liu,\nJonathan North Washington, Yuval Pinter, Duygu\nAtaman, Marcin Wolinski, Totok Suhardijanto,\nAnna Yablonskaya, Niklas Stoehr, Hossep Dolatian,\nZahroh Nuriah, Shyam Ratan, Francis M. Tyers,\nEdoardo M. Ponti, Grant Aiton, Aryaman Arora,\nRichard J. Hatcher, Ritesh Kumar, Jeremiah Young,\nDaria Rodionova, Anastasia Yemelina, Taras An-\ndrushko, Igor Marchenko, Polina Mashkovtseva,\nAlexandra Serova, Emily Prud’hommeaux, Maria\nNepomniashchaya, Fausto Giunchiglia, Eleanor\nChodroff, Mans Hulden, Miikka Silfverberg, Arya D.\nMcCarthy, David Yarowsky, Ryan Cotterell, Reut\nTsarfaty, and Ekaterina Vylomova. 2022.UniMorph\n4.0: Universal Morphology. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 840–855, Marseille, France. Euro-\npean Language Resources Association.\nLaurie Bauer. 2001.Morphological productivity. Cam-\nbridge University Press, Cambridge (UK).\nRachel Bawden, Jonathan Poinhos, Eleni Kogkitsidou,\nPhilippe Gambette, Benoît Sagot, and Simon Gabay.\n2022. Automatic normalisation of early Modern\nFrench. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pages 3354–\n3366, Marseille, France. European Language Re-\nsources Association.\nJean Berko. 1958. The child’s learning of English mor-\nphology. Word, 14(2-3):150–177.\nKaj Bostrom and Greg Durrett. 2020.Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4617–4624, Online.\nAssociation for Computational Linguistics.\nRomina Brown, Santiago Paez, Gonzalo Herrera, Luis\nChiruzzo, and Aiala Rosá. 2023.Experiments on au-\ntomatic error detection and correction for uruguayan\nlearners of English. In Proceedings of the 12th\nWorkshop on NLP for Computer Assisted Language\nLearning, pages 45–52, Tórshavn, Faroe Islands.\nLiU Electronic Press.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023.Sparks of Artificial General In-\ntelligence: Early experiments with GPT-4.\nBasilio Calderone, Nabil Hathout, and Olivier Bonami.\n2021. Not quite there yet: Combining analogical\npatterns and encoder-decoder networks for cogni-\ntively plausible inflection. In Proceedings of the\n18th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 274–282, Online. Association for Computa-\ntional Linguistics.\n6517\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022.PaLM: Scal-\ning Language Modeling with Pathways . Arxiv,\n2204.02311.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A. Smith.\n2021. All that’s ‘human’ is not gold: Evaluating\nhuman evaluation of generated text. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 7282–7296,\nOnline. Association for Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nGéraldine Walther, Ekaterina Vylomova, Arya D.\nMcCarthy, Katharina Kann, Sabrina J. Mielke, Gar-\nrett Nicolai, Miikka Silfverberg, David Yarowsky,\nJason Eisner, and Mans Hulden. 2018.The CoNLL–\nSIGMORPHON 2018 shared task: Universal mor-\nphological reinflection. In Proceedings of the\nCoNLL–SIGMORPHON 2018 Shared Task: Univer-\nsal Morphological Reinflection, pages 1–27, Brus-\nsels. Association for Computational Linguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nGéraldine Walther, Ekaterina Vylomova, Patrick Xia,\nManaal Faruqui, Sandra Kübler, David Yarowsky,\nJason Eisner, and Mans Hulden. 2017a. CoNLL-\nSIGMORPHON 2017 shared task: Universal mor-\nphological reinflection in 52 languages. In Pro-\nceedings of the CoNLL SIGMORPHON 2017 Shared\nTask: Universal Morphological Reinflection, pages\n1–30, Vancouver. Association for Computational\nLinguistics.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016a. The SIGMORPHON 2016 shared Task—\nMorphological reinflection. In Proceedings of the\n14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 10–22, Berlin, Germany. Association for Com-\nputational Linguistics.\nRyan Cotterell, Arun Kumar, and Hinrich Schütze.\n2016b. Morphological segmentation inside-out. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, pages\n2325–2330, Austin, Texas. Association for Compu-\ntational Linguistics.\nRyan Cotterell, Thomas Müller, Alexander Fraser, and\nHinrich Schütze. 2015.Labeled morphological seg-\nmentation with semi-Markov models. In Proceed-\nings of the Nineteenth Conference on Computational\nNatural Language Learning, pages 164–174, Bei-\njing, China. Association for Computational Linguis-\ntics.\nRyan Cotterell, Tim Vieira, and Hinrich Schütze. 2016c.\nA joint model of orthography and morphological seg-\nmentation. InProceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 664–669, San Diego, California. As-\nsociation for Computational Linguistics.\nRyan Cotterell, Ekaterina Vylomova, Huda Khayral-\nlah, Christo Kirov, and David Yarowsky. 2017b.\nParadigm completion for derivational morphology.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n714–720, Copenhagen, Denmark. Association for\nComputational Linguistics.\nAntje Dammel and Oliver Schallert. 2019.Morpholog-\nical variation: Theoretical and empirical perspec-\ntives. John Benjamins, Amsterdam.\nDaniel Deutsch, John Hewitt, and Dan Roth. 2018.A\ndistributional and orthographic aggregation model\nfor English derivational morphology. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1938–1947, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRobert M. Dixon. 1994. Ergativity. Cambridge Uni-\nversity Press, Cambridge, UK.\nDaniel Edmiston. 2020.A systematic analysis of mor-\nphological content in BERT models for multiple lan-\nguages. Arxiv, abs/2004.03032.\nAlexander Erdmann, Micha Elsner, Shijie Wu, Ryan\nCotterell, and Nizar Habash. 2020a.The paradigm\ndiscovery problem. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7778–7790, Online. Association\nfor Computational Linguistics.\n6518\nAlexander Erdmann, Tom Kenter, Markus Becker, and\nChristian Schallhart. 2020b. Frugal paradigm com-\npletion. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8248–8273, Online. Association for Computa-\ntional Linguistics.\nYoav Goldberg. 2019.Assessing BERT’s syntactic abil-\nities. Arxiv, 1901.05287.\nOmer Goldman, David Guriel, and Reut Tsarfaty. 2022.\n(Un)solving morphological inflection: Lemma over-\nlap artificially inflates models’ performance. InPro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 864–870, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nColeman Haley. 2020.This is a BERT. now there are\nseveral of them. can they generalize to novel words?\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 333–341, Online. Association for Com-\nputational Linguistics.\nHarald Hammarström and Lars Borin. 2011.Unsuper-\nvised learning of morphology. Computational Lin-\nguistics, 37(2):309–350.\nMartin Haspelmath and Andrea Sims. 2010. Under-\nstanding Morphology. Routledge, Oxford (UK).\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas\nRaunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are GPT models\nat machine translation? a comprehensive evaluation.\nArxiv, 2302.09210.\nCharles F. Hockett. 1954. Two Models of Grammatical\nDescription. WORD, 10(2-3):210–234.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2020a. DagoBERT: Generating deriva-\ntional morphology with a pretrained language model.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3848–3861, Online. Association for Computa-\ntional Linguistics.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2020b. Predicting the growth of morpho-\nlogical families from social and linguistic factors. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7273–\n7283, Online. Association for Computational Lin-\nguistics.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Superbizarre is not superb: Deriva-\ntional morphology improves BERT’s interpretation\nof complex words. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3594–3608, Online. Associa-\ntion for Computational Linguistics.\nValentin Hofmann, Hinrich Schuetze, and Janet Pierre-\nhumbert. 2022. An embarrassingly simple method\nto mitigate undesirable properties of pretrained lan-\nguage model tokenizers. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n385–393, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nValentin Hofmann, Hinrich Schütze, and Janet Pierre-\nhumbert. 2020c. A graph auto-encoder model of\nderivational morphology. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1127–1138, Online. Asso-\nciation for Computational Linguistics.\nAlon Jacovi, Avi Caciularu, Omer Goldman, and Yoav\nGoldberg. 2023. Stop uploading test data in plain\ntext: Practical strategies for mitigating data contam-\nination by evaluation benchmarks. arXiv preprint\narXiv:2305.10160.\nWenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang,\nand ZP Tu. 2023.Is ChatGPT a good translator? Yes\nwith GPT-4 as the engine. arXiv, 2301.08745.\nHuiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya\nMcCarthy, and Katharina Kann. 2020. Unsuper-\nvised morphological paradigm completion. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6696–\n6707, Online. Association for Computational Lin-\nguistics.\nZijia Jin, Xingyu Zhang, Mo Yu, and Lifu Huang. 2022.\nProbing script knowledge from pre-trained models.\nIn Proceedings of the Workshop on Unimodal and\nMultimodal Induction of Linguistic Structures (UM-\nIoS), pages 87–93, Abu Dhabi, United Arab Emi-\nrates (Hybrid). Association for Computational Lin-\nguistics.\nKatharina Kann, Ryan Cotterell, and Hinrich Schütze.\n2016. Neural morphological analysis: Encoding-\ndecoding canonical segments. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 961–967, Austin,\nTexas. Association for Computational Linguistics.\nKatharina Kann, Ryan Cotterell, and Hinrich Schütze.\n2017. Neural multi-source morphological reinflec-\ntion. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n514–524, Valencia, Spain. Association for Compu-\ntational Linguistics.\nKatharina Kann and Hinrich Schütze. 2016.MED: The\nLMU system for the SIGMORPHON 2016 shared\ntask on morphological reinflection. In Proceedings\nof the 14th SIGMORPHON Workshop on Computa-\ntional Research in Phonetics, Phonology, and Mor-\nphology, pages 62–70, Berlin, Germany. Associa-\ntion for Computational Linguistics.\n6519\nChristo Kirov and Ryan Cotterell. 2018.Recurrent neu-\nral networks in linguistic theory: Revisiting pinker\nand prince (1988) and the past tense debate. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:651–665.\nKlaus-Michael Köpcke. 1988. Schemas in German plu-\nral formation.Lingua, 74(4):303–335.\nLeigh Lisker. 1951. Tamil verb classification.Journal\nof the American Oriental Society, 71(2):111–114.\nLing Liu and Mans Hulden. 2020.Leveraging princi-\npal parts for morphological inflection. In Proceed-\nings of the 17th SIGMORPHON Workshop on Com-\nputational Research in Phonetics, Phonology, and\nMorphology, pages 153–161, Online. Association\nfor Computational Linguistics.\nLing Liu and Lingshuang Jack Mao. 2016.Morpho-\nlogical reinflection with conditional random fields\nand unsupervised features. In Proceedings of the\n14th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 36–40, Berlin, Germany. Association for Com-\nputational Linguistics.\nXiaomeng Ma and Lingyu Gao. 2022. How do we\nget there? Evaluating transformer neural networks\nas cognitive models for English past tense inflec-\ntion. In Proceedings of the 2nd Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 12th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1101–1114, Online only.\nAssociation for Computational Linguistics.\nInbal Magar and Roy Schwartz. 2022.Data contami-\nnation: From memorization to exploitation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 157–165, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nGary F Marcus, Ursula Brinkmann, Harald Clahsen,\nRichard Wiese, and Steven Pinker. 1995. German\ninflection: The exception that proves the rule.Cog-\nnitive psychology, 29(3):189–256.\nRowan Hall Maudslay and Ryan Cotterell. 2021.Do\nsyntactic probes probe syntax? experiments with jab-\nberwocky probing. InProceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 124–131, Online. Associ-\nation for Computational Linguistics.\nKate McCurdy, Sharon Goldwater, and Adam Lopez.\n2020. Inflecting when there’s no majority: Limi-\ntations of encoder-decoder neural networks as cog-\nnitive models for German plurals. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1745–1756, On-\nline. Association for Computational Linguistics.\nDavid R. Mortensen, Siddharth Dalmia, and Patrick Lit-\ntell. 2018. Epitran: Precision G2P for many lan-\nguages. InProceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Paris, France. European Language Re-\nsources Association (ELRA).\nKemal Oflazer. 1994. Two-level description of Turk-\nish morphology.Literary and Linguistic Computing,\n9(2):137–148.\nOpenAI. 2023a. ChatGPT. Large language model.\nOpenAI. 2023b.GPT-4 Technical Report.\nTiago Pimentel, Maria Ryskina, Sabrina J. Mielke,\nShijie Wu, Eleanor Chodroff, Brian Leonard, Gar-\nrett Nicolai, Yustinus Ghanggo Ate, Salam Khal-\nifa, Nizar Habash, Charbel El-Khaissi, Omer Gold-\nman, Michael Gasser, William Lane, Matt Coler,\nArturo Oncevay, Jaime Rafael Montoya Samame,\nGema Celeste Silva Villegas, Adam Ek, Jean-\nPhilippe Bernardy, Andrey Shcherbakov, Aziyana\nBayyr-ool, Karina Sheifer, Sofya Ganieva, Matvey\nPlugaryov, Elena Klyachko, Ali Salehi, Andrew\nKrizhanovsky, Natalia Krizhanovsky, Clara Va-\nnia, Sardana Ivanova, Aelita Salchak, Christo-\npher Straughn, Zoey Liu, Jonathan North Wash-\nington, Duygu Ataman, Witold Kieraś, Marcin\nWoliński, Totok Suhardijanto, Niklas Stoehr, Zahroh\nNuriah, Shyam Ratan, Francis M. Tyers, Edoardo M.\nPonti, Grant Aiton, Richard J. Hatcher, Emily\nPrud’hommeaux, Ritesh Kumar, Mans Hulden,\nBotond Barta, Dorina Lakatos, Gábor Szolnok, Ju-\ndit Ács, Mohit Raj, David Yarowsky, Ryan Cotterell,\nBen Ambridge, and Ekaterina Vylomova. 2021.SIG-\nMORPHON 2021 shared task on morphological re-\ninflection: Generalization across languages. In Pro-\nceedings of the 18th SIGMORPHON Workshop on\nComputational Research in Phonetics, Phonology,\nand Morphology, pages 229–259, Online. Associa-\ntion for Computational Linguistics.\nSteven Pinker and Alan Prince. 1988. On language and\nconnectionism: Analysis of a parallel distributed pro-\ncessing model of language acquisition. Cognition,\n28(1-2):73–193.\nIngo Plag. 1999. Morphological productivity: Struc-\ntural constraints in English derivation. De Gruyter,\nBerlin.\nKim Plunkett and Patrick Juola. 1999. A connectionist\nmodel of English past tense and plural morphology.\nCognitive Science, 23(4):463–490.\nDavid E Rumelhart and James L McClelland. 1986a.\nOn learning the past tenses of English verbs. InPar-\nallel Distributed Processing: Explorations in the Mi-\ncrostructure of Cognition, volume 2, pages 216–271.\nMIT Press, Cambridge, MA.\nDavid E. Rumelhart and James L. McClelland. 1986b.\nParallel Distributed Processing: Explorations in the\nMicrostructure of Cognition: Foundations . The\nMIT Press.\n6520\nHarold F Schiffman and Vasu Renganathan. 2009.An\nEnglish dictionary of the Tamil verb. Linguistic Data\nConsortium.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nMiikka Silfverberg, Adam Wiemerslage, Ling Liu, and\nLingshuang Jack Mao. 2017.Data augmentation for\nmorphological reinflection. In Proceedings of the\nCoNLL SIGMORPHON 2017 Shared Task: Univer-\nsal Morphological Reinflection, pages 90–99, Van-\ncouver. Association for Computational Linguistics.\nRadu Soricut and Franz Och. 2015.Unsupervised mor-\nphology induction using word embeddings. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1627–1637, Denver, Colorado. Association for Com-\nputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothee Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. 2023.\nLLaMA: Open and Efficient Foundation Language\nModels.\nMichael T Ullman, Suzanne Corkin, Marie Coppola,\nGregory Hickok, John H Growdon, Walter J Ko-\nroshetz, and Steven Pinker. 1997. A neural dissoci-\nation within language: Evidence that the mental dic-\ntionary is part of declarative memory, and that gram-\nmatical rules are processed by the procedural system.\nJournal of Cognitive Neuroscience, 9(2):266–276.\nEkaterina Vylomova, Ryan Cotterell, Timothy Baldwin,\nand Trevor Cohn. 2017. Context-aware prediction\nof derivational word-forms. In Proceedings of the\n15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 2,\nShort Papers, pages 118–124, Valencia, Spain. As-\nsociation for Computational Linguistics.\nEkaterina Vylomova, Jennifer White, Eliza-\nbeth Salesky, Sabrina J. Mielke, Shijie Wu,\nEdoardo Maria Ponti, Rowan Hall Maudslay, Ran\nZmigrod, Josef Valvoda, Svetlana Toldova, Francis\nTyers, Elena Klyachko, Ilya Yegorov, Natalia\nKrizhanovsky, Paula Czarnowska, Irene Nikkarinen,\nAndrew Krizhanovsky, Tiago Pimentel, Lucas\nTorroba Hennigen, Christo Kirov, Garrett Nicolai,\nAdina Williams, Antonios Anastasopoulos, Hilaria\nCruz, Eleanor Chodroff, Ryan Cotterell, Miikka\nSilfverberg, and Mans Hulden. 2020. SIGMOR-\nPHON 2020 shared task 0: Typologically diverse\nmorphological inflection. In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 1–39, Online. Association for Computational\nLinguistics.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui\nZhang, Dian Yu, Shuming Shi, and Zhaopeng Tu.\n2023. Document-level machine translation with\nlarge language models. arXiv, 2304.02210.\nLeonie Weissweiler, Valentin Hofmann, Masoud\nJalili Sabet, and Hinrich Schuetze. 2022.CaMEL:\nCase Marker Extraction without Labels. InProceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5506–5516, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nColin Wilson and Jane S.Y . Li. 2021.Were we there\nalready? Applying minimal generalization to the\nSIGMORPHON-UniMorph shared task on cogni-\ntively plausible morphological inflection. In Pro-\nceedings of the 18th SIGMORPHON Workshop on\nComputational Research in Phonetics, Phonology,\nand Morphology, pages 283–291, Online. Associa-\ntion for Computational Linguistics.\nShijie Wu, Ryan Cotterell, and Mans Hulden. 2021.Ap-\nplying the transformer to character-level transduc-\ntion. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 1901–1907,\nOnline. Association for Computational Linguistics.\nHongzhi Xu, Mitchell Marcus, Charles Yang, and Lyle\nUngar. 2018. Unsupervised morphology learning\nwith statistical paradigms. In Proceedings of the\n27th International Conference on Computational\nLinguistics, pages 44–54, Santa Fe, New Mexico,\nUSA. Association for Computational Linguistics.\nA Appendices\nA.1 Morphological Tags\nIn Table7, we provide details about the morpholog-\nical tags that are comprised by the train, dev, test,\nand wug test sets for the four languages. The tags\nfor English (eng), German (deu), and Tamil (tam)\nare defined in accordance with the description in\nUniMorph 4.0 dataset. For Turkish (tur),the tags\nare defined in Section3.\nA.2 Hyperparameter Tuning\nFor all baselines, we follow the hyperparameter\nsettings from the publicly available code reposito-\nries. The only exception is AED, where the num-\nber of epochs was increased from 40 to 200.\nA.3 Qualtrics Details\nOur study leveraged Qualtrics, a robust and com-\nprehensive survey software tool that facilitates the\n6521\nLang Tags Train Dev Test Wug\neng V;NFIN 2015 206 202 0\neng V;PRS;NOM(3,SG) 1987 190 213 0\neng V;PST 1981 198 185 50\neng V;V .PTCP;PRS 2018 201 200 0\neng V;V .PTCP;PST 1999 205 200 0\ndeu V .PTCP;PRS 246 19 19 0\ndeu V;IMP;NOM(2,PL) 246 19 16 0\ndeu V;IMP;NOM(2,SG) 241 22 17 0\ndeu V;IND;PRS;NOM(1,PL) 246 20 18 0\ndeu V;IND;PRS;NOM(1,SG) 227 21 17 0\ndeu V;IND;PRS;NOM(2,PL) 250 29 21 0\ndeu V;IND;PRS;NOM(2,SG) 258 25 15 0\ndeu V;IND;PRS;NOM(3,SG) 233 21 22 0\ndeu V;IND;PST;NOM(1,PL) 235 26 28 0\ndeu V;IND;PST;NOM(1,SG) 236 17 11 0\ndeu V;IND;PST;NOM(2,PL) 257 20 23 0\ndeu V;IND;PST;NOM(3,PL) 243 15 29 0\ndeu V;IND;PST;NOM(3,SG) 247 22 27 0\ndeu V;NFIN 248 21 13 0\ndeu V;SBJV;PRS;NOM(1,PL) 234 25 18 0\ndeu V;SBJV;PST;NOM(1,PL) 243 20 20 0\ndeu V;SBJV;PST;NOM(2,SG) 229 27 24 0\ndeu V;SBJV;PST;NOM(3,PL) 247 22 22 0\ndeu N;ACC(PL) 368 44 49 0\ndeu N;ACC(SG) 385 47 53 0\ndeu N;DAT(PL) 361 44 48 0\ndeu N;DAT(SG) 382 47 52 0\ndeu N;GEN(PL) 364 44 48 0\ndeu N;GEN(SG) 385 47 50 0\ndeu N;NOM(PL) 370 44 49 174\ndeu N;NOM(SG) 391 47 53 0\ndeu V .PTCP;PST 242 23 23 0\ndeu V;IND;PRS;NOM(3,PL) 232 25 19 0\ndeu V;IND;PST;NOM(2,SG) 215 25 18 0\ndeu V;SBJV;PRS;NOM(2,PL) 248 20 23 0\ndeu V;SBJV;PRS;NOM(3,PL) 247 26 26 0\ndeu V;SBJV;PRS;NOM(3,SG) 238 26 24 0\ndeu V;SBJV;PST;NOM(3,SG) 261 22 26 0\ndeu V;SBJV;PRS;NOM(1,SG) 246 22 16 0\ndeu V;SBJV;PST;NOM(1,SG) 238 21 28 0\ndeu V;SBJV;PST;NOM(2,PL) 239 17 17 0\ndeu V;SBJV;PRS;NOM(2,SG) 222 18 18 0\ntur V;POS;PAST;A1SG 2005 201 202 10\ntur V;NEG;NARR;A2PL 2005 201 202 10\ntur N;A3SG;P1SG;DAT 2170 214 214 10\ntur N;A3SG;PNON;ACC 2172 214 214 10\ntam V;PRS-1SG 67 16 0 0\ntam V;FUT-1SG 67 16 0 0\ntam V;PST-2SG 67 16 0 0\ntam V;PRS-2SG 67 16 0 0\ntam V;FUT-2SG 67 16 0 0\ntam V;PST-3SG.M 67 16 0 123\ntam V;PRS-3SG.M 67 16 0 0\ntam V;FUT-3SG.M 67 16 0 0\ntam V;PST-3SG.F 67 16 0 0\ntam V;PRS-3SG.F 67 16 0 0\ntam V;FUT-3SG.F 67 16 0 0\ntam V;PST-3SG.HON 67 16 0 0\ntam V;PRS-3SG.HON 67 16 0 0\ntam V;FUT-3SG.HON 67 16 0 0\ntam V;PST-1PL 67 16 0 0\ntam V;PRS-1PL 67 16 0 0\ntam V;FUT-1PL 67 16 0 0\ntam V;PST-2PL 67 16 0 0\ntam V;PRS-2PL 67 16 0 0\ntam V;FUT-2PL 67 16 0 0\ntam V;PST-3PL 67 16 0 0\ntam V;PRS-3PL 67 16 0 0\ntam V;FUT-3PL 67 16 0 0\nTable 7: Distribution of tags over the different splits for\nthe four languages.\nNo change\n+ e\n+ en\n+ er\n+ s\nVowel change\nVowel change + e\nVowel change + er\nReal Word\nUnknown\nPredicted\nNo change\n+ e\n+ en\n+ s\nVowel change + e\nUnknown\nGold\n253 3 0 0 15 0 0 0 1 9\n43 195 51 4 80 0 6 1 16 25\n0 14 51 1 16 3 1 0 0 10\n23 13 2 1 80 0 0 0 0 9\n0 6 0 1 0 1 23 1 0 0\n4 3 0 0 10 0 0 0 0 23\nFigure 4: Confusion matrix for competing German plu-\nral morphemes for the one-shot setting.\nNo change\n+ e\n+ en\n+ er\n+ s\nVowel change\nVowel change + e\nVowel change + er\nReal Word\nUnknown\nPredicted\nNo change\n+ e\n+ en\n+ s\nVowel change + e\nUnknown\nGold\n283 4 1 0 45 1 0 0 3 10\n45 251 80 4 99 0 10 0 5 34\n0 15 63 2 21 2 0 0 1 16\n18 11 2 0 116 0 4 0 0 9\n0 6 0 0 0 3 24 5 1 1\n4 3 0 0 14 0 0 0 1 27\nFigure 5: Confusion matrix for competing German plu-\nral morphemes for the zero-shot setting.\nNo change + ed 2x last consonant + ed Vowel change Real Word Unknown\nPredicted\n+ ed2x last consonant + edUnknown\nGold\n9 70 5 0 18 8\n8 16 59 0 10 12\n0 1 0 1 8 25\nFigure 6: Confusion matrix for competing English past\ntense morphemes for the one-shot setting.\nNo change + ed 2x last consonant + edVowel change + ed Real Word Unknown\nPredicted\n+ ed2x last consonant + edUnknown\nGold\n6 98 8 1 72 35\n13 28 82 0 68 19\n1 1 0 0 28 40\nFigure 7: Confusion matrix for competing English past\ntense morphemes for the zero-shot setting.\n6522\ndesign of intricate online surveys.6\nWe initiated the survey by presenting an intro-\nduction that detailed the concept of a wug test and\nthe associated information for the survey. This in-\ntroductory passage served to inform participants of\nthe nature and intent of the research study, and it\nalso provided examples to further facilitate their\nunderstanding of our task requirements.\nOur data collection phase consisted of two parts:\nthe English wug test and the German wug test.\nUpon consenting to participate, respondents were\nguided through a series of thoughtfully designed\nprompts related to the wug test. These prompts en-\ncouraged them to provide suitable responses based\non their understanding of the task.\nFor the English wug test, we employed the fol-\nlowing exemplary prompt: “Fill in the blank with\nthe correct past tense of the word ‘wug’. There is\nno predetermined correct answer. We encourage\nyou to rely on your linguistic intuition. If you be-\nlieve there are multiple possible responses, simply\nnote the form that seems most accurate to you. For\ninstance, ‘They wug all the time. In fact, they __\njust yesterday!’”. Such prompts stimulated the par-\nticipants to produce responses that were entirely\ntheir own, drawing on the provided information.\nFor the German wug test, we translated the task in-\nstructions and prompts into German, ensuring easy\ncomprehension for native German speakers.\nIn total, the English wug test incorporated 50\nunique words for participants to respond to, while\nthe German version consisted of 174 unique words.\nWe received 28 responses for the English wug test\nand 21 responses for the German wug test.\nA.4 Other Values of k\nTable 8 presents results fork = 1 andk = 3. Re-\nsults fork = 5 are given in Section5.\nA.5 Prompts\nWe leveraged the following prompts for the indi-\nvidual languages:\n• English:\n– Long: “Fill in the blank with the correct\npast tense of the verb X. Answer with\none word. They X all the time. In fact,\nthey _ just yesterday! _ :”\n– Short: “Form the correct past tense of\nthe verb X. Answer with one word. X :”\n6https://www.qualtrics.com/\n• German:\n– Long: “Fülle die Lücke mit dem korrek-\nten Plural des Nomens X aus. Antworte\nmit einem Wort. Hier ist ein X. Jetzt sind\nes zwei _! _:”\n– Short: “Bilde den korrekten Plural des\nNomens X. Antworte mit einem Wort. X\n:”\n• Tamil:\n– Long: “ ேநற்று அவரிடம், \"நீ X\"\nஎன்ேறன். அைதக் ேகட்டு அவன் ேபாய்\n_. _:”\n– Short: “X :”\n• Turkish:\n– Long: “Boşlukları X ile verilen eylemin\nbirinci tekil şahıs geçmiş zaman formları\nile doldurun. Ben her zaman X. Ama\ndün _. _:”\n– Short: “Tek bir sözcük ile farazi X\neyleminin birinci tekil şahıs geçmiş za-\nman hali nasıl olur? X :”\n6523\nEnglish German Tamil\nMethod k = 1 k = 3 k = 1 k = 3 k = 1 k = 3\nARL 66.00 98.00 71.84 91.95 49.18 61.48\nMinGen 56.00 60.00 39.66 60.92 39.34 49.18\nFIT 84.00 ± 2.97 96.20 ± 0.60 70.06 ± 1.67 90.69 ± 0.99 44.75 ± 2.01 59.84 ± 3.07\nPPI 1 72.60 ± 6.00 90.80 ± 4.49 60.17 ± 6.80 82.59 ± 6.56 37.30 ± 2.57 51.07 ± 1.56\nAED 44.20 ± 7.18 56.20 ± 6.54 27.82 ± 3.94 42.87 ± 4.65 46.15 ± 4.18 57.87 ± 5.46\nlong 0-shot 42.60 ± 4.90 55.60 ± 5.99 62.18 ± 2.45 81.55 ± 1.77 0.00 0.00\nlong 1-shot 58.40 ± 7.20 72.40 ± 6.50 63.36 ± 4.01 81.61 ± 3.09 5.27 ± 2.83 12.65 ± 6.19\nlong few-shot 57.60 ± 6.97 74.60 ± 4.90 65.86 ± 3.03 82.59 ± 1.96 15.25 ± 2.98 38.03 ± 4.18\nshort 0-shot 55.40 ± 6.07 72.20 ± 6.35 66.38 ± 2.48 84.43 ± 2.63 0.00 0.00\nshort 1-shot 61.20 ± 8.16 81.60 ± 6.97 67.31 ± 3.92 84.41 ± 2.36 1.99 ± 2.47 3.04 ± 3.58\nshort few-shot 61.40 ± 3.69 77.20 ± 3.12 68.97 ± 2.02 84.43 ± 1.07 17.05 ± 2.64 38.77 ± 2.86\nTable 8: Results for other values ofk.\n6524",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6771159172058105
    },
    {
      "name": "Natural language processing",
      "score": 0.499265193939209
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4125939607620239
    },
    {
      "name": "Programming language",
      "score": 0.3542855978012085
    },
    {
      "name": "Linguistics",
      "score": 0.3279833197593689
    },
    {
      "name": "Philosophy",
      "score": 0.14622336626052856
    }
  ]
}