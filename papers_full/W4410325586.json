{
  "title": "Token-Mol 1.0: tokenized drug design with large language models",
  "url": "https://openalex.org/W4410325586",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2634326520",
      "name": "Jike Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2007352509",
      "name": "Rui Qin",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2108415076",
      "name": "Mingyang Wang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5104346083",
      "name": "Meijing Fang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2115413800",
      "name": "Yangyang Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2284654103",
      "name": "Yuchen Zhu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2113844603",
      "name": "Qun Su",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A4221416988",
      "name": "Qiaolin Gou",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2085305118",
      "name": "Chao Shen",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A4378632070",
      "name": "Odin Zhang",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2117634397",
      "name": "Zhenxing Wu",
      "affiliations": [
        "Zhejiang University",
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2111279273",
      "name": "Dejun Jiang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2101469969",
      "name": "Xujun Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2128802306",
      "name": "Zhao Huifeng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2149976934",
      "name": "Jingxuan Ge",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2616270023",
      "name": "Zhourui Wu",
      "affiliations": [
        "Tongji University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2115397990",
      "name": "Yu Kang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2682934371",
      "name": "Chang-Yu Hsieh",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2140542583",
      "name": "Tingjun Hou",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2634326520",
      "name": "Jike Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007352509",
      "name": "Rui Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108415076",
      "name": "Mingyang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5104346083",
      "name": "Meijing Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115413800",
      "name": "Yangyang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2284654103",
      "name": "Yuchen Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113844603",
      "name": "Qun Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221416988",
      "name": "Qiaolin Gou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2085305118",
      "name": "Chao Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378632070",
      "name": "Odin Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117634397",
      "name": "Zhenxing Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111279273",
      "name": "Dejun Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101469969",
      "name": "Xujun Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128802306",
      "name": "Zhao Huifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149976934",
      "name": "Jingxuan Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2616270023",
      "name": "Zhourui Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115397990",
      "name": "Yu Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2682934371",
      "name": "Chang-Yu Hsieh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140542583",
      "name": "Tingjun Hou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3095883070",
    "https://openalex.org/W3165630607",
    "https://openalex.org/W3206711231",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W4313485929",
    "https://openalex.org/W4309218736",
    "https://openalex.org/W4388845139",
    "https://openalex.org/W4311821345",
    "https://openalex.org/W4388020856",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4385767952",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4318718899",
    "https://openalex.org/W6772452955",
    "https://openalex.org/W4365455552",
    "https://openalex.org/W4213077304",
    "https://openalex.org/W4323304388",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W4409362560",
    "https://openalex.org/W2931367569",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3162011170",
    "https://openalex.org/W4221149941",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4379769091",
    "https://openalex.org/W3172000897",
    "https://openalex.org/W4315703368",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4224220091",
    "https://openalex.org/W4390221451",
    "https://openalex.org/W4280535976",
    "https://openalex.org/W4323650483",
    "https://openalex.org/W4401516728",
    "https://openalex.org/W4403891981",
    "https://openalex.org/W4313214263",
    "https://openalex.org/W3094553402",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2105649494",
    "https://openalex.org/W2009979602",
    "https://openalex.org/W2244785476",
    "https://openalex.org/W4283076183",
    "https://openalex.org/W3214379431",
    "https://openalex.org/W4313279383",
    "https://openalex.org/W4387451313",
    "https://openalex.org/W4386498912",
    "https://openalex.org/W2971690404",
    "https://openalex.org/W4381470101",
    "https://openalex.org/W2403144305",
    "https://openalex.org/W3131767541",
    "https://openalex.org/W3094104502",
    "https://openalex.org/W2031060692",
    "https://openalex.org/W2013178046",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W3043058341",
    "https://openalex.org/W4387129070",
    "https://openalex.org/W1975740938",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W1985588649",
    "https://openalex.org/W2018989832",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2773987374",
    "https://openalex.org/W3207373390",
    "https://openalex.org/W4376618618",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2151697120",
    "https://openalex.org/W2108389384",
    "https://openalex.org/W2891800472",
    "https://openalex.org/W2033495141",
    "https://openalex.org/W2052907531",
    "https://openalex.org/W4248107770",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W2181523240",
    "https://openalex.org/W2155653793",
    "https://openalex.org/W3034806393",
    "https://openalex.org/W3082411326",
    "https://openalex.org/W4309908854",
    "https://openalex.org/W4388455891",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W3181995918",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W3100751385",
    "https://openalex.org/W3105259638"
  ],
  "abstract": "The integration of large language models (LLMs) into drug design is gaining momentum; however, existing approaches often struggle to effectively incorporate three-dimensional molecular structures. Here, we present Token-Mol, a token-only 3D drug design model that encodes both 2D and 3D structural information, along with molecular properties, into discrete tokens. Built on a transformer decoder and trained with causal masking, Token-Mol introduces a Gaussian cross-entropy loss function tailored for regression tasks, enabling superior performance across multiple downstream applications. The model surpasses existing methods, improving molecular conformation generation by over 10% and 20% across two datasets, while outperforming token-only models by 30% in property prediction. In pocket-based molecular generation, it enhances drug-likeness and synthetic accessibility by approximately 11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than expert diffusion models. In real-world validation, it improves success rates and, when combined with reinforcement learning, further optimizes affinity and drug-likeness, advancing AI-driven drug discovery.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-59628-y\nToken-Mol 1.0: tokenized drug design with\nlarge language models\nJike Wang1,4,R u iQ i n1,4, Mingyang Wang1,4, Meijing Fang1, Yangyang Zhang1,\nYuchen Zhu1,Q u nS u1, Qiaolin Gou1, Chao Shen1, Odin Zhang2, Zhenxing Wu1,\nDejun Jiang 1, Xujun Zhang1, Huifeng Zhao1,J i n g x u a nG e1, Zhourui Wu3,\nYu Kang 1 , Chang-Yu Hsieh 1 &T i n g j u nH o u1\nT h ei n t e g r a t i o no fl a r g el a n g u a g em o d e l s( L L M s )i n t od r u gd e s i g ni sg a i n i n g\nmomentum; however, existing approaches often struggle to effectively\nincorporate three-dimensional molecular structures. Here, we present Token-\nMol, a token-only 3D drug design model that encodes both 2D and 3D struc-\ntural information, along with molecular properties, into discrete tokens. Built\non a transformer decoder and trained with causal masking, Token-Mol intro-\nduces a Gaussian cross-entropy loss function tailored forregression tasks,\nenabling superior performance across multiple downstream applications. The\nmodel surpasses existing methods, improving molecular conformation gen-\neration by over 10% and 20% across two datasets, while outperforming token-\nonly models by 30% in property prediction. In pocket-based molecular gen-\neration, it enhances drug-likeness andsynthetic accessibility by approximately\n11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than\nexpert diffusion models. In real-world validation, it improves success rates\nand, when combined with reinforcement learning, further optimizes afﬁnity\nand drug-likeness, advancing AI-driven drug discovery.\nDrug discovery traverses a remarkably intricate journey. Recent years\nhave witnessed profound advancements in artiﬁcial intelligence (AI)\ntechnologies, particularly deep learning (DL), which has been pro-\ngressively impacting multiple facets of drug development. These\ntechnologies are accelerating in innovative drug research. However,\nthe high cost associated with acquiring annotated data sets in drug\ndiscovery remains a signiﬁcant impediment to the advancement in this\nﬁeld. Recently, the rapid evolution of unsupervised learning frame-\nworks, epitomized by BERT\n1 and GPT2, has introduced unsupervised\nchemical and biological pre-training models across disciplines such as\nchemistry\n3– 12, and biology13– 16. These models undergo large-scale\nunsupervised training to learn representations of small molecules or\nproteins, subsequentlyﬁne-tuned for speciﬁc applications. By lever-\naging unsupervised learning on large-scale datasets, these pre-training\nmodels effectively addresses the challenges associated with sparse\nlabeling and suboptimal out-of-distribution generalization, leading to\nimproved performance\n17.\nLarge-scale molecular pre-training models can be broadly cate-\ngorized into two main groups: models based on chemical language and\nmodels utilizing molecular graphs. First, chemical language models\nencode molecular structures using representations such as simpliﬁed\nmolecular input line entry system (SMILES)\n18 or self-referencing\nembedded strings (SELFIES)19. They employ training methodologies\nakin to BERT or GPT, well-established in natural language processing\n(NLP). Notable examples include SMILES-BERT\n20, MolGPT 21,\nChemformer22, and Multitask Text and Chemistry T523,w h i c he x h i b i t\narchitectural similarities to universal or general NLP models such as\nLLaMA\n24.\nReceived: 22 August 2024\nAccepted: 25 April 2025\nCheck for updates\n1College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058 Zhejiang, China.2Paul G. Allen School of Computer Science & Engineering,\nUniversity of Washington, Seattle, WA, USA.3Key Laboratory of Spine and Spinal cord Injury Repair and Regeneration, Ministry of Education, Tongji University,\nShanghai, China.4These authors contributed equally: Jike Wang, Rui Qin, Mingyang Wang.e-mail: yukang@zju.edu.cn; kimhsieh@zju.edu.cn;\ntingjunhou@zju.edu.cn\nNature Communications|         (2025) 16:4416 1\n1234567890():,;\n1234567890():,;\nSecond, graph-based molecular pre-trained models exhibit higher\nversatility. They represent molecules in a graphical format, with nodes\nfor atoms and edges for chemical bonds. Pre-training methodologies\ninclude various techniques, such as random masking of atom types,\ncontrastive learning, and context prediction\n25– 27. Unlike language-\nbased models, graph-based molecular pre-trained models inherently\nincorporate geometric information, as demonstrated by methods like\nGEM\n28 and Uni-Mol29.\nDespite their advancements, both classes of models exhibit dis-\ntinct limitations. Large-scale molecular pre-training models based on\nthe chemical language face a signiﬁcant constraint in their inability to\ninherently process 3D structural information, which is pivotal for\ndetermining the physical, chemical, and biological properties of\nmolecules\n28,29. Consequently, these modelsa r ei n a d e q u a t ef o rd o w n -\nstream tasks that involve 3D structures, such as molecular conforma-\ntion generation and 3D structure-based drug design. In contrast,\ngraph-based molecular pre-trained models can effectively incorporate\n3D information. However, existing approaches primarily focus on\nlearning molecular representations for property prediction rather than\nmolecular generation. Moreover, integrating these models with uni-\nversal NLP models presents considerable challenges. As a result, a\ncomprehensive model capable of addressing all drug design tasks\nremains elusive. Addressing the limitations of these two model types\nto develop a pre-trained model suitable for all drug design scenarios,\nand easily integrable with existing general large language models, is a\npressing need.\nThe emergence of universal artiﬁcial intelligence offers opportu-\nnities in this domain. By leveraging vast amounts of data, these models\nacquire expert knowledge across variousﬁelds, providing valuable\nassistance to practitioners\n2,24,30,31. Recent studies suggest that GPT-4\ndemonstrates a profound understanding of key concepts in drug dis-\ncovery, including therapeutic proteins and the fundamental principles\ngoverning the design of small molecule-based and other types of\ndrugs. However, its efﬁcacy in speciﬁcd r u gd e s i g nt a s k s ,s u c ha sd e\nnovo molecule generation, molecular structure alteration, drug-target\ninteraction prediction, molecular property estimation, and retro-\nsynthetic pathway prediction, requires further reﬁnement\n32. Never-\ntheless, the application of a token-based approach by the above\nmodels to handle continuous spatial data is particularly noteworthy.\nBuilding on this concept, Born et al. introduced the Regression\nTransformer33, which integrates regression tasks by encoding numer-\nical values as tokens. Nonetheless, this method overlooks the intricate\n3D structural complexities of molecules. Additionally, Flam-Shepherd\nand Aspuru-Guzik proposed directly tokenizing 3D atomic coordinates\n(XYZ) to represent molecular 3D structures\n34. Concurrently, the\nBindGPT framework employs a similar approach to generate molecular\nstructures and their corresponding 3D coordinates\n35.W h i l et h ep e r -\nformance of these models still necessitates enhancement, both\napproaches have exhibited promising outcomes in relevant drug\ndesign tasks. These results highlight the potential of large models to\ngrasp the semantics of numerical values and afﬁrm the feasibility of\nemploying token-only models to handle continuous data. However,\ndirectly training language models on Cartesian coordinates of atoms\npresents unique challenges. Speciﬁcally, for larger molecules, the\nextensive XYZ coordinates can result in excessively long sequences,\ncomplicating the model’s learning process. Furthermore, achieving\ninvariance through random translation and rotation does not confer\nequivariance.\nIn this work, to overcome the limitations of current models, we\npresent Token-Mol, a large-scale language model for molecular pre-\ntraining. To enhance compatibility with existing general models, we\nemploy a token-only training paradigm, recasting all regression tasks\nas probabilistic prediction tasks. Token-Mol is constructed with a\nTransformer decoder architecture, integrating essential 2D and 3D\nstructural information via SMILES and torsion angle tokens.\nFurthermore, we utilize a random causal masking strategy during pre-\ntraining, leveraging a combination of Poisson and uniform distribu-\ntions to stochastically mask training data. This strategy enhances the\nmodel’s ﬁll-in-the-blank generation capability, increasing its adapt-\nability to a wide range of downstream tasks. To address the token-only\nmodel’s limited sensitivity to numerical values, we introduce a Gaus-\nsian cross-entropy (GCE) loss function, replacing the traditional cross-\nentropy loss. This innovative loss function assigns weights to each\ntoken during training, enabling the model to learn the relationships\nbetween numerical tokens. Additionally, Token-Mol demonstrates\nexceptional compatibility with other advanced modeling techniques,\nincludingﬁne-tuning and reinforcement learning (RL). This integrative\ncapability facilitates the further optimization of its performance in\ndownstream tasks, thereby enhancing its utility in various applications.\nTo validate the capabilities of Token-Mol, we conduct comprehensive\nassessments across molecular conformation generation, property\nprediction, and pocket-based molecular generation tasks. In the\nmolecular conformation generation task, Token-Mol outperforms\nexisting state-of-the-art methods, achieving over 10% and 20% across\nvarious metrics on two datasets, respectively. In the molecular prop-\nerty prediction task, Token-Mol achieves an average improvement of\n30% in regression tasks compared to token-only models. In the pocket-\nbased molecular generation task, Token-Mol not only achieves mole-\ncules with Vina scores comparable to those produced by state-of-the-\nart models, but also improves drug-likeness (QED) and synthetic\naccessibility (SA) by approximately 11% and 14%, respectively. To fur-\nther validate the generalization capability of Token-Mol, we conduct\ntests in real-world drug design scenarios. Notably, the drug-like\nmolecules generated by Token-Mol demonstrate a 1-fold increase in\naverage success rate across the evaluations on 8 different targets.\nFurthermore, the integration of RLinto Token-Mol effectively enhan-\nces the performance of speciﬁc downstream tasks within more realistic\nscenarios, providing an advantage over large models based on geo-\nmetric graph neural networks for 3D tasks. Finally, we demonstrate\nToken-Mol’s seamless integration with general large language models\nthrough a simple dialogue example. The aforementionedﬁndings\nhighlight the inherent potential of Token-Mol, presenting an outlook\non standardizing AI models for drug design.\nResults\nThe overview of Token-Mol\nThe comprehensive workﬂow of Token-Mol is illustrated in Fig.1.T h e\ninitial phase involves pre-training on the dataset (Fig.1a) through\nrandom causal masking. Subsequently, the model undergoesﬁne-\ntuning on customized datasets tailored to speciﬁc downstream tasks,\nincluding conformation generation, pocket-based molecular genera-\ntion, and prediction on multiple properties (Fig.1b). For regression\ntasks, the GCE loss function (Fig.1c) is utilized during theﬁne-tuning\nprocess. Furthermore, the performance for speciﬁcd o w n s t r e a mt a s k s\ncan be further optimized using reinforcement learning.\nThe preprocessing of the pretraining dataset holds crucial sig-\nniﬁcance in this context. As shown in Fig.1a, a depth-ﬁrst search (DFS)\ntraversal is conducted on the entire molecule in the standard SMILES\nformat to extract the embedded torsion angles within the\nmolecular structure. Following this, each extracted torsion angle is\nassimilated as a token appended to the SMILES string. Throughout the\npretraining phase, random causal masking based on causal regression\nis implemented. After pretraining,ﬁne-tuning is carried out across\ndownstream tasks. Importantly, the task prompts are speciﬁcally\ndesigned for the construction of a dialogue system, as indicated by the\nhighlighted yellow box in Fig.1a. This feature highlights a key advan-\ntage of token-only models over other large-scale models: their cap-\nability to facilitate real-time interaction. At the end of the Results\nsection, examples will be presented to illustrate this particular\nadvantage.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 2\nFor the pocket-based molecular generation task, we have intro-\nduced pocket encoder and fusion block modules to better incorporate\nprotein pocket information into the model. As depicted in Fig.1d, we\nutilized a multi-head condition-attention mechanism to thoroughly\nincorporate information generated at each autoregressive step into\nsubsequent iterations. This mechanism treats each token generated\nduring autoregression as a prerequisite for further generation, thereby\nensuring that the entire query, key, and value matrices originate from\nthe original sequence.\nOne should note that, in practical scenarios, a lead compound\nmust not only exhibit high afﬁnity for the target but also meet a series\nof criteria, including high bioactivity and multiple favorable pharma-\ncological properties. This puts higher requirements for pocket-based\nmolecular design tasks, where the integration of receptor-ligand\nmolecule pairs in the training dataset imposes inherent limitations.\nThe model predominantly generates ligand molecules by utilizing\ninformation derived from the protein pocket. Consequently, the\nproperties of these generated molecules are heavily inﬂuenced by the\ntraining data, restricting the explicitly control over their biophysical\nand chemical properties. These constraints are particularly evident\nwhen a precise modulation of molecular properties is desired. Token-\nMol, built on an autoregressive language model architecture, where\ntoken generation aligns with actions in the RL framework, facilitates\nthe seamless utilization of RL for optimization, thereby ensuring tai-\nlored outcomes.\nMolecular conformation generation\nMolecular conformation is a crucial determinant of the chemical,\nphysical, and biological properties of molecules, underscoring its\nfundamental importance in structure-based drug design. The\nintegrity and diversity of three-dimensional molecular conforma-\ntions are essential for various applications in drug discovery,\nincluding three-dimensional quantitative structure-activity rela-\ntionships, molecular docking and thermodynamic calculations.\nTraditional techniques for obtaining accurate molecular\nconformations, such as X-ray crystallography and nuclear magnetic\nresonance (NMR), are either prohibitively expensive or computa-\ntionally demanding, rendering them impractical for large-scale\ndataset analysis. The emergence of deep geometric learning has\nintroduced promising alternative methodologies for the generation\nof molecular conformations\n34,36– 44.\nIn this study, we benchmarked our proposed approach against\nestablished baselines using widely recognized conformation genera-\ntion benchmarks. We employed the dataset utilized by Zhang et al.,\nwhich includes the dataset from Shi et al. (test set I), comprising 200\nmolecules, each with fewer than 100 conformations. It is noteworthy\nthat this particular dataset is among the most extensively employed\nwithin the conformer generation task. On the other hand, the GEOM-\nDrug dataset presents a broader range of conformation counts per\nmolecule, from 0 to 12,000. To address this variance, Zhang et al.\nintroduced test set II\n26, consisting of 1,000 randomly selected mole-\ncules with conformation counts distributed similarly to the entire\ndataset, ranging from 0 to 500.\nOur evaluation metrics include both Recall and Precision. Recall\nmeasures the diversity of the generated conformations, while Preci-\nsion evaluates the rationality of the generated conformations. We\ncalculated the mean scores of coverage (COV) and matching (MAT) for\nboth Recall and Precision. COV quantiﬁes the extent to which the\nquantum computation conformation set covers the generated con-\nformation set within a speciﬁed RMSD threshold, with higher values\nindicating better coverage. Conversely, MAT assesses the similarity\nbetween the generated conformations and the quantum mechanical-\nlevel training conformations, with lower values suggesting better\nperformance.\nTable 1 presents the results for test set I. It indicates that Token-\nMol surpasses other SOTA methods in both Precision metrics, result-\ning in substantial advantages. Notably, Token-Mol achieves notable\nimprovement in the COV Precision (COV-P) metric, outperforming\nTora3D by approximately 11%, underscoring the superior quality of\nmolecules produced by Token-Mol relative to alternative methods.\nFig. 1 | The overview of Token-Mol. aData processing workﬂow. b The workﬂow of Token-Mol.c The weight allocation in the GCE loss function, where GT stands for\nground truth token.d Pocket encoder and fusion block of pocket-based molecular generation.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 3\nHowever, Token-Mol’s generated conformations exhibit slightly lower\nRecall performance compared to GeoDiff and Tora3D, positioning it as\nthe second-highest performer overall.\nThe ﬁndings for test set II, depicted in Table2, reveal Token-Mol’s\nexemplary performance across all assessment metrics. Remarkably,\nToken-Mol attains the highest performance in both Precision-based\nevaluation metrics, COV-P and MAT-P, surpassing other models by\napproximately 24% and 21%, respectively.\nSubsequently, we investigated the relationship between the\nbenchmark performance and the number of rotatable bonds, as\nillustrated in Fig.2. Our analysis reveals a clear trend: the perfor-\nmance across all assessment metrics declines as the number of\nrotatable bonds increases. This decline becomes particularly pro-\nnounced when the number of rotatable bonds exceeds 10. Notably,\nTora3D exhibits an obvious drop in performance when generating\nconformations for molecules with a higher number of rotatable\nbonds. In contrast, Token-Mol demonstrates substantial advantages\nunder these conditions.\nMoreover, Token-Mol demonstrates impressive speed. During\nour evaluation on test set I, utilizing the Tesla V100 for the generation\nprocess, Token-Mol required an average of 6.37 seconds to generate all\nconformations for a single molecule, compared to 8.78 seconds per\nmolecule for Tora3D.\nMolecular property prediction\nMolecular representation is fundamental to molecular design, as it cri-\ntically inﬂuences the execution of downstream tasks. In this study, we\ninitially assessed the molecular representation capabilities of Token-Mol\nin the context of molecular property prediction. For a detailed\ndescription of the tasks, please refer to the Supplementary Information.\nClassiﬁcation task. For the classiﬁcation task, we selected six com-\nmonly used classiﬁcation datasets and compared Token-Mol against\nﬁve representative baselines: XGBoost\n45 (conventional machine learn-\ning), K-Bert46 (sequence-based model), Chemprop47 (graph neural\nnetworks), GEM28 (geometry-enhanced graph neural networks), and\nMapLight+GNN48 (an integrated model combining traditional machine\nlearning with graph neural networks). As outlined in Table3,T o k e n -\nMol demonstrates noteworthy performance across all datasets, out-\nperforming XGBoost and Chemprop in terms of accuracy, albeit\nmarginally trailing behind MapLight+GNN and GEM. Notably, Token-\nMol achieves state-of-the-art proﬁciency on single-task-focused data-\nsets such as BBBP and BACE.\nRegression task. We employed a set of six regression datasets for a\nthorough comparison and analysis. To extend beyond established\nbenchmarks, we introduced the token-only Regression Transformer\n(RT)\n33, a model conceptually akin to Token-Mol, to enrich our evalua-\ntion framework. Both RT and Token-Mol fully tokenize the input and\noutput, enabling seamless integration with foundational large models,\na feature not shared by other models.\nA key advantage of token-only models over traditional regres-\nsion models is their ability to interface seamlessly with large models\nsuch as LLaMA, enabling real-time interaction. However, previous\nmodels like RT have shown suboptimal performance in prediction\ntasks, limiting their utility for high-quality interactions. In contrast,\nToken-Mol treats each numerical value as a single token, rather than\ndecomposing them into multiple tokens like RT. This approach\nenables one token prediction, thereby accelerating the prediction\nprocess. Combined with the GCE, Token-Mol achieves high-quality\nprediction results. This methodology allows Token-Mol to perform\nfaster and deliver higher prediction quality.\nAs illustrated in Table4, Token-Mol’s capabilities in regression\ntasks are evident, outperforming established benchmarks such as\nXGBoost, K-Bert, and token-only RT. Notably, Token-Mol con-\nsistently surpasses RT across all tasks, showcasing an average per-\nformance enhancement of approximately 30%. Particularly\nremarkable is Token-Mol’s substantial performance boost on the\nAqsol dataset, achieving an improvement of around 50%. Addition-\nally, as depicted in Table5, Token-Mol’s performance closely mirrors\nthat of graph neural network-based models on datasets with large\namounts of data, such as Aqsol, LD50, and Lipophilicity. These\nresults collectively underscore the signiﬁcant potential of Token-Mol\nin property prediction tasks.\nThe efﬁciency of GCE. Token-only generative models conventionally\nemploy cross-entropy loss for regression tasks, but they often exhibit\ninsensitivity to numerical values and fail to capture the relationships\nbetween them. To address this issue, we proposed the GCE loss\nfunction for regression-related downstream tasks in molecular prop-\nerty prediction. To assess the efﬁcacy of GCE, we conducted ablation\nexperiments to compare models with and without GCE (Table4). Our\nresults indicate that the absence of GCE notably impairs Token-Mol’s\nperformance across all datasets, with an average RMSE increase of\napproximately 12%, underscoring the critical role of GCE in regression\ntasks. Compared to RT, which decomposes individual numerical values\ninto multiple token representations, Token-Mol’s one token prediction\napproach, enhanced with GCE, demonstrates substantial improve-\nments in both prediction accuracy and efﬁciency.\nDespite the improvements demonstrated by Token-Mol com-\npared to RT, it still exhibits certain limitations relative to other large\nmodels based on GNN. This discrepancy is primarily due to the\nmodel’s insufﬁcient sensitivity to numerical values. Although we\nproposed the GCE loss function to address this issue, Token-Mol still\nlags behind graph neural network-based regression models. Future\nwork will focus on enhancing the model’s performance in regression\nTable 1 | Performance comparison of models on test set I\nModel COV-R (%) ↑ MAT-R (Å ) ↓ COV-P (%)↑ MAT-P (Å ) ↓\nCGVAE109 0.00 3.0702 - -\nGraphDG37 8.27 1.9722 2.08 2.4340\nCGCF38 53.96 1.248 21.68 1.8571\nConfVAE39 55.20 1.2380 22.96 1.8287\nGeoMol43 67.16 1.0875 - -\nConfGF110 62.15 1.1629 23.42 1.7219\nGeoDiff40 82.96★ 0.9525 48.27 1.3205\nTora3D42 80.37 0.9272★ 62.22☆ 1.1524☆\nToken-Mol 80.65 ☆ 0.9488☆ 69.20★ 1.0865★\n★ represents the best,☆ represents the second best.\nTable 2 | Performance comparison of models on test set II\nnRotb Model COV-R\n(%) ↑\nMAT-R\n(Å ) ↓\nCOV-P\n(%) ↑\nMAT-P\n(Å ) ↓\nAll nRotb CGVAE 40.06 1.3771 - -\nGeoMol 72.50 1.1000 61.15 1.2009\nTora3D 81.92 0.9297 62.16 1.1600\nToken-Mol 82.34 0.8936 76.87 0.9107\nnRotb≤ 10 CGVAE 42.43 1.3296 - -\nGeoMol 76.36 0.9380 57.29 1.1611\nTora3D 83.03 0.8704 63.81 1.0906\nToken-Mol 83.25 0.8404 78.96 0.8108\nnRotb > 10 Tora3D 57.23 1.2455 29.02 1.5583\nToken-Mol 65.09 1.1257 47.52 1.3670\nBold formatting represents the best.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 4\ntasks through approaches such as multi-task prediction and data\naugmentation.\nPocket-based molecular generation\nIn modern drug discovery, structure-based drug design holds para-\nmount importance, driving researchers to rapidly identify high-afﬁnity\nligands within given protein binding pockets. Hence, pocket-based\nmolecular generation, a method for generating potential ligands for\nspeciﬁc pockets, not only avoids computationally intensive physical\nmethods like traditional molecular docking but also broadens the\nexploration of chemical space. Consequently, it serves as a crucial\ndownstream task to demonstrate the effectiveness of our proposed\nmodel. Our goal is to generate ligand molecules tailored to speciﬁc\nprotein pockets. To achieve this, as illustrated in Fig.1d, we amalga-\nmated a pocket encoder and a fusion block. We utilized a pretrained\nencoder to characterize protein pockets, ensuring its parameters\nfrozen during ﬁne-tuning. Furthermore, we employed condition-\nattention to integrate both protein and molecule information, mir-\nroring a prompt-like mechanism that incorporates protein pocket\ninformation into the ligand molecule generation process. The addi-\ntional methodological details are outlined in the Methods section.\nWe compared our model with three popular baseline models in\n3D, which is the mainstream method in the pocket-based molecular\ngeneration task, namely GraphBP\n49,P o c k e t 2 M o l50,a n dT a r g e t D i f f51.\nThe ﬁrst two models employ an autoregressive generative graph\nneural network (GNN) architecture, with Pocket2Mol introducing a\ngeometric deep learning framework that enhances the perception of\nthree-dimensional pocket features. In contrast, TargetDiff adopts a\nnon-autoregressive, probabilistic diffusion model based on an SE(3)-\nequivariance network. However, since Token-Mol differs from these\nmodels with 3D in-situ paradigm by generating only the three-\ndimensional structure of molecules without simultaneously\nFig. 2 | Performance for different number of rotatable bonds on test set II.The x-axis represents the number of rotatable bonds, and the y-axis indicates the prediction\nperformance.a COV-R, (b)M A T - R ,(c) COV-P and (d) MAT-P. Source data are provided as a Source Dataﬁle.\nTable 3 | Performance on different dataset for classiﬁcation tasks\nClassiﬁcation (ROC-AUC %↑)\nDataSets\n#Moleculars\n#Tasks\nBBBP\n2039\n1\nBACE\n1513\n1\nClinTox\n1478\n2\nTox21\n7831\n12\nToxCast\n8575\n617\nSIDER\n1427\n27\nAverage\n-\n-\nXGBoost45 0.888±0.028 0.872±0.016 0.863±0.034 0.801±0.061 0.668±0.164 0.652±0.086 0.791\nChemprop47 0.927±0.021 0.865±0.037 0.877±0.037 0.845±0.015 0.736±0.005 0.639±0.028 0.815\nMapLight+GNN48 0.912±0.026 0.883±0.007 0.895±0.041 0.865±0.067 0.771±0.156 0.695±0.051 0.836\nGEM28 0.940±0.022 0.898±0.019 0.940±0.026 0.862±0.014 0.766±0.009 0.670±0.012 0.846\nK-Bert46 0.945±0.008 0.879±0.028 0.913±0.046 0.665±0.004 0.510±0.003 0.608±0.012 0.757\nToken-Mol 0.934 ±0.001 0.896±0.015 0.927±0.021 0.829±0.005 0.746±0.012 0.644±0.020 0.829\nBold formatting represents the best.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 5\nproducing the Cartesian coordinates that represent the spatial rela-\ntionship of the molecules to the pocket, additional docking is required.\nFurthermore, these 3D in-situ models have faced some criticism in\nrecent studies\n52,53. To address this, we additionally employed a 2D-\nbased pocket-based molecular generation model, namely TamGen54,\nwhich utilizes a pretrain-ﬁnetuning paradigm similar to Token-Mol,\ninvolving pre-training on a large dataset of sequence representations\nof small molecules, followed byﬁne-tuning based on pocket-ligand\npairs, as well as applying the cross-attention mechanism to incorporate\npocket information.\nPerformance on benchmark. We initially evaluated the generalization\ncapability on pocket-based generation (without RL) by the following\nthree criteria: fundamental attributes of the generated molecular sets,\nbinding afﬁnity towards a given pocket, and the physiochemical\nproperties that indicate drug-likeness.\nAs shown in Table 5, the molecules generated by Token-Mol\nexhibit satisfactory performance across the entire molecular set. In\nterms of validity, graph-based models tend to generate some mole-\ncules with structural ﬂaws, leading to a decreased validity\n51.O u r\nexperiments partly conﬁrm this issue in several graph-based models,\nexcluding Pocket2mol. Although Token-Mol is a language model,\ninaccuracies in predicting token counts or values pertaining to torsion\nangles during autoregressive generation can yield invalid structures.\nRegarding the diversity, Token-Mol achieves a comparable result than\nbaseline models in the internal diversity metric, while obtaining a\nrelatively moderate results in the #Circle metric which is more sensi-\ntive to chemical space coverage.\nHowever, for pocket-based molecular generation, the essence lies\nin learning how to generate molecules within a speciﬁc constrained\nchemical space\n55. An ideal molecular generation model should balance\nand weigh both novelty and similarity to known molecules. Conse-\nquently, we also compared the similarity of the generated molecules to\nthe training set and the original ligands in the test targets. The results\nobviously exhibit that graph-based models exhibit relatively poor\nsimilarity, while sequence-based models demonstrate nearly similar\nlevels of similarity. From the perspective of overall similarity and\ndiversity, Token-Mol achieves desirable similarity over 0.1 to training\nset and the original ligands in the unseen pockets while maintaining\nadequate diversity, reaching a better performance among a variety of\nmodels.\nBinding afﬁnity is a crucial metric in measuring the capabilities of\npocket-based generation models. Consistent with established prac-\ntices, we employ the Vina score as the proxy measure of binding afﬁ-\nnity. Weﬁrst probed into the ability to enrich high-afﬁnity molecules,\nwith the Vina score of the original ligands in the test set pockets as\nreferences. On average, approximately 47.2% of the molecules gener-\nated by Token-Mol demonstrated higher afﬁnity, surpassing those\nproduced by baseline models. We further explored the overall afﬁnity\ndistribution of generated molecules (Fig. 3a and Supplementary\nTable 1). From both the median and mean perspectives, the perfor-\nmance of the Token-Mol did not obviously inferior from each optimal\nvalue. When viewed from the perspective of the entire distribution,\nthere was also no statistically signiﬁcant difference in the Vina score\ndistribution of the molecules generated by Token-Mol compared to all\nthe baseline models. Considering the above, it can be concluded that\nthe Token-Mol is capable of generating molecules with binding afﬁ-\nnities comparable to those produced by specialized models through\nﬁne-tuning or training from scratch. Even though there is no statistical\ndifference, it can be observed that the Vina score distribution of two\ngraph-based models, Pocket2Mol and TargetDiff, extend into the\nregion of value less than−10, which is considered to indicate good\nafﬁnity. However, many of these results are possibly outliers caused by\nhallucinations\n56, displaying low Vina scores but containing obvious\nstructural anomalies that render them unsuitable as drug candidates in\nreality. These concerns are further explored in the section Pocket-\nbased generation on real-world targets.\nPhysiochemical properties of molecules play a pivotal role in\ndrug-likeness of drug candidates. In this regard, Token-Mol sig-\nniﬁcantly outperforms the graph-based models in generating mole-\ncules with better QED and SA score, exceeding 5 ~ 10% benchmarks,\nthereby demonstrating its proﬁciency in creating more drug-like\nmolecules. Compared to the sequence-based model TamGen, Token-\nTable 5 | Properties of the generated molecules by our model\nand other baseline models\nMetric Token-\nMol\nTamGen GraphBP Pocket2Mol TargetDiff\nValid 0.973 0.997 0.830 1.000 0.972\nIntDiv 0.849 0.829 0.879 0.812 0.860\n#Circle 52.698 38.573 73.181 33.568 68.897\nSimi Ori. 0.112 0.109 0.051 0.097 0.107\nSimi\nTraining\nset\n0.120 0.123 0.047 0.107 0.093\nHigher\nScore\n0.472 0.450 0.360 0.455 0.411\nValid: Validity of generated 3D structure, calculated as the proportion of 3D structures that can\nbe translated into canonical SMILES; IntDiv: Internal diversity90, an assessment of the distinc-\ntiveness of molecules within a molecular set, calculated using Tanimoto distance based on\nECFP4ﬁngerprints91,92; #Circle: a locality-based chemical space coverage measure with a setting\nmaximum value as 80, refer to the detailed description in the Methods section; Simi: Jaccard\nsimilarity between two molecular sets, also calculated based on ECFP4ﬁngerprints. Higher\nscore: the average ratio of Vina score of generated molecules exceeding the original ligands\nwithin each pocket from the test set (Ori.). The bolded values represent the best performers in\nthat metric. Bold formatting represents the best.\nTable 4 | Performance on differentdataset for regression tasks\nRegression (RMSE↓)\nDataSets\n#Molecules\nESOL\n1128\nFreeSolv\n642\nLipo\n4200\nCaco2\n906\nLD50\n7385\nAqsol\n9012\nAverage\n-\nXGBoost 1.112 ±0.086 1.958±0.245 0.909±0.032 0.455±0.031 0.651±0.024 1.199±0.052 1.047\nChemprop 0.549 ±0.028 1.106±0.125 0.603±0.020 0.429±0.019 0.600±0.021 0.907±0.027 0.699\nMapLight+GNN 0.529±0.062 0.959±0.278 0.623±0.018 0.352±0.016 0.600±0.032 0.906±0.024 0.662\nGEM 0.543 ±0 . 0 4 1 0.976±0.140 0.584±0.030 0.345±0.038 0.576±0.015 0.827±0.008 0.642\nK-Bert 0.671 ±0.086 1.026±0 . 0 7 7 0.641±0.011 0.377±0.022 0.596±0.043 0.931±0.004 0.707\nRT 0.657 ±0.031 1.389±0.235 1.046±0.528 0.483± 0.049 0.698±0.055 1.072±0.048 0.891\nToken-Mol\n(w/o GCE)*\n0.722±0.022 1.468±0.220 0.670±0.028 0.441±0.048 0.644±0.025 0.957±0.023 0.817\nToken-Mol 0.593 ±0.036 1.225±0.211 0.645±0.026 0.399±0.010 0.611±0.038 0.940±0.033 0.735\nToken-Mol (w/o GCE) is the model without GCE. Bold formatting represents the best.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 6\nMol shows no statistical difference in QED but performs worse in\nsynthetic accessibility. We analyze these two metrics fundamentally,\nfocusing on molecular structure (Supplementary Table 3). TamGen\nincorporates a VAE-based contextual encoder sampling the corre-\nsponding ligands within the pocket\n54, resulting in generated molecular\nstructures that closely resemble the originals, which aligns with\nprevious observations of the model’s poor molecular diversity. We\nsuppose that this mechanism enables the generated molecules to\nexhibit more reasonable structures, particularly in substructure fea-\ntures. Since the penalty terms contributing to the SA Score, such as the\nnumber of chiral centers, ring and macrocyclic structure are quite\nsimilar between Token-Mol and TamGen, the signiﬁcant difference\n−14\n−12\n−10\n−8\n−6\n−4\n−2\nVina Score\n−2\n0\n2\n4\n6\nLogP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nQED\n**** ****\n100\n200\n300\n400\n500\n600MW\n2\n3\n4\n5\n6\n7\nSA Score\n**** **** **** ****\n0\n50\n100\n150TPSA\nToken-Mol\n(n=245)\n0\n2\n4\n6\nJ0 1( ecnegreviD S -1)\nTamGen\n(n=221)\nGraphBP\n(n=3)\nPocket2Mol\n(n=153)\nTargetDiff\n(n=139)\nToken-Mol TamGen Pocket2Mol TargetDiff\n0\n1\n2\n3\n01( ecnegreviD SJ -1) *** *** **\nBetter Better Better\na\nb\nc\nFig. 3 | Distributions of molecular properties between Token-Mol and baseline\nmodels. aComparison of the average molecular properties distribution in the test\nset pockets (n = 77) for the molecules generated by Token-Mol and other baseline\nmodels (two models have different sample sizes due to the failed generation:\nGraphBP, n = 53; TargetDiff, n = 74). For monotonic metrics, signiﬁcance markers\nhave been noted with Token-Mol serving as the reference group, while the light\nblue area indicates the ideal range for interval metrics. A detailed description of the\nmetrics can be found in the Benchmark of Methods section. The data are shown\nwith box plots, which display the median at the center line, upper and lower\nquartiles as box limits, and 1.5x interquartile range as whiskers.b Independent\ndistributions of all detectable torsion angles for each model, where n represents the\nnumber of detected torsion angles.c Distributions of frequently occurring torsion\nangles (n = 48) that can be jointly detected in the selected models. In (a)&( c),\nsigniﬁcance tests were analyzed with a two-sided Mann-Whitney U test, p-values\nadjusted for multiple comparisons using the Bonferroni correction. Other details\nare described in the Methods section. All experimental groups were independently\ncompared to the control group (Token-Mol), detailed p-values can be found in\nSupplementary Table 9. *p < 0.05, **p < 0.01, ***p <0 . 0 0 1 ,* * * *p < 0.0001. Source\ndata are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 7\ncould chieﬂy arises from the fragments term that constitutes the main\ncomponents of the SA score57: From TamGen, approximately 10%\nfewer substructure features are extracted compared to those from\nToken-Mol. Moreover, the sampling of known ligand information in\nTamGen may lead to the incorporation of more common fragments,\nwhich could subsequently result in a lower SA score for the generated\nmolecules.\nAdditionally, analysis from the perspective of molecular structural\ninformation yields other interesting conclusions: Token-Mol incorpo-\nrates three-dimensional information through torsional angles, enrich-\ning the number of torsional angles and rotatable bonds. This may allow\nthe generation of moreﬂexible ligands that can betterﬁtc e r t a i n\nuniquely shaped pockets, as well as increasing molecular diversity.\nCompared to TamGen which is also sequence-based, Token-Mol does\nnot exhibit distinct differences in other structural features while\nincorporating 3D information. For instance, the proportion of mac-\nrocyclic structures or fused ring systems is much lower in Token-Mol\nthan in graph-based baseline models, and it is these structural features\nthat enhance the performance of Token-Mol in terms of QED and SA\nscores. The capabilities of Token-Mol in molecular structure provides a\npotential third pathway for molecular representation paradigms,\nbridging the gap between 2D sequence-based and 3D graph-based\napproaches, and holds promise for overcoming some inherent lim-\nitations of both methods.\nIn the three additional physicochemical properties closely related\nto drug-likeness, the overall distributions of molecules generated by\nToken-Mol for LogP and molecular weight entirely fall within the ideal\nrange. Compared to the results from other models, Token-Mol exhibits\na moderate distribution position and range. The metric TPSA deter-\nmines the oral bioavailability and membrane permeability of\nmolecules\n58,w i t hv a l u e sb e l o w1 4 0Å2 for cell membrane traversal and\nbelow 90 Å2 for blood-brain barrier penetration. The TPSA distribution\nof molecules generated by Token-Mol falls within the range of 70-100\nÅ\n2, which is more reasonable compared to other baseline models,\nsuggesting superior absorption and potential for further drug dis-\ncovery in central nervous system diseases\n59.\nBeyond metrics such as binding afﬁnity and molecular proper-\nties, theﬁdelity of torsion angles within generated molecules needs\nto be considered. Torsion angles will be used as an indicator to\nevaluate the reasonableness of the initial conformation. A molecule\nwith torsion angle distribution closer to that of the ground truth\nmolecule suggests that its conformation is more likely to be closer to\nthe real molecule and does not violate inherent physical constraints.\nMoreover, excessively twisted torsion angles in the initial con-\nformation can induce the conformation’s energy to become trapped\nin local minima during molecular docking, making it difﬁcult to\nescape and causing deviations in the docking results. Therefore,\nreasonable torsion angles are also beneﬁcial for virtual screening\nbased on docking.\nOur analysis involved the examination of torsion angles within the\n3D conformations of approximately 100,000 molecules from the\ntraining set in CrossDocked2020 dataset as a reference (Supplemen-\ntary Fig. 1), we extracted a total of 273 different types of torsion angle\ndistributions. Subsequently, we curated a subset of torsion angles,\ncharacterized by their abundance and non-random distribution,\nenabling an in-depth comparative analysis. Jensen-Shannon diver-\ngence (JSD) is used to assess the disparity between the torsion angle\ndistributions in the test set and those of the molecules generated by\nthe models.\nWe ﬁrst conducted a comprehensive evaluation to examine all\ndetectable dihedral angles. For the TamGen, the conformations we\nused were obtained using the ETKDG method from RDKit, which is a\nwidely used conformer generation algorithm that incorporates torsion\nangle preferences derived from experimental data to reﬁne the initial\nconformations generated by distance geometry methods\n60.I n\naddition, utilizing ETKDG ensures a fair comparison with Token-Mol,\nas our approach also derives the initial conformations from ETKDG,\nfollowed by a subsequent reﬁnement with the generated torsion angle\ntokens.\nHowever, independent assessments of the torsion angle dis-\ntributions for molecules generated by each model (Fig.3b) indicate\nthat Token-Mol may capture torsion angles that occur very infre-\nquently in the reference molecular set, achieving approximately 90%\nrecovery for these torsion angles, while the majority of baseline\nmodels fell around 60%-80%. This discrepancy of recovery presents a\nchallenge for making fair comparisons of the overall distribution\nlandscape of all detectable torsion angles between each model. Addi-\ntionally, it explains why the distribution of Token-Mol exhibits outliers\nwith JSD greater than 0.4. Since the model directly learns the numerical\ndistribution of torsion angles, it may struggle to accurately capture the\ndistributions of less frequently occurring torsion angles, resulting in\nperformance that is inferior to the experimentally based ETKDG\nmethod.\nWhile in the more common torsion angles, the advantages of\nlearning the numerical distribution of dihedral angles are clearly\nevident. We selected 48 frequently occurring torsion angles that\nwere present in molecules generated by all the selected models, with\neach angle appearing more than 1,000 times. This indicates that\nthese torsion angles exist in at least 1% of the reference molecules.\nWe plotted the distribution of these torsion angles (Fig.3c). From the\nplots, it is evident that Token-Mol exhibited signiﬁcantly lower JSD\nvalues for these frequently occurring torsion angles compared to the\nother models. This suggests that Token-Mol can more effectively\nlearn the numerical information associated with torsion angles that\noccur with sufﬁcient frequency, aligning with our initial intent to use\ntorsion angles as a representation of molecular three-dimensional\ninformation.\nEventually, we calculated the average molecular generation time\nfor each model. A faster generation speed may signiﬁcantly boost\nresearchers’efﬁciency by facilitating the acquisition of a more diverse\narray of molecules in a shorter timeframe and by speeding up the\nprocesses of screening and validating the generated compounds. This\nefﬁciency also reduces the demand for computational resources,\nenabling researchers with limited resources to effectively utilize\nthe model.\nTo ensure a fair comparison, we measured the cumulative time\nspent by each model in sampling pockets and generating molecules\nuntil an outputﬁle (in sdf/mol2 format) was obtained. As shown in\nSupplementary Table 13, when compared to models utilizing geo-\nmetric deep learning frameworks, Token-Mol demonstrated a\nremarkably higher generation speed, averaging approximately 35\ntimes faster for individual molecules. This efﬁciency stems from the\ndifferent methodologies adopted by competing models. For\ninstance, Pocket2Mol necessitates extensive sampling of molecular\nobjects, excluding invalid or duplicated molecules to maintain\ndiversity and validity. Similarly, TargetDiff requires performing\nthousands of rounds of sampling on the atoms within the pocket\nbefore molecular generation, ensuring high-quality outputs but sig-\nniﬁcantly impeding the generation process for both models. In\ncomparison to TamGen, which is also based on large language\nmodels, Token-Mol’s generation speed is relatively less competitive.\nWe attribute this to differences in model architecture. Token-Mol\nrequires the computation of multi-head conditional attention in the\nfusion block during each inference, which affects the inference\nspeed. Compared to TamGen, Token-Mol also requires additional\ninference for the torsion angle tokens, which necessitates longer\nsequence lengths and a larger vocabulary. These factors contribute\nto an increase in inference time. Additionally, TamGen is developed\nusing the Fairseq toolkit, and the differences in model encapsulation\nmay further contribute to variations in inference speed.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 8\nDrug design for real-world targets\nTo evaluate the generatability of models in designing drug candidates\nfor real-world therapeutic targets, we selected 8 targets from three\nimportant protein families, namely kinases, G-protein coupled recep-\ntors (GPCRs) and viral proteins, which had been widely studied in\nstructure-based drug design\n61– 63 and molecular generation64– 67.S p e -\ncially, our selection includes a unique dimeric pocket from Pro-\ngrammed cell death 1 ligand 1 (PD-L1), aiming to explore the models’\ncapability in designing small-molecule modulators for protein-protein\ninteractions.\nTo mimic a realistic drug discovery scenario, we designed a pro-\ncess similar to virtual screening. This involved generating an equal\nnumber of molecules targeting various receptors, followed by per-\nforming molecular docking to identify high-afﬁnity candidates. Sub-\nsequently, weﬁltered these candidates based on their QED and SA\nScores. Our goal was to produce potent drug-like molecules that\npossess high afﬁnity to the target, excellent drug-likeness, and favor-\nable synthetic accessibility. To this end, we set criteria that a potent\ndrug-like molecule should simultaneously satisfy Vina Score lower\nthan the average Vina Score of reference molecules from its corre-\nsponding target (Supplementary Table 7), QED of at least 0.5, and SA\nScore not exceeding 5.0.\nAs shown in Supplementary Table 15, our approach generated\npotent drug-like molecules across all targets, a capability that none of\nthe baseline models achieved. Approximately 20% of the generated\nmolecules met this criterion, with six out of eight targets achieving\noptimal or second-best proportions.Further analysis of the distribu-\ntions of Vina Scores and QED (Supplementary Fig. 2, Supplementary\nTable 4) for the molecules generated by those models reveals that\nToken-Mol not only produces molecules with high afﬁnity but also\nensures they possess desirable properties. This aligns with the results\nfor our test sets, suggesting that our model is capable of identifying\npromising lead compounds in real-world drug discovery scenarios.\nFrom the perspective of the targets, one factor inﬂuencing the\nresults above is that the selected pockets do not all bind molecules\nwith high afﬁnity for the respective targets, as the input pocket centers\nfor molecular generation and the docking grid centers depend on the\noriginal ligands. For instance, in the case of DDR1, the original ligand is\na low-afﬁn i t yh i tf r a g m e n ti d e n t iﬁed through fragment screening\n68.\nTherefore, most methods, including Token-Mol, encountered chal-\nlenges with this target, while TamGen may have achieved optimal\nresults by sampling the original fragments. TamGen also gained\nadvantages in several target pockets with high-afﬁnity original ligands,\nwith the proportion of drug-like molecules being several times higher\nthan that of Token-Mol.\nHowever, our analysis revealed a strong positive correlation\nbetween the proportion of drug-like molecules generated by TamGen\nand the pIC\n50 values of the original ligands, with a Pearson correlation\ncoefﬁcient of 0.81, while other methods scored below 0.5 (Supple-\nmentary Table 8). This does not reﬂect real drug discovery scenarios,\nespecially when facing targets without reported high-afﬁnity ligands.\nIn such cases, the generated pocket structures may only contain nat-\nural ligands, low-afﬁnity ligands obtained through simple screening, or\neven computational structures without ligands. TamGen’s approach\ncan even be misled. For example, in the case of 3CLPro, the original\nhigh-afﬁnity ligand within the pocket is covalent binding, while the\nactual afﬁnity of the non-covalent, structurally similar ligand telaprevir\nis 18μM\n69,70. In contrast, our method performed exceptionally well for\nthis target, with approximately 25% of the generated molecules sur-\npassing the average afﬁnity of reference ligands while maintaining\nexcellent drug-like properties. Overall, although Token-Mol did not\nachieve the best results across all targets, it demonstrated stable\ngeneralizability, generating an acceptable proportion of promising\nmolecules for unseen targets, showcasing its potential for application\nin real drug discovery scenarios.\nFurthermore, to evaluate the gains of RL within this model fra-\nmework, we selected cyclin-dependent kinase 2 (CDK2), representing\nkinases, and the adenosine A2A receptor (ARA2A), representing\nGPCRs, as two moderately performing targets from two signiﬁcant\nfamilies. As shown in Fig.4, the molecules generated by Token-Mol\nexhibit favorable drug-likeness, synthesizability, and promising afﬁnity\nwithin the target pockets of two proteins that exhibit signiﬁcant\nstructural and functional differences. These molecules possess more\nrational structures compared to those generated by other models and\ndisplay distinct scaffolds between the two different targets.\nAmong the molecules generated byother baseline models, those\nproduced by GraphBP exhibit distorted conformations, while those by\nPocket2Mol are simple aromatic ring derivatives and exhibit minimal\ndifferences between the two targets. Similar phenomena can also be\nobserved in the molecules generated by the aforementioned two\nmodels in other use cases (Supplementary Fig. 3). For the molecules\ngenerated by TamGen, the molecule generated to CDK2 is a widely\nreported pan-JAK inhibitor, with multiple JAK family protein structures\ncontaining this ligand available in the PDB database\n71.A l t h o u g ht h i s\nmolecule does exhibit weak activity against CDK272,i ti sd i fﬁcult to\nascertain whether the model accurately generated the speciﬁcm o l e -\ncule or if it simply overﬁtted and output a molecule present in the\ntraining set. While in the case of ARA2A, the generated molecule is a\nsimple amino acid derivative, as it references the original ligand with a\nsimilar low molecular weight. As for TargetDiff, while the molecules\ndemonstrate favorable results in terms of QED and Vina score, it is\nnoteworthy that molecules for two targets contain tricyclic scaffolds\nand 7-membered cyclic groups, which are challenging to synthesis.\nThis can explain why the molecules generated by TargetDiff have\nhigher Vina scores in the former test, as these groups with large\nvolume occupy as much space as possible within the pocket, creating\nmore hydrophobic contacts. The predicted binding modes in the two\nselected cases exhibit that, as the molecules generated by Token-Mol\nﬁt the shape of the pocket cavity more precisely, whereas those gen-\nerated by other baseline models only occupy part of the pocket cavity.\nTo further demonstrate Token-Mol’s capability to generate\nmolecules that resemble real-world ligands, we calculated the simi-\nlarity of Bemis-Murcko scaffold\n73 and Fréchet ChemNet Distance74\n(FCD) between the molecules generated by Token-Mol and other\nbaseline models and the reference molecules (Supplementary\nTable 6). The results indicate that while TamGen achieved the highest\nscaffold similarity for most targets by generating molecules based on\nthe original ligands as references, Token-Mol produced molecules with\ngreater similarity across all tested targets in a series of models that\nwere not constrained by this reference condition. Despite TamGen\nexcelling in scaffold similarity, it did not completely outperform\nToken-Mol in the FCD comparison, which assesses chemical and bio-\nlogical feature similarities. In contrast, Token-Mol secured higher FCD\nrankings across most targets, underscoring its advantage in overall\nsimilarity. Notably, molecules generated by Token-Mol maintain an\nacceptable similarity to known ligands while also preserving good\ndiversity, offering hope for the discovery of novel chemical entities in\nreal-world targets.\nSimultaneously, we chose ARA2A as the target to visualize the\nsimilarity between ligands and generated molecules. We selected\nseveral molecules from Token-Mol and TargetDiff that exceeded the\naverage Vina score and QED thresholds of reference molecules (Sup-\nplementary Table 7) against ARA2A for display. We present six real\nligands of ARA2A, including agonists and antagonists (Supplementary\nFig. 4). Notably, adenosine, the leftmost ligand, serves as the natural\nligand of ARA2A and contains a purine scaffold. The other discovered\nARA2A ligands all possess a nitrogen heterocyclic core as their scaf-\nfold, similar to purine, which can be monocyclic, bicyclic, or tricyclic\n75.\nFrom the perspective of medicinal chemists, for the antagonists, which\nare majority of ARA2A ligands, their structure-activity relationship\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 9\n(SAR) and co-crystallized structures indicate that, in addition to the\nnitrogen heterocyclic scaffold, there are aromatic rings such as furan,\nthiophene, or benzene directly connected to the scaffold or located\none or two carbon atoms away from it\n76. These aromatic groups can\npenetrate more deeply into the internal space of the orthosteric\npocket, facilitating hydrophobic interactions with the surrounding\nlipophilic residues\n77,78, thereby enhancing the ligand’sa fﬁnity for the\nreceptor and enabling strong competition with the natural ligand.\nAmong the molecules generated by Token-Mol, it can be observed\nthat most contain monocyclic or bicyclic nitrogen heterocyclic scaf-\nfolds resembling real-world ligands, whereas those generated by\nTargetDiff differ obviously from real-world ligands. Furthermore,\nthese molecules with nitrogen heterocyclic scaffolds possess aromatic\nrings, such as benzene or pyrazole, directly connected to the scaffold.\nHowever, we conducted an in-depth analysis of the binding modes of\nthese four ligands, which feature scaffolds similar to those of real\nligands. The results demonstrate (Supplementary Fig. 8) that these\nfour molecules do not achieve optimal recovery rates for key inter-\nactions, for instance, hydrogen bonds with N253 and E169 andπ-π\nstacking interactions with F168. Most of them only formed interactions\nwith either N253 or F168, while only one molecule exhibits interactions\nwith both residues. This indicates that although our model achieved\nCDK2 ARA2A\na\nb\nCDK2\nARA2A\nGraphBPToken-Mol Pocket2Mol TargetDiffTamGen\nN\nOO\nS\nN\nN\nO\nNH\nHO\nVina Score\nQED\nSA Score\nLp5\nTPSA\n-8 -9 -10 -11\n0.6\n0.7\n0.8\n0.9\n5\n4\n3\n2\n2\n3\n4\n5\n30\n60\n90\n120\nVina Score\nQED\nSA Score\nLp5\nTPSA\n-8 -9 -10 -11\n0.6\n0.7\n0.8\n0.9\n5\n4\n3\n2\n2\n3\n4\n5\n30\n60\n90\n120\nToken-Mol\nToken-Mol(RL)\nTamGen\nGraphBP\nPocket2Mol\nTargetDiff\nFig. 4 | Evaluation on real-world targets.Comparison between (a) structures and\nbinding modes, and (b) related molecular properties of drug-like molecules with\nthe highest afﬁnity generated for CDK2 and ARA2A by the Token-Mol and baseline\nmodels. The detailed information of molecular properties is presented in Sup-\nplementary Table 5.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 10\nbetter results in terms of ligand similarity compared to other models,\nthe target-speciﬁc interactions were not well reproduced. One reason\nfor this is the inherent limitations of the docking algorithm used, which\nleads to discrepancies between the predicted binding poses and the\nactual ones. On the other hand, the model still has some shortcomings\nin utilizing information about the protein pocket, which remains a key\narea for improvement for our model and future pocket-based mole-\ncular generation approaches.\nFurther optimization with reinforcement learning\nThe content above has already demonstrated that Token-Mol can\ngenerate molecules with high drug-likeness, ease of synthesis, and\nrational structures in the pockets of given targets. However, when\ncompared to the expert models like TargetDiff for pocket-aware gen-\neration, the molecules generated by Token-Mol still exhibit lower\nafﬁnity to pockets from the test set and selected real-world targets\n(Supplementary Tables 1 and 2). To address this, we employed a\nreinforcement learning approach proposed by Olivecrona et al.\n79 to\noptimize the afﬁnity of generated molecules for speciﬁct a r g e tp o c k -\nets. Within this framework, constraints such as conformational clash\nand drug-likeness are enforced to ensure that the molecules maintain\ndesirable properties. This strategy aims to maximize afﬁnity for target\npockets while preserving the excellent molecular properties demon-\nstrated by the model, as detailed in the Methods section. Notably,\noptimizing geometric graph-based models such as TargetDiff is chal-\nlenging due to their high complexity, and RL has not yet been applied\nto these models for 3D pocket-based molecular generation.\nWe conducted a total of 1,000 steps of reinforcement learning\noptimization on CDK2 and ARA2A, the two targets used for demon-\nstration in the previous section. Throughout the reinforcement\nlearning process, we recorded the average values of key metrics at\neach step (Fig.5a). It can be observed that during the training of the\ntwo targets, the reward score essentially converged within 1,000 steps,\nindicating the stabilization of the agent model’s training. Regarding\nour primary optimization objective, the Vina score, the average value is\noptimized from around−8 to approximately−9.5, with no apparent\noscillations observed after convergence. As for QED, which serves as a\nconstraint condition, although the reward term in the reward function\nis binary rather than positively correlated with QED, it was found that\nt h eQ E Dv a l u ei n i t i a l l yi n c r e a s e da n dt h e nc o n v e r g e da st h er e i n f o r -\ncement learning steps increased for both targets, suggesting that QED\nis also optimized under the set reward function. Although different\ntrends were observed in the SA score during the reinforcement\nlearning process for the two targets, the results remained below the\nthreshold of 5, consistent with our previous tests (Supplementary\nFig. 5) that focused solely on afﬁnity optimization. These trends\ndemonstrate that our model can achieve optimization in molecule\ngeneration tasks for speciﬁc target pockets through reinforcement\nlearning under constraints.\nAdditionally, we selected molecules from theﬁrst step, the last\nstep, and step before the convergence of the reward score, showcasing\nt h em o l e c u l ew i t ht h eh i g h e s ta fﬁnity in those steps (Fig.5a). From the\nperspective of speciﬁc molecules, it is obvious that the molecular\nscaffolds undergo substantial changes at different training stages, and\nthe occurrence of unreasonable structures such as tricyclic structures\nor seven-membered ring groups also decreases. For their binding\nmodes (Fig.5b), it can be seen that the scaffold graduallyﬁts into the\npocket, which explains the gradual improvement of the Vina score\nduring the training process.\nFurthermore, to reduce the bias introduced by a single docking\nmethod, we conducted additional docking tests using Glide\n80 and\nSurﬂex-dock81 for the molecules presented in Fig.5c, and the results\ndemonstrate that reinforcement learning indeed optimized the\nmolecules’ afﬁnity for the targets. In the case of CDK2, the docking\nscores (i.e., the predicted afﬁnities) obtained from all three methods\nimproved as the training steps increased. In ARA2A, a similar trend was\nobserved, with the exception of Glide. Overall, in both targets, the\nmolecules obtained after training convergence achieved the best\nresults across all three docking methods, further conﬁrming the cap-\nability of reinforcement learning to optimize the afﬁnity of generated\nmolecules for the target pockets.\nChat to Token-Mol\nToken-Mol’s token-only framework confers a signiﬁcant advantage\nover traditional regression models by enabling the seamless integra-\ntion of cutting-edge large-scale model techniques, including prompt\nlearning, mixture of experts (MoE)\n82, and retrieval-augmented gen-\neration (RAG)83. In this context, we demonstrate an instance of prompt\nlearning.\nTo illustrate this capability, we present several straightforward\ndialogue use cases. By employing prompt learning, we can control the\nexecution of tasks such as property prediction mentioned in this study.\nInitially, we insert speciﬁc prompts, such as“Predict ESOL” to ﬁne-tune\nthe model. As shown in Supplementary Fig. 6, this enables direct\ninteraction with the model post-prompting, allowing users to request\npredictions of different molecular properties. In this example, we\nqueried various properties of different molecules, and Token-Mol\nsuccessfully provided the corresponding predictions. This demon-\nstrates the potential of Token-Mol for engaging in meaningful dialo-\ngues with chemists. Users may provide molecular conformations, but\nsince Token-Mol can generate the corresponding conformations, the\nﬁnal output will include only the predicted target properties.\nAdditionally, future iterations can incorporate RAG. When\nquerying Token-Mol about a speciﬁc property of a molecule, the sys-\ntem employs vector search based on embeddings to convert the query\ninto a vector. This vector is then matched with highly relevant vector\ndescriptions from a database to provide contextual information. The\nquery, along with the retrieved context such as spatial structure\ninformation and other relevant properties, is then input to Token-Mol,\nwhich then generates the answer.\nThe aforementioned example highlights the unique of token-only\nmodels to seamlessly integrate with general models, a capability that is\nnot exhibited by regression models.\nDiscussion\nThis study proposes Token-Mol, the inaugural token-only, extensive\npre-trained language model tailored for drug design. Rooted in the\nGPT framework, Token-Mol integrates random causal masking to\nenhance its ﬂexible applicability across diverse drug design tasks.\nAdditionally, we propose the Gaussian cross-entropy loss function to\nfoster improved acquisition of continuous spatial information\nthroughout model training, thereby notably reinforcing its perfor-\nmance in regression tasks. Furthermore, through the integration of RL,\nToken-Mol achieves expedited optimization towards predetermined\nobjectives in speciﬁc tasks, aiming to achieve desired outcomes efﬁ-\nciently. To substantiate these capabilities, we conducted assessments\nacross three pivotal drug design tasks.\nIn the pocket-based generation task, Token-Mol achieves results\nclose to expert models in the pocket-based generation task and\nobtains optimal results in terms of drug-likeness and synthesizability\nof molecules. Beneﬁt from the rapid inference of the language model,\nToken-Mol can generate molecules within the pocket in a shorter time.\nAdditionally, tests on speciﬁc real-world targets have also demon-\nstrated that our model can obtain molecules with excellent afﬁnity,\ndrug-likeness, and synthesizability under various conditions simulat-\ning real-world virtual screening with a higher proportion. For speciﬁc\noptimization goals in the speciﬁc targets, we performed reinforcement\nlearning, and the results also proved that Token-Mol can achieve\noptimization under constraint conditions, demonstrating the broad\napplication potential of our model.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 11\nWe subsequently evaluated its capability in molecular conforma-\ntion generation. Token-Mol demonstrated superior performance\nrelative to other SOTA models, exceeding their performance by\napproximately 24% in COV-P and 21% in MAT-P. Notably, Token-Mol\ne x h i b i t e di m p r o v e de fﬁcacy in molecules with a higher number of\nrotatable bonds.\nLastly, we assessed its performance in molecular property predic-\ntion tasks. Leveraging the advantages of the Gaussian cross-entropy loss\n0 200 400 600 800 1000\nStep\n2\n4\n6\n8\n10\n12\n14\nReward Score\nCDK2\nARA2A\n0 250 500 750 1000\n−10\n−9\n−8\nVina Score\nCDK2\nARA2A\n0 250 500 750 1000\n0.5\n0.6\n0.7\n0.8\nQED\nCDK2\nARA2A\n0 250 500 750 1000\nStep\n2.5\n3.0\n3.5\n4.0\nSA Score\nCDK2\nARA2A\nN\nO\nO N O\nN N\nO\nN\nH\nN\nS N\nN\nNH2\nO\nF\nN\nN NH\nN\nN\nF\nN\nO\nNNF F\nF\nOHHO\nNCl\nCl\nOH\nH\nN\nO\nS\nO\nO\nO\nCDK2\nARA2A\nStep 1\nStep 400\nStep 200\nStep 1000\na\nb\nCDK2\nARA2A\nStep 400\nStep 200\nStep 1\nStep 1\nStep 1000\nStep 1000\n0 500 1000\n−10.2\n−10.0\n−9.8\n−9.6\nVina\n-9.5\n-9.8-9.8\n-10.1\n-10.3\nCDK2\nARA2A\nBetter\n0 500 1000\n−10\n−8\n−6\n−4\nGlide\n-3.768\n-6.979\n-3.904\n-5.401 -6.875\n-9.937\nCDK2\nARA2A\nBetter\n0 500 1000\nStep\n−2\n−1\n0\n1\n2\nSurflex-dock\n-2.527\n0.434\n1.1460.810\n2.196\n1.924\nCDK2\nARA2A\nBetter\nc\nFig. 5 | Molecular performance in RL process. aKey metrics such as reward score,\nVina score, QED, and SA score during the process of RL. The 2D structure of\nmolecules from the different stages in the RL are also displayed.b The binding\nmodes of selected molecules predicted with QVina2.c Change trends in the pre-\ndicted afﬁnities of selected molecules for their respective targets using different\ndocking methods. Source data in (a) are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 12\nfunction, Token-Mol demonstratedaccuracy on par with state-of-the-\nart models. In regression tasks, Token-Mol outperformed the token-\nonly model RT by approximately 30% and surpassed existing sequence-\nbased methods, approaching the performance of GNN-based methods.\nMeanwhile, Token-Mol demonstrates the ability to simplify com-\nplex problems, capitalizing on the inherent advantages of large language\nmodels. This proﬁciency is particularly pronounced in sophisticated\ntasks such as pocket generation, where Token-Mol achieves a good\nbalance of speed and efﬁcacy. Notably, in comparison to the state-of-\nthe-art model TargetDiff, Token-Mol’s inference speed is 35 times faster.\nWhile Token-Mol demonstrates considerable potential, several\nareas require further enhancement. In this study, we evaluated its\nperformance on only three representative downstream tasks, leaving\nmany others unaddressed. The molecular diversity within the pre-\ntraining data is also limited. In comparison to most of the expert\nmodels cited in the references, Token-Mol features a larger number of\nparameters. This relatively substantial model size may impose certain\nlimitations on its deployment and application. However, we believe\nthat the multitaskingﬂexibility afforded by the pre-training andﬁne-\ntuning paradigm for the backbone model allows Token-Mol to achieve\na favorable balance between size and application potential. None-\ntheless, this also poses challenges for the iterative development of\nlarger-scale models in the future.\nFuture research will focus on optimizing Token-Mol by expanding\nthe training dataset and developing speciﬁc components tailored to\nparticular downstream tasks. Comprehensive evaluations across a\nbroader range of drug design tasks will be conducted. Additionally, we\naim to integrate Token-Mol with general artiﬁcial intelligence models,\nutilizing techniques from various large language models such as\nprompt learning, MoE, and RAG. This integration will facilitate direct\ninteraction between researchers and Token-Mol through conversa-\ntional interfaces, enhancing its role as a research assistant.\nIn summary, this study presents a token-only foundational model\nfor drug design, introducing the initial version of Token-Mol. Its\ndevelopment offers an approach towards unifying AI drug design\nmodels, paving the way for comprehensive drug design using a single\nfoundational model.\nMethods\nModel architecture\nBackbone. Token-Mol is structured with 12 layers of Transformer\ndecoders, each equipped with 8 attention heads. Employing auto-\nregressive approach, Token-Mol predicts both the 2D and 3D struc-\ntures of molecules while explicitly representing them. To ensure data\nintegrity during autoregressive training and inference, masking\nmatrices are employed to conceal unencoded segments, thus pre-\nventing information leakage. The multi-head attention mechanism,\nintegral to the Transformer architecture, empowers Token-Mol to\nsimultaneously attend to diverse subspaces of the input, facilitating\nthe capture of richer information. Within this mechanism, each\nattention head learns a unique setof weights to compute attention\nscores for different positions in the input sequence, facilitating the\ncalculation of the input sequence’s representation. By harnessing\nparallel computation across multiple attention heads, Token-Mol gains\nthe capacity to interpret the input sequence from various perspectives,\nconsequently enhancing its representational capability and general-\nization performance. The attention mechanism is shown in Eq.1:\nAttention Q, K, VðÞ =s o f t m a x\nQKT\nﬃﬃﬃﬃﬃﬃ\ndk\np\n !\nV: ð1Þ\nwhere Q, K,a n dV represent the query, key, and value matrices,\nrespectively, anddk is the dimension ofK.\nTo indicate the beginning or end of the sequence during sam-\npling, it is necessary to deﬁne a start token and an end token, denoted\nas “<|beginoftext | >” and “<|endofmask | >”, respectively. During the\ntraining, the“<|beginoftext | >” token is concatenated to the sequence\nas the input. The objective during the training phase is to minimize the\nnegative log-likelihood, as shown in Eq.2:\nL= /C0\nX\nn\ni =1\nlog px ijx<i\n/C0/C1\n: ð2Þ\nDuring the generation phase, molecular strings are generated\nusing an autoregressive approach based on smiles, which are then\nconcatenated together as shown in Eq.3:\npxðÞ =\nY\nn\ni =1\npx ijx<i\n/C0/C1\n: ð3Þ\nGaussian cross-entropy (GCE) loss function. Language models\ncommonly employ the cross-entropy loss function as their primary\nloss function. The cross-entropy loss function is generally utilized to\nquantify the disparity between the probability distribution produced\nby the model and the actual labels. Assuming a classiﬁcation problem,\nfor each sample, the model outputs a probability distribution indi-\ncating the likelihood of the sample belonging to each class. The gen-\nuine labels, on the other hand, are one-hot encoded vectors\nrepresenting the class to which the sample belongs. The cross-entropy\nloss function is employed to measure the dissimilarity between the\nprobability distribution produced by the model and the genuine labels.\nIn the context of language models, the speciﬁc equation for calculating\nthe cross-entropy loss function is as follows:\nL = /C0\n1\nm\nXm\ni =1\nXn\nj =1\nyij log qðxijÞ: ð4Þ\nHere, m represents the batch size, andn denotes the length of\neach data point.yij signiﬁes thej-th element of the true label for thei-th\ndata point (taking values of 0 or 1), andqðxijÞ represents thej-th ele-\nment of the probability distribution output by the model. A lower\ncross-entropy loss indicates a closer resemblance between the model’s\noutput probability distribution and the true labels, thereby reﬂecting\nbetter model performance.\nHowever, the conventional employment of the cross-entropy loss\nfunction is primarily conﬁned to discrete category prediction tasks,\nrendering it inadequate for continuous value prediction endeavors\nsuch as regression. In our investigation, we encounter a spectrum of\ntasks encompassing both classiﬁcation, exempliﬁed by SMILES strings,\nand regression, including torsion angles and molecular property pre-\ndiction. In response to this challenge, the regression transformer dis-\nassembles each digit of continuous numerical values into distinct\ntokens and incorporates specialized numerical embeddings. None-\ntheless, their methodology does not fundamentally rectify the issue, as\nit neglects to facilitate the model’s comprehension of the relative\nmagnitude relationships inherent in numerical values. Notably, the\nmodel uniformly assigns loss values in the event of inaccurate pre-\ndictions, irrespective of the predicted token. For instance, if the label\ndenotes a torsion angle ofπ, erroneous predictions of 3 or 0 result in\nidentical loss values.\nTo surmount this constraint, we propose the GCE loss function\ntailored speciﬁcally for regression tasks. As shown in Fig.1,f o re a c h\nprediction, we construct a Gaussian distribution centered around the\nlabel’s value, thereby adjusting the probabilities of surrounding tokens\nfrom their original values of 0 to correspond with the Gaussian dis-\ntribution. Consequently, in Eq.5,w h e r eðpx\nijÞ is initially denoted as\neither 0 or 1, we modify it to signify a Gaussian distribution centered\naround the label’s value, thereby effectively mitigating the issue.\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 13\nThe GCE loss function is deﬁned as:\nL = /C0 1\nm\nXm\ni =1\nXn\nj =1\n1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n2πσ2\np e/C0\nðxij /C0 yij Þ2\n2σ2 log qðxijÞ: ð5Þ\nThrough the implementation of this conﬁguration, tokens in\nproximity to the label are allocated greater weights, whereas tokens\ndistanced from the label receive diminished weights. This methodol-\nogy facilitates the comprehensive learning of relationships between\nnumerical values by the model.\nPocket encoder and fusion block. We utilized the protein pocket\nencoder trained by Odin et al.\n65, maintaining its parameters frozen\nthroughout the training process. To merge the information derived\nfrom the pocket encoder with the existing molecule information\nwithin the model, we employed a multi-head condition-attention\nmechanism. Diverging from traditional cross-attention mechanisms,\nour approach involved the adoption of a multi-head condition-atten-\ntion mechanism to fully integrate the information generated at each\nautoregressive step into subsequent generations. This mechanism\ntreats each token produced during autoregression as a prerequisite\ncondition for iterative generation. Consequently, the entire query, key,\nand value matrices stem from the original sentence itself. Particularly,\nas shown in Fig.1c, this condition-attention fundamentally regards\nprotein information as prompt data, enabling the model to analyze the\ninteraction between protein information and previously generated\ntokens.\nReinforcement learning.R E I N F O R C E\n84, an RL algorithm based on\npolicy gradients, utilizes Monte Carlo methods to determine the\noptimal policy, and it has been applied in various molecular generation\nmethods\n79,85– 87. In this work, we used its variant REINVENT79 algorithm\nto optimize the model. We aim to optimize the pre-trained model\nparameter θ for the task of generating molecular sequences, so that\nthe optimized model can generate molecules with desired properties,\nas shown in Eq.6:\nθ\n* = argmax θðEτ/C24 πθ\nðGðτÞÞÞ: ð6Þ\nThe presented formula elucidates the policyπθ as contingent\nupon model parametersθ,w i t hτ delineating a trajectory spanning\nstates st and actionsat from the initial time stept = 0 to the terminal\nstep t = T. The action probabilities of a sequenceA are deﬁned as\nPAðÞ = QT\nt πθ at jst\n/C0/C1\n,a n dl o gπθ at jst\n/C0/C1\nrepresents the sum of the log\nprobabilities of each action given the prior state. According to the\nREINFORCE, the objective function can be derived as follows:\nJ θðÞ = E\nπθ\nGs t , at\n/C0/C1/C0/C1\n=\nXT\nt =0\nlog πθ at jst\n/C0/C1\nGs t , at\n/C0/C1\n: ð7Þ\nThe reward att within the trajectory is designated asrs t , at\n/C0/C1\n.\nEquation 8 concisely portrays the aggregate reward accumulation\nfrom time stept to theﬁnal state, encapsulating the core essence of\nthe trajectory’s reward accumulation dynamics.\nGs t , at\n/C0/C1\n=\nXT\nk = t\nγk/C0 t rs k , ak\n/C0/C1\n: ð8Þ\nWithin the molecular generation realm, computingGs t , at\n/C0/C1\nfor\neach step in a trajectory, corresponding to incomplete molecules,\nproves impracticable given the inability to reliably estimate the total\nmolecule score from its constituent fragments alone. This scenario\nconverges with the sparse reward paradigm prevalent in reinforce-\nment learning. To surmount this challenge and enable the deployment\nof the REINFORCE algorithm in this context, we advocate for equating\nthe complete molecule score with the score at each step, thereby\nreformulatingJðθÞ as:\nJ θðÞ = G τðÞ\nX\nT\nt =0\nlog πθ at jst\n/C0/C1\n: ð9Þ\nIn REINVENT, two policies are deﬁned: the Prior policy and\nthe Agent policy, with their respective action probabilities\ndenoted as PAðÞ\nPrior and PAðÞ A. An augmented likelihood is\nintroduced as logPðAÞU = PAðÞ Prior + σR AðÞ ,w h e r eR AðÞ represents\nthe reward of sequence A. The expression for GAðÞ is\nrestructured to ½log PAðÞ A /C0 log PAðÞ U/C1382= log PAðÞ A. Consequently,\nthe J θðÞ = ½log PAðÞ A /C0 log PAðÞ U/C1382.\nReward function.T oo p t i m i z ea fﬁnity, the reward function is designed\nto prioritize molecules that meet a promising Vina score. Molecules\nthat exceed the afﬁnity threshold and comply with the QED constraints\nreceive additional rewards. Molecules that do not meet the afﬁnity\nthreshold or are non-compliant are penalized. Thus, the reward func-\ntion R mðÞ is described as Eq.10:\nR mðÞ =\nω /C1 Vina mðÞ /C0 init + 0:1ðÞ + θ\nqed, if VinaðmÞ ≤ init\n0:1, if Vina ðmÞ >i n i t\n0, if invalid\n8\n><\n>:\nð10Þ\nwhere m is molecule;Vina(m) represents Vina score, where a smaller\nvalue is preferable; init is the threshold value of Vina score, which is set\nas −8. To avoid the issue of sparse rewards, we have imposed a reward\nweight ω, set as 5, and a proper penalty term set as 0.1 for molecules\nwhich do not meet the threshold of Vina score.θ\nqed is a reward term for\nmolecules that comply with the restraint of QED, describe as Eq.11:\nθqed = 1, QED ≥ 0:5\n0, QED < 0 :5\n/C26\nð11Þ\nRandom causal masking\nThe conventional left-to-right causal masking method exclusively relies\non the context preceding the generated tokens, thereby proving\ninadequate for accomplishing the inﬁlling task. To enhance the adapt-\nability to a wider array of downstream tasks, we opted to train it using\nrandom causal masking\n88,89 in lieu of the left-to-right causal masking.\nThroughout the training process, we commence by sampling the\nnumber of mask spans from a Poisson distribution centered around a\nmean of 1, while enforcing a limit that conﬁnes the count of mask spans\nwithin the range of 1 to 6. Following this, we employ random sampling to\nestablish the length of each span. The locations of the masks are iden-\ntiﬁed using placeholders denoted as“<|mask:k | >”,w i t h“k” signifying the\nindex of the speciﬁc mask span. Subsequently, the content subjected to\nmasking is afﬁxed to the sequence’s end, preceded by the“<|mask:k | >”\npreﬁx. In the inference phase, a sequence incorporating placeholders“<|\nmask:k | >” is presented as the contextual input, complemented by the\naddition of “<|mask:k | >” at the sequence’s conclusion to steer the\nmodel’s generation of content for the“<|mask:k | >” segments.\nBenchmark\nMolecular conformer generation. COV and MAT scores are funda-\nmental metrics utilized as benchmarks in the Conformer generation\ntask, extensively employed across conformer generation endeavors.\nCOV and MAT metrics are further categorized into Recall and Precision\nmeasures. Recall is deﬁned as:\nCOV /C0 RðS\ng , Sr Þ = 1\nSr\n/C12/C12 /C12/C12\n/C12/C12\n/C12 C 2 S\nr\n/C12/C12\n/C12RMSD C, ^C\n/C16/C17\n≤ δ, ^C 2 S\ng\nno\n: ð12Þ\nMAT /C0 RðSg , Sr Þ = 1\nSr\n/C12/C12 /C12/C12\nX\nC2Sr\nmin\n^C2Sg\nRMSD C, ^C\n/C16/C17\n: ð13Þ\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 14\nwhere Sg denotes the ensemble of generated conformations, whileSr\nrepresents the ensemble of true conformations.C and ^C represent\nindividual conformations from the sets of true and generated con-\nformations, respectively, withδ acting as the threshold, set at 1.25 Å.\nThe COV metric evaluates the percentage of structures in one set\nthat are encompassed by another set, where inclusion indicates that\nthe RMSD between two conformations falls below a speci ﬁed\nthreshold δ. Conversely, the MAT scores gauge the average RMSD\nbetween conformers in one set and their nearest counterparts in\nanother set. Precision, as described in the provided equation, inter-\nchanges S\ng and Sr . Consequently, while Recall entails comparing each\ntrue conformation with all generated conformations, Precision\ninvolves comparing each generated conformation with all true con-\nformations. Precision typically accentuates quality, while Recall is\nmore concerned with diversity.\nPocket-based molecular generation\nUnless otherwise speciﬁed, the following molecular descriptors and\nmolecularﬁngerprints are calculated using RDKit (version 2022.09.1).\nValid. Validity of the generated 3D structure, calculated as the pro-\nportion of 3D structures that can be translated into canonical SMILES.\nIntDiv. Internal diversity90, an assessment of the distinctiveness of\nmolecules within a molecular set, calculated using Tanimoto distance\nbased on ECFP4ﬁngerprints\n91,92, which can be described as:\nIntDiv: = 1\nSjj 2\nX\nx, y 2 S\nx≠y\ndx , yðÞ\nð14Þ\nwhere S is the generated molecules set,d is Tanimoto distance, which\nis deﬁned as 1 minus the Jaccard similarity coefﬁcient calculated using\nmolecularﬁngerprints.\n#Circle. a locality-based chemical space coverage measure93.D e s c r i -\nbed as:\n#Circles S; d, tðÞ : =m a x\nC/C18 S\njCj s:t: dx , yðÞ > t, 8 x ≠ y 2 C ð15Þ\nwheret is a distance threshold set as 0.75.C is a subset ofS,c o n t a i n i n g\nas p e c iﬁc molecule x and it’s non-neighboring molecules. Due to the\nsensitivity of this metric to the number of generated moleculesSjj and\nthe varying counts of available molecules produced by different\nmodels for a given target, this value is calculated by randomly selecting\n80 molecules for each target.\nSimi. Jaccard similarity between two molecular sets, as\nmentioned above.\nVina score. The binding energy of ligands to protein pockets by using\nQVina2\n94.\nHigher score. The average ratio of generated molecules exceeding the\noriginal molecule within each pocket.\nMW. Molecular weights. the optimal range is between 100 and 60095.\nTPSA. topological polar surface area96, the optimal range is between 0\nand 14058.\nLogP. The octanol-water partition coefﬁcient, typically falls within the\nrange of−0.4 to 5.6 for the majority of druglike compounds97.\nLipinski. Lipinski’sr u l e - o f -ﬁve98, which consists of the following cri-\nteria: the molecular weight of the compound is less than 500 Daltons;\nthe number of hydrogen bond acceptors in the compound’ss t r u c t u r e\n(including hydroxyl and amino groups) does not exceed 5; the number\nof hydrogen bond donors in the compound does not exceed 10; the\nlogarithm of the compound’s logP falls within the range of−2t o5 .t h e\nnumber of rotatable bonds in the compound does not exceed 10.\nQED. Quantitative estimation of drug-likeness\n99, subsequent\nresearchers have normalized the properties of Lipinski’sr u l e - o f -ﬁve\ninto continuous values ranging from 0 to 1, where higher values indi-\ncate higher drug-likeness of molecules.\nSA score. Synthetic accessibility score\n57, the SA score represents the\nsynthesis accessibility of molecules and is designated on a scale of 1 to\n10, based on chemical expertise. A higher value indicates greater dif-\nﬁculty in synthesis.\nSigniﬁcance tests.A l ls i g n iﬁcance tests were conducted using the\nMann-Whitney U test with SciPy 1.10.0100 and adjusted for multiple\ncomparisons using the Bonferroni correction with statsmodels101.P r i o r\nto performing these tests, the Kolmogorov-Smirnov test was used to\nconﬁrm that the data groups did not conform to a normal distribution.\nMolecular property prediction. During the evaluation, we employ\ngreedy decoding for property prediction. Each method is run inde-\npendently three times, and the average and standard deviation are\nreported. We utilize the area under the receiver operating character-\nistic curve (ROC-AUC)\n102 metric to evaluate the classiﬁcation datasets.\nFor the regression datasets, root mean square error (RMSE) is used to\nquantify the average difference between predicted values and actual\nvalues, which is often applied in regression analysis.\nFor information regarding the model size of all baseline models in\nthe benchmark, please refer to Supplementary Table 12.\nDataset\nPretraining. The pretraining dataset is sourced from the geometric\nensemble of molecules (GEOM) dataset, which includes conformers\nfor 317,000 species, augmented with experimental data spanning\nbiophysics, physiology, and physical chemistry domains\n103.T h e s e\nconformers are generated using sophisticated sampling methods\ncoupled with semi-empirical density functional theory (DFT). Follow-\ning this, data curation procedures are implemented to exclude mole-\ncules containing heavy metals, lacking torsions, or test molecules.\nSubsequently, each molecule underwent pre-training with a maximum\nof 30 conformers, yielding aﬁnal dataset containing 8,452,080 entries.\nPocket-based molecular generation. The dataset utilized for pocket-\nbased generation is the same as existing work, which is an open-\navailable dataset consisted of over 20 million of pose pairs from nearly\n20,000 protein-ligand complexes from CrossDock2020\n104. Following\nthe protocol outlined in previous studies50,65, we discarded all poses\nwith an RMSD greater than 2 Å, and additionally partitioned the dataset\ninto training and testing sets based on a principle of sequence simi-\nlarity less than 40%, ensuring a fairer evaluation of the generalizability\nto unknown pockets. Additionally, we excluded protein-ligand pairs\nwhich ligand lacked torsion angles from the dataset, resulted in slightly\nsmaller training and testing sets compared to several models we\nmention subsequently.\nThe real-world targets’structure are download from RCSB PDB\n105,\nand reference molecules corresponding to each targets are collected\nfrom ChEMBL30 database106. Molecules with aKd or Ki value less than\n1,000 nM for a given target are considered active, counting into the\nreference sets. If the number of molecules meeting this criterion is low,\nmolecules with an IC50 value less than 1,000 nM are also included in\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 15\nthe reference sets. The collected molecules are deduplicated based on\nSMILES and molecules containing salts are removed.\nMolecular conformation generation. We performedﬁne-tuning using\ndatasets consistent with those utilized in earlier studies34,38,40,42.F o rt h e\ntest set, we employed a dataset akin to Tora3D26.T e s ts e tIc o n t a i n s\n200 molecules, each with fewer than 100 conformations. Test set II\ncomprises 1,000 randomly selected molecules with conformation\ncounts distributed similarly to the entire dataset, spanning from\n0 to 500.\nMolecular property prediction. We assembled a comprehensive col-\nlection of 12 datasets sourced from MolecularNet\n107 and therapeutics\ndata commons (TDC)108, accompanied by comprehensive datasets\ndescriptions provided in the Supplementary. Drawing upon Molecu-\nlarNet’s established status as a primary benchmark for molecular\nproperty prediction, our selection comprised six classiﬁcation data-\nsets and three regression datasets. Furthermore, within TDC, widely\nacclaimed as the premier public benchmark for ADMET analysis, we\nspeciﬁcally identiﬁed three datasets characterized by relatively\nhomogeneous data distributions. Each dataset underwent three ran-\ndom partitions following the 8:1:1 ratio for testing.\nData availability\nThe datasets utilized in our study are as follows: The GEOM dataset is\navailable at https://dataverse.harvard.edu/dataset.xhtml?\npersistentId=doi:10.7910/DVN/JNGTDF. For pocket-based molecular\ngeneration dataset is provided at https://zenodo.org/records/\n15194424. The molecular conformation generation, the dataset can\nbe accessed at https://github.com/zimeizhng/Tora3D. Lastly, the\ndatasets for property prediction are available athttps://moleculenet.\norg/datasets-1 and https://tdcommons.ai/single_pred_tasks/adme/\n. Source data are provided with this paper.\nCode availability\nThe code used in the study is publicly available from the GitHub\nrepository ( https://github.com/jkwang93/Token-Mol) or Zenodo\n(https://doi.org/10.5281/zenodo.1511068).\nReferences\n1. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In:\nProceedings of the 2019 conference of the North American chapter\nof the association for computational linguistics: human language\ntechnologies, volume 1 (long and short papers)(2019).\n2 . R a d f o r d ,A . ,N a r a s i m h a n ,K . ,S a l i m a n s ,T .&S u t s k e v e r ,I .I m p r o v i n g\nlanguage understanding by generative pre-training.https://\nopenai.com/index/language-unsupervised(2018).\n3. Chithrananda, S., Grand, G. & Ramsundar, B. ChemBERTa: large-\nscale self-supervised pretraining for molecular property predic-\ntion. arXiv preprint, arXiv:2010.09885 (2020).\n4. Rong, Y. et al. Self-supervised graph transformer on large-scale\nmolecular data. InProceedings of the 34th international con-\nference on neural information processing systems(2020).\n5 . K i m ,H . ,L e e ,J . ,A h n ,S .&L e e ,J .R .Am e r g e dm o l e c u l a rr e p r e -\nsentation learning for molecular properties prediction with a web-\nbased service.Sci. Rep.11, 11028 (2021).\n6. Liu, S. et al. Pre-training Molecular Graph Representation with 3D\nGeometry. InInternational Conference on Learning Representa-\ntions (2022).\n7. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular con-\ntrastive learning of representations via graph neural networks.\nNat. Mach. Intell.4,2 7 9– 287 (2022).\n8. Ross, J. et al. Large-scale chemical language representations\ncapture molecular structure and properties.Nat. Mach. Intell.4,\n1256– 1264 (2022).\n9. Zeng, X. et al. Accurate prediction of molecular properties and\ndrug targets using a self-supervised image representation learn-\ning framework.Nat. Mach. Intell.4,1 0 0 4– 1016 (2022).\n10. Li, H. et al. A knowledge-guided pre-training framework for\nimproving molecular representation learning.Nat. Commun.14,\n7568 (2023).\n11. Lin, X. et al. PanGu Drug Model: learn a molecule like a human.Sci.\nChina Life Sci.66,8 7 9– 882 (2023).\n12. Ying, C. et al. Do transformers really perform bad for graph\nrepresentation? InProceedings of the 35th International Con-\nference on Neural Information Processing Systems. (2024).\n1 3 . S h u a i ,R .W . ,R u f f o l o ,J .A .&G r a y ,J .J .I g L M :I nﬁlling language\nmodeling for antibody sequence design.Cell Syst.14,\n979– 989.e974 (2023).\n1 4 . N i j k a m p ,E . ,R u f f o l o ,J .A . ,W e i n s t e i n ,E .N . ,N a i k ,N .&M a d a n i ,A .\nProGen2: Exploring the boundaries of protein language models.\nCell Syst.14,9 6 8– 978.e963 (2023).\n1 5 . F e r r u z ,N . ,S c h m i d t ,S .&H ö c k e r ,B .P r o t G P T 2i sad e e pu n s u -\npervised language model for protein design.Nat. Commun.13,\n4348 (2022).\n16. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nProc. Natl Acad. Sci. USA118, e2016239118 (2021).\n17. Xia, J., Zhu, Y., Du, Y. & Li, S. Z. A systematic survey of chemical\npre-trained models. InProceedings of the Thirty-Second Interna-\ntional Joint Conference on Artiﬁcial Intelligence(2023).\n18. Weininger, D. SMILES, a chemical language and information sys-\ntem. 1. Introduction to methodology and encoding rules.J. Chem.\nInf. Computer Sci.28,3 1– 36 (1988).\n1 9 . K r e n n ,M . ,H ä s e ,F . ,N i g a m ,A . ,F r i e d e r i c h ,P .&A s p u r u - G u z i k ,A .\nSelf-referencing embedded strings (SELFIES): a 100% robust\nmolecular string representation.Mach. Learn.: Sci. Technol.1,\n045024 (2020).\n20. Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. SMILES-BERT:\nlarge scale unsupervised pre-training for molecular property\nprediction. InProceedings of the 10th ACM International Con-\nference on Bioinformatics, Computational Biology and Health\nInformatics.A C M( 2 0 1 9 ) .\n2 1 . B a g a l ,V . ,A g g a r w a l ,R . ,V i n o d ,P .K .&P r i y a k u m a r ,U .D .M o l G P T :\nmolecular generation using a transformer-decoder model.J.\nC h e m .I n f .M o d e l i n g62,2 0 6 4– 2076 (2022).\n22. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a pre-\ntrained transformer for computational chemistry.Mach. Learn.:\nSci. Technol.3, 015022 (2022).\n23. Christo ﬁdellis, D. et al. Unifying molecular and textual repre-\nsentations via multi-task language modelling. InInternational\nConference on Machine Learning.( P M L R ,2 0 2 3 ) .\n24. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. arXiv preprint, arXiv:2307.09288 (2023).\n25. Hu, W. et al. Strategies for pre-training graph neural networks. In\nInternational Conference on Learning Representations(2020).\n26. Rong, Y. et al. Self-supervised graph transformer on large-scale\nmolecular data. InAdvances in Neural Information Processing\nSystems (2020).\n27. Xia, J. et al. Mole-BERT: rethinking pre-training graph neural net-\nworks for molecules. InInternational Conference on Learning\nRepresentations(2023).\n28. Fang, X. et al. Geometry-enhanced molecular representation\nlearning for property prediction.Nat. Mach. Intell.4,\n127– 134 (2022).\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 16\n29. Zhou, G. et al. Uni-Mol: A universal 3d molecular representation\nlearning framework. InInternational Conference on Learning\nRepresentations(2023).\n30. Team, G. et al. Gemini: A family of highly capable multimodal\nmodels. arXiv preprint, arXiv:2312.11805 (2023).\n31. Team, G. et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. arXiv preprint,\narXiv:2403.05530 (2024).\n32. Research AI4Science, M. & Azure Quantum, M. The impact of large\nlanguage models on scientiﬁc discovery: a preliminary study\nusing GPT-4. arXiv preprint, arXiv:2311.07361 (2023).\n3 3 . B o r n ,J .&M a n i c a ,M .R e g r e s s i o nTransformer enables concurrent\nsequence regression and generation for molecular language\nmodelling.Nat. Mach. Intell.5,4 3 2– 444 (2023).\n34. Flam-Shepherd, D. & Aspuru-Guzik, A. Language models can\ngenerate molecules, materials, and protein binding sites directly\nin three dimensions as XYZ, CIF, and PDBﬁles. arXiv preprint,\narXiv:2305.05708 (2023).\n35. Zholus, A. et al. BindGPT: A scalable framework for 3D molecular\ndesign via language modeling and reinforcement learning. InPro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence(2025).\n36. Mansimov, E., Mahmood, O., Kang, S. & Cho, K. Molecular geo-\nmetry prediction using a deep generative graph neural network.\nSci. Rep.9, 20381 (2019).\n37. Simm, G. N. C. & Hernández-Lobato, J. M. A generative model for\nmolecular distance geometry. InProceedings of the 37th Interna-\ntional Conference on Machine Learning.( P M L R ,2 0 2 0 ) .\n38. Xu, M., Luo, S., Bengio, Y., Peng, J. & Tang, J. Learning neural\ngenerative dynamics for molecular conformation generation. In\nInternational Conference on Learning Representations(2021).\n39. Xu, M. et al. An end-to-end framework for molecular conformation\ngeneration via bilevel programming. InInternational conference\non machine learning.( P M L R ,2 0 2 1 )\n40. Xu, M. et al. GeoDiff: a geometric diffusion model for molecular\nconformation generation. InInternational Conference on Learning\nRepresentations(2022).\n41. Jing, B., Corso, G., Chang, J., Barzilay, R. & Jaakkola, T. Torsional\ndiffusion for molecular conformer generation. InAdvances in\nNeural Information Processing Systems(2022).\n42. Zhang, Z. et al. Tora3D: an autoregressive torsion angle prediction\nmodel for molecular 3D conformation generation.J. Cheminfor-\nmatics 15, 57 (2023).\n43. Ganea, O.-E. et al. GeoMol: torsional geometric generation of\nmolecular 3D conformer ensembles. InAdvances in Neural Infor-\nmation Processing Systems(2021).\n44. Zhang, H. et al. SDEGen: learning to evolve molecular conforma-\ntions from thermodynamic noise for conformation generation.\nChem. Sci.14,1 5 5 7– 1568 (2023).\n45. Chen, T. & Guestrin, C. XGBoost:A scalable tree boosting system.\nIn Proceedings of the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining.( A C M ,2 0 1 6 ) .\n46. Wu, Z. et al. Knowledge-based BERT: a method to extract mole-\ncular features like computational chemists.Brieﬁngs in Bioinfor-\nmatics 23 (2022).\n47. Heid, E. et al. Chemprop: A machine learning package for che-\nmical property prediction.J. Chem. Inf. Modeling\n64,9 – 17 (2024).\n4 8 . N o t w e l l ,J .H .&W o o d ,M .W .A D M E Tp r o p e r t yp r e d i c t i o nt h r o u g h\ncombinations of molecularﬁngerprints. arXiv preprint,\narXiv:2310.00174 (2023).\n4 9 . L i u ,M . ,L u o ,Y . ,U c h i n o ,K . ,M a r u h a s h i ,K .&J i ,S .G e n e r a t i n g3 D\nmolecules for target protein binding. InProceedings of the 39th\nInternational Conference on Machine Learning. (PMLR, 2022).\n50. Peng, X. et al. Pocket2mol: efﬁcient molecular sampling based on\n3 d protein pockets. InInternational Conference on Machine\nLearning.( P M L R ,2 0 2 2 ) .\n51. Guan, J. et al. 3D equivariant diffusion for target-aware molecule\ngeneration and afﬁnity prediction. InThe Eleventh International\nConference on Learning Representations(2023).\n52. Zheng, K. et al. Structure-based drug design benchmark: Do 3d\nmethods really dominate? InICML 2024 AI for Science Work-\nshop (2024).\n53. Liu, H. et al. How good are current pocket-based 3D generative\nmodels?: The benchmark set and evaluation of protein pocket-\nbased 3d molecular generative models.Journal of Chemical\nInformation and Modeling(2024).\n54. Wu, K. et al. TamGen: drug design with target-aware molecule\ngeneration through a chemical language model.Nat. Commun.\n15,9 3 6 0( 2 0 2 4 ) .\n55. Chan, L., Kumar, R., Verdonk, M. & Poelking, C. A multilevel gen-\nerative framework with hierarchical self-contrasting for bias con-\ntrol and transparency in structure-based ligand design.Nat. Mach.\nIntell. 4, 1130– 1142 (2022).\n56. Renz, P., Van Rompaey, D., Wegner, J. K., Hochreiter, S. &\nKlambauer, G. On failure modes in molecule generation\nand optimization.Drug Discov. Today.: Technol.32-33,\n55– 63 (2019).\n57. Ertl, P. & Schuffenhauer, A. Estimation of synthetic accessibility\nscore of drug-like molecules based on molecular complexity and\nfragment contributions.J. Cheminformatics1, 8 (2009).\n58. Veber, D. F. et al. Molecular Properties That inﬂuence the oral\nbioavailability of drug candidates.J. Medicinal Chem.45,\n2615– 2623 (2002).\n59. Hitchcock, S. A. & Pennington, L. D. Structure−brain exposure\nrelationships.J. Medicinal Chem.49, 7559– 7583 (2006).\n60. Riniker, S. & Landrum, G. A. Better informed distance geometry:\nUsing what we know to improve conformation generation.J.\nC h e m .I n f .M o d e l i n g55,2 5 6 2– 2574 (2015).\n6 1 . L i u ,S .e ta l .D i s c o v e r yo fN o v e lB enzo[4,5]imidazo[1,2-a]pyrazin-1-\namine-3-amide-one derivatives as anticancer human A2A adeno-\nsine receptor antagonists.J. Medicinal Chem.\n65,\n8933– 8947 (2022).\n62. Clyde, A. et al. High-throughput virtual screening and validation of\na SARS-CoV-2 main protease noncovalent inhibitor.J. Chem. Inf.\nModeling62, 116– 128 (2022).\n63. Sun, C. et al. Novel small-molecule PD-L1 Inhibitor Induces PD-L1\ninternalization and optimizes the immune microenvironment.J.\nMedicinal Chem.66,2 0 6 4– 2083 (2023).\n64. Zhang, O. et al. Learning on topological surface and geometric\nstructure for 3D molecular generation.Nat. Computational Sci.3,\n849– 859 (2023).\n65. Zhang, O. et al. ResGen is a pocket-aware 3D molecular genera-\ntion model based on parallel multiscale modelling.Nat. Mach.\nIntell. 5,1 0 2 0– 1030 (2023).\n66. Zhavoronkov, A. et al. Deep learning enables rapid identiﬁcation of\npotent DDR1 kinase inhibitors.Nat. Biotechnol.37,1 0 3 8– 1040\n(2019).\n67. Li, S. et al. LS-MolGen: Ligand-and-structure dual-driven deep\nreinforcement learning for target-speciﬁc molecular generation\nimproves binding afﬁnity and novelty.J. Chem. Inf. Modeling63,\n4207– 4215 (2023).\n68. Murray, C. W. et al. Fragment-based discovery of potent and\nselective DDR1/2 inhibitors.ACS Medicinal Chem. Lett.6,\n798– 803 (2015).\n69. Qiao, J. et al. SARS-CoV-2 Mpro inhibitors with antiviral activity in a\ntransgenic mouse model.Science 371,1 3 7 4– 1378 (2021).\n70. Kneller, D. W. et al. Malleability of the SARS-CoV-2 3CL Mpro\nactive-site cavity facilitates binding of clinical antivirals.Structure\n28,1 3 1 3– 1320.e1313 (2020).\n71. Williams, N. K. et al. Dissecting Speciﬁcity in the janus kinases: The\nStructures of JAK-speciﬁc inhibitors complexed to the JAK1 and\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 17\nJAK2 protein tyrosine kinase domains.J. Mol. Biol.387,\n219– 232 (2009).\n72. Thompson, J. E. et al. Photochemical preparation of a pyridone\ncontaining tetracycle: A jak protein kinase inhibitor.Bioorg. Med-\nicinal Chem. Lett.12,1 2 1 9– 1223 (2002).\n7 3 . B e m i s ,G .W .&M u r c k o ,M .A .T h ep r o p e r t i e so fk n o w nd r u g s .1 .\nMolecular frameworks.J. Medicinal Chem.39, 2887– 2893 (1996).\n7 4 . P r e u e r ,K . ,R e n z ,P . ,U n t e r t h i n e r ,T . ,H o c h r e i t e r ,S .&K l a m b a u e r ,G .\nFréchet chemnet distance: A metric for generative models for\nmolecules in drug discovery.J. Chem. Inf. Modeling58,\n1736– 1741 (2018).\n75. Baraldi, S. et al. InThe Adenosine Receptors.( e d s .P .A .B o r e a ,K .\nVarani, S. Gessi, S. Merighi & F. Vincenzi) 91-136 (Springer Inter-\nnational Publishing, Cham; 2018).\n76. Yu, F., Zhu, C., Xie, Q. & Wang, Y. Adenosine A2A receptor\nantagonists for cancer immunotherapy.J. Medicinal Chem.63,\n12196– 12212 (2020).\n77. Spinaci, A. et al. Inpurinergic receptors and their modulators.( e d s .\nV. Colotta & C. T. Supuran) 101-141 (Springer International Pub-\nlishing, Cham; 2023).\n78. Jaakola, V.-P. et al. The 2.6 angstrom crystal structure of a human\nA2A adenosine receptor bound to an antagonist.Science 322,\n1211– 1217 (2008).\n7 9 . O l i v e c r o n a ,M . ,B l a s c h k e ,T . ,E n g k v i s t ,O .&C h e n ,H .M o l e c u l a rd e -\nnovo design through deep reinforcement learning.J. Cheminfor-\nmatics 9,4 8( 2 0 1 7 ) .\n80. Friesner, R. A. et al. Glide: Anew approach for rapid, accurate\ndocking and scoring. 1. Method and assessment of docking\naccuracy.J. Medicinal Chem.47,1 7 3 9– 1749 (2004).\n8 1 . S p i t z e r ,R .&J a i n ,A .N .S u rﬂex-dock: Docking benchmarks and\nreal-world application.J. Computer-Aided Mol. Des.26,\n687– 699 (2012).\n82. Jacobs, R. A., Jordan, M. I., Nowlan, S. J. & Hinton, G. E. Adaptive\nmixtures of local experts.Neural Comput.3,7 9– 87 (1991).\n8 3 . L e w i s ,P .e ta l .R e t r i e v a l - a u gmented generation for knowledge-\nintensive NLP tasks. InProceedings of the 34th International Con-\nference on Neural Information Processing Systems.( P M L R ,2 0 2 0 ) .\n84. Williams, R. J. Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning.Mach. Learn.8,\n229– 256 (1992).\n85. Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning\nfor de novo drug design.Sci. Adv.4, eaap7885 (2018).\n86. Wang, J. et al. Multi-constraint molecular generation based on\nconditional transformer, knowledgedistillation and reinforcement\nlearning.Nat. Mach. Intell.3,9 1 4– 922 (2021).\n87. Wang, J. et al. Molecular generation with reduced labeling\nthrough constraint architecture.J. Chem. Inf. Modeling63,\n3319– 3327 (2023).\n88. Aghajanyan, A. et al. CM3: A Causal masked multimodal model of\nthe internet. arXiv preprint, arXiv:2201.07520 (2022).\n89. Fried, D. et al. InCoder: A generative model for code inﬁlling and\nsynthesis. InThe Eleventh International Conferenceon Learning\nRepresentations(2023).\n90. Polykovskiy, D. et al. Molecular Sets (MOSES): A benchmarking\nplatform for molecular generation models.Frontiers in Pharma-\ncology 11 (2020).\n91. Rogers, D. & Hahn, M. Extended-connectivityﬁngerprints.J.\nC h e m .I n f .M o d e l i n g50,7 4 2– 754 (2010).\n92. Bajusz, D., Rácz, A. & Héberger, K. Why is Tanimoto index an\nappropriate choice forﬁngerprint-based similarity calculations?J.\nCheminformatics7,2 0( 2 0 1 5 ) .\n9 3 . X i e ,Y . ,X u ,Z . ,M a ,J .&M e i ,Q .H o wm u c hs p a c eh a sb e e n\nexplored? measuring the chemical space covered by databases\nand machine-generated molecules. InThe Eleventh International\nConference on Learning Representations(2023).\n94. Alhossary, A., Handoko, S. D., Mu, Y. & Kwoh, C.-K. Fast, accurate,\nand reliable molecular docking with QuickVina 2.Bioinformatics\n31, 2214– 2216 (2015).\n95. Shultz, M. D. Two decades under the inﬂuence of the rule ofﬁve\nand the changing properties of approved oral drugs.J. Medicinal\nChem. 62,1 7 0 1– 1714 (2019).\n96. Ertl, P., Rohde, B. & Selzer, P. Fast calculation of molecular polar\nsurface area as a sum of fragment-based contributions and its\napplication to the prediction of drug transport properties.J.\nMedicinal Chem.43,3 7 1 4– 3717 (2000).\n97. Ghose, A. K., Viswanadhan, V. N. & Wendoloski, J. J. A knowledge-\nbased approach in designing combinatorial or medicinal chem-\nistry libraries for drug discovery. 1. A qualitative and quantitative\ncharacterization of known drug databases.J. Combinatorial Chem.\n1,5 5– 68 (1999).\n98. Lipinski, C. A., Lombardo, F., Dominy, B. W. & Feeney, P. J.\nExperimental and computational approaches to estimate solubi-\nl i t ya n dp e r m e a b i l i t yi nd r u gd i s c o v e r ya n dd e v e l o p m e n ts e t t i n g s .\nAdv. Drug Deliv. Rev.23\n,3 – 25 (1997).\n99. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S. & Hopkins,\nA. L. Quantifying the chemical beauty of drugs.Nat. Chem.4,\n90– 98 (2012).\n100. Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientiﬁc\ncomputing in Python.Nat. Methods17,2 6 1– 272 (2020).\n101. Seabold, S. & Perktold, J. Statsmodels: econometric and statistical\nmodeling with python.SciPy 7,9 2– 96 (2010).\n1 0 2 . B r a d l e y ,A .P .T h eu s eo ft h ea r e au n d e rt h eR O Cc u r v ei nt h e\nevaluation of machine learning algorithms.Pattern Recognit.30,\n1145– 1159 (1997).\n103. Axelrod, S. & Gómez-Bombarelli, R. GEOM, energy-annotated\nmolecular conformations for property prediction and molecular\ngeneration.Sci. Data9, 185 (2022).\n104. Francoeur, P. G. et al. Three-dimensional convolutional neural\nnetworks and a cross-docked data set for structure-based drug\ndesign. J. Chem. Inf. Modeling60,4 2 0 0– 4215 (2020).\n105. Burley, S. K. et al. RCSB protein data bank (RCSB.org): delivery of\nexperimentally-determined PDBstructures alongside one million\ncomputed structure models of proteins from artiﬁcial intelli-\ngence/machine learning.Nucleic Acids Res.51,\nD488– D508 (2022).\n106. Zdrazil, B. et al. The ChEMBL Database in 2023: a drug discovery\nplatform spanning multiple bioactivity data types and time peri-\nods. Nucleic Acids Res.52,D 1 1 8 0– D1192 (2023).\n107. Wu, Z. et al. MoleculeNet: a benchmark for molecular machine\nlearning.Chem. Sci.9,5 1 3– 530 (2018).\n108. Huang, K. et al. Therapeutics data commons: Machine learning\ndatasets and tasks for drug discovery and development. InThirty-\nﬁfth Conference on Neural Information Processing Systems Data-\nsets and Benchmarks Track (Round 1)(2021).\n109. Liu, Q., Allamanis, M., Brockschmidt, M. & Gaunt, A. Constrained\ngraph variational autoencoders for molecule design. InAdvances\nin neural information processing systems(2018).\n110. Shi, C., Luo, S., Xu, M. & Tang, J. Learning gradientﬁelds for\nmolecular conformation generation. InInternational conference\non machine learning.( P M L R ,2 0 2 1 ) .\nAcknowledgements\nThis work wasﬁnancially supported by National Key Research and\nDevelopment Program of China (2022YFF1203003 to Y.K.), National\nNatural Science Foundation of China (22220102001 to T.H, 22303083 to\nJ.W., 81973281 and 82373791 to Y.K.), China Postdoctoral Science\nFoundation (2023M733128 and 2023TQ0285 to J.W.), Postdoctoral Fel-\nlowship Program of CPSF (GZB20230657 to J.W.), and Scientiﬁc\nResearch Fund of Zhejiang Provincial Education Department\n(Y202457041 to R.Q.). We appreciate Dr Liwei Liu and Dr Xiaozhe Wan, at\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 18\nthe Advanced Computing and Storage Laboratory, Central Research\nInstitute, Huawei Technologies Co., Ltd., for their invaluable discussions\nand collaborative spirit throughout this research.\nAuthor contributions\nY.K., C.-Y.H., and T.H. designed the research study. J.W. developed the\nmethod and wrote the code. J.W., R.Q., M.W., M.F., D.J., X.Z., H.Z., J.G.\nc o l l e c t e dt h ed a t a ,J . W . ,R . Q . ,M . W . ,M . F . ,Y a n g y a n gZ h a n g ,G . L . ,Q . S . ,\nQ.G. performed the analysis. J.W., R.Q., M.W., M.F., Yangyang Zhang,\nZhourui Wu, Y.K., C.-Y.H., and T.H. wrote the original draft. J.W., R.Q.,\nM.F., Yuchen Zhu, C.S., O.Z., Zhenxing Wu, T.H., C.-Y.H., and Y.K. wrote\nthe review and editing, all authors read and approved the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-59628-y.\nCorrespondenceand requests for materials should be addressed to\nYu Kang, Chang-Yu Hsieh or Tingjun Hou.\nPeer review informationNature Communicationsthanks Jannis Born,\nMarcos Quiles and Morgan Thomas for their contribution to the peer\nreview of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-59628-y\nNature Communications|         (2025) 16:4416 19",
  "topic": "Drug",
  "concepts": [
    {
      "name": "Drug",
      "score": 0.5490444898605347
    },
    {
      "name": "Computer science",
      "score": 0.49942851066589355
    },
    {
      "name": "Medicine",
      "score": 0.38922229409217834
    },
    {
      "name": "Pharmacology",
      "score": 0.2426396608352661
    }
  ]
}