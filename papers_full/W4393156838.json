{
  "title": "LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs",
  "url": "https://openalex.org/W4393156838",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A3041532456",
      "name": "Zhixuan Chu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2098812053",
      "name": "Xin Ouyang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2131854039",
      "name": "Simeng Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2102430400",
      "name": "Hongyan Hao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2107015671",
      "name": "Yue Shen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2224966603",
      "name": "Jinjie Gu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A3199901665",
      "name": "Siqiao Xue",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2096145939",
      "name": "James Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2096205332",
      "name": "Qing Cui",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2101837192",
      "name": "Longfei Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2095676552",
      "name": "Jun Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A1978672188",
      "name": "Sheng Li",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A3041532456",
      "name": "Zhixuan Chu",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2098812053",
      "name": "Xin Ouyang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2131854039",
      "name": "Simeng Wang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2102430400",
      "name": "Hongyan Hao",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2107015671",
      "name": "Yue Shen",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2224966603",
      "name": "Jinjie Gu",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A3199901665",
      "name": "Siqiao Xue",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2096145939",
      "name": "James Zhang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2096205332",
      "name": "Qing Cui",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2101837192",
      "name": "Longfei Li",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2095676552",
      "name": "Jun Zhou",
      "affiliations": [
        "University of Virginia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3113595157",
    "https://openalex.org/W6603787529",
    "https://openalex.org/W3171442082",
    "https://openalex.org/W6917172014",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W3206127589",
    "https://openalex.org/W3168953488",
    "https://openalex.org/W2937556626",
    "https://openalex.org/W2953243284",
    "https://openalex.org/W6785424798",
    "https://openalex.org/W6785869379",
    "https://openalex.org/W6790680487",
    "https://openalex.org/W4384655952",
    "https://openalex.org/W3101707147",
    "https://openalex.org/W4388997968",
    "https://openalex.org/W3133849783",
    "https://openalex.org/W2966483207",
    "https://openalex.org/W3100324210",
    "https://openalex.org/W4386081833",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W4389713747",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4386081835",
    "https://openalex.org/W4387559593",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4388032291",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4390962814",
    "https://openalex.org/W2963911286",
    "https://openalex.org/W4381252438"
  ],
  "abstract": "Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.",
  "full_text": "LLMRG: Improving Recommendations through Large Language Model\nReasoning Graphs\nYan Wang1*, Zhixuan Chu1*‚Ä†, Xin Ouyang1, Simeng Wang1, Hongyan Hao1, Yue Shen1, Jinjie\nGu1, Siqiao Xue1, James Zhang1, Qing Cui1, Longfei Li1, Jun Zhou1, Sheng Li2\n1Ant Group\n2University of Virginia\n{luli.wy,chuzhixuan.czx,xin.oyx,simeng.wsm,hongyanhao.hhy,zhanying,jinjie.gujj,siqiao.xsq,james.z,cuiqing.cq,longyao.llf,\njun.zhoujun}@antgroup.com, shengli@virginia.edu\nAbstract\nRecommendation systems aim to provide users with relevant\nsuggestions, but often lack interpretability and fail to capture\nhigher-level semantic relationships between user behaviors\nand profiles. In this paper, we propose a novel approach that\nleverages large language models (LLMs) to construct per-\nsonalized reasoning graphs. These graphs link a user‚Äôs pro-\nfile and behavioral sequences through causal and logical in-\nferences, representing the user‚Äôs interests in an interpretable\nway. Our approach, LLM reasoning graphs (LLMRG), has\nfour components: chained graph reasoning, divergent exten-\nsion, self-verification and scoring, and knowledge base self-\nimprovement. The resulting reasoning graph is encoded using\ngraph neural networks, which serves as additional input to im-\nprove conventional recommender systems, without requiring\nextra user or item information. Our approach demonstrates\nhow LLMs can enable more logical and interpretable rec-\nommender systems through personalized reasoning graphs.\nLLMRG allows recommendations to benefit from both en-\ngineered recommendation systems and LLM-derived reason-\ning graphs. We demonstrate the effectiveness of LLMRG on\nbenchmarks and real-world scenarios in enhancing base rec-\nommendation models.\nIntroduction\nRecommendation systems are now prevalent across the in-\nternet, smartly surfacing personalized content and products\nto users based on their individual profiles and historical\nbehavioral data. However, most recommendation systems\nrely solely on conventional machine learning techniques,\nwhich can only identify patterns and relationships within\nsequences of interactions without actually comprehending\nthe true meaning or semantics behind the items themselves.\nDevoid of any logical or causal reasoning capacities (Li\nand Chu 2023; Chu, Rathbun, and Li 2021), these recom-\nmender systems struggle to effectively capture the full spec-\ntrum of conceptual relationships and connections spanning\na user‚Äôs diverse interests and behavioral patterns over time.\nIn addition, recent work (Wang et al. 2019; Chen et al.\n2021; Wu et al. 2019; Wang et al. 2020; Sheu et al. 2021)\n*These authors contributed equally.\n‚Ä†Corresponding author.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhas sought to enhance recommendations by incorporating\ngraph-structured information, which provides valuable con-\ntextual data beyond standard tabular data formats. However,\neven these more advanced knowledge graphs used in recom-\nmendation systems still lack the ability to perform complex\nreasoning or inference - simply overlaying factual relation-\nships is not enough to enable a system to deeply understand\nusers‚Äô interests and generate insightful recommendations.\nIn parallel, tremendous progress in large language mod-\nels (LLMs) has demonstrated powerful new capacities for\nreasoning, inference, and logic without the need for explicit\ntraining on such tasks. These models exhibit remarkable ap-\ntitudes for causal, logical, and analogical reasoning, illumi-\nnating new opportunities to leverage their strengths to de-\nvelop superior knowledge representations that can capture\nnuanced semantic relationships between users‚Äô interests. By\nleveraging LLMs to reason behavioral sequences and com-\nprehend user interests at a deeper conceptual level, there is\nimmense potential to revolutionize next-generation recom-\nmendation systems (Chu et al. 2023b; Wang et al. 2023).\nTherefore, we propose using an LLM to construct person-\nalized reasoning graphs for recommendation systems. The\nLLM inputs a user‚Äôs profile and behavioral sequences and\noutputs a graphical representation linking concepts through\nchained causal and logical reasoning. This results in an\nexpansive graph embedding higher-level semantic relation-\nships between the user‚Äôs interests and behaviors. We then\napply graph neural networks to learn a dense feature repre-\nsentation that summarizes the graph‚Äôs structure and mean-\ning. This graph embedding is provided as additional input\nto a conventional recommendation model. Our approach al-\nlows recommendations to consider conceptual relationships\nderived through reasoning while still benefiting from the rec-\nommendation abilities of traditional models. Moreover, the\ngraph provides interpretability by surfacing the explicit rea-\nsoning behind recommendations.\nWe designed four interlocking modules, powered by large\nlanguage models (LLMs), to construct personalized reason-\ning graphs that model each user‚Äôs interests: 1) a chained\ngraph reasoning module that conducts chained causal and\nlogical reasoning, 2) a divergent extension module that ex-\npands the graph by associating and reasoning about the\nuser‚Äôs interests, 3) a self-verification and scoring module that\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19189\nvalidates the reasoning procedure through abductive reason-\ning and scoring, and 4) a knowledge base self-improving\nmodule that caches validated reasoning chains for later\nreuse. Together, these four modules construct the Large Lan-\nguage Model Reasoning Graphs (LLMRG) paradigm, which\nemploys a prompt-based framework leveraging large lan-\nguage models to imaginatively generate plausible new rea-\nsoning chains, given their behavioral history and features.\nBesides, it can perform imaginary continuations of each rea-\nsoning chain to predict the next items the user is likely to\nengage with. This divergent thinking allows us to go beyond\nreactive recommendations based on consumed content to\nproactively recommend new items tailored to modeling the\nuser‚Äôs motivations. Experiments demonstrate our model‚Äôs\nability to improve recommendation performance without re-\nquiring additional user or item data. This work illustrates\nhow large language models can enable logical and inter-\npretable recommender systems.\nBackground\nGraph-based Recommendation System\nRecent work explores graph-based methods that can incor-\nporate additional relationship information into recommen-\ndation systems (Sheu et al. 2021; Chu et al. 2024). For ex-\nample, knowledge graphs have emerged as a powerful way\nto represent relationships between entities to capture com-\nplex entity interactions (Wang et al. 2019; Chen et al. 2021).\nBeyond predefined knowledge graphs, some methods (Wang\net al. 2019) learn to construct an informative graph from\nuser-item interactions. While knowledge graphs provide ex-\nternal information, graph learning methods (Wu et al. 2019)\ncan extract latent structures. Combining the two concepts,\n(Wang et al. 2020) jointly leverage a knowledge graph and\ninteraction graph. In summary, graph-based methods allow\nrecommendation models to encode richer connectivity pat-\nterns. However, there are some potential disadvantages of\ngraph-based recommendation systems compared to reason-\ning graph construction by large language models (LLMs):\n(1) Knowledge graphs require extensive human expertise to\nbuild and maintain relationships, whereas LLMs can auto-\nmatically extract relational knowledge from large text cor-\npora; (2) Predefined knowledge graphs may have coverage\ngaps for certain entities or domains. LLMs can learn to rea-\nson about any entity mentioned in the text; (3) Graph learn-\ning methods that construct graphs from interactions are lim-\nited to observable user-item connections. LLMs can infer\nmore abstract and latent relationships through reasoning; (4)\nKnowledge graphs and graphs are static after construction.\nLLMs can continue to expand their knowledge and reason-\ning capabilities as they are trained on more data.\nReasoning of LLM\nRecent advances in large language models (LLMs) like\nGPT-3 and PaLM have enabled strong capabilities in log-\nical and causal reasoning (Chu et al. 2023a; Guan et al.\n2023; Xue et al. 2023). This progress stems from three\nkey strengths. First, natural language understanding allows\nLLMs to parse meaning and relationships from text (Devlin\net al. 2018; Brown et al. 2020). Models can identify enti-\nties, actions, and causal chains through techniques like self-\nattention and contextual embeddings. Second, LLMs have\naccumulated vast commonsense knowledge about how the\nworld works (Shin et al. 2021; Chowdhery et al. 2022).\nGPT-3 was trained on over a trillion words from the inter-\nnet, absorbing implicit knowledge about physics, psychol-\nogy, and reasoning. Models like PaLM were further trained\nwith constrained tuning to better incorporate common sense.\nThis enables filling in missing premises and making deduc-\ntions. Third, transformer architectures impart combinatorial\ngeneralization and symbolic reasoning abilities (Wei et al.\n2022). Self-attention layers allow LLMs to chain ideas, fol-\nlow arguments step-by-step, and make coherent deductions.\nTogether, these strengths of understanding language, lever-\naging knowledge, and combinatorial reasoning empower\nLLMs to parse scenarios, tap relevant knowledge, and rea-\nson through implications and causes.\nLLMRG\nProblem Statement\nLet U={u1, u2, . . . , u|U| } denote the set of users,\nV={v1, v2, . . . , v|V|} be the set of items, and list\nSu=[v(u)\n1 , . . . , v(u)\nt , . . . , v(u)\nnu ] denote the sequence of\ninteractions for user u ‚àà U in chronological order, where\nv(u)\nt ‚àà V is the item interacted with at time step t and\nnu is the length of the sequence. We use relative time\nindices instead of absolute timestamps. In addition, let\nAu=[a(u)\n1 , . . . , a(u)\ni , . . . , a(u)\nna ] represent user attributes\nfor modeling personalization, where na is the number\nof attributes. Given a user‚Äôs interaction history Su, the\nsequential recommendation task is to predict the item user\nu will interact with at the next time step nu + 1.\nIn this work, we propose constructing Large Language\nModel Reasoning Graphs (LLMRG), a new paradigm that\nutilizes LLMs to improve recommendation system perfor-\nmance. We first use a large language model (LLM) to\nconstruct personalized reasoning graphs based on Su and\nAu, which reason a user‚Äôs profile and behavioral sequences\nthrough causal and logical inferences. The graph provides an\ninterpretable model of a user‚Äôs interests and embeds rich se-\nmantic relationships. We propose an adaptive reasoning ar-\nchitecture with self-verification based on the capabilities of\nLLMs, which includes four components: 1) chained graph\nreasoning, 2) divergent extension, 3) self-verification and\nscoring, and 4)a self-improved knowledge base. By encod-\ning the resulting conceptual reasoning graph using graph\nneural networks, it can be provided as an additional input\ninto conventional recommender systems. This allows recom-\nmendations to benefit from both engineered recommenda-\ntion algorithms and the explanatory knowledge derived from\nthe LLM graph reasoning process.\nAdaptive Reasoning Architecture\nChained Graph Reasoning. Along with the user behav-\nioral sequences Su, for each item, we construct reasoning\nchains RCn that link it to existing chains if there are logical\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19190\n& \nLLM Knowledge Store\nLLM\nLLM\n&\nReasoning Graph\nKnowledge Base Self-improving\nDivergent\nChain\nQuery\nUpdate\nInteractions Sequence\n Base SeqRes Model\nAdaptive Reasoning\n SR-GNN\nMLP\nChained Graph Reasoning\nFramework\nDivergent ExtensionSelf-veriÔ¨Åcation and Scoring\nDivergent Graph\nLegend\nForword\nLLM\nReasoning\nLLM Abductive\nReasoning¬†\nLLM¬†\nDivergent\nAdaptive Reasoning Architecture\nFusion\nMatch Not Match\nSelf\nImproving\nReasoning\nInteractions Sequence User attributes\nInput\nReasoning Chain\n? \n ? \nRandom Masking\n? \n ? \n: [Mask]¬†in Item\n? \n ? : [Mask] in User attributes\n? \n ? \n? \n ? \nAbductive Reasoning\nScoring\nFigure 1: LLMRG framework has two main components, i.e., an adaptive reasoning module with self-verification and a base\nsequential recommendation model. Our model concatenates the embeddings from the adaptive reasoning module (E ori and\nEdiv) and the base model (Ebase) to obtain Efusion . This fused embedding is used to predict the next item for the user.\nconnections or start entirely new chains rooted in the item\nitself if there are no applicable links to existing reasoning\nchains. Relevant user attributes Au are incorporated where\npossible to further customize the reasoning chains for rec-\nommending items. This iterative reasoning chain construc-\ntion process is carried out progressively along the user‚Äôs be-\nhavioral sequence up until the last item. Specifically, we em-\nploy a prompt-based framework leveraging large language\nmodels to imaginatively generate plausible new reasoning\nchains that could logically motivate the user to engage with\nthe next known item in their sequences. The prompt takes\nas input the known next item, existing reasoning chains con-\nstructed thus far, and available user attributes. It outputs a\ncomprehensive set of possible new reasoning chains explain-\ning why the user might want to take the next item. These\ndynamically generated new chains are integrated into the\nevolving logical reasoning graph to enable the modeling of\nincreasingly complex interdependent motivations and inter-\nests underlying the user‚Äôs evolving behavioral trajectory.\nDivergent Extension. Besides the observed behavioral se-\nquences, we aim to conduct divergent thinking according to\nthe established reasoning graph. We propose a new diver-\ngent extension module that performs imaginary continua-\ntions of each reasoning chain to predict the next items the\nuser is likely to engage with. Specifically, for each reason-\ning chain digging into the user‚Äôs motivations and thinking\nprocess, the divergent extension module employs an imag-\nination engine to divergently extend the chain beyond the\nlast known item. This involves using the language model to\nsample plausible continuations of the reasoning trajectory\nthat predict what other related items the user might be in-\nterested in next. For example, if the chain represents an in-\nterest in sci-fi movies with complex philosophies, the ex-\ntension could generate new sequences predicting more cere-\nbral sci-fi films with similar themes and tones that the user\nmight enjoy. Critically, the imagination engine outputs mul-\ntiple diverse possible extending items per reasoning chain,\ncapturing the user‚Äôs multifaceted interests. These imaginary\nnew items represent predictions of movies the user is likely\nto watch soon. We aggregate the predicted new items from\nall the extended reasoning chains to form a comprehensive\nset of personalized recommendations tailored to the user‚Äôs\npreferences. It is worth noting that the generated new item\nrecommendations may not exist in the original item list for\nour recommendation task. Therefore, we need to use another\nsmall language model to calculate the similarity between the\ngenerated items and the original list in order to retrieve the\nmost relevant item recommendations. Divergent thinking al-\nlows us to go beyond reactive recommendations based on\nconsumed content to proactively recommend new items tai-\nlored to modeling the user‚Äôs motivations. In this procedure, a\nprompt-based framework based on LLM is still employed. It\nis worth noting that rather than just predicting the single next\nmovie, our divergent extension module enables the general-\nization of multiple future trajectories per reasoning chain.\nThis allows for properly capturing the user‚Äôs diverse inter-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19191\nests and possibilities they may take next.\nSelf-verification and Scoring. The self-verification mod-\nule utilizes the abductive reasoning capability (Xu et al.\n2023) of LLM to check the plausibility and coherence of the\ndynamically generated reasoning chains from the chained\ngraph reasoning and divergent extension modules. Before\nadding a new reasoning chain to the graph, the module\nmasks the key items or engaged user attributes that the chain\nis meant to logically link to. It then prompts the large lan-\nguage model to fill in the [Mask] in the masked chains\nMC (u)\nn with the most reasonable prediction. If the predicted\nitem or attribute matches what was originally masked, this\nprovides evidence that the reasoning chain logically flows\nand is consistent with the user‚Äôs behavioral history and at-\ntributes. The higher the match score, the more robust the rea-\nsoning graph is as a whole. On the other hand, a low match\nscore indicates potential flaws in the coherence or plausi-\nbility of some reasoning chains. The system can then selec-\ntively filter out or recalibrate the problematic chains before\nintegrating them into the graph. Therefore, we set a thresh-\nold score for this self-verification to judge the rationality\nof reasoning. This improves the overall soundness of the\ndynamically constructed reasoning chains for the chained\ngraph reasoning and divergent extension modules, ensur-\ning reliable reasoning for recommendations aligned with the\nuser‚Äôs interests. Specifically, this module mainly involves\nthree steps, i.e., random masking, abductive reasoning, and\nscoring, which are exemplified in Figure 1.\nKnowledge Base Self-improving. In our system‚Äôs\nchained graph reasoning, divergent extension, and self-\nverification modules, we make extensive use of a language\nmodel to conduct inference and reasoning. This repeated\nlanguage model invocation incurs significant computa-\ntional costs. However, we observed that many knowledge\nelements and reasoning procedures are applied repeatedly\nacross queries. To avoid redundant work, we introduce a\nknowledge base that caches validated reasoning chains for\nlater reuse. By reusing previous reasoning results rather\nthan re-computing them, we substantially reduce language\nmodel usage. We employ a self-improving approach to\nmaintain knowledge base quality over time. Using the\nscores from our self-verification and scoring module, which\nassess reasoning chain validity, we retain only high-quality\nchains in the knowledge base. Low-scoring chains are\ndiscarded to filter out low-quality or erroneous inferences.\nBefore conducting new reasoning, we first check whether\nthe knowledge base already contains a relevant chain. If so,\nwe retrieve and leverage that pre-computed chain instead\nof invoking the language model. This knowledge base of\ncached, high-quality reasoning chains significantly reduces\ncomputational requirements. Our experiments demonstrate\nit can cut language model usage by about 30% compared to\ninferences from scratch after 3000 times of reasoning and\nverification steps in Figure 4.\nLLMRG Framework\nSequential recommendation approaches typically view the\nuser‚Äôs history of interactions as an ordered sequence and at-\ntempt to model the user‚Äôs dynamically evolving interests. In\nthis work, we propose to use an LLM to construct personal-\nized reasoning graphs for recommendation systems. There-\nfore, as shown in Figure 1, our proposed LLMRG has two\ncomponents, i.e., an adaptive reasoning module with self-\nverification and a base sequential recommendation model.\nThe adaptive reasoning module takes the user‚Äôs interac-\ntion sequence Su=[v(u)\n1 , . . . , v(u)\nt , . . . , v(u)\nnu ] and attributes\nAu=[a(u)\n1 , . . . , a(u)\ni , . . . , a(u)\nna ] as input. This input goes\nthrough chained graph reasoning, self-verification and scor-\ning, and divergent extension repeatedly to construct a rea-\nsoning graph and a divergent graph. The adaptive reason-\ning module is expressed as a mapping œï : {Su, Au} ‚Üí\n{Grea, Gdiv}, where Grea and Gdiv represent the reasoning\ngraph and divergent graph, respectively. We utilize SR-GNN\n(Wu et al. 2019) to automatically extract embeddings from\nthe graphs, considering the rich node connections. This pro-\ncess produces two embeddings Eori for the reasoning graph\nand Ediv for the divergent graph by g1 : Grea ‚Üí Eori and\ng2 : Gdiv ‚Üí Ediv. In parallel, the base sequential recom-\nmendation model directly processes the input to produce\nan embedding Ebase. Finally, we concatenate the embed-\ndings from the adaptive reasoning module (E ori and Ediv)\nand the base model (E base) to obtain Efusion . This fused\nembedding is used to predict the next item for the user by\nœà : Efusion ‚Üí v(u)\nnu+1.\nThe key advantages of our approach are that the adap-\ntive reasoning module can construct personalized reasoning\ngraphs, and the divergent extension employs divergent think-\ning to go beyond reactive recommendations to proactively\nrecommend new items following the evolving behavioral\ntrajectory. The self-verification and scoring also help im-\nprove the reasoning process. Fusing this with a standard se-\nquential recommendation model allows for combining com-\nplementary strengths without accessing extra information.\nExperiments\nSettings. To evaluate our proposed method, we conduct\nexperiments on three benchmark datasets: the Amazon\nBeauty, Amazon-Clothing, and MovieLens-1M (ML-1M)\ndatasets (McAuley et al. 2015; Harper and Konstan 2015).\nThe statistics of the three datasets after preprocessing are\nsummarized in Table 1. To evaluate the performance of our\nrecommendation system, we utilize a leave-one-out strategy\nwhere we repeatedly hold out one item from each user‚Äôs se-\nquence of interactions. We report two widely used ranking\nmetrics - Top-n metrics HR@n (Hit Rate) and NDCG@n\n(Normalized Discounted Cumulative Gain) where n is set\nto 5 and 10. HR@n measures whether the held-out item\nis present in the top-n recommendations, while NDCG@n\nconsiders the position of the held-out item by assigning\nhigher scores to hits at the top ranks. Following the exper-\niment comparison (Du et al. 2023), we include four base-\nline methods: BERT4Rec (Sun et al. 2019) adopts a bidirec-\ntional Transformer as the sequence encoder. FDSA (Zhang\net al. 2019) applies self-attention blocks to capture transition\npatterns of items and attributes. CL4SRec (Xie et al. 2022)\nproposes data augmentation strategies for contrastive learn-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19192\nSpecs. Beauty Clothing ML-1M\n# Users 22,363 39,387 6,041\n# Items 12,101 23,033 3,417\n# A\nvg.Length 8.9 7.1 165.5\n# Actions 198,502 278,677 999,611\nSparsity 99.93% 99.97% 95.16%\nTable 1: Statistics of the datasets after preprocessing.\nDataset Metric FDSA BERT4Rec CL4SRec\nDuoRec\nML-1M\nHR@5 0.0909 0.1124 0.1141 0.2011\nHR@10 0.1631 0.1910 0.1866 0.2837\nND@5 0.0599 0.0713 0.0721 0.1265\nND@10 0.0878 0.0980 0.1013 0.1663\nHR@5 0.0237 0.0201 0.0398 0.0552\nAmazon\nHR@10 0.0418 0.0413 0.0664 0.0839\nBeauty\nND@5 0.0195 0.0192 0.0221 0.0350\nND@10 0.0275 0.0263 0.0322 0.0447\nHR@5 0.0119 0.0128 0.0166 0.0190\nAmazon\nHR@10 0.0197 0.0202 0.0273 0.0311\nClothing\nND@5 0.0073 0.0081 0.0093 0.0118\nND@10 0.0109 0.0113 0.0125 0.0155\nTable 2: Performance comparison of baseline models on\nthree benchmark datasets. Higher is better.\ning in the sequential recommendation. DuoRec (Qiu et al.\n2022) proposes both supervised and unsupervised sampling\nstrategies for contrastive learning in the sequential recom-\nmendation.\nResults and Analysis. As evidenced in Table 2 and 3, we\nconduct comprehensive benchmarking experiments on three\nwidely-used datasets - ML-1M, Amazon Beauty, and Ama-\nzon Clothing. We compare our proposed LLMRG model\nbuilt on top of GPT3.5 or GPT4 with several strong baseline\nmethods, including FDSA (Zhang et al. 2019), BERT4Rec\n(Sun et al. 2019), CL4SREC (Xie et al. 2022), and DuoRec\n(Qiu et al. 2022). We observe significant performance gains\non HR@5, HR@10, NDCG@5, and NDCG@10 after ap-\nplying LLMRG, compared to the original baseline mod-\nels. This indicates that conventional recommender systems\nstruggle to model the conceptual relationships and behav-\nioral sequences of diverse user interests. In contrast, our pro-\nposed LLMRG framework can boost recommendation per-\nformance without needing any additional information. These\nimprovements showcase how large language models can\nbring logical reasoning and interpretability to recommender\nsystems. Furthermore, LLMRG performance scales with\nthe underlying LLM capability - the GPT4-based LLMRG\nconsistently outperforms its GPT3.5 counterpart. In addi-\ntion, when comparing the ML-1M movie dataset to the\nBeauty and Clothing product datasets, we observed that our\nLLMRG approach led to greater improvements across all\nevaluation metrics on the ML-1M dataset. This suggests that\nmovie items contain richer semantic information and en-\nable more semantically logical reasoning relationships than\nAmazon product items. As movies often have complex plots,\ncharacter arcs, and artistic themes, recommending movies\nlikely requires more sophisticated relational reasoning be-\ntween items than recommending simple retail products. The\ncomplexity of logical relations between movie entities en-\nables our LLMRG method to better leverage its relational\nmodeling capabilities. In contrast, beauty and clothing prod-\nucts have less narrative complexity, so there is less opportu-\nnity for relational reasoning to improve recommendations.\nAblation Study. To demonstrate the effectiveness of our\nproposed reasoning graph, we conduct ablation studies on\nour LLMRG model using two benchmark datasets: ML-1M\nand Amazon Beauty. We compare LLMRG to the DuoRec\nbaseline model as well as DuoRec augmented with a sim-\nple sequence graph, as proposed by Wu et al. (2019). The\nsequence graph directly models interaction sequences with-\nout reasoning. We also compare against combining DuoRec\nwith large language models - GPT-3.5 and GPT-4 - with-\nout constructing a reasoning graph. Here, the LLM simply\noutputs recommended items based on prompts containing\nhistorical sequences and user profiles, without a reasoning\ngraph. As shown in Table 4, the DuoRec model augmented\nwith a sequence graph provides only minor improvements\ncompared to our full LLMRG model. The DuoRec+GPT3.5\nmodel without reasoning graph integration fails to sig-\nnificantly improve DuoRec performance on ML-1M, and\neven decreases performance on the Amazon Beauty dataset.\nThanks to its greater capability, DuoRec+GPT4 boosts per-\nformance over DuoRec+GPT3.5 but still lags far behind our\nLLMRG model. These results demonstrate that the reason-\ning graph constructed by our proposed instructions is criti-\ncal for performance, and simple next-item prediction is in-\nsufficient (DuoRec+GPT3.5 and DuoRec+GPT4). By ex-\nplicitly modeling the reasoning process between user pro-\nfiles and interaction sequences, LLMRG is able to make ac-\ncurate, explainable recommendations. Our ablation studies\nconfirm the reasoning graph‚Äôs necessity and value in effec-\ntively leveraging the power of large language models for rec-\nommendation systems.\nOur additional ablation studies further explore the effec-\ntiveness of each module in our LLMRG framework. Us-\ning DuoRec as a baseline model, we compared it to ab-\nlation versions of LLMRG with or without the divergent\nextension and self-verification modules based on GPT3.5\nor GPT4. The results in Table 5 reveal that LLMRG (with\nGPT3.5 or GPT4) without the divergent extension mod-\nule provides only marginal improvement compared with the\ncomplete LLMRG. However, removing the self-verification\nmodule from LLMRG (GPT3.5) actually decreases perfor-\nmance. This demonstrates the limited reasoning capability\nof GPT3.5 - without verification, uncontrolled reasoning in-\ntroduces noise that reduces overall performance. Overall,\nthese ablation experiments clearly demonstrate the value of\nboth our divergent extension and self-verification modules in\nenabling more advanced reasoning while maintaining accu-\nracy. The modules work synergistically to expand the search\nspace of possible solutions while filtering out inaccurate or\nincoherent lines of reasoning.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19193\nDataset Metric\nFDSA BERT4Rec CL4SRec DuoRec\nGPT3.5 GPT4 GPT3.5 GPT4 GPT3.5 GPT4 GPT3.5 GPT4\nML-1M\nHR@5 + 20.70% + 25.79% + 26.67% + 32.56% + 19.98% + 21.02% + 12.87% + 14.76%\nHR@10 + 17.93 % +\n22.87% + 13.52 % +\n16.49 % + 17.30 % +\n19.31 % + 14.10 % +\n15.53 %\nND@5 + 21.33% + 30.27\n% + 25.74 % +\n32.82 % + 14.97 % +\n16.78 % + 23.55 % +\n26.01 %\nND@10 + 21.78% + 28.25% + 23.34% + 28.06% + 17.67% + 20.42% + 12.86% + 13.77%\nHR@5 + 13.89 % +\n17.53 % + 19.17 % +\n23.22 % + 11.15 % +\n14.15 % + 9.31 % +\n11.93 %\nAmazon HR@10 + 15.02 % +\n17.78 % + 17.79 % +\n22.14 % + 10.22 % +\n11.32 % + 5.14 % +\n6.61 %\nBeauty ND@5 + 16.20 % +\n18.64 % + 14.21 % +\n17.63 % + 8.45 % +\n10.18 % + 7.42 % +\n9.24 %\nND@10 + 14.78 % +\n17.64 % + 11.53 % +\n14.76 % + 8.17 % +\n9.68 % + 6.67 % +\n7.95 %\nHR@5 + 20.67 % +\n23.92 % + 16.09 % +\n19.10 % + 7.90 % +\n10.92 % + 9.98 % +\n11.40 %\nAmazon HR@10 + 14.45 % +\n17.88 % + 10.52 % +\n13.72 % + 11.21 % +\n14.99 % + 7.65 % +\n9.48 %\nClothing ND@5 + 8.16 % +\n10.86 % + 7.39 % +\n10.39 % + 6.02 % +\n9.09 % + 6.74 % +\n9.19 %\nND@10 + 6.01 % +\n8.13 % + 5.21 % +\n5.94 % + 4.32 % +\n8.07 % + 7.89 % +\n9.29 %\nTable 3: Performance comparison on three benchmark datasets. We set the original models as baselines (Table 2) to compare\nwith our proposed LLMRG model based on GPT3.5 or GPT4. Higher is better.\nMethod\nML-1M Amazon Beauty\nHR@5 HR@10 ND@5 ND@10 HR@5 HR@10 ND@5 ND@10\nDuoRec 0.2011 0.2837 0.1265 0.1663 0.0552 0.0839 0.0350 0.0447\nDuoRec\nw/ seq + 6.36 % +\n7.12 % + 12.25 % + 4.50 % + 3.26 % +\n2.74 % + 3.71 % + 2.68 %\nDuoRec+GPT3.5 + 0.94 % +\n0.81 % + 0.55 % + 1.80 % - 1.26 % -\n0.71 % - 0.85 % - 0.89 %\nLLMRG(GPT3.5) + 12.87 % +\n14.10 % + 23.55 % + 12.86 % + 9.31 % +\n5.14 % + 7.42 % + 6.67 %\nDuoRec+GPT4 + 3.28 % +\n2.29 % + 3.95 % + 2.22 % + 0.72 % +\n0.71 % + 0.86 % + 0.67 %\nLLMRG(GPT4) + 14.76 % +\n15.53 % + 26.01 % + 13.77 % + 11.93 % +\n6.61 % + 9.24 % + 7.95 %\nTable 4: Ablation studies of our LLMRG model on two benchmark datasets, i.e., ML-1M and Amazon Beauty. We take the\nDuoRec as a baseline model to compare with the DuoRec with sequence graph and DuoRec with direct recommendation results\nvia naive GPT3.5 or GPT4 without constructing a reasoning graph. Higher is better.\nLLM Method\nML-1M Amazon Beauty\nHR@5 HR@10 ND@5 ND@10 HR@5 HR@10 ND@5 ND@10\nNA DuoRec 0.2011 0.2837 0.1265 0.1663 0.0552 0.0839 0.0350 0.0447\nw/o div + 5.12 % +\n3.87 % + 8.30 % + 4.75 % + 3.62 % +\n2.86 % + 4.57 % + 3.80 %\nGPT3.5 w/o ver - 4.72 % -\n3.94 % - 10.90 % - 4.14 % - 2.17 % -\n1.43 % - 2.57 % - 2.46 %\nw/ div & ver + 12.87 % +\n14.10 % + 23.55 % + 12.86 % + 9.31 % +\n5.14 % + 7.42 % + 6.67 %\nw/o div + 7.06 % +\n4.68 % + 13.35 % + 8.11 % + 4.89 % +\n3.45 % + 4.28 % + 5.81 %\nGPT4 w/o ver + 5.86 % +\n2.36 % + 5.77 % + 3.72 % + 1.26 % +\n1.31 % + 1.71 % + 1.56 %\nw/ div & ver + 14.76 % +\n15.53 % + 26.01 % + 13.77 % + 11.93 % +\n6.61 % + 9.24 % + 7.95 %\nTable 5: Ablation studies of our LLMRG model on two benchmark datasets, i.e., ML-1M and Amazon Beauty. We take the\nDuoRec as a baseline model to compare with the ablation models w/ or w/o divergent extension and self-verification modules\nbased on GPT3.5 or GPT4. Higher is better.\nAs shown in Figure 4, we also analyze the effective-\nness of the proposed knowledge base self-improving. Based\non LLMRG (GPT3.5), we calculate the average access\nfrequency of the model call to LLM on two benchmark\ndatasets. The experimental results show that the average\naccess frequency decreases significantly as the number of\nreasoning steps increases. After 3,000 times of reasoning\nand verification, the average access frequency decreases by\nabout 30% compared to not using this module, proving that\nthe knowledge base contains high-quality reasoning chains\nthat can be reused. Moreover, we observed that the reuse\nrate of high-quality reasoning chains in Amazon Beauty is\nhigher than that of ML-1M, and the long-tailed distribution\nof Amazon products is one of the reasons for this difference.\nTo provide intuitive examples corroborating our quan-\ntitative results, we examined real case studies from the\nML-1M dataset using (a) our complete LLMRG model,\n(b) LLMRG without the divergent extension module, and\n(c) LLMRG without the self-verification module. The case\nstudies in Figure 2 illustrate the differences in reasoning be-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19194\nDivergent Interactions\n1. Gender: Male,¬†2. Age: 25\n3. Occ.: Writer\nUser Attributes\n¬†User \nInteractions \nDivergent \nInteractions \nUser \nAttributes \nd1: Blade Runner,¬†d2: The Terminator\nd3: Die Hard\nd4: Close Encounters of the¬†Third Kind\nSequence of Interactions\n1. Star Wars: Episode VI¬†\n2. E.T. the Extra-Terrestrial\n3. Hustler, The,¬†¬†4. Jurassic Park\n5. Predator¬†¬†6. Star Wars: Episode IV\n7. Star Wars: Episode V\n8. Raiders of the Lost Ark,¬†¬†9. Jaws\n10. Saving Private Ryan\nDivergent Interactions\nd1: National Treasure¬†¬†\nd2: Schindler's List\nd3: Die Hard,¬† d4: Psycho\nLegend\n(c) w/o self-veriÔ¨Åcation\n(b) w/o divergent extension\n¬†(a) LLMRG\nFigure 2: The real case studies (ML-1M) on our (a) LLMRG and ablation models, i.e., (b) LLMRG w/o divergent extension\nand (c) LLMRG w/o self-verification. The black arrow represents the reasoning procedure.\n(c) ùíçùíïùíìùíñ in ML-1M (d) ùíçùíïùíìùíñ in Beauty(b) ùùâ in Beauty(a) ùùâ in ML-1M\nFigure 3: Sensitivity analysis of threshold of verification scoring œÑ and sequence truncation length ltru on HR and NDCG\nperformance based on ML-1M and Amazon Beauty benchmarks.\n10 100 1000 3000\nNumber of Reasoning\n20\n22\n24\n26\n28\n30Avg access frequency of LLM\nML-1M\nAmazon Beauty\nFigure 4: The average access frequency of LLM.\ntween the models. LLMRG generates coherent recommen-\ndations with sound justifications, leveraging both divergent\nthinking to expand possibilities and self-verification to fil-\nter out poor options. Without divergent extensions, LLMRG\nstruggles to move beyond obvious choices. Further, without\nself-verification, LLMRG‚Äôs recommendations become more\nspeculative and sometimes nonsensical, as the model lacks\nthe ability to check its own thinking. These qualitative anal-\nyses mirror the patterns in our numerical results, serving as\nfurther validation of the value added by each reasoning mod-\nule working in concert with our full LLMRG framework.\nThe case studies provide intuitive examples of how our ap-\nproach combines creative thinking and critical evaluation to\nproduce logical recommendations.\nSensitivity Analysis. We evaluate LLMRG‚Äôs sensitivity to\nthe two most crucial parameters, œÑ and ltru, on HR and\nNDCG, which control the threshold for verification scoring\nand sequence truncation length, respectively. Figure 3 (a)\nand (b) show that larger œÑ values yield more robust reason-\ning and filter out inferior options, thus boosting the model‚Äôs\nperformance on the ML-1M dataset. However, on the Beauty\ndataset, performance starts to decrease from œÑ = 30, likely\nbecause higher verification scoring thresholds filter out more\nreasoning chains, increasing the sparsity of the graph. Fig-\nures 3 (c) and (d) indicate that, generally, longer sequences\nbring better recommendation results by incorporating more\ninformation. In summary, largerœÑ and longer sequences both\ntend to improve performance. œÑ exhibits a peak value, be-\nyond which sparser reasoning graphs degrade results, espe-\ncially for less logical sequences, such as Amazon products.\nConclusion\nWe present LLMRG that utilizes LLM to construct personal-\nized reasoning graphs. This method demonstrates how LLM\ncan bring logical reasoning and interpretability to recom-\nmendation systems without needing any additional informa-\ntion. We demonstrate that our plug-and-play method can ef-\nfectively enhance multiple existing recommenders.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19195\nReferences\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877‚Äì\n1901.\nChen, H.; Li, Y .; Sun, X.; Xu, G.; and Yin, H. 2021. Tempo-\nral meta-path guided explainable recommendation. In Pro-\nceedings of the 14th ACM international conference on web\nsearch and data mining, 1056‚Äì1064.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nChu, Z.; Guo, H.; Zhou, X.; Wang, Y .; Yu, F.; Chen, H.;\nXu, W.; Lu, X.; Cui, Q.; Li, L.; et al. 2023a. Data-\nCentric Financial Large Language Models. arXiv preprint\narXiv:2310.17784.\nChu, Z.; Hao, H.; Ouyang, X.; Wang, S.; Wang, Y .; Shen,\nY .; Gu, J.; Cui, Q.; Li, L.; Xue, S.; et al. 2023b. Lever-\naging large language models for pre-trained recommender\nsystems. arXiv preprint arXiv:2308.10837.\nChu, Z.; Rathbun, S. L.; and Li, S. 2021. Graph infomax\nadversarial learning for treatment effect estimation with net-\nworked observational data. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery & Data Min-\ning, 176‚Äì184.\nChu, Z.; Wang, Y .; Cui, Q.; Li, L.; Chen, W.; Li, S.; Qin, Z.;\nand Ren, K. 2024. LLM-Guided Multi-View Hypergraph\nLearning for Human-Centric Explainable Recommendation.\narXiv preprint arXiv:2401.08217.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDu, H.; Yuan, H.; Zhao, P.; Zhuang, F.; Liu, G.; Zhao, L.;\nLiu, Y .; and Sheng, V . S. 2023. Ensemble Modeling with\nContrastive Knowledge Distillation for Sequential Recom-\nmendation. arXiv preprint arXiv:2304.14668.\nGuan, Y .; Wang, D.; Chu, Z.; Wang, S.; Ni, F.; Song, R.; Li,\nL.; Gu, J.; and Zhuang, C. 2023. Intelligent Virtual Assis-\ntants with LLM-based Process Automation. arXiv preprint\narXiv:2312.06677.\nHarper, F. M.; and Konstan, J. A. 2015. The movielens\ndatasets: History and context. Acm transactions on inter-\nactive intelligent systems (tiis), 5(4): 1‚Äì19.\nLi, S.; and Chu, Z. 2023. Machine Learning for Causal In-\nference. Springer Nature.\nMcAuley, J.; Targett, C.; Shi, Q.; and Van Den Hengel, A.\n2015. Image-based recommendations on styles and substi-\ntutes. In Proceedings of the 38th international ACM SIGIR\nconference on research and development in information re-\ntrieval, 43‚Äì52.\nQiu, R.; Huang, Z.; Yin, H.; and Wang, Z. 2022. Contrastive\nlearning for representation degeneration problem in sequen-\ntial recommendation. In Proceedings of the fifteenth ACM\ninternational conference on web search and data mining ,\n813‚Äì823.\nSheu, H.-S.; Chu, Z.; Qi, D.; and Li, S. 2021. Knowledge-\nguided article embedding refinement for session-based news\nrecommendation. IEEE Transactions on Neural Networks\nand Learning Systems, 33(12): 7921‚Äì7927.\nShin, R.; Lin, C. H.; Thomson, S.; Chen, C.; Roy, S.; Platan-\nios, E. A.; Pauls, A.; Klein, D.; Eisner, J.; and Van Durme, B.\n2021. Constrained language models yield few-shot semantic\nparsers. arXiv preprint arXiv:2104.08768.\nSun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang,\nP. 2019. BERT4Rec: Sequential recommendation with bidi-\nrectional encoder representations from transformer. In Pro-\nceedings of the 28th ACM international conference on infor-\nmation and knowledge management, 1441‚Äì1450.\nWang, H.; Zhang, F.; Zhang, M.; Leskovec, J.; Zhao, M.;\nLi, W.; and Wang, Z. 2019. Knowledge-aware graph neural\nnetworks with label smoothness regularization for recom-\nmender systems. In Proceedings of the 25th ACM SIGKDD\ninternational conference on knowledge discovery & data\nmining, 968‚Äì977.\nWang, X.; Jin, H.; Zhang, A.; He, X.; Xu, T.; and Chua,\nT.-S. 2020. Disentangled graph collaborative filtering. In\nProceedings of the 43rd international ACM SIGIR confer-\nence on research and development in information retrieval,\n1001‚Äì1010.\nWang, Y .; Chu, Z.; Ouyang, X.; Wang, S.; Hao, H.; Shen, Y .;\nGu, J.; Xue, S.; Zhang, J. Y .; Cui, Q.; et al. 2023. Enhancing\nrecommender systems with large language model reasoning\ngraphs. arXiv preprint arXiv:2308.10835.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824‚Äì24837.\nWu, S.; Tang, Y .; Zhu, Y .; Wang, L.; Xie, X.; and Tan, T.\n2019. Session-based recommendation with graph neural net-\nworks. In Proceedings of the AAAI conference on artificial\nintelligence, volume 33, 346‚Äì353.\nXie, X.; Sun, F.; Liu, Z.; Wu, S.; Gao, J.; Zhang, J.; Ding, B.;\nand Cui, B. 2022. Contrastive learning for sequential recom-\nmendation. In 2022 IEEE 38th international conference on\ndata engineering (ICDE), 1259‚Äì1273. IEEE.\nXu, F.; Lin, Q.; Han, J.; Zhao, T.; Liu, J.; and Cambria,\nE. 2023. Are Large Language Models Really Good Log-\nical Reasoners? A Comprehensive Evaluation From De-\nductive, Inductive and Abductive Views. arXiv preprint\narXiv:2306.09841.\nXue, S.; Wang, Y .; Chu, Z.; Shi, X.; Jiang, C.; Hao, H.; Jiang,\nG.; Feng, X.; Zhang, J. Y .; and Zhou, J. 2023. Prompt-\naugmented temporal point process for streaming event se-\nquence. arXiv preprint arXiv:2310.04993.\nZhang, T.; Zhao, P.; Liu, Y .; Sheng, V . S.; Xu, J.; Wang, D.;\nLiu, G.; Zhou, X.; et al. 2019. Feature-level Deeper Self-\nAttention Network for Sequential Recommendation. In IJ-\nCAI, 4320‚Äì4326.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n19196",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5642858147621155
    },
    {
      "name": "Natural language processing",
      "score": 0.4115154445171356
    }
  ]
}