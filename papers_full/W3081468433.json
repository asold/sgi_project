{
  "title": "Quantum Language Model With Entanglement Embedding for Question Answering",
  "url": "https://openalex.org/W3081468433",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2098524793",
      "name": "Chen Yi-wei",
      "affiliations": [
        "State Key Laboratory of Industrial Control Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1987602885",
      "name": "Pan Yu",
      "affiliations": [
        "State Key Laboratory of Industrial Control Technology"
      ]
    },
    {
      "id": "https://openalex.org/A140871868",
      "name": "Dong Daoyi",
      "affiliations": [
        "UNSW Sydney",
        "University of Canberra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2921526458",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2469060249",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W3024507639",
    "https://openalex.org/W3007551217",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6734862562",
    "https://openalex.org/W2962790223",
    "https://openalex.org/W2990961515",
    "https://openalex.org/W2958187629",
    "https://openalex.org/W2792315573",
    "https://openalex.org/W1711649566",
    "https://openalex.org/W2059836092",
    "https://openalex.org/W2559394418",
    "https://openalex.org/W3213744715",
    "https://openalex.org/W2794444783",
    "https://openalex.org/W2879447321",
    "https://openalex.org/W2999912861",
    "https://openalex.org/W2521267242",
    "https://openalex.org/W3130292943",
    "https://openalex.org/W6632732432",
    "https://openalex.org/W2491030916",
    "https://openalex.org/W2160416736",
    "https://openalex.org/W2128739123",
    "https://openalex.org/W2756668352",
    "https://openalex.org/W2784311771",
    "https://openalex.org/W2889425359",
    "https://openalex.org/W6765237052",
    "https://openalex.org/W3037737784",
    "https://openalex.org/W6640033935",
    "https://openalex.org/W6772203101",
    "https://openalex.org/W4211080241",
    "https://openalex.org/W2003458629",
    "https://openalex.org/W3090383127",
    "https://openalex.org/W3173744006",
    "https://openalex.org/W6632118081",
    "https://openalex.org/W2951680627",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W6635189695",
    "https://openalex.org/W1966443646",
    "https://openalex.org/W6685536428",
    "https://openalex.org/W2538374209",
    "https://openalex.org/W6685356407",
    "https://openalex.org/W6685160515",
    "https://openalex.org/W6757123339",
    "https://openalex.org/W2090552429",
    "https://openalex.org/W2969720288",
    "https://openalex.org/W2087942425",
    "https://openalex.org/W2287577556",
    "https://openalex.org/W2951359136",
    "https://openalex.org/W3104263599",
    "https://openalex.org/W1533946607",
    "https://openalex.org/W3142513648",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2904759072",
    "https://openalex.org/W3102961503",
    "https://openalex.org/W2954058703",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4286812972",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4246691300",
    "https://openalex.org/W1926201870",
    "https://openalex.org/W3044919273",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2170608991",
    "https://openalex.org/W1533213248",
    "https://openalex.org/W2264105282",
    "https://openalex.org/W2173361515",
    "https://openalex.org/W2963773425",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W1591825359",
    "https://openalex.org/W4298392952",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2995971510",
    "https://openalex.org/W3138819813",
    "https://openalex.org/W2798434869",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2950193743",
    "https://openalex.org/W4242765389",
    "https://openalex.org/W3104396616",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2170738476",
    "https://openalex.org/W2173681125",
    "https://openalex.org/W3100210443",
    "https://openalex.org/W1485275430"
  ],
  "abstract": "Quantum language models (QLMs) in which words are modeled as a quantum superposition of sememes have demonstrated a high level of model transparency and good post-hoc interpretability. Nevertheless, in the current literature, word sequences are basically modeled as a classical mixture of word states, which cannot fully exploit the potential of a quantum probabilistic description. A quantum-inspired neural network (NN) module is yet to be developed to explicitly capture the nonclassical correlations within the word sequences. We propose a NN model with a novel entanglement embedding (EE) module, whose function is to transform the word sequence into an entangled pure state representation. Strong quantum entanglement, which is the central concept of quantum information and an indication of parallelized correlations among the words, is observed within the word sequences. The proposed QLM with EE (QLM-EE) is proposed to implement on classical computing devices with a quantum-inspired NN structure, and numerical experiments show that QLM-EE achieves superior performance compared with the classical deep NN models and other QLMs on question answering (QA) datasets. In addition, the post-hoc interpretability of the model can be improved by quantifying the degree of entanglement among the word states.",
  "full_text": "1\nQuantum Language Model with Entanglement\nEmbedding for Question Answering\nYiwei Chen, Yu Pan, Senior Member, IEEE, Daoyi Dong, Senior Member, IEEE\nAbstract—Quantum Language Models (QLMs) in which words\nare modelled as a quantum superposition of sememes have\ndemonstrated a high level of model transparency and good post-\nhoc interpretability. Nevertheless, in the current literature word\nsequences are basically modelled as a classical mixture of word\nstates, which cannot fully exploit the potential of a quantum\nprobabilistic description. A quantum-inspired neural network\nmodule is yet to be developed to explicitly capture the non-\nclassical correlations within the word sequences. We propose a\nneural network model with a novel Entanglement Embedding\n(EE) module, whose function is to transform the word sequence\ninto an entangled pure state representation. Strong quantum en-\ntanglement, which is the central concept of quantum information\nand an indication of parallelized correlations among the words,\nis observed within the word sequences. The proposed QLM with\nEE (QLM-EE) is proposed to implement on classical computing\ndevices with a quantum-inspired neural network structure, and\nnumerical experiments show that QLM-EE achieves superior\nperformance compared with the classical deep neural network\nmodels and other QLMs on Question Answering (QA) datasets.\nIn addition, the post-hoc interpretability of the model can be\nimproved by quantifying the degree of entanglement among the\nword states.\nIndex Terms—quantum language model, complex-valued neu-\nral network, interpretability, entanglement embedding.\nI. I NTRODUCTION\nN\nEURAL Network Language Model (NNLM) [1] is\nwidely used in Natural Language Processing (NLP)\nand information retrieval [2]. With the rapid development of\ndeep learning models, NNLMs have achieved unparalleled\nsuccess on a wide range of tasks [3]–[10]. It becomes a\ncommon practice for the NNLMs to use word embedding\n[4], [11] to obtain the representations of words in a feature\nspace. While NNLM has been very successful at knowledge\nrepresentation and reasoning [7], its interpretability is often\nin question, making it inapplicable to critical areas such as\nthe credit scoring system [12]. Two important factors have\nbeen summarized in [13] for evaluating the interpretability of\na machine learning model, namely, Transparency and Post-hoc\nInterpretability. The model transparency relates to the forward\nmodelling process, while the post-hoc interpretability is the\nThis work was supported by the National Natural Science Foundation\nof China (No. 61703364) and the Australian Research Council’s Discovery\nProjects funding scheme under Project DP190101566.\nY . Chen is with the Institute of Cyber-Systems and Control, Zhejiang\nUniversity, Hangzhou, 310027, China. (email: ewell@zju.edu.cn).\nY . Pan is with the Institute of Cyber-Systems and Control, College of\nControl Science and Engineering, Zhejiang University, Hangzhou, 310027,\nChina. (email: ypan@zju.edu.cn).\nD. Dong is with the School of Engineering and Information Technology,\nUniversity of New South Wales, Canberra, ACT 2600, Australia. (email:\ndaoyidong@gmail.com).\nability to unearth useful and explainable knowledge from a\nlearned model.\nAnother emerging area is quantum information and quan-\ntum computation where quantum theory can be utilized to\ndevelop more powerful computers and more secure quantum\ncommunication systems than their classical counterparts [14].\nThe interaction between quantum theory and machine learning\nhas also been extensively explored in recent years. On one\nhand, many advanced machine learning algorithms have been\napplied to quantum control, quantum error correction and\nquantum experiment design [15]–[18]. On the other hand,\nmany novel quantum machine learning algorithms such as\nquantum neural networks and quantum reinforcement learning\nhave been developed by taking advantage of the unique char-\nacteristics of quantum theory [19]–[26]. Recently, Quantum\nLanguage Models (QLMs) inspired by quantum theory (espe-\ncially quantum probability theory) have been proposed [27]–\n[34] and demonstrated considerable performance improvement\nin model accuracy and interpretability on information retrieval\nand NLP tasks. QLM is a quantum heuristic Neural Network\n(NN) deﬁned on a Hilbert space which models language units,\ne.g., words and phrases, as quantum states. By embedding the\nwords as quantum states, QLM tries to provide a quantum\nprobabilistic interpretation of the multiple meanings of words\nwithin the context of a sentence. Compared with the classical\nNNLM, the word states in QLM are deﬁned on a Hilbert\nspace which is different from the classical probability space.\nIn addition, modelling the process of feature extraction as\nquantum measurement which collapses the superposed state\nto a deﬁnite meaning within the context of a sentence could\nincrease the transparency of the model.\nDespite the fact that previous QLMs have achieved good\nperformance and transparency, the state-of-the-art designs still\nhave limitations. For example, the mixed-state representation\nof the word sequence in [32], [34] is just a classical ensemble\nof the word states. As shown in the left of Fig. 1, a classical\nprobabilistic mixture of quantum states is not able to fully\ncapture the complex interaction among subsystems. Although\nQuantum Many-body Wave Function (QMWF) [33] method\nhas been applied to model the entire word sequence as the\ncombination of subsystems, it is still based on a strong premise\nthat the states of the word sequences are separable, as shown\nin the middle of Fig. 1. A general quantum state has the ability\nto describe distributions that cannot be split into independent\nstates of subsystems like in the right of Fig. 1. In quantum\nphysics, the state for a group of particles can be generated as\nan inseparable whole, leading to a non-classical phenomenon\ncalled quantum entanglement [35]. Quantum entanglement\narXiv:2008.09943v3  [cs.CL]  20 Dec 2021\n2\nFig. 1. Comparison between mixed state, product state and entangled state\nwith a bipartite example. |ψ⟩is the system state. |ψ1⟩and |ψ2⟩are the states\nof subsystems Aand B, respectively. Parallelized correlations (denoted as\narrows) exist between the two subsystems in the entangled state, while for\nmixed and product states the superposition only exists within the subsystem\nitself. Separable state is deﬁned as the classical probabilistic mixture of\nproduct states. The product state and entangled state are deﬁned on the tensor\nproduct (⊗) of two Hilbert spaces.\ncan be understood as correlations (between subsystems) in\nsuperposition, and this type of parallelized correlations can\nbe observed in human language system as well. A word can\nhave different meanings when combined with other words.\nFor example, the verb turn has four meanings {move, change,\nstart doing, shape on a lathe }. If we combine it with on\nto get the phrase turn on , the meaning of turn will be in\nthe superposition of change and start doing . However, this\nkind of correlation has not been explicitly generated in the\npresent NN-based QLMs. Besides, a statistical method has\nbeen proposed in [36] to characterize the entanglement within\nthe text in a post-measurement conﬁguration, which still lacks\nthe transparency in the forward modelling process.\nIn this paper, we propose a novel quantum-inspired En-\ntanglement Embedding (EE) module which can be conve-\nniently incorporated into the present NN-based QLMs to learn\nthe inseparable association among the word states. To be\nmore speciﬁc, each word is ﬁrstly embedded as a quantum\npure state and described by a unit complex-valued vector\ncorresponding to the superposition of sememes. The Word\nEmbedding neural network module is adopted from [34].\nWord sequences (phrases, N-grams, etc.) are initially given\nas the tensor product of the individual word states, and then\ntransformed to a general entangled state as the output of the\nEE module. The EE module is realized by a complex-valued\nneural network, which is essentially approximating the unitary\noperation that converts the initial product state to an entangled\nstate. After the entanglement embedding, high-level features\nof the word sequences are extracted by inner products between\nthe entangled state vector and virtual quantum measurement\nvectors [14]. All the parameters of the complex-valued neural\nnetwork are trainable with respect to a cost function deﬁned on\nthe extracted features. Entanglement measures for quantifying\nand visualizing the entanglement among the word states can\nbe directly applied on the output of the learned model. We\nconduct experiments on two benchmark Question Answering\n(QA) datasets to show the superior performance and post-\nhoc interpretability of the proposed QLM with EE (QLM-\nEE). In addition, the word embedding dimension can be\ngreatly reduced when compared with previous QLMs, due\nto the composition of word embedding and EE modules in\nthe hierarchical structure of the neural network. Note that\nthe current QLM-EE is proposed to implement on classical\ncomputing devices with a quantum-inspired neural network\nstructure.\nThe main contributions of this paper are summarized as\nfollows.\n• A novel EE neural network module is proposed. The out-\nput of the EE module represents the correlations among\nthe word states with a quantum probabilistic model which\nexplores the entire Hilbert space of quantum pure states.\nThe embedded states can reveal the possible entanglement\nbetween the words, which is an indication of parallelized\ncorrelations. The entanglement can be quantiﬁed to pro-\nmote the transparency and post-hoc interpretability of\nQLMs to an unprecedented level.\n• A QLM-EE framework is presented by cascading the\nword embedding and EE modules. The word embedding\nmodule captures the superposed meanings of individual\nwords, while the EE modules encode the correlations be-\ntween the words at a higher level. The resulting cascaded\ndeep neural network is more expressive and efﬁcient than\nthe shallow networks used by previous QLMs.\n• The superior performance of QLM-EE is demonstrated\nover the state-of-the-art classical neural network models\nand other QLMs on QA datasets. In addition, the word\nembedding dimension in QLM-EE is greatly reduced and\nthe semantic similarity of the embedded states of word\nsequences can be studied using the tools from quantum\ninformation theory. The entanglement between the words\ncan be quantiﬁed and visualized using analytical methods\nunder the QLM-EE framework.\nThis paper is organized as follows. Section II provides\na brief introduction to preliminaries and related work. En-\ntanglement embedding is presented in Section III. Section\nIV proposes the QLM-EE model. Experimental results are\npresented in Section V and the results show that QLM-\nEE achieves superior performance over ﬁve classical models\nand ﬁve quantum-inspired models on two datasets. Post-hoc\ninterpretability is discussed in Section VI and concluding\nremarks are given in Section VII.\nII. P RELIMINARIES AND RELATED WORK\nA. Quantum State\nMathematically, an n-level quantum system can be de-\nscribed by an n-dimensional Hilbert space H= Cn. A pure\nstate of the quantum system is described by Dirac notation\nwhere a ket represents a state vector, written as |ψ⟩(equivalent\nto a complex-valued column vector). The conjugate transpose,\ndenoted by †, of a state vector is called bra, denoted as ⟨ψ|,\ni.e., ⟨ψ| = ( |ψ⟩)†. Denote a chosen orthonormal basis of\nHas {|0⟩,|1⟩,..., |n−1⟩}. Any quantum pure state can be\ndescribed by a unit vector in H, which may be expanded on\nthe basis states as\n|ψ⟩=\nn−1∑\ni=0\nαi|i⟩, (1)\n3\nwith complex-valued probability amplitudes {αi}satisfying\nn−1∑\ni=0\n|αi|2 = 1. (2)\nNote that the set {|αi|2}deﬁnes a classical discrete probability\ndistribution. A quantum system can be in the superposition of\ndistinct states at the same time, with the probability of being|i⟩\ngiven by |αi|2. For example, we consider a quantum bit (qubit)\nthat is the basic information unit in quantum computation and\nquantum information, which can be physically realized using\ne.g., a photon, an electron spin or a two-level atom [14]. The\nstate of a qubit can be described as\n|ψ⟩= α0|0⟩+ α1|1⟩, (3)\nwhere α0, α1 ∈C and\n|0⟩=\n[\n1\n0\n]\n,|1⟩=\n[\n0\n1\n]\n. (4)\nThe state of a composite system |ψAB⟩consisting of two\nsubsystems Aand Bcan be described by the tensor product\n(⊗) of the states of these two subsystems |ψA⟩and |ψB⟩as\n|ψAB⟩= |ψA⟩⊗|ψB⟩. (5)\nFor example, if two qubits are in |ψ1⟩= α0|0⟩+ α1|1⟩and\n|ψ2⟩= α3|0⟩+ α4|1⟩, respectively, the state of the two-qubit\nsystem can be described by\n|ψ12⟩= α0α3|00⟩+ α0α4|01⟩+ α1α3|10⟩+ α1α4|11⟩, (6)\nwhere we have denoted\n|0⟩⊗|0⟩= |00⟩=\n\n\n1\n0\n0\n0\n\n,|0⟩⊗|1⟩= |01⟩=\n\n\n0\n1\n0\n0\n\n,\n|1⟩⊗|0⟩= |10⟩=\n\n\n0\n0\n1\n0\n\n,|1⟩⊗|1⟩= |11⟩=\n\n\n0\n0\n0\n1\n\n.\nFor an open quantum system or a quantum ensemble, its\nstate needs to be described by a density matrix ρ satisfying\ntr(ρ) = 1, ρ†= ρ and ρ≥0. In this paper, we mainly focus\non quantum pure states, and thus the inputs and outputs of\nthe neural network modules are complex-valued vectors that\nstand for the pure states. If a quantum system is in the state\nin (1), then the system is physically in the superposition state\nof {|i⟩}. Similar superposition, although not physically, may\nalso exist in the human language systems, which is expressed\nas the superposition of multiple meanings of a semantic unit.\nB. Quantum Entanglement\nQuantum entanglement is one of the most fundamental\nconcepts in quantum theory. Entanglement describes the non-\nclassical correlation between quantum systems. To be more\nspeciﬁc, a many-body quantum system is in an entangled\nstate if the state of one subsystem is determined by the\nmeasurement result of the other subsystem. Mathematically\nspeaking, the joint state of an entangled quantum system\ncannot be decomposed into the states of subsystems by tensor\nproduct. For example, we consider two quantum systems A\nand B deﬁned in Hilbert spaces HA and HB, respectively.\nAssume the basis state vectors of the two subsystems are {|i⟩}\nand {|j⟩}. The joint state is then deﬁned on the tensor product\nspace HA⊗HB, whose basis state vectors are given by the set\n{|i⟩⊗|j⟩}. A general pure state |ψ⟩of the composite quantum\nsystem can be written as follows\n|ψ⟩=\nn∑\ni,j\nαij|i⟩⊗|j⟩, (7)\nwhere {αij}are complex-valued probability amplitudes. The\npure state is separable if it can be decomposed as\n|ψ⟩= |ψ1⟩⊗|ψ2⟩, (8)\nwhere |ψ1⟩= ∑\niα1i|i⟩and |ψ2⟩= ∑\njα2j|j⟩are pure states\nof the subsystems. Otherwise, the pure state is entangled.\nAccording to (7) and (8), separable pure states only constitute\na small potion of the quantum states that can be deﬁned\non HA ⊗HB, which means that a signiﬁcant amount of\ncorrelations between the subsystems cannot be characterized\nby the separable states. For example, one of the entangled Bell\nStates or EPR pairs [14] is deﬁned by\n|ψ⟩= 1√\n2(|00⟩+ |11⟩), (9)\nwhich can not be written as a tensor product of two pure\nstates of the subsystems. The composite system is in the\nsuperposition of two basis states |00⟩and |11⟩. If we measure\nthe state of the ﬁrst system and the measurement result is |1⟩,\nthen the state of the second system is |1⟩. However, since\nthe ﬁrst system is a superposition of two states |0⟩and |1⟩,\nthe measurement result can be |0⟩with equal probability. In\nthat case, the state of the second system is |0⟩. This kind\nof non-classical correlation cannot be modelled by classical\nprobability. Quantum entanglement can be used to model the\nsuperposition of correlations between the subsystems, or in\nour case, the superposition of multiple meanings between the\nword states.\nC. Quantum Measurement\nQuantum measurement is used to extract information from\na quantum system. A widely-used measurement is the projec-\ntive measurement (von Neumann measurement). For example,\nwhen measuring a pure state |ψ⟩= ∑n−1\ni=0 αi|i⟩by projecting\nonto the measurement basis states {|i⟩}, the quantum state will\ncollapse to one of the basis states with the probability of\npi(|ψ⟩) = |αi|2 = |⟨i|ψ⟩|2, (10)\nand the inner product ⟨i|ψ⟩of |i⟩and |ψ⟩is calculated as\n⟨i|ψ⟩= (|i⟩)†|ψ⟩. (11)\nIn a more general setting, projective measurements can be\nperformed using any state vector |x⟩(i.e., not just the com-\nputational basis), with the probability of obtaining |x⟩given\nby\npx(|ψ⟩) = |⟨x|ψ⟩|2. (12)\n4\nFig. 2. Pipeline of the word embedding module for a 3-word sequence.\nD. Quantum Fidelity\nIn quantum information theory, ﬁdelity is a real-valued\nmeasure of the similarity between two quantum pure states,\nwhich is deﬁned as\nF(|ψa⟩,|ψb⟩) = |⟨ψa|ψb⟩|2. (13)\nAccording to (10), ﬁdelity is just the probability of collapsing\n|ψa⟩to |ψb⟩if |ψa⟩is measured by |ψb⟩, or the probability of\ncollapsing |ψb⟩to |ψa⟩if |ψb⟩is measured by |ψa⟩. In other\nwords, ﬁdelity is the probability that one quantum state will\npass the test to be identiﬁed as the other.\nE. Complex-valued Word Embedding\nComplex-valued word embedding module aims to model\nwords as quantum pure states in the semantic Hilbert space.\nIn this paper, the complex-valued word embedding module is\nadopted from [34]. As shown in Fig. 2, each word is ﬁrstly\nencoded to a one-hot vector with a ﬁxed length. Then, the\namplitude embedding and phase embedding modules map the\none-hot vector into a pair of real-valued amplitude and phase\nvectors {[r1 ···rn]T,[φ1 ···φn]T}. After that, the amplitude\nvector is normalized to a unit vector and the polar form\nrepresentation of the word state is given by\n|s⟩=\nn∑\nj=1\nrjeiφj |j⟩, (14)\nwhere i is the imaginary number with i2 = −1. Note that\n{|j⟩}n\nj=1 are the basis sememes of the semantic Hilbert space,\nwhich represent the minimum semantic units of the word\nmeaning. Finally, the pair of vectors is transformed into a\ncomplex-valued vector as [α1 ···αn]T, and the word state can\nbe written as\n|s⟩=\nn∑\nj=1\nαj|j⟩ (15)\nwith ∑n\nj=1 |αj|2 = 1. The complex-valued word embedding\nin [37] deﬁnes the real-valued amplitude as the semantic\nmeaning of the word, and the complex phases as the positional\ninformation of word in the sequence. In contrast, the complex-\nvalued word embedding in this paper aims to model words\nusing quantum state representation, and no speciﬁc meaning\nis given to the phase or amplitude. Instead, the semantic\nmeanings and their quantum-like superposition are jointly\ndetermined by the amplitude and phase.\nF . Related Work\nIn [38], van Rijsbergen argued that quantum theory can\nprovide a uniﬁed framework for the geometrical, logical and\nprobabilistic models for information retrieval. Coecke et al.\n[27] introduced DisCo formalism based on tensor product\ncomposition and Zeng et al. [28] presented a quantum al-\ngorithm to categorize sentences in DisCo model. Kartsaklis\n[39] used the traditional machine learning method to quantify\nentanglement between verbs in DisCo model. Sordoni et al.\n[29] proposed a quantum language modelling approach for\ninformation retrieval and the density matrix formulation was\nused as a general representation for texts. The single and\ncompound terms were mapped into the same quantum space,\nand term dependencies are modelled as projectors. In [30],\nQuantum entropy minimization method has been proposed\nin learning concept embeddings for query expansion, where\nconcepts from the vocabulary were embedded in rank-one\nmatrices, and documents and queries were described by the\nmixtures of rank-one matrices. In [31], a quantum language\nmodel was presented where a “proof-of-concept” study was\nimplemented to demonstrate its potential.\nTwo NN-based Quantum-like Language Models (NNQLMs)\nhave been proposed, namely NNQLM-I and NNQLM-II [32].\nWords were modelled as quantum pure states in the semantic\nHilbert space and a word sequence was also modelled in the\nsame space by mixing the word states in a classical way as\nρ=\n∑\nk\nck|sk⟩⟨sk|, (16)\nwhere |sk⟩ was the word state representing the k-th word\nin the sentence and ck is the weight of k-th word state\nsatisfying ∑\nkck = 1. By (16), the semantic meaning of the\nword sequence is mainly determined by the word states with\nlarger weights. In NNQLM-I, the representation of a single\nsentence was obtained by the ﬁrst three layers as a density\nmatrix corresponding to a mixed state, and then the joint\nrepresentation of a question/answer pair was generated in the\nfourth layer by matrix multiplication. The last softmax layer\nwas invoked to match the question/answer pair. NNQLM-II\nadopted the same ﬁrst 4-layer network structure to obtain the\njoint representation of a question/answer pair as NNQLM-I,\nand employed a 2-dimensional convolutional layer instead to\nextract the features of the joint representation for comparing\nthe question/answer pairs. In [33], a Quantum Many-body\nWave Function (QMWF) method was presented in which the\nrepresentation of a single sentence was given by the tensor\n5\nproduct of word vectors. Operation that mimics the quantum\nmeasurement was applied on the product state to extract the\ncorrelation patterns between the word vectors. To be more\nspeciﬁc, a three-layer Convolutional Neural Network (CNN)\nwas used, in which the ﬁrst layer generated the product state\nrepresentation of a word sequence and the projective measure-\nment on the product state was simulated by a 1D convolutional\nlayer with product pooling. A complex-valued network called\nCNM was presented in [34]. Similar to NNQLMs, CNM\nembedded the word sequence as a mixed state but with a\ncomplex-valued neural network. Then a number of trainable\nmeasurement operations were applied on the complex-valued\ndensity matrix representations to obtain the feature vectors of\nquestion/answer for comparison. CNM achieved comparable\nperformance over the state-of-the-art models based on CNN\nand recurrent NN. More importantly, CNM has shown its\nadvantage in interpretability, since the model has simulated\nthe generation of a quantum probabilistic description for the\nindividual words with complex-valued word states, and the\nprojection of the superposed sememes onto a ﬁxed meaning\nby quantum-like measurement within a particular context. The\nwork [40] presented a survey of the quantum-inspired informa-\ntion retrieval and drew a road-map of future directions. Ref.\n[41] proposed a decision-level fusion strategy for predicting\nsentiment judgments inspired by quantum theory.\nIn the existing works, some potential of quantum-inspired\nNN modules have been explored for language modelling.\nHowever, most of the existing works assume that the word\nstates are placed in the same Hilbert space, while in QLM-\nEE the word states are placed in the tensor product of Hilbert\nspaces. This enables an explicit entanglement analysis between\nthe word states, which is not possible if the word states are\nmixed in the same space. Although [33] has modelled the\nwords in independent Hilbert spaces, it was assumed that\nthe interaction among the words are described by product\nstates, which limits the expressive power of quantum state\nspace. This paper continues these efforts and proposes a novel\nentanglement embedding module for question answering task.\nIII. E NTANGLEMENT EMBEDDING\nQuantum probabilistic superposition of states models the\npolysemy of words and sequences. Quantum states span the\nentire complex Hilbert space, while the amplitudes and com-\nplex phases of the states give the probability distribution of the\nmultiple meanings of words. An N-gram is the composition\nof N words whose joint state is deﬁned on the tensor product\nof N Hilbert spaces. However, the joint state itself is not\nnecessarily the tensor product of N states of the Hilbert spaces.\nThe set of entangled state is signiﬁcantly larger than that of the\ntensor-product states, and compound semantic meaning lying\nwithin the N-gram is mainly captured by the entanglement\nrepresentation. Then the parameters of the entangled states is\ntrained by the EE module, which characterizes the features of\nentanglement between the word states within the sequence.\nThe EE module is trained to capture high-level features\nbetween the words in the N-gram, while the word embedding\nmodule can focus on encoding the basic semantic meanings of\n\u0013\u000eHSBN\n \n \n  \nFig. 3. Pipeline of the EE module for a 3-word sequence. ⊗denotes the\ntensor product of input vectors.\nindividual words with reduced dimension. The clear separation\nof duties in the hierarchical structure of the neural network\nincreases the transparency of the model to an unprecedented\nlevel, which allows an accurate interpretation of the intermedi-\nate states using the tools borrowed from quantum information\ntheory.\nIn line with the previous works, the complex-valued word\nembedding module is used to transform the one-hot rep-\nresentation of the word into a quantum pure state in the\nHilbert space of word states Hw, expressed as a state vector\n|ψ⟩= [ α0 ··· αi ···]T, with ∑\ni|αi|2 = 1 . The general\nquantum pure state for describing a word sequence is deﬁned\non the tensor-product Hilbert space Hs := ⊗i(Hw)i, and can\nbe formulated as\n|ψs⟩=\n∑\ni1,...,iN\nβi1...iN |i1⟩⊗···⊗| iN⟩, (17)\nwhere ∑\ni1,...,iN\n|βi1...iN |2 = 1 . |ψs⟩is used to characterize\nthe probability distribution of the sememes. If the word\nembedding dimension is D, then a general pure state vector\ndeﬁned on the tensor product of N Hilbert spaces contains\nDN elements.\nThe EE module starts with composing the contiguous se-\nquences of words as N-grams [42]; see Fig. 3 for an example.\nFor each N-gram, a separable pure state is generated as the\ntensor product of its word states taking the following form\n|ψss⟩= |ψ1⟩⊗···⊗| ψN⟩. (18)\nThe word states are deﬁned on Hilbert spaces which are\nisomorphic to each other, with the same semantic basis states.\nFor this reason, N-grams are deﬁned on the same tensor-\nproduct space before entering the EE module. The complex-\nvalued EE module is then connected to transform the initial\nseparable state to an unnormalized vector |˜ψs⟩ = [ ˜βi1...iN ]\nwhich will then be normalized to determine a general pure\nstate |ψs⟩= [βi1...iN ] in the form of (17). The transformation\ninduced by the NN can be formally written as\n|˜ψs⟩= W|ψss⟩, (19)\nwhere Wis the weight matrix. Then the output vector must\nbe normalized by\nβi1...iN =\n˜βi1...iN∑\ni1,...,iN\n|βi1...iN |2 (20)\nto be consistent with quantum theory. Note that the operations\n(19)-(20) induce an endomorphism of this Hilbert space and\nthe EE module transfers one pure state to another. The pure\n6\nFig. 4. The structure of QLM-EE.\nstates are unit vectors in the tensor-product Hilbert space. Any\nunit vectors in the same Hilbert space can be connected by a\nunitary matrix which induces a rotation. It is known that for\nany two quantum pure states |a⟩and |b⟩of an N-qubit register,\nthere exists a gate sequence to physically realize the unitary\nmatrix U such that |b⟩= U|a⟩[43]. Since the model proposed\nin this paper is only quantum-inspired, we are using a linear\nmatrix multiplication and a normalization layer to approximate\nthe effect of the transformation matrix by the classical way of\ntraining. In principle, if one layer of linear matrix multipli-\ncation is not sufﬁcient for learning the transformation, it is\nalways possible to stack several layers of linear operations to\naccurately approximate any transformation, which is consistent\nwith the traditional neural network theory.\nWe take Fig. 3 as an example to illustrate the working\nmechanism of the EE module. We denote the state vectors\nfor the ﬁrst two words as [α1 α2]T and [α3 α4]T. The two\nword states form the separable state as the input to the NN\nlayer by the following tensor product\n[ α1\nα2\n]\n⊗\n[ α3\nα4\n]\n=\n\n\nα1α3\nα1α4\nα2α3\nα2α4\n\n. (21)\nThe output of the NN layer is an unnormalized vector\n[ ˜β00 ˜β01 ˜β10 ˜β11]T. After normalization, we obtain\n\n\nβ00\nβ01\nβ10\nβ11\n\n, (22)\nwhich is in the most general form of the state vector on the\njoint Hilbert space. If [β00 β01 β10 β11]T cannot be written in\na decomposable form just like the RHS of (21), then the output\nvector is a representation for an entangled state which captures\nthe non-classical correlations between the word states.\nIV. M ODEL\nThe structure of QLM-EE for QA is shown in Fig. 4. It\nis a complex-valued, end-to-end neural network optimized by\nback-propagation on classical computing devices. The QLM-\nEE can be divided into three major steps.\n• Word embedding and entanglement embedding. The word\nembedding module generates the word states, and the\nentanglement embedding module generates the complete\nquantum probabilistic description of the sequence of\nword states. The complete probabilistic description is\ngiven by a generic quantum pure state vector, which\nmay be entangled to encode the information on the non-\nclassical correlations between the word states. Entangle-\nment embedding and word embedding modules encode\nthe states as feature vectors at different levels, which\nimproves the transparency of the quantum probabilistic\nmodelling process. In particular, the distance between the\nfeature vectors in the embedding space can be calculated\nbased on the well-established measures from the quantum\ninformation theory for comparing quantum states, which\ncould be used to reveal the relations between the words\nand phrases on a deeper semantic level. For example, in\nthe classical Word2Vec, the cosine similarity is deﬁned\non real-valued embedded vectors as\nC(⃗ a,⃗b) = ⃗ a·⃗b\n||⃗ a||·||⃗b||\n. (23)\nIn our case, the cosine similarity is deﬁned on the\ncomplex-valued unit vectors as\nC(⃗ a,⃗b) = ⃗ a†⃗b\n||⃗ a||·||⃗b||\n= ⃗ a†⃗b. (24)\nIn general, C(⃗ a,⃗b) is a complex number, which means\nthat the state representations could differ by a complex\nphase factor. Since complex phases are hard to visualize,\n7\nA⊗B\nsolar\nsystem system\nparty ordered\ncommittee\nsolar sun\ncrazydominated\nA\nB\nsolar \u0001system\nthe \u0001sun\nunited \u0001nations\nthe \u0001comet\nEntanglement \u0001\nEmbedding\nWord \u0001\nEmbedding\nWord \u0001\nEmbedding in\u0001fact\nsuch\u0001as\nFig. 5. The forward process for the phrase solar system. The distance between state vectors is deﬁned by the quantum ﬁdelity. The ﬁdelities of ( solar system,\nunited nations), (solar system, the comet), (solar system, the sun) are 0.523, 0.365, 0.335, respectively, while the ﬁdelities of randomly selected combinations\n(solar system, in fact), (solar system, such as) are 0.026, 0.015, respectively.\nwe employ the quantum ﬁdelity measure to compare the\nwords and word sequences.\nIn Fig. 5, we visualize the process how the state of\nthe phrase solar system evolves in our model using the\nquantum ﬁdelity measure. After the word embedding\nlayer, the state vector that represents the word solar is\nclose to {sun, crazy, dominated}, which reﬂects how the\nmodel comprehends the meaning of the input words. The\nword state vector of system is embedded close to {party,\ncommittee, ordered}. After the EE module, the phrase\nsolar system can be linked to other high-level phrases,\ne.g., united nations , while its state vector is still very\nclose to the phrase the sun and the comet which share a\nsimilar meaning.\n• Measurement operation. Projective measurement opera-\ntion is equivalent to the calculation of ﬁdelity between\nquantum states. That is, the output of the measurement\ncan be seen as a distance between the measured state and\na measuring state, corresponding to the distance between\nthe semantic entangled state and a measuring sememe\nwhich is used for comparison. Therefore, performing\nthe same set of measurement operations provides a way\nto compare the semantic similarity of the question and\nanswering word sequences. After entanglement embed-\nding, a series of parameterized measurements {|mi⟩}are\nperformed on the state |ψs⟩via the formula\npi(|ψs⟩) = |⟨mi|ψs⟩|2. (25)\nHere pi is deﬁned as the measurement output, which\nindicates the probability of the state |ψs⟩possessing the\nsemantic meaning represented by the measurement vector\n|mi⟩. Note that the unit vectors {|mi⟩}are optimized in\na data-driven way. By using pure states as measurement\nbasis, the computation cost for measuring a quantum\nstate virtually is reduced from O(n3) to O(n) compared\nto CNM [34], in which density matrices were used as\nthe measurement basis. For a sentence described by the\nconcatenation of L word sequences, the feature matrix\nwhich stores all the measurement results has L ×M\nentries if M measurement vectors are used.\n• Similarity Measure. A max-pooling layer is applied on\neach row of the feature matrix for down-sampling. To\nTABLE I\nSTATISTICS OF THE DATASETS\nDataset Train(Q/A) Dev(Q/A) Test(Q/A)\nTREC-QA 1,229/53,417 65/1,134 68/1,478\nWIKIQA 873/8,627 126/1,130 243/2,351\nbe more speciﬁc, the down-sampling takes the maximum\nvalue of each row of the matrix to form a reduced\nfeature vector. Then a vector-based similarity metric can\nbe employed in the matching layer for evaluating the\ndistance between the pair of feature vectors for question\nand answer. The answer with the highest matching score\nis chosen as the predicted result among all candidate\nanswers.\nV. E XPERIMENT\nA. Experiment Details\n1) Dataset: We conduct the experiments on two benchmark\ndatasets for QA, namely TREC-QA [44] and WIKIQA [45].\nTREC-QA is used in the Text REtrieval Conference. WIKIQA\nis an open-domain QA dataset released by Microsoft Research.\nThe statistics of the datasets are given in TABLE I.\n2) Evaluation metrics: The metrics called Mean Average\nPrecision (MAP) and Mean Reciprocal Rank (MRR) [46] are\nutilized to evaluate the performance of the models. MAP for\na set of queries Qis the mean of the Average Precision scores\nAveP(q) for each query, formulated as\nMAP =\n|Q|∑\nq=1\nAveP(q)/|Q|.\nMRR is the average of the Reciprocal Ranks of results for a\nsample of queries Q, calculated as\nMRR = 1\n|Q|\n|Q|∑\ni=1\n1/ranki,\nwhere ranki refers to the rank position of the ﬁrst relevant\ndocument for the i-th query.\n8\n3) Baselines:\na) Classical models for TREC-QA including\n• Unigram-CNN [47]: Unigram-CNN is a CNN-based\nmodel that utilizes 1-gram as the input to obtain the\nrepresentations of questions and answers for compari-\nson. It is composed of one convolutional layer and one\naverage pooling layer.\n• Bigram-CNN [47]: Bigram-CNN has the same network\nstructure as Unigram-CNN but it extracts the represen-\ntations from bi-gram inputs.\n• ConvNets [48]: ConvNets is built upon two distribu-\ntional sentence models based on CNN. These underly-\ning sentence models work in parallel to map questions\nand answers to their distributional vectors, which are\nthen used to learn the semantic similarity between\nthem.\n• QA-LSTM-avg [49]: QA-LSTM-avg generates dis-\ntributed representations for both the question and an-\nswer independently by bidirectional LSTM outputs\nwith average pooling, and then utilizes cosine simi-\nlarity to measure their distance.\n• aNMM-1 [50]: aNMM-1 employs a deep neural net-\nwork with value-shared weighting scheme in the ﬁrst\nlayer, which is followed by fully-connected layers to\nlearn the sentence representation. A question attention\nnetwork is used to learn question term importance and\nproduce the ﬁnal ranking score.\nb) Classical models for WIKIQA including\n• Bigram-CNN [47];\n• PV-Cnt [45]: PV-Cnt is the Paragraph Vector (PV)\nmodel combined with Word Count. The model score of\nPV is the cosine similarity score between the question\nvector and the sentence vector.\n• CNN-Cnt [45]: CNN-Cnt employs a Bigram-CNN\nmodel with average pooling and combines it with Word\nCount.\n• QA-BILSTM [51]: QA-BILSTM uses a bidirectional\nLSTM and a max pooling layer to obtain the represen-\ntation of questions and answers, and then computes the\ncosine similarity between the two representations.\n• LSTM-attn [52]: LSTM-attn ﬁrstly obtains the repre-\nsentations for the question and answer from indepen-\ndent LSTM models, and then adds an attention model\nto learn the pair-speciﬁc representation for prediction\non the basis of the vanilla LSTM.\nc) Quantum models for TREC-QA and WIKIQA including\nQLM-MLE [29], NNQLM-I, NNQLM-II [32], QMWF-\nLM [33], and CNM [34]. These models have been brieﬂy\nintroduced in Section II.E.\n4) Hyper-parameters: Cosine similarity deﬁned by (23) is\nused as the distance metric between the real-valued feature\nvectors after pooling. The hinge loss [53] for training the\nmodel is given by\nL= max{0,0.1 −C+ + C−}, (26)\nwhere C+ is the cosine similarity of a ground truth answer,\nC− is the cosine similarity of an incorrect answer randomly\n(a)\n(b)\nWord Embedding Dimension\nMAP\nMRR\nFig. 6. Experimental results of models with different N and different word\nembedding dimensions on TREC-QA. N = 1,2,3 refers to the model in\nwhich 1-gram, 2-gram and 3-gram entanglement embeddings are applied in\nparallel on the questions and answers, and the feature vectors are concatenated\nas a single vector for comparison.\nTABLE II\nEXPERIMENTAL RESULTS ON TREC-QA AND WIKIQA\nTREC-QA WIKIQA\nModel MAP MRR Model MAP MRR\nUnigram-CNN 0.5470 0.6329 Bigram-CNN 0.6190 0.6281\nBigram-CNN 0.5693 0.6613 PV-Cnt 0.5976 0.6058\nConvNets 0.6709 0.7280 CNN-Cnt 0.6520 0.6652\nQA-LSTM-avg 0.6819 0.7652 QA-BILSTM 0.6557 0.6695\naNMM-1 0.7385 0.7995 LSTM-attn 0.6639 0.6828\nQLM-MLE 0.6780 0.7260 QLM-MLE 0.5120 0.5150\nNNQLM-I 0.6791 0.7529 NNQLM-I 0.5462 0.5574\nNNQLM-II 0.7589 0.8254 NNQLM-II 0.6496 0.6594\nQMWF-LM 0.7520 0.8140 QMWF-LM 0.6950 0.7100\nCNM 0.7701 0.8591 CNM 0.6748 0.6864\nQLM-EE 0.7713 0.8542 QLM-EE 0.6956 0.7003\nchosen from the entire answer space.\nThe parameters in the QLM-EE are determined by the set of\nhyper-parameters Θ := {N,D,M }, where N is the number\nof words in a sequence, D is the word embedding dimension\nand M is the number of measurement vectors. N = 1 means\nembedding the words without composition, and in this case no\nentanglement can be generated. We test single-layer and two-\nlayer fully connected neural networks with {128,256,512}\nneurons for entanglement embedding. A grid search is con-\nducted using N ∈ {1,2,3}, D ∈ {6,8,10,12,14,16},\nM ∈{500,1000,2500,5000}, batch size in {16,32,64}and\nlearning rate in {0.01,0.1,0.5}. In line with CNM, the con-\ncatenation of the feature vectors for N = 1,2,3 has also been\ntested. Larger N has been tried, but N ∈{1,2,3}shows better\nperformance than N ≥4. All the parameters are initialized\n9\nTABLE III\nABLATION RESULTS ON TREC-QA. QLM WITH SEPARABLE -STATE EMBEDDING (QLM-SE) AND QLM WITH MIXED -STATE EMBEDDING (QLM-ME)\nARE COMPARED WITH QLM-EE. QLM-EE-R EAL IS THE REAL -VALUED QLM-EE.\nModel D N MAP MRR Params. FLOPs\nQLM-EE 8 2 0.7302±0.0101 0.8071±0.0156 1.45M 4.20M\nQLM-ME 8 2 0.6905 ±0.0711 0.7584 ±0.0059 0.99M 4.81M\nQLM-ME 64 2 0.7267 ±0.0194 0.8171±0.0151 7.88M 124.56M\nQLM-EE-Real 16 2 0.6778 ±0.0139 0.7505 ±0.0187 1.97M 6.01M\nQLM-SE 8 2 0.6934 ±0.0149 0.7804 ±0.0156 1.32M 3.28M\nfrom standard normal distributions except the measurement\nvectors, which are initialized by orthogonal vectors.\nB. Performance\nThe experimental results on TREC-QA and WIKIQA are\nshown in TABLE II. We compare the performances of the\nclassical models, including CNNs, Recurrent NNs and atten-\ntion models with the performances of QLMs. QLM-EE is\nconsistently better than all the QLMs and classical models on\nboth datasets if MAP is used as the metric. If we use MRR\nas the metric, QLM-EE performs slightly worse than CNM\non TREC-QA while better than the other models, and slightly\nworse than QMWF-LM while better than all the other models\non WIKIQA.\nOur word embedding dimension is selected from D ∈\n{6,8,10,12,14,16}, which is much smaller than the word\nembedding dimension of previous QLMs selected from\n{50,100,200}. As a consequence, the amount of parameters\nfor the word embedding layer has seen dramatic reduction\nwhile the performance of model has been improved. Fig. 6\nillustrates how the word embedding dimension affects the\nperformances of different models in terms of MAP and MRR\non TREC-QA. It is clear that the model with the concatenation\nof the feature vectors for N = 1,2,3 performs best and the\nbest word embedding dimension is 8, which is signiﬁcantly\nsmaller than {50,100,200}for the previous QLMs.\nAs D increases, the dimension of the state vector after\nthe entanglement embedding is increased according to the\nformula DN. As pointed out in the literature [54], learning\nword embedding has the risk of underﬁtting or overﬁtting.\nEmbedding dimension that is too small (less than 50) or\ntoo large (more than 300) will degrade the performance. In\nQLM-EE, the N-gram lies in the tensor-product space, which\ntends to result in a high-dimensional representation and poor\nperformance. For example, if the word embedding dimension\nis 8, then 3-grams are described by complex-valued 512-\ndimensional vectors, or real-valued 1024-dimensional vectors.\nIn addition, the dimension of EE module and measurement\nvectors will increase accordingly, leading to a model that easily\noverﬁts the data. In our case, the performance of 3-gram model\nis worse than that of the 2-gram model, which is a sign of\noverﬁtting in this relatively small-sized dataset. However, it is\npossible that high-dimensional representation will improve the\nperformance if a much larger dataset is considered.\nTABLE IV\nTHE TWO -SAMPLE K-S TEST STATISTICS kAT pSIGNIFICANCE LEVEL\nBETWEEN THE PERFORMANCE METRICS OF QLM-EE ( D= 8) AND\nOTHER MODELS IN ABLATION TEST . k= 1INDICATES THAT THE TWO\nSETS OF METRIC DATA ARE SAMPLED FROM DIFFERENT DISTRIBUTIONS .\nModel kMAP pMAP kMRR pMRR\nQLM-ME (D= 8) 1 0.17% 1 0.02%\nQLM-ME (D= 64) 1 3.18% 1 6.75%\nQLM-SE (D= 8) 1 0.01% 1 0.02%\nQLM-EE-Real (D= 8) 1 1.12% 1 1.18%\nC. Ablation Test\nAblation test studies the contribution of certain components\nto the overall performance by removing these components\nfrom the model. We conduct ablation tests to evaluate the\neffectiveness of entanglement embedding on 2-gram QLM.\nQLM-SE has removed thxe entanglement embedding module\nand measurements are directly performed on the separable\njoint states. QLM-ME generates the mixed-state embedding by\n(16), and measurements are performed on the density matrix\nρ. In the setting of complex-valued QLM-EE, we let D = 8\nand M = 3000 . For the complex-valued QLM-ME whose\nstructure is the same as CNM, we set D = 8 and D = 64\nwith M = 3000 for comparison. Note that when D = 64 ,\nthe complex-valued QLM-ME has the same dimension for\n2-grams as QLM-EE. However, since the word embedding\ndimension has been increased, the number of parameters for\nthe word embedding module has been increased accordingly.\nWe have doubled the word embedding dimension to D = 16\nin the real-valued QLM-EE (QLM-EE-Real), and thus the\ndimension of 2-grams is 256 which is four times larger than\nthe dimension of 2-grams in the complex-valued QLM-EE.\nBesides, the dimension of the measurement vectors has been\nincreased four times accordingly. To make the number of\nparameters compatible, we set M = 1500 for this case.\nWe have run the experiment 10 times. The mean and\nstandard deviation of metrics, the number of parameters and\nFloating Point Operations (FLOPs) are reported in TABLE III.\nWe can see that QLM-EE achieves the best performance for\n2-gram model when the word embedding dimension is 8. The\nperformances QLM-ME with D = 64 and QLM-EE with\nD = 8 are close, but the number of parameters and FLOPs\nused by the former is signiﬁcantly greater than the latter.\nThe result of QLM-EE-Real conﬁrms that the complex-valued\nneural network indeed promotes the performance of QLMs.\n10\nFig. 7. Entanglement entropy of the 2-grams in the sentences. Darker color is an indication of larger entanglement between adjacent words.\nTABLE V\nSELECTED ENTANGLED 2-GRAMS IN TREC-QA\nType Word combinations\nMost entangled\nin questions\nannual revenue; as a; is cataracts; how long;\ntale of; ﬁrst movie; ethnic background\nLeast entangled\nin questions\nintroduced Jar; the name; what year; the main;\nwho is; whom were ; is a; in what\nMost entangled\nin answers\nends up; never met; plane assigned; in kinder-\ngarten; academy of; going to; agricultural farm-\ning; secure defendants\nLeast entangled\nin answers\nskinks LRB; while some; grounded in;\nof Quarry; of seven; he said; in China;\nresponsibility and\nThe two-sample Kolmogorov-Smirnov (K-S) test result on the\nperformance metrics is shown in TABLE IV, which veriﬁes\nthe performance differences of these models.\nVI. P OST-HOC INTERPRETABILITY\nvon Neumann entanglement entropy S[55] is an accurate\nmeasure of the degree of quantum entanglement for a bipartite\nquantum pure state. The entanglement entropy is calculated as\nfollows\nS= −\nK∑\ni=1\n|λi|2log(|λi|2), (27)\nwhere λi is the Schmidt coefﬁcient of the composite pure\nstate and K is the minimal dimension of the subsystems, i.e.,\nK = min(dim( HA),dim(HB)). Apart from the analytical\nentanglement measure (27) for bipartite states, it is also worth\nmentioning that efﬁcient numerical methods are available for\nquantifying quantum entanglement for multipartite states [56].\nTABLE V shows the selected most and least 2-grams of\nwords, ranked by the von Neumann entanglement entropy.\nThe most entangled pairs are mostly set phrase or some well-\nknown combinations of words, e.g., how long . However, is\ncataracts is clearly not a set phrase or well-known combination\nof words. We found that is cataracts appears many times,\nTABLE VI\nSELECTED ENTANGLED 3-GRAMS IN TREC-QA\nType Word combinations\nMost entangled\nin questions\nthe company Rohm; Hale Bopp comet; how of-\nten does; Insane Clown Posse; Capriati ’s coach\nLeast entangled\nin questions\nthere worldwide ?; What is Crips; Criminal\nCourt try; When did James\nMost entangled\nin answers\nafter a song; comet ’s nucleus; Out of obligation;\nat times .; Black Panther Party\nLeast entangled\nin answers\nQueen , Tirado; came from Britain; Nobel Prize\nlast; appears in the; Americans over 50\nand cataracts is always next to is in this particular training\ndataset. This may be the reason why the learned model takes is\ncataracts as a ﬁxed combination of words. The least entangled\npairs consist of words with ﬁxed semantic meaning such as\nnames {Quarry, China}and interrogatives {what, who}, some\nof which appear only once in the dataset. In other words,\nthere is not so much semantic ambiguity or superposition\nwith these combinations that demands a quantum probabilistic\ninterpretation.\nTABLE VI shows the selected most and least entangled\n3-grams, whose von Neumann entanglement entropies are\ncalculated to indicate the entanglement between the ﬁrst two\nwords, and the remaining word (the third word). Similar to\n2-grams, the most entangled 3-grams are ﬁxed collocations\nand combinations that often appear together in the training\nset, such as a set phrase. Interestingly, punctuation marks\nalso appear in the most entangled 3-grams. For example,\nentanglement in at times . is large, which implies that the\nphrase at times is often at the end of the answers. However,\nthere worldwide ? is among the least entangled combinations,\nsince there worldwide is not in any of the question sentences\nfor training. The third word in the least entangled 3-grams are\nmainly names and numbers, which cannot combine with the\nﬁrst two words to form a ﬁxed phrase.\nIn Fig. 7, we also visualize the entanglement entropy\n11\nin some selected sentences, where the degree of darkness\nindicates the level of entanglement. It can be seen that words\nwith multiple meanings in different contexts, e.g., {is, does,\nthe, ﬁlm}, have greater capabilities to entangle with their\nneighboring words.\nVII. C ONCLUSION\nIn this paper, we proposed an interpretable quantum-\ninspired language model with a novel EE module in the neural\nnetwork architecture. The EE enables the modelling of the\nword sequence by a general quantum pure state, which is\ncapable of capturing all the classical and non-classical corre-\nlations between the word states. The expressivity of the neural\nnetwork is greatly enhanced by cascading the word embedding\nand EE modules. The complex-valued model has demonstrated\nsuperior performance on the QA datasets, with much smaller\nword embedding dimensions compared to previous QLMs. In\naddition, the non-classical correlations between the word states\ncan be quantiﬁed and visualized by appropriate entanglement\nmeasures, which improves the post-hoc interpretability of the\nlearned model.\nThe future plan is to apply the adaptive methods [57], [58]\nfor optimizing the virtual measurement operations to increase\nthe efﬁciency in feature extraction. The QLM-EE model is\nexpected to be more powerful on huge datasets in which the\nsemantic meanings of words and their correlations are far more\ncomplex. With a larger dataset and richer semantic superpo-\nsitions between the words, several entanglement embedding\nmodules can be cascaded to form a deeper neural network,\nwhich could encode the multipartite correlations within the\ntext at different scales.\nREFERENCES\n[1] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\nbilistic language model,” J. Mach. Learn. Res. , vol. 3, pp. 1137–1155,\n2003.\n[2] L. Wang, X. Qian, Y . Zhang, J. Shen, and X. Cao, “Enhancing sketch-\nbased image retrieval by cnn semantic re-ranking,”IEEE Trans. Cybern.,\nvol. 50, no. 7, pp. 3330–3342, 2020.\n[3] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock`y, and S. Khudanpur,\n“Extensions of recurrent neural network language model,” in Proc. IEEE\nICASSP, May 2011, pp. 5528–5531.\n[4] T. Mikolov, G. Corrado, K. Chen, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” in Proc. ICLR, 2013.\n[5] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” in Proc. NIPS, Dec. 2014, pp. 3104–3112.\n[6] H. He and J. Lin, “Pairwise word interaction modeling with deep neural\nnetworks for semantic similarity measurement,” in Proc. NAACL HLT ,\nJun. 2016, pp. 937–948.\n[7] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the usages\nof deep learning for natural language processing,” IEEE Trans. Neural\nNetw. Learn. Syst. , vol. 32, no. 2, pp. 604–624, 2021.\n[8] X. Wang, L. Kou, V . Sugumaran, X. Luo, and H. Zhang, “Emotion\ncorrelation mining through deep learning models on natural language\ntext,” IEEE Trans. Cybern. , pp. 1–14, 2020.\n[9] J. Du, C. M. V ong, and C. L. P. Chen, “Novel efﬁcient rnn and lstm-\nlike architectures: Recurrent and gated broad learning systems and their\napplications for text classiﬁcation,” IEEE Trans. Cybern. , pp. 1–12,\n2020.\n[10] X. Wang, L. Kou, V . Sugumaran, X. Luo, and H. Zhang, “Emotion\ncorrelation mining through deep learning models on natural language\ntext,” IEEE Trans. Cybern. , vol. 51, no. 9, pp. 4400–4413, 2021.\n[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their composi-\ntionality,” in Proc. NIPS, Dec. 2013, pp. 3111–3119.\n[12] F. Doshi-Velez and B. Kim. (2017) Towards A Rigorous Science of\nInterpretable Machine Learning. [Online]. Available: https://arxiv.org/\nabs/1702.08608\n[13] Z. C. Lipton, “The mythos of model interpretability,” Commun. ACM,\nvol. 61, pp. 36–43, 2018.\n[14] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum\nInformation. Cambridge University Press, 2010.\n[15] D. Dong, X. Xing, H. Ma, C. Chen, Z. Liu, and H. Rabitz, “Learning-\nbased quantum robust control: Algorithm, applications, and experi-\nments,” IEEE Trans. Cybern. , vol. 50, pp. 3581–3593, 2020.\n[16] V . Dunjko and H. J. Briegel, “Machine learning & artiﬁcial intelligence\nin the quantum domain: a review of recent progress,” Rep. Prog. Phys.,\nvol. 81, no. 7, p. 074001, 2018.\n[17] C. Chen, D. Dong, B. Qi, I. R. Petersen, and H. Rabitz, “Quantum\nensemble classiﬁcation: A sampling-based learning control approach,”\nIEEE Trans. Neural Netw. Learn. Syst. , vol. 28, no. 6, pp. 1345–1359,\n2016.\n[18] C. Chen, D. Dong, H.-X. Li, J. Chu, and T.-J. Tarn, “Fidelity-based\nprobabilistic Q-learning for control of quantum systems,” IEEE Trans.\nNeural Netw. Learn. Syst. , vol. 25, no. 5, pp. 920–933, 2013.\n[19] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and\nS. Lloyd, “Quantum machine learning,” Nature, vol. 549, no. 7671, pp.\n195–202, 2017.\n[20] Y . Du, M.-H. Hsieh, T. Liu, S. You, and D. Tao. (2020) On\nthe learnability of quantum neural networks. [Online]. Available:\nhttps://arxiv.org/abs/2007.12369\n[21] V . Havl ´ıˇcek, A. D. C ´orcoles, K. Temme, A. W. Harrow, A. Kandala,\nJ. M. Chow, and J. M. Gambetta, “Supervised learning with quantum-\nenhanced feature spaces,”Nature, vol. 567, no. 7747, pp. 209–212, 2019.\n[22] J. R. McClean, S. Boixo, V . N. Smelyanskiy, R. Babbush, and H. Neven,\n“Barren plateaus in quantum neural network training landscapes,” Nat.\nCommun., vol. 9, no. 1, pp. 1–6, 2018.\n[23] N. H. Nguyen, E. Behrman, M. A. Moustafa, and J. Steck, “Benchmark-\ning neural networks for quantum computations,” IEEE Trans. Neural\nNetw. Learn. Syst. , vol. 31, pp. 2522–2531, 2020.\n[24] J.-A. Li, D. Dong, Z. Wei, Y . Liu, Y . Pan, F. Nori, and X. Zhang,\n“Quantum reinforcement learning during human decision-making,” Nat.\nHum. Behav., vol. 4, no. 3, pp. 294–307, 2020.\n[25] V . Dunjko, J. M. Taylor, and H. J. Briegel, “Quantum-enhanced machine\nlearning,” Phys. Rev. Lett., vol. 117, no. 13, p. 130501, 2016.\n[26] Q. Wei, H. Ma, C. Chen, and D. Dong, “Deep reinforcement learning\nwith quantum-inspired experience replay,” IEEE Trans. Cybern., pp. 1–\n13, 2021.\n[27] B. Coecke, M. Sadrzadeh, and S. Clark. (2010) Mathematical\nfoundations for a compositional distributional model of meaning.\n[Online]. Available: https://arxiv.org/abs/1003.4394\n[28] W. Zeng and B. Coecke. (2016) Quantum algorithms for compositional\nnatural language processing. [Online]. Available: https://arxiv.org/abs/\n1608.01406\n[29] A. Sordoni, J.-Y . Nie, and Y . Bengio, “Modeling term dependencies with\nquantum language models for ir,” in Proc. 36th Int. ACM SIGIR Conf.\nRes. Develop. Inf. Retr. , Jul. 2013, pp. 653–662.\n[30] A. Sordoni, Y . Bengio, and J.-Y . Nie, “Learning concept embeddings\nfor query expansion by quantum entropy minimization,” in Proc. 28th\nAAAI Conf. Artif. Intell. , Jul. 2014, pp. 1586–1592.\n[31] I. Basile and F. Tamburini, “Towards quantum language models,” in\nProc. EMNLP, Sep. 2017, pp. 1840–1849.\n[32] P. Zhang, J. Niu, Z. Su, B. Wang, L. Ma, and D. Song, “End-to-end\nquantum-like language models with application to question answering,”\nin Proc. 32nd AAAI Conf. Artif. Intell. , Feb. 2018, p. 5666–5673.\n[33] P. Zhang, Z. Su, L. Zhang, B. Wang, and D. Song, “A quantum many-\nbody wave function inspired language modeling approach,” inProc. 27th\nACM CIKM, Oct. 2018, pp. 1303–1312.\n[34] Q. Li, B. Wang, and M. Melucci, “Cnm: An interpretable complex-\nvalued network for matching,” in Proc. NAACL-HLT, Jun. 2019, pp.\n4139–4148.\n[35] R. Horodecki, P. Horodecki, M. Horodecki, and K. Horodecki, “Quan-\ntum entanglement,” Rev. Mod. Phys., vol. 81, no. 2, p. 865, 2009.\n[36] M. Xie, Y . Hou, P. Zhang, J. Li, W. Li, and D. Song, “Modeling quantum\nentanglements in quantum language models,” in Proc. IJCAI, Jul. 2015,\npp. 1362–1368.\n[37] B. Wang, D. Zhao, C. Lioma, Q. Li, P. Zhang, and J. G. Simonsen,\n“Encoding word order in complex embeddings,” in Proc. ICLR , Dec.\n2019.\n[38] C. J. van Rijsbergen, The geometry of information retrieval. Cambridge\nUniversity Press, 2004.\n12\n[39] D. Kartsaklis and M. Sadrzadeh. (2014) A study of entanglement\nin a categorical framework of natural language. [Online]. Available:\nhttps://arxiv.org/abs/1405.2874\n[40] S. Uprety, D. Gkoumas, and D. Song, “A survey of quantum theory\ninspired approaches to information retrieval,” ACM Computing Surveys\n(CSUR), vol. 53, no. 5, pp. 1–39, 2020.\n[41] D. Gkoumas, Q. Li, S. Dehdashti, M. Melucci, Y . Yu, and D. Song,\n“Quantum cognitively motivated decision fusion for video sentiment\nanalysis,” in Proc. of 35th AAAI Conf. Artif. Intell. , Feb. 2021, pp. 827–\n835.\n[42] B. C. William and M. T. John, “N-gram-based text categorization,” Ann\nArbor MI, vol. 48113, no. 2, pp. 161–175, 1994.\n[43] V . J. J. B. V . Mottonen, M. and M. M. Salomaa, “Transformation of\nquantum states using uniformly controlled rotations,” Quantum Infor-\nmation and Computation , vol. 5, no. 6, pp. 467–473, 2005.\n[44] E. M. V oorhees and D. M. Tice, “Building a question answering test\ncollection,” in Proc. 23rd Int. ACM SIGIR Conf. Res. Develop. Inf. Retr.,\nJul. 2000, pp. 200–207.\n[45] Y . Yang, W.-t. Yih, and C. Meek, “Wikiqa: A challenge dataset for open-\ndomain question answering,” in Proc. EMNLP , Sep. 2015, pp. 2013–\n2018.\n[46] C. D. Manning, P. Raghavan, and H. Sch ¨utze, Introduction to Informa-\ntion Retrieval. Cambridge University Press, 2008.\n[47] L. Yu, K. M. Hermann, P. Blunsom, and S. Pulman, “Deep learning\nfor answer sentence selection,” in Proc. NIPS deep learning workshop ,\n2014.\n[48] A. Severyn and A. Moschitti, “Learning to rank short text pairs with\nconvolutional deep neural networks,” in Proc. 38th Int. ACM SIGIR\nConf. Res. Develop. Inf. Retr. , Aug. 2015, pp. 373–382.\n[49] M. Tan, C. d. Santos, B. Xiang, and B. Zhou, “LSTM-based deep\nlearning models for non-factoid answer selection,” in Proc. ICLR, 2016.\n[50] L. Yang, Q. Ai, J. Guo, and W. B. Croft, “aNMM: Ranking short answer\ntexts with attention-based neural matching model,” in Proc. ACM CIKM,\nOct. 2016, pp. 287–296.\n[51] C. d. Santos, M. Tan, B. Xiang, and B. Zhou. (2016) Attentive Pooling\nNetworks. [Online]. Available: https://arxiv.org/abs/1602.03609\n[52] Y . Miao, L. Yu, and P. Blunsom, “Neural variational inference for text\nprocessing,” in Proc. ICML, Jun. 2016, pp. 1727–1736.\n[53] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network\narchitectures for matching natural language sentences,” in Proc. NIPS,\nDec. 2014, pp. 2042–2050.\n[54] Z. Yin and Y . Shen, “On the dimensionality of word embedding,” in\nProc. NIPS, Dec. 2018, pp. 895–906.\n[55] R. Horodecki and P. Horodecki, “Quantum redundancies and local\nrealism,” Phys. Lett. A , vol. 194, no. 3, pp. 147–152, 1994.\n[56] M. Zhang, G. Ni, and G. Zhang, “Iterative methods for computing\nu-eigenvalues of non-symmetric complex tensors with application in\nquantum entanglement,” Computational Optimization and Applications ,\nvol. 75, no. 3, pp. 779–798, 2020.\n[57] F. Husz ´ar and N. M. Houlsby, “Adaptive bayesian quantum tomography,”\nPhys. Rev. A, vol. 85, no. 5, p. 052120, 2012.\n[58] B. Qi, Z. Hou, Y . Wang, D. Dong, H.-S. Zhong, L. Li, G.-Y . Xiang,\nH. M. Wiseman, C.-F. Li, and G.-C. Guo, “Adaptive quantum state\ntomography via linear regression estimation: Theory and two-qubit\nexperiment,” NPJ Quantum Inf. , vol. 3, no. 1, pp. 1–7, 2017.",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.7596545219421387
    },
    {
      "name": "Quantum entanglement",
      "score": 0.7142705917358398
    },
    {
      "name": "Computer science",
      "score": 0.6361873745918274
    },
    {
      "name": "Word (group theory)",
      "score": 0.5223796963691711
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4855463206768036
    },
    {
      "name": "Language model",
      "score": 0.48123764991760254
    },
    {
      "name": "Embedding",
      "score": 0.429132878780365
    },
    {
      "name": "Quantum",
      "score": 0.41619592905044556
    },
    {
      "name": "Algorithm",
      "score": 0.34259647130966187
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.332334965467453
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28993427753448486
    },
    {
      "name": "Mathematics",
      "score": 0.2702937722206116
    },
    {
      "name": "Quantum mechanics",
      "score": 0.21686193346977234
    },
    {
      "name": "Physics",
      "score": 0.13853371143341064
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391767838",
      "name": "State Key Laboratory of Industrial Control Technology",
      "country": null
    },
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I188329596",
      "name": "University of Canberra",
      "country": "AU"
    }
  ],
  "cited_by": 40
}