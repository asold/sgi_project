{
  "title": "Are Pretrained Convolutions Better than Pretrained Transformers?",
  "url": "https://openalex.org/W3170719901",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112331270",
      "name": "Mostafa Dehghani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2550488236",
      "name": "Jai Prakash Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3135710007",
      "name": "Vamsi Aribandi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2890704379",
      "name": "Dara Bahri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096947334",
      "name": "Zhen Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151486164",
      "name": "Donald Metzler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3034696692",
    "https://openalex.org/W3024786184",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2953333557",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2540646130",
    "https://openalex.org/W2540404261",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2792764867",
    "https://openalex.org/W3011718307",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4301368689",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2952468927"
  ],
  "abstract": "Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, Donald Metzler. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4349–4359\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4349\nAre Pre-trained Convolutions Better than Pre-trained Transformers?\nYi Tay\nGoogle Research\nMountain View, California\nyitay@google.com\nMostafa Dehghani\nGoogle Research, Brain Team\nAmsterdam, Netherlands\ndehghani@google.com\nJai Gupta\nGoogle Research\nMountain View, California\njaigupta@google.com\nVamsi Aribandi∗\nGoogle Research\nMountain View, California\naribandi@google.com\nDara Bahri\nGoogle Research\nMountain View, California\ndbahri@google.com\nZhen Qin\nGoogle Research\nMountain View, California\nzhenqin@google.com\nDonald Metzler\nGoogle Research\nMountain View, California\nmetzler@google.com\nAbstract\nIn the era of pre-trained language models,\nTransformers are the de facto choice of model\narchitectures. While recent research has\nshown promise in entirely convolutional, or\nCNN, architectures, they have not been ex-\nplored using the pre-train-ﬁne-tune paradigm.\nIn the context of language models, are con-\nvolutional models competitive to Transform-\ners when pre-trained? This paper investigates\nthis research question and presents several in-\nteresting ﬁndings. Across an extensive set of\nexperiments on 8 datasets/tasks, we ﬁnd that\nCNN-based pre-trained models are competi-\ntive and outperform their Transformer counter-\npart in certain scenarios, albeit with caveats.\nOverall, the ﬁndings outlined in this paper\nsuggest that conﬂating pre-training and archi-\ntectural advances is misguided and that both\nadvances should be considered independently.\nWe believe our research paves the way for a\nhealthy amount of optimism in alternative ar-\nchitectures.\n1 Introduction\nIn the modern era of pre-training, there appears\nto be an unbreakable tie between Transformer ar-\nchitectures (Vaswani et al., 2017) and pre-trained\nlanguage models. Models such as BERT (Devlin\net al., 2018), RoBERTa (Liu et al., 2019), and T5\n(Raffel et al., 2019) have all adopted Transformers\nas their underlying architecture. As a matter of fact,\nthere are barely any recent pre-trained models not\nbased on Transformers.\nWhile the contextual representation learning has\na rich history (Pennington et al., 2014; Dai and Le,\n∗Google AI Resident\n2015; Chidambaram et al., 2018; Liu et al., 2020;\nQiu et al., 2020), modern pre-trained language mod-\neling started with models like ELMo (Peters et al.,\n2018) and CoVE (McCann et al., 2017) which are\nbased on recurrent (e.g. LSTM (Hochreiter and\nSchmidhuber, 1997)) architectures. Although they\nwere successful, research using these architectures\ndwindled as Transformers stole the hearts of the\nNLP community, having, possibly implicitly, been\nperceived as a unequivocal advancement over its\npredecessors.\nRecent work demonstrates the promise of en-\ntirely convolution-based models (Wu et al., 2019;\nGehring et al., 2017) and questions the necessity of\nself-attentive architectures like Transformers. For\nexample, in (Wu et al., 2019), the proposed convo-\nlutional seq2seq models outperform Transformers\non a series of canonical benchmarks such as ma-\nchine translation and language modeling. From\nthese ﬁndings emerge a rather natural line of ques-\ntioning - should we consider pre-trained models\nbeyond Transformers?\nDespite early success, the relevance of convo-\nlutional models in the era of pre-trained language\nmodels remains an open question. To the best of\nour knowledge, convolutional architectures have\nnot yet been rigorously evaluated under the pre-\ntrain-ﬁne-tune paradigm. This is the primary pur-\npose of this work. Concretely, this paper seeks to\nempirically validate whether pre-trained convolu-\ntions are competitive with pre-trained Transformers\nacross a range of tasks.\nThe interaction between pre-training schemes\nand model architectures is an under-studied topic.\nAre only Transformers able to capitalize on the\n4350\nbeneﬁts of pre-training? If we use a different ar-\nchitectural inductive bias, would there also be a\nsubstantial gain unlocked by pre-training? Are pre-\ntrained convolutions better in particular scenarios?\nThis paper investigates these questions.\nThere are a number of obvious beneﬁts of\nconvolution-based models. Firstly, convolutions\ndo not suffer from the quadratic memory complex-\nity of self-attention - a problem signiﬁcant enough\nthat it spawned the creation of the entirely new cat-\negory of “efﬁcient” Transformer architectures (Tay\net al., 2020b, 2021). Secondly, convolutions oper-\nate locally and do not rely on positional encodings\nas an order signal to the model. That said, convo-\nlutions also come with a slew of downsides. For\nexample, being unable to access global information\nmeans such models are unable to perform a form of\ncross-attention across multiple sequences. We dive\ninto the details of this more in subsequent sections.\nIn this paper, we present a pre-trained convolu-\ntional sequence-to-sequence, or Seq2Seq, model.\nWe train our convolutional model using span-based\nsequence-to-sequence denoising objectives similar\nto those employed in T5 (Raffel et al., 2019). We\nevaluate a variety of convolutional variants (e.g., di-\nlated, lightweight, dynamic (Wu et al., 2019), etc.)\nunder both raw (no pre-training) and pre-train-ﬁne-\ntune paradigms. Our goal is to understand the true\ncompetitiveness of convolutional architectures in\nthe era of pre-training.\nWe show that pre-trained convolutions are com-\npetitive against pre-trained Transformers via a\nset of experiments on a potpourri of NLP tasks,\nlike toxicity detection, sentiment classiﬁcation,\nnews classiﬁcation, query understanding and se-\nmantic parsing/compositional generalization (Kim\nand Linzen, 2020). Moreover, we ﬁnd that pre-\ntrained convolutions can outperform, in terms of\nmodel quality and training speed, state-of-the-art\npre-trained Transformers (Raffel et al., 2019) in\ncertain scenarios. However, to provide a balanced\nperspective, we also describe scenarios where pre-\ntrained convolutions do not perform well and may\nbe deemed unsuitable.\nContributions Overall, the main contributions\nof this paper can be summarized as follows:\n•We perform a comprehensive empirical evalu-\nation of convolutional Seq2Seq models under\nthe pre-train-ﬁne-tune paradigm. To the best\nof our knowledge, the competitiveness and\nrelevance of pre-trained convolutions still re-\nmains an open question.\n•We make several important observations.\nSpeciﬁcally, we ﬁnd that (1) pre-training helps\nconvolutional models just as much as it helps\nTransformers, and (2) pre-trained convolu-\ntions are competitive alternatives in certain\nscenarios in terms of model quality and train-\ning speed.\n•We conduct extensive experiments across 8\ndatasets spanning a diverse range of tasks and\ndomains. On 7 out of 8 tasks, we ﬁnd that\npre-trained convolutions outperform a recent\nstate-of-the-art transformer (T5 (Raffel et al.,\n2019)) with and without pre-training. We ex-\namine the speed and operation count (FLOPS)\nof convolutions versus Transformers and ﬁnd\nthat convolutions are not only faster but also\nscale better to longer sequence lengths.\n2 Related Work\nPre-training on a large corpus has become the pri-\nmary method of learning universal language rep-\nresentations to solve different downstream NLP\ntasks. The ﬁrst generation of pre-trained mod-\nels aimed at learning embedding for words, like\nSkip-Gram (Mikolov et al., 2013) and Glove (Pen-\nnington et al., 2014), and quickly developed to\nlearning contextualized representation for words,\nlike ELMO (Peters et al., 2018), GPT (Radford\net al., 2018), and BERT (Devlin et al., 2018). This,\nhowever, is not the only axis in which pre-trained\nmodels have evolved.\nDifferent objective functions and various tasks,\nboth supervised and unsupervised, have been ex-\nplored for pre-training. For instance, CoVe (Mc-\nCann et al., 2017) uses machine translation as the\npre-training task, ELMO (Peters et al., 2018) and\nGPT (Radford et al., 2018) use language modeling\nobjectives, BERT (Devlin et al., 2018) uses masked\nlanguage modeling, T5 (Raffel et al., 2019) and\nMASS (Song et al., 2019) use Seq2Seq masked\nlanguage modeling, and XLNet (Yang et al., 2019)\nutilizes permuted language modeling. In addition\nto this, BART (Lewis et al., 2019) uses a denois-\ning autoencoder setup during pre-training, where\nthe model takes a partially corrupted input and is\ntrained to recover the original, undistorted input.\nSome models use a contrastive learning setup dur-\ning pertaining, like replaced token detection, used\n4351\nby ELECTRA (Clark et al., 2020), and sentence or-\nder prediction, used by ALBERT (Lan et al., 2019)\nand StructBERT (Wang et al., 2019).\nAnother axis where pre-trained models in NLP\nexplored different ideas is model architecture.\nELMO (Peters et al., 2018) and CoVe (McCann\net al., 2017) used LSTMs as the base model. Later,\nTransformers (Vaswani et al., 2017) became the\nde facto architecture of pre-trained NLP models.\nBERT (Devlin et al., 2018), XLNet (Yang et al.,\n2019) and RoBERTa (Liu et al., 2019) use the\nTransformer encoder, while GPT (Radford et al.,\n2018), GPT-2 (Radford et al.), and GPT-3 (Brown\net al., 2020) use the Transformer decoder as the\nbackbone. Some pre-trained models are also are\nbased on the encoder-decoder transformer archi-\ntecture, like T5 (Raffel et al., 2019), MASS (Song\net al., 2019), and BART (Lewis et al., 2019). In this\npaper, we investigate another model architecture\nvariation by studying the power of convolutional\nneural network as the backbone of pre-trained mod-\nels for NLP.\nConvolutions have always been an interesting\nchoice for sequence modeling and NLP applica-\ntions (Kim, 2014; Bai et al., 2018; Kalchbrenner\net al., 2016). Convolutions are lightweight and fast\nand have many interesting use-cases, notably for\nlightweight classiﬁcation. In the era when LSTMs\nwere the workhorses of NLP applications, convolu-\ntions were positioned nicely on the pareto frontier\nof the compute-performance curve. They are fast\nand lightweight, and unlike Transformers, they do\nnot suffer from quadratic complexity. Our work\nis also well-aligned with the resurgence of interest\nin convolutions where (Wu et al., 2019) showed\nthat convolutions can outperform self-attention on\nseveral sequence transduction tasks. Moreover,\nthe necessity of the self-attention inductive bias\nin transformers have been also a subject of recent\ninterest. Synthesizer models (Tay et al., 2020a)\nshowed that transformers can still do pretty well\nwithout token-token dot product self-attention and\na random attention matrix can perform competi-\ntively on certain tasks.\n3 Pre-Trained Convolution Models\nThis section describes the pre-trained Convolution\nModel. For most of our experiments, we adopt\ndepthwise separable convolutions (Kaiser et al.,\n2017; Sifre and Mallat, 2014; Chollet, 2017) which\nhave shown to be fast and efﬁcient variants of the\nstandard convolution.\n3.1 Lightweight Depthwise Convolution\nThis section introduces Lightweight Depthwise\nConvolutions (Wu et al., 2019) which forms the\nbackbone of our pre-trained convolution model.\n3.1.1 Depthwise convolutions\nDepthwise convolutions convolve independently\nover every channel. Given an input tensor X\nof dimensions n×d, the depthwise convolution,\nD(X,Wc,:,i,c ) is deﬁned as:\nOi,c =\nk∑\nj−1\nWc,j ·Xi+j−⌈k+1\n2 ⌉),c (1)\nwhere W ∈ Rd×k are the learnable parameters\nof the layer. Oi,c is the output at position i and\nchannel c. The overall output is a tensor of n×d\nof identical shape as the input.\n3.1.2 Lightweight Convolutions\nL(.) are depthwise separable convolutions with (1)\nsoftmax-normalized kernels and (2) shared output\nchannels and weight tying. Speciﬁcally, this is\nwritten as:\nOL\ni,c =\nk∑\nj−1\nsoftmax(Wˆc,j) ·Xi+j−⌈k+1\n2 ⌉),ˆc (2)\nwhere ˆc = cH\nd . In short, parameters are shared\nevery d\nH output channels. When H = 1, this is\nequivalent to sharing all the weights of all channels.\n3.1.3 Dynamic Convolutions\nDynamic Convolutions DY (.) are a new form of\nlightweight convolutions introduced by (Wu et al.,\n2019). The key idea is to learn position-speciﬁc\nkernels for performing lightweight convolutions.\nThis can be written as:\nDY = L(X,f(Xi)h,:,i,c ), (3)\nwhere f(.) is a linear transformation with param-\neters WQ ∈RH×k×d that learns a position depen-\ndent kernel.\n3.2 Span-based Seq2Seq pre-training\nWe adopt span-based sequence-to-sequence pre-\ntraining as per (Raffel et al., 2019). Speciﬁcally,\ngiven an input sequence, we randomly mask spans\nof lengths Land replace them with a special sen-\ntinel token. The pre-training task is then to generate\nthe masked tokens as targets. For example: Inputs:\nThe happy cat sat [mask]. and Outputs: on the mat.\n4352\n3.2.1 Convolutional Seq2Seq Architecture\nWe implement a Seq2Seq (Sutskever et al., 2014)\narchitecture similar to (Wu et al., 2019). The key\ndifference when compared with Transformer archi-\ntectures is that we replace the multi-headed self-\nattention with convolutional blocks. Instead of\nquery-key-value transforms, we use gated linear\nunit projections following (Wu et al., 2019). Each\nconvolution block be written as:\nX1 = WIX⊙sigmoid(WSX),\nX2 = ConvBlock(X1),\nX3 = WO(X2),\nwhere WI,WS,WO are trainable parameters. We\nexperiment with simple lightweight convolutions,\ndynamic convolutions and dilated convolutions\nin our experiments. Following (Wu et al., 2019;\nGehring et al., 2017), the encoder-decoder atten-\ntion remains untouched. The convention follows\nthe backbone Transformer model in which we wrap\neach submodule with layer normalization and resid-\nual connectors. Hence, each Conv block is written\nas:\nXA = LayerNorm(Conv(X)) +X,\nXB = LayerNorm(FFN(XA) +XA,\nwhere Conv is any of the convolution models that\nwe explore in our experiments. FFN(.) is a two\nlayer feed-forward network with ReLU activations\nin the middle.\n3.2.2 Optimization\nThe model optimizes the token-wise cross-entropy\nloss and is trained with teacher forcing.\nL=\nL∑\nt=1\nn∑\ni=1\nlog(πt\ni) + (1−yt\ni) log(1−πt\ni),\nwhere πt\ni is the prediction of class iat time step\ntand yt\ni is the ground truth label of the class iat\ntime step t.\n4 Research Questions and Discussion\nBefore we delve into our experiments, we establish\na set of research questions and agenda we hope this\nwork aims to bring clarity to.\n•RQ1: Do convolutions beneﬁt from pre-\ntraining as much as Transformers?\n•RQ2: Are convolutional models, pre-trained\nor otherwise, competitive with Transformer\nmodels? When do they perform well?\n•RQ3: What are the beneﬁts (if any) of us-\ning pre-trained convolution models over pre-\ntrained Transformers? Are convolutions faster\nalternatives to self-attention based Transform-\ners?\n•RQ4: What are the failure modes, caveats and\nreasons to not use pre-trained convolutions?\n•RQ5: Are certain convolution variants better\nthan others?\n5 Experiments and Analysis\nThis section presents our analysis and results.\n5.1 Datasets\nOur evaluation is based on the following datasets\nand tasks.\n•Toxicity Detection - We use the CIVIL COM-\nMENTS (Borkan et al., 2019) andWIKI TOXIC\nSUBTYPES dataset (Wulczyn et al., 2017).\nGiven a piece of short text (originating from\nsocial media or wikipedia), the goal is to de-\ntermine if the content is toxic, i.e., a binary\nclassiﬁcation task. For this task, we evaluate\non both accuracy and F1 score.\n•Sentiment Classiﬁcation - This is a binary\nclassiﬁcation task that determines the polarity\nof documents, sentences and/or tweets. We\nuse the IMDb reviews dataset (Maas et al.,\n2011), Stanford Sentiment Treebank (SST-\n2) (Socher et al., 2013) dataset, along with\nTwitter Sentiment140 (S140) (Go et al., 2009)\ndataset.\n•News Classiﬁcation - This is a task of topic\ncategorization for news articles. We use the\nAGNews dataset (Zhang et al., 2015). This is\na four-way classiﬁcation task.\n•Question Classiﬁcation We use the TREC\nﬁne-grained question classiﬁcation dataset (Li\nand Roth, 2002). This task involves classi-\nfying questions into 46 ﬁne-grained question\ncategories.\n•Semantic Parsing / Compositional Gener-\nalization Compositional generalization is the\n4353\nability of models to generalize composition-\nally outside of the training distribution. To\nbe speciﬁc, it needs be able to handle unseen\ncombinations at test time. For this task, we use\nthe COGS dataset (Kim and Linzen, 2020), a\ntask of generating semantic representation of\na given English sentence. For example, A cat\nsmiled →cat(x1) AND smile.agent(x2,x1).\nAll of the datasets, with the exception of the re-\ncent COGS dataset (Kim and Linzen, 2020), are\nTensorﬂow datasets1.\nFor each dataset, we evaluate all models with\nand without pre-training (details in subsequent sec-\ntions). Table 1 reports the statistics of the datasets\nused in this paper.\nDataset / Task # Train # Test # Class\nCivil Comments 3,820,210 205,781 2\nWiki Toxicity 561,808 234,564 2\nIMDb 25,000 25,000 2\nSST-2 67,000 1,800 2\nS140 1,600,000 359 2\nTREC 4,500 500 46\nAGNews 120,000 7,600 4\nCOGS 24,000 3000 N/A\nTable 1: Statistics of datasets used in our experiments.\nDatasets are diverse in terms of domains, tasks and\namount of labeled data.\n5.2 Experimental Setup\nThis section describes our experimental setup.\n5.2.1 Models\nOur models are largely based on sequence to se-\nquence models, a paradigm that has demonstrated\ngreat success made evident by models such as\nBART (Lewis et al., 2019) and T5(Raffel et al.,\n2019). We implement our models in Mesh Ten-\nsorﬂow (MTF) (Shazeer et al., 2018), a library\nfor distributed and efﬁcient parallel model train-\ning that has similar API to Tensorﬂow. We train\nmodels that are of base size, which corresponds to\n12 layers each in the encoder and decoder, along\nwith 3072 dimensions for the feed-forward layers,\na model dimension of 768 and a total of 12 heads.\nOur Transformer models are largely based on T5\n(Raffel et al., 2019), which is considered the cur-\nrent state-of-the-art Transformer model for NLP\ntasks and hence serves as a strong baseline. For the\nconvolution models, our lightweight convolution\n1https://www.tensorflow.org/datasets/\ncatalog/overview.\nand dynamic convolution models have a window\nsize2 of 7 across all layers, the number of unique\ndepth ﬁlters is 2. For dilated models, we use a ﬁlter\nsize of [4,4,7,7,15,15,15,15,31,31,31] for our\n12 layer convolution model.\n5.2.2 Pre-training\nWe pre-train both our convolutional and Trans-\nformer models for 524K steps with a batch size\nof 128. Given the input sequence length of 512,\nthis corresponds to 65536 tokens per batch. For\npre-training, we use the Colossal Cleaned Com-\nmonCrawl Corpus (C4) (Raffel et al., 2019) dataset\nwhich has demonstrated impressive results on\ndownstream tasks. We use the span based seq2seq\nobjective as the pre-training objective as mentioned\nin earlier sections. The span size is set to 3 and\na corruption rate of 15% is adopted. We use the\nAdafactor optimizer (Shazeer and Stern, 2018) with\nan inverse square root learning rate scheduler. Each\npre-training run is performed using 16 TPU-v3\nchips and takes approximately 12 hours to com-\nplete for models of base size.\n5.2.3 Downstream Fine-tuning\nWe ﬁne-tune the pre-trained models using the\nfollowing set of hyperparameters: We use a\nconstant learning rate which is tuned amongst\n{0.001,0.0005,0.0001}. The batch size is gener-\nally set to 64 but occasionally set to 32 for smaller\ndatasets. Intuitively, sequence length is task de-\npendent but generally approximately the 90th per-\ncentile for each task. We ﬁne-tune for a maximum\nof 100K steps and report peak validation perfor-\nmance. Fine-tuning uses the same Adafactor opti-\nmizer as during training. We perform ﬁne-tuning\non similar hardware, i.e., typically 16 TPUv3 chips\nare used per ﬁne-tuning job.\n5.3 Experimental Results\nThis section describes our experimental setup and\nresults.\n5.4 Results on Toxicity Detection\nTable 2 reports results on toxicity detection. On\nboth toxicity detection datasets the pre-trained and\nno-pre-training (raw) setup, the best models are the\ndilated convolution models and the dynamic con-\nvolution models. In fact, all convolutional models\n2We believe that tuning the hyperparameters of the convo-\nlution models can result in even better performance. However,\nwe decided to keep these hyperparameters simple for the start.\n4354\noutperform Transformers on both CivilComments\nand WikiToxic. Before pre-training, convolutions\noutperform Transformers by approximately 1.5 ab-\nsolute percentage points. The gap narrows after pre-\ntraining where Transformers see a better gain (e.g.,\n+5.1% against +4.3%) from pre-training over con-\nvolutions on the CivilComments dataset. However,\nthe converse is true on WikiToxic - the only case of\nperformance degradation after pre-training. Over-\nall, on this task, convolutions are competitive to\nTransformers and outperform them.\n5.5 Results on Sentiment Classiﬁcation\nResults on Sentiment Classiﬁcation (IMDb, SST-2\nand S140) can be found in Table 2. On the IMDb re-\nviews dataset, the best non-pre-trained model is the\nlightweight convolution model, outperforming the\nTransformer model. The best pre-trained model is\nthe Transformer model. However, all convolutional\nmodels come in close with less than a percentage\npoint gap difference with pre-trained Transformers.\nOn the SST-2 and S140 tasks, we observe that the\nbest models are convolution-based, regardless of\nwhether the model is pre-trained or not.\n5.6 Results on Question Classiﬁcation\nThe best non-pre-trained model is the Lightweight\nConvolution model. For pre-trained models, con-\nvolutional models also outperform the pre-trained\nTransformer. On this task, while most models ben-\neﬁt signiﬁcantly from pre-training, Transformers\nseem to beneﬁt slightly more from pre-training.\n5.7 Results on News Classiﬁcation\nResults on news classiﬁcation seems to follow sim-\nilar trends as other benchmarks. Convolutional\nmodels outperform Transformers both in non-pre-\ntrained and pre-trained setups. The highest gain\nfrom pre-training is obtained from the dilated con-\nvolution model.\n5.8 Results on Compositional Generalization\nChallenge and Semantic Parsing\nWe conduct additional experiments on semantic\nparsing and compositional generalization. The task\nis framed as a sequence generation task. We use the\nrecently proposed (Kim and Linzen, 2020) dataset.\nOn the in-distribution test set, Transformers and\nconvolutions have identical performance (95%).\nOn the generalization or out of distribution set,\nTransformers perform at 77.5% while convolutions\ncome in at 76.9. While convolutions do not ex-\nactly outperform Transformers, they come in close\nenough to be considered competitive.\n5.9 Summary of Results\nOn the seven tasks across a broad range of do-\nmains we ﬁnd that (1) non-pre-trained convolutions\nare competitive and frequently outperform non-pre-\ntrained Transformers, (2) pre-trained convolutions\noutperform pre-trained Transformers on six out of\nseven tasks. This answers RQ2.\nWe also ﬁnd that convolutions are able to ben-\neﬁt from pre-training, in a similar fashion to\nself-attention-based models. Hence, the beneﬁts\nachieved by pre-training are not exclusive to Trans-\nformer models. This answers RQ1.\nAmongst the pre-trained convolutional models,\nwe ﬁnd that dilated convolutions and dynamic con-\nvolutions are generally better than lightweight con-\nvolutions, thus answering RQ5.\nFinally, we observe that relative performance\n(i.e., rankings) do change with pre-training. This\ndeﬁnitely shows that there is some kind of effect\nfrom composing architectures with pre-training.\nThe direct implication of this effect is that a model\nthat performs well (relatively) without pre-training\nwill not necessarily perform the best when pre-\ntrained (and vice versa). Hence, aside from conﬂat-\ning architectures with pre-training schemes, we do\nalso need to take note that different architectures\nmay behave differently under pre-training.\n6 Discussion and Analysis\nThis section expands on the results via a detailed\nanalysis and discussion. We discuss the pros/cons\nof pretrained convolutions, the impact of pre-\ntraining on performance and also recommendations\nto the broader community.\n6.1 When do we expect pre-trained\nconvolutions to fail?\nIn our experimental section, we observed the po-\ntential upsides of convolutional models over well-\nestablished pre-trained Transformers and observe\nthat we are able to get quality improvements in\ncertain cases. However, it might be good to further\nunderstand the drawbacks of convolutions.\nOne obvious weakness of pre-trained convolu-\ntions are their lack of cross-attention inductive\nbias that comes for free with self-attention in the\nTransformer encoder. For this reason, it is not a\n4355\nCIVIL COMMENT WIKI TOXIC IMDb SST-2 S140 TREC News\nModel Acc F1 Acc F1 Acc Acc Acc Acc Acc\nNo pre-training\nTrans. 77.22 85.09 91.93 95.45 84.81 78.44 58.84 78.00 84.25\nLight 78.58 85.82 91.05 94.65 85.88 81.65 60.64 82.20 87.22\nDilat. 79.94 86.50 92.29 94.91 85.84 79.01 55.62 79.60 81.24\nDyna. 78.49 84.71 90.06 95.66 85.69 82.80 60.84 80.20 85.13\nWith pre-training\nTrans. 81.16 86.56 91.46 95.12 94.16 92.09 61.65 93.60 93.54\nLight 81.47 87.58 93.61 96.48 93.60 92.20 61.65 93.60 93.63\nDilat. 81.67 87.78 93.84 96.21 93.92 92.09 62.85 94.20 93.26\nDyna. 81.83 87.71 93.76 96.53 93.35 91.59 62.45 92.40 93.93\nGain from pre-training\nTrans. +5.1% +1.7% -0.6% -0.4% +11.0% +17.4% +4.7% +20.0% +11.0%\nLight +3.7% +2.1% +2.8% +1.9% +9.0% +13.0% +1.7% +14.0% +7.3%\nDilat. +2.1% +1.5% +1.7% +1.4% +9.4% +17.0% +13.0% +18.0% +14.8%\nDyn. +4.3% +3.5% +4.1% +1.0% +8.9% +10.6% +2.6% +15.2% +10.4%\nTable 2: Comparison of pre-trained Convolutions and pre-trained Transformers on toxicity detection, sentiment\nclassiﬁcation, question classiﬁcation and news classiﬁcation. All models have approximately 230M parameters\nand are 12 layered seq2seq architectures. Our ﬁndings show that convolutions (1) also beneﬁt from pretraining and\n(2) are consistently competitive to transformer models with and without pretraining.\ngood idea to use pre-trained convolutions for tasks\nthat requires modeling the relationship between\ntwo or more sequences. To verify this, we run ex-\nperiments on SQuAD and MultiNLI and ﬁnd that\nconvolutions do not come close to Transformers\njust because of this missing inductive bias. This\nshould be clearly distinguished when examining\nand evaluating models, as how the early SNLI\nleaderboard3 distinguished between models that\nused cross-attention and models that did not.\nOur initial evaluations on benchmarks like\nSQuAD/MNLI (Rajpurkar et al., 2016; Williams\net al., 2017) showed that pre-trained convolutions\nare indeed signiﬁcantly lackluster. For exam-\nple, convolutions only achieve ≈75% accuracy\non MultiNLI, while transformers easily achieve\n≈84% accuracy. Likewise, while transformers\nachieve about ≈90% F1 on SQuAd, convolutions\ncome in around ≈70%. This is entirely expected\nbecause there is no way the premise/question can\ninteract with the hypothesis/context. (RQ4). How-\never, our experiments show that this was only\nbecause they lack this cross-attention property.\nWhen we augment convolutions with a single layer\nof cross attention at the encoder, we ﬁnd that\npre-trained convolutions come close (a delta of\n3https://nlp.stanford.edu/projects/\nsnli/\n(≈1%)) to pre-trained Transformers on datasets\nsuch as MultiNLI (Williams et al., 2017), achieving\nabout ≈83% accuracy.\nThat said, we leave it to the practitioner to decide\nwhether the cross-attention inductive bias is actu-\nally important for the problem at hand. We also like\nto emphasize that the pattern of concatenating sen-\ntence pairs is not necessary practical when scaling\nup since this requires inference on every permuta-\ntion of sentence pairs. For this reason, dual encoder\nsetups that do fast embedding space look-ups are\nmore practical and feasible in practice (Guo et al.,\n2020). Given the strong performance of convolu-\ntions in a series of encoding tasks, we can expect\npre-trained convolutions to do well in a dual en-\ncoder setup.\n6.2 What are the beneﬁts of pre-trained\nconvolutions over Transformers?\nWe observed a reasonable quality improvement\nfrom using convolutions over Transformers. This\nsection discusses the additional beneﬁt.\n6.2.1 Convolutions are faster and scale better\nto long sequences\nFigure 1 reports training speed of convolution\n(LightConvs) versus transformers on a sequence\nto sequence task. The input lengths are varied\nfrom {64,128,256,512,1024,2048,4096}. We\n4356\nFigure 1: Effect of sequence length on processing\nspeed (examples per second) on a seq2seq masked lan-\nguage modeling task. Results are benchmarked on 16\nTPUv3 chips on C4 pre-training. Results are in log\nscale.\nshow that convolutions are not only consistently\nfaster (even at shorter sequences) but scale bet-\nter than transformers. Convolution scales linearly\nwhile transformers are not able to scale to longer\nsequences.\n6.2.2 Convolutions are FLOPs efﬁcient\nWe measure the number of FLOPs of convolutions\nversus transformers as we increase the sequence\nlength. Figure 2 shows the phenomenon while\nvarying sequence length. In general, across all\nsequence lengths, convolutions are more efﬁcient\nin the number of ﬂoating point operations.\nFigure 2: Effect of sequence length on number of\nFLOPs (einsum ops) on a seq2seq masked language\nmodeling task. Results are benchmarked on 16 TPUv3\nchips on C4 pre-training. Results are in log scale.\nThe overall ﬁndings that convolutions are faster\nboth in wall clock time and in FLOPs answersRQ3.\nMoreover, we ﬁnd that the FLOP efﬁciency of con-\nvolutions scales better across sequence lengths.\n6.3 Are we suggesting to completely replace\nTransformers with convolution?\nWhile Transformers have dominated the research\nlandscape in NLP, this paper suggests that there\nare commonly overlooked beneﬁts to convolutions\nsuch as model quality, speed, FLOPs and scalabil-\nity. Moreover, it is previously unknown to whether\nconvolutions beneﬁt from pre-training. In this pa-\nper, we showed that they are competitive on some\ntasks and also beneﬁt from pre-training in simi-\nlar fashion to transformer models. However, on\nthe ﬂip side, we also highlighted that they are un-\nable to handle tasks that require cross-attention or\nwhen there is a need to model > 1 sentence or\ndocuments within the same sequence. We believe\nthat practitioners have good options and it might\nbe worthwhile to explore architectures outside the\nwell-established transformer models.\n6.4 On not conﬂating pre-training with\narchitectural advances\nIn this paper, we showed that three other\n(convolutional-based) architectures (e.g.,\nlightweight, dymamic and dilated) also ben-\neﬁt from pre-training to the same extent as\ntransformer models.\nIn the current research landscape, pre-training\nhas always be tightly coupled and associated with\ntransformers architectures. As a result, the success\nof BERT, transformers and large language models\nseem to be pretty conﬂated. While it is true that,\nto this date, the only model that large-scale pre-\ntraining has been applied to are transformer mod-\nels, we believe there might be potential in other\narchitectures.\nBased on our empirical ﬁndings, we believe\nthere is still signiﬁcant room for the improving\nthe understanding of the compositional effects of\narchitecture and pre-training. Hence, we believe\nthat the impact of this work extends beyond show-\ning the competitiveness of convolution models in\nNLP. More concretely, the take home message is\nthat there should be a healthy level of optimism in\nexploring architectural alternatives.\n7 Conclusion\nIn this paper, we conducted an extensive study of\nthe viability and feasibility of pre-trained convolu-\n4357\ntions. Our experimental results show that convo-\nlutions can outperform Transformers in both pre-\ntrain and non-pre-trained setups. Our extensive\nexperiments across 8 datasets spanning a diverse\nrange of tasks, show that convolutions are able\nto beneﬁt from pre-training to the same (or some-\ntimes greater) extent than Transformers. While\npre-trained transformers are the de-facto choice of\narchitecture, our results show that they might not\nbe the best in certain scenarios. Additionally, we\ndiscussed the caveats, trade-offs pertaining with\nruntime, scalability, number of FLOPS and model\nquality. Finally, we discussed the situations or data\ntypes that convolutions are not well equipped to\nhandle and make an empirically informed recom-\nmendation for practitioners.\nReferences\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence modeling.\narXiv preprint arXiv:1803.01271.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum\nThain, and Lucy Vasserman. 2019. Nuanced metrics\nfor measuring unintended bias with real data for text\nclassiﬁcation. CoRR, abs/1903.04561.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nMuthuraman Chidambaram, Yinfei Yang, Daniel Cer,\nSteve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2018. Learning cross-lingual sentence\nrepresentations via a multi-task dual-encoder model.\narXiv preprint arXiv:1810.12836.\nFranc ¸ois Chollet. 2017. Xception: Deep learning with\ndepthwise separable convolutions. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 1251–1258.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nAndrew M Dai and Quoc V Le. 2015. Semi-\nsupervised sequence learning. arXiv preprint\narXiv:1511.01432.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\ntional sequence to sequence learning. arXiv preprint\narXiv:1705.03122.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twit-\nter sentiment classiﬁcation using distant supervision.\nCS224N project report, Stanford, 1(12):2009.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In International Conference on\nMachine Learning.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nLukasz Kaiser, Aidan N Gomez, and Francois Chol-\nlet. 2017. Depthwise separable convolutions\nfor neural machine translation. arXiv preprint\narXiv:1706.03059.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\nAaron van den Oord, Alex Graves, and Koray\nKavukcuoglu. 2016. Neural machine translation in\nlinear time. arXiv preprint arXiv:1610.10099.\nNajoung Kim and Tal Linzen. 2020. Cogs: A compo-\nsitional generalization challenge based on semantic\ninterpretation. arXiv preprint arXiv:2010.05465.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classiﬁcation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nXin Li and Dan Roth. 2002. Learning question clas-\nsiﬁers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nQi Liu, Matt J Kusner, and Phil Blunsom. 2020. A\nsurvey on contextual embeddings. arXiv preprint\narXiv:2003.07278.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\n4358\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies-volume 1, pages 142–150. Asso-\nciation for Computational Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in neural in-\nformation processing systems, pages 6294–6305.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. arXiv preprint arXiv:1310.4546.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, pages 1–26.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, et al. 2018. Mesh-tensorﬂow: Deep learning\nfor supercomputers. In Advances in Neural Informa-\ntion Processing Systems, pages 10414–10423.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235.\nLaurent Sifre and St ´ephane Mallat. 2014. Rigid-\nmotion scattering for image classiﬁcation.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. arXiv preprint\narXiv:1905.02450.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020a. Synthesizer: Re-\nthinking self-attention in transformer models. arXiv\npreprint arXiv:2005.00743.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nJiangnan Xia, Liwei Peng, and Luo Si. 2019. Struct-\nbert: Incorporating language structures into pre-\ntraining for deep language understanding. arXiv\npreprint arXiv:1908.04577.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.\nEx machina: Personal attacks seen at scale. In Pro-\nceedings of the 26th International Conference on\nWorld Wide Web, WWW ’17, pages 1391–1399, Re-\npublic and Canton of Geneva, CHE. International\nWorld Wide Web Conferences Steering Committee.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\n4359\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5392171144485474
    },
    {
      "name": "Transformer",
      "score": 0.502692461013794
    },
    {
      "name": "Natural language processing",
      "score": 0.46560055017471313
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.41026240587234497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38474419713020325
    },
    {
      "name": "Engineering",
      "score": 0.22381100058555603
    },
    {
      "name": "Physics",
      "score": 0.15227213501930237
    },
    {
      "name": "Electrical engineering",
      "score": 0.14767420291900635
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}