{
  "title": "Assessing the power of AI: a comparative evaluation of large language models in generating patient education materials in dentistry",
  "url": "https://openalex.org/W4411401066",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2233072252",
      "name": "Gowri Sivaramakrishnan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118330486",
      "name": "Maryam Almuqahwi",
      "affiliations": [
        "Royal College of Surgeons in Ireland - Bahrain"
      ]
    },
    {
      "id": "https://openalex.org/A2972878225",
      "name": "Sufyan Ansari",
      "affiliations": [
        "Royal College of Surgeons in Ireland - Bahrain"
      ]
    },
    {
      "id": "https://openalex.org/A3134886803",
      "name": "Mohammed Lubbad",
      "affiliations": [
        "Royal College of Surgeons in Ireland - Bahrain"
      ]
    },
    {
      "id": "https://openalex.org/A4314517241",
      "name": "Emad Alagamawy",
      "affiliations": [
        "Royal College of Surgeons in Ireland - Bahrain"
      ]
    },
    {
      "id": "https://openalex.org/A2742576369",
      "name": "Kannan Sridharan",
      "affiliations": [
        "Arabian Gulf University"
      ]
    },
    {
      "id": "https://openalex.org/A2233072252",
      "name": "Gowri Sivaramakrishnan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5118330486",
      "name": "Maryam Almuqahwi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972878225",
      "name": "Sufyan Ansari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3134886803",
      "name": "Mohammed Lubbad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4314517241",
      "name": "Emad Alagamawy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742576369",
      "name": "Kannan Sridharan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1991280013",
    "https://openalex.org/W4400215420",
    "https://openalex.org/W2101950949",
    "https://openalex.org/W4383187389",
    "https://openalex.org/W4402432594",
    "https://openalex.org/W4386958277",
    "https://openalex.org/W4403300633",
    "https://openalex.org/W4405021649",
    "https://openalex.org/W1524030924",
    "https://openalex.org/W2019086535",
    "https://openalex.org/W4400262532",
    "https://openalex.org/W2153612906",
    "https://openalex.org/W4306939544",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4386152215",
    "https://openalex.org/W4399567363",
    "https://openalex.org/W4410215381",
    "https://openalex.org/W4402692877",
    "https://openalex.org/W4404782882",
    "https://openalex.org/W4407405478",
    "https://openalex.org/W4407134979",
    "https://openalex.org/W4408519728",
    "https://openalex.org/W4406995514"
  ],
  "abstract": "Abstract Background This study evaluates the use of large language models (LLMs) in generating Patient Education Materials (PEMs) for dental scenarios, focusing on their reliability, readability, understandability, and actionability. The study aimed to assess the performance of four LLMs—ChatGPT-4.0, Claude 3.5 Sonnet, Gemini 1.5 Flash, and Llama 3.1–405b—in generating PEMs for four common dental scenarios. Methods A comparative analysis was conducted where five independent dental professionals assessed the materials using the Patient Education Materials Assessment Tool (PEMAT) to evaluate understandability and actionability. Readability was measured with Flesch Reading Ease and Level scores, and inter-rater reliability was assessed using Fleiss’ Kappa. Results Llama 3.1–405b demonstrated the highest inter-rater reliability (Fleiss’ Kappa: 0.78–0.89). ChatGPT-4.0 excelled in understandability, surpassing the PEMAT threshold of 70% in three of the four scenarios. Claude 3.5 Sonnet performed well in understandability for two scenarios but did not consistently meet the 70% threshold for actionability. ChatGPT-4.0 generated the longest responses, while Claude 3.5 Sonnet produced the shortest. Conclusions ChatGPT-4.0 demonstrated superior understandability, while Llama 3.1–405b achieved the highest inter-rater reliability. The findings indicate that further refinement and human intervention is necessary for LLM-generated content to meet the standards of effective patient education.",
  "full_text": "ARTICLE OPEN\nAssessing the power of AI: a comparative evaluation of large\nlanguage models in generating patient education materials in\ndentistry\nGowri Sivaramakrishnan 1 ✉, Maryam Almuqahwi2, Sufyan Ansari2, Mohammed Lubbad2, Emad Alagamawy2 and\nKannan Sridharan 3\n© The Author(s) 2025\nBACKGROUND: This study evaluates the use of large language models (LLMs) in generating Patient Education Materials (PEMs) for\ndental scenarios, focusing on their reliability, readability, understandability, and actionability. The study aimed to assess the\nperformance of four LLMs— ChatGPT-4.0, Claude 3.5 Sonnet, Gemini 1.5 Flash, and Llama 3.1–405b— in generating PEMs for four\ncommon dental scenarios.\nMETHODS: A comparative analysis was conducted whereﬁve independent dental professionals assessed the materials using the\nPatient Education Materials Assessment Tool (PEMAT) to evaluate understandability and actionability. Readability was measured\nwith Flesch Reading Ease and Level scores, and inter-rater reliability was assessed using Fleiss’Kappa.\nRESULTS: Llama 3.1–405b demonstrated the highest inter-rater reliability (Fleiss’Kappa: 0.78–0.89). ChatGPT-4.0 excelled in\nunderstandability, surpassing the PEMAT threshold of 70% in three of the four scenarios. Claude 3.5 Sonnet performed well in\nunderstandability for two scenarios but did not consistently meet the 70% threshold for actionability. ChatGPT-4.0 generated the\nlongest responses, while Claude 3.5 Sonnet produced the shortest.\nCONCLUSIONS: ChatGPT-4.0 demonstrated superior understandability, while Llama 3.1–405b achieved the highest inter-rater\nreliability. The ﬁndings indicate that further reﬁnement and human intervention is necessary for LLM-generated content to meet\nthe standards of effective patient education.\nBDJ Open           (2025) 11:59 ; https://doi.org/10.1038/s41405-025-00349-1\nINTRODUCTION\nPatient education materials (PEMs) play a crucial role in dental\ncare, serving as an essential resource for educating individuals\nabout their oral health, post-treatment care, and emergency\nmanagement [1, 2]. These materials help bridge the gap between\nprofessional dental advice and patient understanding, ensuring\nthat individuals can follow appropriate self-care practices at home\n[2]. In a dental setting, well-structured educational content can\nenhance patient compliance with treatment recommendations,\nreduce the risk of complications, and improve overall oral health\noutcomes [1]. Effective patient education materials should be\nclear, accurate, accessible, and actionable, enabling individuals to\neasily comprehend and apply the information provided [1, 3].\nWith advancements in artiﬁcial intelligence (AI), Large Language\nModels (LLMs) have emerged as a potential tool for generating\nPEMs efﬁciently and at scale [4, 5]. LLMs, such as Chat GPT 4.0,\nClaude 3.5 Sonnet, Gemini 1.5 Flash, and Llama 3.1–405b, are\ntrained on vast amounts of textual data and can generate human-\nlike responses to various prompts. These AI-powered models are\nincreasingly being explored for their ability to simplify complex\nmedical information, personalize health education, and improve\naccessibility for patients with diverse literacy levels [6–8]. While\nthese models can generate ﬂuent and coherent text, concerns\nremain regarding their accuracy, reliability, and readability when\napplied to healthcare contexts [ 9]. Given the high stakes of\nmedical and dental information, it is critical to assess whether\nLLM-generated materials meet the standards of clarity, medical\naccuracy, and practical usability.\nThis study aims to evaluate the effectiveness of LLM-generated\nPEMs for common dental scenarios, focusing on their reliability,\nreadability, and actionability. By assessing materials generated for\nfour key dental situations, this study sought to determine whether\nAI-generated content aligns with the principles of effective health\ncommunication. The ﬁndings of this study provide insights into\nthe strengths and limitations of LLMs in generating PEMs, helping\nto inform future applications of AI in dental communication and\npatient care.\nMETHODOLOGY\nStudy design and selection of large language models\nThis study used a comparative analytical design to evaluate the\nreliability, readability, and actionability of patient education\nmaterials generated by four LLMs: ChatGPT-4.0, Claude 3.5 Sonnet,\nReceived: 7 April 2025 Revised: 25 May 2025 Accepted: 27 May 2025\n1Bahrain Defence Force Royal Medical Services, Riffa, Bahrain.2Dental and Maxillofacial Center, Bahrain Defence Force Royal Medical Services, Riffa, Bahrain.3College of Medicine\nand Health Sciences, Arabian Gulf University, Manama, Bahrain.✉email: Gowri.sivaramakrishnan@gmail.com\nwww.nature.com/bdjopen\n1234567890();,:\nGemini 1.5 Flash, Llama 3.1–405B. Figure 1 summarizes the key\nfeatures of these LLMs. The selection of these models ensured a\nbalanced evaluation of both proprietary and open-source LLM’si n\ndental health communication. Due to the nature of the study,\nethical approval was not sought. However, adherence to the latest\nDeclaration of Helsinki guidelines was maintained.\nEach model was prompted to generate patient education\nhandouts for four speciﬁc dental scenarios:\n● Post-operative instructions following a tooth extraction\n● Immediate steps for managing an avulsed tooth\n● Proper daily tooth brushing technique for optimal oral\nhygiene\n● Self-examination for oral cancer screening\nFor consistency, the prompts were carefully structured to\nensure that each LLM received identical instructions without\nadditional context or examples. The prompts were designed to be\nsuccinct and clear, ensuring that they were easily understandable\nand replicable by anyone using the same set of instructions. This\napproach was aimed at minimizing any biases or variations that\nmight arise from differing interpretations or added context. By\nusing straightforward, concise prompts, the study ensured that\neach model ’s performance was based solely on the input\nprovided, allowing for an unbiased and uniform evaluation of\nthe generated materials. The generated materials were then\nassessed using multiple standardized evaluation metrics.\nAssessment of readability, actionability and understandability\nThe Patient Education Materials Assessment Tool (PEMAT) [3] was\nused to assess the understandability and actionability of the\nmaterials generated by each LLM. Five independent dental\nprofessionals, rated each of the four generated materials using\nthe PEMAT criteria. The ratings focused on evaluating how easy\nthe content was to understand (understandability) and how\nclearly patients could identify and apply the actions or steps\nrecommended (actionability). For each material, mean scores were\ncalculated for both understandability and actionability, allowing\nfor a comprehensive evaluation of each LLM ’s output. For\nunderstandability, a score of 70% or above indicates that the\nmaterial is understandable for most patients. For accountability, a\nscore of 70% or higher is considered good, meaning the material\nclearly outlines actions that are easy to follow [3].\nIn addition to the PEMAT, other readability scores, such as the\nFlesch Reading Ease and Reading Level scores, were also\ncalculated to evaluate the accessibility of the materials in terms\nof their linguistic complexity. These scores were calculated using\nthe online calculators that are freely available. These scores\nallowed for further comparison between the LLMs, focusing on\nthe ease of reading and the suitability of the language used for\nvarious patient populations.\nInter-rater reliability\nTo assess the consistency and agreement among theﬁve raters,\nFleiss’Kappa was used to measure inter-rater reliability. The level\nof agreement was categorized according to standard Fleiss’Kappa\ninterpretation:\n● No agreement (≤0)\n● Slight (0.01–0.20)\n● Fair (0.21–0.40)\n● Moderate (0.41–0.60)\n● Substantial (0.61–0.80)\n● Almost perfect agreement (0.81–1.00)\nRESULTS\nAll LLMs provided responses to each of the scenarios outlined, and\nthese responses are presented in Supplementary File 1.\nInter-rater reliability\nLlama 3.1–405b demonstrated the highest level of inter-rater\nreliability, with Fleiss’ Kappa values ranging from 0.78 to 0.89,\nindicating almost perfect agreement among the raters for both\nunderstandability and actionability across the ﬁve evaluations.\nChat GPT 4.0 displayed substantial agreement, particularly in the\nratings for actionability (κ = 0.69), but showed moderate agree-\nment in the other areas, with Fleiss’Kappa values ranging from\n0.52 to 0.57. Claude 3.5 Sonnet exhibited moderate inter-rater\nFig. 1 Large language models (LLMs) evaluated in this study.The ﬁgure summarizes key attributes of four LLMs analyzed. Developer\ninformation and feature highlights are provided for each model.\nG. Sivaramakrishnan et al.\n2\nBDJ Open           (2025) 11:59 \nreliability, with Fleiss’ Kappa values ranging from 0.45 to 0.66.\nGemini 1.5 Flash demonstrated reasonable consistency with Fleiss’\nKappa values ranging from 0.73 to 0.79, reﬂecting a consistent\nlevel of agreement, though not as strong as Llama 3.1 –405b.\nThese ﬁndings suggest that while all models demonstrated\nacceptable inter-rater reliability, Llama 3.1–405b emerged as the\nmost reliable model, particularly in generating materials with high\nconsistency across raters (Supplementary File 2). The radar plot is\npresented in Fig.2.\nUnderstandability and actionability\nScenario 1- post-operative instructions following tooth extraction .\nThe understandability scores for this scenario varied across the\nmodels. Chat GPT 4.0 scored 61% for understandability in Scenario\n1, indicating moderate clarity, while Llama 3.1–405b scored 49%,\nsuggesting that the content generated by this model may have\nbeen more complex and less clear. Chat GPT 4.0 had the highest\nactionability score at 71% in Scenario 1, indicating a high level of\npractical guidance. Llama 3.1–405b and Gemini 1.5. Flash scored\nlower at 60%, suggesting that the instructions were less actionable\nand may have lacked speci ﬁc details for patients to follow\neffectively (Fig. 3).\nScenario 2- immediate steps for managing an avulsed tooth. Chat\nGPT 4.0 scored 72% for understandability indicating clear\ninstructions for handling the situation. Gemini 1.5 Flash scored\nthe lowest at 48%, reﬂecting potentially more technical language\nor unclear phrasing. Llama 3.1–405b led in actionability with a\nscore of 65%, demonstrating practical guidance for handling the\navulsed tooth and seeking dental care. The other models, such as\nClaude 3.5 Sonnet, Gemini 1.5 Flash and Chat GPT, had relatively\nlower actionability scores (54%, 51% and 62%, respectively),\nindicating that the instructions were less detailed or actionable\n(Fig. 3).\nScenario 3- proper daily tooth brushing technique . Gemini 1.5\nFlash scored relatively low for understandability at 51%. Chat GPT\n4.0 and Claude 3.5 Sonnet scored higher (75%), reﬂecting more\naccessible content. Llama 3.1 –405b scored the highest for\nactionability at 65%, indicating that the material included clear,\nstep-by-step instructions (Fig.3).\nScenario 4- self-examination for oral cancer screening. Chat GPT\nachieved the highest understandability scores (70%) indicating\nclearer communication of the steps involved in oral cancer self-\nexamination. Claude 3.5 Sonnet and Gemini 1.5 Flash led in\nactionability, with a score of 60%, indicating actionable guidance\non how to perform the self-examination and seek professional\ncare when necessary (Fig. 3). Among the evaluated language\nmodels, only Chat GPT 4.0 and Claude 3.5 Sonnet achieved\nunderstandability scores above 70% in certain scenarios. Chat GPT\n4.0 demonstrated strong performance in understandability,\nsurpassing the 70% threshold in Scenario 2, 3 and 4. Additionally,\nit was the only model to achieve an actionability score above 70%\n(Scenario 1). Claude 3.5 Sonnet also performed well in under-\nstandability, exceeding 70% in Scenario 1 and 3. However, none of\nthe models consistently met the 70% benchmark for both\nFig. 2 Inter-rater reliability across scenarios for each LLM.Radar\nchart comparing Fleiss Kappa scores of four large language models\n(LLMs) across four scenarios. Each axis represents a scenario, with\nFleiss Kappa values plotted radially from the center. Model\nperformance is shown as distinct geometric lines. Higher values\ntoward the outer edges indicate stronger inter-rater agreement.\nFig. 3 PEMAT understandability and actionability scores across scenarios for each LLM.Line graphs comparing Patient Education Materials\nAssessment Tool (PEMAT) scores for four large language models (LLMs) across four scenarios. Scenarios are plotted along the X-axis, with\nPEMAT percentage scores (0–100%) on the Y-axis.\nG. Sivaramakrishnan et al.\n3\nBDJ Open           (2025) 11:59 \nunderstandability and actionability across all scenarios, high-\nlighting the variability in performance depending on the context.\nReadability\nChat GPT 4.0 showed a moderate readability range, with scores\nbetween 52.2 and 69.9 across all scenarios. This suggests that its\nresponses were readable at an 8th or 9th-grade level. Claude 3.5\nSonnet displayed variability in readability, with the most difﬁcult\ntext in Scenario 1 (Flesch score of 57.4), while Scenario 2 was\neasier to read (74.0). Scenarios 3 and 4, however, were harder to\nread (41.7 and 49.8), which could present challenges for a general\naudience. Gemini 1.5 Flash produced text that was relatively\ndifﬁcult to read across all scenarios, indicating a high reading\ndifﬁculty level (10th to 12th-grade level). Llama 3.1 –405b\nexhibited more variation, with Scenario 3 showing the best\nreadability (76.8, equivalent to a 7th-grade level), while other\nscenarios scored lower (Table1).\nWord count and sentence structure\nThe length of responses and sentence structure varied across the\nmodels. Chat GPT 4.0 generated the longest responses, averaging\n505–655 words per scenario. Its sentence length ranged from 9.9\nto 12.2 words, indicating that it tended to produce more detailed\nresponses with relatively long sentences. Claude 3.5 Sonnet\ngenerated the shortest responses, averaging between 456 and\n551 words. Its sentence structure was more concise, with an\naverage of 4.3–5.8 words per sentence, indicating a more direct\nand to-the-point approach in response generation. Gemini 1.5\nFlash produced responses of similar length to Claude 3.5 Sonnet,\nwith an average of 470 words. Llama 3.1 –405b produced\nresponses averaging between 362 and 467 words, with sentence\nlengths ranging from 9.1 to 11.4 words (Table1).\nDISCUSSION\nThe aim of this study was to evaluate the performance of four\nLLM’s— Chat GPT 4.0, Claude 3.5 Sonnet, Gemini 1.5 Flash, and\nLlama 3.1–405b— in generating dentistry-related content across\nfour different scenarios. The focus was on assessing inter-rater\nreliability, understandability, actionability, readability, and\nresponse characteristics. The ﬁndings indicate notable variations\nin model performance based on these criteria. Llama 3.1–405b\ndemonstrated superior inter-rater reliability, indicating consistent\nratings across raters, but it performed less well in understand-\nability and accountability compared to Chat GPT 4.0.\nBased on the recommended 6th to 8th grade reading level by\nboth the American Medical Association (AMA) and the National\nInstitutes of Health (NIH) [1, 10, 11], this range is recommended\nbecause many patients have reading skills at or below this level,\nand health materials above this threshold risk being too complex,\npotentially limiting comprehension and effective self-care. The\nresults of this analysis showed a mixed performance across the\nLLMs. Llama 3.1–405b and Claude 3.5 Sonnet were the closest to\nmeeting this recommendation, with one scenario each falling\nwithin the 7th to 8th grade range. However, Chat GPT 4.0 and\nGemini 1.5 Flash tended to produce content at a higher grade\nlevel, for all scenarios, which may make the material more\nchallenging for patients to understand. Readability formulas like\nFlesch-Kincaid provide quantitative estimates but may not fully\ncapture complexity due to medical jargon or sentence structure.\nThis highlights the importance of human oversight to ensure\nlanguage is appropriately simple and clear for diverse patient\npopulations. While these models performed well in many aspects,\nnone of them consistently hit the ideal 6th grade level,\nhighlighting the need for human intervention to simplify the\ncontent to align with the recommended readability levels.\nOur ﬁndings highlight notable differences in readability, word\ncount, and sentence structure across the LLMs evaluated.\nTable 1. Characteristics of responses received.\nSource Scenario Flesch reading ease score Reading level Average words per sentence Total sentences Total words\nChat GPT 4.o 1 52.2 10th to 12th grade (Fairly dif ﬁcult to read) 10.7 47 505\n2 67.6 8th & 9th grade (Plain English) 12.2 43 524\n3 60.8 8th & 9th grade (Plain English) 10.5 57 599\n4 69.9 8th & 9th grade (Plain English) 9.9 66 655\nClaude 3.5 Sonnet 1 57.4 10th to 12th grade (Fairly dif ﬁcult to read) 5.5 31 171\n2 74 7th grade (Fairly easy to read) 5.8 37 213\n3 41.7 College (Dif ﬁcult to read) 4.3 51 218\n4 49.8 College (Dif ﬁcult to read) 4.7 58 274\nGemini 1.5 Flash 1 54.2 10th to 12th grade (Fairly dif ﬁcult to read) 8.7 63 551\n2 54.9 10th to 12th grade (Fairly dif ﬁcult to read) 8 57 456\n3 52.6 10th to 12th grade (Fairly dif ﬁcult to read) 10.3 51 525\n4 52.3 10th to 12th grade (Fairly dif ﬁcult to read) 10.6 53 563\nLlama 3.1–405b 1 53.8 10th to 12th grade (Fairly dif ﬁcult to read) 9.1 40 362\n2 62.1 8th & 9th grade (Plain English) 9.2 39 360\n3 76.8 7th grade (Fairly easy to read) 11.4 39 443\n4 69.6 8th & 9th grade (Plain English) 9.9 47 467\nG. Sivaramakrishnan et al.\n4\nBDJ Open           (2025) 11:59 \nInterestingly, these factors can be inﬂuenced by how the prompts\nare framed. For example, explicitly instructing the models to“use\nsimple and easy words so that a sixth grader can understand” or\n“limit responses to 100 words ” may improve readability and\nconciseness. Such strategies are valuable for tailoring LLMs\noutputs to different audiences or scenarios, especially in health\ncommunication or patient education contexts. Future work could\nexplore systematically how prompt modiﬁcations affect readability\nand length across various models and scenarios.\nPatient education materials should be clear, concise, and easily\nunderstandable to ensure effective communication [ 12]. Key\nfeatures include simple, non-technical language that is accessible\nto a wide range of literacy levels, along with a logical structure\nthat guides the reader through the content [13]. Visual aids, such\nas diagrams, infographics, or images, are crucial in enhancing\nunderstanding and providing clarity for complex medical concepts\n[14]. Actionable steps or instructions should be prominently\nhighlighted to help patients follow through with care recommen-\ndations. Furthermore, the material should be culturally sensitive\nand tailored to the patient ’s speciﬁc needs, ensuring that it\nresonates with their background and health conditions [15, 16]. It\nshould also include clear contact information for further questions\nor assistance, fostering patient engagement and empowerment.\nLastly, materials should be visually appealing, with a clean layout\nand ample white space to make it easy for patients to navigate\nand focus on important information. The responses received from\nall four models included in this study did not include any images,\ninfographics, or visual representations primarily because these\nmodels are designed to generate and process text-based content\nonly. While they excel at providing written responses, they are not\ninherently equipped to produce or interpret visual elements like\nimages or diagrams [17]. However, it is important to note that\nChatGPT 4.0 does have the capability to generate images in some\ncontexts, depending on the platform and settings used. Despite\nthis, the models remain focused on generating human-readable\ntext for a variety of applications, including healthcare commu-\nnication, but generally lack the integration of image creation or\nediting functionalities [17–19]. As a result, their output is limited to\ntextual information, making it necessary for human intervention to\nadd visual aids, such as images or infographics, during theﬁnal\nstages of content development, especially for PEMs where visual\naids play a crucial role in improving comprehension.\nIn addition to images, LLMs cannot offer personalized content\ntailored to an individual’s speciﬁc health condition, demographic,\nor preferences, as they rely on general inputs. To overcome the\ngeneral-purpose nature of these models and improve their\ndomain speciﬁcity, recent efforts have focused on ﬁne-tuning\nLLMs using approaches such as Retrieval Augmented Generation\n(RAG). RAG combines LLMs with external knowledge retrieval,\nallowing models to access up-to-date and specialized information\nrelevant to a user’s query. This method can enhance the accuracy\nand contextual relevance of generated content in healthcare\nsettings. Batool et al. [20]. demonstrated the use of an embedded\nGPT model tailored for post-operative dental care, showing\nimproved performance compared to standard ChatGPT. Similarly,\nUmer et al. [ 21]. applied RAG-enhanced LLM techniques to\ntransform educational journal clubs, addressing speciﬁc learning\nchallenges. Incorporating such domain-adapted models may\nbridge the gap between generalist LLM outputs and the need\nfor precise, personalized patient education materials. They also\nlack the ability to generate real-time updates or access live data,\nmeaning that the content may not reﬂect the most current clinical\nguidelines or patient outcomes. These models also do not provide\nclinical decision support, patient-speciﬁc instructions, or ensure\ncompliance with local healthcare regulations, making human\noversight necessary. Furthermore, LLMs cannot replicate the\nhuman element of empathy, which is essential for reassuring\npatients, nor do they always account for cultural sensitivities or\nprovide reliable citations [22, 23]. As a result, while LLMs can\ngenerate informative content, they are not fully equipped to\nproduce dynamic, personalized, and compliant patient informa-\ntion materials without human intervention.\nOne limitation of the current study relates to the simplicity of\nthe prompts provided to the LLMs. Although identical base\nprompts were used for all models in our study to maintain\nconsistency and minimize variability due to prompt design, these\nprompts were intentionally kept basic. It is well-established in the\nliterature that the quality of LLM outputs depends heavily on the\nquality and speci ﬁcity of the prompts given [ 24–26]. More\ncomplex or detailed prompts could potentially elicit more\naccurate or nuanced responses from the models [27]. However,\nwe deliberately chose simple prompts to simulate typical real-\nworld scenarios where users may not craft elaborate instructions.\nThis approach re ﬂects practical conditions under which PEMs\nmight be generated by users with limited expertise in prompt\nengineering. Future research could explore how varying prompt\ncomplexity impacts the quality of generated health communica-\ntion materials.\nThis study evaluated LLM performance using only four dental\nscenarios. While these scenarios were chosen for their clinical\nrelevance and diversity — covering preventive care, emergency\nmanagement, routine post-treatment instructions, and early\ndetection— they represent only a subset of the broad range of\npatient education needs in dentistry. Consequently, theﬁndings\nmay have limited generalizability to other dental topics or more\ncomplex clinical situations. Future research should include a wider\nvariety of scenarios to better assess the comprehensive capabil-\nities of LLMs in dental patient education.\nIn conclusion, while LLMs demonstrate promising capabilities\nin generating patient education materials, their current limita-\ntions underscore the critical need for human oversight and\nintervention. Although these models excel at producing\ncoherent text-based content, they generally lack the ability to\ncreate visual aids, tailor information to individual patient\ncharacteristics, or integrate real- time clinical data. Additionally,\nLLMs cannot fully replicate essential human qualities such as\nempathy and cultural sensitivity, which are crucial for effective\nhealthcare communication. Recent advancements, including\nﬁne-tuning approaches like RAG, offer pathways to enhance\nmodel speciﬁcity and relevance in healthcare domains. However,\neven with these improvements, LLM-generated content should\nbe considered as a supportive tool for healthcare professionals\nrather than a standalone solution. Ensuring optimal patient\nunderstanding and engagement requires continued reﬁnement\nof these models combined with active human involvement to\naddress their current shortcomings.\nDATA AVAILABILITY\nThe datasets used and/or analyzed during the current study are available from the\ncorresponding author on reasonable request.\nREFERENCES\n1. Alexander RE. Readability of published dental educational materials. J Am Dent\nAssoc. 2000;131:937–42. https://doi.org/10.14219/jada.archive.2000.0312.\n2. Ho JCY, Chai HH, Lo ECM, Huang MZ, Chu CH. Strategies for effective dentist-\npatient communication: a literature review. Patient Preference Adherence.\n2024;18:1385–94. https://doi.org/10.2147/PPA.S465221.\n3. Shoemaker SJ, Wolf MS, Brach C. Development of the patient education materials\nassessment tool (PEMAT): a new measure of understandability and actionability\nfor print and audiovisual patient information. Patient Educ Counsel.\n2014;96:395–403. https://doi.org/10.1016/j.pec.2014.05.027.\n4. Chiesa-Estomba CM, Lechien JR, Vaira LA, Brunet A, Cammaroto G, Mayo-Yanez\nM, et al. Exploring the potential of Chat-GPT as a supportive tool for sia-\nlendoscopy clinical decision making and patient information support. Eur\nArch Otorhinolaryngol. 2024;281:2081 –6. https://doi.org/10.1007/s00405-023-\n08104-8.\nG. Sivaramakrishnan et al.\n5\nBDJ Open           (2025) 11:59 \n5. Shamil E, Jaafar M, Fan KS, Ko TK, Schuster-Bruce J, Eynon-Lewis N, et al. The use\nof large language models such as ChatGPT on delivering patient information\nrelating to surgery. Facial Plastic Surg. 2024.https://doi.org/10.1055/a-2413-3529.\n6. Patil S, Shankar H. Transforming healthcare: harnessing the power of AI in the\nmodern era. Int J Multidiscip Sci Arts. 2023;2:60–70.\n7. Alowais SA, Alghamdi SS, Alsuhebany N, Alqahtani T, Alshaya AI, Almohareb SN,\net al. Revolutionizing healthcare: the role of artiﬁcial intelligence in clinical practice.\nBMC Med Educ. 2023;23:689.https://doi.org/10.1186/s12909-023-04698-z.\n8. Thacharodi A, Hassan S, Vithlani A, Ahmed T, Kavish S, Geli Blacknell NM, et al.\nRevolutionizing healthcare and medicine: the impact of modern technologies for\na healthier future— a comprehensive review. Health Care Sci. 2024;3:329 –49.\nhttps://doi.org/10.1002/hcs2.115.\n9. Bélisle-Pipon J-C. Why we need to be careful with LLMs in medicine. Front Med.\n2024;11. https://doi.org/10.3389/fmed.2024.1495582.\n10. National Institutes of Health. MedlinePlus: how to write easy to read health mate-\nrials. Available athttps://cir.nii.ac.jp/crid/1571417126226435840. Accessed 2025.\n11. Centers of Disease Control, Prevention. Simply put: a guide for creating easy-to-\nunderstand materials. Atlanta, GA: Centers for Disease Control and Prevention.\n12. Hansberry DR, Agarwal N, Shah R, Schmitt PJ, Baredes S, Setzen M, et al. Analysis\nof the readability of patient education materials from surgical subspecialties.\nLaryngoscope. 2014;124:405–12. https://doi.org/10.1002/lary.24261.\n13. Hoffmann T, Worrall L. Designing effective written health education materials:\nconsiderations for health professionals. Disabil Rehabilit. 2004;26:1166 –73.\nhttps://doi.org/10.1080/09638280410001724816.\n14. Cohen SM, Baimas-George M, Ponce C, Chen N, Bain PA, Ganske IM, et al. Is a\npicture worth a thousand words? A scoping review of the impact of visual aids on\npatients undergoing surgery. J Surg Educ. 2024;81:1276 –92. https://doi.org/\n10.1016/j.jsurg.2024.06.002.\n15. Ho EY, Tran H, Chesla CA. Assessing the cultural in culturally sensitive printed\npatient-education materials for Chinese Americans with type 2 diabetes. Health\nCommun. 2015;30:39–49. https://doi.org/10.1080/10410236.2013.835216.\n16. Haynes D, Hughes KD, Okafor A. PEARL: a guide for developing community-\nengaging and culturally-sensitive education materials. J Immigr Minority Health.\n2023;25:666–73. https://doi.org/10.1007/s10903-022-01418-5.\n17. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large\nlanguage models in medicine. Nat Med. 2023;29:1930 –40. https://doi.org/\n10.1038/s41591-023-02448-8.\n18. Safranek CW, Sidamon-Eristoff AE, Gilson A, Chartash D. The Role of large lan-\nguage models in medical education: applications and implications. JMIR Med\nEduc. 2023;9:e50945 https://doi.org/10.2196/50945.\n19. Deng J, Zubair A, Park Y-J. Limitations of large language models in medical appli-\ncations. Postgrad Med J. 2023;99:1298–9. https://doi.org/10.1093/postmj/qgad069.\n20. Batool I, Naved N, Kazmi SMR, Umer F. Leveraging Large Language Models in the\ndelivery of post-operative dental care: a comparison between an embedded GPT model\nand ChatGPT. BDJ Open. 2024;10:1–7. https://doi.org/10.1038/s41405-024-00226-3.\n21. Umer F, Naved N, Naseem A, Mansoor A, Kazmi SMR. Transforming education:\ntackling the two sigma problem with AI in journal clubs– a proof of concept. BDJ\nOpen. 2025;11:1–5. https://doi.org/10.1038/s41405-025-00338-4.\n22. Wang D, Zhang S. Large language models in medical and healthcare ﬁ\nelds:\napplications, advances, and challenges. Artif Intell Rev. 2024;57:299 https://\ndoi.org/10.1007/s10462-024-10921-0.\n23. Laskar MTR, Alqahtani S, Bari MS, Rahman M, Khan MAM, Khan H, et al. A sys-\ntematic survey and critical review on evaluating large language models: chal-\nlenges, limitations, and recommendations. In: Al-Onaizan Y, Bansal M, Chen Y-N,\neditors. In: Proceedings of the 2024 conference on empirical methods in natural\nlanguage processing. Miami, Florida, USA: Association for Computational Lin-\nguistics; 2024. p. 13785–816.\n24. Jacobsen LJ, Weber KE. The promises and pitfalls of large language models as\nfeedback providers: a study of prompt engineering and the quality of AI-driven\nfeedback. MDPI. 2025;6:35.\n25. Mao W, Wu J, Chen W, Gao C, Wang X, He X. Reinforced prompt personalization\nfor recommendation with large language models. ACM Trans Inf Syst.\n2025;43:1–27. https://doi.org/10.1145/3716320.\n26. Zhou H, Hu C, Yuan D, Yuan Y, Wu D, Chen X, et al. Large language models for wireless\nnetworks: an overview from the prompt engineering perspective. In: IEEE Wireless\nCommunications. IEEE; 2025;1-9.https://doi.org/10.1109/MWC.001.2400384.\n27. Maaz S, Palaganas JC, Palaganas G, Bajwa M. A guide to prompt design: foun-\ndations and applications for healthcare simulationists. Front Media SA. 2025:11.\nACKNOWLEDGEMENTS\nWe acknowledge the use of Chat GPT for checking language and grammatical errors.\nAUTHOR CONTRIBUTIONS\nGS, MA, SA, ML, EA contributed to conceptualization and design; GS, MA, SA, ML, EA,\nKS analyzed and interpreted the patient data. GS, MA, SA, ML, EA, KS contributed in\nwriting the manuscript. All authors read and approved theﬁnal manuscript.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41405-025-00349-1.\nCorrespondence and requests for materials should be addressed to\nGowri Sivaramakrishnan.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nG. Sivaramakrishnan et al.\n6\nBDJ Open           (2025) 11:59 ",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.9721471071243286
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.7220749855041504
    },
    {
      "name": "Kappa",
      "score": 0.5119248628616333
    },
    {
      "name": "Medical physics",
      "score": 0.48090124130249023
    },
    {
      "name": "Cohen's kappa",
      "score": 0.46005910634994507
    },
    {
      "name": "Sonnet",
      "score": 0.4297780394554138
    },
    {
      "name": "Medical education",
      "score": 0.42415809631347656
    },
    {
      "name": "Computer science",
      "score": 0.4095118045806885
    },
    {
      "name": "Psychology",
      "score": 0.4075712561607361
    },
    {
      "name": "Medicine",
      "score": 0.37975749373435974
    },
    {
      "name": "Natural language processing",
      "score": 0.3274843096733093
    },
    {
      "name": "Power (physics)",
      "score": 0.2621130049228668
    },
    {
      "name": "Mathematics",
      "score": 0.12656143307685852
    },
    {
      "name": "Linguistics",
      "score": 0.10179391503334045
    },
    {
      "name": "Machine learning",
      "score": 0.09880629181861877
    },
    {
      "name": "Poetry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I53218197",
      "name": "Arabian Gulf University",
      "country": "BH"
    }
  ]
}