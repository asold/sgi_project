{
  "title": "An LLM-based hybrid approach for enhanced automated essay scoring",
  "url": "https://openalex.org/W4409800192",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1965241839",
      "name": "John Atkinson",
      "affiliations": [
        "Empowerment Program"
      ]
    },
    {
      "id": "https://openalex.org/A2116722748",
      "name": "Diego Palma",
      "affiliations": [
        "Empowerment Program"
      ]
    },
    {
      "id": "https://openalex.org/A1965241839",
      "name": "John Atkinson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116722748",
      "name": "Diego Palma",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093534489",
    "https://openalex.org/W2799207367",
    "https://openalex.org/W2804545603",
    "https://openalex.org/W2964413085",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W2569115912",
    "https://openalex.org/W2922211092",
    "https://openalex.org/W2094401778",
    "https://openalex.org/W2799272084",
    "https://openalex.org/W2480317955",
    "https://openalex.org/W2898140483",
    "https://openalex.org/W2090733459",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3037207300",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W2962851685",
    "https://openalex.org/W2963830885",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W1732828232"
  ],
  "abstract": null,
  "full_text": "An LLM-based hybrid approach for \nenhanced automated essay scoring\nJohn Atkinson & Diego Palma\nAutomated Essay Scoring systems have traditionally relied on shallow lexical data, such as word \nfrequency and sentence length, to assess essays. However, these approaches neglect crucial factors \nlike text structure and semantics, resulting in limited evaluations of coherence and quality. To address \nthese limitations, we propose a hybrid approach to AES that combines multiple features from different \nlinguistic levels. By leveraging the complementary nature of these features, our model captures the \nintricate relationships underlying coherent texts. Through extensive experimentation using standard \nessay datasets, we demonstrate that our large language model based hybrid model surpasses state-\nof-the-art methods based on shallow features and pure neural networks. This research represents \na significant advancement towards the development of an accurate and effective tool for assessing \nstudent writing.\nKeywords Automated essay scoring, Large language models, GPT, Natural-language processing, Neural \ncontext embeddings, Transformer\nWith the recent advances in computer technology, the possibility of grading essays with the assistance of \ncomputers has become a reality. Automated Essay Scoring (AES) has made it possible for teachers to assign \nscores to essays through computer analysis 1,2. AES programs operate by extracting a variety of features, such \nas word count, vocabulary choice, error density, sentence length variance, and paragraph structure from high-\nscoring essays to establish a model of essay quality3.\nEssay assessment is a rigorous and time-consuming task for human graders. As such, text/essay assessment \ncomputational approaches have been developed in recent years to automate the process. However, many of \nthese methods only use statistical features extracted from a text, such as word frequency, length, and sentence \nfrequency. As a result, these methods can overlook the text’s syntactical structure, leading to nonsensical essays \nthat receive high ratings. Conversely, text coherence is often not accounted for, resulting in poorly structured or \nsemantically phrased texts receiving good scores.\nWhile all these approaches deal with coherence evaluation, they use shallow lexical features and/or lexical \nsemantics with no syntax structure to assess a text’s coherence. This, in turn, largely restricts the models’ power \nto address a text’s semantic and discourse properties that make it understandable in terms of syntactic, semantic \nand discourse connections, as a whole. Furthermore, many linguistic features complement each other, and \nwhen combined with context embeddings4, hidden relationships that underlie coherent texts can be taken into \naccount. This hybrid model extends our past research on shallow linguistic features and discourse patterns by \nincorporating neural network context embeddings, resulting in an effective assessment of essay quality5,6.\nUnlike previous work, combining multiple features into different linguistic levels might be effective to grade \nan essay and be comparable to human performance. The rationale for this is that many linguistic features are \ncomplementary with each other and combining them with context embeddings can give account for hidden \nrelationships that underlie coherent texts. Hence in this work, a hybrid model that combines shallow liguistic \nfeatures, discourse patterns and neural context embeddings is proposed to effectvely assess the quality of essays5.\nGold standards for a discourse are provided from essay datasets previously graded by human experts. A \nsimple supervised classifier is then used to predict an essay score7,8, ensuring that the model accurately assesses \nthe quality of the essay. Our AES approach offers a more efficient and reliable way of grading essays, ultimately \nleading to improved student learning outcomes.\nAccordingly, our main claim is that an AES method that considers shallow features discourse patterns \nand neural context embeddings can be more effective (i.e., well correlated with humans) than state-of-the-art \napproaches using purse shallow features or transformer-based neural learning.\nRelated work\nCurrent AES approaches vary widely, from using traditional linguistic features and discourse analysis methods \nto purely deep neural network models that learn efficient low-dimensional context representations, known as \nAI Empowered, Santiago, Chile. email: john.atkinson@uai.cl\nOPEN\nScientific Reports |        (2025) 15:14551 1| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports\n\nembeddings. However, using shallow linguistic features to evaluate essays has limitations, as it does not consider \nthe overall cohesion and coherence of the essay. This can lead to highly rated but meaningless essays9.\nShallow features based methods\nAES methods have usually been approached by extracting shallow linguistic features from a an essay as indicators \nof its quality. It often includes the frequency of words and/or sentences, frequency of grammar errors, lexical \ncategories, and readability indices10,11, or even more complex indicators such as a text’s lexical diversity regarded \nto as the used vocabulary. By using these features, an AES task can simply be seen as a regression problem task in \nthese features are weighted so as to predict an essay’s score, where the features’ weights are estimated by collecting \nsamples from human-assessed essays9.\nOne earlier popular system, E-Rater12, can provide a holistic assessment for an essay as well as making real-\ntime diagnostic feedback on grammar, word usage, style, and organization. Using this approach, human-model \ncorrelations greater than 0.8 were achieved. However, the method did not receive a proper acceptance in the \neducational community as using such simple and indirect measures, the approach would have a negative impact \non the students, since they would concentrate more on satisfying the metrics rather than on producing a good \nessay13–18.\nReadability-based approaches\nSome AES methods approach an essay’s quality as a measure of its readability in which text difficulty is assessed \nby identifying grammatical and semantic features in the Inter-agency Language Roundtable (ILR)  (   h t t p s : / / w w \nw . g o v t i l r . o r g /     ) scale. This language proficiency metrics can be used to assess the complexity of the language \nfrom no profciency skills to native proficiency. Since the domain is a continuous spectrum, assessing a text’s \ndifficulty is seen as a regression problem that estimates the level of difficulty on the ILR scale (i.e., 0–5). In this \napproach, the Morfessor Categories-Map algorithm is applied for essay segmentation and uses relative entropy \nand statistical language models to measure its semantic complexity. Experiments showed that when compared \nto a traditional machine learning classifier such as Support Vector Machines8 the prediction errors differ by less \nthan 0.07. The analysis considered three types of word usage features (i.e., word usage vectors), length measures \n(i.e., average length of words or sentences per document, relative entropy, average length of morphemes) and \nlinguistic models (i.e., statistical language models according to the ILR level).\nCoherence-based methods\nIn order to convey a communicative goal for a specific reader, a text must contain consistent and logical \nrelationships between its utterances. Thus, a high-quality essay must be coherent, that is, it must have logical \nconnections between units of information, both local (i.e., between adjacent statements) and global (i.e., all \nconnections make sense in the full text—a discourse). In addition, a good essay should have high cohesion, that \nis, each sentence must be logically interpreted in relation to the others by using discourse markers, or semantic \nrelationships connecting different sentences and paragraphs. Some coherence-based approaches use discourse \npatterns that can be extracted from semantic feature sequences contained in essays9. The baseline for a coherent \ndiscourse is usually provided by texts that have previously been assessed by human experts, which are passed on \nto an automatic classifier that can predict an essay’s grade19.\nUnlike previous approaches, this method combines latent space representation semantic coherence metrics \nand probabilistic entity grids-based discourse patterns in order to predict an essay’s score 16,19. To this end, the \nmethod extracts three kinds of features: \n 1. Shallow Features: they are obtained by estimating metrics such as word frequencies, syllabes, etc.\n 2. Semantic Features: they are extracted from GPT-3 embeddings for words and sentences.\n 3. Discourse Patterns: they are estimated by computing the transition probabilities from entity grids, which are \ngenerated from previously assessed essays. Patterns are then represented as vectors of entity transition prob-\nabilities.\nA prediction model is then trained using those features from a training essay dataset to estimate the essay grade.\nDiscourse patterns based methods\nThis approach uses discourse pattern features to estimate an essay’s coherence by combining two elements: \n 1. Entity Grids (EG): Each essay is represented a two-dimensional matrix that captures the probabilistic distri-\nbution of discourse entities in an essay’s sentences (aka. entity grids), in the form of transition patterns. This \nassumes that the distribution of the entities in coherent essays show some regularities in the corresponding \nEG so that the underlying essays will receive a better evaluation than those that do not show these regulari-\nties. Sample essays are divided into k sets of distinct evaluations. Cosine similarity is then calculated between \na new essay and the test essay dataset, based on their probability transition vectors. This is the probability \nthat, being in a state Ei at time n, it passes to state Ei at time n +1 .\n 2. Semantic Metrics: These are divided into two groups: \n (a) Lexicon: it compares the lexical content of a new essay with that of human-assessed essay. Each essay is \nrepresented as a vector containing the frequency of the most relevant words.\n (b) Semantics: it measures an essay’s overall coherence as the semantic similarity between adjacent sentenc-\nes using the semantic embeddings extracted from GPT-3.\nScientific Reports |        (2025) 15:14551 2| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\nThe experiments used standard metrics such as weighted quadratic Kappa and Spearman’s correlation 20 to \ndetermine the model-human rater correlation. These were compared against a reference method that uses \nshallow features. The results show a good correlation above 0.7 in most tests, outperforming shallow features-\nbased approaches21. However, it’s worth noting that traditional AES models are effective when evaluating essays \non a specific topic, but when it comes to free text, their performance is reduced. One way to address this issue is \nby learning low-dimensional vector representations, such as word embeddings, that can be used to measure an \nessay’s local coherence without the need for reference essays.\nNeural context embedding based and LLM approaches\nTraditional AES models are effective when evaluating essays on a specific topic. However, when a free text is \napproached, their performance is substantially reduced. One way to address this problem is by learning low-\ndimensional vector representations, aka. word embeddings that can later be used to compare adjacent units in \nan essay as a measure of its local coherence, without the need for reference essays.\nSimple word embeddings can be learned from methods such as Word2vec, GloVe which use simple neural \nnetwork architectures21–24. One of the major problems with simple word embeddings methods is that they \ncannot capture complex contextual relationships in texts. In order to deal with these issues, deep neural network \nmethods based on transformer architectures such as BERT, GPT, LLama have been introduced in order to train \nlarge language models  (LLM)25–27. Unlike previous word embeddings methods, LLMs give account for some \nsemantic connections underlying the word/sentences by learning context embeddings using bi-directional \nrelationships and focusing in the most prominent part of the sentences via attention mechanisms 28,29. Unlike \nprevious approaches, fine-tune these LLMs on essay-specific datasets may be even more effective to the task of \nAES30–33.\nA significant issue for AES may be due to that LLM models even those handling large context windows \ncannot account for deeper semantic relationships and discourse patterns that underlie coherent essays. \nFurthermore, classical approaches are unable to generalize to new new topics in essays, especially in free texts. \nWhile transformers-based LLM are sensitive to the data they were trained on, they are well-suited for domain \ntransfer tasks, mostly unseen in some NLP problems7.\nSome of the features that have been used in recent approaches include lexical items, word embeddings4,22,34,35, \nword categories6, readability difficulty of an essay, syntactical and semantic items, discourse features. The latter \nones can encode an essay’s discourse structure which may be produced by the following methods: \n 1. Entity Grids (EG): these are probabilistic representations of discourse that are used to capture the local coher-\nence of a text so that local coherence can be derived from estimating utterances’ centers as they move along \nwith a discourse.\n 2. Discourse Analysis Trees: these are based on rhetorical structures that encode a text’s hierarchical discursive \nrelations and are used to capture an essay’s local and global coherence.\n 3. Lexical Chains: these are sequences of related words in a text, which are used as indicators of its cohesion. \nIntuitively, an essay containing many lexical chains tends to be more cohesive.\nA hybrid model for AES combining LLM-based neural embeddings and shallow \nlinguistic data\nIn the realm of AES, this research presents a novel hybrid model that propels the scoring process to new \nheights. By combining the strengths of shallow feature approaches and the improved accuracy of neural context \nembeddings derived from GPT-based models, our model offers an innovative solution for AES. This approach \nbuilds upon our prior work on extracting shallow linguistic features for AES 19, extending it by incorporating \nstate-of-the-art contextual embeddings from generative transformer-based architectures like GPT (Generative \nPretrained Transformer). Through the integration of rich contextual information into a straightforward classifier, \nour model achieves unparalleled accuracy in predicting essay grades. To evaluate the efficacy of our model, a \ncomparative analysis was conducted against a dataset of essays that had been previously assessed by human \nexperts. The results yielded promising findings, establishing a high correlation between our model’s predictions \nand human assessments. This work details the architecture and components of our model, as illustrated in Fig. 1.\nShallow features\nThe approach uses shallow features (SF) that allow evaluating the following aspects of an essay:\n• Readability: it aims at estimating how easy is to read a text by using four metrics: \n 1. Gunning-Fog index: it estimates the number of years of formal training a person would need to understand \na text on a first reading, and is calculated as Average length of a sentence + HW where HW is the number \nof words containing more than two syllables.\n 2. Flesh Index (FRE):  it evaluates the readability of a text and is calculated as \n206.835 − (1.015ASL) − (8.46ASW ), where ASL is the Average Sentence Length and ASW is the Av-\nerage number of Syllables per Word.\n 3. ARI Index: it estimates the degree of text comprehension by measuring the number of linguistic features \nused and then estimating the training required to understand a text: 0.5(WPS )+4 .71(CPW ) − 21.43\n, where WPS is the average number of Words Per Sentence and CPW is the number of Characters Per \nword.\nScientific Reports |        (2025) 15:14551 3| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\n• Lexical Diversity: it estimates the diversity of the vocabulary used by a writer in an essay and can be obtained \nusing the following metrics. Diversity is accomplished by using the Guiraud Index (GI) which penalizes a text \ndepending on its length (e.g., the longer, the worse) and is calculated as v√\nN , where v is the number of distinct \nwords and N the number of words.\n• Grammar: it aims at measuring the efficient use of linguistic resources such as lexical categories, n-gram se -\nquences, frequency of words that belong to some lexical category (i.e., verbs, nouns).\n• Complexity: Our novel approach to AES sets itself apart from previous methods by not only grading an essay’s \ncoherence but also assessing its complexity. Our model estimates an essay’s degree of information gain by \nanalyzing adjacent units such as sentences or noun phrases (NPs) and determining the amount of newness or \ngiveness they convey. This provides a more holistic evaluation of an essay’s overall quality, taking into account \nits level of complexity. By incorporating this additional feature, our model offers a more comprehensive and \nnuanced analysis of essays that is unmatched by other AES methods.\n• Accordingly, the components of an essay can be grouped into three classes: given, partially given (i.e., based \non various types of inferential availability), and not given (i.e., newness). When assessing a text, it becomes \nmore natural to include new information as that is not given. Hence the complexity should first measure how \nmuch giveness is contained in an essay and then estimate the remaining information as newness. In addition, \ntwo additional metrics are considered to assess a text complexity:\n• Giveness: in order to estimate this metric, sentences of an essay are converted into a vector space representa-\ntion. An hyperplane of all the sentence vectors but the target sentence (G) are then built. The current sen-\ntence vector is projected onto that hyperplane. This projection is seen as a vector component shared with a \nprevious essay whereas the vector component that is orthogonal to the hyperplane is seen as a new sentence \n(N). Giveness is then calculated as N\n(N+G)\n• Pronoun Density: An essay with a higher density of pronouns will be more difficult to read than a text \nwith a lower density of pronouns as more comprehension inferences must be made. It can be estimated by \ncalculating the proportion between the pronouns (PN) and the total number of words (Nwords) an essay: \nPN\n(Nwords) .\n• Pronoun-Noun relationship: Since pronouns are co-references to a previously introduced proper name or \nnoun, this relationship becomes a measure of complexity of reading the text. Thus, it can be estimated as \nthe ratio between the number of pronouns (PN) and the total number of nouns (TN), that is, PN\nTN .\nNeural context embeddings from generative models\nOne of the problems with traditional embedding generation models is that vectors representing meaning are \nstatic and unique regardless of the context where words are used. Therefore, different meanings of a word share \nthe same representation, which bring serious problems with dealing with polysemy and synonymy. One way to \naddress this issue is by learning low-dimensional vectors, aka. embeddings using transformer-based generative \nmodels such as GPT, which allow learning context-sensitive word representations. Unlike previous approaches \nto AES19, our model uses neural context embeddings as a major feature to measure local coherence based on the \nsimilarity between the representation of adjacent sentences.\nUnlike BERT and other recent LLMs, generative models like GPT offers several advantages for our target \nproblem:\n• Unidirectional Context: these generate text in a unidirectional manner. This allows them to generate coherent \nand contextually appropriate responses to prompts, making them suitable for tasks where understanding the \ncontext of the entire input is crucial, such as essay writing.\nFig. 1. Our GPT-based hybrid model for AES.\n \nScientific Reports |        (2025) 15:14551 4| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\n• Generative Capability: these are generative, meaning they can produce human-like text based on a given \nprompt. This capability is particularly useful for tasks that require producing language output, such as text \ncompletion and essay production.\n• Longer Contextual Understanding: these have a broader contextual understanding over longer passages com-\npared to BERT. This enables them to capture more nuanced relationships and dependencies within the text, \nleading to better performance on tasks that require understanding complex language structures.\n• Scalability: these can be scaled up to include a larger number of parameters, which has been shown to improve \nperformance on various NLP tasks.\nIn our hybrid model for AES, these neural context embeddings are learnt from a decoder-only transformer \nprovided by GPT as described in Fig. 2. GPT operates by first pretraining on vast amounts of text data using \nunsupervised learning, where it learns to predict the next word in a sequence given the preceding words. This \npretraining process enables GPT to capture general language patterns and context from diverse text sources. \nGPT employs a transformer architecture with self-attention mechanisms, allowing it to effectively weigh the \nimportance of different words in the input sequence and capture long-range dependencies. Stacked transformer \nlayers refine the learned representations, enabling the model to capture increasingly complex linguistic patterns. \nDuring fine-tuning on specific tasks, GPT adapts its pretrained knowledge to the requirements of the target \ndomain, optimizing its performance for a target task.\nIn order to generate neural embeddings, GPT first tokenizes input text into subword units or tokens and \nthen representing each token as an embedding vector (Fig. 2). These embeddings capture the semantic meaning \nof the tokens within the context of the entire input sequence. GPT incorporates positional encoding to provide \ninformation about the position of tokens within the sequence, allowing the model to distinguish between tokens \nat different positions and capture their relative positions accurately. The embeddings are generated through \na series of transformer layers, where each layer refines the representations learned from the previous layers. \nThrough self-attention mechanisms, GPT effectively weighs the importance of different tokens in the input \nsequence and captures long-range dependencies, leading to rich and contextualized embeddings that encode the \nsemantic and positional information of the input text effectively.\nExperiment and results\nIn order to assess the efficacy of our hybrid model for AES, we conducted a rigorous evaluation by implementing \ndiverse shallow feature (SF) extraction techniques in conjunction with a pretrained GPT-3 model for contextual \nembeddings learning. To validate our model’s effectiveness, we leveraged a range of standard metrics and \nbenchmarked it against datasets of human-annotated essays. We set our model apart by comparing it with \nestablished state-of-the-art AES techniques such as discourse patterns (DP), Semantic Coherence (SC), and GPT-\nonly approaches. This comparison allowed us to demonstrate the superiority of our hybrid model over existing \nmethods in terms of precision, recall, and other critical evaluation metrics.\nFig. 2. A Decoder-only architecture in GPT for essay scoring.\n \nScientific Reports |        (2025) 15:14551 5| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\nEssay collection\nSeveral essay corpora from AES competitions available at Kaggle ( https://www.kaggle.com/c/asap-aes) were \ncollected. These are organized into four essay categories: Persuasive (PER), Narrative (NAR), Expository (EXP), \nand Source-Dependent Response (SDR). Essays were written by high school students (8–10th degree) and \nmanually scored by humans, except dataset no. 2, which was rated using different criteria (Table 1).\nIt is important to highlight that essays dataset 3, 4, 5, and 6 are not free-form texts but the result of a script \nanswering a question based on a given source. Hence, this type of essay has a higher semantic closeness to a \nquestion/query compared to free-form texts on a topic.\nLinguistic feature extraction\nA set of shallow features, metrics of essay complexity, and GPT-3 embeddings were extracted to create a feature \nvector that can be used to predict an essay’s final grade. This included Shallow Features (i.e., Readability and \nlexical diversity indices, and grammatical features), Essay’s complexity which considered Pronoun Density and \nPronoun-Noun Relationship:, and Context Embeddings from GPT-3.\nPredicting an essay’s score\nTo generate the prediction model, a simple model based on Random Forests was trained and used. To this end, \nthe dataset was divided into a training set and a test set using cross-validation methods such as K-fold (k = 10), \nusing the Random Forest Regression and Random Forest Classifier implementations in Python.\nAssessing the hybrid AES model\nThe performance of our approach was measured by using four standard metrics: \n 1. Spearman Correlation (r): it measures the correlation between the model and a human evaluator.\n 2. Exact Agreement (EA): it evaluates the proportion of essays that were assessed with the same grade as a hu -\nman evaluator.\n 3. Adjacent Agreement (AA): it measures the proportion of essays that were assessed with a maximum of one \nscore of difference with that of a human evaluator.\n 4. Quadratic Weighted Kappa (QWK): it evaluates the level of agreement between two raters who classify ítems \ninto an ordinal scale. This is specially important for our dataset as the majority of score range is small so AA \nis not enough for the assessment.\nFirstly, experiments were conducted to compare our model against two main approaches: (1) Shallow Features \n(SF) + Discourse Patterns (DP) and (2) GPT + SF as shown in Tables 2, 3, and 4 respectively for all the metrics.\nThe results show that our model outperforms the approaches based on PD+SF and GPT+SF on the QWK \nmetric, with a significant difference in essays 1, 2a, 2b and 8. In general, the average QWK of our approach is \nhigher than those of the other approaches, indicating a higher correlation with human evaluation.\nOur hybrid model for AES has shown significant improvements in the assessment of Persuasive, Narrative \nand Expository essays when compared to state-of-the-art approaches such as discourse patterns (DP), local \nsemantic coherence (| and pure GPT-based methods. This can be partially attributed to GPT’s focus on the \ncontextual meaning of words, enabling it to better evaluate essays with greater writing flexibility. However, the \nlimitations of GPT’s inability to generate embeddings for words not appearing in the training dataset emphasizes \n1 2a 2b 3 4 5 6 7 8 Avg.\nOur model 0.72 0.79 0.81 0.75 0.76 0.77 0.76 0.29 0.38 0.67\nDP 0.64 0.77 0.76 0.71 0.74 0.74 0.73 0.26 0.35 0.63\nSC 0.44 0.66 0.66 0.60 0.55 0.59 0.56 0.17 0.17 0.48\nGPT-3 0.41 0.62 0.74 0.68 0.67 0.70 0.68 0.22 0.25 0.55\nTable 2. EA for different AES approaches.\n \nID Type Grade level Size of training dataset (essays) Size of test dataset (essays) Average length (words) Score range\n1 PER, NAR, EXP 8 1785 592 350 2–12\n2 PER, NAR, EXP 10 1800 600 350 1–6, 1–4\n3 SDR 10 1726 575 150 0–3\n4 SDR 10 1772 589 150 0–3\n5 SDR 8 1805 601 150 0–4\n6 SDR 10 1800 600 150 0–4\n7 PER, NAR, EXP 7 1730 576 250 0–30\n8 PER, NAR, EXP 10 918 305 650 0–60\nTable 1. Essays dataset description.\n \nScientific Reports |        (2025) 15:14551 6| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\nthe importance of incorporating shallow feature (SF) extraction techniques to supplement the contextual \ninformation provided by GPT.\nAdditionally, our hybrid model exhibits a slight drop in performance for essays that require a specific answer \nto a given question. Here, SF-based approaches prove to be more effective than our approach in terms of the \nQWK metric. This is because there is only one correct answer for such essays, leaving little room for the student’s \ncreativity and interpretation.\nHowever, for the Exact Agreement (EA) and Adjacent Agreement (AA) metrics, our approach demonstrates \nsignificant improvements over other AES models. It is noteworthy that for some essay sets, the grade ranges \nbetween 1 and 60, making it difficult for any model to obtain an exact score. Despite this, our hybrid model \noutperforms GPT + SF and DP + SF approaches in these challenging sets. In order to investigate the extent to \nwhich a GPT-3 fine-tuned model may impact the performance, the metrics were also recomputed for the whole \nessay dataset using normalized scores as seen in Table 5.\nFurthermore, our approach was compared against popular state-of-the-art AES systems, where it \noutperformed them on the QWK metric, as shown in Table 6. These results demonstrate the effectiveness and \nsuperiority of our hybrid model for AES compared to both academic and commercial models.\nOur hybrid method for AES has demonstrated better performance over traditional statistical or SF \napproaches, as well as commercial systems such as Lexile, Brookete, LightSide, and Autoscore. These systems \nrely on shallow features and readability measures to predict an essay grade, which limits their ability to capture \ncontextual information and to deal with polysemy and synonymy.\nAlthough SAGE outperforms our approach in several sets of essays, our method significantly outperforms \nSAGE on datasets where no contextual knowledge can be inferred. SAGE relies on multiple centrality-based \nmeasures of textual coherence and ontologies to determine whether certain essay clauses are correct or incorrect. \n1 2a 2b 3 4 5 6 7 8 Avg.\nSAGE 0.93 0.79 0.67 0.83 0.81 0.87 0.78 0.88 0.81 0.82\nPEG 0.82 0.72 0.70 0.75 0.82 0.83 0.81 0.84 0.73 0.78\nOur model 0.91 0.79 0.77 0.71 0.84 0.84 0.84 0.87 0.92 0.83\ne-rater 0.82 0.74 0.69 0.72 0.80 0.81 0.75 0.81 0.70 0.76\nIntelliMetric 0.78 0.70 0.68 0.73 0.79 0.83 0.76 0.81 0.68 0.75\nCRASE 0.76 0.72 0.69 0.73 0.76 0.78 0.78 0.80 0.68 0.74\nLightSIDE 0.79 0.70 0.63 0.74 0.81 0.81 0.75 0.77 0.65 0.74\nAutoScore 0.78 0.68 0.66 0.72 0.75 0.82 0.76 0.67 0.69 0.73\nIEA 0.79 0.70 0.65 0.68 0.77 0.81 0.80 0.78 0.72 0.73\nLexile 0.66 0.62 0.55 0.65 0.67 0.64 0.65 0.58 0.63 0.63\nTable 6. QWK for different state-of-the-Art AES systems. Significant values are in bold.\n \nQWK EA AA\n0.83 0.84 1.0\nTable 5. Results of fine-tuned GPT-3 on the essay dataset.\n \n1 2a 2b 3 4 5 6 7 8 Avg.\nOur model 0.91 0.79 0.77 0.71 0.84 0.84 0.84 0.87 0.92 0.83\nDP 0.88 0.77 0.71 0.69 0.84 0.84 0.84 0.85 0.90 0.81\nSC 0.68 0.55 0.60 0.45 0.52 0.65 0.48 0.66 0.66 0.58\nGPT-3 0.75 0.62 0.69 0.66 0.74 0.79 0.78 0.82 0.56 0.71\nTable 4. QWK for different AES approaches.\n \n1 2a 2b 3 4 5 6 7 8 Avg.\nOur model 0.99 1.0 1.0 0.99 0.99 0.99 0.99 0.82 0.86 0.96\nDP 0.99 1.0 1.0 0.98 0.99 0.99 1.0 0.81 0.86 0.96\nSC 0.94 1.0 1.0 0.98 0.99 0.99 0.99 0.71 0.72 0.92\nGPT-3 0.68 1.0 1.0 0.98 0.99 0.99 0.99 0.78 0.86 0.92\nTable 3. AA for different AES approaches.\n \nScientific Reports |        (2025) 15:14551 7| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\nHowever, our approach’s use of contextual representations provides a deeper sense of language context and flow, \nallowing it to better assess essays where there is greater freedom of writing.\nThe proposed approach, which combines shallow features and neural contextual embeddings, outperforms \nseveral state-of-the-art approaches AES systems for different kinds of essays. Only two working systems, SAGE \nand PEG, outperform our approach due to their use of additional domain knowledge bases. These results \ndemonstrate the effectiveness of our hybrid approach and its potential to contribute to the development of more \naccurate and reliable AES systems.\nConclusions\nThis work introduced a novel hybrid method for AES that combines the power of shallow linguistic features and \ngenerative neural context embeddings via LLM models such as GPT-3. The proposed AES method represents \na significant advancement in essay grading, offering a comprehensive and accurate assessment of essay quality \nby considering various factors such as readability, lexical and grammatical diversity, and reading complexity. By \nincorporating word contexts, the approach effectively evaluates essays with greater writing freedom.\nThrough extensive experimentation, our results suggested that the proposed hybrid approach surpasses \nnumerous state-of-the-art methods and even outperforms several AES systems in terms of human-model \ncorrelation. While some systems, like SAGE, leverage additional knowledge bases for further inferences that our \napproach currently lacks, our hybrid method remains competitive with notable AES systems including SAGE, \nPEG, and e-rater in terms of correlation and agreement metrics.\nThe findings suggest that the proposed hybrid approach exhibits high effectiveness in assessing diverse types \nof essays. Particularly, in datasets with wider grade value ranges, reaching up to 60, our approach achieved \nagreement rates exceeding 70%, significantly surpassing other systems. Overall, the proposed AES method offers \nan accurate and reliable assessment of essay quality, rendering it a valuable tool for educational institutions, \nwriting centers, and individuals aiming to enhance their writing skills.\nFuture research endeavors could focus on exploring additional knowledge bases or integrating external \nresources to further enhance the capabilities of our hybrid approach. Moreover, investigations into addressing \nclauses’ truth inference, similar to systems like SAGE, would contribute to expanding the scope of our method. \nWith continuous advancements, the proposed hybrid AES method holds the potential to reshape the landscape \nof AES, enabling more effective and efficient evaluations in educational settings.\nData availability\nThe data that support the findings of this study are available from the corresponding author, J. Atkinson, upon \nreasonable request.\nReceived: 4 June 2024; Accepted: 17 January 2025\nReferences\n 1. Adedoyin, O. B. & Soykan, E. Covid-19 pandemic and online learning: The challenges and opportunities. Interact. Learn. Environ. \n0, 1–13 (2020).\n 2. Guangul, F ., Suhail, A. & Khalit, M. Challenges of remote assessment in higher education in the context of covid-19: A case study \nof middle east college. Educ. Access. Eval. Account. 32, 519–535 (2020).\n 3. Seifert, T. Student assessment in online learning: Challenges and effective practices during covid-19. In Proceedings of EdMedia + \nInnovate Learning 2020, 106–108 (Association for the Advancement of Computing in Education (AACE), 2020).\n 4. Jin, C., He, B., Hui, K. & Sun, L. TDNN: A two-stage deep neural network for prompt-independent automated essay scoring. In \nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1088–1097 (2018).\n 5. Radford, A. et al. Language models are unsupervised multitask learners. OpenAI blog 1, 9 (2019).\n 6. Amorim, E., Cançado, M. & Veloso, A. Automated essay scoring in the presence of biased ratings. In Proceedings of the 2018 \nConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume \n1 (Long Papers), 229–237 (2018).\n 7. Ke, Z. & Ng, V . Automated essay scoring: A survey of the state of the art. In IJCAI, 6300–6308 (2019).\n 8. Houlsby, N. et al. Parameter-efficient transfer learning for NLP . In International Conference on Machine Learning , 2790–2799 \n(PMLR, 2019).\n 9. Zupanc, K. & Bosnić, Z. Automated essay evaluation with semantic analysis. In Knowledge-Based Systems, vol. 120, 20–32 (Elsevier, \n2017).\n 10. Mellor, A. Essay length, lexical diversity and automatic essay scoring. Tech. Rep. 2, Osaka Institute of Technology (2011).\n 11. Rupp, A. A., Casabianca, J. M., Krüger, M., Keller, S. & Köller, O. Automated essay scoring at scale: A case study in Switzerland and \nGermany. ETS Res. Report Ser. 2019, 1–23 (2019).\n 12. Burstein, J., Braden-Harder, L. & Chodorow M.. Computer analysis of essay content for automated score prediction: A prototype \nautomated scoring system for GMAT analytical writing assessment essays. ETS Research Report Series 1998, 1–67 (1998).\n 13. Lottridge, S., Wood, S. & Shaw, D. The effectiveness of machine score-ability ratings in predicting automated scoring performance. \nAppl. Meas. Educ. 31, 215–232 (2018).\n 14. Landauer, T., McNamara, D., Dennis, S. & Kintsch, W . Handbook of Latent Semantic Analysis (Psychology Press, New Y ork, 2014).\n 15. Rich, C., Schneider, C. & Brot, J. Applications of automated essay evaluation in west Virginia. In Handbook of Automated Essay \nEvaluation: Current Applications and New Directions, 99–123 (2013).\n 16. Jin, C. & He, B. Utilizing latent semantic word representations for automated essay scoring. In IEEE 12th Intl Conf on Ubiquitous \nIntelligence and Computing, 1101–1108 (2015).\n 17. Landauer, T., Laham, D. & Foltz, P . Automated scoring and annotation of essays with the intelligent essay assessor. Automated Essay \nScoring: A Cross-Disciplinary Perspective, 87–112 (2003).\n 18. Elliot, S. Automated essay scoring: A cross-disciplinary perspective. In IntelliMetric: From Here to Validity , 71–86 (Lawrence \nErlbaum Associates, NJ, 2003).\n 19. Palma, D. & Atkinson, J. Coherence-based automatic essay assessment. IEEE Intell. Syst. 33, 26–36 (2018).\n 20. Islam, M.  M. & Hoque, A.  L. Automated essay scoring using generalized latent semantic analysis. In 2010 13th International \nConference on Computer and Information Technology (ICCIT), 358–363 (IEEE, 2010).\nScientific Reports |        (2025) 15:14551 8| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/\n 21. Smith, M. The reading-writing connection. Reading 400, 200L (2009).\n 22. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed representations of words and phrases and their \ncompositionality. In Advances in Neural Information Processing Systems, 3111–3119 (2013).\n 23. Mayfield, E. & Black, A. W . Should you fine-tune BERT for automated essay scoring? In Proceedings of the Fifteenth Workshop on \nInnovative Use of NLP for Building Educational Applications, 151–162 (Association for Computational Linguistics, 2020).\n 24. Altszyler, E., Sigman, M. & Fernández, D. Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in \ndreams database. CoRR (2016).\n 25. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805 (2018).\n 26. Touvron, H., Martin, L., Stone, K. et al. Llama 2: Open foundation and fine-tuned chat models. arxiv: 2307.09288 (2023).\n 27. Brown, T. B., Mann, B., Ryder, N., Subbiah, M. et al. Language models are few-shot learners. arxiv: 2005.14165.\n 28. Thoppilan, R., Freitas, D. D., Hall, J. et al. LaMDA: Language models for dialog applications. arxiv: 2201.08239 (2022).\n 29. Hsieh, C.-Y ., Li, C.-L., Y eh, C.-K. et al. Distilling step-by-step! outperforming larger language models with less training data and \nsmaller model sizes. arxiv: 2305.02301 (2023).\n 30. Boiko, D. A., MacKnight, R. & Gomes, G. Emergent autonomous scientific research capabilities of large language models. arxiv: \n2304.05332 (2023).\n 31. Ouyang, L., Wu, J., Jiang, X. et al. Training language models to follow instructions with human feedback. arxiv: 2203.02155 (2022).\n 32. OpenAI, Achiam, J. et al. GPT-4 technical report. arxiv: 2303.08774 (2024).\n 33. Sun, Z. et al. Principle-driven self-alignment of language models from scratch with minimal human supervision. arxiv: 2305.03047 \n(2023).\n 34. Tay, Y ., Phan, M., Tuan, L.  A. & Hui, S.  C. SkipFlow: Incorporating neural coherence features for end-to-end automatic text \nscoring. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32 (2018).\n 35. Cozma, M., Butnaru, A. M. & Ionescu, R. T. Automated essay scoring with string kernels and word embeddings. arXiv preprint \narXiv:1804.07954 (2018).\nAuthor contributions\nAll authors have contributed to this research: 1. made substantial contributions to the conception or design of \nthe work; or the acquisition, analysis, or interpretation of data; or the creation of new software used in the work; \n2. drafted the work or revised it critically for important intellectual content; 3. approved the version to be pub -\nlished; and 4. agree to be accountable for all aspects of the work in ensuring that questions related to the accuracy \nor integrity of any part of the work are appropriately investigated and resolved.\nDeclarations\nCompeting interests\nThe authors report there are no competing interests to declare.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.A.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:14551 9| https://doi.org/10.1038/s41598-025-87862-3\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6329665780067444
    },
    {
      "name": "Data science",
      "score": 0.4172295331954956
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3357943892478943
    }
  ],
  "institutions": [],
  "cited_by": 6
}