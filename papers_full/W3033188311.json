{
  "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
  "url": "https://openalex.org/W3033188311",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2352997497",
      "name": "Dai, Zihang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288920133",
      "name": "Lai, Guokun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098367636",
      "name": "Yang, Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202120575",
      "name": "Le, Quoc V.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2945918281",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2963175980",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970528773",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2251849926",
    "https://openalex.org/W1523493493",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3105163367",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2811124557",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3000103182",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2975059944"
  ],
  "abstract": "With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.",
  "full_text": "Funnel-Transformer: Filtering out Sequential\nRedundancy for Efﬁcient Language Processing\nZihang Dai∗12, Guokun Lai∗1, Yiming Yang1, Quoc V . Le2\n1Carnegie Mellon University, 2Google AI Brain Team\n{dzihang,guokun,yiming}@cs.cmu.edu, qvl@google.com\nAbstract\nWith the success of language pretraining, it is highly desirable to develop more\nefﬁcient architectures of good scalability that can exploit the abundant unlabeled\ndata at a lower cost. To improve the efﬁciency, we examine the much-overlooked\nredundancy in maintaining a full-length token-level presentation, especially for\ntasks that only require a single-vector presentation of the sequence. With this intu-\nition, we propose Funnel-Transformer which gradually compresses the sequence\nof hidden states to a shorter one and hence reduces the computation cost. More\nimportantly, by re-investing the saved FLOPs from length reduction in constructing\na deeper or wider model, we further improve the model capacity. In addition, to\nperform token-level predictions as required by common pretraining objectives,\nFunnel-Transformer is able to recover a deep representation for each token from\nthe reduced hidden sequence via a decoder. Empirically, with comparable or fewer\nFLOPs, Funnel-Transformer outperforms the standard Transformer on a wide\nvariety of sequence-level prediction tasks, including text classiﬁcation, language\nunderstanding, and reading comprehension.1\n1 Introduction\nWith the recent success of unsupervised language pretraining [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\nthe power of neural self-attention models (a.k.a. Transformer) [13] has been pushed to a new level,\nleading to dramatic advancements in machine learning and natural language processing (NLP). More\nimportantly, it has been observed that with more FLOPs invested in longer pretraining and/or larger\nmodels, the performance of pretrained Transformer models consistently improve. However, it is\nextremely expensive to pretrain or even just to ﬁnetune the state-of-the-art self-attention models, as\nthey require much more FLOPs and memory resources compared to traditional models in NLP. This\nlargely limits their applications and success in more ﬁelds.\nGiven this challenge, there has been an increasing amount of efforts to reduce the costs of pretraining\nand ﬁnetuning self-attention models. From the perspective of post-pretraining processing, typical\napproaches include distillation, pruning and quantization of various kinds, which try to derive a\nlighter model from an well-pretrained model by taking advantage of the richer signals in the larger\nmodel or learning to remove less important operations. Another line of research aims at designing\nan architecture that not only has a lower resource-to-performance ratio (more efﬁcient) but also\nscales as well as the Transformer, at least in certain domains. Most of such methods build upon\nthe Transformer backbone and focus on redesigning its building blocks. Representative solutions\ninclude searching for better micro operation or macro module designs [ 14, 15], replacing the full\npairwise attention with local operations such as convolution [16] and dynamic convolution [17], and\noptimizing the hidden size combinations for existing blocks [18].\n∗Equal contribution.\n1The code and pretrained checkpoints are available at github.com/laiguokun/Funnel-Transformer.\nPreprint. Under review.\narXiv:2006.03236v1  [cs.LG]  5 Jun 2020\nAcross the wide variety of ideas mentioned above, a common strategy is to identify redundant\noperations or representations and replace them with more efﬁcient ones. Inspired by this line of\nthinking, in this work, we will be focusing on the potential redundancy induced by always maintaining\na full-length sequence of hidden representations across all layers in Transformer. Intuitively, for many\nsequence-level NLP tasks such as text classiﬁcation and ranking, the most common use case is to\nextract a single vector from the entire sequence, which does not necessarily preserve all information\ndown to the token-level granularity. Hence, for such tasks, the full-length sequence of hidden states\nmay contain signiﬁcant redundancy. This is analogous to the case of image recognition, where\nthe convolution neural network gradually reduces the spatial resolution/size of feature maps as the\nneural network goes deeper. In addition, linguistic prior also encourages gradually merging nearby\ntokens (words) into larger semantic units (phrases), which naturally leads to a shorter sequence of\nrepresentations.\nConcretely, we propose to gradually reduce the sequential resolution (i.e. length) of the hidden\nrepresentation in self-attention models. Immediately, the reduction in sequence length can lead\nto signiﬁcant savings in both FLOPs and memory. More importantly, the saved computational\nresource can be directly re-invested in constructing a deeper (or wider) model to boost the model\ncapacity without additional computational burden. In addition, to address the challenge that common\npretraining objectives such as masked language modeling (MLM) [2] require separate representations\nfor each token, we design a simple strategy to decode a full-length sequence of deep representations\nfrom the hidden state of reduced length. As a result, the proposed model can be directly trained\nwithout modifying the pretraining objectives, as well as adopted for downstream tasks that require\ntoken-level representations.\nEmpirically, with comparable or even fewer FLOPs, by trading sequential resolution for depth, our\nproposed model achieves an improved performance over the standard Transformer on a wide variety\nof sequence-level prediction tasks, including text classiﬁcation, language understanding, and reading\ncomprehension.\n2 Method\n2.1 Background\nTransformer Architecture The Transformer architecture [ 13] is a highly modularized neural\nnetwork, where each Transformer layer consists of two sub-modules, namely the multi-head self-\nattention (S-Attn) and position-wise feed-forward network (P-FFN). Both sub-modules are wrapped\nby a residual connection and layer normalization. Schematically, given a lengthT sequence of hidden\nstates h = [h1,...,h T], the computation of a single Transformer layer can be expressed as\nh ←LayerNorm(h + S-Attn(Q = h,KV = h)), (1)\nhi ←LayerNorm(hi + P-FFN(hi)), ∀i= 1,··· ,T. (2)\nPretraining Objectives The most commonly used pretraining objective is the masked language\nmodeling (MLM) proposed by BERT [2]. For a length-T natural language sequence x sample from\na large unlabeled set D, the MLM objective ﬁrst constructs a corrupted sequence ˆx by randomly\nreplacing 15% of the tokens ofx with a special token [mask] and then trains a Transformer model [2]\nto reconstruct the original x based on ˆx, i.e.,\nmax\nθ\nJMLM(θ) =Ex∼DEI\n∑\ni∈I\nlog Pθ(xi |ˆxI) =Ex∼DEI\n∑\ni∈I\nlog exp\n(\ne(xi)⊤hi(ˆxI)\n)\n∑\nx′exp (e(x′)⊤hi(ˆxI)),\nwhere Iis the positions of masked tokens, the subscript in ˆxIemphasizes its dependence on I, e(x)\ndenotes the embedding of the token x, and hi(ˆxI) the last-layer hidden state at position iproduced\nby the Transformer model. After pretraining, the entire model is ﬁnetuned in downstream tasks.\nTo show the generality of our proposed model, we also experiment with another pretraining objective\nELECTRA [5]. Different from MLM, ELECTRA relies a pair of jointly trained generator and\ndiscriminator. Speciﬁcally, the generator usually has a smaller size (1/4 of that of the discriminator)\nand is directly trained via the MLM objective, i.e., maxθG JMLM(θG). Then, for each masked\nposition, a token is sampled from the reconstruction distribution of the generator to replace the\n[mask] token and form a new sequence ˜x, i.e., if i∈I, ˜xi ∼PθG(xi |ˆxI) else ˜xi = xi. Given the\nnew sequence ˜x, the discriminator is then trained to distinguish whether each token in ˜x is real (same\nas x) or fake (different from x) via binary classiﬁcation. After pretraining, only the discriminator\nwill be used during ﬁnetuning and the generator is simply discarded.\n2\nDiscussion Note that both pretraining objectives introduced above require the ability to produce a\nhidden state for each input token, i.e., hi(ˆxI) and hi(˜x). Due to this requirement, it seems natural\nto keep a full sequence of hidden states. However, in contrast, many sequence-level downstream\ntasks like classiﬁcation or ranking only need a single-vector summary of the entire sequence. Funda-\nmentally, this suggests that some kind of compression is usually required to remove the unnecessary\nredundancy during ﬁnetuning. This observation immediately leads to the following two questions:\n• Can we design a general model that is equally expressive but more efﬁcient by compressing the\nfull sequence of hidden states into a more compact form?\n• With the compressed representations, how can the model retain the ability to produce token-level\nrepresentations for pretraining?\nTo answer these two questions, we next present our proposed architecture.\n2.2 Proposed Architecture\n… … … … … …\nBlock 1 Block 2 Block 3\n+\n+\n+\n+\n+\n+\n+\n+\n=\n=\n=\n=\n=\n=\n=\n=\nPool Pool\nUp-sample\nUp-sample\nResidual/Skip Connection\n…\nEncoder Decoder (optional)\nFigure 1: High-level visualization of the proposed Funnel-Transformer.\nTo inherit the high capacity and optimization advantages of the Transformer architecture, the proposed\nmodel keeps the same overall skeleton of interleaved S-Attn and P-FFN sub-modules wrapped by\nresidual connection and layer normalization. But differently, to achieve representation compression\nand computation reduction, our model employs an encoder that gradually reduces the sequence length\nof the hidden states as the layer gets deeper. In addition, for tasks involving per-token predictions like\npretraining, a simple decoder is used to reconstruct a full sequence of token-level representations\nfrom the compressed encoder output.\nEncoder As illustrated in the left part of Fig. 1, the encoder consists of several blocks of consecutive\nTransformer layers. Within each block, the sequence length of the hidden states always remains the\nsame. But when going from a lower-level block to a higher-level block, the length of the hidden\nsequence is reduced by performing certain type of pooling along the sequence dimension, i.e.,\nh′←Pooling(h), (3)\nwhere h ∈RT×D and h′∈RT′×D for some T′<T . Importantly, instead of directly feeding the\npooled sequence h′into the ﬁrst S-Attn layer of the new block, we only use pooled sequence h′to\nconstruct the query vector (and the residual signal) of the self-attention, while the unpooled sequence\nh serves that role of key and value vectors, i.e.\nh ←LayerNorm\n(\nh′+ S-Attn\n(\nQ = h′,KV = h\n))\n. (4)\nNote that the output sequence of this special S-Attn module has the same length as the pooled\nsequence h′. To understand the advantage of this particular design, it is helpful to compare the\nproposed “pool-query-only” variant with the naive alternative of using h′for both the query and\nkey-value vectors, i.e., S-Attn\n(\nQ = h′,KV = h′)\n:\n• Under the naive approach, the compression is solely controlled by the pooling operation, which\nis ﬁnished before the attention module. Hence, relatively simple pooling methods such as aver-\nage/mean pooling won’t be able to achieve good compression.\n• Under the pool-query-only variant, the compression depends on not only how the pooling is\nperformed, but also how the self-attention weighted sums the unpooled sequence to form each\n3\npooled vector. Effectively, the particular attention here can be seen as a type of linear compression\nthat combines T bases into a smaller number of T′“compressed bases”. Therefore, with minimum\ncomputational overhead, this variant makes compression operation more expressive.\nWith this particular pool-query-only design in place, we ﬁnd the simplest strided mean pooling\napplied to each sliding window of the sequence work very well in practice. For simplicity, we only\nexperiment with stride 2 and window size 2 in this work. Hence, the pooling operation will reduce\nthe sequence by half and each pooled hidden state corresponds to a window of 2 unpooled hidden\nvectors. Intuitively, this type of pooling roughly follows the linguistic prior that nearby tokens could\nbe gradually merged (or compressed) into a larger semantic component. Once the sequence length is\nhalved after the pooling and pool-query-only attention, the rest of the encoder computation simply\nfollows the standard updates in Eqn. (2) and (1).\nFinally, as an extra implementation detail, recall that a particular design in language pretraining is\nto add a special token [cls] to the beginning of the original input sequence, and use the last-layer\nhidden state corresponding to [cls] (i.e., h1) as the representation of the sequence. To prevent\nthe pooling from destroying this special structure, we ﬁrst separate the [cls] hidden state and the\nrest of hidden states and only apply the pooling to the rest of hidden states. For some practical\nimplementation issues and an efﬁcient solution, we refer readers to Appendix A.1.\nDecoder In order to recover a full sequence of hidden states from the encoder output of reduced\nlength, a natural idea would be performing some kind of up-sampling. For instance, in image\ngeneration or super-resolution, deconvolution (transposed convolution) or parameter-free resizing\nwith bilinear interpolation are often used to increase the spatial resolution of the feature map. Hence,\nwe can simply adapt these ideas from 2D processing to our 1D case and apply proper up-sampling to\nthe encoder output.\nHowever, instead of performing multiple up-samplings with small expansion rate (e.g. increasing the\nsequence length by 2x each time) as in image domain, we here choose to employ a single up-sampling\nwith a large expansion rate, as shown on the right part of Fig. 1. Speciﬁcally, given the output\nsequence hM of length TM = T/2M−1 from an M-block encoder, we directly up-sample it to a\nfull-length sequence hup =\n[\nhup\n1 ,··· ,hup\nT\n]\nby repeating each hidden vector 2M−1 times:\n∀i= 1,··· ,T, h up\ni = hM\ni//2M−1 , (5)\nwhere ·//·denotes ﬂoor division. However, note that every 2M−1 consecutive vectors in hup are\nexactly the same and hence do not contain detailed token-level information. Hence, we further extract\nthe last-layer hidden states from the ﬁrst block of the encoder h1, which still has the full length\nT and contains the uncompressed token-level information. Then, the lower-level representation\nh1 and up-sampled higher-level representation hup are added together to form a deep token-level\nrepresentation g = h1 + hup. Effectively, this forms a residual/skip connection that enables detailed\ntoken information and potentially easier optimization. In addition, we stack a few more Transformer\nlayers upon g to achieve a better deep fusion of the low-level and high-level features. In this work,\nwe always use 2 Transformer layers in decoder.\nIt is important to emphasize that the decoder is only used if the task requires token-level prediction,\nsuch as in standard pretraining or sequence labeling. For tasks that only requires a single vectorial\nrepresentation of the sequence like classiﬁcation, the decoder is discarded after pretraining and only\nthe encoder is ﬁnetuned. Finally, to emphasize the ﬁltering/compression property of the encoder as\nwell as its shape, we name the proposed model Funnel-Transformer (F-TFM).\n2.3 Complexity & Capacity Analysis\nWith the architecture design speciﬁed, we now analyze how the sequence compression affects the\ncomplexity and capacity of the proposed model, especially compared to the standard Transformer.\nFirstly, for a Transformer layer with an S-Attn and a P-FFN module of hidden size D, the complexity\nof processing a length- T sequence is O(T2D+ TD2).2 Hence, every time the sequence length\nis reduced by half in the encoder, we enjoy a super-linear (more than half) complexity drop. In\npractice, as the O(TD2) term has a large constant, a near-linear speedup is observed more often. The\nsuper-linear effect is more detectable when the sequence length is relatively long like in pretraining.\n2Since the corresponding memory complexity is simply O(T2 + TD), which is always offset by a multiplier\n1/D, we will focus on the computation complexity with the conclusion directly carried through.\n4\nTherefore, given the same FLOPs, we can at least trade a full-length layer in the 1st block for 2m−1\nlayers in the m-th block, which provides an economical way to increase the depth of network.\nOn the other hand, the capacity of a compressed-length layer is clearly upper-bounded by that of a\nnormal full-length layer. In most cases where the compression is lossy, reducing the sequence length\nwill inevitably lead to capacity drop. The good news is that the capacity drop of a single layer could\nbe well compensated by re-investing the saved FLOPs in stacking more cheaper layers of reduced\nlength or increasing the width of the model.\nAs a concrete example, for a Transformer of BERT Base size, i.e., 12 layers of hidden size 768\n(L12H768), we may construct a Funnel-Transformer of 3 blocks where each block has 6 layers of\nhidden size 768 (B6-6-6H768). Despite having 18 layers in total, when ﬁnetuned for classiﬁcation,\nthe FLOPs of the B6-6-6H768 architecture only corresponds to at most 6 + 6/2 + 6/4 = 10.5\nfull-length layers, clearly fewer than that of L12H768. More importantly, as we will show in the\nexperiments, B6-6-6H768 signiﬁcantly outperforms L12H768. While intuitive, how to construct an\noptimal block layout given this depth-length trade-off remains an open challenge. For this work, we\nonly consider relatively regular layout and leave more systematic studies for future work.\nFinally, notice that trading sequential resolution for depth or width has a side effect of increasing the\ntotal number of parameters. For instance, B6-6-6H768 has 1.5x Transformer parameters compared to\nL12H768. In practice, more parameters may increase communication cost in distributed training as\nwell as the memory consumption and memory access time. A simple remedy is to perform certain\nparameter sharing, as used in ALBERT, to recover the same parameter count. Taking B6-6-6H768\nas an example, one may tie the parameters for every two layers in the 2nd and 3rd blocks, denoted\nas B6-3x2-3x2H768, which gives back the same number of parameters to L12H768. However,\nparameter sharing could result in performance loss. Fundamentally, this brings us another trade-off\nbetween the gain (capacity) and cost (memory and communication cost) of using more parameters,\nwhich can be highly device dependent.\n3 Related Work\nAs far as we know, no previous work achieves performance gain via compressing the sequence length\nof the hidden states under language pretraining. Meanwhile, our proposed model is quite similar to\nthe bottom-up model proposed by a contemporary work [19] for causal language modeling. The key\ndifferences include the pool-query-only design for down-sampling, how the up-sampling is performed,\nand our relative attention parameterization. Another closely related idea is Power-BERT [20], which\nlearns to soft-eliminate word vectors that are less “signiﬁcant” during ﬁnetuning. Hence, for post-\nﬁnetuning inference, the sequence length can be reduced to achieve acceleration. More generally, our\nwork is also related to previous work on hierarchical recurrent neural networks [21] and Transformer\nmodels [22, 23]. Different from these methods, our model does not rely on any pre-deﬁned hierarchy\nor boundary of semantic meanings and always captures the full-length dependency input with\nattention.\nIn contrast, our work draws many inspirations from the computer vision domain. The contracting\nencoder and expanding decoder framework with residual connections is conceptually similar to the\nResUNet [24] for image segmentation. The strided pooling is also widely used to construct modern\nimage recognition networks [25]. Despite the similarities, apart from the obvious difference in data\ndomain and computation modules, our encoder employs a special pool-query-only design to improve\nthe compression, and our decoder only requires a single up-sampling with a large expansion rate.\nIn addition, a line of research in graph neural networks has tries to gradually reduce the number of\nnodes in different ways and obtain a single vectorial representation for supervised classiﬁcation. [26,\n27, 28] While these methods could potentially be plugged into our model as alternative compression\noperations, it remains an open question whether compression techniques developed for supervised\ngraph classiﬁcation can be extended the large-scale language pretraining.\n4 Experiment\nIn this section, we empirically evaluate the proposed F-TFM by ﬁrst pretraining it and then ﬁnetuning\nit in downstream tasks. Following previous work, for pretraining, we consider two common settings:\n• Base scale: Pretraining models for 1M steps with batch size 256 on Wikipedia + Book Corpus.\nThis is the setting used by original BERT [2]. We will rely on this setting to perform fair comparison\nbetween F-TFM and the standard Transformer as well as some ablation studies.\n5\n• Large scale: Pretraining models for 500K steps with batch size 8K on the ﬁve datasets used by\nXLNet [3] and ELECTRA [5] (Wikipedia + Book Corpus + ClueWeb + Gigaword + Common\nCrawl). We will compare F-TFM trained at this scale with previous state-of-the-art methods.\nFor ﬁnetuning, we mainly focus on sequence-level tasks that only requires a single vectorial represen-\ntation of the input sequence, since F-TFM is designed with such a purpose in mind. Speciﬁcally, such\ntasks include the GLUE benchmark for language understanding [29], 7 widely used text (sentiment /\ntopic) classiﬁcation tasks (IMDB, AD, DBpedia, Yelp-2, Yelp-5, Amazon-2, Amazon-5) [30], and\nthe RACE reading comprehension dataset [ 31]. In addition, to see how F-TFM performs when\ntoken-level prediction is needed, we consider the SQuAD question answering task which requires\nthe model to select a token span from the context paragraph as the answer. For more details of the\nexperiment setting, we refer readers to Appendix B.\nFinally, for all models implemented in this work including Transformer baselines in the base-scale\ncomparison section 4.1, we always use the relative positional attention parameterization proposed by\nTransformer-XL [32] (see Appendix A.2 for some implementation details of Transformer-XL).\n4.1 Base-scale Results\nFirstly, we evaluate how F-TFM performs compared to the standard Transformer under similar\namount of computation (i.e., FLOPs). For this purpose, we consider three commonly used model\nsizes for the standard Transformer, namely large (L24H1024), base (L12H768) and small (L6H768).\nThen, for each Transformer baseline, we construct F-TFMs of different block layouts and parameters,\nwhile ensuring the F-TFMs always have fewer or similar FLOPs. Based on the MLM pretraining\nobjective, the results on GLUE benchmark and text classiﬁcation are presented in Table 1, where we\nalso include the relative FLOPs and #Params. Here, we can make a few key observations:\nModel size CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE GLUE-A VG\nL24H1024 63.2 94.8 91.8/88.5 91.1 88.7/91.7 88.7 94.0 80.5 86.6\nB10-10-10 64.8 95.0 92.5/89.5 90.7 88.6/91.5 88.9 94.0 81.5 87.0\nB8-8-8 63.5 94.7 92.2/89.0 90.7 88.9/91.7 88.8 93.6 81.2 86.7\nL12H768 60.5 93.0 92.2/89.0 89.4 88.1/91.2 86.0 92.2 73.6 84.4\nB6-6-6 62.5 94.0 92.2/89.0 89.5 88.4/91.4 87.0 92.7 76.5 85.3\nB6-3x2-3x2 60.5 93.6 92.4/89.2 89.4 88.2/91.3 86.4 92.5 75.0 84.7\nB4-4-4 59.1 92.7 91.8/88.7 89.1 88.2/91.3 85.5 92.0 73.2 83.9\nL6H768 55.2 91.5 91.1/87.8 88.1 87.2/90.6 82.7 90.0 64.6 81.3\nB3-4-4 59.0 92.8 91.8/88.5 88.5 87.8/90.9 84.8 91.8 73.2 83.7\nModel size IMDB AG DBpedia Yelp2 Yelp5 Amazon2 Amazon5 FLOPs #Params\nL24H1024 4.440 4.987 0.646 1.758 28.73 2.409 32.78 1.00x 1.00x\nB10-10-10 4.404 5.026 0.617 1.734 28.52 2.400 32.65 0.73x 1.22x\nB8-8-8 4.552 5.079 0.664 1.713 28.84 2.438 32.87 0.58x 1.00x\nL12H768 5.328 5.184 0.663 2.013 29.35 2.571 33.14 1.00x 1.00x\nB6-6-6 4.908 5.079 0.654 1.939 29.03 2.518 32.91 0.88x 1.39x\nB6-3x2-3x2 5.144 5.342 0.649 1.892 29.03 2.570 33.01 0.88x 1.00x\nB4-4-4 5.348 5.250 0.670 1.979 29.37 2.596 33.16 0.58x 1.00x\nL6H768 6.252 5.421 0.697 2.203 30.33 2.801 33.69 1.00x 1.00x\nB3-4-4 5.520 5.342 0.670 2.042 29.51 2.603 33.16 1.00x 1.53x\nTable 1: MLM pretraining results at the base scale: GLUE dev performances (the higher the better)\nin the upper panel and text classiﬁcation error rates (the lower the better) in the lower panel . The\nFLOPs and #Params both refer to the ﬁnetuning setting with only the encoder. The corresponding\nnumbers with the decoder are included in Appendix C.2. The FLOPs is a rough estimation assuming\nlinear complexity w.r.t. the sequence length. The #Params is exact including the embedding matrix.\n• Given similar or fewer FLOPs, by trading sequential resolution for more layers, the F-TFM\noutperforms the standard Transformer in most tasks except STS-B, especially for smaller models.\n• When we only compress the sequence length without increasing the depth (and #Params), F-TFM\ncould suffer from some performance loss in certain settings on the GLUE datasets. However, as\nthe model size increases, such performance gaps become smaller or even disappear.\n• In addition, we ﬁnd partial parameter-sharing often harms the performance. Therefore, the practical\ntrade-off should be made according to the actual task and computation device.\n6\nTo further test generality of F-TFM, we additionally consider ELECTRA for pretraining. The results\nare summarized in Table 2. Overall, we see a similar trend, though the gain is slightly smaller on\nthe GLUE benchmark. This could be attributed to reusing two key hyper-parameters (discriminator\nloss coefﬁcient and generator size multiplier) tuned for Transformer to train F-TFMs without any\nadjustment at all.\nModel size CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE GLUE-A VG\nL24H1024 66.5 94.3 92.8/90.0 91.5 89.6/92.2 89.4 94.1 84.5 87.8\nB10-10-10 68.6 95.0 93.0/90.0 91.0 88.9/91.7 89.1 93.6 84.5 87.9\nB8-8-8 66.6 94.8 92.6/89.7 90.7 88.8/91.7 89.0 93.6 82.1 87.3\nL12H768 64.3 93.1 92.1/89.2 90.8 88.7/91.7 86.4 92.1 75.4 85.4\nB6-6-6 64.3 94.2 92.8/89.7 90.1 88.7/91.6 87.4 92.5 78.3 86.0\nB6-3x2-3x2 63.9 94.2 93.0/90.2 89.5 88.4/91.4 87.0 92.2 77.6 85.7\nB4-4-4 62.8 93.6 92.5/89.2 89.2 88.4/91.3 86.0 91.6 74.3 84.8\nL6H768 62.1 91.1 90.8/86.8 88.9 88.2/91.3 83.9 89.7 66.7 82.6\nB3-4-4 59.0 93.1 90.8/87.5 88.7 88.1/91.0 85.8 91.1 72.5 83.6\nModel size IMDB AG DBpedia Yelp2 Yelp5 Amazon2 Amazon5 FLOPs #Params\nL24H1024 4.724 5.053 0.653 1.874 28.84 2.425 32.85 1.00x 1.00x\nB10-10-10 4.324 5.250 0.639 1.789 28.68 2.419 32.72 0.73x 1.22x\nB8-8-8 4.364 5.408 0.651 1.729 28.76 2.447 32.85 0.58x 1.00x\nL12H768 5.248 5.355 0.657 1.953 29.24 2.596 33.04 1.00x 1.00x\nB6-6-6 4.792 5.237 0.650 1.850 28.73 2.499 32.79 0.88x 1.39x\nB6-3x2-3x2 4.924 5.342 0.671 1.913 29.00 2.523 32.85 0.88x 1.00x\nB4-4-4 5.152 5.382 0.659 2.032 29.33 2.566 33.03 0.58x 1.00x\nL6H768 6.220 5.395 0.674 2.287 30.16 2.759 33.57 1.00x 1.00x\nB3-4-4 5.396 5.342 0.653 2.000 29.60 2.591 33.09 1.00x 1.53x\nTable 2: ELECTRA pretraining results at the base scale.\nRunning Time Comparison While FLOPs count offers a general idea of the model speed, it still\ndiffers from the actual running time, especially when other overhead exists. Hence, for completeness,\nwe show the speedup provided by the F-TFM in terms of actual running time in Appendix C.2. We\nalso compare the actual memory footprint of F-TFM and TFM in Appendix C.2.\n4.2 Large-scale Results\nGiven the encouraging results of F-TFM at base-scale, we next consider training F-TFM under\nthe large-scale setting and compare it with previous models pretrained in similar settings. Due to\nthe slightly better performance of ELECTRA over MLM, we will use the ELECTRA objective for\nall large-scale experiments. Given the pretrained F-TFM of different sizes, we ﬁrst compare the\nﬁnetuning performance on the GLUE benchmark in Table 3. Similar to the base-scale results, with\nfewer or comparable FLOPs, F-TFM outperforms the corresponding baselines in the majority of\ntasks, suggesting the good scalability of F-TFM. We also test the models on the 7 text classiﬁcation\ntasks. But due to the page constraint, we refer readers to Appendix C.1.\nNext, we consider the RACE dataset, which is quite different from the GLUE benchmark. At the core,\nRACE is a multiple-choice reading comprehension task requiring complex reasoning, which though,\ncan be formulated as classifying the correct choice. Also, paragraphs in RACE are much longer.\nTo F-TFM, this presents both a challenge, as it requires detailed reasoning, and an opportunity to\ncompress long paragraph. As we can see in Table 4, F-TFM achieves better performances compared\nto all previous models. In particular, within the base model group, the gain is very signiﬁcant. It\nshows that F-TFM can also excel for sequence-level task that involves long text and reasoning.\nFinally, although F-TFM is mainly designed for tasks that only require a sequence-level representation,\nit is possible to apply F-TFM to token-level tasks by additionally ﬁnetuning the decoder. To test\nthis ability, we ﬁnetune F-TFM on the SQuAD datasets and compare it with previous models in\nTable 5. While F-TFM outperforms previous models in the base group by a large margin, in the\nlarge model group, the F-TFM with about 83% FLOPs (B10-10-10) still falls behind the standard\nTransformer that always maintains a full-length token-level representations. This suggests sequential\ncompression could harm the performance when detailed token-level information is critical. On the\nother hand, compared to the results on SQuAD1.1, F-TFMs perform relatively better on SQuAD2.0,\n7\nModel CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI A VG\nDev set results (single model)\nROBERTALarge [4] 68.0 96.4 -/90.9 92.4 -/92.2 90.2 94.7 86.6 - 88.9\nXLNetLarge [3] 69.0 97.0 -/90.8 92.5 -/92.3 90.8 94.9 85.9 - 89.2\nELECTRALarge [5] 69.1 96.9 -/90.8 92.6 -/92.4 90.9 95.0 88.0 - 89.5\nB10-10-10H1024 72.4 96.8 93.5/90.9 92.1 89.8/92.4 91.1/- 95.1 89.5 - 90.0\nB8-8-8H1024 71.3 96.8 93.1/90.7 91.7 89.8/92.4 90.8/- 94.7 89.2 - 89.7\nROBERTABase [4] 63.6 94.8 -/90.2 91.2 -/91.9 87.6/- 92.8 78.7 - 86.4\nMPNetBase [12] 65.0 95.4 -/91.5 90.9 -/91.9 88.5/- 93.3 85.2 - 87.7\nB6-6-6H768 70.1 96.3 93.2/90.4 91.1 89.2/92.0 89.7/- 93.7 83.4 - 88.3\nB6-3x2-3x2H768 68.5 95.6 92.5/89.5 91.0 89.3/92.0 89.1/- 93.0 83.4 - 87.8\nB4-4-4H768 68.2 95.0 92.8/90.2 90.3 89.0/91.8 88.6/- 92.6 79.1 - 87.0\nLeaderboard test set results (single task & single model)\nELECTRALarge [5] 68.1 96.7 89.2/92.0 92.1/91.7 74.8/90.4 90.7/90.2 95.5 86.1 65.1 85.2\nB10-10-10H1024 68.9 97.2 89.4/92.1 91.6/91.3 74.3/90.2 90.9/90.9 95.5 86.5 65.1 85.4\nB8-8-8H1024 68.3 96.9 89.2/92.0 91.5/91.1 73.8/90.1 90.7/90.7 95.1 85.3 65.1 85.0\nELECTRABase [5] 64.6 96.0 88.1/91.2 91.0/90.2 73.2/89.5 88.5/88.0 93.1 75.2 65.1 82.7\nB6-6-6H768 68.3 96.5 89.1/91.9 90.6/89.9 73.3/89.9 89.7/89.4 94.0 80.4 65.1 84.0\nB6-3x2-3x2H768 65.9 96.0 87.8/91.0 90.0/89.6 73.3/89.8 88.9/88.7 93.8 79.9 65.1 83.4\nLeaderboard test set results (multi-task & ensemble)\nROBERTALarge [4] 67.8 96.7 89.8/92.3 92.2/91.9 74.3/90.2 90.8/90.2 95.4 88.2 89.0 88.1\nELECTRALarge [5] 71.7 97.1 90.7/93.1 92.9/92.5 75.6/90.8 91.3/90.8 95.8 89.8 91.8 89.4\nB10-10-10H1024 70.5 97.5 91.2/93.4 92.6/92.3 75.4/90.7 91.4/91.1 95.8 90.0 94.5 89.7\nTable 3: Comparison with previous methods on the GLUE benchmark under large-scale pretraining.\nModel RACE\nTotal High Middle\nROBERTALarge [4] 83.2 81.3 86.5\nXLNetLarge [3] 85.4 84.0 88.6\nB10-10-10 85.7 84.4 88.8\nB8-8-8 85.2 83.9 88.4\nALBERTBase [6] 66.0 - -\nMPNetBase [12] 72.0 76.3 70.3\nB6-6-6 79.7 78.2 83.4\nB6-3x2-3x2 78.8 77.5 82.0\nB4-4-4 76.2 74.6 80.0\nTable 4: RACE test performance comparison.\nModel SQuAD2.0 SQuAD1.1\nEM F1 EM F1\nROBERTALarge [4] 86.5 89.4 88.9 94.6\nELECTRALarge [5] 88.0 90.6 89.7 94.9\nB10-10-10 87.6 90.4 89.0 94.7\nB8-8-8 87.1 89.8 88.7 94.4\nROBERTABase [4] 80.5 83.7 84.6 91.5\nMPNetBase [21] 80.5 83.3 86.8 92.5\nB6-6-6 85.1 87.7 87.4 93.3\nB6-3x2-3x2 84.2 87.0 87.0 93.0\nB4-4-4 82.6 85.5 85.9 92.2\nTable 5: SQuAD dev performance comparison.\nwhich additionally requires the model to make a sequence-level prediction on whether the question is\nanswerable. This again shows the general effectiveness of the F-TFM in sequence-level tasks.\n4.3 Ablation Study\nID Layout (FLOPs / Params) Pool-Op Pool-query-only Sep [cls] Rel-Attn GLUE-A VG\n(1) B6-6-6 (1.00x / 1.00x) Mean ✓ ✓ ✓ 83.5\n(2) Mean ✓ ✓ 82.9\n(3) Mean ✓ ✓ 83.0\n(4) Mean ✓ ✓ 81.4\n(5) Max ✓ ✓ ✓ 83.4\n(6) Top-Attn ✓ ✓ ✓ 75.8\n(7) B8-8 (1.14x / 0.91x) Mean ✓ ✓ ✓ 83.4\n(8) B5-5-5-5 (0.89x / 1.08x) Mean ✓ ✓ ✓ 82.9\nTable 6: Ablation study of F-TFMs with different designs.\nFinally, based on the GLUE benchmark, we perform a series of ablation studies on the importance\nof various designs in F-TFM, including the block layout design, the type of pooling operation, the\n8\npool-query-only technique, maintaining a separate [cls] vector and the usage of Transformer-XL\nparameterization.\n• Pooling operation: Including the mean pooling we ﬁnally employ in F-TFM, we actually test two\ntypes of pooling operations.\n(1) The ﬁrst type is just the strided mean/max pooling as described in section 2.\n(2) The second type aims to select a subset of “hub” states, which refer to those hidden vectors\nthat are attended most in the previous S-Attn layer and hence likely to carry most critical\ninformation about the sequence. Concretely, given the attention map from the previous S-Attn\nlayer, we reduce sum the scores along the number of head and query length dimensions to a\nscore for each position. Then, we simply choose the top 50% of states to achieve the same\ncompression rate. Note that, this type of pooling operation is essentially the same as the\nimportant states selection procedure in Power-BERT [20].\n• Pool-query-only design\n• Separating [cls] in the pooling operation\n• Block layout design: In our experiments, all models actually utilize a 3-block design. Here, we\ncompare the 3-blocks design with the 2-blocks and the 4-blocks design.\n• Relative attention parameterization proposed in Transformer-XL [32]. We compare this parameter-\nization with the learned absolute position embedding as used in the BERT [2].\nThe ablation results are included in Table 6. To save the computation resources, the size of model hid-\nden states in table 6 is set as 512. From the ablation results, we can make the following observations:\n• Comparing pooling different operation ((1), (5), and (6)), we found that the performance of the\nmean and max pooling operation is similar. But they are signiﬁcantly better than the idea of\nutilizing attention score (Top-Attn pooling) to select the “hub” states.\n• Comparing (1) with (2) and (3) respectively, we see that the two special designs, i.e. “pool-query-\nonly” and maintaining a separate non-pooled [cls] , can both bring a clear improvement to the\nproposed model.\n• Comparing (1) and (4), we ﬁnd that the relative positional parameterization is key to the perfor-\nmance of the proposed F-TFM. We suspect that the pooling operation could destroy the positional\ninformation carried by the absolute position encoding, which is only injected to the model in the\ninput embedding layer. As a result, the higher blocks may not have enough positional information\nto learn a good enough attention pattern. In comparison, the positional information is injected to\neach layer under the relative positional attention scheme. Therefore, to achieve good result with\nF-TFM based on absolute positional embedding, one may inject the absolute positional embedding\ninto each attention layer. Actually, a contemporary application of Transformer to the detection\nproblem in computer vision shows injecting positional embedding into each layer is important [33].\n• Finally, we study the inﬂuence of block layout design in our framework. With B6-6-6 as the\n3-block benchmark, we consider two other layout design with similar FLOPs and number of\nparameters. Speciﬁcally, we consider B8-8 for the 2-block design and B5-5-5-5 for the 4-block\ndesign. Comparing the results in (1), (7), and (8), we ﬁnd that the performance of the 3-block\n(B6-6-6) design achieves the best performance, which is signiﬁcantly better than the 4-block design\nand slightly better than the 2-block design. However, if we further taking the FLOPs/#Params into\nconsideration, it is more clear that the 3-block design is superior. Therefore, in the main paper, we\nalways use the 3-block design.\n5 Conclusion & Discussion\nIn this work, under the pretraining-ﬁnetuning paradigm, we investigate a largely overlooked dimension\nof complexity in language processing. With the proposed Funnel-Transformer, we show how\nsequential resolution can be compressed in a simple form to save computation and how the saved\nFLOPs can be re-invested in improving the model capacity and hence the performance. Open\nchallenges for future research include the better ways to improve the compression scheme, to\noptimize the block layout design and to re-invest the saved FLOPs. In addition, combining Funnel-\nTransformer with model compression techniques like knowledge distillation and quantization would\nbe an important direction towards the enhancement of practical impact.\n9\nReferences\n[1] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[3] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in\nneural information processing systems, pages 5754–5764, 2019.\n[4] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[5] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training\ntext encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[6] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv\npreprint arXiv:1909.11942, 2019.\n[7] Lingpeng Kong, Cyprien de Masson d’Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani\nYogatama. A mutual information maximization perspective of language representation learning.\narXiv preprint arXiv:1910.08350, 2019.\n[8] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n[9] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461, 2019.\n[10] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\nsequence pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.\n[11] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\nfor natural language understanding. arXiv preprint arXiv:1901.11504, 2019.\n[12] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted\npre-training for language understanding. arXiv preprint arXiv:2004.09297, 2020.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[14] David R So, Chen Liang, and Quoc V Le. The evolved transformer. arXiv preprint\narXiv:1901.11117, 2019.\n[15] Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun\nHuang, Wei Lin, and Jingren Zhou. Adabert: Task-adaptive bert compression with differentiable\nneural architecture search. arXiv preprint arXiv:2001.04246, 2020.\n[16] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short\nrange attention. arXiv preprint arXiv:2004.11886, 2020.\n[17] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\n10\n[18] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\nMobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint\narXiv:2004.02984, 2020.\n[19] Sandeep Subramanian, Ronan Collobert, Marc’Aurelio Ranzato, and Y-Lan Boureau. Multi-\nscale transformer language models. arXiv preprint arXiv:2005.00581, 2020.\n[20] Saurabh Goyal, Anamitra Roy Choudhary, Venkatesan Chakaravarthy, Saurabh ManishRaje,\nYogish Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference for classiﬁcation\ntasks. arXiv preprint arXiv:2001.08950, 2020.\n[21] Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. Hierarchical recurrent\nneural network for document modeling. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 899–907, 2015.\n[22] Xingxing Zhang, Furu Wei, and Ming Zhou. Hibert: Document level pre-training of hierarchical\nbidirectional transformers for document summarization. arXiv preprint arXiv:1905.06566,\n2019.\n[23] Vikas K Garg, Inderjit S Dhillon, and Hsiang-Fu Yu. Multiresolution transformer networks:\nRecurrence is not essential for modeling hierarchical structure.arXiv preprint arXiv:1908.10408,\n2019.\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical image computing and\ncomputer-assisted intervention, pages 234–241. Springer, 2015.\n[25] Dominik Scherer, Andreas Müller, and Sven Behnke. Evaluation of pooling operations in\nconvolutional architectures for object recognition. In International conference on artiﬁcial\nneural networks, pages 92–101. Springer, 2010.\n[26] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.\nHierarchical graph representation learning with differentiable pooling. In Advances in neural\ninformation processing systems, pages 4800–4810, 2018.\n[27] Hongyang Gao and Shuiwang Ji. Graph u-nets. arXiv preprint arXiv:1905.05178, 2019.\n[28] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. arXiv preprint\narXiv:1904.08082, 2019.\n[29] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018.\n[30] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text\nclassiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015.\n[31] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale\nreading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.\n[32] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[33] Nicolas Carion, F. Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander M Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. ArXiv, abs/2005.12872, 2020.\n[34] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data\naugmentation for consistency training. 2019.\n11\nA Implementation Optimization\nA.1 Sequence Truncation for Separating [cls] trick\nAs discussed in Section 2.2, to avoid breaking the [cls] structure commonly used in pretraining,\nwe do not apply the pooling operation to the [cls] and keep the hidden state corresponding to\n[cls] intact. While conceptually simple, a naive implementation could slow down the computation\nby 15% due to the “irregular” sequence length caused by such an operation. Speciﬁcally, assume that\nsequence length of an input sample is a power of two, i.e., 2p, which usually is 512 in the pretraining\nphase. After one pooling operation with the [cls] intact, the length of the pooled sequence becomes\n2p−1 + 1, which is not a power of 2 anymore. As a result, it can cause memory misalignment and the\nwaste of paralleled computation power in accelerators, leading to substantial speed loss.\nTo resolve this issue, we employ a simple strategy to truncate the last token after the pooling. Formally,\ndenoting the pooled hidden state as h = {h[cls] ,h1,··· ,h2p−1 }, the truncation can be expressed as\nˆh = truncate(h) = [h[cls] ,h1,··· ,h2p−1−1] (6)\nWith this simple trick, we can always keep the sequence length a power of 2, hence avoiding the\nslowdown caused by maintaining an independent [cls] hidden state.\nA.2 Relative Positional Attention Implementation\nIn this work, we use the relative positional attention parameterization proposed in the Transformer-\nXL [32]. To facilitate further discussion, we ﬁrst review the details of this parameterization. Taking\nthe case of single head attention as the example head. Let T,D be the sequence length and hidden\ndimension respectively. Then, the pre-softmax attention score Aij between a pair of positions iand j\nconsists of two terms:\nAij = (WQhi + v)⊤(WKhj)  \ncontent term\n+ (WQhi + u)⊤(WRri−j)  \nposition term\n. (7)\nwhere v,u ∈RD are two trainable bias vectors, WQ,WK,WR ∈RD×D are three trainable pro-\njection matrices, and ri−j ∈RD is the sinusoidal positional encoding that represents the relative\ndistance i−jbetween the two positions.\nTo compute the entire attention score matrix A, the content term can easily be obtained via two head\nprojections and an outer product of complexity O(TD2 + T2D):\nAcontent = (HWQ + v)(HWK)⊤,\nwhere H = [h1,··· ,hT] ∈RT×D collects all hidden states into a matrix. However, we cannot\ncompute the position term in the same way as each Aposition\nij corresponds to a different ri−j. Hence, a\nnaive solution will be stacking T2 pairs of position encodings into a tensor ˆR ∈RT×T×D where\nˆRij = ri−j, and then perform the following tensor product:\nAposition = einsum(\"id,ijd->ij\",HWQ + u, ˆRWR).\nNote that the head projection RWK now has a complexity of O(T2D2) and a memory footprint of\nO(T2D), dominating all other computations.\nA.2.1 Standard Solution: Gather / Shift\nTo resolve the computation burden above, a common technique is to instead collect a matrix R ∈\nR2T−1×D, where\nR = [rT−1,...,r 0,··· ,r1−T]\nwhich includes all possible position encodings arranged from the maximum possible distance value\nT −1 to the minimum one 1 −T. Note that the full ˆR can be formed by gathering speciﬁc elements\nfrom R with an index matrix I of shape [T ×T], i.e.,\nˆR = gather(R,I), I ij = T + i−j.\nMathematically, this is equivalent to using a permutation tensorP ∈RT×T×2T−1 to multiply R, i.e.,\nˆR = PR, where Pij ∈R2T−1 is a one-hot vector used to select/gather a single position of R. As\n12\nthe attention score computation only involves linear operations, we can rearrange the computation of\nthe position term as follows\nAposition = einsum(\"id,ijd->ij\",HWQ + u,(PR)WR)\n= einsum\n(\n\"ijk,jk->ij\",P,\n[\n(HWQ + v)(RWR)⊤])\n= gather\n(\n(HWQ + v)(RWR)⊤,I\n)\nNote that, assuming gathering T2 elements only has a complexity of O(T2), which is true for\nCPU/GPU, this trick reduces the computation complexity back to O(2TD2 + 2T2D). In practice,\nthe gather operation can be implemented via a smart reshape operation, that is even cheaper.\nA.2.2 Optimization for TPU: factorized relative positional attention\nHowever, on TPUs, the assumption that gathering T2 elements only has a complexity of O(T2) does\nnot hold. Instead, we found that such a gather operation is dramatically slower on TPU. Hence, we\nhere consider another implementation which is signiﬁcantly faster on TPU.\nFirstly, let’s rewrite the position term as follows\nAposition\nij = (WQhi + u)⊤(WRri−j)\n=\n[\nW⊤\nR(WQhi + u)  \nqi\n]⊤\nri−j\n= q⊤\ni ri−j. (8)\nFor easier derivation, we have introduced a notation of qi. Then, recall the ri−j is the sinusoidal\nencoding that consists of the sine and the cosine components ri−j = cat(sini−j,cosi−j), where\nsint =\n[\nsin\n(\nt/100002/D\n)\n,sin\n(\nt/100004/D\n)\n,··· ,sin\n(\nt/10000D/D\n)]\n∈RD/2,\ncost =\n[\ncos\n(\nt/100002/D\n)\n,cos\n(\nt/100004/D\n)\n,··· ,cos\n(\nt/10000D/D\n)]\n∈RD/2.\nHence, we similarly divide qi deﬁned above into two parts, i.e.,\nqi = cat(qsin\ni ,qcos\ni ).\nGiven the deﬁnitions, we can further break Eqn. (8) into two terms:\nAposition\nij = q⊤\ni ri−j = qsin\ni\n⊤\nsini−j+qcos\ni\n⊤cosi−j.\nNow, using the trigonometric identities sin(a−b) = sin(a) cos(b) −cos(a) sin(b) and cos(a−b) =\ncos(a) cos(b) + sin(a) sin(b), the two terms can be respectively reformulated into\nqsin\ni\n⊤\nsini−j = qsin\ni\n⊤\n[sini⊙cosj−cosi⊙sinj]\n= qsin\ni\n⊤\n(sini⊙cosj) −qsin\ni\n⊤\n(cosi⊙sinj)\n=\n[\nqsin\ni ⊙sini\n]⊤\ncosj+\n[\nqsin\ni ⊙(−cosi)\n]⊤\nsinj\nand\nqcos\ni\n⊤cosi−j = qcos\ni\n⊤[cosi⊙cosj+ sini⊙sinj]\n= qcos\ni\n⊤(cosi⊙cosj) +qcos\ni\n⊤(sini⊙sinj)\n= [qcos\ni ⊙cosi]⊤cosj+[qcos\ni ⊙sini]⊤sinj\n13\nHence, combining these two parts together, it follows that\nq⊤\ni ri−j = qsin\ni\n⊤\nsini−j+qcos\ni\n⊤cosi−j\n=\n[\nqsin\ni ⊙sini\n]⊤\ncosj+\n[\nqsin\ni ⊙(−cosi)\n]⊤\nsinj+[qcos\ni ⊙cosi]⊤cosj+[qcos\ni ⊙sini]⊤sinj\n=\n{[\nqsin\ni ⊙sini\n]⊤\ncosj+[qcos\ni ⊙cosi]⊤cosj\n}\n+\n{[\nqsin\ni ⊙(−cosi)\n]⊤\nsinj+[qcos\ni ⊙sini]⊤sinj\n}\n=\n[\ncat(qsin\ni ,qcos\ni )  \n=qi\n⊙cat(sini,cosi)  \n:=φi\n]⊤\ncat(cosj,cosj)  \n:=ψj\n+\n[\ncat(qsin\ni ,qcos\ni )  \n=qi\n⊙cat(−cosi,sini)  \n:=πi\n]⊤\ncat(sinj,sinj)  \n:=ωj\n= [qi ⊙φi]⊤ψj + [qi ⊙πi]⊤ωj,\nwhere φi,ψj,πi,ωj above are simply 4 positional encodings formed by concatenating the cosine\nand sine vectors of the corresponding iand jin different ways. Note that, each term of the last line\nhas a factorized form that can be computed via an outer product, just like the standard content term.\nTherefore, by stacking φi,ψj,πi,ωj of all positions (i.e. i = 1,...,T and j = 1,...,T ) into the\ncorresponding Φ,Ψ,Π,Ω ∈RT×D respectively, the full position term can be expressed in a simple\nform\nAposition =\n{[\n(HWQ + u)W⊤\nR\n]\n⊙Φ\n}\nΨ⊤+\n{[\n(HWQ + u)W⊤\nR\n]\n⊙Π\n}\nΩ⊤\nwhich leads to the complexity of O(2TD2 + 4T2D), which is comparable to the content term.\nA.3 Potential Model Extensions\nIn this section, we discuss some potential model extensions of Funnel-Transformer. As described in\nsection 2, Funnel-Transformer can be divided into an encoder with a compression functionality and\na decoder that recovers the full-length token-level representations. To further extend the proposed\nmodel, ﬁrst note that the encoder-decoder framework can be formulated into a more general form:\nhenc = Encoder(xenc),\nhdec = Decoder(henc,xdec),\nwhere xenc and xdec are the encoder input sequence and the optional and problem-speciﬁc decoder\ninput, respectively. The goal of encoder is to compressing the input sequence xenc into the hidden\nrepresentations henc with a reduced length. Then, conditioned on the decoder input henc if any, the\ndecoder will extract relevant information/representations from henc to solve the speciﬁc NLP problem\nat hand. Next, we will how the general form of Funnel-Transformer can be instantiated into speciﬁc\nforms to solve corresponding NLP problems.\nSequence-level prediction This is essentially the case we consider in most of our experiments\nwhere we want to obtain a vectorial representation of the input sequence such as text classiﬁcation.\nIn this case, we don’t really need the decoderxdec (i.e. xdec = ∅) and the decoder simply extracts the\nhidden representation corresponding to the [cls] token from henc and feeds it into the task-speciﬁc\nstructure (e.g. classiﬁer).\nToken-level prediction In the token-level prediction tasks such as the MLM pretraining, SQuAD\nand sequence labeling, we need a decoder to recover the token-level representations from the\ncompressed sequence henc. In many cases, xdec could simply be the original sequence or a token-level\nhidden representation of it to provide ﬁne grained low-level information of each token and hence ease\nthe optimization. In this paper, we utilize the last-layer hidden states of the 1st block (before the ﬁrst\npooling operation) as the additional decoder input.\nBut for problems that utilize additional input signals, such as the permutation order used for permuted\nlanguage modeling in XLNet [3]. This additional information can be injected into Funnel-Transformer\nvia the decoder input xdec to (approximately) recover some more complex control of attention\nmechanism.\n14\nSequence-to-sequence problems Another important category of NLP task is sequence-to-sequence\nproblems, including machine translation, text summarization, and dialog generation, whose state-of-\nthe-art solution is the conventional encoder-decoder framework. Hence, Funnel-Transformer naturally\nﬁts these tasks, where the decoder input xdec corresponds to the target text sequence and the encoder\ninput xenc the source text sequence. This way, the key difference compared to conventional models is\nthe source side compression Funnel-Transformer provides.\nOverall, we summarize some potential directions to extend Funnel-Transformer presented in section\n2.2 to NLP problems. Finally, although we focus on discussion on the NLP tasks in this paper,\nFunnel-Transformer could be applied to any tasks dealing with sequential data, such as time series\nand video stream analysis.\nB Experiment Setting and Hyper-parameters\nB.1 Preprocessing & Tokenization\nFor all experiments conducted in this work, we simply adapt the “uncased” word piece model\noriginally used by BERT [2], where the vocabulary size is about 30K. Other than lower case and\nthe default preprocessing included in the word piece tokenizer, the only additional preprocessing we\nperform is to remove some http symbols (e.g. <b>) in the 7 text classiﬁcation tasks.\nB.2 Pretraining\nHparam Base Scale Large Scale\nHidden dropout 0.1\nGeLU dropout 0.0\nAttention dropout 0.1\nMax sequence length 512\nBatch size 256 8192\nLearning rate 1e-4 2e-4\nNumber of steps 1M 500K\nWarmup steps 10K 30K\nOptimizer Adam Weight Decay\nLearning rate decay Linear\nAdam epsilon 1e-6\nWeight decay 0.01\nTable 7: Hyper-parameters for pretraining.\nThe hyper-parameters used for the two different pretraining settings are summarized in Table 7. One\nexception is the learning rate used for B10-10-10H1024 at the base scale. Speciﬁcally, we ﬁnd the\ntraining can be unstable when the depth goes beyond 24 layers (in the case of B10-10-10H1024)\nat base scale, especially for the MLM objective. Hence, we reduce the learning to 8e-5 for the\nB10-10-10H1024 F-TFM during base-scale pretraining. This has a side effect of a slower training\npace and potentially a slightly worse ﬁnetuning performance. However, we does not observe such\ninstability when the batch size is increased such as in the large-scale setting.\nFor ELECTRA, there are two additional important hyper-parameters, i.e., the discriminator loss\ncoefﬁcient and the relative size multiplier of the generator. In this work, we does not tune these two\nhyper-parameters at all and simply use the numbers from the original paper, i.e., the discriminator loss\ncoefﬁcient of 50 and size multiplier of 1/4 for all architectures trained with ELECTRA. In addition,\nin ELECTRA training, whenever F-TFM is used as the discriminator, the generator also uses the\nF-TFM.\nIn additional, in the all experiments, we only annotate the size of hidden states the rest of model sizes\ncan be derived from on it:\n• The embedding size = hidden size\n• The size of inner states of P-FFN is “4 ×hidden size”.\n• The attention head dimension is always 64.\n• The number of attention heads is “hidden size/64”.\n15\nFinally, another important element in pretraining is the mask sampling strategy. For MLM training,\nfollowing previous work, we always complete word span (up to 5 complete words) sampling.\nHowever, for ELECTRA training, we notice a weird phenomenon that under the base-scale setting,\nthe performance of both the Transformer and the F-TFM drops signiﬁcantly if we use word span\nsampling rather than the single-token sampling. On the other hand, under the large-scale setting,\nusing word span sampling works ﬁne. Hence, we use single-token sampling for base-scale ELECTRA\ntraining, and word span sampling for large-scale ELECTRA training.\nB.3 Finetuning\nHparam RTE MRPC STS-B CoLA SST-2 QNLI MNLI QQP\nHidden dropout 0.1\nGeLU dropout 0.0\nAttention dropout 0.1\nMax sequence length 128\nBatch size 16 16 16 16 32 32 64 64\nNumber of epochs 10 10 10 10 5 3 3 5\nLearning rate decay Linear\nWeight decay 0.01\nWarmup proportion 0.1\nAdam epsilon 1e-6\nHparam IMDB AG DBpedia Yelp-2 Yelp-5 Amazon-2 Amazon-5\nHidden dropout 0.1\nGeLU dropout 0.0\nAttention dropout 0.1\nMax sequence length 512 128 128 512 512 512 512\nBatch size 32 32 64 128 128 128 128\nNumber of epochs 5 3 3 3 3 3 3\nLearning rate decay Linear\nWeight decay 0.01\nWarmup proportion 0.1\nAdam epsilon 1e-6\nTable 8: Hyper-parameters for ﬁnetuning on the GLUE benchmark and 7 text classiﬁcation datasets.\nFor all the ﬁnetuning experiments, we essentially inherit the hyper-parameters used by XLNet [3].\nAll the performance numbers reported are obtained on TPUs with TensorFlow 2.2.\nB.3.1 GLUE & Text Classiﬁcation\nFor GLUE and text classiﬁcation datasets, we ﬁrst ﬁx the values of most hyper-parameters shown in\nTable 8. Then, we only search the learning rates from the set [1e-5, 2e-5, 3e-5], and choose the best\none according to the validation set.\nFollowing previous work [3, 4, 5], all GLUE performances correspond to the median result of 5 runs\nfrom different random seeds in the base setting and 15 runs in the large setting, respectively.\nFor the text classiﬁcation, the base-scale results are the median performance among 5 runs with\ndifferent random seeds. However, for the large-scale experiments, to be compatible with previous\nwork [34, 3], the results are the best performance among 5 random runs.\nB.3.2 Reading Comprehension\nAgain, following XLNet [3], the hyper-parameters used for ﬁnetuning on the RACE and SQuAD\ndatasets are summarized in Table 9. “Layer-wise decay” means exponentially decaying the learning\nrates of individual layers in a top-down manner. For example, suppose the 24-th layer uses a learning\nrate l, and the Layer-wise decay rate is α, then the learning rate of layer mis lα24−m. In addition,\nfor the two versions of SQuAD, we simply reuse the model trained on SQuAD v2.0 when evaluated\non SQuAD v1.1.\n16\nHparam RACE SQuAD\nDropout 0.1\nAttention dropout 0.1\nMax sequence length 512 512\nTraining epochs/steps 5 epochs 8000 steps\nWarmup proportion/steps 0.1 1000 steps\nBatch size [16, 32] 48\nLearning rate [1e-5, 2e-5] 3e-5\nLearning rate decay linear\nWeight decay 0.01\nAdam epsilon 1e-6\nLayer-wise lr decay 1.0 0.75\nTable 9: Hyper-parameters for RACE and SQuAD.\nC Additional Experimental Results\nC.1 Text Classiﬁcation at Large Scale\nModel IMDB AG DBpedia Yelp-2 Yelp-5 Amazon-2 Amazon-5\nBERT-Large 4.51 - 0.64 1.89 29.32 2.63 34.17\nROBERTA-Large 3.50 - - - - - -\nXLNet-Large 3.20 4.45 0.64 1.37 27.05 2.11 31.67\nB10-10-10H1024 3.36 4.66 0.60 1.33 27.14 2.10 31.64\nB8-8-8H1024 3.42 4.96 0.63 1.39 27.20 2.14 31.74\nMPNet 4.40 - - - - - -\nB6-6-6H768 3.72 5.00 0.64 1.50 27.73 2.27 32.11\nB6-3x2-3x2H768 3.82 5.12 0.64 1.58 27.96 2.32 32.23\nB4-4-4H768 4.12 5.09 0.67 1.70 28.40 2.35 32.46\nTable 10: Text classiﬁcation performance comparison under the large-scale pretraining.\nTable 10 includes the performance comparison on 7 text classiﬁcation tasks under the large-scale\ntraining setting. Similar to the GLUE benchmark results, compared with the previous result based on\nTransformer, with fewer FLOPs, the proposed F-TFM achieves comparable results.\nC.2 Training Cost Comparison\nIn this section, we test the pretraining and ﬁnetuning speed of the F-TFM in comparison to the\nstandard Transformer on the TPU and GPU platform. For the pretraining speed evaluation, we test\nF-TFM on TPU v3-16 (16 cores x 16Gb) with TensorFlow. For the ﬁnetuning speed evaluation,\nwe test F-TFM on TPU v2-8 (8 cores x 8Gb) with TensorFlow and on Nvidia-V100 (16Gb) GPU\nwith the PyTorch. The TensorFlow version is 2.2.0, and the PyTorch version is 1.5.0. For the GPU\nexperiments, we use an 8-GPU node on the Google Cloud Platform. All running speeds are reported\nwith the FP16 optimizer. In the PyTorch implementation, we use “O2” options of AMP manager\nin the apex3 package to handle the FP16 optimization. For ﬁnetuning, we consider three different\nsequence lengths, namely 128, 256 and 512. For pretraining, we only consider the sequence length\n512. In each case, we choose the maximum possible batch size allowed by the memory size of the\ndevice(s). We measure the actual model running time by performing 1000 steps gradient descent with\nrandom input sequences with the ﬁxed length.\nFirstly, we compare the model speed in the ﬁnetuning stage. Note that the decoder is not used in this\nsetting. Table 11 and 12 summarize the ﬁnetuning running time comparison on GPUs and TPUs,\nrespectively.\n• In the base model (L12H768) group, we observe that the speed of B6-6-6H768 is similar or faster\nthan the base Transformer model, despite the fact that B6-6-6 is deeper, has more parameters.\n3https://github.com/NVIDIA/apex\n17\nSequence length 128 256 512\nMetrics Run time Mem Run time Mem Run time Mem GLUE1 GPU 8 GPUs 1 GPU 8 GPUs 8 GPUs\nBatch size / GPU 64 32 16\nL12H768 1.00x 1.00x 9.2G 1.00x 1.00x 11.0G 1.00x 14.3G 84.40\nB6-6-6 0.97x 0.99x 9.1G 0.95x 0.97x 10.3G 0.94x 12.5G 85.37\nB6-3x2-3x2 0.93x 0.93x 8.4G 0.91x 0.92x 9.5G 0.90x 11.8G 84.78\nB4-4-4 0.67x 0.67x 6.6G 0.65x 0.66x 7.5G 0.64x 9.0G 83.99\nBatch size / GPU 32 12 4\nL24H1024 1.00x 1.00x 14.8G 1.00x 1.00x 14.4G 1.00x 13.9G 86.62\nB10-10-10 0.87x 0.92x 14.0G 0.90x 0.93x 13.0G 0.96x 12.7G 87.03\nB8-8-8 0.70x 0.73x 11.6G 0.73x 0.75x 10.8G 0.78x 10.5G 86.70\nTable 11: Running time and memory consumption comparison between F-TFMs and the standard\nTransformer on the GPU. In each model group, the standard Transformer (ﬁrst model) is used as the\nbenchmark for the rest of F-TFM models. Note that, given the same batch size per GPU, the memory\nconsumption is roughly the same for 1 GPU and 8 GPUs.\nSequence length 128 256 512\nMetrics Run time on 8 TPU cores (TPUv2-8) GLUE\nBatch size / TPU core 64 32 16\nL12H768 1.00x 1.00x 1.00x 84.40\nB6-6-6 0.99x 0.88x 0.81x 85.37\nB6-3x2-3x2 0.97x 0.87x 0.77x 84.78\nB4-4-4 0.69x 0.62x 0.55x 83.99\nBatch size / TPU core 16 8 4\nL24H1024 1.00x 1.00x 1.00x 86.62\nB10-10-10 0.89x 0.81x 0.73x 87.03\nB8-8-8 0.66x 0.60x 0.56x 86.70\nTable 12: Running time between F-TFMs and the standard Transformer on the TPU v2-8. In each\nmodel group, the standard Transformer (ﬁrst model) is used as the benchmark for the rest of F-TFM\nmodels.\nMoreover, B6-6-6H768 achieves better results compared with the base Transformer model. The\nsimilar conclusion applies to the B6-3x2-3x2 model, which has the same amount of parameters as\nthe base model. The B4-4-4 model, which has the same depth and model parameters as the base\nmodel, is able to provide 30%-50% speedup without losing too much performance.\n• In the large model (L24H1024) group, the conclusion is similar. The speed of the larger model\nB10-10-10 is almost the same as the large model, and the speed of B8-8-8 is signiﬁcantly faster\nthan the large model. In addition, when sequence length equals 512, the acceleration of F-TFM on\nthe TPU is more obvious than the GPU.\n• In the both groups, all the tested F-TFM variants have smaller memory footprint compared with\nthe standard TFM models, showing the memory efﬁciency of F-TFM.\nNext, we compare the model speed during pretraining under the MLM objective in table 13, which has\nan additional cost due to the decoder. The results show that the proposed method can still substantially\nimprove the pretraining speed compared to the standard Transformer, though the speed gain is slightly\nsmaller than the ﬁnetuning stage. In summary, this study demonstrates that the proposed method is\nmore efﬁcient in both the ﬁnetuning and pretraining stages in modern parallel computing platforms.\n18\nSequence Length 512\nRunning Time FLOPs\n#TPU cores / Total bsz 16 / 512\nL12H768 1.00x 1.00x\nB6-6-6H768D2 0.99x 1.04x\nB6-3x2-3x2H768D2 0.97x 1.04x\nB4-4-4H768D2 0.79x 0.75x\n#TPU cores / Total bsz 16 / 128\nL24H1024 1.00x 1.00x\nB10-10-10H1024D2 0.83x 0.81x\nB8-8-8H1024D2 0.71x 0.66x\nTable 13: TPU pretraining speed comparison. The sufﬁx “D2” means that the F-TFM model has 2\ndecoder layers.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7673959732055664
    },
    {
      "name": "Transformer",
      "score": 0.6812605857849121
    },
    {
      "name": "Security token",
      "score": 0.6195189356803894
    },
    {
      "name": "Scalability",
      "score": 0.541628360748291
    },
    {
      "name": "Exploit",
      "score": 0.4875349700450897
    },
    {
      "name": "Language model",
      "score": 0.471706360578537
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.4399428367614746
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4331570565700531
    },
    {
      "name": "Sequence labeling",
      "score": 0.4191124439239502
    },
    {
      "name": "Speech recognition",
      "score": 0.3205447793006897
    },
    {
      "name": "Engineering",
      "score": 0.12156021595001221
    },
    {
      "name": "Computer network",
      "score": 0.11153808236122131
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 104
}