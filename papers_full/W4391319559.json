{
  "title": "Foundation Models",
  "url": "https://openalex.org/W4391319559",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2087734415",
      "name": "Johannes Schneider",
      "affiliations": [
        "University of Liechtenstein"
      ]
    },
    {
      "id": "https://openalex.org/A2037936149",
      "name": "Christian Meske",
      "affiliations": [
        "Ruhr University Bochum"
      ]
    },
    {
      "id": "https://openalex.org/A3047890850",
      "name": "Pauline Kuss",
      "affiliations": [
        "Ruhr University Bochum"
      ]
    },
    {
      "id": "https://openalex.org/A2087734415",
      "name": "Johannes Schneider",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2037936149",
      "name": "Christian Meske",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3047890850",
      "name": "Pauline Kuss",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118271039",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W6780161852",
    "https://openalex.org/W4361298663",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4360620450",
    "https://openalex.org/W3025819524",
    "https://openalex.org/W4377235854",
    "https://openalex.org/W4280523315",
    "https://openalex.org/W4380319827",
    "https://openalex.org/W3153990350",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4384345640",
    "https://openalex.org/W3164854573",
    "https://openalex.org/W4293059627",
    "https://openalex.org/W4312238169",
    "https://openalex.org/W3177667502",
    "https://openalex.org/W3199400376",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3111875263",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4224250640",
    "https://openalex.org/W4381572755",
    "https://openalex.org/W3135939397",
    "https://openalex.org/W4309222128",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W2967831706",
    "https://openalex.org/W4283218921",
    "https://openalex.org/W4385302156",
    "https://openalex.org/W4360980513",
    "https://openalex.org/W6764372214",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W6600277017",
    "https://openalex.org/W6600164255",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W3155739706"
  ],
  "abstract": null,
  "full_text": "CATCHWORD\nFoundation Models\nA New Paradigm for Artiﬁcial Intelligence\nJohannes Schneider • Christian Meske • Pauline Kuss\nReceived: 19 May 2023 / Accepted: 3 December 2023 / Published online: 29 January 2024\n/C211 The Author(s) 2024\nKeywords Foundation models /C1Artiﬁcial intelligence /C1\nGenerative AI /C1Emergent behavior /C1Prompting\n1 Introduction\nRecently, the domain of artiﬁcial intelligence (AI) has\nexperienced a profound transformation with the emergence\nof foundation models as a new paradigm for developing AI\nsystems (Bommasani et al. 2021). Foundation models\nconstitute large-scale AI models that are pre-trained on vast\namounts of general data and that can be adapted for\ndownstream applications (e.g., by ﬁne-tuning them through\nfurther training on application-speciﬁc data). Through this\npre-train and adapt approach they expedite the develop-\nment of innovative AI products and services and accelerate\nthe accessibility of high-performance AI solutions in var-\nious industries (Teubner et al. 2023).\nFoundation models show remarkable abilities to com-\nprehend, generate, and adapt content across diverse\ndomains, including creative generations (Chen et al. 2023),\nsoftware debugging (Sobania et al. 2023), protein\nsequencing (Madani et al. 2023), or cross-modality outputs\nsuch as text-to-image creations (Ramesh et al. 2021). With\nscaling, foundation models are becoming increasingly good\nat performing tasks they were not explicitly trained for,\nthereby broadening the scope of applications achievable by\na single model without the need for additional training data\nor ﬁne-tuning (Brown et al. 2020). When needed, task-\nspeciﬁc performance can be further enhanced through ﬁne-\ntuning or effective prompt engineering techniques; both of\nwhich incur signiﬁcantly lower costs in comparison to\ndeveloping a new model from scratch (Liu et al. 2023; Niu\net al. 2020).\nThe paradigm shift brought about by foundation models\nreshapes the design and deployment of AI applications.\nThe advantages of large-scale foundation models including\ntheir emerging capabilities encourage convergence within\nthe AI industry, leading to a growing number of AI\napplications being adjustments of only a few foundation\nmodels, owned by a few organizations and trained on a few\ndatasets (Bommasani et al. 2021). Such homogenization\npromises great leverage to accelerate AI advancements\nacross various domains. But it also raises concerns\nincluding monopolistic power structures, economic\ndependencies, or the potential dissemination of model\nvulnerabilities across a great number of downstream\napplications (Fishman and Hancox-Li 2022). As founda-\ntion models establish themselves as a cornerstone of state-\nof-the-art AI advancement, the dynamics of value creation\nand accumulation in the AI industry can be expected to\nchange, and organizations might be forced to reconsider\nhow they can differentiate their AI products and services in\nan age in which high-performance, multi-functional AI\nsolutions are widely available. Lastly, because the design\nand control over AI systems is becoming dispersed across\nAccepted after two revisions by Christine Legner.\nJ. Schneider ( &)\nDepartment of Computer Science and Information Systems,\nUniversity of Liechtenstein, Fuerst Franz Josef Str., 9490 Vaduz,\nLiechtenstein\ne-mail: johannes.schneider@uni.li\nC. Meske /C1P. Kuss\nFaculties of Mechanical Engineering and Computer Science,\nInstitute of Work Science, Ruhr University Bochum, Wasserstr.\n221, 44799 Bochum, Germany\ne-mail: christian.meske@rub.de\nP. Kuss\ne-mail: pauline.kuss@rub.de\n123\nBus Inf Syst Eng 66(2):221–231 (2024)\nhttps://doi.org/10.1007/s12599-024-00851-0\nan evolving ecosystem of actors, the shift from disparate\nmodels to a foundational approach challenges existing\napproaches of AI governance (Koniakou 2023; Schneider\net al. 2023).\nBy redeﬁning existing premises of AI development,\nmanagement, and governance, the rise of foundation\nmodels will shape the trajectory of AI research, bringing\nforth important questions and opportunities for the ﬁeld of\nInformation Systems (IS) research and Business and\nInformation Systems Engineering (BISE) (Dwivedi et al.\n2023; Teubner et al. 2023). With this catchword article, we\nintend to contribute to the ﬁeld’s comprehension of foun-\ndation models and to outline a sociotechnical perspective\n(Sarker et al. 2019) on the intricate implications of this new\nparadigm for the construction and deployment of AI\napplications. To do so, we introduce the concept of foun-\ndation models and their deﬁning features, followed by\ndescribing the implications of foundation models as a new\nparadigm of AI, and, ﬁnally, outlining opportunities for\nfurther IS research.\nWe begin Sect. 2 by deﬁning the concept of foundation\nmodels and describing their emergence in the historical\ncontext of machine learning advancements to clarify their\npivotal role in the current AI landscape. We then elaborate\non the key features of foundation models – namely emer-\ngent capabilities, homogenization, and prompt sensitivity –\nand discuss the implications of these characteristics for AI\ndevelopment and deployment. In Sect. 3, we outline mul-\ntiple avenues for future research, particularly focusing on\nopportunities relevant to the BISE community as a\nsociotechnical and construction-oriented discipline. In\nSect. 4, we conclude the article with a summary and\nindicate its limitations.\n2 Foundation Models as a New Paradigm for AI\nFollowing Bommasani et al. ( 2021, p. 1), we deﬁne a\nfoundation model as ‘‘any model that is trained on broad\ndata that can be adapted to a wide range of downstream\ntasks’’. Through this combination of task-agnostic pre-\ntraining and subsequent ﬁne-tuning, foundation models\nenable new approaches to building and deploying AI sys-\ntems. Therefore, the rise of foundation models constitutes a\nparadigm shift in AI that promises unique potential and\nrisks (Li et al. 2022), which is a key focus of this manu-\nscript as elaborated in the following sections. Foundation\nmodels can be pre-trained on a speciﬁc modality (e.g.,\nlanguage, vision, robotics, reasoning) or show multi-modal\ncapabilities (Reed et al. 2022). Current examples of foun-\ndational models include Open AI’s GPT-Series (Brown\net al. 2020), BERT, and CLIP. While entering numerous\ndomains, the pre-training paradigm of foundation models\ntakes shape most strongly in pushing benchmarks in the\nﬁeld of natural language processing (NLP) where appli-\ncations such as ChatGPT, an application built on top of the\nGPT foundation model, can now generate texts that are\nincreasingly indistinguishable from human writing (Bender\net al. 2021).\nTechnically, foundation models are nothing new: they\nare based on long-existing deep neural networks and\nstandard transfer learning. However, their size and large-\nscale training data result in newly emergent capabilities\nthat can be transferred across applications and ﬁne-tuned\nfor the creation of numerous AI applications. Before\nintroducing the deﬁning features of foundation models and\ntheir respective implications for AI development and use,\nwe describe the emergence of foundational models in the\nhistorical context of machine learning advancements.\n2.1 History: From Expert Systems to Foundation\nModels\nFoundation models can be seen as a ‘‘logical’’ step in the\ndevelopment of machine learning as shown in Fig. 1.\nBefore learning from data, decision-making by machi-\nnes was done by expert systems which encoded explicit\nrules extracted from experts on how to turn an input into an\noutput. Later, machine learning systems could operate\n‘‘without explicitly being programmed’’, as coined by\nSamuel Jackson in 1959. At ﬁrst, machine learning systems\nlearned decision rules based on a set of criteria, i.e., fea-\ntures, still deﬁned by experts. ( Simple) representation\nlearning aimed at automating the identiﬁcation of features,\nwhich was marked by a breakthrough based on deep\nlearning using artiﬁcial neural networks. Deep learning\nenabled machine learning systems to learn a hierarchy of\nfeatures directly from the data, reducing the need for fea-\nture engineering (Janiesch et al. 2021). It demanded much\nlarger datasets and models. Deep learning also allowed\nbuilding models in a modular, ﬂexible way by stacking\nvarious layers of artiﬁcial neurons on top of each other.\nThis made it easy to enlarge models or to combine models\ntrained on different modalities of data such as text and\nimages. Consequently, deep learning was adopted in vari-\nous areas of AI, including computer vision, speech, and\nnatural language processing, and speciﬁc deep learning\nmodels were developed through different compositions of\nbasic elements.\nInitially, neural networks were mostly trained using\nsupervised learning. Supervised learning relies on a labeled\ndataset in which each input is associated with an output\nlabel (Janiesch et al. 2021) commonly provided by a\nhuman. Model training was constrained by the availability\nof labeled training data. Addressing this challenge while\nalso reducing training costs, the method of transfer learning\n123\n222 J. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024)\nlater allowed reusing the ‘‘knowledge’’ a neural network\nlearned from one task (e.g., recognizing objects in images)\nfor another task (e.g., recognizing behavior in videos) by\nkeeping most of the model’s architecture unchanged and\nretraining only parts of the network (Niu et al. 2020). Put\ndifferently, transfer learning enables pre-training a model\non a surrogate task, and then adapting it via ﬁne-tuning to a\ndownstream task.\nTransfer learning, together with scale, enabled the suc-\ncess of foundation models starting around 2017 (Bom-\nmasani et al. 2021). While transfer learning based on\nlabeled datasets had been a practice before (e.g., Deng\net al. 2009), the required pre-training was still limited by\nthe cost of data annotation. This limitation was overcome\nby a wave of developments in self-supervised learning. In\nself-supervised learning the pre-training task is derived\nfrom task-agnostic data with self-generated labels. For\nexample, language models such as the GPT-1 to GPT-4 are\ntrained to predict the next word in a text, i.e., the label to\npredict is simply the next word in the text (OpenAI 2023a).\nWhile foundation models achieve state-of-the-art results\nthat on some tasks outperform tailored models (Ziems et al.\n2023), additional ﬁne-tuning based on supervised learning\n(using a small-scale, task-speciﬁc dataset), reinforcement\nlearning (using human feedback to generate output), or\ninstruction tuning (based on a dataset described via\ninstructions) often improves application-speciﬁc perfor-\nmance and model alignment with user intent (Ouyang et al.\n2022). Fine-tuning of foundation models furthermore\nconstitutes an important tool to increase models’ alignment\nwith ethical standards (Ouyang et al. 2022). For example,\nhuman feedback and additional datasets can be used to\npenalize toxic outputs or counterweight learned biases of a\nmodel.\nSelf-supervised learning hence revolutionized the utility\nof low-cost training data, which could now be abstracted\nthrough web crawling. This enabled the scaling of larger\nand more expressive pre-trained models that can be opti-\nmized in subsequent ﬁne-tuning (Bender et al. 2021).\nScaling was further supported by improvements in com-\nputer hardware and the development of the transformer\nmodel architecture (Vaswani et al. 2017). Unprecedented\nscalability and the resulting ability to efﬁciently handle\nlarge amounts of diverse data and to ﬂexibly capture\ndiverse information therefrom (model expressiveness) is\nwhat sets foundation models apart from prior deep learning\nmodels. This can be observed, for example, in models like\nGPT-4 (OpenAI 2023a) or Llama-2 which both perform\nextremely well on a broad set of tasks by increasing model\nand training data size compared to the original transformer\nmodel (Vaswani et al. 2017) and earlier models. Additional\nrelevant properties of foundation models include multi-\nmodality (the processing of multimodal data such as ima-\nges and text in one model), memory (storing and retrieving\nknowledge, possibly from a model external source), and\ncompositionality (modularity of the model and generaliz-\nability) (Bommasani et al. 2021). Jointly, these properties\nresult in the key features of foundation models such as\nemergent capabilities of in-context learning (Brown et al.\n2020). In-context learning refers to models’ ability to solve\ntasks without explicitly being trained on them, constituting\na key feature of foundational models as detailed below.\n2.2 Key Features of Foundation Models\nTo frame our consideration of foundation models from the\nperspective of IS research, we start with a description of\ntheir essential characteristics. Following previous research,\nwe identify emergent capabilities and homogenization as\nkey features of foundation models (Bommasani et al. 2021;\nFishman and Hancox-Li 2022). We furthermore add\nprompt sensitivity, given its contingency on foundation\nmodel pre-training and its critical implications for AI\ndevelopment and deployment as detailed in Sect. 2.3.\nFig. 1 History of machine learning\n123\nJ. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024) 223\nEmergent Capabilities The ﬁrst key feature of founda-\ntion models is the emergence of behavioral capabilities that\nwere not explicitly constructed, nor expected, by human\ndevelopers, which frequently is referred to as in-context\nlearning (Min et al. 2022). Emergent capabilities are pri-\nmarily investigated in the context of large-language models\n(e.g., Wei et al. 2022). During training, the large-scale\nmodel extracts a rich set of patterns and broad skills from\nthe diverse training data, for example abstracting ‘‘an\nunderstanding’’ of the vocabulary and grammar of a lan-\nguage from a text corpus without being trained for a\nspeciﬁc task (e.g., translation or question answering).\nDuring application, it can then ﬂexibly employ respective\nmodeling to perform various downstream tasks by the\nprovision of a prompt, i.e., a description of the task in\nnatural language or through a visual representation and,\npossibly, a few examples. For instance, GPT-3 and later\nversions show strong performance on mathematical tasks\nand analogical reasoning although they were not speciﬁ-\ncally trained for such (Brown et al. 2020; Webb et al.\n2023). In contrast to GPT-3 (being a model of 175 billion\nparameters), the smaller GPT-2 (1.5 billion parameters)\ndoes not show comparable capabilities of in-context\nlearning, demonstrating the contingency of emergent\ncapabilities on model scale (Bommasani et al. 2021).\nThe feature of emergent capabilities is essential to\nmodels’ high performance on complex tasks such as nat-\nural language understanding and generation. The proﬁ-\nciency of large-scale foundation models at ‘‘tasks deﬁned\non-the-ﬂy’’ suggests that the relevance of ﬁne-tuning for\ntask-speciﬁc performance might decrease as models grow\nfurther in size (Brown et al. 2020, p. 9). This would\ndecrease the cost and required effort to deploy foundation\nmodels for an even wider range of downstream tasks.\nMoreover, in-context learning allows to improve model\nperformance by crafting adequate prompts without\nparameter updates (frequently referred to as prompt\nengineering).\nThe property of emergent capabilities also raises con-\ncerns. Emergence implies a substantial uncertainty over the\ncapabilities, ﬂaws, and limitations of foundational models,\nmaking it hard to understand, explain, and predict their\nbehavior and potential failure modes (Bommasani et al.\n2021). This is illustrated by the extent to which slightly\naltered prompts may cause considerably different out-\ncomes; that is, small changes to the model input can cause\na large change in its qualitative behavior. In this context,\nthe complexity and relevance of prompt engineering can be\nexpected to increase as new, unexpected model behavior\nemerges, a point we will return to below. Uncertainty over\nmodel behavior particularly demands caution where\nundesired behaviors of a foundation model could be passed\non to adapted models downstream, implying that the key\nfeatures of emerging properties and homogenization ‘‘in-\nteract in a potentially unsettling way’’ (Bommasani et al.\n2021, p. 6).\nHomogenization Foundation models give rise to\nunprecedented degrees of homogenization, referring to the\nconsolidation of methodologies and models across AI\napplications and research communities. Homogenization is\nvisible in three interrelated developments: new AI models\nare adjustments of (i) a few foundational models, (ii)\ntrained on a few datasets, and/or (iii) by a few organiza-\ntions (Bommasani et al. 2021). Homogenization is driven\nby the immense costs of training foundation models (e.g.,\ncomputational and data aggregation costs), the monopoly\nof few companies on some large-scale proprietary datasets\n(e.g., social media platforms), and the self-perpetuating\ncycle of improved model performance, user engagement,\nand model-improving user feedback. These factors push\ntoward a winner-takes-it-all dynamic that can be expected\nto result in the development of a few large-scale foundation\nmodels upon which a large share of future AI systems will\nbe built.\nHomogenization promises great leverage, as advances in\nthe foundation model are automatically inherited by\ndownstream AI systems. This leverage accelerates and\ndecreases the cost of developing task-speciﬁc AI systems,\nnow requiring only small-scale training or skillful\nprompting without any ﬁne-tuning at all. Possibly, this\nmakes AI available to new domains in which model\ntraining was previously unaffordable or impeded by the\nunavailability of rich datasets.\nOn the other hand, homogenization describes a cen-\ntralization of ﬁeld-leading AI research with a few compa-\nnies, raising concerns related to power, dependencies, and\nthe safeguarding of social and ethical interests within the\neconomically driven private sector (Fishman and Hancox-\nLi 2022). Moreover, it risks algorithmic monoculture and\nimplies that the same datasets – namely those underlying\nthe training of the foundation models – are encoded within\nnumerous AI applications (Kleinberg and Raghavan 2021).\nIntuitively, this points toward another major concern:\nhomogenization leverages not only the beneﬁts but also\npotential risks and ﬂaws of the underlying model and\ndatasets across the AI landscape (Bommasani et al. 2022).\nPut differently, any bias or undesirable behavior of a\nfoundation model will, in the absence of preventative\nmeasures, likely be inherited by downstream applications.\nBesides the centralization of AI advancements,\nhomogenization also holds implications for the involved\nactors and their respective roles in developing AI appli-\ncations (Hacker et al. 2023). Increasingly, AI applications\nwill be the product of an emerging ecosystem comprised of\nfoundation model providers, foundation model adapters\nand integrators, and end users, as illustrated in Fig. 2.\n123\n224 J. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024)\nOrganizations operating as adapters and integrators ﬁne-\ntune foundation models with labeled data or prompting\ncreating task-speciﬁc models (Liu et al. 2023) and embed\nmodels into larger systems that are directly consumable by\nend users. Their respective choice of adaptation mechanism\ndepends mostly on three factors: compute budget, data\navailability, and access to the foundation model. Adjusting\nthe model itself to a speciﬁc task through transfer learning,\nusing a (small) task-speciﬁc dataset, relies on the ability to\nmodify model parameters directly or indirectly. Hence, it is\ncontingent upon the provisions and allowances made by the\nfoundation model providers, providing an example of\npower centralization through homogenization as noted\nabove. Alternatively, adapters can ﬁne-tune the behavior of\na foundation model for a speciﬁc use case by means of\nprompting. Prompt-based ﬁne-tuning does not depend on\nmodel access, but model providers can potentially restrict\nthe permissibility of certain prompts. End users can simi-\nlarly use prompting to impact model outputs. In the fol-\nlowing section, we will consider the implications of this\nmultilayered relevance of prompting in more detail.\nPrompt Sensitivity Prompting refers to the use of natural\nlanguage to guide the behavior of AI systems (Reynolds\nand McDonell 2021). Hence, the possibility of using\nprompts to instruct respective systems is contingent on\ntheir sophisticated capabilities to process natural language\nwhich, as detailed above, emerge from the foundation\nmodels pre-training on large-scale text corpora. However,\nthe relevance of prompts for model behavior extends\nbeyond pure language models and also encompasses multi-\nmodal foundation models such as text-to-image AI (Liu\nand Chilton 2022).\nIn the context of foundation models, prompting is highly\nrelevant because the ability of the respective system to\ngenerate diverse and possibly objectionable output\ndemands the possibility to specify and constrain desirable\ngenerations (Hacker et al. 2023). In this context, the exact\nwording of a prompt including instructions on how to\ntackle a problem (e.g., reasoning instructions, adoption of a\nrole) and inclusion of exemplary input–output pairs (to\nleverage in-context learning) can have a signiﬁcant impact\non the behavior of respective systems. This makes prompt\nengineering, i.e., optimizing the most suitable prompt to\nrealize desired outputs, an important tool to ﬁne-tune,\nregulate, and query AI applications based on task-agnostic\nfoundation models (Liu et al. 2023). For example, Kojima\net al. ( 2022) showed that by adding ‘‘Let’s think step by\nstep’’ to the textual instruction of a prompt and therewith\nrequiring serializing reasoning, the model performed sig-\nniﬁcantly better on various benchmarks. This has been an\nunexpected phenomenon that, once again, is only visible\nwith large-scale models (Chowdhery et al. 2023). Other\ntechniques of prompt engineering similarly improve model\nperformance, including task speciﬁcation by example or\nthe use of memetic proxies (Reynolds and McDonell\n2021). The art of prompting receives increasing (scholarly)\nattention including the emergence of frameworks for sys-\ntematic, reusable solutions to prompt engineering (e.g.,\nWhite et al. 2023).\nReferring to the above illustration of actors involved in\nthe development and use of AI systems built on foundation\nmodels (Fig. 2), prompts can be used by each to guide\nmodel behavior: by the foundation model provider, by the\nfoundation model adapter, and by the end user. Publica-\ntions on prompt engineering techniques frequently fore-\nground the prompts of end users querying downstream\napplications such as ChatGPT or DALL-E. However, nat-\nural language prompts can also be used by providers and\nadapters to address the foundation model directly. Besides\nﬁne-tuning, model providers and adapters may use\nprompting to constrain unwanted model behavior (Ouyang\net al. 2022). Respective prompts intended to control system\noutcomes are also referred to as system prompts and\nreceive particular attention in the context of jailbreaks –\ni.e., intentional attacks intended to circumvent system\nprompts through adversarial user prompts (Wei et al. 2023;\nFig. 2 AI development involving foundation models\n123\nJ. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024) 225\nZou et al. 2023). Recent research demonstrates that the\ngeneration of successful attack queries can be automated,\nindicating an issue of major concern with language-based\nfoundation models that still requires solving (Zou et al.\n2023). The feature of prompt sensitivity is thus essential\nwith respect to the customization, control, and use of\nfoundation models and their downstream applications.\nMore work to better understand respective possibilities and\nlimitations can be expected.\n2.3 Implications for AI Development, Deployment,\nand Use\nThe characteristics of emergent capabilities, homogeniza-\ntion, and prompt sensitivity have implications for the\ndevelopment, deployment, and use of foundation models\nand their downstream applications. Most prominently, they\ndrive the emergence of a new AI ecosystem and increase\nthe speed and accessibility of AI advancements while\nchallenging organizational and regulatory control.\nWith respect to the development of AI the rise of foun-\ndation models is realized by a new technology stack that\nfuels the emergence of an intricate AI ecosystem and a new\ndistribution of inﬂuence over critical products and services.\nRelevant actors include the developers of foundation\nmodels and downstream applications, as well as hosting\nand hardware providers. For developers of foundation\nmodels, access to computational resources to train their\nlarge-scale models is essential, making processing tech-\nnology a critical bottleneck in the development of new\nfoundation models. The advancement of foundation model\nperformance further hinges on large, high-quality training\ndatasets. The rise of large-scale models is therefore\naccompanied by newly emergent markets for data genera-\ntion and annotation, typically characterized by precarious\nlabor conditions (Veselovsky et al. 2023). Due to homog-\nenization, foundation model providers are incentivized to\nwin market share by installing their models as the back-\nbone of user-facing AI applications. OpenAI’s recent\nreduction of model usage prices by 50% exempliﬁes this\nwant to boost deployment. In addition, a shift from product\nto platform business is visible: OpenAI recently announced\nthe launch of a platform, similar to the Apple App Store, to\nprovide access to and the possibility to publish AI appli-\ncations based on the OpenAI foundation models (OpenAI\n2023c). The repositioning of foundation model providers as\nplatform owners will likely accelerate homogenization\nprocesses and strengthen their inﬂuence on downstream AI\napplications. The emerging AI ecosystem also sees the rise\nof open-source models such as the Llama series by Meta\noffering an alternative to proprietary foundation models.\nOpen-source models reduce engineering costs for applica-\ntion developers and allow them more insights into and\ncontrol over model internals, while raising challenges\nsimilar to those for traditional software package reuse\n(Jiang et al. 2023). It remains to be seen how open-source\nalternatives will compete against or ﬁnd integration in AI\nplatforms like the one announced by OpenAI.\nThe reuse of foundation models will likely become a\nmore prevalent paradigm for the development of AI\napplications. AI Developers’ engagement with model\ndesign and training can thus be expected to decrease.\nCompared to classical paradigms of software engineering,\nnew skills and organizational workﬂows are needed to\nrealize AI solutions, including expertise in prompt engi-\nneering, knowledge of model reuse and customization, and\nthe handling of ethical and safety risks in modular AI\napplications. Particularly given the foreshadowed emer-\ngence of platforms providing a multitude of ﬁne-tuned\nmodels, the need to build a custom AI solution from\nscratch, declines. Instead, stacking and connecting modular\nAI applications can be expected to emerge as the leading\napproach for developing AI solutions. The expected trend\ntoward cross-connecting AI applications to develop use-\ncase-speciﬁc solutions requires revisiting existing frame-\nworks for risk assessment and mitigation of (foundation\nmodel) AI developers: forecasting and reducing the risks of\na given AI model typically involves delineating the\nboundaries of its reach and capabilities. However, devel-\nopers of foundation models or downstream applications\nmight not be able to conclusively foresee how their models\nwill be integrated with other models, and how such inte-\ngration could create new risks, including emergent capa-\nbilities, or subvert capability restrictions that were\nintentionally implemented as safety envelopes (Asatiani\net al. 2021). Lastly, AI platforms backed by foundation\nmodels will enable developers to engage as a community\nand to directly market their applications to users, sug-\ngesting the emergence of new business models and blurring\nlines between AI developers and AI users.\nWith respect to the deployment of AI by and within\norganizations, foundation models revolutionize the avail-\nability of deployment-ready, low-cost, and high-perfor-\nmance AI applications. Consequentially, organizations will\nincreasingly shift from custom-built AI systems toward\ndeploying, and possibly customizing, pre-trained AI mod-\nels. Deployment of AI will hence require new skill proﬁles,\nincluding expertise in choosing and integrating the right\nmodels for internal use cases or prompt engineering and\nﬁne-tuning skills for model customization. For the\ndeployment of large-scale foundation models, organiza-\ntions need to consider not only technical issues such as\nperformance and reliability, but also sociotechnical aspects\nincluding necessary changes to existing workﬂows, novel\nprotocols for human-AI collaboration, and cultivating a\nsuitable safety culture.\n123\n226 J. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024)\nAs experimentation and implementation of AI use cases\nbecome easier, faster, and economically less risky,\ndeployment cycles will accelerate. Moreover, bottom-up\nadoption of publicly accessible AI tools might jeopardize\nthe centralized, managerial selection, evaluation, and\norchestration of AI solutions deployed in organizational\nworkﬂows. This decentralization and the black-box nature\nof foundation models intensify existing challenges of\norganizational AI deployment, including system interop-\nerability, explainability of decision outcomes, and the\ngovernance of blurring accountability boundaries (Benbya\net al. 2020; Minkkinen et al. 2023). The property of\nemergent capabilities in foundation models further com-\nplicates questions of accountability and the prevention of\nharmful outcomes, as unwanted, model behavior might\narise unexpectedly (Bender et al. 2021; Bommasani et al.\n2021). Consequentially, continuous monitoring and the\nimplementation of fast-response safety protocols for worst-\ncase scenarios become ever more critical in AI deploy-\nment, as organizational abilities to accurately foresee and\nprevent potential risks decrease. In the context of effective\nrisk mitigation, some consider the homogenization of AI\nmodels and the centralization of computing capacities in\nthe hands of a few commercial providers a regulatory\nopportunity: regulatory frameworks could leverage the\ncentral position of foundation model and infrastructure\nproviders, for example through know-your-customer obli-\ngations, rather than posing a high regulatory burden on\neach deploying organization individually (Mo ¨ kander et al.\n2023).\nLastly, the carbon emissions and energy consumption\nassociated with the training and deployment of large-scale\nfoundation models are gaining attention (Bender et al.\n2021). Organizations hence need to consider ways to\nincorporate respective environmental impact into sustain-\nability strategies and corporate responsibility efforts.\nWith respect to the use of AI , new skills will be required\nto realize the economic potential of using the emerging\nclass of AI. Examples include the skill of effective\nprompting to elicit accurate and relevant model responses.\nOrganizational knowledge is needed on the individual and\norganizational factors inﬂuencing prompting skills (e.g.,\nemployees’ comprehension of NLP or the speciﬁc model\ndeployed; domain-speciﬁc expertise). Rapid fact-checking\nconstitutes another important skill for efﬁcient and\nresponsible use, given the unresolved issue of hallucina-\ntions with AI applications based on large-scale foundation\nmodels (Ji et al. 2023). Besides awareness of the risk of\nhallucinations, users need a possibility, and possibly aux-\niliary tools, to cross-reference information with reliable\nsources. As a consequence, novel usage patterns of AI\nmight emerge, ultimately changing how individuals fulﬁll\ntheir (work) tasks. The use of AI will likely also be affected\nby individuals’ perception of personal risks. The impor-\ntance of minimizing end-user risk to incentivize model\nadaptation is illustrated by OpenAI’s copyright shield\nthrough which the company takes on legal responsibility if\ncustomers should be accused of copyright infringements\nbecause of their use of OpenAI’s models (OpenAI 2023c).\nUsers are faced with a decreasing explainability of AI\nsolutions, and it remains to be seen how this affects their\nwillingness to integrate respective systems for various use\ncases in work and private life. As noted above, as per-\nsonalization and cross-integration of AI applications\nbecome easier, the line between users and developers will\nblur. Users no longer require sophisticated technical\nunderstanding and programming skills to build AI appli-\ncations but can use natural language to customize solutions\nthat best serve their needs.\n3 Opportunities for IS Research\nIn this section, we outline future research directions for IS\nresearch, focusing on opportunities relevant to the BISE\ncommunity as a sociotechnical and construction-oriented\ndiscipline. Some opportunities overlap with those generally\ndescribed for deep learning and generative AI, including\nchallenges of AI explainability (Meske et al. 2022), the\norganizational deployment of (generative) AI (Feuerriegel\net al. 2023), and arising ethical concerns such as fairness\n(Feuerriegel et al. 2020).\nWe structure our overview according to the implications\nof the foundation models paradigm for AI development,\ndeployment, and use as indicated above. We start with I.\ndesign and implementation of AI applications under the\nparadigm of foundation models, II. business models and\nvalue creation in the evolving AI ecosystem, III. AI man-\nagement and governance, and IV. ecological and ethical\ndilemmas of foundation models.\n3.1 Design and Implementation of AI Applications\nUnder the Paradigm of Foundation Models\nRe-using existing foundation models can be expected to\nsubstantially change organizations’ approaches to design-\ning AI products and services. Instead of training large\nmodels themselves, organizations will focus on model ﬁne-\ntuning or prompt engineering to adapt foundation models\nto their speciﬁc use cases. Foundation model providers will\nrelevantly inﬂuence AI application design through their\nusage policies and development guidelines, as illustrated\nby OpenAI (OpenAI 2023b). Future research is required to\nbetter understand the implications of this within and across\norganizations including new development processes,\nexpertise, or resources required to leverage the potential\n123\nJ. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024) 227\nand mitigate the organizational hazards of externally\ndeveloped foundation models. In this context, existing\napproaches to assessing and addressing ethical and safety\nconcerns during design and engineering, including solu-\ntions focused on model explainability (Meske et al. 2022)\nor safety envelopment (Asatiani et al. 2021), require revi-\nsion. The fragmentation of control between foundation\nmodel providers and downstream application developers is\nrecognized in the proposed EU AI Act, which will be\ndecisive for the allocation of regulatory obligations. More\nknowledge is needed regarding the relevant factors orga-\nnizations ought to consider when deciding whether to use a\nfoundation model or a custom model, selecting a speciﬁc\nfoundation model, and deciding how to integrate it for their\nrespective use cases. For the latter, additional knowledge is\nrequired on how to evaluate if model customization is\nnecessary, and whether ﬁne-tuning or prompt engineering\nis more suitable for the context of a particular organization\nand use case. Moreover, research should derive prescriptive\ndesign knowledge regarding the construction of viable\ndownstream AI applications and investigate how organi-\nzations can establish a competitive advantage to position\nthemselves in the evolving AI technology stack. Exemplary\nresearch questions include:\n• How does the paradigm of foundation models challenge\nor reform established structures and processes for AI\nproduct and service development?\n• What design principles can be established regarding the\ndesign of viable AI applications based on proprietary or\nopen-source foundation models, and regarding model\ncustomization through either prompt engineering or\nﬁne-tuning?\n• How does the paradigm of foundation models impact\nthe comparable advantages of different organizational\ntypes (e.g., start-ups, corporates, SMEs) for construct-\ning viable AI applications?\n3.2 Business Models and Value Creation\nin the Evolving AI Ecosystem\nThe paradigm of foundation models implies unprecedented\naccessibility of high-performance AI models, accelerating\nAI development cycles, inviting low-cost experimentation,\nand enabling new applications such as generative AI (Chen\net al. 2023) or life science innovations (Madani et al.\n2023), The development of foundation models and their\nseamless integration into downstream applications neces-\nsitates a complex ecosystem of stakeholders. This spans\nfrom computational and storage hardware suppliers to\nfoundation model developers, hosting providers, and an\narray of service providers, including prompt engineering\nmanagers. Future research should investigate the dynamics\nof this ecosystem, including inter-actor dependencies, dri-\nvers of value creation and accumulation, and the conse-\nquential emergence of novel business model innovations\nincluding platform solutions as foreshadowed by OpenAI\n(2023c). Moreover, a better understanding is needed of\nhow organizations can realize a competitive advantage and\ndifferentiate their AI products and services in a homoge-\nnizing AI landscape. Existing works, e.g., on AI as a ser-\nvice (Lins et al. 2021), might provide a starting point.\nExemplarily research questions include:\n• Which opportunities for business model innovation\nemerge within the AI ecosystem, currently character-\nized by model, data and provider homogenization?\n• What are the drivers of value creation in the evolving\nAI ecosystem? Where can we expect the accrual of\nvalue? Where can we expect the commodiﬁcation of\nproducts and services? If a move from product to\nplatform business model will be realized by foundation\nmodel providers like Open AI, how will this impact\nrespective dynamics?\n• How can organizations realize competitive advantages\nin the emerging ecosystem, including differentiation of\ntheir AI products or services despite homogenization?\n3.3 AI Management and Governance\nThe integration of foundation models within business\nprocesses or AI products and services raises novel ques-\ntions for managing and governing the organizational AI\nlandscape. On a macro level, the allocation of responsi-\nbilities and liabilities among the diverse stakeholders\nremains to be formalized. The regulatory discourse on how\nto foster responsible behavior and ensure ethical, legally\ncompliant AI systems is ongoing, illustrated by negotia-\ntions on the EU AI Act. Some aspects of concerns parallel\nthose of the general public debate on AI, such as what an\nAI model should be allowed to do (autonomously) or how\nto balance freedom of speech and censorship in an attempt\nto realize ethical and legally compliant outputs. For\nexample, should it be allowed for an AI to tell a joke\nrelated to gender or religion? With respect to agency,\nquestions relate to what tasks an AI should be allowed to\nconduct autonomously: driving a car, providing medical\nadvice, opening bank accounts and conducting online\nbusiness?\nBesides identifying the risks arising with pre-trained\nlarge-scale models, considerations involve an assessment\nof which risks can be addressed most effectively by whom,\nand how such mitigation could be executed. This includes\npreventing that model vulnerabilities are passed on from\nfoundation models to downstream applications (Fishman\nand Hancox-Li 2022). Future research ought to investigate\n123\n228 J. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024)\nrespective best practices and derive strategic knowledge on\nhow organizations can maneuver the balance between\ncaution and safety without stiﬂing innovation. Within\norganizations, established structures and processes of AI\nmanagement and governance might require adaptation to\nthe foundational model paradigm. This includes the man-\nagement of bottom-up implementations of easily available\nAI solutions, solutions to privacy and copyright issues, and\nnew governance approaches to assess and constrain the\nrisks arising from a growing reliance on opaque black-box\nmodels or inaccessible training data. Existing data and AI\ngovernance frameworks might be leveraged (e.g., Schnei-\nder et al. 2023). In this context, the relevance of prompt\nengineering and related best practices to facilitate viable\nand safe AI systems require further investigation. More-\nover, new benchmarks ought to be developed for assessing\nmodel performance including approaches to measure and\ncontrol sociotechnical effects of foundation models within\norganizational structures and processes. Exemplarily\nresearch questions include:\n• How can organizational AI management contribute to\nthe realization of the value of foundation models with\nrespect to internal business processes, and product or\nservice development?\n• How does organizational AI governance (have to) adapt\nto the increasing relevance of (opaque) foundation\nmodels in internal business processes, and product or\nservice development?\n• How can the responsibility to mitigate the risks of\nfoundation models, including ethical, legal, and busi-\nness concerns caused by the properties of emergent\nproperties and homogenization, be distributed between\ndifferent actors of the AI technology stack in a way that\nfosters safe and aligned AI applications?\n3.4 Ecological and Ethical Dilemmas of Foundation\nModels\nAs foundation models become increasingly integral to\nvarious industries, ensuring their ethical deployment and\nlong-term viability becomes paramount. Suitable frame-\nworks and effective corporate governance mechanisms are\nrequired to guide decision-making processes related to the\nresponsible development and deployment of foundation\nmodels and downstream applications. Future research\nshould inquire into how organizations deﬁne and uphold\nethical principles in this context, including the resolution of\npotential ethical dilemmas. Respective research can inform\nthe establishment of robust corporate responsibility\nguidelines and policy recommendations to incentivize\nindustry-wide adoption. Future research should also\nexplore the broader economic and social implications of\nfoundation models and investigate opportunities for cor-\nporate governance to inﬂuence respective effects, including\nthe impact on workforce dynamics, market competition, or\neconomic value distribution. Training and deploying\nfoundation models come with signiﬁcant ecological costs\n(OECD 2022). More research is needed to holistically\ncapture the environmental footprint of foundation models\nand their associated infrastructure, including energy con-\nsumption, carbon emission, and resource utilization\nthroughout the AI lifecycle. Avenues for sustainability\nimprovements should be devised, including technical\nsolutions (e.g., optimizing model architecture, reducing\nredundancy in training data, enhancing model compression\ntechniques) and structural solutions (e.g., shared computing\nresources among industry players, federate learning net-\nworks, shared standards for model evaluation). Moreover,\nresearchers could explore effective structures for incen-\ntivizing organizations to prioritize sustainability and stim-\nulating industry-wide engagement in eco-friendly\npractices. Exemplarily research questions include:\n• How can the training and deployment of (fewer)\nfoundation models be realized in such a way that it\ncontributes to improving the sustainability of the AI\nindustry?\n• How can corporate sustainability strategies integrate\nthe environmental costs of recurrent inferences of\nfoundation models integrated into internal business\nprocesses, or the organization’s products and services?\n• How do the key features of foundation models\nchallenge existing (corporate) frameworks and criteria\nfor responsible AI development or deployment? How\ncan these frameworks be adapted to address the\nemerging challenges (e.g., lack of explainability of\nblack-boxed foundation models)?\n4 Conclusion\nThis article has elucidated the emergence of foundation\nmodels as a transformative paradigm for AI. Foundation\nmodels promise unprecedented opportunities to advance\nthe performance and accessibility of AI applications across\nvarious sectors and represent a signiﬁcant shift in how\nnear-future AI systems will be developed, deployed, and\nused. We delineated key features of foundation models\nsuch as emergent capabilities, homogenization, and prompt\nsensitivity. These features redeﬁne the stakeholders and\ndynamics of the AI ecosystem, including a pull toward a\ncentralization of power and the rise of regulatory chal-\nlenges through the diffusion of accountability and control.\nFor organizations, foundation models provide a remarkable\nchance to revolutionize their operations, services, and\n123\nJ. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024) 229\nproducts. Avenues for future research include investigating\nhow foundation models change organizations’ approaches\nto designing AI applications, business models and value\ncreation dynamics in the evolving AI ecosystem, hurdles\nand remedies concerning AI management and governance,\nand the formulation of organizational strategy and practices\nto harness the potential of foundation models responsibly\nand sustainably.\nWhile we aimed to address a diverse array of aspects\nrelevant to the BISE community in the context of foun-\ndation models, we recognize the limitation that there might\nbe nuanced issues (e.g., potential differences in foundation\nmodels pre-trained on language vs. images or other\nmodalities; prompt engineering; impact of model scaling\non performance and key features) that were not exhaus-\ntively examined given the complexity of the topic. More-\nover, due to the rapid speed of current technological\nadvancements, we acknowledge a temporal constraint in\nthis article’s insights. However, we propose that an\nunderstanding of the fundamental characteristics of the\nfoundation model paradigm, as elucidated within this\narticle, establishes a critical groundwork for forthcoming\ndiscussions. Ultimately, the outlined avenues for future\nresearch offer an agenda for the BISE community that sets\nthe stage for impactful contributions toward the viable,\nresponsible, and sustainable realization of AI systems in\nthe age of foundation models.\nFunding Open access funding provided by University of\nLiechtenstein.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\nAsatiani A, Malo P, Nagbøl PR, Penttinen E, Rinta-Kahila T,\nSalovaara A (2021) Sociotechnical envelopment of artiﬁcial\nintelligence: an approach to organizational deployment of\ninscrutable artiﬁcial intelligence systems. J Assoc Inf Syst\n22(2):325–252\nBenbya H, Davenport TH, Pachidi S (2020) Artiﬁcial intelligence in\norganizations: current state and future opportunities. MIS Q\nExec 19(4)\nBender EM, Gebru T, McMillan-Major A, Shmitchell S (2021) On\nthe dangers of stochastic parrots: can language models be too\nbig? In: Proceedings of the 2021 ACM conference on fairness,\naccountability, and transparency, pp 610–623\nBommasani R, Creel KA, Kumar A, Jurafsky D, Liang PS (2022)\nPicking on the same person: does algorithmic monoculture lead\nto outcome homogenization? Adv Neur Inf Proc Syst\n35:3663–3678\nBommasani R, Hudson DA, Adeli E et al (2021) On the opportunities\nand risks of foundation models. arXiv preprint arXiv:2108.07258\nBrown T, Mann B, Ryder N et al (2020) Language models are few-\nshot learners. Proc Adv Neural Inf Process Syst 33:1877–1901\nChen L, Sun L, Han J (2023) A comparison study of human and\nmachine generated creativity. J Comput Inf Sci Eng\n23(5):051012\nChowdhery A, Narang S, Devlin J et al (2023) Palm: Scaling\nlanguage modeling with pathways. J Mach Learn Res\n24(240):1–113\nDeng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet:\na large-scale hierarchical image database. In: IEEE conference\non computer vision and pattern recognition, pp 248–255\nDwivedi YK, Kshetri N, Hughes L et al (2023) ‘ ‘So what if chatgpt\nwrote it?’’ Multidisciplinary perspectives on opportunities,\nchallenges and implications of generative conversational AI for\nresearch, practice and policy. Int J Inf Manag 71:102642\nFeuerriegel S, Dolata M, Schwabe G (2020) Fair AI. Bus Inf Syst Eng\n62(4):379–384\nFeuerriegel S, Hartmann J, Janiesch C, Zschech P (2023) Generative\nAI. Bus Inf Syst Eng. https://doi.org/10.2139/ssrn.4443189\nFishman N, Hancox-Li L (2022) Should attention be all we need? The\nepistemic and ethical implications of uniﬁcation in machine\nlearning. In: Proceedings of the ACM conference on fairness,\naccountability, and transparency, pp 1516–1527\nHacker P, Engel A, Mauer M (2023) Regulating ChatGPT and other\nlarge generative AI models. In: Proceedings of conference on\nfairness, accountability, and transparency, pp 1112–1123\nJaniesch C, Zschech P, Heinrich K (2021) Machine learning and deep\nlearning. Electron Mark 31(3):685–695\nJi Z, Lee N, Frieske R et al (2023) Survey of hallucination in natural\nlanguage generation. ACM Comput Surv 55(12):1–38\nJiang W, Synovic N, Hyatt M et al (2023) An empirical study of pre-\ntrained model reuse in the hugging face deep learning model\nregistry. arXiv preprint arXiv:2303.02552\nKleinberg J, Raghavan M (2021) Algorithmic monoculture and social\nwelfare. Proc Natl Acad Sci 118(22):e2018340118\nKojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y (2022) Large\nlanguage models are zero-shot reasoners. Adv Neural Inf Proc\nSyst 35:22199–22213\nKoniakou V (2023) From the ‘‘rush to ethics’’ to the ‘ ‘race for\ngovernance’’ in artiﬁcial intelligence. Inf Syst Front\n25(1):71–102\nLi X, Tian Y, Ye P, Duan H, Wang FY (2022) A novel scenarios\nengineering methodology for foundation models in metaverse.\nIEEE Trans Syst Man Cybern Syst 53(4):2148–2159\nLins S, Pandl KD, Teigeler H, Thiebes S, Bayer C, Sunyaev A (2021)\nArtiﬁcial intelligence as a service. Bus Inf Syst Eng\n63(4):441–456\nLiu V, Chilton LB (2022) Design guidelines for prompt engineering\ntext-to-image generative models. In: Proceedings of the CHI\nconference on human factors in computing systems. https://doi.\norg/10.1145/3491102.3501825\nLiu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G (2023) Pre-train,\nprompt, and predict: a systematic survey of prompting methods\nin natural language processing. ACM Comput Surv 55(9):1–35\n123\n230 J. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024)\nMadani A, Krause B, Greene ER et al (2023) Large language models\ngenerate functional protein sequences across diverse families.\nNat Biotechnol 41:1099–1106\nMeske C, Bunde E, Schneider J, Gersch M (2022) Explainable\nartiﬁcial intelligence: objectives, stakeholders, and future\nresearch opportunities. Inf Syst Manag 39(1):53–63\nMin S, Lyu X, Holtzman A, Artetxe M, Lewis M, Hajishirzi H,\nZettlemoyer L (2022) Rethinking the role of demonstrations:\nWhat makes in-context learning work? In: Proceedings of the\nconference on empirical methods in natural language processing,\npp 11048–11064\nMinkkinen M, Zimmer MP, Ma ¨ ntyma¨ ki M (2023) Co-shaping an\necosystem for responsible AI: ﬁve types of expectation work in\nresponse to a technological frame. Inf Syst Front 25(1):103–121\nMo¨ kander J, Schuett J, Kirk HR, Floridi L (2023) Auditing large\nlanguage models: a three-layered approach. AI and Ethics.\nhttps://doi.org/10.1007/s43681-023-00289-2\nNiu S, Liu Y, Wang J, Song H (2020) A decade survey of transfer\nlearning (2010–2020). IEEE Trans Artif Intell 1(2):151–166\nOECD (2022) Measuring the environmental impacts of artiﬁcial\nintelligence compute and applications: the AI footprint. OECD\nDigital Economy Papers, No. 341, OECD Publishing, Paris.\nhttps://doi.org/10.1787/7babf571-en\nOpenAI (2023a) GPT-4 technical report. https://arxiv.org/abs/2303.\n08774\nOpenAI (2023b) Usage policies. https://openai.com/policies/usage-\npolicies. Accessed 28 Oct 2023\nOpenAI (2023c) Introducing GPTs. https://openai.com/blog/introdu\ncing-gpts. Accessed 8 Nov 2023\nOuyang L, Wu J, Jiang X et al (2022) Training language models to\nfollow instructions with human feedback. Adv Neural Inf Proc\nSyst 35:27730–27744\nRamesh A, Pavlov M, Goh G et al (2021) Zero-shot text-to-image\ngeneration. In: International conference on machine learning,\npp 8821–8831\nReed S, Zolna K, Parisotto E et al (2022) A generalist agent. arXiv\npreprint arXiv:2205.06175\nReynolds L, McDonell K (2021) Prompt programming for large\nlanguage models: beyond the few-shot paradigm. In: Extended\nabstracts of the CHI conference on human factors in computing\nsystems, pp 1–7\nSarker S, Chatterjee S, Xiao X, Elbanna A (2019) The sociotechnical\naxis of cohesion for the is discipline: its historical legacy and its\ncontinued relevance. MIS Q 43(3):695–720\nSchneider J, Abraham R, Meske C, Vom Brocke J (2023) Artiﬁcial\nintelligence governance for businesses. Inf Syst Manag\n40(3):229–249\nSobania D, Briesch M, Hanna C, Petke J (2023) An analysis of the\nautomatic bug ﬁxing performance of ChatGPT. arXiv preprint\narXiv:2301.08653\nTeubner T, Flath CM, Weinhardt C, van der Aalst W, Hinz O (2023)\nWelcome to the era of ChatGPT et al. The prospects of large\nlanguage models. Bus Inf Syst Eng 65(2):95–101\nVaswani A, Shazeer N, Parmar N et al (2017) Attention is all you\nneed. Proc Adv Neural Inf Process Syst 30:5999–6009\nVeselovsky V, Ribeiro MH, West R (2023) Artiﬁcial artiﬁcial\nartiﬁcial intelligence: crowd workers widely use large language\nmodels for text production tasks. arXiv preprint arXiv:2306.\n07899\nWebb T, Holyoak KJ, Lu H (2023) Emergent analogical reasoning in\nlarge language models. Nat Hum Behav 7:1526–1541\nWei J, Tay Y, Bommasani R et al (2022) Emergent abilities of large\nlanguage models. Trans Mach Learn Res. https://doi.org/10.\n48550/arXiv.2206.07682\nWei A, Haghtalab N, Steinhardt J (2023) Jailbroken: how does LLM\nsafety training fail? arXiv preprint arXiv:2307.02483\nWhite J, Fu Q, Hays S et al (2023) A prompt pattern catalog to\nenhance prompt engineering with ChatGPT. arXiv preprint\narXiv:2302.11382\nZiems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D (2023) Can\nlarge language models transform computational social science?\narXiv preprint arXiv:2305.03514\nZou A, Wang Z, Kolter JZ, Fredrikson M (2023) Universal and\ntransferable adversarial attacks on aligned language models.\narXiv preprint arXiv:2307.15043\n123\nJ. Schneider et al.: Foundation Models, Bus Inf Syst Eng 66(2):221–231 (2024) 231",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7041293382644653
    },
    {
      "name": "Engineering",
      "score": 0.43840643763542175
    },
    {
      "name": "Computer science",
      "score": 0.35809195041656494
    },
    {
      "name": "Library science",
      "score": 0.3216002285480499
    },
    {
      "name": "Political science",
      "score": 0.20710885524749756
    },
    {
      "name": "Law",
      "score": 0.06337714195251465
    }
  ]
}