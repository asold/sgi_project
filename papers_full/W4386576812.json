{
  "title": "The NLP Task Effectiveness of Long-Range Transformers",
  "url": "https://openalex.org/W4386576812",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2468518544",
      "name": "Guanghui Qin",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2156934580",
      "name": "Yukun Feng",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2113965177",
      "name": "Benjamin Van Durme",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W4353089798",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W1965693266",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3175081470",
    "https://openalex.org/W3155888652",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4287888039",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4309087688",
    "https://openalex.org/W3166148088",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3105478763",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W3173529047",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W3103682594"
  ],
  "abstract": "Transformer models cannot easily scale to long sequences due to their O(Nˆ2) time and space complexity. This has led to Transformer variants seeking to lower computational complexity, such as Longformer and Performer. While such models have theoretically greater efficiency, their effectiveness on real NLP tasks has not been well studied. We benchmark 7 variants of Transformer models on 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the effect of pretraining and hyperparameter settings, to focus on their capacity for long-range attention. Moreover, we present various methods to investigate attention behaviors to illuminate model details beyond metric scores. We find that the modified attention in long-range transformers has advantages on content selection and query-guided decoding, but they come with previously unrecognized drawbacks such as insufficient attention to distant tokens and accumulated approximation error.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3774–3790\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nThe NLP Task Effectiveness of Long-Range Transformers\nGuanghui Qin Yukun Feng Benjamin Van Durme\nDepartment of Computer Science, Johns Hopkins University\n{gqin2, yfeng55, vandurme}@jhu.edu\nAbstract\nTransformer models cannot easily scale to long\nsequences due to their O(N2) time and space\ncomplexity. This has led to Transformer vari-\nants seeking to lower computational complex-\nity, such as Longformer and Performer. While\nsuch models have theoretically greater effi-\nciency, their effectiveness on real NLP tasks\nhas not been well studied. We benchmark 7\nvariants of Transformer models on 5 difficult\nNLP tasks and 7 datasets. We design experi-\nments to isolate the effect of pretraining and hy-\nperparameter settings, to focus on their capacity\nfor long-range attention. Moreover, we present\nvarious methods to investigate attention behav-\niors to illuminate model details beyond metric\nscores. We find that the modified attention in\nlong-range transformers has advantages on con-\ntent selection and query-guided decoding, but\nthey come with previously unrecognized draw-\nbacks such as insufficient attention to distant\ntokens and accumulated approximation error.\n1 Introduction\nTransformer-based models (Vaswani et al., 2017)\nhave advanced the state of the art in natural lan-\nguage processing. However, their quadratic time\nand space complexity hinder their application on\nlong texts. Various proposals have been made to ad-\ndress these concerns (Tay et al., 2020c), with math-\nematical guarantees on improved time or space.\nThese models have been evaluated primarily via\nperplexity (Dai et al., 2019) and non-NLP bench-\nmarks (Tay et al., 2020b). These metrics may not\nbe ideal (Sun et al., 2021) and may not reflect per-\nformance on complex NLP tasks (Arutiunian et al.,\n2020; Thorne et al., 2021). We argue these met-\nrics have not been sufficient for the development\nof efficient Transformers and their practical appli-\ncation on long texts, and that existing benchmarks\nare insufficient guides for architecture selection.\nIt is not straightforward to have a fair and side-\nby-side comparison among those models due to the\ndifferences between their pretraining and hyperpa-\nrameter settings (Tay et al., 2020c), and the metrics\nalone cannot convey detailed information about the\nself-attention blocks (Sun et al., 2021). We wish\nto fairly validate the effectiveness of long-range\nattention techniques, and to uncover the underlying\nfactors behind model behaviors. We critique the re-\nliance on perplexity evaluations in previous work,\nexperimenting with five difficult, long-text NLP\ntasks. These tasks cover typical NLP modeling sce-\nnarios: token or span-level prediction, sequence-\nlevel classification, and sequence-to-sequence gen-\neration. To our knowledge, this is the first work to\nevaluate long-range transformers on such a wide\nspectrum of representative NLP tasks.\nTo verify the key features of long-range trans-\nformers, we ablate distant attention to measure\nwhat they gain from long-range mechanisms. For\nmodels without pretrained checkpoints, we migrate\nparameters from their prototype models for fairness.\nWe cover 3 main kinds of long-range transformers,\nincluding pattern-, recurrence-, and kernel-based\nmethods. To our knowledge, we are the first to\nadopt all these methods to probe transformers. To\ninvestigate the relationship between performance\nand document lengths we break down the metric\nwith a customized algorithm (Bagga and Baldwin,\n1998). Also, we use entropy and attribution anal-\nysis (Li et al., 2017) to test the effectiveness of\ncached memories in recurrent transformers and the\nglobal tokens for query-based problems.\nWe find that long-range context brings perfor-\nmance gains to transformers in some cases, which\nwe attribute to more selective attention, especially\nfor query-based tasks like QA. Surprisingly we\nobserve that some long-range models do not ef-\nfectively utilize distant information, and the accu-\nmulated error of approximation is unacceptable.\nWe hope this analysis helps practitioners better un-\nderstand the current state of the art of long-range\nattentions and suggests paths for future research.\n3774\n  Pre-speciﬁed sparsity patterns\nSparsify the attention matrix (QK^T)\nSliding window Dilated Global \n Blocked \nSparse Transformer\nLongformer\nLongformer Longformer\nETC\nBig Bird\nGMAT\nBlock-wise\nETC\nBig Bird\nSinkhorn\nBig Bird\nRandom\nA variety of patterns has been explored in the past work\nFigure 1: Illustration of 5 patterns used by long-range transformers, from Beltagy et al. (2021) with permission.\n2 Background\n2.1 Long-Range Transformers\nResearchers have proposed a number of Trans-\nformer variants (Tay et al., 2020c). Most of these\nmodels support decoding (causal masking) (Peng\net al., 2021a), while only a few of them have pre-\ntrained checkpoints (Beltagy et al., 2020, inter alia).\nWe cluster these approaches into 3 main categories.\nSparsified Patterns Pattern-based methods try\nto make self-attention sparse. Some apply pre-\nspecified attention patterns. Specifically, Long-\nformer (Beltagy et al., 2020) applies 3 patterns:\nSliding window requires that each token can only at-\ntend to the tokens in a local window,dilated pattern\nlets each token only attend at fixed intervals, while\nthe global pattern requires a few tokens as globally\nattended and lets them to attend all tokens in the\nsequence. In addition to the global pattern, Big-\nBird (Zaheer et al., 2020) applies a blocked pattern,\nwhich splits the sequence into fixed-length blocks,\nand random patterns, by which tokens can attend to\nany other tokens randomly. An illustration is shown\nin fig. 1. Although the attention of each layer is not\nfull, the receptive field can be increased as multi-\nple layers are stacked. The selected or appended\n“global” tokens can be task-specific (Beltagy et al.,\n2020), allowing for direct distant information ex-\nchange. Instead of pre-defined attention patterns,\nsome use content-based patterns so they become\nlearnable, with techniques including locality sensi-\ntive hashing (Kitaev et al., 2020), the differentiable\nSinkhorn algorithm (Tay et al., 2020a), or the learn-\nable routing algorithm (Roy et al., 2020).\nRecurrence & Compressed Memory These\nmethods use segment-level recurrence to reuse\nthe cached hidden states of previous steps.\nTransformer-XL (Dai et al., 2019) and XL-\nNet (Yang et al., 2019) connects different chunks\nwith cross-attention, where the tokens in a block\nattend to the hidden states of the previous blocks\nin addition to their self-attention. Note that the\nx1 x2 x4x3 x8x5 x6 x7\nNew Segment\nx12x9 x10 x11\nFixed (No Grad)\nx1 x2 x4x3 x8x5 x6 x7\nFixed (No Grad) New Segment\n(a) Training phase.\nx1 x2 x4x3 x8x5 x6 x7 x12x9 x10 x11\nExtended Context (b) Evaluation phase.\nFigure 2: Illustration of the Transformer-XL model with a segment length 4.\nper-segment, which differs from the same-layer\nrecurrence in conventional RNN-LMs. Conse-\nquently, the largest possible dependency length\ngrows linearly w.r.t. the number of layers as well\nas the segment length, i.e., O(N ×L), as vi-\nsualized by the shaded area in Fig. 2b. This\nis analogous to truncated BPTT (Mikolov et al.,\n2010), a technique developed for training RNN-\nLMs. However, different from truncated BPTT,\nour method caches a sequence of hidden states in-\nstead of the last one, and should be applied to-\ngether with the relative positional encoding tech-\nnique described in Section 3.3.\nBesides achieving extra long context and re-\nsolving fragmentation, another beneﬁt that comes\nwith the recurrence scheme is signiﬁcantly faster\nevaluation. Speciﬁcally, during evaluation, the\nrepresentations from the previous segments can\nbe reused instead of being computed from scratch\nas in the case of the vanilla model. In our ex-\nperiments on enwiki8, Transformer-XL is up to\n1,800+ times faster than the vanilla model during\nevaluation (see Section 4).\nFinally, notice that the recurrence scheme does\nnot need to be restricted to only the previous seg-\nment. In theory, we can cache as many previous\nsegments as the GPU memory allows, and reuse\nall of them as the extra context when processing\nthe current segment. Thus, we can cache a prede-\nﬁned length- M old hidden states spanning (pos-\nsibly) multiple segments, and refer to them as the\nmemory mn\nτ ∈RM×d, due to a clear connection to\nthe memory augmented neural networks (Graves\net al., 2014; Weston et al., 2014). In our experi-\nments, we set M equal to the segment length dur-\ning training, and increase it by multiple times dur-\ning evaluation.\n3.3 Relative Positional Encodings\nWhile we found the idea presented in the pre-\nvious subsection very appealing, there is a cru-\ncial technical challenge we haven’t solved in or-\nder to reuse the hidden states. That is, how can\nwe keep the positional information coherent when\nwe reuse the states? Recall that, in the standard\nTransformer, the information of sequence order is\nprovided by a set of positional encodings, denoted\nas U ∈ RLmax×d, where the i-th row Ui corre-\nsponds to the i-th absolute position within a seg-\nment and Lmax prescribes the maximum possible\nlength to be modeled. Then, the actual input to the\nTransformer is the element-wise addition of the\nword embeddings and the positional encodings. If\nwe simply adapt this positional encoding to our\nrecurrence mechanism, the hidden state sequence\nwould be computed schematically by\nhτ+1 = f(hτ, Esτ+1 + U1:L)\nhτ = f(hτ−1, Esτ + U1:L),\nwhere Esτ ∈ RL×d is the word embedding se-\nquence of sτ, and f represents a transformation\nfunction. Notice that, both Esτ and Esτ+1 are as-\nsociated with the same positional encoding U1:L.\nAs a result, the model has no information to dis-\ntinguish the positional difference betweenxτ,j and\nxτ+1,j for any j = 1,...,L , resulting in a sheer\nperformance loss.\nIn order to avoid this failure mode, the funda-\nmental idea is to only encode the relative posi-\ntional information in the hidden states. Concep-\ntually, the positional encoding gives the model a\ntemporal clue or “bias” about how information\nshould be gathered, i.e., where to attend. For the\nsame purpose, instead of incorporating bias stati-\ncally into the initial embedding, one can inject the\nsame information into the attention score of each\nlayer. More importantly, it is more intuitive and\ngeneralizable to deﬁne the temporal bias in a rela-\ntive manner. For instance, when a query vectorqτ,i\nattends on the key vectors kτ,≤i, it does not need\nto know the absolute position of each key vector\nto identify the temporal order of the segment. In-\nstead, it sufﬁces to know the relative distance be-\ntween each key vectorkτ,j and itself qτ,i, i.e. i−j.\nPractically, one can create a set of relative posi-\nFigure 2: Recurrent transformers. “No Grad” means\nthat the gradients do not back-propagate to this block.\nObtained from Dai et al. (2019) with permission.\ngradients remain in the same segment and are not\npropagated to previous segments (fig. 2). To re-\nduce the number of history hidden states, Rae et al.\n(2020) compress them as memories for efficient\nre-use with pooling or convolutions. From a dif-\nferent perspective, Izacard and Grave (2021) use\nretrieval-based methods to collect evidences from\nexternal knowledge, resulting in a more targeted\ncontext information.\nLow-Rank & Kernels These methods approxi-\nmate the self-attention with low-rank approxima-\ntion (Wang et al., 2020) or kernelization with-\nout explictly computing the matrix production.\nAmong them, Choromanski et al. (2021); Peng et al.\n(2021b) use random features. Katharopoulos et al.\n(2020) reduce the time complexity to linear and\nspace complexity to constant by replacing softmax\nwith linear kernel features.\n2.2 Benchmarks and Analysis\nThere is not an agreed-upon standard benchmark\nfor long-range transformers. Researchers have con-\nsidered various tasks and domains, including lan-\nguage (Dai et al., 2019), protein sequences (Choro-\nmanski et al., 2021), and images (Katharopoulos\net al., 2020). Few conduct experiments on long\nsequence NLP tasks, including question answer-\ning (Beltagy et al., 2020) and summarization (Za-\nheer et al., 2020). Tay et al. (2020b) propose Long\nRange Arena comprised of six non-NLP tasks to\n3775\nexclude the factor of pretraining. Concurrent to our\nefforts, Shaham et al. (2022) propose a suite of text-\nto-text tasks as a long sequence NLP benchmark.\nResearchers are also interested in the utility of\ncontext in transformers. Rae and Razavi (2020)\nfind that Transformer-XL does not necessarily need\nlong and deep contexts. Sun et al. (2021) reveal\nthat Longformer and Routing transformers can only\nreduce the perplexity of LMs on a small set of\ntokens. More related to our work, Lai et al. (2020)\nshow that BERT can make use of a larger scope of\ncontext than a BiLSTM.\n3 Setup\n3.1 Settings\nIt is non-trivial to compare distinct transformer\nmodels, due to differences between their pretrain-\ning and hyper-parameter settings. Our goal is to\nminimize these confounding factors to allow a fo-\ncus on the long range attention ability of each\nmodel on different tasks. We therefore propose\ntwo sets of experimental conditions.\nRestricted Attention Range To evaluate the per-\nformance gain from long-range attention, we eval-\nuate models in both their default context-aware\nsettings and a context-agnostic setting that restricts\nthe receptive range of the self-attention blocks. For\npattern-based transformers, we achieve the restric-\ntion by segmenting the input sequence into chunks\nand running the transformers on segments inde-\npendently. For recurrent models, we ablate the\nrecurrence to eliminate the dependencies between\nsegments. In practice, we segment the texts into\nchunks with length L,1 which ranges from 128 to\n1536. L=∞indicates no segmentation is used.\nParameter Migration Kernel-based models usu-\nally do not come with checkpoints for general tasks,\nbut they may have similar structures to other pre-\ntrained models like BERT and may be designed\nto approximate the original results. Therefore, it\nis feasible to migrate parameters from pretrained\nprototypes to their “efficient version” to observe\nif the performance could be preserved. This type\nof method can be suitable for models without addi-\ntional parameters, such as Performer.\n3.2 Transformers and Tasks\nTransformers We consider three approaches: 1)\npattern-based: Longformer (Beltagy et al., 2020),\n1We use wordpieces instead of words in this paper.\nDataset Task #tokens #docs\nOntonotes Coref. 467 3493\nTriviaQA eQA 2895 95k\nDocNLI NLI 399 1.44m\nSummFD Summ. 5.6k 4.3k\nGovRep Summ. 7.9k 19k\nQasper aQA 3.7k 5.7k\nQuALITY aQA 4.2k 6.7k\nTable 1: Task and dataset overview. The #tokens is the\nnumber of tokens per doc on average.\nand BigBird (Zaheer et al., 2020); 2) recurrent:\nXLNet (Yang et al., 2019); and 3) kernel-based:\nPerformer (Choromanski et al., 2021). We also\ninclude the results of RoBERTa (Liu et al., 2019)\nand SpanBERT (Joshi et al., 2020), where some of\nour approaches are initialized from those two non-\nlong-range models. Due to memory requirements,\nwe use the base version for all models.2 You may\nrefer to appendix B for more details.\nTasks We cover five tasks of three types, includ-\ning 1) span-level predictions: coreference resolu-\ntion (Coref.) (Weischedel et al., 2011) and extrac-\ntive question answering (eQA) (Joshi et al., 2017);\n2) sequence classification: natural language infer-\nence (NLI) (Yin et al., 2021); 3 and 3) seq2seq:\nsummarization (Summ.) (Chen et al., 2021; Huang\net al., 2021), abstractive QA (aQA) (Dasigi et al.,\n2021; Pang et al., 2022). We pick seven datasets\nthat involve long texts, whose statistics are shown\nin table 1. For more details about the data prepro-\ncessing, please refer to appendix A.4\n4 Experiments\n4.1 Coreference Resolution\nCoreference resolution (coref.) is the task of\nidentifying mention spans and clustering them\ninto entities. We consider multiple coreference\nstrategies: 1) the widely used Coarse2Fine (C2F)\nmethod5 (Lee et al., 2018) which relies on span\nrepresentations and 2) the current state-of-the-art\nmethod called Start2End (S2E) (Kirstain et al.,\n2021) that works on token representations. The\n2We used the codebase of Katharopoulos et al. (2020) for\nPerformer and Huggingface (Wolf et al., 2020) for the rest.\n3DocNLI is modified from ANLI (Nie et al., 2020),\nSQuAD (Rajpurkar et al., 2016), DUC2001, DailyMail (Nal-\nlapati et al., 2016), and Curation (Curation, 2020)\n4Our codebase is available onhttps://github.com/\nhiaoxui/long-range-transformers.\n5We used the re-implementation by Gardner et al. (2018).\n3776\nEncoder L=128 L=256 L=512\nModel: Coarse2Fine\nLongformer 75.74 76.72 77.36\nLongformerG 75.68 76.25 77.23\nBigBird 75.95 76.78 77.64\nXLNet 74.57 74.48 74.33\nXLNetm 74.73 75.76 76.29\nRoBERTa 74.64 76.45 76.83\nRoBERTap 51.58 51.71 50.39\nSpanBERT 75.04 75.84 76.59\nSpanBERTp 52.46 52.06 50.51\nLongformer 76.77 (1024) 76.32 ( ∞)\nBigBird 77.31 (1024) 77.57 ( ∞)\nModel: Start2End\nLongformer 74.77 76.27 77.73\nLongformerG 74.15 76.19 77.41\nBigBird 73.68 75.57 77.40\nXLNet 45.89 60.05 68.23\nXLNetm 52.61 56.37 66.91\nRoBERTa 71.96 76.27 77.78\nRoBERTap 40.06 42.35 41.69\nSpanBERT 68.70 74.27 75.32\nSpanBERTp 38.69 41.93 42.10\nLongformer 77.54 (1024) 77.57 ( ∞)\nBigBird 77.43 (1024) 77.66 ( ∞)\nTable 2: Coref. results with the C2F and the S2E models.\nNumbers are averaged F1 (MUC, B 3, and CEAF ϕ4 ).\nLongformer with G uses global tokens. XLNet with m\nuses recurrence memory. Encoders with p have their\nself-attention replaced with a Performer kernel.\ndataset we use is Ontonotes 5.0. Transformers\nconsidered include Longformer, XLNet, and Per-\nformer. We migrate the parameters of SpanBERT\nand RoBERTato Performer and include results on\nthese models as well. We segment the input tokens\ninto chunks with lengths of L. For models with\nglobal tokens, we lack a natural choice so we con-\nsider all tokens to be global. For XLNet, we keep\nthe memory of the same length of the segments (e.g.\nwe keep a memory length of 256 for a model with\nsegment length 256). 6 The results are shown in\ntable 2. Refer to appendix F.1 for complete results.\nSome observations are consistent across two\ncoref. models. 1) Though further pretrained upon\nRoBERTa, pattern-based methods do not show im-\nprovement over RoBERTa, even with longer at-\ntention range. 2) Models gain advantage when\nthe segments get longer, but it is saturated when\nthe segment length reaches 512. Distant contexts\n6We adopt the same strategies with segmentation and mem-\nories in the remainder of the paper.\n1 8 16 32 64 128 256 512\nMention Distances\n40\n50\n60\n70B3 F1\nLongformer (L=128)\nLongformer (L=512)\nLongformer (L=4096)\nXLNetm (L=512)\nFigure 3: B3 breakdown scores of 4 models for mention\npairs with ranges from [1,8) to [512,∞).\nmight not be exploited. 3) Performer-based models\nunder-perform their corresponding non-kernelized\nmodels by a huge gap. 4) XLNet performs better\nwith cached memory, but the performance gain is\nless observable when for shorter segments. 7\nTo further show the performance of those models\non documents with different lengths, we conduct\nmetric breakdown with a few typical configurations.\nInstead of simply clustering the document accord-\ning to the lengths, we propose a breakdown-version\nB3 metric. Given a mention distance range[L1,L2],\nwe calculate its corresponding B 3 value by only\nconsidering the mention pairs whose distances fall\ninto this range. The breakdown metrics are shown\nin fig. 3. We can see that the performance of all\nmodels follows the same trend and peaks at the\n[16,32) bucket. Also, the graph shows that Long-\nformer with longer context encoding does NOT\nbenefit on distant mentions (pvalue < 0.01). 8 On\nthe contrary, they suffer more from distance than\nshorter-context models, showing that long-range at-\ntention fails to capture long-distance information.\n4.2 Natural Language Inference\nNLI is a classification task concerning a premise\nand hypothesis with variable lengths. The DocNLI\ndataset uses document-length inputs with binary\nlabels (entailment and not entailment).\nWe adopt the model proposed by Yin et al. (2021),\nand consider Longformer, BigBird, Performer,\nand XLNet. We make the prediction on theCLS to-\nken, which is at the beginning for Longformer and\nthe end for XLNet. Since we are only interested\nin the encoding of CLS, we adopt the strategy of\nYin et al. (2021) which truncates the sequence to\nLwhile preserving the hypothesis for Longformer\nand RoBERTa. The results are shown in table 3.\n7The observations on XLNet and Performer are consistent\nacross all the tasks in this paper.\n8We conduct significance test for the comparison between\ncurves. Please refer to appendix D for more details.\n3777\nEncoder L=128 L=256 L=512\nXLNet 29.95 40.39 24.31\nXLNetm 32.94 45.97 30.42\nRoBERTa 48.96 47.78 46.04\nRoBERTap 17.83 24.91 23.65\nLongformer 29.11 25.73 45.28\nBigBird 28.95 24.71 31.72\nLongformer 45.96 (1024) 44.42 ( ∞)\nBigBird 33.58 (1024) 18.08 ( ∞)\nTable 3: F1 scores on the test set for DocNLI.\n1 64 128 256 512 1024\nDocument Length\n30\n50\n70F1 Score\nRoBERTa (L=128)\nLongformer (L=1024)\nXLNetm (L=256)\nFigure 4: The breakdown analysis of DocNLI. We pick\nthe best configuration for each model for brevity.\nObserving table 3, surprisingly, the best perfor-\nmance is achieved with short segments, though the\noptimal lengths vary from model to model. Also,\nthe performance of Longformer and BigBird is\nmuch lower than their baseline RoBERTa (c.f. Yin\net al. (2021)). Observing the breakdown analysis\nin fig. 4, the performance of 3 best models follow\nthe same trend w.r.t. the document length. 9 We\nspeculate that all models are unable to comprehend\nthe relationship between long documents, and long-\nrange attention does not bring any advantage.\n4.3 Question Answering\nFor question answering we experiment with eQA\n(TriviaQA) and aQA ( Qasper and QuALITY).\nPerformer, Longformer, BigBird, and XLNet\nare tested for encoder-only tasks; BART and\nLongformer-Encoder-Decoder (LED) is used for\nseq2seq tasks. For LongformerG and BigBird, we\nset the question text (and candidate answers for\nQuALITY) as global tokens.10 TriviaQA is a ques-\n9The graph looks less smooth than fig. 3, possibly because\nDocNLI is made up of examples pulled from different datasets,\nwhich may have examples of different average lengths (cf. tab\n1 in Yin et al. (2021)). Therefore the length of an example in\nDocNLI may correlate with different domains making up the\ndataset, which would interfere with our analysis. Future work\nwill consider other datasets without this confounding concern.\n10The global tokens of BigBird are fixed in the first 2 blocks,\nso we place the query at the beginning of the sequence.\nEncoder L=128 L=256 L=512\nLongformer 54.26 58.83 63.88\nRoBERTa 55.81 60.29 63.45\nRoBERTap 23.17 21.87 21.11\nBigBird 55.28 59.39 63.51\nXLNet 51.46 56.26 60.05\nXLNetm 52.71 57.96 62.85\nLongformer 63.91 (1024) 63.66 ( ∞)\nLongformerG - (1024) 72.96 ( ∞)\nBigBird 66.50 (1024) 71.78 ( ∞)\nTable 4: Results for TriviaQA. LongformerG indicates\nthat the Longformer sets question as the global tokens.\nChunk 512 1024 1536 ∞\nQasper\nBART 24.70 26.30 - -\nLED 8.40 15.80 17.86 18.79\nLEDG - - - 28.64\nQLTY\nBART 26.80 26.00 - -\nLED 30.73 31.35 31.78 31.21\nLEDG - - - 29.87\nTable 5: Performance Qasper and QuALITY . Qasper is\nevaluated with F1 and QuALITY with accuracy. The\nresults on BART are from Shaham et al. (2022).\ntion answering dataset that involves extracting an-\nswer spans from reference documents. We adopt\nthe method and codebase of Joshi et al. (2017). F1\nis used as evaluation metrics. Qasper addresses\nthe QA task in the domain of academic papers and\ninvolves various answer types: extractive, abstrac-\ntive, boolean, and unanswerable. We unify such\ntasks as an abstractive QA task (cf. Shaham et al.\n(2022)) and implement an LED-based decoder to\ngenerate answers. F1 score is used for the evalu-\nation of Qasper. QuALITY is a multiple-choice\nQA task. Given a question and a passage, the task\nis to select the correct answer from several candi-\ndates. We regard it as a seq2seq problem, with the\nobjective to predict the correct answer conditioned\non the concatenation of query, candidate answers,\nand passage. During inference, the answer with the\nleast perplexity is selected. All results are shown\nin tables 4 and 5, and the full results with more\nmetrics are shown in appendix F.2.\nAcross all results, we find that larger receptive\nfields lead to better performance in most cases.\nMore importantly, setting queries as global tokens\ngreatly benefits the performance on both eQA and\naQA. For QuALITY , it slightly hurt the perfor-\n3778\n1 2 3 4\nDocument Length (thousands)\n40\n60\n80F1 Score\nLongformerG\nBigBird (L=∞ )\nLongformer (L=∞ )\nBigBird (L=512)\nFigure 5: The performance of Longformer and BigBird\non different lengths of TriviaQA documents. Note that\nLongformerG and BigBird (L=∞) have global tokens.\nChunk 512 1024 1536 ∞\nSFBART 26.30 27.20 - -\nLED 32.81 33.07 33.22 33.57\nGR\nBART 45.60 47.90 - -\nLED 53.86 54.13 54.83 56.60\nTable 6: Results for Summarization on SummFD and\nGovRep. ROUGE unigram is used as the metric. Re-\nsults on BART are from Shaham et al. (2022).\nmance to set both query and candidate as global\ntokens. We reckon too many global tokens might\nintroduce more noise, similar to the case of coref.\nWe speculate that the performance gain mostly\ncomes from enhanced attention to the query, which\nif further verified by the metric breakdown that\nis shown in fig. 5. All models perform well on\nshort texts, while models with global tokens obtain\nan observably greater advantage over the baseline\nmodels for longer documents (pvalue < 0.01). We\nthink that the global token mechanism could help\nthe model be less distracted on long texts via more\nattention on the queries (section 5.3), which conse-\nquently improves the performance.\n4.4 Summarization\nAs a typical seq2seq problem, we adoptLED to per-\nform the summarization task. We chunk the source\nsequence into segments (no segmentation for L=\n∞) to restrict the receptive range. Intuitively, the\nsummary may be benefited from the contextual rep-\nresentation with a broader view of the document.\nTwo datasets are used: SumScreen addresses the\ndomain of TV shows. Following Shaham et al.\n(2022), we use the subset of ForeverDreaming\n(SummFD) consisting of 88 different shows. The\ngoal is to summarize the transcript of an episode,\nfor which the recap is used as the ground truth\nsummary. GovReport is a long-document sum-\nmarization dataset in the domain of government\n0 200 400 600 800 1000\nDoc Length\n0.2\n0.4\n0.6Gradient Percentage\nXLNet (L=128)\nXLNet (L=256)\nXLNet (L=512)\nFigure 6: The gradient distribution over tokens for\nXLNetm on DocNLI documents.\npolicies with human-written summaries. ROUGE\n(Lin, 2004) is used as the evaluation metric. Re-\nsults are shown in Table 6, and the full results are\nshown in appendix F.3.\nFrom table 6, we observe slight superiority of\nthe context-aware models. Note that cross attention\nwill attend to all source tokens whether we segment\nit or not, but the intuition of summarization is to\nskim over the document, so we speculate that the\nperformance improvement may be related to the\nselectivity of the encoder-side attention, which is\nfurther analyzed in section 5.2. We do not conduct\nbreakdown analysis for summarization because the\nsequence length directly contributes to the metric.\n5 Analysis\n5.1 The Attribution of Recurrence Memories\nEven if we know that distant contexts can help or\nhurt performance, it’s still unclear how much they\ncontribute to the predictions. One way to quantify\nthis is attribution analysis (Simonyan et al., 2014;\nLi et al., 2017). Suppose ℓis our loss function and\nei ∈Rd is the word embedding of the i-th token.\nWe use αi to measure the attribution of the i-th\ntoken to the final prediction where\nαi =\n\n∂ℓ\n∂ei\n\nl\n. (1)\nWe set l= 1to take the L1 norm in practice, and\nthe ground truth labels in the test set are used to cal-\nculate the gradient. Intuitively, tokens with higher\ncontribution have greater gradient norms. Also, re-\ncurrent models like XLNet stop the gradient from\nbeing propagated back to the cached memory, and\nwe temporarily turn off this feature for analysis.\nWe apply this method to XLNet on DocNLI,\nwhere we pick 128 documents of lengths between\n1,000 and 1,024. We normalize the αi over all\ntokens for each document, and take an average on\n3779\n2 4 6 8 10 12 14\nDocument Length (thousands)\n3.5\n4.0\n4.5\n5.0\n5.5Entropy\n LED (L=∞ )\nLED (L=1536)\n2 4 6 8 10 12 14\nToken Index /  103\n0.5\n1.0\n1.5Gradient Dist. / 10−4\nLED (L=∞ )\nLED (L=1536)\nFigure 7: Attention distribution entropy (above) for doc-\numents with different lengths and attribution analysis\n(below) over source tokens for LED on SummFD.\neach token index across all documents. The results\nare shown in fig. 6. Note that prediction is made in\nthe last segment, and previous segments contribute\nonly through the memory. We see the attribution\nof tokens is stratified according to their segment\nlengths, with a minor peak at theCLS token of each\nsegment. As we have more and more segments, the\nattribution of distant tokens to the final prediction\nbecomes negligible. For example, in the case of\nL=128, last segment made 53.4% of attribution,\nwhile the first segment made less than 0.01%.\n5.2 Content Selection in Cross Attention\nThe cross attention of LED attends to the whole\ndocument no matter if we segment the inputs or\nnot, thus we suppose source tokens should have\nsimilar attribution to decoding, which can be veri-\nfied by attribution analysis in fig. 7. However, the\ncrucial problem for summarization is whether the\nattention is selective, given that only a portion of\nthe document is helpful. Therefore, we inspect the\nentropy 11 of the cross attention distribution over\nthe source tokens fig. 7.\nReading the entropy curve, we find that the en-\ntropy of models without segmentation ( L=∞) is\nconsistently lower (pvalue < 0.01), which can be\ntranslated to higher selectivity of cross attention\nand explains the superiority of LED (L=∞) in ta-\nble 6. Reading the gradient curve, we find that both\n11The distribution entropy is averaged over decoding tokens,\nattention heads, and transformer layers.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nQuery Proportion\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Query Gradient\nLongformer\nLongformerG\nBigBird\nFigure 8: The accumulated gradient on query vs. the\nproportion of query, i.e. (query length / doc length) with\nLongformer and BigBird (L=∞) on TriviaQA.\nmodels have a relatively uniform attribution over\ntokens with a slight slope on the left side due to\nthe existence of short sequences. This is reason-\nable because summarization requires the decoder to\nskim over the whole document, thus cross attention\nshould not have a locality preference.\n5.3 Query-Guided Extraction and Decoding\nDifferent from other problems, the queries in QA\ncan be treated as a “guidance” on how to read the\ndocuments. To see if the encoders can exploit this\nstructure, we conduct attribution analysis on Triv-\niaQA with Longformer and BigBird. The results\nare shown on fig. 8, where we plot the relationship\nbetween the proportion of accumulated gradients\non queries and the proportion of query tokens in\nthe document. It is clear that models with global\ntokens pay more attention to queries (p value <0.01\nexcept for one exception), which is consistent with\nthe purpose of their design.\nFor scenarios where seq2seq decoding meets\nqueries, intuition suggests queries could instruct\nthe cross-attention to attend to specific tokens, mak-\ning the decoding moreselective. From fig. 9, the en-\ntropy of attention distribution on source sequence\nagainst the doc length, we see Longformer with\nglobal token unquestionably has lower entropy (p\nvalue < 0.01), implying more targeted decoding.\n5.4 Error Accumulation of Kernel Methods\nWe find that Performer could not match the results\nof its prototypes (tables 2 to 4). We suspect that the\nerror incurred by the kernel approximation may not\nbe acceptable for span-level tasks like coref. We\nconducted another set of experiments: Instead of\ntraining the performer model from the checkpoints\nof SpanBERT, we directly replace some layers from\na fine-tuned SpanBERT model with Performer lay-\ners with their parameters preserved. Experiments\n3780\n0 2 4 6 8 10\n2.5\n3.0\n3.5\n4.0\n4.5Entropy (Qasper)\nLED (L=∞ )\nLEDG (L=∞ )\n2 3 4 5 6 7 8 9 10\nDocument Length (thousands)\n1.4\n1.6\n1.8Entropy (QuALITY)\nLED (L=∞ )\nLEDG (L=∞ )\nFigure 9: Entropy of source-side attention distribution\nvs. document length on Qasper and QuALITY .\n1 3 5 7 9 11\n#Replaced Layers\n20\n40\n60\n80Avg. F1\nBaseline\n#Fea=4\n#Fea=16\n#Fea=64\nFigure 10: Results of SpanBERT (L=512) with layers\nreplaced with Performer layers with “#Fea” features.\nThe baseline is the performance of the original model.\nare conducted with C2F on Ontonotes 5.0.\nIn fig. 10, we try to replace P layers of\nSpanBERT in a top-down manner, where P =\n1,2,..., 12. We also try using different feature di-\nmensions to exclude the possibility of insufficient\nfeatures. We find that although large dimension of\nrandom features brings marginal advantages to the\nperformance, Performer is not very sensitive to this\nfactor. Instead, the performance drops dramatically\nas we replace more layers, from the baseline F1\n78 to ~20 with all layers replaced. Based on our\nfindings, we conclude that Performer is a good ap-\nproximation for shallow transformers, even with\nvery low-dimensional random features. However,\nas we stack more transformer layers, the accumu-\nlated errors can be unacceptable, which leads to a\nfailure in the performance.\n6 Experiment Confounders\nPretraining and Adaptation The purpose of this\npaper is to evaluate the effectiveness of different\nlong-range attention approaches by reducing the\ncat cat+LSTM overlap overlap+LSTM\n74\n76\n78F1 on coref.\nFigure 11: Results of different pooling strategies.\nconfounder of pre-training, but it is still unclear\nhow the results would change if we pre-train those\nmodels with the same configuration from scratch.\nUnfortunately, it requires much more resources and\nintroduces more confounders of training settings,\nand we adopt the most straightforward way to ab-\nlate the long-range attention or migrate parameters.\nFor each model-task pair, we make the most natural\nchoice, e.g. setting queries as global tokens, trying\nto minimize human biases of model adaptations.\nPooling Strategies Concatenating the represen-\ntation of segments, while being natural and com-\nmonly used, is not the only pooling strategy. As\na comparison, we incorporate the results of other\n3 solutions: 1) Split the documents into segments\nof Ltokens with L/2 overlapped between adjacent\nsegments (Joshi et al., 2019); 2) Stack an LSTM\nlayer over the Transformer representation of the\nsegment ; 3) A combination of above methods. We\nconduct experiments on the coreference resolution\ntask with both C2F and S2E solution and many vari-\nants of transformers as our encoder. We leave the\nexperiment details and discussions in appendix E\nand show a brief result in fig. 11. We have simi-\nlar observation with Joshi et al. (2019) that over-\nlapping does not bring performance improvement.\nMoreover, though introducing more parameters, a\nstacked LSTM even hurts the performance. We\nconclude that pooling strategies do not affect our\nanalysis in section 5.\n7 Discussion\nResearchers have proposed many innovative meth-\nods for efficient self-attention over long sequences.\nThe key ideas work as desired in certain cases,\nthough we demonstrate several drawbacks.\nSurprisingly, pattern-based methods , as the\nmost popular approach, are not necessarily ben-\nefited from long-range attention in the general case.\nLarger sliding windows are helpful, but the benefit\nwould quickly saturate or become negative (table 2).\n3781\nHowever, when a small portion of guidance text\n(e.g. query in QA) exists, setting it as global to-\nkens can make it more attended and significantly\nboost the performance (section 5.3). When such\ntext doesn’t exist, setting all tokens as global would\nhurt the performance (table 2). Moreover, we find\nthat long-range attention and global tokens are cor-\nrelated with the selectivity of seq2seq problems,\nwhich consequently helps the decoding.\nThe memory of recurrence models generally\nimproves performance, proving historical hidden\nstates are beneficial for transformers in various\ntasks. However, XLNet does not fully exploit\nthe history tokens, with distant information less\nattended (section 5.1). We speculate that it is be-\ncause XLNet is pretrained to predict masked tokens,\nwhich does not frequently need the participation\nof long-range context (Sun et al., 2021). Also, the\nstop-gradient trick may hinder the model from ef-\nfectively attending to memories.\nThe approximation of kernel-based methods\nworks very well for shallow networks, but faces\nserious error accumulation problems when trans-\nformer layers are deeply stacked, which cannot be\nremedied by having high-dimensional random fea-\ntures (section 5.4). The resulting performance drop\nis not acceptable even for the “base” version of\ntransformer encoders with 12 layers (table 2).\n8 Conclusion\nWe conduct experiments with various long-range\ntransformers on NLP tasks that involve long se-\nquences, trying to fairly evaluate their long-range\nattention ability. While some methods are validated\non certain tasks, we also find some previously un-\nrecognized drawbacks. We further analyze the at-\ntention behaviors of these transformers with metric\nbreakdown, attribution analysis, and entropy anal-\nysis, revealing the performance of those models\nmight be correlated with the attribution of distant\ntokens, selectivity of attentions, or the approxima-\ntion errors. We hope our work would shed light on\nthe future development of long-range transformers.\nModel Selection Takeaways Based on our find-\nings, we have the following suggestions. For\ncommon tasks, such as sequence classification or\ntoken-level prediction, it is still competitive to\nchunk the inputs and apply short-range transform-\ners. When explicit guiding text, such as queries, ex-\nists, pattern-based models with global token mech-\nanism is preferred. For seq2seq problems, long-\nrange transformers with pretrained checkpoints de-\nliver better performance.\nAcknowledgement\nWe appreciate the proofreading done by Patrick\nXia, Marc Marone, Nils Holzenberger, Elias\nStengel-Eskin, Yunmo Chen, and Zhengping Jiang\n. Thanks to the anonymous reviewers for their valu-\nable feedback.\nThis work was supported in part by IARPA BET-\nTER (#2019-19051600005). The views and con-\nclusions contained in this work are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, or endorsements of ODNI, IARPA, or\nthe U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for\ngovernmental purposes.\n9 Limitations\nEnergy Cost Our experiments involve a massive\namount of training with many transformers on vari-\nous tasks. Although we don’t conduct any pretrain-\ning, the energy cost is still non-negligible. How-\never, our hope is our findings here would enable\nothers to more efficiently select a particular archi-\ntecture for their task, rather than reproducing the\nwork done here.\nExperimental Bias Due to the lack of pretrained\ncheckpoints for general purposes, we focus on rep-\nresentative instead of each type of transformer vari-\nant. It is possible that these observations are par-\nticular to specific artifacts and implementations\nconsidered here. Our goal is foremost to provide\na roadmap for continued study on questions raised\nin this article, with new architectures being evalu-\nated in the future by model developers themselves.\nFor similar reasons, existing artifacts are biased\ntowards English, as are many of the datasets em-\nployed in this study. We do not believe our findings\nare specific to English, but it remains for future\nwork in long range transformer evaluation to ex-\ntend our analysis into multilingual conditions.\nLanguage Bias For similar reasons, existing ar-\ntifacts are biased towards English, as are many of\nthe datasets employed in this study. We do not\nbelieve our findings are specific to English, but it\nremains for future work in long-range transformer\nevaluation to extend our analysis into multilingual\nconditions.\n3782\nReferences\nArtashes Arutiunian, Morgan McGuire, Hallvar Gis-\nnås, Sheik Mohamed Imran, Dean Pleban, Priyank\nNegi, and David Arnoldo Ortiz Lozano. 2020. Re-\nproducibility Challenge: Reformer. In Advances in\nNeural Information Processing Systems (NeurIPS),\npage 10.\nAmit Bagga and Breck Baldwin. 1998. Algorithms for\nScoring Coreference Chains.\nIz Beltagy, Arman Cohan, Hanna Hajishirzi, Sewon\nMin, and Matthew Peter. 2021. Beyond Paragraphs:\nNLP for Long Sequences.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The Long-Document Transformer.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2021. SummScreen: A Dataset for Abstrac-\ntive Screenplay Summarization.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking Attention with Per-\nformers. In International Conference on Learning\nRepresentations (ICLR).\nCuration. 2020. Curation Corpus Base.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models Be-\nyond a Fixed-Length Context. In Association for\nComputational Linguistics (ACL).\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A Dataset\nof Information-Seeking Questions and Answers An-\nchored in Research Papers. In North American Asso-\nciation for Computational Linguistics (NAACL).\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A Deep Semantic Natural Language Pro-\ncessing Platform. In Association for Computational\nLinguistics (ACL).\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient Attentions for Long\nDocument Summarization. In North American As-\nsociation for Computational Linguistics (NAACL) ,\npages 1419–1436. Association for Computational\nLinguistics.\nGautier Izacard and Edouard Grave. 2021. Leverag-\ning Passage Retrieval with Generative Models for\nOpen Domain Question Answering. In European\nAssociation for Computational Linguistics (EACL).\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving Pre-training by Representing and\nPredicting Spans. Transactions of the Association\nfor Computational Linguistics (TACL), 8:64–77.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A Large Scale Dis-\ntantly Supervised Challenge Dataset for Reading\nComprehension. In Association for Computational\nLinguistics (ACL).\nMandar Joshi, Omer Levy, Daniel S. Weld, and Luke\nZettlemoyer. 2019. BERT for Coreference Resolu-\ntion: Baselines and Analysis. In Empirical Methods\nin Natural Language Processing (EMNLP).\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nRNNs: Fast Autoregressive Transformers with Linear\nAttention. In International Conference on Machine\nLearning (ICML).\nYuval Kirstain, Ori Ram, and Omer Levy. 2021. Coref-\nerence Resolution without Span Representations. In\nAssociation for Computational Linguistics (ACL).\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The Efficient Transformer. In In-\nternational Conference on Learning Representations\n(ICLR).\nYi-An Lai, Garima Lalwani, and Yi Zhang. 2020. Con-\ntext Analysis for Pre-trained Masked Language Mod-\nels. In Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 3789–3804. Association for\nComputational Linguistics.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order Coreference Resolution with Coarse-to-\nfine Inference. In North American Association for\nComputational Linguistics (NAACL).\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Juraf-\nsky. 2017. Visualizing and Understanding Neural\nModels in NLP. In Association for Computational\nLinguistics (ACL).\nChin-Yew Lin. 2004. ROUGE: A Package for Auto-\nmatic Evaluation of Summaries. In Text Summariza-\ntion Branches Out, page 8.\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. S. Zettlemoyer, and V . Stoy-\nanov. 2019. RoBERTa: A Robustly Optimized BERT\nPretraining Approach.\nXiaoqiang Luo. 2005. On Coreference Resolution Per-\nformance Metrics. In Empirical Methods in Natural\nLanguage Processing (EMNLP), page 8.\nRamesh Nallapati, Bowen Zhou, Cicero Nogueira dos\nsantos, Caglar Gulcehre, and Bing Xiang. 2016. Ab-\nstractive Text Summarization Using Sequence-to-\nSequence RNNs and Beyond. In Computational\nNatural Language Learning (CoNLL).\n3783\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversar-\nial NLI: A New Benchmark for Natural Language\nUnderstanding. In ACl.\nRichard Yuanzhe Pang, A. Parrish, Nitish Joshi, N. Nan-\ngia, J. Phang, A. Chen, V . Padmakumar, J. Ma,\nJ. Thompson, H. He, and S. R. Bowman. 2022.\nQuALITY: Question Answering with Long Input\nTexts, Yes! In North American Association for Com-\nputational Linguistics (NAACL).\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani\nYogatama, Zhaofeng Wu, Lingpeng Kong, Roy\nSchwartz, and Noah A. Smith. 2021a. ABC: At-\ntention with Bounded-memory Control.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A. Smith, and Lingpeng Kong.\n2021b. Random Feature Attention. In International\nConference on Learning Representations (ICLR).\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nand Timothy P. Lillicrap. 2020. Compressive Trans-\nformers for Long-Range Sequence Modelling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nJack W. Rae and Ali Razavi. 2020. Do Transformers\nNeed Deep Long-Range Memory. In Association for\nComputational Linguistics (ACL).\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Compre-\nhension of Text. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efficient Content-Based\nSparse Attention with Routing Transformers. Trans-\nactions of the Association for Computational Linguis-\ntics (TACL).\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nSCROLLS: Standardized CompaRison Over Long\nLanguage Sequences.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2014. Deep Inside Convolutional Networks: Vi-\nsualising Image Classification Models and Saliency\nMaps.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do Long-Range\nLanguage Models Actually Use Long-Range Con-\ntext? In Empirical Methods in Natural Language\nProcessing (EMNLP), page 16.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020a. Sparse Sinkhorn Attention.\nIn International Conference on Machine Learning\n(ICML).\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-\nbastian Ruder, and Donald Metzler. 2020b. Long\nRange Arena: A Benchmark for Efficient Transform-\ners.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020c. Efficient Transformers: A Survey.\nJames Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio\nSilvestri, Sebastian Riedel, and Alon Halevy. 2021.\nDatabase Reasoning Over Text. In Association for\nComputational Linguistics (ACL).\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\n2017. Attention Is All You Need. In Advances in\nNeural Information Processing Systems (NeurIPS).\nMarc Vilain, John Burger, John Aberdeen, Dennis\nConnolly, and Lynette Hirschman. 1995. A model-\ntheoretic coreference scoring scheme. In Conference\non Message Understanding, page 45. Association for\nComputational Linguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-Attention with\nLinear Complexity.\nR. Weischedel, E. Hovy, M. Marcus, M. Palmer,\nR. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.\n2011. OntoNotes: A large training corpus for en-\nhanced processing. In Handbook of Natural Lan-\nguage Processing and Machine Translation. Springer.\nSpringer.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-Art Natural Language Processing. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 38–45. Association for Computa-\ntional Linguistics.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhut-\ndinov, and Q. V . Le. 2019. XLNet: Generalized\nAutoregressive Pretraining for Language Understand-\ning. In Advances in Neural Information Processing\nSystems (NeurIPS).\nWenpeng Yin, Dragomir Radev, and Caiming Xiong.\n2021. DocNLI: A Large-scale Dataset for Document-\nlevel Natural Language Inference. In Association for\nComputational Linguistics (ACL).\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big Bird: Transformers for\nLonger Sequences. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS).\n3784\nA Data Preprocessing\nThe coref task usually consumes a large amount of\nGPU memory. For the experiments on Ontonotes,\nwe truncate the sequence longer than 2000 tokens\n(for c2f model) or 1400 tokens (for s2e model) for\nthe memory concern. The truncation was applied\nonly to the training set.\nWe didn’t do any pre-processing steps for Doc-\nNLI (Yin et al., 2021), except for the truncation that\nwe discussed in section 4. Notably, DocNLI is an\naggregated dataset constructed from ANLI (Nie\net al., 2020), SQuAD (Rajpurkar et al., 2016),\nDUC2001, CNN/DailyMail (Nallapati et al., 2016),\nand Curation (Curation, 2020). Documents from\ndifferent sources may be distinguishable from their\nlengths (refer to tab 1 in Yin et al. (2021).\nFor TriviaQA, we use the scripts of Long-\nformer (Beltagy et al., 2020) to pre-process the\ndata. There is no further modifications on the data.\nWe adopt the dataset of QuALITY and GovRe-\nport from Shaham et al. (2022), which picked long\nsequences from those datasets.\nFor Qasper and SummScreen, we simply adopt\nthe original dataset and scripts for preprocessing.\nB Implementation Details of\nTransformers\nIn this section, we discuss the details of modifica-\ntion we did to the transformers.\nRecurrence-Based Methods We tested XLNet\nfor recurrence-based methods. We adopt the code-\nbase of Huggingface as the base model, and fol-\nlowed the common strategy to stop gradient from\nbeing propagated back into the cached memory.\nFor each segment of the recurrence, we appended\ntwo special tokens SEP and CLS to the sequence,\nmaking it like an ordinary input sequence to the\nXLNet model except for the possible existence of\nmemory states. We concatenated the token repre-\nsentations after recurrences and remove the special\ntokens from all but the last segment. Empirically,\nwe found that having special tokens can signifi-\ncantly boost the performance.\nPattern-Based Methods For the pattern-based\nmethods, when we chunked the input sequence into\nsegments, we appended the SEP as we did for the\nXLNet, and prepended the CLS token as a conven-\ntion of other transformers. After concatenation, we\nremoved all the special tokens except for the first\nCLS the last SEP, which made it structurally simi-\nlar to the outputs of non-segmented transformers.\nWhat’s more, for sliding window mechanism, we\nmight reduce the window size to segment length if\nneeded to save compute and memory.\nKernel-Based Methods We tested Per-\nformer (Choromanski et al., 2021) as a repre-\nsentative of kernel-based methods. Given that\ntraining from random initialization would lead\nto suboptimal results, we migrate the parameters\nfrom base models (e.g. BERT, RoBERTa) to kernel\nmethods. In detail, we replace the self-attention\nlayers of the base models with kernels. Given that\nPerformer does not require any additional param-\neters, except for the orthogonal random feature\nvectors in the FA VOR+ mechanism. Following\nthe default implementation of the fast transformer\ncodebase12, we set the feature dimension as the\nquery dimension by default for main experiments,\nthough we found the performance isn’t sensitive to\nthose features in section 5.4.\nC Experiments Details\nIn this section, we introduce the details of our\nexperiments in the main paper, including hyper-\nparameters, training strategies, data split and load-\ning, and other configurations that are necessary to\nreplicate our results.\nComputational Resources All of our experi-\nments were done with NVIDIA RTX 6000 GPU\nwith 24GB of memory. We did most of the experi-\nments with single cards, except for TriviaQA, for\nwhich we used multi-GPU training with 4 cards.\nCoreference Resolution We used two mod-\nels: coarse2fine (C2F) (Lee et al., 2018) and\nstart2end (S2E) (Kirstain et al., 2021). For C2F\nmodel, we used the codebase reimplemented by Al-\nlenNLP (Gardner et al., 2018) for its flexibility on\nencoder exchange. We used the official codebase\nof S2E model for other experiments. We didn’t\nchange any hypermeters except for the difference\non the encoders. We adopted the same training\nstrategy of these repos without any modifications,\ni.e. we train the model with certain epochs (40\nfor C2F and 129 for S2E) or until convergence\nand pick the model performed best on the dev set\nfor evaluation. The typical training time of coref\nmodels was 10h for C2F and 24h for S2E.\n12https://github.com/idiap/\nfast-transformers\n3785\nNatural Language Inference Due to the size of\nDocNLI dataset (942k training and 234k dev ex-\namples), it’s infeasible to adopt the common train-\ning strategies. Instead, we train the model with\nmini-batch gradient descent with a batch size of\n4 for only one epoch. Because the training and\ndev set are too large to fit into CPU memory, we\nsplit the training set into smaller datasets consisting\nof 32768 examples, and iterate over training and\ndev set during training. We pick the model with\nbest performance on the dev set (not the whole\nset but one iteration) for test. We use the whole\ntest set consisting of 267k examples for the final\nevaluation. We adopt the same architecture as the\nmodel used in Yin et al. (2021) and reimplement it\nwith AllenNLP Gardner et al. (2018). The typical\ntraining time is around 2 days and the test time is\naround 4 hours.\nQuestion Answering For TriviaQA, we adopted\nthe training scripts and hyperparameters used by\nBeltagy et al. (2020) except for that we set the\ntraining batch size as 4 and number of epochs as\n8. The performance is evaluated on the dev set and\nwe pick the best checkpoint with patience of 3.\nFor Qasper, we follow the training scripts and\nhyper-parameters in its official repository. 13 We\ndisable the evidence setting, and extend the training\nto a maximum epoch of 20. The performance is\nevaluated on the dev set with patience of 5.\nFor QuALITY , we adopt the LED model with\n“allenai/led-base-16384” configuration from Hug-\ngingface. 14 We concatenate the question, candi-\ndate answers, and passages as the encoder input,\nand the correct answer as the decoder input for\ntraining. During inference, we feed each candidate\nanswer as the decoder input, and consider the one\nwith the lowest perplexity as the predicted answer.\nWe use a warmup steps of 1000 and learning rate\nof 5 ×10−5 for training. Evaluation is performed\non the dev set with a patience of 5.\nSummarization For both the SummScreen and\nGovReport datasets, we use the LED model with\n“allenai/led-base-16384” configuration. We use a\nwarmup steps of 1000 and learning rate of5×10−5\nfor training and patience of 5 for testing. GovRe-\nport is evaluated on the dev set and SummScreen\nis evaluated on its official test set.\n13https://github.com/allenai/\nqasper-led-baseline\n14https://huggingface.co/allenai/\nled-base-16384\nD Significance Test\nFor the comparison between curves in the section 5,\nwe conduct significance test using bootstrapping\nmethods to verify our conclusions. Let Dbe the\ntest set for a task. For the performance comparison\nbetween two configurations, we sample a new D∗\nfrom D with replacement and we keep |D∗| =\n|D|. We treat the event “configuration A performs\nbetter than B” as a Bernoulli random variable P,\nand compute the probability of the null hypothesis\nP <0.5 as the pvalue. We sample B = 1024test\nsets for each comparison. If multiple significance\ntests are conducted, we only report the larges value\nthat we obtain. 15\nFor example, in fig. 3, we claim that the per-\nformance of Longformer ( L=512) is better than\nany other encoders regardless of the mention dis-\ntances. To verify it, we conduct significance test\nbetween Longformer (L=512) and other 3 encoders\nfor every mention distance. The greatest pvalue\namong 24 ptests is smaller than 0.01, so our claim\nis secured by our significance test.\nE More Pooling Strategies\nWe conduct experiments with 4 transformers, in-\ncluding 2 short-range transformers (RoBERTa and\nSpanBERT) and 2 long-range transformers (Long-\nformer and BigBird) on the coreference resolution\ntask. We set L=512, which is the maximum accept-\nable length for short-range transformers.\nThe full results are shown in table 7, and a box\nplot can be found in fig. 11. In overall, we have\nsimilar observations as Joshi et al. (2019) that over-\nlapped segments do not offer improvements on the\nperformance. Similar findings can be found for\nLSTM settings and the combination of them. More\nimportantly, the performance difference of table 7\nis consistently with tables 8 and 11 except for a\nfew outliers. Thus, we conclude that direct concate-\nnation is already enough to exploit the pretrained\ntransformers, and changing pooling strategies do\nnot greatly interfere our analysis.\nF Full Experiment Results\nIn this section, we list the full results of all the\nexperiments in the main paper.\n15For the curves in fig. 8, we exclude one exception case at\nx = 0.6. For the curves in fig. 9, we exclude one exception\ncase at x = 500.\n3786\nF.1 Coreference Resolution\nThe full results of coreference resolution are\nshown in tables 8 and 11. We use MUC (Vilain\net al., 1995), B3 (Bagga and Baldwin, 1998), and\nCEAFϕ4 (Luo, 2005) as the evaluation metrics. Fol-\nlowing the convention, we use the “Avg.” as the\nmain metric, which is an average among the F1\nscore of 3 metrics. All the results are reported on\nthe test set.\nF.2 Question Answering\nThe full experiment results on TriviaQA is shown\nin table 9. We use both F1 and exact match (EM)\nas the metrics. A few cells are left blank because\nof the constraints of the transformers.\nF.3 Summarization\nThe full results on summarization is shown in ta-\nble 10. We used ROUGE (Lin, 2004) as the metric.\nR1, R2, and R3 stands for ROUGE unigram, bi-\ngram, and longest common subsequence. Note\nthat 1536 is the windows size of the LED model,\nand 1024 is the maximum length supported by the\nBART model.\n3787\nEncoder MUC B 3 CEAFϕ4 Avg.P R F1 P R F1 P R F1\nModel: Coarse2Fine\nRoBERTa 81.6 85.0 83.3 72.1 78.2 75.1 72.1 72.2 72.2 76.8\nRoBERTaoverlap 82.9 84.5 83.7 73.1 77.1 75.0 73.5 71.7 72.5 77.1\nRoBERTaLSTM 84.3 83.9 84.1 75.0 76.2 75.6 74.5 71.0 72.7 77.5\nRoBERTaLSTM\noverlap 84.1 84.8 84.5 75.4 77.4 76.4 74.5 72.4 73.5 78.1\nLongformer 82.4 85.3 83.8 73.0 78.6 75.7 72.6 72.5 72.5 77.4\nLongformeroverlap 82.4 83.8 83.1 72.9 76.0 74.4 72.3 70.5 71.4 76.3\nLongformerLSTM 84.2 83.8 84.0 75.3 75.8 75.5 73.8 71.7 72.8 77.4\nLongformerLSTM\noverlap 84.3 84.2 84.2 75.4 76.6 76.0 74.4 72.3 73.4 77.9\nBigBird 81.5 86.9 84.1 71.5 80.9 75.9 72.7 73.1 72.9 77.6\nBigBirdoverlap 83.0 84.3 83.6 73.9 76.7 75.3 72.4 72.0 72.2 77.0\nBigBirdLSTM 84.2 84.4 84.3 75.0 77.1 76.0 74.0 72.1 73.1 77.8\nBigBirdLSTM\noverlap 84.5 84.3 84.4 75.7 76.8 76.2 74.8 72.2 73.5 78.0\nSpanBERT 83.3 82.9 83.1 74.4 74.8 74.6 72.6 71.7 72.1 76.6\nSpanBERToverlap 83.1 83.0 83.0 74.4 75.0 74.7 72.2 72.2 72.2 76.7\nSpanBERTLSTM 83.4 82.8 83.1 74.3 74.7 74.5 72.8 70.9 71.8 76.5\nSpanBERTLSTM\noverlap 83.6 82.6 83.1 74.4 74.6 74.5 72.6 70.8 71.7 76.4\nModel: Start2End\nRoBERTa 85.7 82.6 84.1 78.1 74.3 76.2 75.2 70.9 73.0 77.8\nRoBERTaoverlap 85.0 82.6 83.8 77.7 73.5 75.6 74.4 71.5 73.0 77.4\nRoBERTaLSTM 83.8 81.4 82.6 75.1 72.3 73.7 72.1 69.4 70.8 75.7\nRoBERTaLSTM\noverlap 83.7 82.9 83.3 75.6 74.6 75.1 73.7 71.4 72.5 77.0\nLongformer 85.5 82.6 84.0 78.0 74.5 76.2 75.2 70.7 72.9 77.7\nLongformeroverlap 83.9 82.2 83.1 75.5 73.3 74.4 73.8 69.9 71.8 76.4\nLongformerLSTM 84.0 80.8 82.3 75.7 71.2 73.4 72.5 68.3 70.3 75.3\nLongformerLSTM\noverlap 83.7 81.6 82.6 75.3 72.5 73.8 72.5 69.5 71.0 75.8\nBigBird 85.7 82.3 84.0 77.9 73.9 75.9 75.7 69.4 72.4 77.4\nBigBirdoverlap 84.1 82.4 83.2 76.3 74.2 75.2 74.7 70.8 72.7 77.1\nBigBirdLSTM 83.2 81.7 82.4 74.2 72.5 73.3 72.1 68.5 70.3 75.3\nBigBirdLSTM\noverlap 83.7 81.9 82.8 75.5 73.0 74.2 73.3 69.9 71.6 76.2\nSpanBERT 83.5 81.3 82.4 74.6 71.9 73.2 72.3 68.5 70.3 75.3\nSpanBERToverlap 82.9 81.2 82.0 74.0 72.3 73.2 72.2 68.5 70.3 75.2\nSpanBERTLSTM 70.4 58.9 64.1 47.8 44.7 46.2 59.6 26.7 36.9 49.1\nSpanBERTLSTM\noverlap 68.3 61.5 64.7 43.6 47.8 45.6 59.3 26.3 36.5 48.9\nTable 7: The full results of all experiments with different pooling strategies. All models use the segment length\nL=512. Models with superscript “LSTM” indicate it uses LSTM, and subscript “overlap” indicates it uses overlapped\nconcatenation method. Note that both methods can be applied in the meantime.\n3788\nEncoder MUC B 3 CEAFϕ4 Avg.P R F1 P R F1 P R F1\nBigBird (L=128) 80.5 85.5 83.0 69.8 78.5 73.9 70.8 71.2 71.0 75.9\nBigBird (L=256) 81.1 86.2 83.6 70.5 79.8 74.9 72.1 71.7 71.9 76.8\nBigBird (L=512) 81.5 86.9 84.1 71.5 80.9 75.9 72.7 73.1 72.9 77.6\nBigBird (L=1024) 82.2 85.5 83.8 72.8 78.4 75.5 72.7 72.5 72.6 77.3\nBigBird (L=4096) 81.8 87.0 84.3 71.5 81.0 76.0 72.7 73.2 73.0 77.7\nLongformer (L=128) 81.7 83.7 82.7 71.6 75.8 73.7 71.3 70.4 70.9 75.7\nLongformerG (L=128) 81.1 84.3 82.7 70.6 76.5 73.4 71.0 70.9 70.9 75.7\nLongformer (L=256) 81.6 85.2 83.4 71.8 78.1 74.8 71.6 72.3 72.0 76.7\nLongformerG (L=256) 81.4 84.8 83.1 71.4 77.6 74.4 71.0 71.7 71.3 76.3\nLongformer (L=512) 82.4 85.3 83.8 73.0 78.6 75.7 72.6 72.5 72.5 77.4\nLongformerG (L=512) 82.6 85.0 83.8 73.2 77.9 75.5 72.4 72.4 72.4 77.2\nLongformer (L=1024) 82.1 84.9 83.5 72.3 77.7 74.9 72.0 72.0 72.0 76.8\nLongformer (L=4096) 82.0 84.2 83.1 72.4 76.2 74.3 71.6 71.6 71.6 76.3\nRoBERTa(L=128) 81.4 82.5 81.9 70.7 73.9 72.2 71.4 68.2 69.7 74.6\nRoBERTap (L=128) 69.4 57.7 63.0 55.9 42.5 48.3 49.9 38.4 43.4 51.6\nRoBERTa(L=256) 82.0 84.5 83.2 72.2 77.2 74.6 72.3 70.7 71.5 76.5\nRoBERTap (L=256) 68.7 57.9 62.9 55.7 43.1 48.6 49.3 39.2 43.7 51.7\nRoBERTa(L=512) 81.6 85.0 83.3 72.1 78.2 75.1 72.1 72.2 72.2 76.8\nRoBERTap (L=512) 68.0 56.1 61.5 55.1 40.8 46.9 48.4 38.4 42.8 50.4\nSpanBERT (L=128) 82.0 82.2 82.1 72.0 73.7 72.8 71.5 69.0 70.2 75.0\nSpanBERTp (L=128) 70.6 56.8 63.0 58.1 42.9 49.4 50.8 40.5 45.1 52.5\nSpanBERT (L=256) 82.7 82.8 82.7 73.0 74.1 73.5 71.9 70.6 71.3 75.8\nSpanBERTp (L=256) 70.0 56.4 62.5 58.5 41.7 48.7 50.1 40.8 45.0 52.1\nSpanBERT (L=512) 83.3 82.9 83.1 74.4 74.8 74.6 72.6 71.7 72.1 76.6\nSpanBERTp (L=512) 67.6 55.7 61.1 56.2 40.8 47.3 47.2 39.8 43.2 50.5\nXLNet (L=128, m=0) 81.6 82.7 82.1 71.2 73.4 72.3 70.0 68.6 69.3 74.6\nXLNet (L=128, m=128) 81.7 82.6 82.1 71.9 73.3 72.6 69.9 69.0 69.5 74.7\nXLNet (L=256, m=0) 79.4 84.2 81.7 68.5 76.0 72.0 68.5 70.9 69.7 74.5\nXLNet (L=256, m=256) 84.3 81.8 83.0 75.4 72.2 73.8 72.2 68.9 70.5 75.8\nXLNet (L=512, m=0) 79.0 85.2 82.0 67.4 77.4 72.1 69.1 68.8 69.0 74.3\nXLNet (L=512, m=512) 82.1 84.1 83.1 72.5 76.1 74.3 72.8 70.3 71.5 76.3\nTable 8: Full results on Ontonotes with the coarse2fine model. Lis the segment length used to chunk the text.\nmis the memory length used for the XLNet model. G denotes that the global tokens are used. p denotes that th\nself-attention computation is replaced with Performer kernels.\nEncoder L=128 L=256 L=512 L=1024 L=∞\nF1 EM F1 EM F1 EM F1 EM F1 EM\nLongformer 54.26 50.02 58.83 54.48 63.88 59.13 63.91 58.91 63.41 58.89\nLongformerG - - - - - - - - 72.96 67.88\nRoBERTa 55.81 50.73 60.29 56.11 63.45 58.84 - - - -\nRoBERTap 23.17 16.80 21.87 15.56 21.11 15.09 - - - -\nBigBird 55.28 50.66 59.39 54.34 63.51 58.50 66.50 61.15 71.78 66.86\nXLNet 51.46 47.10 56.26 52.08 60.05 55.62 - - - -\nXLNetm 52.71 48.03 57.96 52.93 62.85 58.13 - - - -\nTable 9: Full results on TriviaQA. We adopt the same notation as we used in table 8.\n3789\nEncoder L=512 L=1024 L=1536 L=∞\nR1 R2 RL R1 R2 RL R1 R2 RL R1 R2 RL\nSF BART 26.3 5.1 16.2 27.2 4.9 16.7 - - - - - -\nLED 32.8 7.0 18.8 33.1 7.3 18.9 33.2 7.0 18.6 33.6 7.1 18.7\nGR\nBART 45.6 16.9 21.8 47.9 18.6 22.7 - - - - - -\nLED 53.9 24.7 27.1 54.1 25.1 27.9 54.8 25.7 27.8 56.6 26.6 29.1\nTable 10: Full results on summarization. “SS” stands for the SummScreen dataset, and “GR” stands for the\nGovReport dataset. The BART model does not support sequence longer than 1024 tokens.\nEncoder MUC B 3 CEAFϕ4 Avg.P R F1 P R F1 P R F1\nBigBird (L=128) 84.5 78.5 81.4 75.2 68.6 71.7 73.5 63.2 68.0 73.7\nBigBird (L=256) 85.1 80.3 82.6 76.7 71.2 73.8 74.7 66.4 70.3 75.6\nBigBird (L=512) 85.7 82.3 84.0 77.9 73.9 75.9 75.7 69.4 72.4 77.4\nBigBird (L=1024) 85.2 82.5 83.8 77.1 74.5 75.8 76.3 69.6 72.8 77.4\nBigBird (L=4096) 85.1 82.8 83.9 77.7 75.1 76.4 75.6 70.1 72.7 77.7\nLongformer (L=128) 84.4 80.0 82.1 75.3 70.5 72.8 72.9 66.0 69.3 74.8\nLongformerG (L=128) 84.4 79.1 81.7 75.1 69.2 72.0 72.8 65.2 68.8 74.2\nLongformer (L=256) 84.8 81.7 83.2 76.2 72.5 74.3 73.9 68.8 71.2 76.3\nLongformerG (L=256) 84.4 81.8 83.1 75.6 73.1 74.3 74.3 68.3 71.2 76.2\nLongformer (L=512) 85.5 82.6 84.0 78.0 74.5 76.2 75.2 70.7 72.9 77.7\nLongformerG (L=512) 84.5 83.4 83.9 76.2 75.2 75.7 74.3 70.9 72.6 77.4\nLongformer (L=1024) 86.0 82.1 84.0 78.7 73.4 76.0 75.2 70.2 72.6 77.5\nLongformerG (L=1024) 82.4 79.2 80.8 72.2 69.5 70.8 72.2 65.4 68.6 73.4\nLongformer (L=4096) 85.2 82.9 84.1 77.4 74.6 76.0 74.8 70.7 72.7 77.6\nRoBERTa(L=128) 81.1 78.0 79.6 70.5 68.0 69.3 71.2 63.3 67.0 72.0\nRoBERTap (L=128) 61.3 45.0 51.9 45.9 30.3 36.5 41.4 25.8 31.8 40.1\nRoBERTa(L=256) 84.7 81.8 83.2 76.0 72.6 74.3 74.2 68.5 71.3 76.3\nRoBERTap (L=256) 67.7 46.0 54.8 53.0 30.8 39.0 43.8 26.9 33.3 42.4\nRoBERTa(L=512) 85.7 82.6 84.1 78.1 74.3 76.2 75.2 70.9 73.0 77.8\nRoBERTap (L=512) 67.0 45.0 53.9 53.0 29.8 38.1 43.2 26.8 33.1 41.7\nSpanBERT (L=128) 78.2 75.5 76.8 66.6 64.3 65.5 68.2 60.5 64.1 68.7\nSpanBERTp (L=128) 56.5 45.7 50.5 39.8 31.5 35.1 38.4 25.1 30.4 38.7\nSpanBERT (L=256) 83.2 79.6 81.4 74.4 70.1 72.2 71.7 67.0 69.3 74.3\nSpanBERTp (L=256) 64.1 46.5 53.9 49.8 31.8 38.8 40.7 27.9 33.1 41.9\nSpanBERT (L=512) 83.5 81.3 82.4 74.6 71.9 73.2 72.3 68.5 70.3 75.3\nSpanBERTp (L=512) 63.7 47.0 54.1 48.7 32.0 38.6 42.2 27.9 33.6 42.1\nXLNet (L=128, m=0) 79.4 39.8 53.0 69.1 30.1 41.9 61.6 32.7 42.7 45.9\nXLNet (L=128, m=128) 78.1 49.1 60.3 66.0 39.4 49.3 63.8 38.8 48.2 52.6\nXLNet (L=256, m=0) 78.8 59.5 67.8 68.0 48.6 56.7 66.4 47.9 55.7 60.1\nXLNet (L=256, m=256) 64.6 67.5 66.0 48.4 55.0 51.5 59.8 45.3 51.6 56.4\nXLNet (L=512, m=0) 80.3 71.7 75.7 70.7 61.4 65.7 66.9 60.0 63.3 68.2\nXLNet (L=512, m=512) 76.2 73.0 74.6 64.2 63.8 64.0 66.4 58.3 62.1 66.9\nTable 11: Full results on Ontonotes with the start2end model. We adopt the same notation as we used in table 8.\n3790",
  "topic": "Hyperparameter",
  "concepts": [
    {
      "name": "Hyperparameter",
      "score": 0.8656162619590759
    },
    {
      "name": "Transformer",
      "score": 0.7650266885757446
    },
    {
      "name": "Computer science",
      "score": 0.7052837610244751
    },
    {
      "name": "Machine learning",
      "score": 0.5466994047164917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5298264622688293
    },
    {
      "name": "Decoding methods",
      "score": 0.45843252539634705
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4454527497291565
    },
    {
      "name": "Metric (unit)",
      "score": 0.4130796492099762
    },
    {
      "name": "Natural language processing",
      "score": 0.3702716827392578
    },
    {
      "name": "Algorithm",
      "score": 0.12355166673660278
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}