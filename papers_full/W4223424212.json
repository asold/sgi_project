{
    "title": "Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator",
    "url": "https://openalex.org/W4223424212",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2945591293",
            "name": "Ann Franchesca Laguna",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A4223619702",
            "name": "Mohammed Mehdi Sharifi",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2383323301",
            "name": "Arman Kazemi",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2337428223",
            "name": "Xunzhao Yin",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A319491239",
            "name": "Michael Niemier",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2629951506",
            "name": "X. Sharon Hu",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2945591293",
            "name": "Ann Franchesca Laguna",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A4223619702",
            "name": "Mohammed Mehdi Sharifi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2383323301",
            "name": "Arman Kazemi",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2337428223",
            "name": "Xunzhao Yin",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A319491239",
            "name": "Michael Niemier",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2629951506",
            "name": "X. Sharon Hu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2981822321",
        "https://openalex.org/W3037644476",
        "https://openalex.org/W2782046614",
        "https://openalex.org/W6750371619",
        "https://openalex.org/W2136308753",
        "https://openalex.org/W3101493110",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2307193480",
        "https://openalex.org/W6824329548",
        "https://openalex.org/W2331783522",
        "https://openalex.org/W2885334747",
        "https://openalex.org/W6732703391",
        "https://openalex.org/W2615320339",
        "https://openalex.org/W1536792390",
        "https://openalex.org/W3189270397",
        "https://openalex.org/W6797560891",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W3110777056",
        "https://openalex.org/W6762604738",
        "https://openalex.org/W6762816929",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W3121647714",
        "https://openalex.org/W2959627720",
        "https://openalex.org/W3200825586",
        "https://openalex.org/W3200824652",
        "https://openalex.org/W6779124799",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W2988640543",
        "https://openalex.org/W2946337031",
        "https://openalex.org/W6779516005",
        "https://openalex.org/W6790309730",
        "https://openalex.org/W2889210320",
        "https://openalex.org/W3023271296",
        "https://openalex.org/W3131922516",
        "https://openalex.org/W3092601938",
        "https://openalex.org/W3013080934",
        "https://openalex.org/W2518281301",
        "https://openalex.org/W3189205905",
        "https://openalex.org/W6767997687",
        "https://openalex.org/W6774418012",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6781364056",
        "https://openalex.org/W3096017728",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W2102754081",
        "https://openalex.org/W3014426053",
        "https://openalex.org/W2906313120",
        "https://openalex.org/W6737575733",
        "https://openalex.org/W2433248078",
        "https://openalex.org/W6781533629",
        "https://openalex.org/W2591601611",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4301409763",
        "https://openalex.org/W3016339201",
        "https://openalex.org/W4239608493",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3127045726",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3098380984",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W1539686131",
        "https://openalex.org/W3098780461",
        "https://openalex.org/W4286397771",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3098873988",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W3033330790",
        "https://openalex.org/W3105120247",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3047388929",
        "https://openalex.org/W2953044442",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2952956051",
        "https://openalex.org/W3004165599",
        "https://openalex.org/W3183791462",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2913668833",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W1590272789",
        "https://openalex.org/W4242222372",
        "https://openalex.org/W2964343359"
    ],
    "abstract": "Transformer networks have outperformed recurrent and convolutional neural networks in terms of accuracy in various sequential tasks. However, memory and compute bottlenecks prevent transformer networks from scaling to long sequences due to their high execution time and energy consumption. Different neural attention mechanisms have been proposed to lower computational load but still suffer from the memory bandwidth bottleneck. In-memory processing can help alleviate memory bottlenecks by reducing the transfer overhead between the memory and compute units, thus allowing transformer networks to scale to longer sequences. We propose an in-memory transformer network accelerator (iMTransformer) that uses a combination of crossbars and content-addressable memories to accelerate transformer networks. We accelerate transformer networks by (1) computing in-memory, thus minimizing the memory transfer overhead, (2) caching reusable parameters to reduce the number of operations, and (3) exploiting the available parallelism in the attention mechanism computation. To reduce energy consumption, the following techniques are introduced: (1) a configurable attention selector is used to choose different sparse attention patterns, (2) a content-addressable memory aided locality sensitive hashing helps to filter the number of sequence elements by their importance, and (3) FeFET-based crossbars are used to store projection weights while CMOS-based crossbars are used as an attentional cache to store attention scores for later reuse. Using a CMOS-FeFET hybrid iMTransformer introduced a significant energy improvement compared to the CMOS-only iMTransformer. The CMOS-FeFET hybrid iMTransformer achieved an 8.96× delay improvement and 12.57× energy improvement for the Vanilla transformers compared to the GPU baseline at a sequence length of 512. Implementing BERT using CMOS-FeFET hybrid iMTransformer achieves 13.71× delay improvement and 8.95× delay improvement compared to the GPU baseline at sequence length of 512. The hybrid iMTransformer also achieves a throughput of 2.23 K samples/sec and 124.8 samples/s/W using the MLPerf benchmark using BERT-large and SQuAD 1.1 dataset, an 11× speedup and 7.92× energy improvement compared to the GPU baseline.",
    "full_text": "Hardware-Software Co-Design of an\nIn-Memory Transformer Network\nAccelerator\nAnn Franchesca Laguna1*, Mohammed Mehdi Shariﬁ1, Arman Kazemi1, Xunzhao Yin2*,\nMichael Niemier1 and X. Sharon Hu1\n1Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, United States,2College of\nInformation Science and Electronic Engineering, Zhejiang University, Hangzhou, China\nTransformer networks have outperformed recurrent and convolutional neural networks in\nterms of accuracy in various sequential tasks. However, memory and compute\nbottlenecks prevent transformer networks from scaling to long sequences due to their\nhigh execution time and energy consumption. Different neural attention mechanisms have\nbeen proposed to lower computational load but still suffer from the memory bandwidth\nbottleneck. In-memory processing can help alleviate memory bottlenecks by reducing the\ntransfer overhead between the memory and compute units, thus allowing transformer\nnetworks to scale to longer sequences. We propose an in-memory transformer network\naccelerator (iMTransformer) that uses a combination of crossbars and content-\naddressable memories to accelerate transformer networks. We accelerate transformer\nnetworks by (1) computing in-memory, thus minimizing the memory transfer overhead, (2)\ncaching reusable parameters to reduce the number of operations, and (3) exploiting the\navailable parallelism in the attention mechanism computation. To reduce energy\nconsumption, the following techniques are introduced: (1) a con ﬁgurable attention\nselector is used to choose different sparse attention patterns, (2) a content-\naddressable memory aided locality sensitive hashing helps to ﬁlter the number of\nsequence elements by their importance, and (3) FeFET-based crossbars are used to\nstore projection weights while CMOS-based crossbars are used as an attentional cache to\nstore attention scores for later reuse. Using a CMOS-FeFET hybrid iMTransformer\nintroduced a signi ﬁcant energy improvement compared to the CMOS-only\niMTransformer. The CMOS-FeFET hybrid iMTransformer achieved an 8.96× delay\nimprovement and 12.57× energy improvement for the Vanilla transformers compared\nto the GPU baseline at a sequence length of 512. Implementing BERT using CMOS-FeFET\nhybrid iMTransformer achieves 13.71× delay improvement and 8.95× delay improvement\ncompared to the GPU baseline at sequence length of 512. The hybrid iMTransformer also\nachieves a throughput of 2.23 K samples/sec and 124.8 samples/s/W using the MLPerf\nbenchmark using BERT-large and SQuAD 1.1 dataset, an 11× speedup and 7.92× energy\nimprovement compared to the GPU baseline.\nKeywords: transformer network, processing-in-memory, accelerator, sparsity, crossbars, CAMs, FeFET, CMOS\nEdited by:\nRam Krishnamurthy,\nIntel, United States\nReviewed by:\nJae-Sun Seo,\nArizona State University, United States\nPriyadarshini Panda,\nYale University, United States\n*Correspondence:\nAnn Franchesca Laguna\nalaguna@nd.edu\nXunzhao Yin\nxzyin1@zju.edu.cn\nSpecialty section:\nThis article was submitted to\nIntegrated Circuits and VLSI,\na section of the journal\nFrontiers in Electronics\nReceived: 01 January 2022\nAccepted: 14 March 2022\nPublished: 11 April 2022\nCitation:\nLaguna AF, Shariﬁ MM, Kazemi A,\nYin X, Niemier M and Hu XS (2022)\nHardware-Software Co-Design of an\nIn-Memory Transformer\nNetwork Accelerator.\nFront. Electron. 3:847069.\ndoi: 10.3389/felec.2022.847069\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470691\nORIGINAL RESEARCH\npublished: 11 April 2022\ndoi: 10.3389/felec.2022.847069\n1 INTRODUCTION\nTransformer networks (or simply transformers) have continually\nrisen in popularity because of their capability to outperform\nrecurrent neural networks (RNNs) and convolutional neural\nnetworks (CNNs), particularly for sequence-based tasks.\nDifferent transformer network variants, such as BERT (Devlin\net al., 2018), ALBERT (Lan et al., 2019), Megatron (Shoeybi et al.,\n2019), GPT3 (Brown et al., 2020), and XLNet (Yang et al., 2019)\ncurrently hold the best performance in various natural language\nprocessing (NLP) applications such as machine translation,\nnamed entity recognition, and question answering.\nTransformer networks are also applicable to other sequential\ntasks such as audio (Boes and Van hamme, 2019; Yang S.-W.\net al., 2020; Wei et al., 2020) and video (Boes and Van hamme,\n2019; Wei et al., 2020; Li et al., 2020b) applications. Transformer\nnetworks have also recently been used in computer vision\napplications (Dosovitskiy et al., 2020; Liu et al., 2021). The\ntransformer’s superior performance is attributed to the scaled\ndot-product attention (SDPA) mechanisms that determine the\ncorrelation between sequence elements. The attention\nmechanism in transformer networks can achieve O (1)\ncomplexity when completely parallelized and can better model\nlong-range dependencies, making them superior to CNNs\nand RNNs.\nBecause of the transformer network’s capability to learn long-\nrange dependencies, the transformer network can better analyze\nlonger sequence lengths compared to CNNs and RNNs. This\nleads to the increase in sequence lengths in NLP datasets (Rae\net al., 2019 ; Sharir et al., 2020 ). For example, in language\nmodeling, the Penn Treebank ( Marcus et al., 1993 ) and\nWikiText-103 ( Merity et al., 2016 ) datasets, which are\nobtained from news and Wikipedia articles, have an average\nsequence length of 355 and 3.6 K words, respectively. On the\nother hand, PG-19 (Rae et al., 2019), a newer dataset for language\nmodeling which is obtained from Project Gutenberg books, has\nan average sequence length of 69 K words. The use of transformer\nnetworks in image and video applications can also contribute to\nthe sequence length explosion. As transformer networks improve\nand are used in more complex applications, the number of\nparameters also continues to increase (Sharir et al., 2020). The\nvanilla (original) transformer (Vaswani et al., 2017) began with\nmillions of parameters. Later, transformer network models, e.g.,\nMegatron (Shoeybi et al., 2019) and GPT-3 (Brown et al., 2020),\ncontain billions of parameters. Recently, switch transformers\n(Fedus et al., 2021) used trillions of parameters to account for\nlong-range dependencies in the language model of the PG-19\ndataset.\nTransformer networks are commonly implemented using\ngeneral-purpose graphical processing units (GPUs) to exploit\nthe parallelism inherent in the attention mechanism. However,\nthe complexity of implementing the attention mechanism in the\nGPU is limited toO (dn\n2/c), wheren is the sequence length,d is\nthe number of feature embedding dimensions, and c is the\nnumber of parallel cores. Increasing the sequence length and\nthe number of parameters greatly increases the computation\nlatency, memory bandwidth, and energy requirements of\ntransformer networks (Vaswani et al., 2017 ) because of the\nquadratic time and space complexity with respect to the\nsequence length. Transformer networks with linear time\ncomplexity have been proposed (Beltagy et al., 2020; Zaheer\net al., 2020), but incur the cost of additional space complexity,\ncausing increased memory demand. Moreover, large transformer\nnetworks are severely limited by the memory bandwidth. For\nexample, Megatron (Shoeybi et al., 2019), one of the largest\ntransformer networks to date, only achieves 30% of the\ntheoretical peak FLOPS of a GPU because of the memory\nbandwidth bottleneck.\nDifferent techniques have been proposed to alleviate problems\nassociated with the explosion in memory and time complexity.\nThese techniques include model parallelism using multi-GPUs\n(Shoeybi et al., 2019), caching attention weights (Beltagy et al.,\n2020), cross-layer parameter sharing (Lan et al., 2019), model\ncompression ( Zafrir et al., 2019 ; Li et al., 2020c ), and\nsparsiﬁcation (Child et al., 2019 ; Kitaev et al., 2020 ; Fedus\net al., 2021). Model parallelism (Shoeybi et al., 2019) further\nexacerbates the memory bandwidth bottleneck because of sparse\nrandom memory accesses and communication between different\nGPUs. Transformer network model compression is implemented\nvia cross-layer parameter sharing (Lan et al., 2019), quantization\n(Zafrir et al., 2019), and pruning (Li et al., 2020c). Transformer\nnetwork sparsity can either be (temporal) locality-based (Child\net al., 2019; Beltagy et al., 2020) or content-based (Kitaev et al.,\n2020; Roy et al., 2021). An example of content-based sparsity is\nusing locality-sensitive hashing (LSH), an approximate nearest\nneighbor search algorithm that hashes nearby points to the same\nhash signature. However, these techniques do not solve the\nmemory bandwidth bottleneck problem.\nProcessing-in-memory (PIM) (Mutlu et al., 2020; Sebastian\net al., 2020) has been proposed to solve the memory bandwidth\nbottleneck by eliminating the communication overhead between\nthe compute unit and the memory. PIM has been used in various\napplications such as few-shot learning (Ni et al., 2019; Ranjan\net al., 2019; Challapalle et al., 2020; Reis et al., 2021), DNA\nassembly (Kaplan et al., 2018; Huangfu et al., 2018; Laguna et al.,\n2020), and security (Reis et al., 2020b). In particular, PIM-based\nattention mechanisms have been proposed using content-\naddressable memories (CAMs) ( Laguna et al., 2019a ,b\n),\ncrossbar arrays (Ranjan et al., 2019; Challapalle et al., 2020),\nand general-purpose computing-in-memory arrays (GP-CiM)\n(Reis et al., 2020a). CAMs can perform fast parallel searches\nin a single cycle, while crossbar arrays can perform matrix-vector\nmultiplications in a single cycle. GP-CiM can perform bitwise and\narithmetic operations in memory. However, PIM-based attention\nmechanisms have primarily focused on recurrent and memory\naugmented neural networks (MANNs). Transformer networks\nthat use SDPA have more parallelization opportunities than\nrecurrent and MANN-based attention mechanisms. The SDPA\nused in transformer networks can be efﬁciently implemented\nusing crossbar arrays to perform matrix-vector multiplications.\nCAM arrays can be used to implement content-based sparse\nattention via LSH.\nPIM architectures are either based on complementary metal-\noxide-semiconductor (CMOS) memories or emerging\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470692\nLaguna et al. In-Memory Transformer Network Accelerator\ntechnologies (Jeloka et al., 2016; Kang et al., 2017; Zhang et al.,\n2017; Reis et al., 2018; Ranjan et al., 2019; Yin et al., 2019). While\nimprovements in CMOS technology due to transistor scaling\nhave continuously reduced the cost of on-chip and off-chip\nmemories, PIM devices implemented in CMOS technology\nhave low density and high leakage power and require periodic\ndata refreshing. This makes it difﬁcult to apply CMOS solutions\nto large, data-centric workloads. Alternatively, non-volatile\nmemories (NVM) based on emerging technologies such as\nferroelectric ﬁeld-effect transistors (FeFET), resistive memories\n(ReRAM), and phase change memories (PCM) have high density,\nconsume low power, and are non-volatile. However, NVMs\nrequire higher write times and energy than CMOS technology,\nmaking them less ideal for high-write scenarios. FeFETs are\nCMOS compatible and have been co-integrated in CMOS\nplatforms by GlobalFoundries (Beyer et al., 2020).\nIn this paper, we present iMTransformer, an in-memory\ncomputing-based accelerator for transformer network inference.\niMTransformer employs a combination of crossbars and CAMs.\nWe also use algorithm-based techniques to improve the latency\nand energy consumption of iMTransformer. iMTransformer\nreduces the execution time by (1) mitigating the memory-\nbandwidth bottleneck with processing-in-memory-based\nhardware, (2) reducing the computational requirementsvia data\nreuse using attention caches, and (3) maximizing the parallelism\nthat can be achieved with different types of attention mechanisms.\niMTransformer further improves energy ef ﬁciency by (1)\nemploying an attention selector that can implement masked\nattention and locality-based sparse attention, (2) using CAMs to\nimplement content-based sparsity through LSH, and (3) using\nnon-volatile FeFET-based crossbars for high-read sublayers and\nwrite-efﬁcient CMOS-based crossbars for high-write sublayers.\nThe standard CMOS implementation of iMTransformer\nachieves a delay improvement of 7.7× and an energy\nimprovement of 7.81× compared to the GPU baseline for a\nsequence with length 512 by using PIM. After including model\nparallelization, sparsity, and using CMOS-FeFET hybrid\nimplementation, iMTransformer achieves 8.96× delay\nimprovement and 12.58× energy improvement compared to\nthe GPU baseline. Furthermore, implementing BERT achieves\na delay improvement of 4.68× for the standard implementation\nand 13.71× after including model parallelization, sparsity, and\nCMOS-FeFET hybrid iMTransformer implementation. The\nBERT energy improvement is 4.78× for the standard\nimplementation and 8.95× for the implementation with model\nparallelization, sparsity, and using CMOS-FeFET hybrid\niMTransformer implementation. The hybrid iMTransformer\ncan process 2.23 K samples/s and 125 samples/s/W of the\nSQuAD 1.1 dataset using BERT-large and achieves an end-to-\nend improvement of 11× for the delay and 7.92× for the energy\ncompared to the GPU baseline.\n2 BACKGROUND\nTransformer networks (discussed inSection 2.1) currently hold\nthe state of the art accuracy in NLP, computer vision, and various\nﬁelds and have the capacity to model long-range dependencies\n(Tay et al., 2020b). That said, the impressive results achieved by\ntransformer networks come with high computational and\nmemory costs as the sequence length increases. Algorithm-\nbased solutions that aim to reduce the space and the\ncomputational complexity of transformer networks are\npresented in Section 2.2. These algorithm-based solutions,\nhowever, do not solve the memory-bandwidth bottleneck\nproblem. We propose using a PIM-based solution to remove\nthe need for massive data transfers. The PIM-based computing\nkernels are presented inSection 2.3.\n2.1 Transformer Networks\nTransformer networks have outperformed CNNs and RNNs in\nvarious tasks because of their ability to model long-range\ndependencies through attention mechanisms. Because of this,\nthe transformer networks have been used in various applications\nsuch as machine translation, text generation, and language\nmodeling. These different applications have led to different\ntransformer designs. Section 2.1.1 discusses a key component\nof transformer networks: the attention mechanism, particularly\nscaled-dot product attention (SDPA) and multi-head attention\n(MHA). Section 2.1.2 then discusses the different types of\ntransformer networks, while Section 2.1.3 considers the\nexecution time distribution of transformer networks.\n2.1.1 Attention\nAttention, a crucial component of human intelligence, allows\nhumans to determine the most relevant parts of a sequence\n(i.e., text) or object and pay less attention to less relevant\nparts. Neural attention mechanisms work similarly to the\nhuman attention mechanism, where more important regions\nare given more attentional weights than less important ones.\nTransformer networks rely on SDPA (Figure 1A) to determine\nthe relationship between sequence elements. SDPA uses a key-\nvalue-based retrieval where each key corresponds to a value. The\nquery vectorq is compared to a set ofn key vectorsK ={ k\n1, k2, k3,\n... , kn} to retrieve similar values in the set ofn value vectorsV =\n{v1, v2, v3, ... , vn}. The SDPA is then calculated as a linear\ncombination of value vectors weighted by the scaled probability\ndistribution of the similarity betweenq and eachk\nj in K. To keep\nthe variance equal to one, the dot-product attention is scaled by\nthe number of dimensions of the key vectorsd\nk.\nattention q, K, V() /equals softmax qKT\n/radicaltpext/radicaltpext\ndk\n√() V (1)\nTransformer networks also introduced the concept of MHA\n(Figure 1B) that allows the network to look at the input in\ndifferent subspaces. MHA allows transformer networks to\nanalyze the relationships among sequence elements in a highly\nparallelizable manner. In MHA, the feature embedding is\nprojected into different subspaces (one subspace per head)\nwhere the sequence elements can be attended in parallel. Each\nhead (or subspace) can reveal different information regarding the\ninput. The i-th head projects the query vectorq′, the set of key\nvectors K′, and the set of value vectorsV′ by using projection\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470693\nLaguna et al. In-Memory Transformer Network Accelerator\nmatrices WQ\ni , WK\ni , WV\ni before calculating the SDPA. The output\nof the SDPA for each attention head is then concatenated and\nprojected by a linear layer.\n2.1.2 Encoder and Decoder Layers of Transformer\nNetworks\nThe (original) vanilla transformer (Vaswani et al., 2017)i sa\nneural network model that uses an encoder-decoder architecture\n(Figures 1C,D). The encoder layer ( Figure 1C) accepts a\nvariable-length input and transforms it to a ﬁxed-length\nfeature vector. The decoder layer ( Figure 1D) accepts this\nﬁxed-length feature vector and then transforms it into a\nvariable-length feature vector. This allows the network to\naccept inputs of varying lengths. Sequence-to-sequence models\n(Vaswani et al., 2017; Junczys-Dowmunt et al., 2018; Lewis et al.,\n2019; Raffel et al., 2019) follow the encoder-decoder structure of\nthe vanilla transformer (Vaswani et al., 2017) and are commonly\nused in machine translation tasks, question answering, and\nsummarization. Other applications, however, require different\ntopologies. Some applications (embedding to sequence), such as\ntext generation, only require decoder layers. Others, such as\nlanguage modeling and sentence classi ﬁcation (sequence to\nembedding), only require encoder layers. Transformer\nnetworks hence can be categorized into (1) sequence-to-\nsequence models, (2) decoder-only (or auto-regressive) models,\nand (3) encoder-only (auto-encoding) models.\nDecoder-only transformer networks are naturally used in text\nand image generation. Decoder-only (auto-regressive)\ntransformer models, such as GPT-2 (Radford et al., 2019) and\nTransformer-XL (Dai et al., 2019), use masked MHA where the\nattention scores of the current input are only based on attention\nscores of past inputs and not on future inputs. Because of their\nauto-regressive nature, caching the attention scores (keys and\nvalues) can reduce the computational requirements of each time\nstep at the cost of higher storage demands (Dai et al., 2019).\nEncoder-only transformer networks are usually used for\nlanguage modeling and sentence/token classiﬁcation. Encoder-\nonly (auto-encoding) transformer models, such as BERT (Devlin\nFIGURE 1 |Transformers use scaled dot product attention(A) and multi-head attention(B) to model long-range dependencies and use an encoder-decoder\narchitecture (Vaswani et al., 2017). The encoder(C) and decoder (D) layers are composed of multihead attention, feedforward and normalization sublayers. The\nexecution time distribution(E) shows the transformer networks becomes more dominated by the multi-head attention as sequence length increases. The execution time\n(F) of transformers increases as the sequence length increases.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470694\nLaguna et al. In-Memory Transformer Network Accelerator\net al., 2018) and ALBERT (Lan et al., 2019), do not use masking,\nand each input is in ﬂuenced by past and future inputs\n(bidirectional). Due to their bidirectional nature, auto-\nencoding transformer models can be greatly parallelized via\ndata and model parallelization (Shoeybi et al., 2019).\n2.1.3 Transformer Network Execution Time\nTo investigate the execution time needed by different functions in\nthe encoder and decoder layers of transformer networks, we\nproﬁled the Vanilla Transformer running on a Titan X GPU.\nFigures 1E,Fshows the resulting execution time distribution of\nthe transformer network. The encoder and decoder layers of\ntransformer networks are primarily composed of MHA,\nfeedforward, and normalization sublayers, as shown inFigures\n1C,D. As the sequence lengthn increases, the execution time of\nthe transformer network increases quadraticallyO (n\n2) due to the\nMHA. The feedforward layers only increase linearly O(n).\nBecause of this, the MHA dominates the execution time of the\ntransformer network as the sequence length increases, as shown\nin Figure 1E.\nIn this work, we focus on accelerating MHA. In particular,\ndifferent types of transformer networks have different MHA\nproperties, which can be exploited when designing transformer\nnetwork accelerators. For example, decoder-only transformer\nmodels require masked MHA. By not executing the masked\noperations, the number of computations can be reduced. The\nencoder-only transformer models use bidirectional MHA. By\nexploiting model parallelism for bidirectional MHA,\ntransformer networks can be accelerated. We utilize these\ntransformer network properties in designing our transformer\nnetwork accelerator.\n2.2 Algorithm-Based Transformer Network\nAcceleration\nTransformer networks have high computational and space\ncomplexity because of the employed attention mechanism.\nTransformer network GPU implementations are bounded by\nthe memory, particularly with longer sequences, because of the\nO (dn + dn\n2) spatial complexity, whered represents the feature\nembedding dimension, and n i st h es e q u e n c el e n g t h .A\ntransformer can also be computation-limited because of the\nO (dn\n2) serialized time complexity of the MHA. TheO (dn2)\ncomplexity comes from each time step (sequence element)\nattending to every other time step (sequence element).\nHowever, an O (1) time complexity can be achieved with\nadequate parallelism. The following sections review four types\nof algorithm-based acceleration: quantization (Section 2.2.1),\nattention caching (Section 2.2.2), model parallelism (Section\n2.2.3 ) and sparse attention (Section 2.2.4 ).\n2.2.1 Quantization\nReducing the representation precision by quantization can\nalleviate the memory demand and reduce the time complexity\nof transformer networks by reducing the amount of data transfer\nrequired between the compute and memory units. FullyQT and\nQ8BERT studied transformer quantization. FullyQT (Prato et al.,\n2019) used k-bit uniform quantization. Transformer networks\nquantized in 8-bits performed better in 21 out of 35 experiments\nmade in FullyQT, and there was minimal degradation on the\nother experiments. Q8BERT ( Zafrir et al., 2019 ) used\nquantization-aware training and has shown that it performs\nbetter than using dynamic quantization. Both FullyQT and\nQ8BERT have found that 8-bit quantization is found to have\ncomparable accuracy to transformer networks at full precision.\nNon-uniform quantization has also been proposed for\ntransformer networks ( Chung et al., 2020 ) and has shown\nbetter compression without sacri ﬁcing accuracy. These\nproposed non-uniform quantization techniques decouple\nfeature vectors into a set of weights and binary vectors. These\nbinary codes are also more hardware-friendly than other\nquantization methods.\n2.2.2 Attention Caching\nThe decoder layer of the transformer is auto-regressive (i.e., the\ncurrent output is the next input). The attention keys and values\nfrom previous time steps affect the current time step. These keys\nand values have been computed in previous time steps and can be\nreused instead of recomputing from scratch as implemented in\nthe vanilla transformer (Vaswani et al., 2017). Transformer-XL\n(Dai et al., 2019) implemented attention caching and achieved up\nto 1800× improvement during inference at a sequence length of\n3.8 K.Attention cachingimproves the computational complexity\nof the transformer during inference at the cost of higher space\ncomplexity. Furthermore, attention caching allows the modeling\nof a longer range of dependencies by using in conjunction with\nsparse attention (Child et al., 2019; Dai et al., 2019). Sparse\nattention is explained in detail inSection 2.2.4. We use attention\ncaching in designing our transformer network accelerator.\n2.2.3 Model Parallelism\nModel parallelismaims to distribute a neural network model into\nmultiple compute units to reduce time complexity and improve\ncompute unit utilization. Megatron (Shoeybi et al., 2019) used\nmodel parallelism to split different attention heads into multiple\nGPUs. By implementing model parallelism. Megatron improved\nthe GPU utilization from 30% theoretical peak FLOPS for a single\nGPU to 52% theoretical peak FLOPS on 512 GPUs (Shoeybi et al.,\n2019). Model parallelism is particularly beneﬁcial in accelerating\nencoder layers of the transformer network because of its\nbidirectional properties. We utilize model parallelism in\naccelerating the encoder layers of the transformer network\naccelerator.\n2.2.4 Sparse Attention\nThe SDPA mechanism described inSection 2.1.1 uses the full\nattention mechanism where the query vector is compared to all\nkey and value vectors. The full attention mechanism is the most\naccurate among different attention patterns and is ideal for\nshorter sequences. However, because of the quadratic\ncomputational complexity of the full attention pattern, it may\nbe necessary to use an attention pattern with less computational\ncomplexity at higher sequence lengths. Introducing sparsity in the\nattention layers has been one of the prominent algorithm\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470695\nLaguna et al. In-Memory Transformer Network Accelerator\noptimizations to reduce the computational complexity of the full\nattention mechanism. Sparse attention only attends to the keys\nand values relevant to the query. Two major types of sparse\nattention have been proposed: locality-based sparsity and\ncontent-based sparsity. We use both types of sparsity in\ndesigning our transformer network accelerator.\nLocality-based sparsity uses ﬁxed/random patterns based on\nthe query’s relative position to the current time step. The type of\ndata determines the choice of attention pattern, and each\nattention pattern has its strengths and weaknesses. Longformer\n(Beltagy et al., 2020) proposed the sliding window and dilated\nsliding window attention patterns and achieved state-of-the-art\nresults for character modeling tasks. The sliding window\nattention, obtained by focusing on sequence elementst\nsld time\nsteps away, is ideal for data with high spatial locality. The sliding\nwindow can also be dilated where it only pays attention to every\nother tsld time step. Sparse transformers (Child et al., 2019)\nproposed strided attention and strided + sliding window\nattention patterns and achieved equal or better accuracy\ncompared to full attention mechanisms while reducing the\nnumber of operations. Sparse attention is used for music and\nimage generation and machine translations of text. Strided\nattention is obtained by skipping t\nstr time steps and performs\nwell in repeating or oscillating sequences such as audio or video\ndata. A combination of strided global attention with sliding\nwindow attention is recommended for long documents\n(Beltagy et al., 2020) where there may be a correlation in texts\nthat are farther away.\nContent-based sparsity is another type of sparsity and is based\non the similarity of key-value attention pairs with the query.\nContent-based sparsityuses the similarity of the current time step\nwith the previous time step to determine the sparsity. The outing\ntransformer (Roy et al., 2021) uses k-means clustering while\nSinkhorn network (Tay et al., 2020a) uses sorting to determine\nsparsity. Reformer (Kitaev et al., 2020) uses angular locality-\nsensitive hashing (LSH) to accelerate the transformer for long\nsequences. Angular LSH uses random hyperplanes that pass\nthrough the origin, and the angle of a point is determined by\nits position with respect to the different hashing hyperplanes. By\nusing LSH to hash, the complexity of the SDP attention is reduced\nfrom O (n\n2)t o O (n log n). We use angular LSH to introduce\ncontent-based sparsity in iMTransformer.\nLSH is an approximate nearest neighbor search (NNS)\napproach for alleviating the curse of dimensionality when\nsearching a large number of data points, thus, allowing for a\nfast NNS. This is accomplished by hashing similar items with the\nsame binary signature. To perform an NNS, the binary signature\nof a query is compared to the binary signatures of the keys using a\nHamming distance. LSH attention has been used in multiple\nattention-based neural network architectures (Kaiser et al., 2016;\nKitaev et al., 2020).\n2.3 In-Memory Computing Based Hardware\nKernels for Acceleration\nTo accelerate transformer networks, iMTransformer leverages\ncrossbars to implement the SDPA and CAMs to realize\ncontent-based sparsity using LSH. Crossbars and CAMs are\ndescribed in detail inSection 2.3.1. Crossbars and CAMs can\nbe implemented in CMOS or non-volatile devices based on\nemerging technologies such as FeFET. These devices are\ndiscussed in Section 2.3.2. Finally, we review attention and\ntransformer network accelerators in Section 2.3.3 based on\ncrossbars and CAMs using CMOS and FeFET devices.\n2.3.1 Circuits\nThe transformer network attention mechanism uses linear layers\nand SDPA, requiring matrix-vector multiplications. Crossbars\ncan accelerate matrix-vector multiplications, and have been used\nin a variety of DNN applications such as CNNs (Shaﬁee et al.,\n2016; Chen P.-Y. et al., 2018) and MANN (Ranjan et al., 2019). A\ncrossbar (Gokmen and Vlasov, 2016) is an array-like circuit\nstructure where each input is connected to every output, and\nvice versa as shown in Figure 2A. To perform matrix-vector\nmultiplications, the matrix must be encoded as conductances\nstored in the crossbar crosspointsg\ni,j, and the vectors as input\nvoltages Vi. The output of the matrix-vector multiplication is read\nas currents Ij at the columns using analog-to-digital converters\n(ADCs). The ADCs, typically, consume the majority of the energy\n(58%) and area (81%) of crossbar arrays (Roy et al., 2020). This\nenergy and area consumption also increases exponentially with\nincreased precision (Shaﬁee et al., 2016). NeuroSim, a DNN\nsimulator ( Chen P.-Y. et al., 2018 ), uses crossbars to\nbenchmark convolutional and multilayer perceptron-based\nneural networks.\nContent-addressable memories (CAMs) have been used to\naccelerate attention mechanisms for MANNs (Ni et al., 2019;\nLaguna A. F. et al., 2019). CAMs are a special type of memory\nthat can perform fast parallel searches across the entire memory\n(Figure 2B). Different CAM designs have been proposed for\naccelerating various search operations.Binary CAMs (BCAMs)\nstore either a logic“0” or logic “1” in each cell whileTernary\nCAMs (TCAMs) can store an additional don’tc a r ev a l u e“X,” to\nsignify that the bit can match to either a logic“0” or a logic“1.”\nHamming distance is the most straightforward metric for\napproximate search in BCAMs/TCAMs. CAMs, which are\ntraditionally used in routers and caches (Karam et al., 2015;\nYin et al., 2020) ,h a v eb e e ng a i n i n gp o p u l a r i t yi nd a t a - i n t e n s i v e\napplications such as nearest neighbor search (Kohonen, 2012;\nKazemi et al., 2020, 2021b), bioinformatics (Laguna et al., 2020),\nneural networks (Chang, 2009; Wang et al., 2010; Li C. et al.,\n2020, Li et al., 2021 H.; Kazemi et al., 2021a), etc. CAMs also\nhave been used in implementing LSH-based attention (Ni et al.,\n2019).\n2.3.2 Devices\nCrossbars and CAMs can be implemented using CMOS or non-\nvolatile memories (NVMs) based on emerging technologies such\nas resistive RAMs (ReRAMs) and FeFETS. CMOS-based\ncrossbars and CAMs have low write latency and energy,\nmaking them ideal for transformer sublayers requiring many\nwrite operations. However, CMOS-based circuits have high\nleakage power ( Jerry et al., 2018 ) which is detrimental for\nstoring static weights.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470696\nLaguna et al. In-Memory Transformer Network Accelerator\nUnlike CMOS-based memories, NVMs, such as ReRAM and\nFeFET devices, can store static weights without periodic refreshes.\nMoreover, NVMs also have a higher density compared to CMOS-\nbased memories. ReRAMs offer good density and fast reads,\nmaking them a good candidate for applications requiring a large\nnumber of read operations. However, ReRAMs can suffer from\ncycle-to-cycle (C2C) variation and smallG\nmax/Gmin ratios (Jerry\net al., 2018). Moreover, ReRAMs have low endurance (10 6)\ncompared to CMOS-based devices (10 16) putting them at a\ndisadvantage for write operations (Yu and Chen, 2016).\nFeFET based crossbars and CAMs are also great candidates for\nhigh-read operations because of their high density and fast read\noperation. FeFETs have acceptableG\nmax/Gmin and observe lower\nC2C variations compared to ReRAMs (Jerry et al., 2018). As\nshown inFigure 2C, FeFETs have a similar structure to the metal-\noxide-semiconductor ﬁeld-effect transistors (MOSFETs) in\nstandard CMOS. The only difference between the two is the\nextra layer of FE oxide deposited in the FeFET’s gate stack.\nBecause of this similarity, FeFETs can be integrated into the\nCMOS fabrication process (Beyer et al., 2020). This enables us to\nconsider a combination of FeFETs and CMOS devices in our\narchitecture to realize better performance. One of the\nshortcomings of FeFETs (as well as other emerging devices) is\ndevice variation. In order to alleviate device variation, we need to\nuse write-verify programming schemes (Shariﬁ et al., 2021) which\nincreases the write time. FeFET devices also have lower\nendurance (10\n10) than CMOS devices (Yu and Chen, 2016).\nThis makes the use of FeFETs challenging for applications that\ndemand a high number of write operations. Transformer\nnetworks also may require large-scale memories. Large-scale\nFeFET memories have been demonstrated (Beyer et al., 2020).\n2.3.3 Attention and Transformer Network Accelerators\nDifferent attention-based accelerators have previously been\nproposed utilizing CAMs (Laguna A. et al., 2019; Challapalle\net al., 2020; Laguna A. F. et al., 2019), crossbar arrays (Challapalle\net al., 2020; Ranjan et al., 2019), and GP-CIMs (Reis et al., 2020a).\nThese accelerators were used to accelerate the attention\nmechanism for MANNs and RNNs. However, the attention\nmechanism for transformer networks has different properties\nthan the ones in MANNs and RNNs. Existing attention\naccelerators (Laguna et al., 2019a,b; Reis et al., 2020a) use k-\nnearest neighbors, which are not applicable for the transformer\nnetwork since the attentional weights need to be passed to the\nnext layer. The transformer network also uses MHA, which can\nbe highly parallelized and has not been utilized in existing\nattention accelerators. An increase in parallelism can also be\nachieved by exploiting the autoregressive and bidirectional\nproperties of the MHA. These properties have not been\nconsidered in existing attention accelerators.\nA ReRAM-based transformer (ReTransformer) (Yang X. et al.,\n2020) has been proposed to accelerate SDPA using ReRAM-based\ncrossbars. ReTransformer uses matrix decomposition to avoid\nwriting the intermediate results. ReRAM has lower endurance\ncompared to CMOS and FeFETs; hence writing in the crossbars\nmust be minimized (Yu and Chen, 2016). However, attention\ncaching (Dai et al., 2019), which necessitates writing in the\ncrossbars, has been shown to improve the execution time of\ntransformer networks for long sequences by reducing the number\nof operations. The low endurance of ReRAMs makes them\nundesirable as memory devices for attentional caches.\nCompared to the GPU approach, ReTransformer achieves a\nspeedup of 23.21× with a 1086× power reduction.\n3 METHODOLOGY\nTo eliminate the limitation from memory bandwidth and exploit\ntransformer networks’ high degree of achievable parallelism as\nsequence length increases, we propose an in-memory computing-\nbased transformer network architecture referred to as\niMTransformer. iMTransformer follows a hierarchical design\nstyle. Section 3.1 presents an overview of iMTransformer.\nTransformer networks have three types of MHA: bidirectional,\nmasked, and encoder-decoder. Each of these MHA types has\ndifferent characteristics that can be exploited to improve the\nlatency and energy performance of iMTransformer. The mapping\nof attention mechanisms for different types of MHA is\nFIGURE 2 | (A)Crossbars are array-like circuit structures where each input is connected to every output typically by a resistor at their crosspoints.(B) CAMs can\nperform fast parallel searches in their memory.(C) FeFET devices are CMOS compatible devices that provide non-volatility, high density, and low power consumption.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470697\nLaguna et al. In-Memory Transformer Network Accelerator\nexpounded in Section 3.2. The computational complexity of\ntransformer networks can be reduced by introducing either a\ncontent-based or locality-based sparsity. Section 3.3 aims to\nreduce energy consumption by utilizing sparsity. Energy can\nalso be improved by utilizing FeFETs for sublayers with high\nread rates and CMOS for sublayers with high write rates. This\ntechnology-level mapping is explained inSection 3.4. Finally, we\nsummarize the circuit and device level mapping inSection 3.5.\n3.1 In-Memory Transformer Network\nArchitecture\niMTransformer is organized in a hierarchical pattern of banks,\ntiles, and mats as shown inFigure 3. This hierarchical pattern\nfollows existing memory hierarchies and the hierarchical pattern\nof transformer networks as shown inFigure 1. The transformer\nnetwork is composed of encoder layers and/or decoder layers.\nEach encoder or decoder layer is composed of MHA, FF, and\nnormalization layers. The MHA then can be greatly parallelized\ninto multiple attention heads.\nThe encoder and decoder layers of transformer networks have\ndifferent properties which can be exploited to increase parallelism\nand reduce the number of computations in implementing\ntransformer networks. Hence, iMTransformer uses two types\nof banks: encoder banks and decoder banks. For an encoder-\ndecoder transformer network, the informationﬁrst ﬂows through\nthe encoder banks (i.e., Enc Banks 1– 6i nFigure 3A) before being\nprocessed by the decoder banks (i.e., Dec Banks 1 – 6i n\nFigure 3A). Enc Banks processes data one after another. The\noutput of theﬁnal Enc Bank is then passed to all the Dec Banks as\nthe stored key-value pairs of the SDPA. These key-value pairs are\nthen processed in parallel. Because the decoder layers are\nautoregressive, the input query of Dec Bank 1 is the output of\nDec Bank 6.\nAs shown inFigures 1C,D, each encoder and decoder layer of\nthe transformer network consists of multi-head attention,\nfeedforward, and normalization sublayers. The iMTransformer\nbanks (Figures 3B,C) are hence composed of a normalization\nunit (NU), the feedforward tile (FF Tile), and the multi-head\nattention memory tile (MHA Tile). The NUs execute the layer\nnormalization using additions and shift operations. Since no\nweights are stored in the NUs, they are shared between\nsublayers. The FF Tile is composed of two crossbar sub-arrays\nand performs feedforward layer operations with ReLu activation\nin between. Encoder banks (Figure 3B) have one MHA tile while\ndecoder banks (Figure 3C) have two MHA tiles. To follow the\ninformation ﬂow in Figure 1C, the encoder bank (Figure 3B)\npasses the information to the MHA tile and then to the NU. Then,\nthe NU passes the information to the FF tile and then back to the\nNU for the output. To map the decoder layer inFigure 1D to\niMTransformer, the decoder bank’s input (Figure 3E) is passed to\none of its MHA TilesⒶ. The attention vector from the MHA tile\nis then passed to the NUⒷ. The normalized attention vector\nfrom the NU is then passed to a different MHA tileⒸand back to\nthe NUⒹand then to the FF TileⒺ. Theﬁnal attention vector is\nthen passed to the NU for outputⒻ.\nThe MHA (Figure 1B) splits the input into queries, keys, and\nvalues and passes them to different attention heads. The different\nattention heads are then concatenated together using a linear\nfunction. Since each attention head can be executed in parallel\nwith each other, we decompose each attention head into multiple\nattention memory mats➊ (AH Mat), where each mat represents\nan attention head. The encoder MHA tile (Figure 3D) also\ngroups multiple AH mats to take advantage of the\nFIGURE 3 |The iMTransformer follows a hierarchical memory structure. The iMTransformer(A) is composed of encoder(B) and decoder(C) banks. The encoder\nbank is composed of an encoder MHA tile(D) and FF tiles and normalization units. The decoder bank is composed of two decoder MHA tiles(E), an FF tile and a\nnormalization unit. The encoder(D) and decoder (E) MHA tiles are composed of AH Mats(F) and an aggregator unit.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470698\nLaguna et al. In-Memory Transformer Network Accelerator\nbidirectional property of encoder MHA. The details for this are\nfurther discussed inSection 3.2. Afterward, the aggregator unit\n(AU) in the MHA Tile ( Figure 3D ➋) computes a linear\ncombination of the different attention heads.\nEach attention head of the MHA is mapped onto an AH Mat.\nThe AH Mat (Figure 3E) is composed of three projection units\n(PU), two caching units (CU), a hashing unit (HU), a sparsity unit\n(SU), an attention selector (AS), and a softmax lookup table\n(LUT). As shown inFigure 1B, each MHA head is composed of\nthree linear layers that project the query, key, and value into a\ndifferent subspace. These linear layers are implemented by PU-Q,\nPU-K, and PU-V, which store the projection matrices of the\nlinear layers (W\nQ, WK, WV) to project the input to different\nfeature spaces. These projection matrices have static weights\nduring inference. The projected queryq /equals W\nQqt′, obtain from\nPU-Q, is then compared to the present and previous projected\nkeys K ={ kt, kt−1, kt−2 ... }. and valuesV ={ vt, vt−1, vt−2 ... }\nobtained from PU-K (kt /equals WKkt′) and PU-V vt /equals WVvt′\nrespectively. ➀ The PUs (shown in green boxes as in\nFigure 3F) process the input in parallel. The output of the\nPUs represent the input to the SDPA, which are implemented\nusing the CUs and the softmax LUT (shown as dark blue boxes in\nFigure 3F ➁– ➅).\nThe SDPA is mapped to the CUs and the softmax LUT.\nSpeciﬁcally, the keys and values are cached in crossbars (CU-K\nand CU-V, respectively) for reuse to reduce computation➁. The\noutputs of PU-K(k\nt /equals WKkt′) and PU-Vvt /equals WVvt′ are written\nto a column of CU-K and a row of CU-V, respectively. During\ninference, CU-K and CU-V are constantly written to, while PU-\nQ, PU-K, PU-V have static weights.➂ The output by PU-Qq is\nused as the input to CU-K.➃ The output of CU-K,qK\nT, passes\nthrough the softmax LUT. The dot product scaling(1/\n/radicaltpext/radicaltpext/radicaltpext\ndk\n√\n) is\nimplemented by dropping the three least signiﬁcant bits for adk =\n64. This is equivalent to dividing 8. [dk = 64 is used in most\ntransformer network (Vaswani et al., 2017; Devlin et al., 2018)]. ➄\nThe output of the softmax unit is then used as the input to CU-V.\n➅ CU-V produces theﬁnal output of the AH Mat. To combine\nthe outputs of AH Mats, the AU concatenates and pools the\noutput of each AH Mat in an MHA Tile. The AU stores the\nweights W\nO which are static during inference, and➋ outputs the\nﬁnal multi-head attention score.\nThe iMTransformer stores a pre-trained transformer network\nmodel. Hence, the sizes and number of crossbar arrays for the\nPUs and HU can be set to exactly store the trained weights.\nHowever, the sequence length of each input is variable. Hence the\nsize of the crossbars in the CUs and CAMs in the SU cannot be\nﬁxed. Since the keys are stored column-wise, increasing the\nnumber of keys (increasing the sequence length) will only\nrequire crossbar arrays that are implemented in parallel. On\nthe other hand, as the sequence length increases, the values\nwill require more rows. This will also require more crossbar\narrays where each crossbar array outputs a partial sum that needs\nto be aggregated. We limit the sequence length to 512 to prevent a\nlarge number of partial sums and use content-based sparsity for\nsequence lengthn > 4096. The CAMs are also designed to hold up\nto sequence lengths up ton = 4096. Beyond this, a replacement\nalgorithm must be designed to determine which sequence\nelements can be removed from the memory. We have not\nexplored this replacement algorithm in this research, and it is\na topic for future work.\n3.2 Multi-Head Attention Operation\nMapping\nAs discussed inSection 2.1.3, transformer networks are heavily\ndominated by MHAs. Hence iMTransformer focuses on\naccelerating MHA, which relies primarily on matrix-vector\nmultiplications. Transformer networks have three different\ntypes of attention: bidirectional MHA, masked MHA, and\nencoder-decoder MHA. Each type has different properties,\nwhich are exploited by iMTransformer to improve the time\nand energy consumption of transformer networks. The\nbidirectional MHA can be parallelized during inference as the\nentire input sequence is available and does not need to be\ncomputed. On the other hand, the energy consumption of the\nmasked MHA can be reduced by turning off columns in the CUs.\nFinally, encoder-decoder MHA can parallelize the computation\nof the projected keys and values across different layers. In this\nsection, we discuss the operation mapping for the masked MHA\n(Section 3.2.1), bidirectional MHA ( Section 3.2.2) and the\nencoder-decoder MHA inSection 3.2.3.\n3.2.1 Masked Multi-Head Attention\nThe masked MHA uses a decoder MHA tile and does not use\nmodel parallelism (Figure 4A). The transformer’s decoder layer is\nautoregressive by nature (i.e., the input is a delayed version of the\noutput) and uses masked MHA. The masking is essential to\nprevent a backward informationﬂow (i.e., the future affects the\npast). The masked MHA computes the projection layers and\nSDPA for each sequence element before computing the next\nsequence element.\nWe utilize an AS, a SU, and a HU to implement masking and\nsparsity (i.e., locality-based sparsity and content-based sparsity).\nWhen masking is implemented, the AS disables the rows of the\nCU-V that represent future time steps when selecting the values\nfor the SDPA. The AS is a conﬁgurable circular shift register that\nallows different types of locality-based sparsity based on the\nstored pattern in the register. More details regarding locality-\nbased sparsity and the different attention patterns are discussed in\nSection 3.3.1. The HU is a crossbar array that hashes the keys\nusing LSH based on random projection and stores the hash\nsignatures H(K) in the SU. Content-based sparsity mapping is\nfurther expounded inSection 3.3.2.\n3.2.2 Bidirectional Multi-Head Attention\nThe encoder layer uses bidirectional MHA to attend to each\nsequence element’s previous, current, and future values. The\nbidirectional MHA can be parallelized by duplicating weights\nin the PUs.Figure 4Bshows the operations for each time step.\n(Step 1) The PU-K and PU-V of all AH Mats in the same tile\nperform matrix-vector multiplications in parallel. (Step 2) The\nﬁrst AH Mat then broadcasts the computed key-value pair (k\n1, v1)\nto other AH Mats. (Step 3) Each AH Mat then writes (k1, v1)t o\nthe CU-K column-wise and CU-V row-wise. Steps 2-3 are\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 8470699\nLaguna et al. In-Memory Transformer Network Accelerator\nrepeated for (k2, v2) and so on, until the whole sequence is\nprocessed. For a sequence length of n and the degree of\nparallelism p (Step 2n/p to 2n/p+5), each AH Mat performs the\nSDPA operation (qKT) for each sequence element. However, the\nMHA still has anO(n) complexity because of the number of writes.\nThe complexity of the SDPA is reduced toO(n)f o rn ≤ p (p =3i n\nFigure 3). Ifn > p, the time complexity of the SDPA isO (n/p).\nHowever, the increased parallelism requires more AH Mats and\nthus a higher peak power. As such, the attention parallelism is\nlimited by the total memory size and/or the thermal design power.\n3.2.3 Encoder-Decoder MHA\nThe encoder-decoder MHA (Figure 1D) is implemented by the a\ndecoder MHA tileⒸ in a Dec Bank (Figure 1C). The key-value\nFIGURE 4 |The timeline of implementation for the different types of MHA namely masked MHA(A), bi-drectional MHA(B) and encoder-decoder MHA(C). The bi-\ndirectional MHA uses model parallelism across multiple AHUs while the encoder-decoder MHA parallelizes the computation of the keys and values across different\ndecoder banks.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706910\nLaguna et al. In-Memory Transformer Network Accelerator\npair inputs of the MHA tiles (K′, V′) all come from the output of\nthe last encoder bank (Enc Bank 6). Hence the encoder-decoder\nMHA requires communication between the encoder and the\ndecoder banks. Instead of serializing the computation of the\nkeys and values per decoder layer as in the standard\niMTransformer implementation discussed in Section 3.1, the\ncomputation of the projected key-value pairs (K, V) can be\nparallelized for all decoder banks (Dec 1– 6) in Figure 3A. The\nencoder-decoder MHA processes the decoder sequence query\none element at a time as shown inFigure 4C. However, the keys\nand values do not need to be recalculated.\n3.3 Sparse Attention for iMTransformer\nA full self-attention mechanism (Figure 5A) computes for the\nglobal correlation between elements in a sequence. However,\ncomputing for the full self-attention mechanism can be costly as\nthe sequence length increases (Child et al., 2019). Introducing\nsparsity in the attention mechanism can reduce the transformer\nFIGURE 5 |The full attention(A) and masked attention pattern(B) have high computational complexity. To improve the computational efﬁciency, different locality-\nbased sparse attention patterns can be used to improve the computational efﬁciency. Examples of these attention patterns are strided(C), sliding window(D), dilated\nsliding window(E) and strided(F) sliding window attention. The conﬁgurable attention selector(G) uses a circular shift register which contains a pre-deﬁned attention\npattern based on the type of attention matrix used.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706911\nLaguna et al. In-Memory Transformer Network Accelerator\nnetwork’s computational complexity without sacri ﬁcing\naccuracy. ADCs in a crossbar array consume most of the\nenergy in performing matrix-vector multiplications (Roy et al.,\n2020). By introducing sparsity, the ADCs in a crossbar that does\nnot contribute to improving the accuracy can be turned off to\nreduce energy consumption. As discussed inSection 2.2.4, there\nare two main types of sparse attention: locality-based sparse\nattention and content-based sparse attention. Locality-based\nsparse attention ( Section 3.3.1 ) focuses on the temporal\nlocality between sequence elements while content-based sparse\nattention (Section 3.3.2) focuses on the similarity between\nsequence elements.\n3.3.1 Locality-Based Sparse Attention\nLocality-based sparse attention focuses on the sequence elements\nbased on their position relative to the query. Different attention\npatterns have been proposed that allow more ef ﬁcient\ncomputation of attention for longer sequences without\nsacriﬁcing accuracy. Examples of supported attention patterns\ninclude: strided attention (Figure 5C), sliding window attention\n(Figure 5D), dilated sliding window attention (Figure 5E) and\nstrided sliding window attention (Figure 5F). It is also possible to\ncombine masking with any of these attention patterns.\niMTransformer uses a con ﬁgurable AS (as shown in\nFigure 5G), which consists of a circular shift register to store\na predeﬁned attention pattern for determining the crossbar\ncolumns to be activated. The shift register is conﬁgurable to\nimplement different sparsity patterns. We use a 128-bit circular\nshift register to control a 64-column crossbar (Figure 5). Theﬁrst\n64 bits determine the crossbar columns to be activated. The last\n64 bits function as a buffer necessary to implement certain\nattention patterns and masking. The shift register is shifted to\nthe right by 1 bit at every time step. The stored pattern in the AS is\ndifferent for each attention pattern, as shown inFigure 5G. The\nAS has all 1’s in its register for the full attention pattern. The\nstrided attention activates every otherc column. For the 128-bit\ncircular shift register, c must be a factor of 128. The sliding\nwindow width can be conﬁgured by setting the number of 1’s\nstored in the AS.\n3.3.2 Content-Based Sparse Attention\nAnother type of sparsity is based on the similarity of the query\nwith the keys, or content-based sparsity. Content-based sparsity\ncan be implemented using LSH. LSH reduces the number of\nattention computations for CU-K and CU-V. We implemented\nangular LSH using random hyperplanes to represent cosine\ndistance. To implement an LSH-based attention mechanism,\nthe keys need to be hashed to a binary signature, where each\nbit in the signature is hashed using equationH(q) = (sign (q · r)+\n1)/2. The logic“0” or “1” determines if the point is in the left or\nright of the hyperplane, respectively.\nIn iMTransformer, the hash functionH(q) is implemented by\nthe HU using crossbars. The binary signature of the queryH(q)i s\ncompared to the signature of keysH(K), using a CAM-based SU.\nThe SDPA, implemented by the CU-K and CU-V, is only\nimplemented on the keys with m most similar buckets as\nH(q). By only focusing on the most similar keys and values\nwith the query, we reduce the required multiplications that are\nmore computationally expensive. The hashing function\nintroduces an additional latency and energy overhead when\ncomputing the signature. However, it can reduce the softmax\ncomputations and value comparisons as the sequence length\nincreases. Thus, LSH-based sparsity is only beneﬁcial when the\ncomputational savings associated with avoiding comparisons\nwith all key-value pairs exceeds the hashing overhead. A study\nof the effect of the hashing overhead and the effect of increasing\nsequence length is later explained inSection 4.3.4.\n3.4 Device-Level Mapping\nDifferent devices have different properties that make them ideal\nfor certain operations. CMOS devices, for example, are ideal when\nthe devices need to perform a large number of write operations\nbecause of their low write energy and high endurance. However,\nCMOS devices also have high leakage power, making them\nundesirable when weights must be stored for extended periods.\nAlternatively, FeFETs, are non-volatile and have low leakage\npower. However, FeFETs have much lower endurance\ncompared to CMOS devices. Thus, CMOS devices are better\nfor operations tasks that require a high number of write\noperations, while FeFETs are better when there are minimal\nwrites and non-volatility is important.\nTo determine which devices are goodﬁts for iMTransformer,\nwe examine the usage of IMC kernels for iMTransformer.\niMTransformer employs crossbars for two different reasons: (i)\nto store attentional (in PUs and AU) and feedforward weights (in\nFF Tiles), and (ii) to serve as attentional caches to store\nintermediate activation (in CUs). The PUs, AU, FF Tiles, and\nCUs perform different operations and require different\nproperties. The weights in PU, AU, and FF Tiles, once trained,\ndo not change. Hence, they do not require additional write\noperations and would bene ﬁt from using NVMs such as\nFeFET devices. Alternatively, CUs require frequent write\noperations and would beneﬁt from memories with lower write\ntimes and energy and higher endurance, such as CMOS devices.\nTherefore, iMTransformer employs FeFET-based crossbars for\nattentional and feedforward weights because of the FeFET’s non-\nvolatility and CMOS-based crossbars as attentional caches\nbecause of the CMOS’s low write energy and high endurance.\nThe FF Tiles and AU also use FeFET-based crossbars as they also\ndo not require writes and can beneﬁt from FeFET’s non-volatility.\nFeFET-based crossbars have been proposed inChen X. et al.\n(2018). However, this crossbar design stores binary weights and\nperforms XNOR operations. To utilize FeFET-based crossbars,\nwe use a binary-code-based quantization technique introduced\nfor transformer networks (Chung et al., 2020). The binary-code-\nbased quantization uses non-uniform quantization and decouples\na feature vector into a scaling factor and a binary vector. Scaling\ncan be done in the crossbar’s ADCs, and only bitwise XNOR\noperations are necessary.\n3.5 Summary of Mapping\nTo summarize the mapping of the iMTransformer architecture,\nTable 1shows the functional units of iMTransformer and their\nmapping to circuits and devices. The PUs, HUs, AUs, and FFUs\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706912\nLaguna et al. In-Memory Transformer Network Accelerator\nare implemented using FeFET-based crossbars, while CUs are\nimplemented using CMOS-based crossbars. On the other hand,\nthe SU is implemented using CMOS-based CAM, and AS is\nimplemented using a CMOS-based shifter.\n4 RESULTS AND EVALUATION\nThis section discusses the evaluation of executing transformer\nnetworks using iMTransformer. We follow a bottom-up\napproach beginning with an array-level evaluation inSection\n4.1 and experiment setup inSection 4.2. The latency and energy\nevaluation of using MHA is then shown inSection 4.3. For the\nevaluation in Section 4.3 , we only assume CMOS-based\ncrossbars. An end-to-end accuracy, latency, and energy\nevaluation are then presented in Section 4.4. We also show\nthe effect of using CMOS-FeFET hybrid iMTransformer\nimplementations where CMOS-based crossbars are used as\nattentional cache, and FeFET-based crossbars are used to store\nstatic weights inSection 4.4. Finally, we compare iMTransformer\nusing the MLPerf Inference benchmark for edge devices in\nSection 4.5.\n4.1 Array-Level Evaluation\niMTransformer relies heavily on crossbars and CAMs in\nimplementing transformer networks. We consider crossbars\nand CAMs implemented in a 14 nm technology node. Both\nCMOS and FeFET devices are used for crossbars, while only\nCMOS devices are evaluated for CAMs. We used Neurosim to\nobtain the energy and delay results for reading and writing in\ncrossbar arrays. For the 64 × 64 crossbar arrays, each synapse has\n8-bit precision (8 SRAM cells). Each crossbar has one 8-bit ADC\nper 8 columns, and their overhead is accounted for in the results.\nWe obtained the CAM results using SPICE simulations based on\nthe model used inYin et al. (2017). Based on our estimation, the\ninterconnects between the arrays add about 20% overhead to\nlatency and energy of the overall architecture. We calculated the\ninterconnection delay by estimating the length of the longest wire\nin the interconnections. We used the 22 nm design rules (wire\nwidth and pitch) to calculate the capacitance and resistance of the\nlongest wire. Using the calculated numbers, we simulated the RC\nmodel for the wires and calculated the delay of the longest wire\nusing SPICE simulations.\nTable 2 shows the latency and energy results for 64 × 64\nCMOS-based and FeFET-based crossbars as well as the 64 × 64\nCMOS-based CAM array. The write latency and energy shown\nis a write for a single row, while the read latency and energy\nare for the whole array. CMOS-based crossbars have\nlower latencies than FeFET-based crossbars. Alternatively,\nFeFET-based crossbars have 4.12× lower read energy than\nCMOS-based crossbars. CMOS-based crossbars have faster\nwrites and lower write energy than FeFET-based crossbars.\nHowever, these array-level evaluations do not consider the\nleakage current necessary to store the weights in a CMOS-\nbased crossbar. FeFET-based c rossbars are superior for this\nﬁgure of merit.\n4.2 Experiment Setup\nWe use the following as our baseline hardware for comparison:\nan Intel Core i7-10750H CPU (2.60GHz, 2592 Mhz, six cores)\nwith a Titan RTX GPU (672 GB/s memory bandwidth\nand peak performance of 130 TFLOPS). To measure the\nlatency and energy of the baseline implementation of the\nvanilla transformer ( V a s w a n ie ta l . ,2 0 1 7), we use NVIDIA\nNSight and python line-proﬁler. We usenvidia-smi to collect\nthe operating power while the transformer network runs.\nThe average power is then multiplied by the average latency\nto obtain the energy. We use 8-bit quantization for both\nthe GPU and iMTransformer using quantization-aware\nﬁne-tuning ( Zafrir et al., 2019 ). 8-bit quantization is the\nlowest quantization for both weights and activations and\nachieves acceptable accuracies ( Zafrir et al., 2019 ; Li et al.,\n2020c).\nUsing the python line-proﬁler, we obtain the number of\noperations executed in each stage and map them to the\nTABLE 1 |Summary of hardware mapping of Transformer Network to iMTransformer.\nTransformer network iMTransformer Crossbar CAM Shifter CMOS FeFET\nLinear Unit PU-Q ✓ ✓\nLinear Unit PU-K ✓ ✓\nLinear Unit PU-V ✓ ✓\nAttention Cache CU-V ✓✓\nAttention Cache CU-K ✓✓\nHash Function HU ✓ ✓\nHash Table SU ✓✓\nSparsity AS ✓✓\nLinear Layer AU ✓ ✓\nFeedforward Layer FFU ✓ ✓\nTABLE 2 |Latency and energy array-level results for CMOS and FeFET crossbars\nand CAMs.\nLatency (ns) Energy (fJ)\nCMOS Crossbar Read (whole array) 17.06 78270\nWrite (single row) 1.04 2.5\nFeFET Crossbar Read (whole array) 17.39 1900\nWrite (single row) 175 118\nCMOS CAM Read (whole array) 0.181 441\nWrite (single row) 0.25 1570\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706913\nLaguna et al. In-Memory Transformer Network Accelerator\nnumber of operations in iMTransformer. The total number of\noperations varies depending on the sequence length, the number\nof layers, the embedding size, and the number of heads in the\nMHA. We then calculate the latency and energy of\niMTransformer from the number of operations and the\ncorresponding array level results.\n4.2.1 Transformer Models and Datasets\nIn our experiments, we use three transformer models: the Vanilla\ntransformer, the BERT-base, and BERT-large. The vanilla\ntransformer has six encoder and six decoder layers. The\nvanilla transformer also has 512-dimensional embedding and\neight attention heads in each MHA. The BERT-base has 12\nencoder layers, with each layer having 12 heads in each MHA.\nIn comparison, BERT-large has 24 encoder layers with 16 heads\nin each MHA. The multihead attention model used inSection\n4.4.1 uses a 512-dimensional embedding (similar to the Vanilla\nTransformer). We use the Vanilla transformer and BERT-base\nparameters in Section 4.4.2. The maximum sequence length is\nnot restricted. BERT-base is then used to evaluate the GLUE\ndataset inSection 4.4.3. MLPerf inference edge uses BERT-large,\nhence, it is used when comparing accelerators inSection 4.5.\nWe simulated various sequence lengths by truncating a large\npassages of text to a speciﬁed sequence length to obtain the delay\nand energy metrics inSections 4.3, 4.4.1, 4.4.2.\nSection 4.4.3 uses the General Language Understanding\nEvaluation (GLUE) benchmark, which is a collection of NLP\ndatasets for various NLP tasks. The GLUE dataset is composed of\nthe Stanford Sentiment Treebank (SST-2), the Microsoft\nResearch paraphrase Corpus (MRPC), the Quora Question\nPairs (QQP) dataset, the Semantic Textual Similarity\nbenchmark (STSB), Multi-Genre Natural Language Inference\n(MNLI) Corpus, Question Natural Language Inference (QNLI)\ndataset, Recognizing Textual Entailment (RTE), Winograd\nNatural Language Inference (WNLI) Schema Challenge.\nTo evaluate and compare with other hardware\nimplementations, Section 4.5 uses the setup of MLPerf, an\nindustry-standard machine learning benchmark to evaluate\nhardware devices in a myriad of machine learning tasks.\nMLPerf Inference Edge, in particular, speci ﬁcally evaluates\nmachine learning systems in edge devices during inference.\nOne of the categories in MLPerf Inference Edge is the\nlanguage processing task that uses BERT-large and the\nStanford Question Answering Dataset (SQuAD) 1.1. The\nQNLI dataset of the GLUE benchmark is derived from the\nSQuAD dataset. The SQuAD 1.1 dataset is a reading\ncomprehension dataset consisting of 100k + question and\nanswer pairs where the answers to the questions can be\nobtained from a set of 500 + Wikipedia articles.\n4.3 Multi-Head Attention Evaluation\nThis section presents the multi-head attention evaluation of\niMTransformer compared to the GPU baseline. The input\nsequence length affects the computational demands of\ntransformers. By using crossbars and attention caches,\niMTransformer can improve the execution time of\ntransformers, particularly the MHA, as the sequence length\nincreases (Section 4.3.1). Furthermore, bidirectional MHA can\nbe parallelized to achieve higher speedups (Section 4.3.2). Energy\nconsumption can be further improved by using locality-based\nsparsity (Section 4.3.3) and content-based sparsity ( Section\n4.3.4). We assume a CMOS-based crossbar in this evaluation.\n4.3.1 Effect of Increasing Sequence Length\nWe ﬁrst evaluate the unparalleled MHA with respect to the GPU\nbaseline. The unparallelized MHA implementation reduces the\nmemory transfer overhead via PIM, reduces the number of\ncomputations by caching the keys and values, and increases\nparallelism by using crossbars. Figures 6A – C shows the\nimprovement of implementing MHA in iMTransformer when\nrunning a bidirectional transformer for inference. At a sequence\nlength of n = 64, the standard speedup (shown as red bars in\nFigure 6A) of running the transformer using crossbars is 10.6×.\nThe GPU fully utilizes the memory and compute units at this\nsequence length and achieves optimal performance.\nAs the sequence length n increases, the GPU ’s memory\nbandwidth and the compute limitations are reached. At n =\n256, the execution time of the baseline starts to grow quadratically\nby approximately n\n2/256. By caching the keys and values and\nusing crossbars, iMTransformer only grows linearly. Caching the\nkeys and values reduces the required number of matrix-vector\nmultiplications for W\nQ, WK, and WV by n, thus reducing the\ndelay byn. Since the PUs contains static weights, their execution\ntime still grows byn, as we need to compute the projection of the\nq′, K′, andV′ for each segment. The number of required columns\nincreases forK while the number of required rows increases forV\nas n increases. Since the columns in the crossbars can be treated\nindependently, ⌈n/64⌉ 64 × 64 arrays can be used to compute the\nmatrix-vector multiplications for K in parallel. Increasing the\nnumber of rows in V requires adding partial sums from the\ncrossbars, incurring additional latency. This additional latency,\nhowever, is still not signiﬁcant atn = 4096. Hence, the complexity\nof iMTransformer is O(n). Thus, iMTransformer achieves a\nspeedup close to n/256 compared to a GPU-based solution as\nshown by the red bars inFigure 6A.\n4.3.2 Improvement due to Model Parallelism\nFor the encoder layers, latency can be improved by introducing\nmore parallelismvia duplicating the crossbars in each head byp\n(Section 3.2.2). The projection operation is parallelized but is\nbottlenecked by the write operation. SDPA, on the other hand,\nresults in p× additional improvement in the execution time. By\nusing crossbars and parallelization (p = n), we can reduce the time\ncomplexity of the projection to O(n) and SDPA to O (1).\nAccounting for the projections and SDPA, the latency\nimprovement is shown as yellow bars inFigure 6A.\nMemory and power, however, are bounded. As an example, we set\nt h em e m o r ys i z eo fe a c ha t t e n t i o nh e a dt o1 5 M B ,s h o w na sb l u eb a r s\nin Figure 6Aas bidirectional MHA BM (Bounded Memory). Once\nthe memory limit of iMTransformer is reached, the execution time\nreturns toO (n/p), where p i st h en u m b e ro fA HM a t su s e dt o\nrepresent a single attention head to achieve attention-level parallelism\nthrough duplication. Atn = 64, this speedup equates to 682× speedup\n(in which ≈ 10 × speedup is from computing in-memory in\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706914\nLaguna et al. In-Memory Transformer Network Accelerator\ncrossbars, and around≈ 64 × speedup from parallelization). Atn =\n4096, a speedup ofﬁve orders of magnitude is achieved (×10 from the\nstandard implementation, 100× from duplicating the crossbars, and\n100× from attention duplication).\nHowever, duplicating crossbars increases the number of writes\nand requires communication between AH Mats, resulting in\nlower energy improvements. In the parallel scenario\n(Figure 6B), increasing attention-level parallelism with respect\nto sequence lengthn also requiresn× more writes (becausep = n),\nresulting in a signiﬁcant degradation in energy improvement. If\nn < p, onlyn× more writes are required. Ifn ≥ p, there arep× more\nwrites. The memory requirement also increases by a factor ofn.\nHaving more parallelism results in a higher speedup but lower\nenergy gains. However, perFigure 6C, increased parallelism translates\nto better EDP owing to the exponential decrease of delay\nimprovement despite the linear increase in energy improvement.\n4.3.3 Improvement due to In-Memory Locality-Based\nSparse Attention\nDifferent transformer models have used different attention\npatterns to reduce the space and computational complexity of\nthe transformer networks. In iMTransformer, the computation of\nattention scores is highly parallelized. Therefore changing the\nattention pattern does not reduce latency. However, because of\nfewer parallel computations, energy consumption is reduced.\nFigure 6D shows the energy improvement of using full,\nmasked, strided, sliding window, dilated, and sliding window +\nstrided attention patterns as compared to the full attention pattern\nimplementation in the GPU.\nCompared to the full MHA (red bars in Figure 6D), the\nmasked MHA (orange bars inFigure 6D) reduces the number of\nrows in CU-V that needs to be queried by 2× regardless of the\nsequence length. This is equivalent to turning off the ADCs in\nCU-K. Masked MHA achieves a speedup of 1.9× and 2×\ncompared to the full SDPA at a sequence length 512 and\n4096, respectively. The energy consumption of the strided\nwindow MHA (shown as yellow bars inFigure 6D) depends\non the stride length. (In our example, it is 4). Hence, it can achieve\nup to 4× energy improvement than the full MHA. For sequence\nlengths of 512 and 4096, sliding window attention (shown as\ngreen bars inFigure 6D) achieves energy improvements of 3.43×\nand 3.91×, respectively. The sliding window MHA achieves\nhigher energy improvement as the sequence length increases.\nThe sliding window MHA only focuses on w sequence\nelements for each iteration, where w is the sliding window\nlength. Hence, the sliding window attention increases linearly\nas the sequence length increases. Compared to the full MHA,\nsliding window MHA results in an energy improvement of 13.92×\nat a sequence length equal to 512, and energy improvement of\n106× at a sequence length equal to 4096. As some sequence\nelements are skipped, the dilated sliding window has a small\nimprovement against the sliding window MHA and achieves\nenergy improvements of 15.51× and 118.14× for sequence lengths\nof 512 and 4096, respectively. Strided MHA consumes more\nenergy than the sliding window MHA and dominates the sliding\nwindow + strided MHA. The dilated sliding window attention\npattern shows the highest energy improvement, followed by the\nsliding window attention pattern among the six different patterns\nFIGURE 6 |Comparison between CMOS-based iMTransformer and GPU Baseline in terms of the speedup(A), energy(B) and EDP(C) evaluation results for multi-\nhead attention (with and without parallelism) for varying sequence length. Energy improvements of MHA without parallelism for locality-based(D) and content-based(E)\nsparsity.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706915\nLaguna et al. In-Memory Transformer Network Accelerator\nshown. However, as stated before, which attention pattern should\nbe used is application-dependent.\n4.3.4 Improvement due to In-Memory Content-Based\nSparsity\nThe accuracy of using LSH followed by SDP (LSH + SDP) is\ndependent on signature length. We have found that comparable\naccuracies with SDP can be achieved when using a signature\nlength of 1024 bits with ak-nearest neighbor ofk = 16 on the\nGeneral Language Understanding Evaluation (GLUE) dataset\n(Wang et al., 2018). This setup is also similar to using 64\nbuckets of four hashes each or 1024 = 64 · 2\n4 bits for the\nangular LSH used in the Reformer network ( Kitaev et al.,\n2020). A pre-trained BERT (Devlin et al., 2018) is used for\nevaluation. The GLUE dataset is a common benchmark used\nin transformers (Devlin et al., 2018; Lan et al., 2019).\nSince we are only appending the LSH in a pre-trained network,\nan 11% (1.11×) increase in delay is incurred because of the CAM\nsearch. LSH reduces the number of dot-products required as the\nsequence length increases. Latency overhead still outweighs\nlatency improvement due to fewer attention computations that\nmust be performed at a sequence length ofn = 4096. However, as\nCAM searches require fewer numbers of bits and are more\nenergy-efﬁcient than searching via dot-product on crossbars,\nthis results in an exponential improvement in energy as the\nsequence length increases ( Figure 6E). The bars below the\ndashed line (speedup = 1) have worse execution times,\nrepresenting a slowdown. LSH is only advisable for longer\nsequence lengths (greater than 256).\n4.4 End-to-End Evaluation\nWe evaluate the end-to-end accuracy, delay, and energy of\niMTransformer in implementing the transformer network.\nSection 4.4.1 considers the end-to-end energy and delay\nevaluation of iMTransformer. Section 4.4.2 discusses the\nimprovement in energy consumption by iMTransformer using\nFeFET-based crossbars for high-read operations, and CMOS-\nbased crossbars for high-write operations. Finally,Section 4.4.3\nreports the effect of device-to-device resistance variation on\napplication-level accuracy.\n4.4.1 Energy and Delay Evaluation\nFigures 7A,B shows the delay and energy improvement of\nfeedforward and MHA with parallelism and LSH enhancements\non the Vanilla and BERT-based transformer at sequence lengthsn =\n512 andn = 4096. The standard implementation (without attention-\nlevel parallelism) achieves a speedup of 16× and 6.4× for the vanilla\ntransformer and BERT, respectively, whenn =5 1 2 .T h i si n c r e a s e st o\n305.6× and 167.2× atn = 4096. For the vanilla transformer, only the\nencoder layers can be parallelized, as opposed to bidirectional\ntransformers such as BERT, where all layers can be parallelized.\nTherefore, as the sequence length increases, the speedup gained from\nattention parallelization is smaller for vanilla transformers than\nbidirectional transformers. LSH slows down the iMTransformer as\nshown in Figure 7Abecause of the additional hashing operation\nachieving a 456× speedup for Vanilla transformer and 39.49K×\nspeedup for BERT forn =4 0 9 6 .\nThe energy improvement of the standard implementation of\niMTransformer for the vanilla transformer is 16.84× atn = 512.\nThis increases to 56.78× atn = 4096 because of fewer number of\ncomputations due to caching compared to the GPU\nimplementation. For the standard implementation of BERT,\nthe iMTransformer has an energy improvement of 6.78× at\nn = 512 which increases to 31.16 × n = 4096. The parallel\nimplementation is faster than the standard implementation but\nrequires hardware duplication, thus, consuming more energy.\nLSH improves the energy consumption to 40.18× atn = 4096\ncompared to the GPU implementation.\nBy including layer normalization as shown in Figure 7C,\niMTransformer can achieve a delay improvement of 9.01× for\nVanilla Transformer atn = 512, 13.71× for BERT atn = 512, 76×\nfor Vanilla Transformer atn = 4096 and 199× for BERT atn =\n4096. iMTransformer also achieves an energy improvement of\n5.83× for Vanilla Transformer atn = 512, 1.49× for BERT atn =\n512, 60.4× for Vanilla Transformer atn = 4096 and 33.6× for\nBERT at n = 4096. Algorithmic replacements for layer\nnormalization that are more hardware friendly are needed to\nimprove the iMTransformer accelerator further.\n4.4.2 Improvement due to Using Emerging Technology\nBased on the data inTable 2, CMOS-based crossbars are 1.02×\nfaster than FeFET-based crossbar. On the other hand, FeFET-\nbased crossbars have 4.12× lower read energy than CMOS-based\ncrossbars. CMOS-based crossbars have faster writes and lower\nwrite energy than FeFET-based crossbars. As discussed before,\nCMOS crossbars also have higher endurance than FeFET-based\ncrossbars. Because of this, we utilize CMOS-based crossbars for\ncrossbars that require a high number of write operations and\nFeFET crossbars for crossbars that do not require write\noperations. In the CMOS-FeFET hybrid iMTransformer,\nCMOS is used for attentional caches, while FeFETs are used to\nstore trained weights.\nFigure 7D shows the end-to-end latency and energy\nimprovements of iMTransformer using CMOS, FeFET and\nCMOS + FeFET hybrid iMTransformer implementations when\nimplementing the Vanilla Transformer atn = 512. The CMOS-\nbased iMTransformer achieves an end-to-end speedup and\nenergy improvement of 9.01× and 5.83×, respectively. In\ncontrast, the FeFET-based iMtransformer achieves 4.68× end-\nto-end latency and 12.03× end-to-end energy improvements. The\nCMOS-based iMTransformer performs 1.93× faster than the\nFeFET-based iMTransformer. However, the FeFET based\niMTransformer has 2.06× lower energy consumption than the\nCMOS-based transformer.\nFor the CMOS + FeFET hybrid iMTransformer\nimplementation, we can achieve an 8.96× speedup, which is\nclose to the latency improvement of the CMOS\niMTransformer implementation (9.01×). The CMOS + FeFET\nhybrid iMTransformer implementation achieves a 12.57× energy\nimprovement, higher than either the CMOS or the FeFET\niMTransformer implementation. This is because we utilize\nCMOS crossbars (which are more energy ef ﬁcient for write\noperations) for attentional caches and FeFET crossbars (which\nare more energy efﬁcient for read operations) for storing trained\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706916\nLaguna et al. In-Memory Transformer Network Accelerator\nweights. Thus the CMOS + FeFET iMTransformer\nimplementation exhibits a much better energy-delay-product\nthan using a CMOS or FeFET iMTransformer\nimplementation alone.\n4.4.3 Accuracy Evaluation\nDevice programming methods for writing speci ﬁcv a l u e st o\nFeFETs can result in different stochastic variations of the stored\nresistance in crossbars. We evaluate the effect of the resistance\nvariation in the crossbar array on the accuracy of the GLUE dataset\nusing the BERT-base model. Crossbar resistance variation is\nusually modelled as a log-normal distribution (Li P. et al., 2021;\nLastras-Montaño et al., 2021) R\nd2d = Ridealeθ where θ /equals N(0, σ2).\nThe resistance variation of crossbars can affect the accuracy of\nimplementing the transformer network in iMTransformer.\nFigure 7E shows the effect of the resistance variation on the\nGLUE benchmark. Atσ < 0.3 (equivalent to 12.7% either below\nor above the mean), we achieve iso-accuracy for almost all the\ndatasets in the GLUE benchmark. The accuracy then continues to\ndrop untilσ > 0.5 or at 19.15% either below or above the mean. This\nmeans that the maximum allowable resistance variation in the\ncrossbar must be below ±12.7%.\n4.5 Comparison With Other Accelerators\nTo provide a comparison with GPU baselines and PIM-based\naccelerators for transformer networks, we use the MLPerf\nInference Single Stream setup. The MLPerf uses BERT-Large\nand the SQuAD 1.1 dataset. The SQuAD 1.1 dataset has a\nmaximum sequence length of 384, with most of the questions/\nanswers in the dataset in the 100– 200 sequence length range.\nTable 3 shows the comparison between results from the best\nMLPerf baseline, the GPU baseline ReTransformer, and the\nCMOS-based and CMOS-FeFET hybrid Transformer baselines.\nThe MLPerf Inference Edge results represent the best Single\nStream (batch size of one) results\n1 as of the time of this\nwriting. It uses Intel® Xeon® Platinum 8358, NVIDIA A100-\nSXM-80GB using TensorRT 8.0.1 and CUDA 11.3. The Titan X\nGPU (the GPU used in this paper as the main baseline)\nthroughput is obtained by averaging all dataset samples. The\nReTransformer results are obtained from the best latency\nperformance (81.85 GOps/s), and energy efﬁciency (467.68/s/\nW) reported inYang X. et al. (2020). The iMTransformer results\nare obtained by the hardware implementation for BERT-Large\nmodel parallelization and attention caching. iMTransformer did\nnot use sparsity because the SQuAD 1.1 dataset has short\nsentences (low sequence length); hence, sparsity will not\nimprove the results signiﬁcantly.\nAs shown inTable 3, the MLPerf Inference Edge achieves an\naverage throughput of 649.35 samples/sec. The Titan X GPU\nbaseline has a slower GPU and slower memory bandwidth\nleading to a 204.84 samples/sec throughput. The\nReTransformer also has smaller operations/second than\nMLPerf GPU (A100) and Titan X GPU hence the smaller\nthroughput of 2.73 samples/sec. However, this implementation\nfor ReTransformer is not optimized for BERT and does not utilize\nthe available parallelism of Transformers. The iMTransformer\nuses model parallelization and attention caching which are not\npresent in the ReTransformer. The iMTransformer achieves a\nthroughput of 2.25 K for the CMOS iMTransformer\nFIGURE 7 |Latency (A) and energy(B) improvements of evaluating the MHA with and without model parallelism (MP) and sparsity and Feedforward units of Vanilla\ntransformers and BERT with sequence length of 512 and 4096 using CMOS-based iMTransformer compared to GPU Baseline. End-to-end improvement(C) of CMOS-\nbased iMTransformer for Vanilla transformers and BERT with sequence length of 512 and 4096 compared to GPU baseline(D) End-to-end improvement of\niMTransformer for Vanilla transformers with sequence length of 512 using CMOS, FeFET and CMOS + FeFET iMTransformer implementation compared to the GPU\nbaseline. (E) Effects of crossbar resistance variation on the accuracy the GLUE dataset.\n1Results taken 15 February 2022\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706917\nLaguna et al. In-Memory Transformer Network Accelerator\nimplementation and 2.23 K for the CMOS-FeFET hybrid\niMTransformer implementation, which are about 11× better\nthan Titan X and 3.43× improvement over the current MLPerf\nstate of the art results.\nThe energy results are summarized inTable 3. The energy\nresults are not available for the MLPerf Inference Edge baseline\nand hence are not reported inTable 3. The Titan X does not\nachieve its peak compute utilization and uses an average dynamic\npower of 13 W (peaking at 35 W) to run the transformer network,\ngiving a throughput/W of 15.76 samples/s/W and throughput/J of\n3.23 K samples/s/J as shown inTable 3iMTransformer reduces\nthe memory transfer overhead by using in-memory computing\ncompared to Titan X and reduces the computationvia attention\ncaching compared to ReTransformer. iMTransformer achieved\naround 1.5× energy improvement compared to Titan X and\nReTransformer. The CMOS-FeFET hybrid iMTransformer\nachieves an additional 5.31× energy improvement leading to\n125 samples/s/W. Given in-memory computing, model\nparallelization, attention caching, and using a CMOS-FeFET\nhybrid iMTransformer implementation, we achieve 278K\nthroughput/J, which is 5.26× better than the CMOS\niMTransformer and 86× better than the Titan X (the\nbaseline GPU).\n5 DISCUSSION\nIn this work, we introduced iMTransformer, an in-memory\ncomputing-based transformer network accelerator\n(iMTransformer) that uses FeFET-based and CMOS-based\ncrossbars and CAMs to accelerate transformer networks,\nspeciﬁcally the multi-head attention computations.\niMTransformer achieves latency and energy improvements by (1)\nreducing the memory bottleneckvia computing-in-memory, (2)\nreducing the number of computations by storing reusable data in\ncrossbars, and (3) maximizing the parallelism for bidirectional MHA\nand encoder-decoder MHA. Computing-in-memory alleviates the\nmemory transfer bottleneck by reducing the need to move data from\nthe memory to the compute unit and vice versa. By using crossbars as\nattentional caches, we can keep data computed from previous time\nsteps to be reused for succeeding time steps. Finally, different types of\nMHA have parallelism characteristics that can be exploited, such as\nthe bi-directionality of encoder MHAs and parallelizing the\ncomputation of encoder-decoder keys and values across different\nlayers.\nFurthermore, iMTransformer improves energy ef ﬁciency\nby (1) using an attention selector, (2) exploiting content-\nbased sparsity using CAMs, and (3) using CMOS and FeFET\ndevices. The attention selector allows masking and locality-\nbased sparse attention. The attention selector reduces the\nnumber of computations and activation of unnecessary\nADCs based on the attention patterns. CAMs are employed\nto implement LSH for realizing content-based sparsity.\nThough the LSH computations incur latency overhead, they\nsigni ﬁcantly reduce energy consumption as the sequence\nlength increases. Finally, non-volatile FeFET-based\ncrossbars are used for crossbars that involve highly\nfrequent read operations in the feedforward tile and\nprocessing unit. Because of their high write requirements,\nCMOS-based crossbars are used for the attentional\ncaching units.\nFor the Vanilla Transformer at sequence length of 512,Figure 8A\nshows a delay improvement of 7.7× for the standard implementation\ncompared to the GPU baseline. Including model parallelization\nf u r t h e ri m p r o v e st h ee n d - t o - e n dd e l a yb y1 . 1 7 ×o ra ne n d - t o - e n d\nimprovement of 9.01× compared to the GPU baseline. Introducing\nsparsity does not improve the delay while using the CMOS-FeFET\nhybrid iMTransformer reduces the end-to-end improvement to\n8.96×. The end-to-end energy improvement of the standard\nimplementation is 7.81× compared to the GPU implementation.\nBecause of the increase in the writeoperations, the energy is reduced\nto 5.71× and slightly improves with sparsity at 5.83× compared to the\nGPU baseline. The standard implementation achieves a 60.17× EDP\nimprovement while adding all the enhancements improves the EDP\nby 112.66× compared to the GPU.\nFigure 8Bshows the delay, energy, and EDP improvement of\nBERT for a sequence length of 512. The standard implementation\nof iMTransformer achieves a 4.68× energy improvement\ncompared to the GPU implementation. Because of its bi-\ndirectional nature, BERT greatly bene ﬁts from model\nparallelism and achieves 13.76× energy improvement (a 2.94×\nincrease) compared to the GPU baseline. Introducing sparsity\nand using the hybrid iMTransformer slightly reduces the\nimprovement to 13.71×. The standard implementation has an\nenergy improvement of 4.78×. Because of the additional energy\ndue to writes and greater utilization of model parallelism, the\nTABLE 3 |Comparison of the leading MLPerf Inference - Edge results, the Titan X baseline, ReTransformer and the CMOS-based and Hybrid iMTransformer using MLPerf\nsetting. MLPerf uses BERT-large and the SQuAD 1.1 dataset with a maximum sequence length of 384.\nAccelerator Throughput (Samples/ s) Throughput/W (Samples/ s/W) Throughput/J (Samples/ s/J)\nMLPerf (Best delay) 649.35 Not available Not available\nTitan X (Baseline) 204.84 15.76 3.23 K\nReTransformer 2.73 15.60 42.59\niMTransformer-CMOS 2.25 K 23.48 52.83 K\niMTransformer-Hybrid 2.23 K 124.8 278 K\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706918\nLaguna et al. In-Memory Transformer Network Accelerator\nimplementation with model parallelism achieves a 1.78×\nimprovement while adding sparsity further reduces the\nimplementation to 1.49×. However, introducing the use of\nhybrid transformer improves the energy to 8.95× when\ncompared to the GPU. The standard implementation achieves\na 22.34× EDP improvement while adding all the enhancements\nimproves the EDP to 122.65× compared to the GPU baseline.\nFor both Vanilla and BERT, introducing model parallelization\nimproves the delay. However, because BERT is an encoder-type\ntransformer, it can beneﬁt from model parallelization more than the\nVanilla transformer. However, model parallelization comes at the\ncost of more energy usage. Using FeFET devices for static weights\nslightly slows down the iMTransformer but greatly improves the\nenergy. The results inFigure 8are only for a sequence length of 512.\nGreater improvements can be achieved at longer sequence lengths.\nUsing the MLPerf benchmark, the hybrid iMTransformer can query\n2.23 k samples/sec at 125 samples/s/W as shown inTable 3.\nIn this paper, we have introduced iMTransformer, an in-\nMemory computing Transformer Network accelerator. We\nhave been able to greatly improve the delay and energy\nconsumption of the transformer ’s attention mechanism and\nfeedforward layers. Transformer networks are evolving, and\ndifferent variants for various applications have been developed\nin the past few years. Transformer network accelerators must\nadapt to different types of attention mechanisms and transformer\nnetwork conﬁguration. This work is catered to the original\ntransformer network, where the input is a sequence. This\nmakes transformer networks ideal for NLP applications.\nBecause of the transformer ’s ability to model long-range\ndependencies, the sequence length can continue to increase\nand require more sophisticated algorithms and accelerators.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. This data\ncan be found here: https://gluebenchmark.com/, https://\nrajpurkar.github.io/SQuAD-explorer/.\nAUTHOR CONTRIBUTIONS\nAL performed the application level and architectural simulations\nand was the primary proponent of the conception of the idea. MS\nperformed the simulations for the peripherals and interconnects.\nAK performed the crossbar simulations. XY performed the CAM\nsimulations. MN and XH supervised all experiments. AL wrote\nthe manuscript with input from all authors. All authors reviewed\nthe document.\nFUNDING\nThis work was supported in part by ASCENT, one of six centers\nin JUMP, a Semiconductor Research Corporation (SRC) program\nsponsored by DARPA. The funder was not involved in the study,\ndesign, collection, analysis, interpretation of data, the writing of\nthis article or the decision to submit it for publication.\nREFERENCES\nBeltagy, I., Peters, M. E., and Cohan, A. (2020).Longformer: The Long-Document\nTransformer.\nB e y e r ,S . ,D u n k e l ,S . ,T r e n t z s c h ,M . ,M u l l e r ,J . ,H e l l m i c h ,A . ,U t e s s ,D . ,e ta l .\n(2020). FeFET: A Versatile CMOS Compatible Device with Game-Changing\nPotential.\nBoes, W., and Van hamme, H. (2019).“Audiovisual Transformer Architectures for\nLarge-Scale Classiﬁcation and Synchronization of Weakly Labeled Audio\nEvents,” in Proceedings of the 27th ACM International Conference on\nMultimedia. Editors L. Amsaleg, B. Huet, M. A. Larson, G. Gravier,\nH. Hung, C.-W. Ngo, et al. (New York, NY, USA: Association for\nComputing Machinery), 1961– 1969. doi:10.1145/3343031.3350873\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., et al.\n(2020). Language Models Are Few-Shot Learners.\nChallapalle, N., Rampalli, S., Jao, N., Ramanathan, A., Sampson, J., and Narayanan,\nV. (2020). FARM: A Flexible Accelerator for Recurrent and Memory\nAugmented Neural Networks. J. Sign Process. Syst. 92, 1247– 1261. doi:10.\n1007/s11265-020-01555-w\nChen, P.-Y., Peng, X., and Yu, S. (2018a). NeuroSim: A Circuit-Level Macro Model\nfor Benchmarking Neuro-Inspired Architectures in Online Learning. IEEE\nTrans. Comput.-Aided Des. Integr. Circuits Syst.37, 3067– 3080. doi:10.1109/\ntcad.2018.2789723\nChen, X., Yin, X., Niemier, M., and Hu, X. S. (2018b).“Design and Optimization of\nFeFET-Based Crossbars for Binary Convolution Neural Networks,” in 2018\nDesign, Automation Test in Europe Conference Exhibition (DATE) (IEEE),\n1205– 1210. doi:10.23919/date.2018.8342199\nFIGURE 8 |Delay, Energy and EDP improvement for the Vanilla Transformer(A) and BERT (B) with sequence length of 512 with respect to additional\nenhancement: Model Parallelism (MP), Locality Sensitive Hashing (LSH) and using FeFET devices.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706919\nLaguna et al. In-Memory Transformer Network Accelerator\nChild, R., Gray, S., Radford, A., and Sutskever, I. (2019).Generating Long Sequences\nwith Sparse Transformers.\nChua-Chin Wang, C., Chia-Hao Hsu, C., Chi-Chun Huang, C., and Jun-Han\nWu, J. (2010). A Self-Disabled Sensing Technique for Content-Addressable\nMemories. IEEE Trans. Circuits Syst. 57, 31 – 35. doi:10.1109/tcsii.2009.\n2037995\nChung, I., Kim, B., Choi, Y., Kwon, S. J., Jeon, Y., Park, B., et al. (2020).“Extremely\nLow Bit Transformer Quantization for On-Device Neural Machine\nTranslation,” in Findings of the Association for Computational Linguistics:\nEMNLP 2020 (Online: Association for Computational Linguistics),\n4812– 4826. doi:10.18653/v1/2020.ﬁndings-emnlp.433\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. (2019).\nTransformer-XL: Attentive Language Models beyond a Fixed-Length Context.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An Image Is worth 16x16 Words: Transformers for Image\nRecognition at Scale\nFedus, W., Zoph, B., and Shazeer, N. (2021). Switch Transformers: Scaling to\nTrillion Parameter Models with Simple and Efﬁcient Sparsity.\nGokmen, T., and Vlasov, Y. (2016). Acceleration of Deep Neural Network Training\nwith Resistive Cross-Point Devices: Design Considerations.Front. Neurosci.10,\n333. doi:10.3389/fnins.2016.00333\nHuangfu, W., Li, S., Hu, X., and Xie, Y. (2018).“RADAR: A 3D-ReRAM Based\nDNA Alignment Accelerator Architecture,” in Proceedings of the 55th Annual\nDesign Automation Conference (New York, NY, USA: Association for\nComputing Machinery), 1– 6. Article 59 in DAC ’18. doi:10.1109/dac.2018.\n8465882\nJeloka, S., Akesh, N. B., Sylvester, D., and Blaauw, D. (2016). A 28 Nm Conﬁgurable\nMemory (TCAM/BCAM/SRAM) Using Push-Rule 6T Bit Cell Enabling Logic-\nIn-Memory. IEEE J. Solid-state Circuits51, 1009– 1021.\nJerry, M., Dutta, S., Kazemi, A., Ni, K., Zhang, J., Chen, P.-Y., et al. (2018). A\nFerroelectric Field Effect Transistor Based Synaptic Weight Cell.J. Phys. D Appl.\nPhys. 51, 434001.\nJunczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T., Hoang, H., Heaﬁeld, K.,\nNeckermann, T., et al. (2018). Marian: Fast Neural Machine Translation\nin C++.\nKaiser, L., Nachum, O., Roy, A., and Bengio, S. (2016).“Learning to Remember\nRare Events,” in 5th International Conference on Learning Representations.\nKang, W., Wang, H., Wang, Z., Zhang, Y., and Zhao, W. (2017). In-Memory\nProcessing Paradigm for Bitwise Logic Operations in STT-MRAM.IEEE Trans.\nMagn. 53, 1– 4. doi:10.1109/tmag.2017.2703863\nKaplan, R., Yavits, L., and Ginosar, R. (2018).RASSA: Resistive Pre-alignment\nAccelerator for Approximate DNA Long Read Mapping,4 4– 54.\nKaram, R., Puri, R., Ghosh, S., and Bhunia, S. (2015). Emerging Trends in Design\nand Applications of Memory-Based Computing and Content-Addressable\nMemories. Proc. IEEE 103, 1311– 1330. doi:10.1109/jproc.2015.2434888\nKazemi, A., Sahay, S., Saxena, A., Shari\nﬁ, M. M., Niemier, M., and Sharon Hu, X.\n(2021a). A Flash-Based Multi-Bit Content-Addressable Memory with\nEuclidean Squared Distance.\nKazemi, A., Shariﬁ, M. M., Laguna, A. F., Müller, F., Rajaei, R., Olivo, R., et al.\n(2020). Memory Nearest Neighbor Search with FeFET Multi-Bit Content-\nAddressable Memories.\nKazemi, A., Shariﬁ, M. M., Zou, Z., Niemier, M., Hu, X. S., and Imani, M. (2021b).\n“MIMHD: Accurate and Efﬁcient Hyperdimensional Inference Using Multi-Bit\nIn-Memory Computing,” in 2021 IEEE/ACM International Symposium on Low\nPower Electronics and Design (ISLPED),1 – 6. doi:10.1109/islped52811.2021.\n9502498\nKitaev, N., Kaiser, L., and Levskaya, A. (2020). “Reformer: The Ef ﬁcient\nTransformer,” in 8th International Conference on Learning Representations.\nKohonen, T. (2012). Associative Memory: A System-Theoretical Approach, 17.\nSpringer Science & Business Media.\nLaguna, A. F., Gamaarachchi, H., Yin, X., Niemier, M., Parameswaran, S., and Hu,\nX. S. (2020). “Seed-and-Vote Based In-Memory Accelerator for DNA Read\nMapping,” in 2020 IEEE/ACM International Conference on Computer Aided\nDesign,1 – 9. doi:10.1145/3400302.3415651(ICCAD)\nLaguna, A. F., Yin, X., Reis, D., Niemier, M., and Hu, X. S. (2019b).“Ferroelectric\nFET Based In-Memory Computing for Few-Shot Learning,” in Proceedings of\nthe 2019 on Great Lakes Symposium on VLSI. Editors H. Homayoun, B. Taskin,\nT. Mohsenin, and W. Zhao (New York, NY, USA: Association for Computing\nMachinery), 373– 378. GLSVLSI ’19. doi:10.1145/3299874.3319450\nLaguna, A., Niemier, M., and Hu, X. S. (2019a).“Design of Hardware-Friendly\nMemory Enhanced Neural Networks,” in Design, Automation Test in Europe\nConference Exhibition (DATE), 2017 . Editors J. Teich and F. Fummi,\n1583– 1586. ieeexplore.ieee.org. doi:10.23919/date.2019.8715198\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. (2019).\n“ALBERT: A Lite BERT for Self-Supervised Learning of Language\nRepresentations,” in ICLR.\nLastras-Montaño, M. A., Del Pozo-Zamudio, O., Glebsky, L., Zhao, M., Wu, H.,\nand Cheng, K.-T. (2021). Ratio-based Multi-Level Resistive Memory Cells.Sci.\nRep. 11, 1351. doi:10.1038/s41598-020-80121-7\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., et al.\n(2019). BART: Denoising Sequence-To-Sequence Pre-training for Natural\nLanguage Generation, Translation, and Comprehension.\nLi, C., Graves, C. E., Sheng, X., Miller, D., Foltin, M., Pedretti, G., et al. (2020a).\nAnalog Content-Addressable Memories with Memristors.Nat. Commun. 11,\n1638. doi:10.1038/s41467-020-15254-4\nLi, H., Chen, W.-C., Levy, A., Wang, C.-H., Wang, H., Chen, P.-H., et al. (2021a).\nSAPIENS: A 64-kb RRAM-Based Non-volatile Associative Memory for One-\nShot Learning and Inference at the Edge.IEEE Trans. Electron. Devices 68,\n6637– 6643. doi:10.1109/ted.2021.3110464\nLi, P., Song, G., Cai, K., and Yu, Q. (2021b). Across-Array Coding for Resistive\nMemories with Sneak-Path Interference and Lognormal Distributed Resistance\nVariations. IEEE Commun. Lett. 25, 3458– 3462. doi:10.1109/lcomm.2021.\n3111218\nLi, Z., Li, Z., Zhang, J., Feng, Y., Niu, C., and Zhou, J. (2020b).Bridging Text and\nVideo: A Universal Multimodal Transformer for Video-Audio Scene-Aware\nDialog.\nLi, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., et al. (2020c).“Train Big,\nThen Compress: Rethinking Model Size for Efﬁcient Training and Inference of\nTransformers,” In Proceedings of the 37th International Conference on Machine\nLearning. Editors H. D. Iii and A. Singh, 5958– 5968. (PMLR), vol. 119 of\nProceedings of Machine Learning Research.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021).Swin Transformer:\nHierarchical Vision Transformer Using Shifted Windows.\nMarcus, M., Santorini, B., and Marcinkiewicz, M. A. (1993).Building a Large\nAnnotated Corpus of English: The Penn Treebank.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. (2016). Pointer sentinel Mixture\nModels\nMutlu, O., Ghose, S., Gómez-Luna, J., and Ausavarungnirun, R. (2020).A Modern\nPrimer on Processing in Memory.\nNi, K., Yin, X., Laguna, A. F., Joshi, S., Dünkel, S., Trentzsch, M., et al. (2019).\nFerroelectric Ternary Content-Addressable Memory for One-Shot Learning.\nNat. Electron. 2, 521– 529. doi:10.1038/s41928-019-0321-3\nPrato, G., Charlaix, E., and Rezagholizadeh, M. (2019). Fully Quantized\nTransformer for Machine Translation.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).\nLanguage Models Are Unsupervised Multitask Learners.OpenAI blog 1, 9.\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019).Compressive\nTransformers for Long-Range Sequence Modelling.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., et al. (2019).\nExploring the Limits of Transfer Learning with a Uni ﬁed Text-To-Text\nTransformer, 10683. arXiv preprint arXiv:1910.\nRanjan, A., Jain, S., Stevens, J. R., Das, D., Kaul, B., and Raghunathan, A. (2019).\n“X-MANN: A Crossbar Based Architecture for Memory Augmented Neural\nNetworks,” in Proceedings of the 56th Annual Design Automation Conference\n2019 (New York, NY, USA: Association for Computing Machinery), 1– 6.\nArticle 130 in DAC’19.\nReis, D., Laguna, A. F., Niemier, M., and Hu, X. S. (2020a).“A Fast and Energy\nEfﬁcient Computing-In-Memory Architecture for Few-Shot Learning\nApplications,” in 2020 Design, Automation Test in Europe Conference\nExhibition (DATE) , 127 – 132. ieeexplore.ieee.org. doi:10.23919/date48585.\n2020.9116292\nReis, D., Laguna, A. F., Niemier, M., and Hu, X. S. (2021).Attention-in-Memory for\nFew-Shot Learning with Conﬁgurable Ferroelectric FET Arrays. In Proceedings\nof the 26th Asia and South Paciﬁc Design Automation Conference. New York,\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706920\nLaguna et al. In-Memory Transformer Network Accelerator\nNY, USA: Association for Computing Machinery, 49– 54. ASPDAC’21. doi:10.\n1145/3394885.3431526\nReis, D., Niemier, M., and Hu, X. S. (2018).“Computing in Memory with FeFETs,”\nin Proceedings of the International Symposium on Low Power Electronics and\nDesign (New York, NY, USA: ACM), 1– 6. Article 24 in ISLPED’18. doi:10.\n1145/3218603.3218640\nReis, D., Takeshita, J., Jung, T., Niemier, M., and Hu, X. S. (2020b). Computing-\nin-Memory for Performance and Energy-Ef ﬁcient Homomorphic\nEncryption. IEEE Trans. VLSI Syst. 28, 2300– 2313. doi:10.1109/tvlsi.2020.\n3017595\nRoy, A., Saffar, M., Vaswani, A., and Grangier, D. (2021). Efﬁcient Content-Based\nSparse Attention with Routing Transformers.Trans. Assoc. Comput. Linguistics\n9, 53– 68. doi:10.1162/tacl_a_00353\nRoy, K., Chakraborty, I., Ali, M., Ankit, A., and Agrawal, A. (2020).“In-Memory\nComputing in Emerging Memory Technologies for Machine Learning: An\nOverview,” in 2020 57th ACM/IEEE Design Automation Conference (DAC),\n1– 6. doi:10.1109/dac18072.2020.9218505\nSebastian, A., Le Gallo, M., Khaddam-Aljameh, R., and Eleftheriou, E. (2020).\nMemory Devices and Applications for In-Memory Computing. Nat.\nNanotechnol. 15, 529– 544. doi:10.1038/s41565-020-0655-z\nShaﬁee, A., Nag, A., Muralimanohar, N., Balasubramonian, R., Strachan, J. P., Hu,\nM., et al. (2016).Isaac.SIGARCH Comput. Archit. News44, 14– 26. doi:10.1145/\n3007787.3001139\nShariﬁ, M. M., Pentecost, L., Rajaei, R., Kazemi, A., Lou, Q., Wei, G.-Y., et al.\n(2021). Application-driven Design Exploration for Dense Ferroelectric\nEmbedded Non-volatile Memories.\nSharir, O., Peleg, B., and Shoham, Y. (2020).The Cost of Training NLP Models: A\nConcise Overview.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.\n(2019). Megatron-LM: Training Multi-Billion Parameter Language Models\nUsing Model Parallelism\nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. (2020a).“Sparse Sinkhorn\nAttention,” in International Conference on Machine Learning, 9438– 9447.\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., et al. (2020b).Long\nRange arena: A Benchmark for Efﬁcient Transformers.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention Is All You Need,”.I n Advances in Neural Information\nProcessing Systems. Editors I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, et al. (Long Beach, California: Curran Associates,\nInc.), 30, 5998– 6008.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2018).“GLUE:\nA Multi-Task Benchmark and Analysis Platform for Natural Language\nUnderstanding,” in Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP\n(Brussels, Belgium: Association for Computational Linguistics), 353 – 355.\ndoi:10.18653/v1/w18-5446\nWei, L., Zhang, J., Hou, J., and Dai, L. (2020). Attentive Fusion Enhanced Audio-\nVisual Encoding for Transformer Based Robust Speech Recognition.\nYang, S.-W., Liu, A. T., and Lee, H.-Y. (2020a). Understanding Self-Attention of\nSelf-Supervised Audio Transformers.\nYang, X., Yan, B., Li, H., and Chen, Y. (2020b).“ReTransformer: ReRAM-Based\nProcessing-In-Memory Architecture for Transformer Acceleration, ” in\nProceedings of the 39th International Conference on Computer-Aided\nDesign (New York, NY, USA: Association for Computing Machinery), 1– 9.\nArticle 92 in ICCAD’20.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V. (2019).\nXLNet: Generalized Autoregressive Pretraining for Language\nUnderstandingAdv. Neural Inf. Process. Syst. (Vancouver, Canada: Curran\nAssociates, Inc.) 32, 5753– 5763.\nYen-Jen Chang, Y. (2009). A High-Performance and Energy-Efﬁcient TCAM\nDesign for IP-Address Lookup. IEEE Trans. Circuits Syst. 56, 479– 483.\ndoi:10.1109/tcsii.2009.2020935\nYin, X., Li, C., Huang, Q., Zhang, L., Niemier, M., Hu, X. S., et al. (2020). FeCAM: A\nUniversal Compact Digital and Analog Content Addressable Memory Using\nFerroelectric. IEEE Trans. Electron. Devices 67, 2785– 2792. doi:10.1109/ted.\n2020.2994896\nYin, X., Ni, K., Reis, D., Datta, S., Niemier, M., and Hu, X. S. (2019). An Ultra-dense\n2FeFET TCAM Design Based on a Multi-Domain FeFET Model.IEEE Trans.\nCircuits Syst. 66, 1577– 1581. doi:10.1109/tcsii.2018.2889225\nYin, X., Niemier, M., and Hu, X. S. (2017).“Design and Benchmarking of Ferroelectric\nFET Based TCAM,” in Design, Automation Test in Europe Conference Exhibition\n(DATE) (Lausanne, Switzerland), 1444– 1449. doi:10.23919/date.2017.7927219\nYu, S., and Chen, P.-Y. (2016). Emerging Memory Technologies: Recent Trends\nand Prospects.IEEE Solid-state Circuits Mag.8, 43– 56. doi:10.1109/mssc.2016.\n2546199\nZafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M. (2019).Q8BERT: Quantized\n8bit BERT.\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., et al.\n(2020). “Big Bird: Transformers for Longer Sequences,” in NeurIPS.\nZhang, J., Wang, Z., and Verma, N. (2017). In-Memory Computation of a\nMachine-Learning Classiﬁer in a Standard 6T SRAM Array.IEEE J. Solid-\nstate Circuits 52, 915– 924. doi:10.1109/jssc.2016.2642198\nCon\nﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors, and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Laguna, Shariﬁ, Kazemi, Yin, Niemier and Hu. This is an open-\naccess article distributed under the terms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or reproduction in other forums is permitted,\nprovided the original author(s) and the copyright owner(s) are credited and that the\noriginal publication in this journal is cited, in accordance with accepted academic\npractice. No use, distribution or reproduction is permitted which does not comply\nwith these terms.\nFrontiers in Electronics | www.frontiersin.org April 2022 | Volume 3 | Article 84706921\nLaguna et al. In-Memory Transformer Network Accelerator"
}