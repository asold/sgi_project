{
    "title": "Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography",
    "url": "https://openalex.org/W4284890590",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2332008982",
            "name": "Md. Nazmul Islam",
            "affiliations": [
                "BRAC University"
            ]
        },
        {
            "id": "https://openalex.org/A2103495201",
            "name": "Mehedi Hasan",
            "affiliations": [
                "Bangladesh University of Health Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2306080219",
            "name": "Md. Kabir Hossain",
            "affiliations": [
                "Bangabandhu Sheikh Mujib Medical University"
            ]
        },
        {
            "id": "https://openalex.org/A2168073006",
            "name": "Md. Golam Rabiul Alam",
            "affiliations": [
                "BRAC University"
            ]
        },
        {
            "id": "https://openalex.org/A2528834951",
            "name": "Md Zia Uddin",
            "affiliations": [
                "SINTEF Digital",
                "SINTEF"
            ]
        },
        {
            "id": "https://openalex.org/A2007662698",
            "name": "Ahmet Soylu",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2332008982",
            "name": "Md. Nazmul Islam",
            "affiliations": [
                "BRAC University"
            ]
        },
        {
            "id": "https://openalex.org/A2103495201",
            "name": "Mehedi Hasan",
            "affiliations": [
                "Bangladesh University of Health Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2306080219",
            "name": "Md. Kabir Hossain",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2168073006",
            "name": "Md. Golam Rabiul Alam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2528834951",
            "name": "Md Zia Uddin",
            "affiliations": [
                "SINTEF",
                "SINTEF Digital"
            ]
        },
        {
            "id": "https://openalex.org/A2007662698",
            "name": "Ahmet Soylu",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2164249826",
        "https://openalex.org/W2897257410",
        "https://openalex.org/W2914257201",
        "https://openalex.org/W2911788487",
        "https://openalex.org/W2914448549",
        "https://openalex.org/W2786597035",
        "https://openalex.org/W2726542547",
        "https://openalex.org/W6739851908",
        "https://openalex.org/W1822931791",
        "https://openalex.org/W4280597039",
        "https://openalex.org/W4280594600",
        "https://openalex.org/W3193285410",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6819060087",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W4302275239",
        "https://openalex.org/W6635231071",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6600351811",
        "https://openalex.org/W2755984634",
        "https://openalex.org/W3169786763",
        "https://openalex.org/W3081114420",
        "https://openalex.org/W3210945687",
        "https://openalex.org/W2899321136",
        "https://openalex.org/W2964273253",
        "https://openalex.org/W3170903420",
        "https://openalex.org/W2951166644",
        "https://openalex.org/W2790161230",
        "https://openalex.org/W2519616506",
        "https://openalex.org/W2978631110",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2962858109"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports\nVision transformer and explainable \ntransfer learning models for auto \ndetection of kidney cyst, stone \nand tumor from CT‑radiography\nMd Nazmul Islam 1, Mehedi Hasan 2, Md. Kabir Hossain 3, Md. Golam Rabiul Alam 1, \nMd Zia Uddin 4 & Ahmet Soylu 5*\nRenal failure, a public health concern, and the scarcity of nephrologists around the globe have \nnecessitated the development of an AI‑based system to auto‑diagnose kidney diseases. This research \ndeals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered \nand annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an \nAI‑based kidney diseases diagnostic system and contribute to the AI community’s research scope \ne.g., modeling digital‑twin of renal functions. The collected images were exposed to exploratory \ndata analysis, which revealed that the images from all of the classes had the same type of mean \ncolor distribution. Furthermore, six machine learning models were built, three of which are based on \nthe state‑of‑the‑art variants of the Vision transformers EANet, CCT, and Swin transformers, while \nthe other three are based on well‑known deep learning models Resnet, VGG16, and Inception v3, \nwhich were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the \nswin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. \nThe F1 score and precision and recall comparison reveal that the Swin transformer outperforms \nall other models and that it is the quickest to train. The study also revealed the blackbox of the \nVGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and \nInceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior \naccuracy of our Swin transformer‑based model and the VGG16‑based model can both be useful in \ndiagnosing kidney tumors, cysts, and stones.\nKidney disease is a public health concern since the disease is spreading despite current control  attempts1. Chronic \nkidney disease affects more than 10% of the world  population2, and it was ranked 16th among the leading causes \nof death in 2016 and is expected to jump to 5th by  20403. Among the other kidney diseases, cyst formation, \nnephrolithiasis (kidney stone), and renal cell carcinoma (kidney tumor) are the most frequent kidney illnesses \nthat impede kidney function. A kidney cyst is a fluid-filled pocket that forms on the surface of the kidney and \nis enclosed by a thin wall. Within the kidneys, one or more cysts may develop with water density: From 0 to 20 \nHounsfield  units4–6. Kidney stone disease is characterized by the formation of crystal concretions within the \nkidneys, which affects about 12% of the world  population7. Renal cell carcinoma (RCC), often known as kidney \ntumor, is one of the ten most prevalent cancers in the  world8.\nX-ray, computed tomography (CT), B-ultrasound machines (US), and MRI (magnetic resonance imaging) \nmachines are often used in conjunction with pathology tests to diagnose kidney diseases. The CT machine scans \nthe desired part of the human anatomy with X-ray beams to obtain a cross-sectional image which provides \nthree-dimensional information about the desired  anatomy9. CT scans in kidney examinations are ideal for \nstudy because they provide three-dimensional information and slice-by-slice images. If kidney abnormalities \nsuch as cysts, stones, and tumors are not detected and treated early, they might lead to renal  failure10. For this \nOPEN\n1Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh. 2Radiology & Imaging \nTechnology, Bangladesh University of Health Sciences, Dhaka, Bangladesh. 3Department of Nephrology, \nBangabandhu Sheikh Mujib Medical University, Dhaka, Bangladesh. 4Software and Service Innovation, SINTEF \nDigital, Oslo, Norway. 5Department of Computer Science, Norwegian University of Science and Technology, \nGjøvik, Norway. *email: ahmet.soylu@ntnu.no\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nreason, early diagnosis of renal disorders like kidney cysts, stones, and tumors appears to be an important step \nin preventing kidney  failure11.\nOn the other hand, the number of nephrologists and radiologist is very limited. In South Asia, there is barely \none nephrologist per million people, where in Europe there are 25.3 nephrologists per million  people12.\nConsidering the sufferings of the population due to kidney diseases, the shortage of nephrologists and radiolo-\ngists around the globe, and the advancement of deep learning research in vision tasks, it has become imperative \nto build an AI (artificial intelligence) model to detect kidney radiological findings easily to assist doctors, and \nreduce the sufferings of people. A few studies have been published in recent years in this domain. However, the \npublicly available data set is scarce. In addition, most past studies have utilized traditional machine learning \nalgorithms to classify single classes of disease only; either cysts, or either tumors, or either stones. Some studies \nutilised ultrasound (US) images.\nIn this work, we created and annotated the “CT KIDNEY DATASET: Normal-Cyst-Tumor and Stone” \n dataset13, implemented a total of six models, and evaluated each of them to come to the conclusion which model \nis best suited to use in realtime. The proposed auto-detection model for the diagnosis of kidney diseases will also \nhelp to build a digital twin of renal function at the pathology level, such as tumor growth. No study that we are \naware of has done an analysis based on a transformer model with renal cyst, tumor and stone auto detection. \nThe following are the major contributions of this work:\n• A dataset namely “CT KIDNEY DATASET: Normal-Cyst-Tumor and Stone” is collected and annotated with \n12,446 images utilizing the whole abdomen and the eurogram protocol.\n• Three CNN-based deep learning models (i.e., VGG16, Resnet50, and Inception v3) using transfer learning \napproach are applied to detect kidney abnormalities and presented a thorough performance study, including \nexplanation of the black-box of the suggested models using gradient weighted class activation mapping (i.e., \nGradCam).\n• Three recent state-of-the-art Vision transformer variants (i.e., EANet, CCT, and Swin transformers) are \napplied on the CT kidney dataset and the performances of the models are presented using the confusion \nmatrix, accuracy, sensitivity, specificity, and F1 score.\nThe rest of the paper is organized in the following manner. Section II provides background and details on utiliz-\ning deep learning to identify kidney abnormalities. The methodology for this letter is discussed in Section III, \nwhich includes data collection processes, data preprocessing, neural network models employed in this study and \nthe result evaluation processes. Section IV deals with the result study, and the concluding remarks are presented \nin Section V .\nBackground study\nBecause of the advent of deep learning and its implementation in image processing and classification, a consid-\nerable amount of research has grown in deep learning applications, specifically in autodiagnosis of radiological \nfindings and segmentation tasks. In the classification task that employs a transfer learning technique,  ResNet14 \n inception15,  exception16,  EfficientNet17 networks have grown in prominence over time. Transfer learning is an \napproach in deep learning where pre-trained models are used as the starting point for specified tasks. It refers to \nthe application of a previously learnt model to a new challenge. In recent days, popularly used transformer models \nfor natural language processing are being introduced in computer vision tasks, which are showing supremacy \nand good results over other models while doing classification tasks. The Vision transformer (ViT)18 and several \nvariations of the Vision transformer, like the Big Transformer (BiT)19, EANet (External Attention Transformer)20, \nCompact Convolutional Transformer (CCT)21, and Swin Transformer (Shifted Window Transformer)22 are utiliz-\ning attention based mechanism where basic analysis unit is pixels of images.\nNumerous deep learning methods are employed in research on kidney disease classification. The renal ultra-\nsound pictures are enhanced with a median filter, a Gaussian filter, and morphological operations in the  article23, \nand then characteristics from the images are retrieved with Principal Component analysis (PCA) and the K-near-\nest neighbor (KNN) classifier. The authors  in24 evaluated different traditional ML algorithms, such as Decision \nTrees (DT), Random Forest (RF), Support Vector Machines (SVM), Multilayer Perceptron (MLP), K-Nearest \nNeighbor (KNN), Naive Bayes, and deep neural networks using Convolutional Neural Network (CNN) and got \nthe highest F1 score of 0.853.  In25, pre-trained DNN models such as ResNet-101, ShuffleNet, and MobileNet-v2 \nare used to extract features from kidney ultrasound pictures, which are then classified using a SVM, with final \npredictions made using the majority voting technique. The authors used ultrasound images there for classifica-\ntion problem and got the highest accuracy of 95.58%. The residual dual-attention module (RDA module) was \nemployed for the segmentation of renal cysts in CT images  in26.  In27, the authors integrated the features of using \nconventional and deep transfer learning techniques, and finally, features are used by the SVM Classifier to clas-\nsify normal and abnormal images using US images.  In28, two CNN models are used consecutively, where the \nfirst CNN was used to identify the urinary tract, and the second CNN to detect the presence of stone and got \n95% accuracy. An automated detection of kidney stones (i.e., having/not having stone) was proposed  in29 using \ncoronal Computed Tomography (CT) images and a deep learning technique, yielded a detection accuracy of \n96.82%. The authors used 1,799 images there in total to train and validate the model. The authors  in30 proposed \ntwo morphology convolution layers, modified feature pyramid networks (FPNs) in the faster RCNN and com-\nbined four thresholds. They got an area under the curve (AUC) value of 0.871. The kidney cyst image detection \nsystem for abdominal CT scan images using a fully connected CNN was developed  in31 and the authors got a \ntrue-positive rate of 84.3%.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nIn summary, the efforts utilizing machine  learning32 and deep  learning33 approaches to classify a few kidney \nradiological findings have provided promising results, but the majority of the tasks, we found are performed on \nxray or ultrasound images.A few approaches were there with CT scan images only with dual class classification. \nConsidering the scarcity of data and the above findings of research articles, we created a database of kidney \nstone, cyst and tumor CT images. We implemented three deep learning techniques (VGG16, Inceptionv3 and \nResnet50) to classify four classes of kidney disease and demystified the blackbox of the models to show why our \nmodel came to a certain conclusion about a class. We also implemented the latest state-of-the-art innovations \nin vision learning (EANet, CCT, and Swin transformer algorithms) to classify the four classes and have shown \nthat our model has promising accuracy which can reduce the suffering of the world population through early \ndiagnosis of diseases.\nMethodology\nWe first collected and annotated the datasets to create a database for Kidney Stone, Tumor, Normal, and Cyst \nfindings. Data augmentation, image scaling and normalization, and data splitting are among the preprocessing \ntechniques utilized. After that, we employed six models to investigate our data, including three Visual Trans-\nformer variants (EANet, CCT, and Swin Transformer), Inception v3, and Vgg16 and Resnet 50. The model’s \nperformance was evaluated using previously unseen data. The Block contains details about our experiment’s \ndiagram can be found in Fig. 1\nThe methodology is presented in this part in the following order: dataset description, image preprocessing, \nneural network models, and evaluation strategies of the experiments.\nDataSet description. The dataset was collected from PACS (Picture archiving and communication system) \nand workstations from a hospital in Dhaka, Bangladesh where patients were already diagnosed with having a \nkidney tumor, cyst, normal or stone findings. All subjects in the dataset volunteered to take part in the research \nexperiments, and informed consents were obtained from them prior to data collection. The experiments and \ndata collection were pre-approved by the relevant hospital authorities of Dhaka Central International Medical \nCollege and Hospital (DCIMCH). Besides, the data collection and experiments were carried out in accordance \nwith the applicable rules and regulations.\nBoth the Coronal and Axial cuts were selected from both contrast and non-contrast studies with protocol \nfor the whole abdomen and urogram. The Dicom study was then carefully selected, one diagnosis at a time, and \nfrom those we created a batch of Dicom images of the region of interest for each radiological finding. Following \nthat, we excluded each patient’s information and meta data from the Dicom images and converted the Dicom \nimages to a lossless joint photographic expert group (jpeg/jpg) image format. The Philips IntelliSpace Portal 9.034 \napplication is used for data annotation, which is an advanced image visualization tool for radiology images, and \nthe Sante Dicom editor  tool35 is used for data conversion to jpg images, which is primarily used as a Dicom viewer \nwith advanced features to assist radiologists in diagnosing specific disease findings. After the conversion and \nFigure 1.  Complete Block Diagram of Experiments to diagnose Kidney tumor, cyst and stone.\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nannotation of the data manually, each image finding was again verified by a doctor and a medical technologist \nto reconfirm the correctness of the data.\nOur created dataset contains 12,446 unique data within it in which the cyst contains 3,709, normal 5,077, \nstone 1,377, and tumor 2,283. The dataset was uploaded to Kaggle and made publicly available so that other \nresearchers could reproduce the result and further analyze it. Figure 2 depicts a sample selection of our datasets. \nThe red marks represent the finding area or region of interest that a radiologist uses to reach a conclusion for \nspecific diagnosis classes.\nFigures 3 and 4 show the image color mean value distribution and the image color mean value distribution \nby four classes for our dataset respectively. From both these distributions, it can be concluded that the whole \ndataset is very similar to the distribution of individual normal, stone, cyst, and tumor images. The mean and \nstandard deviation of the image samples plot show that most of the images are centered, whereas stones and \ncysts have lower mean and standard deviation which can be visualized in Fig.  5. Since the data distributions of \ndifferent renal disease classes are partially overlapped therefore, classification of cyst, tumor, and stone is not \npossible using only analyzing the statistical features.\nImage Processing. After converting DICOM images into jpg images, we scaled the images as per the stand-\nard size requirement of neural network models. For all the transformer variant algorithms, we resized each \nimage to 168 by 168 pixels. Images for Inception v3 were resized to 299 by 299 pixels, while images for VGG16 \nand Resnet were reduced to 224 by 224 pixels.We then randomized all the images and took 1,300 examples of \neach diagnosis for the models’ consideration to avoid data imbalance problems, as we have 1,377 images avail-\nable for the kidney stone category. The rotation operation for image augmentation was performed by rotating \nthe images clockwise at an angle of 15 degrees. We evaluated all the models using a scheme where 80% of the \nimages were taken to train the model and 20% to test the data. Within 80% of the training images, we took 20% \nto validate the model to avoid overfitting. The dataset is normalized using Z-normalization36 using following (1):\nFigure 2.  sample image data of kidney cysts, normal, stone and tumor findings.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nFigure 3.  colour mean value distribution of images.\nFigure 4.  Image colour mean value distribution by class.\nFigure 5.  mean and standard deviation of Image samples.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nHere, µi is the mean and σi is the standard deviation value of the feature.\nTransfer Learning Based Neural Network Models. From the dataset, i. e., the CT KIDNEY DATASET: \nNormal-Cyst-Tumor and Stone, we randomly chose 1300 images of each class and trained our six models. All \nthe neural network models were trained on Google Colab Pro Edition with 26.3 GB of GEN Ram and 16160 MB \nof GPU RAM using Cuda version 11.2. All the models were trained with a batch size of 16 and up to 100 epochs.\nVgg16. In our experiment, the 16-layer VGG  1637 model was tweaked in the last few layers by using the first \n13 layers of the original VGG16 model, and we added average pooling, flattening, and a dense layer with a relu \nactivation function. A dropout and finally another dense layer is added to classify the normal kidney as well as \ncysts, tumors, and stones. The total number of parameters in our modified VGG16 is 14,747,780, out of which \n4,752,708 are the trainable parameters and 9,995,072 are the non-trainable parameters. Table 1 shows the num-\nber of parameters of the different models used in our study.\nResnet50. To avoid the vanishing gradient problem, and performance degradation of deep neural networks, \nskip connections are being used in the original Resent model. We utilized 50-layer  resnet5014 models and modi-\nfied them as the same as the Vgg16 and Inception v3 layers in the final few layers to achieve the classification \ntask. The total number of parameters in our modified Resnet 50 model is 23,719,108. Trainable and nontrainable \nparameters are 135,492 and 23,583,616 respectively.\nInception v3. A variant of the Inception family neural network, Inception v3 based on Depthwise Separable \nConvolutions, is used in our study to classify images. Similar to VGG 16, we modified the original Inception  v315 \nmodel in the last few layers, by keeping all the layers except the last three. We added average pooling, flattening, \na dense layer, a dropout, and finally a dense layer to do the classification task. The total number of parameters \nin inception v3 is 22,327,396 with 524,612 trainable parameters. The total number of non-trainable parameters \nis 21,802,784.\nTransformer Based Models. External Attention Transformer(EANet). Though the transformer-based \nmodels were popular in Natural Language Processing, the recent advent of the vision transformer is gaining \npopularity over time, which utilizes the transformer architecture that uses self-attention to sequences of im-\nage  patches18. The sequence of image patches is the input to the multiple transformer block in this case, which \nuses the multihead attention layer as a self-attention mechanism. A tensor of batch_size, num_patches, and \nprojection_dim is produced by transformer blocks, which may subsequently be passed to the classifier head \nusing softmax to generate class probabilities. One variant of the Vision Transformer EANet is shown in Fig.  6. \n EANet20 utilizes external attention, based on two external, small, learnable, and shared memories, M k and M v . \nThe purpose of EANet is to drop patches that contain redundant and useless information and hence improve \nperformance and computational efficiency. External attention is implemented using two cascaded linear layers \nand two normalization layers. EANet computes attention between input pixels and external memory unit via \nfollowing formulas (2), (3)\nFinally, input features are updated from M v by the similarities in Attention A.\nWe utilized TensorFlow Addons packages to implement EANet. After doing data augmentation with random \nrotation at scale 0.1, random contrast with a factor of 0.1, and random zoom with a height and width factor of \n0.2, we implemented the patch extraction and encoding layer. Following that, we implemented an extraneous \nattention block, and transformer block. The output of the transformer block is then provided to the classifier head \nto produce class probabilities to calculate the probabilities of kidney normality, stone, cyst, and tumor findings.\n(1)ˆX = X [:i]−µ i\nσi\n(2)A = Norm\n(\nFMT\nk\n)\n(3)Fout= AM v\nTable 1.  No of parameters of different models.\nModel Total Parameter Trainable parameter\nVGG16 14,747,780 4,752,708\nInception v3 22,327,396 524,612\nResnet50 23,719,108 135,492\nEANet 600,907 600,900\nSwin Transformers 412,788 396,372\nCCT 407,365 407,365\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nCompact convolutional transformer(CCT). Convolution and transformers are combined on CCT to maximize \nthe benefits of convolution and transformers in vision. Instead of using non overlapping patches, which are used \nby the normal vision transformer in CCT 21, the convolution technique is used where local information is well-\nexploited. Figure 7 illustrates the CCT procedure.\nCCT is run using TensorFlow Addons, where first data is augmented using random rotation at scale 0.1, \nrandom contrast with a factor of 0.1, and random zoom with a height and width factor of 0.2.To avoid gradient \nvanishing problems in CCT, a stochastic  depth38 regularization technique is used, which is very much similar to \ndropout except, in stochastic depth, a set of layers is randomly dropped. In CCT, In CCT, after doing convolution \ntokenization, data is fed to a transformer encoder and then sequence pooling. Following the sequence pooling \nMLP head gives the probabilities of different classes of the kidney diagnosis. The total number of parameters in \nour proposed CCT model has 407,365 parameters and all the parameters are trainable.\nShifted Window Transformers (Swin Transformers). Another variant of the Vision Transformer is the Swin \n Transformer22, which is another powerful tool in computer vision. Detailed block diagram of the Swin trans-\nformer is shown in Fig.  8. In the picture, we can see four unique building blocks. First, the input image is split \ninto patches by the patch partition layer. The patch is then passed to the linear embedding layer and the swin \ntransformer block. The main architecture is divided into four stages, each of which contains a linear embedding \nlayer and a swin transformer block multiple times. The Swin transformer is built on a modified self-attention and \na block that includes multi-head self-attention (MSA), layer normalization (LN), and a 2-Layer Multi-Layer per-\nceptron (MLP). In this paper, we utilized the swin transformer to tackle the classification problem and diagnose \nkidney cysts, tumors, stones, and normal findings.\nFigure 6.  External attention of EANet model.\nFigure 7.  Compact Convolutional Transformer (CCT) used in the study.\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nPerformance Evaluation Methods. The quantitative evaluation of all the six models is calculated based \non the parameters of accuracy, sensitivity or recall, precision, or PPV . True positive(TP), false positive(FP), true \nnegative(TN), and false negative(FN) samples are used to calculate the accuracy (4), precision (5), sensitivity (6) \n. The recall, also known as sensitivity, is the model’s ability to identify all relevant cases within a data set. The \nnumber of true positives is divided by the number of true positives plus the number of false negatives. It refers \nto the study’s capability to appropriately identify sick patients with the disease. Diseases are frequently defined \nas a positive category in medical diagnosis. Omitting this (positive category) has serious consequences, such as \nmisdiagnosis, which can lead to patient treatment delays. As a result, high sensitivity or recall is critical in medi-\ncal image diagnosis. Precision (PPV) is necessary when out of all the examples that are predicted as positive, if \nwe desire to know how many are really positive. With precision, the number of true positives is divided by the \nnumber of true positives plus the number of false positives. High precision is desired in the medical imaging \ndomain. The F1 score (7) of all the models is calculated by using those models’ sensitivity and precision. The \nfollowing formulas are applied to accuracy, precision, sensitivity, and F1 score.\nWhere,\n• i=Kidney Tumor or Cyst or Normal or Stone class for the classification task.\n• TP= True Positive\n• FN= False Negative.\n• TN=True Negative\n(4)Accuracyi = TPi+ TN i\nTPi+ TN i+ FPi+ FN i\n× 100%\n(5)Precisioni = TP i\nTP i + FP i\n(6)Sensitivityi = TP i\nTP i + FN i\n(7)F1_scorei = 2 × Precisioni × Sensitivityi\nPrecisioni + Sensitivityi\nFigure 8.  Shifted Window Transformer(Swin Transformer) diagram used in the study.\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nFurthermore, we plotted a receiver operating characteristic (ROC) curve with the transverse axis being the false \npositive rate (FPR) and the longitudinal axis being the true positive rate (TPR). The AUC, or area under the \nROC curve, measures the ROC curve’s ability to classify inputs. The higher the AUC, the better the classification \ncapabilities of the model. The area under the curve is also calculated for each developed model, and finally, all \nthe models are compared to take a decision on which model is superior compared to other models.\nThis paper used the gradient weighted Class Activation Mapping (GradCAM) 39 algorithm to make models \nmore transparent by visualizing the input areas crucial for model predictions in the last convolution layers of \nCNN networks. Figure 9 describes complete process for Gradcam analysis in our paper.\nFirst, we passed a picture through the model to get a prediction, and then we developed the image’s class \nprediction based on the prediction value. After that, we computed the gradient of the class known as Feature \nMap activation Ak(8).\nThese gradients flowing back are global-average-pooled across the width and height dimensions (indexed by i \nand j, respectively) to calculate neuron significance weights (9).\nThen neuron significance weights and feature map activations are summed and applied the Relu activation to \nthe summed result to get the GradCam(10).\nWhere,\n• Ak = feature map activation\n• wc\nk = neuron significance weights\nWe created a visualization by superimposing the original image with the heatmap. This visualization helps us \nto determine why our model came to the conclusion that an image may belong to a certain class, like kidney \ntumor, cyst, normal, or stone.\nResult analysis\nThe results of the implemented six models using different tests are evaluated by calculating the accuracy, recall, F1 \nscore (F1), accuracy (Acc), positive predictive value (PPV), and ROC curve area of interest (AUC) from unseen \ndata. We used Tenfold cross-validation and the result was averaged to produce the ROC curve, confusion matrix, \n(8)Ak = ∂yc\n∂A k\nij\n(9)w c\nk = 1\nZ\n∑\nj\n∑\ni\n∂yc\n∂A k\nij\n(10)Lc\nGradC AM = ReLU\n(∑\nk\nwc\nkAk\n)\nFigure 9.  The complete process for Gradcam analysis for Kidney stone, cyst, tumor and normal classes.\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nand evaluation matrices. Table  2, Figs. 10 and 12 summarizes the performance of the six networks studied in \nthis paper. Figure 14 presents us with the gradcam analysis of the Inception v3, Resnet50, and Vgg16 models. \nFigure 12 provides the ROC curves for Transfer and Transformer based models consecutively. Figures 10 and 12 \nshows the normalized Confusion Matrices for Transfer and Transformer based models consecutively.\nFrom the table  2, we can see that the InceptionV3 model performed worse with our dataset and gave an \naccuracy of 61.60%. EANet and Resnet 50 performed moderately by giving accuracy of 77.02% and 73.80%. CCT, \nVGG16 and Swin Transformers provided accuracy of 96.54%, 98.20% and 99.30% accuracy respectively. The Swin \ntransformer, which is a transformer-based model, is outperforming all the other models in respect of accuracy.\nThe Swin Transformer is providing reasonable recall while detecting cyst, normal, stone, and tumor class \nimages and providing a recall of 0.996, 0.981, 0.989, and 1 consecutively. Higher recall means there is the lowest \nchance of misdiagnosing the cyst, normal, stone, and tumor class images. From the table we can see, the Swin \ntransformer is providing a recall of 1 for kidney stone classes and it is good at detecting kidney tumor classes, \nwhereas CCT is good at detecting stone class images and providing a recall of 1 for the stone class images. How-\never, for the other class images, recall for the CCT model is slightly lower than the Swin transformer model and \nprovides a recall of 0.923, 0.975, and 0.964 for the cyst, normal, and tumor class images, respectively.\nFrom the transfer learning based approaches, VGG16 provides a recall of 0.968, 0.973, 0.988, and 0.996 \nrespectively for Kidney Cyst, Normal, Stone, and Tumor class images. But Inception v3 and Resnet are providing \nTable 2.  MEASURES OF PERFORMANCE FOR THE SIX MODELS STUDIED IN THE RESEARCH.\nModels Accuracy Class Precision (PPV) Recall (Sensitivity) F1 Score AUC \nEANet 77.02%\nCyst 0.593 1 0.745 0.98\nNormal 0.896 0.848 0.871 0.98\nStone 0.845 0.495 0.624 0.91\nTumor 0.93 0.777 0.847 0.97\nSwin Transformers 99.30%\nCyst 0.996 0.996 0.996 0.99993\nNormal 0.996 0.981 0.988 0.9998\nStone 0.981 0.989 0.985 0.99975\nTumor 0.993 1 0.996 1\nCCT 96.54%\nCyst 0.968 0.923 0.945 0.99605\nNormal 0.989 0.975 0.982 0.99841\nStone 0.94 1 0.969 0.99924\nTumor 0.964 0.964 0.964 0.99723\nVGG16 98.20%\nCyst 0.996 0.968 0.982 0.99856\nNormal 0.985 0.973 0.979 0.99844\nStone 0.966 0.988 0.977 0.99908\nTumor 0.982 0.996 0.989 0.99902\nInception v3 61.60%\nCyst 0.645 0.826 0.724 0.92689\nNormal 0.584 0.898 0.708 0.90642\nStone 0.568 0.462 0.509 0.78185\nTumor 0.76 0.295 0.425 0.8029\nResnet50 73.80%\nCyst 0.735 0.641 0.685 0.90721\nNormal 0.77 0.79 0.78 0.95069\nStone 0.745 0.692 0.717 0.9314\nTumor 0.706 0.827 0.762 0.94447\nFigure 10.  ROC curves for Transfer Based Models Used in Our study.\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nlower recall for all the classes. The recall for the Kidney Tumor class is 0.295 for the Inception v3 model and 0.462 \nfor the Kidney Stone classes. This means the Resnet model in our study is the least effective at detecting kidney \ntumors and kidney stones. Since in medical image diagnosis recall is a priority matrix to consider, a model built \nbased on Resnet and Inception v3 can’t be used in diagnosis in our case.\nFrom the transformer based model, we can see in the table 2 the precision is highest for the Swin transformer \nmodel and provides 0.996, 0.996, 0.981, and 0.993 respectively for Kidney Cyst, Normal, Stone, and Tumor class \nimages. From the transfer based approach, we can see VGG16 is providing better precision than Inception V3 \nand Resnet50.\nFor the cyst, normal, stone, and tumor classes, the highest F1 score is provided by the swin transformer also, \nand the numbers are 0.996, 0.998, 0.985, and 0.996 consecutively. The Swin transformer also provides the high-\nest precision for Stone and Tumor classes, and readings are 0.981 and 0.993. For the cyst class, the Swin trans -\nformer and VGG 16 are providing the same value of 0.996, whereas for the normal class, the Swin transformer \nis performing better and giving a reading of 0.996. Considering the above, the Swin transformer is superior and \noutperforms all the models, and can be of great use in kidney medical imaging diagnosis.\nFrom Figs.  10, 11, 12 and 13, we can see that the Area Under the ROC Curve is superior in the case of \nCCT, VGG16, and SWin Transformers than Resnet50, EANet, and Inception v3. AUC is closer to 1 while diag-\nnosing Kidney Cyst, Normal, Stone, and Tumor categories for Swin Transformers, CCT, and VGG16 models. \nFigure 11.  Confusion Matrices for Transfer Based Models Used in Our study.\nFigure 12.  ROC curves for Transformer Based Models Used in Our study.\nFigure 13.  Confusion Matrices for Transformer Based Models Used in Our study.\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\nConsidering precision, recall, and F1 Score, we can conclude that though VGG16 and CCT are performing well, \nthe Swin transformer outperformed all the models. Though CCT and VGG16 can be used while diagnosing \nkidney stones, cysts, and tumors, Swin Transformer can be considered the most effective option.\nAfter randomly providing four images of different classes from the CT machine in the GradCam algorithm, \nwe analyzed the GradCam of the last convolution layer of the Transfer-based algorithm. From the Fig. 14, First \nrow shows images that contain cysts. We can see from the Fig. 14a, e and i that VGG16 is watching a very small \nregion (high level features) to take a decision about cyst class images, whereas Resnet50 and Inceptionv3 are \nlooking at more dispersed regions, hence low-level features to classify. For the stone class images Fig. 14c, g and \nk, we can observe that Vgg 16 is watching the region of interest perfectly. Other models are watching dispersed \nregions, whereas VGG16 is watching a very small region to make a decision. A similar condition applies to the \ntumor and normal classes as well. In our case, VGG16 is predicting all the images as correct class and watching \nthe region of interest perfectly, whereas Resnet is predicting normal findings such as tumors and stones as normal \nin this case and also not watching where the model should watch to make a decision. Inception V3 is also not \nwatching the region of interest perfectly and watching more low-level features, and in this case, it predicated the \ntumor class as the normal class.\nConclusion\nFor this work, we collected and annotated a total of 12,446 whole abdomen and urogram CT scan images con -\ntaining cysts, tumors, normal, and stone findings. Exploratory data analysis of the images was performed and \nshowed that the images from all the classes had the same type of mean colour distribution. Furthermore, this \nstudy has developed six models and out of which, three models are based on recent state-of-the-art variants of \nthe Vision transformers EANet, CCT, and Swin transformers, and the other three are based on popularly known \nFigure 14.  GradCam analysis of kidney Cyst, Normal, Stone and Tumor class photos at the final convolution \nlayer in the Inception v3, Vgg16, and Resnet models. First row: shows the Gradcam images from inception \nv3 model for different classes. Second row: shows the Gradcam images from Resnet50 model for different \nclasses. Third row: shows the Gradcam images from Vgg16 model for different classes. The GradCam activation \nmapping for the xray image is shown in the second row. The first, second, third, and fourth columns are for \nkidney cysts, normal, stone, and tumor classes respectively.\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\ndeep learning models, Resnet, Vgg16, and Inception v3, which are tweaked in the last few layers. A comparison \nof all the models performed revealed that, while VGG16 and CCT performed well, the Swin transformer outper-\nformed all the models in terms of accuracy, providing an accuracy of 99.30%. The F1 score, precision, and recall \ncomparisons provide evidence that the Swin transformer is outperforming all the models.Besides, compare to \nall the models, the Swin transformer has taken less time to train with the same number of epochs. The study has \nalso tried to reveal the blackbox of VGG16, Resnet50, and Inception models and found that the VGG16 model \nis better compare to Resnet50 and Inceptionv3 by showing the desired abnormalities in the anatomy better. We \nbelieve the superior accuracy of our model based on the Swin transformer and the VGG16-based model can both \nbe of great use in detecting kidney tumors, cysts, and stones, and can reduce the pain and suffering of patients.\nReceived: 25 December 2021; Accepted: 27 June 2022\nReferences\n 1. Jacobson, S. Chronic kidney disease-a public health problem?. Lakartidningen 110(21), 1018–1020 (2013).\n 2. Jha, V . et al. Chronic kidney disease: global dimension and perspectives. The Lancet 382(9888), 260–272 (2013).\n 3. Foreman, K. J. et al. Forecasting life expectancy, years of life lost, and all-cause and cause-specific mortality for 250 causes of death: \nreference and alternative scenarios for 2016–40 for 195 countries and territories. The Lancet 392(10159), 2052–2090 (2018).\n 4. Rediger, C. et al. Renal cyst evolution in childhood: a contemporary observational study. J. Pediatric Urol. 15(2), 188-188e1 (2019).\n 5. Brownstein, A. J. et al. Simple renal cysts and bovine aortic arch: Markers for aortic disease. Open Heart 6(1), e000862 (2019).\n 6. Sanna, E. et al. Fetal abdominal cysts: Antenatal course and postnatal outcomes. J. Perinatal Med. 47(4), 418–421 (2019).\n 7. Alelign, T. & Petros, B. Kidney stone disease: an update on current concepts. Adv. Urol. 2018 (2018).\n 8. Hsieh, J. J. et al. Renal cell carcinoma. Nat. Rev. Dis. Primers 3(1), 1–19 (2017).\n 9. Saw, K. C. et al. Helical CT of urinary calculi: Effect of stone composition, stone size, and scan collimation. Am. J. Roentgenol.  \n175(2), 329–332 (2000).\n 10. Gunasekara, T. et al. Urinary biomarkers indicate pediatric renal injury among rural farming communities in sri lanka. Sci. Rep.  \n12(1), 1–13 (2022).\n 11. Bi, Y ., Shi, X., Ren, J., Yi, M. & Han, X. Transarterial chemoembolization of unresectable renal cell carcinoma with doxorubicin-\nloaded callispheres drug-eluting beads. Sci. Rep. 12(1), 1–8 (2022).\n 12. Sozio, S.M., Pivert, K.A., Caskey, F .J. & Levin, A. The state of the global nephrology workforce: A joint asn–era-edta–isn investiga-\ntion. Kidney Int., (2021).\n 13. Islam, M. CT kidney dataset: Normal-cyst-tumor and stone 2021. [Online]. Available: https://  www. kaggle. com/ nazmu l0087/ ct- \nkidney- datas et- normal- cyst- tumor- and- stone.\n 14. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. in Proceedings of the IEEE conference on computer \nvision and pattern recognition, 2016, pp. 770–778.\n 15. Szegedy, C., Liu, W ., Jia, Y ., Sermanet, P ., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., Rabinovich, A. Going deeper with \nconvolutions. in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.\n 16. Chollet, F . Xception: Deep learning with depthwise separable convolutions. in Proceedings of the IEEE conference on computer \nvision and pattern recognition, pp. 1251–1258 (2017).\n 17. Tan, M., & Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. in International Conference on Machine \nLearning. PMLR, 2019, pp. 6105–6114.\n 18. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., \nGelly, S. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv: 2010. 11929, (2020).\n 19. Kolesnikov, A. et al. 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16. Springer 2020, 491–507 \n(2020).\n 20. Guo, M.-H., Liu, Z.-N., Mu, T.-J. & Hu, S.-M. Beyond self-attention: External attention using two linear layers for visual tasks. \narXiv preprintarXiv: 2105. 02358, (2021).\n 21. Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J. & Shi, H. Escaping the big data paradigm with compact transformers. arXiv \npreprintarXiv: 2104. 05704, (2021).\n 22. Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S. & Guo, B. Swin transformer: Hierarchical vision transformer using shifted \nwindows. arXiv preprintarXiv: 2103. 14030, (2021).\n 23. Verma, J., Nath, M., Tripathi, P . & Saini, K. Analysis and identification of kidney stone using k th nearest neighbour (knn) and \nsupport vector machine (svm) classification techniques. Pattern Recognit. Image Anal. 27(3), 574–580 (2017).\n 24. AKSAKALLI, I., KAÇDIOĞLU, S., & HANAY , Y .S. Kidney x-ray images classification using machine learning and deep learning \nmethods. Balkan J. Electr. Comput. Eng. 9(2), 44–551.\n 25. Sudharson, S. & Kokil, P . An ensemble of deep neural networks for kidney ultrasound image classification. Comput. Methods Progr. \nBiomed. 197, 105709 (2020).\n 26. Fu, X., Liu, H., Bi, X. & Gong, X. Deep-learning-based CT imaging in the quantitative evaluation of chronic kidney diseases. J. \nHealthcare Eng. (2021).\n 27. Zheng, Q., Furth, S. L., Tasian, G. E. & Fan, Y . Computer-aided diagnosis of congenital abnormalities of the kidney and urinary \ntract in children based on ultrasound imaging data by integrating texture image features and deep transfer learning image features. \nJ. Pediatric Urol. 15(1), 75-75e1 (2019).\n 28. Parakh, A. et al. Urinary stone detection on CT images using deep convolutional neural networks: evaluation of model performance \nand generalization. Radiol.: Artif. Intell. 1(4), e180066 (2019).\n 29. Yildirim, K. et al. Deep learning model for automated kidney stone detection using coronal CT images. Comput. Biol. Med. 104569 \n(2021).\n 30. Zhang, H. et al. Automatic kidney lesion detection for CT images using morphological cascade convolutional neural networks. \nIEEE Access 7, 83 001-83 011 (2019).\n 31. Blau, N. et al. Fully automatic detection of renal cysts in abdominal CT scans. Int. J. Comput. Assisted Radiol. Surg. 13(7), 957–966 \n(2018).\n 32. Siddiqi, M. H., Alam, M. G. R., Hong, C. S., Khan, A. M. & Choo, H. A novel maximum entropy markov model for human facial \nexpression recognition. PloS one 11(9), e0162702 (2016).\n 33. Munir, M.S., Abedin, S.F ., Alam, M.G.R., & Hong, C.S. et al. Rnn based energy demand prediction for smart-home in smart-grid \nframework. pp. 437–439, (2017).\n 34. Healthcare, P . Radiology and cardiology diagnostic imaging solution | philips healthcare. (2022). [Online]. Available: https:// www. \nusa. phili ps. com/ healt hcare/ produ ct/ HC881 072/ intel lispa ce- portal- advan ced- visua lizat ion- solut ion.\n 35. LTD, S. Sante dicom viewer pro | santesoft ltd. 2022. [Online]. Available: https:// www. sante soft. com/ win/ sante- dicom- viewer- pro/ \nsante- dicom- viewer- pro. html.\n14\nVol:.(1234567890)Scientific Reports |        (2022) 12:11440  | https://doi.org/10.1038/s41598-022-15634-4\nwww.nature.com/scientificreports/\n 36. Patro, S., & Sahu, K.K. Normalization: A preprocessing stage. arXiv preprintarXiv: 1503. 06462, (2015).\n 37. Simonyan, K., & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv: 1409. \n1556, (2014).\n 38. Huang, G., Sun, Y ., Liu, Z., Sedra, D., & Weinberger, K.Q. Deep networks with stochastic depth. in European conference on computer \nvision. Springer, 2016, pp. 646–661.\n 39. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. & Batra, D. Grad-cam: Visual explanations from deep networks \nvia gradient-based localization. in Proceedings of the IEEE international conference on computer vision, 2017, pp. 618–626.\nAuthor contributions\nM.N.I. and M.G.R.A. contributed to design the novel idea, experimental results, and initial draft of the paper. \nM.H. and M.K.H. contribued with collecting and validating the data of the datasets for the experiments. M.Z.U. \nand A.S. contributed in revising and reviewing the idea, paper and results from the experiments, and coordinated \nthe overall process and study.\nFunding\nOpen access funding provided by Norwegian University of Science and Technology.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to A.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}