{
    "title": "Complex QA and language models hybrid architectures, Survey",
    "url": "https://openalex.org/W4321393106",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5003745196",
            "name": "Xavier Daull",
            "affiliations": [
                null,
                "Aix-Marseille Université",
                "Laboratoire d’Informatique et Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A5059940694",
            "name": "Patrice Bellot",
            "affiliations": [
                null,
                "Aix-Marseille Université",
                "Laboratoire d’Informatique et Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A5070020969",
            "name": "Emmanuel Bruno",
            "affiliations": [
                null,
                "Aix-Marseille Université",
                "Laboratoire d’Informatique et Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A5101756859",
            "name": "Vincent Martin",
            "affiliations": [
                null,
                "Naval Group (France)"
            ]
        },
        {
            "id": "https://openalex.org/A5028041353",
            "name": "Élisabeth Murisasco",
            "affiliations": [
                null,
                "Aix-Marseille Université",
                "Laboratoire d’Informatique et Systèmes"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4382202774",
        "https://openalex.org/W4385573947",
        "https://openalex.org/W4288000169",
        "https://openalex.org/W4291291092",
        "https://openalex.org/W4317910584",
        "https://openalex.org/W4303648904",
        "https://openalex.org/W4385569771",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4281250694",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4293320163",
        "https://openalex.org/W4297633153",
        "https://openalex.org/W4225407091",
        "https://openalex.org/W4385570060",
        "https://openalex.org/W4309877123",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W2983309655",
        "https://openalex.org/W4309129329",
        "https://openalex.org/W4389524038",
        "https://openalex.org/W4385573003",
        "https://openalex.org/W1542791059",
        "https://openalex.org/W2550936021",
        "https://openalex.org/W3205556807",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W4296413526",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3208821253",
        "https://openalex.org/W4293138840",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W2964118342",
        "https://openalex.org/W2964322347",
        "https://openalex.org/W3103031657",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W4385572965",
        "https://openalex.org/W2987553933",
        "https://openalex.org/W4308902180",
        "https://openalex.org/W4309416490",
        "https://openalex.org/W4313304293",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4289598671",
        "https://openalex.org/W3109919947",
        "https://openalex.org/W3189921439",
        "https://openalex.org/W2977942577",
        "https://openalex.org/W4315882015",
        "https://openalex.org/W4304194220",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4306294746",
        "https://openalex.org/W4287214436",
        "https://openalex.org/W3161820423",
        "https://openalex.org/W2962781380",
        "https://openalex.org/W4296557505",
        "https://openalex.org/W4320165905",
        "https://openalex.org/W3102999298",
        "https://openalex.org/W4287827771",
        "https://openalex.org/W2963681467",
        "https://openalex.org/W4386566888",
        "https://openalex.org/W3153839026",
        "https://openalex.org/W4221150592",
        "https://openalex.org/W2126209950",
        "https://openalex.org/W2901707424",
        "https://openalex.org/W4220794927",
        "https://openalex.org/W4313433600",
        "https://openalex.org/W4287780085",
        "https://openalex.org/W4297097408",
        "https://openalex.org/W2962910007",
        "https://openalex.org/W4389524426",
        "https://openalex.org/W4312634749",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3034188538",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4313304857",
        "https://openalex.org/W3196731672",
        "https://openalex.org/W4225080353",
        "https://openalex.org/W4287332927",
        "https://openalex.org/W4305033123",
        "https://openalex.org/W4303939357",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W2888302696",
        "https://openalex.org/W4287207977",
        "https://openalex.org/W3174510164",
        "https://openalex.org/W4385573672",
        "https://openalex.org/W4302011807",
        "https://openalex.org/W4316116392",
        "https://openalex.org/W3173343821",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W3190540921",
        "https://openalex.org/W4302447128",
        "https://openalex.org/W4312107542",
        "https://openalex.org/W4319997768",
        "https://openalex.org/W4313680149",
        "https://openalex.org/W4385572953",
        "https://openalex.org/W3176495666",
        "https://openalex.org/W4225366662",
        "https://openalex.org/W4286982890",
        "https://openalex.org/W4224993409",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W4296605665",
        "https://openalex.org/W3044495139",
        "https://openalex.org/W2997789966",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4313484599",
        "https://openalex.org/W4283383808",
        "https://openalex.org/W4223908421",
        "https://openalex.org/W4319453300",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W3035068386",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W4385573161",
        "https://openalex.org/W4307311867",
        "https://openalex.org/W2162526436",
        "https://openalex.org/W4385573667",
        "https://openalex.org/W3195153845",
        "https://openalex.org/W4303441863",
        "https://openalex.org/W4221152849",
        "https://openalex.org/W3171975879",
        "https://openalex.org/W2791374212",
        "https://openalex.org/W4224247158",
        "https://openalex.org/W4385573762",
        "https://openalex.org/W4280507250",
        "https://openalex.org/W4293350112",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4309955236",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W3099246072",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W4320235125",
        "https://openalex.org/W4293576670",
        "https://openalex.org/W3016339201",
        "https://openalex.org/W3204302053",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4321855256",
        "https://openalex.org/W4309953112",
        "https://openalex.org/W4385572270",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3172267148",
        "https://openalex.org/W3192678369",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W3099766584",
        "https://openalex.org/W4318768054",
        "https://openalex.org/W4364382874",
        "https://openalex.org/W4385573035",
        "https://openalex.org/W4221164017",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3193647133",
        "https://openalex.org/W4225887996",
        "https://openalex.org/W4385567121",
        "https://openalex.org/W4385571158",
        "https://openalex.org/W2998099211",
        "https://openalex.org/W4385567216",
        "https://openalex.org/W4283796192",
        "https://openalex.org/W4281488000",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W4307207888",
        "https://openalex.org/W3206798125",
        "https://openalex.org/W4226146935",
        "https://openalex.org/W4287025617",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4285595435",
        "https://openalex.org/W2977843956",
        "https://openalex.org/W2963890755",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W4292102312",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W4312122095",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4224909620",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4255639968",
        "https://openalex.org/W4382202847",
        "https://openalex.org/W4385572426",
        "https://openalex.org/W2912680822",
        "https://openalex.org/W4387345085",
        "https://openalex.org/W2171278097",
        "https://openalex.org/W4387427785",
        "https://openalex.org/W4385565212",
        "https://openalex.org/W3154565472",
        "https://openalex.org/W3175689281",
        "https://openalex.org/W4236521339",
        "https://openalex.org/W3173566921",
        "https://openalex.org/W2963541336",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W4389519535",
        "https://openalex.org/W3189701585",
        "https://openalex.org/W4385572830",
        "https://openalex.org/W4320166805",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4386566474",
        "https://openalex.org/W4313014461",
        "https://openalex.org/W4311997173",
        "https://openalex.org/W3101771976",
        "https://openalex.org/W4289523162",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4385570209",
        "https://openalex.org/W4385572845",
        "https://openalex.org/W3009561768",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4287855194",
        "https://openalex.org/W4286903242",
        "https://openalex.org/W1621791442",
        "https://openalex.org/W4321177655",
        "https://openalex.org/W3105826487",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W4385573418",
        "https://openalex.org/W4308165205",
        "https://openalex.org/W4285595056",
        "https://openalex.org/W3210011271",
        "https://openalex.org/W4385573777",
        "https://openalex.org/W4304697829",
        "https://openalex.org/W2971236147",
        "https://openalex.org/W4307934157",
        "https://openalex.org/W4289445724",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W4385574345",
        "https://openalex.org/W3100436891",
        "https://openalex.org/W4307123345",
        "https://openalex.org/W1525961042",
        "https://openalex.org/W4221165966",
        "https://openalex.org/W3116010554",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4313304663",
        "https://openalex.org/W4287854450",
        "https://openalex.org/W3202580838",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W3130196849",
        "https://openalex.org/W4283455204",
        "https://openalex.org/W4385573164",
        "https://openalex.org/W3097611736",
        "https://openalex.org/W3045958725",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W4392637287",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4385572712",
        "https://openalex.org/W4385894687",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W3023419341",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W4283167130",
        "https://openalex.org/W2971068072",
        "https://openalex.org/W4389921502",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W4385570969",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W4385572727",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4300886482",
        "https://openalex.org/W4385574214",
        "https://openalex.org/W1981065051",
        "https://openalex.org/W4221145545",
        "https://openalex.org/W4317897811",
        "https://openalex.org/W4298187912",
        "https://openalex.org/W4293998609",
        "https://openalex.org/W2606982687",
        "https://openalex.org/W4367185264",
        "https://openalex.org/W3035497479",
        "https://openalex.org/W2963963993",
        "https://openalex.org/W3213896436",
        "https://openalex.org/W4305028650",
        "https://openalex.org/W3211777899",
        "https://openalex.org/W2963368301",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W4285704036",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W3102554603",
        "https://openalex.org/W4385570998",
        "https://openalex.org/W3153834700",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4385570984",
        "https://openalex.org/W4303684020",
        "https://openalex.org/W4311557185",
        "https://openalex.org/W4286903249",
        "https://openalex.org/W4306385094",
        "https://openalex.org/W3162670395",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W4315588847",
        "https://openalex.org/W4302011175",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W4312643954",
        "https://openalex.org/W4385572867",
        "https://openalex.org/W4320858367",
        "https://openalex.org/W3192298118",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W4221157342",
        "https://openalex.org/W4281806276",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4311991106",
        "https://openalex.org/W4297162632",
        "https://openalex.org/W2988396768",
        "https://openalex.org/W4308243058",
        "https://openalex.org/W4307536106",
        "https://openalex.org/W4205477024",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W1604489931",
        "https://openalex.org/W2970745243",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3176693010",
        "https://openalex.org/W4385573855",
        "https://openalex.org/W3206996280",
        "https://openalex.org/W4385573007",
        "https://openalex.org/W4375949262",
        "https://openalex.org/W3049574859"
    ],
    "abstract": "This paper reviews the state-of-the-art of language models architectures and strategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on hybridization. Large Language Models (LLM) are good at leveraging public data on standard problems but once you want to tackle more specific complex questions or problems (e.g. How does the concept of personal freedom vary between different cultures ? What is the best mix of power generation methods to reduce climate change ?) you may need specific architecture, knowledge, skills, methods, sensitive data protection, explainability, human approval and versatile feedback... Recent projects like ChatGPT and GALACTICA have allowed non-specialists to grasp the great potential as well as the equally strong limitations of LLM in complex QA. In this paper, we start by reviewing required skills and evaluation techniques. We integrate findings from the robust community edited research papers BIG, BLOOM and HELM which open source, benchmark and analyze limits and challenges of LLM in terms of tasks complexity and strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as a baseline. We discuss some challenges associated with complex QA, including domain adaptation, decomposition and efficient multi-step QA, long form and non-factoid QA, safety and multi-sensitivity data protection, multimodal search, hallucinations, explainability and truthfulness, temporal reasoning. We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.",
    "full_text": "Complex QA & language models hybrid architectures, Survey\nXAVIER DAULL,Naval Group, Toulon Université, Aix Marseille Univ, CNRS, LIS, France\nPATRICE BELLOT,Aix Marseille Univ, CNRS, LIS, Marseille, France\nEMMANUEL BRUNO, Toulon Université, Aix Marseille Univ, CNRS, LIS, Toulon, France\nVINCENT MARTIN, Naval Group, France\nELISABETH MURISASCO, Toulon Université, Aix Marseille Univ, CNRS, LIS, Toulon, France\nThis paper reviews the state-of-the-art of large language models (LLM) architectures and strategies for\n\"complex\" question-answering with a focus on hybridization. LLM based chatbot services have allowed\nanyone to grasp the potential of LLM to solve many common problems, but soon discovered their limitations\nfor complex questions. Addressing more specific, complex questions (e.g., \"What is the best mix of power-\ngeneration methods to reduce climate change ?\") often requires specialized architectures, domain knowledge,\nnew skills, decomposition and multi-step resolution, deep reasoning, sensitive data protection, explainability,\nand human-in-the-loop processes. Therefore, we review: (1) necessary skills and tasks for handling complex\nquestions and common LLM limits to overcome; (2) dataset, cost functions and evaluation metrics for measuring\nand improving (e.g. accuracy, explainability, fairness, robustness, groundedness, faithfulness, toxicity...); (3)\nfamily of solutions to overcome LLM limitations by (a) training and reinforcement (b) hybridization, (c)\nprompting, (d) agentic-architectures (agents, tools) and extended reasoning.\nCCS Concepts: • Information systems →Question answering; Information extraction ; • Security and\nprivacy →Information accountability and usage control .\nAdditional Key Words and Phrases: complex question answering, context engineering, knowledge protocol en-\ngineering, semantic search, NLP, transformers, neural language models, taxonomy, neuro-symbolic, attention,\npre-training, post-training, fine-tuning, reinforced fine tuning RFT, prompting, non-factoid QA, multi-hop QA,\nmulti-step QA, long-form QA, knowledge graph, multimodal search, human-in-the-loop, RLHF, RLAIF, DPO,\nORPO, KTO, PRM, agents, tools, function calling, MoE, PEFT, QLoRA, DoRA, RAG, Self-RAG, Corrective RAG,\nGraphRAG, FActScore, RAGAS, long-context\nACM Reference Format:\nXavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco. 2025. Complex QA &\nlanguage models hybrid architectures, Survey. 1, 1 (November 2025), 59 pages. https://doi.org/XXXXXXX.\nXXXXXXX\nAuthors’ addresses: Xavier Daull, Naval Group, Toulon Université, Aix Marseille Univ, CNRS, LIS, , France; Patrice Bellot,\nAix Marseille Univ, CNRS, LIS, Marseille, Marseille, France; Emmanuel Bruno, Toulon Université, Aix Marseille Univ, CNRS,\nLIS, Toulon, Toulon, France; Vincent Martin, Naval Group, France; Elisabeth Murisasco, Toulon Université, Aix Marseille\nUniv, CNRS, LIS, Toulon, Toulon, France.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\ncontact the owner/author(s).\n© 2023 Copyright held by the owner/author(s).\nXXXX-XXXX/2025/11-ART\nhttps://doi.org/XXXXXXX.XXXXXXX\n, Vol. 1, No. 1, Article . Publication date: November 2025.\narXiv:2302.09051v5  [cs.CL]  3 Nov 2025\n2 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n1 INTRODUCTION\nResearch in the field of question answering could route from earliest examples of AI system designed to\nanswer questions like ELIZA [233] developed at MIT in the 1960s; or engineering approaches to solve complex\nproblems; or foundational research in the psychology field on how people understand and interpret language,\nprioritize and focus on relevant information in context [38], investigate and make decision, retrieve and use\ninformation from memory to answer [165], innovate... Even Socrates’ philosophical approach to questioning\nand critical thinking has recently been directly used to improve training [169], common sense reasoning [103],\nor helping to solve some complex questions.\nWhen a question or problem is too complex to be solved as it is, such as “ how does the concept of personal\nfreedom vary between different cultures and societies ? ”, a common strategy is to break it down into solvable\nquestions, and then combine solutions to provide an overall answer if there is consensus, or the possible\nalternatives and nuances. We aim to answer complex questions, or problems formulated as a question, which\nare non-factoid and so require decomposition (multi-step), multi-source of knowledge combination, higher\norder reasoning or tasks resolution. All of these may vary a lot depending on the field and question. LLMs\nhave demonstrated their ability to outperform an average human on complex QA tasks across knowledge\nfields [105], and can mimic methods or uncover appropriate methods for a given problem. However, even\nmost advanced systems will fail on some basic questions [13], or could assert totally false or biased knowledge\nwithout any caution. This can seriously impact the credibility of a system by a human. Involving humans in\nthe question-answering loop, strengthening training, hybridizing with third parties like programs, symbolic\nAI or other, can greatly improves those models and help ensure ethical and safe outcomes.\nTo be able to build those efficient hybrid systems, properly trained and aligned to human expectations in\norder to better solve increasingly complex problems, it is thus necessary to precisely know strengths and\nlimitations of each language models alone or in collaboration.\nTherefore we use benchmarks and insights from collective papers as consensus baselines. Latest evaluations\nof large community evaluation project HELM [ 132], and BIG [ 8] focus on evaluating, democratizing and\nimproving LLM capabilities, particularly on question answering and tasks that will also be useful for more\ncomplex questions. HELM provides a comprehensive analysis and benchmark for evaluating global strengths\nand weaknesses of reference large language models across a range of different scenarios and complementary\nmetrics. Training large language models is done by resource-rich organizations and are mostly impossible\nto train by any individual public research group. BigScience1, a collaboration of hundreds of international\nresearchers, trained on the French government-funded Jean Zay supercomputer during 3.5 months the largest\nopen source multilingual language models in July 2022 called BLOOM and shared all models, codes and results\nin the BLOOM paper [9]. BIG spots difficult tasks to better assess current skills and limitations of language\nmodels.\nTo overcome identified limitations and tackle more specific or complex questions, after those benchmarks\nand insights, we review hybrid architectural patterns solutions, training, prompting techniques, and reinforce-\nment strategies to acquire the necessary knowledge, skills (general abilities), tasks (goal oriented actions),\nmethods and human alignment to answer complex questions.\n1.1 Structure of the paper\nThis survey covers the main topics related to complex question answering systems. In section 2, we present\ntypical process and architectures to answer questions, and new approach brought by large language models\n(LLM) and transformers. Next, we delve into complex QA definition, required tasks & skills, limitations\nto overcome (section 3) to answer complex questions. For each limitation we link to potential resolution\nstrategies presented later. In section 4, we review the evaluation metrics, methods and datasets of the scientific\ncommunity used to assess current tasks, skills and limitations of LLMs, or to further develop them through\ntraining. We then review the complementary resolving strategies which could be combined to solve complex\nQAs for the target usage.\n1https://bigscience.huggingface.co/\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 3\nWe first explore training techniques in section 5, from pre-training, mid-training to post-training, as well\nas methods for dealing with lack of data, poor quality, learning to reason, and adaptation to new tasks and\ndomains.\nSecond, in section 6, we review and classify different hybridization architectural patterns with their pros\nand cons to augment LLM. In addition, we present a unified section on modern agentic meta-architectures\ntogether with inference-time strategies (see section 8).\nThird, in section 7, we review prompting techniques and how it can also be used to decompose complex\nquestions down to solvable tasks; we also formalize in-context learning , context engineering and protocol\nengineering for principled prompt-time assembly of instructions, evidence, tools and memory (subsection 7.2).\nFourth, in section 8, we cover agentic meta-architectures (agents, tools) and the inference-time strategies they\ninvoke to allocate extra “reasoning-time” on hard queries; preference alignment and reinforcement is treated\nwithin training (subsection 5.6). We introduce reasoning-time (inference-time) compute as a strategy to trade\ncompute for quality on difficult queries (subsection 8.1).\nFinally, section 9 highlight tougher challenges, partial solutions that have been identified, and research\navenues to overcome them, including safety and multi-sensitivity usage. All along the survey, we provide an\nextensive bibliography for readers who wish to delve deeper into the subject matter (section 10).\n1.2 Contribution\nThis survey provides :\n– a systematic review & analysis of literature on complex QA with LLM, including an enricheddefinition\n(subsection 3.1) and taxonomy (subsection 3.2, Figure 2), and an extensive bibliography of the field.\n– a qualitative analysis of the skills, tasks, and limitations of LLMs (section 3) aimed at better framing\ncomplex QA requirements and complexity.\n– an overview of evaluation metrics, methods, datasets and SOTA (section 4) to better evaluate skills &\ntasks, and estimate LLM limits and strategies.\n– a classification and aggregation of hybridization architectural patterns (section 6) that can\naugment LLM and overcome their limitations in complex QA.\n– a set of resolving strategies to combine (training: section 5, hybridization: section 6, prompting:\nsection 7, post-training preference alignment: subsection 5.6, and inference-time strategies within\nagentic meta-architectures: section 8).\n– a list of major research challenges (section 9) and a focus on some blind spots (i.e. data multi sensitivity).\n1.3 Survey methodology\nTo build this survey,first, we collected surveys in the last two years related to “ complex question answering ”\nor “ complex problem solving ” and “ language models ” that we cite throughout the article. From these\nelements, we systematically extracted the major concepts and their plans (table of contents). We fused all their\nplans into one to ensure a complete coverage of major concepts and adopt a similar methodology.\nSecond, we gathered:\n– latest challenges from CLEF QA (nlp.uned.es), NTCIR (research.nii.ac.jp), SemEval (semeval.github.io)\nand TREC (microsoft.github.io/msmarco) related to question answering;\n– the list of the major conferences; then, list research papers from latest edition of conferences SIGIR\n(sigir.org), NeurIPS (nips.cc), NAACL (naacl.org), EMNLP (emnlp.org), ICLR (iclr.cc), AAAI (aaai.org),\nIJCAI (ijcai.org), CIKM, SIGKDD (kdd.org), WSDM (wsdm-conference.org) about “ complex question\nanswering ” and “ language models ”;\n– research publications from influential organizations in the field: Deepmind, OpenAI, Google, Microsoft,\nMeta, and Anthropic related to \"question answering\" and \"language models\".\n– living evaluation sources that now shape SOTA perception and track reasoning progress like\nArena-Hard (and Chatbot Arena Elo), HELM updates, LiveBench, and harder reasoning sets such as Big\nBench Extra Hard , Humanity’s Last Exam [176], MMLU-Pro, GPQA. These complement frozen test sets\nfor reproducible comparisons.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n4 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nFig. 1. IBM DeepQA Architecture (2010) [ 59] CQA pipeline steps \"patched\" to reflect new practice with\nverifier/critic loops, retrieval gating, and programmmatic research actions.\nFrom these documents, we clustered major LLM limitations which are listed in subsection 3.4 and solutions in\nsections 5, 6, 7, 5.6, 9.\nThird, we enriched our bibliography using a search engine of scientific articles to identify all the articles\npublished during the last four years on the main subject of this investigation (search queries: \"complex question\nanswering\" AND \"language model\"; \"question answering\" AND \"language model architecture\"; \"question answering\"\nAND \"language model\" AND \"hybrid architecture\"; \"hybrid language models\" OR \"hybrid language model\" OR\n\"hybrid neural language models\"; \"language model\" \"hybrid architecture\" ).\nThe bibliography collected in all the previous steps was then used throughout this survey to ensure that we\nsummarize the main concepts at the state-of-the-art by automatically detecting articles semantically close\n(subscription to \"zeta alpha\" and \"semantic scholar\").\nLast but not least, we searched the most cited papers in this bibliography and extended with recent connected\npapers or some historical papers often cited, and investigated relevant citations. Through this research, we\nidentified three recent research papers (HELM capabilities, and BIG) each involving hundreds of researchers\nfrom many organizations, so we decided to use them as a baseline or major reference in this study.\n2 CORE CONCEPTS & ARCHITECTURES\n2.1 Question answering typical pipeline and modular approaches\nFrom question capture and refinement, to answer generation and knowledge capitalisation, question answering\n(QA) or complex question answering (CQA) pipeline can follow a variable number of steps depending on\narchitecture and features. Some steps are explicit and well separated, some can be implicit and fused with\nothers in the same model operation. However, we can identify most frequent steps and options. The \"IBM\nDeepQA Architecture\" (see figure 1) seems a dated architecture compared to some current end-to-end neural\nlanguage models but it defines clearly some major steps; in practice, since 2024, many research pipelines add\nexplicit verifier/critic loops and retrieval gating, and support tool use (function calling) for programmatic\nresearch actions.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 5\n(1) Question understanding and analysis, it can include question and context refine, parsing and un-\nderstanding the question, context, and intent (for task identification). It can also embed a dialogue\nmanagement to interact with the user and understand the conversation context and state.\n(2) Query construction with optional decomposition of complex questions and multi-step queries.\n(3) Information retrieval (IR) and optional knowledge expansion, to add new knowledge to the system.\n(4) (a) Information extraction, and (b)evaluation, scoring, ranking, filtering, verification/attribution\n(judge models, quote checks) and retrieval gating (e.g. self-corrective RAG).\n(5) Answer generation, natural language or defined format (e.g. language program, table, ...)\n(6) Feedback loop & new knowledge capitalisation : learning and improving from users and models\nfeedback, plus storing linked and generated knowledge for improving answering skills.\nThis process is only a baseline as complex questions answering can be a dynamic and progressive process, and\ncan also be collaborative. Architectures of QA systems have importantly evolved recently with the arrival of\ntransformers architectures and large language models. We first quickly review typical modular architectures,\nthen the transformers with large language models and, later, the hybrid architectures. Will we go to gigantic\nknowledge models or more complex and composed architectures, maybe in network of smaller specialized\nmodels and other components ?\nTypical architectures of QA systems before transformers were modular approaches such as:Rule-based\nsystems using a set of predefined rules to answer questions; Retrieval-based systems using a search\nengine or database to find answers to questions; Information extraction systems using natural language\nprocessing (NLP) to extract relevant information from text documents and often leverage information retrieval\n(IR) systems; Knowledge-based systems storing and retrieving information from a knowledge base; Case-\nbased reasoning systems using a database of previously solved problems to find solutions to new questions;\nHybrid architectures could assemble the some of the modules listed (e.g. \"IBM DeepQA Architecture,\n2010\" [59]) to deliver a more advanced QA system integrated with natural language models for understanding\ninitial question for example.\n2.2 LLM with transformers evolution\nThe emergence of deep feedforward layers, allowed to learn and infer a much wider range of relationships\nwith inputs; Then the raise of attention mechanism, allowed a model to selectively focus on certain parts\nof the input for better understanding and contextualizing. It led to language models surpassing humans on\ncertain tasks. We can group current transformer based language models in three types:\nEncoders only (e.g. BERT [45], RoBERTa [144]) encode a sequence of a text input into a rich represen-\ntation (vector embedding) which can be used by a task-specific model or function for classification,\nnamed entity recognition (NER), semantic similarity measure used in IR and QA or topic modeling.\nThis is often called bidirectional attention because they can take the context of the words before and\nafter the target word analyzed, which allow them to perform better on some tasks. BERT is one of the\nmost well-known encoder-only models, and RoBERTa is an optimized version of BERT.\nDecoders only (e.g. GPT family [16, 281]) complete an input sequence (mostly text prompt) by the most\nprobable next words (generation). This left to right generation can be very rich like writing a story or\nanswering a question (e.g. used by ChatGPT). The input prompt can be formatted with appropriate\ninstructions (prompt & instructions engineering) to use the generated text as a task-specific model (e.g.\nclassification, summarization, decomposition...). This is often called causal or autoregressive attention\n(non-causal decoders exist but has limited adoption in the literature). GPT family of models are one of\nthe most well-known decoder-only models, known for their ability to generate human-like text.\nEncoders-Decoders (e.g. T5 [187], BART [121]) encode the input text and decode it into a different form\n(text-to-text mapping), they are suitable for translation, summarization, or generating a response to\na question. They consist of both an encoder and a decoder, where the encoder generates a fixed-size\nrepresentation of the input text, and the decoder generates the output text. T5 is known for its multi-task\nability; BART is mainly used for text generation and summarization.\nRecently, closed-source providers, as well as open-weight and open-source community efforts focus on\npractical inflections for complex QA: (i) long-context models (hundreds of thousands to > 1M tokens) make\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n6 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nmulti-document “full book” reasoning far more feasible; (ii) multimodal models unify text, image, audio and\ndiagrams in a single loop; (iii)reasoning-centric models and practices (deliberatethink-then-answer decoding,\nplan/verify loops) substantially improve math, science, and tool-using tasks; (iv) mixture-of-experts (MoE)\ndesigns regain traction to balance quality/cost; and (v) agentic tool use via function calling is now default in\nmany deployments.\n3 ANALYZING: COMPLEXITY, SKILLS, TASKS, AND LIMITS\nIn order to design a system able to answer complex questions, we first propose to analyze targeted questions\ncomplexity, identify required skills and tasks, as well as limitations to handle. This analysis allows to properly\ndefine the problem to be solved gradually and constraints to integrate in order to compose among the different\ncomplementary solving approaches further reviewed (training: section 5, hybridization: section 6, prompting:\nsection 7, experience: section 5.6). This analysis could also be done \"a posteriori\" if a system fails to properly\nanswer in order to better characterize or identify the causes.\n3.1 What are complex questions ?\nComplex questions can be defined as those that involve multiple factors of complexity, requiring higher\nlevels of cognitive processes and domain-specific knowledge to answer accurately. Ullrich and Geierhos [221]\nsuggests using Bloom’s taxonomy to evaluate question complexity by integrating required knowledge (ranging\nfrom easier to harder: factual, conceptual, procedural, metacognitive) with cognitive processes (ranging from\neasier to harder: remember, understand, apply, analyze, evaluate). For instance, a simple factual question such\nas \"What is the capital of France?\" requires only basic recall, while a more complex question like \"What are the\nmain causes of climate change and their potential solutions?\" demands an understanding of multiple concepts\nand the ability to evaluate different solutions.\nWe think that a question complexity is also highly dependent on users’ or systems’ expertise involved in\nthe response. We propose to assess the complexity of questions by also considering the main difficulties and\nefforts required for an LLM to solve them:\n•skills and knowledge required (section 3.2): simple memorization or higher reasoning (e.g. evaluation,\nconstraints solving, deduction, induction, abduction), single or multiple types of logic; prior domain-\nspecific knowledge, retrieve and process one easily accessible piece of information or multiple rare\ninformation, reason over long distance between pieces of information to combine; expected answer\nformat and explanation; ambiguity and nuances to handle (especially in non-factoid questions). and the\nneed for specific decomposition and multi-step resolution.\n•new or challenging tasks to solve like specific decomposition, and multi-step resolution (section 3.3).\n•major limitations of LLMs to overcome in this context (sections 3.4 and 9).\n•difficulty in designing appropriate metrics and sufficient datasets to measure LLM skills and\nknowledge performance (section 4).\n•training effort to develop those skills & tasks (section 5) in a model, or in different hybrid models\n(section 6) to align or train end-to-end to solve questions in a coherent way.\n•complexity to engineer robust prompt questions on trained models such as additional context and\ninstructions required (section 7), and then decompose questions down to solvable tasks .\n•progressive reinforcement and knowledge capitalization , or system’s experience learning, to solve\ntargeted complex questions (section 5.6).\n3.2 Skills\nConsidering the QA/CQA standard pipeline presented in introduction, a task of question answering requires\ndifferent complementary skills in the domain of machine reading comprehension (MRC), information retrieval\n(IR), and also knowledge capitalisation and reinforcement.\nWe leverage the QA skills taxonomy from Rogers et al. [190] which we augmented with the \"Experience\nlearning\" skill concept (see figure 2), important for a CQA system. This later has shown to be a major skill to\nenable calibration or alignment to intent and values [5], and continuous improvement by usage [30].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 7\nLLM mixed resolving strategies\nSkills (3.2)\n(general and domain specific)\nWorld modeling\nSituation modeling\nSpatial relations, Causal relations, \nEntity tracking, Temporal relations, \nBelief states, Entity properties\nMeta-analysis\nText type, Author intentions, Narrator / \nProtagonist….\nTool using\nRun / Test\nExecute, Result capture, \nError capture\nLocate & extract\nInformation retrieval, \nCommonsense, Factual, \nContext\nReasoning\nInference direction\nDeductive, Abductive\nInference mechanism\nAnalogy, Best explanation, Rule based, \nHeuristic, Iterative, Hierarchical… \nInference strength\nDeductive, Inductive\nDecomposing, \nplanning\nSimple\nMulti-step planning on single task\nComplex\nMultiple task decomposing & planning\nInterpreting & manipulating\nMultimodal perception\nLexical knowledge, Vision, \nCoreference…\nOperations\nUnion, Intersection…\nNumeric skills\nNumbers, Addition…\nExperience \nlearning\nReinforcement learning, Incremental & \ntransfer learning, Self practice, \nReinforcement strategies\nTasks (3.3)\nunsolvable task are decomposed \ndown to chains of solvable tasks\nTraining (5) Hybridization (6) Agentic (8.2)\narchitecturesPrompting (7)\nEvaluations (4)\n(datasets, metrics…)\nComplex question \nanswering (3.1)\nevaluate or train on\nconstrained by\nrequires a set of\nsolved by\nevaluate or train a \nset of\ncomposed of\nLimits of LLM (3.4, 9)\ncomposed of\novercome\nestimate\nTask adaptation and tuning patterns; \nDecomposition, multi-processing \npatterns; External information & \nreasoning patterns; Feedback, \ndialog, reflection patterns\nUnsupervised pre-training;\nSupervised fine-tuning;\nReinforced FT; PEFT\nSingle step optimization;\nMulti-step optimization;\nProcess optimization;\nExternal tool optimization;\nEnhanced knowledge\nExtended \ninference (8.1)\nThinking; Process \nsupervision; \nMultiple/tree sampling \nTool using; Dynamic \nplanning; Role based; \nSelf-critique \nFig. 2. QA/CQA taxonomy links skills, tasks and limits of LLMs to complementary resolving strategies (training, hybridization, prompting, agentic architectures,\nextended reasoning-time), as well as evaluation datasets & metrics (skills inspired by Rogers et al. [190]) - Each concept is provided with reference to section\nin this paper.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n8 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n3.2.1 Interpreting & manipulating input. Like humans, machines should capture the meaning of the individual\nconstituent elements of the input (words, numbers) and the global syntax and semantic, and manipulate them\nin the context of the task and in respect to the language and other shared system (e.g. mathematics). This\nrequires a set of skills, including:\n•Linguistic skills - e.g. recognizing word categories and meaning, translating, understanding the\ncontext, relationships and implications of words and phrases; it might be decomposed into syntactic,\ngrammatical, and semantic skills.\n•Numeric skills - e.g. performing calculations, dealing with precise and imprecise numbers.\n•Operation on sets - e.g. selecting, combining, intersection, operating on elements of a set of input (e.g.\nAlice, Bob and Charlie are in the room. Bob goes out. Who are the persons in the room?);\n3.2.2 Information Retrieval. It can be summarized as determining whether an answer exists, if yes, to look for\nit and provide most useful information :\n•Answerability: ability to identify whether a given query is a valid question and can be answered with\nprovided information. Optionally identify additional information to correctly answer.\n•Where to look for the required knowledge? : ability to identify the correct source of knowledge to\nget the best answer. It the required knowledge for the answer is in the question, process is to extract\nthe good piece of information. Otherwise, we need to know if it is a precise fact or non factual, then\nwhere to look for it. Additionally, a proper answer may require common sense and potential domain\ninformation.\n3.2.3 Inferring & reasoning. Inferring and reasoning can be summarized as the process of drawing logical\nconclusions from available facts or other premises. Inferring is used in language models to understand a text,\nand generate responses to questions posed. There are three main aspects to inference in language models:\n•Inference Strength: could draw general conclusions from specific facts (inductive), or draw specific\nconclusions from general facts (deductive).\n•Inference Mechanism: draw conclusions from a comparison of two or more elements (analogy), draw\nconclusions based on the best explanation for a given situation (best explanation)...\n•Inference Direction: conclusion follows necessarily from the premises or from general to specific\n(Deductive), conclusion is reached through a process of elimination or reasoning from the specific to\nthe general (abductive).\n3.2.4 World modeling. It can be summarized as the ability to understand and make decisions based on the\nunderstanding of the world. It is a complex type of question answering skill that requires understanding of\nphysical and mental states, as well as relationships between them. It involves the following categories:\n•Spatial reasoning: understand and reason about objects and their locations in space.\n•Temporal reasoning: understand and reason about event order, event attribution to time, script\nknowledge, event duration, temporal commonsense knowledge, factoid/news questions with answers\nwhere the correct answers change with time, temporal reasoning in multimodal setting.\n•Belief states: understand and track beliefs, opinions, and mental states.\n•Causal relations: understand and reason about the cause-and-effect relationships between events.\n•Other relations between events : understand and reason about relationships between events, such as\nsub-events, conditionals, and counterfactuals.\n•Entity properties and relations : properties of characters, physical properties, numerical properties,\nsocial interactions.\n•Tracking entities: understand and track entities over time, across locations, in co-reference chains.\n3.2.5 Decomposing, multi-step. Complex questions require decomposition down to solvable tasks and reso-\nlution in the best chain of action steps. Simple question may use multi-step resolution but all necessary\nknowledge are located in one place. Complex questions rely on several knowledge, necessitating the combi-\nnation of information across sentences, paragraphs, documents, or other modalities. It also includes questions\nthat require a combination of context and world knowledge. It can be even broader than simply combining\nseveral facts, and could also be taken as combining the “skills” from different dimensions and different methods\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 9\nof resolution. This decomposition and multi-step resolution can be resolved inside a model having these skills\nand all other necessary for the question, or distributed across multiple components.\n3.2.6 Experience learning. A complex QA system should be able to permanently improve itself through:\nreinforcement by aligning answers to target intent, format, method, values expectations with solutions\nwhich could vary with requester person (e.g. knowledge, culture...) or system; capitalization by integrating\nnew knowledge generated or linked in order to improve knowledge enabling to solve more complex problems.\nExperience skills could be classified under meta-analysis in world modeling skills but it may not fully capture\nself-modeling, self-practice or integration of external feedback, incremental learning towards a coherent\noptimization of all skills.\n3.3 Tasks\nA task of complex question answering could be solved in one inference task incorporating all the skills viewed\nin previous section, or subdivided in several sub-tasks, planned, each with a set of skills and, maybe, different\ndomains.\n3.3.1 Integrated (C)QA task. In this case, the CQA system answers from question using only one inference\nin the model but could include multi-step reasoning inside the model. LLM (large language models) should\ntherefore embed all the necessary skills and knowledge for interpretation & manipulation, information\nretrieval, world knowledge, reasoning & inference, decomposition & multi-step resolution. Otherwise, the\nmodel should be further trained with adapted datasets to acquire those new skills and knowledge, or rely on\ntask decomposition and LLM hybridation. BIG et al. [8] and Liang et al. [132] provide an overview of limits\nof integrated QA/CQA. To make these ceilings concrete, Table Table 1 contrasts human average and expert\nperformance with the best open models on representative hard BBH tasks.\nDomain Task Human Human Top open LLM Top open LLM\naverage expert Score (%) Name\nLogic\nboolean_expressions 84.0 100.0 96.0 T3Q-qwen2.5-14b-v1.0-e3\nweb_of_lies 71.0 92.0 93.2 calme-3.2-instruct-78b\nformal_fallacies 67.2 90.0 98.0 Llama-3.2-1B-Instruct\nlogical_deduction_three_objects 73.6 94.0 98.4 internlm2_5-20b-chat\nlogical_deduction_five_objects 50.8 78.0 82.0 T3Q-qwen2.5-14b-v1.0-e3\nlogical_deduction_seven_objects 34.0 54.0 86.0 Llama-3.2-1B-Instruct\nAlgorithmic\ntracking_shuffled_objects_three_objects 62.8 86.0 79.6 Llama-3.2-1B-Instruct\ntracking_shuffled_objects_five_objects 34.0 56.0 60.4 Llama-3.2-1B-Instruct\ntracking_shuffled_objects_seven_objects 23.2 44.0 54.8 Llama-3.2-1B-Instruct\nnavigate 54.0 74.0 87.2 test-2.5-72B\nobject_counting 77.0 96.0 89.2 Llama-3.2-1B-Instruct\nLinguistic\ndisambiguation_qa 76.2 89.2 97.6 Llama-3.2-1B-Instruct\nhyperbaton 73.1 90.0 99.6 Llama-3-Refueled\nruin_names 70.0 92.0 97.6 internlm2_5-7b-chat\nsalient_translation_error_detection 68.8 88.0 97.6 Llama-3.2-1B-Instruct\nsnarks 78.4 91.0 96.6 Llama-3.2-1B-Instruct\nReasoning\ncausal_judgement 63.8 82.2 87.7 Llama-3.2-1B-Instruct\ndate_understanding 74.0 95.6 94.8 Llama-3.2-1B-Instruct\nmovie_recommendation 70.8 94.0 100.0 BunderMaxx-1010\nreasoning_about_colored_objects 81.2 97.2 90.8 ultiima-72B-v1.5\nsports_understanding 69.0 86.0 97.6 Llama-3.2-1B-Instruct\ntemporal_sequences 70.0 90.0 100.0 calme-2.3-llama3.1-70b\nTable 1. Big bench hard (BBH) leaderboard - human average and top performance vs top open source LLM\nmodels.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n10 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n3.3.2 CQA tasks decomposition and primitives. Answering by decomposition could be grouped in:\n– Standard sub-tasks include intent detection, word sense disambiguation, entity recognition (NER)\nand linking, topic classification, sentiment classification, information extraction, fact retrieval, ranking,\nand summarization, including query focused summarization...\n– Advanced sub-tasks include multi-hop & decomposition, domain oriented tasks, sources & fact\nchecking, code generation & program synthesis, causal explanation (or possible consequences), temporal\nexplanation...\n– External sub-tasks leverage resources out of model like program synthesis [50], using a solver, or\ninvoking tools (function calling) and retrieval variants (e.g., self-critique/corrective RAG).\nWu et al. [238] propose a taxonomy of primitive tasks in decomposed and chained LLM which could\nbe applicable to CQA:\n– Validate and categorize input such as classification which assigns the input to categories. Most\nuseful for branching logic and validation (e.g. is the question answerable?).\n– Gather additional information from the LLM such asFactual Query to ask LLM for a fact, Generation\nto ask LLM to do some creative “hallucination” on the input, Ideation to ask a list of ideas or examples.\n– Re-organize input such as Information extraction from the context, Rewriting (1-1 mapping)\ninput to more machine-readable formats (e.g. JSON to natural language) or other usage (e.g. translation),\nSplit Points (1-N mapping) for splitting contexts, digging concepts, Compose Points (N-1 mapping to\nsynthesise, reverse operation of decomposition like merge multiple results back together.\n3.3.3 CQA tasks hybrid program decomposition examples. To better illustrate how CQA tasks could be\ndecomposed between a LLM and an external software module to better solve complex problems, we invite\nyou to see how: university level math problems (CQA) can be solved by splitting the task between a LLM and\na Python language interpreter [50], physical reasoning question can be solved by splitting the task between a\nLLM and a physics engine [141]. Those solutions design can be extrapolated to many complex QA challenges\nas proposed in section 6. Each of these patterns can be combined to split sub-tasks of complex problem to\nleverage most adapted module for the task by ensuring necessary context is provided wherever needed.\n3.4 LLM limits\nScaling up language models has been shown to predictably improve performance[ 231] on a wide range\nof downstream tasks. The HELM [ 132] and BIG [ 8] studies show that state-of-the-art on most scenarios\nare led by those very larges models but still lack on different sides (e.g. fairness, robustness across tasks\n& domains, reasoning). Many additional components are used or investigated to face limitations of those\nmodels by augmentation or hybridization with LLM. For example, ChatGPT and InstructGPT [ 4] added\nreinforcement learning with human feedback to its pre-trained large language model to highly improve their\nanswer performance (e.g. calibration, human expectation alignment). Therefore we decided to cover in this\nstudy the improvement of the language model itself (see next section) and the hybridation patterns which can\novercome different limits of base models (below). Those different limits or challenges have been identified in\nour systematic review, and we linked them to different potential solutions presented in next section. Some\nemerging challenges like resistance to adversarial attacks [6] are not covered.\nTraining Hybridization Prompting Agentic & reasoning-time\nLimitation Potential solutions (see also section 9)\nQuestion/intent understanding & conversational state — en-\nhancing context, intent, goals identification, through question\nexpansion, clarification, dialog.\nHybridization: 6.5.2, 6.2.2, 6.2.3; Prompting: 7; Post-\ntraining alignment: 5.6; Mid-training (retrieval/tool-\naware, long-context): 5.4; Agentic & reasoning-time: 8,\n8.1.\nQuestion decomposition & planning — breaking down com-\nplex questions into simpler sub-questions, enabling multi-hop/step\nreasoning and action design.\nPrompting: 7; Hybridization: 6.3.1, 6.3.2, 6.3.4, 6.5.4;\nAgentic & reasoning-time: 8, 8.1; Mid-training: 5.4.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 11\nTraining Hybridization Prompting Agentic & reasoning-time\nLimitation Potential solutions (see also section 9)\nAlignment to human expectations/preferences [4] — ensur-\ning models align with human expectations and values, cultural\ndifferences.\nPost-training alignment (RLHF / DPO / ORPO / KTO):\n5.6; Reinforcement/experience: 5.6; Hybridization: 6.5.1,\n6.3.3; Agentic workflows: 8; Training: 5.5.6, 5.5.3.\nFactuality, grounding & attribution (hallucina-\ntions/faithfulness) [97] — reducing hallucination, ensuring\nanswer accuracy, and providing evidence/attribution.\nHybridization: 6.4.2, 6.4.7, 6.3.4, 6.5.4; Post-training\nalignment & PRMs: 5.6; Agentic & reasoning-time (ver-\nify/rerank, self-consistency/cascades): 8, 8.1; Prompting:\n7; Training: 5.5.8, 5.8.\nReasoning — LLM progressed a lot on reasoning with Chain-\nof-Thought and reasoning at inference-time strategies but they\nare nor optimal in time and quality, nor universal on domains.\nincorporating higher logical reasoning, causality, and learning\nfrom code to improve problem-solving capabilities.\nAgentic & reasoning-time: 8, 8.1; Hybridization: 6.2.2,\n6.4.4, 6.4.5, 6.4.1, 6.3.4, 6.5.4; Post-training (RFT/PRMs):\n5.6; Training: 5.5.2, 5.3.3.\nHandling long-context questions & long-form answers\n— handling long inputs, long-range dependencies, and multi-\ndocument summarization/synthesis. Most LLM have limitation\nin output size (or drift to goal) and input (question but also all\nnecessary to knowledge to add), as well as in the reasoning length\ndependencies.\nHybridization: 6.3.1, 6.4.2, 6.4.3, 6.3.2, 6.4.1; Mid-training\n(long-context adaptation): 5.4; Agentic & reasoning-time\n(cascades/verification): 8, 8.1.\nMulti-modal understanding, search and reasoning - knowl-\nedge and world model cannot always be captured with text only\nand require other forms of information.\nHybridization: 6.4.6, 6.4.4, 6.4.5; Training: 5.5.7, 5.3.3,\n5.5.2; Agentic orchestration: 8.2.\nTemporal reasoning & freshness - handling time-based rea-\nsoning, knowledge update, and understanding sequences or work-\nflows.\nHybridization: 6.4.8, 6.4.2, 6.4.1; Mid-training / updating:\n5.4, 5.5.4; Agentic workflows: 8.2.\nData sensitivity protection - utilizing & protecting sensitive\ndata, such as private, intellectual property, organizational, or gov-\nernmental sensitive data.\nHybridization/RAG with access control & verification:\n6.4.2, 6.4.3, 6.4.7; Deployment patterns: 9.4; Mid-training\n(offsite/federated): 5.4; Agentic guardrails/gating: 8.\nExperience / knowledge and skills capitalization - like a hu-\nman, it should be able to continually improve itself by experience\non skills and knowledge using implicit and explicit feedback. GPT-\n4 technical report highlighted it still does not have experience\nlearning skill [166]\nReinforcement/experience: 5.6; Hybridization: 6.2.2, 6.5.1,\n6.2.3, 6.5.2, 6.4.1, 6.5.3. Agentic & reasoning-time: 8, 8.1.\ndeliberate reasoning-time scaling (8.1) and process-level\nsupervision (PRMs) improve step quality without updating\nbase weights; persistent “experience” still requires explicit\nmemory (6.4.1) or retraining.\nAdaptating and updating - adapting models to specific domains\nand tasks, and ensuring they stay updated with new knowledge\nHybridization: 6.2.1, 6.2.2, 6.4.2, 6.4.3, 6.5.3; Training:\n5.3.2, 5.5.4, 5.5.3, 5.7, 5.4; Reinforcement/experience: 5.6;\nPrompting: 7.\nBias [132] - mitigating representational and statistical biases\nacross topics, domains, and contexts\nPost-training alignment: 5.6. Hybridization: 6.5.1, 6.5.3.\nReinforcement/experience: 5.6.\nEfficiency & scaling (cost/latency/energy) [132] — train-\ning/inference efficiency and cost control.\nHybridization: 6.4.2, 6.2.3, 6.3.3; Training: 5.7, 5.8, 5.4;\nAgentic & reasoning-time (cascades/adaptive compute): 8,\n8.1.\n4 EVALUATING: METRICS, COST FUNCTIONS, DATASETS\nThe performance of language models on question answering can vary greatly depending on factors such as\nthe domain, question complexity, necessary subtasks, bias, fairness, toxicity, and human expectations. A large\nlanguage model may perform well overall but struggle with some type of questions, or areas of evaluation,\nwhile a model that is specialized for certain questions or domains may perform poorly on more general tasks.\nTo orient readers across the most relevant living and frozen evaluations used in complex QA, Table 3\nconsolidates widely used suites, what they measure, and their headline metrics.\nThe following section will examine different metrics and datasets used for evaluating and training these\nmodels.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n12 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nTable 3. Main living and frozen leaderboards covering the different aspects of LLM complex QA evaluation.\nAspect Name (link) Task description Metric\nCapability anchor (reason-\ning+knowledge)\nHELM Capabilities Standardized LLM capabilities evalua-\ntions (e.g., MMLU-Pro, GPQA, IFEval,\nWildBench, Omni-MATH).\nAggregate, rescaled mean\nover scenario-specific met-\nrics\nAcademic level QA HLE Humanity’s Last\nExam\n2,500 closed-ended, expert-created ques-\ntions across >100 subjects; 14% mul-\ntimodal (text+image); private held-out\nsplit for anti-overfitting\nAccuracy, RMS calibration\nerror\nGPQA-Diamond Expert-vetted, graduate-level science\nquestions.\nAccuracy (% correct)\nSafety anchor HELM Safety Multi-risks evaluation (violence, fraud,\ndiscrimination, sexual content, harass-\nment, deception).\nCategory-specific scores\nand overall aggregates\nBroad academic knowl-\nedge\nMMLU-Pro Harder, contamination-resistant revi-\nsion of MMLU across many subjects.\nAccuracy (% correct)\nMath reasoning (annual) AIME-2025 30 AIME 2025 problems; strict final-\nanswer grading.\nAccuracy (% solved)\nLong-context\nQA/comprehension\nLongBench v2 QA, summarization, dialogue history,\ncode and structured-data understanding\nwith very long contexts.\nTask-specific (mostly accu-\nracy); with/without CoT\nKnowledge-intensive QA /\nRAG\nKILT Unified open-domain QA, fact-\nchecking, entity linking, slot-filling\nwith Wikipedia provenance.\nKILT-EM/F1 gated by\nretrieval; retrieval R-\nPrecision/Recall@k\nCRAG (Comprehen-\nsive RAG)\nBenchmark suite for retrieval-grounded\nQA that emphasizes corrective retrieval\nand explicit attribution/citation; evalu-\nates end-to-end RAG quality across do-\nmains.\nComposite: retrieval\nprecision/recall, ground-\nedness/attribution, and\ntask-specific answer\ncorrectness. [253]\nLong-form factuality &\ngrounding\nFACTS Grounding Judge-based evaluation of whether long-\nform answers are fully grounded in a\nprovided document.\nAggregate factual-\nity/grounding score\nSoftware reasoning / cod-\ning\nSWE-bench (Verified /\nLite / Bash-Only)\nEnd-to-end bug fixing on real OSS repos-\nitories; multiple tracks.\n% Resolved (issues solved)\nTrustworthiness (fair-\nness/toxicity/robustness/privacy)\nTrustLLM Integrated suite across truthfulness,\nsafety, fairness, robustness, privacy, ma-\nchine ethics.\nDimension-specific met-\nrics and aggregates\nHuman preference (help-\nfulness)\nLMSYS Chatbot Arena Crowdsourced pairwise battles of model\noutputs (text & multimodal).\nElo-style Arena score (pair-\nwise win rate)\nAbstract reasoning “limits” ARC Prize (ARC-AGI-\n2)\nAbstraction & reasoning puzzles; easy\nfor humans, hard for current AI.\nAccuracy on private test\n(score and cost)\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 13\n4.1 Metrics & SOTA performance\n4.1.1 Standard metrics. There are a variety of metrics that can be used to evaluate the performance of QA\nmodels, each with its own strengths and weaknesses. In this section, we will discuss some of the most common\nmetrics used to evaluate QA models:\nRecall@k Measures the proportion of relevant answers retrieved by the model among the first k answers.\nIt is a measure of the model’s ability to find all relevant answers, regardless of their position in the\nranking. The main weaknesses are that it does not take into account the position of the relevant answer\nin the ranking and does not penalize irrelevant answers that appear in the top k.\nAccuracy@k Measures the proportion of correct answers among the first k answers returned by the\nmodel. It is a measure of the model’s ability to return the correct answer and can be used to evaluate\na model’s performance in a closed-domain QA task. As with recall@k, the main weaknesses are that\nit does not take into account the position of the correct answer in the ranking and does not penalize\nirrelevant answers that appear in the top k.\nnDCG Normalized Discounted Cumulative Gain, a measure of rank quality that takes into account the\nrelevance and position of responses. It is often used in information retrieval and web search to measure\nthe effectiveness of a ranking algorithm. The main weaknesses are that it does not take into account\nthe number of irrelevant answers in the ranking.\nMAP Mean Average Precision, a measure of the quality of a set of ranked responses. This is a commonly\nused metric for evaluating QA models in open domain tasks, where the model should return a list of\npossible answers. its main weakness is that it does not take into account the position of the correct\nanswer in the ranking.\nMRR Mean Reciprocal Rank, a measure of the quality of a set of ranked answers, with a higher value\nindicating better performance. It is often used in information retrieval and web search to measure the\neffectiveness of a ranking algorithm. Weakness: It only considers the position of the first correct answer\nin the ranking.\nCWS Cross-entropy word-level perplexity, a measure of the model’s ability to predict the next word in a\nsentence. It is often used to evaluate the quality of the language model. Weakness: it cannot be used for\nanswer quality and perplexity is not well correlated with a language model tasks performance.\nF1 (macro, micro) F1 score, a measure of a model’s accuracy that takes into account both precision and\nrecall. It is used to evaluate the model’s performance in a classification task. F1 macro averages the\nper-class F1 scores, used for imbalanced datasets, F1 micro computes metrics globally by counting total\ntrue positives, false negatives, and false positives, used for balanced datasets. Weakness: It does not\ntake into account the relative importance of false positives and false negatives.\nEM Exact Match, a binary metric that measures whether the model’s answer is exactly the same as the\nreference answer. It is often used in closed-domain QA tasks where the correct answer is a single word\nor phrase. Weakness: It is not able to capture the semantic similarity between the model’s answer and\nthe reference answer.\nJaccard similarity measures similarity between two sets of data, based on the size of the intersection\ndivided by the size of the union of the sets. Weakness: It does not take into account the overall size of\nthe sets, which can lead to errors in similarity measurements.\nCosine similarity measures similarity between two non-zero vectors based on the cosine of the angle\nbetween them. Weakness: It is sensitive to the magnitude of the vectors (e.g. zero), which can lead to\nerrors in similarity measurements.\nBLEU Bilingual Evaluation Understudy, a measure of the similarity between a model’s answer and a\nreference answer. It is commonly used in machine translation and natural language generation tasks.\nWeakness: It does not take into account the meaning of the words (semantic similarity).\nROUGE Recall-Oriented Understudy for Gisting Evaluation, a measure of the similarity between a\nmodel’s answer and a reference answer. It is commonly used in natural language summarization tasks.\nAdditionally to BLEU, it takes into account the longest common sequence (LCS). Weakness: it does not\ntake into account the meaning of the words (semantic similarity).\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n14 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nMETEOR Metric for Evaluation of Translation with Explicit ORdering, a measure of the similarity\nbetween a model’s answer and a reference answer. It is similar to BLEU, but takes into account word\nalignment and synonymy. Weakness: It is computationally expensive and can be sensitive to the\nreference translations used.\nLLM-as-judge [273] Uses a LLM to evaluate the quality, helpfulness, or factuality of another model’s\noutput, often by scoring or pairwise comparison. Weakness: Can exhibit position bias, self-consistency\nbias (favoring its own style), and requires careful prompting and calibration against human preferences\nto be reliable.\nHuman evaluation human judges provides a subjective measure of the quality of the model’s answers.\nIt is considered the gold standard for evaluating QA models. Weakness: it is time-consuming and can\nvary greatly depending on the individual evaluators and their level of expertise.\n4.1.2 Metrics for free-form/natural language QA. In the context of free-form QA, standard metrics limits\nquestion complexity [21] and do not capture many good answers semantically close. So new metrics have\nbeen proposed using PLM having higher correlations to human expectations:\nBERTScore [270] compute a semantic similarity score through a sum of cosine similarities between the\ncontextualized embeddings of answer tokens and those of the reference text.\nBARTScore [261] similar to BERTScore, it uses the more recent model BART pre-trained using a more\nrobust technique (denoising autoencoding). A BARTScore variant adds faithfulness in the measure.\nMAUVE [179]: similar to BERTScore and add divergence frontiers. It claims to better correlate with\nhuman judgement and identify quality differences.\nT5Score this hybrid metric [185] based on mT5 model is not yet compared to date with MAUVE but is\nglobally more robust than BERTScore and BARTScore.\nHowever He et al. [77] highlight that all those PLM based metrics have flaws, they could assign a high\nlikelihood to degenerate, repetitive text and could be insensitive to perturbations such as word order shuffling,\nnegation, etc. We can balance those blind spots by composing with complementary metrics.\n4.1.3 Multi-metrics and scenarios. Liang et al. [132] referenced a large space of targeted use cases of LLMs\nwith a focus on QA, and identifies 7 key categories of metrics required to create useful systems: accuracy,\ncalibration, robustness, fairness, bias, toxicity, and efficiency (speed/cost). For accurate definition of theses\ncategories of metrics please refer to HELM taxonomy. Liang et al.[132] highlights that performance of models\nare unequal and even best performer is not the best choice depending on metrics preference regarding your\nneeds. The relationship between accuracy and calibration depends on the scenario (task & domain) and\nadaptation procedure, with some scenario showing trade-offs between accuracy and calibration. Across all\nscenarios, there is a strong correlation between accuracy, robustness, and fairness, but trade-offs where the\nmost accurate model is not the most robust or fair. The study also found performance disparities in models\nwhen demographic metadata is available, and low overall average biases and toxicity in model generations, but\nnotes that targeted evaluations are needed to obtain a more detailed characterization. The study also found that\nthere is not a strong trade-off between accuracy and efficiency, but as models become larger, accuracy improves\nbut with higher training and inference cost. Only a subset of all models are on the accuracy-efficiency Pareto\nfrontier for each scenario. There is no model leading on all metrics, QA performances also vary depending on\nthe scenario (task & domain) and model, so weighting or defining a decision tree among the 7 metrics then\nevaluating on target scenario is necessary for choosing your model.\nLiang et al. [132] introduced a holistic, multi-metric view (accuracy, calibration, robustness, fairness, bias,\ntoxicity, efficiency) that remains relevant for complex QA. However, static 2022 leaderboards are no\nlonger representative of the frontier landscape in 2024–2025. Current practice complements HELM with\nharder reasoning tests and living leaderboards—e.g., MMLU-Pro, GPQA, Arena-Hard/Chatbot Arena Elo,\nHumanity’s Last Exam , and LiveBench—to avoid saturation and contamination and to better probe complex\nQA. Accordingly, any summary figures based solely on 2022 snapshots (e.g., “InstructGPT best on many axes”)\nshould be replaced by panels that track these evolving sources; the core HELM finding (“no single winner\nacross metrics and scenarios”) still holds.\nIn practice, we orient readers to the evaluation landscape summarized in Table 3, which consolidates widely\nused living and frozen leaderboards, what they measure, and headline metrics.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 15\n4.1.4 Which metrics for \"complex\" QA?. Considering that complex QA is not well defined and answers highly\ndepends on human expectations and values, we could not identify standard metrics in the literature. Ullrich\nand Geierhos [221] propose to use Bloom’s taxonomy to assess question complexity but does not offer a metric\nfor measuring the relevance of answers to question. Metrics identified in previous section for free-form QA\ncould be used when there is a gold/reference answer but shall be unstable considering that a good non-factoid\nanswer can be semantically distance to gold answer. In this survey we aim to focus on complex questions with\nthe following characteristics: non-factoid, multi-step (requiring decomposition), multi-source of knowledge,\nhigher order of reasoning questions. We could separately measure each skill of a language model on those\ncharacteristics to estimate the capacity to answer using decomposition. Using data from BIG bench, we\ncreated a summary evaluation of similar QA capacities comparing human performance to open source LLM\nmodels (see 1). However, it will not assess the end-to-end capacity to provide a relevant final answer. Current\nsystems like ChatGPT which solves some complex questions, with high differences in quality but a clear\nimprovement curve, mainly used human feedback for alignment and evaluation [166]. A path investigated\nin papers WebGPT [164] and Constitutional AI [5] is to build Elo-style preference maps (e.g., helpfulness vs\nharmlessness, compute efficiency) and seek Pareto frontiers.\n4.1.5 Explainability, truthfulness, hallucination metrics. Papers reviewed often highlight the recurrent problem\nof hallucination [97] and urge for explainability [119, 234] and truthfulness [134] when delivering an answer.\nConventional metrics measuring quality of writing including answers are not adequate for quantifying the\nlevel of hallucination [97]. Except human evaluation, there are no mature metrics:\n– statistical metrics mainly focus on lexical matching [97] such as PARENT-T, bag-of-vectors sentence\nsimilarity (BVSS) [154], Knowledge F1.\n– model based metrics expect to handle more complex syntactic and semantic variations: they can\ncompare LLM performance on known gold answer in different scenarios (e.g. QuestEval [200]) or on\nfree parameters using LLM-as-judge approach. When using LLM-as-judge to scale evaluation, it is\nnecessary to add a human based calibration with human reviews to mitigate bias and ensure alignment.\n– faithfulness/attribution metrics for retrieval-grounded QA are increasingly used in practice (e.g.,FActScore\nfor atomic factual precision; RAGAS for groundedness and context/answer relevance) to complement\nhuman judgments; these are now de facto in many RAG evaluation stacks.\n– human evaluation is the most commonly used method considering the currently imperfect automatic\nmethods, most common usages are: (1) scoring: annotator rate the evaluation level out of a range; (2)\ncomparing: annotator compares the output texts with baselines or ground-truth references.\nWe identified three types of explanation in the literature [234]: highlights, free-text, and structured expla-\nnations. Those explanations could be intrinsic, explaining LM internal logic, or extrinsic, related to external\nsources and proofs. Therefore some metrics for the extrinsic explanation could be the number of sources,\nauthority and reliability of sources. The intrinsic explanation could be measured trough their quality: com-\npactness (short and coherent), sufficiency, comprehensiveness. Some indirect metrics could be related to the\nexplaination task performance (source identification, fact-checking, coherence...). Explaination are usually\nevaluated on plausibility and faithfulness (coherent decision process), a common approach is to provide a\nchain of facts that detail the reasoning steps to reach an answer.\nFor further details, we invite you to look into the taxonomy in Table 2 [119], main references [97, 119, 134,\n234], and research topics in the further section \"Hallucination & credibility\".\n4.1.6 Domain/task matrix of performance. As performance of a model is unequal depending on knowledge\ndomain and tasks [8, 132], metrics assessment should be segmented per knowledge domain & key tasks within\na matrix of comparison or database like in the 42 scenarios of HELM [132].\n4.2 Cost functions\nCross-entropy (negative log-likelihood) is the primary objective used to train and fine-tune language models.\nThis objective is adapted to each training phase (see sections 5.3, 5.5, 5.7, 5.8) and admits richer variants for\nknowledge distillation [235]. We will see in later sections that it is frequent to add a reinforcement learning\nmechanism to align QA with humans’ expectations. Importantly, this RLHF/RFT stage is not a simple additive\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n16 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nTable 2. Taxonomy of existing explainable evaluation metrics (extracted from table of Leiter et al. [119])\nWork Type Method Goals\nEval4NLP 2021: (Fomicheva et al. 2021) Various\nRubino, Fujita, and Marie (2021) FI Expl. by Design AL\nTreviso et al. (2021) FI Various AL\nSemEval 15/16: (Agirre et al. 2015, 2016) Various\nMagnolini, Feltracco, and Magnini (2016) CAl Neural Networks AL, E\nYuan, Neubig, and Liu (2021) CA Generation Prob. E\nAdversarial Attacks (Section 7) EbE Perturbations D, E\nKaster, Zhao, and Eger (2021) EbS/CA Linear Regression D, E\nSai et al. (2021) CA Perturbations B, D, E\nThe first column is the research work reference. The second is the explanation types: Concept Attribution\n(CA), Chunk Alignment (CAl), Feature Importance (FI), Explanation by Example (EbE) and Explanation\nby Simplification (EbS). The column “Goals” specifies which aspect is measured amongst (B)ias detection,\n(D)iagnosis, (E)xpressiveness and automated labeling(AL).\nterm of “CE +reward. ” After supervised fine-tuning, the policy is optimized tomaximize an estimated reward\nwhile staying close to a reference policy via a KL penalty; e.g.,\nmax\n𝜃\nE𝑥,𝑦 ∼𝜋𝜃 [𝑟𝜙 (𝑥,𝑦)] −𝛽KL(𝜋𝜃 (·|𝑥)∥𝜋ref(·|𝑥)),\nas in PPO-based RLHF [168]. Cross-entropy is used in earlier stages (pre-training/SFT) and may be mixed\nwith PPO gradients (e.g., PPO-ptx), but the RL objective itself is distinct from CE.\nEarly large language models were optimized with the negative log-likelihood (cross-entropy) loss, which\nremains the foundation for pre-training and supervised fine-tuning of QA systems [235]. Since 2023, however,\nthe design of cost functions has diversified to address alignment, reasoning, and hybrid retrieval-based\narchitectures. The current landscape can be grouped into four main families:\n(1) Likelihood-based objectives. Token-level cross-entropy or its masked variants are still used during\npre-training, mid-training, and task-specific fine-tuning (§5.3–§5.7). They model next-token prediction\nor reconstruction and provide the base likelihood signal for all later objectives.\n(2) Preference-based objectives. Reinforcement learning from human feedback (RLHF) [168] has evolved\ntoward lighter, reinforcement-free formulations that directly optimize preference data. Notable examples\ninclude Direct Preference Optimization (DPO) [186], Odds-Ratio Preference Optimization (ORPO) [79],\nand Kahneman–Tversky Optimization (KTO) [54]. These objectives are widely adopted in 2024–2025\ndue to their simplicity and lower cost, especially in open-source pipelines; however, multiple studies\nreport that carefully tuned PPO-based RLHF can outperform DPO on several benchmarks, so the choice\nis task- and resource-dependent [90, 247].\n(3) Reinforcement and reasoning-oriented objectives. A recent line of work termed Reinforcement\nFine-Tuning (RFT) optimizes verifiable rewards that target reasoning quality (e.g., correctness and\nformat) rather than only final answers. The Group Relative Policy Optimization (GRPO) algorithm used\nin DeepSeek-R1 [70] exemplifies this trend; GRPO removes the critic and estimates baselines from\ngroups of samples, and reports strong gains on mathematical and scientific QA.\n(4) Process-level and hybrid objectives. Step-wise supervision with process reward models (PRMs) [133]\nand verifier-guided learning introduces auxiliary losses on intermediate reasoning steps, improving\nfaithfulness and robustness. In retrieval-augmented and agentic settings, models are also trained with\nRAG-aware losses, e.g., Self-RAG [3], which adds control tokens and reflection losses for retrieval and\nself-critique decisions.\nIn contemporary systems, these objectives are most often optimized sequentially across stages (pre-training\n→SFT →preference tuning →RL/RFT) rather than as a single simultaneous loss [168, 280]. When objectives\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 17\nTable 2. Comparison of representative long-form answer datasets used for QA-style evaluation.\nDataset name Type Task Format Answer length Evaluation metric Made by # In-stances\nELI5 Open-domainLFQA Explanatory answersfrom web evidenceFree-form paragraphs;evidence docs ≈130 words (an-swers) ROUGE-1/2/L;human flu-ency/correctness/preference\nFAIR 270k\nEXPERTQA Expert LFQALong answers with cita-tions Attributed answers +claim judgments≈160 words ROUGE; QA-basedfactuality (QAFactEval);claim-level FActScore\nUPenn, UW 2.2k\nBioASQ (Task B,ideal) Biomedical ex-pert QA Ideal summary + exactanswers Multi-sentenceideal+evidence Cap≤200 words(ideal) ROUGE-2/ROUGE-SU4+ manual (ideal); EM/F1(exact)\nBioASQ consor-tium ∼5.4k(train)\nGovReport Long-docsumm.(gov) Summarize govt. re-ports Report→ multi-paragraph abstractivesummary\n≈553 words (sum-mary) ROUGE-1/2/L; QA-based APESsrc; humanUMich & UIUC 19.5k docs\nSummScreenDialoguesumm.(TV) Summaries of TVepisode transcriptsTranscripts→humanrecaps ∼114 / 381tokens(FD/TMS recaps)ROUGE; entity-centric(character/pair over-lap); human\nTTIC & DukeUniv. ∼27kepisodes\nSQuALITY Question-focused long-docsumm.\nSummaries that answerquestions Story + question→4ref. summaries 237tokens(resp.)/442tokens(plot)Human ratings (correct-ness/coverage/overall);ROUGE/METEOR/BERTScore(low corr.)\nNYU (MLL) 500 Q / 2krefs\nAQuaMuSeQuery-basedMDS qMDS across web docsQuery +∼6 docs→summary ≈106 words (sum-mary) ROUGE-1/2/L Google Re-search 5.5k\nBookSum(chapter) Long-narrativesumm. Chapter-level narrativesummarization Chapters→abstractivesummaries ∼505tokens(sum-mary) ROUGE-1/2/L;BERTScore; humanLikert\nSalesforceResearch 12.6k chap-ters\nBookSum(book) Long-narrativesumm. Book-level narrativesummarization Books → multi-paragraph summaries∼1,167 tokens(summary) ROUGE-1/2/L;BERTScore; humanLikert\nSalesforceResearch 405 books\nare optimized jointly (e.g., monolithic preference alignment or multi-task RAG/verifier training), a composite\nobjective can be written as:\nLtotal = LCE +𝜆1 Lpref +𝜆2 LRL/RFT +𝜆3 Lproc/RAG,\nwhere the weights balance next-token likelihood, preference alignment, reasoning reinforcement, and\nretrieval/verification feedback. A concrete joint example is ORPO, which appends a log-odds–based preference\npenalty to the standard NLL during SFT in a single stage [79].\nBy contrast, PPO-based RLHF typically optimizes an expected-reward objective with a KL penalty to a\nreference policy rather than an additive CE term:\nmax\n𝜃\nE𝑥,𝑦 ∼𝜋𝜃 [𝑟𝜙 (𝑥,𝑦)] −𝛽KL(𝜋𝜃 (·|𝑥)∥𝜋ref(·|𝑥)),\nwith optional interleaving of SFT-style updates (e.g., PPO-ptx) [168, 280].\nThis evolution of objectives parallels the move from single-model language modeling toward hybrid, agentic,\nand verifier-coupled architectures capable of tackling complex question answering.\n4.3 Datasets\nTo train and evaluate QA/CQA systems, a variety of datasets have been developed to cover main skills,\ntasks and knowledge which we reference in the following sections. They can assess current performance\nor train model from end-to-end question answering on different field and complexity, to specific logic or\ntask, such as decomposition. We also cover the generation of datasets or improvement of existing ones to\novercome quality issues or challenge to create domain or logic specific dataset. In addition to legacy QA\nsets, community practice now relies on harder multi-task reasoning (e.g. MMLU-Pro, GPQA (graduate-level,\n“Google-proof” science)), plus living leaderboards such as Arena-Hard and LiveBench for open-ended QA. For\nlong-context/multidoc and multimodal QA, newer suites (e.g., LongBench variants, MMMU/MathVista) are\nincreasingly used alongside earlier multi-hop sets. Long-form free answers of QA datasets above a page (> 250\nwords) was rare and are difficult to evaluate, but increasingly needed. Table Table 2 compares representative\nlong-form QA/summarization datasets to help select resources by answer length, attribution requirements,\nand evaluation metrics.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n18 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n4.3.1 Legacy QA/CQA text datasets (monomodal). By “legacy monomodal” we refer to text–only QA corpora\nthat shaped modern information retrieval (IR), machine reading comprehension (MRC), and open-domain QA\nprior to retrieval-grounding and multimodality. These resources remain canonical testbeds for span extraction,\nshort-answer and multiple-choice QA, and dialogue QA, and are still widely used for pretraining, ablations,\nand backwards-compatible comparisons.\nMS Marco (Microsoft Machine Reading Comprehension Dataset): largest publicly available collection of\nrelevance judgments, with 100,000 to 1,000,000 human generated QA, it has been central to the progress\nin neural IR/QA over the past several years (standard QA task, human performance: Rouge-L: 0.539,\nBLEU-1: 0.485) [132].\nSQUAD (Stanford Question Answering Dataset): 100,000+ questions posed by crowdworkers on a set of\nWikipedia articles (human performance F1-score: 86.8%).\nSQuAD v2 : add 50,000 unanswerable questions written adversarially by crowdworkers to look similar to\nanswerable ones, to avoid training on unreliable guesses on questions (human performance F1-score:\n86.8%).\nTriviaQA : 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored\nby trivia enthusiasts and independently gathered evidence documents, six per question on average,\nwith distant supervision for answering. In comparison to other QA datasets in 2017, TriviaQA (1)\nhad relatively complex, compositional questions, (2) considerable syntactic and lexical variability\nbetween questions and corresponding answer-evidence sentences, and (3) required more cross sentence\nreasoning to find answers.\nMMLU (Measuring Massive Multitask Language Understanding): 15,908 multiple-choice questions pack-\nages across 57 different tasks/datasets from subjects in the humanities, social sciences, hard sciences,\nand many others (unspecialized human performance accuracy: 34.5%).\nNarrativeQA : 46,765 human generated questions & answers requiring understanding of stories (books,\nmovie scripts) requiring summarization (human performance: Bleu-1:44.3, Bleu-4:18.9, Meteor:24.0,\nRouge-L:57.1)\nNaturalQuestions (closed-book, open-book): 323,000 questions and documents (full dataset is 42Gb)\nconsisting of real anonymized, aggregated queries from Google search engine providing several long\ndocuments (e.g. 5 wikipedia pages) with a long answer (e.g. a paragraph) and a short answer (one or\nmore entities) if present on the pages, or marks null if no long/short answer is present (standard human\nperformance: short answers F1: 57.5%, long answer F1: 73.4%).\nQuAC : >100,000 questions and their corresponding answers, based on dialogues between two persons\nwhere many questions requires understanding of the dialog (human performance F1: 80.9%).\nSemi-structured datasets : semi-structured data with tables-and-text are abundant on the web and\nin companies. Wang et al. [225] list the following datasets: HybridQA, OTT-QA, GeoTSQA, FinQA,\nTAT-QA, TAT-HQA, MultiHiertt. Semi-structured dataset could also be samples of JSON, XML.... For\nStructured, graph, table only or SQL like data (not bundled with text), Rogers et al. [190] list the\nfollowing datasets: WikiTableQuestions, TableQA, WikiSQL, WikiOps.\nCQA on knowledge bases datasets : knowledge bases like ontologies and knowledge graphs offers\nvaluables structured symbolic data (e.g. Wikidata, all resources from lod-cloud.net...) not always easy\nto query to non experts. Lan et al. [115] list the following datasets: WebQuestions, ComplexQuestions,\nWebQuestionsSP, ComplexWebQuestions, QALD series, LC-QuAD, LC-QuAD 2.0, MetaQA Vanilla,\nCFQ, GrailQA, KQA Pro.\nDomain specific datasets are numerous and can be specific to sectors and perimeters (e.g. Covid19 in\nmedical sector, Qasper QA about research papers in NLP [41]).\n4.3.2 Hard tasks datasets. . We use \"hard tasks\" to denote stress tested tasks that probe different types of\nreasoning, multi-step deduction, long-range dependencies, and robustness under minimal spurious cues [132].\nThey complement legacy monomodal QA sets by focusing on capabilities that typically limit end-to-end\ncomplex QA systems. In practice, frozen test sets are paired with living leaderboards that resist contamination\nand track progress over time; see the evaluation stack in Table 3.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 19\nBBH and BBEH : BIG-bench [8] collects 200+ tasks across linguistics, math, science, and commonsense;\nits \"Hard\" selection (BBH) aggregates tasks that remained difficult for earlier LLMs and serves as a standard\nreasoning stress test. BIG-bench Extra Hard (BBEH) [105] further isolates subsets that remain challenging for\nfrontier models, and is increasingly reported alongside BBH to measure reasoning progress. The BIG-bench\nfamily mixes algorithmic, logical, linguistic and temporal competencies that are directly implicated in complex-\nQA decomposition and verification, and interacts well with inference-time strategies (e.g., self-consistency)\nand verifier-based selection discussed elsewhere in this survey. For concrete ceilings, Table 1 summarizes\nrepresentative BBH tasks (human averages/experts vs. top open models); Appendix Table 11 contrasts multiple\nrecent models side-by-side on selected BBEH tasks.\nVery-hard academic QA (closed-ended). To complement BBH/BBEH with exam-style difficulty, recent\nsuites target graduate and upper-undergrad knowledge with contamination-resistant procedures and calibra-\ntion reporting. Notably, Humanity’s Last Exam (HLE) [176] provides ∼2,500 expert-authored, closed-ended\nquestions across > 100 subjects (with a modest multimodal share), and reports both accuracy and calibration\n(e.g., RMS calibration error), using a private held-out split to discourage overfitting. This positions HLE as\na high-signal “very hard” anchor for complex QA beyond BBH/BBEH. See Table 3 for its placement among\nliving leaderboards and metrics.\n4.3.3 Multimodal QA datasets. When answering a question, humans build a coherent understanding of the\nworld by actively exploring and reasoning over multiple evidences (multi-hop) from different modalities\n(multimodal) like illustrated by Yang et al. [252]. Therefore, natural QA requires to leverage more than text\n(natural or structured) like images, videos, sensors... Training & evaluation datasets are emerging like:\nImage QA datasets : Zakari et al. [264] provide a list of images/visual question-answering (VQA) includ-\ning reasoning tasks.\nAudio QA datasets : DAQA [56] on audio temporal reasoning, Clotho-AQA [135] on binary and multi-\nchoice audio QA.\nVideo QA datasets : such as VideoQA [274] for multi-domain, MovieQA [214]/MovieFIB [152]/TVQA [118]/KnowIT\nVQA [65] for movies and shows, MarioQA [ 163] for games, PororoQA [ 111] for cartoons, Turori-\nalVQA [35] for tutorials, CLEVRER [153] for physical & causal reasoning.\nMulti-modal QA datasets : MultiModalQA/MMQA [211] for multi-modal and multi-hop QA, WebQA [20]\non web multi-modal QA, MAQA focus on negation learning and testing [122].\nUnified dataset format is proposed by Xie et al. [241] to unify multiple formats of different modality to\nenable training, inference and evaluation on multi-tasks and sources.\n4.3.4 Structured knowledge datasets. . As seen in first section \"QA/CQA text datasets (monomodal)\", we can\nuse structured (e.g. RDBMS, graph, table) or semi-structured (e.g. table and text, JSON samples) datasets to\nlearn to extract factual information from structured knowledge sources in natural language. These datasets\nare also key to improve general or domain specific reasoning abilities as we see in further section 4.3.7.\n4.3.5 Decomposition and multi-hop datasets. . Decomposition skill is required for CQA to break down\ncomplexity, and the related ability to resolve it in multiple hops or steps. Table Table 3 summarizes widely used\nmulti-hop QA datasets [156] with hops, context granularity, and answer types; use it to select datasets aligned\nwith your target reasoning depth and supervision format. BIG et al. [8] also provide advanced decomposition\nand multi-step tasks datasets in 4.3.2 such as strategyQA or multistep arithmetics. In order to improve problem\nspecific decomposition and resolution ability, emerging datasets are providing reasoning decomposition\nexamples to be provided in context like chain-of-thoughts (e.g. FLAN CoT dataset[33]). They are mainly used\nas examples to be provided with the question but could be also used at training. In a different manner, Meta’s\nGalactica model was trained on scientific papers where step-by-step reasoning were wrapped between 2\ntokens <WORK>[217] both to explicitly learn reasoning and activate working memory which lacks in standard\nLLM.\n4.3.6 Instructions (IFT, CoT) datasets. . Instructions fine-tuning (IFT) are collection of written instructions\nused to teach model user intent declaration to solution logic & format which can be model generated such\nas: Unnatural Instructions: [83], large community effort Super-natural instructions [230], small high-quality\ncrafted [229], converted existing large datasets to instructions [91], NaturalInstructions [161].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n20 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nTable 3. Multi-hop QA datasets (base: Mavi et al. [156], updated to 2025) - OD: Open domain context, MCQ\nA: multi-choice question with A being the number of possible answers\nContext\ngranularity Dataset Total context\nof dataset Context source Domain Number of\nquestions\nContext\nper question\nAverage\n# hops Answer type\nPassage HotpotQA Wikipedia Wikipedia Generic 112,779 10/0 ODa 1/2/3 Span\nTable,\nPassage HybridQA Tables: 13k\nPassages: 293k\nWikitables,\nWikipedia Generic 69611 1 table\npassages 2/3 Span\nSentence NarrativeQA Books: 783\nMovies: 789 Multiple Fiction 46765 1 story - Generative\nSentence MultiRC 871 Multiple Generic 9872 1 passage 2.37 MCQ A : 5.44\nPassage Medhop Medline Medline Medicine 2508 OD - MCQ A : 8.9\nPassage Wikihop |Wikipedia| Wikipedia Generic 51318 OD - MCQ A1:19.8\nSentence QASC Core: 928\nOther: 7672 WorldTree Science 9980 OD 2 MCQ A:8\nSentence OpenBookQA Core: 1326\nOther: 6000 WorldTree Science 5947 OD 2 MCQ A:4\nPassage 2WikiMultiHopQA Wikipedia +\nWikidata\nWikipedia,\nWikidata Generic 192,606 10 paragraphs\n(distractor) 2/3/4 Span / Yes–No;\nevidence triples\nPassage MuSiQue-Ans Wikipedia Wikipedia Generic 24,814 20 paragraphs 2/3/4 Span\nMulti-doc FanOutQA English Wikipedia Wikipedia Generic 1,034 ≥5 docs\n(avg ∼7) ≈7 Short generative\n(list/number)\nPassage MoreHopQA Derived from\nHotpotQA, 2Wiki, MuSiQue Wikipedia Generic 1,118 varies 2–4 + extra step Generative\nMulti-doc MultiHop-RAG English news\narticle corpus News News 2,556 2–4 evidence\npieces 2–4 Short answer\nPassage MEQA WikiEvents-based\ndocuments Wikipedia/IE Event-centric 2,243 intra-/inter-doc multi-step Entities (lists)\n+ explanations\n4.3.7 Reasoning datasets. . Dedicated datasets for specific reasoning abilities [183] have been developed, or\nexisting sets could be derived to take advantage of different abilities.\n(1) general combination of reasoning such as bAbI tasks categorized by type & complexity;\n(2) spatial or temporal reasoning: spatial question (e.g. SpartQA), event order (e.g. QuAIL, TORQUE),\nevent attribution to time (e.g. TEQUILA, TempQuestions), event duration (e.g. MCTACO, QuAIL),\ntemporal commonsense knowledge (e.g. MCTACO, TIMEDIAL), questions where the correct answers\nchange with time (e.g. ArchivalQA, SituatedQA), temporal reasoning in multimodal setting (e.g. DAGA,\nTGIF-QA); additionally, recent synthetic, contamination-resistant temporal logic suites (e.g., Test of\nTime) systematically probe LLM temporal reasoning outside world knowledge memorization [55].\n(3) belief states (e.g. Event2Mind, QuAIL);\n(4) causal relations (e.g. ROPES, QuAIL, QuaRTz, ESTER); large-scale, structured causal benchmarks such\nas COLD (closed daily-activity graphs) [101] and long-chain visual causal QA like CausalChaos! [170]\nstress multi-step causal inference with explanations.\n(5) other relations between events such as subevents, conditionals, and counterfactuals (e.g. ESTER); event-\ncentric, multi-hop question answering with explicit explanation chains (e.g., MEQA) targets reasoning\nover event–entity and event–event relations [124].\n(6) entity properties and relations : social interactions (e.g. SocialIQa), properties of characters (e.g. QuAIL),\nphysical properties (e.g. PIQA, QuaRel), numerical properties (e.g. NumerSense); complex aggregative\nreasoning that composes extracted facts into tabular calculations is captured by TACT (text →tables\nwith arithmetic/aggregation) [19].\n(7) tracking entities: across locations (e.g. bAbI), in coreference chains (e.g. Quoref, resources in the\nWinograd Schema Challenge family).\n(8) multi-document / multi-hop reasoning datasets designed to discourage extractive shortcuts and test\ngenuine composition: MoreHopQA extends Hotpot/2Wiki/MuSiQue with generative, higher-difficulty\nmulti-hop questions [197]; MultiHop-RAG benchmarks retrieval and reasoning over multi-evidence\nchains for RAG systems [213].\n4.3.8 Explainable & truthfulness QA datasets. . The veracity and explainability of an answer is a significant\nchallenge for language models where answers are mostly provided without evidence, logic, confidence/trust.\nExplainability can be trained by models or evaluated through different tasks like source highlighting or URL\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 21\nTable 4. Explainability tasks datasets (data from Wiegreffe and Marasović [234] enriched)\nEx type Dataset Task Ex Format Made by # Instances\nhighlight\nTRUTHFULQA check false belief & ref in QA URL ref experts 817\nTriviaQA with evidence or filtered QA with evidence URL ref crowd & experts 95k / 2099\nFEVER or filtered (GopherCite) verifying claims from text sentences or URLs crowd 136K\nWIKIQA open-domain QA sentence crowd + authors 1,473\nMULTIRC reading comprehension QA sentences crowd 5,825\nHOTPOTQA reading comprehension QA sentences crowd 112,779\nHanselowski et al. verifying claims from text sentences crowd 6,422 (varies)\nCoQA conversational QA none crowd 127K (1 or 3)\nCOS-E v1.0 [100] commonsense QA none crowd 8,56\nCOS-E v1.11 commonsense QA none crowd 10,962\nBOOLQ reading comprehension QA none crowd 199\nSCIFACT verifying claims from text 1-3 sentences experts 995 (1-3)\nKutlu et al. webpage relevance ranking 2-3 sentences crowd 700 (15)\nfree-text\nNaturalQuestions QA with evidence sentences crowd & experts 323k / 7638\nor filtered version (GopherCite)\nELI5 or filtered (GopherCite) long QA sentences crowd & experts 270k / 3999\nJansen et al. science exam QA authors 363\nLing et al. solving algebraic word problems auto + crowd 101K\nLIAR-PLUS verifying claims from text auto 12,836\nCOS-E v1.0 [100] commonsense QA crowd 8,56\nCOS-E v1.11 commonsense QA crowd 10,962\nECQA commonsense QA crowd 10,962\nPUBHEALTH verifying claims from text auto 11,832\nESPRIT reasoning about qualitative physics crowd 2441 (2)\nstructured\nProofWriter Reasoning QA with proof rules, QA, chain of facts 500k\nWORLDTREE V1 science exam QA explanation graphs authors 1,68\nOPENBOOKQA open-book science QA 1 fact from WORLDTREE crowd 5,957\nWORLDTREE V2 science exam QA explanation graphs experts 5,1\nQED reading comp. QA inference rules authors 8,991\nQASC science exam QA 2-fact chain authors + crowd 9,98\nEQASC science exam QA 2-fact chain auto + crowd 9,980 ( 10)\nYe et al. SQUAD QA semi-structured text crowd + authors 164\nYe et al. NATURALQUESTIONS QA semi-structured text crowd + authors 109\nR4C reading comp. QA chains of facts crowd 4,588 (3)\nSTRATEGYQA implicit reasoning QA reasoning steps w/ highlights crowd 2,780 (3)\nproviding and importance, claim check, commonsense check, answer explanation, logic check...\nLeiter et al. [119] propose three types of ground truth explanations: highlights (rationales or feature impor-\ntance), free-text explanations, and structured explanations. In Table 4 we enriched an existing comparison [234]\nlisting important datasets with explainability tasks in field with number of instances, mode of creation,\nexplanation type and format, task.\nRogers et al. [190] propose an \"evidence format\" for the explainable part of a dataset composed of Modality\n(Unstructured text, Semi-structured text, Structured knowledge, Images, Audio, Video, Other combinations)\nand Amount of evidence (Single source, Multiple sources, Partial source, No sources).\n4.3.9 Local & multi-lingual datasets. . Multilingual datasets can not only help to assess or expand the answering\nskills to new languages, but also to transfer or share concepts between languages: CCQA focuses on general QA\npre-training, MLQA on extractive question answering, MKQA on open domain QA, TydiQA is a topologically\ndiverse QA dataset to learn more robust concept and answer without the need of translation, XQuAD\ndemonstrates performance on pre-training mono-lingual and fine-tuning on new languages, MGSM deals\nwith solving math problems and transfering reasoning abilities across 10 languages.\n4.3.10 Dialogue datasets. . The \"Dialogue System Technology Challenge\" started as an initiative to provide a\ncommon testbed on dialog state tracking and is now a reference in terms of dialog dataset [266] with each\nyear several tacks (challenge). The most recent 10 tracks were released for DSTC10 and DSTC11 in 2022.\nSome other reference datasets are: CommaQA on complex tasks solving by talking to agents, SODA with\nmillion exchange with social commonsense contextualization, DeliData for multi-party problem solving with\ndeliberation, TIMEDIAL for temporal commonsense reasoning.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n22 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n4.3.11 Synthetic or improved datasets by generation. . Creating datasets is very expensive but often necessary\nfor domain adaptation. A growing trend is the generation of synthetic QA datasets from models [95] or\nunstructured text using different techniques. Yu et al. [258] compares ICT, GPL, GenQ, Promptagator and\nCOCO-DR. Dataset generation can also be used to robustify reasoning skills learning. Trivedi et al . [220]\nconvert an existing QA dataset provided with question decomposition meaning representation (QDMR)\nto generate new contexts and questions. Some other techniques like natural language augmentation [ 46]\naims at enriching existing datasets for a more robust training through transformation and data filtering. An\ninteresting paper from Yuan and Liu [260] highlights the \"signals\" present in datasets for learning knowledge\nand capabilities and propose to restructure formats of pre-training datasets to improve learning for different\nof signals (e.g. skills or knowledge type).\nNew practice: for retrieval-grounded complex QA, synthetic data pipelines increasingly include re-\ntrieval/critique labels (e.g., Self-RAG control tokens and reflection signals) so that generated corpora train\nwhen to retrieve, how much to retrieve, and how to self -evaluate. In parallel, programmatic optimizers (e.g., DSPy)\nare used to automatically compile prompts, demonstrations, and retrieval policies that yield higher-quality\nQA data and more robust evaluation harnesses [3, 107].\n5 SOLVING WITH TRAINING\nNow that we surveyed the skills we need to develop, the tasks and challenges to solve, the datasets needed for\ntraining: how to train for complex QA ? We will see the importance of pre-training, domain adaptation and\nfine-tuning.\n5.1 Training dataset quality\nEven before pre-training, a very important step is to maximize the quality of the input datasets (eg, accuracy,\ncompleteness, consistency, relevance, uniformity). It is commonly said that machine learning project could\nspend up to 80% of time on data preparation with a major part dedicated to data cleaning [175]. The efficiency\nof a model task is directly and heavily affected by the quality of the training dataset and its improvement [17].\nWe will not dig this subject because, surprisingly, our review methodology did not preselect any scientific\narticle with the word \"quality\" in their title and is anecdotal in their abstracts. It might be that it is not\nspecific to QA/CQA or language models training but an assumption in any machine learning related subject.\nDevelopers of GPT-3 spent important efforts to filter on a high-quality pre-training dataset [281], and Sun\net al. [209] highlight this important preparation task in similar search tasks.\n5.2 Type of LLMs training\nAs introduced in the \"core concepts\" section, training can apply to:\n(1) a pretrained language model (PLM) which is trained without supervision (unsupervised or self-\nsupervised) mainly on large text (e.g. reddit, Wikipedia...) to discover general knowledge and logic. It\ncould then be re-used, optionally augmented with a \"head\", to be further trained (mid or post-training)\non specific tasks (supervised training), on preferences (alignment), on new reasoning capabilities. It can\nalso be trained on very large corpus of data to uncover enough knowledge and logic to be used \"as is\"\nwithout additional training but oftenly some additional instructions to better align with requester ex-\npectations (e.g. AI chatbot services). Kalyan et al. [104] identify several type of pre-training: Pretraining\nfrom Scratch (PTS), Continual Pretraining (CPT - initialization from an existing PLM), Simultaneous\nPretraining (SPT - synchronized mix of general and domain-specific corpus from beginning), Task Adap-\ntive Pretraining (TAPT - continually adapts mix of general and specific training examples), Knowledge\nInherited Pretraining (KIPT - adds knowledge distillation in the process).\n(2) fine-tuned model on specific task(s) in a supervised manner (each training example is provided with\ninput and expected output solution), by re-purposing a pre-trained language model.\n(3) adapting an existing model (PLM or fine-tuned model) to a new domain of knowledge ( domain\nadaptation - e.g. COVID19 terms and facts) or to new task(s) ( knowledge transfer ) leveraging\nexisting knowledge and logic in the model.\n(4) a distinct mid-training stage (continued/skill-targeted training) that sits between generic pre-training\nand post-training alignment: (i) CPT/DAPT/TAPT to expand domain or task coverage with largely\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 23\nunlabeled corpora; (ii) retrieval-/tool-aware continued training (e.g. learning when to retrieve, how much\nto retrieve, and how to critique) to support hybrid pipelines; (iii) long-context adaptation to sustain\nmulti-document reasoning; and (iv) privacy-/federation-constrained adaptation (e.g. offsite/federated\nrecipes) for sensitive deployments (see subsection 5.4).\n5.3 Pre-training techniques\n5.3.1 Self-supervised learning (SSL). This type of machine learning technique, largely used for PLM, trains on\na dataset without any pre-labeled outputs. Instead, the model must learn to generate the correct output itself,\nusing information from the input data. It is often based on an unlabeled training converted to a supervised\ncoherence task. Kalyan et al. [104] identify three major techniques:\nGenerative SSL , depending on chosen technique, the model learns to predict different scenarios: (1)\nnext token based on current tokens (CLM (causal language model) is used by GPT-1, GPT-2, GPT-3\nmodels); (2) masked tokens in a phrase or MLM (masked language model is the most used technique,\nbut variants exists such as TLM (translation language model) or Seq2SeqLM used by RoBERTa, XLM,\nXLM-R models); (3) reconstruct original text which has been corrupted (denoising autoencoder, DAE, is\nused by BART, mBART models).\nContrastive SSL augments learning by comparison. It is not used alone but to further improve a model\nlike in continual pretraining, to learn sentence-level semantics. Different techniques exist such as\nnext-sentence prediction NSP [45], sentence order prediction (SOP), simple contrastive learning like\nSimCSE [64], bootstrapping with BYOL [67], cross lingual contrastive pretraining with XLCo [29].\nAdversarial SSL learns by distinguishing corrupted tokens (replaced or shuffled), can be used alone or\nin continual pretraining like contrastive. Different techniques exist: replaced token detection (RTD -\nused by ELECTRA model), multi-lingual replaced token detection (MRTD) is used by XLM-E model,\ntranslation replaced token detection (TRTD), shuffled token detection (STD) is used by RoBERTa model.\nHybrid SSL uses more than one type of SSL - e.g. U-Palm uses up to 7 denoising objectives as mixtures-\nof-denoisers [215], BERT uses MLM (generative) and NSP (contrastive), ALBERT used MLM and SOP\n(contrastive), infoXLM uses TLM (generative) and XLCo (contrastive).\n5.3.2 Transfer learning, domain adaptation, knowledge distillation. Those techniques are also used as super-\nvised learning & finetuning, we cover them in the next section.\n5.3.3 Program execution learning. This technique[177] learn to mimic inputs to ouputs logics of program\nto capture new logic or more general skills like numerical reasoning, logical reasoning, better multi-hop\nreasoning. This technique useful at pre-training stage can be viewed as a self-supervised learning.\n5.4 Mid-training: continued pre-training for retrieval, long-context and deployment constraints\nWe call mid-training the family of continued, largely unlabeled training procedures applied after generic\npre-training but before (or alongside) post-training alignment. The goal is to endow a pre-trained LLM (PLM)\nwith capabilities that complex QA systems rely on at inference time—retrieval policy, critique/verification hooks,\nlong-context robustness, and privacy-constrained adaptation —without requiring fully supervised task labels.\n(A) CPT/DAPT/TAPT as a stage. Beyond the definitions given earlier, we groupcontinual pre-training and its\ndomain/task-adaptive variants as a distinct stage to expand knowledge and styles relevant to downstream QA\nwithout overfitting small datasets. This reduces the burden on post-training alignment for domain transfer\nand improves robustness in knowledge-intensive pipelines.\n(B) Retrieval -/tool-aware continued training. Mid-training can teach models to decide when to retrieve,\nhow much to retrieve, and how to critique generated answers. Self-RAG trains special control tokens for\nretrieval decisions and self-evaluation and shows reliable factuality/grounding gains on QA [3]. Salemi and\nZamani [193]’s methodology further clarifies how to evaluate retrieval quality inside RAG systems. CRAG\nbenchmark [253] operationalizes corrective retrieval and attribution requirements that modern mid-training\naims to satisfy.\n(C) Long-context adaptation. [26, 162] For multi-document reasoning and book-length contexts, mid-training\nincludes position-handling and train-short, test -long strategies (e.g., ALiBi/positional interpolation) combined\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n24 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nwith continued training on long inputs so that later SFT/RLHF does not shoulder length generalization alone.\nThis ties directly to long-context QA evaluations highlighted in section 4.\n(D) Privacy -/federation-constrained adaptation. In sensitive settings, mid-training adopts offsite or federated\nrecipes so adaptation occurs where the data lives. Hong et al . [80] introduced DP-OPT, which privately\nlearns prompts/adapters client-side and transfers them to cloud models, offering a practical bridge between\npre-training and post-training in privacy-constrained QA deployments.\nWhen to use mid -training vs. SFT/PEFT?. Use mid-training when you needbehavioral priors (retrieval/tool use,\nlong-context coping, privacy constraints) that generalize across tasks and reduce label needs. Use SFT/PEFT\nrather for formatting and instruction following or tight training latency constraints. In practice, many strong\nQA stacks follow: pre-training →mid-training for retrieval/long -context/constraints →SFT →preference\noptimization (DPO/ORPO/KTO) or RLHF , with verifiers/PRMs applied at inference.\n5.5 Post-training: Supervised fine-tuning\nSupervised learning is the ancestor and most well-known ML technique. It trains on labeled dataset to predict\nthe expected ouput from given input. This allows the model to learn from the data and make predictions about\nnew, unseen data but similar task. This assumes that dataset is representative of new, unseen data. We will\nsee in sections 5.7 and 7 that task specific fine-tuning, which can require a lot of compute and examples, can\nbe avoided via complementary strategies like prompt engineering, tuning adapters, soft prompts prefix, late\nprompts.\n5.5.1 (Task specific) Vanilla Fine-Tuning. is commonly used to refer to the standard method of fine-tuning\nonly the few layers near the output of the pre-trained model (\"the head\") with a task-specific loss. Kalyan et al.\n[104] highlight that the main drawback is that PLM having large parameters are prone to overfit on small task\nspecific datasets. Intermediate fine-tuning or multi-task fine-tuning overcome this.\n5.5.2 Multi-task learning. trains a model to perform multiple different tasks to help the model learn [104]\nmore generalizable features (regularization effect), and improve its performance on multiple tasks (transverse\nknowledge and skills acquired from multiple datasets). Raffel et al. [187]demonstrated how one model can reach\nstate-of-the-art on many different tasks, and the field did stop since. This learning can be done simultaneously\non all tasks [143], in sequence [151], mixed [178], or optimized per task (e.g. hypernetwork [96]). Multi-task\nlearning can spread on similar tasks from different domains and cross-language (e.g. similar summarization\ntasks), or related auxiliary tasks [100] to improve different skills.\n5.5.3 Instruction fine-tuning. can highly increase the number of tasks, reasoning capabilities and global\nperformance of a model [33] by finetuning a pre-trained multi-task model with example with instructions to\nbetter align with intent, expected task reasoning and format.\n5.5.4 Transfer learning, knowledge & domain adaptation, continual learning. leverage knowledge and/or logic\nof an already trained model to re-use it in a target domain or application, which we then fine-tune. This\nenables faster adaptation to target usage and allows to address a task even when there is not enough data\navailable. Transfer learning could be further divided into inductive (related task) and transductive (same task,\nnew domain) learning, unlabeled to labeled transfer (similar to unsupervised pre-training to fine-tuning), and\nfeature and parameter transfer (capture high level concepts of domain). A model can be very efficient on a\ngiven knowledge domain in 2021 but will be later enable to process new questions requiring new or updated\nfacts, this can be addressed by continual learning techniques [ 106], or adding context to question prompt\n(subsubsection 7.6.1), or by training to use an external information retriever (e.g. subsubsection 6.4.2).\n5.5.5 Knowledge distillation (KD). [11] enables a smaller, more efficient model (student) to be trained to\nimitate the predictions of a larger, more complex model (teacher), leveraging the knowledge learned by the\nlarger model. For a given QA question/answer pair, it can not only provide answer but rich information like\nconfidence, attention map and activated features. The KD can be jointly used with active learning to reduce\neven more the training examples needed.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 25\n5.5.6 Active learning. enables a language model to be trained on a small initial set of examples and then, in\na iterative manner, the model can request additional labeled data based on its own uncertainty, in order to\nimprove its accuracy on a given task with minimal effort (e.g. Jukić and Šnajder [102]). This highly reduces\nthe manual creation and training time of a dataset when required. This can also help to craft better examples\nand avoid over-fitting due to excessive examples on same subject.\n5.5.7 Meta learning. gives the ability to learn faster new tasks with lesser data and time, like \"learning to\nlearn\" [218]. This ability is well illustrated in the capacity to learn instructions to be addressed to a language\nmodel with the example of Tk-INSTRUCT supporting >1600 NLP tasks from 76 types reaching a performance\nnear SOTA supervised tasks [230].\n5.5.8 Multi-view learning. [131] learns multiple representations or \"views\" of the same input data to improve\nthe model’s performance on a specific task. The idea is to leverage those different representations to better\ncapture different knowledge facets or aspects of the data, leading to a more nuanced and effective representation\nfor the task at hand in order to improve the performance of the model.\n5.6 Post-training: Preference alignment and reinforcement\nHow to create a system able to align to human expectations, enable LLM to develop different reasoning\nstrategies, solve questions step by step, to use tools more efficiently? Recent progress in this last step of the\nLLM training pipeline has markedly improved capabilities: with the same pre-training knowledge, models\nlearn to better capture human intent and expected answer structure, to reason step-by-step, to better parallel\nand sequence tool calls, and to discover new solving strategies; over time this forms a kind of “experience” for\ntackling harder questions. Our survey identifies two main levers: preference alignment and reinforcement.\nIn practice, a strong post-training stack follows:\n(1) Supervised fine-tuning (SFT) on expected answers optionally with rationales/instructions.\n(2) Preference optimization to better match human preferences: RL-based (e.g., RLHF) [4], PPO) or RL-\nfree methods such as DPO [186], ORPO [79], and KTO [54]; efficient RL variants such asReMax [130].\nRecent “reinforcement fine-tuning (RFT) ” strengthens reasoning itself (e.g., GRPO used in DeepSeek-\nR1 [70]).\n(3) Train verifiers, process reward models (PRMs) to score final answers or intermediate steps ; these\nmodels are trained during post-training, and then applied at inference together with self-consistency\ndecoding [34, 228]. See also subsection 8.1.\n5.6.1 Supervision sources: human-in-the-loop (RLHF) and AI-in-the-loop (RLAIF). Human-in-the-loop is meant\nto improve the outcome of an event or process via user input in the system loop. Humans can intervene at\nmany steps in a QA system from task definition or data creation, to final answer assessment. We focus on the\nQA feedback loops. Human can explicitly validate, rank, correct, provide guidance for answering (instructions),\nor implicitly rate via click-through. The outcome of each question should fit the user’s intention, explicit\n(following instructions) or implicit (helpful, truthful, safe) [4]. We therefore need to:\n•capture user rich explicit and implicit feedback through different human-in-the-loop input feedback.\n•estimate and maximize user explicit and implicit intentions satisfaction for each answer and also in\ntotal through diverse reinforcement learning (RL) techniques or RL-free.\nThis is typically done through RLHF (reinforcement learning with human feedback) [ 4, 40, 62, 167, 212].\nWhile RLHF (often with PPO) is foundational but expensive, it is frequently complemented or replaced by\nsimpler, more stable RL-free preference learning methods (DPO/ORPO/KTO) that learns directly from\npreferences without a separate reward model or on-policy RL [54, 79, 186]. These include Direct Preference\nOptimization (DPO) [186], Odds Ratio Preference Optimization (ORPO) [79], and Kahneman-Tversky\nOptimization (KTO) [54], which learn from preference data directly without a separate reward model or\ncomplex reinforcement learning. RLHF can be enhanced by an AI supervision process to better scale (RLAIF),\nreducing human workload and biases, as illustrated in the figure 3.\nCompared to GPT-3 with 175B parameters (SOTA general LLM model in 2022), InstructGPT with only 1.3B\nparameters but built with RLHF is more than 100 times smaller and answers are 85% preferred by humans [168].\nHowever, learning from human expertise has limits:\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n26 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nFig. 3. From reinforcement learning with human feedback to AI feedback in order to scale and maximize\nhelpfulness vs harmless tradeoff (Bai et al. [5], Ouyang et al. [168])\n.\n(1) scaling cost selection : manual labeling of data is slow and expensive so maybe restricted to some\nwealthy organizations or labeled with less expertise.\n(2) labeler biases : longer RLHF training can bias language model with stronger political views and\nundesired goal pursuit.\n(3) expertise problem: some queries require specific expertise; query–teacher selection matters.\n(4) harmlessness vs helpfulness trade-off : optimizing one can hurt the other [5].\n5.6.2 Reinforcement learning formulations (background). We can categorize reinforcement learning (RL) [210]\nin this survey as follows:\n•Reward-based reinforcement learning (RL standard technique): the LM is trained to maximize a\nreward signal (e.g. positive or negative feedback) that is provided by a human or some other external\nsource. This could involve providing the model with a fixed reward whenever a correct answer is\ngenerated, or using more complex reward functions that take into account the quality and specificity\nof the model’s answers. A prominent modern application of this approach is Reinforcement Fine-\nTuning (RFT). Pushing an LLM to reason via RFT can highly improve the answer quality of the same\nmodel. RFT is often implemented using specific algorithms like Group Relative Policy Optimization\n(GRPO), which optimizes policies with relative, group-wise baselines to explicitly encourage longer,\nhigher-quality reasoning tokens. For example, DeepSeek-R1 used GRPO to incentivize reasoning and\nreported strong gains without explicit chain-of-thought supervision [70].\n•Imitation learning (includes procedure cloning [251]): the LM is trained to imitate the behavior of\nan expert (e.g. human, system...). This can be a useful way to incorporate domain knowledge, search\nmethodology, or other types of expertise into the model, and can help the model learn to generate\nhigh-quality answers more quickly by mimicking.\n•Inverse reinforcement learning [277]: here the reward is not direct, the LM attempts to infer the\nreward function from indirect human feedback or other forms of guidance. This can be a more flexible\napproach, as it allows the model to learn from a wider range of feedback signals and to adapt to changing\nrequirements over time. Inverse RL could be classified as a type of imitation learning.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 27\n5.6.3 Where to learn in the CQA pipeline. This loop acts at multiple CQA stages; for each, we indicate whether\nit is mainly Alignment (pref. learning), Reinforcement (policy optimization), and/or Reasoning (step\nquality):\n(1) Question understanding / context — learn to ask for clarifications and follow instructions(Alignment;\nReinforcement) via SFT + preferences/RLHF; e.g., instruction-following from Ouyang et al. [167].\n(2) Decomposition strategies — learn to plan steps/tools, reward good plans (Reinforcement; Reasoning) ;\nPRMs can score intermediate steps; self-consistency encourages robust plans [228].\n(3) Query construction (prompting) — optimize prompts, formats, and tool call arguments (Alignment);\npreference optimization (DPO/ORPO/KTO) improves adherence to desired styles [54, 79, 186].\n(4) Information retrieval — learn search/browse/citation behaviors (Reinforcement; Reasoning) with\nbrowser-assisted QA and retrieval-grounded scoring (e.g., WebGPT) [164]; ReAct-style search couples\nreasoning and acting.\n(5) Answer generation — align helpfulness/harmlessness and factuality (Alignment; Reinforcement; Rea-\nsoning): preference-based alignment (RLHF / DPO/ORPO/KTO), and verifier/PRM scoring trained\npost-training and applied at inference ; verifiers improve selection among candidates [34].\n(6) Knowledge capitalization — prefer answers with evidence and coherent chains(Alignment; Reasoning);\nPRMs trained on step quality; at inference, self-consistency and reranking select robust solutions [228].\n5.7 Parameter-efficient tuning (PEFT)\nAs per HELM study [ 132] and compared models in the survey, larger models lead to better performance\nalthough some architecture allows to get them a bit smaller. Those large-scale PLM are very expensive\nto retrain or just fine-tune. The parameter-efficient tuning alternative targets a small fraction of model\nparameter update with similar performance than full-model fine-tuning, sometimes better [48]. We frame\nPEFT as selecting an update family and deployment recipe along four axes: (A) memory/throughput budget, (B)\nstructure and placement of updates, (C) task portfolio and domain shift, and (D) system constraints (context\nlength, federation, personalization).\nA. Budget: quantization -aware and memory -efficient recipes. Use low-precision bases plus adapters when\nVRAM is the bottleneck. QLoRA combines 4-bit quantization with low-rank adapters [44]; LoftQ aligns quanti-\nzation and adapter training [129]. LoRAM reduces training memory via optimizer/checkpoint schedules [268].\nCompressed designs (NOLA, VB-LoRA) shrink adapter families by random-basis combination or shared vector\nbanks [113, 127]. Light-PEFT prunes early to preserve accuracy under tight budgets [68].\nB. Structure: how to place and shape updates. Baseline LoRA injects rank-𝑟 adapters into attention/MLP\nprojections [85]; expressivity results guide rank/depth choices [265]. For more capacity at similar parameter\ncounts, choose asymmetric factorization (HydraLoRA) [219], Hadamard high-rank transforms (HiRA) [87],\nor gradient-approximate updates ( LoRA-GA) [227]. If layer utility is heterogeneous, use rank allocation\n(ALoRA) [145]; randomized bases (RandLoRA) regularize training [1]. Structured alternatives like RoCoFT\nconstrain updates to row/column subspaces to improve stability [114].\nC. Portfolio: multi -task, domain, and composition. Adapters enable composition/fusion across tasks and\nlanguages [76, 174]. For domain breadth, combine universal and domain-specific updates (MoDULA) [149],\nand consider multi-task LoRA sharing with task-conditioned heads (MTL-LoRA) [254]. Sparse expert routing\nintegrates with LoRA for multi-domain applications (MOELoRA) [140]. Achievement-based training across\nLoRA experts (C-LoRAE) supports complementary specialization [259]. For structure-heavy summarization\nand extended discourse, use RST-LoRA and LongLoRA [26, 182].\nD. System constraints: federation, personalization, and retrieval. In federated settings, residual adapter\naggregation respects data locality and communication limits ( FRLoRA) [249]. For personalization, adapter\nselection and capacity control must balance privacy, drift, and latency [15]. If the shift is knowledge-intensive,\nretrieval may dominate or complement PEFT; comparative studies offer guidance on PEFT vs. retrieval\nintegration [60, 117].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n28 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nEditing rather than tuning. Where the goal is targeted concept adjustment with minimal retraining, repre-\nsentation editing (RED) can be effective [236]. Activation-sparsity control (DEFT) reduces effective compute\nduring adaptation [192].\nPractical recipe. Default: QLoRA or LoftQ for memory budget; LoRA with rank guided by expressivity\nand validation [44, 129, 265]. If capacity is limiting: asymmetric/high-rank (HydraLoRA, HiRA) or dynamic\nrank (ALoRA) [87, 145, 219]. For many related tasks/domains: composition (MoDULA, MTL-LoRA, MOELoRA,\nC-LoRAE) [140, 149, 254, 259]. For long-context: LongLoRA , RST-LoRA [26, 182]. For edge/federated: FRLoRA [249].\nClassic adapters and prompt/prefix tuning remain strong baselines and compose with the above [76, 120].\n5.8 Techniques for improving training\nAdditionally to the different training options at each phase, these complementary techniques have been proven\neffective for improving training speed, quality, efficiency and safety:\nRegularization and adaptive computation : as in other deep learning settings, dropout, weight decay and early\nstopping curb over-fitting during training. For inference-time efficiency at similar accuracy, confidence-based\nearly-exit cascades and adaptive decoding allocate more compute only to hard inputs [49, 198].\nMixture-of-denoisers: a small extra round of training ( ≈0.1% of pre-training compute) with mixtures of\ndenoisers yields sizable gains in accuracy and reasoning [216].\nCompute-optimal scaling & token budget : training at the Chinchilla compute-optimal point (balancing model\nsize and total tokens) systematically improves downstream performance for a fixed budget [78].\nDomain/data-mixture routing : mixing domains explicitly during pre-training (e.g., DEMix domain experts)\nand embarrassingly-parallel expert training with Branch-Train-Merge reduce negative interference and\nimprove generalization under domain shift [72, 123]. Sparse Mixture-of-Experts further increases capacity at\nfixed FLOPs [58].\nLength generalization (train -short, test -long): positioning schemes such as ALiBi enable training on short\nsequences while generalizing to much longer contexts at inference, reducing training cost for long-context\nmodels [181].\nTasks disambiguation for generalization : Tamkin [212] train on ambiguous examples in different contexts to\ndramatically improve the accuracy of LLMs trained without large-scale human feedback training (RLHF).\nPost-training pruning : Frantar and Alistarh [61] prune model size by 50% of a very large model (GPT family)\nin one-shot without any retraining with a minimal loss of accuracy.\nCross-lingual learning : training with cross-lingual objectives (e.g., InfoXLM) helps learn language-agnostic\nfeatures and improves transfer and robustness across languages [29].\nRed teaming for safety : automatic red teaming generates stress-tests to uncover unsafe failure modes and\nimprove safety alignment [172].\nData restructuring and curation : re-structuring pre-training corpora to expose clearer supervision signals\n(e.g., metadata, task-like formats) improves learning efficiency and downstream QA quality [260].\nTraining on synthetic data and teacher/student : leveraging high-quality synthetic data, often generated by\nmore capable \"teacher\" models, to distill complex reasoning, coding, or mathematical skills into a smaller\n\"student\" model. This \"textbooks are all you need\" approach has proven highly effective for creating strong,\nspecialized models [69].\nParameter-efficient adaptation : as per previous section, for low-budget specialization, parameter-efficient\nfine-tuning (e.g. LoRA) require very low training memory and duration while retaining quality; see subsec-\ntion 5.7 for details [44, 129].\n6 SOLVING WITH HYBRIDIZATION PATTERNS\nTo address the different limits of LLMs and skills identified for complex QA, we referenced architectural\ncomponents which could augment a general-purpose base LLM such as a task-specific fine tuned model, a\nsearch engine, a software, a code interpreter... To help design the different ways to increase an LLM both at\ninference or training, even if some may overlap, we propose to classify them into the following list of key\nhybrid architectural patterns each with description, strengths (S), weaknesses (W), references (e.g.).\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 29\n6.1 LLM base transformer\nIt is the general-purpose backbone, pretrained on web-scale corpora, ready for hybridization by extra lay-\ners/tools/retrieval/verifications/... to extend capabilities [16, 45, 187].\nStrengths: leverage knowledge from large unstructured data, can handle wide and versatile knowledge, some\nlong-range dependencies and reasoning, and serves as the foundation for all subsequent architectures.\nWeaknesses: training is too long/costly to allow frequent update, prones to hallucinate with confidence,\nlimited reasoning without massive scale, no mechanism to protect sensitive data.\ne.g. encoders BERT [45] or its optimized version RoBERTa [144]; decoders GPT2/GPT-3 [16]; encoders-decoders\nT5 [187] or BART.\n6.2 ADD Task specific adaptation, tuning, routing\nThese patterns specialize or route the base model for specific tasks/domains .\n6.2.1 LLM + Task-specific head. : attach supervised head (classification/QA/CRF, etc.) to specialize base\nrepresentations for a target task by training on a labeled dataset [121]\nStrengths: achieves SOTA performance for targeted task and/or domain with lower computational resources\nthan retraining an entire LLM.\nWeaknesses: requires structured dataset for training, limited to the specific task it is designed for, may\nstruggle with more general or open-ended tasks.\ne.g. BART with classification head for question answering [121]\n6.2.2 LLM + Prompt tuning module. : learns best prompt, context, instructions to query a LLM for given tasks\nand domains (or to better fine-tune) without modifying LLM weights [108, 120]. Prompting is further studied\nin 7.\nStrengths: improve LLM performance on single or multiple task with no retraining, and can dynamically\nadapt reasoning skills according to context.\nWeaknesses: highly sensitive to slight prompt variations, may require substantial context, finding the\nright/robust prompt can be complex.\ne.g. prompt optimization programming [7, 28, 108, 262], programmatic retrieval-augmented in-context learn-\ning [107], instructions generation [229].\n6.2.3 LLM + Router or Task discriminator. : routes task/domain to best model/tool/expert with prompt\ninstructions & context [123, 244]\nStrengths: can accelerate LLM training, improve inference LLM performance by routing to the most appro-\npriate model (performance vs resource) with most appropriate instructions.\nWeaknesses: complex to implement and maintain, shared reasoning and long term dependencies might be\ncompromised.\ne.g. universal discriminator for zero-shot generalization [244], Branch-Train-Merge for fast parallel training\nof experts LM [123].\n6.3 ADD Decomposition, multi-processing & fusion\nThese patterns enable multi-step problem solving by planning/decomposing tasks, chaining calls, compos-\ning/ensembling models, and adding generator–verifier loops for robust selection or fusion [34, 49, 228, 256].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n30 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n6.3.1 LLM + Question/task decompose, plan, act module. : efficient break down of complex tasks into address-\nable subtasks following a plan, optionally revised from observations.\nStrengths: efficient at solving more complex tasks requiring multiple steps or sources by converting into\nseveral manageable subtasks and an efficient resolution plan (a priori, iterative or recursive). It can benefit\nfrom incorporating external knowledge sources or reasoning capabilities.\nWeaknesses: more time and resources to implement, depending on implementation may struggle with long\ncontext and reasoning dependencies.\ne.g. agentic dynamic decomposition, like plan–act–observe–reflect, is a major practice (e.g., ReAct/ToT-style\nplanners) using LLM or specific models for the task [ 204], iterated decomposition w/ reasoning process\nsupervision [188], separated reasoning and acting [256], unsupervised QA decomposition [173], successive\nprompting decomposition for CQA [52].\n6.3.2 Cascaded / chained / looped LLM. : solves by multiple LLM calls—possibly iterative/recursive—mirror\nand enable confidence-/cost-adaptive cascades [49, 238]..\nStrengths: facilitates human control over design and execution process, can solve higher complexity problems\nthan LLM core skills by breaking down tasks & design, provides a causal chain useful for explainability, allows\noptimization of the pipeline and leverage specialization to avoid potential bottlenecks or inefficiencies.\nWeaknesses: may be less effective at tasks requiring extensive context and long reasoning dependencies.\ne.g. solving by cacasding language models [49], AI Chains [238], in a collaborative visual chain of prompt [237],\nlogical and robust reasoning with selection-inference [37], human readable multi-step logical deduction on\nscientific QA improving accuracy and faithfulness [36], iterative prompting an LLM [52, 224].\n6.3.3 LLM Ensembling and LLM composing. : combines diverse LLMs/modules via consensus or learned\ncomposition to boost robustness and reduce single-model idiosyncrasies [123, 125]..\nStrengths: improves inference accuracy, generalization and stability by combining diversity.\nWeaknesses: cost & complexity, be aware of trade-offs (e.g. additional computation).\ne.g. ensemble learning for validation and explanation [89], parallel training of expert LLM [123], ensembles of\nLLMs via iterative consensus [125], automatic neural module composition [2].\n6.3.4 LLM + Generators/Verifiers. : couples a generator proposing solutions with a verifier/judge (plus self-\nconsistency) that filters/reranks for improved solutions [34, 128, 228].\nStrengths: can solve complex tasks (large gains on math/code, factual QA), out of training knowledge, by\ncombining generation and adversarial skills.\nWeaknesses: resource-intensive & costly; may not always lead to performance improvements proportional to\neffort.\ne.g. AlphaCode [126], DiVeRSe [ 128], CoRe (cooperative reasoning) [ 279], self-consistency for chain of\nthought [228], training verifiers to solve math [34]. self-grading RAG (generate–critique–select) variants are\noften paired with retrieval.\n6.4 ADD External information & reasoning\nThey leverage external knowledge and tools: These patterns augment the LLM with external knowledge, tools\nand computation—memories [12, 63, 92, 166, 194].\n6.4.1 LM + Memory. : an external working memory stores and retrieves facts, summaries, and states across\nsteps to overcome context limits and support long-horizon reasoning/dialogue [199, 245].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 31\nStrengths: without LLM modification can simulate any algorithm, process unbounded inputs, strengthen\ncontrollability, long-term dependencies (for reasoning, dialogs, summarization, retrieval, algorithmic...) and\nrobustness by incorporating counterfactual & irrelevant contexts.\nWeaknesses: scalability issues with increasing model size.\ne.g. universal memory augmented large LM [ 199], Recurrent Memory Transformer [ 18], long-term open-\ndomain conversations [245].\n6.4.2 LLM + Semantic-unstructured Information Retriever. : retrieves (by similarity) contextual information\nfrom a source to be added to a question or task sent to an LLM in order to augment its knowledge [12, 92].\nStrengths: incorporates any up-to-date external sources without increasing LLM size, allowing much smaller\nmodels (RETRO is 1/25 size of GPT-3 for same perforamnce), control over sources (sensitivity, explainability,\nknowledge update) and a variety of retrieval techniques.\nWeaknesses: may struggle with tasks requiring abstract or creative reasoning, may be limited by the quality\nand coverage of the external sources; in practice, self-/corrective-RAG couples generation with critique and\nretrieval-backed verification to mitigate these issues.\ne.g. Deepmind RETRO [12], Atlas [92], training RL agents to query external knowledge [137]; RAG is now a\ndefault backbone in many QA systems (often combined with verifiers).\n6.4.3 LLM + Symbolic-structured Information Retriever. : queries structured sources (ontologies, graphs,\ndatabase) to extract structured information to be added to a question or task sent to an LLM in order to\naugment its knowledge [180, 241].\nStrengths: allows cold start tasks and fast domain adaptation with low data, follows rules and concepts more\neffectively.\nWeaknesses: neuro-symbolic integration is complex, creating symbolic data is time-consuming and labor-\nintensive.\ne.g. UNIQORN [180], Heterformer [99], UnifiedSKG [241]; RAG with Symbolic/Structured Retriever (e.g.,\nGraphRAG) for graph-guided global+local search across narrative corpora.\n6.4.4 LLM + Tools. : delegates sub-tasks to external tools (search engines, solvers or simulators) on tasks\nhard for LLM [141, 164, 194].\nStrengths: Leverages proven performance and robustness of external software/services in information retrieval\n(IR), logic, world modelling.\nWeaknesses: challenging end-to-end learning and potential complex integration (e.g. additional pre-processing\nor post-processing steps)\ne.g. physics with MindsEye [141], WebGPT [164], SeeKeR [206], Toolformer [194]; mainstream APIs expose\nfunction calling to orchestrate tools.\n6.4.5 LLM + Code interpreter. : generates code to be executed in order to delegate complex or heavy processing\ntasks (LLM can also learn complex logics by program’s input/output traces) [50, 63, 73]\nStrengths: leverage robust reasoning & algorithmic capabilities, and language ecosystem.\nWeaknesses: may struggle with tasks requiring deeper understanding of context or concepts, dependency on\nexternal code interpreters.\ne.g. PAL [63], solving math problems via cooperative reasoning [ 279] or program synthesis [50], LM self-\nimprove its programming capabilities [73], AlphaCode [126].\n6.4.6 LLM + Multimodal. : integrates and reasons over non-textual inputs (image, audio, video, sensors...). [166,\n204, 252]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n32 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nStrengths: leverages knowledge from non-textual sources and combines each modality to enhance under-\nstanding, reasoning and problem-solving.\nWeaknesses: integration complexity (representation, alignment, reasoning, generation...) & cost, increased\ndifficulty in addressing explainability and hallucination issues\ne.g. foundation models and recent trends in multimodal ML [166, 204, 252].\n6.4.7 LLM + Veracity/evidence checker. : provides sources and facts assessment [22, 71, 158]\nStrengths: guarantees information credibility & reliability while mitigating hallucinations by providing\nverifiable sources and evidence assessment.\nWeaknesses: limited by the quality and coverage of external sources, do not eliminate risk of misinformation.\ne.g. GopherCite supports answers with verified quotes [158], logic-regularized reasoning for interpretable fact\nverification [22], survey on automated fact-checking [71], hallucinated content detection [275], RL approach\nfor explainability using entailment trees [ 142]; self-/corrective-RAG couple generation with critique and\nretrieval-backed verification.\n6.4.8 LLM + Temporal/spatial reasoning. : provides temporal (and spatial) reasoning layers, helping with\nordering, duration and event queries [57, 226]\nStrengths: allows temporal estimation, ranking & clustering, reasoning, incoherence detection, addresses\nknowledge forget and update.\nWeaknesses: few resources, efficient integration might be challenging.\ne.g. TimeBERT: Extending Pre-Trained Language Representations with Temporal Information [226], improves\ntemporal reasoning through added audio modality [57].\n6.5 ADD Feedback, dialog, reflection & learning from teacher\nThese patterns incorporatecritique and guidance to align behavior and improve reasoning [4, 84, 168, 171, 205].\n6.5.1 LLM + Human/AI RL feedback. : learns alignment/preference and optimal policy for goals (answer\nquality, safety, data sources...).\nStrengths: support in most critical challenge of LLMs design such as human expectation alignment and\npersonalization, safety and quality control.\nWeaknesses: human feedback and convergence is highly time-consuming. This could be mitigated by\nincorporating active learning and AI feedback learning, but that means added complexity.\ne.g. reinforcement learning with human feedback RLHF [ 4, 40, 168], with AI feedback RLAIF [ 5], experts\nimitation learning [251], web search RL in WebGpt [164], citation RL in GopherCite [158].\n6.5.2 LLM + Dialog module. : enable dialog/state tracking, clarification, disambiguation, and long-context\ncontrol for interactive complex QA [27, 109, 171]\nStrengths: enhances understanding of complex contexts, problems and concepts through human interaction,\nguidance, progressive refinement and problem-solving.\nWeaknesses: might not fit targeted usage format, more time and resources implementation.\ne.g. GODEL [171], OPAL [27], CommaQA [109], ChatGPT.\n6.5.3 LLM + (multi) Teacher. : develop new LLM capacities through expert teaching or supporting on do-\nmains/skills/tasks [84, 235]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 33\nStrengths: accelerates and improves knowledge learning, expansion, adaption, multi-tasking, reinforces\nreasoning capabilities (e.g. temporal).\nWeaknesses: cost & complexity, control of teacher knowledge transfer (e.g. domain scope, biases).\ne.g. Teacher-student architecture survey [84], learning from multiple teachers [235].\n6.5.4 LLM + Reflection module. : improves reasoning and reliability by self-critic/analysis to revise its own\nsteps/outputs using episodic memory[205].\nStrengths: boost LLM internal capacities by self-awareness, thoughts, given goal and reasoning trace.\nWeaknesses: require additional resources (e.g. external memory and management, heuristic to trigger self-\nreflection).\ne.g. Shinn et al. [205] present a reflection agent allowing a LLM to beat previous results by a large margin.\n7 SOLVING WITH PROMPTING\nNow that we have trained models to acquire the required skills to solve a complex QA or rely on hybridization,\nlet’s see how to design questions (i.e. prompt) with proper instructions and context. At the outset, we\ndistinguish tool-use prompting , covered separately in subsection 7.4, because it explicitly bridges prompting\nwith hybridization and agentic control (subsubsection 6.4.4, subsubsection 6.4.5, subsection 8.2).\n7.1 Prompt design (basics)\nLLM can highly improve its ability to solve a problem by leveraging information provided with a question\n(\"problem well stated is half solved\"). Engineering a good prompt (text provided to a LLM when posing a\nquestion) can rival or beat model finetuning on QA in many cases [230]. Different information can be added\nto improve answer success probability while posing a question to a LLM such as additional context and\nknowledge, constraints, instructions and examples ... Question can also be designed to be answered in\nmultiple steps, for example just by asking to answer step by step [112, 232] or reasoning compositionally\nlike humans [51], [52, 276].\n7.2 Context and protocol engineering\nBeyond single strings, modern prompting increasingly treats the context 𝐶 as a structured payload that is\nassembled at inference time from typed components: (i) system/task instructions, (ii) external knowledge\n(retrieved or generated), (iii) tool/function interfaces, (iv) short/long-term memory, and (v) dialog/agent\nstate. The discipline of Context Engineering (CE) systematizes this: retrieve/generate the right information,\nprocess/organize it, then manage/compress/persist it under context-length and latency constraints [ 157].\nConcretely, CE decomposes into: (a) retrieval & generation (e.g., RAG, self-generated hints), (b) processing (seg-\nmentation, ranking, self-refine, structured/relational integration), and (c) management (memory hierarchies,\ncompression, KV-cache policies) [157]. In practice this unifies patterns already surveyed here (RAG: subsubsec-\ntion 6.4.2, memory: subsubsection 6.4.1, tools: subsubsection 6.4.4, multi-agent orchestration: subsection 8.2).\nA key empirical observation is an asymmetry: LLMs—augmented by CE—parse complex contexts well, yet\noften struggle to emit equally sophisticated, long, structured outputs without additional scaffolding [157].\nTyped context assembly at prompt-time. Useful operational decomposition is:\n𝐶 = 𝑐instr|{z}\nrules & rubric\n⊕ 𝑐know|{z}\nretrieved evidence\n⊕ 𝑐tools|{z}\nfunction schemas\n⊕ 𝑐mem|{z}\nepisodic/summary memory\n⊕ 𝑐state|{z}\ndialog/plan state\n.\nThis “fielded prompting” supports: (i) retrieval gating and self-critique (e.g., Self-RAG control/reflection tokens\nfor when/how to retrieve and whether the answer is grounded) [3]; (ii) schema-constrained outputs (JSON,\ntables, checklists) that reduce drift; (iii) memory hooks to carry forward distilled facts (subsubsection 6.4.1).\nEvaluation should pair answer correctness with attribution/groundedness (e.g., FActScore, RAGAS; see section 4).\nFrom CE to Knowledge Protocol Engineering(KPE).. For expert domains, factual context (RAG) is insufficient\nif the model lacks the procedure. KPE proposes translating human SOPs/manuals into a machine-executable\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n34 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nknowledge protocol that constrains and guides the LLM’s reasoning and tool use [267]. The goal is methodology\ninjection: endow a generalist LLM to “think like a specialist” by encoding workflow steps, decision trees,\nadmissible tool calls, success/failure checks, and required artifacts (e.g., citations, tables, calculations).\nKPE—minimal workflow. (1) Protocol extraction: elicit or mine domain SOPs (tasks, sub-tasks, invariants,\nacceptance criteria). (2) Encoding: compile into structured, prompt-time templates (typed fields, role/rubric,\ntool schemas) and step validators (unit checks, equation or API assertions). (3) Execution: drive the LLM/agent\nto produce typed intermediates and invoke tools under the protocol; use retrieval to fill method gaps as well as\nfacts. (4) Verification & audit : ensure evidence-linked outputs and process-level logs (plan/act/observe) for\nhuman review. In regulated or high-stakes settings, the protocol acts as a methodological contract that enables\ntransparency and step-wise audit [267].\nWhen to favor KPE.. Use KPE when (i) domains have stable procedures (e.g., legal HHI analysis; scien-\ntific/biomedical pipelines), (ii) outputs must be auditable/reproducible, or (iii) multi-step tasks benefit from\nexplicit role artifacts (requirements →design →code →tests) with typed checks. KPE composes naturally\nwith CE (RAG/memory/tools) and agentic controllers (subsection 8.2), and complements post-training/RFT\n(subsection 5.6, subsection 8.1).\nSummary. Context engineering provides the prompt-time information logistics (what to include, how to\ncompress/manage); KPE provides the methodological scaffold (how to proceed and verify). Together they turn\nprovide prompting with structured contextual information assembly and protocolized execution , dovetailing\nwith hybrid/agentic and inference-time strategies later in this survey 8.\n7.3 In-Context Learning (ICL)\nIn-Context Learning [16] refers to adapting a frozen LLM at inference time by adding task instructions,\ninformation, demonstrations directly in the prompt. ICL is therefore aninference-time learning via a prompting\nstrategy, not training. ICL can be paired with retrieval capabilities to dynamically include (a) demonstrations\nand (b) evidence snippets (subsubsection 6.4.2); pair with verifier/attribution metrics in evaluation (e.g.,\nFActScore/RAGAS). ICL can (i) rapidly teach formats (e.g., evidence→claim, tool-call JSON), (ii) steer reasoning\nstyles (brief CoT, tabular working), and (iii) adapt to domain terminology without fine-tuning. It composes\nnaturally with agentic controllers that reuse demonstrations across steps and with reasoning-time strategies\nthat sample multiple chains then verify (subsection 8.1).\nWhen to prefer fine-tuning/PEFT instead ? If the task requires strict latency, or needs domain general-\nization and generalization beyond information retrieval, prefer SFT/PEFT (subsection 5.7, subsubsection 5.5.3)\nand keep ICL for light adaptation and formatting control.\nCommon failure modes (and mitigations).\n•Spurious cues / overfitting to exemplars. Mitigate with diverse demos and retrieval gating; add a\ncritic/verifier step (subsubsection 6.4.7, subsubsection 6.3.4).\n•Context overrun / dilution. Trim boilerplate and prefer structured fields; chunk evidence and gate\nretrieval (self-/corrective-RAG, subsubsection 6.4.2).\n•Evaluation pitfalls. Avoid leakage between demo pool and test; report with and without CoT; for\ngrounded QA, include attribution metrics (FActScore/RAGAS) alongside accuracy.\nSummary. We treat ICL as a first-class prompting tool: fast, controllable, and synergistic with retrieval and\nreasoning-time allocation. It stays here (not in training), with cross-references to PEFT (when to train) and to\nagentic controllers (when to route or verify).\n7.4 Tool-use prompting (program-aided & API calls)\nBeyond optimizing the text of a prompt, a distinct family of prompting patterns instructs models to invoke\nexternal tools (e.g., code interpreters, APIs, simulators) during inference. This bridges prompting with hy-\nbridization/agents by turning the model into a planner that delegates sub-steps to executors [194, 204, 256].\nRepresentative instances include using a code interpreter to carry out precise computation and structured\ntransformations [25, 63, 150], and domain simulators (e.g., physics engines) for grounded reasoning about\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 35\nthe world [93, 141]. This section focuses on the prompting interfaces (how to elicit tool calls); architectural\naspects of tools are detailed in subsubsection 6.4.4, subsubsection 6.4.5, and agentic control in subsection 8.2.\nThis is a fast growing field such as the use of code interpreter to enrich LLM answers for complex structures\nand calculations [ 25, 63, 148, 150]; simulators (e.g. physics engine) to simulate processes and aid LMs in\nreal-world reasoning [93, 141]; or any dedicated software tool or AI model [194, 204].\n7.5 Enhancing LLM knowledge or skills via prompting\nPrompt can be engineered in order to enhance model capacities such as (according to taxonomy from Qiao\net al. [183]): knowledge retrieval which could be divided into implicit knowledge to enrich LLM answers\nthrough few-shot prompting and reinforcement learning [138, 139], and explicit knowledge to retrieve and\nprovide in-context labeled examples in prompt to improve explicit knowledge, reduce hallucination and\nenrich LLM answers [207]; Arithmetic reasoning with many different type of reasoning and approaches\n(e.g. [7, 25, 63, 86, 128]); Commonsense reasoning on general background knowledge and reasoning\n(e.g. [112, 138, 139, 150, 228]); Creativity reasoning to support inventive process like automated diverse\nprompting ideas (e.g. [116]); Logical reasoning learnt from synthetic rule bases, entailment trees, or diagnostic\nbenchmarks [36, 37]; Symbolic reasoning [63, 112, 228] uses examples that contain symbolic rationales,\nrules; Multimodal reasoning trained on dataset such as ScienceQA, ALERT (e.g. [147]).\n7.6 Optimization\nIn this section, we review standard prompt based optimization strategies.\n7.6.1 Single-pass prompt optimization. aims to get the best answer from one prompt:\nZero-shot prompting provides a prompt for task without prior training on this task and no additional\ncontext or guidance.\nIn-Context learning and Few-shot Prompting provides prompt with relevant context [16] or demon-\nstration [159] for expected task helping LLM to better answer. Most well-known example is few-shot\nlearning which provides a prompt with a few examples of expected task for helping to generate the\nbest answer.\n(Hard) prompt tuning adjusts the initial prompt, often by trial and error, to improve answer accuracy.\nThis improving process can be manual, automated or even programmed [7, 28, 108].\nSoft prompt tuning creates soft prompts [120] which are concatenated to the input text. Tokens of this\nsoft prompt are learned vectors optimized end-to-end over a training dataset. subsection 5.7 provides\nsome additional details. Some innovative examples are:\n– Exploring Universal Intrinsic Task Subspace via Prompt Tuning: adapt to many NLP tasks with\nsmall-scale data by optimizing only a few free parameters in a unified low-dimensional intrinsic task\nsubspace [184].\n– Compositional Task Representations learn specific codebook for compositional tasks [201].\nChain-of-thought prompting [232] ask to reason step by step and can provide relevant examples of\nmulti-steps of reasoning/thoughts up to the solution to improve reliability or more easily spot errors in\nthe result. It largely outperforms the state-of-the-art results with zero and few-shots learning with the\nsame model on many advanced natural language processing tasks and fine-tuned models trained with\nhundreds times of examples, with the advantage of being interpretable.\nChain-of-hindsight or contrastive prompting [136] provides examples with qualitative feedback\n(e.g. comparison and critiques) to better align the model output to preferences. It is mostly used for\nfinetuning model but could be used when prompting.\nSelf-reflection [205] prompts the model to improve its next action given its previous track and mistakes.\n7.6.2 Multi-step prompt optimization. designs a solving process in progressive prompt steps:\nLeast-to-most prompting [276] improves chain-of-thought with multi-step examples that gradually\nbecomes more specific or complex; Chain-of-thought often performs poorly on tasks requiring to solve\nproblems harder than those in demonstration examples. To tackle this, LtM first reduces a complex\nproblem into a list of easier subproblems, and then sequentially solves these subproblems with gradual\ncomplexity. LtM can be combined with self-consistency to improve robustness.Dynamic least-to-most\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n36 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nprompting (compositionality) [51] refines it with the following steps:(1) prompts LLM to perform a\nsynthatic parsing to create a tree-structured decomposition, (2) select matching demonstration examples,\n(3) linearize decomposition tree and prompt to sequentially generate answers to subproblems.\nSuccessive prompting [52] develops successive prompting decomposing a complex problem into a first\nsimple problem, with each next subproblem prediction having access to the answers to each previous\nsubproblems.\nMaieutic prompting [103] is inspired by Socratic way of questioning, it generates a tree of logical\nexplanations up to the truth values that max-satisfy these relations to verify its veracity. It surpass\nmany approaches and provides intrinsic interpretations of inference.\n7.6.3 Process- and verification-based optimization. is designed to follow a parallel or iterative process optimiz-\ning final output.\nSelf-Optimization covers self refining processes (e.g. calibrators, filters) [257].\nEnsemble-Optimization encompasses ensembling techniques used to generate more consistent answers\nby majority vote or ensembling decision process. A good example isSelf-consistency [228] which gen-\nerates multiples prompts, verifies and votes; in practice, ensembling is often paired with verifier/PRM-\nbased reranking that scores final answers or intermediate steps before selection [34, 133].\nIterative-Optimization iteratively fine-tunes to improve reasoning process and answers [86, 224].\n8 SOLVING WITH AGENTIC ARCHITECTURES & INFERENCE-TIME STRATEGIES\nAgentic meta-architectures, using agents and tools, and inference-time strategies are different strategies but\nboth are dynamic mechanisms at inference. The former provides a control-plane (plan–act–observe–reflect;\nrouting to tools and memories), while the latter is a compute-allocation knob that decides how much “thinking”\n(branching, verification, retries) to spend on a given step or query. Each can be used on its own: a simple agent\nmay execute a straight-through plan without extra reasoning, and a single model can use CoT/self-consistency\nwithout any agent controller. Used together, controllers can trigger deep thinking only when needed on its\ndifferent LLM processing steps , and reasoning-time methods improve the quality and faithfulness of each agent\nstep, leading to new high potential QA solving capabilities. A good example are different deep research\narchitectures [74, 196, 242, 250] using inference-time reasoning for most complex steps in their answering\ngeneration process.\n8.1 Extended inference-time compute strategies\nComplex QA often benefits from allocating more thinking at inference to refine its understanding, internal\nknowledge extraction, structuring its answering plan, evaluating hypothesis, selecting and refining answer.\nThis inference-time compute and reasoning strategies can be scaled along three practical axes agreed upon\nacross recent reviews:\n(1) Deliberate decoding and multi-sample search : generate multiple chains/trees of thought (CoT/ToT/Graph-\nof-Thoughts), then self-consistency or a verifier selects among candidates; cost grows with sam-\nples/branches [228, 255].\n(2) Process supervision: learn or score intermediate steps withprocess reward models (PRMs) or step-level\npreferences; improves stability on math/science and reduces unfaithful reasoning (e.g., verifiers and\nentailment-tree supervision) [39, 133].\n(3) Adaptive thinking allocation : trigger “deep thinking” (extra tokens, tool calls, solver code, retrieval\npasses) only when uncertainty/critique signals are high (e.g., a verifier rejects, or attribution is missing),\nvia cascades/adaptive decoding [53, 198].\nReasoning-time scaling complements (not replaces) training-time improvements (SFT, RLHF/RLAIF, DPO) and\nhybridization (retrievers/tools). In deployment, it is common to route easy queries through “fast paths” and\nreserve intensive loops for hard cases to balance cost, latency and quality.\n8.2 Agentic meta-architectures (agents, tools)\nIn this context, agents are LLM based architectures having each a role and could plan, act, or\ncoordinate other agents, while tools are callable modules or services that perform specific operations\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 37\ndelegated by the agent . Tools include retrieval over text/graphs, web/file access, code/SQL execution, parsers,\nsolvers, and verifiers (subsubsection 6.4.2–subsubsection 6.4.5, subsubsection 6.4.7, subsubsection 6.4.6).\nMulti-agent designs distribute roles (e.g., planner, researcher, writer, reviewer) to enable division of labor ,\narbitration, and self -critique [222, 271]; hierarchical designs let a manager decompose work and spawn\nsub-agents [250].\nAcross systems we observe a stable template that the controller instantiates:(i) plan & route tasks (subsubsec-\ntion 6.3.1, subsubsection 6.2.3); (ii) invoke tools with structured calls (subsubsection 6.4.2–subsubsection 6.4.5,\nsubsubsection 6.4.6); (iii) verify & revise with judges/PRMs and process-level supervision (subsubsection 6.4.7,\nsubsubsection 6.3.4); (iv) use structured communication and typed artifacts between roles (schemas rather\nthan free chat); (v) persist memory/artifacts for reuse (subsubsection 6.4.1); (vi) learn from dialog/feedback for\nalignment (subsubsection 6.5.2, subsubsection 6.5.1); (vii) allocate reasoning-time adaptively (self-consistency,\ncascades; subsection 8.1). These patterns let readers map concrete systems to common building blocks without\ncommitting to a single implementation.\nRole- and structure-driven variants. Beyond generic “roles, ” multi-agent systems might explicitly encode\nagreed rich structures like Standard Operating Procedures (SOPs) so that each role must produce structured\nintermediate artifacts (e.g., PRD →interface/design →code →tests) and communicate over a shared mes-\nsage bus (publish/subscribe) with executable feedback (run tests, debug/repair). A representative design is\nMetaGPT [82], which encodes SOPs into prompts, constrains outputs to schemas (PRD, design docs, API specs),\nprovides a shared message pool, and executes code to iteratively fix errors—reducing idle chatter and cascading\nhallucinations and reporting strong coherence/executability on software-engineering benchmarks. We treat\nSOP-style orchestration as a specialization of the control-plane above that tightens planning, verification, and\npersistence via schema-constrained artifacts, and is particularly useful when tasks naturally decompose into\nwell-defined deliverables (e.g., long-form writing, software).\nHybridization (section 6) gives the data-plane building blocks (retrievers, executors, verifiers, memories). A\nmeta-architecture supplies the control-plane: (a) plan & route tasks (subsubsection 6.3.1, subsubsection 6.2.3);\n(b) invoke tools (subsubsection 6.4.2–subsubsection 6.4.5, subsubsection 6.4.6); (c) verify & revise (sub-\nsubsection 6.4.7, subsubsection 6.3.4); (d) persist artifacts/memory (subsubsection 6.4.1); and (e) learn\nfrom dialog/feedback (subsubsection 6.5.2, subsubsection 6.5.1). This control-plane/data-plane view does\nnot define specific systems; rather, our surveyed systems are instances that instantiate different points in this\ndesign space. It underlies recent long-form scientific QA/writing agents [98, 146, 196, 202, 242, 248, 250] and\necosystem-level platforms that organize evaluation and capitalization [195, 269] which we review in table 10.\nRows 1–6 are controller-level meta-architectures that plan–act–observe–reflect over tools and retrieval\nto produce long -form answers (articles, reports) [ 74, 98, 146, 196, 202, 250]. Rows 7–8 are one meta level\nhigher: they provide an ecosystem that evaluates, refines, and capitalizes outputs from many agents, driving\nfield-level complex QA via iterative review (aiXiv) and a shared memory of results (AgentRxiv) [195, 269]. As\nemphasized by our long-form paragraph (§4), multi-agent collaboration and hierarchical planning improve\ncoherence and factuality over single-pass prompting [222, 250, 271].\nOptimization and feedback; the answering meta-architecture could be optimized online (at execution) or\noffline (between execution). Deep Researcher [74], when answering to complex question requiring long answers\nand planning, progressively select among multiple answers and refine its plan, content, and RAG resources\nat inference time, improving progressively its long answer. For this, it requires tocalibrate its LLM-as-judge\nfeedback against human preferences. It shows Pareto gains in cost–quality trade-offs via denoising-with-\nretrieval and self-evolution. Those meta-architectures can also be automaticallyoptimized offline with different\nfeedback (pairwise preferences, rubric scoring, LLM-as-judge) via programmatic optimizers (e.g. DSPy) on\ncompiled prompts, retrieval, and routes , or even the code of agents and architecture using generative optimizers\n(e.g. Trace).\nResearch-grade long-form QA seem to evolve to a robust stack of multi-agents with dynamic plan, retrieval,\nevaluation, alignment and improvement loop controller (rows 1–6), and some new research head toward an\nhigher level ecosystem layer (rows 7–8) that wouldevaluate and capitalize complex question answers; allowing\nto orchestrate research question exploration and knowledge progress capitalization [195, 269].\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n38 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nTable 10. Representative agentic systems as meta -architectures (rows 1–6) and ecosystem-level platforms for\nvaluation & capitalization (rows 7–8).\nScope System Control-plane →Data-plane mapping (our patterns)\nLong-form\ngeneration\n(planner\n+ workers)\nSTORM [202] Multi-LLM perspective-guided research and outline; retrieve (sub-\nsubsection 6.4.2) →sectioned rafting; consolidation with citations;\nartifacts cached (subsubsection 6.4.1).\nCo-STORM [98] STORM + human-in-the-loop moderator guiding breadth/depth (sub-\nsubsection 6.5.1, subsubsection 6.5.2); same retrieve–draft–verify\ntoolchain (subsubsection 6.4.2, subsubsection 6.4.7).\nWriteHere\n(hierarchical\nplanner) [242]\nRecursive plan/execute across retrieval–reasoning–composition;\ntask-graph scheduling (subsubsection 6.3.1); section -level arbitra-\ntion; memory of intents/edits (subsubsection 6.4.1).\nAgent Labora-\ntory [196]\nStage-wise controller for literature →experiments→paper; code\ntools and execution subsubsection 6.4.4, subsubsection 6.4.5); re-\nviewer/rubric loops (subsubsection 6.4.7); human oversight (subsub-\nsection 6.5.1).\nAI\nScientist [146,\n248]\nEnd-to-end propose–run–write–review; parallel tree -search exper-\niments, VLM-aided figure checks; generate–critique–revise loops\n(subsubsection 6.4.7, subsubsection 6.3.4).\nDeep\nResearcher\n(TTD-DR) [74]\nDraft-as-diffusion controller: alternating denoising with retrieval\nand component-wise elf -evolution under LLM-as-judge calibration;\ncloses cost–quality Pareto gaps vs. public DR agents by coupling\nlan–search–revise loops with attribution-aware synthesis (subsub-\nsection 6.4.2, subsubsection 6.4.7, subsubsection 6.3.4).\nEcosystem level\nevaluation &\ncapitalization\naiXiv [269] Closed-loop platform for proposals/papers: RAG -grounded\nmulti-agent reviewers (subsubsection 6.4.2, subsubsection 6.4.7),\npairwise assessment, prompt-injection defenses; APIs/MCP orches-\ntrate heterogeneous agents (subsubsection 6.4.4).\nAgentRxiv [195] Shared preprint server for agent labs to upload/retrieve results; en-\nables capitalization of easoning methods and measurable cross-lab im-\nprovement; persistent memory of field progress (subsubsection 6.4.1),\nwith valuation signals feeding back into planners.\n9 DISCUSSION: LIMITATIONS AND RESEARCH TOPICS FOR SOLVING MORE COMPLEX QA\nAND PROBLEMS\nIn the architectural patterns section, we listed the most frequent topics identified as challenge or limits of\nLLMs. After reviewing the collected literature and identifying different solutions in this study, some limits\nseem tougher research limits to enable more complex QA and problems solving:\n•Complex question decomposition, robust and explainable.\n•The hallucination problem which limits a clear expectation of credibility/truthfulness in \"Alignment to\nhuman expectation & values in answer\" (mitigations in practice couple retrieval-grounding with attri-\nbution metrics and verifier loops; metrics like FActScore/RAGAS are now commonly used; researchers\nwarn when using LLM-as-judge without calibration [273]).\n•The scalability problem, which today includes not only training scale but also theallocation of compute at\ninference time (reasoning-time) and the ability to mix fast paths and deep thinking under budget [49, 198].\n•Data availability & quality which limits \"domain adaptation & task specialization\" and \"bias\" .\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 39\n•Data multi-sensitivity in LLM is now an active area with defense-in-depth deployments (retrieval\ngating, guard agents, offsite/federated adaptation, privacy-preserving inference, unlearning); see sub-\nsection 9.4 [24, 42, 160, 240].\nFrom 2022 to 2025 practice has converged from answering with a single LLM inference toagentic, retrieval-grounded,\nverifier-guided pipelines that allocate extrareasoning-time on hard queries [34, 132, 228]. Controllers plan–act–observe–reflect\nover tools (RAG, code, search), while verifiers/PRMs and self-consistency select robust chains; for long-form\nresearch/writing agents, this pattern materially improves coherence and factuality [74, 98, 202, 242].\n9.1 Hallucination & credibility\nEarly debates about Galactica [ 217] and ChatGPT [ 191] shade the light of limits and credibility of such\nlanguage models concerning hallucination. It generates plausible-looking statements that are irrelevant or\nfactually incorrect. It predicts without giving clues about which part of a false claim goes wrong, even\nsources given are not trustworthy. It even has difficulty to learn correct associations between entities from\nfactual text corpus (e.g. Wikipedia). Explainability of an answer with a supported citation is a pointer but\ndoes not mean it is true. By late 2025, mitigations that combine retrieval grounding with verifier/critic loops ,\nand that report attribution/groundedness metrics alongside accuracy are widely adopted in complex QA;\nwhen using LLM-as-judge to scale evaluation, calibration against human preferences is required to reduce\nbias [34, 158, 164, 228, 273]. We have identified different topics of research to address the challenge of\nhallucinations such as:\n•Robust training & prompting (self-consistency, context optimization, prompt tuning, denoising...).\n•Hallucination detection [275].\n•Providing references, traceability, faithful explanation logic [22] or the emerging field of entailment tree\nexplanation [142, 189] or automated fact-checking [71]; in retrieval-grounded settings, practitioners\nalso track FActScore/RAGAS-style attribution and groundedness.\n•Identifying faithfulness performance per tasks/domain [132] and biases, to better ensemble experts [32].\n•Contrastive learning to reduce hallucination [208].\n•Reinforcement learning seems one of the strongest area of research to reduce hallucination in QA.\n9.2 Compute, scaling... Costs\nMore than 8 million TPUv4 hours was the time taken to train PalM 540B parameters model [216]. For a far\nsmaller model \"T5 11B paremeters\" and its variants, the cost of the project is estimated $10 millions [ 203].\nThose models continue to scale. Time of compute for training is therefore reserved to few organization.\nOperational costs for usage (inference) are less impressive [132] but limits the use cases considering inference\nlatency and minimal required hardware. We can apply standard model size reductions like quantization,\ndistillation, pruning, early stopping at training... Different research avenues try to reduce this computing and\ncosts required and inverse this scaling law :\n•Frozen PLM techniques : we presented in previous sections prompt tuning and parameter-efficient\ntuning (PEFT) in our first survey in 2022; these continue to be the most practical path to specialize\nmodels under tight budgets (e.g., QLoRA, LoftQ) [44, 129].\n•Retrieval augmented LLM : keeping a maximum of information out of model while making it easy to\naccess and update without any re-train has an important potential but is often less efficient and may\nrequire equal computation when comparing \"total additional answer generation time\" vs \"training\",\nnew techniques try to close the gap [43].\n•Scaling in-context learning : in-context learning highly improves tasks efficiency without re-training\nbut is limited by maximum length input constraints due to quadratic complexity in computation and\nmemory; different techniques allow efficient scaling [31, 75, 155, 181].\n•Serving/runtime efficiency: employing FlashAttention-2/3 (bandwidth-efficient attention), paged at-\ntention/KV caching (compact context reuse), speculative decoding (parallel token prediction), alongside\nsparse Mixture-of-Experts routing, reduces per-answer cost without compromising model quality.\n•Mixture of denoisers : in \"Transcending Scaling Laws with 0.1% Extra Compute\" [216], same model is\ntrain at half the budget to reach the same performance using mixture-of-denoisers (\" U-PaLM achieves\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n40 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nthe same performance as the final PaLM 540B model at around half its computational budget - saving\napprox. 4.4 million TPUv4 hours\"). UL2, proposes an unification of LM learning paradigms [215].\n•Improve pruning techniques : Frantar and Alistarh [61] reduce LLM size by more than 50% with only\none-shot, without any retraining, and nearly no loss in accuracy.\n•Mixture of experts, parameters sharing and routing techniques : \"Switch Transformers: Scaling\nto Trillion Parameter Models with Simple and Efficient Sparsity\" [58] demonstrated in 2022 the usage\nof internal routing to expert layers in large language models to limit compute to part of the whole\nmodel to allow to scale model without augmenting compute. By the end of 2025, it is used in many\narchitecture and research still improve it.\n•Knowledge distillation improvement [10, 223, 263] and dynamic composition of model [243] to\ncompose optimal and smaller models; RL-free preference optimization (e.g., DPO/ORPO/KTO) is\nwidely used in post-training to align to preferences at lower complexity [54, 79, 186].\n•Adaptive computation and training : all samples are equally computed in standard training. Easy\nsamples could be less worked out than hard ones; adaptive computing enables sample -dependent\ncomputation (e.g., CALM) [198]. Chinchilla [78] demonstrated that we can highly reduce inference\nbudget while improving accuracy by using the same training budget with much more data on a much\nsmaller LLM.\n•Reasoning-time compute and verifiers : move some “scaling” from training into inference via cas-\ncades and confidence triggers [49, 198], combine multi-sample chains with self-consistency [228], score\nintermediate steps withprocess-level verifiers/PRMs [34], and—when training resources are available—use\nreinforcement fine-tuning (e.g., GRPO) to explicitly incentivize reasoning [70].\n•Dedicated hardware: language models are typically accelerated by running on GPU but an area of\nresearch investigate dedicated hardware (e.g. FPGA, ASICS, ) for saving energy and costs [81].\n9.3 Data availability & quality\nThe skills and training datasets section, as well as human feedback, highlighted the need for specialized\ndata and of high quality, to acquire skills and domain knowledge as well as to calibrate to requester intents.\nLarge language models requires huge volume of data to develop each skills and domains targeted. Wealthy\norganization can spend millions to clean or produce data but even them are limited.\n•Frugal and rich data with AI supervision : we saw in previous sections techniques like dynamic\nleast-to-most requiring far less data for training while improving accuracy and skills, or active learning\nidentifying best examples for improvement[47]. Those data could be more and more automatically\ngenerated like in Auto-CoT [272], CAI [5].\n•Simulation, distillation and code interpreter (program execution): simulation [93, 141], existing\nmodels [223], and code interpreters [73] can provide infinite examples in different domains of logic and\nknowledge. We also uncovered that code interpreter allow to learn logic transferable to many domains.\n•More signals: better leverage all available symbolic data which have clear signals, already structured\n(Linked Open Data) or re-structured [260].\n•Open dataset: QA and IR field has continuously progressed through the availability of existing and\nnew open datasets [94] for target skills and knowledge.\n•Teacher-generated “textbooks/datasets” for reasoning : high-quality synthetic curricula distilled\nfrom stronger teachers continue to be effective for math/coding/reasoning specialists [69].\n9.4 Data multi-sensitivity usage & protection\nSensitive data in CQA systems spans personal data, trade secrets, regulated information, and safety-critical\ncontent; a single item can carry multiple sensitivities . Literature outlines the need for a defense-in-depth\napproach (multiple layers of protection) which we organize here across the data plane, control plane, and\nlearning stack. Leakage risks arise at training, indexing/retrieval, and generation/tool-use time, through mem-\nbership/attribute inference and inversion attacks, prompt-injection jailbreaks targeting RAG or agents, and\npoisoning of retrieval indexes [42, 160].\nAccordingly, state-of-the-art deployments use defense-in-depth with controls aligned to a threat-model. The\nchecklist below summarize patterns recommended across privacy and security evaluations and deployment\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 41\nguidance for LLM systems [24, 42, 160, 240]: (1) keep sensitive dataout of weights and behind a policy-enforced\nretriever; (2) sanitize/privatize representations and gate retrieval; (3) surround RAG/agents with guardrails\nand verifiers; (4) adapt with offsite/federated training or cryptographic inference where needed; (5) support\nunlearning and auditable provenance.\nData-plane controls (store/index/evidence).\n•Access control at retrieval time. Keep sensitive corpora outside the model and enforce row/field/passage-level\npolicies in the retriever (ABAC/LBAC) so only authorized content is retrieved into the context window;\nthis avoids baking access decisions into model weights and supports per-user purpose limitation.\n•Segmentation by sensitivity / tenancy. Where policies and risk profiles diverge, maintain separate\nindices or fine-tuned endpoints per sensitivity tier or tenant, with a front-end router enforcing who\nmay query which segment. This complements retrieval-time access control rather than replacing it.\n•Privacy-preserving embeddings and redaction. Prevent embedding-inversion and unintended mem-\norization by (i) sanitizing/PII-redacting at index time and (ii) using privacy-preserving representation\nlearning (e.g., TextHide, TextFusion) that makes recovering plain text from embeddings substantially\nharder [88, 110, 278].\nControl-plane and runtime guards (retrieval →generation). Controls that mediate how retrieved context\ninfluences tools and generation:\n•Guardrails against prompt -injection and policy violations. Enforce multi-stage filters and guard\nagents around search/tool use, and apply retrieval gating /verifier-in-the-loop so untrusted context\ncannot steer tool calls or exfiltrate secrets [42, 239].\n•Self-/corrective-RAG. Couple generation with critique and retrieval-backed verification to reduce\nungrounded disclosure; keep attributions for auditability (see §4).\nLearning & compute with minimal exposure. Mechanisms to adapt models and run inference while minimiz-\ning raw-data exposure:\n•Offsite/partitioned fine-tuning. Adapt models to private data without exposing full weights or raw\ndata (e.g., Offsite-Tuning) [240].\n•Federated learning. Keep data at source and aggregate updates centrally to avoid raw-data sharing\nfor domain adaptation [23].\n•Cryptography & secure computation. Where latency budgets allow, use privacy-preserving trans-\nformer inference/training via homomorphic encryption/secure protocols (e.g., THE-X implements\nencrypted Softmax/GELU/LayerNorm paths) [ 24]; include functional encryption for role-restricted\nqueries when applicable [246].\n•Right-to-be-forgotten. Support machine unlearning to remove specific data influence from adapted\nmodels or indices [14].\nPropositions to use blockchain. Used as an audit or coordination layer rather than a data store, a permissioned\nblockchain can addverifiable provenance and usage accounting without placing raw data on-chain: (i) immutable\nlogs of retrieval/training/inference events, (ii) access-policy enforcement and incentives via smart contracts,\nand (iii) privacy-preserving attestations (e.g., zk-proof-backed claims) that a request complied with policy [66].\nThese patterns complement (not replace) store-level access control and cryptographic/privacy mechanisms\nabove.\nGovernance and safety context . Regulatory requirements (e.g., logging, risk management) are tightening;\nin parallel, organizations increasingly combine policy filters/guard agents with private adaptation methods\n(offsite/federated) for multi-tenant RAG/agent systems [239, 240].\n9.5 Decomposition of very complex QA and explainability\nThe need for decomposition in complex questions is central in their solving process because it breaks down a\nnon solvable complex question (problem) down to solvable questions. As we have seen in chain-of-thought and\ndynamic least-to-most, those traceable steps improve solving capacities of a given model but also makes the\nanswer auditable, truthful and explainable in case of errors. However nearly all examples of papers reviewed\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n42 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nrelated to decomposition are factoid questions. Dua et al. [52] used \"Who kicked the longest field goal in the first\nhalf? \" as the main example of complex question, it is a factoid question. What if we ask \"What are the most\nadapted LM hybrid architectures to answer complex questions ? \". That would be a non-factoid question, requiring\nmultiple sources, reasoning... ChatGPT could provide an answer but reasoning would be nor explainable nor\na robust decomposition process with an acceptable and auditable scientific methodology. We could learn\nthis decomposition behaviour by cloning a human process [ 251] or learn to discover it through a human\ncollaboration. Iterated decomposition [188], a human-in-the-loop workflow for process supervision using\na language model allows to address new type of problems and enables users to inspect and intervene in\nthe LM’s reasoning process. In 2025, long-form agentic systems (e.g., STORM/Co-STORM, WriteHere, Deep\nResearcher) improved decomposition and revision on open-ended research/writing tasks by coupling retrieval,\nplanning, and verifier-guided revision [74, 98, 202, 242]. Open challenges remain: robust planning under\nambiguity, reproducible evaluation beyond task-specific win-rates (tie-back to HELM’s multi-metric view),\nand controlling inference-time cost without hurting factuality [34, 132, 228]. These approaches still require\nsubstantial human expert feedback, domain-specific data and calculations; this is an important limitation to\ngeneralize the resolution of complex non-factoid questions across domains and methodologies.\n10 CONCLUSION\nIn this paper, we present a comprehensive survey of language model hybrid architectures for answering\ncomplex questions. We review the various skills required and typical approach, datasets and metrics that\nare used, the current limits of large language models for complex QA, the potential of hybrid and meta\narchitectures, better training, inferring, and prompting strategies for this goal. We also identify the main\nchallenges and research avenues for solving more complex questions including knowledge capitalization.\nWe identify the need to address multi-sensitivity data defense-in-depth in LLM architectures. Finally, we\noutline research topics and highlight the potential of exploration in this field. Compared to the initial review\ndated 2022, the default architecture for robust complex QA is now agentic (with tools), retrieval -grounded,\nverifiable and aligned : plan–act–observe–reflect controllers that orchestrate tools and retrieval (including\ngraph-structured search), allocate extra reasoning-time when needed (with process supervision), and report\nevidence/attribution alongside answers. This paper aimed to provide a comprehensive and useful resource for\nreaders interested in the development of complex non-factoid question answering.\nREFERENCES\n[1] Paul Albert, Frederic Z. Zhang, Hemanth Saratchandran, Cristian Rodriguez-Opazo, Anton van den Hengel, and\nEhsan Abbasnejad. 2024. RandLoRA: Full Rank Parameter-Efficient Fine-Tuning of Large Models. In The Thirteenth\nInternational Conference on Learning Representations .\n[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural Module Networks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition . 39–48.\n[3] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve,\nGenerate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations .\n[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,\nDeep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk,\nNelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt,\nNeel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and\nJared Kaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.\nhttps://doi.org/10.48550/arXiv.2204.05862 arXiv:2204.05862 [cs]\n[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez,\nDawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi\nMercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk,\nStanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R.\nBowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared\nKaplan. 2022. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073 [cs]\n[6] Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and Douwe Kiela. 2021. Improving Question\nAnswering Model Robustness with Synthetic Adversarial Data Generation. In Proceedings of the 2021 Conference on\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 43\nTask (BBEH) Random Llama Gemma2 Gemini 2.0 Gemini 2.0 GPT-4o Distill DeepSeek o3-mini\n- skill(s) 3.1 8B IT 27B IT Flash-Lite Flash R1 Qwen 32B R1 (high)\nScore (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%) Score (%)\nBoardgameQA 33.3 31.5 39.5 29.5 42.5 41.0 36.0 75.5 53.0\n- multi-step + in-context-learning\nBoolean Expressions 20.0 18.0 25.0 24.0 27.0 22.5 17.5 55.5 67.0\n- multi-step\nCausal Understanding 38.0 37.0 45.5 52.5 52.0 54.0 54.5 54.5 54.0\n- causal, logical, counterfactual\nDisambiguationQA 21.0 36.7 45.0 50.0 48.3 51.7 52.5 50.0 58.3\n- commonsense, linguistics\nDyck Languages 1.4 4.5 2.0 6.5 14.0 8.0 18.0 56.0 55.0\n- errors in reasoning\nGeometric Shapes 6.2 25.5 31.0 30.0 35.0 22.5 4.5 1.5 52.5\n- dealing-with-distractors\nHyperbaton 0.0 2.0 4.0 6.5 4.5 7.5 3.0 6.0 32.0\n- inductive reasoning + against-strong-prior\nMovie recommendation 10.0 30.0 40.0 51.5 59.5 61.0 46.0 59.5 84.0\n- knowledge association\nSARC Triples 12.5 16.5 21.0 27.0 37.5 38.5 22.0 28.5 24.0\n- compositional reasoning\nWord Sorting 4.3 2.5 3.5 12.5 26.0 22.0 36.0 68.0 77.5\n- against-strong-prior + finding-errors\nWeb of Lies 5.5 5.5 6.5 14.0 18.5 14.5 13.0 29.5 43.0\n- multi-step\nSportQA 0.0 1.5 10.0 18.5 23.0 25.0 19.5 29.0 26.5\n- knowledge + compositional reasoning\nShuffled objects 14.3 9.5 12.0 15.0 9.0 14.0 2.0 6.0 49.5\n- long-context + long-range-dep + needle-in-haystack\nBuggy tables 0.0 0.0 0.5 1.5 3.5 0.5 0.5 4.5 59.5\n- in-context-learning + needle-in-haystack\nTemporal sequences 0.0 9.5 1.5 1.0 0.5 0.0 0.5 0.0 68.5\n- temporal reasoning\nTable 11. Big bench extra hard tasks for LLM compared across recent SOTA models.\nEmpirical Methods in Natural Language Processing . 8830–8848. https://doi.org/10.18653/v1/2021.emnlp-main.696\narXiv:2104.08678 [cs]\n[7] Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. 2022. Prompting Is Programming: Query Language for Large\nLanguage Models. https://doi.org/10.48550/arXiv.2212.06094 arXiv:2212.06094 [cs]\n[8] BIG et al. 2022. Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models.\nhttps://doi.org/10.48550/arXiv.2206.04615 arXiv:2206.04615 [cs, stat]\n[9] BigScience et al. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. https://doi.org/10.\n48550/arXiv.2211.05100 arXiv:2211.05100 [cs]\n[10] Cody Blakeney, Jessica Zosa Forde, Jonathan Frankle, Ziliang Zong, and Matthew L. Leavitt. 2022. Reduce, Reuse, Recy-\ncle: Improving Training Efficiency with Distillation. https://doi.org/10.48550/arXiv.2211.00683 arXiv:2211.00683 [cs]\n[11] Yasaman Boreshban, Seyed Morteza Mirbostani, Gholamreza Ghassem-Sani, Seyed Abolghasem Mirroshandel, and\nShahin Amiriparian. 2023. Improving Question Answering Performance Using Knowledge Distillation and Active\nLearning. Engineering Applications of Artificial Intelligence (Aug. 2023), 106137.\n[12] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den\nDriessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,\nRoman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela\nPaganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. arXiv:2112.04426 [cs] (Feb. 2022), –.\narXiv:2112.04426 [cs]\n[13] Ali Borji. 2023. A Categorical Archive of ChatGPT Failures. https://doi.org/10.48550/arXiv.2302.03494 arXiv:2302.03494\n[14] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang,\nDavid Lie, and Nicolas Papernot. 2020. Machine Unlearning. arXiv:1912.03817 [cs] (Dec. 2020). arXiv:1912.03817 [cs]\n[15] Marco Braga. 2024. Personalized Large Language Models through Parameter Efficient Fine-Tuning Techniques. In\nProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR ’24) . 3076. https://doi.org/10.1145/3626772.3657657\n[16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In Advances in Neural Information\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n44 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nProcessing Systems , Vol. 33. 1877–1901.\n[17] Lukas Budach, Moritz Feuerpfeil, Nina Ihde, Andrea Nathansen, Nele Noack, Hendrik Patzlaff, Felix Naumann, and\nHazar Harmouch. 2022. The Effects of Data Quality on Machine Learning Performance. https://doi.org/10.48550/\narXiv.2207.14529 arXiv:2207.14529 [cs]\n[18] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. 2022. Recurrent Memory Transformer. arXiv:2207.06881 [cs]\n[19] Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir\nGloberson. 2024. TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools. Advances in\nNeural Information Processing Systems (Dec. 2024), 34775–34799.\n[20] Yingshan Chang and Yonatan Bisk. 2022. WebQA: A Multimodal Multihop NeurIPS Challenge. In Proceedings of the\nNeurIPS 2021 Competitions and Demonstrations Track . 232–245.\n[21] Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Evaluating Question Answering Evaluation.\nIn Proceedings of the 2nd Workshop on Machine Reading for Question Answering . 119–124. https://doi.org/10.18653/v1/\nD19-5817\n[22] Jiangjie Chen, Qiaoben Bao, Changzhi Sun, Xinbo Zhang, Jiaze Chen, Hao Zhou, Yanghua Xiao, and Lei Li. 2022.\nLOREN: Logic-Regularized Reasoning for Interpretable Fact Verification. Proceedings of the AAAI Conference on\nArtificial Intelligence 36, 10 (June 2022), 10482–10491. arXiv:2012.13577 [cs]\n[23] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2021. FedMatch: Federated Learning Over\nHeterogeneous Question Answering Data. In Proceedings of the 30th ACM International Conference on Information &\nKnowledge Management . 181–190. arXiv:2108.05069 [cs]\n[24] Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu\nWei. 2022. THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption. https://doi.org/10.\n48550/arXiv.2206.00216 arXiv:2206.00216 [cs]\n[25] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of Thoughts Prompting: Disen-\ntangling Computation from Reasoning for Numerical Reasoning Tasks. https://doi.org/10.48550/arXiv.2211.12588\narXiv:2211.12588 [cs]\n[26] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. LongLoRA: Effi-\ncient Fine-tuning of Long-Context Large Language Models. In The Twelfth International Conference on Learning\nRepresentations.\n[27] Zhi Chen, Yuncong Liu, Lu Chen, Su Zhu, Mengyue Wu, and Kai Yu. 2022. OPAL: Ontology-Aware Pretrained Language\nModel for End-to-End Task-Oriented Dialogue. https://doi.org/10.48550/arXiv.2209.04595 arXiv:2209.04595 [cs]\n[28] Ching-An Cheng, Allen Nie, and Adith Swaminathan. 2024. Trace Is the Next AutoDiff: Generative Optimization\nwith Rich Feedback, Execution Traces, and LLMs. arXiv:2406.16218\n[29] Zewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan\nHuang, and Ming Zhou. 2021. InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model\nPre-Training. arXiv:2007.07834 [cs]\n[30] Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, and Michael C. Yip. 2022. Knowledge-Grounded Reinforcement\nLearning. https://doi.org/10.48550/arXiv.2210.03729 arXiv:2210.03729 [cs]\n[31] Hyunsoo Cho, Hyuhng Joon Kim, Junyeob Kim, Sang-Woo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk Kim.\n2022. Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners. https:\n//doi.org/10.48550/arXiv.2212.10873 arXiv:2212.10873 [cs]\n[32] Prafulla Kumar Choubey, Jesse Vig, Wenhao Liu, and Nazneen Rajani. 2021. MoFE: Mixture of Factual Experts for\nControlling Hallucinations in Abstractive Summarization.\n[33] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun\nChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language\nModels. https://doi.org/10.48550/arXiv.2210.11416 arXiv:2210.11416 [cs]\n[34] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\nTworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve\nMath Word Problems. https://doi.org/10.48550/arXiv.2110.14168 arXiv:2110.14168 [cs]\n[35] Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Zhe Wang, and Doo Soon Kim. 2020. Tu-\ntorialVQA: Question Answering Dataset for Tutorial Videos. InProceedings of the Twelfth Language Resources and\nEvaluation Conference . 5450–5455.\n[36] Antonia Creswell and Murray Shanahan. 2022. Faithful Reasoning Using Large Language Models. https://doi.org/10.\n48550/arXiv.2208.14271 arXiv:2208.14271 [cs]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 45\n[37] Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-Inference: Exploiting Large Language Models\nfor Interpretable Logical Reasoning. arXiv:2205.09712 [cs]\n[38] CARLOS A. CUADRA and ROBERT V. KATTER. 1967. OPENING THE BLACK BOX OF ‘RELEVANCE’.Journal of\nDocumentation 23, 4 (Jan. 1967), 291–303. https://doi.org/10.1108/eb026436\n[39] Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter\nClark. 2021. Explaining Answers with Entailment Trees. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing . 7358–7370. https://doi.org/10.18653/v1/2021.emnlp-main.585\n[40] Oliver Daniels-Koch and Rachel Freedman. 2022. The Expertise Problem: Learning from Specialized Feedback.\nhttps://doi.org/10.48550/arXiv.2211.06519 arXiv:2211.06519 [cs]\n[41] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A Dataset of\nInformation-Seeking Questions and Answers Anchored in Research Papers. https://doi.org/10.48550/arXiv.2105.03011\narXiv:2105.03011 [cs]\n[42] Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern,\nChongyang Shi, Andreas Terzis, and Florian Tramèr. 2025. Defeating Prompt Injections by Design. https://doi.org/\n10.48550/arXiv.2503.18813 arXiv:2503.18813 [cs]\n[43] Michiel deJong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, and William Cohen.\n2023. Pre-Computed Memory or on-the-Fly Encoding? A Hybrid Approach to Retrieval Augmentation Makes the\nMost of Your Compute. https://doi.org/10.48550/arXiv.2301.10448 arXiv:2301.10448 [cs]\n[44] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of\nQuantized LLMs. https://doi.org/10.48550/arXiv.2305.14314 arXiv:2305.14314 [cs]\n[45] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arXiv:1810.04805 [cs]\n[46] Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya\nMahendiran, Simon Mille, Ashish Shrivastava, Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi,\nEduard Hovy, Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla Cabezudo, Samuel Cahyawijaya,\nEmile Chapuis, Wanxiang Che, Mukund Choudhary, Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan,\nMayukh Das, Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di Giovanni,\nTanya Goyal, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada, Antoine Honore,\nIshan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev, Kalpesh Krishna, Ashutosh Kumar, Stefan\nLanger, Seungjae Ryan Lee, Corey James Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko,\nVukosi Marivate, Gerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi, Niklas Muennighoff,\nTimothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova, Priti Oli, Nivranshu Pasricha, Jan Pfister,\nRichard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg,\nNicolas Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos P. H. S., Ananya B. Sai, Robin M. Schmidt, Thomas\nScialom, Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen, Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel,\nDamien Sileo, Jamie Simon, Chandan Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman\nSrivastava, KV Aditya Srivatsa, Tony Sun, Mukund Varma T, A. Tabassum, Fiona Anting Tan, Ryan Teehan, Mo Tiwari,\nMarie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie J. Wang, Fuxuan Wei, Bryan Wilie, Genta Indra\nWinata, Xinyi Wu, Witold Wydmański, Tianbao Xie, Usama Yaseen, Michael A. Yee, Jing Zhang, and Yue Zhang.\n2022. NL-Augmenter: A Framework for Task-Sensitive NL Augmentation. https://doi.org/10.48550/arXiv.2112.02721\narXiv:2112.02721 [cs]\n[47] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active Prompting with Chain-of-Thought for Large\nLanguage Models. https://doi.org/10.48550/arXiv.2302.12246 arXiv:2302.12246 [cs]\n[48] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\nChan, Weize Chen, Jing Yi, Weilin Zhao, Xiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei Chen, Yang Liu, Jie\nTang, Juanzi Li, and Maosong Sun. 2022. Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for\nPre-trained Language Models. https://doi.org/10.48550/arXiv.2203.06904 arXiv:2203.06904 [cs]\n[49] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk\nMichalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and Charles Sutton. 2022. Language Model\nCascades. https://doi.org/10.48550/arXiv.2207.10342 arXiv:2207.10342 [cs]\n[50] Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny\nTran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene\nWu, and Gilbert Strang. 2022. A Neural Network Solves, Explains, and Generates University Math Problems by\nProgram Synthesis and Few-Shot Learning at Human Level. arXiv:2112.15594 [cs]\n[51] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet,\nand Denny Zhou. 2022. Compositional Semantic Parsing with Large Language Models. https://doi.org/10.48550/\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n46 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\narXiv.2209.15003 arXiv:2209.15003 [cs]\n[52] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. 2022. Successive Prompting for Decomposing\nComplex Questions. https://doi.org/10.48550/arXiv.2212.04092 arXiv:2212.04092 [cs]\n[53] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. 2020. Depth-Adaptive Transformer. In International\nConference on Learning Representations (ICLR) .\n[54] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Model Alignment as\nProspect Theoretic Optimization. In Forty-First International Conference on Machine Learning .\n[55] Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo,\nJonathan Halcrow, and Bryan Perozzi. 2024. Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning.\nIn The Thirteenth International Conference on Learning Representations .\n[56] Haytham M. Fayek and Justin Johnson. 2019. Temporal Reasoning via Audio Question Answering. https://doi.org/\n10.48550/arXiv.1911.09655 arXiv:1911.09655 [cs, eess]\n[57] Haytham M. Fayek and Justin Johnson. 2020. Temporal Reasoning via Audio Question Answering. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing (2020), 2283–2294. https://doi.org/10.1109/TASLP.2020.3010650\n[58] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models w/\nSimple & Efficient Sparsity. https://doi.org/10.48550/arXiv.2101.03961 arXiv:2101.03961 [cs]\n[59] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J William\nMurdock, Eric Nyberg, John Prager, Nico Schlaefer, and Christopher Welty. 2010. Building Watson: An Overview of\nthe DeepQA Project. AI Magazine (Sept. 2010), 59–79. https://doi.org/10.1609/aimag.v31i3.2303\n[60] Aleksander Ficek, Jiaqi Zeng, and Oleksii Kuchaiev. 2024. GPT vs RETRO: Exploring the Intersection of Retrieval and\nParameter-Efficient Fine-Tuning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). 19425–19432. https://doi.org/10.18653/v1/\n2024.emnlp-main.1081\n[61] Elias Frantar and Dan Alistarh. 2023. SparseGPT: Massive Language Models Accurately Pruned in One-Shot.\nhttps://doi.org/10.48550/arXiv.2301.00774 arXiv:2301.00774 [cs]\n[62] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez,\nNicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn\nDrain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan\nHume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei,\nTom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red Teaming Language\nModels to Reduce Harms: Methods, Scaling Behaviors, & Lessons Learned. https://doi.org/10.48550/arXiv.2209.07858\narXiv:2209.07858 [cs]\n[63] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\nPAL: Program-aided Language Models. https://doi.org/10.48550/arXiv.2211.10435 arXiv:2211.10435 [cs]\n[64] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2022. SimCSE: Simple Contrastive Learning of Sentence Embeddings.\nhttps://doi.org/10.48550/arXiv.2104.08821 arXiv:2104.08821 [cs]\n[65] Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2019. KnowIT VQA: Answering Knowledge-Based\nQuestions about Videos. https://doi.org/10.48550/arXiv.1910.10706 arXiv:1910.10706 [cs]\n[66] Caleb Geren, Amanda Board, Gaby G. Dagher, Tim Andersen, and Jun Zhuang. 2025. Blockchain for Large Language\nModel Security and Safety: A Holistic Survey. ACM SIGKDD Explorations Newsletter 26, 2 (Jan. 2025), 1–20. https:\n//doi.org/10.1145/3715073.3715075\n[67] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl\nDoersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,\nRémi Munos, and Michal Valko. 2020. Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.\nhttps://doi.org/10.48550/arXiv.2006.07733 arXiv:2006.07733 [cs, stat]\n[68] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-\nEfficient Fine-Tuning via Early Pruning. InFindings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei\nKu, Andre Martins, and Vivek Srikumar (Eds.). 7528–7541. https://doi.org/10.18653/v1/2024.findings-acl.447\n[69] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\nJavaheripi, Piero Conti Kauffmann, Gustavo Henrique de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Behl,\nXin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are\nAll You Need.\n[70] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong\nMa, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin\nLiu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen,\nGuowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 47\nChen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu,\nKaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,\nLiyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun\nWang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang\nChen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu,\nTao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao\nZhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin\nXie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha\nChen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang,\nY. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,\nYiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou,\nYuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,\nY. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z.\nRen, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang\nYan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng\nXu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1 Incentivizes Reasoning in LLMs through Reinforcement\nLearning. Nature 645, 8081 (Sept. 2025), 633–638. https://doi.org/10.1038/s41586-025-09422-z\n[71] Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A Survey on Automated Fact-Checking. https:\n//doi.org/10.48550/arXiv.2108.11896 arXiv:2108.11896 [cs]\n[72] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer. 2021. DEMix Layers: Disen-\ntangling Domains for Modular Language Modeling. https://doi.org/10.48550/arXiv.2108.05036 arXiv:2108.05036 [cs]\n[73] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2022. Language Models Can Teach Themselves to\nProgram Better. arXiv:2207.14502 [cs]\n[74] Rujun Han, Yanfei Chen, Zoey CuiZhu, Lesly Miculicich, Guan Sun, Yuanjun Bi, Weiming Wen, Hui Wan, Chunfeng\nWen, Solène Maître, George Lee, Vishy Tirumalashetty, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk,\nTomas Pfister, and Chen-Yu Lee. 2025. Deep Researcher with Test-Time Diffusion. https://doi.org/10.48550/arXiv.\n2507.16075 arXiv:2507.16075 [cs]\n[75] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured Prompting: Scaling\nIn-Context Learning to 1,000 Examples. https://doi.org/10.48550/arXiv.2212.06713 arXiv:2212.06713 [cs]\n[76] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, and Luo Si. 2021.\nOn the Effectiveness of Adapter-Based Tuning for Pretrained Language Model Adaptation. InThe 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th International Joint Conference on NLP (Vol 1: Long Papers) .\n2208–2222. https://doi.org/10.18653/v1/2021.acl-long.172\n[77] Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, and Yulia Tsvetkov. 2022. On\nthe Blind Spots of Model-Based Evaluation Metrics for Text Generation. arXiv:2212.10020 [cs]\n[78] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and Laurent Sifre. 2022. Training Compute-Optimal Large Language Models. arXiv:2203.15556 [cs] (March\n2022). arXiv:2203.15556 [cs]\n[79] Jiwoo Hong, Noah Lee, and James Thorne. 2024. ORPO: Monolithic Preference Optimization without Reference\nModel. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan,\nMohit Bansal, and Yun-Nung Chen (Eds.). 11170–11189. https://doi.org/10.18653/v1/2024.emnlp-main.626\n[80] Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, and Zhangyang Wang. 2024. DP-OPT:\nMake Large Language Model Your Privacy-Preserving Prompt Engineer. https://doi.org/10.48550/arXiv.2312.03724\narXiv:2312.03724 [cs]\n[81] Seongmin Hong, Seungjae Moon, Junsoo Kim, Sungjae Lee, Minsub Kim, Dongsoo Lee, and Joo-Young Kim. 2022.\nDFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation. In 2022 IEEE Hot\nChips 34 Symposium (HCS) . 1–17. https://doi.org/10.1109/HCS55958.2022.9895626\n[82] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,\nSteven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.\n2023. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. https://doi.org/10.48550/arXiv.\n2308.00352 arXiv:2308.00352 [cs]\n[83] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural Instructions: Tuning Language Models\nwith (Almost) No Human Labor. https://doi.org/10.48550/arXiv.2212.09689 arXiv:2212.09689 [cs]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n48 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n[84] Chengming Hu, Xuan Li, Dan Liu, Xi Chen, Ju Wang, and Xue Liu. 2022. Teacher-Student Architecture for Knowledge\nLearning: A Survey. https://doi.org/10.48550/arXiv.2210.17332 arXiv:2210.17332 [cs]\n[85] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. https://doi.org/10.48550/arXiv.2106.09685\narXiv:2106.09685 [cs]\n[86] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large\nLanguage Models Can Self-Improve. https://doi.org/10.48550/arXiv.2210.11610 arXiv:2210.11610 [cs]\n[87] Qiushi Huang, Tom Ko, Zhan Zhuang, Lilian Tang, and Yu Zhang. 2024. HiRA: Parameter-Efficient Hadamard High-\nRank Adaptation for Large Language Models. In The Thirteenth International Conference on Learning Representations .\n[88] Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. 2020. TextHide: Tackling Data Privacy in\nLanguage Understanding Tasks. arXiv:2010.06053 [cs, stat]\n[89] Ngo Quang Huy, Tu Minh Phuong, and Ngo Xuan Bach. 2022. Autoencoding Language Model Based Ensemble Learning\nfor Commonsense Validation and Explanation. https://doi.org/10.48550/arXiv.2204.03324 arXiv:2204.03324 [cs]\n[90] Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin\nChoi, and Hannaneh Hajishirzi. 2025. Unpacking DPO and PPO: Disentangling Best Practices for Learning from\nPreference Feedback. In Proceedings of the 38th International Conference on Neural Information Processing Systems\n(NIPS ’24, Vol. 37) . 36602–36633.\n[91] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\nWang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli\nCelikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. 2023. OPT-IML: Scaling Language Model Instruction Meta Learning\nthrough Lens of Generalization. arXiv:2212.12017 [cs]\n[92] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot Learning with Retrieval Augmented Language\nModels. arXiv:2208.03299 [cs]\n[93] Ilya Jackson and Maria Jesus Saenz. 2023. From Natural Language to Simulations: Applying GPT-3 Codex to Automate\nSimulation Modeling of Logistics Systems. https://doi.org/10.48550/arXiv.2202.12107 arXiv:2202.12107 [cs]\n[94] Yacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin Danchev, Samson Tan, Alexan-\ndra Sasha Luccioni, Nishant Subramani, Isaac Johnson, Gerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir\nRadev, Aaron Gokaslan, Somaieh Nikpoor, Peter Henderson, Rishi Bommasani, and Margaret Mitchell. 2022. Data\nGovernance in the Age of Large-Scale Data-Driven Language Technology. In 2022 ACM Conference on Fairness,\nAccountability, and Transparency . 2206–2222. https://doi.org/10.1145/3531146.3534637\n[95] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira.\n2023. InPars-v2: Large Language Models as Efficient Dataset Generators for IR. https://doi.org/10.48550/arXiv.2301.\n01820 arXiv:2301.01820 [cs]\n[96] Shaoxiong Ji and Pekka Marttinen. 2023. Patient Outcome and Zero-Shot Diagnosis Prediction with Hypernetwork-\nGuided Multitask Learning. arXiv:2109.03062 [cs]\n[97] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and\nPascale Fung. 2023. Survey of Hallucination in NL Generation. Comput. Surveys 55, 12 (March 2023), 248:1–248:38.\nhttps://doi.org/10.1145/3571730\n[98] Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, and Monica S. Lam. 2024. Into the Unknown Unknowns:\nEngaged Human Learning through Participation in Language Model Agent Conversations. https://doi.org/10.48550/\narXiv.2408.15232 arXiv:2408.15232 [cs]\n[99] Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. 2022. Heterformer: A Transformer Architecture for Node Representation\nLearning on Heterogeneous Text-Rich Networks. https://doi.org/10.48550/arXiv.2205.10282 arXiv:2205.10282 [cs]\n[100] Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, and Peter Szolovits. 2020. Hooks in the Headline: Learning to Generate\nHeadlines with Controlled Styles. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. 5082–5093. https://doi.org/10.18653/v1/2020.acl-main.456\n[101] Abhinav Joshi, Areeb Ahmad, and Ashutosh Modi. 2024. COLD: Causal reasOning in cLosed Daily Activities.Advances\nin Neural Information Processing Systems (Dec. 2024), 5145–5187.\n[102] Josip Jukić and Jan Šnajder. 2022. Smooth Sailing: Improving Active Learning for Pre-trained Language Models with\nRepresentation Smoothness Analysis. https://doi.org/10.48550/arXiv.2212.11680 arXiv:2212.11680 [cs]\n[103] Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022.\nMaieutic Prompting: Logically Consistent Reasoning w/ Recursive Explanations. arXiv:2205.11822 [cs]\n[104] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha. 2021. AMMUS : Survey of Transformer-\nBased Pretrained Models in NLP. arXiv:2108.05542 [cs]\n[105] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta,\nLalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 49\nChiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. 2025. BIG-Bench Extra Hard. https:\n//doi.org/10.48550/arXiv.2502.19187 arXiv:2502.19187 [cs]\n[106] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2023. Continual Learning of Language\nModels. In 11th International Conference on Learning Representations .\n[107] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.\n2023. Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP.\nhttps://doi.org/10.48550/arXiv.2212.14024 arXiv:2212.14024 [cs]\n[108] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful\nHaq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts.\n2023. DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines. In The Twelfth International\nConference on Learning Representations .\n[109] Tushar Khot, Kyle Richardson, Daniel Khashabi, and Ashish Sabharwal. 2021. Learning to Solve Complex Tasks by\nTalking to Agents. arXiv:2110.08542 [cs] (Oct. 2021). arXiv:2110.08542 [cs]\n[110] Donggyu Kim, Garam Lee, and Sungwoo Oh. 2022. Toward Privacy-preserving Text Embedding Similarity with\nHomomorphic Encryption. In Proceedings of the Fourth Workshop on Financial Technology and Natural Language\nProcessing (FinNLP) . 25–36.\n[111] Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017. DeepStory: Video Story QA by Deep\nEmbedded Memory Networks. https://doi.org/10.48550/arXiv.1707.00836 arXiv:1707.00836 [cs]\n[112] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models\nAre Zero-Shot Reasoners. https://doi.org/10.48550/arXiv.2205.11916 arXiv:2205.11916 [cs]\n[113] Soroush Abbasi Koohpayegani, Navaneet K. L, Parsa Nooralinejad, Soheil Kolouri, and Hamed Pirsiavash. 2023.\nNOLA: Compressing LoRA Using Linear Combination of Random Basis. In The Twelfth International Conference on\nLearning Representations .\n[114] Md Kowsher, Tara Esmaeilbeig, Chun-Nam Yu, Chen Chen, Mojtaba Soltanalian, and Niloofar Yousefi. 2025. RoCoFT:\nEfficient Finetuning of Large Language Models with Row-Column Updates. In Proceedings of the 63rd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) , Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar (Eds.). 26659–26678. https://doi.org/10.18653/v1/2025.acl-long.1293\n[115] Yunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Complex Knowledge Base\nQuestion Answering: A Survey. arXiv:2108.06688 [cs]\n[116] Yoonjoo Lee, John Joon Young Chung, Tae Soo Kim, Jean Y Song, and Juho Kim. 2022. Promptiverse: Scalable\nGeneration of Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation. In Proceedings of the\n2022 CHI Conference on Human Factors in Computing Systems (CHI ’22) . 1–18. https://doi.org/10.1145/3491102.3502087\n[117] Youngwon Lee, Seung-won Hwang, Daniel F Campos, Filip Graliński, Zhewei Yao, and Yuxiong He. 2025. Inference\nScaling for Bridging Retrieval and Augmented Generation. InFindings of the Association for Computational Linguistics:\nNAACL 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). 7324–7339. https://doi.org/10.18653/v1/2025.findings-\nnaacl.409\n[118] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg. 2019. TVQA: Localized, Compositional Video Question\nAnswering. https://doi.org/10.48550/arXiv.1809.01696 arXiv:1809.01696 [cs]\n[119] Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. 2022.\nTowards Explainable Evaluation Metrics for Natural Language Generation. https://doi.org/10.48550/arXiv.2203.11131\narXiv:2203.11131 [cs]\n[120] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning.\nhttps://doi.org/10.48550/arXiv.2104.08691 arXiv:2104.08691 [cs]\n[121] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov,\nand Luke Zettlemoyer. 2019. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. https://doi.org/10.48550/arXiv.1910.13461 arXiv:1910.13461 [cs, stat]\n[122] Judith Yue Li, Aren Jansen, Qingqing Huang, Joonseok Lee, Ravi Ganti, and Dima Kuzmin. 2023. MAQA: A Multimodal\nQA Benchmark for Negation. https://doi.org/10.48550/arXiv.2301.03238 arXiv:2301.03238 [cs, eess]\n[123] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettlemoyer.\n2022. Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models. https://doi.org/10.48550/\narXiv.2208.03306 arXiv:2208.03306 [cs]\n[124] Ruosen Li, Zimu Wang, Son Q. Tran, Lei Xia, and Xinya Du. 2024. MEQA: A Benchmark for Multi-hop Event-\ncentric Question Answering with Explanations. Advances in Neural Information Processing Systems (Dec. 2024),\n126835–126862.\n[125] Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, and Igor Mordatch. 2022. Composing Ensembles of\nPre-trained Models via Iterative Consensus. arXiv:2210.11522 [cs]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n50 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n[126] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling,\nFelix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\nLevel Code Generation with AlphaCode. https://doi.org/10.48550/arXiv.2203.07814 arXiv:2203.07814 [cs]\n[127] Yang Li, Shaobo Han, and Shihao Ji. 2024. VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks. In\nThe Thirty-eighth Annual Conference on Neural Information Processing Systems .\n[128] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the Advance of\nMaking Language Models Better Reasoners. https://doi.org/10.48550/arXiv.2206.02336 arXiv:2206.02336 [cs]\n[129] Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao. 2023. LoftQ:\nLoRA-Fine-Tuning-aware Quantization for Large Language Models. In The Twelfth International Conference on\nLearning Representations .\n[130] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. 2023. ReMax: A Simple,\nEffective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. https://doi.org/10.\n48550/arXiv.2310.10505 arXiv:2310.10505 [cs]\n[131] Zehan Li, Nan Yang, Liang Wang, and Furu Wei. 2022. Learning Diverse Document Representations with Deep Query\nInteractions for Dense Retrieval. https://doi.org/10.48550/arXiv.2208.04232 arXiv:2208.04232 [cs]\n[132] Liang et al. 2022. Holistic Evaluation of Language Models. https://doi.org/10.48550/arXiv.2211.09110\narXiv:2211.09110 [cs]\n[133] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,\nIlya Sutskever, and Karl Cobbe. 2023. Let’s Verify Step by Step. arXiv:2305.20050 [cs]\n[134] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods.\nhttps://doi.org/10.48550/arXiv.2109.07958 arXiv:2109.07958 [cs]\n[135] Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. 2022. Clotho-AQA: A\nCrowdsourced Dataset for Audio Question Answering. https://doi.org/10.48550/arXiv.2204.09634 arXiv:2204.09634 [cs,\neess]\n[136] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of Hindsight Aligns Language Models with Feedback.\narXiv:2302.02676 [cs]\n[137] Iou-Jen Liu, Xingdi Yuan, Marc-Alexandre Côté, Pierre-Yves Oudeyer, and Alexander G. Schwing. 2022. Asking for\nKnowledge: Training RL Agents to Query External Knowledge Using Language. https://doi.org/10.48550/arXiv.2205.\n06111 arXiv:2205.06111 [cs]\n[138] Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022.\nRainier: Reinforced Knowledge Introspector for Commonsense Question Answering. https://doi.org/10.48550/arXiv.\n2210.03078 arXiv:2210.03078 [cs]\n[139] Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi.\n2022. Generated Knowledge Prompting for Commonsense Reasoning. arXiv:2110.08387 [cs]\n[140] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. 2024. When MOE Meets\nLLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications. https://doi.org/10.48550/arXiv.2310.18339\narXiv:2310.18339 [cs]\n[141] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M.\nDai. 2022. Mind’s Eye: Grounded Language Model Reasoning through Simulation. https://doi.org/10.48550/arXiv.\n2210.05359 arXiv:2210.05359 [cs]\n[142] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. 2022. RLET: A Reinforcement\nLearning Based Approach for Explainable QA w/ Entailment Trees. https://doi.org/10.48550/arXiv.2210.17095\narXiv:2210.17095 [cs]\n[143] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-Task Deep Neural Networks for Natural\nLanguage Understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\n4487–4496. https://doi.org/10.18653/v1/P19-1441\n[144] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nand Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/\narXiv.1907.11692\n[145] Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, and Yvette Graham. 2024. ALoRA: Allocating Low-Rank Adaptation\nfor Fine-tuning Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) , Kevin Duh, Helena\nGomez, and Steven Bethard (Eds.). 622–641. https://doi.org/10.18653/v1/2024.naacl-long.35\n[146] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards\nFully Automated Open-Ended Scientific Discovery. arXiv:2408.06292 [cs]\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 51\n[147] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. 2022. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.\nhttps://doi.org/10.48550/arXiv.2209.09513 arXiv:2209.09513 [cs]\n[148] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-\nBurch. 2023. Faithful Chain-of-Thought Reasoning. https://doi.org/10.48550/arXiv.2301.13379 arXiv:2301.13379 [cs]\n[149] Yufei Ma, Zihan Liang, Huangyu Dai, Ben Chen, Dehong Gao, Zhuoran Ran, Wang Zihan, Linbo Jin, Wen Jiang,\nGuannan Zhang, Xiaoyan Cai, and Libin Yang. 2024. MoDULA: Mixture of Domain-Specific and Universal LoRA for\nMulti-Task Learning. InProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). 2758–2770. https://doi.org/10.18653/v1/2024.emnlp-main.161\n[150] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language Models of Code Are\nFew-Shot Commonsense Learners. https://doi.org/10.48550/arXiv.2210.07128 arXiv:2210.07128 [cs]\n[151] Diwakar Mahajan, Ananya Poddar, Jennifer J. Liang, Yen-Ting Lin, John M. Prager, Parthasarathy Suryanarayanan,\nPreethi Raghavan, and Ching-Huei Tsou. 2020. Identification of Semantically Similar Sentences in Clinical Notes:\nIterative Intermediate Training Using Multi-Task Learning. JMIR medical informatics 8, 11 (Nov. 2020), e22508.\nhttps://doi.org/10.2196/22508\n[152] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. 2017. A Dataset and Exploration\nof Models for Understanding Video Data through Fill-in-the-Blank Question-Answering. https://doi.org/10.48550/\narXiv.1611.07810 arXiv:1611.07810 [cs]\n[153] Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah Goodman, and Jiajun Wu. 2022. CLEVRER-Humans: Describing\nPhysical and Causal Events the Human Way. In Thirty-Sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track .\n[154] Marianna Martindale, Marine Carpuat, Kevin Duh, and Paul McNamee. 2019. Identifying Fluently Inadequate Output\nin Neural & Statistical Machine Translation. In Proceedings of Machine Translation Summit XVII: Research Track .\n233–243.\n[155] Pedro Henrique Martins, Zita Marinho, and André F. T. Martins. 2022. $\\infty$-Former: Infinite Memory Transformer.\nhttps://doi.org/10.48550/arXiv.2109.00301 arXiv:2109.00301 [cs]\n[156] Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022. A Survey on Multi-hop Question Answering and Generation.\narXiv:2204.09140 [cs]\n[157] Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen\nZhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, and Shenghua Liu. 2025. A Survey of Context Engineering\nfor Large Language Models. https://doi.org/10.48550/arXiv.2507.13334 arXiv:2507.13334 [cs]\n[158] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah\nYoung, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching Language Models to Support\nAnswers with Verified Quotes. https://doi.org/10.48550/arXiv.2203.11147 arXiv:2203.11147 [cs]\n[159] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.\nRethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv:2202.12837 [cs]\n[160] Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, and Reza Shokri. 2022. Quantifying\nPrivacy Risks of Masked Language Models Using Membership Inference Attacks. arXiv:2203.03929 [cs]\n[161] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-Task Generalization via NL\nCrowdsourcing Instructions. arXiv:2104.08773 [cs]\n[162] Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, and Anxiang Zeng. 2025. Mid-\nTraining of Large Language Models: A Survey. https://doi.org/10.48550/arXiv.2510.06826 arXiv:2510.06826 [cs]\n[163] Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han. 2017. MarioQA: Answering Questions by\nWatching Gameplay Videos. https://doi.org/10.48550/arXiv.1612.01669 arXiv:1612.01669 [cs]\n[164] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,\nMatthew Knight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted Question-Answering with\nHuman Feedback. https://doi.org/10.48550/arXiv.2112.09332 arXiv:2112.09332 [cs]\n[165] Donald Norman. 1972. Memory, Knowledge & the Answering of Questions.. In Loyola Symposium on Cognitive\nPsychology.\n[166] OpenAI. 2023. GPT-4 Technical Report.\n[167] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow\nInstructions with Human Feedback. Advances in Neural Information Processing Systems (Dec. 2022), 27730–27744.\n[168] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n52 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow\nInstructions with Human Feedback. arXiv:2203.02155 [cs]\n[169] Artidoro Pagnoni, Alexander R. Fabbri, Wojciech Kryściński, and Chien-Sheng Wu. 2022. Socratic Pretrain-\ning: Question-Driven Pretraining for Controllable Summarization. https://doi.org/10.48550/arXiv.2212.10449\narXiv:2212.10449 [cs]\n[170] Paritosh Parmar, Eric Peh, Ruirui Chen, Ting En Lam, Yuhan Chen, Elston Tan, and Basura Fernando. 2024.\nCausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded\nin Dynamic Visual Scenes. Advances in Neural Information Processing Systems (Dec. 2024), 92769–92802.\n[171] Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng\nGao. 2022. GODEL: Large-Scale Pre-Training for Goal-Directed Dialog. arXiv:2206.11309 [cs]\n[172] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese,\nand Geoffrey Irving. 2022. Red Teaming Language Models with Language Models. https://doi.org/10.48550/arXiv.\n2202.03286 arXiv:2202.03286 [cs]\n[173] Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised Question Decompo-\nsition for Question Answering (EMNLP 2020). https://doi.org/10.48550/arXiv.2002.09758 arXiv:2002.09758 [cs]\n[174] Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for\nMulti-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP) . 7654–7673. https://doi.org/10.18653/v1/2020.emnlp-main.617\n[175] Florian Pfisterer, Janek Thomas, and Bernd Bischl. 2019. Towards Human Centered AutoML. arXiv:1911.02391 [cs]\n[176] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed\nShaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren,\nJason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail\nDoroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark\nLevin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan\nWang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro\nSanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai\nNaiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart\nFinke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben\nMcCarty, Alexis C. Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam,\nJohn B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani,\nEmily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke\nTang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre\nMarion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, Gözdenur\nDemir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R.\nGreen, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang,\nNick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate\nStambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P. Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous,\nStefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M. Bran, Andres Algaba, Kelsey Van den Houte,\nLynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut,\nEvgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Martí\nOller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia\nVilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao,\nPierrot Arsene, Joseph M. Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan,\nSergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav\nPoritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan,\nDan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang,\nRichard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish,\nVeit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger,\nKaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, Václav Rozhoň,\nVincent Ginis, Christian Stump, Niv Cohen, Rafał Poświata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr\nPadlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers,\nTom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez\nAziz, Mark H. Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K. Wang, Kalyan\nRamakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown,\nKelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, J. P. Heimonen, Kaustubh Sridhar, Ido\nAkov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev,\nDemosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 53\nXu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner,\nMaksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Anna Sztyber-Betley, Paolo\nFaraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao\nMa, Jérémy Andréoletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov,\nSeri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh,\nPaul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley,\nKunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin,\nZixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint,\nEve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes, Jeremiah Milbauer, M. Saiful Bari, Zihao Wang, Behzad Ansarinejad,\nYewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew\nWolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O, Shailesh Shah, Xiaoxiang\nZhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello,\nG. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J. Fossum,\nNiklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman,\nWilliam Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones,\nShashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn\nDobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William\nHeld, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X. Yuan, Laila Yacar,\nJohannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu\nChidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani,\nDavid Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez,\nRobert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekström, Angela\nHammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun,\nAlena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric\nHallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto\nPereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld,\nAnna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb\nGurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan\nSteinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang,\nMinghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson,\nAbram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir\nVinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen\nSponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller,\nGuglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence\nHollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel Obikoya, Rai, Filippo Bigi,\nM. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie,\nThomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle\nHäggström, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey,\nRynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt,\nPablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao\nQi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao,\nChristoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan,\nBlake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra\nAdoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia,\nNate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena,\nElizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole,\nDanyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister,\nAlejandro José Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid\nTaheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Ricardo Lorena,\nKrishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo,\nJuehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk\nGloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf\nBrown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long,\nLian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu,\nPawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella\nPinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya\nAgarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n54 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nRemy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah\nHoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub Łucki, Russell Campbell, Asankhaya Sharma,\nKhalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie\nVargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari,\nAbdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca Arnaboldi,\nKunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, Mátyás\nVincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen,\nAleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu,\nSam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy,\nDarling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek,\nImad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P. V, Michael Richmond, Joseph\nMcGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubić, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel\nSchottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S. Ashwin Hebbar, Lorenzo\nVaquero, Xianjun Yang, Jason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David\nAnugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He,\nNitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał Perełkiewicz, Ioannis Pantidis, Tianbo\nQi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen\nLi, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio\nA. W. L. Wong, Maria del Rio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer,\nJenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis,\nQizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian,\nBonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David,\nSoler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De\nMaddalena, Xing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao\nLiang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li,\nAlex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Briański, Alessandro Stolfo, Truong An Nguyen, Mike\nZhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar,\nMy Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan\nTodoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos,\nJonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang\nWei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu,\nXingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng,\nShreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen,\nSara Vera Marjanović, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun\nWang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-\nMiguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany,\nHan Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan, Jun Yuan,\nRishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy\nKang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan\nRannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal,\nDuarte V. Gonçalves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline\nGeirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon Christof, Veerupaksh\nSingla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua\nMak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men,\nJiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi\nHoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher\nToukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder,\nJames Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan\nJain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra,\nCore Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu,\nRui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett,\nChristopher R. Scotese, Phuong M. Cao, Ben Wu, Jacek Karwowski, Davide Scaramuzza, Aakaash Nattanmai, Gordon\nMcKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek,\nKasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu,\nLiam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan\nZhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben\nZhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 55\nYang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir\nNasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex\nZhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer\nYue, Alexandr Wang, and Dan Hendrycks. 2025. Humanity’s Last Exam. https://doi.org/10.48550/arXiv.2501.14249\narXiv:2501.14249 [cs]\n[177] Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Qiang Fu, Yan Gao, Jian-Guang Lou, and Weizhu Chen. 2022.\nReasoning Like Program Executors. arXiv:2201.11473 [cs]\n[178] A. J. Piergiovanni, Wei Li, Weicheng Kuo, Mohammad Saffar, Fred Bertsch, and Anelia Angelova. 2022. Answer-Me:\nMulti-Task Open-Vocabulary Visual Question Answering. arXiv:2205.00949 [cs]\n[179] Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui.\n2021. MAUVE: Measuring the Gap Between Neural Text and Human Text Using Divergence Frontiers. https:\n//doi.org/10.48550/arXiv.2102.01454 arXiv:2102.01454 [cs]\n[180] Soumajit Pramanik, Jesujoba Alabi, Rishiraj Saha Roy, and Gerhard Weikum. 2022. UNIQORN: Unified Ques-\ntion Answering over RDF Knowledge Graphs and Natural Language Text. arXiv:2108.08614 [cs] (April 2022).\narXiv:2108.08614 [cs]\n[181] Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input\nLength Extrapolation. https://doi.org/10.48550/arXiv.2108.12409 arXiv:2108.12409 [cs]\n[182] Dongqi Pu and Vera Demberg. 2024. RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document\nAbstractive Summarization. In Proceedings of the 2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers) . 2200–2220. https://doi.org/10.\n18653/v1/2024.naacl-long.121\n[183] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun\nChen. 2022. Reasoning with Language Model Prompting: A Survey. arXiv:2212.09597 [cs]\n[184] Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei\nHou, Peng Li, Maosong Sun, and Jie Zhou. 2022. Exploring Universal Intrinsic Task Subspace via Prompt Tuning.\narXiv:2110.07867 [cs]\n[185] Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5Score: Discriminative Fine-tuning of Generative\nEvaluation Metrics. arXiv:2212.05726 [cs]\n[186] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct\nPreference Optimization: Your Language Model Is Secretly a Reward Model.Advances in Neural Information Processing\nSystems (Dec. 2023), 53728–53741.\n[187] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\nhttps://doi.org/10.48550/arXiv.1910.10683 arXiv:1910.10683 [cs, stat]\n[188] Justin Reppert, Ben Rachbach, Charlie George, Luke Stebbing, Jungwon Byun, Maggie Appleton, and An-\ndreas Stuhlmüller. 2023. Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes.\narXiv:2301.01751 [cs]\n[189] Danilo Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henry Zhu, Xinchi Chen, Zhiheng Huang, Peng\nXu, Andrew Arnold, and Dan Roth. 2022. Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner.\nhttps://doi.org/10.48550/arXiv.2205.09224 arXiv:2205.09224 [cs]\n[190] Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2022. QA Dataset Explosion: A Taxonomy of NLP Resources for\nQuestion Answering and Reading Comprehension. Comput. Surveys (Sept. 2022), 3560260. arXiv:2107.12708 [cs]\n[191] Jürgen Rudolph, Samson Tan, and Shannon Tan. 2023. ChatGPT: Bullshit Spewer or the End of Traditional Assessments\nin Higher Education? Journal of Applied Learning and Teaching 6, 1 (Jan. 2023). https://doi.org/10.37074/jalt.2023.6.1.9\n[192] Bharat Runwal, Tejaswini Pedapati, and Pin-Yu Chen. 2025. From PEFT to DEFT: Parameter Efficient Finetuning for\nReducing Activation Density in Transformers. Proceedings of the AAAI Conference on Artificial Intelligence 39, 19\n(April 2025), 20218–20227. https://doi.org/10.1609/aaai.v39i19.34227\n[193] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In\nProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR ’24) . 2395–2400. https://doi.org/10.1145/3626772.3657957\n[194] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda,\nand Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. https://doi.org/10.\n48550/arXiv.2302.04761 arXiv:2302.04761 [cs]\n[195] Samuel Schmidgall and Michael Moor. 2025. AgentRxiv: Towards Collaborative Autonomous Research. https:\n//doi.org/10.48550/arXiv.2503.18102 arXiv:2503.18102 [cs]\n[196] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad\nBarsoum. 2025. Agent Laboratory: Using LLM Agents as Research Assistants. https://doi.org/10.48550/arXiv.2501.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n56 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\n04227 arXiv:2501.04227 [cs]\n[197] Julian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, and Akiko Aizawa. 2024. MoreHopQA:\nMore Than Multi-hop Reasoning. https://doi.org/10.48550/arXiv.2406.13397 arXiv:2406.13397 [cs]\n[198] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q. Tran, Yi Tay, and Donald Metzler. 2022.\nConfident Adaptive Language Modeling. https://doi.org/10.48550/arXiv.2207.07061 arXiv:2207.07061 [cs]\n[199] Dale Schuurmans. 2023. Memory Augmented Large Language Models Are Computationally Universal.\narXiv:2301.04589 [cs]\n[200] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick\nGallinari. 2021. QuestEval: Summarization Asks for Fact-based Evaluation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing . 6594–6604. https://doi.org/10.18653/v1/2021.emnlp-main.529\n[201] Nan Shao, Zefan Cai, Hanwei Xu, Chonghua Liao, Yanan Zheng, and Zhilin Yang. 2023. Compositional Task\nRepresentations for Large Language Models. In International Conference on Learning Representations .\n[202] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in\nWriting Wikipedia-like Articles From Scratch with Large Language Models. https://doi.org/10.48550/arXiv.2402.14207\narXiv:2402.14207 [cs]\n[203] Or Sharir, Barak Peleg, and Yoav Shoham. 2020. The Cost of Training NLP Models: A Concise Overview.\narXiv:2004.08900 [cs]\n[204] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving\nAI Tasks with ChatGPT and Its Friends in Hugging Face. In Advances in Neural Information Processing Systems .\n[205] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: An Autonomous Agent with Dynamic Memory\nand Self-Reflection. https://doi.org/10.48550/arXiv.2303.11366 arXiv:2303.11366 [cs]\n[206] Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language\nModels That Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion.arXiv:2203.13224\n[cs] (March 2022). arXiv:2203.13224 [cs]\n[207] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke\nZettlemoyer, Noah A. Smith, and Tao Yu. 2022. Selective Annotation Makes Language Models Better Few-Shot\nLearners. https://doi.org/10.48550/arXiv.2209.01975 arXiv:2209.01975 [cs]\n[208] Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. 2022. Contrastive Learning\nReduces Hallucination in Conversations. https://doi.org/10.48550/arXiv.2212.10400 arXiv:2212.10400 [cs]\n[209] Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022. On the Importance of Building High-quality Training\nDatasets for Neural Code Search. arXiv:2202.06649 [cs]\n[210] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning, Second Edition: An Introduction . MIT Press.\n[211] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Ha-\njishirzi, and Jonathan Berant. 2021. MultiModalQA: Complex Question Answering over Text, Tables and Images.\narXiv:2104.06039 [cs] (April 2021). arXiv:2104.06039 [cs]\n[212] Tamkin. 2022. Task Ambiguity in Humans and Language Models. InThe Eleventh International Conference on Learning\nRepresentations.\n[213] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking Retrieval-Augmented Gener- Ation for Multi-Hop\nQueries. COLM 2024 (2024).\n[214] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016.\nMovieQA: Understanding Stories in Movies through Question-Answering. https://doi.org/10.48550/arXiv.1512.02902\narXiv:1512.02902 [cs]\n[215] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,\nTal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2022. UL2: Unifying Language\nLearning Paradigms. arXiv:2205.05131 [cs]\n[216] Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng,\nJinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, and Mostafa\nDehghani. 2022. Transcending Scaling Laws with 0.1% Extra Compute. https://doi.org/10.48550/arXiv.2210.11399\narXiv:2210.11399 [cs]\n[217] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton,\nViktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science. https://doi.org/10.48550/\narXiv.2211.09085 arXiv:2211.09085 [cs, stat]\n[218] Sebastian Thrun and Lorien Pratt. 1998. Learning to Learn: Introduction and Overview. InLearning to Learn , Sebastian\nThrun and Lorien Pratt (Eds.). Springer US, Boston, MA, 3–17. https://doi.org/10.1007/978-1-4615-5529-2_1\n[219] Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Cheng-zhong Xu. 2024. HydraLoRA: An Asymmetric LoRA\nArchitecture for Efficient Fine-Tuning. In The Thirty-eighth Annual Conference on Neural Information Processing\nSystems.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 57\n[220] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Teaching Broad Reasoning Skills\nfor Multi-Step QA by Generating Hard Contexts. https://doi.org/10.48550/arXiv.2205.12496 arXiv:2205.12496 [cs]\n[221] Sabine Ullrich and Michaela Geierhos. 2021. Using Bloom’s Taxonomy to Classify Question Complexity. InProceedings\nof the Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021) . 285–289.\n[222] Saranya Venkatraman, Nafis Irtiza Tripto, and Dongwon Lee. 2024. CollabStory: Multi-LLM Collaborative Story\nGeneration and Authorship Analysis. arXiv preprint arXiv:2406.12665 (2024). arXiv:2406.12665\n[223] Jan Philip Wahle. 2023. A Cohesive Distillation Architecture for Neural Language Models. https://doi.org/10.48550/\narXiv.2301.08130 arXiv:2301.08130 [cs]\n[224] Boshi Wang, Xiang Deng, and Huan Sun. 2022. Iteratively Prompt Pre-trained Language Models for Chain of Thought.\nhttps://doi.org/10.48550/arXiv.2203.08383 arXiv:2203.08383 [cs]\n[225] Dingzirui Wang, Longxu Dou, and Wanxiang Che. 2022. A Survey on Table-and-Text HybridQA: Concepts, Methods,\nChallenges and Future Directions. arXiv:2212.13465 [cs]\n[226] Jiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa. 2022. TimeBERT: Extending Pre-Trained Language Represen-\ntations with Temporal Information. arXiv:2204.13032 [cs]\n[227] Shaowen Wang, Linxi Yu, and Jian Li. 2024. LoRA-GA: Low-Rank Adaptation with Gradient Approximation. In The\nThirty-eighth Annual Conference on Neural Information Processing Systems .\n[228] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs]\n[229] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\n2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. https://doi.org/10.48550/arXiv.2212.\n10560 arXiv:2212.10560 [cs]\n[230] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Ar-\njun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary\nLai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravse-\nhaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro,\nTanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. 2022.\nSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. arXiv:2204.07705 [cs]\n[231] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and\nWilliam Fedus. 2022. Emergent Abilities of Large Language Models. https://doi.org/10.48550/arXiv.2206.07682\narXiv:2206.07682 [cs]\n[232] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\n2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs]\n[233] Joseph Weizenbaum. 1966. ELIZA—a Computer Program for the Study of Natural Language Communication between\nMan and Machine. (1966).\n[234] Sarah Wiegreffe and Ana Marasović. 2021. Teach Me to Explain: A Review of Datasets for Explainable Natural\nLanguage Processing (Accepted NeurIPS). https://doi.org/10.48550/arXiv.2102.12060 arXiv:2102.12060 [cs]\n[235] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021. One Teacher Is Enough? Pre-trained Language Model\nDistillation from Multiple Teachers. https://doi.org/10.48550/arXiv.2106.01023 arXiv:2106.01023 [cs]\n[236] Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Zhu JianHao, Cenyuan Zhang,\nXiaoqing Zheng, and Xuanjing Huang. 2024. Advancing Parameter Efficiency in Fine-tuning via Representation Editing.\nIn Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). 13445–13464. https://doi.org/10.18653/v1/2024.acl-long.726\n[237] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie Cai. 2022.\nPromptChainer: Chaining Large Language Model Prompts through Visual Programming .\n[238] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent and Controllable Human-AI\nInteraction by Chaining Large Language Model Prompts. In CHI Conference on Human Factors in Computing Systems\n(CHI ’22) . 1–22. https://doi.org/10.1145/3491102.3517582\n[239] Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl\nYang, Dawn Song, and Bo Li. 2024. GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled\nReasoning. arXiv:2406.09187 [cs]\n[240] Guangxuan Xiao, Ji Lin, and Song Han. 2023. Offsite-Tuning: Transfer Learning without Full Model. https:\n//doi.org/10.48550/arXiv.2302.04870 arXiv:2302.04870 [cs]\n[241] Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming\nZhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,\nDragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2022.\n, Vol. 1, No. 1, Article . Publication date: November 2025.\n58 Xavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent Martin, and Elisabeth Murisasco\nUnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models.\nhttps://doi.org/10.48550/arXiv.2201.05966 arXiv:2201.05966 [cs]\n[242] Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, and Jürgen Schmidhuber. 2025. Beyond Outlining: Heterogeneous\nRecursive Planning for Adaptive Long-form Writing with Language Models. https://doi.org/10.48550/arXiv.2503.08275\narXiv:2503.08275 [cs]\n[243] Canwen Xu and Julian McAuley. 2022. A Survey on Dynamic Neural Networks for Natural Language Processing.\narXiv:2202.07101 [cs]\n[244] Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, and Zhilin Yang. 2022. A Universal Discriminator for Zero-Shot\nGeneralization. https://doi.org/10.48550/arXiv.2211.08099 arXiv:2211.08099 [cs]\n[245] Jing Xu, Arthur Szlam, and Jason Weston. 2021. Beyond Goldfish Memory: Long-Term Open-Domain Conversation.\nhttps://doi.org/10.48550/arXiv.2107.07567 arXiv:2107.07567 [cs]\n[246] Runhua Xu, Nathalie Baracaldo, and James Joshi. 2021. Privacy-Preserving Machine Learning: Methods, Challenges\nand Directions. arXiv:2108.04417 [cs]\n[247] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024. Is\nDPO Superior to PPO for LLM Alignment? A Comprehensive Study. InProceedings of the 41st International Conference\non Machine Learning (ICML’24, Vol. 235) . 54983–54998.\n[248] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha.\n2025. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search. https:\n//doi.org/10.48550/arXiv.2504.08066 arXiv:2504.08066 [cs]\n[249] Yunlu Yan, Chun-Mei Feng, Wangmeng Zuo, Rick Siow Mong Goh, Yong Liu, and Lei Zhu. 2024. Federated Residual\nLow-Rank Adaptation of Large Language Models. In The Thirteenth International Conference on Learning Representa-\ntions.\n[250] Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, and Grey Yang. 2025. DocAgent:\nA Multi-Agent System for Automated Code Documentation Generation. arXiv preprint arXiv:2504.08725 (2025).\narXiv:2504.08725\n[251] Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. 2022. Chain of Thought Imitation with Procedure\nCloning. https://doi.org/10.48550/arXiv.2205.10816 arXiv:2205.10816 [cs]\n[252] Qian Yang, Qian Chen, Wen Wang, Baotian Hu, and Min Zhang. 2022. Enhancing Multi-modal and Multi-hop\nQuestion Answering via Structured Knowledge and Unified Retrieval-Generation. arXiv:2212.08632 [cs]\n[253] Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze D. Gui, Ziran W.\nJiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan E. Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen\nZha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin L.\nDong. 2024. CRAG - Comprehensive RAG Benchmark. Advances in Neural Information Processing Systems (Dec. 2024),\n10470–10490.\n[254] Yaming Yang, Dilxat Muhtar, Yelong Shen, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Weiwei Deng,\nFeng Sun, Qi Zhang, Weizhu Chen, and Yunhai Tong. 2025. MTL-LoRA: Low-Rank Adaptation for Multi-Task\nLearning. Proceedings of the AAAI Conference on Artificial Intelligence 39, 20 (April 2025), 22010–22018. https:\n//doi.org/10.1609/aaai.v39i20.35509\n[255] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree\nof Thoughts: Deliberate Problem Solving with Large Language Models. https://doi.org/10.48550/arXiv.2305.10601\narXiv:2305.10601 [cs]\n[256] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. ReAct: Syner-\ngizing Reasoning and Acting in Language Models. https://doi.org/10.48550/arXiv.2210.03629 arXiv:2210.03629 [cs]\n[257] Xi Ye and Greg Durrett. 2022. The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.\nhttps://doi.org/10.48550/arXiv.2205.03401 arXiv:2205.03401 [cs]\n[258] Yue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and Arnold Overwijk. 2022. COCO-DR: Combating Distribution Shifts\nin Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning. (2022).\n[259] Yuan. 2025. Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal\nInformation Extraction | Request PDF. In ResearchGate. https://doi.org/10.24963/ijcai.2025/772\n[260] Weizhe Yuan and Pengfei Liu. 2022. reStructured Pre-training. https://doi.org/10.48550/arXiv.2206.11147\narXiv:2206.11147 [cs]\n[261] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation.\nhttps://doi.org/10.48550/arXiv.2106.11520 arXiv:2106.11520 [cs]\n[262] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. 2024.\nTextGrad: Automatic \"Differentiation\" via Text. https://doi.org/10.48550/arXiv.2406.07496 arXiv:2406.07496\n[263] Manzil Zaheer, Ankit Singh Rawat, Seungyeon Kim, Chong You, Himanshu Jain, Andreas Veit, Rob Fergus, and Sanjiv\nKumar. 2022. Teacher Guided Training: An Efficient Framework for Knowledge Transfer. https://doi.org/10.48550/\n, Vol. 1, No. 1, Article . Publication date: November 2025.\nComplex QA & language models hybrid architectures, Survey 59\narXiv.2208.06825 arXiv:2208.06825 [cs]\n[264] Rufai Yusuf Zakari, Jim Wilson Owusu, Hailin Wang, Ke Qin, Zaharaddeen Karami Lawal, and Yuezhou Dong. 2022.\nVQA and Visual Reasoning: An Overview of Recent Datasets, Methods and Challenges. arXiv:2212.13296 [cs]\n[265] Yuchen Zeng and Kangwook Lee. 2023. The Expressive Power of Low-Rank Adaptation. In The Twelfth International\nConference on Learning Representations .\n[266] Chen Zhang, João Sedoc, Luis Fernando D’Haro, Rafael Banchs, and Alexander Rudnicky. 2021. Automatic Evaluation\nand Moderation of Open-domain Dialogue Systems. https://doi.org/10.48550/arXiv.2111.02110 arXiv:2111.02110 [cs]\n[267] Guangwei Zhang. 2025. Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge\nWork. https://doi.org/10.48550/arXiv.2507.02760 arXiv:2507.02760 [cs]\n[268] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, and Kunlong Zhou. 2024.\nTrain Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models. InThe Thirteenth International\nConference on Learning Representations .\n[269] Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo\nYin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren\nQiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, and Xinyu Liu. 2025. aiXiv: A Next-Generation Open\nAccess Ecosystem for Scientific Discovery Generated by AI Scientists. https://doi.org/10.48550/arXiv.2508.15126\narXiv:2508.15126 [cs]\n[270] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. https://doi.org/10.48550/arXiv.1904.09675 arXiv:1904.09675 [cs]\n[271] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. 2024. Chain-of-Agents: Large\nLanguage Models Collaborating on Long-Context Tasks. arXiv preprint arXiv:2406.02818 (2024). arXiv:2406.02818\n[272] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic Chain of Thought Prompting in Large\nLanguage Models. https://doi.org/10.48550/arXiv.2210.03493 arXiv:2210.03493 [cs]\n[273] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\nDacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench\nand Chatbot Arena. Advances in Neural Information Processing Systems (Dec. 2023), 46595–46623.\n[274] Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, and Tat-Seng Chua. 2022. Video Question Answering:\nDatasets, Algorithms and Challenges. https://doi.org/10.48550/arXiv.2203.01225 arXiv:2203.01225 [cs]\n[275] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad.\n2021. Detecting Hallucinated Content in Conditional Neural Sequence Generation. https://doi.org/10.48550/arXiv.\n2011.02593 arXiv:2011.02593 [cs]\n[276] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet,\nQuoc Le, and Ed Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.\narXiv:2205.10625 [cs]\n[277] Li Zhou and Kevin Small. 2020. Inverse Reinforcement Learning with Natural Language Goals. https://doi.org/10.\n48550/arXiv.2008.06924 arXiv:2008.06924 [cs]\n[278] Xin Zhou, Jinzhu Lu, Tao Gui, Ruotian Ma, Zichu Fei, Yuran Wang, Yong Ding, Yibo Cheung, Qi Zhang, and Xuanjing\nHuang. 2022. TextFusion: Privacy-Preserving Pre-trained Model Inference via Token Fusion. InConference on Empirical\nMethods in Natural Language Processing .\n[279] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022. Solving Math\nWord Problem via Cooperative Reasoning Induced Language Models. https://doi.org/10.48550/arXiv.2210.16257\narXiv:2210.16257 [cs]\n[280] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and\nGeoffrey Irving. 2020. Fine-Tuning Language Models from Human Preferences. https://doi.org/10.48550/arXiv.1909.\n08593 arXiv:1909.08593 [cs]\n[281] Mingyu Zong and Bhaskar Krishnamachari. 2022. A Survey on GPT-3. arXiv:2212.00857 [cs]\n, Vol. 1, No. 1, Article . Publication date: November 2025."
}