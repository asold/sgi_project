{
  "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
  "url": "https://openalex.org/W4389520274",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2111184121",
      "name": "Sheng Shen",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2778836142",
      "name": "Zhewei Yao",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110028274",
      "name": "Chunyuan Li",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2174985400",
      "name": "Trevor Darrell",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2003317548",
      "name": "Kurt Keutzer",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2166872174",
      "name": "Yuxiong He",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2083291282",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4287547182",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W4281709371",
    "https://openalex.org/W4382491206",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W4287121196",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W4387113775",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4287124167",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4296406182",
    "https://openalex.org/W1613249581",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4311252752",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4286986381",
    "https://openalex.org/W3207645655",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4382763281",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W1593114658",
    "https://openalex.org/W4281922990",
    "https://openalex.org/W4282028729",
    "https://openalex.org/W4382490555",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4386185600",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4386076027",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4378473852",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4221166856",
    "https://openalex.org/W1549825062",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4225390052",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W3199518308",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3083962988",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W4226153346",
    "https://openalex.org/W4320855762",
    "https://openalex.org/W4301914798",
    "https://openalex.org/W2809290718",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W4375869762",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4224232320",
    "https://openalex.org/W2963530300"
  ],
  "abstract": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11329–11344\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nScaling Vision-Language Models with Sparse Mixture of Experts\nSheng Shen†§∗ Zhewei Yao‡∗ Chunyuan Li‡∗\nTrevor Darrell† Kurt Keutzer† Yuxiong He‡\n†UC Berkeley ‡Microsoft\nsheng.s@berkeley.edu, {zheweiyao,chunyl}@microsoft.com\nAbstract\nThe field of natural language processing (NLP)\nhas made significant strides in recent years,\nparticularly in the development of large-scale\nvision-language models (VLMs). These mod-\nels aim to bridge the gap between text and vi-\nsual information, enabling a more comprehen-\nsive understanding of multimedia data. How-\never, as these models become larger and more\ncomplex, they also become more challenging\nto train and deploy. One approach to address-\ning this challenge is the use of sparsely-gated\nmixture-of-experts (MoE) techniques, which\ndivide the model into smaller, specialized sub-\nmodels that can jointly solve a task. In this\npaper, we explore the effectiveness of MoE in\nscaling vision-language models, demonstrating\nits potential to achieve state-of-the-art perfor-\nmance on a range of benchmarks over dense\nmodels of equivalent computational cost. Our\nresearch offers valuable insights into stabiliz-\ning the training of MoE models, understanding\nthe impact of MoE on model interpretability,\nand balancing the trade-offs between compute\nperformance when scaling VLMs. We hope our\nwork will inspire further research into the use\nof MoE for scaling large-scale vision-language\nmodels and other multimodal machine learning\napplications.\n1 Introduction\nThe ability to understand and generate natural language\nfrom visual information is a critical component of many\nreal-world applications, including visual question an-\nswering (VQA), visual reasoning, and multimodal in-\nformation retrieval. In recent years, the success of deep\nlearning in natural language processing (NLP) has led to\nthe development of large-scale vision-language models\n(VLMs) (Tan and Bansal, 2019; Chen et al., 2020; Li\net al., 2021b; Gan et al., 2020; Kim et al., 2021a; Alayrac\net al., 2022; Wang et al., 2022c; Shen et al., 2022b; Li\net al., 2021a; Shen et al., 2022a; Jia et al., 2021; Li et al.,\n∗equal contribution; §work initiated during an intern-\nship at Microsoft. Code is available at https://vlmoe.\ngithub.io.\n2022; Yu et al., 2022) that leverage powerful neural net-\nwork architectures to encode and decode multimodal\ninformation. However, state-of-the-art vision-language\nmodels like Flamingo-80B (Alayrac et al., 2022), BEIT-\n3-1.9B (Wang et al., 2022b), and PaLI-17B (Chen et al.,\n2022) can be computationally expensive and difficult to\ntrain, which has motivated researchers to explore ways\nof improving their efficiency and effectiveness.\nRecently, sparsely activatedMixture of Experts (MoE)\nmodels have been successfully employed to scale both\nvision (Riquelme et al., 2021; Lou et al., 2021; Mustafa\net al., 2022) and text models (Shazeer et al., 2017; Lep-\nikhin et al., 2020; Zoph et al., 2022; Du et al., 2022).\nThese models are motivated by the need to increase\nmodel parameters while controlling compute costs. In\naddition, these models provide other advantages, includ-\ning sparsity that can mitigate catastrophic forgetting in\ncontinual learningg (Collier et al., 2020; Komatsuzaki\net al., 2022), and an inductive bias that can enhance\nperformance in multitask learningg (Ma et al., 2018;\nKudugunta et al., 2021; Kim et al., 2021b). Overall, the\nuse of MoEs has proven to be a promising strategy for\nscaling deep learning models across various domains.\nBuilding on the success of MoEs in individual do-\nmains and applying the intuition that sparse models\nmay better handle different tasks versus dense counter-\nparts, we investigate the potential of MoEs for vision-\nlanguage modeling. To this end, we take the first step in\nthis direction and explore models that can process both\nimages and text for vision-language tasks. One simi-\nlar effort has been studied in LIM OE (Mustafa et al.,\n2022), where the authors proposed a modal-agnostic\nCLIP-style (Radford et al., 2021) multimodal MoEs ar-\nchitecture, but their focus is mainly on the contrastive\npre-training objective and vision-only downstream tasks.\nThere are two limitations in this setting: (1) The increas-\ning model capacity of MoEs under the the simple con-\ntrastive objective can easily lead to over-fitting issues.\n(2) The vision-only benchmarking does not reveal the\nfull power of scaling up multimodal models. Alterna-\ntively, our goal is to demonstrate the effectiveness of\nMoEs under generative modeling for vision-language\ntasks and provide a more comprehensive foundation for\nfuture research in this area.\nSpecifically, we propose a novel VLM architecture\nthat employs MoE to scale both the text-based and\n11329\nVL-FFNT-FFNV-FFN\nMulti-Head Self-Attention\nV-MoET-MoE\nMulti-Head Self-Attention\na dog is chasing a ball .\nSwitch toV-FFN\nSwitch toV-MoE\nF x\n(L-F) x\n(b) Encode Image Input (Patch Embedding)\nT-FFNVL-FFNV-FFN\nMulti-Head Self-Attention\nV-MoET-MoE\nMulti-Head Self-Attention\nSwitch toT-FFN\nSwitch toT-MoE\nF x\n(L-F) x\n(c) Encode Text Input (Word Embedding)\nVL-FFNT-FFNV-FFN\nMulti-Head Self-Attention\nV-MoET-MoE\nMulti-Head Self-Attention\na dog is chasing a ball .\nSwitch toV-FFN\nSwitch toV-MoE\nF x\n(L-F) x\n(b) Encode Image Input (Patch Embedding)\nT-FFNVL-FFNV-FFN\nMulti-Head Self-Attention\nV-MoET-MoE\nMulti-Head Self-Attention\nSwitch toT-FFN\nSwitch toT-MoE\nF x\n(L-F) x\n(c) Encode Text Input (Word Embedding)\nT-FFNVL-FFNV-FFN\nMulti-Head Self-Attention\nV-MoET-MoE\nMulti-Head Self-Attention\na dog is … ball .\nSwitch toVL-FFN\nSwitch toV & T-MoE\nV-MoE\nF x\n(L-F) x\n…\n(a) Encode Image-Text Pair Input (Patch Embedding + Word Embedding)\nV-Router\nV-FFN Expert 1V-FFN Expert 2V-FFN Expert 32…\n…T-MoE\nT-Router\nT-FFN Expert 1T-FFN Expert 2T-FFN Expert 32…\na dog is … ball .\nV-MoE\nV-Router\nV-FFN Expert 1V-FFN Expert 2V-FFN Expert 16…\n…T-MoE\nT-Router\nT-FFN Expert 1T-FFN Expert 2T-FFN Expert 16…\na dog is … ball .\n(a) Encode Image Only (b) Encode Text Only (c) Encode Image-Text Pair (d) V-MoE & T-MoE\nFigure 1: The encoding process of VL-MoE for various modality inputs, for which gray and colored blocks indicate\nnon-activated and activated modules, respectively. (a) For image input only, the encoding process switches to\nV-MoE or V-FFN (b) For text input only, the encoding process switches T-MoE or T-FFN. (c) For image-Text Pair\ninput, the encoding process switches, V-MoE & T-MoE and VL-FFN. (d) For the early layers, we scale the V-FFN\nand T-FFN with Sparse Mixture-of-Experts as V-MoE and T-MoE, respectively. VL-MoE will utilize conditional\ncomputation to allocate tokens in a modality-specific fashion. V/T-MoE converts multiple V/T-FFNs as experts,\nwhere the image/text input will be conditionally routed by V/T-Router Network.\nvision-based feed-forward networks (T-FFN and V-FFN,\nrespectively) in a unified framework. Our approach di-\nvides the model into multiple sub-models, each of which\nis responsible for processing a modal-specific subset of\nthe input data. The text and vision input representa-\ntions are then aligned via three mask data modeling\nobjectives (Wang et al., 2022b).\nWe train a range of VL-MoE models and evaluate\nthe model on vision-language classification, vision-\nlanguage retrieval, vision-only and language-only tasks,\nOur experiments demonstrate that MoE can significantly\nimprove the efficiency and effectiveness of VLMs, en-\nabling them to handle large-scale, real-world multime-\ndia data. We scale BASE -size model up to a 1.8B pa-\nrameter VL-MoELARGE /16E , which only applies 560M\nparameters per token and achieves competitive perfor-\nmance with dense models that make use of similar or\nmore pre-training image-text pair data and apply 3-4×\nmore parameters per token.\nIn summary, our contributions are as follows:\n• We propose VL-MoE, the first large-scale genera-\ntive MoEs multimodal models for vision/langauge-\nonly, as well as vision-and-language tasks.\n• We explore various scaling strategies, including in-\ncreasing dense model size, increasing expert num-\nbers, and scaling either T-FFN or V-FFN alone,\nto investigate the trade-offs between model com-\nplexity and performance on various downstream\ntasks.\n• We present ablations to understand VL-MoE\nmodel’s behavior, interpretability, and our design\nchoices.\n2 Related Work\nVision-Language Modeling. Vision-language pre-\ntraining (Tan and Bansal, 2019; Lu et al., 2019; Su\net al., 2020; Zhang et al., 2021; Radford et al., 2021; Li\net al., 2020; Kim et al., 2021a; Li et al., 2021a; Wang\net al., 2022c; Bao et al., 2022b; Wang et al., 2022a;\nAlayrac et al., 2022; Yu et al., 2022; Wang et al., 2022b;\nLi et al., 2022; Chen et al., 2022; Radford et al., 2021;\nJia et al., 2021; Shen et al., 2022b,a; Yuan et al., 2021;\nSingh et al., 2021; Liu et al., 2023b) involves developing\nmodel architecture and pretraining objectives to learn\neffective multimodal representations from large-scale\nimage-text pairs. Two main approaches are encoding\ndistinct modalities separately with different encoders.\nFor model architecture, there are two main designs.\nThe first design, utilized by models such as (Radford\net al., 2021; Jia et al., 2021; Yuan et al., 2021) separately\nencodes each modality with different encoders. While\nthis approach performs well for image-text retrieval\ntasks, it struggles with complex vision-language tasks\nlike visual reasoning. The second design, employed by\nmodels like (Tan and Bansal, 2019; Li et al., 2021a; Lu\net al., 2019; Li et al., 2019; Kim et al., 2021a; Chen et al.,\n2022; Alayrac et al., 2022), uses a complex fusion mod-\nule with cross-modal attention to combine modalities.\nHowever, this design sacrifices efficiency for improved\nperformance. Recently, a new design has emerged with\nthe MOME Transformer used in bothVLM O and BEIT-\n3. This design unifies the dual-encoder and fusion-\nencoder models by introducing a mixture-of-modality-\nexperts technique. With MOME, various modalities are\nencoded within a shared Transformer block, allowing\nfor improved scalability and achieving state-of-the-art\nperformance on vision-language tasks. There is an in-\ncreasing interest to grow the VL model capacity with\nan affordable compute budget, including MoE (Mustafa\net al., 2022) and the injection of new trainable modules\non pre-trained models (Alayrac et al., 2022; Shen et al.,\n2022a; Liu et al., 2023b; Li et al., 2023d,b; Koh et al.,\n2023); the former remains less studied.\nFor pretraining objectives, multiple cross-modal pre-\ntraining objectives have been studied. They can be cate-\ngorized into two classes: (1) Discriminative modeling,\nincluding image-text contrastive learning (Radford et al.,\n2021; Jia et al., 2021), image-text matching (Tan and\nBansal, 2019; Kim et al., 2021a; Li et al., 2021a; Bao\net al., 2022b) and word-patch/region alignment (Chen\n11330\net al., 2020; Kim et al., 2021a); (2) Generative mod-\neling, including masked language modeling (Tan and\nBansal, 2019; Su et al., 2020; Kim et al., 2021a) or pre-\nfix language modeling (Wang et al., 2022c), masked\nregion modeling (Tan and Bansal, 2019), multimodal\nprefix language modeling (Wang et al., 2022c). Re-\ncently, BEIT-3 shows strong scaling results by unifying\nthe generative multimodal pretraining objective with\nmasked data modeling, which comprises masked im-\nage modeling and masked language modeling on the\nmonomodal encoders and masked multimodal modeling\non the multimodal encoder. In this paper, we perform\nMoE study, by adopting the MOME Transformer as the\nbackbone dense network and generative (masked data)\nmodeling as pretraining objectives given its simplicity\nand scaling ability.\nMore recently, with the introduce of LLaMA (Tou-\nvron et al., 2023), PaLI’s research (Chen et al., 2022)\nfocused on the scaling of V&L components, while\nPaLM-E explored the embodied domain more deeply.\nBLIP-2 (Li et al., 2023c) introduced the innovative Q-\nformer to bridge image and language encoders, and\nthis was further enhanced by InstructBLIP (Dai et al.,\n2023). Otter (Li et al., 2023a) augmented the instruction-\nfollowing capabilities of OpenFlamingo (Lauren c ¸on\net al., 2023; Alayrac et al.; Awadalla et al., 2023). Both\nMiniGPT-4 (Zhu et al., 2023) and LLaV A (Liu et al.,\n2023a; Sun et al., 2023) draw inspiration from GPT4’s\ncapabilities but place emphasis on the efficiency and\nintegration of visual and linguistic models. In a fresh\napproach, mPLUG-Owl (Ye et al., 2023) first aligns vi-\nsual features and subsequently fine-tunes the language\nmodel using LoRA. Shikra (Chen et al., 2023) and\nKosmos (Peng et al., 2023) leverage grounded image-\ntext pairs during their training process. Lastly, QWen-\nVL (Bai et al., 2023) made notable strides in scaling\nLMM pre-training.\nSparse Mixture of Experts models. We build upon\nthe concept of deep sparse MoEs, which have been stud-\nied independently in both Computer Vision (Riquelme\net al., 2021; Lou et al., 2021; Mustafa et al., 2022) and\nNatural Language Processing (Riquelme et al., 2021;\nLou et al., 2021; Mustafa et al., 2022; Shazeer et al.,\n2017; Lepikhin et al., 2020; Fedus et al., 2021; Du et al.,\n2022; Zoph et al., 2022; Clark et al., 2022; Zhou et al.,\n2022; Komatsuzaki et al., 2022; Kudugunta et al., 2021;\nShen et al., 2023) in the context of conditional computa-\ntion. The goal of conditional computation is to increase\nthe number of model parameters without a proportional\nincrease in computational cost, which is achieved by\nselectively activating only relevant parts of the model\nbased on input-dependent factors (Bengio, 2013; Chen\net al., 1999; Davis and Arel, 2013). MoE models use a\nlearned gating mechanism that activates only a subset\nof kexperts out of E ≫kfor a given input, allowing\nan input to select either all experts (Eigen et al., 2013)\nor only a sparse mixture thereof, as in recent massive\nlanguage models (Fedus et al., 2021; Du et al., 2022).\nWhile many works aim to improve the gating mecha-\nnism itself (Hazimeh et al., 2021; Lewis et al., 2021;\nRoller et al., 2021; Zhou et al., 2022), MoE models have\nalso been studied for multitask learning (Hazimeh et al.,\n2021; Kudugunta et al., 2021) with per-task routers (Ma\net al., 2018), although a shared pool of experts is typi-\ncally used.\nMoE models have been explored for multimodal\nlearning as well, with LIM OE (Mustafa et al., 2022) and\nUni-MoE (Zhu et al., 2022) being most relevant to our\nwork. However, LIM OE considers the CLIP-style con-\ntrast as the pre-training objective, and vision/retrieval\ntasks as the downstream evaluation. Uni-MoE focuses\non routing decisions with limited experts and evaluates\non caption/vision/language/retrieval tasks. To the best\nof our knowledge, the proposed VL-MoE is the first the\nMoE scaling study to consider the generalized genera-\ntive modeling objective in the VL pre-training, and we\nevaluate its scaling performance in a more comprehen-\nsive manner, including vision/language-only, as well as\nvision-and-language tasks.\n3 Method\nWe first describe the masked data modeling pretrain-\ning objectives. We next discuss MoEs, sparse MoEs\nand present how we apply sparse MoEs methodology to\nvision-language models, before explaining our design\nchoices for the routing algorithm and the implementa-\ntion of VL-MoE.\n3.1 Vision-Language Masked Data Modeling\nWe utilized a unified masked data modeling objec-\ntive (Wang et al., 2022b) to pretrain VL-MoE on\nmonomodal (i.e., images and texts) and multimodal\ndata (i.e., image-text pairs). This approach has been\ndemonstrated to be scaling-friendly with small batch-\nsizes. Our pretraining process involved masked image\nmodeling on monomodal image data, masked language\nmodeling on monomodal text data, and masked vision-\nlanguage modeling on multimodal image-text pairs.\nMasked Language Modeling We use masked lan-\nguage modeling (MLM) to learn language representa-\ntions from large-scale text-only data. For MLM, 15%\nof tokens in monomodal text data are randomly masked,\nand the model is trained to recover the masked tokens\nfrom the corrupted input text. Masked tokens are re-\nplaced by a [MASK] token 80% of the time, a random\ntoken 10% of the time, and kept the original tokens 10%\nof the time, following BERT (Devlin et al., 2019).\nMasked Image Modeling In addition to masked lan-\nguage modeling, VL-MoE uses masked image modeling\n(MIM) to learn vision representations from large-scale\nimage data. For MIM, block-wise masking is applied\nto 40% of image patches, and the pretraining objective\nis to reconstruct the discrete visual tokens of masked\npatches, following BEiT (Bao et al., 2022a). The im-\n11331\n0 2K 4K 6K\nTrain GFlops\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45MIM Loss (Valid)\nDensesmall MoEsmall/E8 MoEsmall/E16 MoEsmall/E32 Densebase MoEbase/E16 Denselarge MoElarge/E16\n0 2K 4K 6K\nTrain GFlops\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30Total Loss (Valid)\n0 2K 4K 6K\nTrain GFlops\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8MLM Loss (Valid)\n0 2K 4K 6K\nTrain GFlops\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90VLM Loss (Valid)\n0 2K 4K 6K\nTrain GFlops\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45MIM Loss (Valid)\n(a) Total Validation Loss (b) MLM Validation Loss (c) VLM Validation Loss (d) MIM Validation Loss\nFigure 2: Effect of VL-MoE scaling on three mask language modeling (MLM), mask image modeling (MIM), and\nmasked vision-language modeling (VLM) pre-training tasks across training flops.\nage tokenizer of BEITv2 (Peng et al., 2022) is used to\nobtain the discrete tokens as the reconstructed targets.\nMasked Vision-Language Modeling To learn\naligned vision-language representation, we use masked\nvision-language modeling (VLM), which extends\nmasked language modeling and masked image model-\ning to multimodal data. The task aims at recovering\nmasked image patches and text tokens based on visual\nand linguistic clues. In VLM, text tokens (with 50%\nmask ratio) are randomly masked as in MLM, and the\nmodel is trained to recover the masked text tokens based\non the joint image-text representations. Image patches\nare also masked with the same ratio as in MIM, and\nthe corresponding visual tokens are predicted based on\nthe image-text pair. The VLM task further encourages\nthe model to learn alignments between image and text\npairs.\n3.2 VL-MoE Architecture\nInput Representation. To obtain text representations,\nthe input text is tokenized and projected onto word\nembeddings ({wi}M\ni=1), where M is the length of the\ntokenized text sequence. Two special tokens, a start-\nof-sequence token ([T CLS]) and a special boundary\ntoken ([T SEP]), are added to the sequence. Text rep-\nresentations are obtained by summing the word em-\nbeddings and text position embeddings, resulting in\nHw = [w[T CLS],w1,..., wM,w[T SEP]] + Tpos.\nFor image representations, the input 2D image v ∈\nRH×W×C is split and reshaped into N = HW/P 2\npatches vp ∈RN×(P2C), where C is the number of\nchannels, (H,W ) is height and width of the input im-\nage, and P is the patch size. These patches are then flat-\ntened into vectors and linearly projected to obtain patch\nembeddings following vision Transformers (Dosovit-\nskiy et al., 2020; Touvron et al., 2020; Bao et al., 2022a).\nWe prepend a learnable special token [I CLS] to the\nsequence. The resulting image input representations are\ngiven by Hv = [v[I CLS],v1,..., vN] + Vpos, where\nHv ∈R(N+1)×D, V ∈R(P2C)×D is a linear projec-\ntion, Vpos ∈R(N+1)×D are learnable 1D position em-\nbeddings.\nTo form image-text input representations, we con-\ncatenate image and text input vectors, resulting in\nHvl\n0 = [Hw\n0 ; Hv\n0].\nBackbone Network. The dense backbone network\nof VL-MoE is a shared multimodal Transformer, illus-\ntrated in Figure 1. To encode different modalities, we\nutilize a mixture-of-modality-experts (MOME) Trans-\nformer(Bao et al., 2022b; Wang et al., 2022b), which\ntakes image and text representations of monomodal data,\nas well as representations of image-text pairs as input.\nThe MOME Transformer comprises multiple layers of\nblocks, each consisting of a multi-head self-attention\nlayer and a feed-forward expert layer. While the self-\nattention module is shared across modalities, each feed-\nforward expert layer contains a pool of modality-specific\nexperts (V-FFN, T-FFN, or VL-FFN) that act as a sub-\nstitute for the feed-forward network in standard Trans-\nformers. This allows for hard routing over the pool of\nfeed-forward networks based on the modality of the\ninput tokens.\nConditional Computation with MoEs. The concept\nof conditional computation involves selectively activat-\ning different parts of a neural network based on the\ninput (Bengio, 2013). One specific approach is to use a\nmixture-of-experts (MoE) model, where different “ex-\nperts” handle different regions of the input space (Jacobs\net al., 1991). In this paper, we adopt the MoE layer pro-\nposed in (Shazeer et al., 2017), which consists of E\nexperts and is defined as MoE(x) = ∑E\ni=1 g(x)i ei(x).\nHere, x is the input to the layer, ei : RD ↦→RD is the\nfunction computed by expert i, and g: RD ↦→RE is the\n“routing” function that determines the input-dependent\nweights for the experts. Both ei and gare implemented\nas neural networks. Although this formulation still in-\nvolves a dense network, it can be made sparse by restrict-\ning gto assign only k≪Enon-zero weights, thereby\neliminating the computation of unused experts. This\napproach allows for super-linear scaling of the number\nof model parameters in both training and inference.\nVL-MoE. We apply sparse MoE to vision-language\nmodels in the context of the MOME. As illustrated in\nFigure 1, inputs from different modalities are routed\nto V-FFN and T-FFN in the first ( L −F) layers,\nand V-FFN, T-FFN, or VL-FFN in the last F lay-\ners. To avoid instability due to modality input im-\nbalance when applying MoEs to modal-agnostic VL-\n11332\nmodules in V-MOE (Riquelme et al., 2021), we only\nuse MoE for V-FFN and T-FFN in the first ( L−F)\nlayers. V-FFN and T-FFN have two layers and a\nGeLU (Hendrycks and Gimpel, 2016) non-linearity:\nV/T-FFN(x) = W2 σgelu(W1x). For VL-MoE, we\nreplace a subset of V-FFN and T-FFN with V-MoE\nand T-MoE layers, where each expert is an FFN with\nthe same architecture ei(x) = FFNθi (x) but differ-\nent weights θi = ( Wi\n1,Wi\n2). This design pattern is\nsimilar to that of GShard (Lepikhin et al., 2020) and\nV-MOE (Riquelme et al., 2021) models. In V-MoE\nand T-MoE layers, each token x ∈RD is processed\nsparsely by k out of E available experts. To select\nwhich one, a lightweight V/T-Router predicts gating\nweights per token: g(x) = softmax(Wgx) ∈RE,\nwhere Wg ∈RD×E is learned. The kactivated experts’\noutputs are combined linearly according to the gating\nweights: MoE(x) = ∑k\ne=1 g(x)e ·FFNe(x).\nTo ensure computational efficiency and implemen-\ntation constraints, each expert in VL-MoE has a fixed\nbuffer capacity, which determines the maximum number\nof tokens it can process. The assumption is that tokens\nare approximately balanced across experts. In case the\ncapacity is exceeded, some tokens are not processed by\nthe expert and are dropped, leading to a decrease in the\nsuccess rate. This rate is a vital indicator of balanced\nrouting and training stability. To mitigate this problem,\nwe employ Batch Priority Routing (BPR) (Riquelme\net al., 2021; Mustafa et al., 2022), which selectively\nskips tokens based on their routing weights. BPR pri-\noritizes tokens with larger routing weights, as they are\ndeemed more informative. Our results show that BPR\nis crucial for stable training of VL-MoE. We further an-\nalyze token routing decisions in Section 5 and dropped\ntokens in Appendix.\n4 Experiment\n4.1 Pretraining Setup\nPretraining Data. Our pretraining process uses both\nmonomodal and multimodal data. The monomodal\ndata comprises ImageNet-22K for images and English\nWikipedia and BookCorpus (Zhu et al., 2015) for text.\nThe multimodal data combines four datasets of image-\ntext pairs: Conceptual Captions (Sharma et al., 2018),\nSBU Captions (Ordonez et al., 2011), COCO (Lin et al.,\n2014), and Visual Genome (Krishna et al., 2017), con-\ntaining a total of 4 million images and 10 million image-\ntext pairs.\nPretraining Setting. For the large-size model, we em-\nploy a 24-layer Transformer network with 1024 hidden\nsize and 24 attention heads, following VIT (Dosovitskiy\net al., 2020), BEiT (Bao et al., 2022a), and VLMO (Bao\net al., 2022b). The use of VL-FFN starts at 20th layer.\nThe base/small-size model is an12/8-layer Transformer\nnetwork with 768/384 hidden size and 12/6 attention\nheads, where VL-FFN is used in 10/8th layer. We ran-\ndomly initialize the model parameters using the method\ndescribed in BEiT (Bao et al., 2022a). The image reso-\nlution is set to 224 ×224, and the patch size is 16 ×16.\nThe maximum sequence length for text is 96. We use\na batch size of 6,144 and train the model from scratch\nfor 200k steps, which is equivalent to 40 epochs of the\nimage-text pairs. Each batch contains 2,048 images,\n2,048 texts, and 2,048 image-text pairs. We perform\nimage augmentation using random resized cropping,\nhorizontal flipping, and color jittering, following the\nsame method as BEiT (Bao et al., 2022a). The text data\nis tokenized using a SentencePiece (Kudo and Richard-\nson, 2018) tokenizer with a vocabulary size of 64k. We\nuse the Adam optimizer (Kingma and Ba, 2015) with\nβ1 = 0.9 and β2 = 0.999 to optimize the model. The\npeak learning rate is 2e-3, and we use linear warmup\nfor the first 10,000 steps and cosine learning rate decay.\nThe weight decay is 0.05, and we disable dropout and\nuse stochastic depth (Huang et al., 2016) with a rate of\n0.1. The three pretrain losses are equally weighted as in\nBEIT-3 (Wang et al., 2022b).\nMoE Setting. For the default setting of MoEs in VL-\nMoEBASE /16E , we use E = 16 experts for T-FFN and\nV-FFN, respectively. All VL-MoEs activatek = 1 ex-\npert per token, similar to Switch Transformer (Fedus\net al., 2021) and LIMoE (Mustafa et al., 2022). We\nreplace every second dense T-FFN or V-FFN sublayer\nwith MoE sublayer following GShard (Lepikhin et al.,\n2020) and Switch Transformer (Fedus et al., 2021). We\nuse BPR for stability in V-MoE (Riquelme et al., 2021).\nFor auxiliary loss, we use loading loss in (Shazeer et al.,\n2017) for T-FFN’s MoE and averaged loading loss and\nimportance loss in V-MoE (Riquelme et al., 2021) for V-\nFFN’s MoE. The combination ratio for auxiliary loss is\nset as 0.01 in all our experiments We use 32 expert par-\nallelism and TUTEL (Hwang et al., 2022) for fast routing\nand computation. All the models are based on Deep-\nSpeed (Rasley et al., 2020). Pre-training experiments\nare done on 32 Nvidia Tesla V100-32GB GPUs. Follow-\ning ST-MoE (Zoph et al., 2022), we freeze all the MoE\nmodules (router and expert network) during finetuning\nprocess. The capacity factor Cis set to be 1.05 during\ntraining and 1 during inference following (Riquelme\net al., 2021).\nVL-MoE in Pretraining. We present the validation\nperformance of VL-MoE on the three pretraining tasks\nacross different scales. The results show that the cost-\nperformance tradeoff of VL-MoE in terms of pretraining\nflops dominates the dense models by a wide margin, in-\ndicating that VL-MoE offers significant improvements\nacross all scales, from SMALL /8E to LARGE /16E . We\nalso provide a wall-clock time versus validation perfor-\nmance figure in the Appendix, which shows a similar\nscaling trend of VL-MoE. Thanks to careful kernel opti-\nmization and expert parallelism in DeepSpeed (Rasley\net al., 2020), the maximum wall-clock overhead of VL-\nMoELARGE /16E compared to dense counterparts can be\nreduced to only 13%.\n11333\nExpert 1(eyes)Expert 7 (words)Expert 13(ff&v)\n(a) Vision Token Routing Decisions.(b) Language Token Routing Decisions.\nFigure 3: Token routing decisions on COCO. Examples of vision tokens routing decisions and breakdown of\nlanguage token routing decisions at the V/T-MoE layer placed in the6-th encoder block –i.e. middle of the network–\nfor VL-MoELARGE /16E .\nModel # Pretrained # Pretrained # Params VQA NLVR2 COCO Flickr30K\nimages Steps per token test-dev test-std dev test-P TR IR TR IR\nBase-size models pretrained in the similar settings\nUNITERBASE(Chen et al., 2020) 4M 200k 86M 72.70 72.91 77.18 77.85 64.4 50.3 85.9 72.5\nVILLABASE(Gan et al., 2020) 4M 200k 86M 73.59 73.67 78.39 79.30 - - 86.6 74.7\nUNIMOBASE(Li et al., 2021b) 4M 500K 120M 73.79 74.02 - - - - 89.7 74.7\nViLT (Kim et al., 2021a) 4M 200k 120M 71.26 - 75.70 76.13 61.5 42.7 83.5 64.4\nALBEFBASE(Li et al., 2021a) 4M 240k 210M 74.54 74.70 80.24 80.50 73.1 56.8 94.3 82.8\nVLMOBASE(Bao et al., 2022b) 4M 200k 180M 76.64 76.89 82.77 83.34 74.8 57.2 92.3 79.3\nBEIT-3BASE∗ 4M 200k 180M 76.21 76.75 84.93 85.76 78.7 60.3 95.3 83.8\nVL-MoEBASE/16E 4M 200k 180M 78.21 78.63 85.52 86.77 79.4 61.2 96.1 84.9\nPretained with more aggressive cost, including compute / data / model\nVLMOLARGE(Bao et al., 2022b) 4M 200k 560M 79.94 79.98 85.64 86.86 78.2 60.6 95.3 84.5\nALBEFBASE(Li et al., 2021a) 14M 800k 210M 75.84 76.04 82.55 83.14 77.6 60.7 95.9 85.6\nBLIPLARGE(Li et al., 2022) 129M 1.26M 427M 78.24 78.17 82.48 83.08 81.9 64.3 97.3 87.3\nSIMVLMBASE(Wang et al., 2022c) 1.8B 1M 230M 77.87 78.14 81.72 81.77 - - - -\nSIMVLMHUGE(Wang et al., 2022c) 1.8B 1M 1.7B 80.03 80.34 84.53 85.15 - - - -\nBEIT-3HUGE(Wang et al., 2022b) 21M 1M 1.9B 84.19 84.03 91.51 92.58 84.8 67.2 98.0 90.3\nPALIHUGE(Wang et al., 2022b) 1.6B 1M 17B 84.30 84.30 - - - - - -\nBLIP2XL(Li et al., 2023b) 129M 250k 4.1B 81.55 81.66 - - 85.4 68.3 97.6 89.7\nBEIT-3LARGE∗ 4M 200k 560M 78.14 78.23 85.23 86.15 79.2 61.4 95.7 84.1\nVL-MoELARGE/16E 4M 200k 560M 79.91 79.95 86.28 87.14 79.9 62.3 96.5 85.3\nTable 1: Finetuning results of different models on vision-language classification tasks and image-text retrieval tasks.\nWe report vqa-score on VQA test-dev and test-standard split, accuracy for NLVR2 development and public test set\n(test-P) and top-1 recall for image retrieval (IR) and text retrieval (TR). (∗denotes the model that is reproduced by\nus and trained with the same setting as VL-MoE.)\n4.2 Vision-and-Language Downstream Tasks\nIn our study, we explore the performance of VL-MoE\non vision-and-language downstream tasks through fine-\ntuning experiments on three standard tasks: visual ques-\ntion answering (Goyal et al., 2017), natural language\nfor visual reasoning (Suhr et al., 2019), and image-text\nretrieval (Plummer et al., 2015; Lin et al., 2014). Fol-\nlowing BEIT-3, we use 480 ×480 image resolution for\nVQA fine-tuning and 384 ×384 for the other tasks.\nVisual Question Answering (VQA). For VQA, the\ntask is to generate/choose the correct answer given\na natural image and a question. Following previous\nwork (Kim et al., 2021a; Bao et al., 2022b; Wang et al.,\n2022b), we utilize the VQA 2.0 dataset (Goyal et al.,\n2017) and formulate it as a classification problem with\n3,129 most frequent answers. We finetune VL-MoE as\na fusion network to encode both the image and question.\nWe use the final encoding vector of the [T CLS] token\nas the representation of the image-question pair, and\nfeed that into a classifier layer to predict the label.\nNatural Language for Visual Reasoning (NLVR2).\nVisual reasoning task aims to predict whether a text\ndescription is true about a pair of images. We use\nNLVR2 (Suhr et al., 2019) dataset for evaluation. Fol-\nlowing OSCAR (Li et al., 2020), VinVL (Zhang et al.,\n2021) and VLM O (Bao et al., 2022b), we reformulate\nthe triplet input into two image-text pairs, each contain-\ning the text description and one image. We use VL-MoE\nas a fusion network to jointly encode the image and text.\nThe concatenated final vector of [T CLS] token from\nthe two pairs is then fed into a classification layer to\npredict the label.\nImage-Text Retrieval. For image-text retrieval, it con-\ntains both image-to-text retrieval and text-to-image re-\ntrieval for different target modalities. We use the widely\nused COCO (Lin et al., 2014) and Flickr30K (Plummer\net al., 2015) datasets to evaluate the model, and adopt\nthe Karpathy split (Karpathy and Fei-Fei, 2015) follow-\ning common practices. Noted that in the architecture\nof VL-MoE and BEIT-3 (Wang et al., 2022b), it does\nnot involve the image-text matching module as existing\nin CLIP (Radford et al., 2021). To enable image-text\nmatching, we further fine-tune VL-MoE jointly with\nimage-text contrastive and image-text matching with\nhard negative mining objectives as inVLM O (Bao et al.,\n11334\nModels Pretraining Tasks\n# Images # Steps ImageNet MNLI-m\nVision Pretraining\nVITB/16 300M 500k 83.6 -\nBEITB/16 1.2M 500k 85.2 -\nV-MOEB/16-16E 300M 500k 85.3 -\nVision-Language Pretraining\nSIMVLMBASE 1.8B 1M 80.6 64.4\nBEIT-3∗\nBASE 4M 200k 83.2 67.0\nVL-MoEBASE/16E 4M 200k 84.5 68.1\nTable 2: Results of base-size models on image classifi-\ncation (ImageNet-1K) and natural language inference\n(MNLI-m). We report top-1 accuracy for both.\n2022b) and BEIT-3. During inference, VL-MoE is used\nto encode images and text separately and compute the\nmatching scores by the dot product of image and text\nvectors to obtain the top-kcandidates.\nTable 1 presents the results of our vision-language\nmodel on classification and retrieval tasks, including\nVQA, NLVR2, COCO, and Flickr30K. To ensure a\nfair comparison, we provide details on the amount of\npretraining image-text pair data, pretraining steps, and\nthe number of parameters per input token. Following\nLIM OE (Mustafa et al., 2022), we define the number\nof parameters per input token as the number of param-\neters that the model applies to each image-text token\npair. Notably, VL-MoELARGE /16E contains 2 billion pa-\nrameters in total, but only applies 560 million param-\neters per token. Additionally, all routers combined ac-\ncount for less than 0.5 million parameters. Our model\noutperforms previous large/base-size models on VQA,\nNLVR2, COCO, and Flickr30K by a significant mar-\ngin, particularly when compared to a reproduced BEIT-\n3 (Wang et al., 2022b), which was pretrained using the\nsame settings as VL-MoE. Moreover, to the best of our\nknowledge, VL-MoE is the first to demonstrate that a\nmixture-of-experts architecture can successfully scale\nwith a comparably modest architecture size and training\ncounts, while achieving generalization performance on\na range of tasks in the context of vision-language tasks.\nInterestingly, Switch Transformer (Fedus et al., 2021)\nstruggles with generalization for language MoE, while\nV-MOE (Riquelme et al., 2021) and LIM OE (Mustafa\net al., 2022) only evaluate on downstream vision tasks.\nAdditionally, VL-MoE even outperforms VLM OLARGE\nand ALBEF, which are pretrained with more image-\ntext pair data and initialized from pretrained models, on\nCOCO and Flickr30K and achieves competitive perfor-\nmance on VQA and NLVR2. We assume that this may\nbe due to the fact that the capacity of VL-FFN has not\nbeen scaled in VL-MoE, as reflected in the pretraining\nplot in Figure 2 (the difference of VLM loss between\nVL-MoE and dense BEIT-3 model is smaller compared\nto that of MLM and MIM loss). We leave the scale of\nthe VL-FFN module for future work, considering the\nincreasing instability in modal-agnostic MoE architec-\ntures demonstrated in LIM OE (Mustafa et al., 2022).\n0 25 50 75 100 125 150 175\nNumber of Steps (k)\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\n1.30\n1.32\n1.34\nTrain Loss (Total)\nModel\nDensesmall\nMoEsmall/E16-vloss\nMoEsmall/E16-vloss-no_BPR\nMoEsmall/E16-balance_loss\nMoEsmall/E16-zloss\nFigure 4: Effect of auxiliary loss on training stability.\n4.3 Vision/Language-Only Downstream Tasks\nImage Classification. We use the image classification\ntask to evaluate the model on the vision-only down-\nstream task, where the objective of this task is to cat-\negorize an input image into its corresponding class.\nWe employ the ILSVRC-2012 ImageNet dataset (Rus-\nsakovsky et al., 2015), which consists of 1.3M images\nwith 1k classes. Following BEIT (Bao et al., 2022a) and\nVLM O (Bao et al., 2022b), we perform average pooling\nover the final vectors and feed the resulting vector into\na linear classifier layer to predict the label.\nNatural Language Inference. We use the natural\nlanguage inference task to evaluate the model on the\nlanguage-only downstream task. The task involves de-\ntermining the relationship between two pieces of text.\nIn this task, a model is given a premise sentence and a\nhypothesis sentence, and it needs to determine whether\nthe hypothesis is true, false, or undetermined based on\nthe information provided in the premise. We use Multi-\nGenre Natural Language Inference (MNLI) (Williams\net al., 2018) dataset, which contains 433k sentence pairs\nannotated with textual entailment information. We eval-\nuate on matched (MLM-m) setting only.\nAs shown Table 2, we compare VL-MoE with two\nbase-size vision Transformers and V-MOE-B/16-E16\non image classification. For BEIT, BEIT-3BASE and\nVL-MoEBASE /16E , we perform intermediate finetuning\non ImageNet-22k to compare with VIT pretrained on\nImageNet-22k. The model performs competitively with\nprevious state-of-the-art supervised and self-supervised\nmodels on ImageNet-1k. Besides the dense counterpart\nBEIT-3BASE , VL-MoE also outperforms other strong\nvision-language models ( SIMVLM ) pretrained with\nmore data and more steps on MNLI-m.\n5 Discussions\nWe conduct ablation studies to analyze the contributions\nof Mixture-of-Experts module used in VL-MoE from\ndifferent perspectives. We evaluate the models on visual\nreasoning (NLVR2), image-text retrieval (Flickr30k),\nimage classification (ImageNet-1k) and natural lan-\nguage inference (MNLI-m).\n11335\nScaling Strategy NLVR2 Flickr30k ImageNet MNLI-m Avg.T-MoE V-MoE dev test-P TR R@1 IR R@1 Acc@1 Acc\n[1] ✗ ✗ 67.42 68.21 80.4 61.7 67.2 54.3 66.5\n[2] ✓ ✗ 72.42 72.73 83.2 64.7 67.8 58.3 69.9\n[3] ✗ ✓ 71.19 72.23 82.9 64.5 69.2 55.2 69.2\n[4] ✓ ✓ 72.98 73.34 84.7 65.3 69.0 58.1 70.6\nTable 3: Ablation studies of scaling strategies (all the results are based on VL-MoE SMALL /E16 models). All the\n*-MoE uses 16 experts (where T/V stands for applying MoE on the T/V-FFN).\nScaling Strategy. In addition to scaling both T-FFN\nand V-FFN, we have also explored different scaling\nstrategies by applying Mixture-of-Experts (MoEs) mod-\nules for either T-FFN or V-FFN alone. The results of\nour experiments are presented in Table 3. Our findings\nindicate that scaling a single modality can improve the\ndownstream performance on the corresponding modality\nas well as overall vision-language tasks. However, we\nobserved that scaling both vision and language modali-\nties leads to the most balanced performing model with\n70.6% averaged performance. This may be attributed\nto the fact that we employ three different pretraining\nobjectives for each modality, and scaling each modality\ncontributes to better optimization of the specific modal-\nity pretraining loss as well as the VLM loss. For further\nevidence, we include the pre-training loss in Appendix.\nNumber of Experts. The optimal number of experts\nin Mixture-of-Experts (MoEs) is still a topic of debate,\nas there is no agreement on the ideal number. Previous\nNLP research has experimented with a wide range of\nexpert numbers, ranging from thousands in early stud-\nies (Shazeer et al., 2017; Fedus et al., 2021), to as low\nas 32 or 64 in more recent research (Zoph et al., 2022;\nDu et al., 2022; Zhou et al., 2022), which has become\nthe standard for vision models (Riquelme et al., 2021;\nMustafa et al., 2022). In Figure 5, we investigate this\nfurther with VL-MoE, and our findings suggest that\nlarger expert pools consistently yield performance im-\nprovements.\nEffects of the Auxiliary Losses. As previously men-\ntioned, experts in MoEs have a fixed buffer capac-\nity, and without intervention, top-kMoEs tend to col-\nlapse, leading to poor performance as most tokens are\ndropped (Shazeer et al., 2017; Zhou et al., 2022). To pre-\nvent this, prior research has employed auxiliary losses to\npromote balanced routing (Riquelme et al., 2021; Zoph\net al., 2022; Zhou et al., 2022; Mustafa et al., 2022).\nHowever, as shown in LIM OE (Mustafa et al., 2022),\nin multimodal settings, new challenges emerge, such\nas modality misbalance, where one data type may be\nmore prevalent than the other. We design VL-MoE in a\nmodal-specific fashion to prevent the instability caused\nby imbalance of multimodal data and experiment with\ndifferent auxiliary losses for V-MoE: loading balance\nloss (Shazeer et al., 2017), averaged loading balance\nand important loss (“vloss”) (Riquelme et al., 2021),\n1 8 16 32\nNumber of Experts\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nAverage over NLVR2 and Flickr30k\nsmall\nbase\nFigure 5: Effect of Experts Number.\nModels Size Methods Efficiency Val\n# E Param/Mem EP KN TPS Speedup Loss\nBEIT - 180M/0.3G - - 1002.3 - 4.51\nVL-MoE16 180M/1.6G✗ ✗ 450.5 ×0.9 4.49\nVL-MoE16 180M/0.3G✓ ✗ 685.0 ×1.4 4.50\nVL-MoE16 180M/0.3G✓ ✓ 887.5 ×1.8 4.48\nVL-MoE8 180M/0.3G ✓ ✓ 911.5 ×1.4 4.51\nVL-MoE16 105M/0.3G✓ ✓ 1211.3 ×1.3 4.50\nTable 4: Efficiency results of base-size VL-MoE models\nwith different optimizations.\nz-loss (Zoph et al., 2022)). 1 We present the results on\nVL-MoESMALL /E16 in Figure 4, which suggest that Z-loss\npresents to hurt the vision-and-lanaguage pretraininig\nof VL-MoE and using loading balance loss only will\nintroduce unstable training and underperforming mod-\nels. The “vloss” turns out to lead to most stable training,\nwhich is consistent with V-MOE (Riquelme et al., 2021)\nand LIM OE (Mustafa et al., 2022). BPR also helps in\nstablizing training.\nToken Routing Examples in VL-MoE. In Figure 3,\nwe provide a qualitative analysis of token routing deci-\nsions on COCO. For vision tokens, their specialization\nis clear, as they are routed to specific experts such as\nfood and vegetable experts, eyes experts, OCR experts,\netc. On the other hand, language tokens show signs\nof syntax specialization, with some experts processing\nmostly padding tokens, while others focus on nouns and\nadjectives (and some padding), excluding prepositions,\ndeterminers, or verbs.\n1We find that the T-MoE is quite stable using different aux-\niliary losses, and resort to the most common loading balance\nloss in (Shazeer et al., 2017) for T-MoE. We detail the formula\nof each auxiliary loss in the Appendix.\n11336\nEfficiency In Table 4, we use one V100×16 node for\nbenchmarking the efficiency of VL-MoE with various\noptimizations. The EP stands for the expert parallelism\nprovided in DeepSpeed library and KN denotes the spe-\ncialized kernel fusing operation we implemented (ex-\npert dispatch as well as bias gelu fusion). From the\ntable, we see that the throughput for the BEIT model is\n1002.3 sample/s, while the optimized VL-MoE with EP\nand Kernel has a throughput of 887.5 sample/s with the\nsame parameters per token, which add around 11% over-\nhead. Despite the latter being a more complex model,\nits throughput doesn’t fall too short of the simplerBEIT.\nThe Speedup column also suggests that with our opti-\nmizations, VL-MoE can even surpass BEIT to reach the\nsame level of validation loss in terms of speed, given the\nsame parameter per token size. It’s also valuable to note\nthat the naive implementation of VL-MoE without any\noptimization indeed incurs a wall-clock time loss and\nsignificant memory cost, as seen from the throughput\nvalue of 450.5 sample/s and around 5×memory.\nComparision with LIM OE. In LIM OE (Mustafa\net al., 2022), the single-modality MoE architecture and\nthe employed contrastive loss are the two main building\nblocks. To directly compare the two components of mul-\ntimodal LIM OE under our setting, we thoroughly exper-\nimented with optimizing either the single-modality MoE\narchitecture or VL-MoE with contrastive or masked data\nmodeling (MDM) loss. However, we found that the\nmodels fail to converge when optimizing the LIM OE\narchitecture with the MDM loss, likely due to the fact\nthat the MDM losses consist of three losses aiming for\ndifferent modalities, which may exacerbate the modality\nimbalance problem and make it difficult to optimize\nMoEs even equipped with the entropy balancing loss\nin (Mustafa et al., 2022).\nTherefore, we focused on optimizing VL-MoE and\nLIM OE with the contrastive loss, as it yielded more\nstable results. However, it should be noted that while\nLIM OE uses 1.8B image-text pairs, our setting only\nhas 4M. We then report the training and validation loss\nacross steps by optimizing VL-MoE or LIM OE with\nthe contrastive loss in Figure 8. The batch size is set to\nbe 2k. From the zero-shot validation results, it can be\nseen that both models quickly overfit to the 4M image-\ntext pairs, but the single modality MoE architecture\nin LIM OE inherits more instability.\nFurthermore, we use 4M data to enrich the experi-\nments using contrastive loss with different model set-\ntings in Table 5. We can see that LIM OE seems to ex-\nhibit a trend where performance doesn’t improve much\nor even decreases as the number of training steps in-\ncreases (from 75k to 100k), especially in the 105M\nparameter setting. This could be a sign of overfitting,\nwhere the model is starting to fit the training data more\nclosely but is not generalizing as well to the valida-\ntion/test data. Increasing the number of experts for\nLIM OE does not lead to significant performance gains,\nespecially in the 105M parameter setting. This might\nModels Size IN0shot\n# Param # E 50k 75k 100k\nContrastive Pretraining\nD ENSE 105M - 50.3 63.2 67.5\nLIM O E 105M 8 53.7 62.9 62.0\nLIM O E 105M 16 54.6 63.1 62.1\nVL-MoE 105M 8 55.2 64.2 68.3\nVL-MoE 105M 16 57.2 65.1 69.0\nD ENSE 180M - 60.1 70.3 78.2\nLIM O E 180M 8 61.5 70.4 68.2\nLIM O E 180M 16 61.2 69.3 67.5\nVL-MoE 180M 8 62.5 71.7 78.9\nVL-MoE 180M 16 63.2 72.4 79.5\nTable 5: Comparision between VL-MoE and LIM OE\nusing contrastive loss.\nindicate that, at this data scale, the additional capac-\nity introduced by more experts isn’t effectively utilized.\nHowever, VL-MoE, with a higher number of experts,\nshows a better performance progression with increasing\nsteps, suggesting a more efficient use of the additional\ncapacity. VL-MoE consistently outperforms LIM OE in\nmost settings, especially as we increase the number of\ntraining steps. This could be attributed to inherent archi-\ntectural advantages or better synergy with the training\nobjective.\n6 Conclusion\nIn this paper, we have explored the use of Mixture-of-\nExperts (MoE) for scaling vision-language models. Our\nexperiments demonstrate that MoE can be a promising\ntechnique for improving the efficiency and effectiveness\nof vision-language models. Specifically, we have shown\nthat dividing a large vision-language model into smaller,\nspecialized sub-models through MoE can achieve state-\nof-the-art performance on several benchmarks while re-\nducing computational costs. Our experiments have also\nshown that larger expert pools yield consistent perfor-\nmance improvements. Furthermore, we have explored\nthe impact of MoE on model interpretability and found\nit can improve the interpretability of vision-language\nmodels by providing better insights into how the model\nprocesses different inputs.\nIn conclusion, our findings suggest that MoE is a\nvaluable technique for scaling vision-language models,\nenabling them to handle large-scale, real-world multi-\nmedia data. Our work opens up new research directions\nfor exploring the effectiveness of MoEs in other vision-\nlanguage tasks, such as visual question answering, vi-\nsual reasoning and image-text retrieval, and we hope\nour findings will inspire further investigations into this\nresearch area.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\n11337\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. Flamingo: a visual language model\nfor few-shot learning. In Advances in Neural Infor-\nmation Processing Systems.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, An-\ndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh,\nMikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022.\nFlamingo: a visual language model for few-shot learn-\ning. CoRR, abs/2204.14198.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes-\nsel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, et al.\n2023. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei.\n2022a. BEiT: BERT pre-training of image transform-\ners. In ICLR.\nHangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,\nOwais Khan Mohammed, Kriti Aggarwal, Subho-\njit Som, Songhao Piao, and Furu Wei. 2022b. Vlmo:\nUnified vision-language pre-training with mixture-of-\nmodality-experts. In Advances in Neural Information\nProcessing Systems.\nYoshua Bengio. 2013. Deep learning of representa-\ntions: Looking forward. In Statistical Language and\nSpeech Processing: First International Conference,\nSLSP 2013, Tarragona, Spain, July 29-31, 2013. Pro-\nceedings 1, pages 1–37. Springer.\nKe Chen, Lei Xu, and Huisheng Chi. 1999. Improved\nlearning algorithms for mixture of experts in mul-\nticlass classification. Neural networks, 12(9):1229–\n1252.\nKeqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. 2023. Shikra: Unleashing\nmultimodal llm’s referential dialogue magic. arXiv\npreprint arXiv:2306.15195.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022. Pali: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: universal image-text\nrepresentation learning. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK, Au-\ngust 23-28, 2020, Proceedings, Part XXX, volume\n12375 of Lecture Notes in Computer Science, pages\n104–120. Springer.\nAidan Clark, Diego De Las Casas, Aurelia Guy, Arthur\nMensch, Michela Paganini, Jordan Hoffmann, Bog-\ndan Damoc, Blake Hechtman, Trevor Cai, Sebas-\ntian Borgeaud, et al. 2022. Unified scaling laws for\nrouted language models. In ICML, pages 4057–4086.\nPMLR.\nMark Collier, Efi Kokiopoulou, Andrea Gesmundo,\nand Jesse Berent. 2020. Routing networks with\nco-training for continual learning. arXiv preprint\narXiv:2009.04381.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. arXiv preprint\narXiv:2305.06500.\nAndrew Davis and Itamar Arel. 2013. Low-rank ap-\nproximations for conditional feedforward compu-\ntation in deep neural networks. arXiv preprint\narXiv:1312.4461.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al.\n2020. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. preprint\narXiv:2010.11929.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, et al.\n2022. Glam: Efficient scaling of language models\nwith mixture-of-experts. In ICML, pages 5547–5569.\nPMLR.\nDavid Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever.\n2013. Learning factored representations in a deep\nmixture of experts. arXiv preprint arXiv:1312.4314.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. CoRR,\nabs/2101.03961.\n11338\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu,\nYu Cheng, and Jingjing Liu. 2020. Large-scale adver-\nsarial training for vision-and-language representation\nlearning. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding in\nvisual question answering. In 2017 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2017, Honolulu, HI, USA, July 21-26, 2017, pages\n6325–6334. IEEE Computer Society.\nHussein Hazimeh, Zhe Zhao, Aakanksha Chowdh-\nery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul\nMazumder, Lichan Hong, and Ed H. Chi. 2021.\nDselect-k: Differentiable selection in the mixture\nof experts with applications to multi-task learning.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (GELUs). arXiv preprint\narXiv:1606.08415.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q. Weinberger. 2016. Deep networks with\nstochastic depth. In Computer Vision - ECCV 2016 -\n14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part IV,\nvolume 9908 of Lecture Notes in Computer Science,\npages 646–661. Springer.\nChangho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang,\nZe Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin\nJose, Prabhat Ram, et al. 2022. Tutel: Adap-\ntive mixture-of-experts at scale. arXiv preprint\narXiv:2206.03382.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural computation, 3(1):79–87.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n4904–4916. PMLR.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021a.\nViLT: Vision-and-language transformer without con-\nvolution or region supervision. In Proceedings of the\n38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume\n139 of Proceedings of Machine Learning Research,\npages 5583–5594. PMLR.\nYoung Jin Kim, Ammar Ahmad Awan, Alexandre\nMuzio, Andres Felipe Cruz Salinas, Liyang Lu,\nAmr Hendy, Samyam Rajbhandari, Yuxiong He, and\nHany Hassan Awadalla. 2021b. Scalable and effi-\ncient moe training for multitask multilingual models.\narXiv preprint arXiv:2109.10465.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel\nFried. 2023. Grounding language models to im-\nages for multimodal generation. arXiv preprint\narXiv:2301.13823.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,\nYi Tay, Mostafa Dehghani, and Neil Houlsby. 2022.\nSparse upcycling: Training mixture-of-experts from\ndense checkpoints. arXiv preprint arXiv:2212.05055.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. Int. J.\nComput. Vis., 123(1):32–73.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nSneha Kudugunta, Yanping Huang, Ankur Bapna,\nMaxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-\nong, and Orhan Firat. 2021. Beyond distillation:\nTask-level mixture-of-experts for efficient inference.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 3577–3599.\nHugo Laurenc ¸on, Lucile Saulnier, L´eo Tronchon, Stas\nBekman, Amanpreet Singh, Anton Lozhkov, Thomas\nWang, Siddharth Karamcheti, Alexander M Rush,\nDouwe Kiela, et al. 2023. Obelisc: An open web-\nscale filtered dataset of interleaved image-text docu-\nments. arXiv preprint arXiv:2306.16527.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\n11339\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. arXiv preprint\narXiv:2006.16668.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE lay-\ners: Simplifying training of large, sparse models. In\nICML. PMLR.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023a. Otter: A\nmulti-modal model with in-context instruction tuning.\narXiv preprint arXiv:2305.03726.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023b. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023c. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In ICML, pages 12888–12900.\nPMLR.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021a. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn Advances in neural information processing sys-\ntems, volume 34, pages 9694–9705.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\nCoRR, abs/1908.03557.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2021b.\nUNIMO: towards unified-modal understanding and\ngeneration via cross-modal contrastive learning. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 2592–\n2607. Association for Computational Linguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.\n2020. Oscar: Object-semantics aligned pre-training\nfor vision-language tasks. In Computer Vision -\nECCV 2020 - 16th European Conference, Glasgow,\nUK, August 23-28, 2020, Proceedings, Part XXX, vol-\nume 12375 of Lecture Notes in Computer Science,\npages 121–137. Springer.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou\nMu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and\nYong Jae Lee. 2023d. Gligen: Open-set grounded\ntext-to-image generation. CVPR.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\ncommon objects in context. In Computer Vision -\nECCV 2014 - 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V, volume 8693 of Lecture Notes in Computer\nScience, pages 740–755. Springer.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023a. Visual instruction tuning.\nHaotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jian-\nfeng Gao, Yong Jae Lee, and Chunyuan Li. 2023b.\nLearning customized visual models with retrieval-\naugmented knowledge. CVPR.\nYuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang\nYou. 2021. Cross-token modeling with conditional\ncomputation. arXiv preprint arXiv:2109.02008.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. ViLBERT: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks.\nIn Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pages 13–23.\nJiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan\nHong, and Ed H. Chi. 2018. Modeling task relation-\nships in multi-task learning with multi-gate mixture-\nof-experts. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery &\nData Mining, KDD 2018, London, UK, August 19-23,\n2018. ACM.\nBasil Mustafa, Carlos Riquelme, Joan Puigcerver,\nRodolphe Jenatton, and Neil Houlsby. 2022. Mul-\ntimodal contrastive learning with limoe: the\nlanguage-image mixture of experts. arXiv preprint\narXiv:2206.02770.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Advances in Neural Infor-\nmation Processing Systems 24: 25th Annual Confer-\nence on Neural Information Processing Systems 2011.\nProceedings of a meeting held 12-14 December 2011,\nGranada, Spain, pages 1143–1151.\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and\nFuru Wei. 2022. Beit v2: Masked image model-\ning with vector-quantized visual tokenizers. ArXiv,\nabs/2208.06366.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao,\nShaohan Huang, Shuming Ma, and Furu Wei.\n2023. Kosmos-2: Grounding multimodal large\nlanguage models to the world. arXiv preprint\narXiv:2306.14824.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\n11340\nregion-to-phrase correspondences for richer image-\nto-sentence models. In 2015 IEEE International Con-\nference on Computer Vision, ICCV 2015, Santiago,\nChile, December 7-13, 2015, pages 2641–2649. IEEE\nComputer Society.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 8748–8763.\nPMLR.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimizations\nenable training deep learning models with over 100\nbillion parameters. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, pages 3505–3506.\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa,\nMaxim Neumann, Rodolphe Jenatton, Andr ´e Su-\nsano Pinto, Daniel Keysers, and Neil Houlsby. 2021.\nScaling vision with sparse mixture of experts. Ad-\nvances in Neural Information Processing Systems.\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\nand Jason Weston. 2021. Hash layers for large sparse\nmodels. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C Berg, and Li Fei-Fei. 2015. Imagenet large\nscale visual recognition challenge. IJCV.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-20,\n2018, Volume 1: Long Papers, pages 2556–2565. As-\nsociation for Computational Linguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. In ICLR.\nOpenReview.net.\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne\nLongpre, Jason Wei, Hyung Won Chung, Barret Zoph,\nWilliam Fedus, Xinyun Chen, et al. 2023. Mixture-\nof-experts meets instruction tuning: A winning com-\nbination for large language models. arXiv preprint\narXiv:2305.14705.\nSheng Shen, Chunyuan Li, Xiaowei Hu, Yujia Xie, Jian-\nwei Yang, Pengchuan Zhang, Zhe Gan, Lijuan Wang,\nLu Yuan, Ce Liu, et al. 2022a. K-lite: Learning trans-\nferable visual models with external knowledge. In\nAdvances in Neural Information Processing Systems.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal,\nAnna Rohrbach, Kai-Wei Chang, Zhewei Yao, and\nKurt Keutzer. 2022b. How much can clip benefit\nvision-and-language tasks? In ICLR.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2021. FLA V A: A foun-\ndational language and vision alignment model.CoRR,\nabs/2112.04482.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 6418–6428. Association\nfor Computational Linguistics.\nZhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,\nChunyuan Li, Yikang Shen, Chuang Gan, Liang-\nYan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023.\nAligning large multimodal models with factually aug-\nmented rlhf. arXiv preprint arXiv:2309.14525.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 5099–5110.\nAssociation for Computational Linguistics.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. 2020. Training data-efficient image trans-\nformers & distillation through attention. preprint\narXiv:2012.12877.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,\nBaptiste Rozi `ere, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jin-\ngren Zhou, and Hongxia Yang. 2022a. Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. CoRR,\nabs/2202.03052.\n11341\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022b. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao. 2022c. SimVLM:\nSimple visual language model pretraining with weak\nsupervision. In ICLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 1112–\n1122, New Orleans, Louisiana.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,\nMing Yan, Yiyang Zhou, Junyang Wang, An-\nwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.\nmplug-owl: Modularization empowers large lan-\nguage models with multimodality. arXiv preprint\narXiv:2304.14178.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. CoRR, abs/2205.01917.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen\nLiu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang,\nJianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang,\nMichael Zeng, Luowei Zhou, and Pengchuan Zhang.\n2021. Florence: A new foundation model for com-\nputer vision. CoRR, abs/2111.11432.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. VinVL: Revisiting visual representa-\ntions in vision-language models. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2021, virtual, June 19-25, 2021, pages 5579–5588.\nComputer Vision Foundation / IEEE.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping\nHuang, Vincent Y Zhao, Andrew M Dai, Zhifeng\nChen, Quoc V Le, and James Laudon. 2022. Mixture-\nof-experts with expert choice routing. In Advances in\nNeural Information Processing Systems.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nJinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang,\nHongsheng Li, Xiaogang Wang, and Jifeng Dai.\n2022. Uni-perceiver-moe: Learning sparse gener-\nalist models with conditional moes. arXiv preprint\narXiv:2206.04674.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–27.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du,\nYanping Huang, Jeff Dean, Noam Shazeer, and\nWilliam Fedus. 2022. St-moe: Designing stable and\ntransferable sparse expert models. arXiv preprint\narXiv:2202.08906.\nA Appendix\nA.1 Further Analyses\n“Dropped” Tokens. In MoE training, the issue of\n”Dropped Tokens” is inherited (Lepikhin et al., 2020;\nShazeer et al., 2017; Mustafa et al., 2022; Riquelme\net al., 2021; Zhou et al., 2022) and caused by the lim-\nited capacity of each MoE expert, which can lead to\ninstability. To provide a detailed analysis of this issue,\nwe present Figure 6, which illustrates the distribution\nof dropped tokens in VL-MoEBASE /16E across different\npre-training tasks. The figure shows that MLM and\nMIM tasks exhibit a more balanced distribution of to-\nkens compared to VLM task, which may explain the\nimproved performance of using MoEs in the former\ntwo pre-training tasks, as depicted in Figure 2. Addi-\ntionally, the problem of dropped imag tokens is more\nsevere compared to dropped text tokens, which aligns\nwith the results of different scaling strategies presented\nin Section 5 and the findings in (Mustafa et al., 2022;\nRiquelme et al., 2021).\nPretrain Losses for Different Scaling Strategies.\nWe additionaly report the effect of different scaling strat-\negy in Section 5 for VL-MoESMALL /16E scaling on three\nmask language modeling (MLM), mask image modeling\n(MIM), and masked vision-language modeling (VLM)\npre-training tasks across training steps in Figure 7. The\nresults support our hypothesis that using three distinct\npretraining objectives for each modality and scaling\neach modality leads to improved optimization of both\nthe specific modality pretraining loss and the VLM loss.\nAdditional Results We conduct experiments using\nCOCO captions following (Wang et al., 2022b), where\nVL-MoE achieves 139.2 for CIDEr and 23.1 for SPICE,\nwhich outperforms the BEIT-3 with 137.5 for CIDer\nand 22.7 for SPICE using base-size. We also observe\ninteresting routing specialization when generating the\nfinal word “cake” considering the T-MoE in VL-MoE in\nFigure 3. “NN: lady” and “NN: slicing” route to experts\n1 and 13 respectively. “DT: A, a” both route to expert 1.\n“JJ: hairnet, big” route to expert 7. These routings under-\nscore the inherent nature of expert specialization in the\nVL-MoE model, potentially highlighting its advantages.\n11342\n0 5 10 15 20 25 30\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n% of overall routed token\ndropped text token text token dropped imag token imag token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n% of overall routed token\n(a) MLM T-MoE Layer 2 (b) MLM T-MoE Layer 4 (c) MLM T-MoE Layer 6 (d) MLM T-MoE Layer 8\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n% of overall routed token\n(e) MIM V-MoE Layer 2 (f) MIM V-MoE Layer 4 (g) MIM V-MoE Layer 6 (h) MIM V-MoE Layer 8\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n% of overall routed token\n(i) VLM T-MoE Layer 2 (j) VLM T-MoE Layer 4 (k) VLM T-MoE Layer 6 (l) VLM T-MoE Layer 8\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n% of overall routed token\n0 2 4 6 8 10 12 14 16\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\n% of overall routed token\n(m) VLM V-MoE Layer 2 (n) VLM V-MoE Layer 4 (o) VLM V-MoE Layer 6 (p) VLM V-MoE Layer 8\nFigure 6: “Dropped” Token analyses for VL-MoE LARGE /16E with three mask language modeling (MLM), mask\nimage modeling (MIM), and masked vision-language modeling (VLM) pre-training tasks. Above the dashed line\ndenotes the ratio of tokens that exceed the expert capacity and will be dropped.\n0 50K 100K 150K\nTrain Steps\n1.40\n1.42\n1.44\n1.46\n1.48\n1.50Valid Loss (MIM)\nDensesmall MoEsmall/E16 MoEsmall/E16-text MoEsmall/E16-imag\n0 50K 100K 150K\nTrain Steps\n1.18\n1.20\n1.22\n1.24\n1.26\n1.28\n1.30Valid Loss (Total)\n0 50K 100K 150K\nTrain Steps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Valid Loss (MLM)\n0 50K 100K 150K\nTrain Steps\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900Valid Loss (VLM)\n0 50K 100K 150K\nTrain Steps\n1.40\n1.42\n1.44\n1.46\n1.48\n1.50Valid Loss (MIM)\n(a) Total Validation Loss (b) MLM Validation Loss (c) VLM Validation Loss (d) MIM Validation Loss\nFigure 7: Effect of different scaling strategy in Section 5 for VL-MoE SMALL /16E scaling on three mask language\nmodeling (MLM), mask image modeling (MIM), and masked vision-language modeling (VLM) pre-training tasks\nacross training steps.\nA.2 Hyperparameter\nVisual Question Answering (VQA). We fine-tune\nthe base/large-size models for 10 epochs with 128\nbatch size. The peak learning rate is 3e-5. Following\nVLMO (Bao et al., 2022b), the input image resolution\nis 480 ×480.\n11343\n0 50K 100K\nNumber of Steps\n1.35\n1.40\n1.45\n1.50\n1.55\n1.60\n1.65\n1.70\n1.75Train Loss (Constrast.)\nDensesmall\nVL-MoEsmall/E16\nLIMoEsmall/E16\n(a) Contrastive Train Loss\n0 50K 100K\nNumber of Steps\n1.40\n1.45\n1.50\n1.55\n1.60\n1.65\n1.70\n1.75Valid Loss (Constrast.)\nDensesmall\nVL-MoEsmall/E16\nLIMoEsmall/E16 (b) Contrastive Validation Loss\n0 50K 100K\nNumber of Steps\n35\n40\n45\n50\n55\n60\n65\n70ImageNet 0shot\nDensesmall\nVL-MoEsmall/E16\nLIMoEsmall/E16 (c) ImageNet 0shot Acc\nFigure 8: Comparision of Dense, VL-MoE, and LIM OE on contrastive pre-training task across training steps.\nNatural Language for Visual Reasoning (NLVR2).\nFor results of Table 1, the base/large-size models are\nfine-tuned for 10 epochs with 128 batch size. The peak\nlearning rate of the base-size models is set to 5e-5. The\ninput image resolution is 384 ×384. For ablation ex-\nperiments, we fine-tune the models for 10 epochs with\n128 batch size, and choose learning rates from {5e-5,\n1e-4}. The input image resolution is 224 ×224. All the\nablation results of NLVR2 are averaged over3 runs.\nCOCO. We fine-tune the base/large-size model for20\nepochs with 2048 batch size. The peak learning rate is\n2e-5 and the input image resolution is 384 ×384.\nFlickr30K. For results of Table 1, the base/large-size\nmodels are fine-tuned for 40 epochs with a batch size\nof 2048 and a peak learning rate of 1e-5. We use the\nfine-tuned model on COCO as the initialization. The\ninput image resolution is 384 ×384. For all ablation\nexperiments, we fine-tune the models for10 epochs with\n1024 batch size. The peak learning rate is set to 5e-5,\nand the input image resolution is 224 ×224.\nImageNet-1k. We fine-tune the base-size VL-MoE\nwith V-MoE and V-FFN only for15 epochs with 2048\nbatch size. The peak learning rate is 3e-5 and the input\nimage resolution is 384 ×384.\nMNLI. We fine-tune the base-size VL-MoE with T-\nMoE and T-FFN only for 10 epochs with 32 batch size.\nThe peak learning rate is 3e-5.\nA.3 Formula of Auxiliary Loss\nGiven a token x ∈ RD, we denote by g(x) =\nsoftmax(Wx) ∈RE the gating weights across theE\nexperts, with W ∈RE×D being the routing parameters.\nWhen we deal with a batch of multiple tokens {xi}n\ni=1,\nwe use the notation X ∈Rn×D.\nImportance loss. We follow the definition\nfrom (Riquelme et al., 2021; Mustafa et al., 2022). The\nimportance loss Ωimp ensures that the gating weights\nare evenly distributed among the experts, maintaining\na balanced profile. For any expert e∈{1,...,E }, we\nhave\nimpe(X) =\n∑\nx∈X\ng(x)e\nand the loss Ωimp is defined via the squared coefficient\nof variation for imp(X) = {impe(X)}E\ne=1\nΩimp(X) =\n(std(imp(X))\nmean(imp(X))\n)2\n.\nLoad loss. Like previously, we follow (Riquelme\net al., 2021). We assume the gating weights gnoisy(x)\nare obtained by perturbing the routing function with\nnoise, i.e., gnoisy(x) = softmax(Wx + ε) with\nε ∼N (0,σ2I) and σ = 1 /E. We denote ηk the k-\nth largest entry of Wx + ε. The importance loss Ωimp\naims to balance the selection probability of experts by\nfocusing on the likelihood of choosing them, as assign-\ning tasks to experts is a discrete process. The load loss\nΩload complements this by striving to even out the num-\nber of assignments among the experts. To calculate the\nselection probability, the expert e ∈{1,...,E }is as-\nsumed to be among the top- k even when resampling\nonly the noise as\npe(x) = 1 −Φ\n(ηk −(Wx)e\nσ\n)\nwith Φ the cumulative distribution function of a Gaus-\nsian distribution. The load loss Ωload is eventually de-\nfined by\nΩload(X) =\n(std(load(X))\nmean(load(X))\n)2\nwhere load(X) = {loade(X)}E\ne=1 ,\nloade(X) =\n∑\nx∈X\npe(x).\nZ-loss. The z-loss Ωzloss introduced in (Zoph et al.,\n2022) aims at controlling the maximum magnitude of\nthe router activations A = {Wxi}n\ni=1 ∈Rn×E with\nentries ai,e = (Wxi)e. The loss is defined by\nΩzloss(X) = 1\nn\nn∑\ni=1\n(\nlog\n(E∑\ne=1\nexp (ai,e)\n))2\n.\nv-loss. The notation “v-loss” we used in Section 5 is\nessentially the final employed loss inV-MOE (Riquelme\net al., 2021), where Ωvloss(X) = 0.5 ∗Ωimp(X) + 0.5 ∗\nΩload(X).\n11344",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9012835025787354
    },
    {
      "name": "Computer science",
      "score": 0.8139207363128662
    },
    {
      "name": "Language model",
      "score": 0.5827681422233582
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5715681314468384
    },
    {
      "name": "Task (project management)",
      "score": 0.5660648941993713
    },
    {
      "name": "Machine learning",
      "score": 0.5461385250091553
    },
    {
      "name": "Scaling",
      "score": 0.5401415228843689
    },
    {
      "name": "Field (mathematics)",
      "score": 0.47094643115997314
    },
    {
      "name": "Computational model",
      "score": 0.44500306248664856
    },
    {
      "name": "Scale (ratio)",
      "score": 0.44285061955451965
    },
    {
      "name": "Data science",
      "score": 0.32462477684020996
    },
    {
      "name": "Engineering",
      "score": 0.09043446183204651
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210087053",
      "name": "Microsoft (Germany)",
      "country": "DE"
    }
  ],
  "cited_by": 32
}