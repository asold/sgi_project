{
  "title": "Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation",
  "url": "https://openalex.org/W3205190889",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5029830288",
      "name": "Yao Qin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016054994",
      "name": "Chiyuan Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017578300",
      "name": "Ting Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012859570",
      "name": "Balaji Lakshminarayanan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080988309",
      "name": "Alex Beutel",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024842018",
      "name": "Xuezhi Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3122750163",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3163203812",
    "https://openalex.org/W3104962541",
    "https://openalex.org/W3175937066",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3134374554",
    "https://openalex.org/W2791953061",
    "https://openalex.org/W2949194345",
    "https://openalex.org/W3128637142",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2994931756",
    "https://openalex.org/W2902617128",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3108655343",
    "https://openalex.org/W2910992787",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W3202202633",
    "https://openalex.org/W2962729158",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2961301154",
    "https://openalex.org/W2097117768"
  ],
  "abstract": "We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i.e., they process an image as a sequence of image patches. We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans. This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans. Further investigations show that these features are useful but non-robust, as ViTs trained on them can achieve high in-distribution accuracy, but break down under distribution shifts. From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance? We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. This is a complementary view to existing research that mostly focuses on augmenting inputs with semantic-preserving transformations to enforce models' invariance. We show that patch-based negative augmentation consistently improves robustness of ViTs across a wide set of ImageNet based robustness benchmarks. Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation, and together boost the performance further.",
  "full_text": "Understanding and Improving Robustness of Vision\nTransformers through Patch-based Negative\nAugmentation\nYao Qin Chiyuan Zhang Ting Chen\nBalaji Lakshminarayanan Alex Beutel Xuezhi Wang\nGoogle Research\nAbstract\nWe investigate the robustness of vision transformers (ViTs) through the lens of\ntheir special patch-based architectural structure, i.e., they process an image as a\nsequence of image patches. We ﬁnd that ViTs are surprisingly insensitive to patch-\nbased transformations, even when the transformation largely destroys the original\nsemantics and makes the image unrecognizable by humans. This indicates that\nViTs heavily use features that survived such transformations but are generally\nnot indicative of the semantic class to humans. Further investigations show that\nthese features are useful but non-robust, as ViTs trained on them can achieve high\nin-distribution accuracy, but break down under distribution shifts. From this un-\nderstanding, we ask: can training the model to rely less on these features improve\nViT robustness and out-of-distribution performance? We use the images trans-\nformed with our patch-based operations as negatively augmented views and offer\nlosses to regularize the training away from using non-robust features. This is a\ncomplementary view to existing research that mostly focuses on augmenting in-\nputs with semantic-preserving transformations to enforce models’ invariance. We\nshow that patch-based negative augmentation consistently improves robustness\nof ViTs on ImageNet based robustness benchmarks across 20+ different experi-\nmental settings. Furthermore, we ﬁnd our patch-based negative augmentation are\ncomplementary to traditional (positive) data augmentation techniques and batch-\nbased negative examples in contrastive learning.\n1 Introduction\nBuilding vision models that are robust, i.e., that maintain accuracy even on unexpected and out-of-\ndistribution images, is increasingly a requirement to trusting vision models and a strong benchmark\nfor progress in the ﬁeld. Recently, Vision Transformers (ViTs, Dosovitskiy et al. (2021)) sparked\ngreat interest in the literature, as a radically new model architecture offering signiﬁcant accuracy\nimprovements and with hope of new robustness beneﬁts. Over the past decade, there has been\nextensive work on understanding the robustness of convolution-based neural architectures, as the\ndominant design for visual tasks; researchers have explored adversarial robustness (Szegedy et al.,\n2013), domain generalization (Xiao et al., 2021; Khani & Liang, 2021), feature biases (Brendel &\nBethge, 2019; Geirhos et al., 2018; Hermann et al., 2020). As a result, with the new promise of vision\ntransformers, it is critical to understand their properties and in particular their robustness. Recent\nearly studies (Naseer et al., 2021; Paul & Chen, 2021; Bhojanapalli et al., 2021) have found ViTs be\nmore robust than ConvNets in some scenarios, with the hypothesis that the non-local attention based\ninteractions enabled ViTs to capture more global and semantic features. In contrast, we add to this\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2110.07858v2  [cs.LG]  22 Feb 2023\nFigure 1: Patch-based transformations largely destroy images to be unrecognizable to humans\nwhereas ViT recognizes them as the original class (e.g., keeshond or magpie) with high conﬁdence.\nVisualization of patch-based transformations. On the top of each image, we display the predicted\nconﬁdence score of ViT-B/16 pretrained on ImageNet-21k and ﬁnetuned on ImageNet-1k.\nline of research showing a different side of the challenge: we ﬁnd that ViTs are still vulnerable to\nrelying on non-robust features, and propose an algorithm to mitigate it for better out-of-distribution\nperformance.\nSpeciﬁcally, we ﬁrst conduct a study on the robustness properties of ViTs and show that ViTs still\nrely on speciﬁc non-robust features. We start with the architectural traits of ViTs – ViTs operate\non non-overlapping image patches and allow long range interaction between patches even in lower\nlayers. It is hypothesized in recent studies (Naseer et al., 2021; Paul & Chen, 2021; Bhojanapalli\net al., 2021) that the non-local attention based interactions contribute to better robustness of ViTs\nthan ConvNets. To study the ability of ViTs to integrate global semantics structures across patches,\nwe design and apply patch-based image transformations, such as random patch rotation, shufﬂing,\nand background-inﬁlling (Figure 1). Those transformations destroy the spatial relationship between\npatches and corrupted the global semantics, and the resultant images are often visually unrecog-\nnizable. However, we ﬁnd that ViTs are surprisingly insensitive to these transformations and can\nmake highly accurate predictions on these transformed images. This suggests that ViTs use features\nthat survive such transformations but are generally not indicative of the semantic class to humans.\nGoing one step further, we ﬁnd that those features are useful but not robust, as ViTs trained on them\nachieved high in-distribution accuracy, but suffered signiﬁcantly on robustness benchmarks.\nWith this understanding of ViTs’ reliance on non-robust features captured by patch-based transfor-\nmations, we aim to mitigate this shortcoming by answering the following questions: (a) how can we\ntrain ViTs to not rely on such features? and (b) will reducing reliance on such features meaningfully\nimprove out-of-distribution performance and not sacriﬁce in-distribution accuracy? A majority of\npast robust training algorithms encourage the smoothness of model predictions on augmented im-\nages with semantic preserving transformations (Hendrycks et al., 2020; Cubuk et al., 2019). How-\never, the patch-based transformations deliberately destroy the semantic meaning and only leave non-\nrobust features. Taking inspiration from recent research on generative modeling (Sinha et al., 2020),\nwe propose a family of robust training algorithms based onpatch-based negative augmentationsthat\nregularize the training from relying on non-robust features surviving patch-based transformations.\nThrough extensive evaluation on a wide set of ImageNet-based benchmarks, we ﬁnd that our meth-\nods consistently improve the robustness of the trained ViTs. Furthermore, our patch-based negative\naugmentation can be combined with the traditional (positive) data augmentation to boost the per-\nformance further. With this we get a more complete picture: training models both to be insensitive\nto spurious changes (as in positive augmentation) but also to not rely on non-robust features (as in\nnegative augmentation) together can meaningfully improve robustness of ViTs.\nOur key contributions are as follows:\n• Understanding Non-Robust Features in ViT: We show that ViTs heavily rely on non-robust\nfeatures surviving patch-based transformations but are not indicative of the semantic classes to\nhumans.\n2\nFigure 2: ViTs can rely on features surviving patch-based transformations to maintain a high accu-\nracy, even after images have been heavily transformed to be largely unrecognizable. Top-1 accuracy\nof ViT models when tested on patch-based transformed images using the semantic class of the cor-\nresponding clean image as ground-truth. The test accuracy on ImageNet-1k validation set is shown\non the right. All ViT models are pre-trained on ImageNet-21k and ﬁne-tuned on ImageNet-1k.\n• Modeling: We propose a set of patch-based operations as negatively augmented views, com-\nplementary to existing works that focus on semantic-preserving (“positive”) augmentations, to\nregularize the training away from using these speciﬁc non-robust features;\n• Improved Robustness of ViT: We show across 20+ experimental settings that our proposed\npatch-based negative augmented views can consistently improve ViT’s robustness and com-\nplementary to “positive” augmentation techniques as well as batch-based negative examples in\ncontrastive learning.\n2 Preliminaries\nVision Transformers Vision transformers (Dosovitskiy et al., 2021) are a family of architec-\ntures adapted from Transformers in natural language processing (Vaswani et al., 2017), which di-\nrectly process visual tokens constructed from image patch embeddings. To construct visual tokens,\nViT (Dosovitskiy et al., 2021) ﬁrst splits an image into a grid of patches. Each patch is linearly pro-\njected into a hidden representation vector, and combined with a positional embedding. A learnable\nclass token is also added. Transformers (Vaswani et al., 2017) are then directly applied on this set\nof visual and class token embeddings as if they are word embeddings in the original Transformer\nformulation. Finally, a linear projection of class token is used to calculate the class probability.\nModel Variants We consider ViT models pretrained on either ILSVRC-2012 ImageNet-1k, with\n∼1.3 million images or ImageNet-21k, with ∼14 million images (Russakovsky et al., 2015). All\nmodels are ﬁne-tuned on ImageNet-1k dataset. We adopt the notations used in (Dosovitskiy et al.,\n2021) to denote model size and input patch size. For example, ViT-B/16 denotes the “Base” model\nvariant with input patch size 16 ×16.\nRobustness Benchmarks To evaluate models’ robustness, we mainly focus on three ImageNet-\nbased robustness benchmarks, ImageNet-A (Hendrycks et al., 2019), ImageNet-C (Hendrycks &\nDietterich, 2019a), ImageNet-R (Hendrycks et al., 2021). Speciﬁcally, ImageNet-A contains chal-\nlenging natural images from a distribution unlike ImageNet training distribution, which can easily\nfool models to make a misclassiﬁcation. ImageNet-C consists of 19 types of corruptions that are\nfrequently encountered in natural images and each corruption has 5 levels of severity. It is widely\nused to measure models’ robustness under distributional shift. ImageNet-R is composed of images\nobtained by artistic rendition of ImageNet classes, e.g., cartoons, and is widely used to evaluate\nmodel’s robustness on out-of-distribution data.\n3 Understanding the Robustness of Vision Transformers\nRecent works (Naseer et al., 2021; Paul & Chen, 2021; Bhojanapalli et al., 2021) have shown that\nvision transformers achieve better robustness compared to standard convolutional networks. One\nexplanation is that the attention mechanism can capture better global structures. To investigate if\nViT has successfully taken advantage of the long range interactions between patches, we design a\nseries of patch-based transformations which signiﬁcantly destroys the global structure of images.\nThe patch-based transformations (see Fig. 1) are:\n3\nFigure 3: Features preserved in patch-based transformations are useful but non-robust as training\nViT on them impedes robustness. Top-1 Accuracy ( %) on ImageNet-1k validation set and Ima-\ngeNet robustness datasets: ImageNet-A, ImageNet-C, ImageNet-R. The baseline model is ViT-B/16\nin (Dosovitskiy et al., 2021) trained on original images. Other models are trained on patch-based\ntransformed images, e.g., “P-Shufﬂe” stands for a ViT-B/16 model trained on patch-based shufﬂed\nimages. Numbers above the bars are either accuracy (e.g., ViT-B/16) or themax accuracy difference\nbetween each model family and the baseline ViT-B/16. The patch size in P-Shufﬂe and P-Rotate and\nreplacement ratio in P-Inﬁll is denoted by “ps” and “rr” respectively.\n• Patch-based Shufﬂe (P-Shufﬂe): we randomly shufﬂe the input image patches to change their\npositions1.\n• Patch-based Rotate (P-Rotate) : we randomly select a rotation degree from the set Ω =\n{0◦,90◦,180◦,270◦}and rotate each image patch independently.\n• Patch-based Inﬁll (P-Inﬁll) : we replace the image patches in the center region of an image\nwith the patches on the image boundary2.\nEach patch-based transformation is performed to a single image. We make sure the patch size of our\npatch-based transformation is a multiple of the input image patch of ViT so that the content within\neach patch is well-maintained. For P-inﬁll, we use “replace rate” to denote the ratio of replaced\npatches in the center over the total number of patches in an image. Examples of transformed images\nare shown in Fig. 1 (see Appendix E for more examples). In most cases, it is challenging to recognize\nthe semantic classes after those transformations.\nDo ViTs rely on features not indicative of the semantic classes to humans?To validate if ViTs be-\nhave similarly as humans on these patch-based transformed images, weevaluate ViT models (Doso-\nvitskiy et al., 2021) on these patch-based transformed image. Speciﬁcally, we apply each patch-\nbased transformation to ImageNet-1k validation set and report the test accuracy of each ViT on the\ntransformed images. The test accuracy is computed by using the semantic class of the correspond-\ning original image as the ground-truth. As shown in Figure 2, the accuracies achieved by ViTs are\nsigniﬁcantly higher than random guessing (0.1%). In addition, as shown in Figure 1, ViT gives these\npatch-based transformed images a highly conﬁdent prediction even when the transformation largely\ndestroys the semantics and make the image unrecognizable by humans. 3 This strongly indicates\nthat ViT models heavily rely on the features that survive these transformations to make a prediction.\nHowever, these features are not indicative of the semantic class to humans.\nDo features preserved in patch-based transformations impede robustness?Taking one step further,\nwe want to know if the features preserved by simple patch-based transformations, which are not\nindicative of the semantic class to humans, result in robustness issues. To this end, we train a\nvision transformer, e.g., ViT-B/16, on patch-based transformed images with original semantic class\nassigned as their ground-truth. Note that all the training images are patch-based transformed images.\nIn this way, we force the model to fully exploit the features preserved in patch-based transformations.\nIn addition, we reuse all the training details and hyperparameters in (Dosovitskiy et al., 2021) to\nmake sure the “only” difference between our models and the baseline ViT-B/16 (Dosovitskiy et al.,\n2021) is the training images. Then, we test the model on ImageNet-1k validation set and three\nrobustness benchmarks, ImageNet-A, ImageNet-C and ImageNet-R without any transformation.\n1P-Shufﬂe is equivalent to shufﬂing the position embeddings.\n2For example, given an image with size 384 × 384, input patch size is 16 × 16 and replace rate 0.25, we in\ntotal have 576 patches xi,j, where iand jdenotes the row and column index and 1 ≤ i,j ≤ 24. The patches\nin the center xm,n,7 ≤ m,n ≤ 18 are replaced by the remaining patches.\n3Similar patterns are observed when ViT models are pretrained on ImageNet-1k.\n4\nFirst, we can observe that for the baseline ViT-B/16, compared to in-distribution accuracy, all the\nout-of-distribution accuracies have suffered from a signiﬁcant drop (the 4 blue bars in Figure 3).\nThis trend has been observed both for ViTs and convolution-based networks (Paul & Chen, 2021;\nZhai et al., 2021). Second, if we compare the accuracy between the baseline model and models\ntrained on patch-based transformations (i.e., the difference between the blue bar and one of the\nred/green/orange bars in Figure 3), we ﬁnd that ViTs’ in-distribution accuracy drops only slightly,\nbut the robustness drop is signiﬁcant when models are trained on these patch-based transformations.\nTake P-Shufﬂe as an example, the model trained on patch-based shufﬂed images can still achieve\n79.1% accuracy on ImageNet-1k, only 5 percentage point (pp) drop in in-distribution accuracy. In\ncontrast, the accuracy drop on robustness datasets is much more signiﬁcant, e.g., 17pp on ImageNet-\nR. The deterioration rate in robustness is close to 50 % of the baseline ViT-B/16. This strongly\nsuggests that the features preserved in patch-based transformations are sufﬁcient for high-accurate\nin-distribution prediction but are not robust under distributional shifts.\nTaking the above results together, we conclude that even though ViTs are shown to be more robust\nthan ConvNets in previous studies (Naseer et al., 2021; Paul & Chen, 2021), they still heavily rely\non features that are not indicative of the semantic classes to humans. These features, captured by\npatch-based transformations, are useful but non-robust, as ViTs trained on them achieve high in-\ndistribution accuracy but suffer signiﬁcantly on robustness benchmarks.\nWith this knowledge we now ask: based on these understandings, can we train ViTs to not rely on\nsuch non-robust features? And if we do, will it improve their robustness?\n4 Improving the Robustness of Vision Transformers\nBased on the key observations that the patch-based transformations encode features that contribute\nto the non-robustness of ViTs, we propose a negative augmentation procedure to regularize ViTs\nfrom relying on such features. To this end, we use the images transformed with our patch-based\noperations as negatively augmented views. Then we design negative loss regularizations to prevent\nthe model from using those non-robust features preserved in patch-based transformations.\nSpeciﬁcally, given a clean image x, we generate its negative view, denoted as ˜x, by ap-\nplying a patch-based transformation to x. We call it negative augmentation, in contrast\nwith the standard (positive) augmentation that are semantic preserving. Let Lce(B; θ) =\n−1\n|B|\n∑\n(x,y)∈B ylog softmax(f(x; θ)) represent the cross-entropy loss function used to train a vi-\nsion transformer with parameters θ, where B is a minibatch of clean examples, and ydenotes the\nground-truth label. The loss on negative views Lneg(B,˜B; θ) can be easily added to the cross-\nentropy loss Lce(B; θ) via\nLce(B; θ) + λ·Lneg(B,˜B; θ), (1)\nwhere λis a coefﬁcient balancing the importance between clean training data as well as patch-based\nnegative augmentation. Below, we introduce uniform loss and ℓ2 loss to leverage negative views.\nUniform loss Many existing data augmentation techniques (Cubuk et al., 2019, 2020; Hendrycks\net al., 2020) use one-hot labels for semantic-preserving augmented data to enforce the invariance\nof the model prediction. In contrast, the semantic classes of our generated patch-based negative\naugmented data are visually unrecognizable, as shown in Figure 1. Therefore, we propose to use\nuniform labels instead for those negative augmentations. Speciﬁcally, the loss function on negative\nviews that we optimize at each training step can be formulated as:\nLneg(B,˜B; θ) = −1\n|˜B|\n∑\n(˜x,˜y)∈˜B\n˜ylog softmax(f(˜x; θ)), (2)\nwhere ˜ydenotes the uniform distribution: ˜yk = 1\nK where K is the total number of classes. f(x; θ)\ndenotes the function mapping the input image into the logit space.\nℓ2 Loss An alternative to pre-assuming labels for negative augmentation is to add the constraints\non the logit space (or the space of predicted probability). Inspired by existing work (Kannan et al.,\n2018; Zhang et al., 2019; Hendrycks et al., 2020) which provides an extra regularization term en-\ncouraging similar logits between clean and “positive” augmented counterparts, we instead encourage\nthe logits of clean examples and their corresponding negative augmentations to be far away. In this\nway, we prevent the model from relying on the non-robust features preserved in negative views.\n5\nTable 1: Top-1 accuracies for ViT models pre-trained and ﬁne-tuned on ImageNet-1k with or without\nthe proposed negative augmentation. Best results highlighted in bold.\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nViT-B/32 (Dosovitskiy et al., 2021) 72.5 3.7 46.7 19.4\n+ P-Rotate / Uniform 72.9 (+0.4) 3.7 (+0.0) 47.9 (+1.2) 19.9 (+0.5)\n+ P-Rotate / L2 73.2 (+0.7) 3.8 (+0.1) 48.4 (+1.7) 20.4 (+1.0)\nViT-B/16 (Dosovitskiy et al., 2021) 77.6 6.7 50.8 20.3\n+ P-Rotate / Uniform 78.2 (+0.6) 7.0 (+0.3) 52.4 (+1.6) 21.4 (+1.1)\n+ P-Rotate / L2 77.8 (+0.2) 6.7 (+0.0) 51.6 (+0.8) 21.0 (+0.7)\nSpeciﬁcally, we maximize the ℓ2 distance between the predicted probability of clean examples and\ntheir corresponding negative views. The loss on negative views, therefore, can be formulated as:\nLneg(B,˜B; θ) = −1\n|˜B|\n∑\nx∈B,˜x∈˜B\n∥softmax(f(x; θ)) −softmax(f(˜x; θ))∥2. (3)\nHere the ℓ2 distance is computed over the predicted probability rather than the logitsf(x; θ) because\nempirically we observe that maximizing the difference of logits can cause numerical instability.\n5 Experiments\nExperimental setup We follow Dosovitskiy et al. (2021) to ﬁrst pre-train all the models with image\nsize 224 ×224 and then ﬁne-tune the models with a higher resolution 384 ×384. We reuse all their\ntraining hyper-parameters, including batch size, weight decay, and training epochs (see Appendix B\nfor details). For the extra loss coefﬁcient λ in Eqn. 1 we sweep it from the set {0.5,1,1.5}and\nchoose the model with the best hold-out validation performance. Please refer to Appendix C for the\nchosen hyperparameters for each model. We make sure our proposed models and our implemented\nbaselines are trained with exactly the same settings for fair comparison.\n5.1 Effective in improving robustness of vanilla ViTs\nFirst, we apply our proposed patch-based transformations to a ViT model pre-trained and ﬁne-tuned\non ImageNet-1k. The extra loss regularization on negative views is used in both pre-training and\nﬁne-tuning stages to prevent the model from learning non-robust features preserved in patch-based\ntransformations. We use “Transformation / Regularization” to denote a pair of patch-based negative\naugmentation and loss regularization. For examples, “P-Rotate / Uniform” means that we use P-\nRotate to generate the negative views and use uniform loss to regularize the training. We display the\nresults in Table 1, where we can clearly see that our proposed patch-based negative augmentation\neffectively improves the out-of-distribution robustness on ImageNet-C and ImageNet-R. Note the\nimprovement is small over ImageNet-A when the baseline almost does not work, but it becomes\nmore siginiﬁcant when the baseline improves (e.g., Table 5).\nLarger improvement on small-scale datasets We also apply our patch-based negative augmen-\ntation to ViT-B/16 on CIFAR-100 (Krizhevsky, 2009) in the ﬁne-tuning stage and ﬁnd that it can\nalso signiﬁcantly improve the robustness on the corrupted dataset CIFAR-100-C (Hendrycks & Di-\netterich, 2019b), which includes 19 different corruption types with 5 different corruption severities.\nFor example, P-Shufﬂe / Uniform can boost the in-distribution accuracy of ViT-B/16 on CIFAR-100\nfrom 91.8% to 92.6% (+0.8) and the robustness accuracy on CIFAR-100-C can also be improved\nfrom 74.6% to 77.0% (+2.4), as shown in Table 10 in Appendix.\n5.2 Complementary to traditional (“positive”) data augmentation\nTo investigate if our proposed patch-based negative augmentation is complementary to traditional\n(“positive”) data augmentation, we apply our patch-based negative transformation on top of tra-\nditional data augmentation: Rand-Augment (Cubuk et al., 2020), which is widely used in vision\ntransformers (Touvron et al., 2021; Mao et al., 2021), and AugMix (Hendrycks et al., 2020), which\nis speciﬁcally proposed to improve models’ robustness under distributional shift. Crucially, we fol-\nlow (Hendrycks et al., 2020) to exclude transformations used in “positive” data augmentation which\noverlap with corruption types in ImageNet-C (Hendrycks & Dietterich, 2019a). Therefore, the set of\ntransformations used in Rand-Augment and AugMix is disjoint with the corruptions in ImageNet-C.\nWhen we combine “negative” and “positive” augmentation, the cross-entropy loss Lce(B+; θ) in\nEqn. 1 is computed over “positive” examples B+ = {x+\n1 ,··· ,x+\nN }using either Rand-Augment or\n6\nTable 2: Patch-based negative augmentation is complementary to “positive” data augmenta-\ntion. Top-1 accuracies for ViT-B/16 pre-trained and ﬁne-tuned on ImageNet-1k using Rand-\nAugment (Cubuk et al., 2020) or AugMix (Hendrycks et al., 2020). The proposed negative aug-\nmentation is added on top of either positive augmentation. Best results are highlighted in bold.\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment (Cubuk et al., 2020) 79.1 7.2 55.2 23.0\n+ P-Shufﬂe / Uniform 79.3 7.7 (+0.5) 56.2 (+1.0) 23.4 (+0.4)\n+ P-Rotate / Uniform 79.3 8.1 (+0.9) 56.4 (+0.8) 23.8 (+0.8)\n+ P-Inﬁll / Uniform 79.2 7.8 (+0.6) 56.4 (+1.2) 24.0 (+1.0)\n+ P-Shufﬂe / L2 78.9 7.5 (+0.3) 55.6 (+0.4) 22.6 (-0.4)\n+ P-Rotate / L2 79.1 7.9 (+0.7) 56.7 (+1.5) 23.8 (+0.8)\n+ P-Inﬁll / L2 78.8 7.4 (+0.2) 55.4 (+0.2) 23.2 (+0.2)\nAugMix (Hendrycks et al., 2020) 78.8 7.7 57.8 24.9\n+ P-Shufﬂe / Uniform 79.2 8.0 (+0.3) 58.6 (+0.8) 25.7 (+0.8)\n+ P-Rotate / Uniform 79.1 8.2 (+0.5) 58.5 (+0.7) 25.7 (+0.8)\n+ P-Inﬁll / Uniform 79.3 8.3 (+0.6) 58.4 (+0.6) 25.7 (+0.8)\n+ P-Shufﬂe / L2 78.8 7.9 (+0.2) 58.3 (+0.5) 25.7 (+0.8)\n+ P-Rotate / L2 79.0 8.3 (+0.6) 58.8 (+1.0) 26.0 (+1.1)\n+ P-Inﬁll / L2 79.0 7.9 (+0.2) 58.5 (+0.7) 25.6 (+0.7)\nTable 3: Effects of patch-based negative augmentation on ﬁve different levels of corruption severities\non ImageNet-C. Best results are highlighted in bold. See Table 12 in Appendix for full results.\nModel ImageNet-C Corruption Severity Level\n1 2 3 4 5\nRand-Augment (Cubuk et al., 2020) 70.4 63.7 57.9 48.2 36.1\n+ P-Rotate / Uniform 71.1 (+0.7) 64.6 (+0.9) 59.0 (+1.1) 50.0 (+1.8) 37.6 (+1.5)\n+ P-Rotate / L2 71.1 (+0.7) 64.8 (+1.1) 59.5 (+1.6) 50.1 (+1.9) 37.8 (+1.7)\nAugMix (Hendrycks et al., 2020) 71.4 65.2 60.5 51.9 40.2\n+ P-Rotate / Uniform 71.7 (+0.3) 65.7 (+0.5) 61.1 (+0.6) 52.7 (+0.8) 41.4 (+1.2)\n+ P-Rotate / L2 71.9 (+0.5) 66.0 (+0.8) 61.5 (+1.0) 52.9 (+1.0) 41.6 (+1.4)\nAugMix. Meanwhile, the loss regularization on negative views in Eqn. 1 is computed over neg-\natively transformed version of x+. That is: for ∀x+ ∈B+, we apply our patch-based negative\ntransformation to obtain its negative version and then use the negative example to compute the loss\nregularization Lneg. The positive data augmentation is only used in pre-training stage as we ob-\nserve it is slightly better than using them for both stages (Please see more detailed discussion in\nAppendix D.4) and Steiner et al. (2021) have the similar observation. Instead, we apply our negative\naugmentation in both stages, as it is the best design choice as discussed in Appendix D.3.\nAs shown in Table 2, we see that when our patch-based negative augmentations are applied to\neither Rand-Augment or AugMix, we can consistently improve the robustness of vision transformers\nacross all three robustness benchmarks. This is particularly noteworthy as both Rand-Agument and\nAugMix are already designed to signiﬁcantly improve the robustness of vision models. Yet, we\nsee that patch-based negative augmentation provides further robustness beneﬁts. This suggests that\nrobustness of vision models was not adequately addressed by “positive” data augmentation and that\npatch-based negative augmentation is complementary to these traditional approaches.\nTo have a clear picture how negative augmentation improves robustness on data with different de-\ngrees of corruptions, we also display the top-1 accuracy on ImageNet-C with ﬁve different levels\nof corruption severity in Table 3. The general trend emerges: as the corruption level goes higher,\nthe beneﬁt of negative examples becomes larger. This suggests our proposed negative examples are\nespecially useful when data is highly corrupted.\n5.3 Complementary to batch-based negative examples in contrastive loss\nIn addition, we also investigate if our proposed patch-based negative augmentation can be incor-\nporated into the traditional contrastive loss formulation and if they are complementary and provide\nadditional value on top of the batch-based negative examples traditionally used (Oord et al., 2018;\nChen et al., 2020a). We will ﬁrst describe how to use contrastive loss as a regularization term and\nthen introduce how to integrate patch-based negative augmentation into contrastive loss.\n7\nTable 4: Complementary to batch-based negative examples in contrastive loss. Top-1 accuracies of\nViT-B/16 pretrained and ﬁne-tuned on ImageNet-1k with and without P-Rotate.\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nViT-B/16 + Contrastive* 78.7 8.1 53.5 22.8\nViT-B/16 + P-Rotate / Contrastive 78.9 8.6 (+0.5) 54.1 (+0.6) 23.6 (+0.8)\nRand-Augment + Contrastive* 79.7 8.9 57.6 24.7\nRand-Augment + P-Rotate / Contrastive 79.9 9.4 (+0.5) 58.4 (+0.8) 25.4 (+0.7)\nTable 5: Patch-based negative augmentation is helpful even with large-scale pretraining. Top-1\naccuracies of ViT-B/16 pretrained on ImageNet-21k and ﬁnetuned on ImageNet-1k. Mean±standard\ndeviation are reported over 4 independent runs.\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment (Cubuk et al., 2020) 84.4 ±0.0 28.7±0.2 67.2 ±0.0 38.7 ±0.1\n+ P-Shufﬂe / Uniform 84.5 29.9 67.7 38.9\n+ P-Shufﬂe / L2 84.5 ±0.0 29.7±0.3 68.0±0.0 39.6 ±0.2\nFollowing supervised contrastive loss proposed in (Khosla et al., 2020), for an example xi ∈B, we\ncreate a positive set Pi ≡{xj ∈B\\{xi}|yj = yi}with all the examples in the minibatchB sharing\nthe same class as xi. The anchor xi is excluded from its positive set Pi. Next, all the examples in\nthe minibatch B with a different class as xi are used as negative examples. Let the candidate set\nQi ≡B\\{xi}, the contrastive loss can be expressed as\nLcont(B; θ) = −1\n|B|\n∑\nxi∈B\n1\n|Pi|\n∑\nxj ∈Pi\nlog exp(sim(xi,xj)/τ)∑\nxk∈Qi exp(sim(xi,xk)/τ), (4)\nwhere τ is the temperature and sim (xi,xj) = g(xi;θ)⊺·g(xj ;θ)\n∥g(xi;θ)∥∥g(xj ;θ)∥ computes the cosine similarity\nbetween g(xi; θ) and g(xj; θ), and g(x; θ) denotes the representation learned by the penultimate\nlayer of the classiﬁer. We do not use a learnable projection head as in contrastive representation\nlearning (Chen et al., 2020a,b) to avoid extra network parameters. The contrastive lossLcont is used\nas the regularization term Lneg in Eqn 1. We denote this stronger baseline as “Contrastive*”.\nTo integrate our proposed patch-based negative examples into contrastive loss, we expand the neg-\native set in contrastive loss, which now composed of two types of negative examples: 1) all the\nexamples in the minibatch B with a different class as xi, 2) the patch-based negatively transformed\nimages ˜x∈˜B. Therefore, for each anchor xi, we can in total have 2|B|−|Pi|−1 negative pairs,\nwhere |B|is the batch size and |Pi|is the cardinality of the positive set Pi. Accordingly, the candi-\ndate set in Eqn 4 becomes Qi ≡˜B ∪B\\{xi}. In Table 4, we can see that even if we add the patch-\nbased negative augmentation on top of this stronger contrastive baseline, we can still consistently\nachieve extra improvement across robustness benchmarks. This shows our proposed patch-based\nnegative augmentation is also complementary to batch-based negative examples in contrastive loss\nin improving models’ robustness. Similar trend holds on other settings, see Table 14 in Appendix.\nComparing to the other two negative loss regularizations, uniform and ℓ2 loss, we suggest readers\nto use our proposed contrastive loss in practice since it incorporates the extra beneﬁt of constraining\nthe embeddings of positive pairs to be similar and consistently performs better.\n5.4 Robustness improvements even under larger pre-training datasets\nWe further investigate if our proposed method can scale up to larger datasets and continues to be\nnecessary and valuable. To this end, we test if our proposed patch-based negative augmentation\nstill helps robustness when models are pre-trained on ImageNet-21k (10x larger than ImageNet-1k),\nwhere the baseline performance is higher. Take P-Shufﬂe as an example, we display the results in\nTable 5 with negative augmentation in both pre-training and ﬁne-tuning stages. We see that even\nwhen the pre-training dataset is signiﬁcantly increased, our patch-based negative augmentation can\nstill further improve the robustness of ViT. This demonstrates that our approach is valuable at scale\nand improves models’ robustness from an angle orthogonal to larger pre-training data.\nMean and standard deviation: Considering training models from scratch on ImageNet-1k or 21k\nis very costly, we take the strongest baseline as an example to independently run multiple times. As\nshown in Table 5, the small standard deviations suggests the improvement is statistically signiﬁcant.\nIn addition, we have presented 20+ different experimental settings in the paper where our proposed\npatch-based negative examples consistently improves robustness.\n8\n5.5 Patch-based negative augmentation reduces texture bias\nGeirhos et al. (2018) observed that unlike humans, CNNs rely on more local information (e.g.,\ntexture) rather than more global information (e.g., shape) to make a classiﬁcation. Since our patch-\nbased transformations largely destroy the global structure (e.g., shape), we want to investigate if\nthe non-robust features surviving patch-based transformation overlap with local texture biases. To\nthis end, we evaluate ViT-B/16 trained on patch-based transformations on Conﬂict Stimuli bench-\nmark (Geirhos et al., 2018), and we see that ViTs trained only on patch-based transformation have\na 4.9pp to 31.1pp increase on texture bias (Figure 5 in Appendix). This suggests that the useful but\nnon-robust features preserved in patch-based transformation are indeed overlapped with the local\ntexture bias. In addition, using our patch-negative augmentation can also to some extent reduce\nmodels’ reliance on local texture bias, e.g., we decrease the texture accuracy from 71.7% to 66.5%\nfor ViT-B/16 (Table 6 in Appendix).\n6 Discussions\nDoes ViT become more robust w.r.t. transformed images? We further evaluate ViTs trained\nwith our robust training algorithms on the patch-based transformed images. We found all three losses\non negative views can successfully reduce the prediction accuracy of ViTs to be close to random\nguess with the original semantic classes as the ground-truth. In other words, our robust training\nalgorithms make ViTs behave similarly as humans on those patch-based transformed images.\nImage          ViT- B/16\nAttention map\n+  P-Rotate / Uniform  \nAttention map\nJellyfish JellyfishPaper Towel\nShield ShieldFire Screen\nAxolotl AxolotlGoldfish\nStarfish Ocarina Starfish\nFigure 4: Attention visualization of\nimages misclassiﬁed by ViT-B/16 (mid-\ndle column) and correctly classiﬁed by\npatch-based negative augmentation (P-\nRotate) using uniform loss (third col-\numn) on ImageNet-R. The ground truth\n(ﬁrst column) or the predicted class\n(second and third column) are displayed\nat the bottom of each image.\nAre high conﬁdence predictions w.r.t. patch trans-\nformed images due to non-overlapping patch embed-\ndings? To answer this, we test original ViT from Doso-\nvitskiy et al. (2021) on the patch-transformed images\nwhere the patch size of the patch-based transformations\npst is not perfectly aligned with the input patch size psi,\ne.g., pst is not a multiple ofpsi. This can approximate the\nscenario when patch embeddings are extracted from over-\nlapped patches. As shown in Table 11 in Appendix, when\nthe patch size of the patch-based transformation ( pst =\n24) and the input patch size ( psi = 16) of ViT are not\nperfectly aligned, the test accuracy on the patch-based\nshufﬂed images decreases from 84.1 % to 32.9%. This is\nlower than the case that the patch sizes are aligned, e.g.,\nthe test accuracy is 57.8 % when pst = 32, but still sig-\nniﬁcantly higher than humans (a random guess is close to\n0.1%) as well as comparable CNN-based network 9.2%.\nTherefore, we conjecture the high conﬁdence w.r.t. trans-\nformed images is partially from non-overlapping patch\nembedding in ViTs.\nWhen does patch-based negative augmentation help?\nTo answer this question, we follow (Dosovitskiy et al.,\n2021) to visualize the attention maps of models trained\nwith/without our proposed negative data augmentation.\nIn Figure 4, we can see that our negative augmentation\ncan effectively 1) help models attend to the object to make\na correct prediction whereas the standard ViT fails (top\ntwo rows), and 2) mitigate models’ reliance on unrobust\nbiases, e.g., local bias, color bias, texture bias, etc. For example, the cartoon Axolotl (third row) is\nmisclassiﬁed by the standard ViT asGoldﬁsh due to color similarity and the Starﬁsh (bottom row) is\nincorrectly recognized as Ocarina because of local resemblance. This is aligned with other signals\nin previous sections that standard ViT relies on unrobust biases to make a prediction, whereas our\nnegative augmentation effectively encourages the model to rely less on these unrobust features. In-\nterestingly, for the bottom two rows, the standard ViT has successfully attended to the object in both\ncases but still failed to make correct predictions, indicating that the attention on the object does not\nensure a model is relying on semantically meaningful and robust features to make a prediction.\n9\n7 Related Work\nVision transformers (Dosovitskiy et al., 2021; Touvron et al., 2021) are a family of Transformer\nmodels (Vaswani et al., 2017) that directly process visual tokens constructed from image patch em-\nbedding. Unlike convolutional neural networks (LeCun et al., 1989; Krizhevsky et al., 2012; He\net al., 2016) that assume locality and translation invariance in their architectures, vision transform-\ners have no such assumptions and can exchange information globally. A few recent studies ﬁnd\npretrained vision transformers are at least as robust as the ResNet counterparts (Bhojanapalli et al.,\n2021), and possibly more robust (Naseer et al., 2021; Paul & Chen, 2021). Our work studies a\nspeciﬁc aspect of robustness pertaining patch-based visual tokens in ViT, and show it may lead to\na generalization gap. Different from (Naseer et al., 2021) which also shows ViTs are insensitive to\npatch operations such as shufﬂe and occlusion, we design different types of patch-based transforma-\ntions and develop deeper understanding that the features preserved in patch-based transformations\nare non-robust. Further, we propose a mitigation strategy based on these patch-based transforma-\ntions to increase robustness of vision transformers. Another orthogonal line of improving robustness\nof vision transformers is to develop a deeper understanding of the self-attention mechanisms in vi-\nsion transformers (Gu et al., 2022) and further advances the neural network architectures (Zhou\net al., 2022).\nData augmentation is widely used in computer vision models to improve model perfor-\nmance (Howard, 2013; Szegedy et al., 2015; Cubuk et al., 2020, 2019; Touvron et al., 2021). How-\never, most of the existing data augmentations are “positive” in the sense they assume the class\nsemantic being preserved after the transformation. In this work, we explore “negative” data aug-\nmentation operations based on patches, where we encourage the representations of transformed\nexample to be different from the original ones. Most related to our work in this direction is the work\nof Sinha et al. (2020). Although the concept of negative augmentation was proposed in their work,\nthey only apply it for generative and unsupervised modeling without consideration of robustness. In\ncontrast, our work focuses on discriminative and supervised modeling, and demonstrate how such\nnegative examples can reveal speciﬁc robustness issues and such augmentation approaches can offer\nrobustness improvements under large-scale pretraining settings.\nOur work is also related to contrastive learning (Wu et al., 2018; Hjelm et al., 2019; Oord et al.,\n2018; He et al., 2020; Tian et al., 2020). The increasing number of negative pairs has shown to\nbe important for representation learning in self-supervised contrastive learning (Chen et al., 2020a),\nwhere different images serve as negative examples for each other, and supervised contrastive learn-\ning (Khosla et al., 2020), where images with different classes are used as negative examples. Unlike\nthe traditional setting of representation learning, our proposed contrastive loss serves as a regular-\nization term with patch-based negative augmentations as extra negative data points.\n8 Conclusion\nThrough this research we have found concrete evidences of ViTs relying on non-robust features to\nmake predictions and shown that this reliance is limiting out-of-distribution robustness. This opens\nmultiple exciting new lines of research. First, our methodology for analyzing the robustness of ViTs\nprovides a valuable recipe for future research. Through designing patch-wise, semantic-destroying\ntransformations that ViTs are insensitive to, we identiﬁed which non-robust features models rely\non. Second, through negative augmentations during training we reduced the ViTs’ reliance on such\nnon-robust features, and improved the out-of-distribution performance of ViTs signiﬁcantly, with-\nout harming in-distribution accuracy. This approach shows the potential for further improving the\nrobustness of ViTs. While we have identiﬁed multiple such non-robust features in ViTs, we be-\nlieve discovering and addressing more is a promising direction for continued progress toward robust\nvision transformers and vision models in general.\nAcknowledgements\nThe authors would like to thank Alexey Dosovitskiy, Lucas Beyer and Neil Houlsby for helpful\ndiscussions on vision transformer systems. We also want to thank the reviewers for their useful\ncomments.\n10\nReferences\nSrinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and An-\ndreas Veit. Understanding robustness of transformers for image classiﬁcation. arXiv preprint\narXiv:2103.14586, 2021.\nWieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models\nworks surprisingly well on imagenet. In The International Conference on Learning Represen-\ntations, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International Conference on Machine Learning,\npp. 1597–1607. PMLR, 2020a.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big\nself-supervised models are strong semi-supervised learners. Advances in Neural Information\nProcessing Systems, 2020b.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 113–123, 2019.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pp. 702–703, 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-\nage is worth 16x16 words: Transformers for image recognition at scale.International Conference\non Learning Representations, 2021.\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and\nWieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias im-\nproves accuracy and robustness. In International Conference on Learning Representations, 2018.\nJindong Gu, V olker Tresp, and Yao Qin. Are vision transformers robust to patch perturbations? In\nEuropean Conference on Computer Vision, 2022.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 9729–9738, 2020.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In International Conference on Learning Representations, 2019a.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In Proceedings of the International Conference on Learning Repre-\nsentations, 2019b.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\nexamples. arXiv preprint arXiv:1907.07174, 2019.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty.\nIn International Conference on Learning Representations, 2020.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. Interna-\ntional Conference on Computer Vision, 2021.\n11\nKatherine L Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias\nin convolutional neural networks. In Advances In Neural Information Processing Systems, 2020.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. International Conference on Learning Representations, 2019.\nAndrew G Howard. Some improvements on deep convolutional neural network based image classi-\nﬁcation. arXiv preprint arXiv:1312.5402, 2013.\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint\narXiv:1803.06373, 2018.\nFereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups\ndisproportionately. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency, pp. 196–205, 2021.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances In Neural\nInformation Processing Systems, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2015.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University\nof Toronto, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. Advances in Neural Information Processing Systems , 25:1097–1105,\n2012.\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-\nbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural Computation, 1(4):541–551, 1989.\nXiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Shaokai Ye, Yuan He, and Hui Xue. Rethinking\nthe design principles of robust vision transformer. arXiv preprint arXiv:2105.07926, 2021.\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz\nKhan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. arXiv preprint\narXiv:2105.10497, 2021.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nSayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint\narXiv:2105.07581, 2021.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\n(IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.\nAbhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin, and Stefano Ermon.\nNegative data augmentation. In International Conference on Learning Representations, 2020.\nAndreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas\nBeyer. How to train your vit? data, augmentation, and regularization in vision transformers.arXiv\npreprint arXiv:2106.10270, 2021.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n12\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European\nConference on Computer Vision, pp. 776–794. Springer, 2020.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nInternational Conference on Machine Learning, pp. 10347–10357. PMLR, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 5998–6008, 2017.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-\nparametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 3733–3742, 2018.\nKai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of\nimage backgrounds in object recognition. International Conference on Learning Representations,\n2021.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers,\n2021.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.\nTheoretically principled trade-off between robustness and accuracy. International Conference on\nMachine Learning, 2019.\nDaquan Zhou, Zhiding Yu, Enze Xie, Chaowei Xiao, Animashree Anandkumar, Jiashi Feng, and\nJose M Alvarez. Understanding the robustness in vision transformers. InInternational Conference\non Machine Learning, pp. 27378–27394. PMLR, 2022.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes]\n(c) Did you discuss any potential negative societal impacts of your work? [N/A] Not\nApplicable\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [Yes]\n(b) Did you include complete proofs of all theoretical results? [Yes]\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main exper-\nimental results (either in the supplemental material or as a URL)? [Yes] See Supple-\nmentary Materials.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Appendix.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] See Section 5.4.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [No]\n13\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [Yes]\n(c) Did you include any new assets either in the supplemental material or as a URL? [No]\n(d) Did you discuss whether and how consent was obtained from people whose data\nyou’re using/curating? [N/A]\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁ-\nable information or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n14\nAppendix\nA Patch-based Negative Data Augmentation Reduces Texture Bias\nFigure 5: ViTs trained only on our patch-based transformations exhibit stronger texture bias. Each\nbar is the texture accuracy ( %) on Conﬂict Stimuli (Geirhos et al., 2018), and a higher texture\naccuracy indicates the model has a higher bias towards texture. The “texture accuracy” is deﬁned as\nthe percentage of images that are classiﬁed as the “texture” label, provided the image is classiﬁed\nas either “texture” or “shape” label. The baseline model is ViT-B/16 in (Dosovitskiy et al., 2021)\ntrained on original images. Other models are trained on patch-based transformed images, e.g., “P-\nShufﬂe” stands for a ViT-B/16 model trained on patch-based shufﬂed images. Numbers above the\nbars are either accuracy (e.g., ViT-B/16) or themax accuracy difference between each model family\nand the baseline ViT-B/16. The patch size in P-Shufﬂe and P-Rotate and replacement ratio in P-Inﬁll\nis denoted by “ps” and “rr” respectively.\nTable 6: Patch-based negative augmentation effectively reduce models’ texture bias on Conﬂict\nStimuli (Geirhos et al., 2018). A higher texture accuracy indicates the model has a higher bias\ntowards texture. The “texture accuracy” is deﬁned as the percentage of images that are classiﬁed as\nthe “texture” label, provided the image is classiﬁed as either “texture” or “shape” label.\nPre-train on ImageNet-1k Pre-train on ImageNet-21k\nModel Texture Accuracy Model Texture Accuracy\nViT-B/16 71.7 Rand-Augment 57.5\n+ P-Rotate / Uniform 66.5 + P-Shufﬂe / Uniform 56.4\n+ P-Rotate / L2 67.2 + P-Shufﬂe / L2 54.7\nB Training Details\nWe follow (Dosovitskiy et al., 2021) to train each model using Adam (Kingma & Ba, 2015) opti-\nmizer with β1 = 0.9, β2 = 0.999 for pre-training and SGD with momentum for ﬁne-tuning. The\nbatch size is set to be 4096 for pre-training and 512 for ﬁne-tuning. All models are trained with 300\nepochs on ImageNet-1k and 90 epochs on ImageNet-21k in the pre-training stage. In the ﬁne-tuning\nstage, all models are trained with 20k steps except the models pretrained from ImageNet-1k without\nRand-Augment (Cubuk et al., 2020) or Augmix (Hendrycks et al., 2020), which we train them with\n8k steps. The learning rate warm-up is set to be 10k steps. Dropout is used for both pre-training\nand ﬁne-tuning with dropout rate 0.1. If the training dataset is ImageNet-1k, we additionally apply\ngradient clipping at global norm 1.\n15\nTable 7: Training details following (Dosovitskiy et al., 2021).\nPre-train Dataset Stage Base LR LR Decay Weight Decay Label Smoothing\nImageNet-1K Pre-train 3 · 10−3 ‘cosine’ None 10−4\nImageNet-21k Pre-train 10−3 ‘linear’ 0.03 10−4\nImageNet-1K Fine-tune 0.01 ‘cosine’ None None\nImageNet-21K Fine-tune 0.03 ‘cosine’ None None\nTable 8: Models using a different hyperparameter λthan the default value (1.5).\nModel Pre-train Dataset Training stage Hyperparameter λ\nRand-Augment + P-Shufﬂe / Uniform ImageNet-1k Pre-train 1.0\nRand-Augment + P-Shufﬂe / Contrastive ImageNet-1k Pre-train 1.0\nAugMix + P-Shufﬂe / L2 ImageNet-1k Pre-train 1.0\nAugMix + P-Rotate / L2 ImageNet-1k Pre-train 1.0\nAugMix + P-Inﬁll / L2 ImageNet-1k Pre-train 1.0\nAugMix + P-Shufﬂe / Contrastive ImageNet-1k Pre-train 1.0\nRand-Augment + P-Shufﬂe / Uniform ImageNet-21k Pre-train 0.5\nRand-Augment + P-Shufﬂe / L2 ImageNet-21k Pre-train 0.5\nRand-Augment + P-Shufﬂe / Contrastive ImageNet-21k Pre-train 0.5\nRand-Augment + P-Rotate / Uniform ImageNet-1k Fine-tune 0.5\nRand-Augment + P-Inﬁll / Uniform ImageNet-1k Fine-tune 1.0\nAugMix + P-Rotate / Uniform ImageNet-1k Fine-tune 1.0\nRand-Augment + P-Shufﬂe / Uniform ImageNet-21k Fine-tune 0.5\nC Hyper-parameters in Patch-based Negative Augmentation\nFor the temperature τ used in contrastive loss, we consistently observe that τ = 0.5 works better in\npre-training stage and τ = 0.1 works better in ﬁne-tuning stage. Therefore, we keep this setting for\nall the models in our paper.\nSince we sweep the coefﬁcient λin Eqn. 1 from the set {0.5,1.0,1.5}, we observe that for most of\nthe cases, λ= 1.5 works the best. In total we have 48 models using loss regularization on negative\nviews in Table 1, Table 2, Table 5 and Table 3. We use λ = 1.5 for all of them except those listed\nin Table 8, where either λ = 0.5 or λ = 1.0 works better. Actually, we ﬁnd our proposed negative\naugmentation is relatively robust to λ. Therefore, we suggest using λ = 1.5 if readers do not want\nto sweep for the best value for this hyperparameter.\nIn Table. 9, we display the hyperparameters in each patch-based transformation that we use for the\nreported results in this work. Our algorithms are generally insensitive to these parameters, and we\nuse the same hyperparameter for all the settings investigated in this work.\nD Ablation Study\nD.1 Sensitivity analysis\nWe test the sensitivity of our patch-based negative augmentation to various patch sizes in P-Shufﬂe\nand P-Rotate, and different replace rates in P-Inﬁll. We ﬁnd that P-Shufﬂe and P-Rotate are insen-\nsitive to patch sizes from {16,32,48,64,96}for ViT-B/16, and P-Inﬁll is robust to replace rates\nranging from 1/3 to 1/2. The accuracy difference is smaller than 0.5 % on ImageNet-1k as well as\nImageNet-A and ImageNet-R. Therefore, we use the same parameter for all the settings investigated\nin this work (see Table 9 and Appendix C for details).\nD.2 Double batch-size of baselines\nAs we use the negative augmented view per example, the effective batch size is doubled compared\nto the vanilla ViT-B/16 trained with only cross-entropy loss. Therefore, we further investigate if\nthe robustness improvement is a result from a larger batch size. When we increase the batch size\nfrom 4096 to 8192 in pre-training while keeping the same 300 training epochs, it decreases the in-\ndistribution accuracy to 76.0% on ImageNet-1k as well as the accuracy on robustness benchmarks,\n16\nTable 9: Hyperparameters in patch-based transformations.\nImage Size Stage Model Transformation Hyperparameter\n224 × 224 Pre-train ViT-B/32 P-Rotate patch size = 32\n224 × 224 Pre-train ViT-B/16 P-Shufﬂe patch size = 32\n224 × 224 Pre-train ViT-B/16 P-Rotate patch size = 16\n224 × 224 Pre-train ViT-B/16 P-Inﬁll replace rate = 15/49\n384 × 384 Fine-tune ViT-B/32 P-Rotate patch size = 64\n384 × 384 Fine-tune ViT-B/16 P-Shufﬂe patch size = 64\n384 × 384 Fine-tune ViT-B/16 P-Rotate patch size = 32\n384 × 384 Fine-tune ViT-B/16 P-Inﬁll replace rate = 3/8\nTable 10: P-Shufﬂe improve accuracy on CIFAR-100 and CIFAR-100-C when ViT-B/16 is pre-\ntrained on ImageNet-21k and then ﬁne-tuned on CIFAR-100.\nModel CIFAR-100 CIFAR100-C\nViT-B/16 91.8 74.6\n+ P-Shufﬂe / Uniform 92.6 (+0.8) 77.0 (+2.4)\ne.g., ImageNet-R from 20.3 % to 19.3%. Hence we conclude the robustness improvement is from\nthe negative data augmentation.\nD.3 Pre-training vs. Fine-tuning\nWe further disentangle the effect of patch-based negative data augmentation in pre-training and ﬁne-\ntuning. Take P-Shufﬂe as an example, we design experiments to apply negative augmentation 1)\nonly at the ﬁne-tuning stage, 2) only at the pre-training stage, and 3) at both stages. As shown\nin Table 13, compared to the baselines, patch-based negative augmentation can effectively help\nimprove robustness in both stages, and its effect in pre-training is slightly larger than in ﬁne-tuning.\nFinally, we found using negative augmentation in both stages during training yields the largest gain.\nD.4 When to use positive data augmentation\nAs Steiner et al. (2021) observed that traditional (positive) augmentation can slightly hurt the accu-\nracy of ViT if applied to ﬁne-tuning stage, we compare the accuracy of a ViT-B/16 when positive\naugmentation (e.g., Rand-Augment (Cubuk et al., 2020)) is only applied to pre-training stage as well\nas both stages. As shown in Table 15, ﬁne-tuning without Rand-Augment achieves slightly better\nperformance.\nE Visualization of Patch-based Transformations\nWe display more examples with patch-based transformations without cherry-picking in Figure 6,\nFigure 7 and Figure 8.\n17\nTable 11: Test accuracy on P-Shufﬂed images with different patch sizespst and clean ImageNet-1k.\nModel pst=24 pst=32 ImageNet-1k\nViT-B/16 (psi = 16) 32.9 57.8 84.1\nBiT-ResNet101-3 9.2 24.6 84.0\nTable 12: Effects of patch-based negative augmentation on ﬁve different levels of corruption severi-\nties on ImageNet-C. Best results are highlighted in bold.\nModel ImageNet-C Corruption Severity Level\n1 2 3 4 5\nRand-Augment (Cubuk et al., 2020) 70.4 63.7 57.9 48.2 36.1\n+ P-Shufﬂe / Uniform 71.0 (+0.6) 64.4 (+0.7) 59.0 (+1.1) 49.5 (+1.3) 37.3 (+1.2)\n+ P-Rotate / Uniform 71.1 (+0.7) 64.6 (+0.9) 59.0 (+1.1) 50.0 (+1.8) 37.6 (+1.5)\n+ P-Inﬁll / Uniform 71.1 (+0.7) 64.6 (+0.9) 59.1 (+1.2) 49.5 (+1.3) 37.3 (+1.2)\n+ P-Shufﬂe / L2 70.5 (+0.1) 63.9 (+0.2) 58.3 (+0.4) 48.6 (+0.4) 36.6 (+0.5)\n+ P-Rotate / L2 71.1 (+0.7) 64.8 (+1.1) 59.5 (+1.6) 50.1 (+1.9) 37.8 (+1.7)\n+ P-Inﬁll / L2 70.5 (+0.1) 63.8 (+0.1) 58.2 (+0.3) 48.4 (+0.2) 36.0 (-0.1)\nAugMix (Hendrycks et al., 2020) 71.4 65.2 60.5 51.9 40.2\n+ P-Shufﬂe / Uniform 71.6 (+0.2) 65.7 (+0.5) 61.2 (+0.7) 52.8 (+0.9) 41.4 (+1.2)\n+ P-Rotate / Uniform 71.7 (+0.3) 65.7 (+0.5) 61.1 (+0.6) 52.7 (+0.8) 41.4 (+1.2)\n+ P-Inﬁll / Uniform 71.9 (+0.5) 65.8 (+0.6) 61.1 (+0.6) 52.4 (+0.5) 40.8 (+0.6)\n+ P-Shufﬂe / L2 71.8 (+0.4) 65.8 (+0.6) 61.0 (+0.5) 52.4 (+0.5) 40.7(+0.5)\n+ P-Rotate / L2 71.9 (+0.5) 66.0 (+0.8) 61.5 (+1.0) 52.9 (+1.0) 41.6 (+1.4)\n+ P-Inﬁll / L2 71.8 (+0.4) 65.8 (+0.6) 61.3 (+0.8) 52.7 (+0.8) 41.0 (+0.8)\nTable 13: Effect of patch-based negative augmentation in pre-training and ﬁne-tuning stages. Top-1\naccuracies of ViT-B/16 pretrained and ﬁne-tuned on ImageNet-1k. Under ‘Stage’ we denote which\ntraining stage patch-based negative augmentation is used. The best result under each setting is\nhighlighted in bold.\nPre-train on ImageNet-1k\nModel Stage ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment (Cubuk et al., 2020) - 79.1 7.2 55.2 23.0\n+ P-Shufﬂe / Uniform Fine-tune 79.1 7.1 55.3 23.0\n+ P-Shufﬂe / Uniform Pre-train 79.3 7.6 56.2 23.5\n+ P-Shufﬂe / Uniform Both 79.3 7.7 56.2 23.4\n+ P-Shufﬂe / Contrastive Fine-tune 79.5 7.6 56.2 23.7\n+ P-Shufﬂe / Contrastive Pre-train 79.4 8.5 56.8 24.0\n+ P-Shufﬂe / Contrastive Both 79.7 8.9 57.8 24.7\nPre-train on ImageNet-21k\nModel Stage ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment (Cubuk et al., 2020) - 84.4 28.7 67.2 38.7\n+ P-Shufﬂe / L2 Fine-tune 84.5 29.4 67.9 39.0\n+ P-Shufﬂe / L2 Pre-train 84.4 29.9 67.5 38.8\n+ P-Shufﬂe / L2 Both 84.5 29.7 68.0 39.6\n+ P-Shufﬂe / Contrastive Fine-tune 84.4 29.2 67.5 38.7\n+ P-Shufﬂe / Contrastive Pre-train 84.6 29.9 67.7 38.5\n+ P-Shufﬂe / Contrastive Both 84.3 30.8 68.1 38.6\n18\nTable 14: Effect of patch-based negative augmentation in contrastive loss regularization. Top-1\naccuracies of ViT-B/16 trained with or without patch-based negative augmentation.\nPre-train on ImageNet-1k\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nViT-B/16 + Contrastive* 78.7 8.1 53.5 22.8\nViT-B/16 + Shufﬂe / Contrastive 78.9 8.2 54.1 23.2\nViT-B/16 + P-Rotate / Contrastive 78.9 8.6 54.1 23.6\nRand-Augment + Contrastive* 79.7 8.9 57.6 24.7\nRand-Augment + P-Rotate / Contrastive 79.9 9.4 58.4 25.4\nRand-Augment + P-Inﬁll / Contrastive 79.9 9.3 57.9 25.0\nAugMix + Contrastive* 79.6 9.0 59.8 27.2\nAugMix + P-Rotate / Contrastive 79.6 9.8 60.0 27.5\nAugMix + P-Inﬁll / Contrastive 79.6 9.9 60.3 27.3\nPre-train on ImageNet-21k\nModel ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment + Contrastive* 84.1 29.7 67.6 39.2\nRand-Augment + P-Shufﬂe / Contrastive 84.3 30.8 68.1 38.6\nTable 15: Effect of positive augmentation in pre-training and ﬁne-tuning stages. Top-1 accuracies\nof ViT-B/16 pretrained on ImageNet-21k and ﬁne-tuned on ImageNet-1k. Under ‘Stage’ we denote\nwhich training stage Rand-Augment (Cubuk et al., 2020) is used.\nModel Stage ImageNet-1k ImageNet-A ImageNet-C ImageNet-R\nRand-Augment Pre-train 84.4 28.7 67.2 38.7\nRand-Augment Both 84.4 29.1 67.0 38.4\n19\nFigure 6: Examples of original images (on the top) and their corresponding patch-based shufﬂe (at\nthe bottom) with either patch size 32 or 48 without cherry-picking.\n20\nFigure 7: Examples of original images (on the top) and their corresponding patch-based rotation (at\nthe bottom) with either patch size 32 or 48 without cherry-picking.\n21\nFigure 8: Examples of original images (on the top) and their corresponding patch-based inﬁll (at the\nbottom) with either replace rate 0.25 or 0.375 without cherry-picking.\n22",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.831173837184906
    },
    {
      "name": "Computer science",
      "score": 0.7063210010528564
    },
    {
      "name": "Transformer",
      "score": 0.5702638030052185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5464519262313843
    },
    {
      "name": "Machine learning",
      "score": 0.3523464798927307
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33916211128234863
    },
    {
      "name": "Computer vision",
      "score": 0.3367961347103119
    },
    {
      "name": "Engineering",
      "score": 0.09900552034378052
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}