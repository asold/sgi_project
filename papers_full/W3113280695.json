{
  "title": "KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning",
  "url": "https://openalex.org/W3113280695",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1976599103",
      "name": "He Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111694291",
      "name": "Xiao Jinghui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1952241286",
      "name": "Liu Qun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3026990524",
    "https://openalex.org/W3018732874",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2775747321",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2982426914",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2968908603",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W2981320891",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3017231514",
    "https://openalex.org/W3021191241",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2968289784",
    "https://openalex.org/W3011411500"
  ],
  "abstract": "Recent studies on pre-trained language models have demonstrated their ability to capture factual knowledge and applications in knowledge-aware downstream tasks. In this work, we present a language model pre-training framework guided by factual knowledge completion and verification, and use the generative and discriminative approaches cooperatively to learn the model. Particularly, we investigate two learning schemes, named two-tower scheme and pipeline scheme, in training the generator and discriminator with shared parameter. Experimental results on LAMA, a set of zero-shot cloze-style question answering tasks, show that our model contains richer factual knowledge than the conventional pre-trained language models. Furthermore, when fine-tuned and evaluated on the MRQA shared tasks which consists of several machine reading comprehension datasets, our model achieves the state-of-the-art performance, and gains large improvements on NewsQA (+1.26 F1) and TriviaQA (+1.56 F1) over RoBERTa.",
  "full_text": "KgPLM: Knowledge-guided Language Model Pre-training via Generative and\nDiscriminative Learning\nBin He, Xin Jiang, Jinghui Xiao, Qun Liu\nHuawei Noah‚Äôs Ark Lab\n{hebin.nlp, Jiang.Xin, xiaojinghui4, qun.liu}@huawei.com\nAbstract\nRecent studies on pre-trained language models have demon-\nstrated their ability to capture factual knowledge and appli-\ncations in knowledge-aware downstream tasks. In this work,\nwe present a language model pre-training framework guided\nby factual knowledge completion and veriÔ¨Åcation, and use\nthe generative and discriminative approaches cooperatively\nto learn the model. Particularly, we investigate two learning\nschemes, named two-tower scheme and pipeline scheme, in\ntraining the generator and discriminator with shared parame-\nter. Experimental results on LAMA, a set of zero-shot cloze-\nstyle question answering tasks, show that our model contains\nricher factual knowledge than the conventional pre-trained\nlanguage models. Furthermore, when Ô¨Åne-tuned and evalu-\nated on the MRQA shared tasks which consists of several\nmachine reading comprehension datasets, our model achieves\nthe state-of-the-art performance, and gains large improve-\nments on NewsQA (+1.26 F1) and TriviaQA (+1.56 F1) over\nRoBERTa.\n1 Introduction\nPre-trained language models such as BERT or RoBERTa\nlearn contextualized word representations on large-scale text\ncorpus through self-supervised learning, and obtain new\nstate-of-the-art results on many downstream NLP tasks (Pe-\nters et al. 2018; Radford et al. 2018; Devlin et al. 2019; Liu\net al. 2019). Recently, researchers have observed that pre-\ntrained language models can internalize real-word knowl-\nedge into their model parameters (Petroni et al. 2019; Lo-\ngan et al. 2019; Talmor et al. 2019). For example, pre-\ntrained language models are able to answer the questions\nsuch as ‚Äú the sky is ‚Äù or ‚Äú Beethoven was born in ‚Äù\nwith moderate accuracy. To further explore their potential,\nresearchers have proposed various approaches to guide the\npre-training of the language models by injecting different\nforms of knowledge into them, such as structured knowl-\nedge graph or linguistic knowledge (Zhang et al. 2019; Pe-\nters et al. 2019; Xiong et al. 2020; Wang et al. 2020; Roberts,\nRaffel, and Shazeer 2020; Joshi et al. 2020b).\nTable 1 lists some of the previous knowledge-guided pre-\ntrained language models with their training methods. We\ngroup them into two categories: generative tasks and dis-\ncriminative tasks. Generative tasks are often formulated as\nInception is a science fiction film directed by Christopher Nolan.\nChristopher Nolan\nSteven Spielberg\nMartin Scorsese\nDirector\nKnowledge completion\nInception is a science fiction film \ndirected by [MASK] [MASK] .\nKnowledge verification\nInception is a science fiction film \ndirected by Steven Spielberg .\nFigure 1: An example of generative knowledge comple-\ntion and discriminative knowledge veriÔ¨Åcation tasks. The\nspan Christopher Nolan is masked for prediction (left) and\na ‚Äúfake‚Äù spanSteven Spielberg replaces the original span for\ndetection (right).\npredicting the masked tokens given the context. By par-\nticularly masking out the words that contain certain types\nof knowledge (e.g., entities) in generative pre-training, the\nmodel can be more adept in memorizing and completing\nsuch knowledge. While discriminative tasks are often for-\nmulated as a classiÔ¨Åcation problem with respect to the sen-\ntence or the tokens. By training on the positive and negative\nexamples constructed according to the external knowledge,\nthe discriminator can be more capable of verifying the true\nor false knowledge in natural language. Existing research\nhas demonstrated thatgenerative and discriminative training\nhave their advantages: the former has a large negative sam-\nple space so that the model can learn Ô¨Åne-grained knowl-\nedge, while the latter avoids the ‚Äú [MASK]‚Äù tokens in pre-\ntraining, and is therefore more consistent with Ô¨Åne-tuning.\nOn the other hand, generative and discriminative capture the\ndifferent aspects of data distribution and could be comple-\nmentary to each other in knowledge consolidation. However,\nto the best of our knowledge, there is not previous work in\ncombining the two approaches in a systematic way. Inspired\nby the recent success on the generative-discriminative pre-\ntrained model named ELECTRA (Clark et al. 2020), we pro-\npose to learn the generator and discriminator jointly in the\nknowledge-guided pre-training, which we call the KgPLM\nmodel.\narXiv:2012.03551v1  [cs.CL]  7 Dec 2020\nModel Generative tasks Discriminative tasks\nERNIE-Baidu (Sun et al. 2020) phrase and entity masking -\nSenseBERT (Levine et al. 2019) supersense prediction -\nBERT CS (Ye et al. 2019) - multi-choice question answering\nLIBERT (Lauscher et al. 2019) - lexical relation prediction\nLIMIT-BERT(Zhou, Zhang, and Zhao 2019) semantic/syntactic phrase masking -\nKEPLER (Wang et al. 2019) - knowledge representation learning\nSpanBERT (Joshi et al. 2020a) span masking -\nWKLM (Xiong et al. 2020) - entity replacement checking\nK-Adapter (Wang et al. 2020) - relation classiÔ¨Åcation, dependency relation\nprediction\nT5+SSM (Roberts, Raffel, and Shazeer 2020) salient span masking -\nTEK (Joshi et al. 2020b) span masking on TEK-augmented text -\nCN-ADAPT (Lauscher et al. 2020) MLM training on synthetic knowledge corpus -\nKgPLM (ours) knowledge span masking knowledge span replacement checking\nTable 1: Comparison of training methods between different knowledge-guided pre-training language models.\nIn this paper, we design masked span prediction as the\ngenerative knowledge completion task, and span replace-\nment checking as the discriminative knowledge veriÔ¨Åca-\ntion task. Hybrid knowledge, including link structure of\nWikipedia and structured knowledge graph in Wikidata, is\nused to guide the both tasks. The spans covering the fac-\ntual knowledge are more likely to be selected for mask-\ning or replacement, and the choices of their replacements\nare also related to the proximity to the original span in the\nknowledge space. Figure 1 shows an example of the span\nmasking and replacement tasks. To further explore effec-\ntive ways to the joint training of the two tasks, we design\ntwo learning schemes, which we called two-tower scheme\nand pipeline scheme. Basically, the generator and discrim-\ninator are trained in parallel with the shared parameters in\nthe two-tower scheme. While in the pipeline scheme, the\noutput of generator is input to the successive discrimina-\ntive training. The generator and discriminator in our KgPLM\nmodel are both pre-trained based on RoBERTa BASE (Liu\net al. 2019). They have some additional beneÔ¨Åts: 1) the\nmodel can be readily extended to much larger pre-training\ncorpus, which keeps some potential room for further im-\nprovement; 2) the model retains the same amount of param-\neters as RoBERTaBASE, and does not require any modiÔ¨Åca-\ntions in Ô¨Åne-tuning for the downstream tasks.\nWe evaluate the model performance on LAMA (Petroni\net al. 2019), which consists of several zero-shot knowl-\nedge completion tasks, and MRQA shared tasks (Fisch\net al. 2019), which include several benchmark question\nanswering datasets. The experiments show the proposed\nKgPLM, especially that trained with the pipeline scheme,\nachieves the state-of-the-art performance, and signiÔ¨Å-\ncantly outperform several strong baselines (RoBERTa BASE,\nSpanBERTBASE (Joshi et al. 2020a), WKLM (Xiong et al.\n2020)) on some of the tasks. The results indicate that\nthe knowledge-guided generative and discriminative pre-\ntraining provides an effective way to incorporate external\nknowledge and achieve competitive performance on the\nknowledge intensive NLP tasks.\n2 Related Work\nMost knowledge-guided pre-training methods can be cate-\ngorized into groups according to their pre-training objectives\n(1) generative objectives, and (2) discriminative objectives.\nFor masked language models, different masking mecha-\nnisms are always used to design the generative objectives.\nERNIE-Baidu (Sun et al. 2020) introduces new masking\nunits such as phrases and entities to learn knowledge infor-\nmation in these masking units. As a reward, syntactic and\nsemantic information from phrases and entities is implicitly\nintegrated into the language model. SpanBERT (Joshi et al.\n2020a) extends subword-level masking to span-level mask-\ning, and selects random spans of full words to predict. In\naddition to the MLM objective, a span boundary objective is\ndesigned to predict each subword within a span using sub-\nword representations at the boundaries. Joshi et al. (2020b)\nutilize retrieved background sentences for phrases to extend\nthe input text, and combine the TEK-enriched context and\nspan masking to pre-train the language model. Besides, Ros-\nset et al. (2020) introduce entity information into an autore-\ngressive language model, called KALM, which identiÔ¨Åes the\nentity surface in a text sequence and maps word n-grams into\nentities to obtain an entity sequence for knowledge-aware\nlanguage model pre-training.\nYe et al. (2019) propose a discriminative pre-training\napproach for incorporating commonsense knowledge into\nthe language model, in which the question is concatenated\nwith different candidates to construct a multi-choice ques-\ntion answering sample, and each choice is used to predict\nwhether the candidate is the correct answer. KEPLER (Wang\net al. 2019) uniÔ¨Åes knowledge representation learning and\nlanguage modeling objectives, which builds a bridge be-\ntween text representation and knowledge embeddings by\nencoding entity descriptions, and can better integrate fac-\ntual knowledge into the pre-trained language model. Xiong\net al. (2020) introduce entity replacement checking task into\nthe pre-trained language model, which greatly enhances the\nmodeling of entity information. Wang et al. (2020) propose a\nplug-in way to infuse knowledge into language models, and\ndifferent discriminative objectives are used to keep different\nkinds of knowledge in different adapters.\nÔÅêÔÅê\nGenerator\nInception is a science fiction film directed by [MASK] [MASK] .\nNolanChristopher\nDiscriminator\nInception is a science fiction film directed by Steven Spielberg .\nÔÅê ÔÅê ÔÅê ÔÅê ÔÅê ÔÅê ÔÅè ÔÅè ÔÅê\n ùíô  ùíôMasked: Replaced:\nFigure 2: Knowledge-guided pre-training with generative and discriminative objectives. The knowledge span Christopher\nNolan is masked for token prediction in the Generator (left). A ‚Äúfake‚Äù spanSteven Spielbergreplacing the original spanChristo-\npher Nolan is fed to the Discriminator (right) to predict whether the tokens are replaced or not.\nTo the best of our knowledge, this work is the Ô¨Årst to ex-\nplore knowledge-guided pre-training that considers the gen-\nerative and discriminative approaches simultaneously. Be-\nsides, our model does not involve any additional cost to\ndownstream tasks in terms of parameters and computations.\n3 Methodology\n3.1 Knowledge-guided Pre-training\nThe vanilla masked language model performs pre-training\nby maximizing the log-likelihood:\nLMLM(Œ∏) =\nn‚àë\ni=1\nmi log p(xi|x‚ó¶m; Œ∏), (1)\nwhere x = [ x1,...,x n] is the original sequence of to-\nkens, ‚ó¶ denotes the element-wise product, and m =\n[m1,...,m n] is the binary vector where mi = 1 indicates\nreplacing xi with [MASK] or otherwise unchanged. Œ∏rep-\nresents the model parameter.\nFor knowledge-guided pre-trained language model, there\nare two approaches to incorporate information from knowl-\nedge sources. One is the generative approach that pushes\nmodel to predict the masked knowledge-sensitive tokens,\nsuch as tokens in entities or relations. Formally, with the\nmasking scheme mKinduced by knowledge sources K, the\nmasked sequence becomes ¬Øx = K-MASK(x) = x‚ó¶mK.\nThe generative loss is then deÔ¨Åned as the negative log-\nlikelihood of predicting the masked tokens:\nLGen(Œ∏g) = ‚àí\nn‚àë\ni=1\nmK\ni log p(xi|¬Øx; Œ∏g). (2)\nThe other is the discriminative approach where the orig-\ninal sequence is converted to ÀÜx = K-Replace(x) by\nreplacing the knowledge-sensitive parts of sequence with\nother related texts according to the knowledge contained in\nK. Discriminative loss could be deÔ¨Åned at sequence-level,\nspan-level, or token-level. In this paper, we use the Ô¨Åne-\ngrained token-level loss, which is the cross-entropy loss of\nclassifying whether the token is replaced or not:\nLDisc(Œ∏d) = ‚àí\nn‚àë\ni=1\n1 (ÀÜxi = xi) logp(ri = 0|ÀÜx; Œ∏d)\n+ 1 (ÀÜxi Ã∏= xi) logp(ri = 1|ÀÜx; Œ∏d).\n(3)\nHere each ri is a binary variable indicating whether ÀÜxi is\ndifferent from xi.\nFigure 2 illustrate the diagram of the generator and\nthe discriminator from the two perspectives. The proposed\nknowledge-guided pre-training is deÔ¨Åned as the following\noptimization problem:\nmin\nŒ∏g,Œ∏d\nLGen(Œ∏g) + ŒªLDisc(Œ∏d). (4)\nIn the following sections, we detail our approach of us-\ning external knowledge to design the generative (section 3.2)\nand discriminative tasks (section 3.3) respectively, and ex-\nplore different ways of joint learning of the two models (sec-\ntion 3.4).\n3.2 Generative Pre-training\nGiven the input sequence x, unlike the conventional ran-\ndom masking, we particularly extract spans of text which\nare more knowledge intensive as the candidates for mask-\ning. In this work, we use Wikipedia text as the pre-training\ncorpus, and follow the approach proposed in WiNER (Ghad-\ndar and Langlais 2017) to extract the knowledge-sensitive\nspans. BrieÔ¨Çy, the anchored strings in a Wikipedia page\nare Ô¨Årst collected. Then, through mining titles of out-linked\npages and their co-referring mentions in the current page,\nmore salient spans are identiÔ¨Åed. As reported in (Ghaddar\nand Langlais 2017), the coverage of the knowledge-sensitive\nspans (mostly the entities and concepts) in the Wikipedia\ntexts can be increased to 15%.\nAs the example shown in the left part of Figure 2, entities\nor concepts such as ‚Äú Inception‚Äù, ‚Äúscience Ô¨Åction Ô¨Ålm ‚Äù and\n‚ÄúChristopher Nolan‚Äù can be extracted from the sentence. In\ngenerative pre-training, candidate spans are randomly se-\nlected and masked (‚Äú Christopher Nolan ‚Äù in the example)\nto produce the masked sequence ¬Øx. Then the generator is\ntrained to recover the masked tokens. Here we use a 12-layer\nTransformer encoder as the architecture for the generator.\nIn practice, since the coverage of the knowledge span can\nvary in different sentences, we use a mixture of span mask-\ning and subword masking, while still keeping the total mask-\ning ratio of 15%. SpeciÔ¨Åcally, a probabilistic masking strat-\negy is designed to choose whether to mask a knowledge\nspan or a subword every time. For each sentence, knowl-\nedge spans (if available) or subwords are masked iteratively\nùíô\nGenerator Discriminator\nK-Replace\nùíô ùíì\n ùíô\nK-Mask\n ùíô\n ùíô‚Ä≤\nRoBERTa\n ùíô\nDiscriminator\nK-Replace\nùíì\n ùíô\nùíô\nGenerator\nK-Mask\n ùíô\nùíô\n(a) Two-tower\nDiscriminator\nK-Replace\nùíì\n ùíô\nùíô\nGenerator\nK-Mask\n ùíô\nùíô (b) Pipeline\nFigure 3: Knowledge-guided pre-training schemes.\nK-MASK refers to the knowledge span masking and\nK-Replace refers to the knowledge span replacement.\nGenerator works for masked words prediction and discrim-\ninator works for span replacement detection. Double-sided\narrow between the generator and discriminator means their\nparameters can be shared.\nby following the strategy until reaching the masking ratio of\n15%. We denote the whole knowledge masking procedure\nas K-MASK in the paper.\n3.3 Discriminative Pre-training\nWe use the same knowledge spans extraction method as in\ngenerative pre-training. Once the spans are identiÔ¨Åed, we\nrandomly replace them with other pieces of text, and fed the\naltered sequences ÀÜxto the discriminator to distinguish which\ntokens are replaced (ri = 1) and which are not (ri = 0).\nFor example, in the right part of Figure 2, the span\n‚ÄúChristopher Nolan‚Äù in the sentence is selected and replaced\nby ‚ÄúSteven Spielberg‚Äù, and the model learns to detect the\n‚Äúfake‚Äù tokens in the sequence.\nFor each token, the probability of being replaced is com-\nputed as\np(ri|ÀÜx; Œ∏d) = sigmoid(wT GeLU(Wh(ÀÜxi)), (5)\nwhere h(ÀÜxi) is the contextualized representation ofÀÜxi by the\ndiscriminator, which is also a 12-layer Transformer encoder.\nIn order to make the model more discriminative to the fac-\ntual knowledge, we replace the knowledge spans with other\nspans of the same type. In addition to the knowledge span\nsubstitution, we also randomly replace some of the subwords\nwith random tokens. The total replacement ratio of a sen-\ntence is kept at 15%. We denote the whole knowledge span\nreplacement process as K-Replace in the paper.\n3.4 Learning Schemes\nIn this work, generative and discriminative pre-training are\ncombined to promote the integration of knowledge into the\nlanguage model. We design two learning schemes, which are\nreferred to as thetwo-tower scheme and thepipeline scheme,\nrespectively.\nDataset Facts Relations Subwords\nLAMA-Google-RE 4618 3 =1\nLAMA-T-REx 29521 41 =1\nLAMA-UHN-Google-RE 4102 3 =1\nLAMA-UHN-T-REx 23220 41 =1\nLAMA-Google-RE‚àó 909 3 >1\nLAMA-T-REx‚àó 4514 41 >1\nLAMA-UHN-Google-RE‚àó 829 3 >1\nLAMA-UHN-T-REx‚àó 3886 40 >1\nTable 2: Datasets for cloze-style question answering. ‚ÄúSub-\nwords‚Äù means the number of subwords within the answer.\n‚àódenotes that samples with more than one subword in the\nanswer.\nFigure 3(a) illustrates the two-tower architecture, where\nthe generator and discriminator are simply trained in paral-\nlel, and their Transformer parameters are shared.\nFigure 3(b) shows the pipeline model, which connects the\noutput of the generator to the discriminator. More precisely,\nthe original sequence is Ô¨Årst masked and then recovered by\nthe generator. After applying knowledge span replacement\non the recovered sequence, the discriminator learns to tell\nwhich tokens are changed from the original input, including\nthose by the generator and the span replacement. The param-\neter can also be shared across generator and discriminator.\nIn this paper, we denote KgPLM T for the two-tower\nmodel and KgPLMP for the pipeline model.\n4 Experiments\n4.1 Pre-training Setup\nPre-training large-scale language model from scratch is\nexpensive, so we use the RoBERTa BASE implemented by\nHuggingface1 as the initialization of our model. We fol-\nlow the model conÔ¨Åguration in RoBERTaBASE and continue\nthe knowledge-guided pre-training on English Wikipedia 2\nfor 5 epochs. Details of the knowledge span extraction\nfrom Wikipedia is shown in Sec A.1. Different from ran-\ndomly masking a certain number of subwords in general\npre-training, knowledge-related spans are not uniformly dis-\ntributed in the corpus. Therefore, we use dynamic span\nmasking to select the knowledge spans, thus different spans\nare learned in different pre-training epochs. Furthermore, we\nemploy dynamic span replacement to choose different neg-\native knowledge spans for each epoch. The details of pre-\ntraining experimental settings are described in Sec A.2.\n4.2 Cloze-style Question Answering\nDatasets To examine the model‚Äôs ability to memorize and\ncomplete the factual knowledge stored in natural language\nstatements, we use cloze-style question answering bench-\nmark LAMA3 (Petroni et al. 2019) and LAMA-UHN (Po-\nerner, Waltinger, and Sch¬®utze 2019) to evaluate our models.\n1https://github.com/huggingface/\ntransformers.\n2https://hotpotqa.github.io/wiki-readme.\nhtml.\n3https://github.com/facebookresearch/LAMA.\nLARGE Models BASE Models\nDataset RoBERTaLARGE\n‚Ä† K-ADAPTER‚Ä† RoBERTaLARGE RoBERTaBASE WKLM‚Ä° KgPLMT KgPLMP\nLAMA-Google-RE 4.8 7.0 4.8 5.3 6.0 10.5 9.2\nLAMA-T-REx 27.1 29.1 28.1 21.7 22.5 31.1 27.9\nLAMA-UHN-Google-RE 2.5 3.7 2.5 2.2 3.0 5.0 4.9\nLAMA-UHN-T-REx 20.1 23.0 21.1 14.5 16.6 22.7 20.4\nLAMA-Google-RE‚àó - - 0.4 0.1 0.0 6.1 5.7\nLAMA-T-REx‚àó - - 8.8 5.2 1.3 18.5 15.8\nLAMA-UHN-Google-RE‚àó - - 0.4 0.1 0.0 2.7 2.7\nLAMA-UHN-T-REx‚àó - - 2.1 0.7 0.2 9.0 7.5\nTable 3: Performance (Macro P@1) on cloze-style question answering of factual knowledge. ‚àódenotes that samples with only\none subword in the answer are Ô¨Åltered. ‚Ä†denotes that these results are copied from the original paper (Wang et al. 2020). ‚Ä°\nmeans the model is re-implemented and trained by us.\nLAMA-UHN is a subset of LAMA, which is constructed\nby Ô¨Åltering out samples in LAMA that are easy to answer\nfrom entity names alone. On these datasets, we can sim-\nply use masked subwords prediction to answer questions.\nFor example, the question is ‚Äú Christopher Nolan was born\nin [MASK] . ‚Äù, and the target is to predict the masked an-\nswer ‚ÄúLondon‚Äù. We do our experiments on LAMA-Google-\nRE, LAMA-T-REx, LAMA-UHN-Google-RE and LAMA-\nUHN-T-REx, which focus on factual knowledge. In our\nmodels, a byte-level Byte-Pair Encoding (BPE) vocabulary\nis used to tokenize the input sequence, which makes some\nanswers are split into multiple subwords. In order to eval-\nuate the models under the multi-subword setting, we Ô¨Ål-\nter samples with only one subword in the answer and con-\nstruct LAMA-Google-RE‚àó, LAMA-T-REx‚àó, LAMA-UHN-\nGoogle-RE‚àó and LAMA-UHN-T-REx ‚àó. The statistics of\nthese datasets are listed in Table 2. Similar to (Wang et al.\n2020), we remove stop words from the ranking vocabulary.\nSettings We use precision at one (P@1) to evaluate the\npredictions. For the multi-subword answer, the sample is\nconsidered as correct when all the subwords within the an-\nswer are predicted correctly. Macro-averaged P@1 over dif-\nferent relations is reported as the Ô¨Ånal score. In our exper-\niments, data preprocessing and hyperparameter settings re-\nmain the same as (Petroni et al. 2019).\nBaselines We compare our models with RoBERTa and\ntwo knowledge-guided pre-trained language models:\n‚Ä¢ RoBERTa (Liu et al. 2019) Compared with BERT,\nRoBERTa replaces the static masking strategy with dy-\nnamic masking , and is pre-trained over more data and\nlonger steps. Moreover, the next sentence prediction pre-\ntraining task is removed.\n‚Ä¢ K-ADAPTER (Wang et al. 2020) Knowledge triples are\naligned with natural text through a distant supervision\nmethod, and these natural text is utilized to inject knowl-\nedge into the language model by a relation classiÔ¨Åcation\ntask.\n‚Ä¢ WKLM (Xiong et al. 2020) We re-implement the WKLM\nmodel, which supplements the pre-training objective by\nidentifying whether an entity in a certain context is re-\nplaced or not. Unlike that in the original paper, we use the\nDatasets # Train # Dev\nSQuAD 86,588 10,507\nNewsQA 74,160 4,212\nTriviaQA 61,688 7,785\nSearchQA 117,384 16,980\nHotpotQA 72,928 5,901\nNatural Questions 104,071 12,836\nTotal 516,819 58,221\nTable 4: MRQA datasets.\nmore powerful model RoBERTaBASE to be the initializa-\ntion. We follow the pre-training settings used in (Xiong\net al. 2020), and continue the corresponding knowledge-\nguided pre-training on English Wikipedia for 20 epochs.\nResults Table 3 shows the results on cloze-style ques-\ntion answering of factual knowledge. We observe that Kg-\nPLM outperforms RoBERTa BASE by a large margin, even\noutperforms the LARGE models. For LAMA-Google-RE\nand LAMA-T-REx, WKLM improves 0.7 and 0.8 on P@1\nover RoBERTaBASE, while KgPLM T achieves much larger\ngains of 5.2 and 9.4 on P@1. Moreover, compared to\nRoBERTaBASE and WKLM, KgPLM performs much better\non LAMA-UHN datasets. This indicates that KgPLM really\ncaptures richer factual knowledge than the baseline models.\nWe also check the performance comparison in each rela-\ntion type (not listed due to the limited space) and Ô¨Ånd that\nKgPLM outperforms RoBERTa in most relation types, indi-\ncating that KgPLM improves over a wide range of factual\nknowledge.\nAll the answers in LAMA ‚àó and LAMA-UHN ‚àó contain\nmore than one subword, and these datasets can be used to\ndetect model‚Äôs ability on capturing span-level knowledge.\nFrom Table 3, we can see that the P@1 scores of RoBERTa\non LAMA‚àóand LAMA-UHN‚àódrop dramatically, compar-\ning to those on LAMA and LAMA-UHN. WKLM also\nworks not well on LAMA ‚àó and LAMA-UHN ‚àó, and one\nof the possible reasons is that the span-level pre-training\nobjective in WKLM is discriminative but cloze-style ques-\ntion answering is a generative task. Similarly, the perfor-\nmance of KgPLM decreases but with much smaller decline,\nand eventually KgPLM T exceeds 6.0, 13.3, 2.6 and 8.3\nNewsQA TriviaQA SearchQA HotpotQA NaturalQA Avg.\nModel EM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nBERTLARGE‚Ä† - 68.8 - 77.5 - 81.7 - 78.3 - 79.9 - 77.3\nSpanBERTLARGE‚Ä† - 73.6 - 83.6 - 84.8 - 83.0 - 82.5 - 81.5\nSpanBERTBASE 56.27 70.92 74.93 79.77 77.99 83.73 64.78 80.10 69.07 80.35 68.61 78.97\nRoBERTaBASE 57.06 71.11 73.80 78.18 78.56 84.11 63.97 79.37 68.46 79.97 68.37 78.55\nWKLM‚Ä° 57.16 71.51 74.93 79.45 78.73 84.25 63.83 79.60 69.35 80.54 68.80 79.07\nKgPLMT (5 epochs) 57.11 71.69 74.93 79.67 79.07 84.55 64.61 79.76 69.88 80.99 69.12 79.33\nKgPLMP (5 epochs) 57.30 72.37 75.37 79.74 79.02 84.60 64.20 80.03 69.87 80.95 69.15 79.54\nKgPLMP (10 epochs) 57.65 71.86 75.42 79.93 78.82 84.35 64.75 79.97 69.79 80.94 69.29 79.41\nTable 5: Test results of the MRQA-single setting. ‚Ä†denotes that these results are copied from the original paper (Joshi et al.\n2020a). ‚Ä° means the model is re-implemented and trained by us. ‚ÄúNaturalQA‚Äù is short for ‚ÄúNatural Questions‚Äù, hereinafter\nreferred to as ‚ÄúNaturalQA‚Äù.\nP@1 score over RoBERTa on LAMA-Google-RE‚àó, LAMA-\nT-REx‚àó, LAMA-UHN-Google-RE ‚àó and LAMA-UHN-T-\nREx‚àó, respectively. These experimental results demonstrate\nthat span-level knowledge-guided pre-training indeed en-\nhances the model from perspective of span-level knowledge\ncompletion.\n4.3 Machine Reading for Question Answering\nDatasets We evaluate the knowledge-guided language\nmodels on six question answering datasets from the MRQA\nshared task (Fisch et al. 2019). These datasets follow the task\nsetting of machine reading comprehension: given a question\nand a paragraph, the objective is to extract a text span from\nthe paragraph to be the Ô¨Ånal answer. In this paper, we train\nour models on the in-domain training sets and evaluate on\nthe in-domain development sets.\nSettings Table 4 lists the data statistics, and the experi-\nments are conducted in two settings: (1) MRQA-single, we\nfollow the dataset setting used in (Joshi et al. 2020a), which\nsplits the in-domain development set in half to generate the\ntest set, and individual models are trained and evaluated on\neach dataset; (2) MRQA-combine, the training sets of differ-\nent datasets are combined to train a single model, and the\nbest checkpoint is selected by evaluating the entire develop-\nment set. Exact match (EM) and word-level F1 scores are\nused to evaluate model performance. For the Ô¨Årst Ô¨Ånetuning\nsetting, by referring to (Joshi et al. 2020a), we search the\nbest hyperparameters for each dataset. For the second set-\nting, because the training set is large, the chosen hyperpa-\nrameters in (Joshi et al. 2020b) are directly utilized to Ô¨Åne-\ntune our models. See Sec A.4 for Ô¨Ånetuning details.\nBaselines In addition to RoBERTa and WKLM (men-\ntioned in 4.2), we compare our models with two other base-\nlines, and all these models are BASE models:\n‚Ä¢ SpanBERT (Joshi et al. 2020a) This model extends\nsubword-level masking to span-level masking, and selects\nrandom spans of full words to predict. In addition to the\nMLM objective, a span boundary objective is designed to\npredict each subword within a span using subword rep-\nresentations at the boundaries. In this work, the released\nModel EM F1\nRoBERTaBASE‚Ä† - 82.98\nRoBERTaBASE++‚Ä† - 83.2\nTEKF ‚Ä† - 83.44\nTEKP ‚Ä† - 83.3\nTEKPF ‚Ä† - 83.71\nKgPLMT (5 epochs) 75.32 83.72\nKgPLMP (5 epochs) 75.29 83.72\nKgPLMP (10 epochs) 75.20 83.70\nTable 6: Dev results of theMRQA-combine setting. ‚Ä†denotes\nthat these results are copied from the original paper (Joshi\net al. 2020b).\nspanbert-base-cased model4 is utilized to do the\nexperiments.\n‚Ä¢ TEK (Joshi et al. 2020b) Retrieved background sentences\nfor phrases in the context, called ‚Äútextual encyclopedic\nknowledge‚Äù, are utilized to extend the input text. The\nTEK-enriched context can be used for model pre-training\nand Ô¨Ånetuning.\nResults Table 5 shows the test results of the MRQA-\nsingle setting. RoBERTa falls behind the previous state-of-\nthe-art model SpanBERT. In these Ô¨Åve datasets, TriviaQA\nshows the most signiÔ¨Åcant performance gap, where Span-\nBERT exceeds RoBERTa 1.59 F1 score. 92.85% of the an-\nswers in TriviaQA are Wikipedia titles (Joshi et al. 2017)\nand the pre-training steps of SpanBERT on Wikipedia are\nmuch larger than RoBERTa, which make SpanBERT so\nwell-performed. Based on a knowledge-guided discrimina-\ntive objective, our reproduced WKLM improves over the\nRoBERTa baseline, achieving a competitive performance\nwith SpanBERT. Among the BASE models, KgPLM out-\nperforms all the baselines in averaged F1 score. Compared\nwith RoBERTa, KgPLMP achieves improvements in each\ndataset, increasing the F1 score by 1.26, 1.56 and 0.98 on\nNewsQA, TriviaQA and Natural Questions, respectively.\n4https://github.com/facebookresearch/\nSpanBERT/.\nModel NewsQA TriviaQA SearchQA HotpotQA NaturalQA Avg.\nKgPLMT 71.69 79.67 84.55 79.76 80.99 79.33\nw/o K-Replace 71.88 79.53 84.35 79.68 80.73 79.23\nKgPLMP 72.37 79.74 84.60 80.03 80.95 79.54\nw/o K-Replace 71.56 79.33 84.18 79.51 80.94 79.10\nTable 7: Performance (F1) on MRQA test sets for different pre-training schemes.\nModel NewsQA TriviaQA SearchQA HotpotQA NaturalQA Avg.\nRoBERTaBASE 71.11 78.18 84.11 79.37 79.97 78.55\nRoBERTa+\nBASE 71.43 79.31 84.27 79.89 80.51 79.08\nKgPLMP 72.37 79.74 84.60 80.03 80.95 79.54\nw/o LGen 71.92 78.40 84.24 79.81 80.70 79.01\nw/o LDisc 71.99 78.74 84.11 79.71 80.91 79.09\nTable 8: Ablation study of continue general MLM pre-training and do knowledge-guided pre-training with different objectives.\nThese experimental results indicate that our knowledge-\nguided pre-training method (combination ofknowledge span\nmasking and knowledge span replacement checking ) en-\nhances the model‚Äôs capacity in absorbing knowledge and\nbeneÔ¨Åts the downstream QA tasks.\nTable 10 shows the dev results of the MRQA-combine\nsetting. Comparing with TEK models, KgPLM achieves\ncomparable F1 score on the combined development set of\nMRQA datasets. In (Joshi et al. 2020b), the TEK-enriched\ncontext beneÔ¨Åts pre-training and Ô¨Ånetuning. In our work, we\nhaven‚Äôt introduce any data augmentation in pre-training, so\nthe the TEK-enriched context will presumably also help our\nmodels. If we enrich the inputs with TEK in our pre-training\nand Ô¨Ånetuning procedure, there may still exists some room\nfor improvement based on the current model. We leave this\nfor our future work.\n5 Discussion\n5.1 Pre-training Schemes\nIn addition to the two-tower and pipeline pre-training\nschemes proposed in Sec 3.4, we explore another two\nschemes by removing the K-Replace module. Without\nknowledge span replacement, the output of the generator\nin the pipeline-based model is fed into the discriminator di-\nrectly, which brings a great challenge to the learning of the\ndiscriminator. The intermediate sequence recovered by the\ngenerator may destroy the grammatical structure of the sen-\ntence while corrupting the integrity of the knowledge span\nitself, which is not a good learning sample for the discrimi-\nnator. As shown in Table 7, this scheme (Row 4) achieves\nmuch lower averaged score on MRQA test sets than the\noriginal pipeline scheme (Row 3). The comparison of the\ntwo-tower-based models (Row 1 and 2) also shows that the\nK-Replace module is beneÔ¨Åcial to the learning of knowl-\nedge. These experimental results demonstrate the impor-\ntance of theK-Replace module in our models. Knowledge\nspan replacement ensures the integrity of negative knowl-\nedge spans in the input, and do not destroy the grammat-\nical structure of the input sentence. Moreover, the uncor-\nrupted negative knowledge spans improve the learning ef-\nfectiveness of knowledge span replacement checking in the\ndiscriminator.\n5.2 Continue Pre-training\nTo make a fair comparison with knowledge-guided pre-\ntraining, we continue pre-training based on the released\nroberta-base checkpoint via the general MLM ob-\njective on Wikipedia. We call this extended model\nRoBERTa+\nBASE. Table 8 shows comparison between do\nknowledge-guided pre-training and continue general pre-\ntraining. From row 1 and 2, we can see that continue pre-\ntraining on Wikipedia improves averaged F1 score of the\nMRQA test sets by 0.53, which gains largest on TriviaQA,\nfollowed by NaturalQA and HotpotQA. Compared with\nRoBERTa+\nBASE, KgPLM exhibits better performance on all\ndatasets, indicating that our method is more effective than\ngeneral MLM in learning knowledge from text.\n5.3 Effects of Generator and Discriminator\nWe conduct further analysis to differentiate the effects of\nthe generator and the discriminator in our models. Based\non the pipeline-based model KgPLM P , we train another\ntwo models: KgPLM P w/o the generator, and KgPLM P\nw/o the discriminator. The ablation results are shown in Ta-\nble 8. After removing each knowledge injection module,\nthe model performance decreases, which indicates that these\ntwo knowledge-guided pre-training tasks are complemen-\ntary to each other in knowledge consolidation.\n6 Conclusion\nWe have proposed a pre-training method by cooperatively\nmodeling the generative and discriminative knowledge in-\njecting approaches. Our model can be easily extended to\nlarger pre-training corpus and does not introduce any mod-\niÔ¨Åcations for downstream tasks during Ô¨Ånetuning. Experi-\nments show our model consistently outperforms all BASE\nmodels on a variety of question answering datasets, demon-\nstrating that our KgPLM is a preferred choice for the knowl-\nedge intensive NLP tasks.\nReferences\nClark, K.; Luong, M.-T.; Le, Q. V .; and Manning, C. D.\n2020. ELECTRA: Pre-training Text Encoders as Discrim-\ninators Rather Than Generators. In ICLR. URL https:\n//openreview.net/pdf?id=r1xMH1BtvB.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J. G.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978‚Äì2988.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171‚Äì4186.\nFisch, A.; Talmor, A.; Jia, R.; Seo, M.; Choi, E.; and Chen,\nD. 2019. MRQA 2019 Shared Task: Evaluating Generaliza-\ntion in Reading Comprehension. In Proceedings of the 2nd\nWorkshop on Machine Reading for Question Answering, 1‚Äì\n13.\nGhaddar, A.; and Langlais, P. 2017. WiNER: A Wikipedia\nannotated corpus for named entity recognition. In Proceed-\nings of the Eighth International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), 413‚Äì422.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020a. Spanbert: Improving pre-training by\nrepresenting and predicting spans. Transactions of the Asso-\nciation for Computational Linguistics 8: 64‚Äì77.\nJoshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017.\nTriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension. In Proceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 1601‚Äì1611.\nJoshi, M.; Lee, K.; Luan, Y .; and Toutanova, K. 2020b.\nContextualized representations using textual encyclopedic\nknowledge. arXiv preprint arXiv:2004.12006 .\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\nLauscher, A.; Majewska, O.; Ribeiro, L. F.; Gurevych,\nI.; Rozanov, N.; and Glava Àás, G. 2020. Common Sense\nor World Knowledge? Investigating Adapter-Based Knowl-\nedge Injection into Pretrained Transformers. arXiv preprint\narXiv:2005.11787 .\nLauscher, A.; Vuli ¬¥c, I.; Ponti, E. M.; Korhonen, A.;\nand Glava Àás, G. 2019. Informing unsupervised pretrain-\ning with external linguistic knowledge. arXiv preprint\narXiv:1909.02339 .\nLevine, Y .; Lenz, B.; Dagan, O.; Padnos, D.; Sharir, O.;\nShalev-Shwartz, S.; Shashua, A.; and Shoham, Y . 2019.\nSensebert: Driving some sense into bert. arXiv preprint\narXiv:1908.05646 .\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach.\nLogan, R.; Liu, N. F.; Peters, M. E.; Gardner, M.; and Singh,\nS. 2019. Barack‚Äôs Wife Hillary: Using Knowledge Graphs\nfor Fact-Aware Language Modeling. In Proceedings of the\n57th Annual Meeting of the Association for Computational\nLinguistics, 5962‚Äì5971.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextual-\nized word representations. In Proceedings of NAACL-HLT,\n2227‚Äì2237.\nPeters, M. E.; Neumann, M.; Logan, R.; Schwartz, R.; Joshi,\nV .; Singh, S.; and Smith, N. A. 2019. Knowledge Enhanced\nContextual Word Representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n43‚Äì54.\nPetroni, F.; Rockt¬®aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language Models as\nKnowledge Bases? In EMNLP.\nPoerner, N.; Waltinger, U.; and Sch ¬®utze, H. 2019. Bert\nis not a knowledge base (yet): Factual knowledge vs.\nname-based reasoning in unsupervised qa. arXiv preprint\narXiv:1911.03681 .\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training .\nRoberts, A.; Raffel, C.; and Shazeer, N. 2020. How Much\nKnowledge Can You Pack Into the Parameters of a Language\nModel? arXiv preprint arXiv:2002.08910 .\nRosset, C.; Xiong, C.; Phan, M.; Song, X.; Bennett, P.; and\nTiwary, S. 2020. Knowledge-Aware Language Model Pre-\ntraining. arXiv preprint arXiv:2007.00655 .\nSun, Y .; Wang, S.; Li, Y .; Feng, S.; Tian, H.; Wu, H.; and\nWang, H. 2020. Ernie 2.0: A continual pre-training frame-\nwork for language understanding. In AAAI.\nTalmor, A.; Elazar, Y .; Goldberg, Y .; and Berant, J. 2019.\noLMpics‚ÄìOn what Language Model Pre-training Captures.\narXiv preprint arXiv:1912.13283 .\nWang, R.; Tang, D.; Duan, N.; Wei, Z.; Huang, X.; Cao, C.;\nJiang, D.; Zhou, M.; et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv preprint\narXiv:2002.01808 .\nWang, X.; Gao, T.; Zhu, Z.; Liu, Z.; Li, J.; and Tang, J.\n2019. KEPLER: A uniÔ¨Åed model for knowledge embed-\nding and pre-trained language representation. arXiv preprint\narXiv:1911.06136 .\nXiong, W.; Du, J.; Wang, W. Y .; and Stoyanov, V . 2020.\nPretrained Encyclopedia: Weakly Supervised Knowledge-\nPretrained Language Model. In ICLR.\nYe, Z.-X.; Chen, Q.; Wang, W.; and Ling, Z.-H. 2019. Align,\nmask and select: A simple method for incorporating com-\nmonsense knowledge into language representation models.\narXiv preprint arXiv:1908.06725 .\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q.\n2019. ERNIE: Enhanced Language Representation with In-\nformative Entities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 1441‚Äì\n1451.\nZhou, J.; Zhang, Z.; and Zhao, H. 2019. LIMIT-\nBERT: Linguistic informed multi-task bert. arXiv preprint\narXiv:1910.14296 .\nA Appendices\nA.1 Knowledge Span\nInspired by WiNER (Ghaddar and Langlais 2017), we ex-\ntract knowledge spans for each article in Wikipedia by the\nfollowing steps:\n1. Given an article D, all the anchored strings are extracted\nas knowledge spans, then all these anchored strings are\nadded into a span set S.\n2. All these anchors have an out-linked article. We collect\nthe anchored strings in these out-linked articles (2-hop\nanchor) and add them into S.\n3. Most Wikipedia title has a Wikidata item, and aliases\nmay exist for this item. We extend aliases for each knowl-\nedge span in S, and utilize the Ô¨Ånal span set to extract\nknowledge spans in the article D.\nAfter extracting knowledge spans for each article in\nWikipedia, the statistics are shown in Table 9.\nSpan Source # Article # KS Avg./Max.\nAnchor 5,486,211 97,167,509 17.7/9069\n+ 2-hop 5,486,211 133,886,575 24.4/9379\n+ Alias 5,486,211 146,865,593 26.8/9379\nTable 9: Statistics of knowledge spans in Wikipedia. KS,\nknowledge span.\nA.2 Pre-training Details\nAs in RoBERTa, we set the max sequence length to 512, and\ncontinue pre-training for 5 epochs on English Wikipedia.\nThe weight for the discriminative objectiveŒªis set to 25. We\nuse the AdamW (Kingma and Ba 2014) optimizer with the\nlearning rate of 2e-5. The sampling ratio to choose whether\nto mask a knowledge span or a subword is set to 0.5. To ac-\ncelerate the pre-training process, we follow the balanced\ndata parallel strategy used in Transfomer-XL (Dai\net al. 2019) and pre-train the models on 8 V100 GPUs with a\nbatch size of 84. The models following the two-tower struc-\nture cost 7 days to complete the pre-training, and those using\nthe pipeline structure take 3.5 days.\nA.3 Finetuning Details\nWe utilize the script5 for MRQA in SpanBERT to Ô¨Ånetune\nour models, and the difference are: the wordpiece-based to-\nkenizer is replaced by the byte-level BPE-based tokenizer,\nand Huggingface‚Äôs default optimizer AdamW is used. We\ntry max seq lenth 384 and 512. For the Ô¨Årst Ô¨Ånetuning\nsetting described in Sec 4.3, the hyperparameters are se-\nlected from learning rates {1e-5, 2e-5, 3e-5, 5e-5}and batch\nsizes {16, 32}, and the pre-trained model is Ô¨Ånetuned for 4\nepochs on a single V100 GPU. For the second setting, be-\ncause the training set is large, we directly run experiments\nusing the chosen hyperparameters in (Joshi et al. 2020b) on\n4 V100 GPUs. The learning rate, batch size and epoch are\n2e-5, 32 and 3, respectively.\n5https://github.com/facebookresearch/\nSpanBERT/blob/master/code/run_mrqa.py.\nA.4 Results on SQuAD 2.0\nSQuAD 2.0\nModel EM F1\nRoBERTaBASE‚Ä† 80.5 83.7\nSpanBERTBASE‚Ä† - 83.6\nELECTRABASE‚Ä† 80.5 -\nWKLM‚Ä° 80.4 83.4\nKgPLMP (5 epochs) 81.1 84.3\nTable 10: Dev results on SQuAD 2.0.‚Ä†denotes that these re-\nsults are copied from the original paper (Joshi et al. 2020b).\n‚Ä°means the model is re-implemented and trained by us.\nA.5 Comparison with ELECTRA\nThis work is indeed inspired by great previous work includ-\ning ELECTRA, SpanBERT and WKLM. However, there\nare also signiÔ¨Åcant differences. ELECTRA is a general pre-\ntraining model while our KgPLM speciÔ¨Åcally targets inte-\ngrating factual knowledge in the pre-training process. The\ndesign of the knowledge masking (K-Mask in Figure 3) and\nknowledge replacement (K-Replace in Figure 3) are key in-\ngredients in the process while not in ELECTRA. Moreover,\nin ELECTRA, the discriminator is of different size from the\ngenerator, while in this work their parameters are shared.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7838184833526611
    },
    {
      "name": "Discriminative model",
      "score": 0.6854761242866516
    },
    {
      "name": "Language model",
      "score": 0.6552702784538269
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.5947310328483582
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5866953730583191
    },
    {
      "name": "Generative grammar",
      "score": 0.5496387481689453
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.531455397605896
    },
    {
      "name": "Discriminator",
      "score": 0.5163651704788208
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5107336044311523
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5079546570777893
    },
    {
      "name": "Perplexity",
      "score": 0.46885618567466736
    },
    {
      "name": "Question answering",
      "score": 0.4581616520881653
    },
    {
      "name": "Machine translation",
      "score": 0.44915643334388733
    },
    {
      "name": "Natural language processing",
      "score": 0.43839046359062195
    },
    {
      "name": "Machine learning",
      "score": 0.39202380180358887
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}