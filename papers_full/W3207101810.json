{
  "title": "Adversarial Token Attacks on Vision Transformers",
  "url": "https://openalex.org/W3207101810",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5056315092",
      "name": "Ameya Joshi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002536242",
      "name": "Gauri Jagatap",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066142047",
      "name": "Chinmay Hegde",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2401231614",
    "https://openalex.org/W3092171228",
    "https://openalex.org/W2963726920",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2964238361",
    "https://openalex.org/W3202088435",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2985390828",
    "https://openalex.org/W2963343288",
    "https://openalex.org/W2962729158",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3082548577",
    "https://openalex.org/W2970364106",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2808031418",
    "https://openalex.org/W2971109239",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W3044541995",
    "https://openalex.org/W2460937040",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2883285025",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3036294592",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3094984771",
    "https://openalex.org/W2985282977",
    "https://openalex.org/W2963496101",
    "https://openalex.org/W2773726006",
    "https://openalex.org/W2911634294",
    "https://openalex.org/W3122730565",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3176237058",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W2963062382",
    "https://openalex.org/W2963143631",
    "https://openalex.org/W2783784437",
    "https://openalex.org/W2964197269",
    "https://openalex.org/W2180612164",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W2905744087"
  ],
  "abstract": "Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in robust accuracy for single token attacks.",
  "full_text": "ADVERSARIAL TOKEN ATTACKS ON VISION TRANSFORMERS\nAmeya Joshi Gauri Jagatap Chinmay Hegde\nNew York University\n{ameya.joshi, gbj221, chinmay.h}@nyu.edu\nABSTRACT\nVision transformers rely on a patch token based self attention mechanism, in contrast to convolutional\nnetworks. We investigate fundamental differences between these two families of models, by designing\na block sparsity based adversarial token attack. We probe and analyze transformer as well as\nconvolutional models with token attacks of varying patch sizes. We infer that transformer models are\nmore sensitive to token attacks than convolutional models, with ResNets outperforming Transformer\nmodels by up to ∼30% in robust accuracy for single token attacks.\n1 Introduction\n1.1 Motivation\nConvolutional networks (CNNs) have shown near human performance in image classiﬁcation [1] over non-structured\ndense networks. However, CNNs are vulnerable to speciﬁcally designed adversarial attacks [ 2]. Several papers in\nadversarial machine learning literature reveal the brittleness of convolutional networks to adversarial examples. For\nexample, gradient based methods [3, 4] design a perturbation by taking steps proportional to the gradient of the loss of\nthe input image xin a given ℓp neighborhood. This has led to reﬁned robust training approaches, or defenses, which\ntrain the network to see adversarial examples during the training stage and produce the unaltered label corresponding to\nit [5, 6].\nVision transformers (ViT) were recently introduced [7], as a new network architecture inspired by transformers [ 8]\nwhich have been successfully used for modeling language data. ViTs rely on self attention [8], a mechanism that allows\nthe network to ﬁnd correlations between spatially separated parts of the input data. In the context of vision, these are\nsmall non-overlapping patches which serve as tokens to the transformer. ViTs and more recently distillation based Data\nEfﬁcient Image Transformers (DeIT) [9] have shown to have competitive performance on classiﬁcation tasks and rely\non pre-training on very large datasets. It is of imminent interest to therefore study the robustness of self-attention based\nnetworks.\nThere has been some preliminary work on adversarial robustness of vision transformers. [10] show that under certain\nregimes, vision transformers are at least as robust to ℓ2 and ℓ∞PGD attacks as ResNets. While ℓ2 and ℓ∞threat models\nare useful in understanding fundamental properties of deep networks, they are not realizable in the real world and do\nnot capture actual threats. Transformer based networks also introduce the need for tokenizing the image, leading to\nan encoded bias in the input. It is therefore important to understand the sensitivity of the architecture to token level\nchanges rather than to the full image.\nSpeciﬁcally, we attempt to answer: Are transformers robust to perturbations to a subset of the input tokens? We present\na systemic approach to answer this query by constructing token level attacks by leveraging block sparsity constraints.\n1.2 Our contributions\nIn this paper, we propose a patch based block sparse attack where the attack budget is deﬁned by the number of tokens\nthe attacker is allowed to perturb. We identify top salient pixels using the magnitude of their loss gradients and perturb\nthem to create attacks. We extend a similar idea to block sparsity by constraining salient pixels to lie in non-overlapping\npatches. We probe three families of neural architectures using our token attack; self-attention (ViT [ 7], DeIT [9]),\nconvolutional (Resnets [11] and WideResNet [12]) and MLP based (MLP Mixer [13]).\narXiv:2110.04337v1  [cs.CV]  8 Oct 2021\nWe make the following contributions and observations:\n1. We propose a new attack which imposes block sparsity constraints, allowing for token attacks for Transformers.\n2. We show classiﬁcation performance of all architectures on token attacks of varying patch sizes and number of\npatches.\n3. We demonstrate that for token attacks matching the architecture token size, vision transformers are less resilient to\ntoken attacks as compared to MLP Mixers and ResNets.\n4. For token attacks smaller than architecture token size, vision transformers are comparably robust to ResNets.\n5. We also speciﬁcally note the shortcomings of previous studies on robustness of transformers [10], where ViTs are\nshown to be more robust than ResNets.\n6. With our token attacks we can break Vision transformers using only 1% of pixels as opposed to ℓ2 or ℓ∞attacks\nwhich rely on perturbing all image pixels.\nWe therefore motivate designing attacks adaptively modeled after neural architectures.\n1.3 Related work\nThreat models:Deep networks are vulnerable to imperceptible changes to input images as deﬁned by the ℓ∞distance\n[14]. There exist several test-time attack algorithms with various threat models: ℓp constrained [2, 4, 15], black-\nbox [16, 17], geometric attacks [18, 19], semantic and meaningful attacks [20, 21, 22] and data poisoning based [23].\nDefenses: Due to the vast variety of attacks, adversarial defense is a non-trivial problem. Empirical defenses as\nproposed by [5], [6], and [24] rely on adversarial data augmentation and modiﬁed loss functions to improve robustness.\nSeveral works [25, 26] propose preprocessing operations as defenses. However, such defenses often fail to counter\nadaptive attacks [27]. [28], [29] and [30] provide methods that guarantee robustness in terms of a volume around an\ninput. Such methods often fail or provide trivial certiﬁcates for larger networks, and large high resolution images.\nApart from algorithmic approaches, newer papers discuss optimal hyper-parameter tuning as well as combination of\nregularizers from aformentioned techniques, choice of activation functions, choice of architecture and data augmentation\nto extract best possible robust accuracies using pre-existing algorithms [31, 32].\nPatch attacks: Patch attacks [ 33] are practically realizable threat model. [ 34, 35, 36] have successfully attacked\ndetectors and classiﬁers with physically printed patches. In addition, [37, 37] also show that spatially limited sparse\nperturbations sufﬁce to consistently reduce the accuracy of classiﬁcation model. This motivates our analysis of the\nrobustness of recently invented architectures towards sparse and patch attacks.\nVision transformersWhile convolutional networks have successfully achieved near human accuracy on massive\ndatasets [1, 38], there has been a surge of interest in leveraging self-attention as an alternative approach. Transformers [8]\nhave been shown to be extremely successful at language tasks [39, 40, 41]. [42] extend this for image data, where in\nthey use pixels as tokens. While they some success in generative tasks, the models had a large number of parameters\nand did not scale well. [7] improve upon this by instead using non-overlapping patches as tokens and show state of the\nart classiﬁcation performance on the ImageNet dataset. [ 9] further leverage knowledge distillation to improve efﬁciency\nand performance. Further improvements have been suggested by [ 43], [44] and [45] to improve performance using\narchitectural modiﬁcations, deeper networks and better training methods. In parallel, [ 13] instead propose a pure MLP\nbased architecture that achieves nearly equivalent results with faster training time. However, studies on generalization\nand robust performance of such networks is still limited. We discuss a few recent works below.\nAttacks on vision transformers:[10, 46] analyse the performance of vision transformers in comparison to massive\nResNets under various threat models and concur that vision transformers (ViT) are at least as robust as Resnets when\npretrained with massive training datasets. [47] show that adversarial examples do not transfer well between CNNs and\ntransformers, and build an ensemble based approach towards adversarial defense. [48] claims that Transformers are\nrobust to a large variety of corruptions due to attention mechanism.\n2 Token Attacks on Vision transformers\nThreat Model:We deﬁne the speciﬁc threat model that we consider in our analysis. Let x ∈Rd be a d-dimensional\nimage, and f : Rd →[m] be a classiﬁer that takes x as input and outputs one of mclass labels. For our attacks, we\nfocus on sparsity as the constraining factor. Speciﬁcally, we restrict the number of pixels or blocks of pixels that an\nattacker is allowed to change. We consider x as a concatenation of Bblocks [x1,... xb,..., xB], where each block is\nof size p. In order to construct an attack, the attacker is allowed to perturb up to K ≤Bsuch blocks for a K-token\nattack. We also assume a white-box threat model, that is, the attacker has access to all knowledge about the model\nincluding gradients and preprocessing. We consider two varying attack budgets. In both cases we consider a block\n2\nAlgorithm 1Adversarial Token Attack\nRequire: x0:Input image, f(.): Classiﬁer, y : Original label, K: Number of patches to be perturbed, p: Patch size. i ←0\n1: [b1 . . . bK]= Top-K of S(xb) =\n√∑\nxi∈xb\n⏐⏐⏐∂L(f(x,y))\n∂xi\n⏐⏐⏐\n2\n, ∀b.\n2: while dof(x) ̸= y OR MaxIter\n3: xbk = xbk + ∇xbk\nL; ∀bk ∈ {b1, . . . , bK}\n4: xbk = Project ϵ∞(xbk ) (optional)\n5: end while\nOriginal Adversarial (patch) Pertubation (patch) Adversarial (sparse) Pertubation (sparse)\nFigure 1: Patch and sparse attacks on transformers: The attack images are generated with a ﬁxed budget of 20\npatches of size 16 ×16, or 5120 pixels for sparse attack on vision transformer (ViT). Note that the perturbations are\nimperceptible. The third and ﬁfth columns shows the perturbations brightened 10 times.\nsparse token budget, where we restrict the attacker to modifying K patches or “tokens” (1) with an unconstrained\nperturbation allowed per patch (2) a “mixed norm” block sparse budget, where the pixelwise perturbation for each token\nis restricted to an ℓ∞ball with radius ϵdeﬁned as K,ϵ-attack.\nSparse attack:To begin, consider the simpler case of a sparse (ℓ0) attack. This is a special case of the block sparse\nattack with block size is one. Numerous such attacks have been proposed in the past (refer to appendix). The\ngeneral idea behind most such attacks is to analyse which pixels in the input image tend to affect the output the most\nS(xi) :=\n⏐⏐⏐∂L(f(x,y))\n∂xi\n⏐⏐⏐, where L(·) is the adversarial loss, and cis the true class predicted by the network. The next step\nis to perturb the top smost salient pixels for a s-sparse attack by using gradient descent to create the least amount of\nchange in the spixels to adversarially ﬂip the label.\nPatchwise token attacks:Instead of inspecting saliency of single pixel we check the norm of gradients of pixels\nbelonging to non-overlapping patches using patch saliency S(xb) :=\n√\n∑\nxi∈xb\n⏐⏐⏐∂L(f(x,y))\n∂xi\n⏐⏐⏐\n2\n, for all b∈{1,...B }.\nWe pick topKblocks according to patch saliency. The effective sparsity is thuss= K·p. These sequence of operations\nare summarized in Alg. 1.\nWe use non-overlapping patches to understand the effect of manipulating salient tokens instead of arbitrarily choosing\npatches. In order to further test the robustness of transformers, we also propose to look at the minimum number of\npatches that would required to be perturbed by an attacker. For this setup, we modify Alg. 1 by linearly searching over\nthe range of 1 to Kpatches.\nMixed-norm attacks:Most approaches [37, 49] additionally rely on a mixed ℓ2-norm based sparse attack in order to\ngenerate imperceptible perturbations. Motivated by this setting, we propose a mixed-norm version of our modiﬁed\nattack as well. In order to ensure that our block sparse attacks are imperceptible, we enforce an additional ℓ∞projection\nstep post the gradient ascent step. This is enforced via Step 4 in Alg. 1.\n3 Experiments and Results\nSetup: To ensure a fair comparison, we choose the best models for the Imagenet dataset [50] reported in [7], [9] and\n[12]. The models achieve near state-of-the-art results in terms of classiﬁcation accuracy. They also are all trained using\nthe best possible hyperparameters for each case. We use these weights and the shared models from the Pytorch\nImage models [51] repository. We restrict our analysis to a ﬁxed subset of 300 randomly chosen images from the\nImagenet validation dataset.\nModels: In order to compare the robustness of transformer models to standard CNNs, we consider three different\nfamilies of architectures:(1) Vision Transformer (ViT) [7], Distilled Vision Transformers (DeIT) [9], (2) Resnets [11, 12]\nand (3) MLP Mixer [13]. For transformers, [7] show that best performing Imagenet models have a ﬁxed input token size\nof 16 ×16. In order to ensure that the attacks are equivalent, we ensure that any norm or patch budgets are appropriately\n3\nscaled as per the pre-processing used 1. We also scale the ϵ-norm budget for mixed norm attacks to eight gray levels\nof the input image post normalization. Additionally, we do a hyper parameter search to ﬁnd the best attacks for each\nmodel analysed. Speciﬁc details can be found in the Appendix2.\n0.1 1 2 5 10 20 40\n0\n20\n40\n60\n80\n100\n(a) Token Budget(K),p= 16\nAccuracy of target model\n1 4 8 16\n0\n20\n40\n60\n80\n100\n(b) Patch Attack Sizes(p),K= 5\nAccuracy of target model\nViT-224\nViT-384\nDeIT\nDeIT-Distill\nMLP-Mixer\nResnet 50\nResnet 101\nWide-Resnet\nFigure 2: (a) Robustness to Token Attacks with varying budgets (p = 16). Vision transformers are less robust than MLP Mixer\nand ResNets against patch attacks with patch size matching token size of transformer architecture, (b) Token attacks with varying\npatch sizes.K = 5When the attack patch size is smaller than token size of architecture, vision transformers are comparably robust\nagainst patch attacks, to MLP and ResNets. Detailed results can be found in the Appendix\nPatch attacks:We allow the attacker a ﬁxed budget of tokens as per Algorithm 1. We use the robust accuracy as the\nmetric of robustness, where a higher value is better. We start with an attack budget of 1 token for an image size of\n224 ×224 for the attacker where each token is a patch of the size 16 ×16. In order to compensate for the differences\nin the size of the input, we scale the attack budget for ViT-384 by allowing for more patches ( 3 to be precise) to\nbe perturbed. However, we do not enforce any imperceptibility constraints. We run the attack on the ﬁxed subset\nof ImageNet for the network architectures deﬁned above. Fig. 2(a) shows the result of our analysis. Notice that\nTransformer architectures are more vulnerable to token attacks as compared to ResNets and MLP-Mixer. Further,\nViT-384 proves to be the most vulnerable, and ResNet-101 is the most robust model. DeiT which uses a teacher-student\nnetwork is more robust than ViTs. We therefore conclude that distillation improves robustness to single token attacks.\nVarying the Token budget:For this experiment, we start with a block-budget of 1 patch, and iterate upto 40 patches\nto ﬁnd the minimum number of tokens required to break an image. We then measure the robust accuracy for each\nconstraint and for each model. For this case, we only study attacks for a ﬁxed patch (token) size of 16 ×16 and\nrepresent our ﬁndings in Fig. 2(a). We clearly observe a difference in the behavior of ViT versus ResNets here. In\ngeneral, for a given token budget, ResNets outperform all other token based models. In addition, the robust accuracies\nfor Transformers fall to zero for as few as two patches. The advantage offered by distillation for single token attacks is\nalso lost once the token budget increases.\nVarying patch sizes:In order to further analyse if these results hold across stronger and weaker block sparse constraints,\nwe further run attacks for varying patch sizes. Smaller patch sizes are equivalent to partial token manipulation. We\nﬁx the token budget to be 5 or 15 tokens as dictated by the input size. Here, this corresponds to allowing the attacker\nto perturb 5 p×ppatches. As one would expect, a smaller partial token attack is weaker than a full token attack.\nSurprisingly, the Transformer networks are comparable or better than ResNets for attacks smaller than a single token.\nThis leads us to conclude that Transformers can compensate for adversarial perturbations within a tokens. However, as\nthe patch size approaches the token size, Resnets achieve better robustness. We also see that MLP-Mixers, while also\nusing the token based input scheme, perform better than Transformers as the patch attack size increases.\nHowever, this approach allows for unrestricted changes to the tokens. Another approach would be to study the effect of\n“mixed norm” attacks which further constrain the patches to be imperceptibly perturbed.\nMixed Norm Attacks:For the mixed norm attacks, we analyse the robustness of all networks for a ﬁxed ϵℓ∞budget\nof one gray level. We vary the token budgets from 1 to 5. Here, almost all the networks show similar robustness for\na small token budget (K=1,2); refer Table 1. However, as the token budget increases, Transformer and MLP Mixer\nnetworks are far more vulnerable. Note that this behavior contradicts [10], where ViTs outperform ResNets. Since our\n1In case of varying image sizes due to pre-processing, we calculate the scaling factor in terms of the number of pixels and\nappropriately increase or decrease the maximum number of patches.\n2https://github.com/NYU-DICE-Lab/TokenAttacks_Supplementary.git\n4\nthreat model leverages the token based architecture of the Transformers, our attacks are far more successful at breaking\nViTs over Resnets.\nTable 1: Robust Accuracy for Mixed Norm Attacks: The\nmodels are attacked with a K, (1/255) Patch Attack. Note that\nfor smaller token budgets, the models perform nearly the same.\nHowever, as the token budget increases, Resnets are more robust\nthan Transformers.\nModel Clean Token Budget\n1 2 5\nViT-224 88.70 68.77 50.83 15.28\nViT-384 90.03 53.48 28.57 4.98\nDeIT 85.71 72.42 46.84 6.31\nDeIT-Distilled 87.70 68.77 54.15 16.61\nResnet-101 85.71 69.10 55.14 32.89\nResnet-50 85.38 67.44 55.81 31.22\nWide Resnet 87.04 54.81 32.89 11.62\nMLP-Mixer 83.78 63.78 37.87 5.98\nTable 2: Robust accuracies,s= 256sparse andK = 1,\n16 ×16 patch attack .\nModel Norm constraint\nClean Sparse Patch\nViT 224 88.70 5.98 13.62\nViT 384 90.03 3.32 1.33\nDeIT 85.71 4.65 17.27\nDeIT (Distilled) 87.70 14.95 17.94\nMLP Mixer 83.72 5.98 26.91\nResNet 50 85.38 13.95 19.90\nResNet 101 85.71 23.59 49.50\nWide Resnet 87.04 1.33 26.57\nSparse Attacks:The sparse variant of our algorithm restricts the patch size to 1 ×1. We allow for a sparsity budget of\n0.5% of original number of pixels. In case of the standard 224 ×224 ImageNet image, the attacker is allowed to perturb\n256 pixels. We compare the attack success rate of both sparse attack and patch-based token attack at same sparsity\nbudget; to compare we chose 1,16 ×16 patch attack (refer Table 2). We see that as is the case with token attacks, even\nfor sparse attacks, vision transformers are less robust as compared to ResNets. With the same sparsity budget, sparse\nattacks are stronger than token attacks; however we stress that sparse threat model is less practical to implement as the\nsparse coefﬁcients may be scattered anywhere in the image.\n4 Discussion and Conclusion\nAnalysing the above results, we infer certain interesting properties of transformers.\n1. We ﬁnd that Transformers are generally susceptible to token attacks, even for very low token budgets.\n2. However, Transformers appear to compensate for perturbations to patch attacks smaller than the token size.\n3. Further, ResNets and MLP-Mixer outperform Transformers for token attacks consistently.\nAn interesting direction of follow-up work is to develop strong certiﬁable defenses for token attacks. Further directions\nof research also include analysis of the effect of distillation and semi-supervised pre-training.\nAcknowledgements\nThe authors were supported in part by the National Science Foundation under grants CCF-2005804 and CCF-1815101,\nUSDA/NIFA under grant USDA-NIFA:2021-67021-35329, and ARPA-E under grant DE:AR0001215.\nReferences\n[1] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby, “Big transfer (bit): General visual\nrepresentation learning,” in ECCV, 2020.\n[2] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” in ICLR, 2015.\n[3] I. Goodfellow, “Defense against the dark arts: An overview of adversarial example security research and future research\ndirections,” arxiv preprint, vol. 1806.04169, 2018.\n[4] A. Kurakin, I. Goodfellow, and S. Bengio, “Adversarial examples in the physical world,”arxiv preprint, vol. 1607.02533, 2017.\n[5] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep learning models resistant to adversarial attacks,”\nin ICLR, 2018.\n5\n[6] H. Zhang, Y . Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan, “Theoretically principled trade-off between robustness and\naccuracy,” in ICML, 2019, pp. 7472–7482.\n[7] A. Dosovitskiy, L. Beyer, et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” inICLR, 2020.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[9] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J’egou, “Training data-efﬁcient image transformers &\ndistillation through attention,” in ICML, 2021.\n[10] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner, and A. Veit, “Understanding robustness of transformers for\nimage classiﬁcation,” ArXiv, vol. 2103.14586, 2021.\n[11] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CVPR, pp. 770–778, 2016.\n[12] S. Zagoruyko and N. Komodakis, “Wide residual networks,” ArXiv, vol. 1605.07146, 2016.\n[13] I. Tolstikhin, N. Houlsby, et al., “Mlp-mixer: An all-mlp architecture for vision,” ArXiv, vol. 2105.01601, 2021.\n[14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing properties of neural\nnetworks,” arXiv preprint arXiv:1312.6199, 2013.\n[15] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural networks,” IEEE (SP), 2017.\n[16] A. Ilyas, L. Engstrom, and A. Madry, “Prior convictions: Black-box adversarial attacks with bandits and priors,” arxiv preprint,\nvol. 1807.07978, 2018.\n[17] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial attacks with limited queries and information,” in PMLR,\n2018, vol. 80.\n[18] L. Engstrom, D. Tsipras, L. Schmidt, and A. Madry, “A rotation and a translation sufﬁce: Fooling cnns with simple\ntransformations,” arxiv preprint, vol. 1712.02779, 2017.\n[19] C. Xiao, J. Zhu, B. Li, W. He, M. Liu, and D. Song, “Spatially transformed adversarial examples,” arxiv preprint, vol.\n1801.02612, 2018.\n[20] A. Joshi, A. Mukherjee, S. Sarkar, and C. Hegde, “Semantic adversarial attacks: Parametric transformations that fool deep\nclassiﬁers,” in ICCV, 2019.\n[21] Y . Zhang, H. Foroosh, P. David, and B. Gong, “Camou: Learning physical vehicle camouﬂages to adversarially attack detectors\nin the wild,” in ICLR, 2019.\n[22] Y . Song, R. Shu, N. Kushman, and S. Ermon, “Constructing unrestricted adversarial examples with generative models,” in\nNeurIPS, 2018.\n[23] A. Shafahi, W R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras, and T. Goldstein, “Poison frogs! targeted clean-label\npoisoning attacks on neural networks,” in NeurIPS, 2018.\n[24] G. Jagatap, A. Joshi, A. Chowdhury, S. Garg, and C. Hegde, “Adversarially robust learning via entropic regularization,” ArXiV,\nvol. 2008.12338, 2020.\n[25] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-GAN: Protecting classiﬁers against adversarial attacks using generative\nmodels,” in ICLR, 2018.\n[26] H. Yin, Z.and Wang, J. Wang, J. Tang, and W. Wang, “Defense against adversarial attacks by low-level image transformations,”\nInternational Journal of Intelligent Systems, vol. 35, no. 10, pp. 1453–1466, 2020.\n[27] A. Athalye, N. Carlini, and D. Wagner, “Obfuscated gradients give a false sense of security: Circumventing defenses to\nadversarial examples,” in ICML, 2018.\n[28] E. Wong and Z. Kolter, “Provable defenses against adversarial examples via the convex outer adversarial polytope,” inICML.\nPMLR, 2018.\n[29] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certiﬁed adversarial robustness via randomized smoothing,” in ICML. PMLR, 2019.\n[30] H. Salman, G. Yang, J. Li, P. Zhang, H. Zhang, I. Razenshteyn, and S. Bubeck, “Provably robust deep learning via adversarially\ntrained smoothed classiﬁers,” in NeurIPS, 2019.\n[31] S. Gowal, C. Qin, J. Uesato, T. Mann, and P. Kohli, “Uncovering the limits of adversarial training against norm-bounded\nadversarial examples,” ArXiv, vol. 2010.03593, 2020.\n[32] T. Pang, X. Yang, Y . Dong, H. Su, and J. Zhu, “Bag of tricks for adversarial training,” inICLR, 2021.\n[33] T. Brown, D. Man ´e, A. Roy, M. Abadi, and J. Gilmer, “Adversarial patch,”arXiv preprint arXiv:1712.09665, 2017.\n[34] A. Zolﬁ, M. Kravchik, Y . Elovici, and A. Shabtai, “The translucent patch: A physical and universal attack on object detectors,”\nin CVPR, 2021.\n[35] S. Thys, W. Van Ranst, and T. Goedem ´e, “Fooling automated surveillance cameras: adversarial patches to attack person\ndetection,” in CVPR Workshops, 2019.\n6\n[36] Z. Wu, S. Lim, L. Davis, and T. Goldstein, “Making an invisibility cloak: Real world adversarial attacks on object detectors,”\nin ECCV, 2020.\n[37] F. Croce and M. Hein, “Sparse and imperceivable adversarial attacks,” in CVPR, 2019, pp. 4724–4732.\n[38] Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V . Le, “Self-training with noisy student improves imagenet\nclassiﬁcation,” CVPR, 2020.\n[39] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language\nunderstanding,” in NAACL, 2019.\n[40] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf, “Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter,” ArXiv, vol. 1910.01108, 2019.\n[41] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-V oss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A.Radford, I. Sutskever, and D. Amodei,\n“Language models are few-shot learners,” ArXiv, vol. 2005.14165, 2020.\n[42] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran, “Image transformer,” in ICML, 2018, pp.\n4055–4064.\n[43] Z. Dai, H. Liu, Q. Le, and M. Tan, “Coatnet: Marrying convolution and attention for all data sizes,” ArXiv, vol. 2106.04803,\n2021.\n[44] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, “Cvt: Introducing convolutions to vision transformers,”\nArXiv, vol. 2103.15808, 2021.\n[45] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J’egou, “Going deeper with image transformers,” ArXiv, vol.\n2103.17239, 2021.\n[46] D. Hendrycks, X. Liu, E. Wallace, A. Dziedzic, R. Krishnan, and D. Song, “Pretrained transformers improve out-of-distribution\nrobustness,” arXiv preprint arXiv:2004.06100, 2020.\n[47] K. Mahmood, R. Mahmood, and M. Van Dijk, “On the robustness of vision transformers to adversarial examples,” arXiv\npreprint arXiv:2104.02610, 2021.\n[48] S. Paul and P. Chen, “Vision transformers are robust learners,” arXiv preprint arXiv:2105.07581, 2021.\n[49] F. Croce, M. Andriushchenko, et al., “Sparse-rs: a versatile framework for query-efﬁcient sparse black-box adversarial attacks,”\narXiv preprint arXiv:2006.12834, 2020.\n[50] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. Berg, and\nF. Li, “ImageNet Large Scale Visual Recognition Challenge,” Intl. J. Comp. Vision, vol. 115, no. 3, pp. 211–252, 2015.\n[51] R. Wightman, “Pytorch image models,” https://github.com/rwightman/pytorch-image-models, 2019.\n[52] C. Yun, S. Sra, and A. Jadbabaie, “Are deep resnets provably better than linear predictors?,” in NeurIPS, 2019.\n[53] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional networks: Visualising image classiﬁcation models\nand saliency maps,” arXiv preprint arXiv:1312.6034, 2013.\n[54] N. Papernot, P. Mcdaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, “The limitations of deep learning in adversarial\nsettings,” EuroS&P, pp. 372–387, 2016.\n[55] R. Wiyatno and A. Xu, “Maximal jacobian-based saliency map attack,” ArXiv, vol. 1808.07945, 2018.\n7\nA Background\nA.1 Transformers\nThe Transformer block was introduced by [8], for text input. The basic idea of the Transformer model is to leverage an\nefﬁcient form of “self-attention”. A standard attention block is formally deﬁned as,\nxout = Softmax\n(xWQWkxT\n√\nd\n)\nxWV , (1)\nwhere x ∈Rd×n is an input string, xout ∈Rd×n is the output of the self-attention block, WQ, WK andWV are the\nlearnable query, key and the value matrices. Note that x is actually a concatenation of n“tokens” of size d, which\neach represent some part of the input. Multi-headed self attention stacks multiple such blocks in a single layer. The\nTransformer model has multiple such layers followed by a ﬁnal output attention layer with a classiﬁcation token. This\narchitecture makes perfect sense for text where-in tokens are word or sentence embeddings, and each token therefore\nholds some semantic meaning. These models are trained in an auto-regressive fashion with additional losses for\ndownstream tasks.\nHowever, extending the same architecture for images is non-trivial; primarily as the atomic components of an image are\npixels which hold little to no meaning by themselves. [42] propose a solution where they use pixels as tokens and train\ngenerative models to solve problems such as image generation and super-resolution. However, the large dimensionality\nof images forces the Attention blocks to be massively parameterized, leading to issues of scale. In order to remedy this,\n[7] suggest using local image patches as tokens. This instantly reduces the number of tokens while also leveraging\nthe local consistency property of images. They ﬁnd that in most cases, it is enough to use non-overlapping patches of\n16 ×16 as tokens to ensure near state of the art accuracies. One disadvantage of such massive models however is the\nrequirement of very large training datasets. [9] propose a data-efﬁcient distillation based method to train Transformers.\nTheir architecture (DeIT) leverages a custom transformer based distillation token as well as standard student-teacher\ntraining approaches to improve both the sample complexity and the performance over Vision Transformers.\nA standard Resnet model, on the other hand, uses residual blocks:\nxout = ReLU (x + ReLU(Wx)) . (2)\nA Resnet stacks several such residual blocks in succession followed by a classiﬁer. The residual connection allows for\neasy gradient ﬂow and improves training. There have been several works that prove the generalization and efﬁcacy of\nResnets, both empirically [11] and theoretically [52].\nA.2 How resnets differ from transformers\nIn comparison with Resnets, which were the best performing image classiﬁers previously, we see that there are two\nmajor structural differences. The ﬁrst is that most Resnets downsample activations as we go deeper. This is supposed to\nhelp reduce redundancies and propagate discriminative features. However, Vision Transformers with self-attention\nblocks appear to preserve activation sizes throughout their depth. The second major difference is the structure of the\nResnet block in comparison with the Attention block. As is evident, any interaction between non-local pixel groups in\nResnets is happens in deeper layers. The initial layers tend to just focus on neighbourhood pixel interactions. However,\nthe Attention mechanism forces each layer of the transformer to consider both local and non-local interactions. There\nexist additional differences in terms of the non-linearities involved and the number of parameters in each model.\nThe speciﬁc difference in the treatment of local and non-local pixel groups informs our choice of attack. While several\npapers have previously studied the robustness of vision transformers in the standard adversarial setting, we speciﬁcally\nconsider the case where the attacker is only allowed to modify an image locally; for example a set number of tokens.\nA.3 Saliency attacks\nSuch ‘salient’ pixels are often identiﬁed using the magnitudes of gradients. This idea, while not particularly new [53],\nlends itself naturally to constructing adversarial attacks. Speciﬁcally, the idea is to only perturb a subset of the salient\npixels thus implicitly satisfying the sparsity constraint. JSMA [54] and Maximal-JSMA [55] leverage this observation\nto construct k-sparse attacks by maximally perturbing ksalient pixels. In maximal-JSMA, the authors calculate saliency\nof each pixel usign the following equation;\nS+(xi,c) =\n{\n0 if ∂f(x)c\n∂xi\n<0 or ∑\nc′̸=c\n∂f(x)′\nc\n∂xi\n−∂f(x)c\n∂xi\n·∑\nc′̸=c\n∂f(x)′\nc\n∂xi\notherwise,\n(3)\n8\nwhere xi is the pixel in question, cis the true class, and fi is a logit value speciﬁc to class i.\nIn this paper, we propose a patch based block sparse attack where the attack budget is deﬁned by the number of patches\n(blocks) the attacker is allowed to perturb. Our approach builds on JSMA [ 54] Maximal-JSMA [55], wherein the\nattacker identiﬁes top salient pixels using gradients and perturb them to create attacks. We extend a similar idea to block\nsparsity. The main differences between JSMA and our approach lie in two places: (1) We use a simpliﬁed construction\nfor the saliency map that relies on the magnitude of the gradients with respect to each pixel, (2) instead of considering\nsalient pixels, we instead identify the most informative pixel blocks and further rely on gradient updates to generate an\nattack.\nB Experiments\nFor all experiments, we use SGD for optimization with a step size of 0.1 for a maximum of 100 steps for both variants\nfor all models except Resnet-101. For Resnet-101, we use a step size of 0.2 for a maximum of 100 steps.\nB.1 Mixed norm attacks\nFor mixed norm block sparse attacks, we impose an additional ℓ∞bound (ϵ) on each pixel to enforce imperceptibility.\nWe run our experiments with a constraint of one gray level similar to [10]. Since each of these models scales the input\nimages to varying input ranges, we further scale each ϵappropriately. We then use a projection step in Alg. 1 using\nclipping to enforce the constraint. We show some example attack images in Fig. ??. Thus, we conclude that the attack\nalgorithm implicitly creates imperceptible perturbations.\nC Detailed Results\nModel Original Adversarial Pertubation\nViT384\nWideResnet\nDeIT224\nDeIT 224 (Distilled)\nFigure 3: Patch attacks on Transformers: The attack images are generated with a ﬁxed budget of 20 patches. Note\nthat the perturbations are imperceptible. The third column shows the perturbation brightened 10 times.\n9\nViT224\nDeIT\nDeiT-Distilled\nResnet-101\nResnet-50\nMLP-Mixer\nFigure 4: Examples of mixed norm attacksExamples of successful (5,(1/255) mixed norm attacks for various\nmodels\n10\nTable 3: Robustness v/s Token Budget\nModel Token Budget\n1 2 5 10 20 40\nViT-224 13.62 0.9 0.0 0.0 0.0 0.0\nViT-384 1.33 0.0 0.0 0.0 0.0 0.0\nDeIT 17.27 0.9 0.0 0.0 0.0 0.0\nDeIT (Distilled) 17.94 0.0 0.0 0.0 0.0 0.0\nResnet-101 49.50 32.22 8.64 1.66 0.33 0.0\nResnet-50 19.9 4.65 0.33 0.0 0.0 0.0\nWide-Resnet 26.57 9.96 0.66 0.0 0.0 0.0\nMLP-Mixer 26.91 5.31 0.0 0.0 0.0 0.0\nTable 4: Robustness v/s varying patch sizes\nModel Attack patch sizes\n1 4 8 16\nViT-224 71.09 55.15 9.30 0.0\nViT-384 68.77 31.89 0.06 0.0\nDeIT 78.40 68.77 8.31 0.0\nDeIT-Distilled 83.72 68.10 12.29 0.0\nResnet-101d 75.08 64.78 38.87 8.64\nResnet-50 62.12 40.53 11.96 0.33\nWide Resnet 44.85 28.24 9.63 0.66\nMLP-Mixer 76.41 54.49 17.61 5.31\n11",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.9246875643730164
    },
    {
      "name": "Computer science",
      "score": 0.7234161496162415
    },
    {
      "name": "Transformer",
      "score": 0.6846739053726196
    },
    {
      "name": "Adversarial system",
      "score": 0.5388997793197632
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3941946029663086
    },
    {
      "name": "Computer security",
      "score": 0.2760714292526245
    },
    {
      "name": "Engineering",
      "score": 0.1492181420326233
    },
    {
      "name": "Electrical engineering",
      "score": 0.07848083972930908
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 9
}