{
    "title": "TransCloudSeg: Ground-Based Cloud Image Segmentation With Transformer",
    "url": "https://openalex.org/W4288064712",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2100870932",
            "name": "Shuang Liu",
            "affiliations": [
                "Tianjin Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2121675656",
            "name": "Jiafeng Zhang",
            "affiliations": [
                "Tianjin Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2106942868",
            "name": "Zhong Zhang",
            "affiliations": [
                "Tianjin Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2128190750",
            "name": "Xiaozhong Cao",
            "affiliations": [
                "China Meteorological Administration"
            ]
        },
        {
            "id": "https://openalex.org/A2184257126",
            "name": "Tariq S Durrani",
            "affiliations": [
                "University of Strathclyde"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2119531715",
        "https://openalex.org/W2889941518",
        "https://openalex.org/W3017096747",
        "https://openalex.org/W2755992512",
        "https://openalex.org/W3044178454",
        "https://openalex.org/W2733849967",
        "https://openalex.org/W1981121910",
        "https://openalex.org/W2054169080",
        "https://openalex.org/W2169142151",
        "https://openalex.org/W3113799885",
        "https://openalex.org/W2040233346",
        "https://openalex.org/W1497347036",
        "https://openalex.org/W1993731556",
        "https://openalex.org/W2603215277",
        "https://openalex.org/W2962943321",
        "https://openalex.org/W3022935549",
        "https://openalex.org/W2062583931",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3047443805",
        "https://openalex.org/W3048631361",
        "https://openalex.org/W2938450871",
        "https://openalex.org/W2990032788",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3200379731",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W4226213299",
        "https://openalex.org/W3196815773",
        "https://openalex.org/W3158742449",
        "https://openalex.org/W6788071488",
        "https://openalex.org/W6779248606",
        "https://openalex.org/W6788477181",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3214707056",
        "https://openalex.org/W3180045188",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W6604254268",
        "https://openalex.org/W6739696289",
        "https://openalex.org/W2990775046",
        "https://openalex.org/W2133059825",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W1932847118",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3033210410",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W3103695279",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2630837129",
        "https://openalex.org/W3102692100"
    ],
    "abstract": "Cloud image segmentation plays an important role in ground-based cloud observation. Recently, most existing methods for ground-based cloud image segmentation learn feature representations using the convolutional neural network (CNN), which results in the loss of global information because of the limited receptive field size of the filters in the CNN. In this article, we propose a novel deep model named TransCloudSeg, which makes full use of the advantages of the CNN and transformer to extract detailed information and global contextual information for ground-based cloud image segmentation. Specifically, TransCloudSeg hybridizes the CNN and transformer as the encoders to obtain different features. To recover and fuse the feature maps from the encoders, we design the CNN decoder and the transformer decoder for TransCloudSeg. After obtaining two sets of feature maps from two different decoders, we propose the heterogeneous fusion module to effectively fuse the heterogeneous feature maps by applying the self-attention mechanism. We conduct a series of experiments on Tianjin Normal University large-scale cloud detection database and Tianjin Normal University cloud detection database, and the results show that our method achieves a better performance than other state-of-the-art methods, thus proving the effectiveness of the proposed TransCloudSeg.",
    "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022 6121\nTransCloudSeg: Ground-Based Cloud Image\nSegmentation With Transformer\nShuang Liu , Senior Member, IEEE, Jiafeng Zhang, Zhong Zhang , Senior Member, IEEE, Xiaozhong Cao ,\nand Tariq S. Durrani , Life Fellow, IEEE\nAbstract—Cloud image segmentation plays an important role in\nground-based cloud observation. Recently, most existing methods\nfor ground-based cloud image segmentation learn feature repre-\nsentations using the convolutional neural network (CNN), which\nresults in the loss of global information because of the limited re-\nceptive ﬁeld size of the ﬁlters in the CNN. In this article, we propose\na novel deep model named TransCloudSeg, which makes full use\nof the advantages of the CNN and transformer to extract detailed\ninformation and global contextual information for ground-based\ncloud image segmentation. Speciﬁcally, TransCloudSeg hybridizes\nthe CNN and transformer as the encoders to obtain different\nfeatures. To recover and fuse the feature maps from the encoders,\nwe design the CNN decoder and the transformer decoder for\nTransCloudSeg. After obtaining two sets of feature maps from two\ndifferent decoders, we propose the heterogeneous fusion module\nto effectively fuse the heterogeneous feature maps by applying the\nself-attention mechanism. We conduct a series of experiments on\nTianjin Normal University large-scale cloud detection database\nand Tianjin Normal University cloud detection database, and the\nresults show that our method achieves a better performance than\nother state-of-the-art methods, thus proving the effectiveness of the\nproposed TransCloudSeg.\nIndex Terms—Convolutional neural network (CNN), cloud\nimage segmentation, heterogeneous feature maps, transformer.\nI. I NTRODUCTION\nC\nLOUDS are the visible aggregations of a large number of\nsmall water droplets or ice crystals in the atmosphere. They\nplay an important role in the Earth’s atmospheric movement,\nsurface temperature regulation, and hydrological cycle [1]–[3].\nHence, accurate cloud observation is crucial in environmental\nmonitoring, weather forecasting, etc. In general, there are two\nManuscript received 3 June 2022; revised 11 July 2022; accepted 24 July\n2022. Date of publication 27 July 2022; date of current version 5 August 2022.\nThis work was supported in part by National Natural Science Foundation of\nChina under Grant 62171321, in part by the Natural Science Foundation of\nTianjin under Grant 20JCZDJC00180 and Grant 19JCZDJC31500, and in part\nby the Open Projects Program of National Laboratory of Pattern Recognition\nunder Grant 202000002. ( Corresponding author: Zhong Zhang.)\nShuang Liu, Jiafeng Zhang, and Zhong Zhang are with the Tianjin Key Lab-\noratory of Wireless Mobile Communications and Power Transmission, Tianjin\nNormal University, Tianjin 300387, China (e-mail: shuangliu.tjnu@gmail.com;\nm648167095@gmail.com; zhong.zhang8848@gmail.com).\nXiaozhong Cao is with the Meteorological Observation Centre, China\nMeteorological Administration, Beijing 100081, China (e-mail: xzhong-\ncao@163.com).\nTariq S. Durrani is with the Department of Electronic and Electrical\nEngineering, University of Strathclyde, G1 1XQ Glasgow, U.K. (e-mail:\nt.durrani@strath.ac.uk).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3194316\nFig. 1. Structures of (a) common-used encoder–decoder network and\n(b) proposed TransCloudSeg for ground-based cloud image segmentation.\ntypes of cloud observation, i.e., satellite-based [4] and ground-\nbased [5], [6]. Ground-based cloud observation is highly ﬂexible\nand accessible, and it is good at monitoring the bottom charac-\nteristics of clouds in local areas for capturing information [7].\nDue to blurred edges and varied shapes of clouds, ground-\nbased cloud observation is quite challenging. Moreover, it pri-\nmarily relies on professional technicians, which not only leads\nto a waste of human resources but also may obtain inconsistent\nresults toward the same observation. In order to make the results\nof ground-based cloud observation more objective, automatic\nalgorithms are in great need. Many automatic algorithms are pro-\nposed for ground-based cloud observation including cloud base\nheight measurement [8]–[10], cloud type classiﬁcation [11],\n[12], and cloud cover estimation [13]–[15]. In this article,\nwe focus on cloud cover estimation for ground-based cloud\nobservation.\nThe goal of ground-based cloud image segmentation is to\ngenerate a segmentation mask in which each pixel is classiﬁed\nas cloud or sky, so the cloud cover estimation can be imple-\nmented by ground-based cloud image segmentation [16]. Since\nthe light scattering causes sky and clouds to show different\ncolors, traditional methods [1], [14], [17] for ground-based cloud\nimage segmentation usually apply color values as features. Some\nperformance improvements are achieved, but it is still not good\nenough for actual applications.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6122 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nBecause of the powerful representation ability of deep learn-\ning, convolutional neural network (CNN) has become the main-\nstream for many computer vision tasks [18]–[20]. For example,\nHong et al. [21] proposed a uniﬁed multimodality learning\nframework for remote sensing image classiﬁcation and designed\nthe cross fusion strategy for effective transfer information across\nmodalities. Hence, some methods are proposed to employ the\nCNN for ground-based cloud image segmentation, such as\nCloudU-Net [5], CloudSegNet [22], and SegCloud [23]. As\nshown in Fig. 1(a), these methods are usually designed as the\nencoder–decoder architecture where the encoder is composed\nof the CNN. The encoder is used to learn high-level and low-\nresolution features, and the decoder outputs the segmentation\nmask. However, the CNN encoder results in the loss of global\ninformation, because the receptive ﬁeld size of the ﬁlter in the\nCNN is limited.\nTo overcome the aforementioned limitation, a trans-\nformer [24] is proposed, which is ﬁrst successfully applied in\nnatural language processing (NLP). Since transformer relies\non the self-attention (SA) mechanism to learn discriminative\nfeatures, it is naturally introduced into the computer vision\nﬁeld [25]–[27]. Many transformer-based methods for computer\nvision [28]–[30] treat the input image as a sequence of image\npatches, and then, they add the position embedding to build\nlocation information. Although transformer-based methods have\nachieved new state-of-the-art performance compared with CNN-\nbased methods in many topics of computer vision, they are easy\nto lose detailed information of images.\nIn this article, we propose a novel deep learning method\nnamed TransCloudSeg, which makes full use of the advantages\nof the CNN and transformer for ground-based cloud image\nsegmentation. To the best of our knowledge, it is the ﬁrst\ntime to introduce transformer into ground-based cloud image\nsegmentation. Speciﬁcally, TransCloudSeg mainly consists of\ntwo encoders, two decoders, and the heterogeneous fusion mod-\nule (HFM) as shown in Fig. 1(b). The two encoders are the\nCNN encoder and the transformer encoder, which focus on\nextracting detailed information and global contextual informa-\ntion, simultaneously. Meanwhile, we design two decoders, i.e.,\nCNN decoder and transformer decoder corresponding to the two\nencoders. The CNN decoder utilizes the skip connections to\nintegrate the feature maps from different scales of the CNN\nencoder for recovering detailed information. The transformer\ndecoder aggregates the feature maps from different levels of the\ntransformer encoder by assigning different weights. The outputs\nof the two decoders are two sets of feature maps, and these\nfeature maps are heterogeneous because they contain different\nsemantic information.\nAfter obtaining the heterogeneous feature maps from the\nCNN and Transformer decoders, the direct methods to fuse\nthem are concatenation or addition, but it is difﬁcult to mine\nuseful information from them . Hence, we propose HFM to\neffectively fuse the heterogeneous feature maps. The proposed\nHFM transforms the feature maps into the sequences, and then,\nemploys the SA mechanism to learn discriminative features.\nThe main contributions of this article are threefold.\n1) We hybridize the CNN and transformer as the encoder\nof TransCloudSeg, and to the best of our knowledge, we are\nthe ﬁrst to apply the transformer for ground-based cloud image\nsegmentation.\n2) We design two corresponding decoders to fuse feature maps\nfrom the two encoders. Furthermore, as a newly proposed com-\nponent, the HFM is proposed to perform effective heterogeneous\nfeature maps fusion.\n3) We evaluate the segmentation performance of the proposed\nTransCloudSeg on Tianjin Normal University large-scale cloud\ndetection database (TLCDD) [31] and Tianjin Normal Univer-\nsity cloud detection database (TCDD) [32]. The experimental\nresults outperform other state-of-the-art methods, demonstrating\nthe superiority of the proposed method.\nII. R ELATED WORK\nA. Ground-Based Cloud Image Segmentation\nWith the development of acquisition devices of images, many\npowerful algorithms have been proposed for ground-based cloud\nimage segmentation. Long et al. [17] treated the ratio of red\nand blue (R/B) as the ﬁxed threshold for cloud image seg-\nmentation. Afterwards, Heinle et al. [1] utilized R-B as the\nthreshold. Shi et al. [14] combined texture and color features\non the basis of superpixels for cloud image segmentation,\nwhich allows the aggregation of pixels to consider location\ninformation.\nRecently, many approaches employ the encoder–decoder ar-\nchitecture combined with the CNN for ground-based cloud\nimage segmentation. For example, Dev et al. [22] proposed\nCloudSegNet, which is a lightweight deep learning model\nfor effectively segmenting daytime and nighttime cloud im-\nages. Xie et al. [23] presented the SegCloud model, which\nhas a CNN encoder–decoder architecture trained on 400 all-\nsky images with annotation. In order to obtain better seg-\nmentation results, Shi et al. [5] proposed to replace tradi-\ntional convolution operations with dilated convolution opera-\ntions, which could obtain more information. Since the sample\nnumber of cloud database is limited, Zhou et al. [33] em-\nployed transfer learning to train the deep network on other\ndatabases, and then, ﬁne-tune on the ground-based cloud image\ndatabase.\nThe main contribution of CNN-based methods for ground-\nbased cloud image segmentation is to introduce the encoder–\ndecoder architecture into this ﬁeld. The encoder is designed to\nlearn the representation features, which enables to mine semantic\ninformation. The decoder recovers the representation features\ninto the segmentation mask so as to implement the pixel-level\nclassiﬁcation.\nB. Transformer\nTransformer is ﬁrst proposed in [24] and it has achieved\npromising performance in NLP due to well dealing with\nlong-range spatial dependencies of sequences. Recently, re-\nsearchers modify transformer for computer vision tasks [34],\nLIU et al.: TRANSCLOUDSEG: GROUND-BASED CLOUD IMAGE SEGMENTATION WITH TRANSFORMER 6123\nFig. 2. Framework of the proposed TransCloudSeg. We ﬁrst utilize the CNN encoder to learn the local information, and then, apply the transformer encod er to\nlearn the global information. Afterwards, we design the CNN decoder and the transformer decoder to combine the multiscale feature maps from the CNN en coder\nand the multilevel feature maps from the transformer encoder, respectively. Finally, the two sets of feature maps from the CNN decoder and the transfo rmer decoder\nare fed into the HFM to generate the segmentation mask.\nand these transformer-based methods show new state-of-the-art\nperformance in image classiﬁcation [35], [36], segmenta-\ntion [37]–[39], object detection [26], and so on.\nMost transformer-based methods [28], [29], [40] transform\nthe input image to a sequence of patches so as to capture long-\nrange spatial dependencies. In addition, the position embedding\nis utilized to build position information among patches. Vision\ntransformer (ViT) [25] demonstrated that a pure transformer\ncould achieve the state-of-the-art through directly sequencing\nimages for ImageNet classiﬁcation. Zheng et al. [28] applied\ntransformer to semantic segmentation and yielded new state\nof the art on publicly available segmentation databases, which\nprovides an alternative solution to image segmentation. The\ncontributions of [28] lie in twofolds.\n1) They utilized the transformer with a global receptive ﬁeld\nto learn global information.\n2) To extensively examine the SA representation features, it\ndesigns three different kinds of decoders.\nHong et al. [41] applied the transformer to the HS image clas-\nsiﬁcation task and designed the groupwise spectral embedding\nand cross-layer adaptive fusion modules to improve the detail\ncapture of subtle spectral differences and enhance the interaction\nbetween layers.\nDifferent from the aforementioned transformer-based meth-\nods, we hybridize the CNN and transformer as the encoders,\nand design the two corresponding decoders. Furthermore, we\npropose the HFM to fuse the heterogenous feature maps\nfrom the two decoders in order to generate the segmentation\nmask.\nIII. A PPROACH\nIn this section, we ﬁrst clarify the motivation of the proposed\nTransCloudSeg. Then, we present an overall framework of the\nproposed TransCloudSeg, and then, describe the CNN encoder\nand the transformer encoder in detail. Afterwards, we present\ntwo different decoders corresponding to the CNN encoder and\nthe transformer encoder, respectively. Finally, we introduce how\nto fuse the heterogeneous feature maps extracted from the two\ndecoders in order to generate the segmentation mask.\nA. Motivation\nClouds are aggregated with water droplets or ice crystals in\nthe atmosphere, so they possess blurred boundaries and irregular\nshapes. Furthermore, imaging ground-based cloud images is\neasily affected by illumination. These factors are the challenges\nof ground-based cloud image segmentation.\nMost existing deep learning methods for ground-based cloud\nimage segmentation are under the framework of the CNN.\nAlthough the CNN has excellent representational capabilities\nbut neglects to learn global contextual information due to the\nlimited receptive ﬁeld size of the ﬁlter. As shown in Fig. 3(a),\nthe CNN-based methods lack the whole consistency for cloud\nimage segmentation. Meanwhile, the transformer adopts SA\nmechanism to learn global contextual information, but lacks\nlocal detail information processing. From Fig. 3(b), we can see\nthat the transformer-based methods are unfavorable to segment\nthe boundaries of the cloud. Hence, we hybridize the CNN and\ntransformer as the encoder for cloud representation so as to make\n6124 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 3. Visualization of encoder feature maps for different methods.\n(a) Ground-based cloud map sample. (b) CNN-based methods. (c) Transformer-\nbased methods. (d) Our method.\nfull use of the properties of the CNN and transformer, which\ncould learn local information and global contextual information,\nsimultaneously. As shown in Fig. 3(c), our method could balance\nthe whole cloud distribution and the detailed boundaries for\naccurate ground-based cloud image segmentation.\nB. Overall Framework\nThe framework of TransCloudSeg is shown in Fig. 2, where\nit contains three main components: two encoders, two decoders,\nand HFM.\n1) Encoders: The proposed TransCloudSeg consists of two\nencoders, i.e., CNN encoder and transformer encoder, which aim\nto learn high-level features from input cloud images. The CNN\nencoder is ﬁrst utilized to extract multiscale feature maps. Then,\nthe transformer encoder changes the feature maps outputted\nfrom the CNN encoder into 1-D sequences. We add the position\nembedding to retain positional information, and stack multiple\ntransformer layers to obtain multilevel feature maps.\n2) Decoders:We design two different decoders corresponding\nto the CNN encoder and the transformer encoder for TransSeg-\nCloud. The CNN decoder targets to gradually recover the res-\nolution of feature maps and integrate the feature maps via\nskip connections. The purpose of the transformer decoder is\nto aggregate the feature maps with different levels from the\ntransformer encoder. The outputs of the two decoders are two\nsets of heterogeneous feature maps.\n3) Heterogeneous Fusion Module (HFM):We propose HFM\nto fuse the heterogeneous feature maps. Speciﬁcally, we ﬁrst\ntransform the two sets of feature maps into two sequences.\nAfterwards, we concatenate the two sequences and feed them\ninto the SA operation. Finally, the output of SA is reshaped to\ngenerate the segmentation mask.\nC. Encoders\nSince the receptive ﬁeld size of the ﬁlter in the CNN is\nlimited, the CNN encoder results in losing global information,\nand meanwhile, the transformer encoder focuses on learning\nglobal contextual information. Hence, we design the encoder\nof TransCloudSeg as a CNN–Transformer hybrid form so as to\nlearn local and global information, which makes full use of the\nadvantages of the CNN and transformer.\n1) CNN Encoder: We apply ResNet-50 (BiT) [25] as the\nbackbone of the CNN encoder to extract multiscale feature\nmaps, and the structure of ResNet-50 (BiT) is shown in Ta-\nble I. The size of input cloud image is H ×W ×3, where\nTABLE I\nSTRUCTURE OF RESNET-50 (BIT)\nFig. 4. Flowchart of the way to transform from the 2-D feature map f3 into\nthe 1-D sequence S.\nH and W are the height and the width of cloud image, re-\nspectively, and 3 is the channel number of cloud image. f0 ∈\nRH/2×W/2×64, f1 ∈ RH/4×W/4×256, f2 ∈ RH/8×W/8×512, and\nf3 ∈ RH/16×W/16×1024 are the multiscale feature maps of the\nCNN encoder.\n2) Transformer Encoder: We treat the output feature maps\nof the CNN encoder f3 as the input of the transformer encoder.\nThe transformer encoder is composed of patch embedding and\nN transformer layers.\nAs for the patch embedding, we tokenize f3 into the 1-\nD sequence S ∈ RL×C where L is the length of sequences\nand C is the hidden channel size. Since the size of f3 ∈\nRH/16×W/16×1024, L and C are equal to H/16 ×W/16 and\n1024, respectively. Speciﬁcally, similar to SETR [28] and\nViT [25], we utilize ﬁxed-size patches to transform 2-D feature\nmaps into 1-D sequence as shown in Fig. 4. We ﬁrst uni-\nformly partition the feature maps f3 ∈ RH/16×W/16×1024 into\nL patches with the size of P ×P, where L = H/16×W/16\nP×P .F o r\neach patch, EP is with the size of P ×P ×1024. Afterwards,\nwe ﬂatten EP to a 1-D vector V ∈ R1×P·P·1024, and then,\nproject the vector dimension to C through the linear layer.\nLIU et al.: TRANSCLOUDSEG: GROUND-BASED CLOUD IMAGE SEGMENTATION WITH TRANSFORMER 6125\nFig. 5. Flowchart of the transformer layer.\nHence, the 2-D feature maps f3 are mapped into L number of\nC-dimensional vectors, i.e., 1-D sequence S. Furthermore, we\napply the learnable position embedding Ep ∈ RL×D to learn\nthe spatial dependencies between different sequences. Hence,\nthe patch embedding is deﬁned as\nX = SE + Ep (1)\nwhere E ∈ RC×D is a trainable linear projection. Here, to\nreduce the computational cost, we set D<C to change the\nsize of the hidden channel from C to D. The patch embedding\nX contains the information of 1-D sequence S and the position\ninformation, and it is employed to compensate for the spatial\ninformation between sequences when transforming 2-D feature\nmaps into 1-D sequences. Furthermore, the patch embedding X\nis treated as the input of transformer. The transformer utilizes\nthe SA mechanism [24] to enhance the interactions within the\npatch embedding for global information.\nThe SA mechanism is vital to learning global contextual\ninformation for the transformer encoder. We take the output of\npatch embedding X as the input of transformer layer and apply\nN transformer layers to learn complex feature representations.\nFig. 5 shows the ﬂowchart of the transformer layer, which\nconsists of multihead SA (MSA), layer normalization (LN),\nresidual connections, and multilayer perceptron (MLP). The nth\ntransformer layer is deﬁned as\nYn = MLP(LN(An−1)) +An−1 (2)\nAn−1 = MSA(Zn−1)+ Yn−1 (3)\nZn−1 = LN(Yn−1) (4)\nwhere LN(·) denotes the layer normalization operation.\nThe MSA utilizes several independent SA operations to learn\nglobal contextual information from different aspects. The MSA\nin the nth transformer layer is formulated as\nMSA (Zn−1)=[ SA1(Zn−1); ... ; SAM (Zn−1)] Wn (5)\nwhere M is the number of independent SA operations and Wn\nis the trainable linear projection in the nth transformer layer.\nFig. 6. Flowchart of SA.\nThe ﬂowchart of SA operation is shown in Fig. 6. The mth\nSA operation in the nth transformer layer is deﬁned as\nSAm(Zn−1)= softmax\n(QmKT\nm√\nd\n)\nVm (6)\nQm = Zn−1Wq\nm,K m = Zn−1Wk\nm,V m = Zn−1Wv\nm\n(7)\nwhere Wq\nm,W k\nm,W v\nm ∈ RD×d are three independent trainable\nlinear projection for the mth SA operation.\nThe outputs of the transformer layer Yn ∈ RL×D(n =\n1, .., N) consist of multilevel feature maps of the transformer\nencoder. Since the transformer layer keeps the size of input and\noutput, the size of multilevel feature maps is the same.\nD. Decoders\nSince the CNN encoder and the transformer encoder possess\ndifferent mechanisms when extracting features, we design two\ndecoders, i.e., CNN decoder and transformer decoder for Tran-\nsCloudSeg so as to effectively recover and fuse the feature maps\nextracted from the encoders.\n1) CNN Decoder:We design the CNN decoder, which lever-\nages the skip connections to integrate the multiscale feature maps\nfrom the CNN encoder for retaining detailed information. In the\nCNN decoder, [ f0, f1, f2, f3] is regarded as the input. The\nﬂowchart of the CNN decoder can be concisely described as\nf3 →Conv+R → Upsampling×2→Cat(f2)\n→Conv+R →Upsampling×2→Cat(f1) →Conv+R\n→Upsampling ×2→ Cat(f0) →Conv+R→fh1.\nHere, Conv+R is the convolution layer followed by ReLU,\nUpsampling×I indicates that the feature maps are enlarged by\na factor of I using bilinear interpolation, and Cat( f) represents\nthe concatenation of the input feature maps and f. Speciﬁcally,\nthe convolution layer is with the ﬁlter size of 3 ×3, the stride of\n1 and the padding of (1, 1). fh1 ∈ RH/2×W/2×16 is the output\nof the CNN decoder.\n2) Transformer Decoder: We propose the transformer de-\ncoder, which utilizes the weight aggregation to integrate the\nmultilevel feature maps from the transformer encoder for re-\ntaining global contextual information.\n6126 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 7. Framework of HFM.\nThe output of the transformer decoder is deﬁned as\nfh2 = g\n( N∑\nn=1\nn\nN h(Yn)\n)\n(8)\nwhere h(·) indicates that we ﬁrst reshape Yn from the sequence\nwith the size of L ×D to the feature maps with the size\nof H/16 ×W/16 ×D, then apply the convolution layers and\nReLU, and ﬁnally, employ the bilinear interpolation to enlarge\nthe size of the feature maps by a factor of 4. Here, g(·) denotes\napplying the convolution layers, ReLU and the bilinear inter-\npolation to obtain the feature maps where the size is the same\nas fh1. From (8), we can see that it aggregate the multiscale\nfeature maps using a weighted strategy to take full advantage of\ndifferent contextual information.\nE. Heterogeneous Fusion Module (HFM)\nThe outputs of two decoders are two sets of feature maps fh1\nand fh2. They are heterogeneous because they are generated\nby different encoder–decoder architectures and contain different\nsemantic information. For the heterogeneous feature maps fh1\nand fh2, it is difﬁcult to mine useful information from them\nby directly concatenating or adding them together. In order to\nfully exploit the information from heterogeneous feature maps,\nwe propose the HFM to increase the interactions between the\nfeatures by using MSA.\nFig. 7 shows the framework of the HFM. We ﬁrst trans-\nform fh1 and fh2 into the sequences S1 ∈ RH/2×W/2·16 and\nS2 ∈ RH/2×W/2·16, respectively, where H/2 is the length of\nsequences and W/2 ·16 is the hidden channel size. The process\nof sequence construction is shown in Fig. 4. Speciﬁcally, we ﬁrst\nuniformly partition the feature maps fh1 ∈ RH/2×W/2×16 into\nL patches with the size of P ×P, where L =( H/2)/(P ×P).\nFor each patch, the patch EP is with the size of P ×P ×\nW/2 ·16. Afterwards, we ﬂatten EP into a 1-D vector V ∈\nR1×P·P·W/2·16, and then, project the vector dimension to C =\nW/2 ·16 through the linear layer. The feature fh2 constructs\nthe sequence S2 in the same way. Afterwards, we concatenate\nS1 and S2 as the MSA input Shi ∈ RH×W/2·16. Furthermore,\nwe utilize MSA to enhance the interactions between sequences.\nThe MSA operation is described in (9). Finally, we transform\nthe output of MSA Sho ∈ RH×W/2·16 into fhf ∈ RH×W×8 by\nthe reshaping operation.\nfhf = r (MSA (Cat(S1,S 2))) (9)\nwhere r(·) is the reshaping operation. From (9), we can see that\nthe HFM could sufﬁciently fuse the heterogenous feature maps\nand obtain discriminative features.\nAfter obtaining fhf , we employ the convolution layer to\nreduce its channel number to 2, and apply the Sigmoid function\nto generate the segmentation mask. Furthermore, we treat the\nbinary cross-entropy (BCE) loss as the objective function, which\noptimizes the difference between the ground-truth distribution\nand the predicted distribution. It is formulated as\nL = −1\nT\nT∑\ni=1\n[uilog(pi)+( 1 −ui)log(1 −pi)] (10)\nwhere T is the total number of pixels, ui is the ground-truth label\nfor the ith pixel, and pi is the predicted probability that the ith\npixel belongs to cloud.\nIV . E XPERIMENTS\nIn this section, we evaluate the performance of TransCloud-\nSeg for ground-based cloud image segmentation on the TLCDD\nand TCDD. We ﬁrst introduce TLCDD and TCDD, and then,\nthe implementation details of our experiments. Afterwards, we\nconduct a series of experiments to verify the superiority of\nTransCloudSeg, and study the inﬂuence of several important\nparameters.\nA. Tianjin Normal University Large-Scale Cloud Detection\nDatabase (TLCDD)\nThe TLCDD [31] is a large-scale ground-based cloud de-\ntection database, and it is collected over two years from nine\nprovinces of China including Tianjin, Anhui, Sichuan, Gansu,\nShandong, Hebei, Liaoning, Jiangsu, and Hainan. Hence, the\nTLCDD possesses high diversity of cloud images, which makes\nthe experimental results more convincing. The TLCDD consists\nof 5000 ground-based cloud images with corresponding ground-\ntruth cloud masks manually annotated by meteorologists and\ncloud-related researchers. To the best of our knowledge, it is\nthe largest public ground-based cloud segmentation database.\nThis database is divided into 4208 training images and 792 test\nimages. The cloud images are acquired by the visual sensor and\nstored with the resolution of 512 ×512. Fig. 8 shows some cloud\nimages and their corresponding cloud segmentation masks.\nThe TCDD [32] is composed of 2300 ground-based cloud\nimages. This dataset has high diversity of cloud shapes, and\nvariability of scenes, which makes the experiment more con-\nvincing.\nB. Implementation Details and Evaluation Criteria\nIn the training stage, we perform the preprocessing opera-\ntions on the ground-based cloud images. Speciﬁcally, we apply\nrandom resizing with the ratio between 0.5 and 1.5, random\ncropping, random horizontal ﬂipping, and normalization by the\nmean and standard deviation values.\nWe set the total number of epoch to 150 and the batch size to\n2 in the experiments. We employ the stochastic gradient descent\n(SGD) [42] as the optimizer with the initial learning rate of\nLIU et al.: TRANSCLOUDSEG: GROUND-BASED CLOUD IMAGE SEGMENTATION WITH TRANSFORMER 6127\nFig. 8. Some cloud images and the corresponding segmentation masks.\n0.01, the weight decay of 0.0001, and the momentum of 0.9.\nWe apply the “poly” learning rate strategy [43], [44], where the\nlearning rate of the current iteration is equal to the initial learning\nrate multiplied by a factor of (1 − iter\niter_num )power. Here, iter and\niter_num are the number of current iteration and the total number\nof iterations respectively, and power =0 .9.\nWe initialize the CNN encoder and the transformer encoder\nfor TransCloudSeg with pretrained model [30]. We set the num-\nber of transformer layers in the transformer encoder to 12 and\nthe number of heads in MSA to 16. We select [Y3,Y 6,Y 9,Y 12]\nas the input of the transformer decoder.\nIn order to quantitatively evaluate the proposed method, we\nutilize ﬁve evaluation criteria, i.e., Precision (Pre), Recall (Rec),\nF-score (F_s), Accuracy (Acc), and intersection over union\n(IoU), which are commonly used in the segmentation task. These\nevaluation criteria are deﬁned as\nPre = TP/(TP + FP) (11)\nRec = TP/(TP + FN) (12)\nF_s =2 Pre ×Rec/(Pre + Rec) (13)\nAcc =( TP + TN)/(TP + FP + TN + FN) (14)\nIoU = TP/(TP + FP + FN) (15)\nwhere TP, TN, FP, and FN indicate true positive, true negative,\nfalse positive, and false negative, respectively.\nC. Experimental Results\n1) Ablation Studies: The advantages of TransCloudSeg are\nto make full use of the CNN and transformer so as to learn de-\ntail information and global information for ground-based cloud\nimage segmentation, simultaneously. These advantages depend\non the three main components: two encoders, two decoders, and\nHFM. To demonstrate their contributions on the performance\nTABLE II\nCOMPARISON WITH DIFFERENT ABLATION METHODS\nimprovement of TransCloudSeg, we perform ablation experi-\nments on TransCloudSeg, and Table II lists the segmentation\nresults of different ablation methods.\na) C-C version: We design C-C as the CNN encoder–\ndecoder architecture. Speciﬁcally, the CNN encoder is used to\nlearn the multiscale feature maps from the input cloud image.\nThen, the CNN decoder utilizes these feature maps to generate\nsegmentation mask.\nb) T-T version:We implement T-T using the transformer\nencoder–decoder architecture. Concretely, the transformer en-\ncoder is used to learn the multilevel feature maps from the input\ncloud image. Then, the transformer decoder applies these feature\nmaps to generate segmentation mask.\nc) C+T-C version: We design C+T-C as the CNN–\ntransformer encoder and the CNN decoder, which is similar to\nTransUNet [30]. The CNN encoder and the transformer encoder\nare hybridized to learn different feature maps from the input\ncloud image. These feature maps are reshaped, and then, fed\ninto the CNN decoder to generate the segmentation mask.\nd) C+T-T version:We implement C+T-T using the CNN–\ntransformer encoder and the transformer decoder. Speciﬁcally,\nthe hybrid form of the CNN–transformer encoder is used to\n6128 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nlearn different feature maps from the input cloud image. Then,\nthe transformer decoder utilizes these feature maps to generate\nsegmentation mask.\ne) C+T-C+T(A) version: To prove the validity of the\nHFM, we design C+T-C+T(A) as the hybrid encoder–decoder\nstructure including the CNN and transformer encoders, and the\nCNN and transformer decoders. After obtaining the feature maps\nfrom the CNN and transformer decoders, we directly add them,\nand then, learn the segmentation mask.\nf) C+T-C+T(C) version: C+T-C+T(C) has similar struc-\nture with C+T-C+T(A) expect that the two sets of output feature\nmaps from the CNN and transformer decoders are directly\nconcatenated.\nFrom Table II, we can draw several conclusions. First, the\nproposed TransCloudSeg achieves the best results in all ﬁve eval-\nuation criteria, which validates the effectiveness of our method.\nSecond, the methods with the hybrid CNN–Transformer en-\ncoder, i.e., “C+T-C” and “C+T-T” exceed “C-C” and “T-T.”\nIt demonstrates that the hybrid encoders could extract detailed\ninformation and global contextual information simultaneously,\nwhich is beneﬁcial to the ground-based cloud image seg-\nmentation. Third, TransCloudSeg, “C+T-C+T(A),” and “C+T-\nC+T(C)” surpass “C+T-C” and “C+T-T” because the hybrid\ndecoders, i.e., CNN decoder and transformer decoder could learn\ninformation from different aspects. Finally, TransCloudSeg ob-\ntains better results than “C+T-C+T(A)” and “C+T-C+T(C),”\nwhich validates the effectiveness of the HFM. It is because the\nHFM considers the interactions between heterogeneous feature\nmaps.\n2) Comparisons With State-of-the-Art Methods:In this sec-\ntion, we compare TransCloudSeg with other state-of-the-art\nground-based cloud image segmentation methods. The com-\npared methods include both traditional methods and CNN-based\nmethods. Traditional methods usually apply color values as\nthe thresholds including R/B (0.6) [17], B/R (Otsu) [45], B-R\n(Otsu) [45], and (B-R)/(B+R) (Otsu) [45]. Here, R and B repre-\nsent the values of the red and blue channels in the ground-based\ncloud image, respectively. For example, R/B (0.6) indicates that\nwhen the ratio of R and B is smaller than 0.6, the pixel is\nidentiﬁed as sky, otherwise as cloud. Furthermore, (Otsu) means\nthat the algorithm proposed by Otsu [46] is used to automatically\nselect the segmentation threshold.\nAs for the CNN-based methods, we compare TransCloudSeg\nwith ﬁve state-of-the-art CNN-based methods for ground-based\ncloud image segmentation, including FCN [47], CloudSeg-\nNet [22], U-Net [48], SegCloud [23], and PSPNet [49].\nThe comparison results between TransCloudSeg and differ-\nent methods on TLCDD are shown in Table III. From the\ntable, we can see that the proposed TransCloudSeg outperforms\nother methods on all ﬁve evaluation criteria. Speciﬁcally, it\nsurpasses the second best results by 6.59%, 1.03%, 9.95%,\n11.91%, and 11.32% in Precision, Recall, F-score, Accuracy,\nand IoU, respectively. Furthermore, the CNN-based methods\ngenerally perform better than the traditional methods. It is be-\ncause the multilayer structure of CNN could learn complex fea-\nture transformations, and therefore, the discriminative features\nare obtained.\nTABLE III\nCOMPARISON WITH STATE-OF-THE-ART METHODS\nTABLE IV\nRUN TIME COMPARISON OF DIFFERENT METHODS\nTABLE V\nCOMPARISON OF THE RECEPTIVE FIELD SIZES OF THE ENCODER FROM\nDIFFERENT CNN-BASED METHODS\nWe present the runtime comparison of different methods as\nshown in Table IV. From the table, we can see that the traditional\nmethods have less runtime than the CNN-based methods. It\nis because the CNN-based methods require a large number of\nparameters to represent complex features. Moreover, the net-\nwork with large number of parameters can learn more complex\nfeatures, but requires more running time.\nThe proposed method makes tradeoff between performance\nand runtime, and the runtime to process one ground-based cloud\nimage is 87.21 ms. The acquisition device takes approximately\n2 min to acquire one ground-based cloud image. Therefore, our\nmethod could satisfy the practical requirements. Note that all\nthe runtime tests are performed with the workstation equipped\nwith E5-1620V4 CPU, 32-GB RAM and NVIDIA RTX 2080Ti\n12-GB GPUs.\nWe list the comparison of the receptive ﬁeld sizes of the\nencoder from different CNN-based methods in Table V. The\nLIU et al.: TRANSCLOUDSEG: GROUND-BASED CLOUD IMAGE SEGMENTATION WITH TRANSFORMER 6129\nFig. 9. Segmentation masks of different methods. (a) Cloud images. (b) Ground-truth segmentation masks. (c) R/B (0.6). (d) (B-R)/(B+R)(Otsu). (e) F CN.\n(f) U-Net. (g) PSPNet. (h) CloudSegNet. (i) SegCloud. (j) TransCloudSeg.\nTABLE VI\nCOMPARISON WITH STATE-OF-THE-ART METHODS ON THE TCDD\nproposed TransCloudSeg has a global receptive ﬁeld regardless\nof the size of the input images. From Tables III and V, we can\nsee that the proposed method obtains a better performance than\nthese CNN-based methods.\nTo demonstrate the robustness of the proposed method, we\nalso perform a series of comparison experiments on the TCDD.\nThe comparison results of different methods on TCDD are\nshown in Table VI. As we can see from the table, the proposed\nTransCloudSeg achieves the best results on all ﬁve evaluation\ncriteria once again. The experimental results demonstrate the\nrobustness of the proposed method.\n3) Visualization: To qualitatively prove the effectiveness of\nTransCloudSeg, we visualize some segmentation masks of the\ncomparison methods in Fig. 9. We can see from the ﬁgure that\nour method achieves superior performance than other methods,\nespecially for thin clouds, thick clouds, and the areas affected by\nillumination. Furthermore, the CNN-based methods generally\nperform better than the traditional methods.\nIn Fig. 9, the green, red, and blue rectangles indicate thin\nclouds, thick clouds, and areas affected by illumination. As for\nthe thin clouds, they are easily ignored, and as for the thick\nclouds, they are usually classiﬁed into the sky labels. While the\nproposed TransCloudSeg could classify them correctly. In the\nfourth row of Fig. 9, the sample is affected by illumination,\nwhich is a challenge for cloud image segmentation. From the\nblue rectangles in the fourth row of Fig. 9, we can see that most\nmethods misclassify the sky as clouds because the illumination\ncauses the sky to appear white. While our method ensures the\npromising performance with large illumination variations.\n4) Analysis of Different Training Sample Proportion:The\nproportion of training and test samples in Table III is about 4:1.\nTo demonstrate the generalizability of the model, we add two\nsets of experiments with different proportions of training and\ntest samples on TLCDD. We list the comparison performance\nof different methods with different proportions of training and\ntest samples in Table VII. From the table, our method still\noutperforms other methods when the number of training samples\nis different, which proves the generalization of the model. Fur-\nthermore, the traditional methods utilize the prior information\nto segment the cloud images, which is irrelevant to the number\nof training samples.\n5) Parameters Analysis:We investigate the impact of several\nimportant parameters for our method. They are the number of\nskip connections in the CNN decoder, and the level number of\nthe aggregated feature maps in the transformer decoder.\na) Number of skip connections: In the CNN decoder of\nTransCloudSeg, we utilize the skip connections to integrate\nmultiscale feature maps from the CNN encoder, which recov-\ners the detail information. We conduct the experiments with\ndifferent numbers of skip connections as shown in Fig. 10.\nHere, “ Skip =0 , 1, 2, 3” indicates that we treat [f3], [f2,f 3],\n[f1,f 2,f 3], and [f0,f 1,f 2,f 3] as the inputs of the CNN decoder,\nrespectively. From the ﬁgure, we can see that the performance of\n6130 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE VII\nCOMPARISON WITH STATE-OF-THE-ART METHODS WITH DIFFERENT\nPROPORTIONS OF TRAINING AND TEST SAMPLES\nFig. 10. Performance of TransCloudSeg with different number of skip con-\nnections in the CNN decoder.\nﬁve evaluation criteria improves as the number of skip connec-\ntions increases. Hence, we set the number of the skip connections\nto 3 in our experiments.\nb) Level Number of the Aggregated Feature Maps:In the trans-\nformer decoder of TransCloudSeg, we apply the weight aggrega-\ntion to integrate the multilevel feature maps. We conduct the ex-\nperiments with different level number of the aggregated feature\nmaps as shown in Fig. 11. Here, “ Layer =2 , 4, 6” indicates that\nwe treat [Y6,Y 12], [Y3,Y 6,Y 9,Y 12], and [Y2,Y 4,Y 6,Y 8,Y 10,Y 12]\nas the inputs of the transformer decoder, respectively. As can\nbe seen from the ﬁgure, the best performance of the proposed\nTransCloudSeg is achieved when the level number of aggregated\nfeature maps is set to 4.\nFig. 11. Performance of TransCloudSeg with different level number of the\naggregated feature maps in the transformer decoder.\n6) Computational Complexity Analysis:The computational\ncomplexity of the proposed TransCloudSeg is dominated by\nCNN and MSA. According to [24], [41], and [50], the jth\nlayer computational complexity is O(Cj−1 ·Cj ·k2 ·r2), where\nCj−1 is the number of input channels of the jth layer, Cj is the\nnumber of output channels of the jth layer, i.e., the number of\nﬁlters of the jth layer convolution operation, k is the spatial\nsize of each convolution ﬁlter, and r is the spatial size of output\nfeature maps. The computational complexity of each layer of\nMSA is O(L ·D2 + L2 ·D), where L is the length of sequences\nand D is the hidden channel size.\nV. C ONCLUSION\nIn this article, we have proposed TransCloudSeg for the\nground-based cloud image segmentation. To the best of our\nknowledge, we are the ﬁrst to apply transformer to the ground-\nbased cloud image segmentation task. Speciﬁcally, we present\nthe CNN encoder and the transformer encoder to extract de-\ntailed information and global information from ground-based\ncloud images. Meanwhile, we design two decoders, i.e., CNN\ndecoder and transformer decoder to integrate the multiscale and\nmultilevel feature maps extracted from the two encoders. Fur-\nthermore, we propose HFM to effectively exploit the information\ncontained in the heterogeneous feature maps from the CNN and\ntransformer decoders in order to generate accurate segmenta-\ntion mask. Extensive experimental results have demonstrated\nthe effectiveness of the proposed TransCloudSeg on TLCDD\nand TCDD. In the future, we will try to build light-weight\ntransformer-based architecture to reduce the number of param-\neters so as to decrease the complexity and speed up the training\nprocess. Furthermore, we will study some speciﬁc scenes for\nground-based cloud image segmentation, such as nighttime, etc.\nREFERENCES\n[1] A. Heinle, A. Macke, and A. Srivastav, “Automatic cloud classiﬁcation of\nwhole sky images,” Atmospheric Meas. Techn., vol. 3, no. 3, pp. 557–567,\n2010.\n[2] Y . Wang, C. Wang, C. Shi, and B. Xiao, “A selection criterion for the\noptimal resolution of ground-based remote sensing cloud images for cloud\nclassiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 57, no. 3, pp. 1358–\n1367, Mar. 2019.\nLIU et al.: TRANSCLOUDSEG: GROUND-BASED CLOUD IMAGE SEGMENTATION WITH TRANSFORMER 6131\n[3] S. Liu, L. Duan, Z. Zhang, X. Cao, and T. S. Durrani, “Multimodal\nground-based remote sensing cloud classiﬁcation via learning heteroge-\nneous deep features,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 11,\npp. 7790–7800, Nov. 2020.\n[4] D. Hong, N. Yokoya, J. Chanussot,, and X. X. Zhu, “An augmented linear\nmixing model to address spectral variability for hyperspectral unmixing,”\nIEEE Trans. Image Process., vol. 28, no. 4, pp. 1923–1938, Apr. 2019.\n[5] C. Shi, Y . Zhou, B. Qiu, D. Guo, and M. Li, “CloudU-Net: A deep\nconvolutional neural network architecture for daytime and nighttime cloud\nimages’ segmentation,”IEEE Geosci. Remote Sens. Lett., vol. 18, no. 10,\npp. 1688–1692, 2021.\n[6] L. Ye, Z. Cao, and Y . Xiao, “DeepCloud: Ground-based cloud image\ncategorization using deep convolutional features,” IEEE Trans. Geosci.\nRemote Sens., vol. 55, no. 10, pp. 5729–5740, Oct. 2017.\n[7] A. Taravat, F. D. Frate, C. Cornaro, and S. Vergari, “Neural networks\nand support vector machine algorithms for automatic cloud classiﬁcation\nof whole-sky ground-based images,” IEEE Geosci. Remote Sens. Lett.,\nvol. 12, no. 3, pp. 666–670, Mar. 2015.\n[8] M. C. Allmen and W. P. Kegelmeyer Jr., “The computation of cloud base\nheight from paired whole-sky imaging cameras,” Mach. Vis. Appl.,v o l .9 ,\nno. 4, pp. 160–165, 1997.\n[9] E. Kassianov, C. N. Long, and J. Christy, “Cloud-base-height estimation\nfrom paired ground-based hemispherical observations,” J. Appl. Meteorol.\nClimatol., vol. 44, no. 8, pp. 1221–1233, 2005.\n[10] N. B. Blum et al., “Cloud height measurement by a network of all-sky\nimagers,” Atmospheric Meas. Techn., vol. 14, no. 7, pp. 5199–5224,\n2021.\n[11] S. Liu, C. Wang, B. Xiao, Z. Zhang, and X. Cao, “Tensor ensemble of\nground-based cloud sequences: Its modeling, classiﬁcation, and synthe-\nsis,” IEEE Geosci. Remote Sens. Lett., vol. 10, no. 5, pp. 1190–1194,\nSep. 2013.\n[12] S. Liu, C. Wang, B. Xiao, Z. Zhang, and Y . Shao, “Salient local binary\npattern for ground-based cloud classiﬁcation,” Acta Meteorologica Sinica,\nvol. 27, no. 2, pp. 211–220, 2013.\n[13] S. Liu, Z. Zhang, B. Xiao, and X. Cao, “Ground-based cloud detection\nusing automatic graph cut,” IEEE Geosci. Remote Sens. Lett., vol. 12,\nno. 6, pp. 1342–1346, Jun. 2015.\n[14] C. Shi, Y . Wang, C. Wang, and B. Xiao, “Ground-based cloud detection\nusing graph model built upon superpixels,” IEEE Geosci. Remote Sens.\nLett., vol. 14, no. 5, pp. 719–723, May 2017.\n[15] S. Dev, Y . H. Lee, and S. Winkler, “Color-based segmentation of sky/cloud\nimages from ground-based cameras,” IEEE J. Sel. Topics Appl. Earth\nObserv. Remote Sens., vol. 10, no. 1, pp. 231–242, Jan. 2017.\n[16] W. Li, Z. Zou, and Z. Shi, “Deep matting for cloud detection in remote\nsensing images,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 12,\npp. 8490–8502, Dec. 2020.\n[17] C. N. Long, J. M. Sabburg, J. Calb, and D. Pages, “Retrieving cloud charac-\nteristics from ground-based daytime color all-sky images,” J. Atmospheric\nOcean. Technol., vol. 23, no. 5, pp. 633–653, 2006.\n[18] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\npp. 436–444, 2015.\n[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[20] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza,, and J. Chanussot,\n“Graph convolutional networks for hyperspectral image classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 59, no. 7, pp. 5966–5978,\n2020.\n[21] D. Hong et al., “More diverse means better: Multimodal deep learning\nmeets remote-sensing imagery classiﬁcation,” IEEE Trans. Geosci. Re-\nmote Sens., vol. 59, no. 5, pp. 4340–4354, May 2021.\n[22] S. Dev, A. Nautiyal, Y . H. Lee, and S. Winkler, “CloudSegNet: A deep\nnetwork for nychthemeron cloud image segmentation,” IEEE Geosci.\nRemote Sens. Lett., vol. 16, no. 12, pp. 1814–1818, 2019.\n[23] W. Xie et al., “SegCloud: A novel cloud image segmentation model using a\ndeep convolutional neural network for ground-based all-sky-view camera\nobservation,” Atmospheric Meas. Techn., vol. 13, no. 4, pp. 1953–1961,\n2020.\n[24] A. Vaswani, N. Shazeer, N. Parmar, and J. Uszkoreit, “Attention is all you\nneed,”Adv. Neural Inf. Process. Syst., 2017, pp. 6000–6010.\n[25] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020, arXiv:2010.11929. [Online]. Available:\nhttps://arxiv.org/abs/2010.11929\n[26] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable transformers for end-to-end object detection,” in Proc. Int.\nConf. Learn. Representations, 2021, pp. 1–26.\n[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” 2019,\narXiv:1810.04805. [Online]. Available: https://arxiv.org/abs/1810.04805\n[28] S. Zheng et al., “Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers,” in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2021, pp. 6881–6890.\n[29] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler, “U-\nnet transformer: Self and cross attention for medical image segmentation,”\nin Proc. Int. Workshop Mach. Learn. Med. Imag., 2021, pp. 267–276.\n[30] J. Chen et al., “Transunet: Transformers make strong encoders for medical\nimage segmentation,” 2021,arXiv:2102.04306. [Online]. Available: https:\n//arxiv.org/abs/2102.04306\n[31] Z. Zhang, S. Wang, S. Liu, X. Cao, and T. S. Durrani., “Ground-based\nremote sensing cloud detection using dual pyramid network and encoder\ndecoder constraint,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5620912, doi: 10.1109/TGRS.2022.3163917.\n[32] Z. Zhang, S. Wang, S. Liu, B. Xiao, and X. Cao, “Ground based\ncloud detection using multiscale attention convolutional neural network,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, 2022, Art. no. 8019605,\ndoi: 10.1109/LGRS.2021.3106337.\n[33] Z. Zhou et al., “A novel ground-based cloud image segmentation method\nby using deep transfer learning,” IEEE Geosci. Remote Sens. Lett., vol. 19,\n2022, Art. no. 8010805, doi: 10.1109/LGRS.2021.3072618.\n[34] K. Han et al., “A survey on visual transformer,” 2020, arXiv:2012.12556.\n[Online]. Available: https://arxiv.org/abs/2012.12556\n[35] B. Wu et al., “Visual transformers: Token-based image representation\nand processing for computer vision,” 2020, arXiv:2006.03677. [Online].\nAvailable: https://arxiv.org/abs/2006.03677\n[36] L. Yuan et al., “Tokens-to-token ViT: Training vision transformers from\nscratch on imagenet,” 2021, arXiv:2101.11986. [Online]. Available: https:\n//arxiv.org/abs/2101.11986\n[37] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“SegFormer: Simple and efﬁcient design for semantic segmentation with\ntransformers,” 2021,arXiv:2105.15203. [Online]. Available: https://arxiv.\norg/abs/2105.15203\n[38] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” 2021, arXiv:2103.14030. [Online]. Available: https:\n//arxiv.org/abs/2103.14030\n[39] F. Zhu, Y . Zhu, L. Zhang, C. Wu, Y . Fu, and M. Li, “A uniﬁed efﬁcient\npyramid transformer for semantic segmentation,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2021, pp. 2667–2677.\n[40] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection\nwith transformers,” IEEE Trans. Geosci. Remote Sens., vol. 60, 2022,\nArt. no. 5607514, doi: 10.1109/TGRS.2021.3095166.\n[41] D. Hong et al., “SpectralFormer: Rethinking hyperspectral image classi-\nﬁcation with transformers,” IEEE Trans. Geosci. Remote Sens., vol. 60,\n2021, Art. no. 5518615.\n[42] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of\ninitialization and momentum in deep learning,” in Proc. Int. Conf. Mach.\nLearn., 2013, pp. 1139–1147.\n[43] L. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous\nconvolution for semantic image segmentation,” 2017, arXiv:1706.05587.\n[Online]. Available: https://arxiv.org/abs/1706.05587\n[44] F. Zhang et al., “ACFNet: Attentional class feature network for semantic\nsegmentation,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019,\npp. 6798–6807.\n[45] J. Yang, W. Lu, Y . Ma, W. Yao, and Q. Li, “An automatic ground-based\ncloud detection method based on adaptive threshold,” J. Appl. Meteoro-\nlogical Sci., vol. 20, no. 6, pp. 713–721, 2009.\n[46] N. Otsu, “A threshold selection method from gray-level histograms,” IEEE\nTrans. Syst., Man, Cybern., vol. SMC-9, no. 1, pp. 62–66, Jan. 1979.\n[47] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2015, pp. 3431–3440.\n[48] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Proc. Int. Conf. Med. Image\nComput. Comput. Assist. Intervention, 2015, pp. 234–241.\n[49] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017,\npp. 2881–2890.\n6132 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\n[50] K. He and J. Sun, “Convolutional neural networks at constrained time\ncost.,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 5353–\n5360.\nShuang Liu (Senior Member, IEEE) received the\nPh.D. degree in pattern recognition and intelligent\nsystems from the Institute of Automation, Chinese\nAcademy of Sciences, Beijing, China, in 2014 .\nShe is currently a Professor with Tianjin Normal\nUniversity, Tianjin, China. She has authored and\ncoauthored more than 50 articles in major interna-\ntional journals and conferences. Her research interests\ninclude remote sensing, computer vision, and deep\nlearning.\nJiafeng Zhang is currently working toward the mas-\nter degree in information and communication en-\ngineering with Tianjin Normal University, Tianjin,\nChina.\nHis research interests include ground-based cloud\nanalysis and deep learning.\nZhong Zhang (Senior Member, IEEE) received the\nPh.D. degree in pattern recognition and intelligent\nsystems from the Institute of Automation, Chinese\nAcademy of Sciences, Beijing, China.\nHe is a Professor with Tianjin Normal University,\nTianjin, China. He has authored and co-authored\nabout 110 articles in international journals and confer-\nences such as IEEE T RANSACTIONS ON GEOSCIENCE\nAND REMOTE SENSING, IEEE T RANSACTIONS ON\nFUZZY SYSTEMS, Pattern Recognition, IEEE T RANS-\nACTIONS ON CIRCUITS SYSTEMS VIDEO TECHNOL-\nOGY, IEEE TRANSACTIONS ON INFORMATIONFORENSICS AND SECURITY, Signal\nProcessing(Elsevier), IEEE Conference on Computer Vision and Pattern Recog-\nnition), International Conference on Pattern Recognition, and International\nConference on Image Processing. His research interests include remote sensing,\ncomputer vision, and deep learning.\nXiaozhong Cao received the Ph.D. degree in automatic control theory and\napplication from the Institute of Automation, Chinese Academy of Sciences,\nBeijing, China, in 1996.\nHe is currently a Professor with Meteorological Observation Centre, China\nMeteorological Administration. His current research interests include the theory\nof meteorological observation and climate change, and the automatic meteoro-\nlogical observation.\nTariq S. Durrani (Life Fellow, IEEE) received the\nPh.D. degree from the University of Southampton,\nU.K., in 1970.\nHe is a Research Professor with the University\nof Strathclyde, Glasgow, U.K. His research interests\ninclude artiﬁcial intelligence, signal processing, and\ntechnology management. He has authored 350 publi-\ncations and supervised 45 Ph.Ds.\nProf. Durrani is a Fellow of the U.K. Royal\nAcademy of Engineering, Royal Society of Edin-\nburgh, IET, and the Third World Academy of Sci-\nences. He was elected Foreign Member of the Chinese Academy of Sciences\nand the U.S. National Academy of Engineering in 2021 and 2018, respectively."
}