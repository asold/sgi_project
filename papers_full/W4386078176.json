{
  "title": "A Transformer-Based Educational Virtual Assistant Using Diacriticized Latin Script",
  "url": "https://openalex.org/W4386078176",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105794017",
      "name": "Khang Nhut Lam",
      "affiliations": [
        "Can Tho University"
      ]
    },
    {
      "id": "https://openalex.org/A3209043443",
      "name": "Loc Huu Nguy",
      "affiliations": [
        "Can Tho University"
      ]
    },
    {
      "id": "https://openalex.org/A2042387978",
      "name": "Van Lam Le",
      "affiliations": [
        "Can Tho University"
      ]
    },
    {
      "id": "https://openalex.org/A2344103259",
      "name": "Jugal Kalita",
      "affiliations": [
        "University of Colorado Colorado Springs"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3038115231",
    "https://openalex.org/W3013971782",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3194005300",
    "https://openalex.org/W3200959192",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3164971729",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W3014493394",
    "https://openalex.org/W2895875496",
    "https://openalex.org/W3004392311",
    "https://openalex.org/W6851564937",
    "https://openalex.org/W3085904224",
    "https://openalex.org/W3017844056",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W4200020726",
    "https://openalex.org/W4321499901",
    "https://openalex.org/W4378805624",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W4366679995",
    "https://openalex.org/W4377227441",
    "https://openalex.org/W4318257512",
    "https://openalex.org/W6849636505",
    "https://openalex.org/W6850065652",
    "https://openalex.org/W6851455388",
    "https://openalex.org/W2978585381",
    "https://openalex.org/W6772715161",
    "https://openalex.org/W6743716270",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W3035245013",
    "https://openalex.org/W2739401819",
    "https://openalex.org/W4383753013",
    "https://openalex.org/W4363678768",
    "https://openalex.org/W4362466759",
    "https://openalex.org/W3088975003",
    "https://openalex.org/W3205889677",
    "https://openalex.org/W3127222075",
    "https://openalex.org/W3203916972",
    "https://openalex.org/W6794919052",
    "https://openalex.org/W4205201623",
    "https://openalex.org/W4321850160",
    "https://openalex.org/W6851572264",
    "https://openalex.org/W6929487261",
    "https://openalex.org/W6790682369",
    "https://openalex.org/W3151268590",
    "https://openalex.org/W4214877181",
    "https://openalex.org/W4289926633",
    "https://openalex.org/W3212628932",
    "https://openalex.org/W2996524796",
    "https://openalex.org/W3040398327",
    "https://openalex.org/W2954769605",
    "https://openalex.org/W2807551929",
    "https://openalex.org/W3199323859",
    "https://openalex.org/W3135862422",
    "https://openalex.org/W6780739121",
    "https://openalex.org/W2948556593",
    "https://openalex.org/W4251068072",
    "https://openalex.org/W6746512435",
    "https://openalex.org/W2969852700",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4366489368",
    "https://openalex.org/W2751124354",
    "https://openalex.org/W4366989878",
    "https://openalex.org/W3101540747",
    "https://openalex.org/W3038968242",
    "https://openalex.org/W3133056742",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W3159071248",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4361988710",
    "https://openalex.org/W4293459943",
    "https://openalex.org/W3101817723",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4327918021"
  ],
  "abstract": "A virtual assistant or smart chatbot should be able to understand user questions and respond correctly and usefully, even if the questions are posed ungrammatically with misspellings and other errors. This paper describes the design and construction of a text-to-text virtual assistant in Vietnamese, a language that uses the Latin script with a liberal use of diacritics, for supporting students at a large university with over forty thousand students. The flexible virtual assistant consists of two integrated chatbots, both using Transformers: a) a closed-domain chatbot, trained on over thirty-five thousand factual question-answer pairs to engage in university-related conversation, and b) a second open-domain chatbot, trained on a large movie dialog dataset to engage in general conversation. The integrated virtual assistant classifies a question as either factual or general, and engages the appropriate chatbot to respond in a flexible, appropriate and natural manner. Although Vietnamese uses diacritics copiously, even educated users have a propensity to forgo the use of diacritics, and as a result, to facilitate smooth text-based communication, our design includes extensive pre-processing that uses learned Transformers to restore missing diacritics and correct misspellings. Our Transformer models outperform existing approaches for diacritic restoration and are better than several other methods for spelling correction in Vietnamese. In addition, the closed-domain chatbot performs better than other generative chatbots that have been developed to assist students in a university environment, irrespective of language and location.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nA Transformer-based Educational Virtual\nAssistant Using Diacriticized Latin Script\nKHANG NHUT LAM1, LOC HUU NGUY1, VAN LAM LE1, and JUGAL KALITA2\n1Can Tho University, Can Tho, Vietnam\n2University of Colorado, Colorado Springs, USA\nCorresponding author: Khang Nhut Lam (e-mail: lnkhang@ctu.edu.vn).\nABSTRACT A virtual assistant or smart chatbot should be able to understand user questions and respond\ncorrectly and usefully, even if the questions are posed ungrammatically with misspellings and other errors.\nThis paper describes the design and construction of a text-to-text virtual assistant in Vietnamese, a language\nthat uses the Latin script with a liberal use of diacritics, for supporting students at a large university with\nover forty thousand students. The flexible virtual assistant consists of two integrated chatbots, both using\nTransformers: a) a closed-domain chatbot, trained on over thirty-five thousand factual question-answer pairs\nto engage in university-related conversation, and b) a second open-domain chatbot, trained on a large movie\ndialog dataset to engage in general conversation. The integrated virtual assistant classifies a question as\neither factual or general, and engages the appropriate chatbot to respond in a flexible, appropriate and natural\nmanner. Although Vietnamese uses diacritics copiously, even educated users have a propensity to forgo the\nuse of diacritics, and as a result, to facilitate smooth text-based communication, our design includes extensive\npre-processing that uses learned Transformers to restore missing diacritics and correct misspellings. Our\nTransformer models outperform existing approaches for diacritic restoration and are better than several other\nmethods for spelling correction in Vietnamese. In addition, the closed-domain chatbot performs better than\nother generative chatbots that have been developed to assist students in a university environment, irrespective\nof language and location.\nINDEX TERMS chatbot, diacritics restoration, educational chatbot, misspellings, Transformer, virtual\nassistant.\nI. INTRODUCTION\nVirtual assistants, or chatbots, have exploded in use to support\nbusiness activities, answer questions related to citizen public\nservices, and provide answers to users such as patients or\nstudents. This paper discusses building smart chatbots based\non neural network approaches. One can construct a chatbot\nusing retrieval-based neural networks or generation-based\nneural network methods. Chatbots built using the first method\nmay give answers with correct grammar and spelling, whereas\nthe second method may or may not. However, if users ask\nquestions that are not in the training dataset, a chatbot built\nusing the retrieval-based method cannot answer them while\nthe other one can [1].\nOne of our goals is to construct a chatbot to support\nstudents in a university environment such that this chatbot\ncan generate new responses based on inputs and does not\ndepend on third parties. Moreover, Caldarini et al. [1] claim\nthat most existing chatbots in the domain of education are\nconstructed using retrieval-based models, which cannot an-\nswer questions outside the pre-defined question-answer pairs.\nTherefore, constructing a chatbot using the generation-based\napproach is the most suitable choice for us. Although students\ncan find information in many ways, such as perusing websites\nand asking friends, faculties, or advisors, many students do\nnot want to talk to other people or do not know where to\nfind information (a conclusion based on a limited informal\nsurvey). As a proof-of-concept, we build a text-to-text virtual\nassistant for students in Vietnamese, named VietBOT. The\nchatbot answers questions related to programs, staff, aca-\ndemic regulations, courses, semester examination schedules,\nand so on.\nRaval [2] highlights the challenges of chatbots, including\nunderstanding intents behind questions written in various\nwriting styles, handling local vocabularies, synonyms, and\nsentiment analysis. For languages using Latin script with\ndiacritics such as Vietnamese, diacritics are important; miss-\ning or typing incorrect tones and diacritics may change the\nmeaning of a word. However, it is normal for students to type\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nquestions with words without using tones and diacritics, or us-\ning misspelled words. Therefore, we propose a model to help\nthe chatbot handle questions with these issues. In addition,\nour chatbot model trains faster than other existing models for\nconstructing a chatbot to assist students in Vietnamese.\nThe contributions of this paper are enumerated below.\n• A Transformer-based model for word diacritic restora-\ntion that can be used for languages using Latin script\nwith diacritics.\n• A Transformer-based model for spelling correction that\nhandles mistyped and misspelled words created by sev-\neral text input approaches for a highly diacriticized lan-\nguage, viz., Vietnamese.\n• An architecture for a generative method using Trans-\nformer for constructing a chatbot to assist students in a\nuniversity environment, and Transformer-based models\nfor handling questions with various error levels of users’\ntyping.\n• A method using the kNN algorithm to combine the\nclosed and open chatbots in one application to support\nstudents.\n• Suggestion of methods to construct a bank of question-\nanswer pairs for training the closed chatbot in a univer-\nsity domain.\n• Two Vietnamese datasets can be used to train chatbot\nmodels on both closed and open domains.\nThe remainder of this paper is organized as follows. Sec-\ntion II presents related work. In Section III, we discuss di-\nacritics in languages using the Latin script. Our approaches\nto construct chatbots which have the ability to handle mis-\nspelled words and words with no diacritics are described in\nSection IV. The results of our experiments and discussion are\npresented in Section V. Section VI concludes the paper.\nII. LITERATURE REVIEW\nThis section summarizes common neural network approaches\nto construct generative chatbots, and then discusses frame-\nworks used to build virtual assistants in the university en-\nvironment. Next, we present the benefits and drawbacks of\nChatGPT in education, which motivate our selection to use\nTransformer-based models for building a virtual assistant\ninstead of using ChatGPT. Finally, we discuss Google Bard\nand Bing Chat.\nA. GENERATION-BASED NEURAL NETWORK CHATBOTS\nCommon models to construct chatbots use deep learning\nin the form of sequence-to-sequence (seq2seq) models such\nas LSTMs [3]–[5], and Bidirectional RNNs [6]. Additional\nseq2seq models include seq2seq with the evolved Trans-\nformer [7], seq2seq with attention [8], reinforcement learn-\ning for seq2seq [9], knowledge graph and hierarchical bi-\ndirectional attention [10], and deep seq2seq with GRU cells\n[11]. Since Transformer [12] is the state-of-the-art model in\nnatural language processing, several chatbots have been con-\nstructed using Transformer-based models such as Generative\nPre-trained Transformer (GPT) [13], [14], Transformer mod-\nels trained on augmented human data [15], Deep Bidrectional\nTransformer or BERT [16], BERT with Knowledge Graph\n[17], and BERT with Google Dialogflow [18]. Blender-\nBot1, based on a combination of retrieval and generation\napproaches [19] to create responses by using the knowledge\ninside and outside the content of the conversation, outper-\nforms existing chatbots such as Meena [7] and DialoGPT\n[14].\nB. FRAMEWORK-BASED CHATBOTS\nChatbots can be constructed by utilizing chatbot platforms. A\nsocial assistant named LiSA [20] and an APU Admin Bot [21]\nwere constructed using the Chatfuel 2 chatbot platform to help\nstudents in university living, and administrative and academic\nissues, respectively. The commercial Chatfuel platform uses\na rule-based approach for pattern recognition for mapping\ninput questions to outputs through an artificial intelligence\nmodel. Chatfuel is inflexible in conversation flows and does\nnot support knowledge-based flows [22]. Although Chatfuel\nis the most commonly used platform, it allows users to build\nstraightforward chatbots3.\nBarus and Surijati [23] build an assistant for Frequently\nAsked Questions services in Matana University Library in In-\ndonesia using Dialogflow 4, focusing on core NLP. The chat-\nbot does not provide correct responses when users use mixed\nlanguages, abbreviations, or new synonyms. An English-\nArabic chatbot for university admission [24] was built using\nthe Dialogflow Essentials 5 platform, based on BERT-based\nNLU models [25]. Dialogflow is simple for users to create\nchatbots and supports NLP tasks very well, but it does not\nfeature any graphical tools [26].\nThe Microsoft Bot Frameworks 6 provide an all-purpose\nopen-source chatbot platform with two main components,\nincluding Bot Builder SDK for developing bots, and Bot\nConnector Service for relaying information between bots and\nchannels. The Microsoft Bot Frameworks have been used\nto construct a career counseling chatbot to support Cluj-\nNapoca’s students at Technical University in Romania [27],\nand to build an undergraduate admission chatbot for the Pa-\nmantasan ng Lungsod ng Maynila [28].\nAnother common open-source framework to construct\nchatbots, called Rasa7, has been used to construct chatbots for\nadmission [29], university inquiries [30], [31], and answering\nquestions regarding school, academic regulations, and classes\n[32]. The Rasa framework consists of two main components:\nRasa NLU, with the help of neural networks, for classifying\nintents, extracting entities, and understanding user messages;\nand Rasa Core for handling the contexts of conversations\n1https://parl.ai/projects/blenderbot2/\n2https://chatfuel.com/\n3https://www.rootinfosol.com/chatbot-development-chatfuel-platform\n4https://cloud.google.com/dialogflow/docs\n5https://cloud.google.com/dialogflow\n6https://dev.botframework.com/\n7https://rasa.com/\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nusing a probabilistic model. Singh and Singh [33] perform\nexperiments on building chatbots for the Central University\nof Punjab, Bathinda, to analyze the effectiveness of the Rasa\nand Dialogflow chatbot frameworks. The authors claim that\n63% users found that the chatbot built using Rasa was more\neffective than the one created using Dialogflow.\nC. CHATGPT\nSince late 2022, it may be claimed that ChatGPT 8, a text-\nbased chatbot based on a generative pre-trained Transformer\nGPT model, has dominated discussion of chatbots. ChatGPT\nhas rapidly gained attention for its potential applications in\na variety of fields such as healthcare [34], tourism [35],\nmarketing [36], agriculture [37], cosmetic surgery [38], com-\nmunications [39], and education [40].\nThe development of education-oriented chatbots has bene-\nfited significantly from ChatGPT [41]–[43]. ChatGPT-3 has\nbeen impressive, receiving a B on the final exam of an MBA\ncourse [44], obtaining a C+ on law school exams [45], and\nperforming at the level of third-year medical students [46].\nHowever, Wood et al. [47] report that chatGPT ‘‘performed\nbetter in answering true/false and multiple-choice questions’’\nand ‘‘struggled with workout and short-answer questions,\nwith accuracy rates of 28.7 percent and 39.1 percent, re-\nspectively’’. The majority of questions about programs, staff,\nacademic regulations, and examination schedules are nei-\nther true/false nor multiple-choice questions, the strengths of\nChatGPT.\nChatGPT-3 was trained on 175 billion parameters, whereas\nthe latest version, ChatGPT-4, is trained on 100 trillion pa-\nrameters9. ChatGPT is trained on a huge general-purpose cor-\npus up to 2021 and uses reinforcement learning from human\nfeedback [45]. ChatGPT has potential limitations, including\ninaccurate or biased responses because of training on low\nquality data [48], security issues due to the storage of personal\ninformation and sensitive data on the ChatGPT cloud [49],\nand the cost of required hardware and software, maintenance,\nand support [48]. ChatGPT needs to be supplied with specific\ndatasets, which may include sensitive information such as\npersonal information or grades, to allow ChatGPT to respond\naccurately in a university context. This causes serious security\nand privacy concerns for academic institutions due to data\nstorage in the GPT cloud. Based on our best knowledge, Chat-\nGPT has not released any documents regarding collecting and\nstoring data [50], and it is unlikely to receive any kind of data\nsecurity certifications10.\nKocón et al. [51] have discovered that ChatGPT is less\nstable and performs worse than other SOTA models in almost\nall tasks. Last but not least, ChatGPT is not accessible from\nVietnam. As a result, ChatGPT is not a good choice for our\nparticular situation. We have decided to build a generative\n8https://chat.openai.com\n9https://sensoriumxr.com/articles/what-is-chatgpt4\n10https://www.comm100.com/blog/higher-ed-beware-10-dangers-\nchatgpt/\nvirtual assistant to support our students in a specific university\nin Vietnam.\nD. GOOGLE BARD AND BING CHAT\nIn February 2023, Google AI launched a chatbot, named\nBard11, based on Google’s Language Model for Dialogue\nApplication. Bard is available in 3 languages, US English,\nJapanese, and Korean, in over 180 countries. Bard [52] was\nnot only trained on a massive dataset but is also able to search\nthe web in real-time 12. Microsoft has released Bing Chat,\nwhich is based on the GPT-4 and integrated into a search\nengine13. Rudolph et al. [53] perform experiments to evaluate\nthe performance of ChatGPT, Google Bard, and Bing Chat.\nThe authors conclude that none of these AI chatbots reach\nthe level of A-students or B-students. ChatGPT-3.5 performed\nbetter than ChatGPT-4 on some questions. Bing Chat and\nGoogle Bard tended to get an F-grade average.\nIII. DIACRITICS IN LANGUAGES USING LATIN SCRIPT\nOur objective is to develop an architecture for training chat-\nbots in languages that use the Latin script with ample use\nof diacritics. A list of such languages and characters with\ndiacritics (e.g., Vietnamese, Romanian, and French) can be\nfound in Wikipedia 14. To the best of our knowledge, users\nmay type diacritics or accents of a specific language using a\nkeyboard designed for that language, an online keyboard 15,\nor a QWERTY keyboard, adapted to that language in various\nways. Here are some examples:\n• Vietnamese16: ‘‘aw’’ for ‘‘ă’’, ‘‘oo’’ for ‘‘ô’’, or ‘‘as’’ for\n‘‘á’’,\n• Romanian17: opening bracket ‘‘[’’ for ‘‘ă’’, closing\nbracket ‘‘]’’ for ‘‘î’’, or semicolon ‘‘;’’ for ‘‘ş’’,\n• French18: ‘‘′e’’ for ‘‘é’’, ‘‘′′o’’ for ‘‘ö’’, or ‘‘ˆe’’ for ‘‘ê’’.\nOne of our goals is to investigate Transformer-based models\nfor diacritic restoration and spelling correction for languages\nusing the Latin script with diacritics. We notice that a char-\nacter with an accent mark or a diacritic is usually created\nby a combination of a main character and another charac-\nter or a symbol. We take advantage of this common typing\nmethod to solve the problem. Among 23 languages using\ndiacritics Náplava et al. [54] studied, Vietnamese has the\nmost words with diacritics (88.4%) and the highest word error\nrate compared to a dictionary baseline (40.53%), whereas\nother languages have less than 52.5% words with diacritics\nand less than 9% word error rate, except for Romanian with\n29.71% word error rate. In addition, due to Vietnamese being\n11https://bard.google.com/\n12https://zapier.com/blog/chatgpt-vs-bard/\n13https://zapier.com/blog/chatgpt-vs-bing-chat/\n14https://en.wikipedia.org/wiki/Diacritic\n15https://www.lexilogos.com/english/index.htm\n16https://www.vietnamesepod101.com/blog/2020/10/16/how-to-type-in-\nvietnamese/\n17https://www.romanianpod101.com/blog/2020/10/16/how-to-type-in-\nromanian/\n18https://www.thoughtco.com/how-to-type-french-accents-1372770\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nan Asian language of a lower-middle income country, the\namount of resources devoted to it is substantially lower than\nother diacritizied languages using the Latin script. Therefore,\nwe perform experiments and evaluate our proposed approach\nwith Vietnamese. Three of the authors are native speakers of\nVietnamese, and all authors work in academia, making the\ndesign, development, and testing of a student-oriented chatbot\nin an ideal testbed.\nIn Vietnamese, using a different diacritic may change the\nmeaning of a word. For example, the verb ‘‘ nghĩ’’ means\n‘‘think’’, while the verb ‘‘ nghỉ’’ means ‘‘take rest’’. Several\nkeyboards such as Unikey 19 and Vietkey20 are used to input\nVietnamese words. There is another complication with Viet-\nnamese orthography. Vietnamese words are usually written\nwith syllables21 separated by white spaces. Depending on the\ntask to be performed in Vietnamese, some approaches may\nsplit words based on syllables using white spaces, whereas\nsome specialized Vietnamese segmentation toolkits such as\nVnTokenizer22 and Underthesea 23 segment words into syl-\nlables separated by white spaces, but clearly indicate the\nword boundaries. For example, the phrase ‘‘ công nghệ thông\ntin’’ (‘‘information technology’’) can be segmented into 4\nsyllables without word boundaries { công, nghệ, thông, tin}\nby using white spaces as separator in the traditional way or 2\nwords with separated syllables { công nghệ, thông tin} by\nusing a specialized Vietnamese segmentation toolkit. In our\nwork, we use the methods for inputting Vietnamese words to\nimprove the usability of the chatbots constructed. In addition,\nwe experiment with word segmentation methods using white\nspaces and the Underthesea toolkit (UTS).\nIV. PROPOSED APPROACH\nThis section presents the generative method used to construct\na chatbot in Vietnamese, in both closed and open domains.\nA closed domain chatbot can answer questions in a specific\ndomain (or area) only, whereas an open domain chatbot\ncan answer questions in any area. We describe the general\narchitecture first. Next, we introduce approaches to handle\nmisspelled words and missing diacritics in questions. Then,\nwe combine all models, and finally link the two chatbots in\nclosed and open domains.\nA. OVERVIEW\nThe overall architecture for the chatbot is given in Figure 1. To\nhandle misspelled words as well as words without diacritics,\nthe architecture uses a Transformer-based sub-model trained\non especially created datasets, as described in Section IV-B.\nThe goal is to make the chatbot resilient to inputs that are\nmisspelled as well as inputs that do not use diacritics in a\nhighly diacriticized language. The architecture also shortens\nthe questions by removing unimportant words; this module is\n19https://www.unikey.org/en/\n20http://vietkey.com.vn/\n21A syllable is the smallest meaning part of Vietnamese orthography [55].\n22https://vlsp.hpda.vn/demo/?page=resources\n23https://github.com/undertheseanlp/underthesea\nplaced in the processing pipeline after the text has been stan-\ndardized by correcting misspellings and restoring diacritics.\nThe main part of the architecture is comprised of the encoder-\ndecoder set-up trained on chat datasets. The datasets and the\ntraining of the chat module are discussed later.\nFIGURE 1. The architecture of the chatbot.\nB. MODELS FOR HANDLING QUESTIONS WITH NO\nDIACRITICS AND MISSPELLINGS\nNáplava et al. [54] show that users sometimes type words\nwithout diacritics because of the cumbersome requirements\nof a language inputting software, frequent code switching\nbetween English and a specific keyboard, or expedited input\ncompared to typing words with diacritics. In spite of the great-\nest percentage of words with diacritics and the highest word\nerror rate compared to dictionary baseline, as discussed in\nthe previous section, we notice that to search for information\nfaster, Vietnamese people in general and Vietnamese students\nin particular prolifically type words with no diacritics. For\nexample, users type ‘‘ma hoc phan vi tich phan a1’’ instead\nof typing ‘‘ mã học phần vi tích phân a1’’ (class code of\nthe calculus a1 course). Moreover, users forget to turn on the\nnecessary keyboard when typing words with diacritics. For\nexample, in Vietnamese, a user may want to type the word\n‘‘chào’’ (‘‘hi’’), but it turns to the word ‘‘chafo’’, ‘‘chaof’’,\n‘‘cha2o’’, or ‘‘chao2’’. Therefore, we build a Transformer-\nbased model to convert questions having words with no di-\nacritics or tones to questions comprising words with correct\ndiacritics and tones, as presented on the left-hand side of\nFigure 2.\nWe create a confusion dataset, named QSet-NoDiacritics,\nsimilar to the question set (the QuestionSet dataset), then\nremove diacritics and tones written above or below the vow-\nels of words by using the Unicode 24 library. We note that\nQuestionSet includes questions consisting of words with di-\nacritics and correct spellings. The QuestionSet and QSet-\nNoDiacritics datasets are fed to the Transformer model. The\nprocess of training the Transformer model to convert ques-\ntions consisting of words without diacritics to questions com-\nprising words with diacritics is similar to the process of\ntraining chatbots.\nUsers may make typographical errors when entering ques-\ntions, and this affects the quality of the chatbot. Studies have\n24https://pypi.org/project/Unidecode/\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 2. The Transformer-based sub-models for improving the chatbot. The left-hand side is the model for handling questions with no diacritics. The\nright-hand side is the model for transforming questions having words with incorrect spellings to questions having words with correct spellings.\nproposed approaches for diacritic restoration using bidirec-\ntional RNN on 23 languages using Latin script with dia-\ncritics [54], Gated-Recurrent Units on Arabic [56], BERT\non Czech [57], and Bidirectional GRU on Vietnamese [58].\nWe propose two approaches using the kNN algorithm, as\npresented by Lam et al. [32], and a Transformer-based model,\nas shown on the the right-hand side of Figure 2, to solve\nthis problem. We construct another confusion dataset, named\nQSet-Misspellings, consisting of questions with misspellings.\nIn particular, for every sentence in QuestionSet, we randomly\ncreate 15 new sentences with misspelled words such that each\nnew sentence contains a character error rate of 20%. The\nmisspelled words in the 15 new sentences are completely\ndifferent.\nAt the first stage, we separate the two models, a model for\nhandling diacritics and a model for handling misspellings, for\nconvenience in evaluating their performance and comparing\nthem with other existing models. Then, these two models\nare integrated to handle questions with no diacritics and\nmisspellings. In particular, the two confusion datasets, QSet-\nNoDiacritics and QSet-Misspellings, are combined and used\nto train the Transformer model to transform questions to the\ncorrect form with diacritics and correct spellings.\nC. SHORTENING QUESTIONS\nThe questions, obtained after restoring diacritics and correct-\ning misspellings, can be immediately fed to the Transformer-\nbased chatbot to generate responses. However, to help the\nchatbot understand questions better, as shown in Table 3 of\nSection V-C1, we remove unimportant words from the input\nquestions. This task is only applied to the closed domain\nchatbot. To correctly remove unimportant words from the real\ndata, words in the question set are tokenized and manually\nlabeled. For the university domain, we create 16 labels, some\nof them are presented in Table 1.\nAfter the steps of diacritic restoration and misspelling\ncorrection, questions are tokenized and unimportant words\nlabeled O are removed. For example, a given question ‘‘ bot\nơi, bot à mã học phần ct178 là gì’’ (hello bot, dear bot,\nTABLE 1. Some labels constructed and their descriptions.\nLabel Example values Description\nacronym VTPA1, Ths, TS,... Acronyms\nposition Dean, vice dean, ... Position of staff\ncohort cohort, 42, K41 Cohort\ngroup 1, 2, 3, m01, m02,... Groups of classes\nmark A, B+, B, .... Final grade\nmhp CT, TN, XH, ... Class codes\nregulation graduation, tuition fee,... Academic regulation\nverb pay, withdraw,... Action\nO stop words and unimpor-\ntant words\nSystem does not need to\npay attention to this word\nwhat is the class code ct178) is shortened to become ‘‘ mã\nhọc phần ct178’’ because the words “bot ơi”, “bot à”,\n“là”, and “gì” are in the list of words assigned a label of\n‘‘O’’. For future work, we will take the advantage of these\nwords labeled to automatically enrich the training dataset and\ngenerate diverse answers by discovering semantic relations\nfrom a WordNet such as synsets and antonyms. For example,\ninstead of providing an answer ‘‘ CT178 khó nha bạn ’’\n(CT178 is difficult), the chatbot may answer ‘‘ CT178 gay go\nnha bạn’’ (CT178 is hard) or ‘‘ CT178 không dễ nha bạn’’\n(CT178 is not easy), which also has a similar meaning 25.\nD. COMBINING MODELS AND LINKING CHATBOTS\nOur core chatbot model is based simply on the original Trans-\nformer [12] following the encoder-decoder framework. After\nconstructing separate models for handling different tasks, we\ncombine all models, as shown in Figure 1. An input question\nis passed through the pre-processing step. Next, we feed the\nsentence to the trained model to transform the question to\ncorrect form, including diacritic and spelling correction at the\nsame time, followed by shortening questions. The shortened\n25In the Princeton Wordnet, the offset 00744916-a consists of 2 members\n‘‘difficult’’ and ‘‘hard’’ having the same meaning of ‘‘not easy; requiring\ngreat physical or mental effort to accomplish or comprehend or endure’’ .\nThis synset in our Vietnamese WordNet [59], which has the same structure\nas the Princeton WordNet, also has 2 members ‘‘khó’’ and ‘‘gay go’’ with a\ngloss ‘‘không dễ;...’’.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nquestion is considered normal input to the trained chatbot\nmodel to generate a corresponding answer.\nThe architecture for the chatbot in Figure 1 is used to\ntrain chatbots individually on closed and open domains using\nrelevant datasets. Input questions are classified using the kNN\nalgorithm, and the correct chatbot domain is chosen to answer\nthe queries. The two chatbots complement each other such\nthat the closed domain chatbot provides factual information\nand the open domain chatbot serves as a virtual friend.\nV. EXPERIMENTS\nA. CONSTRUCTING DATASETS\nThe architecture presented in Figure 1 is general and will\nwork with any language, in particular a language that uses\nthe Latin script with diacritics. To train VietBOT, we need\nspecific Vietnamese datasets for specific universities. Each\ndataset used to train the chatbot models consists of one text\nfile for questions and another text file for answers with the\nconstraint that the nth sentence in the first file corresponds to\nthe nth sentence in the second file.\nOur specific VietBOT will answer questions primarily in-\nside and secondarily outside the domain of the College of\nInformation Technology and Communication 26 (CICT) of\nCan Tho University 27 (CTU) in Vietnam. Playing an impor-\ntant role in multi-disciplinary education and training in the\nMekong Delta region of Vietnam, CTU in general has 45,791\nstudents, and CICT in particular has more than 5,000 students\nin 2022. A chatbot like VietBOT is absolutely necessary to as-\nsist a large number of students with information regarding the\nCollege, instructors, academic regulation, programs, classes,\nand so on.\nWe construct two training datasets, including the CTUBot\ndataset used for a closed domain chatbot and the OpenSub-\ntitle dataset in Vietnamese, called OpenSubViet, used for an\nopen domain chatbot. The CTUBot dataset is built to help\nour virtual assistant respond to questions related to CICT,\nwhereas the OpenSubViet dataset is built to help the chatbot\ncommunicate with students flexibly and to answer questions\nin an open domain.\n• The CTUBot dataset includes 35,702 question-answer\npairs collected from different resources such as the CICT\nwebsite28, academic regulations for students at CTU,\nbachelor programs of CICT, handbooks for CICT stu-\ndents, and examination schedule for each semester.\n• The OpenSubViet dataset comprises over 419,712 di-\nalogue pairs of characters in movies extracted from\nthe OpenSubtitle29 dataset and translated to Vietnamese\nusing a pre-trained Transformer model 30. The Trans-\nformer translation model was trained on the dataset us-\ning 600,000 sentences extracted from TED 31.\n26http://www.cit.ctu.edu.vn/encict/\n27https://en.ctu.edu.vn/\n28http://www.cit.ctu.edu.vn/\n29https://www.opensubtitles.org/en/search/subs\n30https://github.com/pbcquoc/transformer\n31https://www.ted.com/\nTABLE 2. Statistics on the number of words in sentences and dictionaries\nconstructed.\nSegment Number of CTUBot OpenSubViet\nwords words Q A Q A\nLongest sentence 25 42 44 53\nwhite Shortest sentence 4 4 4 3\nspaces Average length 12.54 8.1 9.64 9.59\nDictionary size 2,742 13,126\nLongest sentence 22 39 44 53\nUTS Shortest sentence 3 3 4 3\ntoolkit Average length 11.68 8.1 9.64 9.6\nDictionary size 3,349 50,769\nIn addition, CICT students were requested to construct\na test dataset manually for evaluating VietBOT. This test\ndataset, named CTUBot-TestSet, comprises 100 questions.\nEach question has one answer written in 3 different ways. An\nexample of a question and its answers is as follows.\nQuestion: Không học học phần điều kiện được không?(Is\nit possible to not take the compulsory classes?)\nAnswer 1: Không nhe bạn(No, it is not possible to not take\nthe compulsory classes.)\nAnswer 2: Bắt buộc học nhe bạn (You must take the\ncompulsory classes)\nAnswer 3: Phải học nhe bạn(You have to take the compul-\nsory classes)\nB. DATASET PRE-PROCESSING\nSeveral steps are used to pre-process the training datasets.\nWe convert sentences to lowercase and remove special char-\nacters (e.g., @, #, $,...), punctuation marks, and sentence\nseparators (e.g., -, !, ?, ...). We eliminate tokens that have\nno meaning (e.g., ‘‘Hmm’’ and ‘‘ak’’). The lines that spec-\nify the subtitle author, such as ‘‘ phim được phụ đề bởi\nBinhKan’’ (‘‘movies subtitled by BinhKan’’), are removed.\nNext, acronyms and abbreviations in the answer files are con-\nverted to original phrases, such as converting ‘‘cntt’’ to ‘‘ công\nnghệ thông tin’’ (‘‘information technology’’) or ‘‘ths’’ to\n‘‘thạc sĩ’’ (‘‘Master’’).\nA starting markup ‘‘<s>’’ and an ending markup ‘‘</s>’’\nare added to the beginning and to the end of every sentence.\nWe segment words in datasets using 2 methods of using white\nspace as a separator and the UTS Vietnamese word seg-\nmentation toolkit, and construct a dictionary. Table 2 shows\nstatistics on the number of words in sentences in the datasets.\nThe maximum length of question sentences in the CTUBot\ndataset is 25. Most questions in the OpenSubViet dataset have\na maximum number of 25 words.\nC. EXPERIMENTAL RESULTS\nWe build the models in the Google Colab environment with\n12GB RAM with TPU. The hyper parameters during the\ntraining process are as follows: the number of identical layers\nin encoder and decoder: 6, the number of sub-layers in each\nlayer: 2, the number of epochs: 200, training batch size: 512,\ndimensionality of input and output: 256, the number of heads:\n8, units: 512, and dropout rate: 0.1. We also use the Adam\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 3. The BLEU scores of VietBOT on the closed domain.\nScore BLEU-1 BLEU-2 BLEU-3 BLEU-4\nNot shortening\nquestions\n0.525 0.509 0.497 0.451\nSplit+shortening\nquestions\n0.529 0.526 0.517 0.477\nUTS+shortening\nquestions\n0.585 0.573 0.561 0.513\noptimizer with the same parameters as suggested by Vaswani\net al. [12]. The final training loss is 0.0023.\nThe BLEU metric [60] is used to evaluate the models. The\nweights of different N-grams used to calculate the BLEU\nscores of our chatbots are the same as those used by Jason\nBrownlee32: weights of BLEU-1=(1; 0; 0; 0); weights of\nBLEU-2=(0.50; 0.50; 0; 0); weights of BLEU-3 (0.33; 0.33;\n0.33; 0) and weights of BLEU-4=(0.25; 0.25; 0.25; 0.25). In\nthe remainder of this section, we report the experimental re-\nsults of the Transformer-based chatbot built, the Transformer\nmodels to help the chatbot handle questions with no diacritics\nand misspellings, and finally the performance results of the\nwhole chatbot system.\n1) The chatbot model based on Transformer\nThe BLEU scores of VietBOT in the closed domain are pre-\nsented in Table 3. The results show that shortening questions\nhelps the chatbot achieve better results. For example, given a\nquestion ‘‘bot ơi, bot à mã học phần ct178 là gì’’ (‘‘hello\nbot, dear bot what is the class code ct178’’), the chatbot\nwithout shortening questions could not understand question\nwell, so that it responds with ‘‘CT178’’. The chatbot with\nshortened questions recognizes the question entities correctly,\nand it provides a correct answer ‘‘ Nguyên lý hệ điều hành\n3 tín chỉ’’ (Principles of Operating System 3 credits). The\nchatbot that segments words using the UTS toolkit gets higher\nBLEU scores than the one using white space as a separator,\nnamed Split.\nWe also experiment with the pre-trained BPE embeddings\nprovided by PhoBERT [61], which is trained on 20GB of\nVietnamese texts. We simply map words in our dictionary\nwith words provided by PhoBERT, and extract the corre-\nsponding embeddings of words. Because the size of pre-\ntrained BPE embeddings is 768, the SVD factorization is used\nto reduce their size to 256, which is accepted by our Trans-\nformer model. Similarly, we experiment with the pre-trained\nembeddings supported by BARTpho [62]. Table 4 presents\nthe BLEU scores of VietBOT using pre-trained embeddings\nsupported by PhoBERT and BARTpho. VietBOT using our\ntrained embeddings achieves the highest BLEU scores among\nVietBOTs using pre-trained embeddings.\nWe do not shorten questions on the open domain chatbot\nbecause we do not know the exact possible entities in open\ndomain questions. The BLEU-1, 2, 3, and 4 scores of the\n32https://machinelearningmastery.com/calculate-bleu-score-for-text-\npython/\nTABLE 4. The BLEU scores of VietBOTs, using the pre-trained embeddings,\non the closed domain.\nPre-trained\nembedding\nprovided by\nBLEU-1 BLEU-2 BLEU-3 BLEU-4\nPhoBERT 0.520 0.512 0.509 0.462\nBARTpho 0.574 0.538 0.503 0.446\nchatbot on the open domain are 0.18, 0.13, 0.10, and 0.08,\nrespectively. The BLEU-4 score of our open domain chatbot\nis 0.08, which is slightly better than the chatbot constructed\nusing the reinforcement learning approach and the LSTM\nmodel (which is 0.07) on the same dataset in Vietnamese\n[63]. Our model needs about 3 hours for training, while\nNguyen’s model [63] needs about 72 hours on the same\nkind of hardware. This shows that the learning speed of the\nTransformer model is better than the reinforcement learning\nand the LSTM.\n2) Models for handling questions with no diacritics and\nmisspellings\nBoth the CTUBot and OpenSubViet datasets are used to train\nthe model to convert questions with typos and no diacritics\nto questions with correct spellings. The BLEU scores of the\ntasks for handling questions having words with no diacritics,\nmistyped words, and both these issues at the same time using\nthe Transformer-based model are presented in Table 5.\nTABLE 5. The BLEU scores of the Transformer-based models used to\nhandle questions with no diacritics, misspellings, and both no diacritics\nand misspellings (the column ‘‘Neither’’).\nDomain Scores No diacritics Misspellings Neither\nClosed\nBLEU-1 0.89 0.84 0.82\nBLEU-2 0.85 0.79 0.76\nBLEU-3 0.82 0.75 0.72\nBLEU-4 0.76 0.69 0.66\nOpen\nBLEU-1 0.98 0.98 0.98\nBLEU-2 0.98 0.98 0.97\nBLEU-3 0.97 0.97 0.95\nBLEU-4 0.95 0.94 0.92\nTable 6 shows some examples of the Transformer models\nused to correct mistyped words and words with no diacritics.\nThe first two mistyped questions are converted correctly. In\nthe last question, the model also performs correction well,\nexcept that there is one incorrectly transformed word ‘‘ biết’’.\nIn this question, the user actually wants to know the process\nfor requesting an I grade (the word ‘‘xin’’), but the model\ntransforms it to simply want to know information about an\nI grade (a status of ‘‘Incomplete’’ the class) in general.\nTo compare, we calculate the accuracy of our Transformer-\nbased models for handling misspelled words and missing\ndiacritics. Our Transformer model gets accuracies of 82% and\n76% for the tasks of handling sentences without diacritics,\nand sentences with mistyped words, respectively. We also\nperform experiments to improve the chatbot model by using\nthe kNN algorithm and Euclidean distance as suggested by\nLam et al. [32]. The values of k are odd numbers from 1 to 17.\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 6. Some examples of handling words with no diacritics and\nmistyped words.\nUser wants to ask User typed Result of the\nmodel for\ncorrection\nCT205 mở lớp ở học kỳ\nnào?(In which semester is\nthe CT205 class offered?)\ngt205 mở lớp o\nhọc kỳ nào\nct205 mở lớp\nở học kỳ nào\nHọc phần tiên quyết\ncủa toán rời rạc là gì?\n(What is the prerequisite\nclass of the Discrete Math-\nematics class?)\nhok phần tiên\nquyết của toán\nrời rạc là j\nhọc phần tiên\nquyết của toán\nrời rạc là gì?\nNếu tôi muốn xin điểm\nI thì phải làm sao?\n(What if I want to ap-\nply for an I grade) Note:\nAn I grade means the stu-\ndent has not completed the\nclass.\nnếu tôy muốn\nxin điểm I thi2\nfai làm sao\nnếu tôi muốn\nbiết điểm I thì\nphải làm sao\n(What if I want to\nknow about an I\ngrade)\nThe kNN algorithm for handling questions having misspelled\nwords achieves the highest accuracy of 72% with k = 1, as\npresented in Table 7. We notice that the Transformer model\ncorrects misspelled words better than the kNN. Therefore, we\nuse the Transformer model to handle this task in the VietBOT.\nTABLE 7. The accuracy (%) of applying kNN handle words mistyped in the\nquestions.\nk Accuracy k Accuracy k Accuracy\n1 72.00 3 69.74 5 69.84\n7 69.74 9 69.54 11 69.54\n13 69.54 15 69.54 17 69.35\n3) Combining models\nFinally, we integrate the models to handle questions with no\ndiacritics and misspellings into the chatbot model. Table 8\nshows a conversation between a user (YOU) and VietBOT.\nThe ‘‘Meaning’’ column shows the actual question a user\nwants to ask when the user makes typos, and the content in-\nside the parentheses ( ) is the meaning of the actual questions\nin English.\nIn addition, we generate noisy questions by randomly re-\nmoving diacritics or replacing correct words with misspelled\nwords from the CTUBot-TestSet and evaluate the robustness\nof VietBOT through its responses, as shown in Table 9. The\npercentages in the columns of ‘‘No diacritics’’ and ‘‘Mis-\nspellings’’ in Table 9 present the error rates in user questions.\nD. DISCUSSION\nOur Transformer-based model to support the chatbot system\nconsists of a word diacritic restoration model and a mistyped\nword correction model. It is not easy to compare the per-\nformance of different approaches on diverse datasets using\nvarious evaluation metrics. For the sake of completeness, we\nmake an attempt at comparing our models with published\npapers.\nTABLE 8. Example of conversation between a user (YOU) and the VietBOT.\nWho Q/A Meaning\nYOU bot biết co Khang\nkhong\nbot biết cô Khang\nkhông - (Do you know\nMs. Khang)\nVietBOT Mình biết nhe, là\ncô Lâm Nhựt Khang\nở bộ môn CNTT\n(I know, she is Ms. Lam\nNhut Khang in the Depart-\nment of IT)\nYOU truong khoa công\nngheej thông tin là ai\ntrưởng khoa công\nnghệ thông tin\nlà ai (Who is\nthe Dean of the\nCollege of Information\nTechnology)\nVietBOT Trưởng khoa công\nnghệ thông tin là thầy\nNguyễn Hữu Hòa\n(The dean is Mr. Nguyen\nHuu Hoa)\nYOU ma hoc phan ltcb a mã học phần ltcb a\n(class code of ltcb a.\n“ltcb a” is an acronym\nof “lập trình căn bản\nA” which means “Basic\nProgramming A”)\nVietBOT CT101 CT101.\nNote: VietBot understands\nthe acronym ‘‘ltcb’’ and re-\nposes correctly.\nYOU lịch thi học kỳ 2 2020\n2021 ct112 nhóm 8\n(examination schedule of\nct112, group 8, semester 2,\nschool year 2020-2021)\nVietBOT lịch thi là 25/5/2021,\nphòng thi là P12\n(the examinations are on\n25/5/2021, in room P12)\nFirst, we compare our proposed model with existing mod-\nels used to handle Vietnamese words with no diacritics.\nTran et al. [58] use several models to solve the diacritic\nrestoration problem. The experimental results show that the\nBidirectional-GRU-RNN is the best model with a BLEU\nscore of 88.06%, whereas the BLEU scores of the GRU-\nRNN and Bidirectional-LSTM-RNN models are 73.47% and\n87.98%, respectively, on their dataset. Our Transformer-\nbased model for word diacritic restoration in both closed and\nopen domains is better than the best model of Tran et al. [58].\nSecond, we compare our Transformer-based model used to\ncorrect Vietnamese mistyped words with other approaches.\nThe N-gram model [64] based on the contexts on both sides of\nsyllables is currently the best approach for spelling correction\nwith 94% precision on their dataset. The LSTM model used\nfor Vietnamese consonant misspelling correction achieves\n90.56% accuracy on sentences [65], whereas the N-gram\nmodel [64] reaches 72.18% accuracy. Do et al. [66] use the\nTransformer model to correct mistyped and misspelled errors.\nEach syllable is used to generate a candidate list of common\ntypes of Vietnamese spelling errors. The training datasets are\ngenerated by adding errors to non-error sentences extracted\nfrom a Vietnamese news corpus 33. Their model achieves\nperformance with 81.5% errors corrected, while the metric\nof the N-gram model [64] is 79.3%. Our model is not good as\ntheir model. However, Do et al. [66] standardize marks and\n33https://github.com/binhvq/news-corpus\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 9. The BLEU scores of the integrated VietBOT models for handling questions with misspellings and no diacritics.\nOption No diacritics Misspellings BLEU-1 BLEU-2 BLEU-3 BLEU-4\n100% - 0.534 0.520 0.509 0.463\n50% - 0.552 0.538 0.520 0.473\n10% - 0.535 0.521 0.507 0.459\nNo - 50% 0.492 0.482 0.471 0429\nshortening questions - 10% 0.517 0.502 0.496 0.449\n100% 50% 0.478 0.463 0.446 0.402\n50% 50% 0.498 0.475 0.461 0.411\n30% 30% 0.490 0.475 0.465 0.423\n10% 10% 0.532 0.517 0.506 0.462\n100% - 0.578 0.562 0.543 0.494\n50% - 0.593 0.574 0.554 0.504\n10% - 0.571 0.555 0.537 0.486\nSplit - 50% 0.519 0.508 0.505 0.449\n& - 10% 0.539 0.523 0.512 0.464\nshortening questions 100% 50% 0.460 0.447 0.428 0.387\n50% 50% 0.511 0.489 0.471 0.418\n30% 30% 0.511 0.495 0.478 0.435\n10% 10% 0.559 0.544 0.531 0.487\n100% - 0.561 0.554 0.546 0.503\n50% - 0.579 0.567 0.554 0.510\n10% - 0.581 0.574 0.563 0.516\nUTS - 50% 0.529 0.253 0.517 0.477\n& - 10% 0.563 0.552 0.547 0.500\nshortening questions 100% 50% 0.498 0.488 0.477 0.437\n50% 50% 0.503 0.488 0.477 0.436\n30% 30% 0.508 0.494 0.485 0.446\n10% 10% 0.562 0.553 0.546 0.505\nsyllables based on Telex typing form which may not let the\nmodel work well with words typed using Vni formats. Our\nmodel can handle mistyped and misspelled words created by\nboth Telex and Vni text input formats. Besides, we can save\ntime and effort required to construct the training confusion\ndatasets containing mistyped and misspelled words because\nour confusion datasets are constructed automatically. The\nnewest published paper in 2022 on simultaneous correcting\ndiacritics and typos [67] suggests the combination of ByT5\nTransformer and the classical dictionary methods can im-\nprove the accuracy to 87.86% in Vietnamese. For future work,\nwe will improve our Transformer-based proposed approach\nto improve the chatbot system by discovering information\nfrom Vietnamese dictionaries [67] and taking into account\nthe confusion set for each syllable based on edit distance and\nchosen Vietnamese characteristics [64].\nOur VietBOT performs better than an Indonesian chat-\nbot for university admissions constructed using the seq2seq\nmodel without and with attention [8] which achieves BLEU\nscores of 43.61 and 44.68, respectively. Based on our best\nknowledge, we have not found many published papers on\nconstructing Vietnamese chatbots. Nguyen and Shcherbakov\n[68] constructed a Vietnamese chatbot using a seq2seq model\nwith an attention decoder. They claim that the BLEU score of\ntheir ‘‘whole model is 1.443%’’. Some Vietnamese chatbots\nhave been created using the Rasa framework. The NEU-\nchatbot [29], created using Rasa, for admission to National\nEconomics University in Vietnam answers 90.29% questions\nand achieves an accuracy of 97.1% on their test set. None of\nthe chatbots discussed [8], [29], [68] have a component to\nhandle misspellings or lack of diacritics. The chatbot of Lam\net al. [32], called CTU-chatbot, also created using Rasa, for\nstudents at Can Tho University in Vietnam responds 92.78%\nquestions and reaches the best accuracy of 94.33%. It is unfair\nfor us to compare straightforwardly our chatbot with these\nchatbots created using the Rasa framework. Therefore, we\nperform experiments with our VietBOT and CTU-chatbot\non our dataset. The results show that VietBOT responds to\nquestions of actual users with an accuracy of 73%; while\nCTU-chatbot only answers questions with an accuracy of\n37%. The BLEU scores of CTU-chatbot are BLEU-1: 0.304,\nBLEU-2: 0.311, BLEU-3: 0.333, and BLEU4: 0.309, which\nare lower than VietBOT in all cases. CTU-chatbot cannot an-\nswer questions which are not in the training dataset, whereas\nVietBOT can. Moreover, constructing a training dataset for\nthe chatbot of Lam et al. [32] costs more time and effort than\nfor our VietBOT. While performing experiments, we intend to\ncollect questions from users to enrich question files for train-\ning VietBOT; then, we may extract answers automatically\nfrom documents or create answers manually. For future work,\nVietBOT is more suitable for expansion than the chatbot of\nLam et al. [32].\nNormally, universities do not have a bank of question-\nanswer pairs for training a chatbot. The training datasets may\nbe obtained from different resources as follows.\n• Individual staff of the university, such as instructors,\nadvisors, and secretaries, may be able to provide the\nhighest quality question-answer pairs for training the\nchatbot because they are the most familiar with the\nschool, faculties, classes, school regulations, and so on.\nHowever, this method is likely to be prohibitively ex-\npensive and time consuming, if at all possible due to\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nlack of interest or lack of time. In our work, advisors\nand instructors helped contribute some question-answer\npairs manually.\n• University websites and published documents of a uni-\nversity, such as handbooks and academic regulations,\nprovide reliable data. For future work, we will inves-\ntigate approaches to automatically generate question-\nanswer pairs from these sources using approaches such\nas the OneStop model [69] based on the canonical\nseq2seq Transformer.\n• We may extract question-answer pairs from the support\napplications provided by a university [8] using What-\nsapp and Zalo, and Fanpage on Facebook of the univer-\nsity [29], [70].\nVI. CONCLUSION\nWe have developed a virtual assistant for students based on\nthe generation-based model using the attention-only Trans-\nformer neural network model. Our method can construct a\nchatbot for a university which has a collection of question-\nanswer pairs. Our Transformer-based models are simple but\neffective for the chatbots to handle questions that lack di-\nacritics and have misspellings in Vietnamese. Although we\nhave not performed experiments using our proposed models\non datasets in other languages, we believe our models could\ncontribute to improve chatbots in other languages which also\nuse Latin script and have diacritics. In actual experiments\nwith real users, VietBOT can answer questions not in the\ntraining dataset, understand questions in which users have\nmisspelled or typed without diacritics, and respond to users\nwith relevant answers. We are improving our virtual assistant\nsystem by exploring approaches to automatically enlarge the\ntraining dataset, providing more functions to meet the needs\nof students.\nREFERENCES\n[1] G. Caldarini, S. Jaf, and K. McGarry, ‘‘A literature survey of recent\nadvances in chatbots,’’ Information, vol. 13, no. 1, p. 41, 2022.\n[2] H. Raval, ‘‘Limitations of existing chatbot with analytical survey to en-\nhance the functionality using emerging technology,’’ International Journal\nof Research and Analytical Reviews (IJRAR) , vol. 7, no. 2, 2020.\n[3] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, ‘‘A diversity-promoting\nobjective function for neural conversation models,’’ in Proceedings of the\n15th Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies , 2016, pp.\n110–119.\n[4] J. Li, M. Galley, C. Brockett, G. P. Spithourakis, J. Gao, and B. Dolan,\n‘‘A persona-based neural conversation model,’’ in Proceedings of the 15th\nAnnual Conference of the North American Chapter of the Association for\nComputational Linguistics, 2016, pp. 994–1003.\n[5] Z. Yin, K.-h. Chang, and R. Zhang, ‘‘DeepProbe: Information directed\nsequence understanding and chatbot design via recurrent neural networks,’’\nin Proceedings of the 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , 2017, pp. 2131–2139.\n[6] M. Dhyani and R. Kumar, ‘‘An intelligent chatbot using deep learning with\nbidirectional RNN and attention model,’’ Materials Today: Proceedings ,\nvol. 34, pp. 817–824, 2021.\n[7] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan,\nZ. Yang, A. Kulshreshtha, G. Nemade, Y . Lu et al., ‘‘Towards a human-like\nopen-domain chatbot,’’ arXiv preprint arXiv:2001.09977 , 2020.\n[8] Y . W. Chandra and S. Suyanto, ‘‘Indonesian chatbot of university admission\nusing a question answering system based on sequence-to-sequence model,’’\nProcedia Computer Science , vol. 157, pp. 367–374, 2019.\n[9] I. V . Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,\nT. Kim, M. Pieper, S. Chandar, N. R. Ke et al. , ‘‘A deep reinforcement\nlearning chatbot,’’ arXiv preprint arXiv:1709.02349 , 2017.\n[10] Q. Bao, L. Ni, and J. Liu, ‘‘HHH: an online medical chatbot system\nbased on knowledge graph and hierarchical bi-directional attention,’’ in\nProceedings of the Australasian Computer Science Week Multiconference ,\n2020, pp. 1–10.\n[11] G. Sperlí, ‘‘A deep learning based chatbot for cultural heritage,’’ in Pro-\nceedings of the 35th Annual ACM Symposium on Applied Computing , 2020,\npp. 935–937.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advances in\nNeural Information Processing Systems , 2017, pp. 5998–6008.\n[13] Z. Lin, P. Xu, G. I. Winata, F. B. Siddique, Z. Liu, J. Shin, and P. Fung,\n‘‘Caire: An end-to-end empathetic chatbot,’’ in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 34, no. 09, 2020, pp. 13 622–\n13 623.\n[14] Y . Zhang, S. Sun, M. Galley, Y .-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu,\nand W. B. Dolan, ‘‘DIALOGPT: Large-Scale Generative Pre-training for\nConversational Response Generation,’’ in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics: System Demon-\nstrations, 2020, pp. 270–278.\n[15] J. J. Bird, A. Ekárt, and D. R. Faria, ‘‘Chatbot interaction with artificial\nintelligence: human data augmentation with T5 and language transformer\nensemble for text classification,’’ Journal of Ambient Intelligence and\nHumanized Computing, pp. 1–16, 2021.\n[16] R. Bathija, P. Agarwal, R. Somanna, and G. Pallavi, ‘‘Guided interactive\nlearning through chatbot using bi-directional encoder representations from\ntransformers (BERT),’’ in The 2nd International Conference on Innovative\nMechanisms for Industry Applications (ICIMIA) . IEEE, 2020, pp. 82–87.\n[17] S. Yoo and O. Jeong, ‘‘An intelligent chatbot utilizing BERT model and\nknowledge graph,’’Journal of Society for e-Business Studies , vol. 24, no. 3,\n2020.\n[18] N. Kanodia, K. Ahmed, and Y . Miao, ‘‘Question answering model based\nconversational chatbot using BERT model and Google Dialogflow,’’ in The\n31st International Telecommunication Networks and Applications Confer-\nence (ITNAC). IEEE, 2021, pp. 19–22.\n[19] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Küttler,\nM. Lewis, W.-t. Yih, T. Rocktäschel et al. , ‘‘Retrieval-augmented genera-\ntion for knowledge-intensive NLP tasks,’’ Advances in Neural Information\nProcessing Systems, vol. 33, pp. 9459–9474, 2020.\n[20] M. Dibitonto, K. Leszczynska, F. Tazzi, and C. M. Medaglia, ‘‘Chatbot in\na campus environment: design of LiSA, a virtual assistant to help students\nin their university life,’’ in International Conference on Human-Computer\nInteraction. Springer, 2018, pp. 103–116.\n[21] J. Singh, M. H. Joesph, and K. B. A. Jabbar, ‘‘Rule-based chabot for student\nenquiries,’’ inJournal of Physics: Conference Series , vol. 1228, no. 1. IOP\nPublishing, 2019, p. 012060.\n[22] A. Vishwakarma and A. Pandey, ‘‘A review & comparative analysis on\nvarious chatbots design,’’ International Journal of Computer Science and\nMobile Computing, vol. 10, no. 2, pp. 72–78, 2021.\n[23] S. P. Barus and E. Surijati, ‘‘Chatbot with Dialogflow for FAQ Services\nin Matana University Library,’’ International Journal of Informatics and\nComputation, vol. 3, no. 2, pp. 62–70, 2022.\n[24] W. El Hefny, Y . Mansy, M. Abdallah, and S. Abdennadher, ‘‘Jooka: A\nbilingual chatbot for university admission.’’ in WorldCIST (3), 2021, pp.\n671–681.\n[25] W. El Hefny, A. El Bolock, C. Herbert, and S. Abdennadher, ‘‘Towards\na generic framework for character-based chatbots,’’ in International Con-\nference on Practical Applications of Agents and Multi-Agent Systems .\nSpringer, 2020, pp. 95–107.\n[26] P. Kostelník, I. Pisařovic, M. Muroň, F. Dařena, and D. Procházka, ‘‘Chat-\nbots for enterprises: outlook,’’ Acta Universitatis Agriculturae et Silvicul-\nturae Mendelianae Brunensis , 2019.\n[27] M. Conţ, A. Ciupe, B. Orza, I. Cohuţ, and G. Niţu, ‘‘Career counseling\nchatbot using microsoft bot frameworks,’’ in 2022 IEEE Global Engineer-\ning Education Conference (EDUCON) . IEEE, 2022, pp. 1387–1392.\n[28] A. G. Usigan, M. I. Salomeo, G. J. L. J. Zafe, C. J. Centeno, A. A. R. C.\nSison, and A. G. Bitancor, ‘‘Implementation of an undergraduate admission\nchatbot using microsoft azure’s question answering and bot framework,’’ in\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nProceedings of the 2022 5th Artificial Intelligence and Cloud Computing\nConference, 2022, pp. 240–245.\n[29] T. T. Nguyen, A. D. Le, H. T. Hoang, and T. Nguyen, ‘‘NEU-chatbot:\nChatbot for admission of National Economics University,’’ Computers and\nEducation: Artificial Intelligence , vol. 2, p. 100036, 2021.\n[30] Y . Windiatmoko, R. Rahmadi, and A. F. Hidayatullah, ‘‘Developing Face-\nbook Chatbot based on deep learning using RASA framework for univer-\nsity enquiries,’’ in IOP Conference Series: Materials Science and Engi-\nneering, vol. 1077, no. 1. IOP Publishing, 2021, p. 012060.\n[31] S. Meshram, N. Naik, V . Megha, T. More, and S. Kharche, ‘‘College\nenquiry chatbot using rasa framework,’’ in 2021 Asian Conference on\nInnovation in Technology (ASIANCON) . IEEE, 2021, pp. 1–8.\n[32] K. N. Lam, N. N. Le, and J. Kalita, ‘‘Building a chatbot on a closed\ndomain using RASA,’’ in Proceedings of the 4th International Conference\non Natural Language Processing and Information Retrieval , 2020, pp.\n144–148.\n[33] S. Singh and S. Singh, ‘‘Effective Analysis of Chatbot Frameworks: RASA\nand Dialogflow,’’ EasyChair, Tech. Rep., 2022.\n[34] F. Muftić, M. Kadunić, A. Mušinbegović, and A. Abd Almisreb, ‘‘Explor-\ning medical breakthroughs: A systematic review of ChatGPT applications\nin healthcare,’’ Southeast Europe Journal of Soft Computing , vol. 12, no. 1,\npp. 13–41, 2023.\n[35] I. Carvalho and S. Ivanov, ‘‘ChatGPT for tourism: applications, benefits\nand risks,’’ Tourism Review, 2023.\n[36] P. Rivas and L. Zhao, ‘‘Marketing with ChatGPT: Navigating the ethical\nterrain of gpt-based chatbot technology,’’ AI, vol. 4, no. 2, pp. 375–384,\n2023.\n[37] S. Biswas, ‘‘Importance of chat GPT in Agriculture: According to chat\nGPT,’’Available at SSRN 4405391 , 2023.\n[38] R. Gupta, P. Pande, I. Herzog, J. Weisberger, J. Chao, K. Chaiyasate, and\nE. S. Lee, ‘‘Application of ChatGPT in cosmetic plastic surgery: ally or\nantagonist?’’ Aesthetic Surgery Journal , p. sjad042, 2023.\n[39] S. Guo, Y . Wang, S. Li, and N. Saeed, ‘‘Semantic Communications with\nOrdered Importance using ChatGPT,’’ arXiv preprint arXiv:2302.07142 ,\n2023.\n[40] D. Baidoo-Anu and L. Owusu Ansah, ‘‘Education in the era of generative\nartificial intelligence: Understanding the potential benefits of ChatGPT in\npromoting teaching and learning,’’ Available at SSRN 4337484 , 2023.\n[41] J. Qadir, ‘‘Engineering education in the era of ChatGPT: Promise and\npitfalls of generative AI for education,’’ in 2023 IEEE Global Engineering\nEducation Conference (EDUCON) . IEEE, 2023, pp. 1–9.\n[42] M. Sullivan, A. Kelly, and P. McLaughlan, ‘‘ChatGPT in higher education:\nConsiderations for academic integrity and student learning,’’ Journal of\nApplied Learning and Teaching , vol. 6, no. 1, 2023.\n[43] J. Rudolph, S. Tan, and S. Tan, ‘‘ChatGPT: Bullshit spewer or the end of\ntraditional assessments in higher education?’’ Journal of Applied Learning\nand Teaching, vol. 6, no. 1, 2023.\n[44] C. Terwiesch, ‘‘Would Chat GPT3 Get a Wharton MBA? A Prediction\nBased on Its Performance in the Operations Management Course,’’ Mack\nInstitute for Innovation Management at the Wharton School, University of\nPennsylvania, Tech. Rep., 2023.\n[45] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, ‘‘Chatgpt goes\nto law school,’’ Available at SSRN , 2023.\n[46] A. Gilson, C. Safranek, T. Huang, V . Socrates, L. Chi, R. A. Taylor, and\nD. Chartash, ‘‘How well does chatgpt do when taking the medical licensing\nexams? the implications of large language models for medical education\nand knowledge assessment,’’ medRxiv, pp. 2022–12, 2022.\n[47] D. A. Wood, M. P. Achhpilia, M. T. Adams, S. Aghazadeh, K. Akinyele,\nM. Akpan, K. D. Allee, A. M. Allen, E. D. Almer, D. Ames et al. , ‘‘The\nchatgpt artificial intelligence chatbot: How well does it answer accounting\nassessment questions?’’ Issues in Accounting Education , pp. 1–28, 2023.\n[48] J. Su and W. Yang, ‘‘Unlocking the power of chatgpt: A framework for\napplying generative ai in education,’’ ECNU Review of Education , p.\n20965311231168423, 2023.\n[49] A. Bahrini, M. Khamoshifar, H. Abbasimehr, R. J. Riggs, M. Esmaeili,\nR. M. Majdabadkohne, and M. Pasehvar, ‘‘Chatgpt: Applications, opportu-\nnities, and threats,’’ in 2023 Systems and Information Engineering Design\nSymposium (SIEDS). IEEE, 2023, pp. 274–279.\n[50] A. Tlili, B. Shehata, M. A. Adarkwah, A. Bozkurt, D. T. Hickey, R. Huang,\nand B. Agyemang, ‘‘What if the devil is my guardian angel: ChatGPT as a\ncase study of using chatbots in education,’’ Smart Learning Environments,\nvol. 10, no. 1, p. 15, 2023.\n[51] J. Kocoń, I. Cichecki, O. Kaszyca, M. Kochanek, D. Szydło, J. Baran,\nJ. Bielaniewicz, M. Gruza, A. Janz, K. Kanclerz et al. , ‘‘ChatGPT: Jack\nof all trades, master of none,’’ Information Fusion, p. 101861, 2023.\n[52] S. Siad, ‘‘The Promise and Perils of Google’s Bard for Scientific Re-\nsearch,’’ 2023.\n[53] J. Rudolph, S. Tan, and S. Tan, ‘‘War of the chatbots: Bard, Bing Chat,\nChatGPT, Ernie and beyond. The new AI gold rush and its impact on higher\neducation,’’Journal of Applied Learning and Teaching , vol. 6, no. 1, 2023.\n[54] J. Náplava, M. Straka, P. Straňák, and J. Hajic, ‘‘Diacritics restoration using\nneural networks,’’ in Proceedings of the eleventh international conference\non language resources and evaluation (LREC 2018) , 2018.\n[55] B. N. Ngo and B. H. Tran, ‘‘The vietnamese language learning framework,’’\nJournal of Southeast Asian Language Teaching , vol. 10, pp. 1–24, 2001.\n[56] A. Alkhatlan, F. Kateb, and J. Kalita, ‘‘Attention-based sequence learning\nmodel for arabic diacritic restoration,’’ in 2020 6th Conference on Data\nScience and Machine Learning Applications (CDMA) . IEEE, 2020, pp.\n7–12.\n[57] J. Náplava, M. Straka, and J. Straková, ‘‘Diacritics Restoration using BERT\nwith Analysis on Czech language,’’ The Prague Bulletin of Mathematical\nLinguistics, no. 116, pp. 27–42, 2021.\n[58] Q.-L. Tran, G.-H. Lam, V .-B. Duong, and T.-H. Do, ‘‘A study on diacritic\nrestoration problem in Vietnamese text using deep learning based models,’’\nin 2021 IEEE International Conference on Communication, Networks and\nSatellite (COMNETSAT). IEEE, 2021, pp. 306–310.\n[59] K. N. Lam, T. H. To, T. T. Tran, and J. Kalita, ‘‘Improving vietnamese word-\nnet using word embedding,’’ in Proceedings of the 2019 3rd International\nConference on Natural Language Processing and Information Retrieval ,\n2019, pp. 110–114.\n[60] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‘‘BLEU: a method for\nautomatic evaluation of machine translation,’’ in Proceedings of the 40th\nAnnual Meeting of the Association for Computational Linguistics , 2002,\npp. 311–318.\n[61] D. Q. Nguyen and A.-T. Nguyen, ‘‘PhoBERT: Pre-trained language models\nfor Vietnamese,’’ inFindings of the Association for Computational Linguis-\ntics: EMNLP 2020 , 2020, pp. 1037–1042.\n[62] N. L. Tran, D. M. Le, and D. Q. Nguyen, ‘‘BARTpho: Pre-trained\nSequence-to-Sequence Models for Vietnamese,’’ in Proceedings of the\n23rd Annual Conference of the International Speech Communication As-\nsociation, 2022.\n[63] V . Nguyen, ‘‘Constructing a chatbot using reinforcement learning ap-\nproach. Bachelor Thesis, Can Tho University,’’ 2019.\n[64] T. X. H. Nguyen, T.-T. Dang, A.-C. Le et al. , ‘‘Using large N-gram\nfor Vietnamese spell checking,’’ in Knowledge and Systems Engineering .\nSpringer, 2015, pp. 617–627.\n[65] H. T. Nguyen, T. B. Dang, and L. M. Nguyen, ‘‘Deep learning approach for\nVietnamese consonant misspell correction,’’ in International Conference of\nthe Pacific Association for Computational Linguistics . Springer, 2019, pp.\n497–504.\n[66] D.-T. Do, H. T. Nguyen, T. N. Bui, and H. D. V o, ‘‘VSEC: Transformer-\nbased model for Vietnamese spelling correction,’’ in Pacific Rim Interna-\ntional Conference on Artificial Intelligence . Springer, 2021, pp. 259–272.\n[67] L. Stankevičius, M. Lukoševičius, J. Kapoči ¯ut˙e-Dzikien˙e, M. Briedien ˙e,\nand T. Krilavičius, ‘‘Correcting diacritics and typos with a ByT5 Trans-\nformer model,’’ Applied Sciences, vol. 12, no. 5, p. 2636, 2022.\n[68] T. Nguyen and M. Shcherbakov, ‘‘A neural network based Vietnamese\nchatbot,’’ in 2018 International Conference on System Modeling & Ad-\nvancement in Research Trends (SMART) . IEEE, 2018, pp. 147–149.\n[69] S. Cui, X. Bao, X. Zu, Y . Guo, Z. Zhao, J. Zhang, and H. Chen, ‘‘OneStop\nQAMaker: Extract Question-Answer Pairs from Text in a One-Stop Ap-\nproach,’’arXiv preprint arXiv:2102.12128 , 2021.\n[70] Y . Windiatmoko, A. F. Hidayatullah, and R. Rahmadi, ‘‘Developing FB\nchatbot based on deep learning using RASA framework for university\nenquiries,’’arXiv preprint arXiv:2009.12341 , 2020.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKHANG NHUT LAM received the Bachelor of\nComputer Science degree from the Can Tho Uni-\nversity, Vietnam, in 2005, her Master of Informa-\ntion Technology degree from the Ewha Womans\nUniversity, Korea, in 2009, and the PhD in Com-\nputer Science from the University of Colorado at\nColorado Springs, USA, in 2015. Since 2015, she\nhas been a senior lecturer at the Department of In-\nformation Technology at Can Tho University. Her\nresearch interests include Natural Language Pro-\ncessing, Question-Answering systems, Image Captioning, and Deep Learn-\ning.\nLOC HUU NGUY is a student at the College of\nInformation and Communication Technology at\nCan Tho University. He has participated in several\nresearch projects in Natural Language Processing\nand Question-Answering systems.\nVAN LAM LEis a senior lecturer at the College\nof Information and Communication Technology –\nCan Tho University. He received bachelor’s degree\nin informatics from Can Tho University, master’s\ndegree in information technology from The Uni-\nversity of Newcastle (Australia), and a PhD degree\nin Computer Science from Victoria University of\nWellington. He has worked as a lecturer at Can\nTho University since 2000. Dr. Le’ research is\nmainly focused on network security, IoT, digital\ntransformation, and machine learning.\nJUGAL KALITA is a Professor of Computer Sci-\nence at the University of Colorado, Colorado\nSprings, USA. His research interests are in Natural\nLanguage Processing, Computational Linguistics,\nand Machine Learning including Deep Learning.\nHe, his students and his collaborators have pub-\nlished over 250 papers with more than 12,000 cita-\ntions. He has written several books, the latest being\nMachine Learning: Theory and Practice, published\nby CRC Press in 2023.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3307635\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.9395120739936829
    },
    {
      "name": "Computer science",
      "score": 0.7907106280326843
    },
    {
      "name": "Vietnamese",
      "score": 0.6339333653450012
    },
    {
      "name": "Conversation",
      "score": 0.590131402015686
    },
    {
      "name": "Transformer",
      "score": 0.5373334884643555
    },
    {
      "name": "Natural language processing",
      "score": 0.4845398962497711
    },
    {
      "name": "Dialog system",
      "score": 0.46631723642349243
    },
    {
      "name": "Question answering",
      "score": 0.45510023832321167
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4549240469932556
    },
    {
      "name": "Spelling",
      "score": 0.44445785880088806
    },
    {
      "name": "World Wide Web",
      "score": 0.4404636323451996
    },
    {
      "name": "Dialog box",
      "score": 0.43846607208251953
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3231235146522522
    },
    {
      "name": "Linguistics",
      "score": 0.22471070289611816
    },
    {
      "name": "Engineering",
      "score": 0.10534060001373291
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177733328",
      "name": "Can Tho University",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I888729015",
      "name": "University of Colorado Colorado Springs",
      "country": "US"
    }
  ]
}